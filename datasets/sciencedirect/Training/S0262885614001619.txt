@&#MAIN-TITLE@&#
Eye detection using discriminatory Haar features and a new efficient SVM

@&#HIGHLIGHTS@&#
A discriminating feature extraction (DFE) method for two-class problems is proposed.The DFE method is applied to derive the discriminatory Haar features (DHFs) for eye detection.An efficient support vector machine (eSVM) is proposed to improve the efficiency of the SVM.An accurate and efficient eye detection method is presented using the DHFs and the eSVM.

@&#KEYPHRASES@&#
Discriminatory feature extraction (DFE),Discriminatory Haar features (DHFs),Efficient support vector machine (eSVM),Eye detection,Fisher linear discriminant (FLD),Principal component analysis (PCA),Face Recognition Grand Challenge (FRGC),BioID database,

@&#ABSTRACT@&#
This paper presents an accurate and efficient eye detection method using the discriminatory Haar features (DHFs) and a new efficient support vector machine (eSVM). The DHFs are extracted by applying a discriminating feature extraction (DFE) method to the 2D Haar wavelet transform. The DFE method is capable of extracting multiple discriminatory features for two-class problems based on two novel measure vectors and a new criterion in the whitened principal component analysis (PCA) space. The eSVM significantly improves the computational efficiency upon the conventional SVM for eye detection without sacrificing the generalization performance. Experiments on the Face Recognition Grand Challenge (FRGC) database and the BioID face database show that (i) the DHFs exhibit promising classification capability for eye detection problem; (ii) the eSVM runs much faster than the conventional SVM; and (iii) the proposed eye detection method achieves near real-time eye detection speed and better eye detection performance than some state-of-the-art eye detection methods.

@&#INTRODUCTION@&#
Accurate and efficient eye detection is important for building a fully automatic face recognition system [1–4], and the challenges for finding a robust solution to this problem have attracted much attention in the pattern recognition community [5–19]. Example challenges in accurate and efficient eye detection include large variations in image illumination, skin color (white, yellow, and black), facial expression (eyes open, partially open, or closed), as well as scale and orientation. Additional challenges include eye occlusion caused by eye glasses or long hair, and the red eye effect due to the photographic effect. All these challenge factors increase the difficulty of accurate and efficient eye detection.We present in this paper an accurate and efficient eye detection method using the discriminatory Haar features (DHFs) and a new efficient support vector machine (eSVM). The DHFs are extracted by applying a discriminating feature extraction (DFE) method to the 2D Haar wavelet transform [20–22]. The DFE method improves upon the popular principal component analysis (PCA) and Fisher linear discriminant (FLD) methods. The PCA method is capable of extracting the optimal features for signal or image representation in the sense of mean square error [23]. However, it does not extract discriminatory features for classification [24]. An alternative to the PCA method is the FLD method, which extracts the optimal features for classification by optimizing a criterion on scatter matrices [23]. However, a significant disadvantage of the FLD method is that the maximum number of features it can derive does not exceed the number of classes minus one [23,25]. For a two-class pattern classification problem, the FLD method thus can derive at most one feature, which is usually inadequate for achieving satisfactory classification performance, especially when the problem becomes complex, such as the eye detection problem. The DFE method, based on two novel measure vectors and a new criterion, is capable of extracting multiple discriminatory features for eye detection.The eSVM method is proposed to address the inefficiency problem of the conventional support vector machine (SVM) for eye detection. Since it was introduced, SVM has become a popular tool for two-class classification problems [26–31]. When the classification problem becomes complex, the conventional SVM tends to generate a large number of support vectors, which subsequently leads to the increase of the model complexity. As a result, SVM becomes less efficient due to the expensive computational cost of the decision function, which involves an inner product of all the support vectors for the linear SVM and a kernel computation of all the support vectors for the kernel SVM. The eSVM, by contrast, significantly reduces the number of support vectors by applying only a single slack variable. In addition, a Θ set is introduced into the definition of the eSVM, which consists of the training samples on the wrong side of their margin derived from the conventional SVM. The Θ set plays an important role in controlling the generalization performance. The eSVM therefore improves the computational efficiency upon the conventional SVM without sacrificing the generalization performance.Fig. 1shows the architecture of our eye detection method. First, we apply the Bayesian Discriminating Features (BDF) method [32] to detect a face from an image and normalize the detected face to a predefined size. Second, we use some geometric constraints to extract an eye strip from the upper portion of the detected face. Illumination variations are then attenuated by means of an illumination normalization procedure that consists of Gamma correction, difference of Gaussian filtering, and contrast equalization as applied in [33] and [34]. Third, the DFE method applies the 2D Haar basis functions to derive the discriminatory Haar features (DHFs). Finally, the eSVM classifier applies the DHFs features for eye detection. Usually there are multiple detections around the pupil center. The average of these multiple detections is eventually chosen as the eye location.We evaluate our proposed eye detection method on the Face Recognition Grand Challenge (FRGC) database [35,36] and the BioID face database [10]. Experimental results show that (i) the DHFs exhibit promising classification capability for the eye detection problem; (ii) the eSVM runs much faster than the conventional SVM; and (iii) the proposed eye detection method achieves near real-time eye detection speed and better eye detection performance than some state-of-the-art eye detection methods.We first present in this section the discriminating feature extraction (DFE) method for two-class problems. The discriminatory Haar features (DHFs) are extracted by applying the DFE method to the 2D Haar wavelet transform.The discriminatory feature extraction (DFE) method extracts the most discriminatory features for two-class problems based on two novel measure vectors and a new criterion in the whitened PCA space.LetX∈RNbe a pattern vector in an N dimensional space. Its covariance matrix∑X∈RN×Nis defined as follows:(1)∑X=εX−εXX−εXtwhere ε(⋅) is the expectation operator. The covariance matrix can be factorized into the following form according to PCA [23]:(2)∑X=ΦΛΦwhere Φ=[ϕ1,ϕ2,⋯,ϕN] is an orthogonal eigenvector matrix and Λ=diag{λ1,λ2,⋯,λN} is a diagonal eigenvalue matrix with diagonal elements in decreasing order: λ1≥λ2≥⋯≥λN. To prevent some principal components from dominating others for pattern classification, the whitening transformation spheres the covariance matrix. The whitening transformation matrix W thus is defined as follows:(3)W=ΦΛ−1/2where W∈ℝN×N.We now define two measure vectors and a criterion vector in the whitened PCA space in order to form the DFE transformation matrix. The fundamental of the DFE method is to choose a smaller set of vectors from W with the most discriminatory capability. This smaller set of vectors will form the DFE transformation matrix. Towards that end, we first define the cluster-measure vector, α∈ℝN, and the separation-measure vector, β∈ℝN, as follows:(4)α=P1∑i=1n1sWtxi1−WtM1+P2∑i=1n2sWtxi2−WtM2(5)β=P1sWtM1−WtM+P2sWtM2−WtMwhere P1 and P2 are the prior probabilities, n1 and n2 are the number of samples, and xi(1) and xi(2) are the pattern vectors of the first and second classes, respectively. M1, M2, and M are the mean vectors of the two classes, and the grand mean, respectively. The s(⋅) function defines the absolute value of the elements of the input vector. The significance of these new measure vectors is that the cluster-measure vector, α∈ℝN, measures the clustering capability of the projection vectors in W, whereas the separation-measure vector, bfβ∈ℝN, measures the separating capability of the vectors in W.In order to choose the vectors from W with the most discriminatory capability, we further define a new criterion vector γ∈ℝNas follows:(6)γ=β./αwhere ./ is element-wise division. The value of the elements in γ indicates the discriminatory power of their corresponding vectors in W: the larger the value is, the more discriminatory power the corresponding vector in W possesses. We therefore choose the top m vectors, Wi1,Wi2,⋯,Wim, in W corresponding to the m largest values in γ to form the DFE transformation matrix T:(7)T=Wi1Wi2⋯Wimwhere T∈ℝN×mand m<N. The DFE features are thus defined as follows:(8)Y=TtX.The DFE features thus reside in a low dimensional space ℝmand capture the most discriminatory information of the original dataX.In comparison with the PCA method, which extracts the most representative features by choosing the top eigenvectors from Φ to form the transformation matrix, the DFE method extracts the most discriminatory features by choosing the top vectors from W according to their corresponding values in γ to form the transformation matrix.In comparison with the FLD method, the DFE method is capable of extracting multiple features in a dimensional space ℝmfor two-class problems. The FLD method looks for a projection matrix Q that maximizes the criterion J(Q)=|QtSbQ|/|QtSwQ|, where Swand Sbare the within-class and between-class scatter matrices, respectively [23]. Mathematically, this criterion is maximized when Q consists of the leading eigenvectors of Sw−1Sb. According to the definition of between-class scatter matrix Sb[23], the rank of Sbis at most L-1 for any L class problems. Consequently, the rank of Sw−1Sbis at most L-1 as well. Therefore, there are at most L-1 valid eigenvectors of Sw−1Sb, which means that the FLD can only derive at most L-1 valid feature for any L class problems. For two-class problems, the FLD method can only derive a single valid feature, which is significantly inadequate for achieving satisfactory performance.The discriminatory Haar features (DHFs) are derived by applying the DFE method to the 2D Haar wavelet transform. Haar wavelet transform has broad applications in pattern recognition and data interpretation, such as cloud classification [37], object detection [38], face detection [39,40], as well as data visualization [41]. The 2D Haar wavelet transform is defined as the projection of an image onto the 2D Haar basis functions [21]. The 2D Haar basis functions display attractive characteristics such as enhancing local contrast and facilitating feature extraction. These characteristics become even more profound when the 2D Haar wavelet transform is applied to eye detection, where a dark pupil is in the center of a colored iris that is surrounded by white sclera.The 2D Haar basis functions are defined as the tensor product of the one dimensional Haar scaling and wavelet functions [20]. The detailed information about the one dimensional Haar scaling and wavelet functions can be found in [21] and [22]. From one aspect of view, the 2D Haar basis functions include a set of scaled and shifted box type functions that encode the differences in average intensities among the regions in different scales [38]. Fig. 2shows a family of 2D Haar basis functions with the size of 8×8, where white, black, and gray represent 1, −1, and 0, respectively. Fig. 2 reveals that the 2D Haar basis functions contain mainly three types of representations in different scales and locations: (i) two horizontal neighboring regions for computing the difference between the sum of the pixels within each of them, (ii) two vertical neighboring regions for computing the difference between the sum of the pixels within each of them, and (iii) four neighboring regions for computing the difference between the diagonal pairs of the regions. Note that the first basis function is for computing the average of the whole image. Since the eye regions possess remarkable intensity and gradient variations, the intensity difference encoding scheme of the 2D Haar wavelet basis functions is shown to be effective to capture the characteristic of eyes in our research.The 2D Haar wavelet features usually reside in a high dimensional space and are highly correlated with each other. The high dimensionality renders the classifier implement such as SVM intractable, and the poor discriminating features weaken the performance of the classifier. We therefore apply the DFE method to the 2D Haar wavelet transform to derive the discriminatory Haar features (DHFs), which reside in a low dimensional space and possess the discriminatory capability. The approach of generating the DHFs is described in Algorithm 1.Algorithm 1. The approach of generating the DHFsInput: An image column vectorX∈Rnr×ncand a family of 2D Haar basis functionsΨ∈Rnr×nc×N, where nrand ncdenote the number of rows and columns of the image, and N denotes the number of 2D Haar basis functions, respectively.Output: The discriminatory Haar features (DHFs)Z∈Rm.Step 1: Compute the 2D Haar wavelet featuresY∈RN:Y=ΨtX.Step 2: Given the 2D Haar wavelet featuresY∈RN, follow the DFE method to compute the DFE transformation matrix T∈ℝN×min Eq. (7).Step 3: Compute the DHFsZ∈Rm:Z=TtY=ΨTtX.Even though the conventional support vector machine (SVM) can achieve satisfactory performance on the eye detection problem, the SVM based eye detection methods usually have slow speed due to the expensive computational cost of the decision function and the excessive number of pixels over an image (in our experiment, each eye strip performs 440 classifications to locate the eyes even after an image downsampling). We therefore present in this section an efficient SVM (eSVM) that significantly improves the computational efficiency upon the conventional SVM without sacrificing the generalization performance. Before we present the eSVM, we first analyze the factors leading to the inefficiency problem (i.e., the large number of the support vectors) of the conventional SVM.Let the training set be {(x1,y1),(x2,y2),⋯,(xl,yl)}, where xi∈ℝn, yi∈{−1,1} indicate the two different classes, and l is the number of the training samples. The conventional SVM defines an optimal separating hyperplane, wtx+b=0, by minimizing the following functional:(9)minw,b,ξi12wtw+C∑i=1lξi,subjecttoyiwtxi+b≥1−ξi,ξi≥0,i=1,2,⋯,lwhere ξi≥0 refers to slack variables and C>0 is a regularization parameter.The Lagrangian theory and the Kuhn–Tucker theory are then applied to optimize the functional in Eq. (9) with inequality constraints [42]. The optimization process leads to the following quadratic convex programming problem:(10)maxα∑i=1lαi−12∑i,j=1lαiαjyiyjxixjsubjectto∑i=1lyiαi=0,0≤αi≤C,i=1,2,…lThe decision function of the SVM is therefore derived as follows:(11)fx=signwx+b=sign∑i∈SVyiαixix+bwhere SV is the set of support vectors (SVs), which are the training samples with nonzero coefficients αi. Eq. (11) reveals that the computation of the decision function involves an inner product of all support vectors (note that the computation of the decision function for the kernel SVM involves a kernel computation of all support vectors). Therefore, the computation cost of the decision function is positively correlated to the number of support vectors. When the number of support vectors is large, the computation cost of the inner product will become expensive and the computational efficiency of the conventional SVM will be compromised.According to the Kuhn–Tucker theory, we have the following conditions for the conventional SVM:(12)αiyiwtxi+b−1+ξi=0,i=1,2,⋯,l.Eq. (12) shows that if αi≠0, then yi(wtxi+b)−1+ξi=0. Therefore, the training samples that satisfy yi(wtxi+b)−1+ξi=0 are support vectors for the conventional SVM. The intuitive interpretation of the support vectors is that they are the training samples that lie on their boundaries or the samples pulled onto their boundaries by the slack variables ξias shown in Fig. 3. In fact, Fig. 3 shows that all the training samples on the wrong side of their margin become support vectors because of the slack variables, which pull the training samples onto their boundaries to make them support vectors. As a complex pattern classification problem often has a large number of the training samples on the wrong side of their margin, the number of support vectors becomes quite large, which significantly reduces the computational efficiency of the conventional SVM.The conventional SVM usually derives a large number of support vectors, because all the training samples on the wrong side of their margin become support vectors as the slack variables pull these samples to their boundaries. The eSVM, however, reduces the number of support vectors significantly because only a small number (can be as few as one) of the training samples on the wrong side of their margin are pulled to their boundaries to become support vectors.Specifically, the eSVM follows a two-round optimization process. The first round of the optimization, which is for the conventional SVM, is used to define a Θ set that corresponds to the training samples on the wrong side of their margin derived from the conventional SVM. The Θ set plays an important role in controlling the generalization performance in the second round of the optimization. The second round optimization, which introduces a single slack variable for all the training samples in the Θ set, finally defines the support vectors and the decision function of the eSVM.The second optimization functional is defined as follows:(13)minw,b,ξ12wtw+Cξ,subjecttoyiwtxi+b≥1,i∈Ω−Θyiwtxi+b≥1−ξ,i∈Θ,ξ≥0where Θ is the set of the training samples on the wrong side of their margin derived from the conventional SVM, and Ω is the set of all the training samples. Compared with the conventional SVM that introduces the slack variables with different values, this optimization functional specifies a fixed value for all the slack variables. The first inequality constraint in Eq. (13) ensures that the training samples on the right side of their margin in the conventional SVM are still on the right side in the eSVM. The second inequality constraint in Eq. (13) ensures that only a small number of training samples in the Θ set become support vectors due to the introduction of the single slack variable that pulls most of the training samples in the Θ set beyond their margin to the right side and thus become non-support vectors. The significance of the second round optimization is thus to simulate the maximal margin separating boundaries of the conventional SVM by using much fewer support vectors.Further analysis on the first inequality constraint in Eq. (13) reveals that this constraint makes the eSVM to maintain a similar maximal margin separating boundary with that of the conventional SVM. The definition of the separating boundary wtx+b=0 is associated with the definition of the maximal margin wtx+b=±1. Given the separating boundary wtx+b=0, the maximal margin is fixed to wtx+b=±1, and vice versa. The first inequality constraint in Eq. (13) ensures that the training samples on the right side of their margin in the conventional soft-margin SVM are still on the right side in the eSVM. If the eSVM derives a separating boundary that is significantly different from the one derived by the conventional SVM, this constraint will not be satisfied. As the eSVM derives a similar separating boundary with the conventional SVM, the maximal margin of the eSVM is thus similar with that of the conventional SVM as well — neither degrade nor upgrade much from that of the SVM. Consequently, the eSVM inherits the advantage of generalization performance of the conventional SVM, and has comparable classification performance with the SVM.The second inequality constraint in Eq. (13) plays the significant role of reducing the number of support vectors. This constraint ensures that only a small number of training samples in the Θ set become support vectors due to the introduction of the single slack variable that pulls most of the training samples in the Θ set beyond their boundary to the right side and thus become non-support vectors. It is possible that support vectors lying on their boundaries for the eSVM are a little bit more than those for the SVM. However, majority of support vectors for the SVM come from the samples on the wrong side of their margin (i.e., the Θ set). This is because the chance that samples happen to fall on their boundaries is significantly lower than the chance that samples fall onto the right side or wrong side of their margin. Therefore, even though support vectors on their boundaries for the eSVM may increase a little bit upon those for the SVM, the total number of support vectors for the eSVM is still significantly less than that for the SVM.As the optimization problem of the eSVM defined in Eq. (13) is different from that of the conventional SVM, its corresponding dual mathematical problem after applying the Lagrangian theory and the Kuhn–Tucker theory is also different from the ones derived in the conventional SVM. In particular, let α1,α2,⋯,αl≥0 and μ≥0 be the Lagrange multipliers; the primal Lagrange functional is defined as follows:(14)Fwbξαiμ=12wtw+Cξ−∑i∈Ω−Θαiyiwtxi−b−1−∑i∈Θαiyiwtxi−b+ξ−1−μξ=12wtw+C−∑i∈Θαi−μξ−∑i∈Ωαiyiwtxi−b−1Next, we maximize the primal Lagrange functionalF(w,b,ξ,αi,μ) with respect to w, b, and ξ as follows:(15)∂F∂w=w−∑i∈Ωαiyixi=0⇒w=∑i∈Ωαiyixi(16)∂F∂b=∑i∈Ωαiyi=0(17)∂F∂ξ=C−∑i∈Θαi−μ=0and then, we derive a convex quadratic programming model by substituting Eqs. (15), (16), and (17) into Eq. (14) as follows:(18)maxα∑i∈Ωαi−12∑i,j∈Ωαiαjyiyjxixjsubjectto∑i∈Ωyiαi=0,∑i∈Θαi≤C,αi≥0,i∈Ω.Furthermore, we have the following constraints for the eSVM from the Kuhn–Tucker theory:(19)αiyiwtx+b−1=0,i∈Ω−Θαiyiwtx+b−1+ξ=0,i∈Θ.Eq. (19) shows that if αi≠0, then either yi(wtx+b)−1=0,i∈Ω−Θ or yi(wtx+b)−1+ξ=0,i∈Θ. The training samples that satisfy either yi(wtx+b)−1=0,i∈Ω−Θ or yi(wtx+b)−1+ξ=0,i∈Θ are support vectors for the eSVM. Therefore, the intuitive interpretation of the support vectors is the training samples that lie on their boundaries or the samples pulled onto their boundaries by the slack variable ξ as shown in Fig. 4. As all the slack variables in the eSVM have the same value, Fig. 4 also reveals that only a small number (can be as few as one) of the training samples on the wrong side of their margin (i.e., samples in the Θ set) are pulled onto their boundaries to become support vectors, while the others are not support vectors because they are pulled to the right side of their margin but do not fall onto the boundaries.To further explain the advantages of the eSVM, we intuitively compare the eSVM with the conventional SVM on a two dimensional synthetic data set — the Ripley data set [43–45]. The Ripley data set defines a two-class non-separable problem, and the numbers of training and testing samples are 250 and 1000, respectively. A linear kernel and a nonlinear (RBF) kernel are applied [44,45].Fig. 5plots the training samples, the support vectors, and the separating boundaries of the conventional SVM and the eSVM with the linear and RBF kernels, respectively. Fig. 5 reveals that the eSVM has almost identical separating boundaries with the conventional SVM using either the linear kernel or the RBF kernel. This is consistent with the theory that the eSVM simulates the maximal margin separating boundaries of the conventional SVM in order to maintain its generalization performance. Fig. 5 also reveals that the number of support vectors of the eSVM is much smaller than that of the conventional SVM. Specifically, Fig. 5(a) and Fig. 5(c) show that the support vectors of the conventional SVM, which are represented by red crosses, are mostly the samples on the wrong side of their margin. The number of support vectors thus is large due to the fact that many training samples are on the wrong side of their margin for the non-separable problems. In contrast, for the eSVM, only a small number of the training samples on the wrong side of their margin, as shown in Fig. 5(b) and Fig. 5(d), become support vectors.Table 1shows the comparison of the SVM and the eSVM on the number of the support vectors, the slope and the y-intercept of the separating boundaries using the linear kernel, as well as the classification rate and running time for the testing data set. In particular, the eSVM reduces the number of support vectors by 88.76% and 75.64%, when compared with the conventional SVM using the linear and RBF kernels, respectively. Consequently, the running time of the eSVM is also reduced compared with the SVM when the same kernel is applied. The similar slope and the y-intercept values of the separating boundaries between the eSVM and the conventional SVM using a linear kernel indicate that they define similar separating boundaries, and hence have comparable generalization performance. Specifically, the classification rate of the eSVM on the testing data set is the same with that of the SVM using the linear kernel, but the classification rate of the eSVM is 0.5% higher than that of the SVM when using the RBF kernel.

@&#CONCLUSIONS@&#
