@&#MAIN-TITLE@&#
Automatic selection of color constancy algorithms for dark image enhancement by fuzzy rule-based reasoning

@&#HIGHLIGHTS@&#
A fuzzy rule-based system operates as a selector of color constancy algorithms.The system selects among the White-Patch, Gray-World and Gray-Edge algorithms.The method attains a high rate of correct selection according to the actual scene.Two problems are addressed simultaneously: color constancy and color enhancement.The framework can be used in engineering applications, like video surveillance.

@&#KEYPHRASES@&#
Fuzzy modeling,Rule-based system,Image processing,Color enhancement,Color constancy,

@&#ABSTRACT@&#
This work introduces a fuzzy rule-based system operating as a selector of color constancy algorithms for the enhancement of dark images. In accordance with the actual content of an image, the system selects among three color constancy algorithms, the White-Patch, the Gray-World and the Gray-Edge. These algorithms have been considered because of their accurate remotion of the illuminant, besides showing an outstanding color enhancement on images. The design of the rule-based system is not a trivial task because several features are involved in the selection. Our proposal consists in a fuzzy system, modeling the decision process through simple rules. This approach can handle large amounts of information and is tolerant to ambiguity, while addressing the problem of dark image enhancement. The methodology consists in two main stages. Firstly, a training protocol determines the fuzzy rules, according to features computed from a subset of training images taken from the SFU Laboratory dataset. We choose carefully twelve image features for the formulation of the rules: seven color features, three texture descriptors, and two lighting-content descriptors. In the rules, the fuzzy sets are modeled using Gaussian membership functions. Secondly, experiments are carried out using Mamdani and Larsen fuzzy inferences. For a test image, a color constancy algorithm is selected according to the inference process and the rules previously defined. The results show that our method attains a high rate of correct selection of the most well-suited algorithm for the particular scene.

@&#INTRODUCTION@&#
Rule-based systems allow representing knowledge, and capturing the personal expertise in a set of IF-THEN rules. In the rules, a set of premises is evaluated for concluding a result. Rule-based systems have shown to be useful in a number of applications [1–3]. Moreover, such systems increase their flexibility, robustness and interpretability, when fused with soft computing techniques [4] like fuzzy logic, which was introduced as an extension of the classical theory of sets [5,6]. Nowadays, fuzzy logic has been recognized as an effective tool for managing information in rule-based systems [7,8] because of its tolerance to imprecision, to ambiguity and to the lack of information, as occurs in tasks such as the extraction of high-level visual information. In our particular case, we determine the algorithm most well-suited to be applied on an image, according to the color content of the scene under evaluation.Color is an important feature for pattern recognition and computer vision fields. Typical computer vision applications related to this study include feature extraction [9], image classification [10], object recognition [11,12], scene categorization [13,14], human–computer interaction [15] and color appearance models [16]. Color also represents an attribute of visual sensation and appearance of objects. It depends on three components: the reflectance of an object, the sensitivity of the cones in the eyes, and the illuminant. For a robust color-based system, the effects generated by the illumination should be removed.The ability of a system to recognize the true colors in objects, independently of the illuminant present in a scene, is known as Color Constancy [17]. The human visual system has this innate capability to correct the color effects of the light source. However, the emulation of the same process is not trivial for machine vision systems in an unknown scene [18]. From the computational point of view, color constancy is defined as the transformation of an input image captured under an unknown lighting, to another picture apparently obtained under a known lighting, normally daylight [14]. For this reason, it is required to estimate the color of the light source in the image. The color values, corresponding to the illuminant, are used to transform the input image.In past decades, researchers have tried to solve the color constancy problem using several methods. Nonetheless, and in spite of the wide range of computer vision applications that require color constancy, a general solution to this problem has not been found. A number of algorithms using simple image statistics have been proposed, like the White-Patch [19,20], the Gray-World assumption [21], Shades-of-Gray [22], Gray-Edge [23], Spatio-Spectral [24] and Category by Correlation [25].The color image enhancement is a very challenging research field, in comparison to the image enhancing in gray scale. A detailed and clear overview about image enhancement issues is the one by Lucchese and Mitra [26]. It is important to mention that color constancy algorithms enhance the chromatic content of images, although they have been originally developed just for the color estimation of a light source. Provenzi et al. [27] already explored the use of two color constancy algorithms for color image enhancement purposes. In particular, their work was oriented to local contrast enhancement using the White-Patch and the Gray-World algorithms in combination with an automatic color equalization technique. Image enhancement using color constancy algorithms seems to be more significant when these algorithms are applied on dark images, a premise taken into consideration in this study.Some research works have been oriented to the selection of color constancy algorithms according to several features. The use of content-based image analysis for automatic color correction was originally proposed by Schroder and Moser [10]. This study classified the images into a number of classes (e.g. indoor, outdoor, vegetation, mountains), associated to a particular algorithm (White-Patch and Gray-World). Gasparini and Schettini [11] proposed a method for the analysis of a cast index in the images and their classification. This classification (e.g. skin, sky, sea, vegetation) also allows to detect the presence of a possible predominant color. The work by van de Weijer et al. [12] uses high-level visual information in order to model images as a mixture of semantic classes (e.g. skin, road, buildings, grass). This latter study, utilizes the visual information for selecting the best-suited color constancy algorithm. Bianco et al. [13] proposed classifying images into outdoor and indoor categories in order to select the best color constancy algorithm for each scenery. Later, they implemented an automatic selector for color constancy algorithms taking into account low-level properties of the images [28]. Most recently, Faghih and Moghaddam [29] used a classifier in order to determine the best group of color constancy algorithms for an input image and then, some of the algorithms in this group are combined using multi-objective Particle Swarm Optimization (PSO). It is important to note that these research works have been mainly addressed to the estimation of the illuminant. However, to our knowledge, there are no selection systems of color constancy algorithms focused on image enhancement purposes.In this work, a fuzzy rule-based system is proposed for the selection of one out of the three basic color constancy algorithms aforementioned: the Gray-World, the White-Patch and the Gray-Edge. This framework is a threefold approach to solving the problem. (1) Our study is focused on the color enhancement using color constancy algorithms, at the same time that the remotion of the influence of the illuminant is solved. Besides, it is particularly focused on processing dark images. (2) An important problem for developing the rule-based system is the correct choice of the image features. Twelve low-level features are chosen carefully: seven color features, three texture descriptors, and two lighting-content features. (3) We use a set of fuzzy rules, encoding the knowledge necessary to take a decision about the most well-suited algorithm to be applied to an image under consideration. In order to perform the selection, a test image is submitted to an inference process, where the best algorithm is chosen if one of its corresponding rules gets the highest firing strength.The rest of this paper is organized as follows. Section 2 presents the proposed framework and introduces the image dataset used and the color constancy algorithms. Section 3 details the image features used and, in Section 4 the design of the fuzzy rule-based system is explained. Section 5 shows the results obtained by experimental tests. Finally, the conclusions are presented in Section 6.We introduce a rule-based system for the selection of a color constancy algorithm, in order to attain chromatic improvement in a given scene, particularly a dark one. Our approach consists in a MISO (Multiple Input, Single Output) system, including twelve inputs computed from the scene under analysis and a single output, the label of the corresponding algorithm. Inputs and outputs are linguistic terms, linked through a set of IF-THEN rules.Basically, our framework has been divided into two main stages. On one hand, a training protocol determines the fuzzy rules, according to features computed from a set of training images. As said before, we use twelve low-level image features for the selection process: seven color features, three texture descriptors, and two lighting-content descriptors. On the other hand, in a testing mode, given a test image, the best algorithm is chosen according to the rule evaluation in two inference models, Mamdani [30] and Larsen [31]. Fig. 1shows a flowchart of the methodology in our study.In order to develop the rule-based system, we use images from the SFU Laboratory dataset [32]. This dataset is conformed by 529 dark images under controlled illuminant. The complete database contains 22 scenes with minimal specularities, 9 scenes with dielectric specularities, 14 scenes with metallic specularities and 6 scenes with at least one fluorescent surface. It is important to note that this dataset was developed for color constancy purposes. However, it is very suitable for testing the color enhancement in dark images.The three algorithms that are considered for the selection are•White-Patch (WP),Gray-World (GW),1st order Gray-Edge (GE1).The three color constancy algorithms considered assume that the illumination is uniform across the scene. The relationship of the color intensity under a light source in an image is given by(1)fi(x,y)=G(x,y)Ri(x,y)Ii,where fi(x, y) is the pixel intensity at the position (x, y), G(x, y) is a geometry factor, Ri(x, y) is the reflectance of the object, Iiis the illuminant, and i corresponds to the color channel.Once a color constancy algorithm is applied over an image fi(x, y), the outcome, oi(x, y), just depends on G(x, y) and Ri(x, y). Color constancy algorithms assume that the output imagesoi(x,y)=G(x,y)Ri(x,y)Ii′, are influenced by a white light source, where I′={1, 1, 1} is the illuminant in the output. Then, the relation between the output and the input is(2)oi(x,y)=G(x,y)Ri(x,y)=fi(x,y)Ii.The Retinex algorithm was proposed by Land and McCann [20]. This algorithm, in its simplest form, is called White-Patch Retinex (WP) [19], which takes into account the highest value in each color channel as the white representation for the image. Computationally, such values are found from the maximum intensity in each channel(3)Ii=max{fi(x,y)}.Later, all pixel intensities are scaled according to the illumination computed using Eq. (2). Finlayson et al. [33] improved this algorithm using a 99% of percentage in the histogram for each color channel for the estimation of the illuminant; this improvement is the WP algorithm considered in our study.The Gray-World assumption (GW) is the most popular algorithm for color constancy. Proposed by Buchsbaum [21], it is based on the assumption that, on average, the colors found in real world images tend to a gray tone, and hence the illuminant is estimated using the average color of all pixels.Basically, this algorithm consists in computing the illuminant based on ai=mean{fi(x, y)}. Some assumptions are proposed in [34] for simplifying the method, and the illuminant is adopted as(4)Ii≈2ai.Because oi(x, y)=fi(x, y)/Ii, the outcome image for this algorithm is given by(5)oi(x,y)=fi(x,y)2ai.The Gray-Edge hypothesis (GE) was proposed as an alternative to the Gray-World assumption [23]. This theory considers that the illuminant is a pth Minkowski norm of the reflectance differences in a scene. This method is based on the Shades-of-Gray algorithm [22], which is adopted adding the spatial derivative of an image and is given by(6)Ii=∑x∑yhiσ(x,y)pMN1/pwherehiσ=((∂nfix)2+(∂nfiy)2)1/2⊗Gσ. M and N are the columns and rows, respectively, and Gσis a local smoothing with a Gaussian filter with standard deviation σ.Eq. (6) describes a method producing different estimations for the illuminant color, based on three variables, (1) the order n of the derivative in the image, (2) the Minkowski norm p, and, (3) the scale of the local smoothing σ. In our study, we use n=1, p=7 and σ=4, according to the recommendation made by van de Weijer et al. [23].A quality measure is fundamental in order to pre-select the images, in the training process for learning the rules, and in the testing protocol in order to compare and assess the performance of our system. The measure used allows us to find the outcome image with the best chromatic adaptation. Shortly after, we can conclude the algorithm that achieves the best enhancement on a particular image.There does not exist a well-established measure without reference for evaluating image color enhancement. Human beings have a natural predisposition to qualify positively an image with intense colors [35]. To understand this, philosophical and psychological aspects of the human mind have been studied. For example, Joshi et al. [35] described aspects about aesthetics and emotions present in images, including references to understand the human mind. Some research works have tried to establish simple measures in this field. Also, we carried out experimental tests, and it was observed that the measure of the Average Chroma [36] provides meaningful information about the quality of color images. This measure is also an image feature in this study and will be discussed in the following section.For this study, 100 images randomly taken from the SFU database are used as training data and the rest 429 for testing purposes. The three algorithms under consideration are applied to the training images. Thereby, the quality measure is computed from the outcomes for each image and thereafter, the three measures are compared. The highest value corresponds to the best algorithm for the particular image. Thus, it is possible to know the best algorithm for each image. This pre-selection will be our ground truth from now on, and we start the learning process for the selector. Table 1illustrates with examples the correct class determined in the training process.The a priori probability of each algorithm for being the best choice was estimated. 48% of the cases, the GW assumption was the best option. In more than one third, 36% of the images, the WP algorithm corresponds to the best choice. The remaining 16% of the images was best processed by the GE1 algorithm.Features are used to describe images numerically and, they should provide information to represent a scene. If the features extracted are chosen carefully, it is expected that the feature set will extract the relevant information from the image in order to perform the desired task using this reduced representation instead of using the full image. The features considered for this work are mainly related to the description of color content. Some of these features have been used in similar studies [28,13]. Furthermore, additional features related to textural description and lighting-content in a scene are considered. We use twelve features in this work. Seven associated with color, three of textural content, and two of lighting-content.Different color descriptors have been used in previous works for measuring the color of an image. We can mention statistical moments of color components in a color space and color histograms [28], cast indices [11] and spectral information [37], among others.The color features used in our system as image descriptors are the following:•Number of colors.Average power spectrum value (APSV).Cast indices (σ, D and Dσ).Average chroma (μcR).Probability of dominant color (PDC).In addition, it is important to note that the RLAB color space is used for the extraction of five out of the seven color features. This space is an extension of the CIELAB perceptual color space [39,38], which incorporates a more accurate model of chromatic adaptation, especially under extreme light conditions. This space is perceptually uniform, and the chromatic components, a* and b*, are not correlated with the lightness component L*. The three cast indices, the average chroma and the percent of dominant color are the features computed in the RLAB color space. The transformation equations from RGB into RLAB can be seen in [16,39]. The cylindrical coordinates of RLAB are given by(7)hR=tan−1bRaR,(8)cR=(aR)2+(bR)2,where aRand bRare the chromatic components in RLAB. Also, hRand cRare the Hue and Chroma, respectively. Thereby, some features are computed according to these components.The number of different colors is our first feature and is related to the color range of the image. Since a number of color constancy algorithms are based on the Gray-World assumption, the number of colors is a referent of whether this assumption holds true or not. The actual colors of the pixels may cancel this assumption, but if an image contains diverse colors, then the average color is likely to be a gray value. For the RGB space, and considering 8 bits per channel, it is possible to quantize a number of 16 million colors (28×28×28). Bianco et al. [13] used a similar approach to obtain the number of colors in an image.The Power Spectrum Metric [37] has been used for quality image assessment and for determining the chromatic information in the image. Transforming an image into the Fourier domain enables signal analysis in the frequency domain, allowing the analysis of information that is not evident in the spatial domain. In this work, the classical Discrete Fourier Transform is used for the conversion.The Average Power Spectrum Value (APSV) is given by(9)APSV=13∑i|Fi|2,where i stands for the color component in RGB space and |F|2 is given by|F|2=(1/MN)∑u∑v|F(u,v)|2, where M and N are the number of columns and rows, respectively.F(u,v)is the image in the Fourier domain. It is important to note that the APSV indicates the quality of each image. That is, the higher the APSV is, the better the quality is. Moreover, the APSV is a no-reference image quality metric. The APSV tends to be higher when the image shows a high chromatic content under a natural illumination.The cast indices are features for the analysis of the color distribution in the image. These indices were proposed by Gasparini and Schettini [11] for image classification (e.g. skin, sky, sea, vegetation). Also, these indices have been used in similar works [28,13].The computation of the cast indices should be made in a color space where the luminance and the chroma components are independent. Firstly, the image is transformed into the RLAB space and the procedure proposed in [11] is carried out. Thus, the means and the variances of the aRand the bRcolor components in RLAB (μaR,μbR,σaR2andσbR2) are used for computing the first cast index(10)σ=σaR2+σbR2,that corresponds to the radius of an Equivalent Circle (EC) with center inC=(μaR,μbR). The other two cast indices are given by(11)D=μ−σ,(12)Dσ=D/σ,whereμ=μaR2+μbR2. D is a measure of how far the color distribution of the EC lies from the neutral axis (aR=0, bR=0). Dσquantifies the strength of the cast.Chroma is an attribute related to the intensity of the colors. This cue corresponds to a component of the RLAB color space in cylindrical coordinates. Also, Chroma is a vector representing the magnitude between aRand bRcomponents in this color space,cR=(aR)2+(bR)2. The higher the chroma value is, the more intense the color is. A study about the Chroma as a measure of quality color images is given in [36]. We use this measure as a simple indicator of the quality perceived by humans, and also as a color descriptor. The Average Chroma is also used in the training stage for the evaluation of the images(13)μcR=mean{cR(x,y)}.The Probability of Dominant Color (PDC) can be extracted from a color histogram. We compute a histogram from the Hue component of RLAB. This histogram has 12 bins, where each bin registers 30 successive degrees of Hue. For example, the first bin has the occurrence of pixels in Hue between 0 and 29 degrees, the second bin registers between 30 and 59 degrees, etc. Afterwards, the probability of each bin is computed as the ratio between the number of occurrences and the total number of pixels in the image.PHRcorresponds to the probability distribution function of the Hue. The maximum probability registered into a bin is given by(14)PDC=max{PHR}.The use of texture descriptors is inspired by the study of Bianco et al. [28]. They used some features related to texture analysis and considered that these features could describe the composition of the image, regardless of whether the color issues were studied. Nonetheless, we take into account other texture features proposed by Haralick et al. [40], but computed using the Sum and Difference Histograms (SDH), the faster alternative proposed by Unser [41]. The relative displacement vector (V) between two picture elements is a SDH parameter, and in our study it is defined as the composition of the Cartesian product R×θ, where R={1, 2} and θ={0, π/4, π/2, 3π/4}.The three texture features considered are the following(15)Entropy=−∑iPs(i)·log(Ps(i))−∑jPd(j)·log(Pd(j))(16)Contrast=∑jj2·Pd(j)(17)Homogeneity=∑j(1+j2)−1·Pd(j)where Psand Pdare the normalized SDH. It is important to mention that these texture features are computed using just the lightness component L* of the RLAB space.Two lighting features are considered in this study.•Average lighting (μLR).Probability of Specularity (PS).(18)μLR=mean{LR(x,y)}Specularity refers to the amount of light that is emitted by reflection of an object. Commonly a dark image has low Probability of Specularity. It is obtained from the luminance histogram (0–100 in RLAB) and normalized. Thereby the probability density function (pdf) of the luminance is obtained. We considered the cumulative distribution function (cdf) between the 91st and 100th bins,(19)PS=∑i=91100PLR(i),wherePLRis the luminance histogram and i is the index of the bin in the histogram.The determination of a rule database is a key aspect of the proposed system. These rules are significantly important because they contain the necessary information for taking a decision about the correct algorithm to be applied. In a training protocol, we need to define the linguistic terms and fuzzy sets. Later, the rules are formulated and tuned for improving the selection. Posteriorly, for an image under test they are used in an inference model in order to choose the best algorithm.The first step for the development of our system consists in the definition of the functional and operational characteristics. Such characteristics include the linguistic terms, the formulation of the rules and the inference model. One set of i inputsX=[x1,x2,…,xi]T∈Ri, is considered for the selection, corresponding to a feature vector with i elements, where a given element is a quantitative feature. Twelve features (see Table 2) were included in the vector X: Number of colors (x1), APSV (x2), σ of the EC (x3), D of the EC (x4), Dσof the EC (x5), Average Chroma (x6), Probability of a dominant color (x7), Entropy (x8), Contrast (x9), Homogeneity (x10), Average lighting (x11), and Probability of Specularity (x12). The output of the system (Ω) takes a value from a setΩ={ω1, ω2, ω3} corresponding to labels of 3 known classes. The output classes represent the color constancy algorithm considered in the selection: WP (ω1), GW (ω2) and GE1 (ω3).Once all the features are extracted for the whole training set, these are normalized between 0 and 1, according to the minimal and maximal values registered for each feature. For each training image, its feature vector was extracted. This feature vector was normalized between 0 and 1, according to the minimum and maximum values of each feature during the training stage (see Table 3).The next step in the learning process, is the definition of the linguistic terms and their corresponding fuzzy sets. From the feature vector X, each input feature xiis a value on the domain of the linguistic term, partitioned in a number of fuzzy sets. Next, the definition of the fuzzy sets will be needed.The definition of fuzzy sets represents one of the most important steps in the design process. According to our experience, linguistic terms should be separated into 4 fuzzy sets in order to obtain a good resolution and achieve an adequate decision capability avoiding an intermediate linguistic term. Fig. 3shows how four Gaussian fuzzy sets are distributed across the feature domain, xi→{0, 1}, considering a standard deviation of σ=0.1.We use Gaussian functions, with amcenters and σ=0.1 with m={1, 2, 3, 4}. The fuzzy set conformation is done using 4 clusters per feature over each of the linguistic variables. The amcluster center, is the maximum degree of membership in the A(i,m) fuzzy set. For specific cases, the A(i,m) labels are replaced by intuitive terms as A(i,1)= “VERY LOW”, A(i,2)= “LOW”, A(i,3)= “HIGH” and A(i,4)= “VERY HIGH”. For the first Gaussian function A(i,1), its left side remains with membership value of 1 for lower values in the domain of xi. Similarly occurs for the last function A(i,4), but now to the right side. Table 4shows the nucleus with maximum degree of membership (center of Gaussians) for each feature.An important step in the learning process is the knowledge acquisition, also known as rule formulation. In this approach, inputs and outputs are represented by linguistic terms related by an IF-THEN rule. These rules are expressions used in an inference process such that if a set of facts is known (antecedents), an algorithm (consequent) can be inferred. The generic form of the nth of these rules is:Rn:IFx1isA(1,m)ANDx2isA(2,m)AND…ANDxiisA(12,m)THENΩisωp,where, A(i,m) is the mifuzzy set in the ith feature for the nth rule, and ωprepresents the pth consequent fuzzy set in the nth rule. Rnis the nth rule that belongs to the set of rules R.The methodology for the formulation of the nth rule is the following. Firstly, the corresponding images are identified in the pth class in the training set. Afterwards, the average of each ith feature is calculated for the subset of images previously identified. Thus, a vector with twelve average values is obtained. The definition of the fuzzy set A(i,m) considered in the antecedent of the rules is given by(20)A(i,m)=max[μ(i,1)(xi),μ(i,2)(xi),…,μ(i,m)(xi)]where μ(i,m)(xi) is the degree of membership of xiin the fuzzy set. Then, the rule is evaluated in the corresponding m fuzzy set. As an example, a rule for the WP algorithm (ω1) is stated as:R1:IFx1isA(1,1)ANDx2isA(2,1)AND…ANDx12isA(12,2),THENΩisω1,where ω1 is the consequent fuzzy set in the rule R1.Algorithm selection is the process of mapping a test image into a known color constancy algorithm. In this part of our approach, test images are submitted to the system to compute their feature vectors. Each feature vector is used for the determination of the best algorithm from the already formulated knowledge base through an inference process. This process starts with the estimation of a firing strength τn(x) in each rule generated. Thus, the firing strength represents to what extent x satisfies the whole set of antecedents.Using the Mamdani inference model [30], the firing strength τn(x) of a nth rule is computed using(21)τn(x)=min[μ(1,m)n(x1),μ(2,m)n(x2),⋯,μ(i,m)n(xi)]or, if the Larsen product inference [31] is used, the firing strength τn(x) is calculated using(22)τn(x)=∏iμ(i,m)n(xi).Finally, the label assigned ωpto the corresponding output Ω is the one that corresponds to the maximum firing strength from all the rules(23)Ω=max[τ1(x),τ2(x),…,τn(x)].Each inference model is considered independently in the experiments, in order to test its performance in the selection, and thus consider it as the best inference model for this task.At the beginning of the training process, the automatic selector was designed using three rules, each one for an output algorithm. Images from the corresponding algorithm were included to generate the corresponding rule. However, results showed a very low selection performance, achieving a value close to 30%. This result has two possible explanations. First, features selected are not sufficiently discriminant. Second, there are images with different characteristics that correspond to the same color constancy algorithm, that is, a specific algorithm is suited for a variety of non-similar images. We experimented with a large number of features. However, the twelve before mentioned were chosen due to their higher discrimination capability. For this reason, we considered the second explanation.Numerous experimental tests were conducted using a heuristic search for determining the number of appropriate rules. As a result, we found that, on one hand, using fewer than five rules provided deficient performance. On the other hand, the inclusion of more than five rules resulted on a negligible improvement in the performance. Consequently, we consider that five rules per algorithm are adequate, enabling us to have a reasonably intuitive control over the information, and exploiting the advantage of handling large amounts of data in a few rules.The k-means algorithm is used for generating five clusters of features per class in a 12-dimensional space. Thus, a rule of each subset is generated using the rule formulation process. As result, a total of 15 rules are formulated in the learning process.A sample out of the 15 rules formulated isR1:IFx1isA(1,1)ANDx2isA(2,1)AND⋯ANDx12isA(12,3),THENΩisω1.Analyzing the 15 rules with this notation could be a hard task. Table 5shows a more practical manner of representing the 15 rules, where A(i,j) is shown only with a j index. Each label is represented by a number that can be associated to an intuitive label: 1 to“VERY LOW”, 2 to “LOW”, 3 to “HIGH”, and finally, 4 to “VERY HIGH”. The output set is written out using the algorithm acronym.The feature space is very large, and the possible number of fuzzy rules covering the whole space is 412, approximately seventeen million rules. This means that if only 15 rules are considered by the expert system, a huge portion of the feature space is not taken into account. However, if each rule is tuned, with the intention to cover a larger section of the space, the selection performance should increase. In order to extend the coverage of the space by each rule, it is necessary to exclude one or several premises in the antecedent. Thus, the rule formulated is more robust and implicitly includes other rules.The tuning process was performed through an empirical experimentation. All possible combinations were incorporated into the knowledge base generated by the 15 rules. Finally, the best combination was adopted and, such 15 rules with tuned premises are shown in Table 6. Also, as we can appreciate in Table 6, the included rules show the symbol ‘–’ which represents the excluded premises. A hyphen denotes the exclusion of such fuzzy set from the premises of the rule. This “do not care” situation results in the coverage of the whole space of the feature. That is, whatever the specific value of the feature is, the rule is not affected. An example out of the 15 tuned rules is the followingR2:IFx3isA(3,2)ANDx6isA(6,3)ANDx7isA(7,2),THENΩisω1

@&#CONCLUSIONS@&#
We presented a framework for developing an automatic selector system, which is oriented to the choice of the best color constancy algorithm for enhancement of dark images. In this study we used three color constancy algorithms: the White-Patch, the Gray-World and the Gray-Edge. These algorithms have been widely used in color constancy tasks due to their simplicity and acceptable performance. However, they have also shown a significant image color enhancement, especially on images under low-light conditions.The design of an automatic system is not a trivial task when diverse image features are involved in the selection. For this reason, we developed a fuzzy rule-based system, in order to model the information through simple rules. The methodology carried out was divided into two main stages. First, a training protocol for the determination of the fuzzy rules according to features computed from a subset of training images. An important advantage of our approach, consists in the use of twelve image features. Thus, we think that these features are very important for the significant selection rate. Secondly, for a given test image the best algorithm was chosen according to the rule evaluation. Moreover, experiments were conducted in separate manner using Mamdani and Larsen fuzzy inferences. Specifically, the Larsen inference model provided an outstanding selection rate in this specific task.The main goal of this work has been the development of a fuzzy rule-based selector in a practical and novel task. Besides, our framework is an adequate tool for solving two problems at the same time: color constancy and image color enhancement. Specifically this system should be applied to images or video frames under low light conditions. The defined framework is easy to be replicated for possible posterior issues. Future works could be oriented to the implementation of the automatic selector in practical engineering applications like mobile devices (e.g. cameras and tablets), video surveillance and security tasks.