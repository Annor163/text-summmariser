@&#MAIN-TITLE@&#
A SVM based classification method for homogeneous data

@&#HIGHLIGHTS@&#
A novel classification method based on SVM is proposed for binary classification tasks of homogeneous data.The proposed method leverages the homogeneity within the same classes and exploits the difference between different classes.Experimental results indicate the power of the proposed method.

@&#KEYPHRASES@&#
Multi-observation samples,Homogeneous data,SVM classification,

@&#ABSTRACT@&#
A novel classification method based on SVM is proposed for binary classification tasks of homogeneous data in this paper. The proposed method can effectively predict the binary labeling of the sequence of observation samples in the test set by using the following procedure: we first make different assumptions about the class labeling of this sequence, then we utilize SVM to obtain two classification errors respectively for each assumption, and finally the binary labeling is determined by comparing the obtained two classification errors. The proposed method leverages the homogeneity within the same classes and exploits the difference between different classes, and hence can achieve the effective classification for homogeneous data. Experimental results indicate the power of the proposed method.

@&#INTRODUCTION@&#
Traditional pattern recognition methods mainly classify the single observation sample, that is to say, the test pattern in each classification task only involves the feature of one single sample. However, when facing homogeneous patterns, which certainly belong to the same class although they seem to be different or incomplete and even very similar to patterns of different classes, traditional recognition methods may fail or degrade their recognition rate sharply since the discriminant information contained only in one single sample may be too little. For instance, in the real-world plant recognition based on leaf images, due to variations in location, period, and illumination conditions, leaf images belonging to the same plant species differ from each other in a thousand ways, which actually means that the inter-class dissimilarity may be large while the intra-class similarity is high in this application scenario. So, it is not easy for traditional plant recognition methods to correctly classify these leaf images to different plant species since their recognition decision is only dependent on one single leaf image without considering the homogeneity between leaf images. In particular, it commonly happens that leaves of different species are similar (see Fig. 1), irregular or incomplete (see Figs. 5 and 6), which will lead to a heavy weakening of the performances of traditional plant recognition methods. Therefore, we should develop a new classifier which is able to effectively realize homogeneous data classification through exploiting the homogeneity between sample data.Our work here seems to be a little related with the so-called homogeneous classifiers. Typical homogeneous classifiers include centroid-based homogeneous classifiers [1] and neural-network-based homogeneous classifiers [2]. However, because these homogeneous classifiers generally contain several sub-classifiers and then focus on exploiting the homogeneity between sub-classifiers, they are in essence different from the proposed classifier in this study where the homogeneity between data is concerned.Our work here is related with multi-observation data classification, since multi-observation data may also be viewed as a kind of homogeneous data. Due to their practical values and encouraging capability on pattern recognition, it seems that classification techniques of multi-observation sets [3] can be directly used to address homogeneous data classification tasks. In the classification of multi-observation sets, multi-observation samples belonging to the same class are considered at the same time [4]. Classification techniques of multi-observation sets take advantage of correlation within the same classes and the difference between different classes. The basic idea is to consider multi-observation samples in the test set as a whole, and then achieve the effective classification by measuring and comparing different similarities between the test set and different training sample sets. Typical classification techniques of multi-observation sets include algorithm MSM [5] based on principal component subspace, algorithm KLD [6] based on density estimation, algorithm GMMS [7] based on Gaussian mixture models, algorithm CMSM [8] based on constraint subspaces, algorithm MASC [4] based on label propagation [9,10], algorithms KMSM [11,12] and KPA [13] based on kernel trick.However, when facing various kinds of homogeneous data, the above mentioned multi-observation data classification techniques have obvious limitations and shortcomings. For example, the nonlinear classification of data is not considered in MSM and CMSM, and the parameter and the dimensionality is difficult to be set in the constraint subspace of CMSM. In MASC, edge weights of the k-NN graph are computed by Gaussian kernel function based on Euclidean distance, and thus this simple similarity measure based on Euclidean distance is difficult to fully reflect the distribution characteristics of a complex data space. Gaussian kernel function of KMSM and KPA has a greater dependence on parameters. KLD is only applicable to the data obeying a single Gaussian distribution, and GMMS has a large computational complexity. All these techniques can work well for a sequence of the coming unlabeled observation samples in the test set which must belong to a same but unknown class. However, when we have a sequence of the coming unlabeled two/multiple-class observation samples in the test set, since these classification techniques of multi-observation sets cannot consider/leverage the difference between two/multiple classes in the test set, they will perhaps become ineffective. What is more, all these classification techniques of multi-observation sets do not consider such a case in which a homogeneous object may perhaps be expressed by its several incomplete parts (see Figs. 5 and 6) among which the homogeneity still remains, therefore, they cannot be directly utilized to handle with such a case. In order to circumvent these limitations and effectively classify homogeneous data in a more general sense, we propose a novel SVM [14] based classification method for the above classification tasks of homogeneous data in this study. When predicting the binary labeling of the sequence of observation samples in the test set, the proposed SVM based classification method can work well by leveraging the homogeneity within each of two parts and exploit the difference between two parts in the test set of homogeneous data. To best of our knowledge, up to date, little attention is paid on such a classification technique with a SVM-like framework.The paper is organized as follows. We first formulate the problem of classification of homogeneous data in Section 2. And we introduce our SVM-based classification method in Section 3. Then we demonstrate the performance of the proposed classification method for leaf classification for complete homogeneous images, leaf classification for incomplete but homogeneous images, and classification of incomplete but homogeneous face images in Sections 4.1.1, 4.1.2 and 4.2, respectively.In this study, we only focus our attention on the following problem of the binary classification of objects represented by homogeneous sets. Typical application scenarios include plant recognition based on leaf images and classification of twins based on face images. Assume we have two homogeneous objects/patterns s1 and s2, the problem is to assign multiple observations of homogeneous patterns/objects to one of two classes. We assume that we have multiple observations of s1 and s2 of the following form:(1)x1i(u)=oi(s1),i=1,…,m1.(2)x1j(u)=oj(s2),j=1,…,m2.where oi(s1) corresponds to ith single observation of a homogeneous object s1 and oj(s2) corresponds to jth single observation of the other homogeneous object s2. The superscript index (u) in (Eqs. (1) and (2)) simply denotes that the different observations are unlabeled. The problem then is to classify s1 and s2 at the same time, using the multiple observations.In order to address the above binary classification problem, we assume that we also have a training data set in our disposal. Therefore, one can organize the whole data set in two parts: X={X(l), X(u)}, where X(l)={x1, x2, …, xl}, xi∈Rd. The l examples in X(l) are labeled, covering different observations of two homogeneous objects. X(u) denotes the unlabeled observation sets of two homogeneous objects,X(u)=X1(u)∪X2(u)={xl+1,xl+2,…,xl+m1,xl+m1+1,…,xn}, xi∈Rd, n=l+m1+m2, whereX1(u)={xl+1,xl+2,…,xl+m1}is associated with the observation set introduced in (1) andX2(u)={xl+m1+1,…,xn}is associated with the observation set introduced in (2), that is so say,X(u)={xl+1,xl+2,…,xl+m1,xl+m1+1,…,xn}≜{x11,…,x1m1,x21,…,x2m2}. LetY=−1,+1denote the label set, since only two different classes of data are assumed in the classification of homogeneous objects in this study.In conclusion, we may formally define the binary classification problem of homogeneous data as follows.Problem 1Given a labeled data set X(l) and a set of homogeneous data X(u), X(u) contains the observation data of homogeneous objects s1 and s2, X(u)≜{x1i(u)=oi(s),x2j(u)=oj(s), i=1, …, m1, j=1, …, m2}, the problem is to give the correct classyˆaof the observation data about object s1 and the correct classyˆbof the observation data about object s2 in X(u), simultaneously.In fact, Problem 1 is a special case of classification of homogeneous sets. In this case, the test set can be dived into two parts. All samples in each part of the test set belong to the same class, and two classes of the two parts are certainly different. In this aspect, in order to realize efficient classification, we can consider how to leverage the homogeneity within each of two parts and exploit the difference between two parts in the test set of homogeneous data, while the existing classification methods of multi-observation sets can only predict the class label of a sequence of the coming multi-observation samples as a whole by only comparing different similarities between the test set and different training sets. A novel SVM based classification method will be proposed in next section to solve Problem 1.Support vector machine (SVM) is a new machine learning method proposed by Vapnik et al. in the last century [14], its development is based on statistical learning theory and structural risk minimization (SRM) principle [15]. The method largely overcomes the “curse of dimensionality”, “over-learning” and other issues in traditional learning methods. In particular, SVM exhibits many unique advantages in solving small-sample, non-linear and high dimension problems [16–19].The main idea of SVM is to seek an optimal separating hyperplane as the decision surface, which can classify two classes of data whilst maximizes the distance between the points over the separation margin and the hyperplane. Considering the classification of two classes of data, given a training set (xi, yi), i=1, 2, …, n,xi∈Rd, yi=±1. The design of a binary classifier based on SVM is to seek an optimal separating hyperplaneH:wTx+b=0in the feature space, wherewis a d-dimensional vector and b is a real number.When the training set is linearly inseparable, we soften the hard constraint by introducing slack variables ξi≥0, i=1, …, n, and the optimal problem of the separating hyperplane becomes:(3)min12w2+C∑i=1nξis.t.,yi(wTφ(xi)+b)≥1−ξi,i=1,2,...,nξi≥0,i=1,2,…,nwhere C>0 is the penalty parameter of the error term. By introducing Lagrange function, the dual problem of quadratic programming is obtained:(4)maxα∑i=1nαi−12∑i,j=1nαiαjyiyjxiTxjs.t.,∑i=1nαiyi=00≤αi≤C,i=1,2,...,nWhen the training set is nonlinearly separable, we transform the lower dimensional feature space of input samples into a higher dimensional feature space via a nonlinear mapping, and construct the optimal separating hyperplane in this high dimensional feature space. Assuming there is a nonlinear mapping: ϕ:X→Z, X∈Rd, Z∈Rk, k≥d, which maps input samples xi∈X into k-dimensional feature space Z, then we have a kernel function k(xi,xj)=φT(xi)φ(xj). In this case, the optimal function in Eq. (4) becomes:(5)Q(α)=∑i=1nαi−12∑i,j=1nαiαjyiyjφT(xi)φ(xj)=∑i=1nαi−12∑i,j=1nαiαjyiyjk(xi,xj)and we get the final decision function:(6)f(x)=sgn∑i=1nαi*yik(xi,x)+b*Considering support vector machine has some advantages, such as simple structure, global optimization, strong compatibility, high generalization ability and so on, now we apply SVM to solve the problem of binary classification of homogeneous data. As we know from the basic principle of support vector machine, the focus of SVM is on the solution of following mathematical problems:(7)min12||w||2+C∑i=1nξis.t.,yi(wTφ(xi)+b)≥1−ξi,i=1,2,...,nξi≥0,i=1,2,…,nIn the classification problem of homogeneous data, since the whole dataset X={X(l), X(u)} contains the l labeled objects in X(l) and the (n−l) unlabeled data objects in X(u) which contains two different classes (i.e.,X1(u)andX2(u)) of samples, the class labels ofX1(u)and X2(u) can be denoted byya∈−1,+1andyb∈−1,+1respectively, and ya=−yb. If we define ya=y, then yb=−y. This means:(8)yl+1=yl+2=⋯=yl+m1=yyl+m1+1=yl+m1+2=⋯=yn=−ywhere y equals −1 or +1. In this case, we can adhere the above constraint conditions in Eq. (8) about unlabeled data to Eq. (7) by assuming the labels:(9)min12||w||2+C∑i=1lξis.t.,yi(wTφ(xi)+b)≥1−ξi,i=1,2,...,lyj(wTφ(xj)+b)≥1,j=l+1,l+2,...,nyl+1=yl+2=⋯=yl+m1=yyl+m1+1=yl+m1+2=⋯=yn=−yξi≥0,i=1,2,…,l⁡Assume that y=−1, namely, ya=−1, yb=+1, we get the optimal solutionA1*and the classification errorg1=C∑i=1lξi. Then assume that y=+1, namely, ya=+1, yb=−1, we get another optimal solutionA2*and other classification errorg2=C∑i=1lξi. If and only if the assuming label agrees with the true label, the hyperplane determined by the optimal solution is an optimal separating hyperplane, and the corresponding classification error is minimal. Hence, y can be obtained by comparing classification errors:(10)yˆ=−1g1<g2yˆ=+1g1≥g2And then we can obtain ya, yb. To solve the optimal problem in Eq. (9), we introduce Lagrange function L:(11)L(w,b,ξi,αi,βj,ri)=12||w||2+C∑i=1lξi−∑i=1lαi[yi(wTφ(xi)+b)−1+ξi]−∑i=1lriξi−∑j=l+1nβj[yj(wTϕ(wj)+b)−1]where αi, βj, riare Lagrange multipliers, αi≥0, βj≥0, ri≥0, ξi≥0. The Karush–Kuhn–Tucker conditions of Eq. (11):(12)∂L(w,b,ξi,αi,βj,ri)∂w=0∂L(w,b,ξi,αi,βj,ri)∂b=0∂L(w,b,ξi,αi,βj,ri)∂ξi=0The solution of Eq. (12):(13)w=∑i=1lαiyiφ(xi)+∑j=l+1nβjyjφ(xj)∑i=1lαiyi+∑j=l+1nβjyj=0C−αi−ri=0Substituting Eq. (13) into Eq. (11), it becomes a dual problem:(14)Q=max∑i=1lαi+∑j=l+1nβj−12∑i=1l∑j=1lαiαjyiyjk(xi,xj)−12∑i=l+1n∑j=l+1nβiβjyiyjk(xi,xj)−∑i=1l∑j=l+1nαiβjyiyjk(xi,xj)s.t.,0≤αi≤C,i=1,...,lβi≥0,i=l+1,...,n∑i=1lαiyi+∑j=l+1nβjyj=0The optimal function in Eq. (14) can be written in matrix form:(15)Q=maxOTA−12AT((YYT)×K)AwhereY=y1,…,yl,yl+1,…,ynT=y1,…,yl,y,…,y,−y,…,−yT,O=1,1,…,1Tis a n-dimensional vector, andA=α1,…αl,βl+1,…,βnTis the kernel matrix of data set X. According to Eq. (14), we can obtain the optimal solution A* and the maximum value Q*, which is also the minimum of objective function in Eq. (9):(16)12||w||2+C∑i=1nξi=Q*Substitutingwobtained by Eq. (13) into Eq. (14), we can get the classification error:(17)c∑i=1nξi=Q*−12||w||2=Q*−12∑i=1lαiyiφ(xi)+∑j=l+1nβjyjφ(xj)2=Q*−12∑i=1l∑j=1lαiαjyiyjφT(xi)φ(xj)+2∑i=1l∑j=l+1nαiβjyiyjφT(xi)φ(xj)+∑i=l+1n∑j=l+1nβiβjyiyjφT(xi)φ(xj)=Q*−12(A*)T((YYT)×K)A*Now, we summarize the proposed SVM based classification method for homogeneous data as follows:The proposed SVM based classification methodInput:X(l),Y(l): labeled data and their labelsX(u): homogeneous datal: number of labeled data samplesm1,m2: number of two different classes of observationsOutput:ya, yb: estimated unknown class of two different classes of observationsInitialization:Form the sample matrix X⊂Rn×daccording to X(l) and X(u)Form the label vector Y⊂Rn×1 according to Y(l)Compute the kernel matrix K of the sample matrix XAssume y=−1, getA1*according to Eq. (15) and getg1=C∑i=1lξiusing Eq. (17)Assume y=+1, getA2*according to Eq. (15) and getg2=C∑i=1lξiusing Eq. (17)If g1<g2, thenyˆ=−1,ya=−1 and yb=+1, else thenyˆ=+1, ya=+1 and yb=−1A distinctive advantage of the above proposed SVM based classification method exists in its simplicity, that is to say, any existing SVM solver can be directly adopted in our method, no additional efforts are particularly required for the design of the SVM solver involved in our method.

@&#CONCLUSIONS@&#
In this paper, we have addressed the problem of the classification of objects represented by homogeneous data sets. The proposed method can also work well for predicting the binary labeling of the sequence of observation samples in the test set, since it considers all samples in the test and training sets at the same time based on SVM, and leverages the homogeneity within the same classes and exploits the difference between different classes. Experimental results on several data sets indicate the effectiveness and efficiency of the proposed SVM based classification method.At present, several techniques including low-rank approximation method [25], core vector machines [26] and Fast KDE [27] have been developed for scaling up SVM for large data. How to extend the proposed SVM based classification method into its scalable version for large homogeneous data is an interesting but challenge topic in near future. Another future work is how to extend our idea here into other frameworks, e.g., graph based classification framework.