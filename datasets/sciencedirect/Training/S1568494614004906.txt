@&#MAIN-TITLE@&#
Fuzzy clustering with semantic interpretation

@&#HIGHLIGHTS@&#
The proposed approach offers more detailed insight into the cluster's structure and the underlying decision making process.We select different features to describe different objects.We evaluate the similarity between the objects based on the descriptions of objects.

@&#KEYPHRASES@&#
Fuzzy clustering,Semantic interpretation,AFS algebra,Selection of simple concepts,Cluster validity index,Unsupervised learning,

@&#ABSTRACT@&#
In the framework of Axiomatic Fuzzy Set (AFS) theory, we propose a new approach to data clustering. The objective of this clustering is to adhere to some principles of grouping exercised by humans when determining a structure in data. Compared with other clustering approaches, the proposed approach offers more detailed insight into the cluster's structure and the underlying decision making process. This contributes to the enhanced interpretability of the results via the representation capabilities of AFS theory. The effectiveness of the proposed approach is demonstrated by using real-world data, and the obtained results show that the performance of the clustering is comparable with other fuzzy rule-based clustering methods, and benchmark fuzzy clustering methods FCM and K-means. Experimental studies have shown that the proposed fuzzy clustering method can discover the clusters in the data and help specify them in terms of some comprehensive fuzzy rules.

@&#INTRODUCTION@&#
Knowledge acquisition and knowledge discovery from data become recently more and more important. One common technique to discover and analyze structure in data is clustering. Fuzzy clustering has gained a significant level of interest considering its role in revealing structure while flagging “borderline” patterns. Fuzzy clustering has been one of the design cornerstones of fuzzy modeling.In fuzzy clustering, the well-known Fuzzy C-means (FCM) clustering algorithm, which was first proposed by Dunn [1] and then generalized by Bezdek [2], is the best-known and has been extensively used in data clustering and related applications. This family of algorithms is based on an iterative optimization of a certain objective function. The convergence of this iterative scheme, proved by Bezdek, indicates that we may reach a local minimum or a saddle point [3]. The performance of the method itself could depend upon some initial guesses as to the number of clusters, initial partition matrix, a value of the fuzzification coefficient and others. These assumptions made a priori are necessary but do not guarantee that the optimization scheme could lead us to the global minimum. Meanwhile, the clustering results cannot be easily interpreted in terms of linguistic labels – information granules defined in the individual feature spaces.Some new fuzzy clustering methods were proposed, Honda et al. [4] used the fuzzy principal component analysis to form the cluster indicator, as well as the weights of individual data for the K-means clustering in order to develop a robust K-means clustering scheme. DBCAMM [5] is an approach to merge the sub-clusters by using the local sub-cluster density information. Tan et al. proposed an improved FCMBP fuzzy clustering method [6] based on evolutionary programming. A proximity fuzzy framework for clustering relational data [7] is presented by Graves et al.. A certain knowledge-guided scheme of fuzzy clustering in which the domain knowledge is represented in the form of viewpoints is introduced [8] by Pedrycz et al., the users point of view at data, which are represented in a plain numeric format or through some information granules, is included in the clustering process.Nevertheless, samples are assigned to clusters by these traditional clustering techniques. In the sequel, users need to manually form descriptions for all clusters. Characterizing clusters by hand can consume a great deal of time. In addition, users sometimes may have no specific idea as to how to explain the clustering results; thus, they could involve inappropriate descriptions. Recently, several fuzzy rule-based clustering algorithms were proposed to construct appropriate descriptions of each cluster.E.G. Mansoori proposed a fuzzy rule-based clustering algorithm (FRBC) [9], the method attempts to automatically explore the potential clusters in the data patterns and identify them by means of some interpretable fuzzy rules. However, the method requires to generate some uniformly distributed instances as auxiliary data. J. Basak and R. Krishnapuram proposed a method for hierarchical clustering based on the decision tree approach [10], the obtained rule corresponding to a leaf node of the decision tree represents a cluster. H.E. Lee et al. proposed an iterative fuzzy clustering algorithm with supervision to construct probabilistic fuzzy rule base from numerical data [11]. F. Barrientos and G. Sainz proposed a new method for the extraction of interpretable knowledge from the activity data based on fuzzy unsupervised decision tree [12]. T.C. Huang et al. presented a clustering technique [13] to discover conjecturable rules, providing descriptions of clusters with a decision tree classification technique, every cluster in a conjecturable tree is depicted by only one conjecturable rule, however, the conjecturable tree is sometimes too complex to easy understandable. I.A. Sarafis et al. proposed a novel rule-based evolutionary algorithms methodology named NOCEA [14] to effectively mine clusters from massive and highly dimensional numerical databases, however they employed numeric represented fuzzy IF–THEN rules so that the descriptions of clusters are not supported by a certain semantic interpretation.AFS (axiomatic fuzzy set) algebra was proposed by Liu [15–19], The algebra delivers a new approach related to the semantic interpretation of fuzzy attribute. In the AFS theory [15–19], fuzzy sets (membership functions) and their logic operations are algorithmically determined according to the distributions of original data and the semantics of the fuzzy sets. The AFS framework, cf. [15–19] facilitates the studies on how to convert the information in databases into the membership functions and their fuzzy logic operations, by taking both fuzziness (subjective imprecision) and randomness (objective uncertainty) into account. More importantly, fuzzy logic operations can be used to the semantic interpretation [20]. In order to design more interpretable models, this paper proposes a new fuzzy clustering method based on semantic interpretation and human clustering procedures. The procedure of clustering imitate a clustering process followed by humans: Firstly, some simple concepts are selected to describe every sample (instance, pattern) according to its nature. Then these simple concepts are aggregated to form a complex concepts serving as a description of this sample. Next, the samples with the same or similar descriptions are regarded as forming a single cluster. Finally, the nature of each cluster is described by the aggregations of the descriptions of the typical samples in this cluster, and in this sense the cluster comes with a clearly articulated semantics.The experimental studies completed for some data show that using the clustering algorithm in [15], which is also based on the AFS theory, it is difficult to obtain satisfactory results. Thus, this algorithm requires some careful refinement. We found that the following two issues contributed to its lower performance: (a) The descriptions of samples are too “rough” to characterize them well enough. (b) All samples are described by the simple concepts present in one predetermined set Λ. These aspects make the method different from a way in which humans carry out a grouping process. In this paper, we propose an algorithm of selecting simple concepts to describe a sample, and offer a method of selecting the best description for a sample and a new fuzzy cluster validity. In the selection of the simple concept, for the sample x, by the proposed 1/k−A-nearest neighbors of x, a special set Λxof simple concepts is selected to form the fuzzy description of x. We evaluate the performance of the proposed clustering algorithms using well-known benchmark data sets [21].The study is organized as follows. In Section2, we provide an overview of data representation and preliminary notions of the AFS theory. The procedure of our clustering algorithm is present in Section3. In Section4, we thoroughly evaluate the efficacy of the proposed model through a number of experiments. A comparative analysis is included in Section5. Finally, in the concluding section, we summarize and discuss our results.In this study, a weather data in Table 1is used as an illustrative example for AFS algebras. It consists of 10 day observations and 3 features which are Temperature (f1), Humidity (f2), and Wind (f3). Let X={x1, …, x10} be a set of 10 day observations, where xi∈R3 (i=1, 2, …, 10) denotes the ith sample, fj(j=1, 2, 3) denotes the jth feature of X. Thus, X=(xij) is an 10×3 matrix representing data, xijis the jth feature value of xi. The following three fuzzy if–then rules [22] describe Class 1 for the clustering model.R1: IF xi1 is cool temperature and xi2 is low humidity and xi3 is strong wind THEN xibelongs to cluster 1.R2: IF xi1 is hot temperature and xi2 is norm humidity and xi3 is weak wind THEN xibelongs to cluster 1.R3: IF xi1 is cool temperature and xi3 is strong wind THEN xibelongs to cluster 1.Let M={mj,k|1≤j≤3, 1≤k≤3} be the set of fuzzy terms (linguistic labels), where m1,1, m1,2, m1,3 are fuzzy terms “cool”, “mild” and “hot” associated with the feature temperature f1, where m2,1, m2,2, m2,3 are fuzzy terms “low”, “norm” and “high” associated with the feature humidity f2, where m3,1, m3,2, m3,3 are fuzzy terms “weak”, “norm” and “strong” associated with the feature wind f3, then the fuzzy rulesR1,R2,R3can be written down in the following form:R1: IF xiis m1,1 and xiis m2,1 and xiis m3,3 THEN xibelongs to cluster 1.R2: IF xiis m1,3 and xiis m2,2, and xiis m3,1 THEN xibelongs to cluster 1.R3: IF xiis m1,1 and xiis m3,3 THEN xibelongs to cluster 1.For each set of fuzzy terms, A⊆M;∏m∈Amrepresents a conjunction of the fuzzy terms in A. For instance, let A={m1,3, m2,1, m3,2}⊆M, a new fuzzy set “m1,3 and m2,1 and m3,2” with the linguist interpretation “hot temperature and low humidity and norm wind” can be represented as∏m∈Am=m1,3m2,1m3,2. Then the fuzzy rulesR1,R2,R3can be represented as follows:R1: IF xiis m1,1m2,1m3,3 THEN xibelongs to cluster 1.R2: IF xiis m1,3m2,2m3,1 THEN xibelongs to cluster 1.R3: IF xiis m1,1m3,3 THEN xibelongs to cluster 1.The antecedent conditions of the three fuzzy rulesR1,R2,R3for cluster 1 can be combined using logic operators “or” as follows:R: IF xiis m1,1m2,1m3,3 or m1,3m2,2m3,1 or m1,1m3,3 THEN xibelongs to cluster 1.In this case, each rule inRis called an item of R. Here, R has three items (the fuzzy rulesR1,R2,R3), so the number of rules is 3.∑u=1r(∏m∈Aum)which is a formal sum of the sets∏m∈Aum, Au⊆M is the disjunction of the conjunctions represented by∏m∈Aum, u=1, …, r. For example, let A1={m1,1, m2,1, m3,3}, A2={m1,3, m2,2, m3,1}, A3={m1,1, m3,3}⊆M, then a new fuzzy set as the disjunction of∏m∈A1m,∏m∈A2m,∏m∈A3m, i.e., “m1,1m2,1m3,3 or m1,3m2,2m3,1 or m1,1m3,3”, can be represented as∑u=13(∏m∈Aum)=∏m∈A1m+∏m∈A2m+∏m∈A3m.Thus,Rcan be denoted as follows:R: IF xiis∑u=13(∏m∈Aum)THEN xibelongs to cluster 1.The above expressions inRcan be formulated as an algebra systems as follows: let M be a non-empty set. The set EM* is defined by(1)EM*=∑i∈I(∏m∈Aim)|Ai⊆M,i∈I,Iis a non-empty indexing setA binary relation R on EM* is defined as follows. For any∑i∈I(∏m∈Aim),∑j∈J(∏m∈Bjm)∈EM*,∑i∈I(∏m∈Aim)R∑j∈J(∏m∈Bjm)⇔(i) ∀Ai(i∈I), ∃Bh(h∈J) such that Ai⊇Bh; (ii) ∀Bj(j∈J), ∃ Ak(k∈I), such that Bj⊇Ak.It is apparent that R is an equivalence relation. The quotient set, EM*/R is denoted by EM. The notation∑i∈I(∏m∈Aim)=∑j∈J(∏m∈Bjm)means that∑i∈I(∏m∈Aim)and∑j∈J(∏m∈Bjm)are equivalent under R. Thus the semantics they represent are equivalent.By a straightforward comparison of m1,1m2,1m3,3 and m1,1m3,3, we conclude that m1,1m2,1m3,3 + m1,3m2,2m3,1 + m1,1m3,3 and m1,3m2,2m3,1 + m1,1m3,3 are equivalent. For any x, the degree of x belonging to the fuzzy set represented by m1,1m2,1m3,3 is always less than or equal to the degree of x belonging to the fuzzy set represented by m1,1m3,3. Therefore, the term m1,1m2,1m3,3 is redundant in fuzzy set∑u=13(∏m∈Aum)and the expressions m1,1m2,1m3,3 + m1,3m2,2m3,1 + m1,1m3,3 and m1,3m2,2m3,1 + m1,1m3,3 are equivalent in semantics.In [16], authors proved that (EM, ∨, ∧) is a completely distributive lattice if the lattice operators ∨ and ∧ are defined as follows: for any fuzzy sets∑i∈I(∏m∈Aim),∑j∈J(∏m∈Bjm)∈EM,(2)∑i∈I(∏m∈Aim)∨∑j∈J(∏m∈Bjm)=∑k∈I⊔J(∏m∈Ckm),(3)∑i∈I(∏m∈Aim)∧∑j∈J(∏m∈Bjm)=∑i∈I,j∈J(∏m∈Ai∪Bjm),where for any k∈I⊔J (the disjoint union of I and J, i.e., every element in I and every element in J are always regarded as different elements in I⊔J), Ck=Akif k∈I, and Ck=Bkif k∈J.Let X be a data set and M be a set of fuzzy terms on X. For A⊆M, x∈X, we define(4)A⪰(x)={y∈X|x⪰myforanym∈A}⊆Xwhere a linearly ordered relation is denoted by “⪰”. For m∈M, “x⪰my” implies that the degree of x belonging to m is larger than or equal to that of y. A⪰(x) is the set of all elements in X whose degrees of belongingness to set∏m∈Amare less than or equal to that of x. A⪰(x) is determined by the semantics of fuzzy set A and the probability distribution of observed data set X.For fuzzy set ξ∈EM, let μξ:X→[0, 1]. {μξ(x)|ξ∈EM} is called a set of coherence membership functions [23] of the AFS fuzzy logic system (EM, ∨, ∧) [23], if the following conditions are satisfied.1.For α, β∈EM, if α≤β in lattice (EM, ∨, ∧), then μα(x)≤μβ(x) for any x∈X;For x∈X,η=∑i∈I(∏m∈Aim)∈EM, ifAi⪰(x)=∅for all i∈I then μη(x)=0;For x, y∈X, A⊆M,η=∏m∈Am∈EM, if A⪰(x)⊆A⪰(y), then μη(x)≤μη(y); if A⪰(x)=X then μη(x)=1.The coherence membership functions are associated with a measure over X. We propose two types of measures for fuzzy sets, which can be constructed by taking the semantics of the fuzzy terms and the probability distribution of the feature of the data into account. In order to achieve this, we first introduce the following definition.Definition 1[18]Let ν be a fuzzy term on X. ρν:X→R+=[0, ∞). ρνis called a weight function of the simple concept ν if ρνsatisfies the following conditions:1.For x∈X, ρν(x)=0, if x does not belong to ν;For x, y∈X, ρν(x)≥ρν(y) if the degree of x belonging to ν is larger than or equal that of y.Theorem 1[23]Let(Ω,F,P)be a probability measure space and M be a set of fuzzy terms on X. Let ργbe the weight function for a fuzzy term γ∈M. Let X⊆Ω be a finite set of observed samples from the probability space(Ω,F,P). If for any m∈M and any x∈Ω;m⪰(x)∈F. Then the following assertions hold:1.{μξ(x)|ξ∈EM} is a set of coherence membership functions of (EM, ∨, ∧), provided that the membership function for each fuzzy setξ=∑i∈I(∏m∈Aim)∈EMis defined as follows:(5)μξ(x)=supi∈Iinfγ∈Ai∑u∈Ai⪰(x)ργ(u)∑u∈Xργ(u),∀x∈X,(6)μξ(x)=supi∈Iinfγ∈Ai∫Ai⪰(x)ργ(t)dPt∑Ωργ(t)dPt,∀x∈Ω,If for every γ∈M, ργ(x) is continuous on Ω and X is a set of samples randomly drawn from the probability space(Ω,F,P), then the membership function defined by(5)converges to the membership function defined by(6), for all x∈X as |X| approaches infinity.Theorem 1 expresses the membership functions based on the fuzzy logic operations expressed on the observed data and the overall space by taking both fuzziness and randomness into account via ργ(x) andAi⪰(x). The following practical relevance of the coherence membership functions can be ensured by Theorem 1.•The membership functions and the fuzzy logic operations determined by the observed data drawn from a probability space will be consistent with those being determined by the probability distribution expressed in the probability space.The results obtained via the AFS fuzzy logic based on the membership functions and their logic operations determined by different data sets drawn from the same probability space will be consistent.The laws discovered based on the membership functions and their logic operations determined by the observed data drawn from a probability space can be applied to the whole space by considering the membership functions of the sets determined by the probability distribution.In practice, the weight function ρνcan be defined according to the specificity of the data and the underlying semantics of ν. In this paper, the weight functions of simple concepts are defined as follows: First, each attribute is rescaled to unit interval [0, 1] by the usage of a linear transformation that preserves the distribution of training patterns. Then, triangular weight functions are used because they are simpler and easy to understand, which are shown in Fig. 1, the semantics ofρmj1,ρmj2,ρmj3is “small”, “medium”, and “large”, respectively, where j=1, 2, …, s.An example to demonstrate the approach of computing the membership functions was presented in [24,25]. The mathematical properties of AFS structure and AFS algebra are discussed in [16–19,24]. The research monograph [23] offers a comprehensive introduction of the AFS theory and its applications.In this paper, for any fuzzy conceptη=∑i∈I(∏m∈Aim)∈EM, the membership function of η is defined as follows: for any x∈X,(7)μη(x)=supi∈I|{y∈X|x⪰ρmy,∀m∈Ai}||X|,The membership function defined by (7) just depends on the order relation. Thus it can be applied to the data with mixed features such as numeric, Boolean, and ordered data.Definition 2Let X be a set of samples and M be the set of simple concepts on X. Let X and M be finite sets. For A⊆M, and x, y∈X, we define dA(x, y) the distance of x, y associating to the simple concepts in A as follows:(8)dA(x,y)=(∑m∈A(ρm(x)−ρm(y))2)1/2,where ρm(x) is a weight function of m.Definition 3Let X be a set of samples and M be a set of simple concepts on X. Let X and M be finite sets. For A⊆M, and x∈X, k is a positive integer, a subset of X is called a 1/k−A-nearest neighbor of x, denoted asD1/k−xA, if for anyy∈D1/k−xAand anyz∈X−D1/k−xA, dA(x, y)<dA(x, z), and|D1/k−xA|/|X|=1/k.The 1/k−A-nearest neighbor of x, which is the set of |X|/k samples in X whose distances from x associating to the simple concepts in A are the nearest of the others |X|−|X|/k samples in X, will be applied to select simple concepts for the descriptions of the sample x.Clustering is an unsupervised learning technique used to discover structure in a data set. While there exist many clustering algorithms, the important issue of feature selection, that is, what features of the data should be used by the clustering algorithms, is rarely touched upon. Feature selection for clustering is difficult because, unlike in supervised learning, there are no class labels for the data and, thus, there are no obvious criteria to guide the selection. Another important problem in clustering is the number of clusters, which is clearly influenced by the feature selection issue. In this section, we introduce a new fuzzy concept selection algorithm based on the proposed distance associating to some simple concepts for the AFS fuzzy clustering scheme.More formally, the procedure can be stated as follows: Let X be a set of samples and M be the set of simple concepts (linguistic terms, or linguistic variables) on X. For each x∈X, Λx⊆M is a set of selected simple concepts to describe x. Then the sample x is described by a fuzzy concept ζx∈EM which is an AFS fuzzy logic composition of the simple concepts m in Λxvia operators ∨ and ∧ defined by formula (2) and (3). Finally, cluster the samples by their similarity degrees determined by their descriptions as follows: for any x, y∈X, their similarity degree S(x, y) is determined byS(x,y)=min{μζx∧ζy(x),μζx∧ζy(y)}, where ζxand ζyare the descriptions of x, y, respectively.μζx∧ζy(.)is the membership function of ζx∧ζydefined by (7). ζx∧ζyis the AFS fuzzy logic “and” defined by (3). It is clear that the larger S(x, y), the more similar x and y are.By adhering to a way in which humans cluster data, the proposed AFS clustering scheme consists of the following design phases, see Figure.2. In what follows, we present the design phases in more detail.For x∈X, and m∈M, we calculate the 1/k−M-nearest neighborD1/k−xMand the 1/k−{m}-nearest neighborD1/k−xmof x. The larger|D1/k−xm∩D1/k−xM|is, the greater, the correlation between the simple concept m and x is. Thus, the importance of simple concept m to describe x is determined by|D1/k−xm∩D1/k−xM|. We choose |M|/k simple concepts in M whose|D1/k−xm∩D1/k−xM|are the largest of the other |M|−|M|/k simple concepts in M as the selected simple concepts and denote the set of the selected |M|/k simple concepts as Λxwhich will be applied to describe sample x.In order to distinguish x from other samples in X to a maximal extent, a fuzzy concept ζx∈EΛx, which satisfies that not onlyμζx(x)is the most adjacent to μϑ(x), but alsoμζx(y)is as small as possible for y∈X, x≠y, so, ζxis defined as:(9)ζx=⋀m∈AxɛmwhereAxɛ={m|μm(x)≥μϑ(x)−ɛ,m∈Λx}.Algorithm 1AFS fuzzy clustering for a given αInput:X: a data set, M: the set of simple concepts on X, k: an integer, ɛ∈[0, 1](In all experiments of this paper, k=3, ɛ=0.3), α∈[0, 1], ϑ=∑m∈Mm;Output:C1, C2, …, Cl,ζC1,ζC2,…,ζCl;1:For each sample x∈X, to obtain its fuzzy description;2:compute Λx⊆M, satisfied that ∀m∈Λx,∀w∈M∖Λx,|D1/k−xm∩D1/k−xM|≥|D1/k−xw∩D1/k−xM|;3:ζx=⋀m∈Axɛm, whereAxɛ={m|μm(x)≥μϑ(x)−ɛ,m∈Λx};4:End for;5:F=(fij) where,fij=min{μζxi∧ζxj(xi),μζxi∧ζxj(xj)};6:Find an integer h such that(Fh)2=Fh, Q=Fhis a fuzzy equivalence relation matrix;7:Determine the initial clustersC¯1,C¯1, …,C¯lfor Q=(qij) under a given threshold α, xi, xjare clustered in the same cluster under the threshold α if and only if qij≥α, the number of samples in a cluster is less than |X|/30 is discarded.8:ζCi=DCi⋁FCito get the fuzzy description of cluster Ci, whereDCi=argmaxζx,x∈C¯i{1|C¯i|∑u∈C¯iμζx(u)−1|X−C¯i|∑v∈X−C¯iμζx(v)},FCi=argmaxζx,x∈C¯i{maxu∈C¯iμζx(u)−maxv∈X−C¯iμζx(v)};9:For each sample x∈X10:ifq=argmax1≤i≤l{μζCi(x)}then x∈Cq;11:End for.We establish the fuzzy relation matrix F=(fij), wherefij=min{μζxi∧ζxj(xi),μζxi∧ζxj(xj)}, evaluates the similarity degree between the sample xiand xjaccording to their descriptions. Theorem 3 in [15] shows that there exists an integer h such that(Fh)2=Fh. i.e., the fuzzy relation matrix Q=Fhcan yield a partition tree with equivalence classes.For the fuzzy relation Q=(qij) and a threshold α∈[0, 1], we have the boolean matrixQα=(qijα),qijα=1↔qij≥α. xi, xjare clustered in the same cluster under the threshold α if and only if qij≥α (i.e.,qijα=1). Thus, some clustersC¯1,C¯2,…,C¯lare obtained for the threshold α. If qii<α then xican not be determined by Q under the threshold α, and if the number of samples in a cluster is less than |X|/30, the cluster is discarded. The fuzzy description of a cluster is defined as:ζCi=DCi⋁FCito get the fuzzy description of cluster Ci, whereDCi=argmaxζx,x∈C¯i{1|C¯i|∑u∈C¯iμζx(u)−1|X−C¯i|∑v∈X−C¯iμζx(v)},FCi=argmaxζx,x∈C¯i{maxu∈C¯iμζx(u)−maxv∈X−C¯iμζx(v)}.DCiintends to select an optimal description which can distinguish between all elements inC¯iand outsideC¯iwell;FCistress the absolute differences between one inC¯iand another outsideC¯i. In the best case,DCi=FCi.All samples in X are re-clustered to C1, C2, …, Clby the fuzzy descriptionsζC1,ζC2, …,ζClby the following formula:ifq=argmax1≤i≤l{μζCi(x)}, thenx∈C¯q.To verify whether the clusters, which are obtained by the proposed method, accurately present the structure of the data set, some validity criteria are needed. Among many indices of fuzzy clustering used in the literature, some of the important ones are covered in an extensive review paper [26]. However, the validity indices in [26] are not suitable for the verification of the fuzzy clusters that are obtained by our proposed method [9]. In this paper a new validity criteria are introduced as follows:For different threshold values α∈[0, 1], the different clustering numbers and results: C1, C2, …, Cland their fuzzy descriptionsζC1,ζC2, …,ζClmay be obtained by the Algorithm 1: AFS fuzzy clustering for a given α. The threshold α controls the degree of roughness of the clustering results or the size of the information granules. We can determine the optimal fuzzy clustering result C1, C2, …, Cland their fuzzy descriptionsζC1,ζC2, …,ζClin terms of the fuzzy cluster validity index expressed as follows:(10)Iα=1l∑x∈X(maxi=1,…,lμζCi(x)−1l∑i=1,…,lμζCi(x))where α∈[0, 1], and l is the number of clusters. The fuzzy cluster validity index Iαevaluates an extent to which the clusters are “clearly” and “distinguishablely” obtained by the algorithm for the specific value of the threshold α∈(0, 1). The larger Iα, the more distinguishable the clustering result is, as it shown in Figs. 5(a), (b), 6(a), (b), 7(a), (b).

@&#CONCLUSIONS@&#
The interpretability of the AFS clustering results offers a visible advantage over the other techniques existing in the literature. Moreover, the fuzzy description ζC∈EM for each cluster C, which determines a degree each sample belongs to the cluster C, has a definitely well expressed semantics with the simple concepts being formed for the individual feature. Thus not only can the high classification rate be obtained via the fuzzy cluster validity index Iα, but also the nature of the cluster C can be easily comprehended through the interpretation of the fuzzy description ζC.The proposed fuzzy clustering algorithm can automatically explore the potential clusters in the data sets. The generated fuzzy description (fuzzy rules), which represent the clusters, are human understandable with acceptable classification rate. Therefore, they can operate as a suitable scheme for comprehensible representation of data and knowledge discovery in data-mining applications, while the other methods only determine the center and members of each cluster.This paper has established the theoretical foundation and the framework of AFS clustering for future work, which can encompass an implementation implement the techniques discussed above with objective of reducing computing overhead and developing schemes of automatic determination of the optimal values of the parameters.