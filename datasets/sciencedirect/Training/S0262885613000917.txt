@&#MAIN-TITLE@&#
Multi-agent event recognition by preservation of spatiotemporal relationships between probabilistic models

@&#HIGHLIGHTS@&#
Modeling/recognition of multi-agent activities (American football plays).Activities modeled as graphs, inexact graph matching used for comparison.Single-agent activity represented as motion patterns, modeled as graph nodes.Spatio-temporal relationships between single-agent behaviors modeled as graph edges.We present our own dataset of football plays called “UCF Football”.

@&#KEYPHRASES@&#
Football play recognition,Multi-agent activity modeling and recognition,Graph matching,Lie algebra,

@&#ABSTRACT@&#
Graphical abstract

@&#INTRODUCTION@&#
Recognition and analysis of multi-agent activity has been an important area of research in artificial intelligence [32] as well as computer vision [7]. A significant amount of effort in both these areas has attempted to leverage a symbolic representation of atomic behaviors [4], and first-order predicate calculus [1] as tools for analysis and understanding of complex activities. Although such principled approaches are desirable in general, they do not explicitly account for the difficulties and uncertainties in obtaining symbolic representations by visual analysis. Moreover, in practical scenarios, it is a prohibitively cumbersome task to manually encode semantically meaningful symbols, rules and productions etc., that would account for all possible permutations in large state spaces. We observe that visual recognition of multi-agent activities can be an unsupervised learning process where the goal is to estimate an appropriate measure of similarity between videos while taking into account the uncertainty in low level representation.We propose a graph theoretic framework which encodes not only the statistical representation of low level, agent-specific actions or behaviors, but also comprehensive, continuous spatial and temporal relationships between such behaviors, as opposed to discrete ones like Allen algebra [1]. In terms of low level behaviors inference, current multi-agent activity analysis methods in computer vision rely on models based on tracking of agents [7,19], or body parts thereof [25], detections without tracking [28,2], or short high confidence tracklets [29,30], etc. In practical scenarios however, tracking is unreliable due to occlusion and unpredictable motion of actors, which is a significant drawback in many of the methods that employ tracking. These methods also do not explicitly model the inherent spatiotemporal structure present in multi-agent activities. Other methods can be found in a recent survey [24].To recognize multi-agent activities, event-based methods are often used. An activity is assumed to be composed of a set of events and is characterized by the relationship of these events. Events were detected based on the interactions of agents in [11,31] and based on individual actions in [10,8,4]. Ivanov and Bobick [11] used probabilistic detectors to propose event candidates. The event set was analyzed with a context-free stochastic parser to recognize interactions between persons and vehicles. Hongeng et al. [8] and Hakeem and Shah [4] detected sub-events performed by individuals, and represented the logical and temporal dependencies between sub-events using graphic models. This kind of methods requires information about all possible events. It becomes quite difficult for a large number of agents and complex interactions between them.One of the specific examples of multi-agent activities is field sports, e.g., football and soccer, analysis of which has been an active research topic [16,9]. These efforts attempted to detect or recognize dynamics and behaviors using camera motion, color, low-level motion, field markers, lines and texture etc. Intille and Bobick in [10] first detected individual goals. These goals and temporal relations between players served as children for a higher level in a Bayesian tree. Li et al. [20] proposed a discriminative temporal interaction manifold based framework for the same problem. Manually annotated player trajectories were used as low level features in both [10,20], while [19] obtained them using a multitarget tracker.Swears and Hoogs [29,30] have proposed to use short, high confidence tracklets in a Non-Stationary Kernel Hidden Markov Model (NSKHMM) for football play recognition, in an attempt to overcome problems associated with tracking of agents throughout activity videos. Recognition of American football plays is a complex problem and much attention has been paid to it, not only because it is an important problem in itself but more so because it is an ideal example for multi-agent activity and is therefore a popular choice for demonstration of recognition algorithms. We observe that the methods most relevant to our approach are by Lin et al. [21,22], Li and Chellappa [18] and Swears and Hoogs [29,30]. We discuss these in more detail in subsequent sections.Individual events are spatio-temporally localized within a multi-agent activity. For example in a traffic scenario, a semantic scene description of an intersection may read as follows: motion from north west to south is followed by north bound motion from south east. Each of these traffic flows can be considered to be one event. For a computer vision algorithm this description would translate to the knowledge of the temporal and spatial beginning and end of each event and other features of these events such as their density and variance of direction and speed etc. Similarly we propose to use salient motion information in a sports scenario to understand sporting activity without requiring tracks for individual players. For example a football play is defined by the motion of certain players which is planned before the play is executed. The motion of the defensive players follows that of the offensive players in most cases. We therefore do not need to keep track of both sets of players, but only the significant activity within the play. Fig. 1shows an example for a clip of a football play along with the expected and automatically generated sub-events occurring within the clip. In order to get a reasonable understanding of the global activity we propose to explicitly compute and leverage the spatial and temporal relationships between motion patterns within the video of an activity. As opposed to symbolic or quantified representations of individual behaviors, in this work we attempt to represent the entire activity as a complete graph, where a vertex represents an agent's behavior, and the edges between two vertices depict the spatiotemporal relationships between them. It is worth mentioning here that, first, due to the probabilistic nature of agent behavior representations, we do not assume the presence or absence of a predefined atomic action, rather any arbitrary action is possible with a certain probability, which in turn is used in the matching and recognition of activity instances. Secondly, the relationships between agent behaviors need not be quantized in space (e.g., above, below or adjacent etc.), or in time (e.g., before, after, during etc.). Once we obtain a graphical representation of an activity, we may compare two activities using inexact graph matching.The purpose of our motion representation is to accurately capture the spatio-temporal location of motion in an event in addition to motion features. To this end we propose to use the motion patterns framework [27] described in detail in the following sub-section.The motion patterns framework was originally proposed for event modeling in static camera scenarios with persistent motion. The entire framework can be broken down into these steps: Optical flow computation [23], clustering of flow and a hierarchical linking of flow clusters based on spatial and temporal proximity. Each connected group of flow clusters is one motion pattern. A feature vector (an optical flow point) given as x=(x,y,u,v), belonging to the motion pattern of the ith agent, can then be written as:(1)x~∑k=1Niωi,kN⋅|μi,k,Σi,k,where μi,k, Σi,k, and ωi,kare the parameters of the kth component of the ith motion pattern, and there are a total of Ni, 4d Gaussian components in the mixture.In the American football domain, a play is defined by the spatio-temporal relationships between subevents. These events may be defined as an observed motion in a certain direction with an associated time. Note that we do not distinguish between offense and defense players but instead make use of the fact that the motion of defense players largely depends on the motion of the offense players and their chosen strategy. Therefore motion of offense and defense players who are partaking in the same subevent is jointly modeled. We demonstrate that the motion pattern representation is ideally suited to modeling events in non-persistent settings such as those in sports where related methods fail.We begin by performing some pre-processing on the videos. This involves ego-motion compensation so that the observed motion is strictly related to the motion of actors rather than the camera. The video of an activity instance, f, is divided into Zfvideo clips, each of which is z frames long. We then compute optical flow for the videos which results in a feature vector (x, y, u, v, t) for each pixel in a frame, where t is quantized into clips. In order to keep the method simple, and avoid costly optimization algorithms for Gaussian mixture learning, we adapt a method similar to the one proposed in [27], which performs a hierarchical clustering of optical flow to simultaneously segment motion in space and time, as well as learning of the parameters of the Gaussian mixture representing each motion pattern. We therefore obtain a set of Gaussian mixtures, V={vi}, 1≤i≤Qf, so that the activity f is represented by Qfagents or motion patterns, and vi={(μi,k,Σi,k,ωi,k,τi,k)}, 1≤k≤Ni, where τi,kis the time at which the kth component of viis observed. Each motion pattern vitherefore, comprises of a set of Niquadruplets representing a 4d Gaussian component's mean, covariance matrix, weight, and the time of occurrence respectively. The obtained motion patterns are then warped using manually computed homographies from the stabilized clip to a field model. Fig. 2shows the outputs from some of these pre-processing steps and a brief illustration of the subsequent graph construction whose details will be provided in the following sections of the paper. For visualization, we illustrate a motion pattern as the conditional expectation of optical flow given pixel locations.The framework for estimating motion patterns is complimented by a three phase filtering step to remove noisy optical flow. Firstly, weak optical flow is thresholded, and the clusters obtained from flow are filtered using their covariances. A final filtering step is performed on the obtained patterns in which any patterns with a very small number of constituent clusters are removed. This provides good robustness to clutter in the scene, for problems such as camera shake, imperfect video registration, and in the specific case of football, any motion in the crowd.We now perform an in depth evaluation of the merits of our framework with the related methods. The most recent work in this direction is by [18]. The authors use a driving force model to characterize a group of agents that share a goal. However in that work, the number of driving forces has to be manually specified. Since different plays may have varying levels of complexity, the actual number of factors that influence motion may be different between classes. Their motion model is defined by a single affine matrix, which is counter-intuitive in a sports scenario even in small regions. The representation does not appear to be discriminatory between play classes and the results in the paper only demonstrate segmentation of sub-activities within a single play.Similar work on motion modeling has been using HDP/LDA models [14]. Those frameworks require repeated flow to construct a model, therefore they do not apply to sports scenarios. However, our motion pattern representation may be considered to be a simplified version of the HDP framework that is applicable to the current setting. Similarly work done by Li et al. [21,22] also requires persistent flow for modeling. In addition, HDP/LDA approaches generate actor representations that are not very easy to manipulate, e.g., transform spatially, which is required for our graph matching method.The multi-agent activity instance f, is represented by a planar, directed, complete graph, Gf=(Vf,Ef), where V, the set of vertices, is a collection of the parameters of Gaussian mixtures described earlier, and |V|=Qf. Each element of the Qf×Qfmatrix, E of edges contains a vector representative of the optimal spatial transformation between Gaussian mixtures of two vertices connected by that edge. Fig. 3provides an illustration of one node pair and the edge between them in a graph.Using this global representation of an activity in time and space using our proposed event modeling we can compare two activities. We do this by attributed inexact graph matching. For comparison between two activities we require a comparison of the location and flow of individual events, their relative times of occurrence, the pairwise spatial relationships between these events and the relative saliency of an event within the entire activity. We now describe how we encode this information into our framework.We can compare the vertices of two graphs which probabilistically represent an agent's behavior by simply computing the KL divergence between the Gaussian mixtures. There are however a few problems with this approach. First, KL divergence is not a distance metric, and is not symmetric. It has a high dynamic range for dissimilar distributions, especially in different directions. Second, KL divergence between Gaussian mixture distributions does not have a closed form, and Monte Carlo point sampling is often used to estimate it, which is a computationally expensive operation. We therefore define a distance measure comparing two Gaussian mixtures which takes into account their location and shape in (x,y), and their motion in (u,v). Specifically, given graphs, Gfand Ggfor activity instances f and g respectively, we compute a vertex–vertex similarity matrixDαof size Qf×Qg, which is symmetric and positive. Given two agent behaviors, viand vjfrom graphs, Gfand Gg, the elements of matrix,Df,gαare defined as:(2)di,jα=exp−Δθi,j22σθ2−ξi,j22σξ2−1−δi,j22σδ2,where,(3)Δθi,j=tan−1∑k=1Niωi,kμi,k4∑k=1Niωi,kμi,k3−tan−1∑k=1Njωj,kμj,k4∑k=1Njωj,kμj,k3,is the difference of mean optical flow orientations of the Gaussian mixtures (adjusted for phase change). The variable ξ computes the minimum Euclidean distance between any two components in each mixture,(4)ξi,j=min1≤m≤Ni,1≤n≤Njμi,m12−μj,n122,and the shape similarity between two motion patterns is measured by the amount of spatial overlap between Gaussian mixtures,(5)δi,j=Ri∩RjRi∪Rj,where the regions are sets of pixels with high probability of belonging to the motion pattern,(6)Ri=x12s.t.∫∫pix|vidudv>κ.The parameter κ was fixed for all our experiments. We now explain how we leveraged the temporal information in the agent behaviors for improved graph matching.Another useful cue towards robust matching of two activity instances is the relative temporal position of each agent behavior within the activity. For example, two instances of football plays should be more likely to match if a specific type of pass is executed after approximately the same relative duration into the play. We observe that by nature of the process of motion patterns estimation, the quantized time (video clip) at which each component of the mixture was observed is known, i.e., τi,k. We can then estimate the approximate time of occurrence of an agent behavior as the mean time of all components. The relative time of occurrence for the ith motion pattern is then written as,(7)Γi=1Ni⋅Zf∑k=1Niτi,k,so that 0<Γi≤1, since τi,k∈{1, …,Zf}, and represents an additional property of the node vi, which will be helpful in more accurate graph matching, vertex correspondences, and therefore estimation of similarity between activity instances. The approximate relative temporal location of agent behaviors within the activity video will be compared with that of behaviors (or nodes) in other activity graphs.Using the relative time of occurrence, we can determine whether two events in two different plays are similar with respect to their temporal positions within the respective plays. We define their temporal similarity of occurrence of two motion patterns viand vjin distinct graphs Gfand Ggas, di,jλ=1−|Γi−Γj|, where dλ∈[0,1]. Therefore two agent behaviors occurring at the beginning and at the end of their respective activities will have the minimum temporal similarity, and vice versa. Computing the temporal similarity of occurrence for all pairs of activities between two plays gives us a Qf×Qgmatrix,Df,gλ.We now describe our method for estimating edge to edge similarities between the graphs, Gfand Gg.One of the main reasons multi-agent activities are modeled as collections of atomic, agent specific behaviors, is that subtle differences in spatiotemporal relationships between these behaviors can represent distinct high level activities. Conversely, two instances of the same activity are likely to have non-rigid transformations between agent behaviors across instances.In order to estimate the spatial relationship between two patterns, we employ an iterative warping procedure which attempts to align multivariate Gaussian mixtures such as the ones representing agent behaviors. If a large set of 4d points are sampled from each Gaussian mixture, this problem is analogous to registration of point clouds. A recent method ideally suited to this problem is proposed in [12]. We leverage a similar method proposed in [13]. The estimated relationship consists of the transformation parameters that would optimally warp one pattern onto the second pattern so that their KL divergence is minimized after warping. A 3×3 similarity matrix depicting this transformation is then written as:(8)T=sRt⊤01,where R is a 2×2 rotation matrix, s is the scale, and t={tx,ty}, is the translation vector. Since G is complete, a transformation Tijis computed between all pairs of vertices i and j. It can be noticed however that we only need to compute either the upper or lower triangle of matrix E, since eij=eji−1, where eij=[sR(1,1), sR(2,1), …, tx, ty]. The 6-long vector e therefore represents how two Gaussian mixtures are translated, rotated, and scaled with respect to each other, and the edge attributes matrix E captures the geometric relationships between all pairs of mixtures.In order to obtain a measure of similarity between the relationships of vertices within two distinct graphs, we need to compare the attributes of the edges connecting the vertices. We therefore create a Qf2×Qg2 matrix Φ, where elements of the matrix will be the similarity values between all possible pairs of edges in the two graphs. The problem however is that the edge attribute e computed previously (Eq. (8)) is not a vector space, and therefore not closed under vector addition or scalar multiplication. Indeed, a simple Euclidean distance between eijand emmmakes little sense without careful but aggressive scaling of the individual elements of the 2d transform, which include translation in pixels, trigonometric functions of rotation angle, and a scaling parameter. It is therefore desirable to map the multiplicative structure of Similarity transforms to a vector space such that the intrinsic geometric structure of the transformation is preserved. To this end, we propose to leverage Lie algebra which can be used to find exactly such a mapping. Several recent works in the literature have used this approach to allow analysis on Affine and Projective groups [21,22]. A more detailed treatment of Lie algebra and Lie groups can be seen in [17,5]. Using the Lie algebraic approach, we therefore define the edge to edge similarity as, ϕij,mn=‖Xij−Xmn‖2, where X is the Lie algebraic representation of the Similarity transformation T computed as,(9)X=logT=∑a=1∞−1a+1aT−Ia.It has been shown in [3] that X can be represented as a linear combination of basis vectors called ‘generators’ of the Lie group, so that the coefficients of the combination serve as a representation of the original transformation in Lie space. Moreover [3], shows that for transformations near identity, the higher order terms of Eq. (9) can be ignored, thus making the mapping tractable. Given an edge to edge similarity matrix, Φ, we seek to convert it into a Qf×Qg, vertex to vertex similarity matrix,Dβ,(10)Dv,v′β=Prmv=v′|Gf,Gg,where the ‘m’ function denotes a mapping between the nodes of the two graphs Gfand Gg. For this purpose, we employ the method of [33], which performs a probabilistic soft hyper-graph matching. An optimization is defined as the following minimization,(11)minXdistΦ,⊗2Dβ.The symbol ‘⊗’ here represents the Kronecker delta product. Given two matrices Ap×qand Br×sthe Kronecker delta product is the resulting pr×qs block matrix,(12)A⊗B=a11B…a1qB⋮⋱⋮ap1B…apqB.This optimization allows us to estimate an edge based similarity of vertices. Fig. 4illustrates how we may use both edges and vertices to compute vertex–vertex similarities between two graphs. Further details are provided in the following sub-sections.One final cue that we estimate for each agent behavior is the spatial saliency. We notice that in videos of football plays, most plays will essentially begin with very similar behaviors, but as the play progresses they become more distinct and discriminative for different classes of plays, which is a rather obvious and intuitive observation. Since the end of the play is signaled by a tackle or touchdown, it would be useful if the location of the corresponding behavior was known. To this end, we leverage a very simple approach, whereby we transform the location of the center of the last frame to the mosaic generated by ego-motion compensation. The assumption implicit in this step is that most of the broadcast videos are captured from dynamic cameras which focus on the point of action within the play, which almost always is the point of tackle or touchdown towards the end of the play. Given the tackle or touchdown location for play f in global coordinates as a 2d vector, pf, we compute the spatial saliency of a motion pattern, vias,(13)Li=exp−12σL2∑k=1Niωi,kμi,k12−pf22,where σLis a constant value fixed at 30pixels. Notice that although the spatial saliency, Li, is a property of the node vi, it cannot be compared with the saliency of another node. It can however, be combined to indicate the joint saliency of a particular pair of nodes, each from a distinct graph, as explained in detail in Section 3.5. For the entire activity video, we create a Qflong vector, Lf={Li}, where i∈{1, …,Qf}. It should be mentioned that although the proposed approach is formally evaluated for the application of football plays analysis, in general, it is applicable to different kinds of multi-agent activity, and the saliency mentioned above is the only application specific step in the process. It is conceivable however, that sub-events in other activity domains can also benefit from different measures of saliency. For example, in a traffic intersection scenario, proximity of an event to the center of the intersection may indicate its importance.We argue that the proposed graph based representation of a multi-agent activity video captures its dynamics and structure in an efficient and comprehensive manner. Information about number of agents, their temporal occurrence, spatial occurrence and inter-agent spatio-temporal relationships is inferred in a completely unsupervised fashion, without the need for tracking individuals, or in the case of football plays, distinction between opposing teams, specific player or ball detection. In addition agent behaviors have a rich probabilistic representation that captures their density, magnitude and direction of per-pixel motion, and spatio-temporal localization. Given such an efficient representation an activity model can be compared to another for retrieval and recognition as we describe in the following section.Finally, given vertex to vertex similarities using multiple criteria, we compute a weighted average to obtain a single matrix that is used to perform graph matching and estimate the cost of the match, which serves as the final similarity metric between videos containing multi-agent activities. Specifically, we write,(14)Df,g=Lf⊤Lg⋅DλcαDα+cβDβ,where the matrices Lf⊤LgandDλserve as element-wise weights for correspondence, based on the joint spatial saliency and temporal similarity of pairs of agent behaviors, while scalars c act as corresponding weights for each cue.Using the vertex to vertex matrixDf,g, we now assume the nodes of each graph, Vfand Vgto be independent sets of a complete bipartite graph, and attempt to find a set of correspondences between them so that the probability of the global assignment is maximized. We employ the well known Hungarian [15] (aka Munkres [26]) algorithm for this purpose, and obtain a binary Qf×Qgmatrix,Mof correspondences where an element is 1 if the relevant agent behaviors in each activity video match. Final similarity between the two videos of multi-agent activities is then given as,(15)Sf,g=∑allrows∑allcolumnsDf,g⋅Mf,g.

@&#CONCLUSIONS@&#
