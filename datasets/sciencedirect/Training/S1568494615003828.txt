@&#MAIN-TITLE@&#
A dynamic multi-colony artificial bee colony algorithm for multi-objective optimization

@&#HIGHLIGHTS@&#
A dynamic multi-colony model was introduced in the multi-objective artificial bee colony algorithm.The proposed algorithm was able to deal with both constrained and unconstrained problems.The parameter settings were carefully investigated.Effectiveness of our algorithms was validated by experimental results.

@&#KEYPHRASES@&#
Multi-objective optimization,Multi-colony model,Artificial bee colony algorithm,Migration strategy,Friedman test,

@&#ABSTRACT@&#
This paper suggests a dynamic multi-colony multi-objective artificial bee colony algorithm (DMCMOABC) by using the multi-deme model and a dynamic information exchange strategy. In the proposed algorithm, K colonies search independently most of the time and share information occasionally. In each colony, there are S bees containing equal number of employed bees and onlooker bees. For each food source, the employed or onlooker bee will explore a temporary position generated by using neighboring information, and the better one determined by a greedy selection strategy is kept for the next iterations. The external archive is employed to store non-dominated solutions found during the search process, and the diversity over the archived individuals is maintained by using crowding-distance strategy. If a randomly generated number is smaller than the migration rate R, then an elite, defined as the intermediate individual with the maximum crowding-distance value, is identified and used to replace the worst food source in a randomly selected colony. The proposed DMCMOABC is evaluated on a set of unconstrained/constrained test functions taken from the CEC2009 special session and competition in terms of four commonly used metrics EPSILON, HV, IGD and SPREAD, and it is compared with other state-of-the-art algorithms by applying Friedman test on the mean of IGD. The test results show that DMCMOABC is significantly better than or at least comparable to its competitors for both unconstrained and constrained problems.

@&#INTRODUCTION@&#
In many real-world optimization applications, decision makers (DMs) often have to handle problems with multiple objectives that are conflicting to each other and should be optimized simultaneously, and they are usually called multi-objective optimization problems (MOPs) which are more difficult than one-objective ones since there is no single solution available for them but a set of Pareto-optimal solutions (PS) or non-dominated solutions that represent a trade-off among the objectives.With the help of some useful techniques, such as objective weighting, method of distance functions or method of min–max formulation, a MOP can be transformed into a single objective problem, and then traditional mathematical programming methods can be applied to deal with it. However, these methods are usually restrained since some objectives may involve noise, discontinuity, concavity and uncertainty in their search space [1]. Alternatively, evolutionary algorithms (EAs) and swarm intelligence (SI) algorithms have been widely extended to find several members of the PS of the MOPs in a single run. Among them, Deb's NSGAII [2], Zitzler's SPEA2 [3], Zhang's MOEA/D [4], Tseng's MTS [5], Liu's DMOEADD [6], Liu's LiuLiAlgorithm [7], Kukkonen's GDE3 [8] and Akay's S-MOABC/NS [1] enjoyed more attention.The island model or multiple-deme model is a very popular scheme used in many EA or SI algorithms [9]. In this model, the algorithms consist of several sub-populations which evolve independently most of the time and exchange members occasionally. The exchange of individuals is called migration and often controlled by some parameters [10]. Over the past decades, many algorithms based on island models have been proposed for single-objective optimization problems [11–16]. It was shown by the experimental results that these algorithms not only decreased the processing time but also performed a much broader search than single-population ones.Recently, modeled as a Markov dynamic system, the island model was theoretically proved to be effective in solving single objective problems [17], and can also be applied to the multi-objective case. In fact, some island model based multi-objective evolutionary or SI-based algorithms have already been proposed. Montan˜o et al. [18] introduced a novel island model based multi-objective evolutionary algorithm: pMODE-LD+SS, where differential evolution (DE) operators were used and two parallel schemes were designed to improve effectiveness as well as efficiency. Cheshmehgaz et al. [19] proposed an effective multiple multi-objective evolutionary algorithms (MOEAs) by using a novel island model where a central island was assisted by multiple VIP islands and a sophisticated immigration strategy was designed to exchange information. Shang et al. [20] suggested a multi-population cooperative coevolutionary algorithm (MPCCA) for Capacitated Arc Routing Problem (CARP). In this algorithm, multiple subpopulations are used to search different objective subregions simultaneously and share individuals with their adjacent subpopulations cooperatively. Zhang et al. [21] proposed a multi-objective parallel evolution algorithm (MPEA) where the migration frequency is dynamically changed according to the diversity of the parent solutions. The algorithm was applied to effectively solve flight assignment problems. Some other island model based multi-objective algorithms can be found in Refs. [22–25].Some of the above algorithms (e.g., [18,20,25]) divide the whole objective space into several sub-regions by a set of uniformly distributed weight vectors or direction vectors. Then each sub-region is assigned with a sub-population. However, it is not always easy to generate a set of uniformly distributed weight/direction vectors, especially when high-dimensional objective space is considered, and the generation of these vectors needs a recursive procedure [25]. The migration strategy used in most of these algorithms is static, i.e., a sub-population only exchanges individuals with its adjacent sub-populations in terms of the used topological structure. Meanwhile, some immigration strategies (e.g., [18,23–25], etc.) involve frequent re-division operations (central re-collection and re-distribution of all solutions from/to sub-populations [19]). This may be a main drawback since a re-division in distributed systems may affect the efficiency of the algorithm when a high level of parallelization is expected. Contrarily, this paper suggests a new island model where each island is equivalent and shares individuals with other islands dynamically. In this model, the migration is realized by using an elite pool (also known as the archive where non-dominated solutions are stored during the search process) and an island exchanges individuals with all other islands randomly. What is more, the proposed model involves no central re-collection, re-division and re-distribution operations.Based on the proposed island model, a dynamic multi-colony multi-objective artificial bee colony algorithm (DMCMOABC) is suggested in this paper. For each island, the artificial bee colony (ABC) algorithm first proposed by Karaboga [26], is selected as the search engine. The ABC algorithm was very competitive to EAs and other global optimization algorithms [27,28], and was successfully applied to many practical problems, such as IIR filters [29], team orienteering problem [30], portfolio optimization problem [31], etc. In order to enhance the efficiency and effectiveness of the ABC algorithm, the parallelization approaches were studied in [32–34]. In recent years, the extension of ABC algorithm for multi-objective optimization has caught much attention from the evolutionary multi-objective (EMO) communities. Some representative works were available, such as MOABC [35], S-MOABC/NS [1], dMOABC [36], MOABC/D [37], etc. However, island models were not utilized in any of these algorithms. Very recently, Chen et al. [38] presented a multi-hive multi-objective bee algorithm (M2OBA) for optimal power flow (OPF) in power systems by combining external archive, comprehensive learning, greedy selection, crowding distance, and cooperative search strategies. In the algorithm, the concept of Pareto dominance and comprehensive learning mechanism were used to determine the flight trajectory of a bee, and non-dominated solutions were kept in an external archive whose diversity was maintained by crowing distance strategies. By constructing a colony-level interaction topology and an information exchange strategy, the single population ABC has been extended to interacting multi-hive model.In this work, the proposed DMCMOABC employs K colonies and each one contains S bees. In each colony, there are only two kinds of bees, i.e., the employed and the onlooker bees. These K colonies simultaneously optimize their own food sources by using neighbor information in one single run, and share information when a randomly generated number between 0 and 1 is smaller than a predefined control parameter R which is called the migration rate. In each migration, an elite, defined as the intermediate individual with the maximum crowding-distance value [2] in the external archive, is identified and used to replace the worst food source in a randomly selected colony. Here, the so-called worst food source refers to a solution with the maximum fitness value used in SPEA2 [3]. It is worthy to notice that the fitness value in SPEA2 is to be minimized. As we can see from the above information exchange mechanism, one colony may be able to share information with any other colonies since the elite in the archive could be found by any colony and the reception colony is also chosen randomly. In other words, the migration direction is not static. The DMCMOABC will be terminated when the number of function evaluations reaches to the maximum allowable function evaluations(max_FEs).Although the M2OBA and DMCMOABC share some commonalities, for example, they both use crowding distance to maintain the external archive and apply the greedy selection mechanism to decide which solution enters into the archive, they are distinguished in terms of the following aspects.•The learning strategy used in each algorithm. In M2OBA, the comprehensive learning strategy is used. Specifically, some dimensions of the new position for each food sourceX→ilearn from a non-dominated solution EAkwhich is randomly selected from the archive, while the remaining dimensions are produced by using another solution EAl(l≠k). In DMCMOABC, however, the basic learning strategy is adopted. That is, each bee generates the new position by using only neighboring information. Here, the neighbor ofX→iis defined as a random food sourceX→kin the same colony. AlthoughX→kis determined stochastically, it has to be different fromX→i. This learning mechanism ensures all colonies search independently in the whole search space. Since K colonies have their own search trajectories, it is good for the exploration process. In the basic learning strategy, only one dimension is modified for each food source that provides a nice exploitation in the sub-space where each colony locates. Thus, a balance between exploration and exploitation might be well kept.The information exchange mechanism adopted by each algorithm. In M2OBA, the interaction of bees occurs in a two-level hierarchical topology. For the colony-level interaction, the ring or star topology is employed, while the topology of the individual-level is always using the star topology [38]. But, DMCMOABC uses a relatively simple information exchange strategy. First, an elite in the archive is identified by using crowding-distance value. Then, it will replace the worst food source in a randomly selected colony. Here, the elite in the archive is defined as the intermediate solution with maximum crowding-distance value, while the worst food source for each colony is the solution with the worst (or largest) fitness value used in SPEA2. The potential benefits of this mechanism lie in that it guarantees a better exploitation in the least crowded region where the elite locates. This is because the elite is used to replace the worst food source in a selected colony, and then it will go into a series of improvements executed by its employed and onlooker bees. Thus, more non-dominated solutions may be found in the sparsest region and the diversity over all archived solutions is naturally kept.The number of control parameters. Both algorithms employ some common control parameters, such as the number of colonies, the size of each colony, the size of the external archive, the maximum number of function evaluations and the parameter controlling migration rate. However, M2OBA uses three additional parameters, such as the integer m controlling the dimensions to be changed in the updating equation of the food source, the size of the sending list K and the exchange factor δ (0<δ<1) [38].The rest of this paper is organized as follows. Section 2 gives some preliminaries on the multi-objective optimization and the basic artificial bee colony algorithm. Section 3 describes details of the proposed DMCMOABC algorithm. The experimental study is presented in Section 4. Finally, Section 5 concludes the paper.In general, there are two kinds of multi-objective problems (MOPs) in the optimization field: one is unconstrained problems and the other is constrained ones. A general unconstrained MOP consists of a number of conflicting objectives to be optimized simultaneously and a set of decision variables which are restricted to a limited interval. It can be formulated as follows:(1)MinimizeF→(X→)=(f1(X→),f2(X→),…,fM(X→)),whereX→=(x1,x2,…,xD)is called the decision vector, andΩ=∏i=1D[LBi,UBi]is the decision space. Here, D is the dimension of the problem to be optimized; LBiand UBiare the lower and upper bounds of the ith decision variable, respectively.F→:X→→RMconsists of M (M≥2) real-valued objective functions, andΦ={F→(X→)|X→∈Ω}is known as the objective space.If a MOP given by formula (1) is associated with a number of equality and inequality constraints, then it becomes a constrained multi-objective problem (CMOP) which can be formulated as below [38]:(2)MinimizeF→(X→)=(f1(X→),f2(X→),…,fM(X→)),subjectto:gi(X→)=0,i=1,2,…,p,hj(X→)≥0,j=1,2,…,q,LBd≤xd≤UBd,d=1,2,…,D,where p and q are the numbers of equality and inequality constraints, respectively.For MOP (1), letX→andV→be two vectors in the decision space,X→is said to dominateV→(denoted asX→≺V→) if and only iffi(X→)≤fi(V→)for all i=1, 2, …, M and there exits at least one index j such thatfj(X→)<fj(V→). If neitherX→dominatesV→, norV→dominatesX→, thenX→andV→are non-dominated with each other denoted asX→≜V→. A pointX→*is called Pareto optimal if there is noX→in the decision space dominatingX→*. Pareto optimal solutions are also called efficient, non-dominated, and non-inferior solutions. The set of all Pareto optimal solutions is called the Pareto set (PS), and all Pareto optimal objective vectors constitute the Pareto front (PF), namely,PF={F→(X→)|X→∈PS}.The ABC algorithm is a relatively new optimization algorithm which mimics the foraging behavior of honey bees [1,26]. In the algorithm, each employed bee is associated with a particular food source and explores a new position around that food source by learning from its neighbors. The onlooker bees then make a decision to choose which food source to exploit by using a probability-based selection process according to the quality of food sources provided by employed bees. The bees of the exhausted food sources become a scout and start to search a random food source. In ABC algorithm, the whole colony consists of equal number of employed and onlooker bees and each food source corresponds to only one employed bee [39].In ABC algorithm, limit is an important control parameter that is used to determine the exhausted food sources. A food source that cannot be improved for limit trials is abandoned. The algorithm starts with randomly generating some food source positions that correspond to the solutions in the decision space. And the counters storing the numbers of trials for solutions are set to 0. After initialization, the population is subjected to repeated cycles of the search processes of the employed bees, onlooker bees and scout bees [40]. The algorithm will be terminated once a maximum number of function evaluations (max_FEs) is reached.For each food sourceX→i, its employed bee will find a neighboring solutionV→idepending on local information (visual information) in her memory. Then,X→iandV→iare compared with each other. IfV→ioutperformsX→i, then the employed bee memories the new position and forgets the old one. Otherwise, the original food source is kept in the memory. IfX→ican not be improved, then its counter holding the number of trials will increase by 1, otherwise, the counter is reset to 0 [41].After every food source is optimized by its employed bee, the onlooker bees will choose food sources using the roulette wheel selection scheme based on the selection probability. The larger the fitness value, the higher the selection probability. Once a food source has been chosen, the onlooker bees will improve it as employed bees do.In each cycle, the algorithm checks to find if there is any food source to be abandoned when the employed and onlooker bees complete their search. If the value of the counter of a food source exceeds limit, then it will be abandoned and replaced with a random solution produced by the scout. In basic ABC, only one scout bee is allowed in each cycle.The proposed algorithm uses a multi-colony model and an information exchange strategy to make the algorithm useful. The algorithm maintains the following control parameters: the migration rate (R), the number of colonies (K), the size of each colony (S), the size of the external archive (archiveSize) and the maximum number of function evaluations (max_FEs). Similar to single objective ABC algorithm, the number of food sources (denoted as FN) is fixed to half of the colony size, i.e., FN=S/2. The effects of parameters R, K, S will be experimentally studied later in Section 4.2. The flow chart of the algorithm is shown in Fig. 1. Main components of DMCMOABC are initialization, send employed bees, send onlooker bees, archive maintenance, and information exchange. The following subsections will describe them in more details.In initialization phase, FN food sources are randomly generated in the decision space for each colony. The position of the ith food sourceX→i=(xi1,xi2,…,xiD)is given by the following equation.(3)xid=LBd+r·(UBd−LBd),where r is a random number uniformly distributing over interval [0,1]; LBdand UBdare the lower and upper bounds of the dth decision variable, respectively. Here d=1, 2, …, D, where D is the dimension of the problem to be optimized. For the ith food source, the global variable trialiis used to hold the number of unsuccessful trials and it is initialized as 0. After all food sources have been produced, those non-dominated individuals will be added into the external archive. In our algorithm, all colonies share the same archive and the maintenance of it is depicted in the next subsection.The external archive is a common technique employed in many multi-objective evolutionary or swarm-based algorithms which is used to keep non-dominated solutions found by an algorithm during the search process. An effective maintenance method is often required since the size of an archive is usually fixed. In DMCMOABC, a candidateS→is directly discarded if there exists an identical individual orS→is dominated by any member in the archive. And it will be added into the archive if and only if one of the following conditions is satisfied.(a)The archive is empty.S→dominates any member in the archive.S→is non-dominated with each archived member, and the archive is not full.S→is non-dominated with each archived member, but the archive is full.An important task for the usage of the external archive is to keep a proper diversity over a set of non-dominated solutions. In case (a) and (c),S→is directly put into the archive. And the members dominated byS→will be removed from the archive in case (b) so as to ensure all archived solutions are non-dominated with each other. When the archive is full and a candidateS→is to be added (case (d)), the crowding distances of all members together withS→are re-calculated, and the one with the minimum crowding distance value will be removed from the archive. The crowding distance introduced in Ref. [2] is used to estimate the density of solutions in the external archive. A solution with a larger crowding distance value locates in a less crowded region, indicating that this solution has more potential to be exploited in terms of the population diversity. Hence, solutions with a larger crowding distance value are preferred.The computation of crowding distance needs the following steps: (1) sort the solutions in the external archive according to each objective function value in ascending order of magnitude; (2) then for each objective, an infinite distance value is assigned to the boundary solutions, i.e., solutions with smallest and largest function values, and all other intermediate solutions are assigned a distance value which is equal to the absolute normalized difference in the function values of two adjacent solutions; (3) the above calculation is repeated with other objective functions. The overall crowding distance value is calculated as the sum of individual distance values corresponding to each objective [2].It should be noted here that each objective function is normalized before calculating the crowding distance. Fig. 2shows the crowding distance calculation in a two-dimensional objective space. The crowding distance of the solution marked with solid black circle is the average side length of the cuboid (shown with a dashed box).In this phase, an employed bee is sent to its corresponding food source to explore a temporary position by using neighbor information. For each colony, the pseudo-code of send employed bees is given in Fig. 3. Given food sourceXi→, its temporary positionVi→is calculated through the following equation.(4)vid=xid+ϕid·(xid−xkd),where d is a random integer representing the index of the dimension to be changed; ϕidis a real number distributed uniformly over [−1, 1]; k is an integer randomly chosen from the set {1, 2, …, FN}. Although k is determined stochastically, it has to be different from i. Usually, we callXk→the neighbor ofXi→.After the generation ofVi→, it is then evaluated and compared withXi→by using domination comparator. Next, a greedy selection mechanism is applied toVi→andXi→so as to determine which one to be kept for the next iterations: ifVi→dominatesXi→(Vi→≺Xi→), then replaceXi→byVi→and update objective values, finally try to putVi→into the archive (see lines 7 and 8 in Fig. 3); if they are non-dominated with each other (Vi→≜Xi→), andVi→can be successfully added into the archive (i.e. archive.add(Vi→)=true), thenXi→is also replaced byVi→. In the case whereVi→can not be added, the procedure will do nothing (line 13 in Fig. 3)); ifVi→is dominated byXi→,Xi→is kept as the candidate andVi→will be discarded.Finally, the function computeFitness() is called to obtain the fitness value of each food source. In DMCMOABC, the fitness assignment method is the same as in SPEA2. That is, the fitnessfit(Xi→)contains two parts: one is the raw fitness valueR(Xi→), and the other is the densityD(Xi→). Mathematically,fit(Xi→)=R(Xi→)+D(Xi→). The raw fitness ofXi→is determined by summing the strengths (representing the number of solutionsXi→dominates) of its dominators in both archive and the colony, while the density is incorporated to discriminate between solutions having identical raw fitness values [42]. It's worth noting that the fitness used in DMCMOABC is to be minimized.In each colony, onlooker bees will select food sources to exploit according to the quality of each food source advised by employed bees. The selection probability for the ith food source is given by Eq. (5).(5)probi=1−fit(X→i)∑m=1FNfit(X→m),wherefit(X→m)returns the fitness value ofX→m. As shown in Eq. (5), a food source with lower fitness value will be assigned a larger selection probability, thus a higher chance it is chosen by onlooker bees.The pseudo-code of send onlooker bees is shown in Fig. 4. In this phase, a roulette wheel method is employed by onlooker bees to choose food sources based on the selection probabilities [43]. Once a food source is selected, the corresponding onlooker bee will try to optimize it just like the employed bee does. A new position is generated by Eq. (4), and is compared with the original food source by using domination comparator. The better one is kept for further improvements in the next iterations. The above procedure will repeat FN times and more potential food sources (with higher selection probability) may be exploited more than once.In DMCMOABC algorithm, K colonies search simultaneously in the whole decision space, and non-dominated solutions found by them are put into the same archive. If there is no information exchange among the colonies, then it will be a multi-deme algorithm with these colonies searching in parallel. Hence, an information exchange strategy is introduced here to enable the migration among colonies. The schematic diagram of information exchange strategy is provided in Fig. 5. First, an elite in the archive is identified by using crowding-distance value (see Section 3.2). Then, it will replace the worst food source in a randomly selected colony. Here, the elite in the archive is defined as the intermediate solution with maximum crowding-distance value, while the worst food source for each colony is the solution with the largest fitness value used in SPEA2 (see Section 3.3).In Fig. 5, the full lines indicate that non-dominated solutions found by each colony are added into the archive, while the dotted lines mean that the elite just replace the worst food source from only one randomly chosen colony. As we can see from this mechanism that one colony may communicate with all other colonies since the elite could be found by any colony and the reception colony is also chosen stochastically. That is to say, we here employ a dynamic information exchange strategy.The elite is located in the least crowded region in the objective space, and it is used to substitute the worst food source in a picked colony. Hence, the region around elite will be exploited more nicely since it may be improved by employed or onlooker bees through some iterations. As a result, the final non-dominated solutions are expected to achieve better qualities in terms of both convergence and diversity.Colonies in DMCMOABC evolve separately most of the time and exchange individuals occasionally. The information exchange phase is controlled by the migration rate R. As pointed out in [10] that it is an important parameter in multi-deme algorithms. The effect of R on the quality of returned solutions will be shown in Section 4.5 with some simulation results.To deal with CMOP (2), a constraint-handling method is imported from NAGA-II [2], which simply modifies domination comparator between two solutionsXi→andXj→to constrained-domination comparator. A solutionXi→is said to constrained-dominate solutionXj→, if and only if one of the following conditions is true.•SolutionXi→is feasible (without violating any constraint), while solutionXj→is not.BothXi→andXj→are infeasible, but solutionXi→has a smaller overall constraint violation.BothXi→andXj→are feasible, and solutionXi→dominatesXj→.In the comparison of two solutionsXi→andXj→, there are at most three cases to be considered: (1) both are feasible; (2) one is feasible, while the other is not; (3) both are infeasible. For the first case, the domination comparator can be directly applied, and for the second one, the feasible one is preferred. For the third case, the overall constraint violations need to be calculated for both solutions. The overall constraint violation of solutionX→is given by the following equation [44].(6)CV(X→)=∑i=1p|gi(X→)|+∑j=1q〈hj(X→)〉,where the bracket operator 〈α〉 returns the negative value of α, if α<0; otherwise, it returns zero. The solution with smaller overall constraint violation is preferred. This constraint-handling method is very simple and proved experimentally to be quite effective when used in real-coded NSGA-II and NSGA-III algorithms. Thus, it could be an alternative approach to handle constraints in other multi-objective evolutionary or swarm-based algorithms.The DMCMOABC is evaluated and compared with other state-of-the-art algorithms in terms of four commonly used metrics on seventeen test functions taken from the CEC2009 special session and competition [45]. The four metrics are EPSILON [46], HV [46], IGD [45] and SPREAD [46]. In multi-objective optimization, the convergence and diversity of the approximated front yielded by an algorithm are usually used to assess the performance of a meta-heuristic. In the above four metrics, EPSILON and SPREAD are intended to measure the convergence and diversity, respectively. While HV and IGD take both criteria into consideration [46]. It should be noted here that all metrics but HV are to be minimized.For the purpose of performance comparison, the DMCMOABC, together with other state-of-the-art multi-objective algorithms, is evaluated on test functions UF1–UF10 and CF1–CF7. The definition and mathematical representation of them are available in the technical report of CEC2009 [45]. The CEC2009 test suite provides a set of complicated functions: UF1–UF7 are unconstrained two-objective problems, and UF8–UF10 are unconstrained three-objective ones, while the CF1–CF7 represent constrained problems with two objectives. Pareto fronts of these test problems have different characteristics, for example, some of them are convex while the other ones are concave, or a part of them are continuous but the others are discontinuous [35]. Usually, a successful multi-objective algorithm can well handle problems with different types of Pareto fronts.Apart from two conventional control parameters archiveSize and max_FEs, there are three additional parameters R, K and S that should be tuned beforehand. To do this, the DMCMOABC is evaluated on the UF1–UF10 test functions with max_FEs being 300,000, and the archiveSize is set to 100 and 150 for two- and three-objective functions, respectively. To investigate the best value of the control parameter R, the following experiment is designed. First, K and S are assigned with initial values 10 and 20, respectively. Then R is varied from 0.0 to 1.0 with a step size 0.1. For each value of R, DMCMOABC is run 15 times one each function and the mean values of HV and IGD are calculated. Finally, by applying Friedman test [46,47] to the recorded mean values, the average rankings for IGD and HV under each parameter configuration are obtained, which are shown in (a) of Fig. 6. The Friedman test assumes that the smaller the average ranking is, the better the computed fronts will be. This is true in all cases but HV since it is a metric to be maximized. In this case, it should be interpreted accordingly.The bar graph in (a) of Fig. 6 shows that the performance tends to decrease in terms of both HV and IGD with the increment of R when its value is larger than 0.1. Compared with the value 0.0, R=0.1 also gives better results. Considering the above experimental results, we may carefully conclude that the information exchange indeed improves the performance of the DMCMOABC algorithm, but it prefers a lower migration rate. It is suggested by the simulation results that the best value of R could be found in the interval (0.0, 0.1].Another experiment is conducted to further study the parameter selection of R by changing its value from 0.0 to 0.1 with a smaller step size 0.01. Average rankings in different cases are presented in the form of a bar graph as shown in (b) of Fig. 6. It is revealed by the graph that values of R between 0.0 and 0.1 produce similar results when taking an overall consideration of HV and IGD. However, the performance is relatively better if R is set to 0.01 since DMCMOABC achieves the best rankings in terms of both metrics under this parameter setting. Thus, R=0.01 is suggested for general usage in our algorithm.To determine the best value of K, a similar experiment is conducted by varying the value of K from 1 to 15 with a step size 1. In this experiment, R and S are set to 0.01 and 20, respectively. It is observed from the experiment results, shown in (a) of Fig. 7, that the DMCMOABC performs better when K is set to a value between 6 and 11. Compared against other considered values, these ones could provide relatively better average rankings in terms of both HV and IGD. The bar graph also reveals that K=9 gives the best and second best rankings in terms of IGD and HV, respectively. Therefore, this value is recommended for common usage. Finally, it is found from the graph that all values of K larger than 1 give better performance than K=1, indicating a real performance improvement with the introduction of the multi-colony model. The effect of this model will be shown with more experimental results in Section 4.5.The results of parameter selection experiment on S are presented in (b) of Fig. 7. In this experiment, R=0.01 and K=9. From the bar graph we find that the best performance is obtained when S is set to 20.As the summary of this subsection, we list all suggested values of the control parameters as below: R=0.01, K=9 and S=20. These values are determined by experiments where two parameters are fixed and the third one is varied over an interval with a step size. The average rankings obtained by Friedman test on HV and IGD are used as the criterion for parameter selection.Under parameter configurations described in the previous subsection, DMCMOABC is evaluated on seventeen test functions UF1–UF10 and CF1–CF7. On each function, the algorithm is independently run 30 times and terminated when the number of function evaluations reaches 300,000 for each run. The archiveSize is set to 100 and 150 for problems with two and three objectives, respectively. The statistics (the best, worst, median, mean and standard deviation) of each metric over 30 runs are computed and listed in Tables 1–4. The metrics EPSILON, HV and SPREAD are calculated by jMetal11http://jmetal.sourceforge.net/software package, an object-oriented Java-based framework for multi-objective optimization with meta-heuristics, which was developed by Durillo and Nebro [46,48]. The IGD is a widely used metric for performance evaluation and comparison, and it is calculated exactly the same as described in the technical report of CEC2009 [45].The DMCMOABC algorithm is compared with other state-of-the-art multi-objective algorithms in terms of the mean value of IGD. These algorithms include dMOABC [36], MOEA/D [4], S-MOABC/NS [1], MTS [5], DMOEADD [6], LiuLiAlgorithm [7], GDE3 [8], NSGAII [2] and SPEA2 [3]. The computational results for unconstrained and constrained test functions are provided in Tables 5and 6, respectively. In these tables, the data of algorithms MOEA/D, S-MOABC/NS, MTS, DMOEADD, LiuLiAlgorithm and GDE3 are directly taken from original works, while those of NSGAII and SPEA2 are obtained by running jMetal software package. The DMCMOABC and dMOABC are implemented by our JAVA codes. To make the comparison process much easier, the best results for each function are bolded and underlined, while the second best ones are only bolded. It is shown by Tables 5 and 6 that DMCMOABC is very competitive compared to other well-known algorithms concerning the number of the cases where it obtains the best and the second best results.To scientifically assess each algorithm, the Friedman test is applied to the obtained results, and average rankings of each method on unconstrained test functions are listed in Table 7. In this test, the Friedman statistic (distributed according to chi-square with 9 degrees of freedom) is 44.978182, and the associated p-value is 0.000001, indicating that there exits significant differences over the whole multiple comparison among all algorithms. Table 7 shows that our DMCMOABC gets the best average ranking outperforming MOEA/D and MTS, which are followed by dMOABC and DMOEADD. The performance of LiuLiAlgorithm and GDE3 are comparable, while S-MOABC/NS, NSGAII and SPEA2 get worse rankings compared to their counterparts.Using the rankings computed by the Friedman test, the unadjusted/adjusted p-values are obtained with the application of a set of post hoc procedures, including the Bonferroni, Holm, Hochberg, Hommel, Holland, Rom, Finner and Li tests [47]. These p-values are shown in Table 8which are computed by the KEEL Software Tool22http://www.keel.es/[49,50]. As we can see in the table, the Friedman test shows a significant improvement of DMCMOABC over SPEA2, NSGAII and S-MOABC/NS for all considered post hoc procedures, with a level of significance α=0.01. The proposed algorithm is significantly better than GDE3 with α=0.1 for the Bonferroni, Holm, Hochberg, Holland and Rom tests, and with α=0.05 for the Hommel, Finner and Li test procedures. Compared to the LiuLiAlgorithm, the Friedman test presents a significant improvement with α=0.1 for all post hoc test procedures but the Bonferroni. The Finner and Li tests exhibit the most powerful behavior, reaching the lowest p-values in this pair-wise comparison. It is revealed by the Li post hoc test procedure that the DMCMOABC obviously outperforms DMOEADD with a level of significance α=0.1. For the remaining pairs of comparisons (i.e., DMCMOABC vs. dMOABC, DMCMOABC vs. MTS, DMCMOABC vs. MOEA/D), the performance differences are not detected for any post hoc procedure, indicating the fact that our algorithm is comparable to these especially competitive algorithms.Tables 9and 10show the average rankings and adjusted p-values obtained by Friedman test on constrained test functions. In this test, the control algorithm is DMOEADD (i.e., the method with the best rankings) and our algorithm gets the second best ranking. As revealed in Table 10, DMOEADD, generally accepted as the best algorithm for solving constrained problems in CEC2009 competition, shows no significant improvement over DMCMOABC since the adjusted p-values for all post hoc procedures are large enough to accept the null hypothesis H0 that is defined as the statement of no effect or difference between these two algorithms.To evaluate the speed of the proposed DMCMOABC, the runtime of our algorithm, together with MOEA/D, dMOABC, MOEA/D, NSGAII and SPEA2, is recorded and listed in Table 11. MOEA/D is a very competitive algorithm for unconstrained optimization problems, and dMOABC is a latest multi-objective algorithm based on the ABC method. The DMCMOABC uses the same crowding distance strategy as in NSGAII to maintain the external archive, and the same fitness assignment procedure as in SPEA2. Therefore, they are chosen as the comparing algorithms. This experiment is conducted on a computer with the following environments: Pentium (R) Dual-Core CPU, T4500 2.30GHz 2.29GHz, 1.99GB memory. From Table 11, we can find that DMCMOABC achieves the best runtime on UF3, UF5 and UF6, while MOEA/D is the fastest algorithm on the remaining test problems. Generally, DMCMOABC is slower than MOEA/D and NSGAII, but runs faster than dMOABC and SPEA2. Similar results are observed on other test problems. In our algorithm, once a better individual is found, the procedure will try to add it into the archive which may involve the re-calculation of crowding distances of all non-domination members (see Figs. 3 and 4, and Section 3.2). This mechanism is beneficial for the improvement of the quality of the returned non-dominated solutions, however, it may also introduce more computational efforts. In dMOABC, the external archive is maintained by a self-adaptive grid that involves a recursion procedure when dividing the objective space. Hence, the runtime of dMOABC naturally increases. This may be the reason why our algorithm is slower than some algorithms, such as MOEA/D, but faster than dMOABC.As the summary of this subsection, we list some findings through the usage of Friedman test. The computed adjusted p-values show that DMCMOABC is significantly better than some state-of-the-art algorithms such as SPEA2, NSGAII, S-MOABC/NS, GDE3 and LiuLiAlgorithm, and achieves comparable performance when compared to the most competitive algorithms such as MOEA/D, DMOEADD, MTS and dMOABC. When comparing algorithms in terms of the runtime, DMCMOABC also outperforms some other algorithms, such as dMOABC and SPEA2.To intuitively compare the performance of DMCMOABC and its competitors, the best Pareto fronts with lowest IGD value over 30 runs are plotted. For space limitation, these fronts are provided in Figs. A.1–A.6 in Appendix A. In these figures, MOEA/D and DMOEADD are chosen as the comparing algorithms since they are the most competitive algorithms in CEC2009 competition for unconstrained and constrained test functions, respectively. The approximated Pareto fronts of these two algorithms are borrowed from [4] and [6]. In the comparison process, two properties of the Pareto fronts are taken into consideration: one is the convergence, and the other is the diversity.It is shown in Fig. A.1 that the best Pareto fronts produced by DMCMOABC are comparable to those of MOEA/D on UF1 and UF2 in terms of both convergence and diversity. But DMCMOABC fails to approximate a well converged and properly distributed Pareto front on UF3, thus the performance of the algorithm is significantly worse than that of MOEA/D.As we can see in Fig. A.2, both the Pareto fronts on UF4 distribute nearly uniformly, but our algorithm is better than MOEA/D in terms of the convergence of the produced front. For test functions UF5 and UF6, the number of non-dominated solutions returned by MOEA/D is larger than that found by DMCMOABC algorithm. However, the front approximated by our algorithm has better convergence on UF5.Next, we take a look at Fig. A.3. For UF7, both the Pareto fronts converge to the true one well, but the diversity of MOEA/D is slightly better than DMCMOABC since a small part of the top-left corner of the produced front is not successfully covered by our algorithm. For UF8 and UF9, the fonts approximated by MOEA/D are better than those of DMCMOABC in terms of the diversity, but are slightly worse when considering the convergence of the fronts. The DMCMOABC shows significant improvement over MOEA/D on UF10 because the computed front found by the former is much better when both criteria are taken into consideration (see (a) and (b) in Fig. A.4). Hence, the IGD value obtained by DMCMOABC naturally decreases.For constrained functions (CF1–CF7), the quality of the Pareto fronts produced by DMCMOABC is comparable to that found by DMOEADD concerning both convergence and diversity (see Figs. A.4–A.6).In this subsection, the approximated Pareto fronts produced by DMCMOABC and its counterparts are intuitively compared, and the results show that DMCMOABC has the ability to generate a set of well converged and properly distributed non-dominated solutions.In this subsection, the effect of some control parameters is investigated by observing the variation tendency of IGD and HV on test functions UF1, UF3, UF5 and UF8. The main reason for choosing them is that they are the most representative functions of the categories to which they belong, where UF1 and UF3 are representatives of test functions with two objectives whose Pareto fronts are continuous. The UF5 is an example with discontinuous fronts, and UF8 represents a set of three-objective functions.The broken-line graphs of the mean values of IGD and HV over 15 runs are shown in Figs. 8–11. It is observed from Fig. 8 that the performance of DMCMOABC tends to decrease as R increases from 0.2 to 1.0 in terms of both considered metrics on all selected test functions. The best value may distribute between 0.0 and 0.1, however, differences of each metric are not significant when R is changed from 0.0 to 0.1 with a step size 0.01 (see Fig. 9). We could also find in this figure that DMCMOABC produces relative better results when R=0.01, which is consistent to our previous finding in Section 4.2.Next we turn to analyze the effect of parameter K. Fig. 10shows that the performance of DMCMOABC decreases sharply as K increases from 1 to 4, and changes slightly when K is larger than 5. Especially, the worst values of both IGD and HV are observed when K is set to 1, indicating that DMCMOABC is significantly better than the algorithm with only one colony and that the proposed multi-colony model indeed improves the performance of our algorithm. In fact, the diversity is more important than convergence in multi-objective algorithms for deal with some MOPs [51]. In DMCMOABC, K colonies search in different parts of the decision space and share information occasionally. This mechanism could enable the algorithm to explore more possible sub-spaces, thus, the diversity of the final solutions may be well kept.Finally, we can find from Fig. 11that the best overall performance is obtained by setting S to 20. As we all know, an important issue we should consider in the design of multi-objective algorithms, is to keep a balance between exploration and exploitation. Too small S is not good for the exploration process since only a small number of new food sources are generated in each iteration, while too large S will result in insufficient iterations or exploitation. Thus, a balance should be kept by setting S to a proper value. In this study, it seems to be quite suitable to set S to 20 for general usage.

@&#CONCLUSIONS@&#
