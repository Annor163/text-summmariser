@&#MAIN-TITLE@&#
A survey on sound source localization in robotics: From binaural to array processing methods

@&#HIGHLIGHTS@&#
We provide a comprehensive state-of-the-art in sound source localization in robotics.Binaural approaches are first reviewed, with their applications in robotics.Array processing techniques for localization in robot audition are then presented.Active variations of methods, soft. frameworks and recent projects are highlighted.

@&#KEYPHRASES@&#
Robot audition,Source localization,Binaural audition,Array processing,

@&#ABSTRACT@&#
This paper attempts to provide a state-of-the-art of sound source localization in robotics. Noticeably, this context raises original constraints—e.g. embeddability, real time, broadband environments, noise and reverberation—which are seldom simultaneously taken into account in acoustics or signal processing. A comprehensive review is proposed of recent robotics achievements, be they binaural or rooted in array processing techniques. The connections are highlighted with the underlying theory as well as with elements of physiology and neurology of human hearing.

@&#INTRODUCTION@&#
“Blindness separate us from things but deafness from people” said Helen Keller, a famous American author who was the first deafblind person to obtain a Bachelor in Arts, in 1904 (Kohlrausch et al., 2013). Indeed, hearing is a prominent sense for communication and socialization. In contrast to vision, our perception of sound is nearly omnidirectional and independent of the lighting conditions. Similarly, we are able to process sounds issued from a nearby room without any visual information on their origin. But human capabilities are not limited to sound localization. We can also extract, within a group of speakers talking simultaneously, the utterance emitted by the person we wish to focus on. Known as the term Cocktail Party Effect (Haykin and Chen, 2005), this separation capacity enables us to process efficiently and selectively the whole acoustic data coming from our daily environment. Sensitive to the slightest tone and level variations of an audio message, we have developed a faculty to recognize its origin (ringtone, voice of a colleague, etc.) and to interpret its contents. All these properties of localization, extraction, recognition and interpretation allow us to operate in dynamic environments, where it would be difficult to do without auditory information. All the above impressive human capabilities have stimulated developments in the area of Robot Audition. Likewise, the recent research topic of human–robot interaction (HRI) may have constituted an additional motivation to investigate this new field, with the aim to artificially reproduce the aforementioned localization, extraction, recognition and interpretation capabilities. Nevertheless, contrarily to computer vision, robot audition has been identified as a scientific topic of its own only since about 15 years (Nakadai et al., 2000). Since then, numerous works have been proposed by a growing community, with contributions ranging from sound source localization and separations in realistic reverberant conditions to speech or speaker recognition in the presence of noise. But as outlined in Argentieri et al. (2013), the robotics context raises several unexpected constraints, seldomly taken into account in signal processing or acoustics. Among them, one can mention:Geometry constraint: Though the aim is to design an artificial auditory system endowed with performances inspired by human hearing, there is no need to restrict the study to a biomimetic sensor endowed with just two microphones. Indeed, bringing redundant information delivered by multiple transducers can improve the analysis and its robustness to noise. Straight connections thus appear with the field of array processing. Yet, the robotics context imposes an embeddability constraint. While array processing can consider large arrays of microphones—e.g. several meters long, robotics implies a trade-off between the size of the whole sensor and its performances, so that it can be mounted on a mobile platform, be it humanoid or not.Real time constraint: Many existing methods to sound analysis rely on heavy computations. For instance, a processing time extending over several tens of seconds is admitted to perform the acoustic analysis of a passenger compartment. Contrarily, localization primitives involved in low-level reflex robotics functions—e.g. sensor-based control or auditive/visioauditive tracking—must be made available within a guaranteed short time interval. So the algorithms computational complexity is a fundamental concern. This may imply the design of dedicated devices or computational architectures.Frequency constraint: Most sound signals valuable to robotics are broadband, i.e. spread over a wide bandwidth w.r.t. their central frequency. This is the case of voice signals, which show significant energy on the bandwidth [300–3300Hz] used for telephony. Consequently, narrowband approaches developed elsewhere do not straightly apply in such broadband contexts. Noticeably, this may imply a higher computational demand.Environmental constraint: Robotics environments are fundamentally dynamic and unpredictable. Contrarily to acoustically fully controlled areas, unexpected noise and reverberations are likely to occur, which depend on the room characteristics—dimensions, walls, type of the building materials, etc.—and may singularly deteriorate the analysis performance. The robot itself participates to these perturbations, because of its self-induced noise, e.g. from fans, motors, and other moving parts. A challenge is to endow embedded sound analysis systems with robustness and/or adaptivity capabilities, able to cope with barge-in situations where both the robot and a human are possibly both speaking together.Generally, most of embedded auditory systems in robotics follow the following classical bottom-up framework: as a first step, the sensed signals are analyzed to estimate sound sources positions; next the locations are used to separate sound of interests from the sensed mixture in order to provide clean noise or speech signals; finally, sound/speaker/speech recognition systems are fed with these extracted signals. Of course, other alternatives have also been proposed (Otsuka et al., 2012), but this approach remains by far the most used framework in robot audition. Nevertheless, it exhibits the importance of sound localization in the overall analysis process. It has been indeed the most widely covered topic in the community, and a lot of efforts have been made to provide efficient sound localization algorithms suited to the robotics context. Moreover, independently of any high-level interpretation of the acoustical scene, having access to the low-level source localization information itself is mandatory for any applications related to HRI. Indeed, a natural intuitive HRI might heavily depends on how responsive a robot will be to acoustical information. Among them, source localization allows the robot to quickly react to an auditory stimulus by turning the head towards the source (turn-to reflex), or even to focus its other sensors in the direction of interest (e.g. by moving a camera field of view towards a speaker). Since, in our opinion, Robot Audition has reached an undeniable level of scientific maturity, we feel that the time has come to summarize and organize the main publications of the literature. This paper then attempts to review the most notable contributions specific to sound source localization. Another intent is to underline their connections with theoretical foundations of the field, including with basics of human physiology and neuroscience.The paper is organized into two parts. First, binaural methods to sound source localization are reviewed in Section 2, from the perspective of performances and human operation. Next, array processing approaches are expounded in Section 3, with a focus on the specificities raised by the robotics constraints.This section describes a first set of methods which try to mimic diverse aspects of the human auditory system, thus defining the topic of binaural robot audition. The common point to all the following works is the use of only two microphones, generally positioned inside a human-like pinna. There is an obvious practical interest to develop biomimetic auditory sensors containing a small number of microphones: the size is minimal and the embedded electronics is simplified. Significant advances in understanding the biological processes which enable the handling of acoustic data by humans have been obtained up to the 1980s (Middlebrooks and Green, 1991). They constitute the natural basis of binaural developments in robotics. Having this in mind, the successive steps of the sound localization process can be described by following the sound propagation, from the source to the binaural sensor:•As a first step, a sound wave generated by an external source is modified by the presence of the robotic torso, head and pinnae prior to interact with the ears. The induced scattering and spectral changes must be modeled so as to precisely capture the time-space relationship linking the sound source to the binaural signals. From a engineering point of view, this relationship is captured by the so-called Head Related Transfer Function (HRTF), which will be studied in the first subsection.Next, human localization capabilities mainly rely on some acoustic features extracted by our ears and integrated by our brain. Those features have been extensively studied; among them, one can cite Interaural Cues for horizontal localization, or spectral notches for vertical localization (Middlebrooks and Green, 1991). A lot of them have also been used in robotics. These will be reviewed in the second subsection.Finally, on the basis of these features, sound localization itself is performed. Numerous approaches have been proposed so far, and the most prominent one in robotics are outlined in the third subsection.In all the following, the left and right microphone signals will be referred to as l(t) and r(t) respectively, with t the time (in s). Their frequency counterparts, obtained through a Fourier transform, will be denoted by L(f) and R(f) respectively, with f the frequency (in Hz). Sound source position is expressed in terms of horizontal azimuth angle θ, elevation angle φ in the median plane (in radian), and distance r, all of them being expressed w.r.t. an origin located at the robot's head center. In the remaining of the paper, the position (θ, φ)=(π/2, 0) corresponds to a sound source in front of the head (i.e. at boresight, see Fig. 2).The HRTF captures the relationships between the signal s(t) originating from a sound source and captured at a certain arbitrary reference position in space and the two signals perceived by the two ears. These relationships can be written in the form(1)L(f)=HL(rs,θs,φs,f)S(f),R(f)=HR(rs,θs,φs,f)S(f),where HL(·) and HR(·) represent the left and right HRTFs respectively, (rs, θs, φs) is the actual sound source position w.r.t. the chosen reference position, and S(f) the Fourier transform of s(t). Importantly, the HRTFs account for all the modifications brought by the body of the listener, including torso, head and pinnas effects, to the incident sound wave. So it varies significantly from a human listener to another as its reflects his/her own morphology. The same applies in robotics, where all the possible acoustic scatters impact on the sensed signals, and is thus captured by the corresponding HRTFs. But it is fundamental to understand that the HRTF strictly corresponds to propagation in free field and does not include room reflections or reverberations. Consequently, the HRTF can be obtained in two ways. The first solution is to accurately model the body and head effects. If the robot shapes are simple, then basic acoustic equations can be sufficient to account for the acoustic effect on the signals. In the case of a more realistic robot, with complex body, shoulders, nose, pinnae, etc., an acoustic simulation software might be necessary to solve the problem through finite-element methods (Otani and Ise, 2006). The second solution consists in identifying the HRTF through real measurements, which must be performed in an anechoic room. This solution may not be so practical for every robotic platform, for it requires specific hardware/software. Consequently, HRTF identification is sometimes performed in the room where the robot will be located, resulting in measurements mixing together the head effect with the room acoustic. Additionally, various HRTF databases are proposed in the literature. One can cite, among others, the celebrated CIPIC database (Algazi et al., 2001), published by the CIPIC Interface Laboratory from the University of California Davis, and accessible from the website http://interface.cipic.ucdavis.edu/, or the recent HRTF database proposed by the Deutsche Telekom Laboratories (TU-Berlin) (Wierstorf et al., 2011), located at https://dev.qu.tu-berlin.de/projects/measurements/wiki. Note that those databases generally include HRTF measurements for human subjects, but also for some Head And Torso Simulators (HATS), like the KEMAR by G.R.A.S.. To the authors knowledge, no robotic HRTF databases have been proposed so far, but theHARKsoftware framework (see Section 4) proposes a systemic approach for measuring them.Typical HRTFs extracted from the CIPIC database is shown in Fig. 1, for the azimuth θ=35° and elevation ϕ=20°.Its time counterpart head related impulse response (HRIR) is also represented. This figure highlights the shadowing effect of the head: depending on the source position, the left and right signals differ in terms of time of arrival (cf. the delay between the left and right HRIR), but also in terms of spectral content (cf. the amplitude difference between the two HRTFs and the spectral notches positions). These last cues will often serve as the basis to infer localization (see Section 2.2). Readers interested in a more complete tutorial on HRTF can refer to Cheng and Wakefield (2001), where experimental and theoretical data are compared.As already outlined, the complex structure of most robotic platforms prevents the access (through simulations or identifications) to the exact robot HRTFs at the two ears. Consequently, some simple head models have been proposed by the robot audition community, with the aim to capture the robot head effect on the binaural signals up to some extent. The three most classical models are depicted on Fig. 2. They consist in considering the left and right microphones in the free field—Auditory Epipolar Geometry (AEG) (Nakadai et al., 2000), placed at the surface of a disk—Revised Epipolar Geometry (RAEG) (Nakadai et al., 2001), or at the surface of a sphere—Scattering Theory (ST) (Nakadai et al., 2003). AEG and RAEG are the most elementary model. Provided that θ and f stand for the azimuth and frequency of a farfield source (i.e. a source which is far enough to consider planar wavefronts in the vicinity of the robot head, see Section 3.1.1), the (R)AEG approximations of the left and right HRTFsHL(R)AEG(·)andHR(R)AEG(·)write as(2)HL(R)AEG(θ,f)=1,HR(R)AEG(θ,f)=e−jϕ(θ)=e−j2πfτ(R)AEG(θ),highlighting the fact that the two binaural signals only differ by a delay τ(R)AEG(θ) which is a function of the source angle (note that the left channel has been arbitrarily considered here as the reference, i.e.HL(R)AEG(·)andHR(R)AEG(·)are defined up to a transfer function P(f) modeling the propagation from the source to the left sensor). The third ST (scattering) model is more involved. Let β be the so-called incidence angle, i.e. the angle between the line from the center of the sphere approximating the head to the source position (rs, θs), and the line from the center of the same head to a measurement point at which the HRTF must be computed. Considering a perfect rigid spherical head, the expression of the diffracted sound pressure wave received at the measurement point allows to write (Duda and Martens, 1998):(3)HST(r,β,f)=rce−jr2πf/cja22πf∑m=0∞(2m+1)Pmcos(β)hm(r2πf/c)hm′(a2πf/c),where HST(r, β, f) is the transfer function linking the sound pressure received at the measurement point and the free-field pressure existing at the head center, with c the speed of sound and a the head radius. Pm(·) and hm(·) are the Legendre polynomial of degree m and the mth-order spherical Hankel functions respectively, whilehm′(·)denotes the derivative of the function hm(·). Assuming that the two microphones are antipodally placed on the surface of the sphere, the left and right HRTFs, respectively denoted byHLSTs(r,θ,f)andHRST(r,θ,f), are then given by, for a sound source located at (r, θ),(4)HLST(r,θ,f)=HSTr,−π2−θ,f,HRST(r,θ,f)=HSTr,π2−θ,f.Once the link between the sound source signal to be localized and the two resulting binaural signals has been modeled, it is necessary to focus on the binaural features which can be extracted from these signals to infer localization. These features are first recalled through a short review on sound source localization in humans. Then, the way how these cues can be coupled with the aforementioned HRTFs is investigated.About 100 years ago, Rayleigh proposed the duplex theory (Rayleigh, 1907), which explains that horizontal localization is mainly performed through two primary auditory cues, namely the Interaural Level Difference (ILD) and the Interaural Time Difference (ITD). The ILD relates to the difference between the intensity of signals perceived by the right and left ears, due to the head frequency-dependent scattering. Noticeably, if a source emits at a frequency higher than about 750Hz, then the head and any small-sized element of the face induce scattering, which significantly modifies the perceived acoustic levels, so that the ILD can exceed 30dB. On the contrary, the ILD is close to 0dB at low frequencies, as fields whose wavelengths are greater than the head diameter undergo no scattering. This property can clearly been deduced from Fig. 1, where the left and right HRTF amplitudes only significantly differ for frequencies greater than about 800 Hz. The second auditory cue is known as the Interaural Phase Difference (IPD)—or its time-counterpart termed Interaural Time Difference (ITD). The ITD is justified by the path difference to be traveled by the wave to reach the ears. It appears on Fig. 1 as a delay between the two HRIR onsets. Note however that the maximum value involved in localization is around 700μs—i.e. one period of a 1400Hz sound—due to the ambiguity of IPD values greater than 2π. So, two frequency domains can be exhibited in human horizontal localization, each one involving a distinct acoustic cue. Frequencies under ∼1kHz are azimuthally localized by means of the IPD, while frequencies above ∼3kHz exploit the ILD.It can be straightly inferred that a source emitting from the vertical symmetry plane of the head produces no interaural difference. However, humans are still able to perform localization in such conditions. Indeed, obstacles—including shoulders, head, outer ear, etc.—play the role of scatters which modify the frequency components of acoustic waves. This filtering effect is essential in our ability to determine the elevation of a sound source. Indeed, the sum of all the reflections occurring around the head induces notches into the perceived spectrum, the positions of which are significantly affected by the source elevation (Humanski and Butler, 1988), see Fig. 1. The acoustic feature for vertical localization is thus a spectral cue, termed “monaural” as it involves no comparison between the signals perceived at the two ears. Consequently, these notches positions are likely to be used by the brain to infer the elevation. Note that it is not possible, even for humans, to make any difference between a notch caused by the head/outer ear sound diffraction and a notch already present in the source spectrum. Indeed, monaural cues appears to be highly dependent on the source properties, and listening tests demonstrated that humans are more affected by the frequency contents emitted by the sound source than by its true location (Butler and Helwig, 1983).While the directional aspects of localization have been widely studied, distance perception has received substantially less attention. Generally, it is admitted that like angular estimations, sound source distance can also be inferred from various acoustical properties of the sounds reaching the two ears. Known distance dependent acoustical cues include sound intensity, which unfortunately also depends on the intrinsic source energy, as well as on interaural differences, on the spectral shape and on the Direct-to-Reverberant sound energy Ratio (DRR). So, one can see that nearly all the aforementioned cues, which are used to estimate the angular position of a sound source, are also directly linked to the distance parameter. Actually, human most likely combine these distant-dependent cues together with a priori information on the surrounding space and the source of interest so as to get the sensation of a stable distance. The reader interested in this topic will find a comprehensive review in (Zahorik et al., 2005). But one has to keep in mind that human performances in distance discrimination are quite poor. Even under ideal acoustic conditions, the estimated distance appears to be a biased estimate of the actual one, and listening tests have also proven that humans use to significantly overestimate the distance to sources closer than 1m, while they underestimate distances greater than 1m (Zahorik et al., 2005).As shown in the previous paragraph, the head effect on the perceived sounds is frequency dependent. Such a frequency decomposition of the signals is often implemented with a FFT, while other authors proposed the use of classical bandpass filters, see (Youssef et al., 2012). Nevertheless, how efficient they may be, these methods do not perform a frequency decomposition similar to the human inner ear. Gammatone filters, modeling the vibration of the basilar membrane inside the cochlea, have proven well suited to describe the cochlear impulse response in the cat's cochlea (Johannesma, 1972). They also constitute a good approximation of human spectral analysis at moderate sound levels (Patterson et al., 1995). Typical gammatone filters frequency responses are reported in Fig. 3. It can be seen that their bandwidths increase with frequency, in such a way that they represent around 15% of the center frequencies. This is one of the main features of the human auditory system, which confirms our better ability to discriminate low frequencies (Stevens et al., 1937). As a consequence of this decomposition, the representation of any sound signal information is more likely close to the human perception of sounds. As it will be shown in the following, this gammatone frequency decomposition is now very commonly used. Readers interested in more involved auditory models can refer to the Auditory Modeling Toolbox (Søndergaard and Majdak, 2013), available at http://amtoolbox.sourceforge.net.Among all the acoustic features that can be extracted with two microphones, binaural cues are the most often used in robotics. The IPD and/or ILD can indeed lead, with just two microphones and simple computations, to a localization in azimuth. Let IPDexp(f) and ILDexp(f) term the experimental IPD and ILD extracted from the two signals. There exist numerous ways to estimate these IPD and ILD values: computations in the time or frequency domain, bioinspired models, etc. Readers interested in a review of these approaches can consult (Youssef et al., 2012). Whatever the approach, from these experimental values, the problem is then to determine the position of the emitting sources. This involves a model, expressed either in a mathematical closed-form or as experimental relationships uniting the source attributes (position, frequencies, etc.) and the induced auditory cues.Considering the AEG or RAEG model represented by Eq. (2), the delay τ(R)AEG(θ) represents the ITD, and its phase counterpart, i.e. the IPD, then verifies (Nakadai et al., 2001)(5)IPDAEG(θ,f)=2πfτAEG(θ)=2πfaccosθ,(a)IPDRAEG(θ,f)=2πfτRAEG(θ)=πfacπ2−θ+cosθ(b)The main advantage of the first AEG formulation is that the azimuth θ can straightly be approximated by inverting (5a), assimilating IPDAEG(θ, f) to the experimental IPDexp. However, as already outlined, it cannot describe the effect of a head located between the two microphones, inducing scattering of the sound wave. To better take into account its presence, the RAEG model can be used (note that RAEG is analog to the classical Woodworth–Schlosberg formalization (Woodworth and Schlosberg, 1962)). Indeed, results from Nakadai et al. (2001) show that simulations obtained from this model fit experimental measurements in an anechoic room within the frequency band [500–800Hz]. But the RAEG model, while being more realistic than AEG, does not fully account for the head waveguide effect. Additionally, both do not provide any meaningful value for the ILD cue, which is accounted for in the theoretical spherical model. Indeed, one has for this ST model(6)IPDST(r,θ,f)=argHLST(r,θ,f)−argHRST(r,θ,f)(a)ILDST(r,θ,f)=20log10|HLST(r,θ,f)||HRST(r,θ,f)|(b).Compared to epipolar geometries, the scattering theory exhibits more reliable theoretical expressions of both IPD and ILD as functions of the azimuth θ and distance r, and can thus lead to more reliable identification schemes for localization. It is however important to notice that the accuracy of the approach still depends on the capacity to cope with the room acoustics, which is not always possible in practice. Indeed, if the binaural cues are obtained inside in a real robotics environment including noisy sound sources and reverberation, the results may get very bad: since the models do not capture the distortion due to the room acoustics, the theoretical binaural cues and the measured one cannot fit with each. Nevertheless, the ST formalism was exploited in (Nakadai et al., 2003; Handzel and Krishnaprasad, 2002) to express the pressure field perceived by two microphones symmetrically laid over a sphere, and experimentally tested by Handzel et al. (2003) on a spherical plastic head. But whatever the model, and as outlined in human audition, the observed inappropriateness of IPD (resp. ILD) for high (resp. low) frequencies extends outside the scope of AEG, RAEG and ST strategies, as can be seen in Fig. 4. As mentioned in Section 2.2.1, frequencies above about 1400Hz lead to IPD values greater than 2π and becomes ambiguous. Noticeably, the human auditory system then relies on the ILD at these frequencies. Indeed Fig. 4 exhibits high ILD values, reaching up to 25 dB for this frequency domain, in the ST model.Historically, most initial contributions to robot audition were rooted into the binaural paradigm. However, as shown in the following, the results remained mixed when facing real-life environments, involving noises and reverberations together with wideband non-stationary sources.In the early 2000s, the use of interaural difference functions for azimuth localization was deeply studied in the framework of the SIG project (Nakadai et al., 2000, 2002). As the robot cover cannot be perfectly isolated from internal sounds, an adaptive filter exploiting the data provided by inner microphones was used to eliminate the motor noises (e.g. ego-noise mentioned in Section 1) from the audio signal perceived by the external pair of microphones. This active auditory system thus allowed to perform measurement during motion (Nakadai et al., 2000), and constituted an interesting improvement over former methods—for instance (Brooks et al., 1999) on the COG humanoid, or (Huang et al., 1997)—based on the stop-perceive-act principle. On this basis, an “Active Direction Pass Filter” (ADPF) grounded on the ST model was proposed in (Nakadai et al., 2002) to determine the origin of a sound source and extract it out of a mixture of surrounding sounds. This model-matching approach has since been used in a lot of contributions. For a fixed distance rs, for all frequencies under (resp. above) fth=1500Hz, and for each θ, the system computes the theoretical IPDST(rs, θ, f) (resp. ILDST(rs, θ, f)) through (6). Then a cost function dIPD (resp. dILD) is defined to measure the distance between the measured ITDexp(f) (resp. ILDexp(f)) interaural differences and theoretical ones. The two distances dIPD and dILD are then integrated into a belief factor PIPD+ILD(θ). The angleθˆs=argmaxθPIPD+ILD(θ)is then regarded as the sound source azimuth. The sound source separation performances were also evaluated and compared in Nakadai et al. (2002), depending on the model (RAEG vs ST) that relates the source azimuth and the interaural cues. Clearly, the scattering theory provided the best results. So far, these contributions have been among the rare complete binaural audition systems, integrating localization, source separation and recognition. Nevertheless, since HRTFs do not capture the acoustic response of the room where the robot operates, their applicability is generally limited to well-identified environments. One solution could consist in learning the head effect in realistic conditions. Such an idea was successfully assessed in Youssef et al. (2013) through a dedicated neural network able to generalize learning to new acoustic conditions. One can also cite (Berglund and Sitte, 2005), or (Hornstein et al., 2006), where the iCub humanoid robot's head was endowed with two pinnae. The localization is performed by mapping the aforementioned sound features to the corresponding location of the source through a learning method. Another approach is proposed in Kim et al. (2008). Auditory events corresponding to relevant ITD values are gathered into histograms, which are then approximated by Gaussian models whose parameters are identified through the EM method. Peaks in the resulting histogram are then regarded as potential sound azimuths. This allows coping with the multisource case, where multiple sound sources are likely active at the same time. Finally, an implementation of the biology-inspired Jeffress model is proposed in Liu et al. (2008) on a simple robot head endowed with two microphones and stereovision. Interestingly, the ILD pathway is also modeled with a 2D spiking map. The merging of the two interaural maps is also addressed, so as to obtain an efficient sound localization system. The proposed method is shown to share some common well-known properties of the human auditive system, like the ITD maximal efficiency reached when the sound source is in front of the observer. But whatever the approach, ITDs and ILDs can be extracted from the binaural signals in numerous ways: through correlation (shik Kim and Choi, 2009), zero-crossing times comparison (Rodemann et al., 2008), or in the spectral domain (Cavaco and Hallam, 1999). A systematic study of binaural cues, and an analysis of their robustness w.r.t. reverberations is proposed in Youssef et al. (2012). Results show that binaural cues extracted from gammatone filters outperforms other techniques.As indicated in Section 2.2.1, the elevation of a sound source is mainly related to the positions of notches in the spectra of the perceived signals, which stem from acoustic reflections due to the head and the outer ear. In robotics, quite few authors have developed techniques based on spectral cues. Most of them are based on the scattering induced by an artificial pinnae in charge of collecting the acoustic pressure information and of driving it to microphones. For humans, the specific shape of the pinnae enables a selective spatial amplification of acoustic pressure variations, with a quality factor reaching up to 20dB. Reproducing such capabilities in robotics is a difficult problem due to the lack of a model of the pinnae shapes which lead to elevation dependent notches. Yet, as a rule of thumb, these shapes must be irregular or asymmetric, and artificial pinnae were proposed in Kumon et al. (2005), Shimoda et al. (2006), Hornstein et al. (2006), Rodemann et al. (2008), or Saxena and Ng (2009).Fig. 5shows some of them. A simplified model, inspired by Hebrank and Wright (1974) and based on the superposition of the incident wave with a single wave reflected by the pinnae, enables the prediction of the elevation from the position of notches. Noticeably, these notches, which appear or disappear depending on the elevation, may be hard to detect or may even be blurred by spurious notches induced by destructive interferences coming from acoustic reflections on obstacles. To solve this problem, Hornstein et al. (2006) introduces the interaural spectral difference as the ratio between the left and right channel spectra. While notches may be indistinct in the complex spectra of the two signals, the interaural spectral difference, when interpolated with a 12-degree polynomial, can enable the extraction of their frequency positions. Another solution is proposed in Rodemann et al. (2008). It consists in computing the difference of the left and right energies coming from 100 frequency channels, ranging from 100Hz to 20kHz. Strictly speaking, this approach does not involve monaural cues anymore, but allows to obtain spectral cues which are said less sensitive to the source signal frequency content. Concerning the design of the pinnae, a model including a more extensive description of the reflected and diffracted sound waves is proposed in Lopez-Poveda and Meddis (1996). Though it leads to new theoretical expressions of the spectra, it remains hardly valuable for the design of artificial outer ears. Saxena and Ng (2009) also exhibits four different pinnae together with their induced frequency responses for an original work on sound localization from a single microphone. On the other hand, inspired by animals that are able to change the configuration of their pinnae, Kumon and Noda (2011) proposed an active ear, which is able to modify its shape to encode elevation (and azimuth).In the topic of robot audition, distance estimation has been so far based on the triangulation idea. For instance, on the basis of the estimationθˆ1andθˆ2of the azimuth at two distinct positions, triangulation allows to estimate the distance between the robot and the sound source, together with the source azimuth. Generally, this is only possible if the sound source is static in the environment. Some recent works by Markovic et al. (2013) and Portello et al. (2012) proposed a filtering strategy to cope with a possibly moving source. The algorithm mainly relies on ITD to provide an estimation of the source position (r, θ) during the movement of a binaural sensor. Distance estimation is also investigated in Rodemann (2010). Several auditory cues, like interaural differences, sound amplitude and spectral characteristics are compared. Convincing results are shown, exhibiting an estimation error lower than 1 m for a 6 m-far sound source. But the author outlines that its study does not capture the full variability of natural environments. Recent contributions also propose to estimate the DRR. Indeed, it has been shown that distance estimation by humans is more accurate in a reverberant space than in an anechoic one. This estimation is not straightforward: Lu and Cooke (2010) proposes a binaural equalization-cancellation technique, while Vesa (2009) hypotheses the use of the frequency dependent magnitude squared coherence between the left and right signals.

@&#CONCLUSIONS@&#
