@&#MAIN-TITLE@&#
Directional acoustic source orientation estimation using only two microphones

@&#HIGHLIGHTS@&#
We estimate an acoustic source orientation using only two microphones.A simple physical model that explains ITD and ILD variations is proposed.We propose a new directivity model that includes backwards emitted energy.Multiframe estimation is more robust than frame estimation in adverse conditions.In multiframe estimation there is a tradeoff between COR and estimation delay.

@&#KEYPHRASES@&#
Source orientation estimation,Binaural cues,Microphone array,Artificial neural network,

@&#ABSTRACT@&#
A simple physical model consisting of a point source displaced from its center of rotation, in combination with a directivity model that includes backwards emitted energy, is considered for the problem of estimating the orientation of a directional acoustic source. Such a problem arises, for instance, in voice-commanded devices in a smart room and is usually tackled with a large or distributed microphone array. We show, however, that when the time difference of arrival is also taken into account, a small array of only two microphones is sufficiently robust against unaccounted factors such as microphone directivity variation and mild reverberation. This is shown by comparing predicted and measured values of binaural cues, and by using them and pairwise frame energies as inputs for an artificial neural network (ANN) in order to estimate source orientation.

@&#INTRODUCTION@&#
The estimation of the head orientation (source orientation) of a speaker has gained interest recently, with applications such as intelligent robots [1], voice-commanded devices in a smart room [2], control of powered wheelchairs for disabled people [3], speech acquisition in reverberant environments [4], and ultra realistic communications [5] for recording 3D sound with high accuracy.Large microphone arrays of dozens or hundreds of units have been used in the estimation of source orientation [6–12], usually as an extension of source localization [13–17] and tracking [18–22]. Recently, binaural approaches for source localization have been explored [23–27] but without exploring the source orientation problem. In some cases, when a large array is considered [6,10], the estimation of the source orientation is based on the directivity of the source and on the intensity of the signals, although in [9] we verified that taking also time difference of arrival of microphone pairs into account led to a significant increase in the precision of the estimate.The head orientation of human speakers in a smart-room affects the quality of the signals recorded by far-field microphones, and consequently influences the performance of the technologies deployed based on those signals [15]. With the orientation information, several multimodal advanced services can be developed, making head orientation estimation an interesting research topic.The use of a large array may provide robustness against unaccounted contributions to signal intensity, such as microphone directivity and reverberation. In this work, however, aiming at greater practical interest, we consider an array of only two microphones and investigate the use of the binaural interaural time difference (ITD) and interaural level difference (ILD) cues for orientation estimation by means of ANNs. An overview of the approach is given in the following.The observed variations in ITD and ILD caused by variation in the source orientation are tackled by a simple model, comprised of a point source that rotates and a microphone pair. The source directivity, which accounts for the ILD variations, has a model that includes the energy emitted backwards. As for the ITD variations, they result from the location of the point source not coinciding with its center of rotation, and are derived using basic trigonometric functions.The modeled and the measured values agree in that using only binaural ITD or ILD cues leads to an ambiguity in the estimation of orientation. The inclusion of pairwise frame energies as inputs for the estimation procedure is effective in solving this ambiguity.Finally, both single frame and multiframe based estimation are considered, the latter being the natural choice in the practical case of voice-commanded applications.The paper is organized as follows. Sections 2 and 3 introduce a simple radiation model for a directional source and the proposed physical models of ITD and ILD, respectively. Section 4 presents the experimental setup. Section 5 presents the results and the final conclusions are presented in Section 6.A Roland DS-7 loudspeaker was taken as the directional acoustic source. For the sake of simplicity, we assumed that the directivity of the source is modeled by an empirical model and is given by(1)D(ϕ,k,J)=k+cos2J(ϕ/2)k+1,where ϕ is the azimuth with respect to the direction the source is aiming, k and J control the directivity (J=0leads to an omnidirectional pattern whereasJ>0leads to a directional pattern;k>0guaranteesD(ϕ=180°,k,J)>0). This model improves the basic cardioid-like emission pattern, like the one presented in [28], by including the energy irradiated backward, controlled by k. In Fig. 1, the measured directivity of the source used in the experiments is compared to the model withk=0.05andJ=3. Additionally, the measured directivity of the source determined by processing a test signal [31,32], which is averaged over the frequency, is similar to the radiation pattern of the human head [7], which is the case of most interest in this study.As a simple physical model, we consider a point sound source at a distance ρ from its center of rotation and with orientation θ (see Fig. 2). This has in mind a human speaker, with ρ accounting for the distance between the mouth and the neck. Also, we assume the ideal case of no reverberation and an array with identical omnidirectional microphones.With this model, the ILD for a two-microphone array is given by(2)ILD(dB)=DdB(ϕA,k,J)−DdB(ϕB,k,J)−20log10(dA(ρ,θ)dB(ρ,θ)),and assuming thatdA(ρ,θ)dB(ρ,θ)≈1, in which case a point source is at a considerable distance from the microphones, we have(3)ILD(dB)=DdB(ϕA,k,J)−DdB(ϕB,k,J),where the directivity plays the most important role while distance can be neglected.DdB(ϕ,k,J)=10log10D(ϕ,k,J)and varies withϕA=ϕA(ρ,θ)andϕB=ϕB(ρ,θ)(see Fig. 2). For an array with microphone separation ofd=13.6cmand placed symmetrically at a distance ofL=1.0mfrom the source, the values of ILD for three values ofρ={0,2,13}cmas a function of the orientation θ are presented in Fig. 3a.With the same model in Fig. 2, the ITD, in its turn, is given by(4)ITD(s)=dA(ρ,θ)−dB(ρ,θ)c,where(5)dA2(ρ,θ)=(d2+ρsin(θ))2+(L−ρcos(θ))2,(6)dB2(ρ,θ)=(d2−ρsin(θ))2+(L−ρcos(θ))2,are the distances from the source to the microphones, and c is the speed of sound. Under the same previous conditions, the values of ITD as a function of θ are presented in Fig. 3b, for the same values of ρ as in ILD, in number of samplesITD(samples)=fs.ITD(s), wherefsis the sampling frequency. Note that ITD is more dependent on ρ than ILD.As can be seen from the graphs of ILD and ITD in Fig. 3, each of them could be used to estimate the orientation θ of the source, though with an ambiguity between the intervals0°⩽θ⩽90°and90°⩽θ⩽180°(roughly) and also between180°⩽θ⩽270°and270°⩽θ⩽360°. This issue will be examined in more detail in Section 5. For now, we note that the absence of ITD in previous works in the literature as a means to estimate source orientation is possibly due to considering a point source coinciding with its center of rotation (ρ=0), in which case the ITD would be always constant, not varying with the source orientation.Fig. 4illustrates the framework for binaural cues estimation.s(t)denotes the clean speech sample,hl(t)andhr(t)denote the measured binaural IR, subscript l and r denote the left and right microphones in the array,xˆl(t)andxˆr(t)denote the artificial signals, andxˆlandxˆrdenote the pairwise frames used to estimate ITD and ILD.An ANN is a massively parallel distributed structure of interconnected “neuron” elements, each one equipped with a non-linear activation function. By adjusting the weights of the connections in a training phase, the structure can be made to implement a wide range of classification functions (input–output mappings) [29]. In this study, the ANNs were simple fully connected feedforward configurations with one input unit (frame ILD or ITD) or three input units (frame ITD and pairwise frame energies, as discussed in Section 5), one hidden layer, and eight output units corresponding to the tested orientations. The networks were trained by mapping the input values to one of the outputs. The data were divided into three sets, 60% for training, 20% for validation, and 20% for evaluation. A cross-validation was performed five times, with permutation of the original data set. Two measures were used to evaluate the performance of the ANN, the correct orientation ratio (COR), which expresses the agreement between the estimated and the true orientations, and the average orientation error (AOE), which denotes the angle mismatch between the estimated and the true orientations.It is important to stress that the estimation algorithm does not require any a priori information about the parameters in Fig. 2. Of course, in the training phase of the ANN the correct orientation is provided to the training algorithm.The experiments were conducted in a soundproof room 3.0 m long, 2.7 m wide, and 3.0 m high. Its reverberation timeT60was around 130 ms. The measured background noise was lower than 30 dB (A-weighted). A Roland DS-7 loudspeaker was placed on a turntable with 360° of freedom and 1 m above the floor. The distance between the rotation axis of the table and the front of the loudspeaker wasρ=13cm. The array consisted of two omnidirectional Le Son microphones separated byd=13.6cm, 1 m above the floor, andL=1maway from the loudspeaker. A device with A/D and D/A converters (Edirol FA-101) operating at 48 kHz sampling frequency was used for playback and recording.For each of 8 loudspeaker orientations (with 45° shift between each one), a binaural impulse response (IR) was measured [31,32], downsampled to 16 kHz, and then convolved with a set of samples from the TIMIT database (two samples for each of 5 male and 5 female speakers). The TIMIT database is a corpus of read speech of American English and was designed to provide speech data for acoustic–phonetic studies [30]. Using a frame length of 256 samples, a frame shift of 128 samples, and Hamming windowing, frame ITDs were obtained from the generalized cross-correlation with phase transform (GCC-PHAT) function [13] and an interpolation method, for greater precision, to obtain a time-delay resolution lower than one sample as expected in the physical model of ITD. Frame ILDs were taken as the ratio of the frame energies. All processing was executed using Matlab.

@&#CONCLUSIONS@&#
