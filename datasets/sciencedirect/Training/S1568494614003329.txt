@&#MAIN-TITLE@&#
An assessment on producing synthetic samples by fuzzy C-means for limited number of data in prediction models

@&#HIGHLIGHTS@&#
Use of synthetic data in indirect determination of rock strength was investigated.Fuzzy C-means is a practical and suitable method for synthetic data production.Prediction models can be implemented by using 40–50% of measured data.

@&#KEYPHRASES@&#
Fuzzy C-means,Sampling strategy,Adaptive neuro-fuzzy inference system,Limited number of data,Uniaxial compressive strength,

@&#ABSTRACT@&#
For most of rock engineering and engineering geology projects, strength and deformability parameters of intact rocks have crucial importance. However, it is highly challenging to obtain these parameters from weak and very weak rocks due to their nature and testing requirements. For this reason, prediction models are commonly used to obtain desired parameters indirectly. When developing a prediction model, data sets having sufficient size are required. If sufficient data size is not provided for a prediction model, insufficient data problem arises. The main purpose of this study was to investigate the use of synthetic data in indirect determination of rock strength by employing fuzzy C-means (FCM) and adaptive neuro-fuzzy inference system (ANFIS). For the purpose, the experiments were carried out in two stages; (i) uniaxial compressive strength (UCS) prediction by using real data with ANFIS, and (ii) production of synthetic data sets having different sizes, and synthetic data set evaluation in modeling. According to the results obtained, FCM is a practical and suitable method for synthetic data production. Development of prediction models for rock strength by using synthetic data is found to be successful based on statistical performance indices. Additionally, the use of proposed size for synthetic data reduces modeling effort significantly because it eliminates the iterative approach in modeling, hence development of models for limited number of data becomes more practical.

@&#INTRODUCTION@&#
Machine learning and soft computing methods aim to perform predictions or classifications by using domain dependent data. The general processing scheme of these methods includes collection of data, splitting data into train and test sets, building model, training and testing of model, and performance evaluation. When recent literature is reviewed, it can be seen that various methods such as artificial neural networks, support vector machine, decision trees, genetic programming, fuzzy inference systems and adaptive neuro-fuzzy systems are employed to perform predictions or classifications. Despite the diversity and differences in all of these methods, resultant models can be seen as mathematical functions which associate input values with corresponding outputs. In other words, it can be defined as a model trying to find a mapper function (namely target function) to reach outputs from given inputs. In fact, each model includes free parameters which enable to learn and to use training samples for adjusting values of these free parameters. In other words, the model enhances its internal state with each training sample. When the model satisfy a tolerable error rate which specifies learning situation, it is assessed to be constructed. This means, free parameters of the model are adjusted by using the produced target function mapping inputs to outputs. Since the search is guided by the training examples, this paradigm is called, naturally enough, learning from examples [1]. As can be seen, model can learn only what it can learn from training samples. Abu-Mostafa [1] emphasized this point, and continues that if the data are deficient – there may be too few training samples or too much irrelevant information contained in them – the machine will not generalize the problem properly. In the same manner, Cho et al. [2] stated the major assumption is that a sufficient number of well-represented training samples are available. If this assumption is not satisfied, Cho et al. [2] concluded that since problem complexity dictates the size of a network, we may end up with too few training samples for the network of a certain size and this causes an overfitting and poor generalization. As a result, characteristics of data used in modeling may affect performance of a model if data does not have necessary qualification requirements at the beginning.In general, insufficient data problem is encountered frequently in engineering geology and rock mechanics. The intact rock strength, especially uniaxial compressive strength (UCS), is one of the most crucial parameters evaluated in engineering geology and rock mechanics. Hence, the UCS test constitutes the most common applied procedure in order to determine intact rock strength for geo-engineering designs. The UCS tests require high-quality testing specimens. However, necessary requirements for the test procedure defined by ASTM [3] and ISRM [4] are not always able to be provided particularly in very weak and weak rock materials [5]. Poor bonding and presence of cavities and discontinuities in weak rock materials do not allow extracting appropriate samples for the UCS tests [6]. Considering these restrictions particularly observed in very weak and weak rocks, the intact rock strength in terms of UCS is commonly determined by indirect methods such as empirical equations or different types of modeling. The point load index, Schmidt hammer, and sonic velocity tests are the most common indirect methods used in application in order to determine the intact rock strength [7]. Additionally, there are some additional approaches such as block punch index [8,9], cone indentor [10], and core strangle [11], nail and needle penetration tests [12,13] in the literature for this purpose. As mentioned, in order to determine the intact rock strength in terms of the UCS by using these indirect techniques, some empirical equations are needed to be developed for different types of rocks in real world applications. Hence, in order to develop suitable empirical equations, necessary amount of data are required; that means necessary amount of samples are required to be appropriately prepared for the UCS tests as well as the other indirect techniques. This situation brings out another problematic issue in very weak and weak rocks: even in order to apply these indirect techniques for these rock types necessary amount of data are not always provided; hence the empirical equations are usually developed by using limited data mostly acquired in inappropriate conditions. Therefore, most probably depending on this situation described here, Zhang [7] mentioned that different empirical equations result drastically different strength values even for individual rock types. This inference given by Zhang [7] reveals a clear lack of sufficient data in producing of the empirical equations published in the literature. Considering this problem, Zhang [7] suggests some additional tests to be done for specific sites in order to calibrate empirical equations and obtain more reliable results. In order to compensate this vagueness and improve the prediction capacity of empirical equations, Nefeslioglu [5] also differentiates the main rock type into genetic types and produce different empirical equations for different genetic rock types. However, even though the researcher obtained an advance in the explained variance of the models, the problem about sufficient data for reliable statistical evaluations still exists in application. The synthetic data production might be a solution or provide an enhancement in producing empirical equations or prediction models for very weak and weak rock materials in application.When considering the problematic issues described, it is possible to say that reaching necessary amount of data to construct prediction models for rock strength of very weak and weak rocks is sometimes impossible. For this reason, investigation of usability of synthetic data in indirect determination of rock strength is the main goal of this study. In detail, this study considers limited number of data problem in the domain of rock strength prediction and proposes the use of fuzzy C-means (FCM) to produce synthetic data from measured data. For this purpose, the data set including total 66 cases with 3 input parameters (water content, w; unit weight, γ; and ultrasonic pulse velocity, Vp) and 1 output (uniaxial compressive strength, UCS) were selected from the publication of Nefeslioglu [5], and the adaptive neuro-fuzzy inference system (ANFIS) was used as the prediction method. In the experiments, the measured data was used for training and testing at first to observe the behavior of the data, then synthetic data with different sizes were produced with FCM. The models were trained with these data, and testing was implemented with the whole measured data.In the literature, size of data is considered in two ways (i) insufficiency in data belonging one class, it is named as “imbalance data problem” [14], (ii) insufficiency in whole data, it is named as “limited number of data problem”. The collection of widespread data could be difficult or almost impossible in many research areas because of many reasons such as environmental factors, cost, security, rare occurrence, scarcity in events, etc. Vladimir [15] states that data size is accepted as small or limited if the ratio of the number of training samples to the Vapnik–Chervonenkis dimensions (VC dimensions) of a learning machine function is less than 20. Aslan et al. [16] define VC dimension as follows; if we can separate the positive and negative data points by using some hypothesis h from the hypothesis class H, then we say H shatters N points and the maximum number of points that can be shattered by H is called VC dimension of H and measures the capacity of the hypothesis class H. As can be seen from the definition, determination of the VC dimension value requires experimental process. Aslan et al. [16] presented VC dimension calculation for decision trees. If large finite set N can be shattered arbitrarily by H, then VC(H)=∞ and practical advice for VC dimension of artificial neural networks (ANNs) is VC(H)=#parameters [17]. Additionally, the continuity characteristic of limited number of data sets may cause additional complexity due to overlapping ranges of input variables which result in different output values and this situation may increase the VC dimension of the learning function [18].Learning from small and continuous data sets is actually difficult because it makes models prone to overfitting, and also makes hard to find specific correlations between inputs and outputs. Use of synthetic data produced from measured or observed data is a promising approach to solve this problem. Previous studies have brought statistical point of view to explore this problem. Many researchers [19–21] proposed mixed linear models to analyze data [22]. Among them, of the model proposed by Schwarz [23] can be assumed to be different because this study suggested the Schwarz Information Criterion using a Bayesian perspective to select the model which has the most probability of being correct. The other approach published in recent literature is to generate virtual data with the aim of extending measured data to reach acceptable size or to be used in training phase of modeling. Abu-Mostafa [1] named domain knowledge of problem as hints and propose to generate virtual samples by using hints. Cho et al. [2] proposed the use of virtual samples generated by picking a nearby point of measured sample for its input. Li et al. [24] proposed the use of functional virtual population (FVP) to generate more samples to train the model. Li and Lin [25] marked the necessity of collection of a big number of data and proposed the use of intervalized kernel methods to improve kernel density estimation and virtual sample generation to produce extra information for expediting learning. Li et al. [26] proposed a mega-trend-diffusion technique to estimate the domain range of a limited number of data set and produce artificial samples for training the modified back propagation neural network. Tsai and Li [27] emphasized that artificial neural networks were widely utilized to extract management knowledge from acquired data, sufficient training data was the fundamental assumption and, they applied bootstrapping to generate virtual samples in order to fulfill the data gaps. Additionally, Ivanescu et al. [28] also proposed bootstrapping to overcome the limited size of data problem. In fact, bootstrapping is based on the assumption that available data set is a particular manifestation of unknown probability distribution [29]. Li and Li [30] paid attention on cost and time limit factors to collect samples in the early stages of manufacturing systems and, they proposed the use of virtual samples generated from Weibull distribution to extend the data set. Li and Liu [22] emphasized insufficiency and incompleteness characteristics of limited number of data sets, and proposed a new neural network that conceptually followed the concept of cognitive theory to deal with the limited number of data set problem. Sen et al. [18] presented the comparative results of ANFIS and genetic programming in which synthetic data was used in training and concluded that ANFIS was more successful than genetic programming. Cailo and Reiter [31] presented the adaptation of random forests to generate partially synthetic data for categorical variables. Sakshaug and Raghunathan [32] presented posterior predictive distribution, and Drechsler [33] presented the use of support vector machines to produce synthetic data for limited number of data set for testing. Hao et al. [34] used synthetic examples along the line segments joining the k nearest neighbors. Gao et al. [35] proposed the use of kernel density estimation of data to obtain synthetic samples. Dubey et al. [36] employed K-Medoids technique for synthetic data production. Most of these studies used the methods which are hard to understand, to overcome the issues sourced from learning with limited number of data sets, and application of these methods in practice has still been challenging. As a result, better and easy to use methods to produce synthetic data for limited number of data sets are needed to represent hidden structures or information within the data.The data published by Nefeslioglu [5] was employed in this study. It includes 5 rock properties of 66 intact rock samples acquired from 5 boreholes [37] drilled in the Firuzkoy area of Istanbul, in Turkey. The intact rock samples were particularly collected from a geological material namely Acmalar member of Danisment formation [38,39] by Nefeslioglu [5]. The Acmalar member of Danisment formation consists of sedimentary rocks mainly including claystones and mudstones [39,40]. Nefeslioglu [5] stated that the geologic member constitutes a good example for very weak and weak rock material. Hence, considering the main purpose of this study, the data published by Nefeslioglu [5] can also be assumed to be a good experimental case for this research. The rock parameters evaluated in the study are given as follows; water content (w, %), unit weight (γ, kN/m3), ultrasonic pulse velocity (Vp, km/s), uniaxial compressive strength (UCS, MPa), and elastic modulus of intact rock (Ei, GPa). Three of these parameters were evaluated as the independent variables while the last two parameters uniaxial compressive strength and elastic modulus of intact rock were handled to be the dependent variables of the prediction models. The descriptive statistics of the independent parameters are given in Fig. 1while those of the dependent parameters are shown in Fig. 2. The water content of intact rock samples vary between the values of 2.43% and 8.37%. The values of the parameter unit weight vary in a narrow interval with a mean value of 18.17kN/m3. The minimum and maximum values of the ultrasonic pulse velocity values of the rock materials were calculated to be 0.468km/s and 1.366km/s, respectively. Additionally, variation ranges of the dependent parameters evaluated in this study are given to be 0.680MPa and 4.058MPa for the uniaxial compressive strength and 0.030GPa and 0.309GPa for the elastic modulus of intact rock. Considering the descriptive statistics given for the strength and deformation parameters, it could be clearly realized that the rock materials investigating in the study are just in the range of very weak and weak rocks in terms of rock strength [5]. In order to evaluate multicollinearity problem in the independent variables, the correlation matrix including both independent and dependent variables were prepared (Table 1). For the purpose, bivariate correlations were investigated and the Pearson correlation coefficients were calculated. The Pearson correlation coefficients calculated for the independent variables were not found to be significant at the significance level of 0.01. This finding points out that there is no multicollinearity problem between the independent variables. Hence, the independent variables are able to be used in the prediction models investigating in the study. According to the results of the bivariate correlation analyses, it is also revealed that the independent variables unit weight and ultrasonic pulse velocity have meaningful relations between both dependent variables uniaxial compressive strength and elastic modulus of intact rock at the significance level of 0.01.In this study, k-fold cross validation was used to produce the measured train data set, FCM was used to produce synthetic train data set and the ANFIS was used to build prediction models. Ongoing part of this section, definitions and implementation details are provided for each of the three methods.In learning based models, train data sets consist of cases which are randomly picked up and remaining cases are used for testing in general. Additionally, this random selection procedure is iterated many times to observe system performance in average, in max and in min. In this scheme, there is no constraint about randomness. In other words, test data sets employed in different iterations may partially overlap. In k-cv (k-fold cross validation), whole data set is divided into k subsets. In each of iterations, one set is selected for testing and the other (k−1) sets are selected for training. As a result, there occur total k iterations and test data sets, which are used in each of iterations, will be completely different from each other. At the end, the average of obtained k results can be used as single output or they can be discussed one by one. k-cv can be used with various artificial intelligence techniques, and also it is used in modeling problems of different areas [i.e. 41, 42]. Although, the integer value from the interval of [5–10] is commonly used for k value, it is specified as 4 in this study. Since, k values which are higher than 4 cause too small test size, “4” is selected without losing acceptable iteration number. As a result, total 66 cases are split into 4 sets with the size of 17, 17, 16 and 16 respectively. In other words, almost total 49 cases are used for training and 17 cases for testing.Fuzzy C-means is proposed by Bezdek [43] and it tries to apply fuzzy approach to clustering operation. The fuzzy C-means algorithm is a clustering method widely used in pattern recognition works. Actually, the algorithm makes soft partitions where a datum can belong to different clusters with a different membership degree to each cluster [44]. Clustering aims to put similar instances in the same cluster and the instance can be member of one cluster in crisp approach. In FCM, each instance can be member of more clusters with the computed membership degrees. As a result, inner instances of the cluster have higher membership degrees than the outer instances of the same cluster. FCM enables to handle fuzziness in clustering as opposed to crisp approach and its algorithm is highly similar to K-means clustering. In the same manner as K-means, FCM algorithm requires specification of desired number of clusters at the beginning. Then each cluster centroid is initialized randomly. Iterative process is then followed for calculation of the centroids until convergence criteria is met. In this step, each centroid is computed by using Eq. (1).(1)cei=∑j=1N(μci(xj))m⋅xj∑j=1N(μci(xj))mwhereμci(xj)means the membership degree of the jth instance “x” to cluster “c”, N is the number of instances and ceiis the centroid of ith cluster. After calculation of the cluster centroid, membership degree of each instance to the cluster, which instance is placed in, is calculated by using Eq. (2).(2)μci(xj)=1∑k=1C(||xj−cei||/||xj−cek||)2/(m−1)where “c” means the total number of cluster and cekis the centroid of the kth cluster.The “m” adjusts the degree of fuzziness which will be handled in clustering and the interval of [1.5–2] is advised for it by Bezdek et al. [45]. The convergence criterion which controls the iterations in the algorithm is a small value denoting the difference in the membership degrees of cases between previous and current iterations. FCM clustering stops when the distance, which is calculated with Euclidian approach in general, between the membership degrees of previous and current iterations, is less than the specified convergence criteria.FCM splits data into the predefined number of clusters instead of natural clusters in the data. This can be seen a disadvantageous at first, but it enables us to create meaningful clusters as many as we need. When the aim of this study is considered, the relationship between the cluster numbers and the amount of synthetic data, which should be produced, becomes clear. In detail, if a data set is used in a supervised method, 80% of it used for training in general. Additionally, if data is also small, 80% of it becomes nearly whole data and a little instances remain for testing. In this study, use of the original data in testing was aimed and the size and quality of the synthetic data used for training were investigated. The data used in this study is small and continues. While small aspect of the data forced us to use of synthetic data, the continuous character of it forced us to consider fuzzy approach because, fuzzy approach enables us paying attention on overlapping ranges of the input parameters which have different output values. In other words, if an instance is on the boundary of a cluster then it means that this instance have some closeness with other instances in other clusters. Centroid calculation scheme of FCM reflects all of these closeness relationships. As a result, the data which are found to be the centroid of the classes in different random samplings can be identified as good candidates for being synthetic data.In this study, two types of models were constructed. One type of these models has 3 input parameters while the other has 2 input parameters. During the modeling processes, same data set (total 66 cases) was employed. In models having 3 parameters, 17 (25% of whole data), 21 (32%), 25 (38%), 29 (44%) and 33 (50%) clusters were produced and their centroids were merged in order to obtain train data set. Additionally, in models having 2 parameters 8 (12% of whole data), 13 (20%), 17 (25%), 21 (32%), 25 (38%), 29 (44%) and 33 (50%) clusters were also produced, and their centroids were again merged in order to obtain train data set. While using 2 parameters, number of 8 and 13 were also investigated because parameter reduction means complexity decrease in prediction models. Since optimal size for synthetic data was explored here, we have to observe less complex models for much smaller sizes. All clustering operations were implemented within MATLAB R2009a (version 7.8.0.347). The details of the sampling methodology carried out in this study is given in Fig. 3.The last point which should be clarified about FCM is that at each run of the algorithm, the initial clusters are generated at random. It means that different centroids may be produced with FCM, if you run it again and again. This is true for large data sets, and many times smaller cluster numbers. However, if limited number of data sets are used with large cluster numbers as used in this study then this randomness is highly eliminated because limited number of data sets mean small data space, and when high cluster numbers are used, it bounds the alternatives for initial clusters. As a result, FCM starts to produce same or very close cluster centroids in different runs of it.Adaptive neuro-fuzzy inference system which is a soft computing method for complex problems is presented by Jang [46]. It is a supervised method and combines the advantages of fuzzy interference system (FIS) and artificial neural networks (ANN) methods. In other words, ANFIS is a special type of ANN which consists of total six layers such as one input layer, one output layer and four hidden layers. ANFIS always produce one output and, number of neurons used in hidden layers reflecting the rule numbers. ANFIS uses Takagi and Sugeno's type rules [47] such as “If x is A and y is B then z is f(x,y)”. When f(x,y) is a constant, a zero order Sugeno fuzzy model is formed, which may be considered to be a special case of Mamdani fuzzy inference system [48] where each rule consequent is specified by a fuzzy singleton and if f(x,y) is taken to be a first order polynomial then a first order Sugeno fuzzy model is formed [49]. In this study, type-3 fuzzy inference model [46] was used and 2 types of ANFIS models were built with 2 (w, Vp) and 3 (w, γ, Vp) parameters. In the ANFIS modeling, number of fuzzy sets for each input specified as 2 (low–high). Consequently, total 4 (2×2) and 8 (2×2×2) rules were employed for models having 2 and 3 parameters, respectively. Typical ANFIS model developed with 2 input parameters in this study is given in Fig. 4.The responsibilities of the layers in Fig. 4 can be summarized as follows: layer 0 is responsible from presentation of the inputs to the layer 1. Layer 1 fuzzifies the inputs by using specified membership functions. Gauss shaped membership function type was used in this study. Layer 2 keeps rules as nodes, in other words, each rule corresponds a node which calculates the firing strength of associated rule. Layer 3 uses nodes as much as rule number and normalizes the firing strengths of rules. Normalized results mean the ratio of rule's firing strength to sum of firing strengths of all rules. Layer 4 includes adaptive nodes which calculate the contribution of each rule toward overall output. Layer 5 contains only one node and computes the output. All properties of the ANFIS models developed in this study by using MATLAB R2009a are given in Table 2.An ANFIS model uses a hybrid learning algorithm that combines the least squares estimator and the gradient descent method [46]. Least squares estimator is employed in forward pass and coefficients in the polynomial expressions are tuned. Additionally, gradient descent method is employed in backward pass, and parameters of membership functions are adjusted. One epoch consist of forward and backward passes and model training goes on until the stopping criteria is met or specified epoch number is reached. In this study, the number 50 was specified as the epoch number for all models.

@&#CONCLUSIONS@&#
