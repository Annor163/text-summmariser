@&#MAIN-TITLE@&#
Notes on technical efficiency estimation with multiple inputs and outputs

@&#HIGHLIGHTS@&#
Collier, Johnson, and Ruggiero (2011), deal with the problem of estimating technical efficiency.They allow multiple inputs and outputs and recommend using regression models.We examine critically their recommendations.We provide an alternative maximum likelihood procedure that performs well.

@&#KEYPHRASES@&#
Efficiency,Least squares,Multiple-output production,Limited information maximum Likelihood,

@&#ABSTRACT@&#
Collier, Johnson and Ruggiero (2011) deal with the problem of estimating technical efficiency using regression analysis that allows multiple inputs and outputs. This revives an old problem in the analysis of production. In this note we provide an alternative maximum likelihood estimator that addresses the concerns. A Monte Carlo experiment shows that the technique works well in practice. A test for homotheticity, a critical assumption in Collier, Johnson and Ruggiero (2011) is constructed and its behavior is examined using Monte Carlo simulation and an empirical application to European banking.

@&#INTRODUCTION@&#
To make things concrete, suppose we have multiple outputs in vector yit∈ ℜM, and there exists an aggregator function of the constant elasticity of substitution (CES) type:(1)g(yit)=(α1y1,itρ+α2y2,itρ)1/ρ,i=1,…,N,t=1,…,T,although other aggregator can serve just as well. The model is:(2)g(yit)=f(xit)+vit−uit,where f(xit) is a functional form that shows how inputsxit=[x1,it,…,xK,it]′∈ℜKcontribute to the production of aggregate output,vitis a two sided error term and uit≥ 0 represents technical inefficiency. For example, with a Cobb-Douglas functional form and K inputs, Eq. (2) is:(3)lng(yit)=β0+∑k=1Kβklnxk,it+vit−uit,i=1,…,N,t=1,…,T.There are certain important issues in technical efficiency analysis with multiple outputs. Since outputs are jointly produced given the inputs, Eqs. (2) or (3) do notspecify a joint system of equations for yit. In fact, with M outputs,M−1equations are missing, as noted by Fernandez, Koop, and Steel (2002). Therefore, the aggregator function approach is not enough for a complete analysis of the problem.It is a, relatively, common practice in the literature to assume, say,11To the author’s knowledge the only reason why a Cobb-Douglas aggregator may be undesirable is because it does not satisfy the second order conditions of profit maximization. A Cobb-Douglas aggregator is, however, consistent with cost minimization. Of course it is also undesirable in the sense that it is not flexible enough like the CES or translog functional forms. Here we use it for simplicity in presentation of the main points.g(yit)=y1,itα1y2,itα2and use OLS in the following equation22This, and similar practices, are nicely reviewed in Kumbhakar and Lovell (2000, pp. 93–95) and the cited references.:(4)α1lny1,it+α2lny2,it=β0+∑k=1Kβklnxk,it+vit−uit,or, alternatively, in obvious notation:(5)lny1,it=β0+γ0lny2,it+∑k=1Kγklnxk,it+1α1(vit−uit).This equation shows plainly that we have an endogenous variable in the right-hand-side of the equation and, therefore, OLS cannotbe used because of correlation with the error term,vit−uit. This is very often forgotten in applied research. This point applies not only to regressions involving aggregator functions but also to distance functions. Input- or output-oriented distance functions are homogeneous of degree one with respect to inputs and outputs, respectively. Therefore, imposition of homogeneity results in a form similar to (5).The proper method of estimation is limited information maximum likelihood (LIML) if the inputs are exogenous and prices are not available33If prices are available the system can be completed using the first order conditions from cost minimization or profit maximization. An alternative has been introduced by Atkinson and Tsionas (2015) where the first order conditions are used, and unobserved prices are treated as latent variables in the context of a Bayesian hierarchical model.. However, this is also a problematic assumption.Under specific behavioral assumptions, inputs are endogenous. For example under profit maximization or cost minimization, inputs are endogenously selected and outputs are, respectively, endogenous or predetermined. In effect, OLS is not the proper method of estimation. Using LIML requires predetermined variables like prices (which, by assumption, are not available.) However, lagged values of xitand yitcan be used as they are predetermined from the point of view of period t, providedvitand uitare not autocorrelated. An alternative estimation technique is the generalized method of moments (GMM) with a one-sided error term and explicit correlation allowed between xitand the error components (Tran & Tsionas, 2013).Collier et al. (2011), CJR henceforth, deal with the problem of estimating technical efficiency using OLS regression analysis that allows multiple inputs and outputs. Specifically, technical efficiency can be estimated using regression models with multiple inputs and outputs without input price data. CJR propose to use DEA analysis in a first stage -without the input constraints since they assume separability from inputs- to derive an aggregate output measure, say Sit. The DEA problem solved in CJR is the following, for each observation “o”:(6)max:Θo,∑j=1NTΛjyjm≥Θoyom,m=1,…,M,∑j=1NTΛj=1,Λj≥0,j=1,…,NT.Aggregate output is defined as:(7)So=1Θo,o=1,…,NT.Then, instead of (2) they propose regression analysis in the following model:(8)logSit=f(xit)+vit−uit,i=1,…,N,t=1,…,T.As xitis often used in logs in (8) a Cobb-Douglas function has the following form:(9)logSit=β0+xit′β+vit−uiti=1,…,N,t=1,…,T.This methodology has been applied by Collier, Mamula, and Ruggiero (2014). Under homotheticity, this bypasses the problem of having an endogenous variable in the right-hand-side, and falls in between DEA and stochastic frontier analysis: It allows measurement error in the aggregate output, and statistical assumptions about the error termsvitand uit. However, it does not addresses concerns about the potential endogeneity of inputs, in which case xitandvitare correlated.The main problem, however, is not to seek an output aggregator, since homotheticity cannot be taken for granted. In Fernandez et al. (2002) the problem in (3) is recognized for what it truly is, viz. several inputs xitare used to jointly produce outputs yit. The problem is not which aggregator function must be used (a CES would be just fine for most purposes) or how to aggregate the outputs, but how to account for the endogenous character of yitin this context. From the econometric point of view, the problem is that with M outputs there are M endogenous variables but only one equation, viz. (2) or (3). Therefore, there areM−1missing equations to complete the system.The question is how to deal with the problem when both outputs and inputs are endogenous and prices are not observed. To complete the model provided by (3) we consider the reduced form:(10)[y˜itxit]=Πzit+ɛit,wherey˜it=[y2,it,…,ym,it]′,zitis a vector of predetermined variables,ɛitis a vector of error terms andΠis a matrix of unknown parameters. Under the assumption thatuit=ui,∀t=1,…,Tare fixed parameters and:(11)[vit,ɛit′]′∼NM(0,Σ),the system of (3) and (10) can be estimated using limited information maximum likelihood (LIML) along the lines suggested by Pagan (1979). The variables in zitinclude only firm and time dummies so that it is not necessary to think about the possibility of other instruments, which may not be available at all in practice. For example, the use of lagged values ofy˜itand xitis often problematic if they are only weakly correlated with the endogenous variables.As in CJR, we assume time-invariant technical inefficiency, that isuit=ui,∀t=1,…,T. As the inclusion of firm-specific effects in (10)prevents the identification of uis we use the nonlinear transformation:(12)ui=exp(−φi2),∀i=1,…,n,where the φis are unrestricted. With this transformation, the uis are always positive and less than one, thus making it unnecessary to apply a corrected ordinary least squares (COLS) transformation, viz.u^i=ui−maxi=1,…,nui,to obtain technical inefficiencies. To the author’s knowledge, this transformation has not been used before although it has considerable merit, at the cost of requiring nonlinear estimation techniques.To see specifically how the problems mentioned in the previous section can be addressed, we consider a model using (1) withρ=12. The first output is are generated from a standard lognormal distribution. Next, we generatevit∼N(0,σv2). The three inputs (in log terms) are generated as follows:(13)x1,it=αvit+Ditγ1+ξ1,it,x2,it=x1,it+Ditγ2+ξ2,it,x3,it=x1,it+Ditγ3+ξ3,it,where ξj, it∼ N(0, 1). Therefore, inputs are mutually correlated as well as correlated withvit. The correlation coefficient between x1, itandvitisϱ=ασv2+1σvα2σv2+1. Givenσvwe can vary α so that we obtain different values of ϱ. Moreover, Ditis a vector of firm and time dummies andγjare respective coefficients, which are generated from a uniform distribution in [1, 1]. We haveN+T−1coefficients in each input equation.The model is the same as in (3) with constant term -1 and slope coefficients equal to13. Finally, the second output is generated from (2)wherevit∼N(0,σv2)and independentlyuit∼N+(0,σu2). Denoteσ=σv2+σu2andλ=σuσv. For practical reasons, we can setσ=0.3andλ=1which is a typical case in empirical studies and examine the rank correlation between true and estimated inefficiencies for various values of ϱ. The number of firms is N, the number of time periods is T and we assumeuit=ui,∀t=1,…,T.LIML is implemented using a standard conjugate-gradients algorithm without analytic derivatives and we consider 10,000 alternative data sets. Our evidence is summarized in Table 1. Clearly, the behavior of OLS is disappointing, as expected, and gets worse in larger sample sizes as ϱ increases, contrary to LIML which performs well across the board. Another problem is how the two-stage method of CJR performs when homotheticity is violated. For simplicity in presentation, we proceed with the following specification:(14)g(yit)=(1−ψ)(α1y1,itρ+α2y2,itρ)1/ρ+ψ(α1x1,itτ+α2x2,itτ+α3x3,itτ)1/τ.We setα1=α2=α3=13andτ=12. The parameter ψ∈ (0, 1) measures the degree of departure from homotheticity (which corresponds toψ=0). As this does not affect the behavior of LIML, we focus on how different values of ψaffect the CJR estimator. A fair comparison with CJR must be based on taking account of the endogeneity between xitandvit. Therefore, the regressors are replaced by the fitted values of regressions in (13)as is commonly done in two-stage least squares.44All computations are performed in Fortran 77 using g77. The envelopment problems in (6) and (15) are solved using DLPRS or DSLPRS. The conjugate gradients technique is implemented using DUMCGG in the IMSL library, version 5.From the evidence in Table 1 it turns out that even minor departures from homotheticity have an important impact on efficiency estimates obtained through CJR. As the rank correlations tend to stabilize by increasing N and T it becomes clear that there is (a significant) asymptotic bias when homotheticity is violated.The important question in relation to the CJR technique is whether we can construct a test that has power against violations of homotheticity. The test can be used to decide whether homotheticity is satisfied so that CJR can be applied in practice. Suppose SitandS˜itare the output indicators obtained using, respectively, CJR (see program (6)) and an “output indicator” that one obtains by including the input constraints in their DEA problem. This problem is the following:(15)max:Θ˜o,∑j=1NTΛjyjm≥Θ˜oyom,m=1,…,M,∑j=1NTΛjxjk≤xok,k=1,…,K,∑j=1NTΛj=1,Λj≥0,j=1,…,NT.Aggregate output is constructed similarly to (7). Consider the test:(16)D=(NT)−1∑i,t|Sit−S˜it|12(Sit+S˜it).The test measures relative deviations between the two aggregate output indicators relative to their simple average. The average is used so that we do not have to choose which case is a benchmark, viz. (6) or (15). We use the same data generating process as before. We consider only the typical caseϱ=0.3,which is more favorable to regressions using (6). We report size and power results in Table 2. The test is, approximately, correctly sized forNT=50,approximately, and it is almost 5% forNT=100. The power of the test in (16)is reasonable even when the sample size is small (NT=20for example) and deviations for homotheticity are minor (ψ=0.01). These results are encouraging and show that the D-statistic can be used in the CJR approach to detect whether violations of homotheticity in the data are statistically significant.In this section we present an empirical application to European banking along with some important clarifications for the implementation of LIML. Our data set includes prices of labor, capital, deposits. The two outputs funds are loans and other earning assets. We have also obtained the quantity values for loans, deposits and other earning assets as well as other funds. Labor quantity is obtained as personnel expenses divided by the price of labor. Capital is defined as total fixed assets divided by the price of capital. Quantities for loans, deposits and other earning assets are obtained from total values divided by respective prices. Total assets and equity (in logs) are included as environmental variables in all models. The data is for 27 European countries over the period 2006–2011 and were retrieved from Datastream. The total number of observations is 27,187. To minimize computational costs in the implementation of CJR we consider only large banks, viz. those who have total assets in excess of the ninth decile.First, we present our translog specification under the assumption of output homotheticity. This is given by the following:(17)∑m=1Mαmym,it+12∑m=1M∑m′≥mMαmm′ym,itym′,it=β0+∑k=1Kβkxk,it+12∑k=1K∑k′≥kKβkk′xk,itxk′,it+vit−uit.Without the assumption of output homotheticity, the specification is as follows:(18)∑m=1Mαmym,it+12∑m=1M∑m′≥mMαmm′ym,itym′,it=β0+∑k=1Kβkxk,it+12∑k=1K∑k′≥kKβkk′xk,itxk′,it+∑k=1K∑m=1Mγkmxk,itym,it+vit−uit.The constant term of the output “aggregator” in the left-hand-side cannot be identified separately and it is merged with an overall intercept, β0. Under the KM restrictionsγkm=0,k=1,…,K,m=1,…,M,(18) reduces to (17). LIML is implemented to estimate the more general form in (18). The instruments zitin (10) now include time-specific and bank-specific dummy variables, log of total assets and log of equity. It is important to mention that LIML does not depend on assigning one output as the “left hand side” variable in (18), see Schmidt (1976, p. 171). Let us write (18) in compact form as follows:(19)α′yit+12yit′Ayit=β′xit+12xit′Bxit+xit′Γyit+vit−uit.The definition of the various vectors and matrices is as follows:α=[α1,…,αm]′,β=[β1,…,βK]′,A=[αmm′,m,m′=1,…,M],B=[βkk′,k,k′=1,…,K],Γ=[γkm,k=1,…,K,m=1,…,M].The matricesAandBare symmetric. Supposeϵit=vit−uit. Taking derivatives,∂ϵit∂yit=α+Ayit+Γxit,an M × 1 vector. In the system consisting of (19) and (10) the Jacobian of transformation from[ϵit,ɛit′]′to yitis:(20)Jit=|∏j=1M[α+Ayit+Γxit]j|,i=1,…,N,t=1,…,T,where [a]jdenotes the jth element of vector a. Under normality ofvitthe likelihood function can be formulated easily taking also into account the Jacobian term in (20).From LIML estimation of (18) the F-statistic to test homothetic production, viz.γkm=0,k=1,…,K,m=1,…,Mhas a p-value very close to zero (0.00072) suggesting that, statistically, homotheticity is rejected through this parametric form. The D-statistic yields an empirical p-value of 0.00014. The empirical p-value is obtained through a bootstrap procedure (using 200 replications to minimize computational costs) where the data are re-sampled in blocks corresponding to the same bank over time. We follow the bootstrap implementation in Simar and Wilson (2000, 2011). This implementation is rather computationally intensive. Since homotheticity is rejected, the CJR techniques are not reliable as it has been stated in the previous section. Nevertheless, a comparison is interesting. Another problem is that the assumption of time-invariant inefficiency, in the second stage of CJR, is not likely to hold. Therefore, both CJR and LIML are implemented using the assumption that technical inefficiency is constant for the sub-periods 2006–2007, 2008–2009 and 2010–2011. The p-values of the F-statistic and the D-statistic are practically zero when the models are re-estimated in the three sub-periods. For a fair comparison between CJR and LIML, a translog functional form is estimated in the second stage of CJR where trend, and logs of total assets and equity are included also as regressors.The empirical results for technical efficiency scores are provided in Table 3. Apart from CJR and LIML we also consider another estimator, CJR/2SLS (2SLS stands for two-stage least squares). This is similar to CJR but instead of OLS, we use 2SLS in the second stage of CJR. The instruments are fitted values from OLS regressions of inputs on the instruments zitin (10) used in LIML. This may provide a more fair comparison between CJR and LIML.Technical efficiency, according to LIML, is 91.9% for 2006–2007, 73.4% for 2008–2009 and 88.4% for 2010–2011. CJR estimates are 87.2%, 81% and 91.3% respectively. The (rank) correlation coefficients between efficiency scores are very low at 0.202, 0.014 and 0.181 for the three sub-periods. The low rank correlations and the differences in estimated scores (as well as their sample standard deviations) are, probably, attributed to the violation of homotheticity in this data set. Notably, efficiency scores from LIML drop during 2008–2009, when the sub-prime crisis occurred, to recover in the sub-period 2010–2011. CJR scores also drop but not as much as those derived from LIML. The use of 2SLS in CJR yields similar results with respect to rank correlations with LIML although the first two moments of CJR and CJR/2SLS seem to be different.

@&#CONCLUSIONS@&#
This study addresses concerns raised by CJR. The main concern is how to use econometric tools in the presence of multiple inputs and multiple outputs when prices are not available. CJR assume homotheticity and propose a two-stage approach to the problem. In the first stage, a DEA problem is formulated that drops the input constraints, and a composite output aggregator is obtained. In the second stage, OLS is used to regress the composite output aggregator on inputs and obtain efficiency measures. This also allows for noise in the second stage. We show that a more promising approach to the same problem is to couple an explicit multiple-output specification with a reduced form for the endogenous inputs and outputs. LIML is used to estimate the system and obtain estimates of technical inefficiency. Moreover, a test is proposed to examine whether deviations from the critical homotheticity assumption are statistically significant. The approach is shown to perform well in Monte Carlo experiments and an empirical application using European banking data.