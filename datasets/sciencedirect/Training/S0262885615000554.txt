@&#MAIN-TITLE@&#
Effects of texture addition on optical flow performance in images with poor texture

@&#HIGHLIGHTS@&#
An initial step for optical flow estimation in poorly-textured images is proposed.The simple yet effective step preserves motion boundaries where other methods fail.The proposed algorithm reduces computation time meaningfully.Mathematical analysis is employed to explain the advantages provided.Quantitative measures have been introduced to assess the performance.

@&#KEYPHRASES@&#
Optical flow,Poor texture,Foreground detection,Laws' masks,F-measure,Boundary displacement error,Condition number,

@&#ABSTRACT@&#
This paper investigates the effects of adding texture to images with poorly-textured regions on optical flow performance, namely the accuracy of foreground boundary detection and computation time. Despite significant improvements in optical flow computations, poor texture still remains a challenge to even the most accurate methods. Accordingly, we explored the effects of simple modification of images, rather than the algorithms. To localize and add texture to poorly-textured regions in the background, which induce the propagation of foreground optical flow, we first perform a texture segmentation using Laws' masks and generate a texture map. Next, using a binary frame difference, we constrain the poorly-textured regions to those with negligible motion. Finally, we calculate the optical flow for the modified images with added texture using the best optical flow methods available. It is shown that if the threshold used for binarizing the frame difference is in a specific range determined empirically, variations in the final foreground detection will be insignificant. Employing the texture addition in conjunction with leading optical flow methods on multiple real and animation sequences with different texture distributions revealed considerable advantages, including improvement in the accuracy of foreground boundary preservation, prevention of object merging, and reduction in the computation time. The F-measure and the Boundary Displacement Error metrics were used to evaluate the similarity between detected and ground-truth foreground masks. Furthermore, preventing foreground optical flow propagation and reduction in the computation time are discussed using analysis of optical flow convergence.

@&#INTRODUCTION@&#
Accurate optical flow computation is crucial in many computer vision tasks, including motion estimation, object detection, and tracking. Three decades after the seminal contribution by Horn and Schunck [1], accuracy of optical flow computation methods have been improved significantly. However, images with poor texture, especially in the background, which occur in many sequences, still remain a major challenge in this field [2]. Since solving for optical flow components using the optical flow constraint is an ill-posed problem with two unknowns and one equation, there is a need for extra constraint(s). Spatial smoothness of optical flow components introduced by Horn and Schunck (HS) is one of the most common constraints used in different publications with various modifications, such as in [3–6]. The smoothness constraint causes the blurring of computed motion at the object boundaries, together with spread of foreground non-zero flow to the neighboring background pixels.As we will see in Section 2.1, while making optical flow computation possible, in images with poorly-textured regions, the smoothness constraint leads to some disadvantages, such as considerable deformations in the size and the shape of the detected foreground objects, and accordingly in the position of the center area, which results in errors for foreground diagnosis and tracking. This is shown in the first row of Fig. 1for a sequence, where a wooden model (only the upper body) and its cast shadows are moving against a background with poor texture. The first and second frames are shown in parts (a) and (b), respectively; the magnitude of optical flow calculated by the method in [4] is shown in part (c), where propagation of the object flow to the neighboring background pixels with poor texture has deformed the object shape and lead to difficulty in foreground detection. In images with multiple moving objects within a small region, smoothness of optical flow can lead to objects merging. This is illustrated in the second row of Fig. 1, where multiple cars with cast shadows are moving close to each other on a highway with insufficient texture. The first and second frames are shown in parts (d) and (e), respectively; the magnitude of optical flow calculated by the method in [6] is shown in part (f), where object merging is observable. The other negative effect of computing optical flow for poorly-textured regions is the considerable computation time due to solving the time-consuming Laplace equation with boundary conditions.Researchers have attempted to overcome the negative effects of the smoothness term following the HS contribution. Nagel and Enkelmann [7] employed oriented derivatives for the smoothness term, observing that the motion boundaries coincide with the abrupt light intensity transitions. Using heuristically determined smoothness across and along the object boundaries, Alvarez et al. [8] proposed a modification for improving the method by Nagel and Enkelmann. A manually-designed probabilistic model using Markov Random Field (MRF) and a statistical model using patch-based motion discontinuity were used to relate the light intensity edges and motion boundaries by Black [9] and Fleet et al. [10], respectively. Lei et al. [11] adopted a variable weight for the effectiveness of the smoothness term in the HS formulation. The variable weight coefficient is adaptive through a threshold function based on the detection of the gray boundaries and on the real-time detection of the movement boundaries in the iterative process. The method by Nir et al. [12] solves for six affine parameters at each pixel position instead of two flow components. Sun as well as Werlberger et al. [13,2] modified the total energy function by adding non-local smoothness terms that employ adaptive weights for each pixel, which is basically equivalent to using median filtering after every warping step.The approach of anisotropic weighting of the smoothness term is a breakthrough employed recently, including substitution of the standard quadratic penalizing function by the anisotropic Huber-L1 Norm, first introduced in [14] and used in [15] and [16], applying smaller weights along the intensity boundaries compared to the orthogonal direction in [2]. A similar approach was proposed by Zimmer et al. [17] in which the brightness constancy is used to determine the weights rather than the intensity gradient. Harmonic constraint has been imposed on the isotropic gradient vector field to create the anisotropic diffusion in [18] and [19], where the authors utilized divergence and curl of the vector field. Aubert et al. [20] added an extra term, which penalizes computing motion in homogeneous blocks and only allows for large values of optical flow components in textured regions. Divergence controls the amount of diffusion, and the curl term controls the diffusion direction.Despite significant improvements in the suppression of motion blurring at the object boundaries, even accurate and sophisticated leading methods in the Middlebury,11http://vision.middlebury.edu/flow/.KITTI,22http://www.cvlibs.net/datasets/kitti/eval_stereo_flow.php?benchmark=flow.and MPI Sintel33http://ps.is.tue.mpg.de/project/MPI_Sintel_Flow.rankings, such as [4,6], and [13] fail to capture the proper size and contour of the foreground in images with poorly-textured background regions. Note that employing more complicated cost functions leads to larger computation times, which is detrimental in real-time tracking procedures. After numerous observations of optical flow results using state-of-the-art methods, we came to the conclusion that no matter how sophisticated the algorithm is, performance could be undesirable if original frames have poor texture. This encouraged us to explore the outcomes of modifying original images, rather than modifying the computing algorithms.Regions with poor texture exist both within the background and the foreground; however the regions in the background are the main reason for propagation of foreground flow to neighboring pixels, object shape distortion, and even object merging. Furthermore, adding a static texture to these regions can be performed with sufficient accuracy. However, to generate a moving texture for the foreground regions, we need to know pixels' correspondence, which is not possible without calculating the optical flow. If we want to calculate optical flow once and use it again for generating a moving texture, even small inaccuracies in the optical flow vector field leads to addition of the texture to wrong pixels and hence induction of erroneous flow. Moreover, any interpolation using feature matching is prone to mismatching errors and it requires knowledge about the type of object motion (rigid or flexible as they need different types of interpolations) which is not known a-priori; thus it cannot be used to generate an accurate moving texture. Therefore, we treat only poorly-textured background regions and leave the modification of foreground regions to future investigations. It is important to mention that the camera is assumed to be stationary in this paper. Therefore, we only add texture to the background pixels. Should the camera be moving, egomotion estimation and compensation are required to be performed before texture addition.To localize and add texture to poorly-textured regions in the background, we first perform a texture segmentation using Laws' masks and generate a texture map. Next, using a binary frame difference, we constrain the poorly-textured regions to those with negligible motion. Finally, we calculate the optical flow for the modified images with added texture. It is shown that if the threshold used for binarizing the frame difference is in a specific range determined empirically, variations in the final foreground detection will be insignificant. Note that optical flow calculations suffer from poor texture, and image differencing cannot provide us with motion information, being significantly sensitive to illumination changes and the binarizing threshold; however, computing optical flow while using image differencing and texture addition as just described provides improved accuracy and robustness in foreground detection, as will be shown.The main contributions of this study are: (1) creation of sharp motion boundaries and more accurate capture of the object size, position, and contour; (2) avoiding or mitigating object merging in sequences with multiple objects moving in a small area with poor texture; (3) reduction in computation time; and (4) mathematical analysis of the effects of texture addition on the optical flow convergence and computation time. This paper is organized as follows: In Section 2, we describe the problem in more detail, together with the texture addition algorithm and effects accompanied by mathematical analysis of optical flow convergence. Section 3 demonstrates representative and quantitative results, and discussion. Section 4 provides limitations and future work, while general conclusions are included in Section 5.In this section, we first discuss the problem in more detail in Section 2.1. Then, we describe the texture addition algorithm and effects in Section 2.2, and explain these effects from the mathematical perspective in 2.3.We will use the formulation of HS throughout this section for simpler explanations, while applying the accurate and leading methods in [4,6], and [13] for demonstrations later. Denoting light intensity by I, first order spatial and temporal derivatives of light intensity by (Ix, Iy) and It, respectively, optical flow components (u,v) in [1] are computed by minimizing the following total energy function:(1)ϕuv=∫∫ρ2ϕduv+ϕcuvdxdywhere the data error energy function is given by:(2)ϕduv=Ixu+Iyv+It2and smoothness energy function is defined as:(3)ϕcuv=∂u∂x2+∂u∂y2+∂v∂x2+∂v∂y2.Parameter ρ determines the relative weight of the data term. Then, the Euler–Lagrange equations yield:(4)Ix2u+IxIyv−ρ2∇2u=−IxItIxIyu+Iy2v−ρ2∇2v=−IyIt.In regions where spatial light intensity variations are negligible (poor texture), including along the x and y axes (Ix≈0,Iy≈0), Eq. (4) will be approximated by the Laplace equations with boundary conditions dictated by the neighboring windows:(5)∇2u≈0,∇2v≈0.If Eq. (5) holds in the background regions (outside the boundaries of the foreground objects), non-zero optical flow of the object blocks will affect the neighboring background pixels, where zero motion is expected. This effect will spread, and the level of influence depends on the magnitude of light intensity variations around the object (as well as the computing method), which can be a region of the image relatively larger than the object size in images with considerably poor texture. This is illustrated in Fig. 2for a sequence in which an airplane44http://www.youtube.com/watch?v=qF9VZSkVZI0.is moving against a uniform background sky, with the first frame, second frame, and the ground-truth of the foreground shown in parts (a), (b), and (c), respectively. The magnitude of optical flow computed according to methods in [4,6], and [13] can be seen in parts (d), (e), and (f) in the second row, respectively. Comparing the magnitudes of optical floww=u2+v2, with the ground-truth of the foreground, we can see that not only are the detected moving pixels significantly larger than the object's size, especially in part (d), but also the contour of the object is not preserved.The first step is to localize poorly-textured regions in a frame. We use the Laws' masks introduced in [21] to measure the texture energy in different regions of an image. Denoting a gray-scale frame by IL, we apply Laws' 2-D convolution kernels on IL, which can be created using the following set of 1-D kernels of length three: L3=[1 2 1] (average gray level), E3=[1 0 −1] (edge extractor), and S3=[1 −2 1] (spot extractor).Although nine 2-D kernels can be built using the outer product of these filters, we did not use L3TL3, since it only measures the average gray level value in a 3×3 window, while we look for pixel-wise light intensity variations. Accordingly, each frame is convolved with the following set of eight 2-D masks: [L3TE3,L3TS3,E3TL3,E3TE3,E3TS3,S3TL3,S3TE3,S3TS3]. This set is capable of measuring light intensity variations in different patterns (i.e. texture) in a region. Denote this kernel set by [K1,K2,…,K8] and show the convolution operation by (∗). Then texture energy (TE) at a pixel in position (i, j) is given by:(6)TEij=∑n=18Fnij,Fn=Kn∗IL.The next step is to create a binary texture energy map, denoted by TEB, which requires a threshold on the values of the texture energy, here denoted by γ. To avoid using an empirically-determined threshold which can fail for sequences not studied, we use an adaptive method based on the histogram of the texture energy values. Based on the knowledge from images processing – the major portion of an image information lies in the low texture energy values – and after investigating a large number of sequences, we acquired assurance that the histogram of TE values for a typical image looks like what is shown in Fig. 3(here using 100 bins). It is a right-skewed distribution with mostly decreasing frequencies as the texture energy increases (there could be sudden increases, but low very texture energy values tend to have significantly larger frequencies).We call those regions “poorly-textured” for which the texture energy levels are sufficiently different, here smaller than the other values (with higher frequencies). In other words, we look for those values with such high frequency that are outliers in this histogram. Since the histogram of texture energy levels for most of the images does not follow a normal distribution, determining outliers is performed using the method introduced in [22] for skewed distributions. In this paper, Vanderviere and Huber introduced an adjusted boxplot taking into account the medcouple MC, a robust measure of skewness for a skewed distribution, which for a data series with sorted entries (Xn={x1,x2,…,xn},x1≤x2≤…≤xn) is given by:(7)MC=medxj−medk−medk−xixj−xiwith med and medkbe the median operator and the median of Xn, and xiand xjhave to satisfy xi≤medk≤xjand xi≠xj. Then, for right-skewed distributions like in Fig. 3 with MC≥0, the boxplot limits given in Eq. (8) can be used to determine the outliers:(8)xi<Q1−1.5e−3.5MCIQR,xi>Q3+1.5e4MCIQRwhere Q1 and Q3 are the first and third quantiles and IQR is the interquartile range. Here Xnrepresents the sorted frequencies of TE values acquired from the histogram. We use the upper limit in (8) to determine those texture energy levels with frequencies in the histogram that are outliers from above. For instance, for the histogram shown in Fig. 3, only the two first bins were found to be outliers; so a threshold of γ=0.02 was used and multiplied by the maximum value of TE, since 100 bins were used.Because the binary texture energy maps for the first and the second frames are not usually identical due to different factors, such as lighting variations, we use the binary intersection operator to ensure that texture will only be added to identical locations in both frames. Denoting the texture energy maps for the first and the second frame by TE1and TE2, the final binary texture map is given by:(9)TEBij=1ifTE1ij≥γ×maxTE1∩TE2ij≥γ×maxTE20otherwise.Fig. 4shows the first and second frames from a laboratory sequence,55http://arma.sourceforge.net/shadows/.and the binary texture energy map in parts (a), (b), and (c), respectively.TEB distinguishes only between regions with rich and poor textures. So, to localize and add texture only to the poorly-textured regions in the background, we must use a type of foreground detection algorithm. We opted to use image differencing due to the small computation time required. Note that in addition to foreground detection (which is not accurate for poorly-textured images), optical flow can also provide further information, such as direction and magnitude of pixels' displacement per frame. Furthermore, image differencing cannot be employed to detect the foreground with sufficient accuracy due to lighting changes, small capture rate, etc., and the shape of the resulting binary map will depend on the threshold utilized. Therefore, image differencing cannot replace optical flow regarding accurate detection of motion; it is merely used to ensure that the static texture is not added to the pixels with apparent motion, and that the added texture does not induce erroneous flow.While the threshold used for binarizing the frame difference will determine the shape of the binary map, FDB, later we show that the final optical flow magnitude using the texture-added frames will not significantly vary provided that the threshold, β, is selected from an empirically-determined range. Denoting the frame difference by (FD=IL2−IL1), we define FDB as:(10)FDBij=1ifFDij≥β×maxFD0otherwise.Note that in order to binarize the frame difference, typically a fixed threshold might be used, but we can improve the results by employing an adaptive threshold. To avoid rendering small light intensity variations (due to illumination changes, camera inherent noise, etc.) as foreground pixels, we tried to only keep those pixels with the largest light intensity variations, which can be determined by comparing their light intensity variation magnitude to the maximum value of light intensity change. Therefore, we used the max( ) function in Eq. (10).We also perform binary image filling on FDB to avoid missing some of the moving pixels within the FDB borders. To generate the texture that will be added to the frames, we can select texture patches from available texture images or simply generate a stochastic texture. Selection of the texture types depends on the optical flow computation method, which will be discussed in Section 3. To create the stochastic texture for each pixel (i, j), first three random numbers (RND) from a normal distribution are generated with μ=0 and σ=1 for three RGB channels (RND(i,j,k)∈N(0,1),k=1,2,3). Then, they are multiplied by a scalar, SC that determines the magnitude of the texture for each pixel. The static stochastic texture image (STX) has the same size as both sequence frames, and is given by:(11)STXijk=SC×RNDijk,k=1,2,3.Finally, the modified frames (IN1, IN2) are created by adding texture only to the regions in both original frames (here original color frames are denoted by IM) that have poor texture and do not show apparent motion:(12)INijk=IMijk+STXijkifTEBij=0andFDBij=0IMijkotherwise.In the first row in Fig. 5, part (a) shows the binary frame difference for the laboratory sequence, and part (b) shows the map that localizes background regions with poor texture. Modified frames with texture addition can be seen in parts (c) and (d), respectively. Here, for frames with 8-bit unsigned integer values, SC=40 was used for all pixel. Selection of SC will be discussed in Section 2.3. Magnitude of optical flow calculated according to [4,6], and [13] for the original and modified frames are shown in the second and third rows, respectively. Comparing the optical flow magnitude results for original and modified frames, we observe that the erroneous background flow has been suppressed, and thus the foreground boundaries have been preserved with higher accuracy in all methods, despite differences in the approaches used to calculate optical flow components. Quantitative improvement in preservation of foreground boundary detection is discussed in Section 3.We have illustrated the effect of β∈[0.002,0.1] on the binary frame difference FDB and the optical flow magnitude using the texture-added frames in Fig. 6. In the first row, part (a) shows the magnitude of optical flow using original images computed according to [13] and part (b) shows the ground-truth for the foreground; in the second row, binary frame difference is shown for β=0.002, β=0.01, β=0.025, β=0.04, and β=0.1; and the third row shows the magnitude of optical flow using modified frames computed according to [13]. Very small thresholds as in part (c) lead to detection of the majority of pixels as belonging to the foreground, since light intensity of pixels do not remain unchanged due to lighting changes, quantization effects, etc. Therefore, texture will not be added to the poorly-textured regions as they are recognized to be foreground regions, and thus the magnitude of optical flow for the modified and original images will be similar without specific improvements. For large thresholds as in part (g), only a fraction of foreground pixels will be correctly detected, and adding texture incorrectly to the moving pixels will result in erroneous flow in the foreground regions (small flow magnitude in part (l) for some pixels). Our empirical results show that if the threshold is selected from the range β∈[0.01,0.04], the magnitude of optical flow remains approximately the same, while the binary frame difference significantly changes with the threshold. Similar experiments with other sequences revealed that the range β∈[0.01,0.04] (enclosed in a red rectangle in Fig. 6) can provide reasonable results.In images with multiple objects moving close to each other, smoothness of optical flow variations and propagation of foreground flow into neighboring background pixels lead to object merging, as observed in the second row of Fig. 1 for the Highway Sequence. Modified frames and the magnitude of optical flow for these frames computed according to [6] are shown in parts (a) to (c) in Fig. 7. (Here we use the method proposed in [6] rather than in [13], since it is designed to handle large displacements of the objects as they exist in this sequence). As compared to part (f) in Fig. 1, adding texture to background regions with poor texture results in the suppression of erroneous flow for the background pixels, thus reducing object merging, although not completely eliminating the problem.The other advantage of adding texture to the images with poorly-textured regions in the background is reduction in the computation time, which is explained in 2.3. For the Laboratory Sequence shown in Fig. 4, we noticed 20.1%, 7.8%, and 11.7% reduction in computation time for methods in [4,6], and [13], respectively. Similar results have been observed in other sequences, which will be discussed in Section 3.In this section, we explain why adding texture to poorly-textured background regions suppresses erroneous flow and why it reduces the computation time. Rewriting Eq. (4) in matrix notation helps analyze the convergence of optical flow values for the background to very small values, and reduced computation time. Mitchie and Mansouri [23] used discretization of the Laplace operator and rewrote Eq. (4) into a set of linear equations of the form Az=b, where A is a tridiagonal matrix, vector z contains 2N2 unknown optical flow components, u and v, of an image with N×Npixels, and a constant vector b of the same size as z. Proof of block-wise convergence for unknown values in vector z using Gauss–Seidel iteration is provided in [23].We can rewrite Eq. (4) for ρ=1 as:(13)∇2vu+Mvu=Rwhere M and R are defined by:(14)M=−Ix2IxIyIxIyIy2,R=IxItIyItand the magnitudes of Ixand Iydepend on SC. For the background regions, where texture has been added, Ixand Iywill no longer be negligible terms, while temporal derivative Itis close to zero (because background pixels do not move and their light intensities do not change significantly with time). If we use finite difference formulas, then the higher the scalar value SC is, the larger the spatial derivative terms become, and the larger the magnitudes of entries in M and R become. Since the entries of the resulting matrix from the Laplacian operator on the left-hand-side of Eq. (13) are not affected by the texture magnitude, and are calculated using only a weighted averaging of the neighboring flow components (which are not large), the Laplacian operator term would be significantly smaller than the entries of M and R for relatively large values of SC (SC>20). Therefore, Eq. (13) can be written as M(vu)≈R, which is approximately a set of linear homogeneous equations, where the solution must converge to zero due to invertibility of the positive definite matrix M[23].For the background blocks in the vicinity of the foreground objects, due to the coupling of optical flow components to the foreground values (as explained in [23]), values for u and v will not converge to zero, but to small values. As we move further from the foreground object, background flow would approximately vanish in a distance that depends on the magnitude of SC. The effect of SC on the suppression the background flow can be clearly seen in Fig. 8for the Airplane Sequence in Fig. 2. First and second frames are shown in parts (a) and (b); modified second frames are shown in parts (c) to (e) for different values of SC (higher values mean stronger texture intensity); and corresponding magnitudes of optical flow are displayed under each modified frame in (f) to (h), respectively. As the value of SC increases, the background flow vanishes in shorter distances from the object.Higher values of SC, however will lead to higher errors in the foreground object boundaries. Consider a background pixel and its immediate neighboring pixels in a poorly-textured background region that are covered by a foreground object only in the first frame (possibly pixels near boundaries). Using I to show light intensity values and indices i, j, and k for the x-axis, y-axis, and time, respectively, we can write the variations in the light intensity derivatives as:(15)Ii,j,k+1→Ii,j,k+1+n1Ii+1,j,k+1→Ii+1,j,k+1+n2Ii,j+1,k+1→Ii,j+1,k+1+n3Ii+1,j+1,k+1→Ii+1,j+1,k+1+n4where “→” indicates “become” and n1 to n4 are random numbers due to texture addition. If we employ the same discretizations used in [1] for numerical differentiations in Ix, Iy, and It, then the expected values of the derivative terms can be given by Eq. (15), where E[n] is the expected value of a random variable n:(16)EIx→EIx+En3+n4−n1−n2EIy→EIy+En2+n4−n1−n3EIt→EIt+En1+n2+n3+n4.Therefore, selection of random numbers in the synthetic texture from a normal distribution with expected value of zero helps to maintain the expected values of spatial and temporal derivative terms unchanged. Note that our zero-mean normal distribution is more likely to produce a larger number of random values close to zero than a zero-mean uniform distribution. This will reduce the foreground errors around the foreground object boundaries. Here, higher SC values would magnify the standard deviation of the foreground flow error, while suppressing the background flow faster.As we can see in Fig. 8(c), employing large values for SC does not change the foreground size and shape considerably and provides negligible advantage. However, higher values of SC cause the optical flow error in the foreground boundary pixels to increase significantly. For the Airplane Sequence in Fig. 8, the object only undergoes a translation of u=−5.11 and v=−2.28pixels between two frames. The percentage of average errorsΔu¯Δv¯in the u and v components are (0.40%, 0.32%), (1.16%, 0.91%), and (2.52%, 2.11%) for the object boundary pixels using texture magnitudes of SC=5, SC=40, and SC=75, respectively. Experiments with multiple sequences have revealed that a texture magnitude of SC=40 for images with 8-bit unsigned integer values (or equivalently 15% of the maximum light intensity value) can provide a reasonable compromise between suppression of erroneous background flow and the errors in u and v components for the object boundary pixels.Eq. (13) can also help investigate the reduction in computation time. In a linear system of equations, such as Az=b, (where A is a positive definite matrix) the rate of convergence is directly related to the condition number (κ) of the coefficient matrix A, which can be defined as the ratio of the largest eigenvalue of a matrix, λmax, to the smallest eigenvalue of the matrix, λmin, for any symmetric positive matrix [24]. The higher the condition number, the slower the convergence would become. If we use a 9-point discretization for the Laplace operator on an image of N×Npixels, the condition number of the resulting positive definite matrix (denoted by L) is of order O(N2) [25], which could be very large (≫1). In images with poorly-textured backgrounds, such as the UFO sequence, since the matrix M for these regions approximately vanishes in Eq. (13), the condition number of the Laplace operator is the dominating term, and it leads to very slow convergence.When synthetic texture is added, entries in matrix M are no longer negligible, so we encounter the problem of eigenvalues of the sum of two Hermitian (here positive definite) matrices. If we denote the eigenvalues of a matrix in a descending order by λ1>λ2>…>λSZ, where SZ is the size of the matrix, then we can find the upper and lower bounds for the largest and smallest eigenvalues of the sum of two positive definite matrices by referring to the Weyl inequality [26]:(17)λ1L+M≤λ1L+λ1Mλ1L+M≥maxλrL+λr+SZ−1M,r=1,2,…,SZλSZL+M≤minλrL+λr+SZ−1M,r=1,2,…,SZλSZL+M≥λSZL+λSZM.The condition number of L+M, denoted by κ(L+M) is the ratio of λ1(L+M) to λSZ(L+M), which is bounded by the following inequalities:(18)maxλrL+λr+SZ−1MminλrL+λr+SZ−1M≤κL+M≤λ1L+λ1MλSZL+λSZM,r=1,2,…,SZ.Within a poorly-textured background region, if the values of Ixand Iyusing a very small scalar value of SC0 (which can correspond to the original image without texture addition) are designated Ix0 and Iy0, then amplifying the texture magnitude using a scalar value of η×SC0 would change these terms to η×Ix0 and η×Iy0 due to the linearity of finite difference calculations. Furthermore, multiplication of scalar SC by a factor of η would magnify all the entries and the eigenvalues of M by a factor of η2, including the smallest and the largest eigenvalues. As a result, all numerators and denominators of the upper bound and the lower bound terms in Eq. (18) increase linearly proportional to η2. This leads to the reduction of both upper and lower bounds, since λ1(L)≫λSZ(L) and the bound terms are decreasing functions with respect to eigenvalues of M, or equivalently η2. Reduction in the upper and the lower bounds will lead to the reduction in the condition number of the equivalent matrix (M+L), such that the convergence rate would increase.

@&#CONCLUSIONS@&#
In this study, we have investigated the effects of adding synthetic texture to images with poorly-textured regions on the optical flow performance, namely the accuracy of foreground boundary detection and computation time. It is demonstrated that texture addition leads to important advantages, including creation of sharp motion boundaries, more accurate capture of object contour and size, avoidance or mitigation of object merging, and reduction in the computation time. Well-known quantitative metrics have been employed to evaluate the effectiveness of combining texture addition with several leading optical flow methods on multiple real and animation sequences. Analysis of optical flow convergence supported the resulting advantages from a mathematical perspective.