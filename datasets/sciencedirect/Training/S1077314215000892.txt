@&#MAIN-TITLE@&#
Recognizing complex instrumental activities of daily living using scene information and fuzzy logic

@&#HIGHLIGHTS@&#
Provides a unique and robust solution to the extremely challenging task of ADL modeling.Incorporates scene information to build ADL models.In the absence of manually labeled surfaces, can still generate high-level activity state summaries.We provide a dataset for the computer vision community that is described in this manuscript.

@&#KEYPHRASES@&#
Scene understanding,Activity analysis,Fuzzy logic,Activities of daily living,Eldercare,Depth images,Image features,

@&#ABSTRACT@&#
We describe a novel technique to combine motion data with scene information to capture activity characteristics of older adults using a single Microsoft Kinect depth sensor. Specifically, we describe a method to learn activities of daily living (ADLs) and instrumental ADLs (IADLs) in order to study the behavior patterns of older adults to detect health changes. To learn the ADLs, we incorporate scene information to provide contextual information to build our activity model. The strength of our algorithm lies in its generalizability to model different ADLs while adding more information to the model as we instantiate ADLs from learned activity states. We validate our results in a controlled environment and compare it with another widely accepted classifier, the hidden Markov model (HMM) and its variations. We also test our system on depth data collected in a dynamic unstructured environment at TigerPlace, an independent living facility for older adults. An in-home activity monitoring system would benefit from our algorithm to alert healthcare providers of significant temporal changes in ADL behavior patterns of frail older adults for fall risk, cognitive impairment, and other health changes.

@&#INTRODUCTION@&#
Activities of daily living (ADLs) are a set of activities that are required for self-care such as walking, eating, dressing, and bathing. They are used to assess the functional capacity of older adults [11]. Instrumental ADLs (IADLs) are a subset of the functional tasks that older adults perform to support their independent lifestyles [9]. Examples of IADLs are housekeeping, cleaning, cooking. These activities, when measured over an extended period of time, can show deviations in health for older adults. Zisberg et al. [5] developed a new instrument called SOAR to evaluate routine patterns in the lives of older adults. Subjects from four retirement communities reported detailed information regarding ADLs like eating, meal preparation, watching television, bathing, etc. The study indicated that any deviation in the routine of frail older adults could correlate with a change in health and provides the motivation behind the work described in this paper. We describe the premise behind our study using the following case study revolving around the IADL cleaning the table. Suppose a healthy older adult living independently performs the IADL cleaning the table once every day at a certain time. However, due to some health related reason, she is unable to do so several days in a row. Once detected, this deviation from her normal routine could be a strong indicator of a health change which could help enable early interventions. The goal of this study is to build a model to learn these ADL or IADL patterns which can then be used for detection, and the changes in daily (or weekly or monthly) behavior patterns can then be used to detect early health changes.The contributions of this paper are the following. We present a unique, vision-based method for recognizing components of ADLs and IADLs by combining their interaction with object surfaces with a set of linguistic fuzzy rules with heuristic parameters to model their activities. Specifically, in this paper we use the activities walk, sit, clean object, clutter object, move near object, rearrange object, and move object to describe our approach. We use the IADLs make bed and eat to describe the importance of combining scene information with moving object features to detect complex activities that are difficult to detect using only the foreground information or only the scene features. These activities further reinforce the importance of ontologies to provide context for each ADL or IADL that can provide the baseline for activity detection and help eliminate false alarms using contextual information. The results using our proposed algorithm are discussed and compared with another popular activity modeling algorithm, the hidden Markov model (HMM) and its variation, the details of which are provided in Section 8. We further test our method on data collected in an apartment at TigerPlace, an independent living facility for older adults. The data comprise depth information from an older resident (age 88, without any ambulatory needs such as a walker) as he goes through his daily routine in the apartment. We conclude with the discussion of the future steps for the ADL activity modeling framework. The next section reviews some of the related work in this field using vision and non-vision based sensors.

@&#CONCLUSIONS@&#
We demonstrate a flexible framework for detecting ADLs in an in-home environment using depth data from the Kinect sensor. Depth data provide the added advantage of unobtrusive monitoring with its ability to perform just as well under different lighting conditions. Silhouette features from the depth data as well as scene features are extracted and input to a fuzzy inference system, and activity states of the individuals are determined using fuzzy confidence measures. The resulting fuzzy rule based outputs are then temporally processed and used to generate temporal activity summaries. These summaries are input to another fuzzy inference system for further activity reasoning. This approach results in human understandable information and confidences regarding activities which can be used to monitor the activity patterns of older adults in their daily routine. The generalized framework can handle uncertainties in activities performed and generate useful information even with automatically extracted unlabeled object surfaces. In addition to the lab experiments, the algorithm has been tested using data collected at TigerPlace and has shown its robustness in unstructured, dynamic environments.