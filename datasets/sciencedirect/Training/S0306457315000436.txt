@&#MAIN-TITLE@&#
Evaluating document filtering systems over time

@&#HIGHLIGHTS@&#
We propose a new way of measuring document filtering system performance over time.Performance is calculated per batch and a trend line is fitted to the results.Systems are compared by their performance at the end of the evaluation period.Important insights emerge by re-evaluating TREC KBA CCR runs of 2012 and 2013.

@&#KEYPHRASES@&#
Time-aware information retrieval,Evaluation,Significance testing,

@&#ABSTRACT@&#
Document filtering is a popular task in information retrieval. A stream of documents arriving over time is filtered for documents relevant to a set of topics. The distinguishing feature of document filtering is the temporal aspect introduced by the stream of documents. Document filtering systems, up to now, have been evaluated in terms of traditional metrics like (micro- or macro-averaged) precision, recall, MAP, nDCG, F1 and utility. We argue that these metrics do not capture all relevant aspects of the systems being evaluated. In particular, they lack support for the temporal dimension of the task. We propose a time-sensitive way of measuring performance of document filtering systems over time by employing trend estimation. In short, the performance is calculated for batches, a trend line is fitted to the results, and the estimated performance of systems at the end of the evaluation period is used to compare systems. We detail the application of our proposed trend estimation framework and examine the assumptions that need to hold for valid significance testing. Additionally, we analyze the requirements a document filtering metric has to meet and show that traditional macro-averaged true-positive-based metrics, like precision, recall and utility fail to capture essential information when applied in a batch setting. In particular, false positives returned in a batch for topics that are absent from the ground truth in that batch go unnoticed. This is a serious flaw as over-generation of a system might be overlooked this way. We propose a new metric, aptness, that does capture false positives. We incorporate this metric in an overall score and show that this new score does meet all requirements. To demonstrate the results of our proposed evaluation methodology, we analyze the runs submitted to the two most recent editions of a document filtering evaluation campaign. We re-evaluate the runs submitted to the Cumulative Citation Recommendation task of the 2012 and 2013 editions of the TREC Knowledge Base Acceleration track, and show that important new insights emerge.

@&#INTRODUCTION@&#
Document filtering is a popular task in information retrieval with many applications (Keiser, 2009; Amigó, Gonzalo, & Verdejo, 2011; Amigó, Gonzalo, & Verdejo, 2013; Robertson & Soboroff, 2002; Frank, Kleiman-Weiner, et al., 2013; Frank, Bauer, et al., 2013). A stream of documents arriving over time is filtered for documents relevant to a set of topics. The distinguishing feature of document filtering, that sets it apart from other document classification tasks, is the temporal aspect introduced by the stream of documents. Because of this temporal dimension, the performance of a system is susceptible to change over time. For example, in a document filtering setting, where topics are being monitored over time, the topics might evolve. A system filtering the stream should be sensitive to this in order to perform well. As another example, a spam classifier should adapt to malignant and cunning adversaries. If it fails to do so effectively, its performance will likely degrade over time.Standard evaluation metrics measure performance for all returned documents, for a given information need (i.e., query), in a single batch (e.g., P@n, recall, nDCG, AP) and they can be averaged over multiple requests (e.g., macro-precision across a set of queries). Recent studies propose to decompose document streams into sequences (e.g., into slices of equal size) and measure effectiveness on a given time period (Azzopardi, 2009; Dietz, Dalton, & Balog, 2013; Aslam, Ekstrand-Abueg, Pavlu, Diaz, & Sakai, 2013). Then, it becomes possible to monitor the changes in system performance over time and to measure performance as a weighted average of slice-based relevance scores. None of these approaches, however, addresses the question we are interested in: how system performance changes over time.In Fig. 1 the performance of three hypothetical systems is plotted over time. The blue dots represent the score of the system at a given point in time. The gray dotted line represents the average performance of the systems over the entire time span. Clearly, all three systems have the same average performance. However, the performance of System A degrades rather strongly over time, the performance of System B less so, while the performance of system C shows improvement over time. With the metrics currently available there is no way to express this difference. In this paper we propose to capture this difference by employing trend analysis. In short, this entails fitting a straight line to the values of any existing performance metric applied to document filtering systems over time. In Fig. 1 these lines are displayed in orange. The derivative of the fitted line provides a simple and intuitive measure of the amount of change in performance over time. Ultimately, we can compare the performance of the three systems as estimated by trend analysis at the end of the evaluation period (the large orange dots in Fig. 1).The main contributions of this paper are the following. We analyze the properties and requirements a document filtering metric should meet. We propose to measure performance in batches and to use trend estimation to measure performance over time. We show that traditional macro-averaged true-positive-based metrics, like precision, recall and F1 fail to capture essential information when applied in a batch setting. In particular, documents returned in a batch for topics that are absent from the ground truth in that batch, false positives, go unnoticed. We propose a new metric, aptness, that does capture false positives. We incorporate this metric in an overall score, Fpra, and show that this new score does meet all requirements. As an important aspect of evaluation is testing for significant differences in observations, we detail the tests for statistical significance for trend estimation and discuss the assumption that need to hold.We test our method on the runs submitted to the Cumulative Citation Recommendation task of the 2012 and 2013 editions of the TREC Knowledge Base Acceleration track (KBA CCR for short). A re-evaluation of the results in terms of our proposed method shows a different ordering of teams, also at the top end. Moreover, while there were teams beating the baseline in 2013 when judged by the official metrics, our tests show that in fact no team did, when our proposed time-aware evaluation is used.Additionally, we find that the assumptions needed for valid significance testing hold in a vast majority of cases considered.The remainder of the paper is organized as follows. Related research is covered in Section 2. In Section 3 we give an overview our time-aware document filtering evaluation method and discuss the required properties. Two key components of our approach, measuring performance per time batches and trend estimation are then presented in Sections 4 and 5, respectively. In Section 5 we also show how significance testing can be performed. In Section 6 we detail our research questions and show results of experiments performed on the runs submitted to two years of the TREC KBA CCR evaluation campaign. We conclude in Section 7.

@&#CONCLUSIONS@&#
In this paper we have proposed to measure the performance of document filtering systems in a time-aware manner. The underlying idea behind our approach is to measure overall system performance in time batches, fit a trend line to the results, and consider the value assumed by the fitted trend line at the end of the evaluation period (referred to as estimated end-point performance) as the time-aware evaluation metric.Our analysis of requirements has revealed that existing measures fail to correctly deal with batches for which the ground truth mentions no relevant documents at all. We have introduced the concept of aptness and incorporated it into a new variant of the F-measure, Fpra. We have evaluated the runs submitted to TREC KBA CCR in 2012 and 2013 using the trend estimation framework and found that the order of top-performing systems changes. Perhaps the most remarkable and important finding is that from the official TREC 2013 KBA CCR results, based on time-agnostic metrics, it would seem that the oracle baseline run was beaten by several systems. However, we show by re-evaluation using our proposed time-aware metrics that in fact none of the systems is able to outperform the oracle run. As the oracle baseline is static and does not attempt to learn during runtime, this has strong implications concerning (the need for) adaptivity strategies for this task and corpus.Additionally, we have shown that the assumptions needed for statistical significance tests with the new method hold in a vast majority of the cases for the TREC KBA CCR runs for both years in which the track ran.Our experiments have focused exclusively on the TREC KBA CCR task, as a representative of a document filtering problem. The proposed trend estimation framework and time-aware evaluation metric, however, are not limited to the task of document filtering, but can also be applied to other retrieval tasks, where a more or less monotonic increase or decrease in performance of the retrieval system over time can reasonably be assumed. Dealing with non-linear trends is left for future work.