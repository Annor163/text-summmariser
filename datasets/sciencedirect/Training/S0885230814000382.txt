@&#MAIN-TITLE@&#
Efficient data selection for speech recognition based on prior confidence estimation using speech and monophone models

@&#HIGHLIGHTS@&#
We propose a method to select highly accurate data for speech recognition.We rapidly estimate prior confidence before speech recognition.Our prior estimation uses the acoustic likelihood of speech and monophone models.The proposed technique is over fifty times faster than the conventional method.Our proposal provides equivalent data selection performance.

@&#KEYPHRASES@&#
Speech recognition,Spoken document retrieval,Data selection,Context independent model,Gaussian mixture model,

@&#ABSTRACT@&#
This paper proposes an efficient speech data selection technique that can identify those data that will be well recognized. Conventional confidence measure techniques can also identify well-recognized speech data. However, those techniques require a lot of computation time for speech recognition processing to estimate confidence scores. Speech data with low confidence should not go through the time-consuming recognition process since they will yield erroneous spoken documents that will eventually be rejected. The proposed technique can select the speech data that will be acceptable for speech recognition applications. It rapidly selects speech data with high prior confidence based on acoustic likelihood values and using only speech and monophone models. Experiments show that the proposed confidence estimation technique is over 50 times faster than the conventional posterior confidence measure while providing equivalent data selection performance for speech recognition and spoken document retrieval.

@&#INTRODUCTION@&#
Massive quantities of videos and dialogs are stored every day; typical examples are video sharing services on the Internet and call center services provided by companies. Speech recognition technologies can transcribe the spoken components of these items automatically thus making the items searchable via their transcripts (Albertia et al., 2009). Several studies have analyzed customer needs by employing text mining (Subramaniam et al., 2009; Garnier-Rizet et al., 2008) and extracting the reasons for the calls (Fukutomi et al., 2011) from stored conversational spoken documents. A typical call center will store several tens of thousands of calls per day, and we believe that not all calls should be transcribed for the following three reasons. (1) The computation cost involved in transcribing all calls is excessive. (2) An informative analysis can be achieved from a subset of the calls. (3) The quality of the recorded speech samples varies (Benzeghiba et al., 2007), and erroneous speech recognition (due to the poor input) will degrade the efficiency of subsequent spoken document retrieval (Sanderson and Shou, 2007) and analysis.Several confidence measures have been proposed for identifying “accurate” speech samples (Jiang, 2005). Unfortunately, they require the computationally expensive step of speech recognition processing to obtain confidence scores, which are estimated from the recognition results; they waste considerable computer resources on samples that will eventually be rejected. Most conventional methods target word or utterance verification. A dialog (similar to spoken document) level confidence measure has been proposed (Litman et al., 1999), but it is also computationally inefficient because it requires several features including speech recognition results to estimate confidence. Several data selection methods have been proposed (Wu et al., 2007), but their target is to select training data, so they fail to reduce the computation cost significantly.Our proposal efficiently identifies speech samples that will be well recognized with an extremely low computation cost prior to speech recognition. It can identify those samples that have high confidence levels from massive numbers of stored speech samples. Prior confidence must be estimated rapidly because speech recognition can only proceed after the estimation results have been received. The proposed estimation technique utilizes the acoustic model used for posterior speech recognition. The proposal uses only context independent (monophone) models and speech models to reduce the computation cost. For even greater efficiency, its confidence estimation step eliminates all processing other than the calculation of acoustic output likelihood from Gaussian mixture models (GMMs). The prior confidence is calculated frame by frame from the difference between the output log-likelihoods of the monophone and speech GMMs. This confidence formulation is an approximation of the state level posterior probability with the state occurrence probability. This paper evaluates the actual efficiency of our technique in speech recognition and spoken document retrieval tasks. Experiments show that the proposed technique is significantly faster than the conventional posterior confidence measure based on speech recognition, while maintaining equivalent data selection performance.The rest of this paper is organized as follows. Related work is outlined in Section 2. The proposed technique is described in Section 3. Section 4 introduces experiments conducted to confirm the effectiveness of the proposed technique. Our conclusion is presented in Section 5.Since there are many factors that cause variability in speech signals (Benzeghiba et al., 2007), the recognition accuracy is strongly dependent on the data. Several data selection methods have been proposed for training (Wu et al., 2007; Lin and Bilmes, 2009) and adapting (Cincarek et al., 2006) acoustic models for speech recognition. Wu et al. also selected data to be transcribed for training by using the confidence score (Wu et al., 2011); this technique is called active learning. A great number of confidence measure methods have been proposed (Jiang, 2005) and they could also be useful for selecting data during speech recognition processing, since inaccurately recognized data impacts negatively on the subsequent application. Stoyanchev et al. detected misrecognized words in spoken dialog systems (Stoyanchev et al., 2012). Seigel et al. estimated a confidence measure at the word/utterance level by using conditional random fields (CRF). Ogawa et al. also used CRF directly to estimate the recognition rate rather than the confidence score both per utterance and per lecture at the spoken document level (Ogawa et al., 2012). Asami et al. also estimated the spoken document confidence score by using contextual coherence (Asami et al., 2011). Senay et al. detected low-quality documents by using a confidence measure and semantic consistency based on the latent Dirichlet allocation (LDA) model for spoken document retrieval (Senay and Lina`res, 2012). Li et al. used semantic similarity to estimate a confidence measure for spoken term detection (Li et al., 2012). There are several confidence measure methods at a variety of levels depending on the application.Conventional confidence measure estimations require speech recognition results; this means that a lot of computation time is required to recognize low-confidence and unuseful data, which should be rejected. Thus, we attempt to reject unuseful data at the document level to prevent harmful effects on the subsequent application prior to speech recognition. In a conventional approach, Lee et al. proposed rejecting data before speech recognition by using noise GMMs (Lee et al., 2004). However, this method could reject data at the utterance level and needs to know the noise type beforehand. Chang et al. also proposed a pre-rejection algorithm that enhances the robustness of speech recognition by using pitch correlation (Seo et al., 2003), which allows it reject seriously distorted speech signal during wireless communication. However, it fails to reject slightly distorted speech with pitch continuity. This paper proposes an efficient method for selecting useful data for speech recognition and subsequent spoken document retrieval at the document level, which consists of many utterances before speech recognition. In addition, since our main target speakers, i.e. operators (agents) in call centers, use headset-type close-talk microphones, the recorded speech has high SNR (speech to noise ratio) without distortion. In call center speech, it is more important to tackle spontaneous speech instead of noisy or distorted speech. Thus, we focus on acoustical confidence.The most common confidence measure is based on the word posterior probability defined as follows:(1)P(Wˆ|O)=P(Wˆ)P(O|Wˆ)P(O)=P(Wˆ)P(O|Wˆ)∑WP(W)P(O|W)whereOand W are an acoustic observation feature sequence (o1,o2, …,oT) and its corresponding word sequence, respectively, P(W) is word occurrence probability as given by the language model, and “ˆ” means the word, state, or sequence with the highest likelihood. The normalization term P(O) cannot be easily computed (Guo et al., 2004), so conventional schemes approximate it using the N-best list from speech recognition results as in Rueber (1997).It requires a high computation cost to extract word sequences by using a language model that covers a large vocabulary. To avoid this cost, our strategy dispenses with the language model and instead targets the state sequence S in Hidden Markov Models (HMMs);(2)P(Sˆ|O)=P(Sˆ)P(O|Sˆ)∑SP(S)P(O|S)Our proposal eliminates all processing steps other than frame-independent acoustic likelihood calculation to further reduce the computation cost; it ignores the transition probability in the same way as several speech recognition decoders (Glass, 2003), and uses only the Gaussian output probability from GMMs to estimate confidence frame by frame (i.e. frame-independent).That is, the posterior probabilityP(Sˆ|O)is approximately calculated from the frame-independent state posterior probabilityP(sˆ|ot)with best statesˆagainst observed featureotat frame t for data length T as shown below;(3)P(Sˆ|o)≃∏t=1TP(sˆ|ot)where the frame-independent state posterior probabilityP(sˆ|ot), is calculated from the output probability bs(ot) of state s frame by frame as follows;(4)P(sˆ|ot)=P(sˆ)bsˆ(ot)∑sP(s)bs(ot)(5)bs(ot)=∑m=1Msws,mNs,m(ot|μs,m,Σs,m)wheresˆis the best state in frame t. Msis the number of distributions belonging to state s,ws,mis the mth mixture weight and Ns,m(·) is the mth Gaussian distribution function with mean vector μs,mand covariance matrix Σs,mof state s.To increase the processing speed further, the proposed technique uses only monophones when calculating Eq. (4);sˆis the best state, i.e. the state with the maximum Gaussian output probability among the states of the monophone HMMs. The assumption is that triphones can be approximated by monophones, and this assumption is often used to improve the speed of speech recognition processing as in Lee et al. (2001). The monophone in our acoustic model is still trained by the acoustic features with triphone-based alignment. Accordingly, this assumption is a very reasonable approach to improving speed.The denominator of Eq. (4), ∑sP(s)bs(ot) is the sum of all states’ (all phonemes’) output probabilities; it can be approximated by the speech model as follows;(6)∑sP(s)bs(ot)∼P(g)bg(ot)where g is the state of the speech model (GMM) that is trained from the acoustic features of all phonemes, i.e. all states. Our speech model has only a single state, so the occurrence probability of the speech model, P(g), must be equal to 1 in speech frames. By assigning 1 to P(g) in Eq. (6), we obtain the following.(7)∑sP(s)bs(ot)∼bg(ot)The second term in Eq. (7) is similar to the denominator in the second term in Eq. (1); thus this approximation is reasonable.By substituting this expression into Eq. (4), the frame-independent posterior probability,P(sˆ|ot), is approximately calculated as follows;(8)P(sˆ|ot)∼P(sˆ)bsˆ(ot)bg(ot)The occurrence probability of statesˆ,P(sˆ), can be calculated from the appearance frequency of state s. We herein assume that there is no significant difference between the state appearance frequencies of the acoustic training speech data and the target speech data; in particular, we use only monophone states, and the difference is not significant at the monophone level. Under this assumption,P(sˆ)is given by the following equation by using total occupancy Γ(s), which reflects the appearance frequency of state s in the acoustic model training data.(9)P(sˆ)≃Γ(sˆ)∑sΓ(s)The frame-independent confidence score, c(ot), is transformed in the log domain from Eq. (8) as follows.(10)c(ot)=log(P(sˆ)bsˆ(ot))−logbg(ot)If the speech model is adopted as the Universal Background Model (UBM) and we ignore the state occurrence probabilityP(sˆ), Eq. (10) is similar to the likelihood ratio often used in speaker verification as in (Reynolds et al., 2000).Prior confidence score C is calculated by normalizing the frame-level prior confidence score, c(ot), by data length T to allow a comparison of speech samples with different lengths as follows;(11)C=∑t=1Tc(ot)TFig. 1summarizes the above-mentioned relational expression from a conventional confidence measure based on posterior word probability to the proposed prior confidence measure. Since our proposed prior confidence can be estimated by using the acoustic likelihood from only speech and monophone models, it is significantly faster than a conventional confidence measure.Fig. 2shows the difference between the log-likelihoods of a monophone and speech model against clear and ambiguous speech; this is a simplified figure just for explanation as each model has only one state and one distribution. The speech model is trained from the acoustic features of all phonemes in speech frames, so the distributions in the speech model have broader variances than the distribution in the monophone model. Accordingly, the speech model provides a comparatively stable log-likelihood regardless of speech quality. If the input speech is clear and similar to the training acoustic data (the expectation is for high accuracy), the input acoustic features are located around the mean of either monophone's distributions. In this case, the log-likelihood of the best monophone is larger than that of the speech model, and the prior confidence becomes higher. In contrast, with ambiguous speech (the expectation is for low accuracy), the input features are located on the side of the distributions and the monophone's log-likelihood becomes smaller, so the prior confidence becomes small. As a result, the difference between the log-likelihoods of the best monophone and the speech model reflects the expected accuracy in posterior speech recognition.The procedure of the proposed system is shown in Fig. 3. The conventional system subjects all data to speech recognition. In contrast, the proposed system estimates confidence and selects the samples to be passed to speech recognition. In prior data selection, the speech data is ranked by the estimated prior confidence. The proposed system selects those samples that have high confidence scores and then performs speech recognition on the selected samples using triphones in the acoustic and language models. The computation cost falls since the speech recognition step is minimized, and our proposal can efficiently identify well-recognized speech samples.Fig. 4focuses on the prior confidence estimation process. It calculates the output probability frame by frame from GMMs of monophone HMMs and the speech model in the acoustic model for each complete speech sample. Frame-level prior confidence c(ot) is estimated from the difference between the log-likelihoods of the best states in the monophone (red filled circle in Fig. 4) and speech models as given by Eq. (10). Here, the acoustic features of speech and pause are significantly different. Thus, the proposed technique discriminates speech and pause by using the output probability of GMMs belonging to pause HMMs and the speech model, frame by frame. g is the best state (blue circle in Fig. 4) in the speech GMMs and pause HMMs in Eq. (10). Our proposed system adopts the utterance segmentation technique described in (Kobashikawa et al., 2013) before confidence estimation. Utterance segmentation uses speech GMMs and pause GMMs belonging to pause HMMs in acoustic model, and when the pause frame continues for longer than τpau (e.g. 0.8s), the utterance is segmented as an end-point. The segments include some pause frames by using a hangover scheme (Sohn et al., 1999). Prior confidence score C is calculated by averaging frame level confidence score as given by Eq. (11). We select data to send for speech recognition by using the prior confidence score C.

@&#CONCLUSIONS@&#
This paper proposed a rapid prior confidence estimation technique for selecting speech samples that will yield accurate recognition results. It reduces the computation cost since it selects only the best samples for speech recognition based on prior confidence estimation; only a Gaussian acoustic likelihood computation with speech and context independent models is needed. Simulations showed that our confidence estimation technique is over 50 times faster than the conventional posterior confidence measure based on speech recognition, and about 3.9 times faster than the monophone-loop confidence measure based on phoneme recognition. The proposed technique matched the selection performance of the conventional technique, and also improved the precision of the spoken document retrieval task.