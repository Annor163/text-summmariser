@&#MAIN-TITLE@&#
Principal component analysis in combination with case-based reasoning for detecting therapeutically correct and incorrect measurements in continuous glucose monitoring systems

@&#HIGHLIGHTS@&#
A data-driven methodology for detecting therapeutically correct and incorrect measurements made by CGMSs is detailed.In this methodology the key point is the use of PCA and CBR.The CGMS information and variables related to the patient's clinical conditions were used.Experimental results showed that PCA–CBR has potential as a CGMS fault detection tool.

@&#KEYPHRASES@&#
Case-based reasoning,Continuous glucose monitoring,Critically ill patients,Principal component analysis,

@&#ABSTRACT@&#
This paper introduces a data-driven methodology for detecting therapeutically correct and incorrect measurements in continuous glucose monitoring systems (CGMSs) in an intensive care unit (ICU). The data collected from 22 patients in an ICU with insulin therapy were obtained following the protocol established in the ICU. Measurements were classified using principal component analysis (PCA) in combination with case-based reasoning (CBR), where a PCA model was built to extract features that were used as inputs of the CBR system. CBR was trained to recognize patterns and classify these data. Experimental results showed that this methodology is a potential tool to distinguish between therapeutically correct and incorrect measurements from a CGMS, using the information provided by the monitor itself, and incorporating variables about the patient's clinical condition.

@&#INTRODUCTION@&#
Continuous glucose monitoring systems (CGMSs) are devices that provide detailed information about glucose variability in the interstitial fluid on a continuous basis: direction, magnitude, duration, and frequency of hypo- or hyperglycemia [1]. This information permits to identify glucose trends throughout the day. Over the past decade, there has been unprecedented technological progress in the development of CGMSs. CGMSs in combination with insulin pumps and control algorithms have fuelled research in the development of the so-called “artificial pancreas”.Nowadays the need to manage glycemia in critically ill patients has been the focus of multiple studies. In intensive care units (ICUs), blood glucose (BG) monitoring is performed intermittently using different bedside BG meters. CGMSs would allow a better control of glycemia in critically ill patients allowing the staff to anticipate episodes of hyper- and hypoglycemia. The potential benefits of the use of the CGMS would be avoiding multiple blood draws from the patient. Other benefits of using the CGMS would be a reduction in the workload of nurses and a reduction in the risk of accidental puncture.Although the CGMS could be clinically useful in the ICU, it is not sufficiently accurate and reliable at present to be used for therapeutic decisions. The lack of accuracy and reliability has been an important limiting factor for insulin delivery automation, and for clinical use. For this reason, none of the CGMS available has been approved by the regulatory agencies as a replacement for traditional self-monitoring of blood glucose (SMBG). Thus, improvement of the accuracy and reliability of these devices is essential.Previous studies have been developed to improve accuracy and reliability in CGMS. To improve the accuracy, new algorithms considering blood-to-interstitial glucose dynamics have been developed and implemented [2–6]. However, accuracy is not as big an issue as reliability and error detection [7]. Reliability is one of the main requirements for a CGMS and in order to improve it, detection of abrupt faults and malfunctions must be included in CGMSs. A limited number of works about fault detection methods for CGMSs have been published [8–13]. For this reason, significant progress needs to be made in CGMS fault detection. Moreover, and according to the idea proposed in [14], this work proposes the use of monitoring techniques and abnormal situation management in CGMS fault detection, in the same way that these techniques are often used in different applications [15].In order to make clinical decisions, the main goal of this work was to develop a data-driven methodology for detecting therapeutically correct (TC) and therapeutically incorrect (TI) measurements made by a CGMS. Measurements were classified using statistical methods (principal component analysis, PCA) in combination with expert systems (case-based reasoning, CBR). PCA was used to extract features (the Q-statistic and the scores). Then, the Q-statistic and the scores obtained from PCA together with the septic status of the patient have been proposed as descriptors for the CBR methodology. CBR was applied to recognize patterns and classify measurements made by a CGMS. The CBR considers two different procedures of the z-Nearest Neighbours (z-NN) to classify measurements: one of them based just on the z-NN two-steps distance criterion (based solely on PCA features) (CBR-2SR), and the other based on the z-NN two-steps distance criterion and the similarity of the septic status between cases (CBR-3SR).The remainder of this paper is organized as follows: Section 2 describes the clinical experimental setup used for data capture, and the transformation of the CGMS fault detection problem into a bi-classification problem. Furthermore, the feature extraction using PCA and the methodology for detecting TC and TI measurements by applying CBR are also outlined. Finally, Section 3 is reported and Section 4 is presented.The clinical experimental setup was obtained from a prospective observational study in an intensive care unit (ICU). The research reported in this manuscript was performed using data from 22 patients admitted to an 18-bed mixed ICU at the Doctor Josep Trueta Hospital (Girona, Spain) (acute physiology and chronic health evaluation (APACHE) II score, 20.0 [range, 15.0–21.8]; sequential organ failure assessment (SOFA) score, 8.5 [range, 6.0–10.0]). The characteristics of the patients are given in Table 1. The overall results of the study were presented in [16]. This study followed the protocol approved by the Ethics Committee of the Doctor Josep Trueta Hospital. All of the patients gave informed consent, either signed by them if they were conscious or signed by family members in cases of unconscious patients.The data set was gathered from patients who presented with hyperglycemia and needed intravenous insulin therapy on admission to the ICU. This data set contains information provided by the Guardian® REAL-Time Continuous Glucose Monitoring System (RTCGMS) (Medtronic, Northridge, CA) and variables about the patient's clinical condition.Arterial BG (ABG) readings were used as the gold standard to classify the data set on TC and TI measurements.ABGwas measured using the whole BG concentration reported by the HemoCue® 201 DM (HemoCue AB, Ängelholm, Sweden). Quality control checks were performed using liquid controls recommended by HemoCue® instructions. The ABG samples were obtained following the glycemic control protocol established in the ICU [16]. According to this protocol, when a patient showed high glycemic instability (hyper- and hypoglycemia),ABGsamples were taken every 30min. Then, if glycemic values were stabilized,ABGmeasurements were spaced to every one, two, three, and until every 4h. If the patient's nutritional intake was stopped for any reason, the glycemic control testing was performed more frequently, even during euglycemia. AdditionalABGmeasurements were used for RTCGMS calibration. After the third day, theABGdata were downloaded to a computer using HemoCue® 201 DM 3.1 software.The RTCGMS provided the following measurements: the electrical signal (Isig, measured in nanoamperes) and the glucose estimation in the interstitial fluid (GRTCGMS, measured in mg/dL). Patients were monitored for 72h using the RTCGMS (MiniMed reference CSS72). This device consists of a disposable subcutaneous needle-type sensor and an external monitor. The sensor is an amperometric system that uses glucose oxidase, which generates an electrical signal (Isig) proportional to the glucose concentration in the interstitial fluid (GRTCGMS) [17]. The sensor measures Isig every 10s and records the mean values at 5-min intervals. The sensor estimates GRTCGMSat 5-min intervals. The RTCGMS was placed in the subcutaneous tissue of the upper leg of each patient. Following a 2-h initialization period, the first ABG measurement was used for RTCGMS calibration. These calibrations were performed according to the RTCGMS manufacturer's instructions (three to four per day). RTCGMS readings were not used to modify insulin therapy. After the third day, the RTCGMS data were downloaded to a computer using Medtronic Carelink Pro version 2.0B software.Variables about the patient's clinical condition corresponding to the dose of continuous infusion of intravenous insulin (CII orInsulin), the axillary body temperature (Temperature) and the septic status (Sepsis) of the patient were recorded.ABGsamples were used by the nurses to administer insulin therapy, according to the protocol established by the ICU. A CII was started once the patient hadABGvalues above 150mg/dL to maintain theABGbetween 120 and 160mg/dL. Short-acting insulin (Actrapid, Novo Nordisk, Bagsværd, Denmark) diluted in 0.9% saline was used. Intravenous insulin therapy was stopped when theABGwas below 110mg/dL.Insulinwas recorded at the same time asABG.Temperaturewas measured at 30-min intervals with electronic thermometers (Thermoval Basic, Hartmann®, Germany). Under continuous glucose monitoring,Sepsisof the patient was recorded every 24h by the patient's medical team. The patient's medical team gave the diagnostic about the septic status of the patient based on the definitions of The American College of Chest Physicians and Society of Critical Care Medicine Consensus Conference Committee [18]. The patient's medical team gave a presumptive diagnosis considering the patient's medical history and the results of laboratory tests such as leucocytosis, C-reactive protein (PCR) and procalcitonin (PCT). Furthermore, cardiogenic, haemorrhagic, and anaphylactic causes of haemodynamic shock were ruled out. According to this diagnostic,Sepsiswas classified in three groups: patients without sepsis, patients with sepsis, and patients with septic shock.Sepsiswas a categorical variable that could take the following values:Sepsis=0 in patients without sepsis,Sepsis=1 in patients with sepsis, andSepsis=2 in patients with septic shock.From the 22 patients, 501ABGreadings were obtained (those readings used for RTCGMS's calibration were excluded). The medianGRTCGMSwas 128 (106–156) mg/dL, and the medianABGwas 138 (121–160) mg/dL. Furthermore, 241ABGmeasurements were used for RTCGMS calibration, resulting in 11 samples per patient on average. The medianABGwas 129 (116–147) mg/dL.ABGmeasurements used for calibration were within the range considered correct according to our protocol.The Clarke error grid analysis (EGA) [19], a clinical accuracy method currently accepted by the U.S. Food and Drug Administration (FDA) to assess the accuracy of CGMSs was considered as the standard method to define the classification scheme associated with TC and TI measurements. The Clarke EGA is divided into five zones. Data in zone A are those values within 20% of the reference sensor. Data in zone B are points that are outside of 20%. Data in zone C are not accurate and would result in unnecessary corrections. Data in zone D are considered to be a dangerous failure that would result in treatment errors, and the data in zone E would result in opposite treatment decisions.ABGmeasurements were synchronized withInsulin,TemperatureandSepsis.Insulinwas recorded at the same time asABG, soInsulinmeasurements were matched to the referenceABG. Besides, at the same time at which the firstABGsample was taken, aTemperaturesample was recorded. Since the temperature was measured at 30-min intervals, each time that anABGmeasurement was taken, a temperature measurement was recorded. On the other hand, each time that anABGmeasurement was taken, this measurement was matched with the septic status previously diagnosed by the patient's medical team. Moreover, theABGand RTCGMS readings (IsigandGRTCGMS) were obtained at different times, and then the RTCGMS readings were matched to the referenceABGwithin±2.5min as in [20].The data set of paired values (ABG/GRTCGMS) was first labelled using the Clarke EGA withABGmeasurements as the gold standard (see Fig. 1). Previous studies to assess the use of CGMSs in ICU using Clarke EGA have concluded, from a safety point of view, that the occurrence of sensor readings in zones C, D, and E are not acceptable as accurate. Besides, the points in zone B, indicating more than 20% deviation from the reference value, are also not acceptable for ICU standards [21]. Thereby, the data distribution shown above was performed according to the following classification scheme:Therapeutically correct (TC) class: The data points that fell in zone A. From the whole data set 72.7% (364 samples) were TC measurements.Therapeutically incorrect (TI) class: The data point that fell in zone B, C, D and E. From the whole data set 27.3% (137 samples) were TI measurements.Consequently, the data set contained 501 samples labelled in two classes, and each sample was composed by five variables:Isig,GRTCGMS,Insulin, TemperatureandSepsis.Data from 20 patients (432 samples) were used to build the proposed methodology. This training set consisted of 88.2% (321 samples) from the whole TC measurements and 81.0% (111 samples) from the whole TI measurements. Furthermore, two representative patients (69 samples), corresponding to one medical patient and one surgical patient, were used to test the generalization performance of the proposed methodology. One trauma patient was not tested because of the insufficient size of this cohort. This testing set consisted of 11.8% (43 samples) from the whole TC measurements and 19.0% (26 samples) from the whole TI measurements.PCA is a technique to find combinations of variables or factors that describe major trends in a confusing data set [22]. The aim of PCA is to find the sub-space in the space of the variables where data mostly vary. The original variables, commonly correlated, are linearly transformed into a lower number of uncorrelated variables: the so-called principal components (PCs) [23]. The formulation of PCA is defined (Eq. (1)):(1)X=TPT+E=∑k=1KtpkTk+E=tp1T1+t2p2T+…+tKpKT+Ewhere X is a n×m matrix of data with n rows (observations, or samples) and m columns (variables), T=[t1,t2,…,tK] is the n×K scores matrix containing the projection of the original data in the PCs sub-space, P=[p1,p2,…,pK] is the m×K loadings matrix containing the linear combination of the variables represented in each of the PCs, E is the n×m matrix of residuals, and K is the number of PCs selected for the model.In summary, this data set can be expressed as a linear combination of K new variables, assuming an error matrix E (Eq. (1)). Scores and loadings matrix are computed to reflect relevant relations among observations and variables.On the other hand, the residual matrix and the scores matrix can be used for detecting an abnormal behaviour in the system. With this aim, the D-statistic (also denoted as Hotelling's T2-statistic) and the Q-statistic (also know as Squared Prediction Error, SPE), are commonly used. The D-statistic is the distance of an observation to the centre of the PCA model, and is computed as the squared sum of each retained scores (j=1,…,K) scaled by their respective eigenvalues λj(Eq. (2)). Q-statistic represents the change of the events that are not explained by the PCA model and is obtained for each observation from the sum of squares of each row of E (ej) (Eq. (3)). During a normal behaviour in a system, Q-statistic is very small, and therefore any minor change in the system characteristics will be observable [24].(2)T2=∑j=1Ktj2λj(3)Q=∑j=1Kej2A training set considering only measurements of four original variables (Isig,GRTCGMS,InsulinandTemperature) was used to build the PCA model. The categorical variableSepsiswas not considered because added noise to the PCA model. A PCA model was built through the following steps, as was explained in [24]:1.Considering a data set as an n×m matrix X, where m represents the number of variables measured in each patient and n represents the number of observations.The training set (432 samples from 20 patients) consisted of 88.2% (321 samples) from the whole TC measurements and 81.0% (111 samples) from the whole TI measurements. 13 out of the original 432 samples were excluded because they were extreme outliers. Identification of outliers was based on the box plot criteria [25] and corresponded to points above the third quartile plus three times the interquartile range. So, the new training set was comprised to 419 samples and it was partitioned again into two subsets: a training subset to build the model PCA (consisted of 308 samples corresponding to TC measurements), and a validation subset (made up of 111 samples corresponding to TI measurements).The PCA model was created using the training subset. Data were auto scaled [26] to have zero mean and unit variance, which granted that all variables would have the same importance.The eigenvectors-eigenvalues of the covariance matrix were calculated. The eigenvectors were sorted by eigenvalues to obtain the components in order of significance. The eigenvectors with the highest eigenvalues represent the PCs of the data set.The percentage of variance explained criterion was used to determine the number of PCs to retain. The number of PCs was selected in order to explain at least 80% of the original data variance, which in this case fixed this value to the three first eigenvectors.The original data were transformed by means of the matrix P (projection), obtaining indices that characterized each one of the training observations. These indices were the scores and Q-statistic. Although the D-statistic was obtained, it was not explicitly used because the information provided to the CBR by the scores and the D-statistic is redundant.Data from the validation subset were projected into the PCA model. Scores and Q-statistic were calculated for the validation subset.CBR is proposed as an artificial intelligence approach, which was applied to recognize patterns and classify data extracted from the PCA approach. The developed methodology used PCA in combination with CBR for detecting TC and TI measurements made by a CGMS.The main feature of CBR is to solve problems by using the experience and knowledge obtained from past experiences, with the possibility to learn new knowledge to solve new similar situations. CBR has been formalized for purposes of computer reasoning as a process of four basic functions that are known as the four R's phases [27]:Retrieve phase: Given a target problem, past similar situations are recovered to solve a new one. It is necessary to define a metric function and the number of cases (z) to retrieve from the case base (CB).Reuse phase: Once the z-nearest cases have been retrieved, they are used to propose a possible solution. According to a similarity index, the diagnosis is given using a voting technique or adapting the cases.Revise phase: The adequacy of the proposed solution is checked, either with a model or by an expert.Retain phase: After the revision process, and according to the proposed solution, the usefulness of retaining the new case for future classifications is decided in this step.In this formulation, a basic case structure was proposed in Eq. (4). The data used to build the model (old case) were stored in the CB. This CB was composed of results from the PCA approach (Q-statistic and scores), and the categorical variableSepsis:(4)Case={Q,t1,…,tK,Sepsis,l}where Q is the value of Q-statistic, t1,…,tKare the K retained scores, andSepsisis the value of the septic status of the patient. Finally, l indicates if a measurement is TC or TI.According to CBR methodology, once the quantitative and categorical variables were selected as descriptors, an initial CB was constructed. In a similar way to Ruiz [28], bearing in mind that the number of TC and TI measurements was different, it was concluded that the best way to build the CB was to use the same number of TC and TI measurements, obtaining a balance between classes. This consideration was done for that in the learning process, the CBR system was not biased towards the majority class. The remaining portion was used to validate the CBR approach.In order to find the best descriptors to retrieve cases from the CB and to obtain the class for new cases, several simulations were run, combining the different PCA features. From all these tests, the following z-NN procedures presented the best results:The z-nearest cases were obtained based just on two-steps distance criterion by using the Q-statistic and scores descriptors described in [27]. In the first step, the distance of a new case to all cases into the CB was calculated using Eq. (5). This distance was called the Q-distance (dQ), and it is the measure of the absolute distance between the Q-statistic descriptor of the new case (Qnew) and the Q-statistic descriptor of ith case stored into the CB (QiCB). Then, the first z1=30 nearest cases (least Q-distance) were kept for the next step of distance computation. In the second step, the distance between the new case and the z1 retained cases was computed using Eq. (6). This distance was called the score distance (dt), and it is the measure of the weighted Euclidean distance calculated using all scores obtained from PCA model. It was accomplished by comparing each score descriptors (t1,t2,…,tK) of the new case with the z1 retained cases from the CB. From this second level of the distance computation, only the z2=15 nearest cases (least score distance) were retrieved. Finally, the class of the new case was obtained by voting, it means that the class with more population among the retrieved cases was selected as the class of the new case.(5)dQ=Qnew−QiCB(6)dt=∑j=1K(tjnew−tjiCB)λj2A critical point in the CBR methodology is to assign weights to variables, because this decision is subject to expert knowledge. This problem was solved by assigning to each descriptor the eigenvalues associate to each score vector (t) as weight (λj), which granted that all variables would have the same importance. These eigenvalues represent the variance for each score.The z-nearest cases were obtained based on three-steps: the two-steps distance criterion and the similarity between cases considering theSepsis. In the same way that the two-steps retrieval, this approach considered dQand dtto retrieve the z1 and z2 nearest cases. In third step, from the z2 retained cases from the CB, the z3 cases that present the sameSepsisthat the new case were identified and retrieved. This consideration was made, because results reported in a previous study showed that the septic status of the patient can influence the accuracy of the CGMS in the ICU [16]. Once the z3 cases were retrieved, the solution of the new case was determined by voting. If neither from the z2 retained cases had the sameSepsisthat the new case, then z3=z2, and all the z2 cases were used to obtain the class for a new case by voting, as in 2SR.2SR and 3SR were based on the Q-statistic and scores, so they contained complementary information (Q-statistic looked in the residual subspace, while scores searched within the projection subspace). Moreover, although the D-statistic was not explicitly used as a CBR input, dtis a score-wise difference between observations, and thus, takes into account the same information than the D-statistic values (but decoupled). The combination of the D-statistic and scores was discarded because the information provided to the CBR was redundant.Finally, in both retrieval processes (2SR and 3SR), the four R's phases were completely implemented and besides CB maintenance and update were considered.In order to evaluate the performance of the approach for detecting TC and TI measurements made by a CGMS, four metrics were calculated: Accuracy (Acc), Sensitivity (Sn), Specificity (Sp) and Gmean.Accuracy is a metric computing the proportion of measurements that are correctly classified by the model (Eq. (7)), where tnegis the number of TC measurements correctly classified, tposis the number of TI measurements correctly classified, Nposand Nnegare the numbers of all TC and TI measurements, respectively.(7)Accuracy(Acc)=tpos+tnegNpos+NnegBy considering accuracy rates of TC and TI measurements separately, the sensitivity (or true positive rate) and the specificity (or true negative rate) metrics are defined as shown in Eqs. (8) and (9), respectively, where sensitivity is the proportion of TI correctly identified among all TI measurements, and specificity is the fraction of TC correctly identified among all TC measurements.(8)Sensitivity(Sn)=tposNpos(9)Specificity(Sp)=tnegNnegFinally, a measure more sensitive to the low positive values and that combines sensitivity and specificity is Gmean, which is defined as their geometric mean, where both TC and TI measurements have the same relevance for the purpose of classification (Eq. (10)).(10)Gmean=(Sn×Sp)

@&#CONCLUSIONS@&#
