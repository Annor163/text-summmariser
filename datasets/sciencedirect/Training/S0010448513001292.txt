@&#MAIN-TITLE@&#
Development of a simulated living-environment platform: Design of BCI assistive software and modeling of a virtual dwelling place

@&#HIGHLIGHTS@&#
SLEP=BCI system+assistive software tailored to motor-disabled people + virtual dwelling place.The platform was configured to provide different levels of workload.Human–computer interaction is strengthened by user guidance.This is important in BCI research, as the prototypes are created for isolated people.This approach could avoid the assumption of locked-in BCI users’ requirements.

@&#KEYPHRASES@&#
Simulated-living environment platform,Brain–computer interface,Assistive software,Virtual dwelling place,Increasingly demanding scenarios,Motor imagery,

@&#ABSTRACT@&#
In brain–computer interfaces (BCIs), the user mental constrains and the cognitive workload involved are frequently overlooked. These factors are aggravated by neuromuscular dysfunction, collateral complications, and side effects of medication in motor-impaired people. We therefore proposed to develop a simulated living-environment platform (SLEP) that was tailored to severely paralyzed people and also allowed the progressive user–system adaptation through increasingly demanding scenarios. This platform consisted of a synchronous motor imagery based BCI system, an everyday assistive computer program, and a virtual dwelling place. The SLEP was tested in 11 healthy users, where the user–system adaptation was evaluated according to the BCI accuracy for classifying the user control tasks. The user heart rate was also incorporated in the evaluation in order to verify the progressiveness of such adaptation. The results of this study showed that user performance tended to increase from the least to the most challenging scenario in learning situations. The results also showed that nine of the eleven users controlled the BCI system in cue-driven mode, completing over half of the tasks. Two of the eleven users controlled the BCI system in target-driven mode, completing two tasks. Taken together, these results suggest that the progressive adaptation in BCI systems can enhance the performance, the persistence and the confidence of the users, even when they are immersed in simulated daily-living situations.

@&#INTRODUCTION@&#
Neural progressive disorders, traumatic brain injuries, and strokes may significantly affect the neuromuscular channels, often leading to severe motor disabilities. At their most severe, these neurological disorders can be manifested as locked-in syndrome  [1]. The consequences of the resulting neuromuscular deficits are extremely high and more than medical solutions are required to palliate the loss of quality of life and the socio-economic pitfalls. So far, researchers, who work in biomedical engineering, have helped motor-impaired people to regain function by utilizing their remaining movement abilities, their peripheral electrical activity, their brain signals, or combinations thereof  [2]. A particular example is the brain–computer interface (BCI), system that decodes users’ brain signals to restore their interaction with the world. A typical non-invasive BCI system records electroencephalographic (EEG) signals that are regulated by users following exogenous or endogenous paradigms. The exogenous paradigm is based on the detection of event-related potentials that are evoked through the attention on auditory, visual or tactile stimuli. The endogenous paradigm depends on the quantification of brain oscillations that are modulated via cognitive tasks such as motor imagery (MI), mental calculation, or association of imaginary words. In line with these two paradigms, it is possible to develop reactive and active BCIs, respectively  [3]. As the present study is limited to MI-based BCIs, we will only refer to active (i.e., via deliberate cognitive task execution) BCIs hereinafter.To date, the BCI community has been mainly interested in developing assistive technologies (ATs) to enhance the quality of life of locked-in (or otherwise severely paralyzed) patients. These ATs can be encompassed under three broad categories: (1) communication tools that allow the interaction with others, (2) environmental controls that offer the ability to live at home, and (3) mobility systems that provide a certain degree of independency  [4–9]. Over the past few years, BCI researchers have focused their efforts on improving user adaptation and system performance. In this respect, they have addressed three main factors: techniques to reduce the number and the length of training sessions, algorithms to select the most suitable EEG features, and methods to increase the classification accuracy among the major number of mental tasks. However, they have rarely investigated human competence (ability to perform a task) and task-oriented demands (workload related to tasks with different levels of difficulty). With regard to human competence, it must be considered that human information processing relies on a limited number of mental resources to transform internal and external stimuli into cognitive responses. Furthermore, the availability of those resources depends on the psycho-physiological state of the person. With respect to the task-oriented demands, the workload involved in BCI systems is frequently overlooked. For instance, the coordination of the BCI control interface could hinder the task at hand, and the repetitive nature of the BCI control tasks might require high levels of attention. This, in turn, may provoke mental fatigue along with aversion and loss of mental focus. Human mental constraints, as well as the cognitive workload issues related to BCI systems, are aggravated by neuromuscular dysfunctions, collateral complications, and side effects of medication in paralyzed people  [10–12].On the basis of the above discussion, we implemented a simulated living-environment platform (SLEP) consisting of a synchronous MI-based BCI system, an everyday assistive computer program, and a virtual dwelling place. The SLEP was carefully designed in accordance with motor-impaired people’s living circumstances, and also, it allowed user learning through increasingly demanding scenarios. After implementing the SLEP, we proceeded to monitor the performance of healthy users at dealing with a variety of tasks under several scenarios. User performance was assessed according to the user ability to master the BCI control tasks, i.e., the highest level at which the BCI system accurately discriminated the control tasks of the user. The effectiveness (in grading the cognitive workload at the SLEP) was also evaluated by recording the user heart rate. This indirect evaluation was based on previous evidence that relates heart rate (HR) deceleration with intake tasks and HR acceleration with rejection tasks  [13].The SLEP presented here consists of a synchronous (i.e., cue-based) MI-based BCI system, a computer program to assist motor-impaired people in everyday situations, and a virtual dwelling place (Fig. 2.1). The BCI system translated MI (left/right hand movements) and non-MI control tasks into commands for the assistive software. In turn, the assistive software attended to priority demands encompassed under four categories: ‘necessities and desires’, ‘mobility’, ‘environment control’, and ‘messenger’. The navigation through the virtual dwelling place was directed by the ‘mobility’ category of the assistive software. In essence, the functional design of the SLEP was based on seven fields:•International classification of functioning, disability, and health [14],ATs, especially those designed for neuromuscular impairments [15–19],augmentative and alternative communication (AAC) tools [20–23],BCI communication devices  [4,6,7,9],smart housing  [24–27],control of living-environments  [8,9], andvirtual worlds (VW) for neurological rehabilitation  [28,29] and BCI applications  [30–35].Furthermore, scenarios with different levels of difficulty were programmed, allowing the observation of the user performance under conditions that gradually increased the following demands: sequential execution of actions, prolonged attention focus, avoidance of interfering stimuli, enrichment of environmental feedback, and coordination of multiple activities.MI is not the only BCI approach for interaction with VEs. Among other viable alternatives we have the so-called SSVEP and P300 approaches, both of which will be incorporated in our system in the future. However, both SSVEP and P300 approaches require absolute focus on the interface, which takes autonomy and freedom away from the user. MI also allows the user to multitask more easily. Hence, the study is focused on MI-based BCI systems.In active BCIs, users control their brain activity by focusing on a specific mental task such as MI, which has become the most prominent control task used in the BCI community. MI modifies the synchrony of brain oscillators mainly within theα(8–12 Hz) andβ(16–24 Hz) frequency bands (although other bands play a role in specific cases as well, e.g.,   [36]) over the primary sensory-motor cortical area  [37]. The deliberate modulation of brain oscillations through MI is not as easy a task as it may seem, especially for users with motor impairments. As users of synchronous MI-based BCIs are often involved in several training sessions prior to the BCI application, these BCIs must have one adaptation phase (offline analysis) and one application phase (online analysis). With these considerations in mind, our BCI system was designed in line with offline and online analyses to distinguish up to three control tasks. The system essentially carried out the same operations on both analyses, taking into account that the online phase requires a device controller. The fundamental structure of the system had six modules: (1) acquisition of EEG signals via Biosemi equipment  [38], (2) EEG signal processing concerning spatial and spectral filtering, (3) feature extraction based on band power estimation, (4) feature selection according to Davies–Bouldin Indexes (DBI), (5) feature classification through Fisher Discriminant Analysis (FDA), and (6) plotting of EEG features by usingxy-graphs. For further information about the development of the BCI system see [39,40].A number of published studies related to ATs for neuromuscular impairments  [15–19] were consulted to identify the most relevant requirements of motor-impaired patients. The selected requirements were grouped under four categories: (1) ‘Necessities and Desires’, i.e., access to essential services such as body position changes, feeding, toileting, or personal hygiene; (2) ‘Mobility’, i.e., transfer from one place to another; (3) ‘Environmental Control’, i.e., management of electrical devices such as lighting, heating, ventilation, or audiovisual entertainment; and (4) ‘Messenger’, i.e., writing of messages by employing a letter-by-letter formulation strategy (this strategy is the most frequently used approach as reported by AAC sources). The four categories were adapted to a four-tab computer application that was programmed in Python (Fig. 2.2). Every tab was structured using a top-down layout where the panel with the four categories was placed on the top, the menu of each category was located in the middle, and the submenu of each menu was allocated at the bottom. A tiny area below the submenu sections was additionally assigned to record the history of the last selected options. The panel of categories and the menus were designed by using a combination of icons and text labels that offered visual interest and fast interpretation. The submenus were created using large buttons, making them an easy target to locate. All text related widgets used high contrast between foreground and background, and the text labels were mainly written by using geometric Sans-Serif typeface to improve the readability on screen. Besides the four tabs, one extra tab was appended to the assistive software (top illustrations in Fig. 2.2). This tab was designed for training purposes and will be described in Section  2.3.In terms of functionality, the assistive software was predominantly driven by two active commands, taking into account that humans often carry out a binary search among options. Right MIs were translated into horizontal forward movements across the panel of categories and the menus, i.e., navigation commands. Being in the panel of categories, left MIs activated the menu corresponding to the current selected category. Being in the menus, left MIs moved the current selected task of the submenu to the history section. Left MIs were referred as selection commands. The navigation through the submenus was automatically done in order to reduce the mental effort of the user, but it was necessary to maintain a non-MI state while waiting for the self-activation of the task of interest. For navigation-purposes, the current position of the user was highlighted using an orange background, while the forthcoming options were marked by applying large and bold text fonts. Alternations between the menus and the panel of categories were done through specific purpose buttons labeled as “EXIT”. These buttons were included twice whenever possible for facilitating the navigation across the interface. At every alternation, the current available tool bar was colored, whereas the previously used bar was faded. In addition, every time the assistive software rightly recognized either navigation or selection commands, it played a “click” sound indicating success.The virtual dwelling place (Fig. 2.3) was modeled pursuant to a ground floor unit modified for a quadriplegic person  [41], hence also considering many of the locked-in patients’ living circumstances. Note that all the furniture was placed beside the walls to prevent obstructions, and all room doors were removed (with the exception of the bedroom and the bathroom ones due to issues of privacy) to facilitate mobility. This virtual dwelling place was modeled using Blender, free open source 3D computer graphics software  [42].Having developed the SLEP, we configured a variety of increasingly demanding scenarios to provide different levels of workload in the SLEP. We thus tuned both the attention to the environment (intake tasks) and the internal cognitive processing (rejection tasks). The SLEP configuration was organized in four levels: (1) familiarization with the assistive software, (2) user–system adaptation, (3) cue-driven system, and (4) target-driven system. Every level was accomplished by employing two or three specific structured scenarios as described below. The scenarios related to levels 1 and 2 exclusively depended on the assistive software. In contrast, scenarios for levels 3 and 4 used the assistive software along with the virtual dwelling place.At the first level, there were three scenarios: MI-training (miTR), MI-functionality (miFUNC), and software-training (softTR). In the miTR scenario, users were trained to imagine left and right hand movements (MI control tasks), as well as to be relaxed but focused (non-MI control task). These three control tasks were cued by the unique visualization of an arrow pointing to the left, an arrow pointing to the right, or a square, respectively. All the cues were followed by a warning beep (top-left tab in Fig. 2.2). In the miFUNC scenario, users learned to recognize the functional role of each control task through a generic template of the assistive software. They continued imagining hand movements in accordance with the previous protocol, but left and right cues were additionally reproduced as downward (selection command) and horizontal forward (navigation command) displacements across the generic template (top-right tab in Fig. 2.2). In the softTR scenario, the same mechanism of converting left/right cues into selection/navigation commands was followed, but here it was applied to the assistive software. In the miFUNC and softTR scenarios, the non-MI control task was prompted but it had no effect on the interfaces. Its utility was explained in Section  2.2.In terms of intake tasks, this level was a learning stage where users interpreted warnings and cues, evaluated the audio feedback, and also decoded the graphics of the generic template and the software interface. In terms of rejection tasks, users imagined left/right hand movements, determined a relaxed and focused mental state, understood the functional roles of the control commands, and also identified the tool set of the software. Note that the sequence of cues, in the two later scenarios, was preprogrammed to promote the interaction with the whole set of tools available on the assistive software.The second level was an interactive stage where users did not only deal with the aforementioned tasks, but they were also encouraged to improve their mental performance. At this level, users were involved in an adjustment cycle where the BCI system first adapted to the users (training phase), and later, the BCI system assessed users’ skills to reproduce the control tasks recorded in the training phase. For the training phase, we made use of the softTR scenario by additionally reprogramming the sequence of cues to foster the navigation through the whole assistive software. For the testing phase, instead of converting the cues into their matching commands, the BCI system classified the inward control tasks to decide between a selection and navigation commands. If the BCI system successfully recognized the control tasks, the assistive software executed the corresponding command along with a pleasing tone. If the control task recognition was not satisfactory, an unlucky tone was played and no command was executed on the assistive software. As expected, this scenario was named software-testing (softTS).At the third level, users were immersed into an environment where they needed to maintain attention and had to use the environmental feedback, apart from the BCI concerns. To satisfy these key points, we set an everyday situation where we programmed a sequence of cues that pursed the selection of 13 activities of daily living (ADLs11Activities of daily living (ADLs) is a term in healthcare referring to the daily care activities of impaired and elderly people.) framed by that situation. Therefore, users were not required to plan a navigation strategy because we preset the necessary cues to select the 13 ADLs. When the BCI system failed to predict the cued control task, neither navigation nor selection commands were executed on the assistive software. Following these premises, users never altered the pathway of the pursing ADLs and they were not occupied in redirecting the navigation strategy. We thus avoided the coordination of unexpected activities and the interference of extra stimuli.As the simulated everyday situation, a sunny morning was selected and users were asked to imagine themselves laying on their beds ready to request daily needs. Those needs were the 13 pursuing ADLs, which were sequentially prompted (middle-left tab in Fig. 2.2) as follows: (1) dressing for a sunny day, (2) going to the bathroom, (3) washing their hands, (4) washing their face, (5) combing their hair, (6) urinating, (7) going to the kitchen, (8) selection of the kitchen submenu, (9) opening the kitchen blinds, (10) opening the kitchen window, (11) eating some fruit, (12) serving cereal, and (13) pouring milk. Every time one of these activities was achieved, the SLEP aurally emulated the activity process or visually simulated the displacement from one room to another. This scenario was named as the level, cue-driven system (cueSYS).An equivalent environment to the third level was run at the fourth level, except for the addition of three abilities: the planning of a sequence of actions, the coordination of unexpected activities, and the avoidance of interfering stimuli. Like the cue-driven system, an everyday situation was set along with nine associated ADLs. The necessary cues to select the nine ADLs were not preset in this occasion, so the assistive software always reflected the control command predicted by the BCI system. As the simulated everyday situation, an evening was selected and users were asked to imagine themselves waiting at home for having dinner with their friends. Based on this scene, the following environmental events were chosen as ADLs: (1) doorbell ringing, (2) woman saying ‘hi’, (3) pouring water, (4) washing their hands, (5) frying bacon, (6) woman saying ‘bye’, (7) passing flatus, (8) brushing their teeth, and (9) yawning. The events were aurally emulated in sequence (bottom-left tab in Fig. 2.2). Every time one of the ADLs was selected, the assistive software audio-visually responded with the corresponding reaction, that is: (1) door opening, (2) not applicable, (3) drinking, (4) going to the bathroom and drying their hands, (5) going to the kitchen and eating, (6) not applicable, (7) going to the bathroom and using the toilet, (8) rinsing out their mouth, and (9) going to the bedroom and making sounds of snoring. The scenario was titled as the level, target-driven system (tgtSYS).

@&#CONCLUSIONS@&#
