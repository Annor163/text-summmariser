@&#MAIN-TITLE@&#
Scene parsing by nonparametric label transfer of content-adaptive windows

@&#HIGHLIGHTS@&#
CollageParsing is a scene parsing algorithm that matches content-adaptive windows.Unlike superpixels, content-adaptive windows are designed to preserve objects.A powerful MRF unary is constructed by performing label transfer using the windows.Gains of 15–19% average per-class accuracy are obtained on a standard benchmark.

@&#KEYPHRASES@&#
Image parsing,Semantic segmentation,Scene understanding,Objectness,

@&#ABSTRACT@&#
Scene parsing is the task of labeling every pixel in an image with its semantic category. We present CollageParsing, a nonparametric scene parsing algorithm that performs label transfer by matching content-adaptive windows. Content-adaptive windows provide a higher level of perceptual organization than superpixels, and unlike superpixels are designed to preserve entire objects instead of fragmenting them. Performing label transfer using content-adaptive windows enables the construction of a more effective Markov random field unary potential than previous approaches. On a standard benchmark consisting of outdoor scenes from the LabelMe database, CollageParsing obtains state-of-the-art performance with 15–19% higher average per-class accuracy than recent nonparametric scene parsing algorithms.

@&#INTRODUCTION@&#
In computer vision, we can analyze an image at many levels of abstraction. We might assume that the image contains one dominant object, and develop algorithms to assign a semantic category to that object, such as ‘airplane’ or ‘motorbike’. Alternatively, we may remove the assumption of a single dominant object in the image, and attempt to detect and localize all instances of multiple object categories. Or instead of objects, we may be interested in identifying the type of scene depicted in the image, such as ‘forest’ or ‘city’. Instead of categorizing objects and scenes, we may wish to describe objects and scenes by associating them with high-level attributes, such as ‘shiny’ or ‘rugged’.In this work, we are interested in assigning a semantic label to every pixel in an image. We would like to explain the entire image, including both foreground objects of interest and background elements. Foreground objects are typically compact objects with well-defined boundaries, sometimes referred to as ‘things’ in the literature [1]; examples include cars and people. Background elements are often amorphous in shape or defined by texture, sometimes referred to as ‘stuff’; examples include grass and mountain. The task of labeling an entire image by its foreground and background semantic categories is referred to as scene parsing.The diversity of foreground and background categories makes the task of scene parsing challenging. Traditionally, object categorization and detection tasks are approached by using machine learning techniques to learn compact, parametric models for the categories of interest. We can learn parametric classifiers to recognize airplanes or motorbikes, for example. However, it is challenging to scale these model-based approaches to detect the wide variety of categories that may be present in an image. A natural question to ask is whether we might adopt a nonparametric, database-driven approach to scene parsing instead. A nonparametric approach to scene parsing attempts to label the query image by matching parts of the image to similar parts in a large database of labelled images. The category classifier learning is replaced by a Markov random field in which unary potentials are computed by nearest-neighbor retrieval. Intuitively, labels are ‘transferred’ from exemplars in the database to the query image.Nonparametric label transfer offers a powerful scene parsing tool especially in the context of open-universe image databases. Open-universe databases are collections that are continually changing as users add new images and annotations; the LabelMe database [2] is a well-known example. Nonparametric approaches to scene parsing are particularly suitable for databases that are continually changing because there is no need to re-train category models as new data are added. Moreover, no special accommodation is required when the vocabulary of semantic category labels is expanded.State-of-the-art nonparametric scene parsing algorithms perform label transfer at the level of pixels or superpixels [3–7]. Superpixels offer a higher level of perceptual abstraction than pixels, enabling more efficient inference in a random field model. Large, cohesive groups of pixels can be labelled at once. However, while superpixel-based methods tend to perform well on large regions of background (‘stuff’) categories, they tend to perform less effectively on foreground object (‘thing’) categories. We argue that, though better than pixels, superpixels are still a relatively low-level unit for nonparametric label transfer. Superpixels tend to fragment objects. Is it possible to perform label transfer at a higher level of organization?In our preliminary work [8], we showed that it is indeed possible to perform label transfer at a higher level of perceptual organization. The CollageParsing algorithm performs label transfer using content-adaptive windows in a Markov random field framework. Windows are content-adaptive in the sense that they are designed to enclose entire objects instead of fragmenting them. There are many ways to generate content-adaptive windows, and we adopted Alexe et al.’s objectness algorithm [9] in our earlier implementation. We showed that performing label transfer using content-adaptive windows allows for the construction of a more effective unary potential than previous approaches, leading to state-of-the-art results on the standard SIFT Flow benchmark [3] among nonparametric scene parsing methods.In this paper, we explore CollageParsing in more depth and make two algorithmic improvements to our original work. First, we compute content-adaptive windows using the state-of-the-art Edge Boxes algorithm [10] instead of the objectness algorithm [9] (Section 3.2). Second, we construct a more powerful unary potential by matching content-adaptive windows using deep learning features instead of HOG features (Section 3.3). To our knowledge, this improvement is the first use of deep learning features in a nearest neighbor search for scene parsing. These extensions significantly improve the scene parsing accuracy of the original CollageParsing algorithm, and to our knowledge enable the best per-pixel and per-class results to date on the SIFT Flow and LM+SUN benchmarks (Section 4).

@&#CONCLUSIONS@&#
