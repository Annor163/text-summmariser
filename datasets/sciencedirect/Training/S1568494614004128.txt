@&#MAIN-TITLE@&#
A comparative study of classifier ensembles for bankruptcy prediction

@&#HIGHLIGHTS@&#
This paper examines the construction issues of classifier ensembles for bankruptcy prediction.The first issue focuses on the classification techniques, which are based on MLP, SVM, and DT.The second issue is the combination method, which is based on bagging and boosting.The third issue is based on examining different numbers of combined classifiers.We show that DT ensembles composed of 80–100 classifiers using the boosting method perform best.

@&#KEYPHRASES@&#
Bankruptcy prediction,Credit scoring,Classifier ensembles,Data mining,Machine learning,

@&#ABSTRACT@&#
The aim of bankruptcy prediction in the areas of data mining and machine learning is to develop an effective model which can provide the higher prediction accuracy. In the prior literature, various classification techniques have been developed and studied, in/with which classifier ensembles by combining multiple classifiers approach have shown their outperformance over many single classifiers. However, in terms of constructing classifier ensembles, there are three critical issues which can affect their performance. The first one is the classification technique actually used/adopted, and the other two are the combination method to combine multiple classifiers and the number of classifiers to be combined, respectively. Since there are limited, relevant studies examining these aforementioned disuses, this paper conducts a comprehensive study of comparing classifier ensembles by three widely used classification techniques including multilayer perceptron (MLP) neural networks, support vector machines (SVM), and decision trees (DT) based on two well-known combination methods including bagging and boosting and different numbers of combined classifiers. Our experimental results by three public datasets show that DT ensembles composed of 80–100 classifiers using the boosting method perform best. The Wilcoxon signed ranked test also demonstrates that DT ensembles by boosting perform significantly different from the other classifier ensembles. Moreover, a further study over a real-world case by a Taiwan bankruptcy dataset was conducted, which also demonstrates the superiority of DT ensembles by boosting over the others.

@&#INTRODUCTION@&#
Developing an effective bankruptcy prediction model is a very important but rather difficult task for financial institutions. The aim of bankruptcy prediction models is to predict whether or not a new applicant (including individual and company) will go bankruptcy or not. If the prediction models could not perform well (i.e. to provide a certain, high prediction error rate) it will lead to make incorrect decisions and hence, very likely to cause great financial crises and distress [29].Similar to the objective of bankruptcy prediction, credit scoring (or rating) focuses on determining if loan customers belong to either a good or a bad applicant group. In other words, an effective credit scoring model can also help financial instructions decide whether or not to grant a credit to new applicants [10]. Particularly, both bankruptcy prediction and credit scoring are regarded as the financial decision making problems as well as binary classification problems. That is, the model is designed to assign new observations to two pre-defined classes, which are ‘good’ and ‘bad’ risk classes [26]. That is, if a credit scoring model classifies a new observation into the ‘bad’ risk class, this is similar to a bankruptcy prediction model that forecasts the new observation to be bankrupt. In other words, a ‘bad’ risk case can be simply regarded as the same as the ‘bankruptcy’ case.Related literature and studies have shown that machine learning techniques, such as neural networks outperform conventional statistical techniques including logistic regression, in terms of prediction accuracy and error [27,29]. In specific, combining multiple classification techniques or classifier ensembles perform far better than single classification techniques [17].Generally speaking, classifier ensembles are based on training a fixed number of classifiers for the same domain problems (or the training sets), and the final output over a given unknown data sample can be obtained by combining the outputs made by the trained classifiers. In literature, bagging and boosting are the two widely used combination methods [17] (c.f. Section 3.2).Although many related studies have demonstrated the superiority of classifier ensembles over many single classifiers, most of them only constructed a specific type of classifier ensembles for bankruptcy prediction, such as neural network ensembles [13,29,31,33] and decision tree ensembles [1,26,32,35]. In addition, most of these classifier ensembles are only based on one specific combination method, i.e. either bagging or boosting (c.f. Section 3.3).Despite some previous works focus on comparing bagging and boosting methods [5,19], where their findings show that the boosting method outperforms the bagging method, they conclude that the performances of classifier ensembles by bagging and boosting are usually domain dependent.Therefore, in the domain problems of bankruptcy prediction and credit scoring assessment there is no comparative study to assess the performances of a good collection of different classifier ensembles. In other words, this fact raises our research question concerning which classifier ensembles perform best.To construct classifier ensembles, three issues in general, need to be carefully addressed/examined. First of all, since there are various classification techniques available, which one can be the best technique for the construction of classifier ensembles? Secondly, how many classifiers should be combined in order to provide a better performance? Thirdly and finally, which combination method should be used to combine multiple outputs produced by individual classifiers for a final output? To take care of these three issues, it is critical to investigate how to construct the optimal classifier ensemble for bankruptcy prediction and credit scoring. More specifically, in addition to using single classifiers as the baseline classifiers, we can further identify the representative baseline of classifier ensembles for future research.This paper is organized as follows. Section 2 overviews the basic concept of classifier ensembles followed after the introduction section. Section 3 discusses the critical issues of constructing classifier ensembles and then, provides a review of related works in this subject area. Section 4 presents the experimental results and the conclusion is provided in Section 5.In the areas of pattern recognition and machine learning, the combination of a number of classifiers has recently been a popular research direction [20,22,23]. Further, this combination approach can be regarded as either ensemble classifiers or modular classifiers. Ensemble classifiers aim at obtaining highly accurate classifiers by combining less accurate ones. They are basically proposed to improve the classification performance of a single classifier [14]. That is, the combination one is able to complement the errors made by the individual classifiers on different parts of the input space. From the above discussion, the performance of modular classifiers is likely to perform better than the one of the best single classifiers used in isolation.The concept is further, inspired by the nature of information processing in the brain which is modular. That is, individual functions can be subdivided into functionally different subprocess or subtasks without mutual interference [8]. This forms the divide-and-conquer principle that a complex problem can be divided into subproblems (i.e. simpler task), which can then be resolved with a different neural net architecture or algorithm. Then, the ultimate solution is reassembled from the results of the subtasks [25].In addition to accuracy improvement (i.e. better generalization), efficiency (i.e. learning speed) is another important advantage in combining classifiers since the modularity results in an architecture with a lesser complexity. Moreover, it is relative easier and faster to train the set of simpler functions. Modular architectures have also found to be favorable over a single model in terms of such advantages as interpretable representation, scaling and ease of modification of architecture [12].Fig. 1shows the general architecture of a classifier ensemble [9]. A number of differently classifiers (i.e. experts) share the input and whose outputs are combined to produce an overall output. Note that the experts can be trained by providing different examples (or different features) of a given training set or different learning models trained by the same training set.Bankruptcy prediction and credit scoring assessment can be approached by designating a single classifier. According to the study of Lin et al. [17], neural networks (especially multilayer perceptron networks), support vector machines, and decision trees are three most popular supervised learning techniques. These techniques are briefly introduced below.Neural networks (or artificial neural networks) contain information-processing units similar to the neurons available in the human brain except that the information-processing units in a neural network are artificial [9]. Neural networks can learn by experience, generalize from previous experiences to new ones, and hence make useful decisions. A neural network consists of neural nodes which are linked to weighted nodes. Nodes and connections among nodes are analogous to brain neurons and synapses connecting brain neurons, respectively.The most common neural network model is the multilayer perceptron (MLP) network, which includes an input layer with a set of sensory nodes as input nodes, one or more hidden layers of computation nodes, and an output layer of computation nodes. The input nodes/neurons are the feature values of an instance whereas the output nodes/neurons are discriminators between the class of the instance and those of all other instances.According to the study of Haykin [9], input vector x in a multilayer architecture passes through the network via the hidden layer of neurons to the output layer. The weight connecting input element i to hidden neuron j is denoted by Wji, and the weight connecting hidden neuron j to output neuron k is denoted by Vkj. The net input of a neuron can be calculating by determining the weighted sum of its inputs while its output can be determined by a sigmoid function. Therefore, for the jth hidden neuron(1)netjh=∑i=1NWjixiandyi=f(netjh)while for the kth output neuron(2)netko=∑j=1J+1Vkjyiandok=f(netko)The sigmoid function f(net) is the logistic function(3)f(net)=11+e−λnetwhere λ controls the gradient of the function.For a given input vector, the network produces an output ok. Each response is then compared to the known desired response of each neuron dk. All weights in the network are then, modified continuously to correct and/or reduce errors until the total error from all training examples is limited to a pre-defined tolerance level.For the output layer weights V and the hidden layer weights W, the update rules are given in Eqs. (4) and (5), respectively(4)Vkj(t+1)=vkj(t)+cλ(dk−ok)ok(1−ok)yj(t)(5)Wji(t+1)=wji(t)+cλ2yj(1−yj)xi(t)∑k=1K(dk−ok)ok(1−ok)vkjVapnik [31] first introduced support vector machines (SVMs) to perform binary classification – i.e., to separate a set of training vectors for two different classes (x1, y1), (x2, y2),…,(xm, ym) where xi∈Rddenotes vectors in a d-dimensional feature space, and yi∈{−1, +1} is a class label. To generate an SVM model, input vectors are mapped onto a new higher dimensional feature space denoted as Φ:Rd→Hfwhere d<f. An optimal separating hyperplane in the new feature space is then constructed by a kernel function K(xi, xj), which is the product of input vectors xiand xjand where K(xi, xj)=Φ(xi)·Φ(xj).The decision tree takes the form of a top-down tree structure, which splits the data to create leaves. A decision tree is built where each internal node denotes a test on an attribute and each branch represents an outcome of the test. The leaf nodes represent either classes or class distributions. The top-most node in a tree is the root node with the highest information gain. After the root node, one of the remaining attribute with the highest information gain is then chosen as the test for the next node. This process continues until all the attributes are compared or there are no remaining attributes on which the samples may be further partitioned [3].The attribute with the highest information gain (or greatest entropy reduction) is chosen as the test attribute for the current node. Such an information-theoretic approach minimizes the expected number of tests needed to classify an object and guarantees that a simple (but not necessarily the simplest) tree is found.Imagine selecting one case at random from a set S of cases and announcing that it belongs to some class Cj. The probability that an arbitrary sample belongs to class Cjis estimated by(6)Pi=freq(Cj,S)|S|where |S| denotes the number of samples in the set S, and so the information it conveys is −log2pibits.Suppose a probability distribution P={p1, p2,…,pn} is given then the information conveyed by this distribution, also called the entropy of P, is well known as(7)Info(P)=∑i=1n−pilog2piIf we partition a set T of samples based on the value of a non-categorical attribute X into sets T1, T2,…,Tm, then the information needed to identify the class of an element of T becomes the weighted average of the information needed to identify the class of an element of Ti, i.e. the weighted average of Info(Ti)(8)Info(X,T)=∑i=1m|Ti|T×Info(Ti)The information gain, Gain(X, T), is then defined as(9)Gain(X,T)=Info(T)−Info(X,T)This represents the difference between the information needed to identify an element of T and the information needed to identify an element of T after the value of attribute X has been evaluated. Thus, it is the gain in information due to attribute X.The simplest method to combine classifiers is actually majority voting. The binary outputs of the k individual classifiers are pooled together. Then, the class which receives the largest number of votes is selected as the final classification decision [14]. In general, the final classification decision that reaches the majority of (k+1)/2 votes is taken.In bagging, several classifiers are trained independently by different training sets via the bootstrap method [2]. Bootstrapping builds k replicate training data sets to construct k independent networks by randomly re-sampling the original given training dataset, but with replacement. That is, each training example may appear to be repetitive but not at all in any particular replicate training data set of k. Then, the k networks are aggregated via an appropriate combination method such as majority voting.In boosting, similar to bagging, each classifier is trained using a different training set. However, the k networks are trained not in a parallel and independent way, but sequentially instead. The original boosting approach, boosting by filtering, was proposed by Schapire [24]. In boosting by filtering, three experts are individually trained. The first one is trained on m training examples. Then, the second expert is also trained by the m training examples. These training data are selected from the pool of the training data such that half of them are classified correctly and half of them are classified incorrectly by the first expert. Therefore, the second expert obtains 50% of patterns for training which were misclassified by expert one. Next, the third expert is trained only on data that the first two experts disagree. The classification decision is made by a majority vote of these three experts. However, this method requires a very large training data set to obtain m that expert one and two disagree.AdaBoost is a combination of the ideas behind boosting and bagging and does not demand a large training data set as the other two. Initially, each training example of a given training set has the same weight. For training the kth classifier as a weak learning model, n sets of training samples (n<m) among S are used to train the kth classifier. Then, the trained classifier is evaluated by S to identify those training examples which cannot be classified correctly. The k+1 network is then trained by a modified training set which boosts the importance of those incorrectly classified examples. This sampling procedure will be repeated until K training samples is built for constructing the kth network. Therefore, the final decision is based on the weighted vote of the individual classifiers [5,6].Note that for the theoretical comparison between different combination methods, please refer to [7,16].Table 1lists these prior studies that had developed classifier ensembles by using neural networks, support vector machines, and decision trees. In addition, several attributed including the combination methods used, the number of combined classifiers, the baseline classifiers compared, and the datasets used for experiments are used to compare and contrast the relative differences.According to Table 1, it is noted that constructing classifier ensembles is an active research area in the areas of bankruptcy prediction and credit scoring where bagging and boosting are widely used combination methods. Related studies applying classifier ensemble techniques have shown that they are superior to many single classification techniques. In literature, there are several works that compare different classifier ensembles. For example, Dietterich [5] construct decision tree ensembles by bagging, boosting, and randomization. The findings are that the better performance depends on the level of classification noise. On the other hand, Opitz and Maclin [19] compare neural network ensembles and decision tree ensembles by bagging and boosting over 23 datasets. They observe that bagging is sometimes much less accurate than boosting, but the performance of the boosting method is dependent on the characteristics of the dataset being examined.However, for the bankruptcy prediction problem, most studies had constructed specific classifier ensembles without considering different classifier ensembles for a comparison purpose (c.f. Table 1). In addition, previous works of comparing different classifier ensembles do not provide clues for constructing the best bankruptcy prediction model by classifier ensembles. Therefore, there is no clear answer to the question about which classifier ensemble can provide the highest rate of prediction accuracy.More specifically, since bagging and boosting/Adaboost are two most widely used combination methods to construct classifier ensembles, many of these aforementioned studies only apply one of them (i.e. either bagging or boosting/Adaboost). In other words, very few studies examine the performances of different classifier ensembles by bagging and boosting respectively.Finally, the numbers of combined classifiers considered in related studies are quite different. That is, some combine a fixed number of multiple classifiers (e.g. 10) while some others use different numbers of combined classifiers for comparisons. This again leads to an urgent need of conducting a comprehensive study of constructing classifier ensembles by different combination methods with different numbers of multiple classifiers for bankruptcy prediction and credit scoring.

@&#CONCLUSIONS@&#
