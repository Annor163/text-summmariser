@&#MAIN-TITLE@&#
Self organizing migrating algorithm with quadratic interpolation for solving large scale global optimization problems

@&#HIGHLIGHTS@&#
In general, complexity of large scale problems increases with rise in dimension.Large scale problems require large population size and computational cost.In this paper SOMAQI has been used to solve large scale problems (dim. 100 to 3000).Only a population size 10 is required to solve all dimensional problems.It can be considered as low computational cost technique.It converges fast as compared to other techniques.

@&#KEYPHRASES@&#
Self organizing migrating algorithm,Quadratic interpolation,Large scale global optimization,

@&#ABSTRACT@&#
Generally the complexity of the large scale optimization problem is considered to increase as the size or dimension of the problem increases and to solve these problems; more efficient and robust algorithms are needed. Several experiments have shown that an increment in dimensions of the problem not only requires an increment in population size but increases the computational cost also. In this paper a Self Organizing Migrating Algorithm with Quadratic Interpolation (SOMAQI) has been extended to solve large scale global optimization problems for dimensions ranging from 100 to 3000 with a constant population size of 10 only. It produces high quality optimal solution with very low computational cost and converges very fast to optimal solution.

@&#INTRODUCTION@&#
Large scale optimization problems arise in many real world applications. With the advent of computers, high speed computational algorithms for solving these problems are in demand. Due to their wide applicability and easy implementation, population based stochastic techniques are considered as better option. But, the main drawback of using these techniques is that they require a reasonable amount of population size with an increment in dimension which in turn incurs high computational cost in terms of function evaluations as well as run time. Many approaches have been proposed in past to solve large scale optimization problems. Ahmed Fouad Ali [1] proposed a hybrid simulated annealing and Nelder–Mead algorithm (SNMRUV) for solving large scale global optimization problems. Its performance was tested on 27 scalable benchmark functions. Among these functions 10 classical functions were tested for 16–512 dimensions only and 17 hard functions were tested for 500 and 1000 dimensions over 30 runs and the algorithm performed well for some functions but not for all. Sheng-Ta Hsieh et al. [2] presented the Efficient Population Utilization Strategy for Particle Swarm Optimizer (EPUS-PSO) and tested its performance on the seven test functions with 100, 500 and 1000 dimensions respectively over 25 runs only and the results achieved for all the problems were not so good. Kazimipour [3] categorized the most powerful initialization method among the earlier developed population based initialization methods and tested its performance on CEC’2008 benchmark functions with 100, 500 and 1000 dimensions for 50 independent runs. The population size was kept constant (i.e. 50) for all initialization methods and it was concluded that QBL could be considered the best method among the eight initialization methods. Rajasekhar et al. [4] provided a new variant of Artificial Bee Colony Algorithm (μABC) for large scale global optimization and tested its performance on 7 shifted benchmark functions given in CEC’2008 with 100 and 500 dimensions for 25 independent runs. Its performance was compared with other algorithms and was found that the algorithm performed well for all the problems except one. Antonio LaTorre et al. [5] selected best hybrid algorithm among the combinations of several well known optimization algorithms and its performance was tested on 15 benchmark functions proposed for the special session on large scale global optimization and compared with the reference algorithm, different population sizes were used for different hybrid algorithms (25–400) and the algorithm performed well for all the problems except two. A modified line search method proposed by Grosan et al. [6] makes use of partial derivatives and restarts the search process after a given number of iterations. Its performance has been tested on a set of 8 standard continuous test functions with dimensions 50–10,000 and compared with Genetic Algorithm (GA) and Particle Swarm Optimization (PSO). Population size has been taken very large as 500. Rahnamayan et al. [7] investigated Opposition Based Differential Evolution (ODE) based on Differential Evolution and tested it on seven benchmark functions with 500 and 1000 dimensions. Population size for the proposed work was taken same as the dimensions and as a result computational cost was very high. Rajasekhar et al. [8] introduced an improved version of Artificial Bee Colony (ABC) algorithm with mutation based on Levy Probability Distributions and tested its performance on 7 standard benchmark functions with 100 dimensions and on a set of non-traditional problems with 500 dimensions suggested in the special session of CEC’2008. Population size was taken as 20 for 100 dimensions problems and 50 for 500 dimensions problems and the results obtained were quiet good for some functions. Literature available for large scale optimization shows that many of the efficient algorithms not only require large population size but also require large computation cost. This paper focuses over all above mentioned issues and proposed an alternative approach for solving large scale problems.SOMAQI has been developed earlier to solve unconstrained optimization problems of dimension up to 50 only. In this paper, SOMAQI has been extended to solve problems of dimension up to 3000 keeping a constant population size of 10. An experiment has been made to fine tune the population size by varying it from 10 to 100. The novelty of this approach is that where other algorithms use population size of 2 to 3 times of dimension size, SOMAQI is using only population size 10 and providing good success rate with high quality solutions. Validity of SOMAQI for large scale problems has been tested on 12 benchmark test functions. Numerical results show that SOMAQI is robust algorithm which gives quality solution of large scale problems within seconds in less number of function evaluations.The paper is organized as follows. In Section 2, SOMA is described. In Section 3, the methodology of SOMAQI has been presented. The performance of SOMAQI for solving large scale problems has been discussed in Section 4. Finally, the paper concludes with Section 5.Self Organizing Migrating Algorithm is a population based stochastic optimization technique modeled on the social behavior of a group of individuals [9]. Rather than competing with each other, individuals in SOMA follow the competitive–cooperative behavior. It is classified as an Evolutionary Algorithm (EA), but in contrast of EA, SOMA does not create any new individual during the search; only the positions of the individuals are changed during a generation called “migration loop” (ML). Like other EAs, its working is also simple.The working of algorithm SOMA can be divided into three steps:1.Parameters are defined.The population is initialized randomly distributed over the search space.The solution space is explored as follows:At each generation or migration loop, all individuals from population are evaluated using the objective function. The individual with highest fitness value is known as leader and the individual with worst fitness value is known as active. This algorithm moves in migration loops and in each migration loop active individual travels a certain distance (path length) toward the leader in n (path length/step size) steps of defined length. The movement of an individual is given by:(1)xi,jMLnew=xi,j,startML+xL,jML−xi,j,startMLtPRTVectorjwhere t ϵ (0, by step to, pathlength) and ML is actual migration loop,xi,jMLNewis the new position of an individual,xi,j,startMLis the position of active individual,xL,jMLis the position of the leader.This path is perturbed randomly by a parameter known as PRT (Perturbation) parameter. This parameter does the same effect for SOMA as mutation does for genetic algorithm (GA). It is defined in the range [0,1]. PRT vector (Perturbation vector) is created using the value of PRT parameter before an individual proceeds toward leader as follows: A random number rnd is generated (for each individual's coordinate), and then compared with PRT.Ifrndj<PRTthenPRTVectorj=1;elsePRTVectorj=0;endif;The randomly generated PRT vector takes only two values either 0 or 1. If an element of the PRT vector is set to 0, then the individual is not allowed to change its position in the corresponding dimension.The pseudo code of SOMA is given as follows:Begin{ define parametersgenerate initial populationevaluate all individuals in the populationwhile termination criterion is not satisfied do{generate PRT Vectorsort the populationselect leader and active individual of the populationfor active individual in population do{for k=1 to n do{ move active individual toward leader as follows in Eq. (1).evaluate fitness values at the new positions}select best position}if (fitness value at best position is better than fitness value at the position of active individual)new best position becomes the position of active individualmove active individual to best position}report the best individual as the final optimal solution}endThe detailed information about SOMA can be found in [10–13].Many variants have been developed earlier to improve the efficiency of SOMA [14–16]. Among these NMSOMA-M has proven its efficiency for large scale optimization problems of size up to 1000. SOMAQI is a hybrid variant of SOMA that incorporates the features of quadratic interpolation crossover operator. SOMAQI has been developed by Dipti et al. for solving the function optimization problems of small scale. Several attempts have been made in past to hybridize other population based algorithms with QI and have shown their robustness [17–23]. Among these, SOMAQI works with lowest population size, converges very fast and takes very low computational time to obtain global optimal solution. Generally, with the rise in dimensionality, increment in population size is also required otherwise there are more chances that algorithm may stuck in local optimal solution. But SOMAQI works well with small population size. This motivates us to extend SOMAQI for solving large scale problems.The methodology of this algorithm is given as follows:First the individuals are generated randomly. At each generation the individual with highest fitness value is selected as leader and the worst one as active individual. Now the active individual moves toward leader in n steps of defined length. The movement of this individual is given by Eq. (1). Fitness values at the new positions are calculated and the best position is selected among them. If fitness value at the best position is better than fitness value of active individual, the active individual is replaced with the position of best one. Now to diversify the solution space, the population is sorted again and the best (leader) and worst (active) individuals from the population are selected. A new point x’ is created using quadratic interpolation at the end of each generation using equation(2)x'=12R22−R23*f(R1)+R32−R12*f(R2)+R12−R22*f(R3)R2−R3*f(R1)+R3−R1*fR2+R1−R2*f(R3)For this we choose three distinct particles R1, R2 and R3, where R1 is the leader and R2 and R3 are randomly chosen particles from the remaining population. Here value of f(R1) is chosen arbitrarily as zero in order to get more diverse search space, f(R2) and f(R3) are the fitness values at R2 and R3. This new point is accepted only if it is better than active individual and is replaced with active individual. The computational steps of SOMAQI are given as follows:Step 1: The population is initialized randomly.Step 2: In each loop population is evaluated.Step 3: The population is sorted and the individual with best fitness value becomes the leader and the individual with worst fitness value becomes an active.Step 4: For active individual new positions are created using Eq. (1). Then the best position is selected and replaces the active individual by the new one.Step 5: New point is created by quadratic interpolation using Eq. (2).Step 6: If new point is better than active, then active is replaced with the new one.Step 7: If termination criterion is satisfied, the best individual is reported as the optimal solution.

@&#CONCLUSIONS@&#
Evolutionary algorithms are preferred to solve global optimization problems to traditional approaches. But as the dimensionality of the problem increase, it affects the complexity of the problem also. To get good success rate and accurate solution, one need to increase population size with rise in dimension otherwise algorithm may stuck in local optima or converges slowly. In this paper an approach SOMAQI has been suggested to solve large scale problems. This algorithm not only works with small population size but also takes less computation time, lesser function evaluations and produce good quality solutions. The performance of SOMAQI is tested on 12 unconstrained benchmark problems for dimensions 100, 500, 1000, 2000 and 3000 and results validate the claim.(1)Ackley functionminf(x)=20exp−0.021n∑i=1nxi2−exp1n∑i=1ncos(2πxi)+20+eforxi∈−30,30,x*=0,0,0,…,0,fx*=0Cosine Mixtureminf(x)=0.1n+∑i=1nxi2−0.1∑i=1ncos(5πxi)forxi∈−1,1,x*=0,0,0,…0,fx*=0Griewankminf(x)=1+14000∑i=1nxi2−∏i=1ncosxiiforxi∈−600,600,x*=0,0,0,…,0,fx*=0Rastriginminf(x)=10n+∑i=1nxi2−10cos(2πxi)forxi∈−5.12,5.12,x*=0,0,0,…,0,fx*=0Zakharovminf(x)=∑i=1nxi2+∑i=1ni2xi2+∑i=1ni2xi4forxi∈−5.12,5.12,x*=0,0,0,…,0,fx*=0De-Jong's Function With Noiseminf(x)=∑i=1n−1i+1xi4+rand(0,1)forxi∈−1.28,1.28,x*=0,0,0,…,0,fx*=0Sphereminf(x)=∑i=1nxi2forxi∈−5.12,5.12,x*=0,0,0,…,0,fx*=0Axis Parallel Hyper Ellipsoidminf(x)=∑i=1nixi2forxi∈−5.12,5.12,x*=0,0,0,…,0,fx*=0Shifted Sphereminf(x)=∑i=1nzi2+f_biasZ=X−O,X=[x1,x2,…,xD,],O=[o1,o2,…,oD,]forxi∈−100,100,x*=0,0,0,…,0,fx*=f_bias=−450Shifted Rastriginminf(x)=10n+∑i=1nzi2−10cos(2πzi)+f_biasZ=X−O,X=[x1,x2,…,xD,],O=[o1,o2,…,oD,]forxi∈−5,5,x*=0,0,0,…,0,fx*=f_bias=−330Shifted Griewankminf(x)=1+14000∑i=1nzi2−∏i=1ncoszii+f_biasZ=X−O,X=[x1,x2,…,xD,],O=[o1,o2,…,oD,]forxi∈−600,600,x*=0,0,0,…,0,fx*=f_bias=−180Shifted Rosenbrockminf(x)=∑i=1n−1100zi+1−zi22+(zi−1)2+f_biasZ=X−O,X=[x1,x2,…,xD,],O=[o1,o2,…,oD,]forxi∈−100,100,x*=0,0,0,…,0,fx*=f_bias=390