@&#MAIN-TITLE@&#
Efficient 3D object recognition using foveated point clouds

@&#HIGHLIGHTS@&#
Object recognition: foveation speedup 7x compared to the non-foveated approach.True recognitions rates are kept high with false recognitions at 8.3%.Faster setups with 91.6% recognition rate and 14x improvement were also achieved.The slowest configuration still shows almost 3x faster computing times

@&#KEYPHRASES@&#
Point cloud,3D object recognition,Moving fovea,

@&#ABSTRACT@&#
Graphical abstract

@&#INTRODUCTION@&#
With current developments experienced in hardware technologies, computer vision systems would be ideally able to capture 3D data of the world and process this data in order to take advantage of their inherent depth information. However, nowadays, most current computer vision systems are still based on 2D images while the use of 3D data can offer more details about geometric and shape information of captured scenes and consequently, of general objects of interest. In this way, the development of 3D object recognition systems has been an active research topic over the last years [1].Recent technology advances have enabled the construction of devices, as for example the Microsoft Kinect [2], that capture 3D data from the real world. The Kinect is a consumer grade RGB-D sensor originally developed for entertainment that has enabled several novel works for research including robotics, commercial, and gaming applications. Mobile phone manufactures have also started to shipping smartphones with stereo vision cameras in the recent years. Other manufacturers already announced camera sensors with depth information as a 4th channel. Furthermore, the price reduction of equipment is driving a wide adoption of 3D capture systems.Although the amount of data provided by 3D point clouds is very attractive for object recognition, it requires intensive computing algorithms that could render systems based on this type of data computationally prohibitive, mainly if real time interaction is needed. Hardware accelerators and optimizations are frequently used for real time computing, however object recognition is still an open research field with several challenging research opportunities, especially when real time performance is desired. One software solution consists in processing point clouds efficiently using algorithms that compute local geometric traits. One example of such system is depicted in Section 4, which enumerates advantages of correspondence grouping algorithms.We are interested on accelerating object retrieval using 3D perception tools and data acquisition from real images (not synthetic images). For this purpose, we propose the usage of a moving fovea approach to downsample 3D data and reduce the processing of the object retrieval system from point clouds. An example of foveated cloud can be seen in Fig. 1. Experimental results show that our system offers up to seven times faster recognition time computing without compromising recognition performance. We also provide two web based tools to interactively view and manipulate point clouds and to capture Kinect point clouds without the need to install any software, which has been used within the Collage Authoring System.This article is structured as follows: Section 2 presents the theoretical background used in this work with reviews of some related works on 3D object retrieval and the moving fovea approach. Section 3 describes 3D moving fovea applied to the object recognition problem and its formulation in the context of our work. Section 4 depicts both the system that forms the base of our implementation and also the proposed scheme, along with implementation considerations. Section 5 describes the experiments, including a performance evaluation that can be executed with the Collage authoring environment, while Section 6 closes the article with our final remarks.Three-dimensional object recognition is a multi-disciplinary area of research, with major contributions originating from the Pattern Recognition, Computer Vision, Robotics and Computer Graphics communities. In this section, relevant contributions from each of these subareas are briefly enumerated, emphasizing the data acquisition method employed in each of them.Vision is so far the most important sensing resource for robotics tasks that can be executed based on devices like web cameras and depth sensors. Unfortunately, the huge amount of data to be processed is limited by the processing time that is a restriction for doing reactive robotics. Several approaches represent image with non-uniform density using a foveated model that mimics the retina mapping to the visual cortex in order to deal with this amount of data [3–9]. The fovea is the area of retina with greatest visual acuity, so foveated models have high resolution nearby the fovea and decrease the resolution according to the distance from the fovea.The foveation process is performed either by subsampling in software [3,4], by hardware with reduced sampling [10] or by using a system with 2 or more cameras, where one is used for peripheral vision and another one is used for foveated vision [11,12]. The software foveation allows greater ease of modification and easily implementable in conventional hardware, but is slower than hardware solutions which are usually more expensive and difficult to change. In terms of coverage, solutions that use specific cameras to peripheral and foveated vision are more open to stimuli of the whole environment by using a wide angle peripheral camera, what would require a huge resolution camera in the case of a single camera system due to high resolution fovea needs. However, a camera specific for foveated vision requires movement of physical devices and a large difference between peripheral and fovea cameras suppress stimuli appearing on a intermediate level of resolution because these are not in the fovea camera field of view neither in the peripheral camera. In this work, the foveation is performed by software. It is important to note that most of these models allow free movement of the fovea, what does not happen at the biological eye's retina. Otherwise, all the vision resources should be moved in order to keep the object at foveal region.In a dynamic and cluttered world, all information needed to perform complex tasks are not completely available and not processed at once. Information gathered from a single eye fixation is not enough to complete these tasks. In this way, in order to efficiently and rapidly acquire visual information, our brain decides not only where we should look but also what is the sequence of fixations [13]. This sequence of fixations, and therefore the way the fovea is guided, is related to cognition mechanisms controlled by our visual attention mechanism. Several works propose saliency maps from which fixations can be extracted [14].It is also known that the human vision system has two major visual attention behaviors or dichotomies [15]. In the top-down attention approach, the task in hand guides attention processing. On the other hand, in bottom-up attention, external stimuli drive attention. Text reading is an example of the top-down behavior of attention, where visual fixations are done systematically, passing through the paper in a character by character and line by line movement. On the opposite, if a ball is thrown toward the same reader, this bottom-up stimulus will make the reader to switch attention to the dangerous situation.Besides in robotic vision, several foveated systems are proposed in order to reduce the amount of data to be coded/decoded also in real-time video transmission [8,6]. In this kind of application, an image should be encoded with foveation thus keeping higher resolution in regions of interests. In a similar way, Basu [16] proposes a foveated system to 3D visualization with limited bandwidth restriction, where the fovea position controls the objects’ texture quality and resolution.Early object recognition systems acquired data from expensive and rarely available range sensors, such as laser scanners [17,18] and structured light patterns [19]. Ashbrook et al. [17] describe an object recognition system that relies on similarities between geometric histograms extracted from the 3D data and the Hough Transform [20]. Johnson and Hebert popularized the Spin Images descriptor [18,19], which was used as the basis to an object recognition algorithm that groups correspondences of Spin Images extracted in a given query model and those extracted in the scene data that share a similar rigid transformation between the model and the scene [18]. Data from 3D scanners and also from synthetic CAD 3D models are employed in the work of Mian et al. [21].Until recently, 3D object recognition systems processed data mostly in an off-line fashion, due to long computing times involved [22]. This paradigm has started to shift as algorithms have been proposed in the Robotics community [23,24] to enable real-time manipulation and grasping for robotic manipulators. In fact, algorithms designed to describe 3D surfaces through histograms of various local geometric traits evaluated on point clouds became a major trend in the last years [25–27,23]. Consequently, faster and more accurate 3D object recognition systems based on keypoint matching and descriptors extracted in the scene and in the sought object point clouds were developed. After being established, point correspondences are grouped by hypotheses sharing a common transformation, which is estimated by voting [28,29], multi-dimensional clustering [30,31] or RANSAC [32] (also used to detect shapes on 3D data [33]). The presence of the object of interest is then inferred if certain conditions are met, such is the number of votes, cluster size, or the number of RANSAC inliers.With the wider availability of consumer-grade depth sensors such as the Microsoft Kinect, several works on 3D object recognition are proposed employing this class of sensor [34–38,24]. Aldoma et al. [24] proposed the global feature coined Clustered Viewpoint Feature Histogram (CVFH) to improve performance of object recognition for robotics. Machine learning based approaches [37,38] were formulated to perform 3D object recognition making heavy use of depth information, without any computation on point clouds involved.Aldoma et al. [34] highlight how algorithms that are part of the Point Clouds Library (PCL) software package could be used to form 3D object recognition systems based on local and global features. There are also 3D object classification/categorization systems, as in the works of Wohlkinger et al. [35,36] and of Lai et al. [37]. In this latter class of systems, every chair in a scene should be labeled as the object of type “chair”, whereas in object recognition only the specific chair being sought should be retrieved from the scene.This work proposes the use of a foveated point cloud in order to reduce the processing time of object detection. The idea is that the point density is higher nearby the fovea and that this density decreases according to the distance from the fovea. In this way, it is possible to reduce the total number of the points reducing also the processing time at the same time that the density around the fovea is enough to keep feasible the object detection. Parts of the point cloud with reduced density may be useful in providing other stimuli which may be part of a context of visual attention. For example, a saliency map can be computed in the foveated cloud in order to drive bottom-up or top-down stimulus. This can be very useful in the context of robotic vision, since the robot can be aware to multiple simultaneous stimuli in the environment.The foveated point cloud proposed here is based on the 2D foveated model proposed by Gomes [4]. This model transforms an image into a set of smaller images with same size but with different resolutions. In order to achieve that, the model defines image patches from the original image that are arranged in a sequence of levels. The first level is a mapping of the whole original image while the last one is a mapping of a patch placed at the original image centered at a fovea. This patch has the same size of each image level. The result is a set of small images that composes a foveated image.In the 3D case, instead of resampling concentric image patches, the foveated point cloud is achieved by downsampling the original point cloud using concentric boxes, each one representing a level as shown in Fig. 2. Each box specifies a point cloud crop each one with a different point cloud density. The outer box has one of its corners placed at a specific 3D coordinate and it defines the model coordinate system. See the axes in Fig. 2. All points outside this box are discarded. Inside it, smaller boxes are linearly placed. The smallest box is centered at a parameter called fovea: a 3D coordinate where the point cloud density is maximum. A downsampling schema is applied in this smallest box. Each bigger box is also downsampled but with a level by level decreasing point cloud density up to the outer box, where the point cloud density is minimum.The proposed foveated point cloud is formalized as follows. We define m+1 3D boxes of sizeSk∈R3, withk=0,1,…,mrepresenting each level. Each level of the foveated point cloud model changes the point cloud density. The first level (level 0) has a density reduction by d0 and the last one (level m) has a density reduction by dm. The density reduction of intermediate levels is given by linear interpolation between d0 and dm.The largest box has three parameters: size (S0), orientation and position (denoted byΔ). Usually, if the whole point cloud should be covered, it is possible to automatically set these three parameters as the bounding box of the entire scene. However in some applications, it could be interesting to place it in a part of a huge point cloud. The last two parameters determine the model coordinate system.The smallest box is guided by a foveaFat that box center. For formalization convenience, the fovea coordinate system origin is (0, 0, 0) at the largest box center. In this way,F=F′−S0/2, whereF′is the fovea at model coordinate system.Letδk∈R3be the displacement of box at level k, thenδ0=(0,0,0)andδm+Sm/2=F′.The displacement of each box using linear interpolation is given by(1)δk=k(S0−Sm+2F)2mNote thatδkis defined only form>0; in other words, the foveated model should have at least 2 levels.The size of each k-th box using linear interpolation is given by(2)Sk=kSm−kS0+mS0mHere, we introduce a fovea growth factorG=(sx,sy,sz)∈R3, wheresx,sy,szare the scale factors applied to directions x, y and z, respectively (see Fig. 3). As detailed in Section 4, this factor increases the number of points by enlarging levels volumes. Observe that this model behaves like there is no foveation whenGgoes to∞.In this way, each level is bounded by the lower limit of maximum betweenδk−Gand (0, 0, 0) and the upper limit of the minimum betweenδk+Sk+Gand S0. These minimum and maximum limit the levels to the original point cloud boundary.After foveated levels boundaries computation, the point cloud is downsampled in order to change the point cloud density. In this step, there are two possibilities of point cloud storage: to create a single point cloud joining all the downsampled points from each level or to store each downsampled point cloud from each level independently. Note that both ways can be adopted simultaneously.However, by joining all points in a single cloud leads to geometric distortions, probably imperceptible on a visual inspection, if the downsampling algorithm modifies the points coordinates. A possible solution to this issue is to join all points from a level that do not belong to an inner level. This way points from a level do not mix with points from another one. In order to ensure the disjunction between levels, it is enough to test if the point from a level k to be inserted in the foveated point cloud is not inside the box k+1 (k≠m) as depicted in Algorithm 1. Example of a foveated cloud point can be seen in Fig. 4Algorithm 1Processing steps applied to foveate a point cloud.As explained before, one of the parameters of the foveated point cloud is the fovea position vector. A dense point cloud is more suitable to successful correspondence matching. If the object is far from the fovea then fewer keypoints are extracted and less correspondences are found. Thus, it is desirable to keep the fovea near the object. In order to achieve better results, the proposed architecture includes a visual attention module that guides the fovea placement.First, a function evaluates if the object is detected by the system. If the object is detected, then the fovea is moved to the object's centroid. Otherwise, if the object is not detected, then some strategy may be applied in order to recover the fovea position. A sequence of fixations can also be used along the time or at once in order to detect where the object is. Once the object is detected, a tracking module can be applied so that the fovea is always in the desirable place.One straightforward strategy is to disable foveation until the object is found. This temporarily increases the processing time, but the original point cloud is used and, then, the object can be found at foveated peripheral areas. Another strategy is to gradually increase the growth fovea factor. By using this strategy, it is possible to gradually increase the number of cloud points and thus avoiding having a processing time peak. Another possible strategy is to use a bottom-up attention strategy. In this case, the fovea is moved to the most salient region, which can be computed considering the class of objects to be found.If the scene has more than one object, then it is possible to foveate each object at a time and process them in sequence. In other words, if two objects, for example, ask for top-down attention, then the visual process pay attention to one in a frame and to the another one in the next frame.As some of these issues are not the main contribution of the current work, we neglect it to be treated in a future work. We just wanted to remark that it is possible to apply several strategies based on visual attention in order to properly place the fovea.In this section, we discuss the core framework that our system is based, the correspondence grouping algorithm [29]. After showing the standard method, the foveated scheme to recognize objects is presented, along with the modifications and implications that were needed to maximize performance using multiresolution data.The proposed object recognition scheme works with point clouds (set of 3D points referenced in a fixed frame) representing the object model and the scene to be processed. Positions in this reference frame supposedly having an instance of the sought object are given as outputs. We note here that our system recognizes objects in a scene if and only if the model of the query object is available, implying that it does not perform object classification/categorization or retrieve similar objects from a previously computed database (as is the case of some systems enumerated on the work of Tangelder and Veltkamp [22]). Put differently, the query object is searched in the scene and not vice versa. As a consequence, this allows the system to find multiple instances of the same object in a single scene.We have chosen to build our system based on the local 3D features framework, which exploits local geometric traits at key positions in point clouds in order to extract discriminative descriptors employed to establish point correspondences between keypoints from the model and from the scene. These point correspondences are further processed to infer possible presences of the object. Moreover, this class of system presents some desirable and important properties, such as robustness to occlusion and scene clutter, dispensing the need to elaborate extensive and cumbersome training stages (mandatory for machine learning approaches) and ability to process point clouds acquired from RGB-D sensors like the Kinect in an efficient manner.The system is based on the correspondence grouping approach of Tombari and Di Stefano [29] (with implementation publicly available [39]), in which a model object is recognized in the scene if, after keypoint correspondences being established, enough evidence for its presence in a given position is gathered. This scheme is shown in Fig. 5a. For the sake of completeness, every step of the system is described as follows.The first step in the correspondence grouping algorithm is to describe both the scene and model point clouds. For this, the normal vector for each point is computed considering a surface generated by a neighborhood of size knaround each point. Then, a uniform downsampling algorithm is applied to extract keypoints as the centroid of all points contained within a radius rk. After this, SHOT (Signature of Histograms of OrienTations) descriptors [27] are computed, assembling a histogram of the normals within a neighborhood of radius rsas the signature of a keypoint. The last step to fully describe point clouds is a very important stage encompassing the estimation of a Local Reference Frame (LRF) for keypoints of the model and scene. Thus, the principal axes spanning a LRF within a neighborhood of radius rlin each keypoint position are estimated robustly by the algorithm of Petrelli and Di Stefano [40]. The result of this computation (three unit vectors for each principal direction) is associated with each keypoint and will be employed in the final stage of the object recognition scheme. Different values for the parameters are set in the scene and in the model, allowing more precise recognition tasks. For clarity, the process which extracts descriptors for the model and scene is shown in Algorithms 2 and 3 respectively, with parametersknm,rkm,rsm,rlmused for the model andkns,rks,rss,rlsused for the scene.Algorithm 2Processing steps applied to the model point cloud. See text for parameter details.Processing steps applied to the scene point cloud. See text for parameter details.For each keypoint and respective descriptor and LRF in the model a match in the scene is searched by finding the closest scene point (in the Euclidean sense) in the n-dimensional space containing the SHOT descriptors. Search procedures are employed in a kd-tree to handle the cumbersome routine involved. If the squared distance between the SHOT descriptors is smaller than a thresholddmax2, a point correspondence is established. This process is highlighted in Algorithm 4.Algorithm 4Keypoint matching. See text for parameter details.Since each model and scene keypoint have a LRF associated, a full rigid body transformation modeled by a rotation and a translation can be estimated between the LRFs associated with each keypoint correspondence. Accordingly, a 3D Hough Space is used to gather evidence of the object presence through a voting process. After the rigid body transformation is applied, the cell of the Hough Space containing this 3D position is calculated and its accumulator incremented. Finally, after repeating these steps for all correspondences, object instances are deemed found at each cell having the number of votes larger than a predefined threshold Vh. The size of each cell is controlled by a parameter Lh. Algorithm 5 illustrates the object recognition scheme through correspondence grouping.Algorithm 5Object recognition based on correspondence grouping. See text for parameter details.We note that the normal vectors are evaluated considering neighborhoods formed by all points in the clouds, whereas the SHOT descriptors and LRFs are computed at the neighborhoods of the keypoints. In this way, computation time is saved, while the local 3D geometric traits are still kept discriminative.To enhance the 3D object recognition capabilities of the correspondence grouping approach, the cloud foveation algorithm is employed after some adaptations. A complete scheme of the proposed 3D object recognition system is shown in Fig. 5b.A foveated model is applied to the cloud acquired from the depth sensor according to Algorithm 1 and the parameters of Table 2. Normal estimation can be done before or after foveation. In the first case, the computation is more expensive, but the captured geometric traits of the scene are less distorted. In the current version of the system, we opted to conserve scene geometry.Since the keypoint extraction (uniform downsampling) would extract keypoints with a single radius (originally rks), the multiresolution of the scene cloud would not be respected, as shows Fig. 6b. Consequently, we adapted the keypoint extraction to be also dependent on different and specified levels of resolution, possibly differing of the used downsampling radiid0…dm. The correspondence grouping algorithm was then modified to accommodate keypoint extraction with foveated point clouds. The scene points are downsampled using a different radius rkfor each levelk=0,…,m. The first level (level 0) uses a radius of r0 and the last (level m) uses a radius of rm. All other radii from the intermediate levels are linearly interpolated. Thus, keypoints can be extracted respecting each level of the foveated scene resolution, as shows Fig. 6c.There are two major consequences about this approach. First, there is a considerable time saving due to the keypoint reduction both in descriptors computation and correspondence step, since different values for the extraction radius are employed instead of a (possibly small) single value. Second, it is possible to greatly increase the keypoint density near the fovea position without significantly increasing the total number of original scene points. This peripheral decrease and foveal increase in the number of points also reduces the number of false descriptors correspondences, improving thus the object detection success rate if the fovea is properly placed.In the foveated version of the recognition scheme, Algorithm 3 (scene processing) would be modified to include the scene foveation after the normal estimation and to extract keypoints using a radius value rkfor each resolution level instead of the rks(see Fig. 5b).

@&#CONCLUSIONS@&#
We have presented in this article the usage of the moving fovea approach to efficiently execute 3D object recognition tasks. We note that although this paper explores the usage of the foveated approach to a specific object recognition algorithm, the proposed technique is well suitable to be used with any other 3D object recognition or retrieval system.One key feature of the proposed approach is the usage of a multiresolution scheme, which uses several levels of resolution ranging from the highest resolution, possibly equal to the resolution of the original scene, to lower resolutions. Lower resolutions are obtained by reducing the point cloud density according to the fovea level, which consequently reduces the processing time. This setup is similar to the human vision system, which focus attention and processing on objects of interests at the same time that keeps attention and processing to peripheral objects, but with lower resolution.Experimental analysis shows that the proposed method dramatically decreases the processing time used to recognize 3D objects on scenes with considerable level of clutter, while keeping accuracy loss to a minimum. In comparison to a state-of-the-art recognition method, a true positive recognition rate of 91.6% was achieved with an improvement of seven fold performance gain in terms of average recognition time per frame. These results are well suitable for usage on mobile and embedded systems with low computational resources or on applications that need faster object recognition processing time, such as robotics.As a future work, we plan to explore the usage of the foveated multiresolution system to best find the fovea position according to possible objects identified on lower scales. As the object is found on the lower scale, then the fovea can focus and process detailed information of the object at the best fovea position.