@&#MAIN-TITLE@&#
Evolving principal component clustering with a low run-time complexity for LRF data mapping

@&#HIGHLIGHTS@&#
A novel approach for data stream clustering to linear model prototypes.Good performance, robust operation, low computational complexity and simple implementation.Validation of results by comparison to well-known algorithms.

@&#KEYPHRASES@&#
Line extraction,Evolving clustering,Laser range finder,

@&#ABSTRACT@&#
In this paper a new approach called evolving principal component clustering is applied to a data stream. Regions of the data described by linear models are identified. The method recursively estimates the data variance and the linear model parameters for each cluster of data. It enables good performance, robust operation, low computational complexity and simple implementation on embedded computers. The proposed approach is demonstrated on real and simulated examples from laser-range-finder data measurements. The performance, complexity and robustness are validated through a comparison with the popular split-and-merge algorithm.

@&#INTRODUCTION@&#
In mobile robotics, localization plays the key role in most applications. In most cases advanced sensor systems are required to estimate the robot pose, mostly due to the fact that there is no single and effective sensor that would directly measure the robot pose in an indoor environment. For an outdoor location the closest approximate is GPS, but this can fail in areas with no satellite signal reception. Therefore, various different sensors are fused together to improve the robustness and quality of the pose estimate [1–4]. In mobile robotics a very popular sensor for this purpose is the laser range finder (LRF), which has good coverage, dense information, high accuracy and a high sampling rate. It can be used for localization purposes, map building or SLAM, as in [5–10]. Using a LRF the robot pose can be estimated by comparing a locally sensed map given by a cloud of reflection points and a known map of the environment. This comparison is usually made by comparing simple geometric features that are extracted from the LRF reflection points. The simplest features are straight lines.To estimate the line feature parameters from 2D laser-range-finder data several line-extraction algorithms have been proposed. The process of line fitting generally requires two steps: first, the input points are investigated to find clusters of points that can be described by a line and, second, a line-fitting method is applied to estimate the straight-line parameters for the identified clusters. This process is usually done recursively. When the points are obtained from a 2D LRF the first step of the procedure can be simplified, because the clusters always consist of consecutive LRF points (here, it is called a sorted data stream). For the second step a least-squares method is normally used.A very popular and powerful approach in image processing is the Hough transform [11], where the data are transformed to a parameter space, and by locating the maxima the number and the parameters of the lines are obtained. Some drawbacks of the classic algorithm are parameter-space quantization as well as substantial computational and storage requirements [12]. To increase the accuracy and avoid the required predefined fine grid of the accumulator in the parameter space, several studies on the randomized Hough transform [13] were proposed that reduce the computational time and the storage requirements. The online adaptive implementations proposed in [14] reduce the space requirements and retain a high parameter precision. However, in general the position and the length of the line segments cannot be determined, and also collinear line segments cannot be separated. Some additional algorithm needs to be implemented to locate the line segments on the identified straight lines.For the data obtained from a LRF used in indoor environments the split-and-merge algorithm [15] is a very common choice made by robot developers. An extensive comparison of line-extraction algorithms is reported in [12], where the split-and-merge and incremental algorithms were preferred because of their simplicity, low computational complexity and good estimation results. Primarily because of their simplicity, incremental algorithms have been used in many applications. The data are incrementally added to the initial line cluster until the data fit the model; otherwise a new cluster is constructed.A binary regression tree obtained by recursive fuzzy clustering and the identification of a hinging hyperplane that consists of two linear submodules is described in [16]. Similarly, a generalized fuzzy C-means clustering is proposed by [17]. A recursive clustering and fuzzy Takagi Sugeno identification is presented in [18–20]. Fuzzy approaches can also be applied to identify a low size feature subset which maximize information and minimize data redundancy as in [29]. These approaches can effectively model general nonlinear dynamics systems, but for linear data several more effective approaches can be applied, as stated in [12].The robust fitting of models in the presence of outliers that can be obtained using the RANSAC (Random Sample Consensus) algorithm is introduced in [21]. A robust expectation–maximization estimate for a mixture of linear-regression models is proposed in [22] and robust clustering around the regression models is proposed in [23].In most presented algorithms, fitting the model to the data involves least-squares methods, which can be hard to code on simple embedded hardware and also require some computational power because the data fitting is carried out many times in the recursive line-extraction algorithms.Our idea is to fit the model using recursive principal component analysis (PCA), which is very easy to implement and computationally effective. A recursive PCA for adaptive process monitoring is used in [24]. An expectation-maximization approach for high-dimensional data model-based clustering using an incremental PCA is presented in [25]. In our study only a recursive covariance matrix needs to be evaluated. From the covariance matrix the model parameters that optimally fit the data in the sense of squared errors are defined by eigenvectors of the covariance matrix. The main advantage of the proposed approach is its low computational complexity and the very simple implementation of the algorithm, as it only requires basic arithmetic operations, such as additions and multiplications, together with high accuracy. This makes the proposed algorithm especially appropriate for SLAM problems where localization is done online.The paper is organized as follows. After the introduction, the structure of the evolving principal component clustering (EPCC) algorithm is presented. Next the performance, computational complexity and robustness of the EPCC are evaluated and compared to the Split-and-Merge algorithm. The experimental and simulation results are described and at the end some conclusions are drawn.In batch clustering the usual problems are the initialization of the clusters and the in-advance guess of the correct number of clusters. The number of clusters can be determined by iteratively increasing the number of clusters and performing a clustering algorithm for each iteration until the terminating criteria are reached (e.g., the split-and-merge algorithm).An alternative approach is recursive clustering, which is especially suitable for data streams. Here, the algorithm recursively estimates the simple statistical properties of the data (mean and variance) and clusters the data into clusters whose centers are defined with linear prototypes. The algorithm starts with one cluster and adds a new cluster when the current data sample does not belong to the existing prototypes. For the dimension of the data r the dimension of the prototype is s=r−1 or less, so s=1 for a straight line, s=2 for a plane and s>2 for a hyperplane.The mean value and the variance for each cluster are calculated recursively. The mean value of the data in cluster j is defined by(1)μj(kj)=kj−1kjμj(kj−1)+1kjz(kj)where kjis the current index of the data in cluster j andz(kj) is the current data sample belonging to the cluster. The initial mean value of the cluster isμj(kj)=z(kj), where kj=1. The covariance matrix of the cluster j is defined as follows(2)Σj(kj)=kj−2kj−1Σj(kj−1)+1kj(z(kj)−μj(kj−1))(z(kj)−μj(kj−1))Twhich is equivalent to the non-recursive formΣj=∑kj=1nj(z(kj)−μj)(z(kj)−μj)Tn−1where njis the number of data in the cluster j.The covariance matrixΣjcontains the elementsΣj=[σio2], i=1, …, r, o=1, …, r, which can be used to estimate the jth linear prototype parameters, as shown in Section2.1.1.The normal vector of the jth linear prototype (hyperplane model of the cluster j) is the eigenvectorpjwith the smallest eigenvalue. The eigenvectors can be obtained by a singular value decomposition of the covariance matrixΣj, where an algorithm like [26] could be applied. However, for two- or three-dimensional data (r=2, the prototype is a straight line or r=3, the prototype is a plane) it is computationally much more efficient if the normal vector is obtained fromΣj. In the following the normal vectorpjto the linear prototype is estimated for two-dimensional data and three-dimensional data. The normal vectorpjbelongs to the smallest eigenvalue, which equals(3)λj=pjTΣjpjThe orthonormal eigenvectors of the cluster j are obtained from the covariance matrixΣjusing its elements. The normal vector in the case of r=2 ispj=−θ1θ12+11θ12+1where(4)θ1=σ12σ11and where σio, i, o∈{1, 2} are the elements of the covariance matrix.In the case of three-dimensional data (r=3) the normal vector is defined bypj=−θ1θ12+θ22+1−θ2θ12+θ22+11θ12+θ22+1where(5)θ1=σ13σ22−σ23σ12σ11σ22−σ122θ2=σ13σ12−σ23σ11σ122−σ11σ22and where σio, i, o∈{1, …, 3} are the elements of the covariance matrix.This approach can also be extended to higher dimensional data, but it becomes more computationally intense. Therefore, for higher dimensions (r>3) the normal vector could instead be obtained by the singular value decomposition ofΣj.The normal vectorpjof the jth prototype that models the dataz(kj) (kj=1, …, nj) in the cluster j defines the jth prototype equation as follows(z(kj)−μj)T·pj=0For the current datum samplez(k), which needs to be classified in one of the existing prototypes j (j∈{1, …, m}), the orthogonal distance dj(k) (in the direction of the normal vector) from the jth prototype is calculated as(6)dj(k)=|(z(k)−μj)T·pj|If dj(k)=0 the data sample lies on the linear prototype j. This orthogonal distance dj(k) is used to determine whether the current data samplez(k) belongs to the jth cluster. To do this test robustly (in the presence of system noise and different data scaling) a criterion compares the orthogonal distance of the current sample to the jth prototype with the variance of the orthogonal distance σjof all the data samples in the cluster. The clustering criterion is formulated as(7)dj(k)<κmaxσjwhere κmaxis a positive constant defining the sensitivity of the clustering criteria relative to the cluster distance varianceσj. If normal LRF sensor noise distribution is supposed then selecting κmax=3 would imply 99.7% off all samples to be properly classified or κmax=4 would result in theoretically 99.9% of all samples belonging to the prototype being also correctly clustered. However by further increasing κmaxthis percentage converges to 100% but also the probability of wrong clustering increases.If the current dataz(k) fulfils criterion (7) the number of data in the jth cluster is increased (nj⟶nj+1) and the cluster distance variance (together with the cluster mean (1) and variance (2)) is updated recursively as follows(8)σj(kj)=σj(kj−1)kj−2kj−1+1kjdj(k)2An additional criterion, when adding the data samplez(k) to the jth cluster, could be the distance to the cluster mean valueμjor the distances between consecutive points (in the case of a data stream from a laser range finder). This distance should be below the average distance between the consecutive data samples in the cluster. Dislocated points will then form a new cluster, although the criterion (7) is fulfilled.The identified strait line segments are defined by the linear prototypes parameters (clusters centers and eigenvectors) and by the end points which are obtained from the two points in the cluster with the maximum distance among them. In sorted data stream this are the first and the last sample in the cluster.The proposed clustering algorithm starts with one cluster and adds a new cluster if the current data samplez(k) does not satisfy the criterion (7) for the current clusters j∈{1, …, m}.To initialize a cluster a set of at least kmin(for r=2, kmin=3) data sample candidates that reliably form a new prototype are required. kminis defined by the data dimensionality r where the linear model can be estimated from at least r data samples. To increase robustness to noise and outliers kminshould be higher so that the identification becomes over determined. First, the mean and variance of the new cluster candidate are calculated using (1) and (2). Then, the distance criterion (7) to the new prototype is validated. To reliably form a new cluster in the initialization phase, the κmaxparameter in (7) can be lowered (e.g., by 50%). Additionally, the distance between consecutive data samples can be validated. The cluster is initialized with the mean value, variance, linear prototype (the last eigenvector of the covariance matrix) and orthogonal distance variance (8).In the case of data streams where the data are arriving in a sorted fashion, as in a laser range finder where consecutive data samples belong to one prototype, the process of outlier detection is as follows.•When the current dataz(k) does not belong to the current cluster it is stored in a buffer. When kminor more data samples are stored in the buffer, they are checked for consistency to form a new cluster initialization. If the initialization is successful then the content of the buffer is cleared. If the buffer contains only outliers or too much outliers then condition (7) for at least kminsamples in the buffer is not fulfilled and the initialization phase is unsuccessful and the buffer data are kept for future iterations. If the buffer is full (i.e., it contains more than 2kminsamples) then the oldest sample is removed. The number of full buffer nbufmust be higher than kminto be able to eliminate outliers. If nbuf=2kminthen correct cluster initialization can be obtained if the buffer contains less than 50% outliers.If less than kminsamples are buffered and the current samplez(k) is successfully clustered to one of the existing prototypes, then those buffered samples are considered as outliers and are removed from the buffer.For general data streams where the data samples are arriving randomly, consecutive data samples do not necessarily belong to the same prototype. In this case the data samples that are not clustered in one of the existing prototypes are stored in a buffer. When the buffer contains more than kmindata it is checked to see whether a new prototype (new cluster initialization) can be identified. When a new cluster is initialized using kminor more data from the buffer the remaining buffer data must remain in the buffer for future iterations. To limit the required memory space the available buffer size nbufis defined and when it is exceed the oldest sample is removed.The proposed evolving clustering algorithm is illustrated in Fig. 1. The pseudo-code of the principal component clustering algorithm in on-line identification is given in Algorithms 1 and 2.For data streams with data samples arriving in a sorted fashion (Algorithm 1) the code is more compact and computationally efficient. These algorithms can be applied in situations where consecutive data samples belong to one prototype only or to the neighboring prototype. Data belonging to one particular prototype are therefore always a sequence. An example of such a data stream is a 2D laser range finder where the reflection points belong to consecutive laser rays that are sent in the environment from a starting angle and they increment to a final angle.In general data streams (Algorithm 2) the data samples are arriving randomly, so consecutive points do not necessarily belong to only one prototype. For sorted data stream Algorithm 2 has the same performance as Algorithm 1, but the computational complexity of Algorithm 2 is greater.An illustrative example of how each step of Algorithm 1 works is presented in Fig. 2.Algorithm 1Pseudo-code of the principal component clustering algorithm in on-line identification for a sorted data stream.1:Definition of the clustering criteria parameter κmax, minimum number of samples kminto add a new prototype and the buffer size nbuf=2kmin.2:Initialization of the first prototype with the first kmindata samples (k=1, …, kmin) consistent with the prototype. Initialization of j=1, m=1, kbuf=0, nj=kmin. Estimation of the jth cluster covarianceΣj, prototype parameterspjand distance variance σj.3:fork=kmin+1:ndo4:Calculate samplez(k) distance to the last prototype j=m bydj(k)=|(z(k)−μj)T·pj|5:ifdj(k)<κmaxσjthen6:Add data samplez(k) to the current cluster j=m and increment jth cluster counter nj=nj+1.7:UpdateΣj,pjand σjof the jth cluster by Eqs. (2), (3) and (8).8:Delete previous data samples in the buffer (outliers) and set kbuf=0.9:else10:Store data samplez(k) in a buffer and increment the buffer counter kbuf=kbuf+1.11:ifkbuf≥kminthen12:ifngood≥kmindata in the buffer are consistentthen13:Add a new prototype (m=m+1, j=m), set the prototype counter nj=ngoodand clear the buffer (kbuf=0).14:EstimateΣj,pjand σj.15:else16:If the buffer is full (kbuf≥nbuf) remove its oldest datum sample.17:end if18:end if19:end if20:end forAlgorithm 2Pseudo-code of the principal component clustering algorithm in on-line identification for a general data stream.1:Definition of clustering criteria parameter κmax, minimum number of samples kminto add a new prototype.2:Initialization of the first prototype: store incoming data samples (k=1, …, nI) until kmindata samples consistent with the prototype are found. Initialization of j=1, m=1, kbuf=nI−kmin, nj=kmin. Estimation of the jth cluster covarianceΣj, prototype parameterspjand distance variance σj.3:fork=nI+1:ndo4:forj=1:mdo5:Calculate samplez(k) distance to the prototype j bydj(k)=|(z(k)−μj)T·pj|6:endfor7:ifminjdj(k)<κmaxσj)then8:Add data samplez(k) to a cluster j and increment jth cluster counter nj=nj+1.9:UpdateΣj,pjand σjfor the jth cluster by Eqs. (2), (3) and (8).10:else11:Store data samplez(k) in the buffer and increment the buffer counter kbuf=kbuf+1.12:ifngood≥kmindata in the buffer are consistentthen13:Add a new prototype (m=m+1, j=m), set the prototype counter nj=ngoodand remove the data from the buffer (kbuf=kbuf−ngood).14:EstimateΣj,pjand σj.15:endif16:endif17:endforA comparison of the proposed clustering algorithm with the very popular split-and-merge clustering algorithm is made. The comparison is made for sorted data streams obtained from a 2D laser range finder.First, the split-and-merge algorithm is explained, then the algorithmic complexity is evaluated, followed by experimental results obtained on a series of 2D scans from a SICK LMS200 [27] laser range finder.Split-and-merge is a very popular algorithm for line extraction [12,6]. Its popularity is due to its simplicity, low computational complexity and good performance. It is an iterative algorithm, applicable to sorted data streams, such as the ones from a laser range finder. The algorithm first assigns all the data samples to one cluster and calculates a linear prototype of the cluster (line for two-dimensional data). The cluster is then iteratively split at the data sample whose distance to the prototype is the largest and higher than the distance threshold dsplit. The choice of the splitting constant dsplitshould consider the expected noise of LRF sensor measurements (in distance and angle). dsplitmust be set higher than expected measurement error due to the noise (e.g. three standard deviations or more).The linear prototype of each cluster j (j=1, …, m) can be expressed in the normal form as(9)[z(k)T,1]θj=0wherez(k) is a datum sample lying on the prototype andθis the vector of the prototype parameters. The prototype parameters are estimated using singular value decomposition. From all the data samplesz(kj) in the cluster j (kj=1, …, nj) the regression matrix is written asψ=z(1)T1⋮⋮z(nj)T1which defines a set of homogenous equationsψθj=0with unknown prototype parametersθj. The solution in the sense of a least-squares minimization is found if the eigenvector (pr) of the matrixψTψwith the minimum eigenvalue is found. This can be calculated using singular value decomposition. The prototype parameters in a normal form are obtained by normalization of the eigenvectorθj=∥pr∥The orthogonal distance of an arbitrary data samplez(k) to the linear prototype j is then obtained by(10)dj(k)=|[z(k)T,1]θj|In the case of two-dimensional data the linear prototype can alternatively be estimated simply by connecting the first and the last data sample in the cluster. This lowers the computational complexity and ensures that the sample that defines the split does not appear at the first or the last data sample in the cluster. The pseudo code of the split-and-merge algorithm is given in Algorithm 3.Algorithm 3Pseudo-code of the split-and-merge algorithm line identification for a sorted data stream.1:Definition of split criteria dsplit.2:Start with a single cluster which contains all the data (k=1, …, n). Initialize current cluster index j=1, number of samples in that cluster nj=n and number of clusters m=1. Mark the cluster as non-final.3:repeat4:forall non-final clusters j=1, …, mdo5:Fit the linear prototype to the data in cluster j.6:forall data kj=1, …, njin cluster jdo7:calculate distance of data samplez(kj) to the cluster prototypeθj.dj(kj)=|[z(kj)T, 1]θj|8:endfor9:Locate the samplezmaxwith the maximum distancedmax=maxkj(dj(kj))10:ifdmax>dsplitthen11:Split the cluster at thezmaxinto two clusters and mark them non-final.12:else13:Mark the cluster as final.14:endif15:endfor16:until all clusters are final17:Merge collinear clusters. This step is optional and is usually not required in ordered data streams.The complexity of the split-and-merge algorithm (Algorithm 3) is O(nlogn) iterations [12] as the algorithm has two nested loops where n is the number of data samples. During each iteration one calculation of the data-sample distance (10) to the cluster prototype is made to find the sample with the worst fit to the prototype that has a complexity of O(2r−1) arithmetic operations (r multiplications and r−1 additions). Additionally, for each cluster at least once the prototype is estimated using the batch least-squares method, so altogether approximately mlogm times, where m is the number of identified clusters. The complexity of the least-squares is O(r2n) [28] due to the singular value decomposition of the regression matrixψ, where r is the dimension of the data sample and n≫r. In total the split-and-merge algorithm needs(11)OSM(nlogn(2r−1)+mlogm·r2n)arithmetic operations.The complexity of the evolving principal component clustering (Algorithm 1) is O(n) algorithm iterations. During each iteration the distance of the current datum sample to the actual prototype is calculated and the cluster variance, mean and distance variance are updated for the actual cluster. An update of the distance (6) requires O(2r−1) arithmetic operations, an update of the variance (2) takes O(5r) (3 multiplications and 2 additions of r dimensional data) arithmetic operations, an update of the mean (1) takes O(3r) (2 summations and 1 division of r dimensional data) and an update of the distance variance (8) requires O(4r) arithmetic operations. In total, the evolving principal component clustering algorithm needs(12)OEPCC(14nr−n)arithmetic operations.From a comparison of (11) and (12) it is evident that the complexity of the proposed algorithm is lower than the split-and-merge algorithm in the case of a large number of data n and decreases with the data dimension r and with the number of identified clusters m. The complexity of the proposed algorithm also does not depend on the number of identified clusters. The relative complexity OEPCC/OSMis shown in Table 1, where it can be seen that the split-and-merge algorithm is computationally more efficient only in the case of low-dimensional data and a small number of identified clusters (e.g., r=2 and m<4).For general data streams Algorithm 2 needs to be employed. Its complexity is greater than the complexity of Algorithm 1, which is only valid for sorted data streams. The complexity of Algorithm 2 is O(nlogn) and requires OEPCCgen(nlogn(14r−1)) arithmetic operations. Note that the split-and-merge algorithm is a batch algorithm valid only for sorted data. This means that on-line identification is not possible. Moreover, its use for general data would require some modifications, which would increase its complexity like in the case of the proposed algorithm.Both clustering algorithms were validated on data obtained from the SICK LMS200 laser range finder, where each scan contains n=180 two-dimensional points.The results of the clustering for both algorithms are shown in Fig. 3. Both algorithms produce clustering results of similar quality if the clustering parameters are properly set. In the split-and-merge (SM) algorithm the threshold distance of the cluster splitting was dsplit=0.06m. The value of this parameter depends on the clustering data noise and scale, and therefore needs to be adjusted for a particular clustering problem. In evolving principal component clustering (EPCC) the choice of initialization parameters was κmax=7 and kmin=3. Those two parameters are independent of the clustering problem. The parameter kminonly defines the minimum number of samples needed to form a new cluster, while κmaxis used to validate whether the current sample distance to the actual cluster prototype is less than the distance variance for all the samples in the cluster (see clustering criteria in line 5 of Algorithm 1). The clustering criteria therefore considers the cluster statistic and adapts automatically; there is no need for manual tuning for different data noise or scaling.In Fig. 3 the EPCC finds 19 clusters and the SM 18 clusters, which is due to the different clustering criteria. This additional cluster (in the EPCC case) is zoomed in Fig. 3. It appears because the distance variance σ of the cluster data is very small and therefore the splitting condition (7) becomes more selective. If dsplitwere to be lowered or κmaxset higher then the same number of clusters could be obtained, but the clusters would not be identical due to the different clustering algorithm. Both algorithms were implemented in the same environment (Matlab) and on the same computer with a similar programming style. For the data in Fig. 3 the EPCC took 0.0098s and the SM took 0.0262s. The computation time for the EPCC is less than half of the computation time needed for the SM, which corresponds to the complexity comparison given in Table 1.Besides the lower computational complexity, the EPCC's advantage over the SM is a simpler implementation on some embedded computers, because only simple mathematical operations are required, as opposed to the SM, where least-squares or singular value decomposition needs to be coded. When splitting clusters in SM it could happen that the sample with the worst fit (with the largest distance from the cluster prototype) appears at the beginning or at the end of the strait line segment where line splitting does not make sense. In this case the cluster prototype can be re-estimated by simply connecting those two edge points of the cluster which guarantee the splitting sample to be somewhere in the middle. So obtained prototype then does not fit the cluster data optimally in the least-squares sense, therefore it should only be used in the mentioned splitting problem.An additional advantage of the EPCC is the adaptive nature of the clustering parameters, which do not need to be fine tuned for each problem separately (different noise and data scaling). The latter advantage is also important when performing clustering on the same data, where data belonging to one cluster can have more noise than the data belonging to the other cluster (e.g., in the case that the laser range finder rays are close to being parallel to the object's borders). The SM is a batch clustering algorithm, so it requires all the data samples to perform clustering, while the EPCC can be used online for data coming sequentially from a process. The former can be an advantage when online clustering is required, or a disadvantage in the case that batch data are available because the clustering algorithm can perform better if it has the whole data set information available.Nevertheless, the quality of clustering is very similar for both approaches.Both algorithms have only one tuning parameter and are very simple to adjust to a particular clustering application. In the SM this parameter is dsplitand in the EPCC it is κmax. The clustering criterion (7) in the EPCC considers data variance, which causes the clustering to adapt to the current data. The former is especially convenient in the case of different data scaling or different noise in the data. The EPCC therefore needs less or no tuning compared to the SM, an example is given in Fig. 4. Data from Fig. 3 are scaled to 10% of the original scale (divided by 10), while the algorithm's parameters remain the same. As seen from Fig. 4, the clustering result remains unchanged in the EPPC, while in the SM the clusters are wrong.The example in Fig. 3 includes real laser-range-finder data with noise. In order to demonstrate the robustness to different noise influences, additional noise with a normal distribution is added to the real data. The obtained clustering results are given in Fig. 5where, the EPCC identifies 16 clusters and the SM 36 clusters. More noise causes the clustering criterion (7) to become less sensitive, so a smaller number of clusters are identified (three less than in Fig. 3). This behavior is desirable. However, in the SM the number of clusters is increased (twice as many as in Fig. 3) because the splitting criterion (line 10 in Algorithm 3) is triggered by the noise and the obtained clusters are not reliable. To obtain more reliable results in the SM, the parameter dsplitmust be increased.The original laser-range-finder data from Fig. 3 are additionally corrupted by salt-and-pepper noise. To the original sorted data stream the uniform random noise sample (outlier) is included so the final data set has 360 samples (180 original samples and 180 outlier samples). The results of the clustering using EPCC are given in Fig. 6, where 16 clusters are identified. On the same data set the basic SM algorithm fails to produce useful clustering results due to the inserted outliers. Therefore, the Hough transform is applied, which can reliably estimate the clusters in the presence of outliers.The basic Hough transform (HT) is implemented where the straight-line parameters α and d are defined by the linear prototype (9) whereθj=[cosα, sinα, d]. The normal line parameters range −π<α≤π and dmin<d≤dmaxare presented in the accumulator by a 720 row and 720 column array. HT therefore requires some a-priori knowledge to properly select quantization of the parameter space and the threshold value to locate maximums in the accumulator. The obtained accumulator is shown in Fig. 7. The HT can locate 8 straight lines that describe 13 data clusters of the data, because the HT cannot separate collinear line segments, as seen from Fig. 7. Other straight lines appear randomly on collinear outlier samples. The HT is a robust algorithm for sorted or general batch data, or also for data streams if online implementations are used. In contrast, the EPCC algorithm is a well-suited algorithm for sorted stream or batch data and can therefore reliably eliminate outlier data that appear during a sequence of good data samples belonging to one cluster. Also, the distance of the consequent data samples can easily be considered in the clustering criteria. If this distance is too long, then the current sample does not belong to the current prototype. Additional advantages of the EPCC are its simplicity of implementation, low parameter-tuning effort and fast operation. The EPCC is much faster than the classic HT implementation (in the example in Fig. 6 the EPCC took 0.03s and the HT took 1.1s). In the HT the result of clustering is dependent on the proper selection of the parameter-space quantization and on the parameter settings for the maximums search in the accumulator.A detailed comparison of commonly used algorithms for straight-lines extraction from laser-range-finder data is given in [12]. To evaluate the proposed evolving principal component clustering (EPCC) algorithm a similar comparison is made by considering the split-and-merge algorithm (SM) and the Hough transform (HT).A laboratory room consisting of 60 scans obtained from different locations using a mobile robot with a SICK LMS200 laser range finder is shown in Fig. 8. Each scan contains 180 points, so in total Fig. 8 consists of 10,800 reflection points. Each scan is evaluated separately by the number of estimated line clusters relative to the number of true clusters. A true line contains at least 4 sample points, where the distance between consecutive points is less than 0.2m. A total of 561 straight lines can be found on all 60 scans or approximately 9 per scan. In Fig. 8 a cumulative scan is shown where local scans are merged together using the SLAM algorithm, which is also the reason for some additional noise in the observed groups of points that belong to a straight line (error in the robot-pose estimate). In separate scans the error due to the SLAM is not present; however, there are many outliers or smaller groups of points belonging to curved objects, chair legs, humans and the like.In Table 2the obtained clustering results are given by average processing time (Matlab implementation on 2.6-GHz personal computer), by the percentage of correctly estimated line segments (according to the number of true straight lines in the scan) and by the percentage of wrong estimates (estimated lines without a match in the true line-set relative to the number of estimated straight lines).The clustering parameters were chosen as follows: κmax=7, kmin=3 for EPCC, dsplit=0.06m for SM and quantization of the normal line parameters and the threshold (for locating maximums in the accumulator) in the Hough transform were 0.5°, 1cm and 5, respectively. In all the algorithms only the estimated lines containing at least 4 points are considered. From the comparison it can be concluded that for sorted data streams the EPCC's performance is at least similar to or better than, the performance of the SM or HT, while the computational time of the EPCC is much shorter. The classic HT algorithm performs the worst; however, its performance can be improved by additionally considering the information from successive data, which is available in sorted (batch or stream) data where a new cluster can be formed only from the successive samples if the distance between the neighboring data is sufficiently small.The clusters in Figs. 3–8 are crisp with no or very little overlapping because the LRF sensor can only measure visible reflection points (i.e., it cannot measure reflection points behind the wall corner). The proposed algorithm is applicable also to the overlapped clusters data (the linear prototypes are not collinear and cannot be described by a single linear model). If the data with overlapped clusters are not sorted data stream then the Algorithm 2 must be used.The proposed algorithm can also be applied to 3D LRF where r=3 and the linear prototype is plane. If the obtained 3D LRF data stream is sorted then Algorithm 1 can be applied otherwise if due to scanning pattern data stream is not sorted then the Algorithm 2 need to be used.

@&#CONCLUSIONS@&#
