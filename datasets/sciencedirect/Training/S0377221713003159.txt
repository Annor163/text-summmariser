@&#MAIN-TITLE@&#
Dynamic sequencing and cut consolidation for the parallel hybrid-cut nested L-shaped method

@&#HIGHLIGHTS@&#
We present acceleration techniques for the nested L-shaped method.Cut consolidation reduces average solution time by 38% on 48 problems.Dynamic sequencing reduces average solution time by more than 12% on 42 problems.We present a parallelized, COIN-OR based implementation of the algorithm.

@&#KEYPHRASES@&#
Stochastic programming,Nested L-shaped method,Sequencing protocols,Cut consolidation,

@&#ABSTRACT@&#
The nested L-shaped method is used to solve two- and multi-stage linear stochastic programs with recourse, which can have integer variables on the first stage. In this paper we present and evaluate a cut consolidation technique and a dynamic sequencing protocol to accelerate the solution process. Furthermore, we present a parallelized implementation of the algorithm, which is developed within the COIN-OR framework. We show on a test set of 51 two-stage and 42 multi-stage problems, that both of the developed techniques lead to significant speed ups in computation time.

@&#INTRODUCTION@&#
Many real world applications can be modeled as a multi-stage stochastic program with recourse, e.g. in applications from supply chain planning, electricity and finance (cf. Wallace and Ziemba, 2005). An algorithm to solve two-stage stochastic linear programs with discrete and finite distributions is the L-shaped method developed by Van Slyke and Wets (1969) which is an adaption of Benders decomposition (cf. Benders, 1962) to two-stage recourse problems. It can be used in a nested application to solve multi-stage stochastic programs with recourse (cf. Birge, 1985). The algorithmic improvement of the L-shaped method is highly relevant and ongoing research. Recent achievements include a variant of the algorithm with aggregated cuts introduced by Trukhanov et al. (2010) and the generation of tighter feasibility cuts (see Aranburu et al., 2012). Zverovich et al. (2012) compare alternative variantes of the L-shaped method, namely, a regularized version based on work of Ruszczyński (1986) and the level decomposition method developed by Fábián and Szőke (2006). They find that regularized versions outperform non-reguralized versions on many of their test models.In this paper we propose further algorithmic techniques that can improve the performance of the parallel nested L-shaped method and its variants. The nested L-shaped algorithm can decide at every stage other then the first or the last in which direction it should push information. Information is pushed up the tree by feasibility or optimality cuts to the ancestor problems. The solution to the current problem can be passed down the tree to form new right hand sides for the successor problems. The algorithm decides where to push the information according to a tree-traversing strategy, a so called sequencing protocol. Several studies showed that the sequencing protocol itself has an impact on the solution time (cf. Gassmann, 1990; Morton, 1996; Altenstedt, 2003). We propose a new dynamic sequencing protocol that leads to faster solution times compared with the well known sequencing protocols FastForwardFastBack and ∊-FastBack.Depending on the level of cut aggregation, a certain number of cuts is added to the subproblems at every iteration of the algorithm. Especially for the multi-cut case (see Birge and Louveaux, 1988) this can become prohibitive in both memory usage and solution time. An approach to reduce the computational burden is the removal of previously added cuts. Ruszczyński and Shapiro (2003) showed, that there is no easy way to keep the number of cuts bounded. However, they also point out, that it is possible to delete inactive cuts, when the objective value of the master problem strictly increases. We suggest a cut removal strategy that not only removes inactive cuts, but retains an aggregated cut so that not all information contained in the removed cuts is lost. Our results show that the removal of old and inactive cuts can lead to shorter solution times on many problems. Furthermore, we investigate the extension of the cut aggregation technique recently employed by Trukhanov et al. (2010) for the two-stage case to the multi-stage case. We evaluate the mentioned techniques using our own parallel implementation of the nested L-shaped method that is partly based upon and embedded into the COIN-OR project (see Lougee-Heimer, 2003).The remainder of the paper is organized as follows. Section 2 describes a basic variant of the nested L-shaped method. Our new sequencing protocol and cut consolidation techniques are presented in Section 3. The parallel implementation is explained in Section 4. In Section 5 we evaluate the presented techniques and implementation based on a computational study. We conclude with a brief summary of our main results and an outlook on future research opportunities in Section 6.This paper is focussed on two- and multi-stage stochastic programs with recourse and discrete and finite distributions. For a general introduction to stochastic programming see Birge and Louveaux (2011) and Shapiro et al. (2009) and for an overview of applications and implemented solution algorithms see Wallace and Ziemba (2005) and Kall and Mayer (2010), respectively. In order to fix our notation and as an introduction for the non-expert we start with describing a basic version of the nested Benders decomposition algorithm.Every multi-stage stochastic program with recourse with discrete and finite distributions is represented by a scenario tree with a certain number of stages T. A tree node is denoted by its stage t,t∈{0, …, T−1} and an index i,i∈{0, …, Kt−1} with Ktdenoting the number of nodes at stage t. The parent node is denoted with a(i, t) whereas the set of children of a node are denoted with d(i, t). Every node (t, i) has an overall probabilitypti. The probabilities must be chosen such that∑i=0Kt-1pti=1holds for every stage t.We can formulate the multi-stage stochastic program with recourse with discrete and finite distributions as follows:(1)minctixti+Qtixti:Wtixti=hti-Ttixt-1a(i,t)=minctixti+θti:Wtixti=hti-Ttixt-1a(i,t),θti⩾Qti(xti)lti⩽xti⩽uti,where(2)Qti(xti)≔min∑j∈d(i,t)pt+1jptict+1jxt+1j+pt+1jptiQt+1j(xt+1j)s.t.Wt+1jxt+1j=ht+1j-Tt+1jxti,j∈d(i,t)lt+1j⩽xt+1j⩽ut+1j,j∈d(i,t)andQT-1i(·)=0,∀i∈{0,…,KT-1-1}, for t=0 and i=0 (for t=0, we dropT0ix-1a(i,0)from the constraints as there is no prior solution to the first stage). This is a recursive, node-based formulation, where the subscript t denotes the stage and the superscript i denotes the ith node of the scenario tree at stage t.xti∈Rntis the vector of decision variables,cti∈Rntis the cost vector andhti∈Rmtis the right-hand side vector, all for nodes i∈{0, …, Kt−1} at stage t. The technology matricesTti∈Rmt,nt-1of stage t belong to the decision variables of stage t−1, whereas the recourse matricesWti∈Rmt,ntbelong to the decision variables of the same stage.The nested Benders decomposition algorithm (see Dempster and Thompson, 1998; Birge and Louveaux, 2011; Gassmann, 1990; Ruszczyński and Shapiro, 2003 for further descriptions) constructs an outer linear approximationθtiof the recourse functionQti(·)in consecutive iterations via cutting planes at every node of the scenario tree. To obtain a LP formulation of this approach the restrictionθti⩾Qti(xti)is removed from the problem (1) and the approximation termθtiis added instead ofQti(·)to the objective function. Subproblems at stage t do no longer rely explicitly on all later stage problems. The cutting planes that bound the approximation variables are optimality cuts. Feasibility cuts restrict the solution set to solutions that are feasible for all subproblems.The subproblem formulation (1) is hence transformed to the following approximation subproblem:(3)minctixti+θti:Wtixti=hti-Ttixt-1a(i,t)(4)Dt,rixti⩾dt,ri,r∈{1,…,it}⧹Fti(it)(5)Et,sixti+θti⩾et,si,s∈Fti(it)(6)lti⩽xti⩽uti,where (5) are the optimality and (4) are the feasibility cuts.Fti(it)contains all iterations where optimality cuts were added to the current problem, up to the current iteration it.The single linear approximationθtiof the recourse function can be split into up toAtiapproximation termsθt,ki, so called aggregates, that form a partition of the descendant node set d(i, t):⋃k=1AtiSt,ki=d(i,t),St,ki∩St,li=∅,∀l≠kEach partitionSt,kicontains distinct nodes from the descendant node set. The multi-cut method (cf. Birge and Louveaux, 1988) is a special case, where the number of partitions is equal to the number of descendant nodes. In this case each partition consists of exactly one descendant node. If only one partition is used, we have the single-cut method. This notation can thus be used to refer to the hybrid method introduced by Trukhanov et al. (2010) as well as the single and multi-cut methods which are just two special cases of the hybrid method.The dual formulation of problem (3) is needed to generate the optimality and feasibility cuts. Its formulation is(7)maxhti-Ttixt-1a(i,t)πti+dtiρti+etiσti+ltiλti+utiμti(8)WtiTπti+DtiTρti+EtiTσti+λti+μti=cti(9)(1)Tσti=1(10)ρti⩾0,σti⩾0,λti⩾0,μti⩽0,whereDtiis the matrix that consists of the column vectorsDt,riandEtiis the matrix that consists of the column vectorEt,sifor all appropriate r and s. In case a problem has no feasibility or optimality cuts, no dual variablesρtiorσtiare present in the dual problem. Constraint (9) is the corresponding dual constraint of theθtivariable, where 1 denotes a vector of appropriately many ones.If a subproblem is solved to optimality, it has a primal and dual optimal solution. With dual feasible solutionsπt+1j,ρt+1j,σt+1j,λt+1j,μt+1jto all the descendants problems of the current node, j∈d(i, t), we can compute an optimality cutEt,k,sixti+θt,ki⩾et,k,itiwhere(11)Et,k,si=∑j∈St,kipt+1jptiπt+1jTTt+1j(12)et,k,si=∑j∈St,kipt+1jptiπt+1jTht+1j+ρt+1jTdt+1j+σt+1jTet+1j+λt+1jTlt+1j+μt+1jTut+1j,anddt+1jdenotes the vector of all feasibility cut right-hand-side values andet+1jthe vector of all optimality cut right-hand-side values, respectively, for problem j at stage t+1. The dual valuesπt+1jcorrespond to the original rows of the problem,ρt+1jto the feasibility cuts andσt+1jto the optimality cuts.λt+1jcorresponds to the lower bounds of the variables andμt+1jto the upper bounds of the variables. If a subproblem is infeasible, and the dual is unbounded, a dual rayπt+1j,ρt+1j,σt+1j,λt+1j,μt+1jexists for the problem j∈d(i, t). Using this ray we can then compute a feasibility cutDt,rixti⩾dt,ri, where(13)Dt,ri=πt+1jTTt+1jdt,ri=πt+1jTht+1j+ρt+1jTdt+1j+λt+1jTlt+1j+μt+1jTut+1j,andDt+1janddt+1jdefined as above. If the primal subproblem is unbounded, the overall problem is unbounded.With these definitions we can now formulate the subproblem with cut aggregation P(i, t) for node i at stage t. It can be expressed as(14)minctixti+∑k=0Atiθt,ki:Wtixti=hti-Ttixt-1a(i,t)(15)Dt,rixti⩾dt,ri,r∈{1,…,it}⧹Fti(it)(16)Et,k,sixti+θt,ki⩾et,k,si,s∈Fti(it),k=1,…,Ati(17)lti⩽xti⩽uti.If more than one descendant problem is infeasible at a given iteration, it is in principle possible to generate more than one feasibility cut, e.g. one cut for every infeasible node. We formally state the general nested L-shaped method as follow:1.Set t=0, i=0, it=0, lb=−∞, ub=∞, dir=forward. Initialize all θ variables with a coefficient of 0 in the corresponding objective function.If ∣ub−lb∣<∊gapor ∣ub−lb∣/ (∣lb∣+1e−10)<∊gap, stop.Solve problem P(i, t).•If infeasible and t=0 stop, problem is infeasible.If infeasible and t>0 store the dual rayπti,ρti,σti,λti,μtiand compute a feasibility cut (15) for problem P(a(i, t), t−1). Set dir=backward and go to step 7.If feasible and t=0 set lb to the objective value of P(i, t).If feasible and t<T−1, store the dual valuesπti,ρti,σti,λti,μtiand the primal valuesxti. If i<Kt−1, set i=i+1 and go to step 3. If i=Kt−1 set i=0.Call sequencing protocol to decide the direction.For all nodes j∈d(i, t) and for all partitionsk=0,…,At,itj•Compute optimality cut coefficientsEt-1,k,itj(11) and right hand side valueset-1,k,itj(12) to form an optimality cut (16) for aggregate k and problem P(t−1, j).Test if generated cut should be added to the problem. If this is the first optimality cut for this aggregate, set the corresponding objective coefficient to 1.If i<Kt−1, set i=i+1 and go to step 5, else set i=0.If t=T−1, compute temporary upper bound tempubby summing uppt′i′ct′i′xt′i′fort′=0,…,T-1,i′=0,…,Kt′. If tempub<ub, set ub=tempub.If dir=forward, set t=t+1, else set t=t−1. Go to step 2.After all problems of a certain stage have been solved, a decision has to be made whether to move back up the tree and thereby give information in the form of optimality or feasibility cuts to the previous stage or to proceed to the next stage with the new solution from the current stage as input which modifies the right-hand-side of the problems at that stage. Sequencing protocols formulate rules for how this decision is made. Sequencing protocols are only needed for multi-stage problems, as there is no choice in which direction to go for the two-stage case.At the first stage of a multi-stage problem it is only possible to move to the next stage and pass the current solution down the tree. At the last stage it is only possible to solve all the subproblems and generate optimality and/or feasibility cuts for the previous stage. When a subproblem was found to be infeasible at a stage, the algorithm moves back towards the first stage, i.e. the direction is backward. Three common strategies were developed by Gassmann (1990). The first strategy is the Fast-Forward–Fast-Back (FFFB) or Fastpass strategy that goes down the whole tree and back up to the root from there. This is called a full sweep, consisting of a full forward and backward sweep. A forward sweep solves all subproblems from stage 0 to stage T−1. A backward sweep consists of adding cuts to all subproblems at stages T−2 to 0, by solving problems from stage T−2 up to stage 1. This is repeated until the algorithm finishes, i.e. the gap between upper and lower bound is small enough.The Fast-Forward (FF) strategy tries to move forward or down the tree whenever possible. It only goes up the tree when the current stage is solved to optimality with respect to the current primal information, i.e. the gap between the lower and upper bound at the current stage is less than a small tolerance ∊gap. The Fast-Back (FB) strategy does the opposite, it tries to move back up the tree whenever possible. It only moves down a further stage, if no new optimality cuts can be generated at the current stage or the gap is below an ∊gap. It requires an initialization period because it needs an initial approximation of the recourse function at every stage.An evaluation conducted by Gassmann (1990) showed that out of the three strategies, the FFFB strategy is the best. Morton (1996) comes to the same conclusion, introducing an ∊-Fast-Back strategy that reached comparable performance. He used the notions of absolute error and discrepancy for a node, which we will define for a stage by summing up the probability weighted node-wise values. In the ∊-Fast-Forward strategy, the algorithm goes back up the tree, when the absolute error is smaller than ∊·min (∣LB∣, ∣UB∣). The ∊-Fast-Back strategy goes further down the tree, when the discrepancy is smaller than ∊·min (∣LB∣, ∣UB∣) instead of a fixed ∊gap. The discrepancy for stage t is defined as(18)Disc(t)=∑i=0Kt-1pti·ctixti+∑l=1Atiθt,li-∑j=0Kt-1-1∑l=1At-1jpt-1jθt-1,lj,which is the difference between the probability weighted sum of the objective function values for stage t and the probability weighted sum of the approximations of the recourse functions at stage t+1. The absolute error for stage t is defined as(19)AbsErr(t)=∑j=t+1T-1∑i=0Kjpjicjixji-∑i=0Kt-1∑l=1Atiptiθt,li,which is the difference between the probability weighted sum of objective functions of all stages after stage t and the probability weighted sum of the approximations of the recourse functions at stage t. To be able to compute the absolute error for stage t, it has to be solved for all stages t′>t. Hence, it is only possible to compute the absolute error after a full forward sweep or during a backward sweep, whereas the discrepancy can also be computed during a forward sweep.Another simple strategy, the bouncing strategy introduced by Altenstedt (2003), is to solve the problem up to stage t, for t<T−1, return to the first stage and then do a full iteration. The stage t is also called the bouncing stage, as the algorithm changes direction at that stage. The motivation for this protocol is the observation that the algorithm spends most of the time at later stages, in particular the last one. This is mostly due to the large number of scenarios at the last stage and, then, the large number of subproblems to be solved on this stage. The idea of the bouncing strategy is to perform partial iterations to achieve better solutions to the later stage subproblems and thereby reduce the overall number of major iterations. However, it is not clear which stage is the best bouncing stage a priori.Our dynamic strategy uses a bouncing stage too, but in a different manner. We declare a stage critical to enforce a full sweep after the algorithm reached this stage. This is done to prevent a cycle to the first stage and back that does not improve the solution much, but costs computation time. The strategy is dynamic because it declares the critical stage after the first full sweep and because the threshold that is used to decide the direction is adapted to the current gap of the algorithm. This is a major advantage over existing strategies which have to be adjusted to specific model instances. The proposed strategy can be summarized as follows:1.Do a full sweep. Repeat until no new feasibility cuts have been generated.Determine a critical stage ct.Solve stage 0 problem, set t=0.Set t=t+1, go to stage t and solve problems at stage t.If a problem is infeasible, do a backward sweep and go to step 3.If Disc(t) is lower than current ForwardThreshold, go to step 7. Otherwise, perform a backward sweep and go to step 3.If t=ct, do a full sweep and go to step 3. Else go to step 4.The critical stage is determined by the first sweep of the algorithm in which no problem was found infeasible. The wall clock time the algorithm stays in each stage is measured. We calculate the time of all stages and the cumulated one for every stage. If the cumulated time for stage k divided by the overall time is greater than a predefined value, e.g. 0.1, the stage k is declared the critical stage. This critical stage heuristic is used to prevent spending time generating cuts for the first stages without getting new dual information from the last stage.We do not use an absolute value as ForwardThreshold, but a relative value compared to the absolute value of the current gap between lower and upper bound. Thus the threshold adjusts along with the absolute gap. We propose to setForwardThreshold=(UB-LB)/10. In contrast to this setting the ∊-FastBack strategy uses the minimum of ∣LB∣ and ∣UB∣ times ∊ as a threshold. This works fine, until either the lower or upper bound has a value of zero. If this is the case, Disc(t) is usually greater than the threshold, namely zero. The algorithm concludes that it should generate new optimality cuts in a backward sweep to improve the discrepancy. But the threshold remains at zero, so the algorithm does not terminate. We observed this behavior for some of our test set problems, see below.In each iteration of the algorithm where all subproblems are feasible new optimality cuts are added to the corresponding master problem. In the multi-stage case, this leads to a growing number of cuts in the respective master problems. In the regularized decomposition method (cf. Ruszczyński, 1986) the algorithm keeps only a limited number of cuts, instead of adding new and keeping all old cuts in the problem. It would be preferable with respect to computational efficiency and memory requirements to keep only those cuts in the problem, that are needed to solve the overall problem to optimality. Unfortunately, as the nested Benders algorithm proceeds in generating new cutting planes to approximate the recourse function, there is no reliable rule to determine which cut can be safely removed (cf. Ruszczyński and Świe¸tanowski, 1997). The simple deletion of old inactive cuts can therefore lead to the recomputation of those cuts in later iterations (cf. Ruszczyński and Shapiro, 2003). Trukhanov et al. (2010) note that the removal of cuts only lead to a reduced memory usage, but had no other effect, e.g., on computing time. We propose a method that reduces the number of cuts significantly, but keeps most of the information that was contained in these cuts. In the case of pure multi-cut, we added the following cuts in some previous iteration s to a node i at stage t,Et,k,sixti+θt,ki⩾et,k,si,k=1,…,Ati.If all these cuts that were generated at the same iteration become redundant, i.e. their corresponding dual values are zero, we can generate a new single cut out of these multiple cuts by just summing up the existing cuts(20)∑k=1AtiEt,k,sixti+∑k=1Atiθt,ki⩾∑k=1Atiet,k,s.The only difference compared to a common single cut is the sum of aggregate variables∑k=1Atiθt,kiinstead of a single aggregate variableθti. We then replace all the cuts in iteration s with the newly generated single cut. Thereby the number of redundant cuts that needs to be stored in the master problem is reduced from as many asAtito one for iteration s. We call this technique cut consolidation. The trade-off between information loss due to aggregation and memory and computing time gains due to smaller problems is evaluated in Section 5.The decision when to aggregate cuts of one iteration into a single cut is called a cut consolidation scheme. Our scheme is controlled via two threshold values that guide the consolidation. The first threshold value ConsecInactive specifies the number of iterations a cut needs to be consecutively inactive, before it is marked as removable. The second threshold value RelativeActive specifies how many cuts generated at the same iteration must be marked as removable before any cuts are consolidated. In our implementation, we set ConsecInactive=5 and RelativeActive=0.95.If the scheme is too aggressive towards reducing the number of cuts, this can lead to longer overall solution times as cuts are consolidated that the algorithm needs to bound some aggregate and thus recomputes later on. The following pseudo-code summarizes our cut consolidation scheme, which is performed at each iteration it for every non-leaf node i at stage t in the tree. The variables ick,s,i,tcount for how many consecutive iterations a cut k generated in iteration s is inactive (step 2(a)). These variables have to be initialized with zero before calling the scheme for the first time. If a counter ick,s,i,texceeds the threshold ConsecInactive the corresponding cut is marked as removable and the counter num_inacs,i,t, which counts the number of removable cuts which were generated in a certain iteration s, is increased (step 2(b)). If the number of removable cuts exceeds the relative tresholdRelativeActive·Atiall the removable cuts generated in iteration s are consolidated (step 3).1.Set num_inacs,i,t=0 for alls∈Fti(it).For all non-aggregated optimality cuts k and all (previous) iterationss∈Fti(it):(a)If the current dual value of cut k is zero, set ick,s,i,t=ick,s,i,t+1. Otherwise, set ick,s,i,t=0.If ick,s,i,t>ConsecInactive set num_inacs,i,t=num_inacs,i,t+1.For all iterationss∈Fti(it): Ifnum_inacs,i,t>RelativeActive·Ati, aggregate all inactive cuts to a single cut (20). Remove old cuts from problem, add new cut and resolve the problem. Store warm start.Trukhanov et al. (2010) show that for their sample of problems it is preferable to choose an aggregation between pure single- and multi-cut. They note, that “a good level of cut aggregation is not known a priori”. For this reason they devise the adaptive multi aggregation algorithm that decreases the number of aggregates during the computing time of the algorithm. However, in our view the aggregation scheme “Redundancy Threshold” as described in their paper is not applicable to the L-shaped method.The scheme “Redundancy Threshold” explicitly requires that all optimality cuts corresponding to an aggregate d are redundant, i.e., their dual values are zero. This is emphasized by their suggestion that implementations can be considered where only a fraction of the optimality cuts are required to be redundant (Trukhanov et al., 2010).The aggregate variables θdare free variables that are only bounded by optimality cuts. To obtain a finite optimal solution for the problem, at least one optimality cut has to be binding, i.e. it has zero slack, for the aggregate variable θdto be bounded below. According to the complementary slackness theorem, it is possible that a binding constraint has a corresponding dual variable with value zero, but it is not necessary. In fact the dual constraint (9) for an aggregate variable requires that the sum of all the dual variables that correspond to the optimality cuts for that aggregate has to be one, so at least one dual variable has a value different than zero. Therefore it is not possible for all optimality cuts to be redundant at the same time.Furthermore, the scheme still requires an a priori setting of the parameter agg−max that specifies the maximal number of scenarios that can get aggregated into a single aggregate. It follows that this value is a bound on the minimal number of aggregates the algorithm should use. For a problem with 1000 scenarios, for instance, a value for agg−max of 10 is equivalent to at least 100 aggregates, as maximal 10 scenarios can be aggregated into one aggregate. This value is determined a priori which does not differ much from the difficulty to find a good value for the number of aggregates in the static method. Trukhanov et al. (2010) conclude that a good value for agg−max lies between 10 and 100. This can be translated to a number of aggregates between 100 and 10 for a problem with 1000 scenarios, for instance. The adaptive algorithm can then adapt dynamically from its starting number of aggregates to the predetermined number of aggregates, but with the advantage to choose the partition of the scenarios dynamically instead of setting it statically a priori. The results from Trukhanov et al. (2010) show that the adaptive approach and the static approach reaches comparable results for equivalent choices of agg−max and a static number of aggregates.Due to these considerations we did not pursue further strategies for an adaptive algorithm that changes the number of aggregates throughout the algorithm, but we focused on the deletion of old cuts to improve computing times.

@&#CONCLUSIONS@&#
