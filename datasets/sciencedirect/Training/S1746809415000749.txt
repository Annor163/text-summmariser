@&#MAIN-TITLE@&#
AVIM—A contactless system for infant data acquisition and analysis: Software architecture and first results

@&#HIGHLIGHTS@&#
We design a new architecture of a system conceived for supporting clinical diagnosis in newborns.Analysis of acoustical and kinematical parameters for neurodevelopmental assessment with contactless techniques.Unique and flexible system to support clinician from the acquisition of the signals to the results.Appreciable decrease in investigation time, costs and errors for the diagnosis.

@&#KEYPHRASES@&#


@&#ABSTRACT@&#
Traditional techniques for the diagnosis of neurological disorders are recently complemented by contact-less methods that provide a semi-quantitative assessment of the patient status. In this framework, the assessment of infant's behaviour based on the analysis of audio and video recordings is appealing thanks to its unobtrusiveness and to the affordable costs of the equipment.This paper presents the architecture of a system, named AVIM, conceived for supporting clinical diagnosis in newborns with contact-less techniques. Its most innovative aspect is the ability of merging in a single tool the management of medical records and reports, audio/video data acquisition, handling and analysis, editing and filling out customized tests. Moreover, unlike other commercial or open source software tools, AVIM allows adding markers and notes while recording audio and video signals and provides detailed reports with both perceptual scores and acoustical and kinematical parameters of clinical interest computed through dedicated innovative techniques. AVIM is therefore a unique and flexible system that could successfully support the clinician during the entire process from the acquisition of the signals to the results. In addition to providing an appreciable decrease in investigation time, costs and errors, AVIM could support the diagnosis integrating clinicians’ qualitative analysis, based on subjective skills, with objective measurements. To highlight its capabilities, AVIM is applied here to the management and analysis of personal and clinical data of newborns audio/video recorded in 5 time points from 10 days to the 24th week of age, according to a specific protocol. Patient data, results of customized tests, tables and plots are provided in a user-friendly environment.

@&#INTRODUCTION@&#
Traditional techniques for the diagnosis of neurological disorders are recently complemented by contact-less methods. These techniques are mainly based on the assessment of parameters obtained both from automatic and perceptual analysis of audio and/or video recordings resulting in a semi-quantitative evaluation of the patient's status [1–5]. Contact-less techniques provide advantages in terms of comfort and safety of the patient with respect to sensor-based/invasive methods, thus they are particularly well suited for vulnerable patients such as neonates and young infants. This paper focuses on this category of subjects.The development of reliable software tools to enhance early diagnosis, especially in home environments, is highly desirable particularly for the neurobehavioural assessment of the newborn. Such tools should provide objective measures to complement clinicians’ qualitative analysis that is based on subjective skills. Acoustical analysis of infant's cry and automatic motor methods may provide objective parameters indicative of neurological pathologies.To this aim, a new system is presented, named AVIM (audio–video infant monitoring), conceived for managing data of a large set of patients. Its most innovative aspect is the ability of merging in a single tool the management of medical records and reports, audio/video data acquisition, handling and analysis, editing and filling out customized tests. Moreover, AVIM supports the clinical perceptual evaluation of cry and spontaneous movements of newborns and infants, which are markerless techniques of relevant clinical interest in the early assessment of neurological disorders. In fact, recently, alterations in infant crying and general movements have been identified in several neurodevelopment disorders [6–16] suggesting their role as early markers of infant disorders.Specifically, crying is the first and primary method of communication among humans. It involves activation of the central nervous system and requires a coordinated effort of several brain regions, mainly brainstem and limbic system. Thus an accurate acoustic analysis of newborn's cry could be helpful to identify risk markers of neurodevelopmental disorders.The most significant acoustical parameters of infant crying are the fundamental frequency (F0) and the first two resonance frequencies of the vocal tract (F1 and F2). F0 reflects the regularity of the vibration of the vocal folds while resonance frequencies are related to the varying shape of the vocal tract during phonation and thus to its control. Both are extremely relevant also in the study of language development in the infant [17]. Few papers describe the F0 developmental pattern in the first months of life. Gilbert et al. [18] analysed the variation of F0 (mean, median and standard deviation) in hunger cries of 4 male infants during the first 12 months of life and found an increase of F0 median in the first 5 months and a decrease at the 12th month. Lind and Wermke [19] reported no significant difference in F0 mean in spontaneous cries of male healthy infants in the first three months of life. Rothganger [20] found that the mean fundamental frequency of crying varied considerably from 441.8 to 502.9Hz in 25 infants recorded at the 3rd–5th day of the 1st, 3rd, 6th, 9th and 12th month. Baeck and De Souza [21] found a 380–435Hz range of F0 when analysing 30 male and female healthy infants recorded at birth and every two weeks until the 6th month of life.This not exhaustive list shows the noticeable scientific interest for the analysis of infant cry. Unfortunately, the variety of cries (feeding, anger, and pain), the lack of dedicated software for infant crying recording and analysis and the difficulties of recruiting a large number of subjects did not allow defining normative values for F0. One reason is that this study requires standardized and appropriate recording settings and a preliminary perceptual analysis of the waveform to extract relevant sequences, the so-called crying episodes or cry units (CUs). However, the workload of clinicians is often incompatible with this analysis as it requires the manual selection of a large number of cry units and the removal of moans, breath, background noise, from recordings lasting up to several minutes. Moreover, the acoustical analysis is commonly performed through commercial or open source software tools like MDVP [22] or Praat [23,24] which however were developed for the analysis of adult's voice. Thus their proper use with infant high-pitched quasi-stationary cry signals requires manual setting of some parameters and thus some technical skill.To the authors’ knowledge only two software tools exist specifically designed for the automatic analysis of infant crying. The first one is BioVoice [4,5,17,25], based on an innovative and robust parametric approach for F0 and formants estimation [10], successfully tested against other software tools [12,26] and a software tool recently proposed by Reggiannini et al. [27] that estimates F0 by means of a cepstrum approach. However, both methods perform only the audio analysis and are not an integrated audio/video system such as the one proposed in the present paper.Besides cry, great interest is paid to newborn spontaneous (i.e. not induced or stimulated) movements, known as general movements (GMs). Regular GMs are gross movements, involving the whole body with a variable sequence of arm, leg, neck and trunk movements rising and waning in intensity, force and speed. They begin and end gradually and may last from a few seconds to several minutes. The majority of sequences of extension and flexion of arms and legs is complex, with superimposed rotations and often slight changes in direction. These added components make the movements fluent and elegant and give the impression of complexity and variability [28–31].At the end of pregnancy and during the first two months after childbirth, GMs are known as writhing movements and are characterized by an elliptic shape of small to moderate amplitude and by slow to moderate speed [32]. At the age of 6 to 8 weeks of life infants’ movements undergo a change in form and a new pattern, called fidgety, emerges [31]. Fidgety movements, commonly observed until 20 weeks of age, are elegant circular movements of small amplitude and moderate speed and variable acceleration of neck, trunk and limbs in all direction. They are persistent when the infant is awake, except during focused attention. Recently, some studies proved that spontaneous motility in infants affected by different brain lesions/pathologies loses elegance, fluency and complexity [30,31].In the last decades Prechtl's method [30] was proven to be a highly accurate and specific marker-less diagnostic tool for the qualitative assessment of GMs, preventing discomfort for the baby without interfering with other measures. To date GMs are commonly assessed by expert clinicians (paediatrician/neurologist/child psychiatrist), scoring their variety and complexity according to strict protocols but solely through a perceptive examination.Beside perceptual evaluation, marker-based techniques have been applied to the analysis of GMs [33,34]. In addition to quite expensive equipment, (building three-dimensional models requires at least two high-speed cameras), marker-based techniques require the application of markers in well-defined anatomical landmarks (often the joints) that, although of reduced invasiveness in adults, can make the newborn movements less spontaneous or even hinder them. Despite these drawbacks, marker-based techniques allow to obtain very reliable models thanks to accurate data processing. Marker-based analysis for the study of the infants’ movements was performed by Coluccini and colleagues [33] with the aim of assessing movements at different stages of development (7, 10, 12 weeks). Other applications concerned the evaluation of signs of spasticity in newborns at risk [35] or the analysis of spontaneous movements in control cases [36].On the other hand, marker-less techniques can be applied on movies recorded with any good quality camera under an appropriate and standardized protocol. Adde et al. [37] assessed fidgety movements identifying the centroid of motion of the subject by a video processing software tool for the diagnosis of cerebral palsy. The centroid of motion is plotted on a diagram called motiongram (motion diagram). In a sample of 140 subjects, the method has shown a sensitivity of 81.5% and a specificity of 70.0%. A similar approach is proposed in the present paper.Though the perception of the clinical specialist is undoubtedly the most accurate method to assess infants’ crying and movements, the procedure of finding and analysing crying episodes and video frames of interest out of the whole recording is operator-dependent. Moreover, the huge amount of recorded data makes a detailed analysis often prohibitive in daily clinical practice being highly time consuming even for trained and qualified clinicians. Finally, clinicians make use of several acquisition devices and software tools not specifically designed for clinical use: that is, they have to use different hardware and software tools and even resort to pen and paper to manage patient data, record and process audio and video signals to obtain parameters of interest, collect and save results, fill tables with diagnostic indexes, scores, etc. Thus, the lack of a unique software tool that aids the clinician in the many different steps from data acquisition to clinical results makes this approach not yet widespread.To overcome such limitations we developed the AVIM system with the aim of guiding the clinician throughout the process of contact-less assessment and early diagnosis of neurological disorders through a user-friendly environment. A first prototype limited to audio/video recording, named Infant Recorder was presented in [13]. To highlight its capabilities AVIM is applied here to the management of data of three newborns recorded five times from 10 days after birth until the 24th week of age according to a specific protocol. Personal and clinical data, markers and notes, as well as acoustical and kinematical parameters of clinical interest are provided and stored with a minimum effort required to the operator.The AVIM system is a flexible monitoring system equipped with webcam and microphone that allows setting up specific tests and adding notes during the recording. It is supported by devoted software for the automatic analysis of newborn crying and movements. AVIM allows the management of personal data and medical history of the patient, data acquisition, setting up customized test, audio/video editing and reporting within a single system. The user can manage through a single system all the steps, from the recording to clinical data management and the results of analysis with a significant simplification of the whole procedure. Thanks to its user-friendly structure and the simple devices used, the system is well suited for performing audio and video recordings both in the hospital and at patient's home. To this aim, AVIM is equipped with two different access levels (users): a restricted one for nurses or paramedics for data acquisition and an unrestricted one for experienced clinicians to manage audio and video recordings and analysis. It can be arranged for transferring data to a server provided with a centralized database.The system is developed in C# language using the OpenCv image processing library for video acquisition and recording. AVIM is designed for Windows OS (Windows 7 or Windows 8).AVIM is made up by four basic modules: “Patient data management”, “clinical test maker”, “Recorder” and “Analyzer” that are depicted in Fig. 1. They will be described in the following sections. The overall framework of the AVIM system is shown in the use-case diagram of Fig. 2.An integrated system to manage the patient clinical storyboard and share data and medical history through a devoted interface. It allows importing and exporting signals, images and tests from external sources: clinicians can automatically import data from already existing medical records depending on the system used in the hospital. The user can enter patient's data for a specific analysis and then proceed using the tool devoted to audio/video recording. The storage of patient's data is managed through a centralized database structured in order to guarantee privacy and personal data protection [2]. Moreover this tool allows loading patient's data and patient analysis and tests. The user can eventually view the patient's report and carry out the diagnosis through the storyboard and the medical data.An interactive environment for the set up and run of specific clinical tests within a modular and customizable structure. It allows recalling a specific patient from the database, adding multimedia content, images, audio/video screenshots, compiling tests and monitoring the results. Its most innovative feature is the possibility of easily designing new tests using a set of user-friendly objects (such as static text boxes, edit boxes and radio-buttons) and individual scoring for each label. It also allows editing or importing already structured tests with specific format. This module is linked to the “Analyzer” module. A previously entered test can be uploaded and compiled while watching a video or listening to the audio of the patient. Through this module is possible to carry out the perceptual analysis of movements as shown in Fig. 2. Fig. 3shows an example concerning a customized test for GMs.Integrated environment for capturing multichannel audio/video signals, entering notes and related information through the wizard and flexible setup of the system. The user interface includes a main window for selecting the recording devices and an acquisition window. The system allows recording from up to two microphones and two webcams at the same time. The acquisition window is equipped with single- or double-window to display video streams. The use of both pieces of equipment and windows may be useful to display both the subject and the surrounding environment (i.e. to assess mother–child interaction, or analyse the subject from two different viewpoints; Dual microphones play a similar task, allowing to record both infant cry and e.g. the mother's voice.The acquisition window allows previewing both video and audio recordings and checking their quality to avoid blurry or distorted images and ensure a good audio quality avoiding both saturation and too low signal amplitude. The user can mark the time frames corresponding to different behaviours of the newborn, assess the duration of the audio signal for the acoustic analysis as well as the calm-awake status for the GMs analysis. The user can also select other markers to point out the status of the patient (such as sleep, restlessness, etc.), enter notes during the recording and add markers for ambient noise or interfering events. It is also possible to make multiple recordings on the same patient.This module is devoted to audio/video editing and to perceptual and objective analysis. It allows playing/cutting/copying/assessing sequences of interest using markers placed during the recordings process. It also allows entering clinical scores, notes and reference markers (e.g. point out a relevant event) on single crying or motion sequences without the need to resort to the use of other software or to pen and paper. Audio and video editing is managed through specific interfaces. Within each interface the selection and extraction of relevant signal segments is obtained positioning a cursor at their starting and ending points: the software automatically merges into a single file all the selected segments one after the other. This option proved to be particularly useful in the assessment of GMs from video recordings [9,10]. Fig. 4a shows the interface for video recordings and Fig. 4b the corresponding one for audio recordings. For audio signals the user may perform the objective acoustical analysis while for the video signals the interface in Fig. 5is used to carry out the analysis of the GMs using one or more tests previously built with the “Clinical Test Maker” tool.With AVIM the following acoustic parameters (and related statistics) are estimated that recently gained great scientific interest [4,5,8,10–13,15–21,38–43]: fundamental frequency (F0), first two resonance frequencies of the vocal tract (F1 and F2), curtosis, skewness and time duration of each cry unit (CU). In the literature different time lengths are considered for CUs, ranging from 60 to 500ms [8,40–43]. However, CUs of very short duration do not allow the assessment of some relevant features such as their melodic shape. Moreover, inspiratory sounds that have a duration less than 200ms must be disregarded [8]. For these reasons, audio analysis is performed here on CUs longer than 280ms. However, the user can set the proper threshold at any other value.The segmentation of the whole recording into single CUs could be performed manually or through a robust Voiced/Unvoiced (V/UV) detection procedure whose reliability was tested on synthetic and real signals [26,44].AVIM implements a new method for the estimation of F0, F1 and F2. For F0estimation a band-pass FIR filter with a Kaiser window is applied in the range 200–1000Hz on each CU. F0 is computed applying an appropriately selected Continuous Wavelet Transform (CWT). The wavelet transform filters a signal f(t) with a shifted and scaled version of a prototype function ψ(t), the so-called “mother wavelet [45,46]. Unlike the Fourier Transform, the CWT allows a time–frequency representation of a signal with very good time and frequency localization. This makes its application particularly effective with quasi-stationary signals such as newborn cries are. The shift parameter b locates the wavelet in time. The scale parameter a is related to the width of the analysis window: it either dilates or compresses the signal. When a is relatively low, the signal is more contracted, which in turn results in a more detailed resulting plot. On the other hand, when a is high, the signal is stretched out and the resulting plot will be presented in less detail. Varying a and b allows locating the wavelet at the desired frequency and time instant [47].The AVIM Audio Analysis module implements an algorithm for F0, F1and F2 estimation based on the Mexican Hat CWT, successfully used for adults’ voice and speech analysis [48–50]. Its choice comes from a comparative analysis with other CWTs for the signals under study. With wavelets, the relationship between the scale parameter a and the frequency is given by the so-called pseudo-frequency (Fa) in Hz, defined by the following equation:(1)Fa=FcaΔwhere Δ is the sampling period, and Fcis the wavelet (Mexican Hat) central frequency that maximizes the module of the wavelet Fourier transform. On each time window of length 10ms the scale parameter a is allowed to vary in the range 1–55. This choice is related to a reasonable range of 200–1000Hz for F0[20] and up to 6kHz for F1 and F2. Thus the Mexican Hat CWT is applied with a=55, Fs=1/Δ=44.1kHz and Fc=0.25Hz and according to Eq. (1)Fa=200Hz. On each time window the maximum of the CWT matrix (size: 55×441) of real coefficients is computed. Its row index corresponds to the ‘optimum’ scale value, a. The autocorrelation function of this row gives the estimated value of F0 according to:(2)F0=Fsτwhere τ is the lag relative to the autocorrelation maximum.The estimation of F1 and F2 is performed in a similar way, with different ranges for the band-pass filter. Specifically:800–2100Hz for F11500–3500Hz for F2Finally, to avoid spurious peaks a three points moving average smoothing function is applied.Moreover, on each recording the Audio Analysis module computes the number of CUs, the mean values of F0, F1 and F2, the vocalic percentage, the number and length of voice breaks (pauses). Kurtosis, skewness and percentiles are computed on each CU. Plots and tables show the results that are saved in separate folders, one for each recording, in easily usable format (jpeg, excel, txt).AVIM provides a support to the semi-automatic analysis of movements from video clips. It implements an approach similar to that proposed by Coluccini et al. [33,37] to extract movements’ features such as speed and acceleration. The purpose of tracking is to extract from the image the 2D position of the body segments to help the study of the movements according to amplitude, average speed and acceleration. Several models can be used: from lower body (LB), based on three points only, to full body (FB) that takes into account 8 points. The procedure is depicted in the functional block diagram in Fig. 6.Fig. 7a and b shows an example with the FB model. Specifically, Fig. 7a shows the selection of the first point on the body (the right hand), and Fig. 7b shows the final FB model.According to Fig. 7a, the user selects on the video frame the points corresponding to the grey circles that progressively appear on the model (Fig. 7b). After selecting more points, highlighted in grey on the image (Fig. 7b), the user moves to the next frame or to the desired frame. If the circles do not identify the position of a limb an error message is displayed and the user must repeat the selection of points and apply the model again. FB limbs are listed in Table 1[33].The interface offers two tracking options: one for the analysis frame by frame and the other every 25 frames. The two approaches meet different needs: the frame by frame tracking allows for higher accuracy at the expense of a larger amount of time required for the analysis, while the second option allows a faster but possibly less accurate tracking. The procedure support the clinician during the perceptive analysis of GMs by displaying data and trajectories.A new algorithm for recognition and tracking is implemented: in each frame x and y coordinates are picked up along with the corresponding time instant of each point on the image plane and saved in a Comma Separated Values (.csv) file. With this format data can be processed through a simple Excel spread sheet or with commonly used software tools such as Matlab®.The system also provides the time instant of the beginning and end of the sequences of motion (SM) and their number. A sequence of motion ends when all limbs values are lower than a fixed threshold (0.5pixel/frame).The tracking procedure returns a matrix of dimension N×M where N is the number of frames and the first column corresponds to time (in ms). In the other M−1 columns, taken two at a time, the x and y coordinates of each point are saved. Thus for each point a N×2 matrix is obtained. From this “position matrix” the speed relative to the origin of the image plane (left upper corner) of the corresponding point is computed as:(3)vi=pi−pi−1hi=2…Nwhere viis speed at the ith frame, piis ith position in the frame, h is sampling interval and N is the number of frames.From the M−1 arrays containing the x and y coordinates of the speed vectors (size: (N−1)×2) the magnitude of the velocity vector of each point is evaluated. Speeds are normalized with respect to the maximum speed value among all vectors. From vivalues acceleration is computed accordingly.Useful plots are obtained showing trajectories of the main body segments, along with their speed and acceleration.To make the trend of a sequence of movements more apparent, a threshold value (0.5pixel/frame) is chosen given by the average value of each magnitude plot. Those values under the threshold are set equal to 0 while those above the threshold are set equal to 1. To smooth fluctuations speed values are filtered with a moving window of 5 samples. This step is applied to all the velocity vectors thus getting for each of them a vector of binary values indicating the presence or absence of movement of that part of the body. The vectors thus obtained are then reassembled in a matrix N×(M−1) and represented as an image where black and white areas correspond to the absence or presence of movement, respectively.Further, the following parameters are computed:1.Number/start/end of sequences of motion (SM).Average speed of selected points.Average acceleration of selected points.Number of movement units (MU), defined as an act of acceleration or deceleration [51,52]. A MU is recorded when increasing velocity exceeds the threshold value of 0.5pixel/frame and when decreasing velocity falls below the same threshold value. This parameter tests whether the frequency of movements of individual limbs varies in infants.Skewness (Sk) of the velocity of the feet and hands, that reflects the distribution of movement velocity [53,54]. In case of a normal distribution the skewness is equal to zero.Kurtosis of acceleration (K) of each limb, to evaluate movement data distribution. The kurtosis quantifies the shape of the probability distribution of acceleration. A distribution with a large K, showing a more sharply peaked and heavy-tailed form, indicates intermittent occurrences of limb movements and deviates from a Gaussian distribution (K=3.0) [51].Finally, based on the cramped synchronized movements observed by Prechtl et al. [32,55] the cross-correlation (CC) of acceleration between hands and feet is computed. The aim is to determine whether the movement of selected points proceeds in the same direction at the same time. Thus the cross-correlation of acceleration between the left and right foot, related to the similarity of the spontaneous movements of both the feet [51], is computed, as well as that between the left and right hand, the left hand and right foot, the right hand and left foot, the right hand and right foot and the left hand and left foot.

@&#CONCLUSIONS@&#
Recent studies highlight the possibility of detecting early signs of several neurodevelopmental disorders in the first year of life through audio and video analysis [7,13,14,58]. In particular, changes in infant crying fundamental frequency, abnormal movements of body and head, delay in babbling have been early detected for example in autism spectrum disorders, Down syndrome, Rett syndrome and in preterm infants with neurological diseases [16,55–66].The development of reliable software tools to help contact-less early detection and diagnosis of neurological disorders especially in home environment would be highly beneficial. Such tools should be able to enhance and support the clinical diagnosis integrating clinicians’ qualitative analysis, based on subjective skills, with objective measurements. To our knowledge, the AVIM system described in the present study is the first tool for recording and analyzing both cry and general movements in infants. In addition to providing appreciable decrease in investigation time, costs and errors, AVIM meets the growing clinical interest for marker-less monitoring for clinical data analysis and diagnosis. It is conceived to support the clinical and perceptual evaluation but it is not meant to replace them.Future studies will be devoted to the analysis of gestures such as the fluidity and elegance of movements that are difficult to be objectively quantified. With an adequate number of cases it will be possible to build a data set that would allow assessing the effectiveness of the proposed method and checking whether the above features can be discriminative for clinical diagnosis.Moreover an automatic procedure that helps in finding correlations between general movements and cry would be of great relevance and could be exploited as an aid to early clinical diagnosis of several pathologies being easy to perform, cheap and completely non-intrusive. This is indeed a timely challenge for health informatics and for biomedical engineering.Finally, AVIM could be adapted for smartphone or tablet applications as it requires only a webcam and a microphone. After data transferring the analysis could be directly performed by clinicians in the hospital or in clinics. In the future, the system could be integrated with other tools for the screening of a wide range of peri-and post-natal pathologies and can be easily adapted for homecare of infants. This research is on-going.