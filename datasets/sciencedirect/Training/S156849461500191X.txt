@&#MAIN-TITLE@&#
A novel support vector regression for data set with outliers

@&#HIGHLIGHTS@&#
Support vector machine is sensitive to the outliers.A novel support vector regression together with fuzzification theory, inconsistency matrix and neighbors match operator is presented.The objective of this novel support vector regression is to increase the generalization ability for data set with outliers.

@&#KEYPHRASES@&#
Support vector regression,Outlier,Fuzzification theory,Inconsistency matrix,Neighbors match operator,

@&#ABSTRACT@&#
Support vector machine (SVM) is sensitive to the outliers, which reduces its generalization ability. This paper presents a novel support vector regression (SVR) together with fuzzification theory, inconsistency matrix and neighbors match operator to address this critical issue. Fuzzification method is exploited to assign similarities on the input space and on the output response to each pair of training samples respectively. The inconsistency matrix is used to calculate the weights of input variables, followed by searching outliers through a novel neighborhood matching algorithm and then eliminating them. Finally, the processed data is sent to the original SVR, and the prediction results are acquired. A simulation example and three real-world applications demonstrate the proposed method for data set with outliers.

@&#INTRODUCTION@&#
The support vector machine (SVM) initially proposed by Cortes and Vapnik [1,2] is drawing close attention due to its high generalization in solving practical problems such as nonlinearity, small samples and over-fitting. The SVM is a learning machine based on the structural risk minimization (SRM) inductive principle to achieve the generalized performance. Unlike some traditional approaches which attempt to minimize the empirical risk, the SVM also considers the minimization of Vapnik–Chervonenkis (VC) dimension. The main idea of the SVM is to compute a linear regression function in a higher dimensional feature space mapped from the original input space. This function is established with a portion of the training data, which are called support vectors. The SVM has been successfully applied to various fields – classification [3,4], time prediction [5–7] and regression [8,9].The SVM falls into two categories, one is the support vector classification (SVC) and the other is the support vector regression (SVR). When the SVM is exploited in the process of time prediction or regression estimation, the approaches are defined as the SVR. The model generated by the SVR only depends on the support vectors, not all of the training samples. An early overview of the fundamental ideas underlying the SVR has been given by Smola and Schölkopf [8], which also includes some popular algorithms for training the SVM, as well as some modifications and extensions.The approximation scheme using the SVR depends only on the support vectors, rather than all the training samples. It is an important advantage, while simultaneously it makes the SVR too sensitive to outliers and increases the risk of over-fitting [10]. If the collected data set contains outliers, the learning process may not recognize such a situation and then try to fit those abnormal data, thus resulting in an erroneous approximation function [6,11].Generally for real-world applications, data sets often contain multiple variables as well as noise or outliers that are inconsistent with the other data. Outliers may occur for a variety of reasons, such as environment changes or erroneous measurements. Developing methods for reducing the influence of outliers in the SVM have attracted considerable researchers to study on it. Lin and Wang [12] proposed the fuzzy SVM by assigning different fuzzy memberships to different training samples. Jin et al. [13] transformed the given data into a higher feature space through a fuzzy system, and exploited genetic algorithms to improve the fuzzy feature transformation. However, it is difficult to define the membership to the training samples especially when there is not any prior knowledge. Williams [14] proposed a new version called scaled SVM based on the extreme value theory and by computing the mean and variance of the generalization errors for reducing generalization error. Li [15] introduced an idea of separating outliers through the K-nearest neighbor algorithm to guarantee high generalization. Zhan and Shen [16] proposed a new training method for the SVM, in which an adaptive penalty term in the objective function is designed to suppress the influences of outliers, while the method is relatively complicated. Zhang and Wang [17] proposed a rough margin based SVM based on the rough set theory, in which more training samples can be adaptively considered with different penalty depending on their positions.Nevertheless, each of the above approaches was proposed mainly for improving the efficiency and generalization of the SVC, rather than the SVR. To our knowledge, there have not yet been quantities of researches in reducing the influence of outliers for SVM regression. Chuang et al. [18] proposed a robust SVR network, in which the traditional robust statistics is exploited to improve the acquired regression model, while this method needs extensive computation and additional parameters. Suykens et al. [11] proposed a weighted least squares SVM (LS-SVM) to reduce the effects of outliers, while the final result was seriously influenced by the selection of the extra parameters.In terms of outlier detection, several of the most popular techniques are distance-based approach [19], distribution-based approach [20], depth-based approach [21]. However, it is well known that these algorithms are subjected to the dimension curse, and some of them also require the weights of multiple variables. In addition to these approaches above, there are also density-based approach [22] and rough set theory-based approach [23], but the former is sensitive to the parameters defining the neighborhood, and the latter applies to discrete data rather continuous data. Moreover, SVR is used for outlier detection with nonlinear functions and multidimensional input [24], but it is difficult to apply because of adjusting parameters and high computational cost.Drove by the above analysis, a novel support vector regression method for data set with outliers is devised in this paper. In this approach, fuzzy similarity is introduced to define the similarities between each pair of two training samples. The inconsistency matrix is exploited to compute the weights of the input variables. A new neighborhood matching algorithm is used to judge whether the training samples are outliers. Then these outliers are eliminated and the data set without outliers is sent to the SVR.The remainder of this paper is arranged as follows: Section 2 reviews a general background of the SVM and its limitations when outliers exist, and the basic theory of fuzzy rough set theory. The proposed SVR for data set with outliers is presented in Section 3, as well as the introduction of several key steps of the proposed method, including fuzzy similarity (FS) calculation, weight calculation, neighborhood matching degree calculation and modeling for the SVR. A simulation example and three real-world applications are used to illustrate the validity of the method in Sections 4 and 5 respectively. Conclusion follows in Section 6.A regression problem can be defined as to determine a function for approximating the output from a set of training data X={(x1, y1), (x2, y2), …(xl, yl)}, where xi∈Rl(l is the number of the training samples) denotes the input space, and yi∈R denotes its corresponding output value for i=1, 2, …, l. As mentioned above, the SVR is to approximate the given observations by a linear function and a nonlinear transformation from Rlto a high-dimensional feature space F. The general function for SVR takes the form as follows:(1)f(x)=(w⋅φ(x))+bwherew∈Rldenotes a weight vector, b∈R denotes a threshold, and φ(·) is the nonlinear transformation from Rlto the feature space F. Based on the SVR theory, the value of w and b should be determined by minimizing the structural risk:(2)RSR(f)=12||w2||+C∑i=1lLε(y)where ||w||2 indicates the complexity of the regression function, which should be minimized in the approximation process, and w can be written by the form of Eq. (3) with Lagrange multiplier αiandαi*; C>0 is a regular constant determining the penalties to the empirical errors, a large value of which tends to minimize the error in the regression process and get lower generalization, while a small value of which allows the error but tends to get higher generalization; Lɛ(y) denotes the loss function determined by the insensitive parameter ɛ and the error between the real output value y and the estimated one f(x) as Eq. (4).(3)w=∑i=1l(αi−αi*)φ(xi)(4)Lεy=0,f(x)−y<εf(x)−y−ε,otherwiseThe general function can be rewritten by substituting Eq. (3) into Eq. (4) as follows:(5)f(x)=∑i=1l(αi−αi*)(φ(xi)⋅φ(x))+b=∑i=1l(αi−αi*)k(xi,x)+bwhere the (φ(xi)·φ(x)) can be replaced with kernel function k(xi, x), which maps the input space into a higher dimensional feature space and reflects the prior knowledge on data. The details of several main kernel functions can be referred to Ref. [25], and the selection of them should be undertaken by the user.The dual problem corresponding to the original optimization problem is to maximize(6)∑i=1lyi(αi*−ε)−ε∑i=1l(αi+αi*)−12∑i,j=1l(αi*−αi)(αj*−αj)k(xi,xj)which subjects to(7)∑i=1l(αi*−αi)=0αi,αi*∈0,C,i=1,2,…lAs mentioned above, only a portion of training samples are support vectors, which have the nonzero values of the corresponding Lagrange multipliers in Eq. (5). If the requirementf(x)−y<εis met, the corresponding training samples will not contribute to the regression as their Lagrange multipliers equaling to zero; whilef(x)−y≥ε, the corresponding training samples may become support vectors with their nonzero Lagrange multipliers.After the value of w is determined in Eq. (3), the variable b can be computed according to the Karush–Kuhn–Tucker (KKT) condition as follows:(8)b=yi−(w⋅xi)−εifαi∈[0,C]orb=yi−(w⋅xi)−εifαi*∈[0,C]Then the construction of the SVR has been completed.It can be seen that if over-fitting phenomena occurs, some inaccurate information like outlier may also be modeled into the regression function, thus making the function unsmooth. During the regression process, SVR tends to minimize the empirical error to some extent, which in turn, lowers the generalization of the acquired regression function. Therefore, these outliers should be removed before the implementation of the SVR.As already mentioned, outliers may bring about over-fitting phenomenon and cause the loss of generalization in regression and prediction problems, while there have only been few researches on the SVR for data set with outliers.In the following part of this section, a novel SVR together with fuzzification theory, inconsistency matrix and neighbors match operator, abbreviated as FINSVR is presented, and the procedure is described in Fig. 1.The process starts with FS calculation for all pairs of training samples in terms of each variable, including each input variable and the output response. In this step, there are multiple variables in the input space, and they have different significances to the output response, so it is necessary to consider the weights of them, and the first step is followed by calculating these weights. The FS on the whole input space is then calculated according to the FS on each input variable and their weights. After the FS on the input space and output response are both obtained, the inconsistency based neighborhood matching degree for each training sample is computed. According to the results, outliers can be distinguished from the training samples and be removed, while the others need to be reserved. Finally, apply the original SVR to the processed data set for regression and prediction. Some of these steps are described detailedly in the following subsections.For continuous variables, they need firstly to be fuzzified, including determining the number of fuzzy intervals n and the membership function. Assume there are l training samples and q input variables in a data set. Then the valuevij(1≤i≤l,1≤j≤q)of the ith training sample tion the jth input variable can be represented as follows:(9)vij={(qj1,mij1),(qj2,mij2),…,(qjn,mijn)}whereqjk(1≤k≤n)is the kth fuzzy interval of the jth input variable, andmijkrepresents the membership of the ith training sample on the kth fuzzy interval of the jth input variable. Here the triangular membership function [26] is applied to the continuous data. For example, the functions with three fuzzy intervals are shown as Eqs. (10)–(12) and Fig. 2.(10)mij1=1,vij≤vj1(vj2−vij)/(vj2−vj1),vj1<vij<vj20,vij≥vj2(11)mij2=(vij−vj1)/(vj2−vj1),vj1<vij<vj2(vj3−vij)/(vj3−vj2),vj2≤vij<vj30,vij≤vj1orvij≥vj2(12)mij2=(vij−vj1)/(vj2−vj1),vj1<vij<vj2(vj3−vij)/(vj3−vj2),vj2≤vij<vj30,vij≤vj1orvij≥vj2whereqj1=(−∞,vj2],qj2=[vj1,vj3],qj3=[vj2,∞], andvjk(1≤k≤3)can be calculated by using fuzzy C-means (FCM) clustering algorithm [27].After data fuzzification, the fuzzy similar relation is to be built. Currently the popular methods of measuring the fuzzy similar relation are distance function mainly including Euclidean distance [28,29] and Manhattan distance [30]. In this paper, Manhattan distance is introduced to calculate the FS. Then the FS of the eth (1≤e≤l) and fth (1≤f≤l) training sample on the jth input variable can be defined as:(13)rj(te,tf)=1−12∑i=1nmeji−mfjiConsidering the different significance of input variables when the data set includes more than one input variable, the FS of the eth and fth training samples on the input space can be defined as:(14)Sin(tm,tn)=∑j=1qwj⋅rj(te,tf)where wjis the weight of the jth input variable, and will be calculated in the following subsection.Similarly, the FS of the eth and fth training sample on the output space can be defined as:(15)Sout(te,tf)=rout(te,tf)where rout(te, tf) is the FS of the eth and fth training sample on the output response.To determine the weights of the input variables, the inconsistency matrix is exploited. Firstly another fuzzy similar relation which only depends on the smallest FS among those on a set of variables is given. The similar relation of the eth and fth training samples on the input space and the output response can be expressed respectively as follows:(16)Rin(te,tf)=min1≤j≤qrj(te,tf)In classification problems, the training samples which have the similar input while belong to different classes are identified inconsistent. However, it is obvious that this definition of inconsistency becomes meaningless for continuous data sets. Therefore, a new definition of inconsistency for continuous data sets is proposed. The inconsistency can be defined that the similar relation on the input space is strong while the similar relation on the output response is weak, and the inconsistency degree of the eth and fth samples in terms of the whole input space can be expressed by the following equation:(17)INC1(te,tf)=Rin(te,tf)−Sout(te,tf)Then the inconsistency matrix in terms of the input space can be expressed as:(18)INM={inm(te,tf)|1≤e,f≤l}=1,ifINC1in(te,tf)>1−β10,elsewhere β1 is the threshold of similar degree, a value between 0.8 and 1 is suggested.By computing the sum of elements of the inconsistency matrix, the significance of the jth variable can be defined as:(19)σj=sum(INM(in−j))sum(INM)where INM(in−j) represents the inconsistency matrix in terms of all the input variables except the jth one, and sum(·) represents the sum of the inconsistency matrix elements, that is the number of element 1 in the inconsistency matrix.Then the weight of the jth input variable is to be calculated by the following equation.(20)wj=σj∑j=1qσjBased on the definition of the neighbors match operator given in Ref. [31] for classification problems, the neighbor match operator of one sample can be defined as the proportion of samples of the same class as the specific sample within a certain range, which reflects the spatial distribution of samples. The equation is given as follows:(21)Neighbor_Match(ti,K)={t|label(t)=label(ti),t∈KNN(ti)}Kwhere label(·) represents the class of the specific training sample, and KNN(ti) is the K-nearest neighbor (KNN) of the ith sample. The basic idea of the KNN algorithm for classification is to find the K nearest samples to the given training sample according to a distance measure, and to determine which class the given training sample mostly belongs to. The smaller the value of the Neighbor_Match(ti, K), the more inconsistent the distribution of the ith sample with its neighbors, and the more likely the ith sample is an outlier.Inspired by the idea of the neighbor match operator for classification, a novel neighbor match degree (NMD) for regression on continuous data set is proposed. Firstly, a new definition of inconsistency for continuous data set is given, which is different from the inconsistency defined in the above subsection. The new inconsistency degree of the eth and fth samples can be expressed by the following equation:(22)INC2(te,tf)=Sin(te,tf)−Sout(te,tf)With the new definition of inconsistency degree, whether the eth and fth sample are inconsistent can be then determined by the following inequality:(23)INC2(te,tf)>1−β2where β2 is another threshold of similar degree, and a value smaller than β1 and between 0.6 to 0.9 is suggested. If the inequality is satisfied, the two samples are considered inconsistent, otherwise they are consistent.Finally, the NMD of one training sample can be expressed as follows:(24)NMD(ti,K)={t|INC2(te,tf)≤1−β2,t∈KNN′(ti)}KNote that here KNN′(ti) is to find no more than K nearest neighbors whose FS with the ith sample on the input space are larger than a suggested threshold ɛ. In general, K is taken as an integer in the range between three to ten. Similarly, the smaller the value of NMD(ti, K), the more inconsistent the distribution of the ith sample with its neighbors, and the more likely the ith sample is an outlier.Model selection and parameter optimization have massive influence on the performance of SVR. However up to now, there is no accepted unified method for selection of SVR kernel function and parameters. The choice of kernel functions depends on what kind of model is to be built for the problem at hand. Because of the superior performance, the radial basis function is more commonly used with the kernel parameter g and the penalty parameter C need to be tuned, since an unsuitable selection of the two parameters may cause under-fitting and over-fitting problems [32,33]. At present, a few parameter optimization techniques are found in the real applications, and the most common are grid-search algorithm and genetic algorithm [34]. In this paper, the grid-search for the optimal values of g and C by carrying out 5-fold cross-validation on the training data is preferred. Different pairs of g and C are tried and the pair obtaining the smallest root mean square error (RMSE) is picked up. The optimized parameters are exploited to train the SVR. By now the developed model has been ready to predict the unknown data.The simulation is conducted by the LIBSVM toolbox provided by Lin [35] in Matlab environment, and three evaluation criterions are used to evaluate the performance of the proposed method. They are mean absolute error (MAE), root mean square error (RMSE) and coefficient of determination the coratio between interpretable sum deviation SSR and SST (SSR/SST), and can be respectively defined as follows:(25)MAE=1m∑k=1mfk−fˆk(26)RMSE=1m∑k=1m(fk−fˆk)2(27)R2=1−∑k=1m(fk−fˆk)2∑k=1m(fk−f¯)2where fkis the target value,fˆkis the corresponding prediction, k is the testing sample size, andf¯is the average of target values. The first two criterions are widely used as the deviation measurement between the target and prediction values, and the third statistic measures the goodness of fitting for regression models. The smaller MAE, the smaller RMSE, or the larger R2, the better fitting performance is. To effectively verify the proposed method, other outlier detection and removal methods with SVM [36,37] (hereinafter referred to as Chuang's method and Wen's method), and a two-layer feed-forward Neural Network with sigmoid hidden neurons and linear output neurons (hereinafter referred to as NN) are applied to be compared. In the section, one simulation example is used to verify the effectiveness of the proposed method, as the following function:(28)z=xi2/3+yi2/3+piwhere xi, yi(1≤i≤200) are generated randomly from the above function within interval [−2, 2]. Another 200 testing samples are also selected randomly. pirepresents outliers. Varying degrees of percentage (2.5%, 5.0%, 7.5%) of outliers in the data are constructed intentionally, that means pi=[0.2, −0.2, 0.2, −0.2, 0.2, 0, 0, …, 0, 0], pi=[0.2, −0.2, 0.2, −0.2, 0.2, −0.2, 0.2, −0.2, 0.2, −0.2, 0, 0, …, 0, 0], and pi=[0.2, −0.2, 0.2, −0.2, 0.2, −0.2, 0.2, −0.2, 0.2, −0.2, 0.2, −0.2, 0.2, −0.2, 0.2, 0, 0, …, 0, 0] respectively.Both of the results acquired using the proposed FINSVR and other three methods are given in Table 1. It can be seen that the proposed method can further increase the generalization ability than the other methods based on the above three evaluation metrics.In order to illustrate the effectiveness of the proposed FINSVR, three real-world datasets are used in this paper. The first example is shown in the attachment, and its full description can be seen in Ref. [38]. This example is about the industrial enterprises economic prosperity index of China from 2006 to 2011. The economic prosperity can be influenced by many factors, such as product demand, technological level, macroeconomic environment, etc. The input variables, 12 economic indicators, have a complex multidimensional nonlinear mapping relation with the industrial enterprises economic prosperity index. In this example, the data from 2006 to 2010 are used as training data, and those in 2011 are used as testing data.A 2001 Ford Taurus model from National Crash Analysis Center (NCAC) [39] is also used to compare Chuang's outlier detection and removal method and the proposed method. Fig. 3shows the CAE model for the full frontal impact. The simulation speed is 56.6km/h against a rigid wall. In this example, the thickness of eight parts from the front-end structure are selected as the deign variables, as shown in Fig. 4, and the description of them are shown in Table 2. Chest G is selected as the monitored output response. To demonstrate the proposed method, 84 design of experiment (DOE) samples are used as training data, and another 20 DOE samples are treated as testing data.The third example is Boston Housing dataset from UCI Machine Learning Repository [40]. This dataset contains 506 cases concerning housing in Boston Mass Area. There are 14 attributes, in which housing values is output response and the other 13 factors affecting the housing values are treated as input variables. To construct the SVR and evaluate its performance, randomly selected 300 cases from the dataset are used as training data, and the others are used as testing data.The comparison results of the FINSVR and the three mentioned methods are also list in Table 1. It can be seen that the proposed FINSVR is superior to the Chuang's method and Neural Network in all the three examples, based on the three evaluation criterions, and is better than Wen's method in all the examples except for the Taurus dataset.

@&#CONCLUSIONS@&#
The SVR is a powerful tool for regression and prediction. However, the developed model acquired by the SVR only depends on a few support vectors, which makes the classical SVR sensitive to the outliers. In this paper, a novel SVR combining fuzzification method, inconsistency matrix and neighbors match operator is proposed. The fuzzification method is used to build the fuzzy similar relation, which provides the premise for the subsequent steps. The inconsistency matrix is constructed based on a new definition of inconsistency, and exploited to calculate the weights of input variables. A novel neighborhood matching algorithm is proposed to determine whether a training sample is an outlier. After all the outliers are found and removed, the SVR is applied to the processed data set. The numerical results of one simulation example and three real-world examples demonstrate that the FINSVR has superior generalization performance than the classical SVR.