@&#MAIN-TITLE@&#
Prediction of Chinese word-formation patterns using the layer-weighted semantic graph-based KFP-MCO classifier

@&#HIGHLIGHTS@&#
A novel layered semantic graph of Chinese semantic word-formation is proposed.The layer-weighted graph edit distance and the similarity kernel are defined.A new algorithm based on KFP-MCOC is used to predict word-formation patterns.Comparison of predictive performance is conducted between KFP-MCOC and SVM.Statistical test showed that the accuracy of the proposed approach is significant.

@&#KEYPHRASES@&#
Semantic,Word-formation,Graph edit distance,Multi-criteria optimization,Pattern classification,

@&#ABSTRACT@&#
Nowadays natural language processing plays an important and critical role in the domain of intelligent computing, pattern recognition, semantic analysis and machine intelligence. For Chinese information processing, to construct the predictive models of different semantic word-formation patterns with a large-scale corpus can significantly improve the efficiency and accuracy of the paraphrase of the unregistered or new word, ambiguities elimination, automatic lexicography, machine translation and other applications. Therefore it is required to find the relationship between word-formation patterns and different influential factors, which can be denoted as a classification problem. However, due to noise, anomalies, imprecision, polysemy, ambiguity, nonlinear structure, and class-imbalance in semantic word-formation data, multi-criteria optimization classifier (MCOC), support vector machines (SVM) and other traditional classification approaches will give the poor predictive performance. In this paper, according to the characteristic analysis of Chinese word-formations, we firstly proposed a novel layered semantic graph of each disyllabic word, the layer-weighted graph edit distance (GED) and its similarity kernel embedded into a new vector space, then on the normalized data MCOC with kernel, fuzzification and penalty factors (KFP-MCOC) and SVM are employed to predict Chinese semantic word-formation patterns. Our experimental results and comparison with SVM show that KFP-MCOC based on the layer-weighted semantic graphs can increase the separation of different patterns, the predictive accuracy of target patterns and the generalization of semantic pattern classification on new compound words.

@&#INTRODUCTION@&#
We know that word is a basic unit of Chinese natural language which forms phrase, sentence, paragraph and discourse by combining different words. Because there is no explicit boundary between two words, word segmentation is a critical and indispensable step before Chinese text processing and parsing. For word segmentation most studies and the practical systems focus on word-based segmentation methods (Huang and Zhao, 2007; Sun and Zou, 2001). Owing to polysemy and ambiguity, accuracy and efficiency of word segmentation using these approaches are the two main constrains on the utilization in the real world applications. Recently, by analyzing the character dependency and the context of words and character-based methods has considerably improved the performance of word segmentation (Li, 2011; Zhang et al., 2013a, 2014a; Zhao, 2009).Besides, text classification has been extensively studied in the past two decades, rule induction and pattern learning are two main types of methods which are often used for topic identification, sentiment classification, information retrieval, and so on. In general, text category can be divided into two phases, firstly based on the textual contents of documents, the classification model is used to learn the categorical characteristics with training documents then unknown documents are assigned to a set of classes by using the learned classifier. And these methods of document category mainly included naive Bayes classifier, Bayesian network, neural network, SVM, nearest neighbor, maximum entropy classifier, decision tree, and so on (Balahur et al., 2014; He et al., 2003; Lam and Ho, 1998; Li and Sun, 2007; Manning and Schutze, 1999; Sebastiani, 2002; Thelwall et al., 2011; Zhang et al., 2008).However, classification of Chinese semantic word-formation patterns is a new challenging and important problem in linguistics, natural language understanding, pattern recognition and Chinese information processing (Fomichov, 2010). Especially, when some new words, unregistered words and new usages of existing words have not been included in dictionary, it is very difficult to accurately understand the meaning of words, for instance, “给力(gelivable)”, “男神(god)”, “超女(super girl)”, “博客(blog)”, “吐槽(debunk)”, “屌丝(diors)”, “Q币(Q coins)”, “城镇化(urbanization)”, “暗物质(dark matter)”, “绿色通道(easy access)”, “知识经济(knowledge economy)”, “发展是硬道理(development is the main principle)”, and so on. Even if the above case is not considered, because of polysemy of words we sometimes face a difficult choice for the existing words, let alone computer, such as “封建(feudal/feudalism)”, “深浅(deep-shallow/shade)”, “跑路(gateway/avoid debt escape)”, “地道(real/pure/authentic/tunnel)”, “不明不白(not clear/shady/doubtful)”, “风花雪月(ageless/romantic/to waste money in houses of ill repute)”, “天马行空(a powerful and unconstrained style/an unrestrained and vigorous style that brims with talent)”, and so on. Most studies of the problem focus on the investigation of Chinese syntax, that is to say, these studies focus on finding the structural and morphemic types of words and the types of part of speech (POS) so as to extract the semantic word-formation rules (Lu, 2006; Yang, 2002; Yuan, 2000, 2007; Zhou, 1999). And these studies did not make the best of the real and large-scale Chinese corpus for the semantic word-formation analysis, where the syntactic and semantic structures, semantic roles, and syntactic components of different words in corpus need to be tagged in advance. Without the support of Chinese semantic corpus, some conclusions of these methods cannot give the right description and interpretation of real text, and in fact they are not helpful for natural language understanding in real world applications. Hence, our motivation for this study comes from the objective that we will try to find the relationship between semantic word-formation patterns and different influential factors. The former enables machine automatically to choose a pattern and so as to give a proper interpretation for a word while the latter mainly includes the types of morpheme senses, the semantic types in different contexts and the type of POS. Then based on the above influencing factors the corresponding classifier for pattern classification of semantic word-formations is constructed. That is to say, based on a large-scale and modern Chinese semantic corpus, in particular automatic classification of different semantic word-formation patterns by using the highly efficient classifiers can help to solve the aforementioned problems (Kang and Sun, 2003; Li, 2011; Shao et al., 2009; Sun and Li, 2009; Zhang et al., 2013a, 2014a; Zhao, 2009).Therefore, this study generally benefits to the following three aspects: (i) extracting and predicting the patterns of Chinese semantic word-formations help computer accurately identify new words and understand their senses so as to improve the precision of machine translation. (ii) Clarifying the semantic structure of different words makes the lexicographic arrangement and paraphrase more scientific and reasonable in dictionary compilation. Now, this study mainly aims to automatically identify, classify and understand new words and new usages by computer. (iii) For Chinese learners, making clear the semantic structures of compound words contributes to word memory and understanding so as to increase their learning efficiencies.Among the classification approaches to intelligent information processing, some graphical methods can be used to find the natural structure in real word applications, and recently they are paid more attention (Neuhaus and Bunke, 2007; Riesen and Bunke, 2009). Since graphs can be used to represent properties of objects and their complex relations simultaneously, that is, it can overcome the limitations of the vector expression with the same length, flat structure and missing data imputation. And the models and algorithms based on graphs have been extensively used in language information processing, bioinformatics, image classification, web and text mining, social network analysis, and other fields of applications (Borgwardt et al., 2005; Bunke and Riesen, 2011; Riesen and Bunke, 2009; Schenker et al., 2004; Yager and Amin, 2004). When graphs are applied to solving classification problem, the procedure can be described as: firstly an appropriate dissimilarity measure is defined by computing the GED between an input graph and the selected prototype graphs which are the representatives of a word-formation pattern class, then the dissimilarity measure is embedded into a feature vector space, thus many available classification models and corresponding algorithms can be utilized in the new vector space embedded graphs (Gibert et al., 2012; Lagraa et al., 2014; Riesen and Bunke, 2010).As a popular technique for classification, SVM based on statistic learning theory and optimization technique recently grew in popularity. The main idea of the SVM algorithm is to separate instances from different classes by fitting a separating hyperplane that maximizes the margin among the classes and minimizes the misclassification simultaneously (Cortes and Vapnik, 1995; Cristianini and Shawe-Taylor, 2000; Hamel, 2009).Another optimization method mentioned above is MCOC which is used to solve classification problems (Shi et al., 2001). The classifier mainly finds a trade-off between the overlapping degree of different classes and the total distance from input points to the separating hyperplane, with the former to be minimized and the latter to be maximized at the same time.However, in many real world applications, for example, language information processing, owing to the essential imprecision and ambiguity of language and the artificial errors, for instance, POS of the Chinese character “把(hold/bundle/about/handle)” in dictionary is often registered as preposition and quantifier but it is sometimes used as verb, so the latter type may be incorrectly labeled as the former type in term of dictionary, other errors include word segmentation error, manually tagged error of sense type in training corpus. Besides, quality problems like noise, outliers and anomalies within the data set are also very common, the collected data set may be class-imbalanced, nonlinearly separable and uncertain. Therefore, in order to deal effectively with the above cases MCOC with fuzzification, kernel, and penalty factors (FKP-MCOC) based on layer-weighted GED was used to analyze the characteristics of Chinese semantic word-formations. The classifier was constructed by firstly computing GED between each semantic graph and prototype graphs to gain the dissimilarity measure, then introducing a fuzzy membership degree to each input point and using kernel function to transform into a new feature space. And the proposed FKP-MCOC gain the better separation of different Chinese word-formation patterns (Gao et al., 2013). Besides, to overcome the class-overlapping, inconsistent, incompatible and contradictory problems in language information processing, an extensible MCOC was put forward and used for prediction of Chinese semantic word-formation patterns (Zhang et al., 2013b).Recently, to further improve the performance of MCOC, by firstly introducing a kernel function to constraints, then a fuzzy membership degree to each input point and penalty factors of imbalanced classes to objective functions, the MCOC model and corresponding algorithm have been reformulated. Thus, the new KFP-MCOC can remarkably improve the performance of the original MCOC approach in stability, efficiency and generalization, which reduces the effects of anomalies, class imbalance and nonlinearly separable problems significantly (Zhang et al., 2009, 2014b,c, 2015). Therefore, combining KFP-MCOC and layer-weighed semantic graphs can achieve better predictive performance on the corresponding corpus of Chinese semantic word-formations.The rest of this paper is organized as follows: The characteristic analysis of Chinese word-formations and the new layer-weighted semantic graph representation of Chinese word-formation are illustrated in Section 2. Section 3 describes the basic principles of the SVM and KFP-MCO classifiers. Then a new classification algorithm based on layer-weighted semantic graphs is shown in Section 4. The experiment on Chinese semantic word-formation pattern analysis and the results are demonstrated in Section 5. Finally, discussions and conclusions will be given in Sections 6 and 7 respectively.In this section by analyzing the characteristics of Chinese word-formations we give a new definition of the layered semantic graph. The layer-weighted graph edit distance (GED) and their similarity kernel for any two semantic graphs of Chinese word-formations are defined and computed respectively.According to the characteristics of Chinese word-formations, the influencing factors of different semantic word-formation patterns can be summarized into in the following three parts:(i) The types of morpheme senses, in term of the rules of word-formation of synonyms in the thesaurus of Chinese words (Mei and Zhu, 1986; HIT; TCCD, 2012; Chinese Dictionary, 2012; DTC, 1998; TPMSCTTC, 2003; WordNet, in press), we know that the number of Chinese character and their morpheme senses are relative stable. Most new words are derived from the existing Chinese characters and their new meanings are interpreted by recombining the meanings of different characters. Therefore, the morpheme senses of word are main influential factors of word-formation patterns and the sense types can be gained from the thesaurus of Chinese words. For all Chinese characters forming the compound words, their sense types are classified as three categories in the light of the semantic scopes, they are respectively big class, medium class and small class, and they constitute a hierarchical tree. In the taxonomic hierarchies the numbers of big class, medium class and small class amount to 12, 94 and 1428 respectively. For a compound word, its sense types can be expressed as a binary tree with three levels which is a subtree of the complete binary tree shown in Fig. 1.Among them for big class the labels of the types of morpheme senses and their meanings are given in Table 1. Besides, medium class is marked by the lower English letters from “a” to “r” while small class is labeled with the number from “01” to “67”.Here we take the disyllabic words for example, the first morpheme sense of “接受(accept)” is labeled as “Hc07” with “H” for big class, “c” for medium class and “07” for small class while the second morpheme sense of the word is tagged by using “Je14” for big class with “J”, medium with “e” and small class with “14”.(ii) The usages or senses type of word, there is the inevitable contact between the word senses and different contexts, so the semantic type of word is also important influencing factor of word-formation patterns. And the semantic types of word can be obtained from the tagged semantic corpus (DTC, 1998; TPMSCTTC, 2003), for instance, the following sentence has been tagged the semantic types or sense type:“我们(our)/R/Aa02 伟大(great)/A/Ed20 的/U/Kd01 祖国(motherland)/N/Di02 充满(is full of)/V/Jd06 生机(vitality)/N/Dd14 和(and)/C/Kc01 希望(hope)/N/Df08.”Form the above tagged sentence we find that the word “希望(hope)” in the current context is labeled as “Df08” for big class with “D”, medium class with “f” and small class with “08” respectively. Actually the word “希望(hope)” in other occasion is also marked by “Gb04” for big class with “G”, medium class with “b” and small class with “04” respectively.(iii) POS of morpheme or word, obviously the word sense is conditioned by POS of morpheme or word and the type of POS for morpheme and word can be got from the above thesaurus of Chinese words or the tagged semantic corpus (TSCCCD, 2001). In this paper, we employ POSs of different morphemes in the classification model, for example, for the above tagged sentence POS of the word “充满(is full of)” is respectively labeled as “Vg” for the Chinese character “充(fill)” and “A” for “满(full)”. Thus we can find that the POS of “充(fill)” is verb with “Vg” while the POS of “满(full)” is adjective with “A”. Of course, we can roughly label the word “充满(is full of)” as a verb with the letter “V” which is directly extracted from the tagged corpus.To sum up, through the characteristic analysis of Chinese word-formations we consider the above three types of influencing factors as input features of the pattern classification in this paper. And the corresponding data extracted from the above three sources form the Chinese semantic word-formation corpus or database. Based on the above data the classifiers are constructed and are used to classify and predict different word-formation patterns.Based on the structural characteristics and semantic analysis of Chinese word-formation patterns, the layered semantic graph is used to represent different components and the mutual relations among them, which can be defined asDefinition 1Layered semantic graphLet LVand LEbe the finite label sets for nodes and arcs in a layered semantic graph. The digraph g is a four-tuple g=(V, E, η, μ, ), where V is the finite set of nodes, E⊆V×V is the finite set of arcs, η:V→SV×CVis a node labeling function which is used to determine the semantic layer and the type of a node in the light of position in graph g, and μ:SV×CV→LVis also a node labeling function for defining the label of a node according to its semantic layer and node type.According to the above definition, a Chinese compound word can be represented as an arbitrarily structured graph with the above constrained labeling functions. For a compound word, the labels of the semantic layers can be obtained from the discrete set SV={big class, medium class, small class} while the labels of the node types are defined as the discrete set CV={sense type, POS type}. For the different semantic layers the node labels can be get from the symbol subsetLVb={A,B,C,D,E,F,G,H,I,J,K,L}for the big class of sense types, the symbol subsetLVm={a,b,c,…,r}for the medium class of sense types, the symbol subset ofLVs={01,02,03,…,67}for the small class of sense types. The symbol subsetLVpb={A,B,C,D,E,F,M,N,O,P,Q,R,S,T,U,V,X,Y,Z}for the big class of POS types, the symbol subsetLVpm=∅for the medium class of POS types and the symbol subsetLVps={g,h,k,r,s}for the small class of POS types respectively. Thus, the labeling set of each node in a graph is defined as different subsets, that is, it may be one of the symbol setsLVb,LVm,LVs,LVpb,LVpm,andLVps, which dependents on the position and type of the node in graph.Besides, an arc points to the different components of the compound word, which is mainly composed of word sense, the first morpheme sense, the second morpheme sense, POS of the first morpheme and POS of the second morpheme. In this paper, the label set of arcs is null, that is, we have LE=∅. Thus a directed graph directly corresponds to a compound word. According to the above illustration now two real-world examples of Chinese semantic word-formations for disyllabic words is expressed as Fig. 2.As the layered semantic graphs shown in Fig. 2, the two disyllabic words are chosen from the semantic word-formation corpus and are comprised of two types of POSs and three types of senses. Three types of senses are word sense, the first morpheme sense and the second morpheme sense while two types of POSs respectively correspond to the two morpheme senses in order from left to right. And each sense type is composed of three semantic layers which are big class, medium class, and small class in top-down order. POS of each morpheme consists of the big and small classes.The first word “带领(lead)”demonstrated in Fig. 2(a) means an activity of a member leading a team or group. And the disyllabic word is labeled as pattern 1, that is, the word sense is semantically explained as the first or second morpheme sense. For the types of word sense and the big class of the two morpheme senses in Fig. 2, the symbol “H” implies an activity or event. The symbol “V” for the big class of the two morpheme senses means that POS of the first compound word is a verb. Similarly, the second word “宫廷(court)”shown in Fig. 2(b) means a place where a king or queen lives and carries out ceremonial or administrative duties. The compound word belongs to pattern 6, its word sense is semantically composed of the basic meaning and the extended meaning, and the former includes the first and second morpheme senses. For the types of word sense and the big class of the first morpheme sense in Fig. 2, the symbol “B” indicates an object while the symbol “D” for the type of the big class of the second morpheme sense shows the abstract thing. The symbol “N” for the big class of the two morphemes “g” for the small class of the second morpheme indicate that POS of the second compound word is a noun. Besides, for the medium and small classes, a semantic interpretation can be given according to the meanings of their label sets.In a word, each compound word can be represented as a graph or a subgraph with different labels similar to the above graph, but for any two graphs their maximum common subgraphs are isomorphic. Actually the above layered semantic graph can be generalized to the representation of the polysyllabic compound words. However, for the polysyllabic compound words, there are more word senses, morpheme senses and POSs in the corresponding layered semantic graphs.In order to obtain the dissimilarity measure of different semantic graphs, a novel layer-weighted GED based on the edit operation cost is defined, which should be minimized. Therefore, the edit distance of two semantic graphs is defined asDefinition 2Layer-weighted GEDLet g1=(V1, E1, η1, μ1) be the source graph and g2=(V2, E2, η2, μ2) the target graph, and LVis the aforementioned label set of nodes in graphs g1 and g2. For two nodes uk(uk∈V1) andvk(vk∈V2), the labeling function η is firstly used to determine the semantic layer and the type of node, then the labeling function μ is used for getting the label value of node. That is to say, we have μ1(ukj)=μ1∘η1(uk)=μ1(η1(uk)) andμ2(vkj)=μ2∘η2(vk)=μ2(η2(vk))where j is the semantic layer number, ∘ is a compound operation, ηiand μi(i=1, 2) are the two node labeling functions which are respectively the same as the labeling functions η and μ of Definition 1. Thus, the layer-weighted GED d(g1, g2) (d:g1×g2→R) between the two graphs g1 and g2 is defined as(1)d(g1,g2)=min(e1,…,em)∈P(g1,g2)∑k=1mwj(μ1(ukj)−μ2(vkj))21/2wherewj(wj∈[0,1])is the user-defined weight of the corresponding semantic layer which satisfies∑j=1swj=1, and s(s=|SV|) is the number of semantic layers in a graph. P(g1, g2) denotes the set of different edit paths transforming graph g1 into g2, which are composed of a serial of the elementary edit operations ekof nodes and arcs, and the shortest edit path should be found. Besides, it is should be pointed out that in this study only node edit operation is taken into account. Besides, in the case of minimum edit cost m is the number of edit operations and we have m≤max(|V1|, |V2|).For the elementary edit operations ekof two nodes there are three types of edit operations in Definition 2, they are: (i) node substitution operation denoted by(u→v)is allowed if the nodes u and v are nonempty; (ii) node deletion operation denoted by (u→ɛ) is carried out if the node u is nonempty; (iii) and node insertion operation denoted by (ɛ→u) is conducted on condition that the node u is empty. In this paper, owing to the structural matching of semantic graphs or sub-graphs for any two disyllabic words, the three types of edit operations of nodes are employed in Eq. (1), and the operation cost can be measured by calculating the difference between two ASCII values of the corresponding nodes. That is to say, the labeling function is considered as a mapping from the symbol set of nodes to their ASCII values. In particular, when the counterpart in graph g1 of a node in graph g2 does not exist, its ASCII value is set to 0, the reverse is also true.After the layer-weighted GED between any two disyllabic words is obtained, we can use the RBF function to calculate the similarity of different word-formations and to construct the kernel function based on the values from Eq. (1). Thus given the edit distance d(g1, g2) of two graphs g1 and g2, by using the radius basic function (RBF) the similarity kernel is defined as(2)K(g1,g2)=exp−d2(g1,g2)2σ2.where σ(σ>0) is a kernel parameter. From Eq. (2) we knew that the method actually turns the dissimilarity measure into a similarity one so that we can integrate the kernel function into the aforementioned SVM and KFP-MCOC classifiers.In this section the two classifiers of SVM and KFP-MCOC are briefly reviewed and are used for classification of Chinese semantic word-formation patterns in the forthcoming sections.Firstly we briefly review the basic theory of SVM in solving classification problems (Cortes and Vapnik, 1995; Cristianini and Shawe-Taylor, 2000). For a binary classification problem, given a training setT1={(x1, y1), …, (xn, yn)}, each input pointxi(xi∈Rd) belongs to either of the two classes with a label yi∈{−1, 1}, i=1, …, m for yi=−1; i=m+1, …, n for yi=1, where d is the dimensionality of the input space and n is the sample size. For the linearly separable data setT1, we may find an optimal decision hyperplanewTxi+b=0where the margin between the two support hyperplaneswTxi+b≤−1andwTxi+b≥1for two different classes is maximized, thus the primal optimization problem is denoted as(3)min(1/2)||w||22subject toyi(wTxi+b)≥1,i=1,…,nwherexiare given training data,wand b are unrestricted variables.If the data setT1 is approximately and linearly separable, the relaxation variable ξi(ξi≥0) is introduced to the above problem such that the SVM classifier in Eq. (3) is rewritten as(4)min(1/2)||w||22+C∑i=1nξisubject toyi(wTxi+b)≥1−ξi,ξi≥0,i=1,⋯,nwhere C is a penalty constant which is also regarded as a regularization parameter, which is used to search a tradeoff between the maximum margin and the minimum classification error.We knew that searching an optimal decision hyperplane of SVM in Eq. (4) is a quadratic programming problem which can be solved by constructing a Lagrangian function and transformed into the dual optimization problem as(5)min(1/2)∑i=1n∑j=1nαiαjyiyjxiTxj−∑i=1nαisubject to∑i=1nyiαi=0,0≤αi≤C,i=1,…,nFor the nonlinearly separable case, we suppose that ϕ(x) is a basic function mapping the input data into a higher dimensional feature space. Given the data setT2={(ϕ(x1), y1), …, (ϕ(xn), yn)}, the SVM classifier in Eq. (5) can be rewritten as(6)min(1/2)∑i=1n∑j=1nαiαjyiyjϕ(xi)Tϕ(xj)−∑i=1nαisubject to∑i=1nyiαi=0,0≤αi≤C,i=1,…,nThe dot product (ϕ(xi)Tϕ(xj)) can be substituted for the kernel function K(xi,xj), thus the SVM classifier is denoted as(7)min(1/2)∑i=1n∑j=1nαiαjyiyjK(xi,xj)−∑i=1nαisubject to∑i=1nyiαi=0,0≤αi≤C,i=1,…,nAfter solving the dual problem (see Eq. (7)) with kernel function we can obtain the optimal solutionα∗=(α1∗,…,αn∗). It is noted that the input pointxicorresponding withαi∗>0is called support vector. In the case of0<αi∗<C, the corresponding input pointxisatisfies the equality constraint in Eq. (4) and ξi=0, while in the case ofαi∗=Cthe input pointxiis misclassified. The point corresponding withαi∗=0is classified correctly.Then the similarity kernel using RBF in Eq. (2) is used for the kernel function in Eq. (7). Additionally, the decision function of the SVM classifier for a new input pointxis written as(8)f(x)=sign(wTϕ(x)+b*)=sign∑i=1nαi∗yiK(xi,x)+b*.And we can obtainb*=yj−∑i=1nαi*yik(xi,xj)(j=1, …, n) from the training set and the above solutionαi*(0<αi*<C).In this section the MCO classifier is simply reviewed before we make an intensive description of KFP-MCOC (Zhang et al., 2014b,c). Generally the MCOC approach can be described as follows: For a two-class classification problem, given the training setT1, let the parameter αi(αi≥0) be the distance where the input pointxideviates from the separating hyperplane, and the sum of αishould be minimized. At the same time, let the parameter βi(βi≥0) be the distance where the pointxideparts from the decision hyperplane, then the sum of βishould be maximized. Thus, the MCOC model can be denoted as(9)minC∑i=1nαi−∑i=1nβisubject towTxi−b=yi(βi−αi),αi≥0,βi≥0,i=1,…,nwhere C (C>0) is a penalty constant of the misclassified instances. And the input pointsxiare given training data,wand b are unrestricted variables.For the class-imbalanced case, let C1 (C1>0) be the misclassification cost or penalty factor if yi=−1. Similarly, let C2C2>0 be the misclassification cost if yi=1. We can write Eq. (9) as(10)minC1∑yi=−1αi+C2∑yi=1αi−∑i=1nβisubject towTxi−b=yi(βi−αi),αi≥0,βi≥0,i=1,…,nFor the nonlinearly separable case, we suppose that ϕ(x) is a basic function mapping the input data into a higher dimensional feature space. Given the data setT2, the weight vectorwcan be denoted as the linear combination of ϕ(xj) and yjwith respect to the nonnegative coefficient λj(λj≥0), that is(11)w=∑j=1nλjyjϕ(xj).Integrating the above weight vectorwinto Eq. (10), we have(12)minC1∑yi=−1αi+C2∑yi=1αi−∑i=1nβisubject to∑j=1nλjyjϕ(xj)Tϕ(xi)−b=yi(βi−αi),0≤λj≤C1,foryj=−1,0≤λj≤C2,foryj=1,αi≥0,βi≥0,i=1,…,nIn the case the dot product (ϕ(xj)Tϕ(xi)) of basic functions is replaced by the kernel function K(xj,xi), we get the classification model with kernel and penalty factors as below(13)minC1∑yi=−1αi+C2∑yi=1αi−∑i=1nβisubject to∑j=1nλjyjK(xj,xi)−b=yi(βi−αi),0≤λj≤C1,foryj=−1,0≤λj≤C2,foryj=1,αi≥0,βi≥0,i=1,…,nBesides, due to the limitations of the MCOC approach for processing uncertain data aforementioned, that is, MCOC is very sensitive to noise and anomalies. KFP-MCOC was built by introducing an appropriate fuzzy membership function based on the distance between the input point and its class representative point so as to smooth the effects of noise and anomalies in the Chinese semantic word-formation corpus. According to the fuzzification method in the literature (Zhang et al., 2009, 2014b,c), we use the mean of class as a representative point. For a two-class classification problem and the given data setT2, ifφ¯yiis define as the mean of class yi, we write it as(14)φ¯yi=∑iyiϕ(xi)∑iyi,yi∈{−1,1}.where i=1, …, m for yi=−1, and i=m+1, …, n for yi=1.Then the radius of class yiin the kernel-induced feature space is denoted as(15)ryi=max||ϕ(xi)−φ¯yi||2=max{d(ϕ(xi),φ¯yi)}.whered(ϕ(xi),φ¯yi)is the distance between the input pointxiand its class meanφ¯yi.Let the fuzzy membership degree tiof each input point be the linear function of the mean and radius of one class, we have(16)ti=1−d(ϕ(xi),φ¯yi)(ryi+δ).where δ (δ>0) is a sufficiently small constant and used to avoid dividing by zero. The distanced(ϕ(xi),φ¯yi)between the point ϕ(xi) and its class meanφ¯yiusing the kernel method is written as(17)d(ϕ(xi),φ¯yi)=||ϕ(xi)−φ¯yi||2=K(xi,xi)−2∑jyjK(xi,xj)/∑jyj+∑j∑kyjykK(xj,xk)/∑jyj∑kyk1/2where j, k=1, …, m for yi=−1, and j, k=m+1, …, n for yi=1.Thus we obtain the new training setT3={(ϕ(x1), y1, t1), …, (ϕ(xn), yn, tn)} with a fuzzy membership degree ti(τ<ti≤1), where the input pointxi∈Rdis partitioned by a target variable yi∈{−1, 1}. From Eq. (16) we know that the input pointxiwith a lower timay be considered as noise, outlier, anomaly and unimportant one, that is ti≤τ, where τ is a predefined threshold (τ>0), otherwise the input pointxiis regarded as important one. Besides, the parameter αi(αi≥0) is a measure of the misclassified input point in Eq. (13), so the term tiαiis a measure of error with different weight. By this way, the effects of noise, outliers and anomalies in data are reduced remarkably, at the same time, and the stability of MCOC is improved considerably. Thus the KFP-MCO classifier is expressed as(18)minC1∑yi=−1tiαi+C2∑yi=1tiαi−∑i=1nβisubject to∑j=1nλjyjK(xj,xi)−b=yi(βi−αi),0≤λj≤C1,foryj=−1,0≤λj≤C2,foryj=1,αi≥0,βi≥0,i=1,…,nBy solving Eq. (18), we can obtain the coefficient λj(j=1, …, n). Integrating the coefficient λjinto Eq. (11), we can calculate the weight vectorw. For all training pointsxi(i=1, …, n) which satisfy αi=0 or βi>0, according to the decision hyperplanewTϕ(xi)=b, we haveb=∑j=1nλjyjK(xj,xi), then an average b ofbis taken. Besides, the decision function of KFP-MCOC for a new input pointxis denoted as(19)f(x)=sign(wTϕ(x)−b)=sign∑j=1nλjyjK(xj,x)−bFinally, in this paper the similarity kernel using RBF in Eq. (2) is chosen for a nonlinear mapping in KFP-MCOC. For a multi-class classification problem, we can transform it into multiple one-vs-one or one-vs-rest binary classification problems.In this section based on the definitions and formulations of the semantic graphs in Section 2 and the above description of the SVM and KFP-MCO classifiers in Section 3, the overall process of classification algorithm for prediction of Chinese semantic word-formation patterns can be summarized into the following steps.Algorithm 1Classification algorithm of Chinese word-formations based on semantic graphInput:The training set Trof the layered semantic graphs of word-formations with class labels, the independent test set Ts and the layered weight parameterwfor different semantic layers.Output:The predictive patterns of Chinese semantic word-formations.Step 1:Selecting graph prototypes pi(1≤i≤n) randomly from the training set Tr.Step 2:Computing the distance d(g, pi) between any graph g from the training set Tr and the prototype graph piaccording to Eq. (1). Then the normalized vectorγ(g)=(d(g,p1)/n,…,d(g,pn)/n)are obtained, which forms a new training set Tr.Step 3:For KFP-MCOC computing the fuzzy membership degree tifor each γ(g) with the new training set Tr′ (see Eq. (16)).Step 4:Turning the graph edit distance into the similarity kernel with the training set Tr′ (see Eq. (2)).Step 5:Solving Eqs. (7) and (18) respectively and getting the optimal solution αifor SVM and λjfor FKP-MCOC based on the training set Tr′.Step 6:Constructing decision functions with αiand λjfor the SVM and KFP-MCO classifiers respectively (see Eqs. (8) and (19)).Step 7:Calculating the distance d(g′, pi) between any graph g′ from the test set Ts and the prototype graph pi, according to Eq. (1). Similarly, the distances are normalized again.Step 8:Turning the graph edit distance into the similarity kernel with the test set Ts (see Eq. (2)).Step 9:Predicting the patterns of an unknown graph g′ on the test set by the above decision function (see Eqs. (8) and (19)).From the illustration in Algorithm 1 we know that the representative prototype graph set is randomly selected from the training set then the dissimilarity measure is computed, that is, the distance between a semantic graph from the training set and the prototype graph from the prototype set can be obtained. After the distance array is normalized and embedded into a new vector space, the new training set is formed. Therefore, by using the similarity kernel the SVM and FKP-MCOC models can be trained on the new training set and the obtained classifiers will be used for the prediction of Chinese semantic word-formation patterns on the dependent test sets.In this section the SVM and KFP-MCO classifiers with the RBF kernel are employed to predict Chinese semantic word-formation patterns, including data sets, experiment design, experimental results and analysis in the following subsections.In our experiment, database is sourced from the Chinese semantic word-formation corpus constructed by using three categories of the influential factors in Section 2, which encompasses different types of semantic graphs. These semantic graphs are obtained by using computer programs to automatically label the real and large-scale Chinese compound word corpus and then the carefully manual check and correction is conducted. The resulting database contains 57,046 semantic graphs of disyllabic words with 8 classes of word-formation patterns as the output of classification. Each semantic graph represents the three types of senses and two types of POS, that is, the type of word sense, the type of the first morpheme sense, the type of the second morpheme sense and the types of POSs of the first and second morphemes. Then each sense type is composed of three semantic layers: the big class, the medium class and the small class while POS is composed of the big class and the small class. Besides, each semantic graph is classified as one of the predefined patterns, the types of patterns and their distributions are listed in Table 2.As far as the meaning of each type of patterns is concerned, the word sense of pattern 1 is composed of the two similar morpheme senses so that its word sense can be explained as one of morpheme senses. For pattern 2 the sense of compound word holds only the first morpheme sense, whereas for pattern 3 the sense of compound word is explained as the second morpheme sense. The two morpheme senses are merged into a new word sense of the compound word for pattern 4 while adding the two morpheme senses together can gain the meaning of the compound word in pattern 5. Extra sense is added to word sense of pattern 5 can get the word sense of pattern 6. The second morpheme sense is changed into a new meaning while the first remains the same for pattern 7. Conversely, for pattern 8 the first morpheme sense is turned into a new one.Besides, from Table 2 we find that class distributions are highly imbalanced. Owing to some potential errors in the process of automatically labeling and manually checking the sense types of compound words, the database may contain potential noise, anomalies and missing values.For Chinese semantic word-formation analysis, the experiment is actually a multi-class classification problem, so it can be transformed into multiple paired two-class classification problems. Then we randomly and respectively select 250 positives and the same number of negatives from any two patterns as the training set and the remaining instances are used for the independent test set.Then we randomly select 50, 70, 90, 110, 130, 150, 170, 190, 210, 230, 250 positives and the same number of negatives from each training set as the prototype sets, and the layer-weighted GED between each semantic graph in the training set and the prototype set is computed, thus the new training sets with different number of instances are obtained. Similarly, the new independent test sets can be gained by calculating the layer-weighted GED between each semantic graph in the dependent set and the prototype set.The 5-fold cross-validation method is used to train FKP-MCOC and SVM on the training sets with different prototypes in size by using Algorithm 1 respectively, and the obtained classifiers are tested on the independent test sets. After we choose the best classifier group from different prototypes in size, the averages of predictive performance are calculated and reported.In our experiments four accuracies are used to evaluate the performance of the classifiers: total accuracy, Type I accuracy, Type II accuracy and F1 score. These measures are defined as:(i)the total accuracy (the total classification accuracy):(20)Total accuracy=(TP+TN)/(TP+FN+TN+FP).the identification rate of positives (the predictive accuracy of target pattern, also called as recall):(21)Type I accuracy=TP/(TP+FN).the identification rate of the negatives (the predictive accuracy of non-target pattern):(22)Type II accuracy=TN/(TN+FP).F1 score (the mixed measure of sensitivity and precision):(23)F1score=2TP/(2TP+FN+FP).True positive (TP) is the number of positives that are correctly predicted as positives. False negative (FN) is the number of positives that are predicted as negatives. True negative (TN) is the number of negatives that are correctly predicted as negatives. False positive (FP) is the number of negatives that are predicted as positives. The F1 score is the harmonic mean of recall and precision (it is defined as TP/(TP+FP)) and it takes into account the effects of the two quantities.Finally, all experiments are carried out on Matlab 8.1 platform. The linear programming problem of KFP-MCOC and the convex quadratic programming of SVM are solved by utilizing Matlab optimization tools.On the training sets with different prototypes in size, the algorithms of SVM and KFP-MCOC based on layer-weighted semantic graphs are trained using 5-fold cross-validation method, and the corresponding classifiers are tested on validation sets respectively. The grid search method is used to determine the best parameters of classifiers with regard to the best accuracy. Then the group of classifiers with the best performance is tested on the independent test sets. Experimental results and comparisons with different methods are separately reported, including total accuracy, type I accuracy, type II accuracy and F1 score defined in Eqs. (20)–(23).In the process of training classifiers the parameters for SVM and KFP-MCOC are chosen from the specific discrete set so as to get the best classification accuracy. For the purpose of using the grid search method, the different parametric sets are set to: for the penalty factor C for SVM, the penalty factors C1 and C2 for KFP-MCOC from the set {100, 200, 500, 1000, 2000, 5000, 10,000, 20,000, 50,000, 100,000}; the kernel parameter σ from the set {0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10,000} for the RBF kernel. Thus the performance evaluations of SVM and KFP-MCOC for predicting Chinese semantic word-formation patterns are shown in Figs. 3–10respectively.From the results demonstrated in Fig. 3, KFP-MCOC has better predictive performance of total accuracy, type I accuracy and F1 score than SVM. Specifically, for the identification of pattern 1 from other patterns KFP-MCOC has the highest averages for total accuracy (91.19%), type I accuracy (90.97%), type II accuracy (90.32%) and F1 score (0.90) while SVM has averages for total accuracy (87.85%), type I accuracy (87.41%), type II accuracy (90.22%) and F1 score (0.86).From the results shown in Fig. 4, KFP-MCOC slightly outperforms SVM for prediction of pattern 2. To be specific, for the identification of pattern 2 from other patterns KFP-MCOC has the highest averages for total accuracy (90.93%), type I accuracy (89.68%), type II accuracy (91.76%) and F1 score (0.81) while SVM has averages for total accuracy (85.92%), type I accuracy (83.31%), type II accuracy (89.37%) and F1 score (0.74).From the results illustrated in Fig. 5, KFP-MCOC totally outperforms SVM for prediction of pattern 3. Specifically, for the identification of pattern 3 from other patterns KFP-MCOC has the highest averages for total accuracy (83.37%), type I accuracy (84.21%), type II accuracy (83.36%) and F1 score (0.31) while SVM has averages for total accuracy (81.38%), type I accuracy (81.23%), type II accuracy (81.42%) and F1 score (0.28).From the results shown in Fig. 6, KFP-MCOC has better predictive performance of type I accuracy for pattern 4 than SVM. For the identification of pattern 4 from other patterns KFP-MCOC has the highest averages for total accuracy (87.97%), type I accuracy (88.40%) and F1 score (0.85) while SVM has averages for total accuracy (86.65%), type I accuracy (84.38%) and F1 score (0.84).As the results illustrated in Fig. 7, the total and type I accuracies of KFP-MCOC are better than that of SVM. For the separation of pattern 5 from other patterns KFP-MCOC has the highest averages for total accuracy (85.25%), type I accuracy (86.05%) and F1 score (0.90) while SVM has averages for total accuracy (81.16%), type I accuracy (80.61%) and F1 score (0.88).As the results demonstrated in Fig. 8, KFP-MCOC outperforms SVM except for type II accuracy. Specifically, for the separation of pattern 6 from other patterns KFP-MCOC has the highest averages for total accuracy (91.19%), type I accuracy (90.26%) and F1 score (0.93) while SVM has averages for total accuracy (87.22%), type I accuracy (85.87%) and F1 score (0.91).From the results shown in Fig. 9, type I accuracy of KFP-MCOC is obviously better than that of SVM. For the separation of pattern 7 from other patterns KFP-MCOC has the highest averages for total accuracy (90.65%), type I accuracy (90.89%), type II accuracy (89.92%) and F1 score (0.83) while SVM has averages for total accuracy (88.92%), type I accuracy (85.79%), type II accuracy (88.83%) and F1 score (0.83).As the experimental results shown in Fig. 10, KFP-MCOC outperforms SVM for prediction of semantic word-formation pattern 8. For the separation of pattern 8 from other patterns KFP-MCOC has the highest averages for total accuracy (90.23%), type I accuracy (89.86%) and F1 score (0.81) while SVM has averages for total accuracy (88.38%), type I accuracy (83.42%) and F1 score (0.79).Generally speaking, we find that by using the RBF kernel SVM and KFP-MCOC based on the layer-weighted semantic graphs the two classifiers achieved the good predictive performance of Chinese semantic word-formation patterns. At the same time, according to the above analysis the classification accuracy of KFP-MCOC is slightly better than that of SVM. Besides, owing to the majority-class instances in the highly class-imbalanced case SVM has better the type II accuracy than that of KFP-MCOC, for instance, for pattern 4, 5, 6 and 8. To sum up, for the prediction of Chinese semantic word-formation patterns, we find that our proposed KFP-MCOC based on layer-weighted semantic graphs has the better performance in efficiency, flexibility, separation and generalization than that of SVM, which can be seen from the experimental results and the comparison analysis.

@&#CONCLUSIONS@&#
In this paper, we proposed a classification model and the corresponding algorithm based on layer-weighted semantic graphs for Chinese word-formation analysis. We find that the KFP-MCO classifier achieves the good predictive accuracy by efficiently solving linear programming problem, and it is characterized by: firstly the layer-weighted GEDs of semantic word-formation patterns are computed and embedded into a new vector space, kernel function can transform a nonlinearly separable problem into a linearly separable one, introducing a fuzzy membership to each input point the classifier can reduce the effects of noise, outliers and anomalies in data and the class-imbalanced penalty factors are used to trade off overfitting majority-class and underfitting minority-class. On the real world database, KFP-MCOC based on layer-weighted semantic graphs was tested and analyzed, and experimental results show that it can effectively predict Chinese semantic word-formation patterns. In future, we attempt to classify and predict Chinese word-formation patterns of the polysyllabic or mixed dissyllabic and polysyllabic words by using the effective approaches based on the layer-weighted semantic graphs so as to further improve the accuracy and efficiency of the pattern classification.