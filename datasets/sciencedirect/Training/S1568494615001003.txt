@&#MAIN-TITLE@&#
GPU accelerated training of image convolution filter weights using genetic algorithms

@&#HIGHLIGHTS@&#
This paper proposes a fast algorithm for training image filter using GPU.Parallelization of genetic algorithms is realized by master–slave method.Sub-image based (SBM) method is proposed to use the GPU efficiently.SBM is developed by discussing other alternative design considerations.Experimental results show about 50× to 90× acceleration using GeForce GTX 660.

@&#KEYPHRASES@&#
GPU computing,CUDA,Genetic algorithms,Image processing,Convolution filter,

@&#ABSTRACT@&#
Genetic algorithms (GA) provide an efficient method for training filters to find proper weights using a fitness function where the input signal is filtered and compared with the desired output. In the case of image processing applications, the high computational cost of the fitness function that is evaluated repeatedly can cause training time to be relatively long. In this study, a new algorithm, called sub-image blocks based on graphical processing units (GPU), is developed to accelerate the training of mask weights using GA. The method is developed by discussing other alternative design considerations, including direct method (DM), population-based method (PBM), block-based method (BBM), and sub-images-based method (SBM). A comparative performance evaluation of the introduced methods is presented using sequential and other GPUs. Among the discussed designs, SBM provides the best performance by taking advantage of the block shared and thread local memories in GPU. According to execution duration and comparative acceleration graphs, SBM provides approximately 55–90 times more acceleration using GeForce GTX 660 over sequential implementation on a 3.5GHz processor.

@&#INTRODUCTION@&#
In image processing applications that use convolution filters, weights determine frequency response characteristics, and therefore, the effect of filtering operations on an output image. Calculating the required image filter weights has been of interest in many image-processing applications, such as noise elimination, blur filtering, and edge detection. In addition to analytical methods, bio-inspired methods are applied widely to train filter parameters using the desired image [1–5]. Image filtering applications usually involve a large number of computing operations, especially in the case of weights training applications for determining the desired parameters. Parallel implementations provide a good means of accelerating numerical algorithms, and find widespread utilization in image processing applications [6–9]. Presently, GPU technology provides good hardware for realizing parallel algorithms to accelerate various intense computing power-demanding numerical methods, including image-processing applications. GPU-based parallel implementations for accelerating GA [10–14] and their applications in scientific and engineering applications are of interest among researchers [15–17].In the presented study, a new GPU-based method is developed to accelerate training image filter weights using GA. Method development is realized by discussing other alternative GPU implementation approaches in detail. Because evaluating the fitness function requires a relatively long time when compared with other GA functions, parallel genetic algorithm (PGA) is realized through a master–slave GA with single-population [18,19]. For this purpose, the fitness values are determined using GPU to filter the input image rapidly using all individuals in the population that represents the filter mask weights. Then, the mean absolute error (MAE) between the original and filtered images is returned as a fitness value to GA for each set of mask weights data in the population. This operation can be realized by calling the kernel separately for each set of weights data in the population or by calling the kernel one time for the entire population. These are realized as direct and population-based methods in the presented paper. For DM, the fitness value for a mask weight in the population is computed on GPU. The sequential fitness function can simply be replaced with the GPU-based fitness function. According to this approach, the fitness values of the entire population are computed separately for each population member. However, this requires calling the GPU kernel a number of times equivalent to population size, which causes significant overhead on computing duration. Because of the repetitive calls to the kernel, GPU utilization is inefficient. PBM computes all fitness values individually on GPU, thus significantly reducing the overhead of calling the GPU kernel. Calculating all population for a pixel individually increases locality through use of the local memory in compute unified device architecture (CUDA) threads. Another population-based approach discussed in the paper is BBM, which utilizes block-shared memory by allocating a mask of pixel size. All threads within the block filter the same pixel in order to compute MAE for each set of weights data in the population. The most effective approach, called SBM, allocates a sub-block of image data instead of a mask size of image data in the shared memory in order to filter a block of pixels. In addition, SBM takes advantage of local memory for storing a selected mask weight from the population that is used repeatedly according to sub-block size.The performance of the developed methods is measured by comparing with the sequential results. Execution times and acceleration gains are obtained for various image, filter mask, and population sizes. GPU algorithms are realized using CUDA, which is provided by NVidia(R) to programmers in order to develop parallel algorithms that use the computing power of NVidia(R) GPUs. The rest of the manuscript is organized as follows. In the following section, we describe the problem and provide brief introductions to GA and CUDA. In Section 3, the proposed method and alternative programming approaches are discussed in detail. In Section 4, comparative experiment results that use the developed method and other alternatives are given. In Section 5, we provide brief conclusions regarding the results.

@&#CONCLUSIONS@&#
GPU-based implementation was shown to be extremely useful in accelerating GA-based optimization of mask weights. Sequential execution of the GA-based image mask training is a computationally heavy operation. In particular, evaluating the fitness function because of image filtering operations for each mask data in the population requires the most computation time. For this purpose, GPU-based acceleration provides a good method for obtaining reductions in computation duration. Compared with CPU architecture, GPU has a large number of thread blocks that can be utilized to realize general purpose computing operations. The methods introduced for GPU-based computation of the fitness function provided significant acceleration when compared to sequential execution. In the experiments, the sequential and GPU-based algorithms DM, PBM, BBM, and SBM were compared. DM, where there is a call to the GPU kernel for each set of mask data in the population in order to compute fitness values, resulted in significant overhead, as shown by the experiment results. Although GPU acceleration using DM showed poor results, GPU acceleration using the other population-based approaches, where the fitness values are computed in one kernel call, showed significant reduction in computing duration. PBM simply filters the same pixel for all mask data in the population. During this operation, a mask size of image data used in the filtering is maintained in the thread memory. For BBM, a mask size of image data is maintained in the shared memory and all the threads in the block filter the same pixel using a mask data in the population. Although this approach performed better acceleration than PBM, SBM, developed in the paper, performed the best acceleration. SBM provides a significantly efficient method by taking advantage of the block shared and thread local memories in GPU. SBM-based GPU acceleration of GA-based image mask training provided approximately 55–90 times more acceleration over sequential implementation using 3.5GHz CPU and GeForce GTX 660 GPU.