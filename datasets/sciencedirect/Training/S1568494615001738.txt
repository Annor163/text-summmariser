@&#MAIN-TITLE@&#
Bee colony optimization for the satisfiability problem in probabilistic logic

@&#HIGHLIGHTS@&#
Solving the satisfiability problem in the logic with approximate conditional probability.Using algorithm based on the bee colony optimization meta-heuristic.Comparing BCO method with Fourier–Motzkin elimination procedures.

@&#KEYPHRASES@&#
Probabilistic satisfiability,Conditional probability,Approximate probability,Meta-heuristics,Swarm intelligence,

@&#ABSTRACT@&#
This paper presents the first heuristic method for solving the satisfiability problem in the logic with approximate conditional probabilities. This logic is very suitable for representing and reasoning with uncertain knowledge and for modeling default reasoning. The solution space consists of variables, which are arrays of 0 and 1 and the associated probabilities. These probabilities belong to a recursive non-Archimedean Hardy field which contains all rational functions of a fixed positive infinitesimal. Our method is based on the bee colony optimizationmeta-heuristic. The proposed procedure chooses variables from the solution space and determines their probabilities combining some other fast heuristics for solving the obtained linear system of inequalities. Experimental evaluation shows a high percentage of success in proving the satisfiability of randomly generated formulas. We have also showed great advantage in using a heuristic approach compared to standard linear solver.

@&#INTRODUCTION@&#
Working with uncertain knowledge has been well documented problem in mathematical logic and computer science, since the first works of Leibnitz and Bool. The unique solution to this problem has not been found yet, but there are many ideas and different variants of solutions for particular problems, that are used in artificial intelligence. Many of the formalisms for representing and reasoning with uncertainty are based on probabilistic logics [1–10]. These logics are extensions of classical logic with probabilistic operators. Satisfiability problem in these logics (PSAT) can be reduced to linear programming problem. However, solving it by any standard linear solver is inapplicable in practice due to the complexity of the problem. For example, the application of Fourier–Motzkin elimination procedure yields the exponential growth in the number of inequalities in the system. Therefore, the application of some other techniques for solving this problem, such as different types of meta-heuristics, could prove very useful.Using meta-heuristics for solving satisfiability problems is not a new idea. The most interesting problems in propositional logic are satisfiability problem (SAT) and maximum satisfiability problem (MAX-SAT), i.e., the problem of determining the maximum number of clauses of a given Boolean formula in conjunctive normal form, that can be made true by an assignment of truth values to the variables. Several methods based on different heuristics have been developed for SAT and MAX-SAT. Many of those methods are based on local search procedure and some of them are presented in [11–14]. Heuristics based on swarm intelligence, like Ant Colony Optimization (ACO) or Bee Swarm Optimization (BSO), were also applied to SAT [15,16]. For this type of problem probabilistic approach is presented in [17]. Genetic Algorithm (GA) is another approach used for dealing with SAT and/or MAX-SAT [18,19], and it is also combined with some other heuristics [20]. In probabilistic logics presented by Nilson in [21], Fagin et al. [1] or by Rašković et al. [2], local search based heuristics [22], Tabu Search (TS) [23], GA [24,25], Variable Neighbourhood Search (VNS) [26], and combination of GA and VNS [27] were used for solving PSAT.Here, we discus the satisfiability problem in approximate conditional probabilities logic described by Rašković et al. in [4]. We denote this version of satisfiability problem with CPSAT-ɛ. The main differences between PSAT and CPSAT-ɛ are:•CPSAT-ɛ involves conditional probability operator on the contrary to PSAT.Probabilities of formulas in CPSAT-ɛ may take infinitesimal values, and not only real-values as in PSAT.The logic, described in [4], enriches the propositional calculus with probabilistic operators which are applied to propositional formulas: CP⩾s(α, β), CP⩽s(α, β) and CP≈s(α, β), with the intended meaning that the conditional probability of α given β is “at least s”, “at most s” and “approximately s”, respectively. This way of knowledge representation and reasoning can be widely used. One of the most obvious examples would be the process of reaching diagnosis in medicine. Occurrence of some symptoms with adequate certainty represented in percents, leads to conclusion that a specific disease is in question. In addition, in [4] is presented a method for modelling default reasoning with logic with conditional probabilities approximately close to 1. That type of non-monotonic reasoning supports reasoning with incomplete information and can be used in many areas such as medicine, legal reasoning, regulations, specifications of systems and software, etc.To the best of our knowledge, CPSAT-ɛ for the logic described in [4] has no practically usable automated solver, and our main effort was to apply meta-heuristics, in particular Bee Colony Optimization (BCO), for solving CPSAT-ɛ in this logic. BCO is a meta-heuristic technique, which showed very good performance in solving hard optimization problems [29–37]. It is a stochastic, random-search technique that belongs to the class of population-based algorithms. This technique is inspired by the nectar collection process of honey bees from nature. Until now, BCO has not been applied to a class of problems involving search for a solution, only to the problems that already have some feasible solutions and one wants to improve them. The first application of BCO to SAT-type problems was not the only reason for selecting this method to deal with CPSAT-ɛ. Our experience with other meta-heuristics shows that better performance is achieved with population based methods since they allow the degradation in solution quality. Local search based methods are easily trapped in neighborhoods of the current best solution that may not lead to the global best, i.e., to the solution that satisfies the considered formula. On the other hand, evolutionary methods involve more randomness that may diversify the search and require more time to get to the desired final solution.To address CPSAT-ɛ we used the improved variant of BCO, denoted by BCOi and proposed in [38]. The original BCO method is constructive: each bee starts from an empty solution and builds feasible solution through the algorithm steps belonging to the single iteration. Contrary to the constructive version, at the beginning of BCOi iteration some complete solutions are assigned to the bees. The role of each bee is to modify the assigned solution with an aim to improve its quality. Since the modification rules are highly problem dependent, the BCOi concept proposed in [38] needed significant adjustment to be applicable to CPSAT-ɛ problem. Constructive BCO in CPSAT-ɛ is not applicable since it is not possible to estimate the quality of final solution based on the partial solutions, i.e., truth values and probabilities assigned to a subset of variables. Our procedure begins with generating random potential solutions for each bee, and in the next steps it tries to improve them, using some additional methods and stochastic moves based on a roulette wheel, in an attempt to produce the real solution. The experimental results obtained by BCOi for CPSAT-ɛ were compared with those obtained using Fourier–Motzkin elimination procedure and thus demonstrated the superiority of BCOi method.The rest of the paper is organized as follows. Section 2 gives a brief description of logic with approximate conditional probability. Section 3 briefly outlines the BCO algorithm, while Section 4 describes our implementation of BCOi for solving CPSAT-ɛ. Section 5 contains some experimental results. Section 6 is devoted to the conclusions.In this section, we present the brief formal introduction to the CPSAT-ɛ (for a more detailed description see [4]).The Hardy field Q[ɛ] is a recursive non-Archimedean field which contains all rational functions of a fixed positive infinitesimal ɛ which belongs to a nonstandard elementary extension *R of the standard real numbers [39,28]. An element ɛ of *R is an infinitesimal if|ɛ|<1nfor every natural number n. Some examples of infinitesimal are (in ascending order, if ɛ>0): ɛ3+ɛ4, ɛ2−5ɛ6,ɛ100, 85ɛ, or negative infinitesimals: −ɛ, −ɛ2, … Field Q[ɛ] contains all rational numbers. Let S be the unit interval of the Q[ɛ] and Q[0, 1] denote the set of rational numbers from [0, 1].The language of the logic with approximate conditional probability consists of: a countable set Var={p, q, r, … } of propositional letters, the classical connectives ¬, and ∧, and binary probabilistic operators(CP⩽s)s∈S,(CP⩾s)s∈S, and(CP≈r)r∈Q[0,1]. The set ForCof classical propositional formulas is defined as usual. The setForPSof probabilistic propositional formulas is the smallest set Y containing all formulas of the forms:•CP⩾s(α, β) for α, β∈ForC, s∈S,CP⩽s(α, β) for α, β∈ForC, s∈S andCP≈r(α, β) for α, β∈ForC, r∈Q[0, 1],CP<s(α, β) denotes ¬CP⩾s(α, β) for α, β∈ForC, s∈S,CP>s(α, β) denotes ¬CP⩽s(α, β) for α, β∈ForC, s∈S,CP=s(α, β) denotes CP⩾s(α, β)∧CP⩽s(α, β) for α, β∈ForC, s∈S.We can perform some easy transformations which will reduce the satisfiability problem to checking probabilistic formulas of simpler form. Let A be a probabilistic formula and p1, …, pnbe the list of all propositional letters from A. An atom a of A is a formula ±p1∧…∧±pn. Note that all pairs of different atoms are mutually exclusive. We use At(A) to denote the set of all atoms from A, and n to denote the number of propositional letters from A. Obviously, |At(A)|=2n.Using propositional reasoning it is easy to show that every probabilistic formula A is equivalent to a formula:DNF(A)=⋁i=1m⋀j=1ki±Xi,j(p1,…,pn)called a disjunctive normal form of A, where:•Xi,j∈{CP⩾s,CP⩽s}s∈S∪{CP≈r}r∈Q[0,1],Xi,j(p1, …, pn) denotes that propositional formulas which are in the scope of the probabilistic operator Xi,jare in the complete disjunctive normal form, i.e., propositional formulas are disjunctions of the atoms of A.Let μ:At(A)→S be a probability measure. We introduce the following abbreviations:•xidenotes the measure of the atom ai∈At(A), i=1, …, 2n,ai⊨α means that the atom aiappears in the complete disjunctive normal form of a classical propositional formula α,∑(α) denotes∑ai∈At(A):ai⊨αxi, andC∑(α, β) denotes∑(α∧β)∑(β).Now, we can easily define that the formula A is satisfiable if the following holds:1.if A∈ForCit is satisfiable if (∃ai∈At(A))ai⊨A,if A has a form CP⩽s(α, β) it is satisfiable if either ∑(β)=0 and s=1 or ∑(β)>0 and C∑(α, β)⩽s,if A has a form CP⩾s(α, β) it is satisfiable if either ∑(β)=0 or ∑(β)>0 and C∑(α, β)⩾s,if A has a form CP≈r(α, β) it is satisfiable if either ∑(β)=0 and r=1 or ∑(β)>0 and for every positive integer n,C∑(α,β)∈[max(0,r−1n),min(1,r+1n)],¬A is satisfiable if A is not satisfiable,A∧B is satisfiable if A is satisfiable and B is satisfiable.Therefore, for every conditional probabilistic formula (±CP⩾s(α, β), ±CP⩽s(α, β), and ±CP≈r(α, β)) from DNF(A) we can distinguish two cases:1.the probability of β is zero, in which case•CP⩾s(α, β), for s∈S, ¬CP⩽s(α, β), for s∈S\{1}, CP⩽1(α, β), and CP≈1(α, β) hold – and can be deleted from DNF(A), while¬CP⩾s(α, β), for s∈S, CP⩽s(α, β), CP≈r(α, β), for s∈S\{1}, r∈Q[0, 1]\{1} ¬CP⩽1(α, β), and ¬CP≈1(α, β) do not hold – and the whole conjunction is not satisfiable,the probability of β is greater than zero.As a consequence, to prove satisfiability of a formula in this logic it is enough to prove satisfiability of formulas which are conjunctions of conditional probabilistic formulas of the forms: ±CP⩾s(α, β), ±CP⩽s(α, β), and ±CP≈r(α, β), such that the probability of β is greater than 0.Rašković et al. showed in [4] how to reduce the satisfiability problem to linear programming problem, using the reduction procedure we will now present. For obtaining linear system of inequalities from a probabilistic formula A it is necessary to eliminate ≈ and ≉ signs.Let us consider a formula A of the form:(1)(∧i=1,I±CP⩾si(αi,βi))∧(∧j=1,J±CP⩽sj(αj,βj))∧(∧k=1,K±CP≈rk(αk,βk)).Then, A is satisfiable iff the following system is satisfiable:∑i2nxi=1xi≥0 fori=1, 2n∑(β)>0  for every formula β appearing in the formulas of the form ±CP♢(α, β) from A and ♢∈{⩾si, ⩽sj, ≈rk}C∑(αi, βi)⩾sifor every formulaCP⩾si(αi,βi)from AC∑(αi, βi)<sifor every formula¬CP⩾si(αi,βi)from AC∑(αj, βj)⩽sjfor every formulaCP⩽sj(αj,βj)from AC∑(αj, βj)>sjfor every formula¬CP⩽sj(αj,βj)from AC∑(αk, βk)≈rkfor every formulaCP≈rk(αk,βk)from AC∑(αk, βk)≉rkfor every formula¬CP≈rk(αk,βk)from AThe first two rows in the system say that the sum of probabilities of all atoms is 1, while the probability of each atom is nonnegative.The above system can be further simplified by observing that every expression of the form C∑(αk, βk)≈rkcan be seen as(2)C∑(αk,βk)−rk≈0andC∑(αk,βk)−rk⩾0or(3)C∑(αk,βk)−rk≈0andrk−C∑(αk,βk)⩾0.Note that (2) corresponds to C∑(αk, βk)⩾rkand (3) to rk⩾C∑(αk, βk).Similarly, every expression of the form C∑(αk, βk)≉rkcan be seen as(4)C∑(αk,βk)−rk≉0andC∑(αk,βk)−rk>0or(5)C∑(αk,βk)−rk≉0andrk−C∑(αk,βk)>0.Thus, we will consider systems containing expressions of the forms (2–5) instead of C∑(αk, βk)≈rk, and C∑(αk, βk)≉rk, respectively. Let us useS(x→,ɛ)to denote a system of that form. Note that(6)C∑(αk,βk)−rk≈0andC∑(αk,βk)−rk⩾0is equivalentto(∃nk∈N)0⩽C∑(αk,βk)−rk<nk·ɛ,(7)C∑(αk,βk)−rk≈0andrk−C∑(αk,βk)⩾0is equivalentto(∃nk∈N)0⩾C∑(αk,βk)−rk>−nk·ɛ,(8)C∑(αk,βk)−rk≉0andC∑(αk,βk)−rk>0is equivalentto(∃nk∈N)C∑(αk,βk)−rk>1nk,(9)C∑(αk,βk)−rk≉0andrk−C∑(αk,βk)>0is equivalentto(∃nk∈N)C∑(αk,βk)−rk<−1nkSince we have only finitely many expressions of the forms (2–5) in our system, we can use a unique n0∈N instead of many nk's in expressions (6–9). We useS(x→,n0,ɛ)to denote the obtained system. Then,(10)S(x→,ɛ)has a solution inQ[ɛ]iffS(x→,n0,ɛ)has a solution inQ[ɛ],and, since Q[ɛ] is dense in *R, for every fixed and finite n0,(11)S(x→,n0,ɛ)has a solution inQ[ɛ]iffS(x→,n0,ɛ)has a solution in*R.As presented in [4], we can freely multiply (in)equalities by the denominators of the expressions of the form C∑(α, β). In that way we obtain linear (in)equalities of the form(12)∑(α∧β)−s∑(β)ρ0,where s is a rational function in ɛ, and ρ∈{⩾, >, <, ⩽}.Example 1Let us consider the formulaA=CP⩾0.36+3ɛ(p∨q,¬p∨q)∧CP≈0.5(p,p∨q)∧CP<0.9−0.3ɛ2(p,q). The set of atoms of A contains a1=p∧q, a2=p∧¬q, a3=¬p∧q, a4=¬p∧¬q. Let xidenotes the measure of atoms ai. The classical formulas of A are p∨q, ¬p∨q, p and q, while the sets of atoms satisfying them are:Then A is satisfiable iff the following system is satisfiable:x1+x2+x3+x4=1xi⩾0,i=1,2,3,4x1+x3+x4>0x1+x2+x3>0x1+x3>0(0.64−3ɛ)x1+(0.64−3ɛ)x3+(−0.36−3ɛ)x4⩾0(0.5−ɛ)x1+(0.5−ɛ)x2+(−0.5−ɛ)x3<0(0.5+ɛ)x1+(0.5+ɛ)x2+(−0.5+ɛ)x3>0(0.1+0.3ɛ2)x1+(−0.9+0.3ɛ2)x3<0For proving satisfiability it is enough to find one solution of the system, and this system is satisfiable for:x1=0.14+2ɛx2=0.24−3.6ɛ+0.6ɛ2x3=0.4−0.1ɛ−0.3ɛ2x4=0.2+1.7ɛ−0.3ɛ2Considering the above transformation procedure, the whole problem is reduced to a linear programming problem and can be described as follows:minz(x)s.t.∑i=12ncijxiρ0j=1,2,…L,ρ∈{⩾,>,<,⩽}∑i=12nxi=1xi⩾0i=1,2,…,2nwhere∑i=12ncijxiρ0are obtained by simplifying inequalities (12), L is a number of these inequalities and all values cij, xi(i=1, 2, …, 2n, j=1, 2, …, L) belong to Hardy field, i.e., they are rational functions depending on ɛ. The objective function z(x) is defined asz(x)=∑i=1Ldi(x),wheredi(x)=(∑i=12ncijxi)2if thejthinequality is not satisfied,0otherwise,In Section 5 we will consider details about the objective function again.BCO [29,30,40] is a nature-inspired meta-heuristic method that belongs to the class of Swarm Intelligence algorithms. It was inspired by a foraging behavior of honeybees, exploiting collaboration and knowledge exchange between bees during the nectar collection process. The basic idea of BCO, was to use a similarity between the way in which bees in nature look for a food [41], and the way in which optimization algorithms search for an optimum of the optimization problems [42]. In order to solve hard optimization problems, the initial constructive BCO was modified and a new concept, named BCOi, based on the improving of complete solutions was developed in [38]. Although BCO proved very suitable for solving non-standard optimization problems, like the ones containing inaccurate data [43,44] or involving optimization according to multiple criteria [45], it has never been applied to a class of satisfiability problems. Upon a short description of the BCO (BCOi) algorithm, we present its implementation for CPSAT-ɛ in the next section.The inspiration for the development of BCO came from the principles and habits used by bees in nature when collecting food [41]. Typically, the food collection process starts when some scout bees search the region in the vicinity of their hive. Completing the search, scout bees return to the hive and inform their hive-mates about the searching results. If they discover nectar, scout bees “advertise” food locations by a ritual called a “waggle dance”. It contains the information about the location, quantity and quality of available food sources with an aim to encourage (recruit) the remaining members of the colony to join the nectar collection process at that particular location.If a bee decides to leave the hive and collect nectar, it follows the directions of the selected dancing scout bee to the advertised food source. Upon arrival, the foraging bee takes a load of nectar and returns to the hive relinquishing the nectar to a food storer. Several scenarios are then possible for a foraging bee: (1) it can abandon the food location and take a role of an uncommitted follower; (2) it can continue the nectar collection process at the discovered location, without advertising it to the others; (3) it can try to recruit its (uncommitted) hive-mates with the dance ritual before continuing the nectar collection process. It is unclear how the above decisions are made, although it has been concluded that “the recruitment among bees is always a function of the quality of the food source” [41]. This is also the key argument in developing the BCO algorithm.The basic idea of designing BCO (BCOi) is to build the multi agent system (colony of artificial bees) to search for good solutions of various combinatorial optimization problems exploring the above mentioned principles from nature. The population of agents (artificial bees) consists of B individuals, each being responsible for one solution to the considered problem. Using collective knowledge and information sharing, artificial bees generate and/or improve their solutions.The BCO (BCOi) search is running in iterations until some predefined stopping criterion is satisfied. The iterations are composed of NC steps each containing two alternating phases (forward pass and backward pass). The forward pass is devoted to the construction or the improvement of solutions currently assigned to the bees, yielding the new set of solutions.The first action of the backward pass is the evaluation of the solutions’ quality. When all solutions are evaluated, each bee makes a stochastic decision whether to stay loyal or to abandon its current solution. The bees with better solutions have more chances to keep and advertise them during the backward pass. Unlike the bees in nature, artificial bees that are loyal to their partial/complete solutions are at the same time recruiters, i.e., their solutions would be considered by the uncommitted bees. The recruitment process is also stochastic in such a way that better among the advertised solutions have more chances to be chosen for further exploration. By this process, within each backward pass all bees are divided into two groups (R recruiters, and remaining B−R uncommitted bees). Values for R and B−R are changing from one backward pass to another one. When all NC steps are completed, the best among B solution is determined. It is used to update the current global best solution and an iteration of the BCO (BCOi) algorithm is competed. At this point all B solutions are deleted, and the new iteration is about to start. BCO (BCOi) runs iteration by iteration until a stopping condition is met. The possible stopping conditions could be, for example, the maximum total number of iterations, the maximum total number of iterations without the improvement of the objective function value, maximum allowed CPU time, etc. At the end, the best found solution (the so called global best) is reported to the user.The main advantage of the BCO (BCOi) algorithm is a small number of parameters. BCO (BCOi) has only two parameters:•B - The number of bees involved in the search.NC - The number of forward (backward) passes in a single iteration.The simplicity of the pseudo-code of the BCO (BCOi) algorithm is also a big advantage. This pseudo-code could be described in the following way:Do1.Initialization: a(n) (empty) solution is assigned to each bee.2.For (s=0;s<NC;s++) //count moves//forward pass(a)For (b=0;b<B;b++)(1) Evaluate all possible moves;(2) Choose one move using the roulette wheel.//backward pass(b)For (b=0;b<B;b++)Evaluate (partial/complete) solution for bee b;(c)For (b=0;b<B;b++)Loyalty decision using the roulette wheel for bee b;(d)For (b=0;b<B;b++)If (b is follower), choose a recruiter by the roulette wheel.3.Evaluate all solutions and find the best one.while stopping criterion is not satisfied.In Section 2 the method for obtaining system of inequalities for a given conjunction of probabilistic formulas is described. The CPSAT-ɛ reduces to a linear program over probabilities and the number of atoms with nonzero probability values necessary to guarantee a solution, if one exists, is equal to L+1, where L is number of inequalities in the system [46,1,26]. The solution is therefore an array containing L+1 atomsa=a1,a2,…,aL+1where ai, i=1, …L+1 are atoms from At(A), with assigned probabilitiesx=x1,x2,…,xL+1.The probabilities of atoms not in a are taken to be 0. If the solution variablea=a1,a2,…,aL+1is known, the system can be rewritten compactly as(13)∑j=1L+1cijxiρi0,i=1,…Lwhere cij(i=1, …, L, j=1, …, L+1) are coefficients and ρi∈{⩽, <, ⩾, >} (i=1, …, L).The BCOi implementation consists of the following four phases. The first phase is generation of initial solutions at the beginning of each iteration. The second phase is devoted to the solution modification. In the phase 3 results comparisons mechanism is performed while recruitment is a subject of phase 4. In the rest of this section we describe in detail all phases of our BCOi implementation.The iterations of the BCOi algorithm begin by generating and evaluating an initial solution for each bee. In our case it is performed, similarly as in [26], in the following way. First, each bee randomly generates 5×(L+1) atoms. To each atom equal probability, i.e., 1/(L+1), and a grade are assigned.The grade of an atom is computed as the sum of contributions that this particular atom makes to the satisfiability of inequalities. An atom ak, (k∈{1, …,L+1}) corresponds to a columnck=c1k,c2k,…,cLkTof the linear system (13). If a coefficient cikfrom the column is positive, and located in a row with ⩾ or > as the relation symbol, it can be used to push toward the satisfiability of this row. In such a case we add its value to the grade. The < and ⩽ cases are not in favor of the satisfying row, so we subtract the corresponding coefficient from the grade. Similar reasoning is applied when the coefficient is negative. Thus, we compute the grade of an atom akasgrade(ak)=∑i=1Lcik·sgn(i),with sgn(i) being the sign of the ith inequality of the systemsgn(i)=1,ifρ∈{⩾,>},−1,ifρ∈{⩽,<}.For each bee the initial solution is formed by selecting the (L+1) atoms with the best grade value from all generated atoms.The main step of BCOi is modification of the solutions through NC forward passes within the single iteration. For evaluating probability assignments, the objective function is defined as the distance between the left hand side of inequality in the linear system (13) and zero. Let a be the current solution, we define an unconstrained objective function z to be(14)z(x)=∑i=1Ldi(x),where diis the distance between the left hand side of inequality in the ith row and zero defined as(15)di(x)=(∑j=1L+1cijxj)2if theithrow is not satisfied,0otherwise,The value of the objective function is non-negative and our goal is to make it zero, under the probability constraints (xi's are non-negative and they sum up to 1). Note, that it is not enough for the objective function to become zero, because of the inequalities where as relation occur < and >, and the value of the left hand side for current solution is zero, so these inequalities are not satisfied.If solution is not found yet, we use a heuristic approach, in the same manner as in [26]. The heuristic optimization consists of the two independent heuristics.With this heuristic we try to “improve” the rows of the system (13) that are the most unsatisfied. We select five of the rows with the largest values di(x). The equations of these rows define the corresponding hyper-plane that border the solution space. In an attempt to reach the solution space, we are performing consecutive projections of probabilities on this hyper-plane. After all projections are performed the probabilities are normalized. Note that the projection of a pointx′=[x′1,x′2,…,x′L+1]onto a hyper-plane that defines the ith row∑j=1L+1cijxj=0is obtained by the formulax′j=xj−∑k=1L+1cikxk∑k=1L+1(cik)2Using this projection formula, the probabilities are changed towards satisfiability of each worst row in the direction normal to that hyper-plane. This procedure is repeated 5 times at most, as long as the solution improves, every time selecting the current 5 worst rows.System (13) is very sparse, i.e., very large percentage of the system coefficients are zeros, and we can try to improve the systems value by solving it “by hand”.Now, from the system we select the worst unsatisfied row (the ith) and the best satisfied row (the jth). Non-zero coefficients are then found in these two rows, but only those that have zero counterparts at the same position in the other row. For example if k1 and k2 is one pair of such coefficient positions then selected rows look like⋯+cik1·xk1+⋯+0·xk2+⋯ρi0⋯+0·xk1+⋯+cjk2·xk2+⋯ρj0Now the probabilitypk1can be changed with the ith row moving towards satisfiability, andpk2can be changed the opposite way, reducing satisfiability of the jth row. This probabilities giveaway is repeated for all the pairs of the coefficient positions from two selected rows. Since the system is very sparse, the change of probabilities does not affect much the satisfiability of other rows.These two heuristics combined together, Worst unsatisfied projection as the first one and Greedy giveaway as the second heuristic, yield a great improvement of the objective function value (14). They are applied for the first time immediately after generating the initial solution, and then in all forward passes. Each forward pass starts with adding the new L/5 atoms with assigned probability 1/(L+1) to a set of atoms which make current solution, then we keep in the solution L+1 atoms with the best grade. Finally, we re-use these heuristics to improve solutions.The evaluation and comparison of solutions associated to each bee is done after modifying solutions, at the beginning of each backward pass. The solutions are evaluated based on the objective function (14).Let us denote by zb(b=1, 2, …, B) value of penalty function for solution generated by bth bee. We normalize the penalty function zb, and denote by Obthis normalized value, i.e.,(16)Ob=zmax−zbzmax−zmin,Ob∈[0,1],b=1,2,…,Bwhere zmin and zmax are respectively minimum and maximum among all penalty functions produced by all bees.The probability that the bth bee (at the beginning of the new forward pass) is loyal to the previous solution does not depend on the problem, and therefore, as in [33], it is calculated in the following way:(17)pb=e−(Omax−Ob)/u,b=1,2,…,B,where u is the counter of the forward pass (taking values 1, 2, …, NC) and Omaxis the maximum among all Obvalues. Every artificial bee, using relation (17) and a random number generator, decides to continue exploring its own solution or to become an uncommitted follower.The probability that the solution of a recruiter b will be chosen by any uncommitted bee, is calculated in the following way [33]:(18)pb=Ob∑k=1ROk,b=1,2,…,Rwhere Okis the normalized objective function value of the kth advertised solution, while R denotes the number of recruiters. Every uncommitted follower chooses one recruiter, using relation (18) and a random number generator. This actually means that solution of the recruiter is copied to a solution of this uncommitted follower. From this point on, both bees are free to again independently search the solution space and apply different modification steps to the same solution.An iteration completes when the solution is found (i.e., all inequalities in the system are satisfied) or forward and backward passes are performed NC times. If solution is not found, data structures are cleaned and the new iteration can start. The BCOi runs iteration by iteration until a stopping condition is met. In this paper a stopping criterion is either of the two: the solution is found or maximum number of iterations is reached. Given this execution order, pseudo-code for the BCOi algorithm presented in Section 3, now can be described in the following way:BCOi(B, NC)Iter = 1; found = true;Do1. //Generating colonyFor (b=0;b<B;b++)(a) Randomly generate 5×(L+1) atoms and assign probability 1/(L+1) to each of them;(b) Grade all atoms and select L+1 with best grade;(c) Improve solution using Worst Unsatisfied Projection and Greedy Giveaway heuristics;(d) If (Solution is found) return found;2. For (s=0;s<NC;s++) //count moves//forward pass(a) For (b=0;b<B;b++)(1) Add new L/5 atoms with probability 1/(L+1) assigned;(2) Grade all atoms and select L+1 with best grade;(3) Improve solution using Worst Unsatisfied Projection and Greedy Giveaway heuristics;(4) If (Solution is found) return found;//backward pass(b) For (b=0;b<B;b++)Evaluate solution for bee b using (16);(c) For (b=0;b<B;b++)Loyalty decision using the roulette wheel and (17) for bee b;(d) For (b=0;b<B;b++)If (b is follower), choose a recruiter by the roulette wheel and (18);3. Iter ++;While (Iter ⩽ Itermax);return not(found)The results we obtained applying the algorithm described above to a set of randomly generated satisfiable formulas are presented in the next section.For testing purposes we randomly generate a set of 57 satisfiable formulas (all files with these formulas can be found on the http://imi.pmf.kg.ac.rs/tstojanovic/publications). We use N and M to denote the number of propositional letters, and the number of CP formulas in (1), respectively. Every propositional formula that appears as subformula of a CP formula is in the disjunctive normal form and has at most M conjunctions, where each conjunction has at most N/3 literals. Relations that appear in these formulas are from the set {⩾, >, ≈, ⩽, <}.We have, also, developed automatic solver for CPSAT-ɛ based on Fourier–Motzkin elimination method. To test and compare the Fourier–Motzkin [47,48] method applied to CPSAT-ɛ and our BCOi method we generated examples with a small number of propositional letters and a small number of CP formulas. The problem instances were generated for each of the following (N, M) pairs: (3, 3), (3, 4), (4, 3), (4, 4), (4, 5), (5, 5).All probabilities and constants in the system (13) are from Hardy field Q[ɛ]. This means that they are represented as polynomial functions depending of ɛ with double precision real coefficients, i.e.,η=h0+∑j=1khjɛjThis type of representation has led to the problem of division by zero when Fourier–Motzkin method is applied. For this reason in the method Fourier–Motzkin we also used representation with rational functions depending of ɛ, with, again, a real coefficient with double precision, i.e.,η=∑j=−kkhjɛj1+∑j=1khjɛjIn both representations, the maximum for parameter k was 4 (using same idea as in [49]). In the BCOi method we are always using polynomial function for representation, since the division by zero happens quite rarely and can be easily resolved by stopping the current iteration and moving to the next one.All our implementations are running on a cluster consisted of nodes with 2 AMD Opteron 2.1GHz 6272 processors with 48GB RAM under kernel 2.6.32 x64, gcc version 4.4.3. The results obtained by testing randomly generated examples are shown in Table 1.The columns Polynom and Rational of Table 1 contain times obtained using Fourier–Motzkin method with polynomial and rational representation, respectively, for the numbers from Hardy field Q[ɛ]. Column BCOi contains average times required for 10 independent runs of BCOi method, with parameters values set to NC=30, B=10 and the maximum number of iterations set to 50. In the case when the time in the column BCOi is given, each run was successful while – for time means that BCOi method cannot find a solution within the given stopping criterion. In Fourier–Motzkin method number of inequalities grows exponentially during eliminations of unknowns, so for a formula that contains 5 probability formulas and 4 propositional letters we obtain a system that contains 44 to 49 inequalities (depending on the relationships that occur in probabilistic formulas) and 16 variables, and, for N=4, M=5, Instance=1 after elimination of 9 variables system has more than 1.5·108 inequalities, so for this problem there is not enough RAM, and it is impossible to finish calculations. In Table 1 problem of insufficient RAM is marked with MEM, while the instances for which a division by zero occurs are marked with DIV 0.In the pre-testing phase, we varied the parameters for heuristics Worst Unsatisfied Projection and Greedy Giveaway, starting from the values given in [26], and ending with the best performing values specified in Section 4. Moreover, we also get that every successful run certainly has less than 50 iterations, and therefore concluded that increasing the number of iterations cannot contribute to the increase in success. To determine the best performing parameters for BCOi method, a small subset of randomly generated examples was chosen. When we tested this set of examples we varied B and NC parameters, while the maximum number of iterations was fixed to 50.Each example was tested 10 times with the same parameters values. The problem instances were generated for each of the following (N, M) pairs: (3, 3), (5, 5), (10, 30), (10, 50), (10, 100), (20, 20), (20, 50), (20, 100), (20, 250), (30, 30), (30, 60), (30, 100), (30, 250), (40, 40), (50, 50). For the obtained results the average number of successful runs, the average time of successful iterations (in seconds), the average number of objective function evaluations and the average total test time (in seconds) were determined. The results are shown in Figs. 1–4and summarized in Table 2.The average success, obviously, increases with the increase of the parameters B and NC, until it reaches some level. Average execution time of each iteration and the average number of objective function evaluations, also increase with an increase of parameters B and NC. The greatest success among all (B, NC) combinations have combinations (10,20), (10,30), (15,20), (15,30), and among them the smallest total execution time has (10,30) and therefore, for testing a larger set of examples parameters NC=30 and B=10 were chosen. The results obtained with these parameters are shown in Table 3. Column Solved contains the success rate in finding a solution out of 10 runs under the same conditions, the third column contains average number of objective function evaluations, and the last column shows the average time needed to find a solution, including unsuccessful runs.As can be seen from Table 3, our BCOi shows very stable performance. It was not able to solve 8 out of 39 instances. For the remaining examples, in most of the cases all runs (10 out of 10) were successful. We can also note that the required CPU time for successful executions is not too large confirming the conclusion from [50] that the properly selected solution modification mechanism represents the key factor for BCOi efficiency.

@&#CONCLUSIONS@&#
