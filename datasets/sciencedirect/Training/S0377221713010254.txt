@&#MAIN-TITLE@&#
Line search methods with guaranteed asymptotical convergence to an improving local optimum of multimodal functions

@&#HIGHLIGHTS@&#
We define and analyze a pattern, called v-pattern, for general line search methods.We derive enhanced golden section, bisection and Brent’s algorithmThe algorithms convergence using composite maps is proven under mild conditions.We analyze the performance of the three enhanced line search methods in practice.

@&#KEYPHRASES@&#
Nonlinear programming,Line search,Golden section method,Brent’s algorithm,Bisection,Multimodal functions,

@&#ABSTRACT@&#
This paper considers line search optimization methods using a mathematical framework based on the simple concept of a v-pattern and its properties. This framework provides theoretical guarantees on preserving, in the localizing interval, a local optimum no worse than the starting point. Notably, the framework can be applied to arbitrary unidimensional functions, including multimodal and infinitely valued ones. Enhanced versions of the golden section, bisection and Brent’s methods are proposed and analyzed within this framework: they inherit the improving local optimality guarantee. Under mild assumptions the enhanced algorithms are proved to converge to a point in the solution set in a finite number of steps or that all their accumulation points belong to the solution set.

@&#INTRODUCTION@&#
This work investigates the solution of the line search optimization problem(1)minimizef(α)=F(a+αd)subjectto0⩽α⩽αmaxwhereα∈R,αmaxis the upper bound forα,F:Rn↦F⊆Ris an arbitrary function (including discontinuous function),f:R↦Ris defined for a given pointa∈Rnand a non-null (usually descent) directiond∈Rn.A solution of the line search problem (1) is typically used inside a search direction optimization algorithm to solve(2)minimizeF(x)subjecttoxmin⩽x⩽xmaxwherex∈Rn,xminandxmaxare the respective lower and upper bounds for x. In this application, a is the point where the problem’s oracle is queried and d is the search direction. This leads to the iterative updatexk+1=xk+αk★dkwhere(3)αk★=argminαF(xk+αdk):0⩽α⩽αmax(4)αmax=maxα:xmin⩽xk+αdk⩽xmaxanddkis defined according to the chosen method.In fact, there is a vast range of exact and inexact methods to solve (1). The simplest approximate solution to the line search problem used inside a search direction algorithm is a constant step sizeαk★. If this constant step is set too small, a slow convergence rate will take place. If it is too large, the search direction algorithm may diverge. This procedure does not guarantee convergence, even when F is assumed to be strictly convex. To overcome these drawbacks, reducing stepsize rules may be employed (Bertsekas, 2008), which guarantee global optimality and termination if F is convex.Another popular technique is the Armijo’s rule, also known as successive stepsize reduction (Armijo, 1966; Shi & Shen, 2005). The Goldstein’s test (Goldstein, 1965) adds the condition that the step size is not too small if the Goldstein’s rule is verified. If the cost to evaluate the derivative of F is small, the Wolf’s test (Wolfe, 1969) can be considered. Some algorithms derived from these tests can be found in the work of Yuan (2010). Non-monotone variations of these techniques are presented in Hu, Huang, and Lu (2010) and Yu and Pu (2008).Based on interval reductions, the bisection line search (Bertsekas, 2008; Luenberger & Ye, 2010; Bazaraa & Shetty, 2006) removes half of the search interval at each iteration. However, it relies on the gradient of f and, hence, on the differentiability of f. Based on two points information, the golden section algorithm reduces the confidence interval to achieve convergence (Kiefer, 1953; Avriel & Wilde, 1966). Section techniques based on curve fitting have also been addressed, like the Brent’s algorithm (Brent, 1973).This work is mainly interested in section like algorithms. The concept of a v-pattern is explored in this paper to derive some novel line search strategies. Moreover, it is proven that the golden section algorithm converges to a local minimum for any line search problem (1), without the need of function unimodality. However, as it is well known by practitioners, the algorithm output can be worse than the starting point.Based on these considerations, this work presents a framework to derive line search section methods for arbitrary functions f, with the guarantee of always keeping a local minimum of (1) whose function value is no greater thanf(0). These ideas are applied to the Golden Section, bisection and Brent’s algorithms. The enhanced algorithms are used as a block in a general algorithm, and the converge of this algorithm is proved under mild conditions.In order to contextualize the improvements proposed in this paper, a brief overview of line search methods is given in the next section.There is no line search method that is the best for all classes of problems, since each one was developed to best explore some conditions. Therefore, the art of matching problems and solvers is a fundamental step in practical optimization. This section describes the most notable line search methods, emphasizing advantages and shortcomings of each one.A constant step requires no oracle queries: just choose a constantαk★,∀k, where the line search problem is actually bypassed. However, convergence guarantees of a search direction algorithm with constant step are typically problem dependent. Since a search direction algorithm performs the iterative updatexk+1=xk+αk★dk, a constant stepαk★implies asymptotical convergence wheneverdk→0ask→∞, considering also thatdk=0is an optimality condition.Moreover, it is possible to build a continuously differentiable strictly convex function (see Fig. 1)(5)F(x)=3(1+x)24-2x-1,x<-1x2,x⩽13(1-x)24+2x-1,x>1such that, fordk=-∇F(xk)andαk★=1, the objective function decreases asymptotically, i.e.F(xk+1)<F(xk),∀k, to a non-optimal value by getting trapped atxk+1=-xk(Bazaraa & Shetty, 2006). The respective sequence is not convergent forx0≠0. This result can be intriguingly stated: taking a sequence of constant non null steps decreasing the objective function, towards shrinking-length directions that are null only at optimal points, does not imply convergence to a finite optimal point, even for continuously differentiable strictly convex problems.A typical theoretical guarantee of search direction methods isF(xk+∊dk)<F(xk)or, equivalently,f(∊)<f(0)for a sufficient small∊>0. However, a small step implies slow convergence rate of the search direction algorithm. Conversely, a large step can make the algorithm diverge or converge to a non-local minimum, as previously seen. The backtracking strategy fundamentally reduces a large step size until some conditions which guarantee convergence are met. Just like a constant step size, this strategy does not actually solve the line search problem (1).Armijo’s condition (Armijo, 1966) for a pointα★>0can be written as(6)f(α★)⩽f̃(α★)f(ηα★)>f̃(ηα★),η>1wheref̃is a variation of the linear approximation atα=0given by(7)f̃(α)=f(0)+∊∇f(0)αfor a fixed∊∈(0,1). The upper bound condition considers∇f(0)<0, otherwise it could not be satisfied by a convex function f. Hence, it guaranteesf(α★)<f(0), which must be satisfied at least at a sufficient smallα★. Goldstein proposed a stricter condition (Goldstein, 1965) where∊∈(0,1/2)andη=(1-∊)/∊. Considering backtracking algorithms, the lower bound condition is useless to be tested. However,1/ηcan be easily related to shrinking rate of the step length in backtracking usingαmaxas starting point, so that Armijo’s lower bound condition follows naturally.Consider a line search problem (1) where f is continuously differentiable and letα2=(α1+α3)/2where[α1,α3]⊆[0,αmax]. LetΔ=α3-α1be the interval length. A strictly positive derivative∇f(α2)implies∃∊∈(0,Δ/2)such thatf(α2-∊)<f(α2), at least for an infinitesimal∊>0, so that(α2,α3]can be cut out. Conversely, a strictly negative derivative allows cutting out the subinterval[α1,α2). This process can be carried out until the localizing interval becomes arbitrarily small. A null derivative∇f(α2)=0is a necessary optimality condition forα2and, hence, a natural stop criterion. Therefore, the convergence is guaranteed by construction: half of the localizing interval is cut out in each iteration.Furthermore, the convergence rate is problem independent:Δk/Δk-1=2-1whereΔkis the localizing interval length at iteration k, so thatΔk/Δ0=2-k. These properties imbue some robustness to the bisection method, nevertheless,∇f(α2)=0is a necessary but not a sufficient optimality condition.In many cases, derivatives are only available through numerical evaluations, which can be computationally expensive and contain numerical errors in practice. Moreover, in this case, for any iteration of the bisection method, at least two function evaluations are necessary.The most important drawback of the classical bisection method is that the best point found so far can be discarded throughout iterations. An extreme example is shown in Fig. 2, where the objective function increases after each iteration.Considering also the typical theoretical guarantee of search directionsf(∊)<f(0)for a sufficient small∊>0, an important desired feature for line search methods arises: it is desired a local minimumα★such thatf(α★)<f(0). These are the fundamental requirements introduced and analyzed in this paper.The bisection method can be modified in order to find a local optimumα★such thatf(α★)⩽f(0), for any line search problem (1) where f is continuously differentiable. This enhanced algorithm and related proofs are introduced in this paper.Unlike the bisection method, golden section line search is not based on derivatives, only on relative position of function values between pairs of points in a sequence of four pointsα1<α2<α3<α4whereα2=α4-ΦΔ,α3=α1+ΦΔ,Δ=α4-α1is the localizing interval length andΦ=(5-1)/2=0.618…is the golden ratio. However, the shrinking rate of its localizing interval[α1,α4]isΔk/Δk-1=Φinstead of bisection’s1/2.Golden section converges to a local minimumα★for any instance of line search problem (1) as proven next in this paper. This is a notable feature: golden section always converges to a local minimum at a constant rate, no matter if the function is discontinuous, multimodal or evaluated to infinite at some points. Optimality of golden section has been proven before only for unimodal or quasiconvex functions (Bazaraa & Shetty, 2006; Luenberger & Ye, 2010). Unfortunately, the local minimumα★may be such thatf(α★)>f(0), as shown in Fig. 3. This paper proposes a golden section algorithm with the extra theoretical guarantee thatf(α★)⩽f(0).Brent (1973), proposed a section line search based on quadratic curve fitting over a sequence of three pointsα1<α2<α3such thatf(α1)⩾f(α2)⩽f(α3). Under this condition, it is possible to fit the three points by a quadratic functionf̃(α)=aα2+bα+cwherea>0and whose minimum is atα̃=-b/(2a), which can be explicitly given as a function ofαiandf(αi),i=1,2,3. At each iteration, a subinterval of the current sequence is cut out, so that the next sequence of three points is given byα2,α̃and eitherα1orα3, so that the patternf(α1)⩾f(α2)⩽f(α3)is preserved. This algorithm is known to always converge and it has a proven optimality for unimodal or quasiconvex function f (Brent, 1973; Luenberger & Ye, 2010).Brent’s algorithm tries to predict function behavior in order to speed up convergence, i.e. its convergence rate depends on the function f, unlike bisection and golden section methods. Since any smooth function behaves locally like a low order polynomial, it is intuitive that Brent’s quadratic fitting implies a higher convergence rate than other section methods when f is smooth. Although this may be true for a vast range of functions, there are cases where the localizing interval of Brent’s algorithm asymptotically shrinks to a non-null interval, even for smooth functions f as shown in Fig. 4, whereα̃∈(α1,α2)butf(α̃)>f(α2), so that(α2,α3]is never cut out whileα1andα2approach to the minimum. Furthermore, Brent’s algorithm can become numerically unstable as the localizing interval shrinks into a discontinuous point of f.Unfortunately, this surprising good performance in practice does not hold true for any function, as shown in Fig. 4.The patternf(α1)⩾f(α2)⩽f(α3)forα1<α2<α3is a key condition in this paper. It guarantees the existence of a local minimum within the open interval(α1,α2)(Bazaraa & Shetty, 2006; Luenberger & Ye, 2010). Brent’s algorithm is further analyzed and modified to find a local minimumα★such thatf(α★)<f(0)for any function f, considering also the two singular casesα̃=α2andf(α1)=f(α2)=f(α3).All definitions and analysis used henceforth in this paper only depend on continuous function domain, so that the functionsf:Rn↦F⊆Rare arbitrary. Let us start with some basic classical definitions of optimal solutions as minimum of functions.Definition 1global minimumA pointx★is a global minimum of a functionf(x):Rn↦F⊆Rsubject tox∈Xiffx★∈Xandf(x★)⩽f(x)for allx∈X, whereXis the feasible space.A pointx★is a local minimum of a functionf(x):Rn↦F⊆Rsubject tox∈Xiffx★∈Xand there exists a neighborhoodEofx★such thatf(x★)⩽f(x)for allx∈X∩E.This is the classical definition of local minimum and it does not rely on differentiability or continuity of f. For the line search problem (1), a pointα★is a global minimum iff(8)f(α★)⩽f(α),∀α∈[0,αmax],and a local minimum iff(9)∃∊>0:f(α★)⩽f(α),∀α∈[0,αmax],α-α★<∊.The analysis introduced in this paper is based on special relative function values of an ordered sequence of three points, defined next as v-pattern. The key idea behind this pattern is that it always contains a local minimum (see Fig. 5), as stated in Theorem 1. Note that, by definition, a local minimum can be surrounded by points with the same function value. The v-pattern theorem is very simple indeed, but it is also very powerful to analyze optimization algorithms based only on local relative function values information.Definition 3v-patternAn ordered sequenceα1<α2<α3presents a v-pattern for a function f ifff(α2)⩽f(α1)andf(α2)⩽f(α3).Every v-pattern forα1<α2<α3,[α1,α3]⊆[αmin,αmax], contains at least one local minimum off(α)subject toα∈[αmin,αmax]in the open interval(α1,α3).The proof comes from the definition of v-pattern and local minimum. Since for a v-patternα1<α2<α3,f(α2)⩽f(α1)andf(α2)⩽f(α3), eitherα2∈(α1,α3)is a local minimum itself given that there are neighborsα1andα3at both sides ofα2whose function values are not smaller, or there must exist another point in(α1,α3)with a function value smaller thanf(α2). In the latter case, choose a point in(α1,α3)with the smallest function value and it will be, by definition, a local minimum. □For a complete analysis, it is necessary to define a pattern related to the v-pattern in the corner of the search interval: the half-v-pattern. This pattern also comes with a supporting theorem: the half-v-pattern theorem.Definition 4half-v-patternAn ordered sequenceα1<α2presents a half-v-pattern for a function f in the interval[αmin,αmax]iff eitherf(α1)⩽f(α2)andα1=αmin, orf(α2)⩽f(α1)andα2=αmax.Every half-v-pattern forα1<α2,[α1,α2]⊂[αmin,αmax], contains at least one local minimum off(α)subject toα∈[αmin,αmax]in the interval[α1,α2)iff(α1)⩽f(α2)andα1=αmin, or in(α1,α2]iff(α2)⩽f(α1)andα2=αmax.The proof comes from the definition of half-v-pattern and local minimum. Sinceα1<α2, considering the kind of half-v-pattern wheref(α1)⩽f(α2)andα1=αmin, eitherα1∈[α1,α2)is a local minimum itself given that there is a neighborα2in the feasible side ofα1whose function value is not smaller, or there must exist another point in[α1,α2)with a function value smaller thanf(α1). In the latter case, choose a point in(α1,α2)with the smallest function value and it will be, by definition, a local minimum. For the kind of half-v-pattern wheref(α2)⩽f(α1)andα2=αmax, the proof follows by symmetry. □Consider a line search method, such as the golden section method, based on the configuration of an ordered sequence of four pointsα1<α2<α3<α4, so that, in each iteration, one subinterval is eliminated and another one is split into two. All patterns for a sequence of four points are given in Table 1. It will be used next to analyze the algorithms.The local optimality of the golden section algorithm can be proved with the help of Lemma 1: once a v-pattern is found, it is always possible to insert a point and eliminate one subinterval to get another v-pattern (see Fig. 6).Lemma 1Any pointα′≠α2inserted in(α1,α3)of a v-pattern forα1<α2<α3will lead to an ordered sequence that contains a v-pattern for a subsequence.By the symmetry of a v-pattern, it is sufficient to prove forα′∈(α1,α2), so that whenα′∈(α2,α3)the analysis is the same. Hence, consider the ordered sequenceα1<α′<α2<α3. By definition of v-pattern,f(α2)⩽f(α1)andf(α2)⩽f(α3). Iff(α2)⩽f(α′), thenα′<α2<α3is a v-pattern sincef(α2)⩽f(α3). Otherwise, iff(α′)⩽f(α2), thenα1<α′<α2is a v-pattern sincef(α′)⩽f(α2)⩽f(α1). □This section explores the properties of the golden section algorithm (Kiefer, 1953; Avriel & Wilde, 1966) which is described in Algorithm 1. This method enjoys a convergence rate independent on the function, as stated in Theorem 3. Theorem 4 shows that the classical golden section algorithm always keeps a local minimum for an arbitrary function f. Another good guarantee is thatmin{f(α2),f(α3)}never gets greater throughout iterations, as stated in Theorem 5.Algorithm 1Classical golden section algorithmInput: A search interval[0,αmax], a functionF:Rn↦R, a pointa∈Rnand a directiond∈Rn.Output: A subinterval[α1,α4]⊆[0,αmax]that still contains a local minimum ofF(a+αd),α∈[0,αmax].1: function GoldenSection(F,a,d,αmax)2:Φ←(5-1)/2▷ golden number:Φ=0.61803…3:k←0▷ iteration counter4:Δ0←αmax5:α1←06:α2←αmax-ΦΔ07:α3←ΦΔ08:α4←αmax9:whileΔk>∊Δ0do▷∊∈(0,1]10:k←k+111:Iff(α2)⩽f(α3)then▷ cut off(α3,α4]12:Δk←α3-α113:α4←α314:α3←α215:α2←α4-ΦΔk16:else▷ cut off[α1,α2)17:Δk←α4-α218:α1←α219:α2←α320:α3←α1+ΦΔk21:end if22:end while23: end functionThe classical golden section algorithm stops after⌈log∊/logΦ⌉iterations.At each iteration k the interval length reduction isΔk/Δk-1=Φ. Thus, after k iterations the overall interval length reduction isΔk/Δ0=Φk. Since the only stop criterion is∊<Δk/Δ0=Φk, the result follows. □The classical golden section algorithm always keeps in[α1,α4]a local minimum off(α),α∈[0,αmax].At each iteration of the golden section algorithm, the ordered sequenceα1<α2<α3<α4presents one pattern (see Table 1) that will be transformed into one of three other patterns according to the function value at the split point relative to its neighbors. The resulting sequence of patterns is graphically represented by a directed graph in Fig. 7, where each node has outdegree 3. The patterns ii, iii, iv and vi always lead to patterns in the same group, as Lemma 1 stated to be the case. Furthermore, they contain a v-pattern and, hence, a local minimum according to the v-pattern theorem. The patterns i and v can only lead to patterns in the previous group ii, iii, iv, vi (from where there is no way out) or to i. Furthermore, they contain a half-v-pattern and, hence, a local minimum according to the half-v-pattern theorem. The same holds for viii and vii. □The classical golden section algorithm never increasesmin{f(α2),f(α3)}.Iff(α2)⩽f(α3), thenα2will become the nextα3. Otherwise, iff(α3)<f(α2), thenα3will become the nextα2. □Even though classical golden section method owns guarantees on termination and local optimality, there is still an unsolved question. Because of the patterns v and vi, the classical golden section method can exclude the whole subinterval where lies points whose function values are smaller thanf(0), as shown in Fig. 3. This is not desired in optimization, sinceα=0usually corresponds to the point where the problem oracle is queried and which must be improved upon. Furthermore, most of the search directions guarantee the existence of better points in the vicinity ofα=0. A solution for this limitation is presented next.An enhanced golden section method is proposed (Algorithm 2), where a local minimumα★is preserved in the localizing interval[α1,α4]with the additional guaranteef(α★)⩽f(0). This algorithm differs from the classical golden section only in the line 11 and guarantees to keep a local minimum no worse thanf(0), as stated in Theorem 6. This improvement comes with no additional oracle queries, considering thatα=0is where the oracle is queried to define the search direction d. Moreover, the convergence rate of the classical golden section is preserved.Algorithm 2enhanced golden section algorithmInput:A search interval[0,αmax], a functionF:Rn↦R, a pointa∈Rnand a directiond∈Rn.Output:A subinterval[α1,α4]⊆[0,αmax]that still contains a local minimum ofF(a+αd),α∈[0,αmax], whose function value is smaller or equal toF(a).1: function EnhancedGoldenSection (F,a,d,αmax)2:Φ←(5-1)/2▷ golden number:Φ=0.61803…3:k←0▷ iteration counter4:Δ0←αmax5:α1←06:α2←αmax-ΦΔ07:α3←ΦΔ08:α4←αmax9: whileΔk>∊Δ0do▷∊∈(0,1]10:k←k+111:ifmin{f(α2),f(α3)}⩾f(0)orf(α2)⩽f(α3)then12:Δk←α3-α113:α4←α314:α3←α215:α2←α4-ΦΔk16:else▷ cut off[α1,α2)17:Δk←α4-α218:α1←α219:α2←α320:α3←α1+ΦΔk21:end if22:end while23: end functionThe enhanced golden section algorithm always keeps in[α1,α4]a local minimumα★off(α),α∈[0,αmax], such thatf(α★)⩽f(0).Oncef(α2)<f(0)orf(α3)<f(0), the enhanced golden section method becomes the classical one, and the golden section monotonicity theorem will guaranteemin{f(α2),f(α3)}<f(0)henceforth. Until that, the subinterval[α1,α2)is not eliminated, which guarantees that0∈[α1,α4]. Therefore, at any iteration,∃α∈[α1,α4]:f(α)⩽f(0).To prove that[α1,α4]always contains a local minimum off(α),α∈[0,αmax], observe that the only difference to the classical golden section, beforef(α2)<f(0)orf(α3)<f(0), called phase I, is that the next pattern of v and vi is iii, v or vii (see Table 1 and Fig. 7), since patterns i and ii implyf(α2)<f(α1)=f(0)so that[α1,α2)can be eliminated. Because the classical golden section algorithm guarantees to keep a local minimum in[α1,α4](see Theorem 4), it is sufficient to prove that every pattern has its local minimum in the exit from phase I.Patterns ii, iii, iv and vi contain a v-pattern and, therefore, a local minimum. The subinterval[α1,α2)is kept during phase I, so that patterns vii and viii contain a half-v-pattern and, therefore, a local minimum. The only way left to miss all local minima is by eliminating(α3,α4]before patterns i and v. During phase I, only patterns v and vi can precede pattern v, so that the last inserted point would beα2. Therefore, sincef(α3)<f(α2)in pattern v, the only way to exit phase I in pattern v is whenf(α3)<f(0)=f(α1)in the first iteration, whereα4=αmax, so that its half-v-pattern and, therefore, its local minimum is guaranteed.During phase I, pattern v cannot precede pattern i (only pattern i can precede itself), so that pattern i is only possible to appear either since the beginning or after a pattern v that satisfiesf(α3)<f(0)=f(α1)in the first iteration. In both cases, its half-v-pattern and, therefore, its local minimum is guaranteed. □Consider a directiond∈Rnsuch that exist an∊>0such thatf(0)>f(α),∀α∈(0,∊]. In this case, d is called a descent direction. Given a descent direction and a large enough number of iterations, the enhanced golden section algorithm will returnα★∈[0,αmax], such thatf(α★)<f(0).Line 11 of the enhanced golden section algorithm guarantees that the interval(α3,α4]is discarded whilef(0)⩽min{f(α2),f(α3)}. Therefore, in the worst case, after a large enough number of iterationsα2∈(0,∊]and, thus,f(0)>f(α2). □A bisection algorithm with the theoretical guarantee of finding a local minimumα★wheref(α★)⩽f(0)of a continuously differentiable line search problem (1) can be derived based on a corollary of the v-pattern theorem.Corollary 1derivative v-patternEvery interval[α1,α2]wheref(α1)⩽f(α2)and∇f(α1)<0, or wheref(α1)⩾f(α2)and∇f(α2)>0, contains a local minimum in(α1,α2)of a function f differentiable atα1andα2.By definition of derivative,∇f(α1)=[f(α1+∊)-f(α1)]/∊for an infinitesimal∊>0, so thatf(α1+∊)=f(α1)+∊∇f(α1)<f(α1)when∇f(α1)<0. Considering also thatf(α1)⩽f(α2),α1<α1+∊<α2forms a v-pattern so that(α1,α2)contains a local minimum by the v-pattern theorem. The same result follows forf(α1)⩾f(α2)and∇f(α2)>0using analogous analysis. □Using the derivative v-pattern corollary, given a v-pattern forα1<α2<α3, either the subinterval(α2,α3]can be cut out whenever∇f(α2)>0or[α1,α2)whenever∇f(α2)<0, so that the remaining subinterval still contains a local minimum. Since the derivative guarantees the existence of a better point in the vicinity ofα2, the remaining subinterval can be bisected towardsα2until such point is found, i.e. untilf(α2)<f(α1)andf(α2)<f(α3)where one inequality follows by the search and the other follows by construction.Iff(α2)⩽f(0)whereα2=αmax/2, then(α1,α3]=(0,αmax]obviously contains a local minimum. Otherwise, the subinterval(α2,α3]can be cut out untilf(α2)⩽f(0)=f(α1). Once anα2wheref(α2)⩽f(0)has been found, the conditionf(α2)⩽f(α1)=f(0)<f(α3)holds true so that(α1,α3)contains a local minimum. In the former case, wheref(α2)⩽f(0)whenα3=αmax, the subinterval[α1,α2)can be similarly cut out untilf(α2)⩽f(αmax)=f(α3). Hence, in case the algorithm did not converge to local minima atα=0or atα=αmax, there exists now a local minimumα★∈(α1,α3)wheref(α★)⩽f(α2)⩽f(0)andf(α1)⩾f(α2)⩽f(α3). Notice that no derivatives are required in this search for a v-pattern, only relative position of points, just like in golden section. This imbues robustness to the search.The same previously seen fundamental ideas to preserve an improving local minimum can be applied to null derivatives in bisection method: find and keep a v-pattern. For∇f(α2)=0, cut out(α2,α3]iff(α¯1)⩽f(α2), otherwise cut out[α1,α2)iff(α¯2)⩽f(α2), otherwise cut out both[α1,α¯1)and(α¯2,α2], whereα¯1=(α1+α2)/2andα¯2=(α2+α3)/2. Hence, any of the three conditions will lead to a bisection. Although two function evaluations are required when∇f(α2)=0, no derivatives are needed.The first v-pattern can be found like in the bisection method or using the enhanced golden section. Once it has been found, the quadratic curve fitting will provide a pointα̃(i.e. its minimum) and a new v-pattern except in two cases: whenα̃=α2or whenf(α1)=f(α2)=f(α3). Both cases can be treated like the bisection method deals with a null derivative.Also the enhanced golden section algorithm can be used to find a v-pattern. As stated by 7, if d is a descent direction, after an adequate number of iterations one v-pattern will be found.Any infinite function value prevents the quadratic approximation from being obtained. When it appears, the function value can be upper bounded by the highest finite value found so far plus a positive constant, so that the relative values guarantee a proper subinterval cut. If the function is evaluated to infinite at all three points, the degenerate casef(α1)=f(α2)=f(α3)is properly treated. A minus infinite function value is clearly the best possible solution.Even though finding a local minimum is not the central concept of the Armijo’s rule and similar techniques, it can also be analyzed in the light of the v-pattern. Sincef(ηα★)>f(α★)<f(0)andα★∈(0,ηα★), there is at least a local minimum in the open interval(0,ηα★).In this section, the algorithm convergence is analyzed using the concept of algorithm mapping. The closed mapping is defined as follows.Definition 5closed maps, Bazaraa and Shetty (2006)LetXandYbe nonempty closed sets inRnandRm, respectively. LetA:X↦Ybe a point-to-set map. The map A is said to be closed atx∈Xif for any sequences{xk}and{yk}satisfying(10)xk∈X,xk→xyk∈Y,yk→ywe have thaty∈A(x). The map A is said to be closed onZ⊆Xif it is closed at each point inZ.The convergence of search direction algorithms using the line search algorithms proposed in this paper is derived from the following theorem, which is a variation of the Zangwill’s convergence theorem.Theorem 8convergence of algorithms with composite maps, Bazaraa and Shetty (2006)LetXbe a nonempty closed set inRnand letX★be a nonempty solution set. Letf:Rn↦Rbe a continuous function, and consider the point-to-set mapB:X↦Xsatisfying the following property: givenx∈X, thenf(y)⩽f(x)fory∈B(x). LetA:X↦Xbe a point-to-set map that is closed over the complement ofX★and that satisfiesf(y)<f(x)for eachy∈A(x)ifx∉X★. Now consider the algorithm defined by the composite mapC=BA. Givenx1∈X, suppose that the sequence{xk}is generated as follows: ifxk∈X★, then stop; otherwise, letxk+1∈C(xk), replace k byk+1, and repeat. Suppose that the setΛ={x:f(x)⩽f(x1)}is compact. Then either the algorithm stops in a finite number of step with a point inX★or all accumulation points of{xk}belong toX★.See Bazaraa and Shetty (2006) [pp. 326–327]. □So, it must be proven that the line search algorithms are closed maps and return a pointx★wheref(x★)<f(0)for a given descent direction.Definition 6section like line searchConsider the intervalSk=[xk-Δ1k,xk+Δ2k], forΔ1k,Δ2k⩾0∀k. LetA:X↦Y. A section like line search is defined such thatxk+1=A(xk), wherexk+1∈Sk,Sk+1⊃Sk,Sk≠∅∀kandΔ1k,Δ2k→0ask→∞.The golden section and bisection algorithms described and enhanced in this paper can be viewed from the general class of section like line search as in Definition 6 given a non-null direction d. The next theorem shows that these algorithm are closed maps.Lemma 2line search as closed mapsAny section like line search algorithm as presented inDefinition 6is a closed map.The iteration of the line search can be written as:(11)xk+1=λ(xk-Δ1k)+(1-λ)(xk+Δ2k),forλ∈[0,1]. Now suppose thatx∈X,xk→xandyk→yask→∞are sequences such thatyk∈A(xk). Takingk→∞in (11), the following is obtained(12)y=x,sinceΔ1k,Δ2k→0ask→∞. Thus,y=A(x), and the map A is a closed map. □For the golden section and bisection algorithmsΔ1k,Δ2k→0, andSk+1⊂Sk. The next theorem concludes with the conditions of the enhanced line search methods to accomplish with the stated conditions.Theorem 9A composite search direction algorithm with the enhanced golden section to the bisection algorithm generates a convergent sequence to a point in the solution set.If d is non-null, the algorithms mapA(x)is closed according to Lemma 2. By construction, given a non-null descent direction, the algorithms return a pointα★such thatf(α★)<f(0). So, by 8, a composite search direction algorithm with the enhanced golden section or the bisection algorithm generates a convergent sequence to a point in the solution set.□The enhanced algorithms derived in this paper can be used together with an algorithm B such that a descent non-null direction is computed ifx∉X★. Note that this theorem could not be applied to the original version of the studied line search algorithms since, even though closed maps, they did not guarantee thatf(y)<f(x)for eachy∈A(x)ifx∉X★.As previously seen in theory, no line search method presented in this paper is the best in every aspect. There is a clear trade-off involving shrinking rate, function continuity dependency and robustness. Table 2presents a comparison among them.In order to analyze the performance in practice of the enhanced line search methods proposed in this paper, consider the unconstrained problem of minimizing the smooth function(13)f(α)=e-a1αcos[10πa1αcos(10a1α)+b1]e1+e-a2αsin[10πa2αsin(10a2α)+b2]e2+e10cαinside the intervalα∈[0,1], wherea∈[1,2]2,b∈[0,1]2,c∈[-1/2,1/2]ande∈{1,2,…,10}2(see Fig. 8).Fig. 9shows the number of function evaluations distribution for100,000instances of (13) spent to achieve an overall shrink of∊, where∊=2-52is the machine double precision. For the enhanced golden section, the number of function evaluations needed to reach such precision is⌈logΦ∊⌉+3=41, no matter which problem is being solved. Analogously, in the enhanced bisection line search, considering that∇f(α2)=0is a very singular case for smooth functions, the expected number of function evaluations is⌈-log2∊⌉+3=29(value or derivative), which was indeed obtained in every instance. Note that this number would typically double if numerical derivatives were used, which makes using derivatives in practice even more arguable. The enhanced Brent’s algorithm confirmed its expected good performance in practice, reaching the specified shrink after about 23 function evaluations in average for87.4%of the instances, although its worst case performance is known to be as worse as shown in Fig. 4. The small step at 29 evaluations is due to a few instances where the bisection search for a v-pattern is not accomplished. The greater step in the end accumulates the cases that took more than 49 function evaluations, which are similar to the one shown in Fig. 4. All enhanced algorithms returned, for every instance, a point no worse than the starting point.Consider now the unconstrained problem of minimizing the smooth function(14)f(α)=d1-ea(α-c)binside the intervalα∈[0,1], wherea∈[1,2],b∈{2,4,6,…,20},c=6/10andd=100. Fig. 10shows the number of function evaluations distribution for100,000instances of (14) spent to achieve an overall shrink of∊. The enhanced bisection algorithm and the enhanced golden section algorithm spent the same number of function evaluations as before: 29 and 41, respectively. The enhanced Brent’s algorithm is not as efficient as it was before and, with the exception of a few instances, it typically spent more than 100 function evaluations to meet the specified overall shrink.Vieira (2012) presented a comparison between the golden section and Armijo’ rule in several contexts. Especially when numerical derivatives are needed, using the golden section can be more effective in the algorithm overall performance. This effect increases as the problem dimension grows, since the number of objective function evaluations become negligible compared to the gradient cost. Therefore, both type of techniques have their place and should be chosen given the problem settings.

@&#CONCLUSIONS@&#
Based on the simple definition of a v-pattern and a half-v-pattern, this paper has derived new line search methods that converge to a local minimum which is better than its starting point or, at most, equally good to it. Indeed, in case the search direction is descent, the algorithms converge to a strict better point.The v-pattern related definition and theorems do not rely on any function property. In fact, givenα1<α2<α3, the function domain can even be discrete, and the main results still hold.The proposed enhanced line search algorithms can be helpful to append theoretical guarantees to search direction algorithms, while also providing good features for real world optimization problems. Given some mild conditions, the enhanced methods were proven to converge to the solution set given a non-null descent direction.Among the enhanced methods introduced in this paper, the golden section line search is the most robust, followed by bisection line search, which is faster but relies on derivatives. Brent’s algorithm has a really bad worst case, but it is the fastest in practice most of times and it only depends on function values. For better or for worse, matching line search problems to methods is still an art where knowledge of capabilities is fundamental.