@&#MAIN-TITLE@&#
Solving multivariate polynomial systems using hyperplane arithmetic and linear programming

@&#HIGHLIGHTS@&#
A new scalable algorithm for solving systems of multivariate polynomials.The concept of hyperplane arithmetic, which is used in our algorithm.A benchmark of example systems that are scalable in the number of variables.Implementation and comparison with previous algorithms.

@&#KEYPHRASES@&#
Subdivision solver,Multivariate solver,Geometric constraints,

@&#ABSTRACT@&#
Solving polynomial systems of equations is an important problem in many fields such as computer-aided design, manufacturing and robotics. In recent years, subdivision-based solvers, which typically make use of the properties of the Bézier/B-spline representation, have proven successful in solving such systems of polynomial constraints. A major drawback in using subdivision solvers is their lack of scalability. When the given constraint is represented as a tensor product of its variables, it grows exponentially in size as a function of the number of variables. In this paper, we present a new method for solving systems of polynomial constraints, which scales nicely for systems with a large number of variables and relatively low degree. Such systems appear in many application domains. The method is based on the concept of bounding hyperplane arithmetic, which can be viewed as a generalization of interval arithmetic. We construct bounding hyperplanes, which are then passed to a linear programming solver in order to reduce the root domain. We have implemented our method and present experimental results. The method is compared to previous methods and its advantages are discussed.

@&#INTRODUCTION@&#
Solving polynomial systems of equations is a crucial problem in many fields such as robotics  [1], computer aided design and manufacturing  [2,3], and many others  [4]. This problem, namely finding the roots of a set of multivariate polynomial equations, is a difficult one and various approaches have been proposed for it. Symbolical approaches, such as Gröbner bases and similar elimination-based techniques  [5], map the original system to a simpler one, which preserves the solution set. Polynomial continuation methods (also known as homotopy methods  [4]) start at roots of a simpler system and trace a continuous transformation of the roots to the desired solution. These methods handle the system in a purely algebraic manner, find all complex and real roots, and give general information about the solution set. Such methods are typically not well-suited if only real roots are required.In recent years a family of solvers that focuses only on real roots in a given domain has been introduced. These methods are based on subdividing the domain and purging away subdomains that cannot contain a root. Thus, they are known as subdivision methods (sometimes such methods are referred to as exclusion or generalized bisection methods). Givennimplicit algebraic equations innvariables,(1)Fi(x1,x2,…,xn)=0,i=1,…,n,we seek allx=(x1,x2,…,xn)that simultaneously satisfy Eq. (1).A typical frame of a subdivision algorithm for finding the roots of a polynomial systemF(x1,…,xn)=0over ann-dimensional domain boxbwithin some predefined toleranceϵgoes as follows:Algorithm: root_isolation_in_boxInput:F(x1,…,xn),Boxb[x1min,x1max]×⋯×[xnmin,xnmax]Output:list〈Box〉boxes.(1)If(max(ximax−ximin)<ϵ)appendbto outputboxesand return.Evaluate bound interval onFinb.If bound does not contain 0, return (there is no solution inb).Otherwise: Splitbinto subdomains,b1,b2.root_isolation_in_box(F,b1,boxes).root_isolation_in_box(F,b2,boxes).There are various modifications and enhancements to this general framework. The no-solution test in step (3) can be enhanced with single solution tests  [6,7], which enable stopping the subdivision process earlier. Then, the algorithm can switch to faster numeric methods such as the Newton–Raphson iteration, which converge to a single root. Another common modification performs a more sophisticated domain reduction  [8,9] in step (4), which enables to find tighter subdomains that contain roots and therefore accelerates the convergence of the algorithm.A common approach for subdivision solvers, popular for its simplicity and wide generality, is interval arithmetic  [10–12]. In interval arithmetic a valuexis represented by a bounding intervalX=[xmin,xmax]. LetFi(x1,x2,…,xn)be a scalar function innunknowns, defined in a boxb=[x1min,x1max]×⋯×[xnmin,xnmax]. An interval evaluation ofFiinbis an interval[Fmin,Fmax]such thatFmin≤Fi≤Fmaxfor any value of(x1,…,xn)∈b. That is, an interval evaluation of a function for the box gives an interval that contains all possible values of the function evaluated on points in the box. Therefore, if the interval evaluation ofFi∈bdoes not contain zero, then no root can exist inb. This makes it suitable for use in the root isolation algorithm described above. There are many implementations of interval arithmetic software packages  [13,14] and in particular the ALIAS library  [15] implements interval methods for the determination of real roots of system of equations and inequalities. The main drawback of interval arithmetic is that the bounds given by the interval evaluation are not tight and with every arithmetic operation the looseness may accumulate. Thus, the interval evaluation may give bounds that are too loose to be useful.Subdivision methods that are based on the tensorial BézierB-spline representation  [16,2,8,9] give tight bounds for an exclusion test, based on the convex hull property of the basis. They have been implemented in recent years and applied successfully to a wide variety of problem domains  [2,3].InB-spline/Bézier subdivision solvers (e.g.,  [2]), theFi,i=1,…,n, from Eq. (1) are usually represented asB-spline or Bézier multivariate scalar functions, i.e.,(2)Fi=∑i1⋯∑inPi1,…,inBi1,ki1(x1)⋯Bin,kin(xn),whereBij,kijare theij’thkij-degree Bézier/B-spline basis functions.Patrikalakis and Sherbrooke  [9] exploited the special properties of the Bézier representation for efficient reduction of the subdomain where roots can exist. In their Projected Polyhedron (PP) algorithm the points of the control polyhedron are projected onto two-dimensional planes and the convex hull of their projection is computed. The intersections of the convex hulls are then used to reduce the domain. To achieve more robustness, Maekawa and Patrikalakis  [17,18,3] extended the PP algorithm to operate in rounded interval arithmetic. This resulted in the Interval Projected Polyhedron (IPP) algorithm. Mourrain and Pavone  [8] proposed a modification of the IPP algorithm so that instead of using the convex hull of the projected control points, the upper and lower envelopes of the projections would be used as control polygons of two new Bézier forms. These Bézier forms still bound the original function from above and below, and therefore can be used as a tighter bound. They use a univariate root solver to find the roots of these Bézier forms, and use them to construct the bounding intervals. Mourrain and Pavone also suggest a preconditioning step that uses an orthogonalization approach, which makes the domain reduction more efficient.A single-solution test forB-spline/Bézier based solvers was proposed in  [19]. This termination criterion was based on computing the normal cones of the function using the Bézier orB-spline representation. The single solution test is then implemented using a dual hyperplane representation of the normal cones (see  [6] for details).As noted above,B-spline/Bézier subdivision solvers have been used successfully in recent years. However, the usage of the tensor form has a scalability limitation  [20,21], which makes it impractical for systems with a large number of variables. It can be seen from Eq. (2), that theB-spline/Bézier representation grows exponentially with the number of variablesn. Given a multivariate polynomial inRn(i.e., of dimensionn, withnunknownsx1,…,xn) and degreed, it is typically represented withO(nd)coefficients using the standard monomial form. However, it will be represented withO((d+1)n)coefficients using the tensorialB-spline/Bézier representation. Thus, theB-spline/Bézier representation grows exponentially withn, whereas the monomial representation only grows polynomially inn. Therefore, when the degreedis much smaller thann, theB-spline/Bézier representation is not efficient.Furthermore, in many cases in practice, the actual monomial representation in standard form is sparse and consists of fewer coefficients. For example, representing the constant 1 in monomial form requires just one coefficient, compared toO((d+1)n)in the dense Bernstein-Bézier representation. Similarly, representing a linear polynomiala0+a1x1+a2x2+⋯+anxnrequiresncoefficients in the power basis andO((d+1)n)in the Bernstein-Bézier basis. Due to its exponential growth and its dense representation, usingB-spline/Bézier subdivision methods is especially problematic for many engineering problems that are characterized by (or can be transformed to) systems of high-dimension (i.e., a large number of variablesn) and relatively low degreed. For example, computing the forward kinematics of a parallel robot  [1,8] can be transformed to a system of quadratic constraints, but the number of variables grows with each joint of the mechanism.Little work has addressed the explosion of theB-spline/Bézierform for high-dimensional polynomials. Elber and Grandine  [20] represent multivariates as expression trees and compute bounds on the expressions using interval arithmetic, to overcome this problem. Their approach is natural for symbolic manipulations of free-form curves and surfaces. It is thus suited to handle problems arising from manipulations of splines with a large number of control points (see  [20]). However, the bounds given by the interval arithmetic over the expression tree are not tight. Furthermore, the expression tree structure is not well suited for more advanced domain reduction algorithms such as the Projected Polyhedron algorithm. Fünfzig et al.  [21] proposed a method based on linear programming (LP  [22]) to address the high-dimensionality problem for quadratic polynomials. They use a linearization of the terms in the polynomials, representing each term of typeXiXjas a separate variable of an LP problem. Tight bounds on these variables are constructed using Bernstein polyhedra (see  [21] for details) and these inequalities are solved using an LP solver, resulting in a domain reduction. While their method is successful in handling relatively high-dimensional systems of quadratic multivariate polynomials, it is not easily extended to higher degrees. Furthermore, the LP problem constructed by this method is relatively large since the number of variables depends on the number of terms in the problem, which is quadratic in the general case.In this paper, we will present a new method for solving systems of multivariate polynomials, which scales nicely for systems with a large number of variables and relatively low degree. In Section  2, we will introduce the basic building blocks of our method and describe our algorithm. In Section  3 we will show experimental results comparing our algorithm to previous algorithms from the literature. We conclude in Section  4, where we discuss our results and future directions of research.The general concept of our algorithm is to bound the multivariate polynomials byn-dimensional hyperplanes and reduce the domain using linear programming. Our bounding scheme is based on the general bounding scheme presented in  [6], which can be computed for any dimension and any degree of Bézier hypersurfaces. However, computing the bounding hyperplanes directly from the dense Bézier representation is infeasible for high-dimensional problems. Therefore, we present a bounding scheme that is based on a sparse representation of the hypersurfaces. This bounding scheme is inspired by interval arithmetic. However, instead of summing bounding intervals of sub-expressions, as is done in interval arithmetic, we sum bounding hyperplanes of sub-expressions. Thus, we call our bounding scheme hyperplane arithmetic.In Section  2.1 we recall the general hyperplane bounding scheme presented in  [6]. In Section  2.2 we propose a naive linear programming algorithm based on the bounding scheme presented in Section  2.1. This naive algorithm does not scale well and therefore is not feasible for high dimensional problems. We then explain in Section  2.3 the concept of hyperplane arithmetic and its use in bounding multivariate polynomials in sparse representation. In Section  2.4 we summarize our algorithm and show how its basic building blocks are assembled together.Our method is based on the hyperplane bounding scheme presented in  [6], we therefore present it here for completeness. If we consider the functionsFi(x1,x2,…,xn)as hypersurfaces inRn+1, we can use the Bézier/B-spline representation to bound the hypersurface with two parallel hyperplanes.Denote by promotion the process of converting the scalar functionFi(x1,x2,…,xn)to its vector function counterpartFˆi:Rn→Rn+1,Fˆi=(x1,x2,…,xn,Fi). The normal of the explicit scalar functionFiis given by the vectorvi=(∂Fi∂x1,…,∂Fi∂xn,−1). Thus, we can compute the normal ofFˆiat a given point by evaluating the partial derivatives at that point. The unit normal vectorv¯iis thenvi‖vi‖.The two bounding hyperplanes are constructed as follows. Compute the unit normal,v¯i=(vx1,…,vxn+1)∈Rn+1, ofFˆiat the midpoint of the subdomain. Then, project all control points ofFˆiontov¯i(see Fig. 1(a) and (b) for an illustration on the one dimensional case).Denote byxmax∈Rn+1(resp.xmin∈Rn+1) the point onv¯ithat is the maximal (resp. minimal) projection of the control points ontov¯i. Then, the(n+1)-dimensional parallel bounding hyperplanes ofFiare:〈x−xmin,v¯i〉=0,〈x−xmax,v¯i〉=0,wherex∈Rn+1(see Fig. 1(c)).Since we are only interested in boundingFi=0, we only need the intersection of these two(n+1)-dimensional hyperplanes with thexn+1=0hyperplane. Eliminating thexn+1coordinate, we remain with thed-dimensional hyperplanes boundingFi=0:K̃imin:〈x,v¯i〉|xn+1=0=x1v1i+x2v2i+⋯+xnvni=bimin,andK̃imax:〈x,v¯i〉|xn+1=0=x1v1i+x2v2i+⋯+xnvni=bimax,wherebimin=〈xmin,v¯i〉andbimax=〈xmax,v¯i〉(see Fig. 1(c)).Denote byKiminandKimaxthe half-spaces bounded byK̃iminandK̃imax, oriented so thatFi=0is on their positive side.Fi=0is thus bounded in the regionKimin⋂Kimax.In  [6] the authors presented this method of bounding a multivariate polynomial and used it to check for subdomains that contain no solution. The authors suggested there that LP methods can be used for this check, but did not implement such a method in practice.It should be noted that while this bounding scheme is general, it is not necessarily optimal. However, finding an optimal oriented bounding box of a set of points is hard even inR3[23]. Therefore, this general scheme, which can easily be implemented for any dimension and degree, is a good choice for our needs. Still, for special cases, tighter specialized bounds can be implemented.For example, a tighter bound of the functionF(x)=x2in a domain[xmin,xmax], can be computed using the convexity property of the function. The function graph is bounded from above by the line connecting its endpoints and from below by the parallel line tangent to the function. Thus, it is bounded by:ax−(xmin+xmax2)2≤F(x)≤ax−(xminxmax),wherea=xmin2−xmax2xmin−xmax=xmin+xmax. We use such a specialized scheme in our implementation (see also Section  3.2).Based on the hyperplane bounding scheme from Section  2.1, we can construct an algorithm that uses linear programming for domain reduction. The main idea of the algorithm is to bound the functionsFi=0by a pair of bounding hyperplanesKiminandKimaxfor all1≤i≤n. Furthermore, the domain[x1min,x1max]×⋯[ximin,ximax]⋯×[xnmin,xnmax]under consideration is bounded by the2nhyperplanes:x1min≤x≤x1max,…,xnmin≤x≤xnmax. Thus, we can give these4nbounding hyperplanes as inequalities to a linear programming solver (e.g., the GLPK library  [24], which we use in our implementations, see Section  3). Assigning goal functions of maximal and minimalxiwill result in2nlinear programming problems, whose solution is a tightern-dimensional box bounding the domain.The algorithm then follows the general scheme described in Section  1.1, and the domain is reduced in Step (3) using the linear programming procedure outlined above.The algorithm can be described as follows:Algorithm: Naive_LP_root_isolationInput:F(x1,…,xn),Boxb[x1min,x1max]×⋯×[xnmin,xnmax]Output:list〈Box〉boxes(1)If(max(ximax−ximin)<ϵ)appendbto outputboxesand return.BoundFinband reduce the domain:(a)BoundFiby two parallel hyperplanes, using scheme from Section  2.1.Perform2nLP problems to compute new[ximin,ximax]values for eachxi.If one of the LP problems is infeasible, return (there is no solution inb).Subdivide eachFito the new domain with the new[ximin,ximax]for eachxi.Splitbinto subdomains,b1,b2along the maximal axis ofb.Naive_LP_root_isolation(F,b1,boxes).Naive_LP_root_isolation(F,b2,boxes).Note that the LP reduction in Step (2) may not be satisfactory. For example, if two roots are in the domain, then the reduction step cannot reduce the domain to be smaller than the distance between the roots, since both roots should be inside the reduced domain. Still, if this happens, Step (3) will split the domain (just as is done in the basic root_isolation_in_box algorithm) until the two roots are separated.While this algorithm enhances the basic root isolation scheme by LP-based domain reduction, it does not address the scalability problem. Namely, the procedure described in Section  2.1 requires going over all the control points of the hypersurface and project them onto the normal vector. Thus, as the dimension grows the procedure runtime complexity grows exponentially and becomes infeasible. We have implemented this algorithm for reference (see Section  3) and denote it a naive LP algorithm. To make it feasible for high-dimensional systems, we need to compute a bound on the function with a procedure that does not grow exponentially. In Section  2.3 we will explain our approach to this problem.Using the bounding scheme from Section  2.1 requires projecting all of the control points onto the normal of the hypersurface. Thus, the construction of the bounding hyperplanes is exponential in the number of variables (i.e., the dimension)nof the system. Therefore, we wish to compute bounding hyperplanes for high-dimensional systems using a more sparse representation.Let the polynomial expression be represented in its monomial form. Then:Fi(x)=∑jcjMj(x),whereMj(x)is the j’th monomial term of the polynomial andcjis its coefficient. The monomial terms of the polynomial consist of the product:M(x1,x2,…,xn)=x1α1x2α2⋯xnαnwhereαiare integer degrees and the sum ofαiis less than or equal to the polynomial degreed. For an illustration, the polynomialF(x1,x2)=x12+x22has two monomial terms, namelyx12andx22. Note that in our context, of a relatively low degree compared to the dimension, the number of variablesxkactually participating in the monomial (i.e., withαk≠0) is relatively small since it cannot exceedd. Furthermore, although the total number of monomial terms in a polynomial isO(nd), in many cases in practice the polynomial is sparse and the actual number of monomial terms is much smaller (see examples in Section  3).In interval arithmetic, a sub-expressionfjis bounded by an interval[fjmin,fjmax], and the sum of sub-expressionsfi+fjis then bounded by the interval[fimin+fjmin,fimax+fjmax]. A generalization of this idea is to bound a sub-expressionfjnot by two values but by two functions[fjmin(x),fjmax(x)]. Thus, iffjmin(x)≤fj(x)≤fjmax(x)for any pointxin the domain and similarlyfimin(x)≤fi(x)≤fimax(x), then we get:fimin(x)+fjmin(x)≤fi(x)+fj(x)≤fimax(x)+fjmax(x).In our context, the sub-expressions are the monomialsMj(x)of the polynomial and the bounding functions are linear functions, i.e., hyperplanes. Therefore, we call this generalization of interval arithmetic hyperplane arithmetic.11Note that the analogy to interval arithmetic is not full as we do not support multiplication of two sub-expressions, only addition, subtraction and multiplication by a scalar.For an illustration, consider the polynomialF=x12+x22−1in the domain[0,1]×[0,1](see also Fig. 2). We can bound the first monomial term from above by the hyperplanef1max=x1(since in the domainx12≤x1) and from below byf1min=x1−0.25(since in the domainx12≥x1−0.25). Similarly, for the second term the bounds will bef2max=x2andf2min=x2−0.25. The total bounds onf1+f2=x12+x22is therefore:x1+x2−0.5≤x12+x22≤x1+x2.For the unit term (and also for linear terms) the bound is tight and thusf3min=−1=f3max. When we addf3to the expression we received (i.e., subtract 1 from the upper and lower bounds), we get the total bounds onF=x12+x22−1(see Fig. 2):x1+x2−1.5≤x12+x22−1≤x1+x2−1.Bounding any monomial term by two hyperplanes can be done using the procedure described in Section  2.1. We store the different term types as representative multivariate hypersurfaces in free-form representation.22In our implementation we use IRIT  [25] multivariates.For example, for a degree-3 system we store the following types of hypersurfaces: one-variable hypersurfaces–x2,x3, two-variable hypersurfaces–xy,x2y, and a three-variable hypersurfacexyz. When a monomial term of typex12x2is encountered, for example, we reduce the representative hypersurfacex2yto the subdomain[x1min,x1max]×[x2min,x2max]and bound thex2yhypersurface in the new subdomain by two hyperplanesKmin(x1,x2)andKmax(x1,x2).Once a monomial termMj(x)is bounded by two hyperplanesKjminandKjmax, we multiply it by its constant coefficientcj. If the coefficient is positive, then we just multiply the hyperplane coefficients bycj. For a negativecj, however, we have to switch between the lower and upper hyperplanes since if:Kmin≤f≤Kmaxandcj<0then:cjKmax≤cjf≤cjKmin.Thus, to bound the polynomialFi(x)=∑jcjMj(x), the bounding procedure, goes over all termsMj, bounds them, and multiplies by their coefficientcj. The upper and lower bounding hyperplanesKijminandKijmaxof each term are summed resulting in two hyperplanesKiminandKimaxthat boundFi.The advantage of hyperplane arithmetic over interval arithmetic (for addition and subtraction) is that it gives tighter bounds since the bound is a linear function and not a constant function. For linear functions, the hyperplane arithmetic is actually exact. Furthermore, the linear bounds can then be used by an efficient LP method for domain reduction.The advantage of hyperplane arithmetic over the naive method from Section  2.1 is that it does not require the full dense Bézier representation for its computation. The sparse monomial representation suffices to compute the bounding hyperplanes. In our implementation we use a sparse representation that stores only the monomial terms actually participating in the system (i.e.,O(nd)in the worst case, but less for sparse systems).Using all the building blocks presented in the preceding sections, our algorithm is now straightforward. Basically, we implement the naive LP algorithm from Section  2.2, but use the hyperplane arithmetic from Section  2.3 to bound the function from above and below.The algorithm can therefore be described as follows:Algorithm: LP_root_isolation_using_HP_arithmeticInput:F(x1,…,xn),Boxb[x1min,x1max]×⋯×[xnmin,xnmax]Output:list〈Box〉boxes(1)If(max(ximax−ximin)<ϵ)appendbto outputboxesand return.BoundFinbusing hyperplane arithmetic and LP:(a)BoundFiby two parallel hyperplanes, using hyperplane arithmetic of bounds on its terms (Section  2.3).Perform2nLP problems to compute new[ximin,ximax]values for eachxi.If one of the LP problems is infeasible, return (there is no solution inb).Setbto be the new domain with the new[ximin,ximax]for eachxi.Splitbinto subdomains,b1,b2along the maximal axis ofb.LP_root_isolation_using_HP_arithmetic(F,b1,boxes).LP_root_isolation_using_HP_arithmetic(F,b2,boxes).Note that unlike the dense BézierB-spline representation, the splitting in Steps (3) and (2.d) does not require a full subdivision process in a sparse representation. Only the box is updated and the subdivision is applied for each term when bounding it in Step (2.a).

@&#CONCLUSIONS@&#
