@&#MAIN-TITLE@&#
Learning gaze biases with head motion for head pose-free gaze estimation

@&#HIGHLIGHTS@&#
Appearance-based gaze estimation with head motion is decomposed into subproblems.Subproblems are solved by compensating for two types of estimation biases.The compensation method only requires a 5-second video for training.Eye images for different head poses are aligned via rectification and optimization.We achieve a gaze estimation accuracy of 3° with free head motion.

@&#KEYPHRASES@&#
Gaze estimation,Free head motion,Head pose compensation,Appearance-based approach,

@&#ABSTRACT@&#
Graphical abstract

@&#INTRODUCTION@&#
Human gaze plays an essential role in conveying human visual attention, desire, feeling, intention, and so on [1]. Therefore, research on human gaze estimation has been an important topic since decades ago [2] and has attracted much attention in recent years. There are already systems in the market that have been used in many applications, such as human–computer interfaces, cognitive study, market research, and driver training. However, these existing systems usually require expensive devices, which stop them from being widely used by ordinary users in their everyday lives.With the fast development of computer vision technology, it seems possible that human gaze direction can be estimated by only using captured eye images without requiring too much additional hardware. This will greatly enhance the applicability of gaze estimation technology. According to recent surveys [3,4], existing computer vision-based methods can be roughly divided into two categories: model-based methods and appearance-based methods. Methods in the first category assume 3D or 2D eyeball models to estimate gaze directions, while they still need some additional devices such as infrared lights and cameras. Therefore, they are more suitable for use in a controlled environment such as in the laboratory. Appearance-based methods have the advantage of using only a single common camera, and thus, they attract increasing attention.In this paper, we focus on the appearance-based methods and solve the major difficulty in allowing free head motion. This problem is difficult because head motion changes eye appearance greatly, and therefore, gaze estimation with changed eye appearances will be inaccurate. A straightforward way to solve this problem is to store all the eye appearances for every individual head pose in the system. However, because head motion has 6 degrees of freedom, it becomes impractical to directly deal with all possible eye appearances due to head motion.The key idea of this paper is to decompose the originally difficult problem into simple subproblems and solve them efficiently. In particular, instead of considering gaze estimation for arbitrary head poses, we first apply gaze estimation by assuming a fixed head pose, and then compensate for the estimation biases caused by the head pose difference. The compensation comprises two stages, namely, eye appearance distortion compensation and geometric compensation. We propose methods to complete these compensations and show that the gaze estimation can be done accurately for arbitrary head poses.The primary contributions of this work are:(1)A gaze estimation approach that decomposes the difficult free head motion problem into much easier subproblems is proposed.Subproblems are solved with our proposed compensation methods, which correct gaze estimation biases due to both eye appearance distortion and geometric factors.The training cost for compensating for head motion is low. It only requires capturing a short video clip of user free head motion for a duration of 5s.Eye images captured under different head poses are robustly aligned via image rectification and iterative optimization.Overall, compared with existing appearance-based methods, our method does not use any other devices, while it allows for head motion by only requiring an additional calibration step to capture a short video clip. If compared with existing model-based methods that also allow for head motion, our method only uses a common camera with known location and intrinsic parameters, while most of the others use multiple cameras/lights and therefore require more complicated camera/geometric calibrations.

@&#CONCLUSIONS@&#
