@&#MAIN-TITLE@&#
Multimodal integration learning of robot behavior using deep neural networks

@&#HIGHLIGHTS@&#
Novel computational framework for sensory-motor integration learning.Cross-modal memory retrieval utilizing a deep autoencoder.Noise-robust behavior recognition utilizing acquired multimodal features.Multimodal causality acquisition and sensory-motor prediction.

@&#KEYPHRASES@&#
Object manipulation,Multimodal integration,Cross-modal memory retrieval,Deep learning,

@&#ABSTRACT@&#
For humans to accurately understand the world around them, multimodal integration is essential because it enhances perceptual precision and reduces ambiguity. Computational models replicating such human ability may contribute to the practical use of robots in daily human living environments; however, primarily because of scalability problems that conventional machine learning algorithms suffer from, sensory-motor information processing in robotic applications has typically been achieved via modal-dependent processes. In this paper, we propose a novel computational framework enabling the integration of sensory-motor time-series data and the self-organization of multimodal fused representations based on a deep learning approach. To evaluate our proposed model, we conducted two behavior-learning experiments utilizing a humanoid robot; the experiments consisted of object manipulation and bell-ringing tasks. From our experimental results, we show that large amounts of sensory-motor information, including raw RGB images, sound spectrums, and joint angles, are directly fused to generate higher-level multimodal representations. Further, we demonstrated that our proposed framework realizes the following three functions: (1) cross-modal memory retrieval utilizing the information complementation capability of the deep autoencoder; (2) noise-robust behavior recognition utilizing the generalization capability of multimodal features; and (3) multimodal causality acquisition and sensory-motor prediction based on the acquired causality.

@&#INTRODUCTION@&#
Humans are known to perceive the external environment, including their own body, by utilizing multiple sources of sensory information, such as vision, audition, and proprioception. To this end, multimodal integration contributes to forming constant, coherent, and robust perceptions by reducing ambiguities regarding sensory environment. Cognitive science research has revealed that by combining sensory information, humans achieve enhanced perceptual clarity and reduced ambiguity regarding their environment  [1,2]. Further, action–effect causality perception is known to have a close relationship with the sense of agency  [3], and thus cross-modal grouping plays an important role for sensation  [4]. Hence, we believe that replicating human multimodal integration learning as a computational model is essential toward realizing sophisticated cognitive functions of robot intelligence, as well as toward fundamentally understanding human intelligence.Unfortunately, multimodal integration has long been a challenging problem in robotics  [5,6]. Although there is relevant research reported in the literature  [7–9], several issues still remain unsolved. First, multimodal sensory-motor integration has typically been applied only to a singular problem, such as self-organizing one’s spatial representation  [7,8]; further functions have not been intensively studied, including such functions as the cross-modal complementation of information deficiencies or the application of cross-modal memory retrieval for behavior generation problems. Second, discussion in the literature regarding how multimodal information should be fused together to realize stable environmental recognition has not reached a comprehensive consensus. Thus, a prevailing multimodal information integration framework has not been available. Subsequently, in robotics, sensory inputs acquired from different sources are still typically processed with dedicated feature-extraction mechanisms  [10]. Third, multimodal causality modeling as a means to implementing sensory-motor prediction for robotic applications has not been adequately investigated. Several preceding studies have proposed computational models developmentally acquiring action–effect causality toward understanding interaction rules  [11,12]; however, most causal models have been represented using a limited number of modalities, often focused on vision and motion.A scalable learning framework that enables multimodal integration learning by handling large amounts of sensory-motor data with high dimensionality has not yet been realized. In line with the growing demand for perceptual precision in regard to the surrounding environment, recent robots are equipped with state-of-the-art sensory devices, such as high-resolution image sensors, range sensors, multichannel microphones, and so on  [13–15]. As a result, remarkable improvements have been achieved in the quantity of available sensory information; however, because of scalability limitations of conventional machine learning algorithms, groundbreaking computational models achieving robust behavior control and environmental recognition by fusing multimodal sensory inputs into a single representation have not yet been proposed.Regarding computational models addressing large-scale data processing with significant dimensionality  [16], deep learning approaches have recently attracted increasing attention in the machine-learning community  [17]. For example, deep neural networks (DNNs) have successfully been applied to unsupervised feature learning for single modalities, such as text  [18], images  [19], or audio  [20]. In such studies, various information signals, even with high-dimensional representations, were effectively compressed in a restorable form. Further, brilliant achievements in deep learning technologies have already succeeded in making advanced applications available to the public. For example, competition results from the ImageNet Large Scale Visual Recognition Challenge  [21] have led to significant improvements in web image search engines  [22]. As another example, unsupervised feature-extraction functions of deep learning technologies have greatly increased the sophistication of a voice recognition engine used for a virtual assistant service  [23]. The same approach has also been applied to the learning of fused representations over multiple modalities, resulting in significant improvements in speech recognition performance  [24]. Yet another study on multimodal integration learning has succeeded in cross-modal memory retrieval by complementing missing modalities  [25]. Most current studies on multimodal integration learning utilize deep networks; however, much work focuses in extracting correlations between static modalities, such as image and text  [21]. Thus, few studies have investigated methods not only for multimodal sensor fusion, but also for dynamic sensory-motor coordination problems  [26] of robot behavior.Our interest in this current study is to investigate the fundamental principles reported by the cognitive science studies and put their findings to practical use by constructing a computational model. In particular, we set out to demonstrate the following functions of our proposed model through experimentation on the sensory-motor coordination learning of robot behavior:•Cross-modal memory retrieval utilizing the information complementation capability of the deep autoencoder.Noise-robust behavior recognition utilizing the generalization capability of multimodal features.Multimodal causality acquisition and sensory-motor prediction based on the acquired causality.As a practical computational model, we construct a multimodal temporal sequence learning framework based on a deep learning algorithm  [27]. The proposed framework first compresses the dimensionality of the sensory inputs acquired from multiple modalities utilizing the same techniques as previous work on dimensionality compression via a deep autoencoder  [27,28]. In combination with a variant of a time-delayed neural network  [29] learning approach, we then introduce a novel deep learning framework that integrates sensory-motor sequences and self-organizes higher-level multimodal features. Further, we show that our proposed temporal sequence learning framework can internally generate temporal sequences by partially masking the input data from outside the network and recursively feeding back the previous outputs to the masked inputs nodes, which is made possible by utilizing the characteristics of an autoencoder that models identity mappings between inputs and outputs.We evaluate our proposed sensory-motor integration learning framework via two behavior learning experiments. First, we train our proposed model with six different object manipulation behaviors of a humanoid robot, generated by direct teaching. Results demonstrate that our proposed method can retrieve temporal sequences over visual and motion modalities and predict future sequences from the past. Further, behavior-dependent unified representations that fuse sensory-motor modalities together are extracted in the temporal sequence feature space. Our behavior recognition experiment, which utilizes the integrated features acquired from the multimodal temporal sequence learning mechanism, demonstrates that the multimodal features significantly improve the robustness and reliability of behavior recognition performance by utilizing joint angle information.In the second experiment, we extend the multimodal integration learning by incorporating sound signals in addition to the image and joint angles. We designed a bell-ringing task performed by the same robot and trained the proposed model utilizing sensory-motor sequences consist of the three modalities. To this end, a model representing the cross-modal causal dependency is self-organized in the abstracted feature space of our proposed model. Results demonstrated that the cross-modal memory retrieval function of our proposed model succeeds in predicting visual sequences in correlation with the sound and joint angles of bell-ringing behaviors. Further, analyzing image retrieval performance, we found that our proposed method correctly models the causal dependencies among the multimodal information.In addition to this introductory section, our paper is organized as follows. In Section  2, we briefly review Hessian-free optimization for training deep networks. In Section  3, we describe the general framework of multimodal temporal sequence learning. In Section  4, we present the practical application and effectiveness of our proposed model by the multimodal integration learning of object manipulation behavior utilizing a humanoid robot. Next, in Section  5, we move on to the bell-ringing task, in which we demonstrate how our proposed model acquires the causality between multiple modalities and supports cross-modal information retrieval based on the causal relationship among multiple modalities. In Section  6, we examine and discuss our proposed framework and results in relation to previous work. Finally, we conclude our work in Section  7.Deep neural networks (DNNs) are artificial neural network models with multiple layers of hidden units between inputs and outputs. Hinton et al. first proposed an unsupervised learning algorithm to use greedy layer-wise unsupervised pretraining followed by fine-tuning methods for overcoming high prevalence of unsatisfactory local optima in learning objectives of deep models  [27]. Subsequently, Martens proposed a novel approach by introducing a second-order optimization method–Hessian-free optimization–for training deep networks  [28]. The proposed method efficiently trained the models by a general optimizer without pretraining. Here, we adopt learning methods proposed by Martens for optimizing multiple autoencoders, for the self-organization of feature vectors, and for temporal sequence learning.The Hessian-free algorithm originates with Newton’s method, a well-known numerical optimization technique. A canonical second-order optimization scheme such as Newton’s method iteratively updates parameterθ∈RNof an objective functionfby computing gradient vectorp, and updatesθasθn+1=θn+αpnwith learning parameterα. The core idea of Newton’s method is to locally approximatefaround eachθ, up to the second order, by quadratic equation,(1)Mθn(θ)≡f(θn)+∇f(θn)Tpn+12pnTBθnpn,whereBθnis a damped Hessian matrix offatθn. AsHcan become indefinite, the Hessian matrix is re-conditioned to beBθn=H(θn)+λI, whereλ≥0is a damping parameter andIis the unit matrix.Using the standard Newton’s method,Mθn(θ)is optimized by computingN×NmatrixBθnthen solving systemBθnpn=−∇f(θn)T. This computation, however, is very expensive for largeN, which is a common case even with modestly sized neural networks. To overcome this issue, the variant of Hessian-free optimization developed by Martens utilizes the linear conjugate gradient (CG) algorithm for optimizing quadratic objectives in combination with the use of a positive semidefinite Gauss–Newton curvature matrix in place of the possibly indefinite Hessian matrix. The name “Hessian-free” indicates that the CG does not necessarily require the costly explicit Hessian matrix; instead, the matrix–vector product between the Hessian matrixHor the Gauss–Newton matrixGand gradient vectorpis sufficient (for more details on the concrete implementation, see  [28,30,31]).In this paper, we propose to apply a deep autoencoder not only for its feature extraction by dimensionality compression but also for its multimodal temporal sequence integration learning. Our main contribution in this study is to demonstrate that our proposed framework serves as a cross-modal memory retriever, as well as a temporal sequence predictor utilizing its powerful generalization capabilities. In the subsections that follows, we first illustrate the basic mechanism of the autoencoder, then explain how the autoencoder is applied for its multimodal temporal sequence learning and further functions.High-dimensional raw sensory inputs, such as visual images or sound spectrums, can be converted to low-dimensional feature vectors by multilayer networks with a small central layer (i.e. a feature-extraction network)  [27]. To this end, the networks are trained with the goal of reconstructing the input data at the output layer with input–output mappings defined as(2)ut=f(rt)(3)rˆt=f−1(ut),wherert,ut, andrˆtare the vectors representing the raw input data, the corresponding feature, and the reconstructed data, respectively. Functionsf(.)andf−1(.)represent the transformation mapping from the input layer to the central hidden layer and the central hidden layer to the output layer of the network, respectively. An autoencoder compresses the dimensionality of inputs by decreasing the number of nodes from the input layer to the central hidden layer. Hence, the number of central hidden layer nodes determines the dimension of the feature vector. In a symmetric fashion, the original input is reconstructed from the feature vector by eventually increasing the number of nodes from the central hidden layer to the output layer.Regarding dimensionality compression mechanisms, a simple and commonly utilized approach is principal component analysis (PCA); however, Hinton et al. demonstrated that the deep autoencoder outperformed PCA in image reconstruction and compressed feature acquisition  [27]. In reference to their work, we utilized the deep autoencoder for our dimensionality compression framework because we prioritized the precision of cross-modal memory retrieval and the sparseness of acquired features to ease the behavior recognition task via a conventional classifier.A time-delay neural network (TDNN) is a method for utilizing a feed-forward neural network for multi-dimensional temporal sequence learning  [29]. Motivated by TDNN, we propose a novel computational framework that utilizes a deep autoencoder for temporal sequence learning.An input to the temporal sequence learning network at a single time step is defined by a time segment of the tuple of joint angle vectors, image feature vectors, and sound feature vectors, formatted as(4)st=(at,uti,uts)(5){t|t−T+1≤t≤t},wherest,at,uti, andutsare the vectors representing the input to the network, the joint angle, the image feature, and the sound feature, at timet, respectively, andTis the length of the time window. Here,trepresents the previousTsteps of the temporal segment fromt, and a vector with subscripttindicates a time series of the vector. The input–output mappings of the temporal sequence learning network are defined as(6)vt=g(st)(7)sˆt=g−1(vt),wherevtandsˆt=(aˆt,uˆti,uˆts)are the multimodal feature vector and a segment of the restored multimodal temporal sequence, respectively. Functionsg(.)andg−1(.)represent the transformation mapping from the input layer to the central hidden layer and the central hidden layer to the output layer of the network, respectively.One of the merits of applying neural networks for multimodal temporal sequence learning is their generalization capability. Because the network can complement deficiencies in the input data, the temporal sequence learning network can be used in two different ways: (1) to retrieve a temporal sequence from one modal for use in another and (2) to predict a future sequence from the past sequence. Thus, the temporal sequence learning network serves as a cross-modal memory retriever or a temporal sequence predictor by masking the input data from outside the network in either spatial or temporal ways; thus iteratively feeding back the generated outputs to the inputs as substitutions for the masked inputs. The practical implementation of these functions is described in the following subsections.Cross-modal memory retrieval is realized by self-generating sequences for a modality inside the network by providing corresponding sequences for the other modalities from outside the network. For the retrieved modality, a recurrent loop from the output nodes to the input nodes is prepared. Hence, in the case of generating an image sequence from motion and sound sequences, input to the network is defined as(8)st=(at,uˆti,uts).As shown in Fig. 1, the time segment of the recurrent input is generated by shifting the previous output of the network to the direction for one step by (1) discarding the oldest time step output and (2) filling the latest time step with the value of the newest time step acquired from the output.Similarly, the temporal sequence prediction is realized by constructing a recurrent loop from the output layer to the input layer. The difference is that among all theTsteps of the time window, only the firstTinsteps (i.e., the pastTinshifts to the present time stept) of both modalities are filled with the input data; the rest (i.e., the futureT−Tinshifts to the predicted time step) are filled with the outputs from the previous time step. Hence, input to the network is defined as(9)s(t)=(at1,aˆt2,ut1i,uˆt2i,ut1s,uˆt2s),(10){t1|t−Tin+1≤t1≤t},(11){t2|t+1≤t2≤t+(T−Tin)}.As shown in Fig. 2, the prediction segment of the recurrent input is generated by shifting the corresponding previous outputs of the network to the time direction for one step.Fig. 3depicts a schematic diagram of our proposed framework. Two independent deep neural networks are utilized for image compression and temporal sequence learning. The image compression network, shown in Fig. 3(a), inputs raw RGB color images acquired from a camera mounted on the head of the robot and outputs the corresponding feature vectors from the central hidden layer. The image features are synchronized with the joint angle vectors acquired from both arm joints and multimodal temporal segments are generated. The multimodal temporal segments are then fed into the temporal sequence learning network (i.e.,  Fig. 3(b)). Accordingly, multimodal features are acquired from the central hidden layer, while reconstructed multimodal temporal segments are obtained from the output layer.The outputs from the temporal sequence learning network are used for both robot motion generation and image retrieval. The joint angle outputs from the network are rescaled and resent to the robot as joint angle commands for generating motion. The network can also reconstruct the retrieved images in the original form by decompressing the image feature outputs, because the image compression network models the identity map from the inputs to the outputs via feature vectors in the central hidden layer.Our proposed mechanisms are evaluated by conducting object manipulation experiments with the small humanoid robot NAO, developed by Aldebaran Robotics  [32]. The multimodal data, including image frames and joint angles, are recorded synchronously at approximately 10 fps. For the image data input, the original 320×240 image is resized to a 20×15 matrix of pixels in order to meet the memory resource availability limitation of our computational environment.11We utilized a personal computer with an Intel Core i7-3930K processor (3.2 GHz, 6 cores), 32 GB main memory, and a single Nvidia GeForce GTX 680 graphic processing unit with 4 GB on-board graphics memory. Because the size of weight matrices of a multi-layered neural network exponentially increases as the input dimension increases, we felt it sensible to keep the number of input dimensions as small as possible, as long as the dimensionality reduction did not critically degrade the quality of our experiments. As a result of preliminary experimentation, we found all of our memory retrieval experiments are feasible even with this reduced image resolution.For joint angle data input, 10 degrees of freedom of the arms (from the shoulders to the wrists) are used.As shown in Fig. 4, six different object manipulation behaviors identified by different colorful toys are prepared for training. We record the multimodal temporal sequence data by generating different arm motions corresponding to each object manipulation by direct teaching. The resulting lengths of the motion sequences are between 100 and 200 steps, which is equivalent to between 10 and 20 s. To balance the total motion sequence lengths between different behaviors, direct teaching is repeated six to 10 times for each behavior, such that the number of repetitions becomes inversely proportional to the motion sequence length. Among all of the repetitions, one result is used as test data and the others are used as training data. For multimodal temporal sequence learning, we use a contiguous segment of 30 steps from the original time series as a single input. By sliding the time window by one step, consecutive data segments are generated.Table 1summarizes the datasets and associated experimental parameters. For both the image feature and temporal sequence learning, the same 12-layer deep neural network is used. In each case, the decoder architecture is a mirror-image of the encoder, yielding a symmetric autoencoder. The parameter settings of the network structures are empirically determined in reference to such previous studies as  [27] and  [33]. The input and output dimensions of the two networks are defined as follows: 900 for image feature learning, which is defined by 20×15 matrices of pixels for the RGB colors; and 1200 for temporal sequence learning, which is defined by the 30-step segment of the 40-dimension multimodal vector composed of 10 joint angles and the 30-dimension image feature vector. For the activation functions, linear functions are used for both the central hidden layers and logistic functions are used for the rest of the layers in reference to  [27].The length of the time window is determined by considering the following two constraints. First, if the length of the time window increases, the network may consider longer contextual information. Second, if the length of the time window becomes too long, the dimension of the multimodal temporal vector becomes too big to be processed in an acceptable amount of time. The implicit policy is to keep the input dimensions below 3000 because of our computational limitation. As the multimodal vector dimension is 40, the temporal sequence length should be below 75. Considering the cyclic frequencies of the joint angle trajectories acquired from the six object manipulation behaviors, we determine that 30 steps are enough to characterize a phase of the behaviors.For multimodal integration learning, we trained the temporal sequence learning network using additional examples that have only a single modality to explicitly model the correlations across the modalities  [24]. In practice, we added examples that have noisy values for one of the input modalities (e.g., the image feature) and original values for the other input modality (e.g., the joint angles) but still require the network to reconstruct both modalities. Thus, one-third of the training data has only image features for input, while another one-third of the data has only joint angles and the last one-third has both image features and joint angles. For the noisy values, we superimpose Gaussian noise with a standard deviation of 0.1 on the original data.We conducted two experiments to evaluate cross-modal memory retrieval performance. One experiment generates the joint angle sequence (motion) by providing image sequences, whereas the other generates an image sequence by providing the joint angle sequence. For these experiments, inputs to either modality of the full 30 steps are provided, and the sequence for the other modality is internally generated in a closed-loop manner (see Section  3.3). In the experiment to evaluate temporal sequence prediction, the input window length is defined asTin=25, and the corresponding future five steps are internally generated as predictions (see Section  3.4). For all of the experimental settings above, although the initial values for the recurrent inputs are randomly generated, the internal values eventually converge to the corresponding states in association with the input values of the other modalities by the generalization capability of the network.Fig. 5shows the example results of joint angle sequence generation from the image sequence input and temporal sequence prediction. We generated full length trajectories of the object manipulation behavior by accumulating the iteratively retrieved joint angle vectors acquired from the 30th (final) step of the temporal window. In the figure, graphs on the top row (Fig. 5(a)) are the original motion trajectories in the test data. Graphs on the second row (Fig. 5(b)) show that the appropriate trajectories are generated and the configurations of the trajectories are clearly differentiated according to the provided image sequences. Graphs on the bottom row (Fig. 5(c)) show that our proposed mechanism correctly predicted future joint angles at five steps ahead of the 25 steps of the multimodal temporal sequence. The low reconstruction qualities of the first 30 steps are attributed to the random values supplied for the recurrent inputs at the initial iteration of the generation process.Fig. 6shows example results of image sequence generation from the joint angle sequence input. The images shown in the figure are single frames drawn from the series of images for each behavior. Although the details of the images are slightly different, the objects showing up in the images are correctly reconstructed, and the locations of the color blobs are properly synchronized with the phases of the motion.We conducted a quantitative evaluation of cross-modal memory retrieval by preparing 10 different initial model parameter settings for the networks and replicating the experiment of learning the same dataset composed of the six object manipulation behaviors. Table 2summarizes these results. In the table, IMG→MTN indicates image to motion, whereas MTN→IMG indicates motion to image; further, the temporal sequence prediction (PRED) performances for the six behavior patterns are also shown. The numbers given in each entry of the table represent the root mean square (RMS) errors of the reconstructed trajectories (normalized by scaling between 0 and 1) on the test data. The RMS errors in Table 2 demonstrate that the reconstruction errors are below 10% for all of the evaluation conditions.In detail, each of the RMS errors are calculated as(12)EIMG→MTN=1Tseq∑t=1Tseq|ãt−aˆt|2,(13)EMTN→IMG=1Tseq∑t=1Tseq|r̃ti−rˆti|2,(14)EPRED=1Tseq∑t=1Tseq|s̃t−sˆt|2,whereEIMG→MTN,EMTN→IMG, andEPREDare RMS errors corresponding to the reconstruction modes identified by subscripts;ãt,aˆt,r̃ti,rˆti,s̃t, andsˆtare the truth and reconstructed vectors representing the raw image data, the joint angle, and the multimodal feature at timet, respectively; andTseqis the length of the test sequence for each of the behaviors.Finally, to analyze the temporal sequence prediction performance in more detail, we evaluated the prediction errors at the last (30th) step of the time window, depending on the prediction length, by varying the input window lengthTinfrom 25 to five in decreasing steps of five. As expected, the RMS errors, as shown in Fig. 7, demonstrate that prediction error increases as prediction length increases. Nevertheless, the reconstruction errors are below 10% in all of the evaluation conditions.As a further experiment, we switched the robot’s behavior according to changes in the objects displayed to the robot. The approach is a combination of cross-modal memory retrieval and temporal sequence prediction in the sense that the joint angles five steps ahead, considering control delay, are predicted from the previous 25 steps of the image input sequence. By iteratively sending the predicted joint angles as the target commands for each joint angle of the robot, the robot generates motion in accordance with environmental changes. For the initial trial, we tested the raw image input and confirmed that the robot can properly select behaviors according to changes of the displayed object. However, we found that the reliability of our current image feature vector is easily affected by the environmental lighting conditions.22We recognize that the instability of the image feature vector under real environment is due to the limitation on the variation in our image dataset utilized for training the image feature-extraction network.Therefore, we adopted the color region segmentation and used the coordinates of the center of gravity of the color blobs as a substitution to the image feature vector for the perception stability under various lighting conditions. As a result, we succeeded in switching multiple behaviors based on the displayed objects. Fig. 8shows photos of the transition from one behavior to the next.Fig. 9presents the scatter plot of the three-dimensional principal components of the acquired multimodal features. The multimodal feature vectors are generated by recognizing the training data from the temporal sequence learning network and recording the activations of the central hidden layer. This figure demonstrates that the feature space is segmented according to the different object manipulation behaviors and the feature vectors are self-organizing multiple clusters. The structure of the multimodal feature space suggests that a supervised discrimination learning of multiple behaviors might be possible by modeling correspondences between the acquired multimodal features and the behavior categories.In this subsection, we examine how the acquired multimodal feature expression contributes to the robustness of a behavior recognition task. In our learning framework, raw sensory inputs are converted to sensory features, and the multiple sources of sensory features are integrated together to generate multimodal features utilizing the dimensionality compression function of an autoencoder. Making efficient use of the higher-level features, we can expect the following two effects in the behavior recognition task: (1) a discrimination model can improve its categorization performance against noisy sensory inputs by exploiting the higher generalization capabilities of the compressed representations; and (2) the integrated representation of multimodal inputs helps to inhibit the degradation of categorization performance by complementing a decrease in reliability of sensory input with information from the other modalities.To verify our hypotheses, we evaluated the noise robustness of a behavior discrimination mechanism under different training conditions using the joint angle test sequences corresponding to the six object manipulation behaviors. More specifically, we compare the variation in behavior recognition rates depending on the differences of the standard deviation of Gaussian noise superimposed on the joint angle sequences. To investigate the effects of the higher-level features acquired from dimensionality compression and multimodal integration, we compare the performance of the classifier under the following four different training conditions:•(1a) MTN (raw): Raw joint angles are used as inputs.(1b) MTN (compressed): Joint angle feature vectors are used as inputs. Feature vectors are generated by compressing the joint angle sequences utilizing an autoencoder.33The structure of the autoencoder used in (1b) is as same as that of the temporal sequence learning network used for the multimodal integration learning in (2a), except that the image feature inputs are excluded.(2a) MTN + IMG: Multimodal feature vectors are used as inputs. Feature vectors are generated by compressing the joint angle sequences and the corresponding image feature sequences utilizing the temporal sequence learning network. Image feature sequences are generated by compressing the clean image sequences acquired from the test data.(2b) MTN + IMG (imaginary): Multimodal feature vectors are used as inputs. In this case, the image feature sequences are self-generated inside the network instead of externally generated from the test data.The compressed feature vector sequences are acquired by recording the activation patterns of the central middle layer of the temporal sequence network. As one of the most popular classification algorithms with an excellent generalization capability, a support vector machine (SVM)–namely, the multi-class SVM using one-against-all decomposition in the Statistical Pattern Recognition Toolbox for MATLAB  [34]–is used as a classifier. An RBF kernel with default parameters (provided by the toolbox) is used to address the one-against-all multiclass non-linear separation of the acquired multimodal features; further, the Sequential Minimal Optimizer (SMO) is used as the solver for the computational efficiency.Fig. 10shows the variations of the behavior recognition rates depending on the changes in standard deviation of the Gaussian noise superimposed on the joint angle sequences. The results demonstrate three remarkable advantages of utilizing higher-level features for the behavior recognition task. First, comparing results of (1b) with (1a) shows the superior performance of compressed joint angle features over raw joint angles in regards to behavior recognition robustness. Second, comparing results of (2a) with (1b) shows that the multimodal features manifest higher noise robustness over single modal features by suppressing the negative effects caused by the degradation of the reliability of joint angles; this is achieved by making effective use of the complementary information from the image features. Third, comparing results of (2b) with (1b) demonstrates that even when the joint angle modality is provided as the sole input, the self-generated sequences for the image features still help to prevent degradation in behavior recognition performance. From these results, we confirmed our hypotheses that the use of higher-level features acquired by compressing raw sensory inputs and integrating multimodal sequences contribute to noise resistance of the behavior recognition tasks.In the previous experiments, we demonstrated that our proposed framework succeeds in cross-modal memory retrieval and stable behavior recognition utilizing the self-organized multimodal fused representations. In this section, we extend the experimental setting by incorporating sound signals as another input modality. Through our experimentation, we investigated how our proposed framework extracts the intersensory causality from the sensory-motor experience in the environment and predicts the sensory outcomes utilizing the acquired causality model.Fig. 11shows a schematic diagram of our proposed framework. Three independent deep neural networks (i.e., autoencoders) are utilized for sound compression, image compression, and temporal sequence learning. The sound data acquired from a microphone mounted on the head of the robot is preprocessed by discrete Fourier transform (DFT). The sound compression network (Fig. 11(a)) inputs the acquired sound spectrums and outputs the corresponding feature vectors from the central hidden layer. Similarly, the image compression network (Fig. 11(b)) inputs raw RGB bitmap images acquired from a camera mounted on the head of the robot and outputs the corresponding feature vectors. The sound and image features are synchronized with the joint angle vectors, and multimodal temporal segments are generated. These multimodal temporal segments are then fed into the temporal sequence learning network (Fig. 11(c)). Accordingly, multimodal features and reconstructed multimodal segments are output from the central hidden layer and the output layer of the network, respectively.The outputs from the temporal sequence learning network can be used for robot motion generation, sound spectrum retrieval, or image retrieval. The joint angle outputs from the network are rescaled and resent to the robot as joint angle commands for generating motion. The networks can also reconstruct the retrieved sound spectrum or images in the original form by decompressing the corresponding feature outputs because the sound compression network and the image compression network model the identity map from the inputs to the outputs via feature vectors in the central hidden layer.The cross-modal memory retrieval performance of our proposed mechanisms is evaluated by conducting bell-ringing tasks with the same robot used in our first experiment. The bell-ringing task is setup as follows: three different desktop bells, which can be identified by either the surface color or the sound pitch, are prepared for the experiment. Correspondences between the colors and the pitch notations are shown in Fig. 12(a). For each bell-ringing trial, two bells are selected and placed in front of the robot side by side. Then, either one of the two bells is rung by hitting a button on top of the bell. Due to the limited outreach of the hands, each side of the bell can be rung only with the corresponding arms. As shown in Fig. 12(b), there are six possible bell placement combinations. Note that under the task configuration, information from at least two different modalities is required to determine the right bell-ringing situation. In practice, the robot cannot (1) determine which bell is going to be rung only from the initial image, (2) determine the placement of the ringing bell only from the sound, and (3) predict what sound will come out only from the arm motion.We record twelve different multimodal temporal sequence datasets by generating the right and left bell-striking motions under the six different bell placement configurations. Arm joint angle sequences corresponding to the bell-striking motions are generated by the angular interpolation of the initial and target postures. Pulse-code modulation (PCM) sound data is recorded with a 16 kHz sampling rate, a 16-bit depth, and a single channel with a microphone mounted on the forehead of the robot.44Because of the physical structure of the robot, the microphone is located close to both of the arms, which are utilized to hit the bells. Therefore, the actuation sounds of the geared reducers equipped to the arm joints are inevitably recorded in addition to the bell sounds. To avoid the degradation of memory retrieval performance arising from the actuation sounds, we introduced a brief pause to the bell hitting motion when the hand contacted the button on top of the bell.The image frames and the joint angles of both arms are recorded at approximately 66 Hz, which includes replicated image frames. To synchronize the sound data with the image and joint angles data, the sound data is preprocessed by a DFT with a 242-sample hamming window and 242 samples of window shift with no overlap. A partial region of 320×200 pixels is cropped from the original 320×240 image and resized to 40×25 pixels to meet the memory resource availability limitation on our computational environment. For the joint angle data input, 10 degrees of freedom of the arms (from the shoulders to the wrists) are used. The resulting lengths of the motion sequence were approximately 200 steps each, which is equivalent to about 3 s each. For multimodal temporal sequence learning, we used contiguous segments of 30 steps from the original time series as a single input. By sliding the time window by one step, consecutive data segments are generated.Table 3summarizes the datasets and associated experimental parameters. For both the sound feature and the image feature learning, the same 12-layered deep neural networks are used. For temporal sequence learning, a 10-layer network is used. In each case, the decoder architecture is a mirror-image of the encoder, yielding a symmetric autoencoder. The parameters for the network structures are empirically determined in reference to such previous studies as  [27] and  [33]. The input and output dimensions of the two networks are defined as follows: 968 for sound feature learning, which is defined by binding consecutive four-step sequences of the sound spectrums (i.e., 242 dimension) into a single vector, 3000 for the image feature learning, which is defined by 40×25 matrices of pixels for RGB colors, and 2100 for temporal sequence learning, which is defined by the 30-step segment of the 70-dimensional multimodal vector composed of a 30-dimensional sound feature vector, a 30-dimensional image feature vector, and 10 joint angles. Especially for the central hidden layer of the temporal sequence learning network, we compared several numbers of nodes, i.e., 30, 50, 70, and 100. By evaluating the performance of image retrieval from the sound and joint angle inputs, we concluded that 100 nodes are needed to achieve the desired memory reconstruction precision. For the activation functions, linear functions are used for all of the central hidden layers, and logistic functions are used for the rest of the layers in reference to  [27].We conducted an evaluation experiment of the cross-modal memory retrieval performance by generating image sequences from the sound and joint angle input sequences. Note that in the following results, the number of sequence steps indicates the generation step rather than the recorded data step. More specifically, data from 29 steps before the beginning of the generation step are used for acquiring the initial step of the generated sequence.Fig. 13shows an example of image generation results from the sound and joint angle inputs. At step 1, the bells in the retrieved image are arbitrarily colored, because the color of the placed bell is not derivable before acquiring any sound input. By contrast, the image of the robot’s right hand is already included in the retrieved image, because the joint angles input data indicate that the right arm is going to be used for striking the bell. At steps 31 and 61, the bell is rung, and the corresponding sound spectrum is acquired. Then, the task configuration becomes evident, and the information that the rung bell on the right side has the pitch ‘F’ is correlated with the color green. Thus, the color of the right bell in the retrieved image changes from the randomly initialized one to green by associating the sound and joint angles information. Conversely, the color of the left bell in the retrieved image is not stable during the run because no information is acquired from the sound input for identifying which bell is placed on the left side. Nevertheless, the retrieved image shows that when the color of the rung bell (i.e., green) is identified, the color of the other bell is selected from the remaining two colors (i.e., red or blue). This result reflects the current task design in which the color of the two bells is always different.From step 91 (or so), the sound of the bell starts to decay, and the actuation noise of the manipulator by the posture initialization becomes dominant. Thus, the colors of the bells again become arbitrary.We conducted an evaluation experiment to quantitatively examine whether our proposed model has succeeded in modeling the causality between the image, sound, and motion modalities. We prepared 10 different initial model parameter settings for the networks and replicated the experiment of learning the same dataset composed of the 12 combinations of the bell placements and bell-striking motion patterns. As a result of cross-modal image retrieval for the 10 learning results, 120 patterns of the image sequences were acquired. Image retrieval performance is quantified by the root mean square (RMS) errors of the manually selected left and right bell regions in the retrieved image (which are 13×13 pixels each, as indicated in Fig. 13) against the corresponding regions of the original image.Fig. 14shows the time variation of the image retrieval error displayed in association with the maximum value of the sound power spectrum and the joint angles sequence. The evaluation results demonstrate that the image retrieval error of the left bell becomes smaller than that of the right bell when the left bell is rung, and vice versa. The time variation of the error trajectory shows that the retrieval error decreases after the sound of the bell is acquired.The shape of the error trajectory is not symmetric between the two graphs when the left bell or the right bell is rung. When the left bell is rung, the image retrieval error for the left bell maintains its value even after arm posture initialization. Conversely, when the right bell is rung, the image retrieval error for the right bell increases after arm posture initialization. These differ primarily because of the asymmetry of the arm actuator noise. Due to the difference in the mechanics of the left and right actuators, which is beyond our control, the right arm produces more sound than the left arm. Hence, when the right arm posture is initialized after striking the bell, the accompanying actuator noise disturbs the internal state of the network (i.e., the data buffered in the recurrent loop), and the retrieved image is altered.To evaluate the significance of the difference between image retrieval performance of the left and right regions in the same image, we conduct a t-test for the image retrieval errors at step 60 of the sequences. At that time step, the arm is brought down and the hand stably contacts the button on top of the bell. Therefore, there is no influence of actuation noise on image retrieval. As shown in Fig. 15, evaluation results show that the differences of the image retrieval errors between the two regions are statistically significant in both the right and left bell-ringing cases. Results further show that the spatial correlation between the bell region in the image and the physical motion is correctly modeled, as are the associations between the colors and sounds of the bells. Thus, the acquired causality model between the image, sound, and motion modalities is utilized for image retrieval.Finally, we conducted an analysis of the multimodal feature space acquired by the temporal sequence learning network. Among the 10 replicated learning results, we took a single result and recorded the activation patterns of the central hidden layer of the network when the 12 patterns of bell-ringing sequences are the input. We applied principal component analysis (PCA) to project the resulting 100-dimension feature vector sequences to a three-dimensional space defined by the acquired principal components. Fig. 16(a) demonstrates that the robot’s motion pattern is represented in the two-dimensional space composed of the first and second principal components. In addition, Fig. 16(b) shows that the bell placement configurations are structured along the coordinate defined by the third principal component. Results of this analysis demonstrate that the causal dependencies between the multiple modalities are self-organized in the temporal sequence learning network.

@&#CONCLUSIONS@&#
