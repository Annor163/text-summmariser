@&#MAIN-TITLE@&#
Sensor fusion-based exploration in home environments using information, driving and localization gains

@&#HIGHLIGHTS@&#
We proposed a sensor fusion-based exploration scheme in home environments.The goal is to optimize the exploration efficiency and improve the map quality.In complex environments, the proposed method showed excellent performance.The localization gain enables the robot to accurately estimate its pose.The driving gain enables the robot to explore unknown environments very efficiently.

@&#KEYPHRASES@&#
Exploration,SLAM,Mobile robot,Indoor navigation,

@&#ABSTRACT@&#
Exploration is one of the most important functions for a mobile service robot because a map is required to carry out various tasks. A suitable strategy is needed to efficiently explore an environment and to build an accurate map. This study proposed the use of several gains (information, driving, localization) that, if considered during exploration, can simultaneously improve the efficiency of the exploration process and quality of the resulting map. Considering the information and driving gains reduces behavior that leads a robot to explore a previously visited place, and thus the exploration distance is reduced. In addition, the robot can select a favorable path for localization by considering the localization gain during exploration, and the robot can estimate its pose more robustly than other methods that do not consider localizability during exploration. This proposed exploration method was verified by various experiments, which verified that a robot can build an accurate map fully autonomously and efficiently in various home environments using the proposed method.

@&#INTRODUCTION@&#
In recent years, there have been various trials to extend robotic technology to non-industrial applications such as surgery, cleaning, patrol, and so on. Indoor mobile home service robots are receiving attention especially, because of their economic potential and social expectations [1–3]. In order to make mobile service robots more accessible in home environments, the problem of environmental modeling, which is one of the fundamental problems in mobile robots, should be solved first. This is because a mobile service robot uses a map to carry out various tasks, including navigation, human-robot interaction, and so on. Therefore, the simultaneous localization and mapping (SLAM) community has developed many efficient and highly accurate map-building techniques [4,5], but most of these techniques offer no proposals on how a robot can be made to function autonomously. However, autonomy is an important factor for environmental modeling of service robots. Therefore, various methods of exploration – the name typically given to automated map-building – have been proposed.Frontier-based exploration, which explores the unknown area in a grid map, was proposed in [6]. In frontier-based exploration, a robot detects the regions between the unexplored area and the open space, designated as the frontier. The robot then moves to the new frontiers to explore them until the entire environment has been explored [7,8]. Frontier-based exploration has the drawback of not being able to use information known about obstacles, which can serve as a guide for the robot to move and correct its localization error. To overcome this problem, an autonomous exploration method using regions of interest was proposed [9,10]. In this research, the view that would result in the sensor data that could be used to maximize exploration efficiency was estimated. While this approach does improve exploration efficiency, it does not address map accuracy at all.The aforementioned strategies are considered metric-based exploration methods. Another type of exploration method exists, known as topological information-based exploration. The most representative topological information-based exploration strategy is based on the Generalized Voronoi Graph (GVG) representation [11]. In Topological SLAM, developed for exploration of an unknown environment, the robot traces all GVG edges and visits all meet points and boundary points [12]. Additionally the extended Voronoi graph (EVG) was proposed [13]. In this research, both mid-line following and wall-following were used to control the robot motion and to model the environment. This strategy applies to complex indoor environments. However, in order to extract topological information and create a precise map, a robot tend to inefficiently navigate through all GVG edges, so map creation takes a long time. To overcome this inefficiency, thinning-based topological exploration (TTE) was proposed [14]. This scheme is based on the position probability of the end nodes of the topological map built in real time. The robot then updates the position probability of each end node sustaining its position at the current location using the range data. By analyzing this probability, the robot can determine whether or not it needs to visit the specific end node to examine the environment around this node. As a result, the TTE scheme can be conducted faster than the GVG-based exploration.Semantic information-based approaches have also been proposed. This includes the classification of space into different categories such as rooms, corridors, roads, buildings and the addition of objects in a hierarchical map representation [15,16].Two good criteria by which exploration effectiveness can be measured are the accuracy of the map and the efficiency. The efficiency means how fast the robot can finish the task of mapping with the minimum travel distance required to cover the entire environment. However, none of the above-mentioned approaches address both of these criteria simultaneously. Therefore, this paper proposed a sensor fusion-based exploration method using several gains. The contributions of this research are twofold.Firstly, a robot can efficiently explore an entire environment using the information and driving gains. The information gain is designed to favor the destination that offers the most information about the unexplored area. This optimal destination is chosen by considering all possible destinations and choosing the best one among the candidates. Choosing destinations based on this gain allows the robot to increase the exploration efficiency. The driving gain was developed by considering traveling distance and direction. Therefore, the proposed method prevents the robot from exploring previously visited places, thus shortening the exploration distance. Secondly, the proposed method improves the accuracy of map building with the localization gain. The robot selects a favorable path for improving localization accuracy by using the localization gain during exploration. Thus, the robot can estimate its pose more robustly than other methods, which do not consider localizability during exploration, and build an accurate map. This paper is organized as follows. Section 2 describes basic techniques for exploration. Section 3 presents the exploration scheme, and experimental results are shown in Section 4. Finally, we present our conclusions in Section 5.In order to conduct exploration, a mobile robot must utilize a model of its environment based on the assumption of perfect localization. In this sense, both the creation of a grid map that is useful for robot operation and localization are the essential components of exploration. Therefore, this section explains two basic techniques necessary for exploration. Firstly, we will introduce occupancy grid mapping, through which we create a map that can be used for essential functions such as motion control and path planning. Then, we explain the extended Kalman filter (EKF) based SLAM method, which maintains and optimizes another map – distinct from the grid map and consisting of landmarks – that is used to localize the robot more precisely.In this section, we briefly introduce the occupancy grid mapping algorithm [17]. In order to build the grid map, a sensor model should be considered. The range data from the sensor possesses some noise. Therefore, the sensor model is designed with Gaussian uncertainty given by(1)p(r|z)=12πσexp−(r−z)22σ2where r is a distance measured by a sensor (i.e., a laser scanner), z is the real distance and σ is the standard deviation associated with the uncertainty of both the grid map and the laser scanner. To allow the incremental composition of sensory information, occupancy grid mapping uses the Bayesian update formula in order to determine the cell occupancy probabilities.(2)P[s(Ci)=OCC|{r}t+1]=p[rt+1|s(Ci)=OCC]⋅P[s(Ci)=OCC|{r}t]∑s(Ci)p[rt+1|s(Ci)]⋅P[s(Ci)|{r}t]where s(Ci) is state of cell Ci, and it has two states: occupied (OCC) and empty (EMP). {r}tis the range data at time t and rt+1 is the newly measured range data at time t+1. Fig. 1shows the procedure for updating a grid map using range data from range sensors by Eq. (2).During navigation, a robot pose should be corrected continuously because the uncertainty of wheel odometry based on encoder data accumulates and gradually increases as the robot moves. Extended Kalman filter (EKF)-based SLAM is used in this research. The EKF is one of the most popular methods used for mobile robot localization and SLAM. It is an optimal sensor fusion method, which has been investigated for decades. The odometric error caused by an encoder can be compensated for by the EKF, which fuses different types of sensor data with weights proportional to the uncertainty of each sensor.The EKF is used to handle nonlinearities involved in the motion of the robot and the state vector is defined as follows:(3)X=[XrT,XL1T,…,XLNT]Twhere Xris the robot pose given by(xrW,yrW,θrW)and XLiis the position of the ith feature denoted by(riW,αiW), as shown in Fig. 2. The superscript W is placed before a value to indicate that it is expressed in the world frame. A robot estimates its pose by continuous prediction and update based on the EKF algorithm.In EKF-based SLAM, the robot pose and feature positions are stored in a state-vector that is represented as X and the positional uncertainties of the elements of the state-vector are stored in a covariance matrix that is denoted by P. At the prediction stage, the prior of the state vector,Xˆt+1−, and the prior of its covariance matrix,Pt+1−, at time t+1 are computed from the state vectorXˆt, its covariance matrix Ptand the encoder reading utas follows:(4)Xˆt+1−=f(Xˆt,ut,t)+wt(5)Pt+1−=FxPtFxT+FuQtFuT(6)Fx=∂f∂Xˆt,Fu=∂f∂utwhereXˆt−andPt−are the predictions of the state vector and its covariance matrix at time t, respectively, and utis the displacement of the robot between times t−1 and t. The vector, wt, represents the process noise with zero mean and Q is the covariance matrix of wt. The matrices, Fxand Fu, are the Jacobian matrices of the motion model, f(·), with respect to the state-vector and the displacement, respectively. Note that the superscript ‘−’ in Eqs. (4) and (5) indicates the prior variables before the observation at time t+1 is taken.If a robot observes a feature, it compares this feature to those in the state vectorXˆt−. If the feature coincides with one of the stored features, the update is conducted. At the update stage, the posteriors of the states are estimated by incorporating the predicted and actual observations. The predicted observationsZˆtis defined by(7)Zˆt=[zˆt,i|1≤i≤np]wherezˆt,iis computed based on the prior state vector at time t,Xˆt−, and npis the number of predicted observations (e.g., point feature i in Fig. 3). For example, the predicted observation of point feature i is computed as follows:(8)zˆt+1,i=h(Xˆt+1−)=xˆiRyˆiR=(xˆLi,t+1−−xˆR,t+1−)2+(yˆLi,t+1−−yˆR,t+1−)2atan2(yˆLi,t+1−−yˆR,t+1−,xˆLi,t+1−−xˆR,t+1−)−θˆR,t+1−The actual observation Ztobtained by the range sensor is defined as(9)Zt=[zt,j|1≤j≤na]where nais the number of actual observations (e.g., point feature j in Fig. 3) and the observation for point feature j is given by(10)zt,j=[xjR,yjR]TwhereRxjandRyjare the positions of point feature j with respect to the center of the robot. By using the difference between the predicted and the actual observations, the stateXˆtand its covariance Ptare updated as follows:(11)Kt=Pt−HtT(HtPt−HtT+Rt)−1(12)Xˆt=Xˆt−+Kt(Zt−Zˆt)(13)Pt=(I−KtHt)Pt−(14)Ht=∂h(Xˆt−)∂Xtwhere Ktrepresents the Kalman gain and Htis the Jacobian matrix of the sensor model with respect to the state vector. Further,Zˆtand Ztare the predicted and real measurements from sensors, respectively. The error on the pose of the robot due to disturbances and noises is compensated by the Kalman gain, which is proportional to the difference between predictions and measurements. If none of the landmarks are matched, the robot pose is calculated using only the motion model and the uncertainty of the robot pose increases.When a robot starts exploration, it rotates 360° to gather as much information on the environment as possible, as shown in Fig. 4. At the same time, the robot builds a grid map and extracts the salient visual features by the method proposed in our previous work [18]. Visual feature candidates are roughly selected using the entropy maps, which measure the level of randomness of information. The selected features are evaluated by the saliency map based on the similarity maps, which measure the level of similarity between the selected features and the given image. Among them, the feature which can be recognized by SIFT [19] is considered as a feature. The extracted salient visual features are used as an observation for the EKF-based SLAM framework. Through the visual feature, the robot can estimate its pose correctly and build a map simultaneously in an unknown environment.After that, the robot can extract the exploration nodes from the region of the extracted door candidates described in Fig. 5a and the frontier area. Fig. 5b presents the exploration nodes extracted from the frontier area as green circles. Fig. 5c presents all the exploration nodes that a robot should visit. In the next step, a robot decides a target node. If node 2 is selected as a target node, the robot moves there, as shown in Fig. 5d. This process is repeated until all the nodes in an exploration node list are visited.Information gain is designed to favor the node that offers the most information about the unexplored area. It uses the entropy concept for each cell. The entropy can be interpreted as the average uncertainty of the information source. Thus, the higher the entropy, the more information about the region is available, and thus it is more attractive for exploration. For example, an unknown cell with high uncertainty has high entropy, whereas a known cell (empty, occupied) has a low entropy as shown in Fig. 6.Many studies used the information gain to select a target node and it is given by(15)Ginfo=1k∑i=1kHiwhereHi=−pgrid,ilogpgrid,i−(1−pgrid,i)log(1−pgrid,i)where k is the number of cells and pgrid,iis the occupancy probability of each cell encountered by a ray radiated from the node (represented by a circle 3) as shown in Fig. 7a. Each node sends out 180 rays until it meets an occupied cell, unknown cell, or reaches the sensor range limit. However, the information gain obtained through the above method did not reflect the information on unknown space, which a robot can gather until it moves to a target node. For example, the robot can obtain more information at node 6 than node 3, as can be seen by Eq. (15). However, in the case that a robot moves to the node 3 along the path with a blue solid line, the amount of information that a robot obtained as it moves to node 3 is larger than the amount of information it would obtain moving to node 6. This is because, when a robot moves to node 6, only the information on the dotted rectangle of Fig. 7b can be available. However, when the robot moves to node 3, it is able to obtain additional information on the area represented by the dotted ellipse of Fig. 7b.Therefore, this study reflected all the information on the unknown space that a robot can obtain during its move by sending a virtual robot to each exploration node, as shown in Fig. 8. Each position of a virtual robot moving to its destination is given by(16)Xvirtual=xwywθw=xw+Δx⋅cos(θw)+Δy⋅sin(−θw)yw+Δx⋅sin(θw)+Δy⋅cos(θw)θw+Δθwhere Δx, Δy, Δθ are presented as follows:(17)ΔxΔyΔθ=v⋅Δt⋅cos(Δθ)v⋅Δt⋅sin(Δθ)w⋅Δtwhere v and w mean the linear velocity and angular velocity of the robot to trace a path to a node, and they are calculated with the use of the control method proposed by Kanayama [20]. In the formula, Δt is a cycle in which the robot controls its wheel and it is set as 0.3s in this study. If all Xvirtual to a target node are calculated, occupancy probability of the grids obtained by ray casting at each Xvirtual can be calculated. At this time, the ray casting method is used within the detectable range of a sensor, as shown in Fig. 8b. The formula is shown as follows:(18)fray(Xvirtual)={pgrid,1,…,pgrid,k}where fray() is a function performing ray casting within the detectable range of a robot sensor, and its input is the information of the virtual robot pose. In this study, the information gain is redefined as follows:(19)Ginfo=1m∑i=1mEiwhereEi=1k∑i=1k(−pgrid,ilogpgrid,i−(1−pgrid,i)log(1−pgrid,i))where Eiis the information entropy calculated through pgrid,i, k is a total number of grids calculated through fray(), and m is the number of each position (Xvirtual) of the virtual robot leading up to the node.The exploration scheme proposed in this study uses the EKF, a feature-based SLAM method. A robot should detect as many features as possible during exploration for accurate localization. Therefore, a localization gain was defined and used to select a favorable path for feature detection. Fig. 9a presents exploration nodes (represented by circle), a path to exploration nodes (represented by blue solid line), and the visual feature registered in the process of exploration (represented by slashed rectangle). To calculate the localization gain of each node, a virtual robot is sent to each node as shown in Fig. 9b.As a result, as presented in Fig. 9c, when a virtual robot moves to node 4, the features of three images come into the detection range of a sensor. For each visual feature that comes into the detection range while moving to the node, the distance and angle (i.e., r, θ) between a robot and the visual feature are calculated as shown in Fig. 9d, to calculate pmatch., which is the probability that the robot matches the visual feature in the actual environment, and it is given by(20)pmatch,i=0.8⋅exp−(r−μr)22σr2+−(θ−μθ)22σθ2where μrand μθrespectively represent the distance and an angle for which the visual feature is most recognizable when an actual sensor is used. In this study, they are set as 0.8m and 0°, and σrand σθare set as 3.5m and 33° respectively. pmatch is modeled according to the Matching Probability Distribution Model (MPDM), a Gaussian-typed probability distribution model as shown in Fig. 10.In this study, the localization gain is defined as(21)Gloc=1L∑i=1Lpmatch,iwhere L represents how many times a visual feature comes within the exploration range of a sensor while the virtual robot is moving. Fig. 11shows the example of the localization gain utility. Fig. 11a–c shows a CAD map (representing the ground truth), the map built with localization gain, and the map built without localization gain, respectively.A long travel distance between the nodes and frequent changes in exploration direction reduce the exploration efficiency. For example, frequent changes in robot direction cause a robot to explore previously visited places, which makes the travel distance longer as shown in Fig. 12a. However, by considering the driving cost (distance and direction), the travel distance can be reduced as shown in Fig. 12b. Suppose the angle difference Δθiis defined by(22)Δθi=θnode,i−θrobotwhere θnode,iand θrobot in Fig. 12c are the angles of the node and the robot relative to the global coordinate frame. The driving gain is then given by(23)Gdri=1−DnodeDtotal⋅ΔθiΔθmaxwhere Dnode is the distance of the generated path to the node, as shown in Fig. 12d, from the robot to the node and Dtotal is the total path distance from the robot to all the nodes. The quantity Dnode/Dtotal is included to make the shorter drives more appealing, and |Δθi/Δθmax| is used to make the robot change the exploration direction less frequently. The gradient method [21] was used in this study to generate the path.This section describes the method of selecting a target node that a robot is to visit by using the different gains calculated at each node. The target node is selected by calculating the node selection gain of each extracted node and selecting the node with the biggest node selection gain as the target node. The node selection gain is calculated as follows:(24)GNS=(1−αloc)⋅(Ginfo+Gdri)+αloc⋅Glocwhere Ginfo, Gdri, and Gloc, are the gains relevant to the information quantity, driving cost of the robot, and localization reliability, respectively. αloc represents the weight of Gloc, signifying the uncertainty of the robot pose at the destination node. To represent the uncertainty of the robot pose, the Shannon information is used [22]. Therefore, αloc is given by(25)αloc=−12log((2πe)nProbot)where n is the dimension of robot state (i.e., x, y, and θ) and Probot is the covariance of the current robot state. The robot can adaptively select a target node. For example, if the uncertainty of the robot pose is large when the robot arrives at the target node, the weights of localization gain αloc increases as per Eq. (25). Thus, the favorable node for localization is selected as the next target node using Eq. (24). Conversely, the node closest to the robot which provides information on the unknown area is selected as the target node. Therefore, the robot can manage localization quality and exploration efficiency autonomously differently for different localization states. If the node selection gain is calculated at each node, then the robot can select a target node as follows:(26)SN=argmaxi(GNS,i)where SN represents the number of the selected target node, and GNS,imeans the node selection gain calculated at node i. If two or more than two nodes have a same node selection gain, the target node is randomly selected. Fig. 13represents the flowchart for the proposed exploration scheme.

@&#CONCLUSIONS@&#
