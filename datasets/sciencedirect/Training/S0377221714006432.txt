@&#MAIN-TITLE@&#
Identifying future defaulters: A hierarchical Bayesian method

@&#HIGHLIGHTS@&#
We develop a hierarchical mixture cure model for credit scoring.We relax the independence assumption of the probability and the time of default.Empirical study shows that the customers with higher risk may default later and thus contribute a higher profit.Traditional classification methods should be carefully applied to the censored data.Our model is successful in identifying future defaulters even when default occurs much later than the censoring date.

@&#KEYPHRASES@&#
Risk management,Credit scoring,Mixture cure model,Bayesian analysis,

@&#ABSTRACT@&#
Traditional methods of applying classification models into the area of credit scoring may ignore the effect from censoring. Survival analysis has been introduced with its ability to deal with censored data. The mixture cure model, one important branch of survival models, is also applied in the context of credit scoring, assuming that the study population is a mixture of never-default and will-default customers.We extend the standard mixture cure model through: (1) relaxing the independence assumption of the probability and the time of default; (2) treating the missing defaulting labels as latent variables and applying an augmentation technique; and (3) introducing a discrete truncated exponential distribution to model the time of default. Our full model is written in a hierarchical form so that the Markov chain Monte Carlo method is applied to estimate corresponding parameters.Through an empirical analysis, we show that both mixture models, the standard mixture cure model and the hierarchical mixture cure model (HMCM), are more advanced in identifying future defaulters while compared with logistic regression. It is also concluded that our hierarchical Bayesian extension increases the model’s predictability and provides meaningful output for risk management.

@&#INTRODUCTION@&#
Under Basel II, banks are allowed to build their own credit scoring models to risk assess their customers and calculate the capital in need for the portfolio. It makes it more important to discover suitable scoring models to better understand customers payment behavior. Traditionally, credit scoring is modeled as a classification problem – loan applicant is either classified as credit worthy or credit worthless. Dozens of techniques have been proposed by numerous researchers, for details we refer the readers to excellent surveys (Crook, Edelman, & Thomas, 2007; Hand & Henley, 1997; Rosenberg & Gleit, 1994). Most of these approaches require a development sample that contains properly labeled customers who have been assigned to default group or non-default group based on their repayment history (Hand & Kelly, 2001) over an observation period. Such assignment is not always available, especially for long term products, which limits the applicability of such methods in practice.Let us take a real industrial application as an example. One of the largest commercial banks in China wanted to build a scorecard before the end of 2008 for their mortgage business. Samples were selected from existing accounts where the average term of loan was over 10years. To obtain accurate label for each customer, we were forced to use data that were at least 10years old. In the rapid developing country like China, there is a dramatic change in economic environments, data that are too old cannot accurately reflect the current situation. Observation period cannot be too short either. Mortgage customers rarely default immediately after taking the loan. It will take some time before we can observe sufficient number of defaults to train any meaningful model. The time window is always determined by some ad hoc rules, such as vintage analysis. In this example, it shows in Fig. 1that historically 70% of the defaults have occurred within 24months. Therefore, 24months was a good time window for selecting training data. The defaulting labels for customers were then determined according to their behavior before 31st December 2008 rather than the loan lifetime: that is, the data was right censored. However, the non-default group included customers that may default after the censoring time. Directly apply classification based method to this data set was inappropriate. Intuitively, ignoring the censoring effect may make the model unable to identify future defaulters since they were labeled as non-default in model development procedure.Survival analysis, with its ability to deal with the censored data, has been introduced in the context of credit scoring. Exploratory works include Banasik, Crook, and Thomas (1999), Hand and Kelly (2001) and Stepanova and Thomas (2002), in which various standard survival models, both parametric and non-parametric, are applied and compared. These works are followed by Bellotti and Crook (2009) and Im, Apley, Qi, and Shan (2012) which both incorporate the time-dependent variables into the proportional hazards model, with (Bellotti & Crook, 2009) including seven macroeconomic variables as the time-varying covariates and (Im et al., 2012) combining all the external factors into one coefficient, which accelerates the occurrence of defaulting. Baesens, Gestel, Stepanova, and Vanthienen (2005) proposes several neural-network based survival models that allow the covariates to be incorporated in a nonlinear mode. Andreeva, Ansell, and Crook (2007) introduces a combination score method to capture the relationship between the customers’ profitability and the time of default. More recently, the mixture cure models are proposed in Yildirim (2008) and Tong, Mues, and Thomas (2012) on the consideration that the customers are latently divided into two groups. One group includes customers who will never default, and the other includes those who will. They use mixture approaches to model the event of default by two parts, an incidence part, which describes the probability of the occurrence of an event, and a latency part, describing the time of the occurrence given it would happen. These papers concluded that advantages of survival analysis are that, it can be used to better discriminate applicants, estimate defaulting probability as a function of time, assist banks to make decisions on credit management.We retain the desirable properties of the survival analysis and complement it by extending the mixture cure model to a hierarchical model, the HMCM. First, we establish a joint multivariate normal distribution of the defaulting predictor and the logarithm of the hazard rate, so that the assumption of the independence of the probability and the time to default is relaxed. The covariance matrix estimated using our method can be used to test whether the dependency exists. It is also meaningful for banks, because a customer with higher defaulting probability may contribute more profit if he defaults later (Stepanova & Thomas, 2002). Second, a discrete truncated exponential distribution model is developed to accommodate two characters of the defaulting time for a fixed-term loan. That is, the reported defaulting time is discrete, usually monthly, and finite, as it is restricted by the loan term. At last, we incorporate the unobserved default labels into the model as latent variables, and augment them according to their posterior distributions, which makes our model flexible and easy to implement. Peng and Dear (2000), Corbire and Joly (2007) and Tong et al. (2012) have introduced such latent variable. However, they do not augment it but compute its posterior expectation which is required by the expectation–maximization algorithm. Since the likelihood under our model has no analytical form, the parameters are estimated using Bayesian method. Markov chain Monte Carlo (MCMC) procedure is one of the approaches to deal with hierarchical models by generating the parameters with a Markov chain.We compare and contrast the performance of the HMCM with the mixture cure model proposed in Yildirim (2008) and a logistic regression model. Via an application on a real data, we conclude that ignoring the effect of censoring will make the model lose its ability to identify the customers who default later than the censoring time. Our extension highly improves the performance of the mixture model in terms of identifying the future defaulters, even some of them default much later than the censoring time. From the correlation coefficient derived in our data, the riskier the customer, the longer he may survive. It reveals the reason why logistic regression underperforms the mixture models.Our method is similar to that in Abe (2009) which used a hierarchical Bayesian extension of the Prato/NBD model to forecast customers future purchases. Through an empirical analysis on three datasets, they conclude that the customer purchasing behavior is better tracked and, most importantly, the correlation estimated provides significant insights in customer relationship management. Unlike the consumer behavior discussed in Abe (2009), the event of default on a fixed-term loan cannot occur repeatedly, so the Poisson assumption of customer behavior is replaced by a Bernoulli distribution.The remainder of this paper is organized as follows. The mathematic notations, the statistical models and how our extensions are made are described in Section 2. In Section 3, we estimate the parameters using MCMC algorithm and list the main results. An empirical analysis is conducted in Section 4 to compare our model with the logistic regression model and the mixture cure models proposed in Tong et al. (2012). Section 5 concludes our paper with several directions of future research.Suppose the observation is taken on N customers, and the ith customer is accepted for a loan with termLi. LetTidenote the defaulting time since the loan is approved.Timay or may not be observed during the observation periodCi. Although we used the same time window to select customers, due to the difference in the starting time of the loans, the observation period for each customer may differ. The ith customer record takes the form(ti,δi,xi). Where,δi=1indicates the customer defaults within the observation period, that is,Ti⩽Ci, andδi=0otherwise;ti=min{Ti,Ci}is the reported time;xiis a1×pvector of the explanatory variables.yiis defined as the unobserved defaulting label, indicating whether a customer will eventually default. If the ith customer defaults beforeCi, i.e.Ti⩽Liandδi=1, thenyi=1, elseyicould be either 0 or 1. For simplicity of presentation we omitted the subscript for customer for all symbols in the following discussion.Letq=P(y=1)denote the probability that the customer will eventually default. Then the probability that a customer survives after t is given by:(1)S(t)=p(T>t)=1-q+qS(t|y=1)whereS(t|y=1)is the probability of the customer survives after t conditioning on that the customer will eventually default. To incorporate explanatory variables into this model, we specify q using logistic regression:(2)logit(q)=βq0+∑j=1pβqjxjwhere the parameterβq=[βq1,βq2,…,βqp]′specifies the impact of the explanatory variables on the probability that a customer will default eventually.There is a long history of applying exponential distribution to model the time of the occurrence of a particular event. Compared with other distributions, like Weibull, gamma or log–logistic distributions, the exponential distribution incorporates only one parameter which is the reciprocal of the mean, making it easy to capture the correlation between the probability of default and the time of default. It is also the reason why the parametric survival model is chosen over the semi-parametric survival models, such as Cox proportional model which make use of a series of non-parametric baseline hazard functions. Note that the standard exponential distribution is defined on the infinite domain. However, the default time of a term loan is finite (T⩽L) and measured in a fixed period (usually monthly). As such, we first truncate the exponential distribution, so that the survival density function is(3)S(t|y=1)=e-λt-eλL1-e-λL.S(0|y=1)=1,S(L|y=1)=0. Then we discretize it by taking the density function to be:(4)f(t|y=1)=S(t-1|y=1)-S(t|y=1),t=1,2,…,L.Thus, the time of default giveny=1follows a discrete truncated exponential distribution. We incorporate covariates by specifying the shape parameter λ as:(5)log(λ)=βλ0+∑j=1pβλjxj.Similarly, parameter vectorβλ=[βλ0,βλ1,…,βλp]captures the impact of the explanatory variables on λ which determines the shape of the distribution.The above Eqs. (1)–(5) constitute the basic framework of the standard mixture cure models. We now generalize it by relaxing the assumption of the independence of the probability and the time of default. The correlations across the two Eqs. (2) and (5) are introduced as follows:(6)β0∼MVN(β¯0,Σ),whereβ0=[βq0,βλ0]′, which is specified in Eqs. (2) and (5). Furthermore,β¯0=[β¯q0,β¯λ0]′is the mean ofβ0, andΣis a2×2variance matrix as in Eq. (7). The diagonal elements specify the variance of the error term, while the off-diagonal elements capture the covariance acrossβq0andβλ0.(7)Σ=σq2ρσqσλρσqσλσλ2.With Eq. (6), the probability of default and the time of default are allowed to be related to each other. Letlogit(q)=θqandlog(λ)=θλ, then according to Eq. (6),θqandθλfollow a multivariate normal distribution:(8)θqθλ=β¯q0+xβqβ¯λ0+xβλ+ε,ε∼MVN(0,Σ).In turn, q and λ follow the multivariate logit-log normal distribution.To lower down the complexity of our model, we introduce the unobserved variable y as a latent variable and apply a data augmentation technique (Tanner & Wong, 1987). Specifically, we draw y for each customer from its posterior distributionP(y=1|t,δ,x)and estimate the parameters with the augmented data. The posterior distribution of y is derived as:(9)P(y=1|t,δ,x)=δ+(1-δ)qS(t|y=1)1-q+qS(t|y=1),whereqS(t|y=1)1-q+qS(t|y=1)=P(y=1|T>t)is the probability that a customer will default given he has survived after t. Two extensions make our model hierarchical with Eq. (1)–(9) as the sub-models.With known y and the observed information, there are three cases of customers. The Customer withδ=1andy=1is observed to default. His contribution to the likelihood is:(10)qf(t|y=1)=eθq1+eθqe-eθλ(t-1)-e-eθλt1-e-eθλL.The Customer withδ=0andy=1survives after t but then defaults before the end of the loan. His contribution to the likelihood is(11)qS(t|y=1)=eθq1+eθqe-eθλt-e-eθλL1-e-eθλL.The last case is observed withδ=0andy=0, contributing(12)1-q=11+eθqto the likelihood.By combining all the three cases, the individual-level likelihood is formulated as:(13)L(θq,θλ,y|t,δ,x)=eθq1+eθqy11+eθq1-ye-eθλ(t-1)-e-eθλt1-e-eθλLδe-eθλt-e-eθλL1-e-eθλLy-δ.Reinstate the index i (i=1,2,…,N) to indicate individual customers. Then the sample likelihood function is given by:(14)L(θq,θλ,y|t,δ,x)=∏n=1NL(θqi,θλi,yi|ti,δi,xi),whereθq=[θq1,θq2,…,θqN]′,θλ=[θλ1,θλ2,…,θλN]′andy=[y1,y2,…,yN]′areN×qcustomer-specific parameters. Furthermore,x=(x1,x2,…,xN)′is theN×pvariable matrix,ttheN×1vector of reported times andδthen×1vector of censoring indicators.The parameters of most interest areβ¯q0,βq,β¯λ0,βλandΣ, since we cannot credit score a new customer without them. Traditionally, one has to take the integration of Eq. (14) overn×1vectorsθqandθλto estimate these parameters (Gelman, Carlin, Stern, & Rubin, 1995). However, there is no analytical form of the integration. As such, Markov chain Monte Carlo (MCMC) methods are applied to estimate the parameters, which are popular in solving hierarchical Bayesian models (Velarde, Migon, & Alcoforado, 2008).We apply an MCMC method, specifically Gibbs sampling (Altuzarra, Moreno-Jimenez, & Salvador, 2007), to estimate the parameterβq0,βq,βλ0,βλandΣ. We sequentially and repeatedly generate each parameter (or the parameter vector) from its full conditional distribution, given the observed data and other parameters (or the vectors) drawn in the last step. Starting with the initial value(β¯q00,βq0,β¯λ00,βλ0,Σ0,y0,θq0,θλ0), at the tth iteration, we:(1)drawβq̃t∼p(βq̃t|βλ̃t-1,Σt-1,yt-1,θqt-1,θλt-1,t,δ,x),drawβλ̃t∼p(βλ̃t|βq̃t,Σt-1,yt-1,θqt-1,θλt-1,t,δ,x),drawΣt∼p(Σt|βq̃t,βλ̃t,yt-1,θqt-1,θλt-1,t,δ,x),for each customer,a.augmentyitwithθqit-1,θλit-1,ti,δi,xiaccording to Eq. (9);drawθqit∼p(θqit|βq̃t,βλ̃t,Σt,yit,θλit-1,ti,δi,xi),drawθλit∼p(θλit|βq̃t,βλ̃t,Σt,yit,θqit,ti,δi,xi).Where,βq̃=[β¯q0,βq′]′andβλ̃=[β¯λ0,βλ′]′are(p+1)×1vector of parameters. We write the intercept and the parameter vector of the explanatory variables together for they can be drawn simultaneously through a joint density function.A Markov chain is produced while we iterate the above steps, because parameters are drawn based their values generated in the last iteration. Two issues may affect the result. One is the choice of the initial values. In this paper, we use the parameters estimated from the standard mixture cure models as the starting point. The other is the number of iterations for burn-in. It depends on the rate of convergence of the Markov chain, which is monitored by G-test (Geweke, 1992). If the Markov chain becomes equilibrium after I iterations, then we run the above steps forI+Mtimes totally. The last M iterations are used to infer the parameters.The full conditional distributions used in Section 3.2 are derived as follows. The joint distribution of all the parameters given the observed data is:(15)p(βq̃,βλ̃,Σ,y,θq,θλ|t,δ,x)∝∏i=1nL(θqi,θλi,yi|ti,δi,xi)p(θqi,θλi|βq̃,βλ̃,Σ)×p(βq̃)p(βλ̃)p(Σ),whereL(θqi,θλi,yi|ti,δi,xi)is defined in Eq. (13).p(θqi,θλi|βq̃,βλ̃,Σ)is the density function of a multi-variate normal distribution defined in Eq. (8).p(βq̃),p(βλ̃)andp(Σ)are the diffuse priors. The former two priors are both set to bei.i.d. normal distributionsnormal(0,σ2), and the last one is set to be inverse-Wishart distributionW-1(Σ0,v0).Then we obtain the full conditional distribution for each parameter by ignoring all terms that are constant with respect to the parameter. The full conditional distribution ofβq̃is:(16)p(βq̃|βλ̃,Σ,y,θq,θλ,t,δ,x)∝∏i=1np(θqi,θλi|βq̃,βλ̃,Σ)×p(βq̃).A little algebra yields that:(17)p(βq̃|βλ̃,Σ,y,θq,θλ,t,δ,x)∝MVN(βˆq,Λq),where(18)Λq=σ2zTz+(1-ρ2)σq2E(1-ρ2)σ2σq2-1,βˆq=Λq∗σλzTθq+ρσqzTzβλ-ρσqzTθλ(1-ρ2)σq2σλ,wherez=[1′x]is theN×(p+1)matrix of explanatory variables.Similarly, the full conditional distribution ofβλ̃is:(19)p(βλ̃|βq̃,Σ,y,θq,θλ,t,δ,x)∝MVN(βˆλ,Λλ),where(20)Λλ=σ2zTz+(1-ρ2)σλ2E(1-ρ2)σ2σλ2-1,βˆλ=Λλ∗σqzTθλ+ρσλzTzβq-ρσλzTθq(1-ρ2)σλ2σq.The full conditional distribution ofΣis(21)p(Σ|βq̃,βλ̃,y,θq,θλ,t,δ,x)∝p(θqi,θλi|βq̃,βλ̃,Σ)p(Σ)∝W-1(Σˆ,vˆ),where(22)Σˆ=θq-zβq̃θλ-zβλ̃θq-zβq̃,θλ-zβλ̃+Σ0,vˆ=n+v0.The full conditional distribution of y is derived by Eq. (9). As y is a binary variable, we drawu∗from a uniform distribution. Ifu∗<=P(y=1|t,δ,x), theny=1, elsey=0.The full conditional distribution ofθqi(i=1,2,…,N) is given as:(23)p(θqi|βq̃,βλ̃,Σ,yit,θλi,ti,δi,xi)∝L(θqi,θλi,yi|ti,δi,xi)p(θqi,θλi|βq̃,βλ̃,Σ).This is not a standard distribution so we apply a random-walk Metropolis–Hastings algorithm (Pang, Leung, Huang, & Liu, 2005) to drawθqi:a.drawτ∗from a standard normal distribution, letθqi∗=θqi+cτ∗, where c is the scaling parameter,compute the Metropolis–Hastings accept-reject ratio,(24)α=minp(θqi∗|βq̃,βλ̃,Σ,yit,θλi,ti,δi,xi)p(θqi|βq̃,βλ̃,Σ,yit,θλi,ti,δi,xi),1,drawu∗from a uniform distributionU(0,1), ifu∗<=α, then acceptθqi∗as the new value ofθqi; otherwise, remainθqithe same as before.Note that the proposal distribution for generatingθqi∗is a normal distribution where the mean isθqiand the variancec2. We need to find an appropriate value of c so as to induce good mixing behavior of the chains. In the following empirical study, c is chosen to be 2 with the acceptance rate of 0.37 which is acceptable.Similar algorithm is also applied to generateθλi(i=1,2,…,N), where the scaling parameter is chosen to be 4 and the related acceptance rate is 0.29.Supposeβq̃∗,βλ̃∗andΣ∗are the posterior mean ofβq̃,βλ̃andΣinferred by the last M iterations of the MCMC procedure. Then for a new customer, the probability of default q is calculated using Eq. (2) as(25)qnew=P(ynew=1|xnew,βq̃∗,βλ̃∗,Σ∗)=∫eθq1+eθqp(θq|βq̃∗,βλ̃∗,Σ∗,xnew)dθq.We use the MCMC method to calculate this integration. That is, withβq̃∗,βλ̃∗andΣ∗, we generate B pairs ofθqandθλ(θq,jandθλ,j,j=1,2,…,B) from the multivariate normal distribution. Eq. (25) is then approximated to(26)qnew=1B∑j=1Beθq,j1+eθq,j.The probability that a new customer defaults before any feasible time t can be calculated using Eq. (3):(27)F(t|xnew)=1-S(t|xnew)=1B∑j=1Beθq,j1+eθq,j1-e-eθλ,jt1-e-eθλ,jL.The data are sampled from one major commercial bank in China, who wanted to build a scorecard for its mortgage business before the end of 2008. The definition of default is determined by the roll-rate analysis. As is illustrated by Fig. 2, customers with overdue payment in three periods (each period contains 30 delinquency days) were defined as defaulters.Under this definition, 24months were chosen as the time-window. Thus, the original data contains 14068 customers who were accepted during the whole year 2006 to eliminate the effect of seasons (see Fig. 3). Each customer in the sample was observed for at least 24months to allow it to ’mature’, and no more than 36months so as to be kept ’fresh’. Models are built based on their performance during the observation period. Among all the 14,068 customers, 461 were observed to default before model building.Besides, the performance of the customers was tracked until then end of 2009. There are 267 customers defaulting during this year. With traditional classification methods, these people are treated as non-default in the model since they were still repaying before censoring.The final data we get include all the customers who defaulted before the end of 2009 and the same number of non-default customers randomly sampled from the original data. In total, 1456 customers are selected for model building and validating. Fourteen features are incorporated in the models, all of which are standardized on their means with a standard error of 1.Logistic regression is a popular classification method which makes the use of Eq. (2) and treats δ as the default/non-default indicator. Parameters are estimated through maximum likelihood estimation. It is popular in the financial industry, and is often used as a benchmark in academic research.We compare our model with the logistic regression model and the mixture cure models applied in Tong et al. (2012). They make use of Eqs. (1) and (2), and link the variables to the hazard rate by the Cox proportional hazards model (Cox, 2009). EM algorithm is run to estimate the corresponding parameters by iteratively maximizing the expected log-likelihood (Sy & Taylor, 2000). Since the baseline hazard function is non-parametric, the probability that a customer default in a time where no default occurs cannot be estimated. We complete the estimation by using the ETAIL constraint scheme (Peng, 2003).Our method to validate the models can be viewed as a combination of the 10-fold cross validation method and an out-of-time method.In the procedure of estimating the parameters, to avoid over-fitting, the total sample are randomly segmented into 10 equally-sized folds. At each time, 9 folds are selected to train the parameters for the customers in the remaining fold. After we repeat it for 10 times, all the customers will be assigned with the estimated parameters. Then we can compute the quantities of interest for the entire sample, such as the probability of default before any feasible time.Note that with the cross validation method, the information used for training and testing are virtually the same, because we develop and validate the model at the same time (i.e. the censoring time). However, customers who are not observed to default before the censoring time may default in the future. Therefore, a model which performs better at the censoring time may deteriorate if it fails to identify the future defaulters. We use a simple example to illustrate it (see Fig. 4).Suppose there are seven customers ranked by the time they default. Two of them are observed to default before censoring, while another four will default in the future. The last one will never default. Three methods are applied to estimate the probability of default for these customers. Define 0.5 as the threshold. It means that any customer whose probability of default is beyond 0.5 will be rejected. The percentage correctly classified of these customers are reported in Table 1. At timetc, M1 correctly classified all the customers, while M3 performs the worst. When time goes tot1, customer C3 becomes bad. The performance of M2 and M3 are improved since they reject this customer. As the number of defaulters increases, M3 becomes better and better as it identifies all the bad customers who default later. It is shown that the fluctuation of the model performance over time is affected by whether a future defaulter is identified.Therefore, in the procedure of validation, we allow validation sample to change over time. As is depicted in Fig. 5, we develop 13 validation samples based on the customer performance before the end of each month from December, 2008 to December, 2009 and name them from VS1 to VS13. To demonstrate the models’ robustness over time, we test the performance of the models in all the 13 validation samples while keeping the training sample unchanged. A good model is expected to not only identify the customers who default before censoring time, but also give a high probability of default to the future defaulters.The KS and Gini are commonly used to measure how well the models discriminate between default and non-default customers (Finlay, 2010). KS is calculated as the maximum distance between the curves of the cumulative number of default and non-default customers when the number of the customers accepted increases. Gini is associated with the area under the ROC curves (Hanley & McNeil, 1982). To calculate them, we ranked the validation sample by the estimated probability of default. LetG(i)denote the cumulative proportion of good customers when the ith observation is accepted, andB(i)the cumulative proportion of bad customers when the ith observation is accepted. Obviously,G(N)=B(N)=1. Then KS and Gini are computed by:(28)KS=max1⩽i⩽N(G(i)-B(i))(29)Gini=1-∑i=1N[G(i)+G(i-1)][B(i)-B(i-1)]Generally, the larger the KS and Gini, the better the model performs.

@&#CONCLUSIONS@&#
