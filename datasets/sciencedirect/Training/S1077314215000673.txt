@&#MAIN-TITLE@&#
Collaborative part-based tracking using salient local predictors

@&#HIGHLIGHTS@&#
A novel part-based tracking algorithm is proposed.Reliable local features are identified through a saliency evaluation mechanism.Salient predictors collaborate to achieve a global prediction of the target state.We handle real-life difficulties such as occlusion and presence of distractors.The drastic decrease of the number of features does not destabilize the tracker.

@&#KEYPHRASES@&#
Part-based tracking,Feature saliency,Keypoint,SIFT,Keypoint layout,

@&#ABSTRACT@&#
This work proposes a novel part-based method for visual object tracking. In our model, keypoints are considered as elementary predictors localizing the target in a collaborative search strategy. While numerous methods have been proposed in the model-free tracking literature, finding the most relevant features to track remains a challenging problem. To distinguish reliable features from outliers and bad predictors, we evaluate feature saliency comprising three factors: the persistence, the spatial consistency, and the predictive power of a local feature. Saliency information is learned during tracking to be exploited in several algorithm components: local prediction, global localization, model update, and scale change estimation. By encoding the object structure via the spatial layout of the most salient features, the proposed method is able to accomplish successful tracking in difficult real life situations such as long-term occlusion, presence of distractors, and background clutter. The proposed method shows its robustness on challenging public video sequences, outperforming significantly recent state-of-the-art trackers. Our Salient Collaborating Features Tracker (SCFT) also demonstrated a high accuracy even if a few local features are available.

@&#INTRODUCTION@&#
Visual object tracking is a fundamental problem in computer vision with a wide range of applications including automated video monitoring systems [1,2], traffic monitoring [3,4], human action recognition [5], robot perception [6], etc. While significant progress has been made in designing sophisticated appearance models and effective target search methods, model-free tracking remains a difficult problem receiving a great interest. With model-free trackers, the only information available on the target appearance is the bounding box region in the first video frame. Tracking is thus a challenging task due to (1) the insufficient amount of information on object appearance, (2) the inaccuracy in distinguishing the target from the background, and (3) the target appearance change during tracking.In this paper, we present a novel part-based tracker handling the aforementioned difficulties, including the lack of information on object appearance and features. This work demonstrates that an efficient way to maximize the knowledge on object appearance is to evaluate the tracked features. To achieve robust tracking in unconstrained environments, our Salient Collaborating Features Tracker (SCFT) discovers the most salient local features in an online manner. Every tracked local feature is considered as an elementary predictor having an individual reliability in encoding an object structural constraint, and collaborating with other features to predict the target state. To assess the reliability of a given feature, we define feature saliency as comprising three factors: persistence, spatial consistency, and predictive power. Thereby, the global target state prediction arises from the aggregation of all the local predictions considering individual feature saliency properties. Furthermore, the appearance change problem (which is a major issue causing drift [7]) is handled through a dynamic target model that continuously incorporates new structural properties while removing non-persistent features.Generally, a tracking algorithm includes two main aspects: the target representation including the object characteristics, and the search strategy for object localization. The contributions of our work relate to both aspects. For target representation, our part-based model includes keypoint patches encoding object structural constraints with different levels of reliability. Part-based representations are proven to be robust to local appearance changes and partial occlusions [8–10]. Moreover, keypoint regions are more salient and stable than other types of patches (e.g. regular grid, random patches), increasing the distinctiveness of the appearance model [11,12]. Regarding the search strategy, the target state estimation is carried out via local features collaboration. Every detected local feature casts a local prediction expressing a constraint on the target structure according to the spatial layout, saliency information, detection scale, and dominant orientation of the feature. In this manner, feature collaboration preserves the object structure while handling pose and scale change without requiring to analyze the relationship between keypoints like in [9], neither calculating homographies such as in most keypoint matching works [13–15].More specifically, the main contributions of this paper are:1.A novel method for evaluating feature saliency to identify the most reliable features based on their persistence, spatial consistency, and predictive power.The explicit exploitation of feature saliency information in several algorithmic steps: (1) local predictions, (2) feature collaboration for global localization, (3) scale change estimation, and (4) for local feature removal from the target model.A dynamic appearance model where persistent local features are stored in a pool, to encode both recent and old structural properties of the target.Extensive experimentation to evaluate the tracker performance against five recent state-of-the-art methods. The experimental work conducted on challenging videos shows the validity of the proposed tracker, outperforming the compared methods significantly.The rest of this paper is organized as follows. In the next section, we review related part-based tracking works. Algorithm steps are presented in details in Section 3. Experimental results are provided and analyzed in Section 4, and Section 5 concludes the paper.

@&#CONCLUSIONS@&#
