@&#MAIN-TITLE@&#
Dual many-to-one-encoder-based transfer learning for cross-dataset human action recognition

@&#HIGHLIGHTS@&#
Proposed a new transfer-learning method for cross-dataset action recognition.A new dual many-to-one encoder method for feature extraction across action datasets.Achieved over 10% increase in recognition accuracy over recent work.

@&#KEYPHRASES@&#
Cross-dataset,Action recognition,Neural network,Transfer learning,Domain adaptation,

@&#ABSTRACT@&#


@&#INTRODUCTION@&#
Human action recognition has drawn immense interests over the years, with its applications in a wide range of fields, including video labeling, video content retrieval, video surveillance, and sports video analysis. With the growing convenience of capturing and sharing videos, the computer vision community has seen a growing variety of human action datasets with substantial amount of videos. While the majority of these video data do not have annotations on them and hand labeling large datasets requires considerable amount of human efforts, researchers are interested in developing mechanisms to automatically generate annotations to these video data. Considering the fact that large-scale datasets always exhibit high intra-class variations, the requirement for a rational number of training data can easily go beyond the number of existing labeled data. Thus, researchers are thinking about the possibility of employing previously annotated datasets to facilitate automatic labeling of new datasets. For the human action recognition problem, different datasets share common actions. If it is possible to transfer knowledge between source and target action datasets, the annotated source dataset can serve as an augmentation to the training data, based on which effective labeling of the target dataset can be carried out. However, the auxiliary domain data may suffer from the serious domain-shift problem. For example, the action ‘run’ in one dataset may consist of videos of athletes running on field tracks while the same action from another dataset may contain videos of people running on the streets. In order to alleviate such problems, an algorithm that can reduce cross-domain variance is required. Algorithms of this type belong to transfer learning, which is a particular branch of machine learning that aims to utilize knowledge from one source or task to assist the same or a different task on another source.In this paper, we tackle the problem of action recognition across four benchmark datasets. The major challenge comes from the significant cross-dataset variations. For example, Diving sequences in UCF Sports dataset [1] consist of TV broadcast videos with steady camera movements, controlled lighting, and trivial viewpoint changes, while videos of the same action class in HMDB 51 dataset [2] exhibit large variances in lighting, background, and viewpoints. Considering the significance of cross-dataset variations, hand-crafted features, which are designed to capture the discriminative properties of images or videos, are incapable of producing domain-invariant features for cross-domain datasets. Thus, we are interested in learning generalized feature representations across datasets, so that the resulting features from the source dataset can be used in the recognition of unseen instances in the target dataset. Our proposed method learns cross-dataset generalized features by training two many-to-one encoders on the source and target datasets in parallel, and maps raw features of instances from the two datasets to the same feature space. To our knowledge, this is the first time a dual many-to-one encoder architecture is used in cross-dataset action recognition. Contributions in this work are as follows:•We have proposed a new transfer-learning method for cross-dataset action recognition. Our method can be easily generalized to other recognition tasks.We have designed a novel dual many-to-one encoder architecture for extracting generalized features across action datasets.We achieved over 10% increase in recognition accuracy over recent work in cross-dataset action recognition.The organization of the paper is as follows: in Section 2 we present some background and related work; in Section 3 we present details of the proposed method; in Section 4 we describe our experimental results, and we conclude the paper in Section 5.In this section, we present the background and related works that are essential to understanding the objective and methodologies behind our proposed method. We briefly review the concepts and methods in transfer learning and its benefits in cross-dataset human action recognition.Traditional machine learning methods follow the convention that the training data and the testing data are from the same distribution. However, this assumption is not always satisfied in real world applications. For example, when classifying web documents into several predefined categories, the test web documents may contain categories that are not sufficiently represented in the training data. Therefore, traditional learning methods may fail in such cases [3,4]. The objective of transfer learning is to transfer the knowledge from a source domain to the target domain. More specifically [5] gives the following definition:DefinitionTransfer learningGiven a source domain Dsand learning task Ts, a target domain DTand learning task TT, transfer learning aims to help improve the learning of the target predictive function fT(⋅) in DTusing the knowledge in DSand TS, where DS≠DT, or Ts≠TT.Surveys like [5] provides taxonomy related to transfer learning. Transfer learning can be further divided into three categories depending on the presence of labeled data in target and source tasks: 1) Inductive transfer learning, when labeled data is available in the target domain, 2) Transductive transfer learning, when labeled data is available only in source domain, and 3) Unsupervised transfer learning, when no labeled data is available in either source or target domains. Inductive transfer learning is closely related to multitask learning, its purpose is to learn both source and target tasks simultaneously, and transfer knowledge between the two tasks to improve performance on both [5]. Our proposed method falls into this category by learning two many-to-one encoders in parallel.The type of knowledge being transferred can be divided into four categories: 1) instance transfer, 2) feature-representation transfer, 3) parameter transfer and 4) relational-knowledge transfer. Instance transfer uses source instances to train a classifier on target instances, usually by selecting or weighting source instances according to a certain metric. Feature-representation transfer reduces the differences between the source and target datasets by mapping one representation to the other, or by mapping both source and target representation to a common representation. Parameter transfer learns parameters shared by the source and target datasets. Finally, relational knowledge transfer uses a network or graph to explore the relationships within a dataset, and transfer the relationships from the source to the target [5].Transfer learning has been adopted by the computer vision community in areas including object detection [6,7], object classification [8--11] and video event detection [12--16]. For example, Duan et al. [14] proposed an adaptive multiple kernel learning method by adapting classifiers based on base kernels and pre-learned average classifiers. Their method was applied to event recognition in consumer videos on the web. Lim et al. [7] proposed an object detection model that augments training data by borrowing and transforming instances from other classes. Gao et al. [17] designed a low-level feature model that could be used as a prior for learning new object class models from scarce training data. Tommasi and Caputo [18] performed domain adaptation of object classification inside the naive Bayes nearest neighbor framework by iteratively learning class metrics that can induce large between-class variance. Kulis et al. [11] addressed domain adaptation between different types of features and dimensionality for object classification by using asymmetric kernel transformation.In human action recognition in particular, transfer learning has received increased interest over the years [19--23]. We will expand on the application of transfer learning in action recognition in Section 2.4.With the revival of neural networks in recent years, many researchers have applied neural networks to transfer learning. Zhang [4] tackled the problem of cross-domain document classification using restricted Boltzman machine to learn a set of hierarchical features from source domain, then select a subset of the features by kernel-task alignment. Girshick et al. [24] adopted region proposal Convolutional Neural Networks in a two-stage object detection framework, where regions of interests were first detected and then semantics were learnt by fine-tuning. Oquab et al.[25] designed a method to reuse CNN layers trained on ImageNet, and transferred parameters for object classification in the PASCAL VOC dataset.Different from the methods used in prior works, we propose a new method by training a pair of many-to-one encoders in parallel, and then map raw features from the source and target datasets to the same feature space. We will expand on the details of our method in Section 3.Human action recognition is a widely studied topic. The goal of human action recognition is to correctly classify actions performed by one or more persons into a pre-defined category. Survey papers like [26] provide taxonomy for this topic.Efficacy of any action recognition method depended on the feature representation method used. Among recent work, Shao et al. [27] proposed a novel descriptor based on spatio-temporal Laplacian pyramid coding, which effectively extracts holistic representation without loss of information. Yu et al. [28] developed a structure-preserving binary representation for videos with depth information. Shao et al. [29] presented a method that extracts discriminative features by efficient spatio-temporal localization of human actions. Inspired by evolutionary method, Liu et al. [30] proposed a genetic programming approach toward learning spatio-temporal representations. To fuse different types of representations into one, Shao et al. [31] designed a spectral coding algorithm based on kernel multiview projections.Although the above methods achieved satisfactory performance in action recognition, they were designed with the assumption that the training data and the testing data came from the same dataset. Unlike these methods, we propose a novel feature extraction method that can handle cross-dataset classification by learning generalized features. We show its efficacy in labeling unseen videos from the target dataset. Though our method was developed for action recognition, we argue that the proposed method can be generalized to other cross-dataset applications as well.There has been an exploding interest in applying transfer learning techniques to action recognition [19,23,32,33]. There are interests in applying transfer learning to datasets generated by different sensors, e.g., cameras, wearable sensors, or other sensor modalities [20,34]. Others are interested in applying transfer learning to datasets of the same domain, and most commonly from video sequences. While video sequences are the most common type of action datasets, the use of cameras to capture actions introduces complex issues; for example, different viewing angles, cluttered background, or changes in illumination, all contribute to significant variance in the captured videos. Therefore, action recognition in videos is a challenging problem.Some researchers focus on transferring knowledge for action recognition between camera views [32]. For example, Liu et al. [10] extracted high level features shared across views by using bipartite graph partitioning on two view-dependent vocabularies. Zheng et al. [35] proposed to learn a pair of dictionaries on videos taken from different views and build a view-invariant sparse representation. Zhang et al. [36] used linear transformation to transform source view to target view via virtual path. Zhu and Shao [37] proposed to learn a sparse representation based on dense trajectory features by learning a view-independent basis dictionary and by forcing the same actions to have an identical representation. For applications in smart homes, Wu et al. [38] presented a multiview activity recognition technique by performing spatio-temporal feature fusion.Other researchers tackled problems that are related to event search and abnormality recognition. Lam et al. [13] integrated transfer learning with relevance feedback to aid user's event query with known classification problems. Nater et al. [39] addressed the problem of abnormal event detection by adapting a Least-Square Support Vector Machine learnt from normal events to unseen events.Some researchers proposed new adaptive transfer learning models. Yang et al. [40] proposed an adaptive support vector machine that transforms a classifier trained on a source dataset to work on a target dataset. Lin adn Li [41] extended the boosting-based learning method which allows classifiers built on a source action dataset to adapt to a new dataset.In this paper, we tackle the problem of cross-dataset action recognition. In recent work, Cao et al. [42] used a probabilistic approach where a Gaussian mixture model learnt from a source dataset serves as a prior and is iteratively adapted to a target dataset for action detection. Sultani and Saleemi [43] proposed to use a weighted histogram by putting more weights on areas with high foreground confidence. This alleviates the problem in cross-dataset action recognition when the background may obscure recognition results. Different from these approaches, our method transfers knowledge across datasets by simultaneously learn a pair of many-to-one encoders. Using this model, we implicitly learn the distribution of raw features from the two datasets and map them to the same feature space.

@&#CONCLUSIONS@&#
