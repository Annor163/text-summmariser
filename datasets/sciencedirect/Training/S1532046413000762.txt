@&#MAIN-TITLE@&#
Analysis of eligibility criteria representation in industry-standard clinical trial protocols

@&#HIGHLIGHTS@&#
We compare textual complexity of full-text and ClinicalTrials.gov (CT) protocols.We use cosine-similarity measures to identify clusters for standardization.We find that CT protocols are very condensed and convey lesser information.Developing a template set is feasible and could lead to efficient criteria design.

@&#KEYPHRASES@&#
Clinical trials,Information retrieval,Natural language processing,Controlled vocabulary,Eligibility determination,

@&#ABSTRACT@&#
Previous research on standardization of eligibility criteria and its feasibility has traditionally been conducted on clinical trial protocols from ClinicalTrials.gov (CT). The portability and use of such standardization for full-text industry-standard protocols has not been studied in-depth. Towards this end, in this study we first compare the representation characteristics and textual complexity of a set of Pfizer’s internal full-text protocols to their corresponding entries in CT. Next, we identify clusters of similar criteria sentences from both full-text and CT protocols and outline methods for standardized representation of eligibility criteria. We also study the distribution of eligibility criteria in full-text and CT protocols with respect to pre-defined semantic classes used for eligibility criteria classification. We find that in comparison to full-text protocols, CT protocols are not only more condensed but also convey less information. We also find no correlation between the variations in word-counts of the ClinicalTrials.gov and full-text protocols. While we identify 65 and 103 clusters of inclusion and exclusion criteria from full text protocols, our methods found only 36 and 63 corresponding clusters from CT protocols. For both the full-text and CT protocols we are able to identify ‘templates’ for standardized representations with full-text standardization being more challenging of the two. In our exploration of the semantic class distributions we find that the majority of the inclusion criteria from both full-text and CT protocols belong to the semantic class “Diagnostic and Lab Results” while “Disease, Sign or Symptom” forms the majority for exclusion criteria. Overall, we show that developing a template set of eligibility criteria for clinical trials, specifically in their full-text form, is feasible and could lead to more efficient clinical trial protocol design.

@&#INTRODUCTION@&#
Clinical trials are an intrinsic part of the medical research and drug development process of most pharmaceutical and biotechnology companies. Policy makers and governmental organizations are also naturally interested in evaluating the efficacy, accuracy and safety of drugs trials that could potentially affect millions of people around the world. Eligibility criteria in clinical trials are a set of requirements that a patient or participant must meet to be eligible for inclusion in a study. From the perspective of a study sponsor, these requirements ensure that all participants in a cohort satisfy some general criteria and thus give a higher confidence in predicting possible outcome of an intervention.Eligibility criteria are usually expressed in human-readable free-text which is easily comprehensible to patient, public and researchers alike. However this form of representation of eligibility criteria makes it challenging for computable and standardized representation. Currently, there are no data or terminology standards for representing or authoring eligibility criteria in a standard format [1–3]. Given the plethora of applications of eligibility criteria, ranging from criteria reuse to patient matching from Electronic Medical Records (EMR) [4,5] it is of great importance to address the problem of computable knowledge-based representation for eligibility criteria. The primary motivation of our study is to determine the feasibility of creating a set of standard representations of eligibility criteria that would be applicable to a broad set of clinical trials. Uniformly represented eligibility criteria can facilitate the process of identification and merging of similar patient populations across studies for the purpose of patient recruitment. Consequently, this can reduce not only the time spent in performing trials that have already been conducted under similar conditions but can also help in reducing the expenses associated with participant recruitment substantially. Secondary outcomes of interest would be easier encoding of eligibility criteria to find patients via EMRs, faster and accurate authoring of eligibility criteria, and higher quality protocols. For example, a standardized representation of an eligibility criterion could be linked to a specific, standard set of ICD-9 codes that could then be used as filters to identify patients across EMR systems.We begin by comparing the textual and characteristic differences of industry-standard full-text protocols to corresponding protocols from ClinicalTrials.gov (CT) [6], a registry of clinical studies from around the world. We then explore methods for deriving computable and standardized representation of eligibility criteria, in the form of templates, from a set of full-text and CT protocols used in Pfizer’s pain medication-related studies.Our contributions in this paper are two-fold. First, we explore the nature of representation of eligibility criteria from industry-standard full-text protocols and compare their characteristics to corresponding CT eligibility criteria. Second, we propose a novel method for standardized representation of eligibility criteria (using sentence similarity and clustering strategies) in the form of “templates”. To the best of our knowledge, this is the first study in the domain of standardized representation of eligibility criteria that deals with in-depth analysis of eligibility criteria characteristics in their full-text form. Most related research deal with considerably simpler and concise eligibility criteria from CT.Computable clinical trial protocols and corresponding eligibility criteria representations have been studied extensively in the past two decades. Studies have been conducted to identify a set of common data elements that can be used for developing standard protocol representation [7]. There have been attempts to use natural language processing for parsing eligibility criteria statements to extract generic query patterns for eligibility criteria representation [8,9]. Research has also been focused on the identification of Unified Medical Language System (UMLS) - based [10] semantic classes for criteria statements [11,12]. The complexity of eligibility criteria representation has also been studied quantitatively with significant proportion of criteria being judged semantically complex [13].There have been extensive studies in computer-based and formal eligibility criteria knowledge representations. A CDISC-sponsored project called ASPIRE [14] aims to provide formal representation of a core set of eligibility criteria and also provides a set of data elements which can be used for searching and filtering protocols. The Eligibility Rule Grammar and Ontology (ERGO) [15], uses an information model, composed of noun phrases, expressions and criteria, to provide a general syntax for representing eligibility criteria. The EliXR system [3] provides a semi-automated data-driven approach for semantic representation of eligibility criteria. It uses an integrated semantic processing framework based on UMLS for eliciting semantic role labels that can be used for annotating eligibility criteria. The Standards-Based Active Guideline Environment (SAGE) [16] provides a set of structured and standard terminologies for encoding computable guidelines into structured templates. The Clinical Research Filtered Query (CRFQ) Project [17] provides a standardization of criteria using various semantic parameters like demographic data, disease data, etc. The use cases of these systems vary from filtering trials satisfying particular conditions to the identification of patients for a protocol.Previous research [18] has demonstrated the benefit of using a set of disease-categorized protocols for designing efficient clinical trial authoring tools. Significant research has also been conducted in developing decision support systems for clinical trials. Expert systems like the protocol inspection and critiquing tool (PICASSO) [19] support critiquing of clinical trial protocols and can be used to standardize new protocols. Knowledge-based decision support systems like Design-a-Trial (DaT) [20] help efficient creation of rigorous protocols documents for designed trials. The ontology-based system, TrialWiz, [21] designed to alleviate the complexity of the protocol encoding process can be used for easy authoring of clinical trial protocols. More advanced authoring tools that facilitate collaboration of protocol authors from different backgrounds have also been proposed [22]. Other than these applications, several open-source (e.g. OpenClinica [23]) and proprietary (e.g. Cytel [24], Medidata [25]) software have also been designed for assisting or automating the clinical trial protocol design process.Although several of the above mentioned tools and applications have been developed for standardized and computable representation of eligibility criteria few can deal with the complexity of eligibility criteria as presented in full-text protocols. Most of these applications [13–15] are either semi-automated or designed using CT protocols. They still require intense manual involvement and lack flexibility for accommodating complex eligibility criteria from full-text protocols. While these tools may be capable of generating computable and knowledge-based representation of basic eligibility criteria, their utility, usability and adoptability to eligibility criteria from industry-standard protocols have not been tested rigorously. We also note that the use of automated tools for criteria standardization has very limited uptake in the industry as few of them cater to the complexities of full-text protocols. Therefore, we perform a detailed comparison of eligibility criteria representation from CT and Pfizer’s clinical trial protocols and propose a novel method which can be used for industry-standard standardization. In contrast to several of the above mentioned tools, our template-based standardization approach caters to criteria reuse, which is a major objective in most pharmaceutical companies [26].In this paper we present an analysis of eligibility criteria from full-text and CT protocols across 3 dimensions. First, we compare the representation characteristics and textual complexity of eligibility criteria from full-text and their corresponding CT protocols. Second, we perform a semantic class-based comparison of these two forms of eligibility criteria representation. Finally, we generate templates based on a novel method for industry-standard full-text protocol standardization.We selected a set of 32 full-text clinical trial protocols in the domain of Pfizer’s pain-related drug research designed between the years 2002 and 2009. In a majority of these studies, the primary objective was to evaluate the efficacy, safety or tolerability of the drugs in various patient groups under different conditions for pains related to diabetic neuropathy, total-knee arthroplasty, fibromyalgia, osteoarthritis, etc. We used the study identifier of the full-text protocols to retrieve the 32 corresponding XML-formatted protocols from CT, which currently houses over 120,000 clinical trial protocols [6]. Organizations that sponsor or conduct clinical trials are required to submit study information to a clinical trial registry like CT if they plan to publish the findings in a major journal. We wanted to compare eligibility criteria from Pfizer’s full-text clinical trial protocols with the corresponding protocols retrieved from CT to assess the characteristic differences between the representation of CT and full-text. In essence, this will inform us of the complexity and processing overhead in terms of computational methods used for various studies on eligibility criteria (such as standardized representation).We first compared the representation characteristics of the eligibility statements of the full-text and CT protocols from our dataset. We then compared the textual complexity of the “Eligibility Criteria” sections of the full-text protocols with the corresponding study protocols downloaded from CT. We use cumulative and average word counts and sentence counts for this purpose.We analyze the eligibility criteria representation across 32 full-text protocols and their corresponding CT protocols, representing a cross-section of Pfizer protocols in the pain therapeutic area. Our goal here is to develop a sentence-similarity based approach to identify groups of similar sentences having similar conceptual properties but different textual representation across various protocols. Fig. 1shows the underlying algorithm for this process.In this step we converted all the full-text PDF files to their corresponding text format using standard conversion tools. The converted text files were verified to not contain Optical Character Recognition (OCR) errors. These files were then parsed using regular expressions to identify the “Eligibility Criteria” sections. Each “Eligibility Criteria” section was further segmented into Inclusion and Exclusion Criteria for further processing. Similar pre-processing was also performed on the XML files from CT.The Inclusion and Exclusion criteria sections for both the full-text and CT protocols generally comprise of related but disjoint sentences grouped together. These were segmented into sentences using a maximum entropy-based sentence boundary detector, MxTerminator [27]. However, since some criteria sections (appearing as bulleted lists) did not end with periods, we had to follow additional steps to split such sections. On the other hand, separated sentences which were in fact coherent and represented continuing ideas had to be coupled together as a single unit.In this step we processed the individual sentences obtained from the previous step using the National Library of Medicine’s MetaMap program [28]. MetaMap can identify coherent words and phrases from a particular sentence and map them to Unified Medical Language System (UMLS) metathesaurus concepts. We then compared each sentence with every other sentence in the pool of criteria sentences and identified the ones where all the UMLS concept identifiers (CUI) matched exactly. This process helped in ironing out the minor differences in similar sentences with identical concepts but different representations. Word to word matching of sentences was not employed here because even the subtle variability in spelling or sentence structure could not be handled by using that method. The output of this step is a set of sentences that are identical to one or more sentences. Box 1shows the advantage of using MetaMap similarity over simple word to word matching based identical sentence detection. For Sentence 1, we can see that the versions 1 and 2 represent the same criteria, except with different values for a specific test (Creatinine clearance). For Sentence 2, ‘less than’ has different representation in the two versions. Also the screening values for platelet count, which are identical in essence, are represented in different format in the two versions. Using concept matching of MetaMap we can overcome these minor structural, grammatical or numeral variations to elicit the fundamental similarities between sentences. The importance of such abstraction has also been explained previously. The set of remaining sentences, classified as non-identical, are processed in the next step.In this step we identify similar sentences from the set of non-identical sentences using cosine similarity [29]. While cosine similarity has been used extensively for clustering similar sentences and documents [30–33], it has not been explored in the domain of eligibility criteria standardization. For this purpose we create an index of all non-identical sentences using the SMART information retrieval system [34]. Each sentence is treated as an individual record and cosine similarity is calculated with every other sentence in the pool. SMART, based on a vector space model, represents each sentence as a vector of words with different weights associated with each term of a sentence. Standard preprocessing steps like stemming and stopword removal were executed before calculating the cosine similarity of sentences. The cosine measure of similarity for length-normalized pair of vectors q and di, where q represents a query and direpresents a document set,is calculated as follows:sim(q,di)=∑j=1twqjwij∑j=1twqj2×∑j=1twij2Here wqjand wijare the term weights for the vectors q and direpresented as t-dimensional vectors. In our case, query q is a given criteria statement which is compared to the document set direpresented as a pool of the criteria statements (not including q).In SMART we select the standard atc term weighting scheme. Here a represents the term frequency component and is calculated as an augmented term frequency, t represents the collection frequency component and is calculated as the inverse document frequency, while c represents the normalization component calculated as the cosine normalization. A set of similar sentences can be, in essence, treated as a coherent cluster.In this step we select sentences which are similar to a particular sentence. A threshold similarity score of 0.5 (selected on an empirical basis) is used for selecting similar sentences. For each sentence in the pool we find a cluster of similar sentences. A sentence included in a particular cluster is eliminated for inclusion in any other cluster. A higher similarity score threshold gives fewer similar sentences and hence greater number of clusters while a lower similarity score gives more similar sentences clustered into fewer groups. Since an absolute threshold scores does not always result in perfect sentence clusters, we used a post-processing step where each cluster of similar sentences was manually verified to not contain unrelated sentences. Further, clusters were merged based on overlapping or associated ideas. Overall our clustering method with fixed clustering threshold performed well and only 10-20% of the sentences from the inclusion and exclusion criteria had to be manually reassigned for full-text or CT criteria. We found this approach and threshold scores reasonable for our study. Given the complexity of the eligibility criteria sentences from our full-text protocols we did not find any automated tools or applications appropriate for standardization. If a particular sentence does not have any similar sentences (based on the criteria defined above), we consider that sentence to be the only entity in that cluster. Such sentences are assumed to form singleton clusters. A sample clustering of sentences is shown in Fig. 3. In this example, sentences 1, 2 and 4 are detected as similar sentences in one cluster while sentences 3 and 5 are detected as similar sentences in a different cluster. However based on the criteria defined above, we merged these clusters together in the post-processing step. This gave us five sentences which have different textual characteristics and representation but linked by the same underlying concept. Thus the sentence clustering step helped us identify the variability in representation of eligibility criteria. This forms the basis of our standardization approach for eligibility criteria which preserves the underlying properties of each sentence cluster while providing an abstraction of the finer details. For a cluster of sentences, we first identify the key concepts (e.g. diseases or symptoms, pathological tests, therapies or surgeries, etc.) represented in the set of inclusion and exclusion criteria. We then identify the range of values (if any) that correspond to these concepts and represent those with a variable while preserving the units associated with such values. These elements are then manually structured in the form of a template.The identical sentences and the sentence clusters obtained from the previous steps were manually labeled using the semantic classification of clinical research eligibility criteria identified in [11,12]. It is important to note here that the semantic classes used for labeling the sentence clusters were derived only from [11,12] and no extraneous semantic classes were introduced for sentence classification. A total of 27 semantic classes under 6 topic groups were used as reference. For example, the topic group “demographics” contains six semantic classes, namely, address, age, literacy, gender, ethnicity, and special patient characteristics (for details on the other topic groups refer to [11,12]).

@&#CONCLUSIONS@&#
This research aimed at developing techniques for standardized representation of eligibility criteria from industry-standard full-text protocols. A comparison of full protocols vs. their representations in Clinicaltrials.gov supported the need for this approach, since the full protocols were generally very complex. We proposed a method built around using sentence similarity and clustering for identifying groups of similar sentence. We then proposed a novel method for template-based representation of eligibility criteria that can lead to standardization of criteria statements. A combination of these templates potentially offers a standard and formal representation of any eligibility criteria. Such template-based criteria would be amenable for computational processing such as information retrieval, information extraction, etc. Ideally, this process should result in representation of computable criteria for eligibility matching across different protocols. We hope that our effort helps in eliciting differences between the commonly used CT protocols and their full-text counterpart and thus pave the path for building systems that can easily handle industry standard full-text criteria.