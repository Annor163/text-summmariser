@&#MAIN-TITLE@&#
Learning machines: Rationale and application in ground-level ozone prediction

@&#HIGHLIGHTS@&#
Address the potential of learning machine to forecast ground-level ozone in urban area.Summarize the existing learning machines used to predict ground-level ozone.Compare the performance of commented models via practical case in Hong Kong.Address the underlying philosophy of using learning machine in ozone related prediction.

@&#KEYPHRASES@&#
Algorithms,Artificial neural networks,Learning machine,Ozone prediction,Multilayer perceptron,Support vector machine,

@&#ABSTRACT@&#
Multilayer perceptron (MLP) and support vector machine (SVM), two popular learning machines, are increasingly being used as alternatives to classical statistical models for ground-level ozone prediction. However, employing learning machines without sufficient awareness about their limitations can lead to unsatisfactory results in modeling the ozone evolving mechanism, especially during ozone formation episodes. With the spirit of literature review and justification, this paper discusses, with respect to the concerning of ozone prediction, the recently developed algorithms/technologies for treating the most prominent model-performance-degradation limitations. MLP has the “black-box” property, i.e., it hardly provides physical explanation for the trained model, overfitting and local minima problems, and SVM has parameters identification and class imbalance problems. This commentary article aims to stress that the underlying philosophy of using learning machines is by no means as trivial as simply fitting models to the data because it causes difficulties, controversies or unresolved problems. This article also aims to serve as a reference point for further technical readings for experts in relevant fields.

@&#INTRODUCTION@&#
Ozone layer, a deep layer of the stratosphere, shields the entire Earth and absorbs much of the harmful ultraviolet radiation (UV) that comes from the sun. If the ozone layer is removed, large amounts of UV light in the wavelength range of 0.2–0.28λm will enter the atmosphere. These high-energy photons can cause chemical reactions on surfaces they come in contact with, including human skin, which may cause high rates of skin cancer. Thus ozone can be regarded as a beneficial UV shield in the stratosphere. However, there is also a bad type of ozone produced near the ground, due to sunlight interacting with atmospheric pollution in cities. It causes breathing problems to some people with respiratory sensitivity, in certain age groups, and usually occurs in summers when pollution builds up over a city because of stagnant air associated with high pressure areas. Our study targets ozone in the troposphere.The elevated ground-level ozone concentration poses risk to public health in various ways. This continuously engages environmental modelers in developing better forecasting models. Over the last decade or more, the multilayer perceptron (MLP) and the support vector machine (SVM), two popular learning machines, have been increasingly used as alternatives to classical statistical models, e.g., Box-Jenkins and multivariate regression, for ground-level ozone prediction because these learning machines are considered superior to classical statistical models for resolving ozone variations in time series. Such varying trends are nonlinearly impacted by regional meteorology and primary emissions [1–12]. Meanwhile, learning machine is also widely applied in other areas such as pattern recognition [13,14], Sales forecasting [15], category and classification [16] and so on.However, both MLP and SVM, are never panaceas, and if used indiscreetly, model performances can even be degraded drastically, e.g., inherent local minima, “black-box” property and over-fitting for MLP; parameters identification for SVM, etc. During the last decade, neuro-fuzzy model and evolving intelligence are proposed and developed fast to improve the interpretability of “black-box” and local minima in time-evolving processes [17–21]. To tackle the over-fitting, an efficient approach using fast leave-one-out cross-validation (FLOO-CV) is proposed and successfully applied in process modeling [22–24]. With some spirit of review, this article discusses the recently developed algorithms/technologies for treating the most prominent model-performance-degradation limitations in ozone prediction context. These include, for MLP, the “black-box” property, i.e. hardly providing physical explanation for the built model, and overfitting and local minima problems; and for SVM, parameters identification and class imbalance problems. The article stresses, for “to-be” environmental modelers, that the underlying philosophy of using the two learning machines is by no means as trivial as simply fitting models to the data because it reflects difficulties, controversies or unresolved problems. This commentary paper is also intended to serve as a reference point for further technical reading for experts in this field.For ozone prediction the MLP is empowered by its training algorithms. The training algorithms aim to discover the highly nonlinear relationship between ozone and factors that influence it by searching the optimal weight matrix in its weight space. Such optimal weight matrix stores information which can largely represent highly nonlinear relationships. Without offering specified remedies from the algorithm perspective, most researchers stick to gradient-based training algorithms developed in the maximum likelihood framework, which are the origin of the three notorious problems of MLP, i.e., “black-box” property, overfitting and local minima, although for addressing these drawbacks great advances have been made in computer science in the past one decade or more.For a set of training examplesD=(xi,Oi),i=1,…,l, training algorithms in the maximum likelihood framework can conceptually be expressed in the following formulae:(1)O=f(x,w)+ξwhere ξ is the intrinsic data noise with a conditional probability density functionp(O|x;w)=qO−f(x,w), O is the target data, f is the general function representing the MLP's mapping from input to output, and l is the number of training examples. With the normal distribution assumption forp(O|x;w), all training algorithms developed in maximum likelihood framework minimize the error terme(w)or find “the lowest point in the error surface” in the following equation, with respect to weight matrixw.(2)e(w)=∑i=1lOi−f(xi,w)2As for gradient-based training algorithms developed in the maximum likelihood framework, most researchers [1,2,25–34] have preferred to use MLP trained by back propagation (BP) learning algorithm and its variants [35] to forecast ozone level. These models only include the first derivative of the error terme(w), i.e., the gradient ofe(w). They adjust the weight matrix in the steepest descent direction, in which the error terme(w)is decreasing more rapidly. However, associated with the three notorious problems mentioned earlier, BP-trained MLP tends to produce a slow convergence rate because the search may fluctuate wildly depending on the inclination of the error surface close to the local minimum and present at the later training stage.Realizing the slow convergence to local minima property of BP family algorithms, some researchers have begun to employ other gradient-based algorithms that include the first and second derivatives ofe(w), i.e. the gradient and Hessian ofe(w). Gardner and Dorling [36], Agirre-Basurko et al. [37] and Lu et al. [38] employed the scaled conjugate gradient (SCG) algorithm to train MLP for ozone forecasts. Such algorithms search along conjugate directions, avoid a line search at each iteration and do not need to compute the Hessian matrix, which produces a generally faster convergence rate than the steepest descent directions [39]. Chaloulakou et al. [40] and Corani [41] reported use of Levenberg-Marquardt (LM) to train MLP; MLP-LM can obtain satisfactory results, compared with other statistical models, for ozone prediction. LM can reach a good compromise between the speed of the Newton's method and the stability of the steepest descent method, and consequently it constitutes a good balance between these two methods. Schlink et al. [42] carried out a remarkable comparison of use of learning machines for ozone prediction and concluded that MLP trained by quasi-Newton techniques, a class of algorithms based on Newton's method, were among the favorite predictive models. The quasi-Newton techniques, similar to LM, are alternatives for fast convergence, since there is no need to compute Hessian matrix.Although modifications of gradient-based algorithms did improve MLPs performance in ozone prediction somehow, the three inherent problems were not addressed from the algorithm perspective. One could find from the above-mentioned publications that the early stopping, regularization, simple validation and cross-validation techniques are popular techniques for tackling the overfitting problem, re-initializing the weight matrix, and re-training the MLP to solve the local minima problem. However, little effort has apparently been made for addressing the “black-box” property problem so far. To tackle such dilemma, some scholars, e.g., Kasabov [17], Leite et al. [21] proposed neuro-fuzzy based methods to mitigate the impact of “black-box” via dynamic intervene during the training processes. Their efforts have sown advantages and improvements to certain degree.Different from MLP developed in maximum likelihood, Bayesian MLP attempts to maximize a posterior probability distributionP(w|D)with respect to weight matrixw, and uses the distribution to estimate the possible distribution of MLP optimal weightwopt. The form ofP(w|D)was expressed by MacKay [43] in the following equations and has been shown to approximatee(w)in Eq. (2) when l goes to infinite [44].(3)P(w|D)=1Zsexp−S(w)(4)S(w)=β2∑n=1lf(xn;w)−tn2+α2∑i=1Vwi2wherewiis the ith weight component inw,vis the number of weight components ofw,f(xn;w)is the MLP output with respect to the nth input data, tnis the nth target data, α and β are two hyperparameters that govern the magnitude ofwand variance of data noise, respectively, and Zsis a normalization factor independent of weight vector. Since the two hyperparameters can be automatically tuned with respect to data complexity in the evidence framework, Bayesian MLP is insensitive to MLP topology and thus resists the overfitting problem.Wang and Lu [45] employed an extension of Bayesian MLP, i.e. MLP with automatic relevance determination, MLP-ARD, for ozone prediction. In that model, input-layer weights are grouped into N sub-weight groups, where N is the number of input variables. The weights in each sub-weight group are defined as weights fanning out from each input variable, and the magnitude and the direction of weights in each sub-weight group are separately governed by each hyperparameterαw1j(j=1, …, N). The trained MLP-ARD thus can rank the relative importance of jth input variable according to the magnitude ofαw1jand provide physical explanation of factors that influence ozone; these factors are MLP inputs. In addition, the authors stressed the interval prediction ability of MLP-ARD, and concluded that ozone peaks were better predicted by the upper interval of model output, compared with the point prediction by MLP trained by LM algorithm. One possible shortcoming of their work could be local minima since in the training process of MLP-ARD, the scaled conjugate gradient (SCG) algorithm is used to maximizeP(w|D)with respect towand other hyperparameters. Cross-validation technique and other population-search algorithms discussed in the following can be remedies.To train MLPs for ozone prediction, gradient-based algorithms, whether within the maximum likelihood or Bayesian framework, are deterministic local optimization algorithms, e.g., Levenberg-Marquardt and scaled conjugate gradient algorithms, which are individual-based search techniques to find the minima in the error surface ofe(w). Although the magnitude ofe(w)is decreased rapidly in the early phase of training, this category of optimization algorithms has a reputation for convergence to the local minima and easy overfitting [46]. Hence, finding the optimal weight matrix may not be guaranteed, as described in Fig. 1. The other category is stochastic global optimization algorithms, which are population-based searching techniques, e.g., genetic algorithm and particle swarm optimization (PSO) algorithm [47], which are believed to be capable of avoiding drawbacks of gradient-based algorithms.Lu et al. [48] first employed the standard PSO to train MLP (named as MLP-PSO) for forecasting several primary air pollutants’ levels, and also showed that MLP-PSO outperformed MLP trained by local optimization algorithms since local minima was avoided. Without applying such a model directly to ozone prediction, Wang and Lu [49] and Wang and Lu [50] proposed two MLPs trained by two hybrid algorithms based on the standard PSO. They argued that for ozone prediction, larger MLP weight space would be spanned, compared with the works of Lu et al. [48,51], since more influential factors accounting for ozone variation would be involved as MLP inputs and thus the standard PSO would suffer the curse of dimensionality. This can inevitably lead to a situation where the larger the dimension of MLP weight space is, the less is the possibility of the solution converging to the global/appropriate minima. In other words, despite the standard PSO's global search ability in optimization, the ‘bad’ local minima can still not be avoided completely during the MLP training process.Instead of initializing the weight matrices population randomly, which is the common practice in the standard PSO for training MLP, Wang and Lu [49] employed the hybrid Monte Carlo (HMC) method, i.e., MLP-HMC, to find an ergodic Markov chain that hasP(w|D)in Eq. (3) as its equilibrium distribution. Then it exploresP(w|D)to sample the initial weight matrix population, which in fact confines the search space to the area where the ‘better’ local minima exists. The standard PSO algorithm continues to train MLP with the weight matrices population initialized from HMC. The hybrid model's prediction performance was reported to be better, especially for ozone peak, compared with the one from MLP-PSO, which justifies the notion that the local minima also plagues global optimization algorithms and can be avoided by hybridizing with other algorithms.Realizing that the standard PSO does encounter the local minima during MLP training for ozone prediction, Wang and Lu [50] hybridized the standard PSO with LM algorithm to formulate MLP-PSO-LM, where the LM algorithm can be inserted automatically to replace the standard PSO when stagnation occurs. The process of training MLP continues with LM instead, until convergence. The key in this work was that the authors indicated the local minima's occurrence with stagnation, which was at the later phase of the standard PSO training MLP; the difference of the interested error criterion between two successive PSO generations becomes extremely small. The results manifested that combining local and global optimization algorithms in MLP training process can avoid local minima associated with the standard PSO, and MLP with such a hybrid training algorithm, i.e. MLP-HMC, outperformed MLP with the standard PSO, i.e. MLP-PSO, in ozone prediction, especially during its episodes.By developing MLP training algorithms, overfitting and local minima problems have been effectively addressed in ozone prediction literature. However, it seems difficult to find or develop algorithms to overcome the two problems simultaneously in the MLP, or even neural network domain; of course, other techniques like cross-validation, early stopping and regularization, are also effective to a degree but not from the perspective of training algorithms. Support vector machine, another category of learning machine, has recently become a promising solution for this purpose.Support vector machine (SVM) was developed from the statistical learning theory [52] and belongs to supervised learning model with associated learning algorithms that analyze data and recognize patterns via statistical classification and regression analysis. More formally, a support vector machine constructs an optimal hyperplane or set of hyperplanes in a high- or infinite-dimensional space, which can be used for transformation of linearly separable or even non-separable patterns, classification, regression, or other tasks. Such learning machine possesses the characteristics of concurrently minimizing the expected value of loss and maximizing the margin of hyperplane. For a given set of training data as below:(5)D=(xi,yi)xi∈Rp,yi∈−1,1i=1nHere, yiis either 1 or −1, indicating the class to which the point xibelongs; each xiis a p-dimensional real vector. SVM aims to find the maximum-margin hyperplane that divides the points having yi=1 from those having yi=−1. Any hyperplane (with the normal vector w and offset of hyperplanebw) can be written as the set of points xisatisfying:(6)w⋅x−b=0If the training data are linearly separable, one can select two hyperplanes in a way that they separate the data and there are no points between them, and then try to maximize the distance between. The region bounded is called “the margin” shown in Fig. 2. These hyperplanes can be described by the equations below:(7)w⋅x−b=1and(8)w⋅x−b=−1From Fig. 2, one can find the distance between the two hyperplanes is2w, so maximum-margin is achievable via minimizingw. Meanwhile, it is also required to prevent data points falling into the margin with the constraints added for eachieitherw⋅xi−b≥1for xiof the first class orw⋅xi−b≤−1for xiof the second one. These two can be rewritten as:(9)yi(w⋅xi−b)≥1,forall1≤i≤nHence, the final optimization problem can be expressed as below:(10)Minimize:w(inw,b)(11)Subjectto:yi(w⋅xi−b)≥1(foranyi=1,…,n)For linearly separable cases, the solution algorithms include ‘Primal form’, ‘Dual form’, ‘Biased and unbiased hyperplanes’, and ‘Soft margin’ [52,53]. For nonlinear classifications, Bernhard et al. [54] and Vapnik and Kortz [55] suggested a way to create nonlinear classifiers by applying the kernel trick to maximum-margin hyperplanes. The resulting algorithm is formally similar to linear cases except that every dot product is replaced by a nonlinear kernel function, which allows the algorithm to fit the maximum-margin hyperplane in a transformed feature space. The most frequent used kernel function is a Gaussian radial basis function, in which the feature space is a Hilbert space of infinite dimensions. The maximum margin classifiers are well regularized, so the infinite dimensions do not spoil the results.In SVM approach, all learning machines are supposed to minimize the expected value of lossRwexpressed by the following equation:(12)R(w)≤Remp(w)+Q(l,hη)For neural networks, e.g., MLP models, when implementing the empirical risk minimization principle, the aim is to minimize the first summand (in this case,R(w)=Remp(w)=e(w)) in Eq. (5), which is, in fact, to minimize the training error and is the root of the three problems mentioned before. While implementing the structural risk minimization principle, SVM minimizes both terms and, in fact, minimizes the upper bound of generalization error, which endows it the strong ability to resist overfitting. In addition, the solver of SVM usually comes down to quadratic programming or linear programming problem, which can make it exempt from local minima, since only unique solutions can be obtained. The structural risk minimization principle, however, is also implicitly associated with other disadvantages mentioned below.Compared with MLP models in ozone prediction application, SVM application has been rare, although SVM has distinctive properties for tackling both overfitting and local minima problem simultaneously. Lu and Wang [4] pioneered the use of support vector regression (SVR) method to predict several primary air pollutants’ levels and found the performance to be better, compared with the classical radial basis function network, a type of neural network. The authors also discussed SVR parameters identification since parameters can considerably impact the model's predictive performance. The computation-intensive trial-and-error method was used to identify the appropriate SVR parameters, although better methods are available in SVM literature [10,56,57–59]. Later, Lu and Wang [60] first brought out the class imbalance problem for the standard support vector classifier (SVC) and SVR applications in ozone days classification/prediction. The authors stated that in most of collected field data, the number of ozone polluted days is much smaller than that of non-polluted days, which deteriorates any regressors/classifiers performance on the minority class, i.e. ozone polluted days. More recently, the single least squares support vector regression (LSSVR) model is proposed for tackling online, multi-grade processes and effectively applied in fine chemical and polymer industries [22,24]. Liu et al. also indicated the importance of proper selection of statistical parameters like α, ɛ, etc. with Gaussian kernel function. Realizing that the SVC has the same high resistance to overfitting and local minima free properties like SVR, they modified SVC by proposing a cost-sensitive classification scheme which can successfully tackle the class imbalance problem and improve prediction accuracy for ozone peak, compared with the standard SVC and SVR. To identify the appropriate parameters for SVC and SVR, unfortunately, the authors used the grid search method which can have an exhaustive search in parameter space but is still computation-intensive, though effective. The extraordinary reward for using SVM in ozone prediction seems to be its good performance in ozone episodes. The most likely reason is the unique trait of resistance to overfitting and avoiding local minima. Similar outcomes can also been found in the literature like [5,6,8–10,22,59,61] as well.

@&#CONCLUSIONS@&#
