@&#MAIN-TITLE@&#
Feature selection based on improved ant colony optimization for online detection of foreign fiber in cotton

@&#HIGHLIGHTS@&#
A feature selection method for online detection of cotton foreign fiber is presented.The proposed method efficiently finds the excellent subset in multi-character feature sets.The selected sets by proposed method efficiently reduce the time of online detection.The proposed method improves the performance of industrial equipment.

@&#KEYPHRASES@&#
Foreign fibers in cotton,Online detection,Feature selection,Ant colony optimization,Group constraint,

@&#ABSTRACT@&#
Feature selection plays an important role in the machine-vision-based online detection of foreign fibers in cotton because of improvement detection accuracy and speed. Feature sets of foreign fibers in cotton belong to multi-character feature sets. That means the high-quality feature sets of foreign fibers in cotton consist of three classes of features which are respectively the color, texture and shape features. The multi-character feature sets naturally contain a space constraint which lead to the smaller feature space than the general feature set with the same number of features, however the existing algorithms do not consider the space characteristic of multi-character feature sets and treat the multi-character feature sets as the general feature sets. This paper proposed an improved ant colony optimization for feature selection, whose objective is to find the (near) optimal subsets in multi-character feature sets. In the proposed algorithm, group constraint is adopted to limit subset constructing process and probability transition for reducing the effect of invalid subsets and improve the convergence efficiency. As a result, the algorithm can effectively find the high-quality subsets in the feature space of multi-character feature sets. The proposed algorithm is tested in the datasets of foreign fibers in cotton and comparisons with other methods are also made. The experimental results show that the proposed algorithm can find the high-quality subsets with smaller size and high classification accuracy. This is very important to improve performance of online detection systems of foreign fibers in cotton.

@&#INTRODUCTION@&#
The foreign fibers in cotton (FFC for short) are usually referred to as the non-cotton fibers and dyed fibers, such as hair, binding ropes, plastic films, candy wrappers, polypropylene twines. FFC, even of small amounts, will seriously affect the quality of the cotton textile products and lead to great economic loss for the cotton textile enterprises [1]. In recent years, the machine-vision-based technology has been studied for FFC online detection [2,3]. However, for machine-vision-based FFC online detection systems, achieving high detection efficiency is still a significant challenge, the reasons may be: (1) most of FFC are small and of different varieties and (2) the adopted feature sets of FFC could not be high discriminative. Currently, finding high discriminative feature sets with small size and high accuracy is one of efficient ways to solve the problem due to it can reduce the time of feature extraction, simplify the design of classifier and improve detection accuracy. As a result, feature selection (FS for short) technology is applied to the domain of FFC online detection for selecting the high-quality feature set.FS is a commonly used step in machine learning, especially when dealing with the high dimensional space of features [4]. FS mainly aims at three aspects: (1) to reduce the cost of extracting features, (2) to improve the classification accuracy, and (3) to improve the reliability of the performance [5,6]. Search strategy, which is an approach to find the best feature combination, is a key unit in FS algorithm [7], and the success of different approaches mainly depends on considering fruitful search strategy in FS process. The different strategies use different ways to generate subsets and progress the search processes. One way is the sequential forward search (SFS) which starts the search process with an empty set and successively add features [8]. Another way is called as the sequential backward search (SBS) which searches with a full set and successively remove features. This sequential strategy is simple to implement and fast but affected by nesting effect, which signifies that once a feature is added (or, deleted) it cannot be deleted (or, added) later. On the other hand, one can start search process with a randomly selected subset involving a sequential strategy, called as the random search strategy. In addition, a good solution can be found using complete search strategy, since it covers searching for the whole feature space. However, such strategy is usually not feasible, since it requires longer time while involving larger feature sets.Most search strategies discussed above, however, attempt to find solutions that range between sub-optimal and near optimal regions, since they involve searching locally rather than globally. On the other hand, finding the solution of optimal or near optimal is quite difficult for those search algorithms due to involving partial search in the solution space or suffering from computational complexity. Therefore, the recent trend of research has been shifted toward meta-heuristics algorithms (or global search algorithms), such as, genetic algorithm (GA) [9,10], particle swarm optimization algorithm (PSO) [11] and ant colony optimization algorithm (ACO) [12]. They find the solution in the full search space by their global search capability with utilizing the local search suitably. The global search algorithms work on basis of the activity of multi-agents, which ultimately enhance to find very high-quality solutions within a reasonable time.Usually, the meta-heuristics-based FS methods are feasible to find the high-quality feature combination of FFC, however, these methods do not consider the characteristic of feature space of FFC, i.e. the high-quality feature sets of FFC consist of three classes of features which are respectively the color, texture and shape features (this has been commonly accepted [5]). Such feature sets are called as the multi-character feature set (MCFS for short) [5]. MCFS consists of several groups of features with completely different characteristic. Features in different groups have different contributions to the classification without group redundancy. That means the (near) optimal set at least includes one feature from every group. For MCFS, there naturally exists the space constrain so that its feature space is smaller than that of general feature set with the same number of features. However, the most existing meta-heuristic-based FS methods do not consider the characteristic of MCFS and regard MCFS as the general feature set, these methods search the whole feature space without constraint to find the (near) optimal subset, so that any possible feature combination could be regarded as a candidate, even if invalid candidates. Here, the invalid candidates are the subsets which lose the information of several subspaces and at least do not contain the features of certain subspace.In this study, to improve the performance of FFC online detection systems, our contributions are as follows:(1)To address MCFS, we proposed an improved ant colony optimization for feature selection (IAFS), which has better performance in MCFS than the general FS methods. In IAFS, according to space partition of MCFS, the group constraint is adopted into the algorithm and the ant searches each subspace in turn to find the good feature combination. Consequently, the algorithm can reduce the effect of invalid candidates and further improve search efficiency. The experimental results show that IAFS could receive the improved searching ability and find the high discriminative subset with small size in MCFS.To select the high-quality feature set of FFC, we extract lots of features of FFC from the FFC image, and use IAFS to select the excellent subsets with small size and high discriminability. According to our experimental results, two FFC feature set, respectively corresponding to two classifier, are recommended to the machine-vision-based FFC online detection systems. The recommended feature sets of FFC can efficiently improve the performance of FFC online detection systems.The proposed method and recommended feature sets show the superiority and important value during the process of industrial application. They can efficiently improve the performance of industrial equipment and promote the development of FFC online detection technology.The remainder of this paper is organized as follows. Section “Preliminaries” introduces some basic concepts of classification, FS and ant colony optimization. Section “The improved ant colony optimization for feature selection” in detail introduces the proposed algorithm. Section “Experiments” provides the experimental results and discussion. Conclusions are summarized in Section “Conclusions”.In this section, we introduce the basic notions of classification and FS, and ant colony optimization for feature selection.Machine learning is usually divided into two main types: supervised learning and unsupervised learning. In the supervised learning approach, the goal is to learn a mapping from inputs x to outputs y, given a labeled set of input–output pairsD={xi,yi}i=1n. Here D is called the training set, and N is the number of training examples.In the simplest setting, each training input xiis a D-dimensional vector of numbers, representing, say, the height and weight of a person. These are called features, attributes or covariates. In general, however, xicould be a complex structured object, such as an image, a sentence, an email message, a time series, a molecular shape, a graph, etc. Similarly the form of the output or response variable can in principle be anything, but most methods assume that yiis a categorical or nominal variable from some finite set, yi∈{1, …, C} (such as male or female), or that yiis a real-valued scalar (such as income level). When yiis categorical, the problem is known as classification or pattern recognition, and when yiis real-valued, the problem is known as regression.For classification, the goal is to learn a mapping from inputs x to outputs y, where yi∈{1, …, C} with C being the number of classes. If C=2, this is called binary classification (in case we often assume yi∈{1, 0}); if C>2, this is called multiclass classification. If the class labels are not mutually exclusive (e.g., somebody may be classified as tall and strong), we call it multi-label classification, but this is best viewed as predicting multiple related binary class labels (a so-called multiple output model). One way to formalize the problem is as function approximation. We assume y=f(x) for some unknown function f, and the goal of learning is to estimate the function f given a labeled training set, and then to make predictions usingy¯=f¯(x). (We use the short line symbol to denote an estimate.) Our main goal is to make predictions on novel inputs, meaning ones that we have not seen before (this is called generalization), since predicting the response on the training set is easy.The high dimensionality of data poses challenges to learning tasks because of the curse of dimensionality. In the presence of many irrelevant and redundant features, learning models tend to overfit and become less comprehensive. FS is one effective way to identify relevant and redundant features for dimensionality reduction. Various studies show that features can be removed without performance deterioration.FS is closely related to two conceptions that are relevance and redundancy. A popular definition of relevance is given as the following.Let F be the full set of features, fibe a feature, Si=F−{fi}. Let C denote the class label and P denote the conditional probability of the class label C given a feature set. The statistical relevance of a feature can be formalized as:Definition 1(Relevance)A feature Fiis relevant iffOtherwise, the feature Fiis said to be irrelevant.Definition 2(Redundancy)A feature Fiis redundant iffDefinition 1 suggests that a feature, Fi, is statistically relevant if its removal from the feature set will decrease the prediction power.Definition 2 suggests that a feature, Fi, can become redundant due to the existence of other relevant features, which provide similar prediction power as Fi.Then, FS is to find the minimum subset S from set F, which has maximal relevance and minimum redundancy as possible. Meanwhile the subset S is optimized for improving the performance of the machine learning algorithm.Obviously, the rigorous way to find the optimal subset S is exhaustive search and evaluation of all the possible subsets of F, which has been demonstrated as NP-Completeness. To circumvent this problem, many heuristic subset search or selection strategies have been proposed [13].Usually, FS process includes four components, as follows:1.Feature subset generation. Subset generation produces candidate subsets based on a certain search strategy.Subset evaluation. Each candidate subset is evaluated by a certain evaluation measure and compared with the previous best one with respect to this measure. If a new subset turns out to be better, it replaces the previous best subset.Stopping criteria. The processes of subset generation and evaluation are repeated until a given stopping criterion is satisfied.Results validation. The finally selected subset is subject to result validation by some given learning algorithms.Ant colony optimization (ACO for short) was proposed by Dorigo and colleagues in the early 1990s [14], which is inspired by social behavior of ant colonies and belongs to a branch of swarm intelligence. ACO is first used for solving traveling salesman problem and quadratic assignment problem. ACO also can be used to obtain approximately optimal solutions in a reasonable amount of computational time for hard combinatorial optimization problems [12,15].Naturally, ants are capable of finding the shortest route between a food source and their nest by chemical materials called pheromone that they leave when moving. A moving ant lays some pheromone on the ground, thus making a path by a trail of this substance. While an isolated ant moves practically at random, exploration, an ant encountering a previously laid trail can detect it and decide with high probability to follow it, exploitation, and consequently reinforces the trail with its own pheromone. What emerges is a form of autocatalytic process through which the more the ants follow a trail, the more attractive that trail becomes to be followed. The process is thus characterized by a positive feedback loop, during which the probability of choosing a path increases with the number of ants that previously choose the same path. The mechanism above is the inspiration for the algorithms of the ACO family.The basic idea applying ACO to FS could be explained by a graph. As shown in Fig. 1, the nodes represent features and the edges denote the paths. The search for the optimal feature subset is an ant traversal through the graph where a minimum number of nodes are visited that satisfies the traversal stopping criterion. The ant is currently at node f1 and has a choice of which feature to add next to its path (dotted lines). It chooses feature f2 according to the probability transition rule, then f3 and f4. Upon arrival at f4, the current subset {f1, f2, f3, f4} is determined to satisfy the traversal stopping criterion. Then the ant finishes its traversal and produces this subset as a candidate [16]. In the process of building candidates, the transition rule and pheromone update rule can be applied. This is ant colony optimization for feature selection (ACOFS for short).In ACOFS, there are three basic terminologies that are respectively heuristic information, pheromone and transition probability rule.Heuristic information, for each feature, usually represents the attractiveness of the features. It is necessary to use, otherwise, the algorithm may become too greedy, and the better subset may not be appeared. A suitable heuristic desirability of each feature could be some evaluation measures, for example, an entropy-based measure and rough set dependency measure [17].Pheromone is another important concept in ACO algorithms. Ants exploit features that are best in prior iterations through the pheromone update rule. The quantity of pheromone indicates how good the features are in prior iterations. All ants can update the pheromone and the best ant deposits additional pheromone on nodes of the best subsets. This leads to the exploration of ants around the optimal solution in next iterations. After all ants completed their subsets, the pheromone values τ of the feature i is updated according to Eq. (1):(1)τi(t+1)=(1−ρ)τi(t)+∑k=1mΔτij(t)+Δτig(t)(2)Δτij(t)=φγ(sj(t))ifi∈sj(t)0otherwisewhere ρ∈(0,1) is the pheromone trail decay coefficient which role is to avoid stagnation, m is the number of ants at iteration t, and g indicates the best ant at iteration t, sj(t) is the feature subset constructed by ant j at iteration t, γ(x) denotes fitness, φ is a parameter that control fitness value, φ∈(0,1).The ants select new feature according to the transition probability rule. Firstly, assign the ants randomly to one feature, then each ant builds its subset according to transition probability. The transition probability can effectively balance the exploitation–exploration trade-off. It is controlled by the heuristic information and pheromone by the following Eq. (3), i.e. the probability of an ant j choosing to travel to the feature i at iteration t:(3)pij(t)=[τi(t)]α[ηi(t)]β∑u=Jj[τu]α[ηu]βifi∈Jj0otherwisewhere Jjis the set of feasible features that can be added to the partial solution, τiand ηidenote the pheromone value and heuristic desirability of the ith feature, respectively. α and β are two parameters that determine the relative importance of the pheromone value and heuristic information.The flow chart of ACOFS can be shown in Fig. 2.Multi-character feature set (MCFS, for short) consists of several groups of features with completely different characteristic. Features in different groups have different contributions to the classification without group redundancy. That is say, the optimal set at least includes one feature from every group. Garbage features may appear in each group.MCFS can be formulized as follows:Given a feature set S which can contain irrelevant or redundant features, Siis the set of all the irrelevant feature in S, and Sris the set of all the redundant features of S, the optimal set of S is defined as So, we haveSo=S−Si−SrAssume S can be divided into separate K groups, we haveS=S1∪S2∪…∪SKandS1∩S2∩…∩SK=ΦifSo=S1*∪S2*∪…∪SK*,∀Sk*≠Φand∀Sk*⊂SK(k∈{1,2,…,K})thenS is MCFS.According to above definition, at least one feature in every subset is contained in the optimal set, that is say that the number of features of the optimal feature set is at least the number of groups, i.e. K. Assume the number of features is n, the number of features in every group is respectively n1, n2, …, nKand n1+n2+⋯+nK=n. We analyze the scale of feature space. For FS, the feature set usually is coded as n×1 binary vector, where n is the number of features, and a feature is only one of two states, i.e. 0 or 1. If one feature is selected, the corresponding element of vector is 1, otherwise, 0. Due to there is at least a feature in the possible selected subset, the space scale of MCFS is (2n1−1)×(2n2−1)×⋯×(2nK−1). But for the general feature set with the same number of features, the space scale is 2n. Obviously, the space scale of MCFS is less than the space scale of general feature set with the same number of features. In contrast to MCFS, the added scale of the general feature sets consists of two kinds of feature combinations, one is such feature combinations which include less than K features (K is the number of groups), the other is such combinations which include at least K feature but the number of groups contributing these features is less than K. Here we call above two kinds of feature combinations (feature sets) as the invalid feature sets of MCFS, which can formalized as follows.For an arbitrary subset Ssubof S where S is MCFS with K groups (S1, S2, …, SK):If∃Si∩Ssub=Φ,i=1,…,KthenSsubis an invalid subset.For ACOFS, it ignores the group information of MCFS and regards MCFS as the general feature set. In ACOFS, the ants search the features in the feature space of the general feature set and construct each candidate by probability transition rule. Since there is not the explicit mechanism for these invalid subsets, it is possible for ACOFS to regard any combination of the features as candidate so that lots of invalid candidates could be produced, especially in the beginning stage. These invalid candidates could delay the progress of ant finding the (near) optimal subsets and reduce the performance of the algorithm. However, the algorithm could finally find the (near) optimal subsets under the guidance of the heuristic information and pheromone.For MCFS, these invalid subsets enlarge the scale of feature space of MCFS and decrease the efficiency of the algorithms. To address the problem, in this paper, we propose the improved ant colony optimization for feature selection (IAFS, for short), which can avoids invalid candidates and provides the better search performance for MCFS.To avoid to produce the invalid feature sets, group constraint is adopted into IAFS. The basic idea of group constraint is that in the algorithm, the group information of features is used to direct the subset construction to prohibit to produce the invalid feature sets as the candidates, so that the algorithm can reduce the scale of the search space and improve the convergence ability, on the other hand it also can partially avoid local convergence. For this, in contrast to ACOFS, there are three improved aspects which are feature space partition, probability transition rule and subset construction, respectively. The feature space partition is the basis of group constraint which corresponds to the groups of MCFS and provides the group information of the features to probability transition rule and subset construction. On the other hand, for probability transition rule and subset construction, they select the next feature and produce the candidate under the constraint of the group information of the features.Feature space partition is that dividing feature space into separate subspaces according to the groups of MCFS, and assign a unique group label to every feature. For example, the feature space of FFC can be divided into three subspaces: color, texture and shape subspace, which correspond to color, texture and shape features, respectively. As is shown in Fig. 3, the color group includes 27 color features which are denoted by f1–f27, the texture group includes 41 texture features which are denoted by f28–f68, and the shape group includes 7 shape features which are denoted by f69–f75. When constructing subsets, according to the group information of the feature, ants decide if one feature is put into its candidate.When constructing the subsets, in IAFS, the ants in turn select the feature in subspaces according to modified probability transition rule instead of the full space in ACOFS. This search strategy can ensure each subspace to contribute at least one feature to feature subset. Consequently, the produced candidates is valid subsets. The detailed steps see Section “Subset construction”. Due to the next selected feature coming from the unselected features of the subspace instead of the full space, we also modify the probability transition rule in terms of group information.In ACOFS, probability transition rule does not consider the group information, however in proposed method, to adapt the proposed subset construction, we modify the transition probability rule, which is shown in Eq. (4). In contrast to the original transition rule, the difference lies in the denominator, which only considers the unselected features in the corresponding subspace. Because of the heuristic information and pheromone value making use of global information, ants still can select the features which have most contribution to fitness. The transition probability of the ant j choosing to travel to the feature i in the group k from its current position at time t is calculated according to the following Eq. (4).(4)pij(t)=[τi(t)]α[ηi(t)]β∑u=Jkj[τu]α[ηu]βifi∈Jkj0otherwisewhereJkjis the set of feasible features in the current group k that can be added to the subset, τiand ηidenote the pheromone value and heuristic desirability of the ith feature in the group k, respectively.Base on above idea, the proposed algorithm achieve the goal of improving search efficiency for MCFS.Fig. 4shows the flowchart of IAFS, which is further explained as follows:Step 1Initialization.Set the population of ants m, the level of pheromone trials τ associated with each feature i by setting the equal value, and the maximum of allowed iterations t.Initialize the heuristic information.The heuristic information η of each feature is set by information gain. Information gain is mentioned in Section “Heuristic information”.Generate m ants.Construct subsets.Construct subset Sjfollowing the scheme of subset construction in Section “Subset construction”. The ants select new feature using the probabilistic transition rule in Eq. (4).Evaluate the subsets.Evaluate the constructed subsets Sjusing the fitness function mentioned in Section “Fitness function”.Update the best global subset Sbest.Check the stop criterion.Check whether the algorithm has executed t iterations or not. If executed then the FS process is stopped and the best subset Sbestis output, otherwise, continue.Pheromone updating.After all ants have completed their candidates, the pheromone value τ of each feature i is updated according to Eq. (1), which is mentioned in Section “Ant colony optimization for feature selection”. After finishing pheromone updating, go to Step 3, generate a new population of ants m then iterate the above procedures.It is clear that the basic steps in IAFS is nearly similar to that in ACOFS but differ in space partition, subset construction and probability transition rule. The following section gives the further detailed steps about subset construction.In IAFS, group constraint is imposed to the subset constructing process. The steps of the detailed subset construction are described in the dotted box of Fig. 4, which are further explained as follows:Step 1The ant randomly selects the first feature, which could come from any one of k groups. For example, for FFC features, the first selected feature may be the color, texture or shape features.Choose a subspace which has not contributed the feature to subset in this iteration. Then in this subspace, select a feature from feasible features by using probability transition rule and add to subset. Evaluate the subset to decide whether the selected feature is added to the subset. Here, the evaluation criterion is mean square error (MSE) of classifier. If the subset, which contains the selected feature, decreases the MSE of the classifier and discards the feature from the subset, then selects next feature and evaluate it in the same way. Once there is a feature to be selected into subset in four times, stop the operation in this group, then determine the next group and repeat the above operation. If there is not the feature in the group to be added into subset in four times, also choose the next group. When the ant has finished operation in all the groups, go to Step 3.Check the subset whether there are new features which is added to subset by Step 2. If there are new features in the subset, go to the Step 2; otherwise continue.For the constructed subset, finding such groups in which all the features are not selected.For each group found by Step 4, add the feature with the maximum transition probability in its group into the subset. The subset construction finished. This step guarantees that each group contributes a relatively good feature to subset.Above steps are complete subsets constructing process, which can guarantee that each candidate is valid.In this study, the information gain (IG) is used to measure heuristic information for each feature, which can provide a statistical measure of the relevance of features to the heuristic information of each feature [18]. The information gain IG(P, Ni) of a feature Ni(i=1, 2, …, n) for a number of examples P of a given dataset can be defined as follows:(5)IG(P,Ni)=Entropy(P)−∑s∈vals(Ni)PsPEntropy(Ps)(6)Entropy(P)=∑i=1c−pilog2piwhere vals(Ni) is the set of all possible values for the feature Ni, and Psis the subset of P for which Nihas value s. Note that Psis the number of sub-examples for a particular value of s among the total number of examples P of the given dataset. However, in practice, for reducing computational cost, the continuous random variables need to be discretized when measuring Entropy(Ps). In our work, we use the equal-width unsupervised method to discretize the continuous feature [19]. In this method, the continuous range of a feature is divided into intervals that have an equal-width and each interval represents a bin which is associated with a distinct discrete value. The number of bins corresponding to each feature is determined in advance. Let the discrete value associated with the specific bin denote the continuous value in the bin.A fitness function is used to evaluate the degree of goodness of selected subset. In the real-time classification problem, if two subsets with different number of features achieve the same performance, the subset with fewer features is preferred.Therefore, the fitness evaluation should consider two aspects: One is the classification accuracy; the other is the number of features. Combining these two concerns, the fitness function is given as:(7)f(Xj)=υJ(Xj)+ψ(1/|Xj|)where Xiis the subset selected by j-th ant, J(Xi) is the classification accuracy of Xi, |Xi| is the number of features of Xi, v∈[0,1] and ψ∈[0,1] are the two parameters which determine the relative importance of the classification accuracy and number of the selected subset.

@&#CONCLUSIONS@&#
FS plays an important role in the FFC online detection because FS can improve the performance of industrial equipment and promote the development of FFC online detection technology. In this paper, an improved ant colony algorithm for feature selection is proposed to solve the problem of feature selection for FFC online detection. According to characteristic of MCFS, the proposed algorithm adopts group constraint to reduce the effect of invalid subset and improve search efficiency of algorithm. The proposed algorithm is validated on the FFC dataset, and the experimental results show that the proposed algorithm is more effective and efficient for FFC dataset than the general FS methods.To find the best feature sets for FFC online detection, we have extracted 75 FFC features from FFC images, and successfully make use of the proposed algorithm to select the high-quality subset with small size. Based on the experimental results, we have recommended two high-quality feature set, respectively corresponding to kNN and SVM classifiers. It is very significant for FFC online detection to improve detection accuracy and decrease computational cost.The proposed method shows the superiority and important value during the process of industrial application. They can efficiently improve the performance of industrial equipment and promote the development of FFC online detection technology.For MCFS, the proposed algorithm is a very efficient FS method. The more the number of the groups is, the more effective the algorithm becomes. However, for the general feature sets, the proposed algorithm is not more efficient than ACOFS. In this case the proposed algorithm has the same performance as ACOFS. For future works, the authors intend to investigate the high-quality subset by taking advantage of other advanced classifiers such as ensemble classifiers. We also will consider combining filter approach with wrapper approach for finding the high-quality subset of FFC.