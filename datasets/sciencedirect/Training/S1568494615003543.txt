@&#MAIN-TITLE@&#
Development of Pareto-based evolutionary model integrated with dynamic goal programming and successive linear objective reduction

@&#HIGHLIGHTS@&#
This paper presents a new Pareto-based evolutionary model incorporated with preference-ordering and objective-dimension reduction to improve the multi-directional searches for multi-objective problems.It induces a convergence toward the Pareto-optimal front by adjusting aspiration levels allocated to objectives and by excluding redundant objectives during optimization.Its usefulness was validated for multi-objective test problems comparing to conventional single- and multi-objective optimization models.

@&#KEYPHRASES@&#
Objective-reduction,Preference-ordering,Evolutionary process,Many-objective problem,Pareto-optimal front,

@&#ABSTRACT@&#
This study investigates the coupling effects of objective-reduction and preference-ordering schemes on the search efficiency in the evolutionary process of multi-objective optimization. The difficulty in solving a many-objective problem increases with the number of conflicting objectives. Degenerated objective space can enhance the multi-directional search toward the multi-dimensional Pareto-optimal front by eliminating redundant objectives, but it is difficult to capture the true Pareto-relation among objectives in the non-optimal solution domain. Successive linear objective-reduction for the dimensionality-reduction and dynamic goal programming for preference-ordering are developed individually and combined with a multi-objective genetic algorithm in order to reflect the aspiration levels for the essential objectives adaptively during optimization. The performance of the proposed framework is demonstrated in redundant and non-redundant benchmark test problems. The preference-ordering approach induces the non-dominated solutions near the front despite enduring a small loss in diversity of the solutions. The induced solutions facilitate a degeneration of the Pareto-optimal front using successive linear objective-reduction, which updates the set of essential objectives by excluding non-conflicting objectives from the set of total objectives based on a principal component analysis. Salient issues related to real-world problems are discussed based on the results of an oil-field application.

@&#INTRODUCTION@&#
The term many-objective problems (MOPs) refers to problems that involve four or more objectives, in general [1]. The purpose of optimizing MOPs is to yield trade-off optimal responses in posterior space (objective space) by adjusting decision variables in prior space (variable space) [2]. More than 90% of multi-objective optimization approaches have been based on Pareto-optimality using meta-heuristic techniques [3]. Most meta-heuristic methods have adopted evolutionary algorithms of which population-based properties strengthen a multi-points search toward an optimal solution domain called a Pareto-optimal front (POF), i.e. a set of Pareto-optimal solutions [4–6]. Pareto-optimality is a state of optimal allocation of resources in which no response can be improved without deteriorating other responses [7]. A variety of evolutionary multi-objective optimization (EMO) algorithms have been proposed to represent the POF by achieving two orthogonal goals simultaneously: convergence of solutions as close to the POF as possible and diversity of solutions as uniformly distributed along the POF as possible [8–25].Many Pareto-based EMO algorithms have demonstrated their applicability in solving two or three-objective problems [11,14,16–23]. Given a finite population size, however, it is difficult to capture the entire POF in high-dimensional objective space because the structure of the POF becomes more complicated in proportion to the number of conflicting objectives [24,25]. High computational cost, poor scalability, and hardness in visualizing the multi-dimensional POF are the main difficulties associated with the increasing complexity of the POF. In short, these difficulties are called a curse of dimensionality[26]. POF is a M−1 dimensional hyperplane under regularity condition if M objectives conflict with each other in the optimal solution domain [27]. At least LM−1 data points are required to approximate the POF provided that L data points be necessary to represent the one-dimension of objective space [26]. Assuming that L is 3 and the population consists of 50 solutions, for example, this population size is too small to approximate the four-dimensional POF because at least 81 data points are needed. As a result, a bias of solutions on a certain part of the POF is inevitable when using population-based algorithms for solving MOPs. Increasing the population size might give rise to enormous computational cost. Furthermore, a large number of objectives deteriorate the convergence speed toward the POF owing to the increasing probability that most non-dominated solutions become stagnated in the non-optimal solution domain [9]. As a compromise, the degeneration of objective space has been investigated for balancing the number of objectives and the population size.Objective-reduction approaches compress the objective space by selecting an essential objective set that can preserve a similar dominance relationship in a total objective set on the basis of feature selection [28–39]. This dimensionality-reduction of objective space is based on the premise that redundant objectives exist in the given MOPs [29]. The POF in reduced objective space needs to be identical to the POF in the original objective space [30]. The redundancy among objectives can be measured in terms of the degree of dependences between pairs of objective vectors obtained from the fitness evaluation of solutions. The redundant objectives omitted in the current generation are excluded from the evolutionary process in subsequent generations. The limitation of the objective-reduction scheme is that an improvement of convergence speed toward a reduced POF is insignificant if the number of essential objectives is greater than four [37]. Furthermore, it is difficult to capture the true Pareto-relation among objectives from non-optimal solutions in high-dimensional objective space. To the best of our knowledge, only a few references pointed out that applying the preference-ordering scheme to the degenerated objective space could delineate the POF more reliably [38,39].Preference-ordering approaches focus on finding solutions that satisfy the aspiration levels reflecting the decision maker (DM)’s preference on the objectives [40–89]. Compared with the objective-reduction scheme, the preference-ordering scheme regards every objective as essential. The most distinguishing feature of these approaches combined with an EMO algorithm is to accelerate the convergence toward a specific part of the POF while enduring a loss in diversity of non-dominated solutions. Otherwise, the computational cost increases exponentially in proportion to the dimension of the POF. With reference to Ishibuchi et al.’s review [40], preference-ordering techniques can be classified as follows: modifying Pareto-dominance relation [41–50]; using scalarizing functions [51–60]; using indicator functions [61–67]; assigning different ranks to solutions [68–72]; and allocating reference points [45,46,73–77]. The drawback of the above techniques is that increasing the selection pressure excessively might provide only a few biased Pareto-optimal solutions. The degree of selection pressure is related to how the preference is articulated on the objectives. Hence, the adaptive preference is of importance for solving complex MOPs since it is difficult to find non-dominated solutions that satisfy every preference simultaneously in high-dimensional objective space [78]. Preference information does not affect the evolution of non-dominated solutions, in the case where all solutions either satisfy or fail to achieve the given preference [79–81]. Meanwhile, the importance of the adaptive control has led some researchers to extend reinforcement learning [82–86] to sequential multi-objective decision-making analyses [87–89]. Nevertheless, it is a job to separate the benefits associated with each objective for identifying trade-offs solutions in the domain of multi-objective reinforcement learning.This study investigates the coupling effects of preference-ordering and objective-reduction schemes on the search efficiency of the evolutionary process for resolving the scalability issue in exploring the high-dimensional POF. Dynamic goal programming (DGP) for prioritizing solutions that satisfy the aspiration levels for the essential objectives and successive linear objective-reduction (SLOR) for updating the set of essential objectives are developed individually, and then combined with a multi-objective genetic algorithm (MOGA). Non-dominated Sorting Genetic Algorithm-II (NSGA-II), one of the widely used MOGAs, is adopted in the proposed framework to provide a set of non-dominated solutions with regard to the essential objectives [18]. DGP adjusts both the aspiration levels and the constraints allocated to the essential objectives adaptively, thereby inducing the convergence of the evolved solution set toward the POF. The induced solutions facilitate a degeneration of the POF using SLOR, which updates the set of essential objectives by excluding non-conflicting objectives based on a principal component analysis. For brevity, this framework is named DS-MOGA (MOGA combined with DGP and SLOR) in this paper. The performance of DS-MOGA is demonstrated in redundant benchmark test problems, non-redundant benchmark test problems, and a subsurface modeling problem.This section explains the parameterization of MOP (see Section 2.1) and the Pareto-dominance relation (see Section 2.2) in brief.Eq. (1) generalizes a multi-objective minimization problem:(1)Minimizey=f(x)=f(x1,…,xN)={f1(x),…,fM(x)}Subject tox∈X,f(x)∈Y,where x is a variable vector in prior space X, f is an objective estimator to compute an objective vector f(x) in posterior space Y, xiis the ith decision variable in x, fj(x) is the jth objective value in f(x), N is the number of decision variables, and M is the number of objectives. Computing f(x) is called forward modeling, while estimating x from f(x) is called inverse modeling. Optimization is an iterative process of forward and inverse modeling in order to explore a set of optimal solutions, minimizing f(x) in objective space. In this paper, the term solution refers to the variable vector x and its corresponding objective vector f(x) interchangeably. In reservoir characterization, for example, the solution indicates a reservoir model of which the variable vector x is a set of static rock and fluid properties to be adjusted (porosity, permeability, capillary pressure) and the objective vector f(x) is a set of dynamic responses to be matched (time-series production/injection data measured at wells).Pareto-optimality is a state of an optimal allocation of resources [7]. Mathematically, Pareto-optimality is defined as the best non-domination that is a state of equivalence where no solution can be improved with respect to any objective without worsening at least one other objective [9]. Assuming a minimization problem, a variable vector x1∈X is said to dominate a variable vector x2∈X if and only if Eq. (2) is satisfied:(2)∀i∈{1,…,M}:fi(x1)≤fi(x2)∧∃j∈{1,…,M}:fj(x1)<fj(x2).Dominance of x1 over x2 implies that x1 is superior to and then preferred over x2 for decision making. Simply, f(x1)≺f(x2). If Eq. (2) is not satisfied, x1 is said to conflict with x2. Both x1 and x2 are non-dominated to each other, thus regarded as equivalent solutions.x1 becomes a member of the Pareto-optimal solutions if no other variable vector x dominates x1 in the M-dimensional objective space Y. Apparently, Pareto-optimal solutions are the best non-dominated solutions in posterior space. The shape of the M−1 dimensional POF represents the conflict among M objectives. Therefore, the practical goal of multi-objective optimization is to yield multiple non-dominated solutions that can approximate the POF undiscovered a priori. A strict application of Pareto-dominance relation, however, deteriorates the discovery in solving a complex MOP; because it is regarded as a high-quality solution showing good fitness only for few objectives even though its overall fitness is poor [18]. Controlling the spreadness of the solution set based on the crowding-distance aggravates this divergence phenomenon, in which solutions tend to move toward the edge of the non-dominated front [7,14,18,26].Over the past decade, the efficacies of objective-reduction and preference-ordering schemes have been investigated to compensate for the poor scalability of the Pareto-dominance relation when solving four or more objective problems. This section provides a literature survey that examines the previous objective-reduction approaches (see Section 3.1) and preference-ordering approaches (see Section 3.2).Objective-reduction is a feature-selection process that extracts m conflicting objectives by excluding non-conflicting objectives from the total M objectives [32]. The essential objective set FSrefers to the subset composed of conflicting objectives of the degenerated POF, while the complementary set of FSis called the redundant objective set Fredn: Fredn=FT\FS. |FS|=m=I is preferable, where I is the number of essential objectives that can maintain the identical Pareto-dominance relation of the original problem. Generally, objective-reducing EMO algorithms can explore the POF accurately if I≤3. Even if 4≤I≤M, the objective-reduction scheme enhances the search efficiency toward the POF [38].Brockhoff and Zitzler [29,33] investigated the effects of dimensionality-reduction on the dominance relations by adding or omitting objectives. Their objective set-based approaches eliminated redundant objectives if the dominance relations did not change or changed slightly in the given non-dominated solutions. The efficiency of the objective-reduction scheme was examined in indicator-based evolutionary algorithms (IBEAs) [29]. With application to a nine-objective radar waveform design problem [90], Brockhoff and Zitzler [33] proposed both the exact and the greedy DRP algorithms for solving two problems, i.e. δ-minimum objective subset (δ-MOSS) and minimum objective subset, of size k with minimum error (k-EMOSS). The former problem found a minimum number of objectives given an error δ, while the latter problem searched for a k-sized objective subset with the minimum possible error. Singh et al. [36] proposed a Pareto corner search evolutionary algorithm (PCSEA) with application to a five-objective water resource problem [91] and the radar waveform design problem. PCSEA detected a few representative solutions located at the corners of the POF with a small population size. This quick searching tool was less sensitive to the number of objectives but difficult to approximate the entire POF.Jaimes et al. [32] divided the total objective set into neighborhoods of size q around each objective. The division was based on the correlation matrix obtained from the set of non-dominated solutions. After selecting the most compact neighborhood, the objective closest to the centroid of this neighborhood was regarded as the most conflicting objective on the basis that the degree of the conflict between two objectives was proportional to the distance between them. This process was iterated until a pre-defined criterion was satisfied. Guo et al. [37] combined NSGA-II with OC-ORA, an objective-clustering method using a partitioning around medoids algorithm. The performance of OC-ORA was tested on DTLZ2(M) with no redundancy, DTLZ5(I,M) with redundancy among objectives, and the storm drainage problem. Notably, the performance of the objective-grouping approach depends on the number of groups.Principal component analysis (PCA) based approaches regard an objective that is positively correlated with other objectives as redundant by evaluating the dependences between pairs of objective vectors [26,30,31,35,38,39]. Linear PCA (L-PCA) extracts the essential objective set by eliminating non-conflicting objectives along the significant eigenvectors of the correlation matrix calculated from the objective pairs [26,30,38], while the nonlinear maximum variance unfolding PCA (NL-MVU-PCA) extracts the essential objective set along the significant eigenvectors of the kernel matrix. A negative coefficient value indicates conflict, while a positive value indicates a dependence between two objectives. The objective accounting for the largest variance among the dependent objectives survives in the essential objective set, whereas the other objectives are discarded. L-PCA and NL-MVU-PCA are useful for removing the second-order and higher-order dependences among objective pairs, respectively. By operating on NSGA-II and ɛ-MOEA, Saxena et al. [38,39] showed that the machine-learning based methods were superior to the DRP based methods for removing the redundancy among objectives.Previous machine-learning based objective-reduction methods are iterated until no more objective becomes redundant. This termination condition assumes that the true Pareto-relation among objectives can be detected in earlier generations of the evolutionary process. Sinha et al. [39] improved the convergence toward the most preferred Pareto-optimal solutions, chosen by a value function [34] that reflected the preference on the essential objective set identified by NL-MVU-PCA. The efficacy of this two-step framework was demonstrated on a modified 3-objective ZDT1 test problem [92], a storm drainage problem, and an 11-objective car side-impact problem [93]. The results showed that the exclusion of redundant objectives during the articulation of the preference could lower the computational cost by increasing the search efficiency toward the POF. Nonetheless, the detection of the true essential objective set is required before articulating the preference.The modified dominance relation expands the dominated region to reject low-quality solutions that might cause severe deterioration for most objectives. Wierzbicki [41] proposed ρ-properly Pareto-dominance that enlarged the area of the dominated region by adjusting the angle ρ/(1+ρ). This modified Pareto-optimality was equivalent to the normal Pareto-optimality if ρ=0. Meanwhile, Ikeda et al. [42] and Sato et al. [43] strengthened the selection pressure toward the POF by allowing a wider angle of the dominated region. The following EMO algorithms have also adopted modified dominance relations: R-NSGA-II [44], g-dominance [45], preference IBEA [46], r-dominance [47], iMOEA/D [48], and PIE [49,50].Minimizing a weighted objective-sum is the most traditional strategy for solving MOPs, but the results obtained are sensitive to the weight vector inevitably [51,52,54–57,59]. Kim et al. [60] aggregated the objective functions using fuzzy measure and fuzzy integral to represent the interactions among objectives. The performance of their approach was demonstrated on a fuzzy path planner for the shooting behavior of a soccer robot [94] by comparison with that of NSGA-II, multi-objective population-based incremental learning (MOPBIL) [95], and multi-objective quantum-inspired evolutionary algorithm (MQEA) [96,97]. Meanwhile, some researchers estimated the overall rank of each solution based on its multiple ranks calculated from scalarizing functions [53,58].Indicator-based evolutionary algorithms (IBEAs) have prioritized solutions based on performance indicator values. Zitzler and Künzli [61] proposed a general framework of IBEAs using a binary performance indicator. Emmerich et al. [62], Beume et al. [63], and Zitzler et al. [66] proposed hypervolume-measure based IBEAs. Ishibuchi et al. [64] proposed an iterative IBEA of which the main feature was to add only one representative solution to the descended solution set in every generation. Wagner et al. [65] compared the performance of Pareto-, aggregation-, and indicator-based methods. Sanchis et al. [67] proposed an aggregate cost index deriving a single Pareto-optimal solution similar to the pre-defined point by the DM.Drechsler et al. [68] and Sülflow et al. [69] preferred solutions that satisfied a favor relation in order to differentiate between non-dominated solutions by counting the number of objectives for which one solution outperformed other solutions as well as by measuring the difference in objective values between two arbitrary solutions. Corne and Knowles [70], Kukkonen and Lampinen [71], and Köppen and Yoshida [72] explored solutions having higher overall ranks.Reference-point-based EMO algorithms aim to provide the solutions that dominate user-defined reference objective vectors. The region of interests (ROI) are highlighted by the reference points that correspond to the most interesting solutions reflecting the DM's preference for the objectives. Apparently, the solutions obtained are expected to be Pareto-optimal solutions within the ROI. The reference points guide the Pareto search while controlling the diversity of the focused POF. Branke and Deb [73] suggested a guided-dominance concept with NSGA-II. Deb and Sundar [44] replaced the crowding-distance sorting with the normalized Euclidean distance to a reference point in NSGA-II. Molina et al. [45] preferred the solutions fulfilling all the aspiration levels defined by the reference points. Thiele et al. [46] proposed a reference-point based IBEA [61] using an achievement scalarizing function (ASF). The quality of each solution was quantified in terms of the modified indicator that was the ratio of the original indicator value to be maximized to the ASF value to be minimized. The ROI became narrower gradually with an improvement of solutions.Recently, López-Jaimes and Coello [74] divided the objective space into two subspaces for comparing solutions obtained from the Pareto-dominance relation and those obtained from the ASF. Their ASF was successfully applied to an airfoil shape design problem [98]. Moreover, they summarized the key features of preference-reflecting EMO algorithms as follows: the way to articulate the DM's preference, the type of Pareto-optimality (normal, weak, ρ-proper), the applicability to a MOP composed of a non-convex POF, and the feasibility of reference points. Ruiz et al. [75] proposed a weighting ASF genetic algorithm (WASF-GA) approximating the nadir vectors for the normalization carried out in the ASF in the case of a large scale-difference among objective functions. Deb and Jain [76] and Jain and Deb [77] proposed NSGA-III, which updated and included new reference points in the framework of NSGA-II. Compared with the two versions of the MOEA/D algorithm [23], this adaptive NSGA-III produced satisfactory results with application to unconstrained and constrained MOPs having 3–15 objectives.It is promising that degenerating the objective space can contribute to gaining higher convergence and diversity of the evolved solution set if the preference is progressively articulated to the reduced problem. For this reason, the machine learning based L-PCA objective-reduction method [38] was adopted with a modification of the iteration and the initialization conditions in the framework of DS-MOGA. The aspiration levels were adjusted using adaptive linear programming based on the statistical parameters of essential objective values. Thus, the aspiration levels have relevance to the location of the reference points. Recently, Min et al. [7] showed that the developed framework could be applied successfully to multi-objective history matching in an oil field.DS-MOGA was designed to generate a synergy from the coupling of preference-ordering and objective-reduction schemes in the evolutionary process. This section explains the operating procedure of DS-MOGA (see Section 4.1) and its main modules for fitness evaluation: non-dominated sorting (see Section 4.2), DGP (see Section 4.3), crowding-distance sorting (see Section 4.4), and SLOR (see Section 4.5).Fig. 1presents a flow chart of the proposed framework. The population comprised of Npopsolutions evolves during Ngengenerations. In each generation, Npopoffspring variable vectors are newly created by recombining Npopvariable vectors in the parent population by using two genetic operators, i.e. crossover and mutation [99]. Forward modeling computes the objective vector f(x) corresponding to its variable vector x for each solution in the offspring population. Subsequently, 2Npopobjective vectors in the mating pool are ranked with respect to the relative superiority of their objective values through the consecutive fitness evaluation procedures: dominance-based ranking allocation (see Section 4.2), goal adjustment and goal-based ranking reallocation (see Section 4.3), and crowding-distance sorting (see Section 4.4). After the fitness evaluation, Npopsuperior solutions survive and then compose the parent population of the next generation, while the other Npopinferior solutions are discarded. This stochastic optimization process is repeated until the termination condition is satisfied as follows: either the number of iterations reaches Ngenor an improvement of performance metrics for the solution set in the current generation is insignificant compared to that in the previous generation. If not terminated, SLOR extracts the essential objective set FSby excluding the redundant objective set Frednfrom the total objective set FT(see Section 4.5). FSin the current generation becomes FTin the next generation.Fig. 2illustrates the ranking classification in DS-MOGA. The first classifier, i.e. non-dominated sorting, allocates NRkranks to 2Npopsolutions in accordance with the objective values. NRkis equivalent to the number of non-dominated fronts Nndfso far. The second classifier, i.e. goal programming, grants the priorities to the solutions that achieve the aspiration levels on the objectives. As a result, NRkincreases from NndftoNRkfull+2NRkpart+NRknone,whereNRkfullis the number of non-dominated fronts on which solutions satisfy all the aspiration levels,NRkpartis the number of non-dominated fronts on which solutions only satisfy a few partial aspiration levels, andNRknoneis the number of non-dominated fronts where no aspiration level is satisfied. If all solutions satisfy the goal,NRk=NRkfull=NndfandNRkpart=NRknone=0.The third classifier, i.e. crowding-distance sorting, cuts off some densely distributed solutions in the kth rank Rkkin accordance with their proximity in order to retain Npopsolutions for the next parent population if the cumulative number of solutions from the first rank Rk1 to Rkkis greater than Npop.Assuming that Npopis 8, 16 solutions are projected in two-dimensional objective space: 8 solutions from the parent population and 8 solutions from the offspring population (see Fig. 3(a)). Fig. 3(b) illustrates the non-dominated sorting based on Pareto-optimality in objective space. The objective f1 conflicts with the other objective f2 on the POF even though pairs of objective vectors may correlate positively in the non-optimal solution domain. In Fig. 3(b), NRkis 3. The filled circles, squares, and triangles represent the best non-dominated solutions given the first rank Rk1, the non-dominated solutions given the second rank Rk2, and the non-dominated solutions given the third rank Rk3, respectively. f(x1) dominates f(x2): f(x1)≺f(x2). f(x3) also dominates both f(x4) and f(x5): f(x3)≺f(x4)≺f(x5). f(x1) and f(x3) on the same non-dominated front are equivalent because the superiority in f1 accompanies the inferiority in f2 and vice versa: f1(x1)≺f1(x3) and f2(x1)≻f2(x3). In brief, a solution given the ith rank Rkiis inferior to any solutions in the lower ranks from Rk1 to Rki−1 and superior to all solutions in the upper ranks from Rki+1 to RkNRk. The solutions in the lower rank are given a higher likelihood to survive as the members of the next parent population.DGP developed in this study is a combination of adaptive goal programming and constraint handling. This proposed linear programming adjusts both goal and constraint on the objectives progressively for prioritizing qualified solutions during the evolutionary process. Goal programming gives preference to the solutions achieving the DM's aspiration levels [100–104]. Similarly, constraint handling discards (hard) or penalizes (soft) any solution of which the objective vector violates the given constraint [105–107]. Graphically, DGP grants priorities to the solutions within the region of interest (ROI), which is the hypervolume of the goal.As demonstrated in Fig. 1, DGP consists of three modules: goal adjustment (see Section 4.3.1), goal programming (see Section 4.3.2), and constraint handling (see Section 4.3.3). Goal programming allocates lower ranks to the solutions that satisfy all the current aspiration levels on the essential objectives. Constraint handling prevents the solutions from being stuck to local minimums. Both the aspiration level and the constraint allocated to each objective are updated according to the quality of the evolved solution set in each generation. These adaptive preference-ordering processes are repeated until every objective reaches the target objective values, called the ultimate aspiration levels. If all target levels are satisfied, both goal programming and goal adjustment are terminated. Then, DGP is converted into classical constraint handling.The term goal refers to the set of aspiration levels that are the expected values to be achieved for the corresponding objectives [102]:(3)gt={g1,t,…,gM,t}∀t=1:Ngen,where gtis the goal and gi,tis the aspiration level for the ith objective fiin the tth generation. The aspiration level is the payoff that a solution aspires to for the given objective. Any solution achieving this level is regarded as a satisfactory solution. In this study, goal adjustment aims to induce the solutions toward the ultimate goal, which is the set of the ideal objective values desired to be achieved by the DM. The ultimate goal is set up based on a priori information on each objective, while the initial goal g1 is composed of the maximum value of each objective in the initial generation. In each generation, the aspiration levels remain unchanged or decrease in accordance with the objective values until the goal becomes identical to the ultimate goal.The ith aspiration level in the next generation gi,t+1 decreases if the goal-updating condition Eq. (4) is satisfied as the proportion of the solutions satisfying fi,t≤gi,tbecomes greater than or equal to the goal adjustment threshold θgad:(4)gi,t−Fi,t−1(θgad)≥0,where Fi,tis the cumulative probability function for fiin the tth generation and θgadis the p-quantile of Fi,tin the ascending order of 2Npopobjective values imported from the mating pool in the tth generation. θgadplays the role of an indicator that allocates an adaptive reference point to each objective in every generation. If θgadis 1.0-quantile, the operation of the EMO algorithm combined with DGP is the same as the typical EMO algorithm since the aspiration level is equivalent to the maximum value for each objective. In this study, as a rule of thumb, θgadis 0.75-quantile.Eq. (5) defines the goal-updating equation:(5)gi,t+1=gi,tξγi,t<fi,tmax∀i=1:M,wherefi,tmaxis the maximum value of fiamong 2Npopsolutions in the mating pool in the tth generation, ξ is the decline rate of the aspiration level, of which the range is between zero and one, and γi,tis a dynamic decline exponent. γi,tis a non-negative integer value that increases from zero until gi,t+1 becomes smaller thanfi,tmax.If Eq. (4) is not satisfied, γi,tis zero: gi,t+1=gi,t. The role of γi,tis to increase the speed of the goal-updating by keeping the next aspiration level smaller than the current maximum objective value. Practically, goal programming is meaningless if all solutions satisfy the given aspiration levels. Each objective may have a different γi,tvalue in accordance with the maximum objective value in each generation. That is, γi,tis neither a fixed nor an inherited value. Meanwhile, ξ is a constant of 0.9 in this study. Table 1depicts an example of updating three aspiration levels. Assuming that both f2 and f3 satisfy Eq. (4), while f1 does not satisfy Eq. (4) in the tth generation, g1,t+1=g1,t=1 as γ1,t=0, irrespective off1,tmax;g2,t+1=90<f2,tmax=120as γ2,t=1; andg3,t+1=810<f3,tmax=850as γ3,t=2. In the same manner, g1,t+2=0.81 as γ1,t+1=2; g2,t+2=0.90 as γ2,t+1=0; and g3,t+2=729 as γ3,t+1=1.Goal programming converts the dominance-based ranking into the goal-based ranking. Fig. 3(c) is a modification of Fig. 3(b), with ROI highlighted in yellow, and demonstrates the ranking reallocation with the aspiration levels expressed as a dashed line. The solution f(x4) achieving the current goal gtis regarded as superior to the solution f(x1) although f(x1) lies on the lower non-dominated front than f(x4). Goal programming subdivides the quality of solutions by increasing the number of ranks NRkfrom NndftoNRkfull+2NRkpart+NRknone.Solutions inside the ROI are given reallocated ranks from Rk1 toRkNRkfull+NRkpartbased on the Pareto-dominance relation. In the same manner, solutions outside the ROI are re-ranked fromRkNRkfull+NRkpart+1toRkNRkfull+2NRkpart+NRknone.Compared to the dominance-based rank in Fig. 3(b), the number of goal-based ranks in Fig. 3(c) increases from three to four as follows:NRkfull=1,NRkpart=1,andNRknone=1.Eq. (6) defines a constraint inequality for the ith objective fiin the tth generation:(6)gi,t−1−fi,t(x)≥03∀i=1:M,where gi,t−1 is the aspiration level in the previous generation. Any solution violating the constraint condition is penalized for being a member of the next parent population. This constraint condition compels the current population to maintain smaller objective values than the previous population in the case where the objective values exceed the updated aspiration levels, thereby relieving the divergence problem.Fig. 3(d) illustrates the constraint handling with the previous goal expressed as a dot-and-dash line. Both f(x3) and f(x4) achieve the current goal. f(x5) fails to satisfy the goal but obeys the constraint condition. However, both f(x1) and f(x2) violate the constraint condition. Consequently, f(x3), f(x4), and f(x5) are regarded as qualified, while f(x1) and f(x2) are regarded as disqualified solutions for composing the next parent population. After the goal adjustment, the present goal plays the role of constraint in the next generation, as shown in Fig. 3(e).Fig. 3(f) illustrates the crowding-distance, which is a measure of density among solutions, given the identical rank as defined in Eq. (7)[18]:(7)ψj=∑i=1Mdijdimax−dimin∀j=1:NRkk,where ψjis the crowding-distance value for the jth solution, M is the number of objectives,dijis the displacement between two neighboring solutions to the jth solution in the direction of the ith objective function fi,dimaxis the maximum displacement between two arbitrary solutions in the direction of fi,diminis the minimum displacement between two arbitrary solutions in the direction of fi, andNRkkis the number of solutions given the kth rank Rkk. After the goal-based ranking reallocation, solutions are gathered from Rk1 to Rkkuntil the cumulative number of solutions is greater than or equal to Npop. Supernumerary solutions in the Rkkrank are eliminated to maintain the fixed population size of Npop. A solution having a larger crowding-distance value acquires a greater chance of being a member of the parent population in the next generation than a solution having a smaller crowding-distance value. That is, sparsely distributed solutions with larger crowding-distance values are preferred over densely distributed solutions with smaller crowding-distance values for preserving the diversity of solutions if their ranks are identical.SLOR identifies the essential objective set FSfrom the total objective set FTvia the important objective set FEbased on the degree of dependences among Npopsurvived solutions. The right-hand side of the flow chart in Fig. 1 depicts the procedure of SLOR. Saxena et al. [38] demonstrated the novelty of the machine-learning based dimensionality-reduction in detail. For brevity, in Section 4.5, M is regarded as the number of total objectives in a certain generation.First, the correlation matrix C of the non-dominated solution set X is constructed using the normalized objective set fnorm:(8)C=1MXXT,(9)X=[f1norm,…,finorm…,fMnorm]T,(10)finorm=[fi1norm,…,fijnorm,…,fiNpopnorm],(11)fijnorm=fij−μfiσfi∀i=1:M∀j=1:Npop,whereμfiandσfiare the mean and standard deviation of fivalues, respectively. Cijis the correlation coefficient between two normalized objective vectorsfinormandfjnorm.Eigenvalues λ and their corresponding eigenvectors V are obtained from C through principal component analysis as follows:(12)Vj=[Vj1,…,Vji…,VjM]T∀j=1:M,where Vjiis the ith component of Vj, which is the projection of Vjin the direction of fi. If i<j,Vihaving a larger λiis considered to be more influential on the overall performance than Vjhaving a smaller λj: V1,…,VMwith λ1≥…≥λM. From the orthonormality of eigenvectors,(13)|Vj|=∑i=1MVji2∀j=1:M.The contribution of fifor all Vjs is given as shown in Eq. (14):(14)ciM=∑j=1MejVji2∀i=1:M,(15)subject to∑i=1MciM=1.0.Eigenvalue analysis determines the important objective set FEfrom FTby eliminating insignificant objectives that barely affect the overall performance. The degree of influence of each eigenvector is normalized as the ratio of eigenvalue e and the cumulative ratio E using Eqs. (16) and (17), respectively:(16)ei=λi∑j=1Mλj∀i=1:M,(17)Ej=∑i=1jei∀j=1:M.Eq. (18) is used for selecting the number of significant eigenvectors:(18)∑i=1NVei≥θvar,where NVis the minimum number of significant eigenvectors to account for θvar variance for handling MOPs with moderate or negligible redundancy: θvar∈[0, 1]. In the previous studies, θvar of 0.997 was selected based on analogy with Gaussian distributions accounting for ±3σ[35,38,39].According to the sign of Vji, each Vjis divided into two subsets, i.e.FVj+andFVj−:(19)FVj+={fi|Vji≥0},FVj−={fi|Vji<0}∀i=1:M∀j=1:NV.LetFE,Vjdenote a subset of FErelated to Vjandfskperforms the objective corresponding to the Vji, of which the magnitude of the contribution is the kth largest in Vj.FE,Vjis obtained according to the following condition:FE,Vj=fs1∪FVj−iffs1is an element ofFVj+;FE,Vj=fs1∪FVj−iffs1is an element ofFVj+;FE,Vj={fs1,fs2}if eitherFVj+=ϕorFVj−=ϕ.FEis the union of allFE,Vj∀j=1:NV.This step identifies one or more objective-clusters Fsub, a subset of FE, comprised of positively correlated objectives satisfying both filtering conditions, as shown in Eqs. (20) and (21):(20)sign(Cik)=sign(Cjk)∀i,j=1:NV∀k=1:M,(21)Cij≥Tcor,where Tcoris a dynamic correlation threshold to cut off poorly correlated objectives in FE:(22)Tcor=1.0−e11.0−Mθvar*/M,whereMθvar*is the minimum number of eigenvalues to account forθvar*variance:θvar*∈[0,1].e1 is calculated using Eq. (16).θvar*is simply chosen as 0.954, analogous with Gaussian distributions accounting for ±2σ with reference to the previous studies [35,38,39].Selection score analysis extracts FSfrom FEto reduce a search range further in posterior space. One representative objective fsubis chosen from each Fsubbased on the selection score sc, as shown in Eq. (23):(23)sci=∑j=1NVej|Vji|∀i=1:M,where sciis the contribution of fifor NVeigenvectors modified from Eq. (14). The objective with the highest sciis selected as the representative objective fsubin each Fsub. Finally, the representative objectives and the complementary set of the union of all Fsubs create FS:FS=Fsub,1c∩⋯∩Fsub,NFsubc∪fsub,1,…,fsub,NFsub,whereNFsubis the number of Fsubs. FSin the tth generation becomes FTin the t+1th generation: FT,t+1=FS,t. In brief, Fig. 4shows a hierarchy of the objective sets.Let VR denote the volume ratio of the ROI to the objective space. VR is a function of the number of objectives. Extracting m objectives enhances the convergence speed by enlarging the ROI because VR increases from pMto pmif the goal adjustment threshold θgadis p-quantile. For example, extracting 5 essential objectives from 10 objectives increases VR from 6% to 24% if θgadis 0.75-quantile.Eq. (24) measures the error incurred by the objective-reduction in the tth generation [38]. This equation computes the variance that is left unaccounted when the redundant objective set Fredn=FT\FSis discarded:(24)εt=∑i∈FrednciM1.0−maxj∈FS(δijCij),where δijis 1 if fiand fjare identically correlated. Otherwise, δijis 0. The cumulative error by the tth generation is obtained using Eq. (25):(25)εcum,t=ε0+∑i=1tεi(1−εi−1).SLOR is identical to the L-PCA objective-reduction except for the iteration and the initialization conditions of the objective sets. L-PCA extracts the essential objectives until no objective is regarded as redundant. In other words, the dimensionality-reduction process is terminated if FS,tis the same as FT,tin any tth generation. On the other hand, SLOR repeats the process in every generation to reflect the possibility of change in the relationship among objectives because conflicting objectives in the POF might show a positive correlation in the non-optimal domain and vice versa. In addition to the iteration condition, all objective sets are initialized in the next generation if the number of essential objectives is less than four in the current generation, where conventional EMO algorithms have shown good performance. This initialization condition aims to relieve side effects from an excessive compression of posterior space compared to the dimension of the true POF, which would enforce solutions to be biased even though they are Pareto-optimal. It is clear that both SLOR and L-PCA would show similar performance unless the modified condition is activated.Table 2summarizes the computational complexity of DS-MOGA. Given that M<<Npopis in a MOP, the overall time complexity of DS-MOGA is approximate toO(MNpop2)governed by the non-dominated sorting. Therefore, the runtime of DS-MOGA is slightly longer than those of NSGA-II and NSGA-II combined with a machine-learning based objective-reduction approach. As pointed out in [38,39], both DRP based algorithms require larger computational efforts.This section demonstrates the performance of DS-MOGA on redundant and non-redundant benchmark test problems selected from the suite of DTLZ [108]. The redundant problem DTLZ5(I,M) and the non-redundant problem DTLZ2(M) are described in Section 5.1. Section 5.2 analyzes the coupling effects of DGP and SLOR through an application to DTLZ5(5,10). Section 5.3 summarizes the simulation results of 20 DTLZ5(I,M) and 5 DTLZ2(M) problems with 4–20 objectives.We verified whether DS-MOGA could extract m objectives up to m=I for any given M objectives in redundant problems. I is the number of essential objectives and M is the number of total objectives, as denoted in Section 3.2. All objectives are to be minimized. Table 3describes variable vector x, objective vector f(x), and constraint c of DTLZ5(I,M). The number of decision variables N depends on both M and the difficulty parameter kdiff: N=M−1+kdiff. The POF of DTLZ5(I,M) can be characterized by g(x)=0.0 with xi=0.5 ∀i=M:M−1+kdiff(see Eq. (33)). On the POF, all objectives in {f1, …, fM−I+1} are positively correlated, while each objective conflicts with any other objective in {fk, …, fM} ∀k=1:M−I+1. Squaring and adding the equations in Table 3 derives the shape of the POF analytically as shown in Eqs. (36) and (37):(36)f1=f2=12f3=122f4=⋯=12M−I−1fM−I+1,(37)2fM−I+12+∑i=M−I+2Mfi2=1.0.The M−I+1 objectives from f1 to fM−I+1 are linearly dependent, while the other objectives are subject to Eq. (37) on the POF. fM−I+1 has the largest variance among any fk∀k=1:M−I+1. On the POF of DTLZ5(5,10), for example, f1–f2–f3–f4–f5–f6 are positively correlated. Thus, FS={fk, f7, f8, f9, f10} ∀k=1:6.We also examined whether the objective-reduction scheme could preserve all M objectives in non-redundant problems. Table 4describes the parameters of DTLZ2(M). The POF of DTLZ2(M) is shown in Eq. (45):(45)2f12+∑i=2Mfi2=1.0.Table 5presents the control variables used for running evolutionary algorithms in the experiments. For NSGA-II, generation number Ngenand population size Npopwere 200 and 100, respectively. This population size has been widely employed for comparative studies to test the performances of EMO algorithms [109]. The probabilities of crossover pcand mutation pmwere 0.9 and 0.1, respectively. The ultimate aspiration level of each objective was set up as the maximum objective value with g(x) of 0.0 for removing noise effects incurred by kdiff. A constant kdiffof 10 was used in the problem formulation. For DGP, the goal adjustment threshold θgadand the decline rate of aspiration level ξ were 0.75 and 0.90, respectively. For SLOR, θvar of 0.997 andθvar*of 0.954 were chosen based on analogy with Gaussian distributions accounting for ±3σ and ±2σ in reference to the previous researches [35,38,39]. For each problem, experiments were performed 25 times in order to calculate the averaged performance metric values from the different sets of non-dominated solutions.The performance of the solution set was measured in terms of three indicators: the averaged Hausdorff distance Δp[109] and g(x) as convergence indicators while the normalized maximum spread indicator Isas a diversity indicator [110]. 0.0 is desirable for each convergence indicator and the ideal value of Isis 1.0. Δpindicates the deviation between the set of non-dominated solutions Z and the set of Pareto-optimal solutions Z* by measuring the averaged distance between the two sets,Z=f(x1),…,f(xNpop)andZ*=f(x1*),…,f(xNPOF*),where NPOFis the number of representative Pareto-optimal solutions. Z* was obtained by discretizing the POF in the user-defined hypergrid. Eq. (46) defines the p-norm distance Δp:(46)Δp(Z*,Z)=maxGDp(Z*,Z),IGDp(Z*,Z),(47)GDp(Z*,Z)=1NPOF∑i=1NPOFdist(f(xi*),Z)p1/p,(48)IGDp(Z*,Z)=1Npop∑i=1Npopdist(f(xi),Z*)p1/p,where GDp[109] is the power-mean of the generational distance GD[111], IGDp[109] is the power-mean of the inverted generational distance IGD[112], and dist is the shortest distance between the associated ith objective vector in one solution set and the other solution set. This study used 2-norm Δp. Similarly, g(x) quantifies a degree of noise effects resulting from kdiffas defined in Eq. (33). g(x) is 0.0 if all solutions converge on the POF, while it increases exponentially up to its maximum of 0.25 kdiffif the solutions grow away from the POF.The diversity indicator Isdecreases from 1.0 to 0.0 as the solutions become more concentrated in a sub-region of the POF. This phenomenon implies a bias of Pareto-optimal solutions. On the other hand, Isincreases from 1.0 to infinity as the solutions spread more widely in the non-optimal solution domain. The definition of Isis written in Eq. (49):(49)Is=DA/DT,(50)DA=∑i=1MmaxzA∈ZAzAi−minzA∈ZAzAi21/2,(51)DT=∑i=1MmaxzT∈ZTzTi−minzT∈ZTzTi21/2,where DAis the actual dispersal of solutions in the obtained non-dominated set, DTis the dispersal of solutions on the true POF, zAis an element of a median level of distance of the solution set ZA,zAiis a median level of distance of the solution set for fi, zTis an element of a continuous globally optimal reference set ZT, andzTiis a continuous globally optimal reference set for fi.This section investigates the coupling effects of objective-reduction and preference-ordering schemes with application to a redundant DTLZ5(5,10) problem. For brevity, S-MOGA and D-MOGA refer to NSGA-II coupled with SLOR and NSGA-II coupled with DGP, respectively. We compare the performance of DS-MOGA with those of NSGA-II, L-PCA-NSGA-II [38], S-MOGA, and D-MOGA.Fig. 5presents the scatter-plots that provide the cross-sectional views for the evolved solution set obtained by running NSGA-II in two-dimensional objective space. Each objective is marked with distinct symbol and color. The gray rectangle indicates the ROI comprised of the ultimate aspiration levels. The red solid curve represents the outer boundary of the POF. The linearity in {f1,…,f6} as well as the conflict in {fk, …, f10} ∀k=1:6 were barely observed. As a result, NSGA-II addressed poor performance indicator values for this 10-objective problem: Δp=2.347, g(x)=2.153, and Is=4.475.It is critical for diversity-preserving EMO algorithms to maintain the proportion of the first-rank solutions to be less than 50% in the mating pool. Otherwise, solutions would become outliers as the crowding-distance sorting adversely prevails over the non-dominated sorting. Fig. 6shows the proportions of the non-dominated solutions from the second to the tenth generation. The number of the first-rank solutions in each generation is inserted in this bar chart. The solutions were grouped into two to four non-dominated fronts. Obviously, crowding-distance sorting amplified the diversity of the evolved solution set by allowing sparsely distributed solutions to survive in the mating pool since the number of the first-rank solutions was greater than Npopof 100 in every generation.Figs. 7 and 8provide the scatter-plots for the distribution of the evolved solution set obtained by running L-PCA-NSGA-II and S-MOGA, respectively. Obviously, the quality of the final solution sets obtained was insufficient to delineate the entire POF owing to the limitation of the Pareto-dominance relation in five-dimensional objective space, although some solutions converged on the POF with a non-uniform distribution. Compared to the results of NSGA-II, the removal of redundant objectives improved the overall performance of the evolutionary process. L-PCA-NSGA-II yielded Δp=2.012, g(x)=1.763, and Is=3.480, and S-MOGA obtained slightly improved values: Δp=1.757, g(x)=1.571, and Is=3.030. The poorer performance of L-PCA-NSGA-II resulted from the essential objective set including one redundant objective since the third generation: FS={f2, f6, f7, f8, f9, f10}. Similarly, S-MOGA selected FS={f1, f6, f7, f8, f9, f10} by the 134th generation. In the 135th generation, the elimination of f6 was devoted to improving the performance by detecting more distinct positive correlations in {f1,…,f6}. When comparing Figs. 7(b) and 8(b), S-MOGA shows clearer linearity in {f1,…,f6} than L-PCA-NSGA-II. The averaged correlation coefficients between any two objectives in {f1,…,f6} are 0.46 from S-MOGA and 0.11 from L-PCA-NSGA-II.Fig. 9shows that DS-MOGA generated well-converged solutions with a uniform distribution along the POF through the progressive articulation of the preference on the reduced problem. The performance indicator values obtained are as follows: Δp=0.252, g(x)=0.072, and Is=1.047. The diversity indicator is similar to the ideal value of 1.0, resulting from the clear linearity in {f1,…,f6}, as shown in Fig. 9(b). The averaged correlation coefficient between positively correlated objective pairs is 1.00. Fig. 10presents a reclassification of the non-dominated solutions from the second to the tenth generation. Notably, all M objectives were regarded as essential before performing the dimensionality-reduction in the second generation. When compared to Fig. 6, integrating the aspiration levels on the essential objectives maintained the proportion of the first-rank solutions below 50% by the fifth generation. For example, the proportion of the first-rank solutions decreased from 65.5% to 26.5% by subdividing the non-dominated fronts into the favored and unfavored zones in the second generation: the numbers of ranks were four for NSGA-II and six for DS-MOGA. In the sixth generation, DGP was converted into constraint handling as the evolved solutions set achieved the ultimate aspiration levels. Most solutions existed around the POF although the proportion exceeded 50% in the subsequent generations. This subdivision of the solution set enabled the dominance of the convergence scheme over the diversity-preservation scheme by pruning extremely non-optimal solutions with consideration for the DM's favor reflected in objective space.Table 6summarizes the procedure of dimensionality-reduction coupled with DGP in the second generation. Notably, not only the selection process but also the objective-reduction process is initiated in the second generation creating the first mating pool. The operating principles of the procedure are the same in the subsequent generations. Table 6(a) presents the correlation matrix C for FT={f1,…,f10}. All pairs of two objectives in {f1,…,f6} show strong positive correlation coefficients greater than 0.7 in the initial condition. Eigenvalues λ and corresponding eigenvectors V were calculated from C using principal component analysis (see Table 6(b)). Table 6(c) depicts the results of the eigenvalue analysis. NV=9 as E9>θvar=0.997 and FEwas identical to FT: FE={f1,…,f10}. In Table 6(d), the dynamic correlation threshold was calculated to determine one or more Fsub: Tcor=0.786. As a result, one Fsubwas created: Fsub={f1, f2, f3, f4, f5}. Table 6(e) shows that f1 was identified as the objective given the highest selection score sc in Fsub. As a result, {f1, f6, f7, f8, f9, f10} was selected as FSin the second generation and became FTin the third generation: FS,2=FT,3.According to Table 3, f6 is the most significant objective having the largest variance value in {f1,…,f6}, while both f1 and f2 are the most insignificant objectives having the smallest variance values in {f1, f2, f3, f4, f5, f6}. In this example, f1 survived, while f2–f3–f4–f5 were eliminated after the objective-reduction in the second generation. Notably, an adaption of L-PCA might derive an inflated FSsince no objective became redundant in the third generation: FT,3=FS,3={f1, f6, f7, f8, f9, f10}. Section 5.2.2 already demonstrated the vulnerability of the normal Pareto-dominance relation for the problem containing redundant objectives. With assistance from DGP, SLOR fixed this misleading relationship in the eighth generation: f1 was eliminated from Fsubconsisting of f1 and f6. Hence, the objective set arrived at the true essential set: FS,8={f6, f7, f8, f9, f10}. This result implies that capturing the Pareto-relation among objectives interrelates with the quality of non-dominated solutions, considering the achievement of the ultimate aspiration levels in the sixth generation. The relation among objectives on the POF might differ from the relation in the non-optimal solution domain according to the state of the solution set. Table 6(f) shows that the repetition of the objective-reduction no longer excluded objectives in the subsequent generations. The cumulative error ɛcum,twas less than 5% in the final 200th generation. As seen in Table 6(g), f1–f2–f3–f4–f5–f6 shows the correlation coefficients of 1.0 in the last generation.As shown in Fig. 11, D-MOGA addressed the comparable performance with DS-MOGA by acquiring the suitable indicator values: Δp=0.272, g(x)=0.127, and Is=1.048. 0.98 is the average of the correlation coefficient between any two objectives in {f1,…,f6} although Fig. 9(b) obtained from DS-MOGA shows clearer linearity than Fig. 11(b). These outstanding results obtained from D-MOGA encouraged us to determine whether the appropriate allocation of the reference points could describe the entire POF without eliminating any redundant objectives in high-dimensional objective space. We performed two sensitivity analyses for investigating the effects of the magnitude of the ultimate aspiration levels as well as the effects of goal adjustment on the consistency of both preference-coupling EMO algorithms.The first analysis examined the effects of goal adjustment by comparing the performances of D-MOGA and NSGA-II coupled with a conventional goal programming (GP) that fixed the aspiration levels. For brevity, the latter algorithm is called GP-NSGA-II. Both goal-based EMO algorithms were applied to three DTLZ5(4,5) problems comprised of different numbers of decision variables by varying kdiff: kdiff=10, 20, and 30. The experiments were carried out under the same condition described in Section 5.1.3. It is obvious that a higher level of difficulty lowers the quality of the evolved solution set. Nevertheless, as shown in Fig. 12, D-MOGA approximated the POF accurately by adjusting the aspiration levels adaptively in each generation for the given problems: g(x)=0.015 and Is=0.997 when kdiff=10;g(x)=0.027 and Is=0.994 when kdiff=20; and g(x)=0.064 and Is=0.995 when kdiff=30 in the final 200th generation. All Isreached close to the ideal value of 1.0.When kdiff=10, GP-NSGA-II yielded comparable performance with D-MOGA because most non-dominated solutions achieved the ultimate aspiration levels in earlier generations: g(x)=0.016 and Is=1.000 in the final 200th generation. Increasing the difficulty of the problem, however, deteriorated the search ability of GP-NSGA-II. When kdiff=20, a delay in convergence was observed while increasing both g(x) and Isin earlier generations. The first solution satisfying the fixed goal appeared in the 39th generation. In the 47th generation, more than half of the solutions achieved the goal in the mating pool. An abrupt decrease in each performance indicator was observed with an improvement of the solution set. The unexpected drop of Isbelow 1.0 was restored while maintaining the convergence of g(x), implying the robustness of the dominance relation near the POF. As a result, GP-NSGA-II obtained moderate indicator values at the end of the simulation run: g(x)=0.051 and Is=0.996 in the final 200th generation. Solving the problem with kdiff=30 was unsuccessful, however, since no solution achieved the goal in any generation. All solutions worsened compared to the initial condition: g(x)=4.085 and Is=7.097 in the final 200th generation. These results signify the numerical stability of the adaptive preference control in MOPs.Another issue related to DGP is how to select the location of the ultimate aspiration levels at which the goal adjustment process is terminated. The levels allocated in the interior of the POF might cause a bias of converged solutions, while the levels allocated in the exterior of the POF can no longer contribute to the evolution once the solution set achieves the levels. Table 7summarizes the simulation results of 13 cases for the DTLZ5(5,10) problem affected by the different magnitude of the ultimate aspiration levels on the search abilities of DS-MOGA and D-MOGA. The case 7 refers to the base case of which the ultimate aspiration levels are the maximum objective values when removing the noise effects, as seen in Table 5, Figs. 9 and 11. In this case, the reference points are located at the corner of the POF. The other twelve cases assume a lack of information on the characteristics of the POF. All ultimate aspiration levels are allocated inside the POF for the cases 1, 2, 3, 4, and 6. All reference points are allocated outside the POF for the cases 10, 11, 12, and 13. For the remaining cases, some reference points are located inside but the other points are located outside the POF. The case 1 represents the ideal but infeasible condition to minimize all objective values simultaneously.Fig. 13compares the averages of the quality indicator values shown in Table 7. Obviously, reflecting the DM's preference on the essential objectives in the reduced problem was more efficient for exploring the entire POF than reflecting the DM's preference on all objectives in the original problem. The average of the number of objectives in the reduced set was close to I=5 for all cases except the case 13 in which the poorest ultimate aspiration levels were given. Among the cases, the evolved solution set of the base case presented the best conformance with the POF. Tightening the ultimate aspiration levels increased the selection pressure excessively, thereby leading to a decrease in Isowing to the collapse of the approximated POF, as seen in Fig. 14. The collapse increased Δpby lengthening the distance between the solutions at the corner of the approximated POF and the true POF. The solutions partially filled the optimal solution domain. Even the case 4 yielded higher g(x) than the base case. On the other hand, Fig. 15shows the increasing trends of g(x) and Isfrom their own ideal values. These results indicate that loosening the ultimate aspiration levels worsens the overall quality of the solution set as most solutions are outside the POF despite the achievement of the ultimate aspiration levels. Meanwhile, the coexistence of the ultimate aspiration levels that were either tightened or loosened amplified the deviation from the ideal value of each quality indicator (see Figs. 16 and 17). The results above highlight the importance of minimizing the discrepancy between the set of Pareto-optimal solutions and the set of ultimate aspiration levels for achieving the convergence as well as the diversity-preservation over the degenerated POF.Table 8provides the simulation results of 20 DTLZ5(I,M) problems. For each problem, the mean and standard deviation of the quality indicator values were calculated over 25 simulation runs. All runs were performed under the experimental condition described in Section 5.1.3. Each scatter-plot depicted in Section 5.2 corresponds to the simulation run of which the g(x) value is the most closest to the averaged g(x) value. In Table 8, the asterisk (*) refers to the simulation results of NLOR-NSGA-II (NSGA-II coupled with NL-MVU-PCA objective-reduction) and NLOR-ɛ-MOEA (ɛ-MOEA coupled with NL-MVU-PCA objective-reduction) quoted from Refs. [35,38], which were obtained from a larger number of simulation runs (Ngenof 2000 with Npopof 200). These reference data give an indirect comparison with the EMO algorithms coupling the nonlinear objective-reduction scheme, which is not yet implemented in the proposed framework.Fig. 18visualizes the averaged performance metrics for eight problems addressed in Table 8. DS-MOGA showed the best conformance with the POF as this algorithm obtained smaller Δpand g(x) with Isclose to 1.0 for most problems. The consistency of DS-MOGA was more distinguishable as the structure of the given problem became more complex in proportion to the number of objectives. With the assistance of the ultimate aspiration levels, D-MOGA yielded comparable performance with DS-MOGA, while NSGA-II-based algorithms implementing either a linear or a nonlinear objective-reduction scheme obtained unsatisfactory quality indicator values in highly redundant problems, e.g. DTLZ5(2,10). Rather, the dimensionality-reduction scheme worked properly in negligibly redundant problems of which the dimension of the degenerated POF was less than five, e.g. DTLZ5(4,5). Detecting the Pareto-relation among objectives was obscured by a high degree of redundancy in non-optimal solution domain. These results emphasize the significance of the preference-ordering in detecting the Pareto-relation even if the enhanced convergence might accompany the loss in diversity as presented in Tables 7 and 8. Interestingly, NLOR-ɛ-MOEA in the references provided remarkable simulation results showing quality indicator values over the objective-reducing algorithms in this study, as presented in Table 8. It is presumed that the ɛ-based discretization of objective space alleviated the poor scalability by allocating solutions uniformly in objective space if the archive size is manageable compared to the population size. It is in our interests to integrate DGP and SLOR in the ɛ-based evolutionary process in the near future. Considering the sensitiveness of the ɛ size, the adaptive control of ɛ should be handled in relation to the location of the reference points as well.Table 9compares the statistical parameters for the number of essential objectives in the final 200th generation obtained over 25 simulation runs. For DTLZ5(5,10), the solutions attracted near the POF contributed to increasing the consistency of the objective-reduction scheme by yielding the averaged m of 4.72 when comparing the averaged m of 6.48 obtained from S-MOGA. The smaller deviation of DS-MOGA over S-MOGA supports the argument that the conflict among objectives is much clearer in the optimal domain. NSGA-II could approximate the POF only when the total number of objectives was less than five regardless of the dimension of the degenerated POF. Apparently, the search efficiency of each EMO algorithm declined with the increase in the total number of objectives. A higher dimension of the POF lowered the quality of the solution set in the fixed dimension of objective space as well.Table 10includes the simulation results when tightening (gi=0.0 ∀i=1:M) or loosening (gi=1.5 ∀i=1:M) the ultimate aspiration levels. The ultimate aspiration levels allocated away from the POF still enhanced the convergence. Meanwhile, a use of global-objective genetic algorithm minimizing the objective-sum provided g(x) of 0.0 with Isof 0.0 for all case studies. Solutions were located around the point (0,0,0,0,0,0,0,0,0,1) for the DTLZ5(I,10) problems, for example, regardless of the dimension of the degenerated POF. This bias observed in the solution set reveals the inadequacy of the objective-aggregation in trade-off analyses.Table 11summarizes the simulation results of 5 DTLZ2(M) problems with 4–20 objectives. The simulation runs were performed under the identical experimental condition used for obtaining Tables 8 and 10. The variance threshold in the objective-reduction method plays a role to prevent from eliminating any non-redundant objectives. DS-MOGA and D-MOGA yielded comparable performance metric values as all objectives were eventually regarded as significant in most simulation runs (see Table 12). Similar results were attained from S-MOGA and NSGA-II. For solving the DTLZ2(5) problem, unexpected elimination of a few objectives occurred in some experiments, thereby resulting in the collapse of the approximated POF with decreasing Isexcessively. Further consistency check in the dimensionality-reduction is desired with concern for the limitation of linear PCA in detecting nonlinearity among objective pairs. It is also interesting that smaller g(x) values accompanied larger Δpvalues for the DTLZ2(5) problem. This disagreement in the behaviors between the different convergence indicators supports the assertion that the combination of unary performance metrics may be difficult to provide a clear indication of whether one solution set outperforms the other sets [113].

@&#CONCLUSIONS@&#
The proposed DS-MOGA, reflecting the DM's preference on the essential objectives, efficiently enhanced the multi-directional search toward the POF. SLOR as the linear objective-reduction method and DGP as the adaptive preference-ordering method were seamlessly coupled with the Pareto-dominance based multi-objective genetic algorithm. During optimization, DGP adjusted both the aspiration levels and the constraints progressively based on the probability density functions of the objective values, articulated this preference information for the essential objectives, and thereby enhanced the convergence speed toward the favored zone on the POF by pruning any unqualified solutions. SLOR determined the essential objective set by excluding redundant objectives from the fitness evaluation of subsequent generations. Applications on benchmark test problems validated the efficacy to integrate both preference-ordering and objective-reduction schemes systematically in the evolutionary process. The test results showed DS-MOGA outperformed NSGA-II, NSGA-II coupled with preference-ordering, and NSGA-II coupled with objective-reduction in terms of convergence and uniformity indicators. The reduced representation of the problem had positive effects on the search efficiency even if the ultimate aspiration levels provided insufficient information on the characteristics of the POF. More sophisticated determination of these reference points will contribute to improving the robustness of the proposed framework. The results of the oil-field application introduced in the authors’ recent paper also showed that the proposed framework could be successfully applied to solve the convergence problem and the unrealistic estimation caused by scale-difference among the multi-objective functions.