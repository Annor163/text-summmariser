@&#MAIN-TITLE@&#
Triangulation and metric of lines based on geometric error

@&#HIGHLIGHTS@&#
The paper proposes a geometric error based triangulation and metric study of lines. As one of the fundamental problems in computer vision, line triangulation is to determine the 3D coordinates of a line based on its 2D image projections from more than two views of cameras. Compared to point features, line segments are more robust to matching errors, occlusions, and image uncertainties. However, when the number of views is larger than two, the back-projection planes usually do not intersect at one line due to measurement errors and image noise. Thus, it is critical to find a 3D line that optimally fits the measured data. This paper, at first time, provides a comprehensive study on the triangulation and metric of lines based on geometric error.In this paper, a comprehensive study of line triangulation is conducted using geometric cost functions. Compared to the algebraic error based approaches, geometric error based algorithm is more meaningful, and thus, yields better estimation results. The main contributions of this study include: (i) it is proved that the optimal solution to minimizing the geometric errors can be transformed to finding the real roots of algebraic equations; (ii) an effective iterative algorithm, ITEg, is proposed to minimizing the geometric errors; and (iii) an in-depth comparative evaluations on three metrics in 3D line space, the Euclidean metric, the orthogonal metric, and the quasi-Riemannian metric, are carried out. Extensive experiments on synthetic data and real images are carried out to validate and demonstrate the effectiveness of the proposed algorithms.

@&#KEYPHRASES@&#
Line triangulation,Metric of lines,Plücker coordinates,Geometric error,Quasi-Riemannian,

@&#ABSTRACT@&#
Line triangulation, as a classical problem in computer vision, is to determine the 3D coordinates of a line based on its 2D image projections from more than two views of cameras. Classical approaches for line triangulation are based on algebraic errors, which do not have any geometrical meaning. In addition, an effective metric to evaluate 3D errors of line triangulation is not available in the literature. In this paper, a comprehensive study of line triangulation is conducted using geometric cost functions. Compared to the algebraic error based approaches, geometric error based algorithm is more meaningful, and thus, yields better estimation results. The main contributions of this study include: (i) it is proved that the optimal solution to minimizing the geometric errors can be transformed to finding the real roots of algebraic equations; (ii) an effective iterative algorithm, ITEg, is proposed to minimizing the geometric errors; and (iii) an in-depth comparative evaluations on three metrics in 3D line space, the Euclidean metric, the orthogonal metric, and the quasi-Riemannian metric, are carried out. Extensive experiments on synthetic data and real images are carried out to validate and demonstrate the effectiveness of the proposed algorithms.

@&#INTRODUCTION@&#
Line triangulation refers to the process of determining a 3D line given its projections in two or more images and the corresponding camera matrices [1,5]. Compared to point features, line segments are more robust to matching errors, occlusions, and image uncertainties. 3D reconstruction from line segments has been widely studied or used in the literature. In order to automatically recognize corresponding lines, a novel line segment recognition algorithm was presented in [20] based on epipolar constraints and straight-line auto-grouping. A toolbox LMATCH was designed in [21] to match and reconstruct 3-D line segments across multiple calibrated images. Linger [22] proposed an algorithm to detect and match straight-line segments between multiple images. Uchiyama et al. [23] adopted the three-view constraints to find the correspondence of line segment, by exploring a color feature of the line segment and the preliminary knowledge of the vehicle motion. By combining projective geometry and statistical information, the uncertainties of line segments are presented in a homogeneous coordinate system to better match and reconstruct 3D line segments [24]. In [28], a novel method was proposed to accurately extract plane intersections from large-scale raw scan points. The line segments were matched from Line-based Eight-directional Histogram Feature (LEHF) in [29].Zhang [19] firstly investigated the problem of structure from motion using line segments. In [30], 3D line segments were used to estimate the pose of an RGB-D camera. Appearance-less epipolar guided line matching was utilized in [31] to create a potentially large set of 3D line hypotheses, which were then verified using a global graph clustering procedure. Then, these 3D line segments were used to improve the 3D model of man-made environments. The generation of an improved reconstruction by imposing global topological constraints given by connections between neighboring lines was studied in [27]. In [18], line segments were reconstructed from visibility information. A linear method for simultaneously reconstructing 3D lines and cameras from many perspective views was proposed in [25] by using a direct reference plane. A geometric error minimization scheme was proposed in [26] to achieve projective reconstruction from line segments.As one of the fundamental problems in computer vision, this problem of line triangulation is trivial in theory since the 3D line is the intersection of the back-projected planes of the image lines. However, when the number of views is larger than two, the back-projected planes usually do not intersect at one line in the 3D space due to measurement error and image noise. Thus, we have to find a 3D line that fits optimally with the measurement; this is called optimal line triangulation. In the literature, some approaches have been proposed for line triangulation based on different optimality criteria, such as minimizing algebraic errors, geometric errors, and the re-projection errors [2,7,9,15,16].The algebraic error minimization of line triangulation is a linear least squares problem with a quadratic constraint (called the Klein constraint, as discussed in Section 2). Bartoli and Sturm [2] proposed a linear algorithm for the algebraic error minimization. This algorithm first finds a solution of the corresponding linear least squares problem by ignoring the Klein constraint, then, the solution is corrected subsequently by singular value decomposition (SVD) with the Klein constraint enforcement. This algorithm provides only an approximation of the optimal solution to the algebraic error minimization. In our recent study [17], we proposed several methods for line triangulation by minimizing algebraic errors. Based on the Lagrange multipliers theory, a formula to compute the Plücker correction is provided, and from the formula, a new linear algorithm is proposed for line triangulation. Two nonlinear optimal algorithms are proposed by minimizing the algebraic error without further Plücker coordinates correction step.Compared with the algebraic error minimization, finding the optimal solution to minimizing the geometric/re-projection error is more difficult, because, as shown in Section 2, we have to solve a problem of sum-of-fractions programming with the Klein constraint. Freund and Jarre [6] pointed out that the fractional programming is an NP-Complete problem under the assumptions that the feasible region is convex and the numerator and denominator of each fraction in the sum are convex and concave, respectively. Thus, any approach to finding optimal solution of the fractional programming will necessarily be very slow unless P=NP. Based on the above assumptions, Josephson and Kahl [9] proposed a branch-bound algorithm for the re-projection error minimization. In this algorithm, the Plücker coordinates of 3D line are parameterized as a form of four variables with two specific planes, in which one of its components is a non-linear function of the four variables. When only the non-linear component is considered as an independent variable, parts of the sum in the cost function can be expressed as a ratio of convex functions, and thus, the algorithm can only provide a suboptimal solution to the re-projection error minimization. Bartoli and Sturm [2] proposed an iterative algorithm for the geometric error minimization based on the orthogonal representation of 3D lines, which does not guarantee to yield an optimal solution. In addition, this algorithm can also be used for the re-projection error minimization.In studies on line triangulation, a natural question is that which one of the above three optimality criteria is the “best”? In order to answer this question, we need a criterion that is independent of the three optimality criteria to describe the “bestness”. One intuitive criterion is the 3D error, i.e. distance between a reconstructed line and its ground truth. The Euclidean distance does not give a reasonable measure since it is not an intrinsic distance on 3D line space. So far, no study on the metrics of 3D lines is available in the literature, and thus, it is still an open problem for the evaluation of line triangulations.This paper focuses on the triangulations and metrics of lines. The main contributions are summarized as follows:•For the geometric/re-projection error minimization, it is proved that finding the optimal solution to minimizing the geometric/ re-projection errors can be transformed to finding roots of(4N+1)or(3N+1)-degree polynomial equations in six variables, where N is number of views.In practice, the above transformation-based approach is computationally intensive for a large number of views. To handle this issue, an effective iterative algorithms, ITEg, is proposed to minimizing the geometric errors.Three metrics on 3D line space, i.e., the Euclidean metric, the orthogonal metric, and the quasi-Riemannian metric, are analyzed and compared in detail. The orthogonal metric is based on the angular distance on rotation groups [8] and the orthogonal representation of 3D lines [2]. The quasi-Riemannian metric is based on the Riemannian metric on a 5-dimensional unit sphere.The rest of the paper is organized as follows. Section 2 presents some preliminaries used in the paper. Three common criteria for optimality are stated in Section 3. The proposed optimal algorithm is elaborated in Section 4. Section 5 presents two new metrics on 3D line space. Some experimental results with synthetic and real data are presented in Sections 6 and 7, respectively. Finally, the paper is concluded in Section 8.In 3D projective space, the Plücker coordinates of a line is defined by a nonzero 6-vector:(1)L=(x×yx4y−y4x)Δ=(uv)whereX=(xx4),Y=(yy4)are two non-coincident points on the line. The Plücker coordinates is homogeneous since the two 6-vectors computed with two different pairs of points on the line are equal up to a nonzero factor. From (1), it is easy to see thatuTv=(x4y−y4x)T(x×y)=0, i.e. the Plücker coordinates satisfiesuTv=0, or written in a matrix form as(2)LTKL=0(K=(I3I3))In 5D projective space, the quadric defined by (2) is called the Klein quadric [14], thus, the Plücker coordinates satisfies the Klein quadric constraint. Conversely, if a nonzero 6-vector satisfies the Klein constraint, it must be the Plücker coordinates of a line in a 3D projective space.In the image plane, the algebraic and geometric distances from a pointx=(x,y,1)Tto a line l are defined as [7](3)da(x,l)=|xTl|(lTl=1)(4)dg(x,l)=|xTl|l12+l22Given a measured point set of a line l,ℓ={xj=(xj,yj,1)T:1≤j≤M}, and letIa=argmin∑j=1Mda2(xj,l)subjecttoITI=1Ig=argmin∑j=1Mdg2(xj,l)then, la andlgare called the linear least squares fitting and the orthogonal least squares fitting of the measured point set ℓ, both of them have linear solutions [7]. The orthogonal least squares fitting is frequently used in practice, since it usually gives a better estimation for the line l.Given N line-projection matrices,Pi(1≤i≤N), and letℓi={xij=(xij,yij,1)T:1≤j≤Mi}be a measured point set from the imaged linePiLof a 3D line L, the line triangulation is meant to estimate the 3D line L from these measured point sets ℓi(1 ≤ i ≤ N). The algebraic and geometric distances of point-line in the image plane leads to the following two optimality criteria to solve this problem [2,7]:Algebraic error minimization:(5)La*=argmina(L)Δ=∑i=1N∑j=1Mida2(xij,PiL)subjecttoLTKL=0andLTL=1Geometric error minimization:(6)Lg*=argminΔ=(L)Δ=∑i=1N∑j=1Midg2(xij,PiL)subjecttoLTKL=0whereLa*andLg*are called the optimal solution to minimize the algebraic error and the geometric error.La*makes the sum of squared algebraic distances from the measured points xijto the re-projected linesPiLa*reach a minimum. Thus,{P1La*,P2La*,…,PNLa*}are the linear least squares fittings of the measured point sets{ℓ1,ℓ2,…,ℓN}. Similarly,{P1Lg*,P2Lg*,…,PNLg*}are the orthogonal least squares fittings of the measured point sets{ℓ1,ℓ2,…,ℓN}.In practice, the measurement liof the image linePiLis first estimated by the orthogonal least squares fitting of the measured points ℓi, then, the line L is estimated using the image line correspondences {li: 1 ≤ i ≤ N}. This process leads to the following optimality criterion [9]:Re-projection error minimization:(7)Lr*=argminr(L)Δ=∑i=1N{(li1−Pi1TLPi3TL)2+(li2−Pi2TLPi3TL)2}×(li=(li1,li2,1)T)subjecttoLTKL=0wherePikTis the kth row vector of the line-projection matrixPi.Lr*is called the optimal solution to the re-projection error minimization.The minimization term in (5) can be expressed asa(L)=∑i=1N∑j=1Mi(xijTPiL)2=LTAL(whereA=∑i=1NPiT(∑j=1MixijxijT)Pi)Thus, (5) can be rewritten as(8)La*=argminLTALsubjecttoLTKL=0andLTL=1which means that the minimization of the algebraic error is a linear least squares problem with the Klein constraint.The cost function in (6) can be expressed as(9)g(L)=∑i=1N∑j=1Mi(xijTPiL)2(Pi1TL)2+(Pi2TL)2=∑i=1NLTGiLLTGi′L(Gi=∑j=1MiPiTxijxijTPi,Gi′=Pi1Pi1T+Pi2Pi2T)which is the sum of N rational quadratic functions. Similarly, the cost function in (7) can also be expressed as the sum of N rational quadratic functions:(10)r(L)=∑i=1N((li1Pi3−Pi1)TL)2+((li2Pi3−Pi2)TL)2(Pi3TL)2=∑i=1NLTRiLLTRi′L(Ri=∑k=12(likPi3−Pik)(likPi3−Pik)T,Ri′=Pi3Pi3T)Compared with (5)–(7) are more difficult to solve. Note that (6) and (7) do not need the unit norm constraintLTL=1. This is because for an arbitrary constant c ≠ 0, there must haveg(cL)=g(L)andr(cL)=r(L), thus, adding such a constraint is unnecessarily except for increasing the difficulty of solving them.This section considers the problems of estimatingLg*in (6) andLr*in (7). In theory, these problems can be converted into the ones of finding the real solutions of systems of(4N+1)-degree or(3N+1)-degree polynomial equations in six variables. Although the continuous homotopy method [3,4,11,12] can be used to solve these algebraic equations, however, the involved computational cost is too large for a large number of views due to the high-degree of the algebraic equations. This section will present an effective iterative algorithm for the estimation ofLg*andLr*.The Lagrange function of (6) is(11)fg(L,α)=g(L)−α(LTKL)and the corresponding Lagrange equations are(12){∂fg∂L=∂g∂L−2αKL=0∂fg∂α=LTKL=0where(13)∂g∂L=2∑i=1N(LTG′iL·Gi−LTGiL·G′i)L(LTG′iL)2By left-multiplying both sides of the first equation in (12) with LTKT, we have(14)LTKT∂g∂L−2αLTKTKL=0SinceKTK=ILTKTKL=LTL≠0, Eq. (14) becomesLTKT∂g∂L−2αLTL=0or(15)α=12LTLLTKT∂g∂LSubstituting the above equation into (12), we have(16){LTL∂g∂L−LTKT∂g∂LKL=0LTKL=0LetQg=12∏i=1N(LTG′iL)2·∂g∂L, each of its elements is a(4N−1)-degree polynomial of L. It is obvious that (16) can be rewritten as(17){LTLQg−LTKTQgKL=0LTKL=0This is a system of(4N+1)-degree polynomial equations in six variables. LetSIIIbe the set of real solutions satisfying∥L∥=1, then, we have the following proposition.Proposition 1The optimal solution to the geometric error minimization is(18)Lg*=argmin{g(L):L∈SIII}Similarly, the following proposition is obtained.Proposition 2The optimal solution to the re-projection error minimization is(19)Lr*=argmin{r(L):L∈SIV}whereSIVis the real solution set of the following system of(3N+1)-degree polynomial equations:(20){LTLQr−LTKTQrKL=0LTKL=0where(21)Qr=12∏i=1N(Pi3TL)3·∂r∂L(22)∂r∂L=2∑i=1N(Pi3TL·Ri−LTRiL·Pi3Pi3T)L(Pi3TL)3These two propositions show that: Determining the optimal solutions ofLg*andLr*can be theoretically transformed to finding the real solutions of the algebraic equations. However, such transformation-based approaches are not feasible if the number of views is large. For example, whenN=6, we need to solve a system of 25-degree or 19-degree polynomial equations in six variables. If the continuous homotopy method is used, then, there will be 256 or 196 one-dimensional connected components to be tracked in the zero-point set of homotopy, and the involved computational cost is unbearable in practice. In the following subsections, we propose two effective iterative algorithms for the estimation ofLg*andLr*, respectively.According to optimization theory, the optimal solutionLg*and the corresponding multiplier α* must be a saddle point of the Lagrange function (11), i.e.fg(Lg*,α)≤fg(Lg*,α*)≤fg(L,α*)Thus, the optimal solutionLg*is the minima of the function fg(L, α*). Therefore, the minimization of (6) is equivalent to the following problem.(23)Lg*=argminfg(L,α*)subjecttoLTKL=0The essential difference of (23) and (6) lies in that the gradient of the cost function fg(L, α*) of (23) in the optimal solutionLg*is zero. i.e.∂fg∂L(Lg*)=0However, the stationary points of the cost function g(L) in (6) do not usually satisfy the Klein constraint, thus, the optimal solutionLg*is not a stationary point of g(L), i.e., the gradient of g(L) inLg*is not zero.∂g∂L(Lg*)≠0By applying a penalty method in the optimization of (23), the minimization (6) can be reformulated to solving the following unconstrained minimization.(24)Lg*=argminG(L,α*)Δ=fg(L,α*)+C(LTKL)2where C is a sufficiently large positive number, called the penalty factor of the constraint term.Note that the penalty method cannot be applied directly to the minimization of (6), i.e., (6) cannot be formulated to solving the minimization of the following function:f(L)Δ=g(L)+C(LTKL)2This is because∂(LTKL)2∂L|L=Lg*=0. Once∂g∂L(Lg*)≠0, there must exist∂f∂L(Lg*)≠0, thus, the optimal solution ofLg*is not a stationary point of f(L).The question now is turned into how to determine multiplier α*. Since (15) gives the relationship between the multiplier α and the variable L, an initial value of the multiplier α can be determined from the initial value of L. Then, in an iterative manner, the multiplier α is updated according to (15). From the above discussion, our iterative algorithm ITEg is outlined as follows.Iterative algorithm ITEgI: Initializationk=0;L0=La*orLs*;α0=12La*TK∂g∂L(La*)or12Ls*TK∂g∂L(Ls*);whereɛis a preset accuracy parameter.II: Iteration(a) Solve the following unconstrained minimization using the Levenberg– Marquardt and the Nelder–Mead methods with the initial valueLk:Lk+1=argminG(L,αk)Δ=fg(L,αk)+C(LTKL)2(b) Update multiplier:αk+1=12Lk+1TLk+1Lk+1TK∂g∂L(Lk+1)(c) If∥Lk+1−Lk∥≤ɛ,∥αk+1−αk∥≤ɛ, the iteration terminates; otherwise,k←k+1, return to the step (a).III: OutputLetL¯=Lk+1/∥Lk+1∥. Compute the Plücker correction ofL¯and outputLg*=Ls*Remark 1Compared with the existing iterative algorithms in the literature, the ITEg avoids the selection of inhomogeneous Plücker coordinates with 4-parameter. Note that the ITEg is of local convergence like the existing iterative algorithms, that is, the convergence of ITEg is guaranteed only when the initial value is close to the optimal solution. According to our large number of numerical simulations, the ITEg with the initial valueLa*is always convergent to the optimal solutionLg*for a small number of views. In addition, by some common multiplier update rules in the literature, such as the famous Hestenes’ rule [10],αk+1=αk+CLk+1TKLk+1, we can also obtain the corresponding local convergence algorithms. Since the relationship between the multiplier and the variables are not taken into account, the convergence speeds of these algorithms are very slow.Remark 2In a similarly manner, when re-projection error is employed, we can have an iterative algorithm based on re-projection errors, which is the same as the ITEg algorithm except for the following two replacements:∂g∂L←∂r∂L,G(L,αk)←R(L,αk)Δ=r(L)−αk(LTKL)+C(LTKL)2.In order to evaluate 3D errors of line triangulations, we need a metric in 3D line space. The Euclidean distancedE(L,L′)Δ=min{∥L−L′∥,∥L−(−L′)∥}, (where L, L′ are the normalized Plücker coordinates of linesL,L′) is not appropriate for the evaluation of line triangulations since is not an intrinsic distance on 3D line space. The aim of this section is to introduce two new metrics on 3D line space, called the orthogonal metric and the quasi-Riemannian metric. Compared with the Euclidean metric and the orthogonal metric, the quasi-Riemannian metric appears more appropriate.In this section, the 5-dimensional unit sphere centered at the origin inR6is denoted byS5(1), and the intersection of the Klein quadricKandS5(1)is denoted byK(1)Δ=S5(1)∩K, which is a 4-dimensional smooth sub-manifold ofS5(1), called the unit Klein quadric.The proposed orthogonal metric is mainly from the angular distance of rotation matrices [8] and the orthogonal representation of 3D lines [2]. We first state briefly the angular metric on rotation group, followed by the proposed orthogonal metric.LetSO(3)={R∈R3×3:RRT=I,det(R)=1}be a 3D rotation group. ForR∈SO(3), we have the following angle- axis representation:R=I+sin(θ)[a]×+(1−cos(θ))[a]×2where θ (0 ≤ θ ≤ π) anda(∥a∥=1)are the rotation angle and rotation axis of R, respectively, and the rotation angle satisfies:θ=arccos(tr(R)−12)The angular distance ofR,R′∈SO(3)is defined as [8](25)d∠(3)(R,R′)=arccos(tr(RR′T)−12)Similarly, for a 2D rotation group,SO(2)Δ={W∈R2×2:WWT=I,det(W)=1}, the angular distance is defined as(26)d∠(2)(W,W′)=arccos(tr(WW′T)2)According to the angular distancesd∠(3)andd∠(2), the angular distance onSO(3)×SO(2)can be defined as(27)d∠(X,X′)=d∠(3)(R,R′)+d∠(2)(W,W′),X=(R,W),X′=(R′,W′)∈SO(3)×SO(2)Since the geodesic distances of metric spaces(SO(3),d∠(3))and(SO(2),d∠(2))are the angular distancesd∠(3)andd∠(2)themselves [8], it is easy to verify thatd∠is also the geodesic distance of the metric space(SO(3)×SO(2),d∠).IfL=(uv)∈K(1), then we have (i): u ≠ 0, v ≠ 0; or (ii):u=0,∥v∥=1; or (iii):∥u∥=1,v=0. By the definition of Plücker coordinates, for the case of (ii), L is the Plücker coordinates of a 3D line passing through the origin and v is its direction; for the case (iii), L is the Plücker coordinates of a 3D line in the infinite plane, and u is its normalized coordinates as a 2D line in the infinite plane.LetKa(1)={L∈K(1):u≠0,v≠0},Kb(1)={L∈K(1):u=0,∥v∥=1}, andKc(1)={L∈K(1):∥u∥=1,v=0}. We defineL=(u,v)forL∈Ka(1), thenL=(u∥u∥,v∥v∥)︸AL(∥u∥00∥v∥)︸BLThus, through the following mappingsAL→(u∥u∥,v∥v∥,u×v∥u×v∥)︸RL∈SO(3);BL→(∥u∥−∥v∥∥v∥∥u∥)︸WL∈SO(2),we obtain the mappingϕ:Ka(1)↦SO(3)×SO(2)by [2]:(28)ϕ(L)=(RL,WL)and it is called the orthogonal representation ofL∈Ka(1).The above mapping fails forKb(1)andKc(1). In order to obtain a complete mapping fromK(1)intoSO(3)×SO(2), we have the following definition.(29)ϕ(L)={(2vvT−I3,Wπ/2),L∈Kb(1)(2uuT−I3,I2),L∈Kc(1)where Wπ/2 is the 2D rotation of angle π/2. An explanation of this definition will be given later. Using the angular distance onSO(3)×SO(2), the following distance onK(1)is obtained:(30)dO(L,L′)=d∠(ϕ(L),ϕ(L′)),L,L′∈K(1)SinceL∈K(1)if and only if L is the normalized Plücker coordinates of a 3D lineL∈L(3); while±L∈K(1)are the normalized Plücker coordinates of the same 3D line, the distance dO leads directly to the following distance in 3D line spaceL(3).(31)dO(L,L′)=min{d∠(ϕ(L),ϕ(L′)),d∠(ϕ(L),ϕ(−L′))},×L,L′∈L(3)and it is called the orthogonal distance of two 3D lines.Now, we can give an explanation for the definition (29). IfL,L′∈Kb(1), thendO(L,L′)=arccos(2(vTv′)2−1)=2·θ(v,v′)Thus, the first mapping in the definition (29) is meant the orthogonal distance of two lines passing through the origin is two times of their included angle.Similarly, whenL,L′∈Kc(1),dO(L,L′)=2·θ(u,u′), since u and u′ are the normalized coordinates of the infinite lines L and L′, they are respectively the normal vectors of a plane passing through L and that passing through L′. Hence, the second mapping in the definition (29) is meant the orthogonal distance of two infinite lines is just two times of the included angle by the two planes.We first state briefly the Riemannian metric onS5(1)in order to introduce the quasi-Riemannian metric onK(1). LetS=(0,…,0,−1)TandN=(0,…,0,1)Tbe the south pole and north pole ofS5(1), respectively. We define the mappingsφ±:U±→R5as follows:(32)Y=φ±(X)Δ=11±x6(x1,x2…,x5),X∈U±whereU+=S5(1)∖{S}andU−=S5(1)∖{N}. Their inverse mappings are(33)X=φ±−1(Y)=11+∑iyi2(2y1,…,2y5,±(1−∑iyi2))T,×Y∈φ(U±)andJ={(U+,φ+),(U−,φ−)}is a smooth structure onS5(1). The Riemannian metric onS5(1)induced by the standard Euclidean metric,h=∑i(dxi)2inR6is(34)g=4(1+∑iyi2)2∑i(dyi)2Letγ={Y(t)=(y1(t),…,yn(t))T:0≤t≤1}be a smooth or piecewise smooth curve inS5(1), its length is defined as(35)L(γ)=∫014(1+∑iyi2(t)2)∑i(dyi(t)/dt)2dt=∫0121+∥Y(t)∥2∥dY(t)dt∥dtForY0,Y1∈S5(1), letΓY0,Y1be the set of all smooth or piecewise smooth curves with the endpoints at Y0 and Y1, the Riemannian distance induced by the metric (34) is defined asdS(Y0,Y1)Δ=inf{L(γ):γ∈ΓY0,Y1}=L(ɛ(Y0,Y1))whereɛ(Y0,Y1)is the short arc from Y0 to Y1 on a great circle inS5(1).(36)=arccos(Y0TY1)It is not difficult to verify that the Riemannian distance dS and the Euclidean distancedE(=∥Y0−Y1∥)both satisfy the following relation.Lemma 1ForY0,Y1,Y2,Y3∈S5(1), we have(37)dS(Y0,Y1)<dS(Y2,Y3)⇔dE(Y0,Y1)<dE(Y2,Y3)ForX0=(u0v0),X1=(u1v1)∈K(1),letX(t)=(1−t)X0+tX1=((1−t)u0+tu1(1−t)v0+tv1)Δ=(u(t)v(t)),0≤t≤1,Then, we have the following lemma.Lemma 2(a)Ifu0±v0≠−(u1±v1), then, u(t) ± v(t) ≠ 0,  t ∈ [0, 1]Ifu0+v0=−(u1+v1), then,u(t)+v(t)≠0,t∈[0,1]∖{1/2};u(1/2)+v(1/2)=0Ifu0−v0=−(u1−v1), then,u(t)−v(t)≠0,t∈[0,1]∖{1/2};u(1/2)−v(1/2)=0ProofFromu(t)±v(t)=(1−t)(u0±v0)+t(u1±v1), we have(38)u(t)±v(t)=0⇔{u0±v0=−s(u1±v1)s=t/(1−t)Clearly, the short arc from X0 to X1 on a great circle inS5(1)isX¯(t)=X(t)∥X(t)∥=1∥X(t)∥(u(t)v(t)),0≤t≤1SinceXT(t)KX(t)=2uT(t)v(t)=2t(1−t)(u0Tv1+v0Tu1)=2t(1−t)X0TKX1,∀0≤t≤1we have(a)IfX0TKX1=0, then,X¯(t)∈K(1)for0≤t≤1;IfX0TKX1≠0, then,X¯(t)∉K(1)for0<t<1.For the case (a), the Riemannian distance onS5(1)leads directly to the Riemannian distance between X0 and X1 inK(1).(39)dK(X0,X1)=arccos(X0TX1)We consider the case (b) next. According to Proposition 1 in [17] and Lemma 2, the best approximation ofX¯(t)(t≠1/2)on the sub-manifoldK(1)under the Euclidean metric isX*(t)=12(u(t)+v(t)∥u(t)+v(t)∥+u(t)−v(t)∥u(t)−v(t)∥u(t)+v(t)∥u(t)+v(t)∥−u(t)−v(t)∥u(t)−v(t)∥)∈K(1)By Lemma 1, X*(t) is also the best approximation ofX¯(t)onK(1)under the Riemannian metric, thus, X*(t) is the orthogonal projection ofX¯(t)onK(1)under the Riemannian metric [13]. By Lemma 2, X*(t) is a smooth or piecewise smooth curve onK(1). Thus, lettingY*(t)=φ(X*(t)), a quasi-Riemannian distance between X0 and X1 inK(1)is obtained using the Riemannian metric onS5(1).(40a)dK(X0,X1)=∫0121+∥Y*(t)∥2∥dY*(t)dt∥︸f(t)dtProposition 3The integration (40a) can be expressed as:(40b)dK(X0,X1)=2∫01/2(a(t2+a)2+b(t2+b)2)1/2dtwherea={(2−q+)/(4q+),q+≠00,q+=0,b={(2−q−)/(4q−),q−≠00,q−=0,q±=1−(X0TX1±X0TKX1)Specifically, ifX0TKX1=0, then,dK(X0,X1)=arccos(X0TX1), i.e., (39) is a special case of (40).ProofBy some mathematical manipulation, the integrand of (40a) can be expressed as:f(t)=12((2−q+)q+(2q+(t2−t)+1)2+(2−q−)q−(2q−(t2−t)+1)2)1/2=12((2−q+)q+(4q+(t−1/2)2+(2−q+))2+(2−q−)q−(4q−(t−1/2)2+(2−q−))2)1/2=12(a((t−1/2)2+a)2+b((t−1/2)2+b)2)1/2IfX0TKX1=0, thena=b=4−1(1+X0TX1)/(1−X0TX1), anddK(X0,X1)=2a∫0121t2+adt=2arctan12a=2arctan1−X0TX11+X0TX1=arccos(X0TX1)□The quasi-Riemannian distance onK(1)leads directly to the quasi-Riemannian distance onL(3)as(41)dQR(L,L′)=min{dK(L,L′),dK(L,−L′)}It is easy to verify that linesLandL′are coplanar if and only if their Plücker coordinates satisfyLTKL′=0. Thus, the quasi-Riemannian distance of coplanar lines is given by the following formula.(42)dQR(L,L′)=min{arccos(LTL′),π−arccos(LTL′)}In order to compare the performance of different metrics, we gerenated a 3D unit cube centered at the origin in space. Fig. 1shows the 12 edges and other 16 line segments of the unit cube. Fig. 2(a)–(c) shows respectively the distances between the edges computed by the Euclidean metric, the Orthogonal metric, and the quasi-Riemannian metric, where different distance values are represented using different colors.Based on their relative positions, the edge pairs belong to either the two parallel relationships (P-I and P-II) or the two orthogonal relationships (O-I and O-II):P−I={(1,3),(1,5),(2,4),(2,6),(3,7),(4,8),(5,7),(6,8),(9,10),(9,12),(10,11),(11,12)}P−II={(1,7),(2,8),(3,5),(4,6),(9,11),(10,12)}O−I={(1,2),(1,4),(1,9),(1,10),(2,3),(2,10),(2,11),(3,4),(3,11),(3,12),(4,9),(4,12),(5,6),(5,8),(5,9),(5,10),(6,7),(6,10),(6,11),(7,8),(7,11),(7,12),(8,9),(8,12)}O−II={(1,6),(1,8),(1,11),(1,12),(2,5),(2,7),(2,9),(2,12),(3,6),(3,8),(3,9),(3,10),(4,5),(4,7),(4,10),(4,11),(5,11),(5,12),(6,9),(6,12),(7,9),(7,10),(8,10),(8,11)}Each of the three metrics can give a unique distance for each relationship, as shown in Table 1. However, from Table 1 it can be seen that the Euclidean metric could not distinguish between O-I and O-II; the orthogonal metric could not distinguish between P-I and O-I; while the quasi-Riemannian metric gives different distances for all four relationships, and these distances are consistent with our intuition that the distances for P-I, P-II, O-I and O-II should increase gradually. This observation implies that the quasi-Riemannian metric is more reasonable than the Euclidean metric or the orthogonal metric.To make a more complete comparison, we analyzed the relationships between all the line segments (C(8,2)=28) through any two points of the unit cube in Fig. 1. According to the geometrical position, 378 (=C(28,2)) pairs of line segments were generated by these line segments, and all these pairs can be divided into 18 categories of relationships as shown in Fig. 3. If a metric recovers the same distance for each pair in a certain category, we call it correct; otherwise, we call it incorrect. In practice, we may have two different types of incorrectness. If a metric yields the same distance for every line segment pairs in two different categories, we call it incorrect type-I; in another case, if a metric obtains different distances for any pairs in the same category, we call it incorrect type-II.According to the above analysis, the sum of the correct relationships and the type-I incorrect ones should equal to 18, the total categories of all relationships. The result is shown in Table 2, from which, we can see that the quasi-Riemannian metric recovers the most categories of the correct relationships with the least incorrect ones, while the orthogonal metric yields the most number of incorrectness. A detailed graphic result of all the metrics on different relationships of line segments is given in the appendix.This experiment aims at comparing the line reconstruction results of different metrics. First, we generate the 2D images of the line segments through randomly selected 3D line segments and camera projection matrices, six images are used in the test. Then, we reconstruct the 3D line segments from the 2D image. In order to obtain a statistically meaningful result, Gaussian white noise is added to the points on the images of the line segments, and 20 independent tests are carried out. Under different metrics, the average value of the line segments are computed. The errors of different estimations under different metrics are ploted in Fig. 4. It is evident that for the Euclidean error, the results of the the quasi-Riemannian metric and the Euclidean metric are smaller than that of the orthogonal metric; for orthogonal error, the results of the quasi-Riemannian metric and the orthogonal metric are smaller than that of the Euclidean metric; for quasi-Riemannian error, the results of the quasi-Riemannian metric and the Euclidean metric are smaller than that of the orthogonal metric; and for geometric error, the result of the quasi-Riemannian metric is smaller than the other two metrics.All these experiments imply that the quasi-Riemannian metric is more reasonable than the Euclidean metric or the orthogonal metric. Therefore, in the following experiments of this paper, the quasi-Riemannian metric is used to evaluate the 3D errors of line triangulations.In the experiments, we simulated eight 3D space lines on symmetrically distributed on two orthogonal planes, as shown in Fig. 5. Using the synthetic lines, we generated six images using different viewpoints and cameras parameters. The size of the images is set at 1024×1024. In order to simulate the effect of image noise, we evenly sample 20 points on each image line segment, and add Gaussian white noise to these sampled image points, then, the actual projected lines are fitted by the orthogonal least squares fitting from these noise-corrupted point set.We evaluated and compared the performance of the linear algorithm LIN [2]; the optimal algorithm based on the algebraic optimality criterion (AOC): OPTa-I [17]; and the iterative algorithm based on the geometric optimality criterion (GOC): LIN / OPTa-I+ITEg. The criteria used in the evaluation are RMS (root mean square) of the 3D errors (i.e., the quasi-Riemannian distance of reconstructed line to its ground truth) and the geometric errors.This experiment aims to test the numerical stability of the algorithms with respect to different noise levels under the same geometric configuration. During the experiment, Gaussian white noise was added to each image point. The noise level was varied from 0.0 to 3.0 pixels in steps of 0.5, and 150 independent trials were carried out under each noise level. We reconstructed the 3D lines from six images using the proposed algorithms, as well as the linear algorithm (LIN) and the algebraic optimal algorithm (OPTa-I). Fig. 6shows the reconstructed 3D errors and geometric errors by different approaches. It is evident that the RMS errors of all algorithms increase with the increase of noise levels; the linear algorithm LIN performs the worst, while the proposed GOC based algorithms yield the lowest 3D errors and geometrical errors. The two iterative algorithms based on the GOC have very close accuracy, although they start with different initial values.We also compared the computational cost of these algorithms. The real computation time of the LIN, OPTa-I, and LIN/OPTa-I+ITEg algorithms are 0.002, 11.681, 7.372, 15.688s, respectively. It can be seen from the results that the OPTa-I+ITEg requires less iterations than the LIN+ITEg algorithm due to a more accurate initialization. However, since the OPTa-I algorithm is computational intensive by itself, the total computation time of the OPTa-I+ITEg is much longer than the LIN+ITEg algorithm, while both algorithms yield very close final results. Therefore, LIN+ITEg is a better choice in practice.This experiment is to test the numerical stability of the algorithms with respect to different geometrical configurations. During the experiment, the number of views was varied from 4 to 12 with the steps of 2. At each number of views, 150 independent trials were carried out to produce a statistical meaningful result. Fig. 7shows an experimental result at noise levelσ=1.5. From this figure, we can see that, with the exception of the linear algorithm, the 3D error and geometric error of all the other algorithms decrease slowly with the increase of image numbers. The image number has a big influence to the LIN algorithm, whose errors decrease greatly when the frame number increase from 4 to 8. In every situation, the two proposed iterative algorithms yield similar results that are better than other approaches.This experiment provides more comparative results using the same configurations as in Sections 6.1 and 6.2. We compared the performance of the proposed iterative algorithm based on the geometric optimality criterion (LIN +ITEg) with respect to the linear algorithm (LIN) and the non-linear optimization of lines using a minimal 4D parameterization (MLE) [2]. In this experiment, we only used the two endpoints of the line segment the same as [2], and the results are shown in Figs. 8and 9, respectively. We can see from the results that, in every situation, the proposed LIN+ITEg outperforms the other two algorithms. If all points on the line segment are utilized, our experiment shows that the performance of the MLE algorithm is almost the same as the proposed algorithm, and the two performance curves overlap with each other, as shown in Figs. 6 and 7, respectively.The proposed algorithms were evaluated using extensive real images. The experimental results on four data sets are reported below. As shown in Fig. 10, the used images include a calibration cube, the Oxford “Wadham College”, the “corridor” datasets, and the “model house” (the datasets were downloaded from the Visual Geometry Group of Oxford. http://www.robots.ox.ac.uk/∼vgg/data/data-mview.html). The lines marked in white and red in these images are used to test the algorithms.For the calibration cube, six images were taken by a Nikon D40 camera, with a resolution of 3008 × 2000 pixels. The correspondences between the 3D points on the cube and their images are used to compute the camera matrices. The camera matrices are computed by the calibration toolbox (http://www.vision.caltech.edu/ bouguetj/calib_doc/). For the Wadham College images, the model house images, and the corridor images, the camera matrices and the coordinates of the two end points of the image lines are provided by the Oxford datasets.Figs. 11–14show the 3D errors and geometric errors obtained from different algorithms using the calibration cube, the Wadham College, the corridor, and the model house datasets, respectively, from which we can obtain the same conclusion as that in the simulations. The two proposed iterative algorithms based on the GOC and the MLE algorithms yield very close results, which outperform the results by the LIN and the OPTa-I algorithms.Fig. 15shows the 3D reconstruction results from the fours datasets using the LIN+ITEg algorithm. We can see that the proposed algorithm correctly recovers the 3D structure of all detected lines. The reconstruction results by other methods are omitted here due to limited space.

@&#CONCLUSIONS@&#
