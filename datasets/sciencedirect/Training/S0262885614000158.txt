@&#MAIN-TITLE@&#
Self-calibration of stationary non-rotating zooming cameras

@&#HIGHLIGHTS@&#
A simple linear method to self-calibrate stationary zooming camerasNeither special camera motion nor scene knowledge requiredTwo zoom images of a stationary camera provide two parallel principle planes.The plane at infinity can be located from two or more such zooming cameras.Simulation, laboratory and real scene experiments with 3D measurements validate our method.

@&#KEYPHRASES@&#
Camera auto/self-calibration,Zooming cameras,Plane at infinity,Parallel planes,3D reconstruction,

@&#ABSTRACT@&#
This paper proposes a new method for self-calibrating a set of stationary non-rotating zooming cameras. This is a realistic configuration, usually encountered in surveillance systems, in which each zooming camera is physically attached to a static structure (wall, ceiling, robot, or tripod). In particular, a linear, yet effective method to recover the affine structure of the observed scene from two or more such stationary zooming cameras is presented. The proposed method solely relies on point correspondences across images and no knowledge about the scene is required. Our method exploits the mostly translational displacement of the so-called principal plane of each zooming camera to estimate the location of the plane at infinity. The principal plane of a camera, at any given setting of its zoom, is encoded in its corresponding perspective projection matrix from which it can be easily extracted. As a displacement of the principal plane of a camera under the effect of zooming allows the identification of a pair of parallel planes, each zooming camera can be used to locate a line on the plane at infinity. Hence, two or more such zooming cameras in general positions allow the obtainment of an estimate of the plane at infinity making it possible, under the assumption of zero-skew and/or known aspect ratio, to linearly calculate the camera's parameters. Finally, the parameters of the camera and the coordinates of the plane at infinity are refined through a nonlinear least-squares optimization procedure. The results of our extensive experiments using both simulated and real data are also reported in this paper.

@&#INTRODUCTION@&#
Camera self-calibration, i.e. computing the camera's intrinsic parameters solely from point correspondences across images, is key to overcome the re-calibration problem of the frequently changing parameters of zooming cameras. These parameters are essential for the recovery of the metric structure of a scene.The self-calibration problem of a “possibly” zooming and moving camera, otherwise nonlinear and challenging to solve [1–5], is drastically simplified when the plane at infinity is first located [6]. Doing so, allows an affine upgrade of the scene and cameras from which the intrinsic parameters can be linearly calculated [7]. However, the difficult step is to locate the plane at infinity with no prior knowledge about the scene. In the case of a moving camera with constant parameters, the modulus constraints [8–10] can be used to recover the plane at infinity by solving a set of nonlinear polynomial equations. In addition to the inherent difficulty of solving nonlinear equations, these constraints cannot be used when the camera parameters are allowed to vary. In the latter case, chirality inequalities [6] may be used to upgrade a projective reconstruction to a quasi-affine one as well as to bound the location of the plane at infinity. This allows the retrieval of the camera parameters and the plane at infinity either through an exhaustive search within the bounded area or using nonlinear optimization [11].As far as stationary cameras are concerned, the assumption of a mandatory pure rotation of the camera has been proposed in the literature [12,13] to obtain its intrinsic parameters. Indeed, pure rotations allow for a linear calculation of inter-image homographies induced by the plane at infinity from which the camera parameters can also be linearly retrieved. However, moving cameras in a purely rotational motion is not easy in practice, and such an assumption is only plausible when the camera is far from the scene [14–16].Whether dealing with stationary or moving cameras, most self-calibration methods, while possibly allowing the cameras to zoom, do not exploit the zooming capability of the capturing device. Only few methods [17,18] relying on the zooming capability of the camera have been reported in the literature. However, such methods have derived only a limited benefit from doing so. Indeed, zooming was only used to estimate the pixel coordinates of the principal point while the linear recovery of the plane at infinity, and hence the remaining intrinsic parameters, required applying a pure translation to the camera. Note that, not only a pure translation of the camera is difficult to obtain in practice, but also that the assumption of a constant principal point throughout zooming is not realistic as shown by several studies among which [19] is probably the most comprehensive one. In addition, it has been shown in [20] that inaccuracies on the position of the principal point may have a significant impact on the calibration of the remaining parameters.The work presented in this paper deals with the problem of self-calibrating a camera system in which two or more stationary non-rotating zooming cameras observe a static scene. To the best of our knowledge, such configuration of cameras has not been addressed in the literature, although this is a realistic and common configuration often encountered in stereo camera systems, surveillance networks and monitoring of events. In such image capture systems, each camera is physically attached to a static structure (wall, ceiling or tripod) and is only allowed to zoom. Because the cameras are not rotating, the methods designed for stationary rotating cameras cannot be employed to self-calibrate each camera independently. Solutions designed for arbitrarily moving cameras may be employed but lead to unnecessarily complicated nonlinear equations. The camera self-calibration method we propose in this paper exploits the zooming capability of the cameras in order to directly estimate the location of the plane at infinity. Unlike existing methods exploiting camera zoom, the principal point is allowed to freely move after zooming and no special motion between the physical cameras is required.Our method for locating the plane at infinity from stationary non-rotating zooming cameras is based on some important observations we have made on the results of the experiments conducted by Willson in his work [19] on designing an active model for zoom lenses. Indeed, the change that affects the intrinsic parameters of a camera while zooming is the result of the displacement of both its optical center and the image plane which undergoes a mostly translational motion with possibly an partial rotation. The camera self-calibration method we propose in this paper precisely exploits the effect of this joint motion of the optical center and the image plane on the camera's so-called principal plane. The principal plane of a camera is a plane parallel to the image plane containing its optical center. Unlike the image plane which requires the knowledge of the focal length in order to be located, the principal plane's coordinates are encapsulated in the camera's perspective projection matrix (camera matrix) from which they can be easily extracted. In particular, a consistent set of camera matrices, i.e. matrices calculated in a projective frame that is common for all views, can be obtained from point correspondences across uncalibrated images. The working assumption in this paper (which is supported by Willson's experiments [19] using several cameras) is that the motion of the principal plane, under the effect of zooming, identifies a pair of parallel planes (or a set of parallel planes in the case of multiple zooms). Neither a special motion between the cameras nor any knowledge about the position of the principal point is required at this stage. Indeed, as parallel planes intersect in a line at infinity, two uncalibrated zooming cameras in general position allow in theory to locate the plane at infinity.Once the plane at infinity is retrieved from parallel principal planes, the no-skew and/or known aspect ratio constraints can be used to linearly estimate the so-called Image of the Absolute Conic (IAC) and hence all the intrinsic parameters. It is also well-known that estimating the camera's intrinsic parameters is very sensitive to the localization of the plane at infinity. Hence, we have investigated two methods for linearly calculating an estimate of the IAC (and as a consequence the camera parameters) from the linearly estimated plane at infinity: (a) the well-known linear least-squares through Singular Value Decomposition (SVD) [7], and (b) a Linear Matrix Inequality formulation which allows the enforcement of the requirement of a positive-definite IAC [21]. Our extensive experiments both on simulated and real images (using a variable number of cameras, zoom settings and image noise) show that the estimate of the intrinsic parameters, obtained by both methods along with the zoom-based candidate plane at infinity, allow for a simple nonlinear least-squares optimization procedure to converge towards the optimal parameters.Our paper is organized as follows. Section 2 presents some necessary background and preliminaries. In Section 3, we describe the zooming camera model that we have considered for developing our method and its relationship to the plane at infinity. Section 4 describes our self-calibrating method for non-rotating zooming stationary cameras, including the linear affine self-calibration step which constitutes the main contribution of the present paper. Our experiments and the results we have obtained are described and discussed in Section 5. Section 6 concludes our work.In this paper we consider a static scene observed by n(n≥2) uncalibrated stationary zooming cameras. The cameras are placed at distinct positions in space and have different orientations. We assume throughout this paper that each camera i (i=1, 2,....., n) captures images at different settings in the subsetSi=12…of possible zooming configurations of its lens. Note that although the cameras and the scene neither rotate nor translate, the geometry of each camera changes under zooming effect. For this, we assume throughout this paper that images captured by the same camera with distinct zoom settings as if they had been captured by distinct cameras each of which following the well-known pinhole model [6]. At any given zoom settings∈Si, a camera i maps a world point Q onto the image point qi,s. Expressing world and image points by their homogeneous coordinates, this mapping is described – up to a scale factor – through a 3×4 camera matrixPi,sas follows(1)qi,s∼Pi,sQwhere (~) represents the equality up to a non-zero scale factor. The notation Pi,sis used to denote the perspective projection matrix of camera i under the sthzoom setting. We assume throughout this paper that all camera matricesPi,shave been calculated from point correspondences with respect to a common projective reference frame. While such set of camera matrices can be obtained using virtually any off-the-shelf method [6], we have used in the present work the method of Rothwell et al. [22]. The resulting matrices allow only for the recovery of the scene and cameras up to a common but unknown projective ambiguity.The projective ambiguity that affects the scene structure and the cameras can be reduced to an affine one by means of an adequate transformation T represented by a 4×4 regular matrix of the form(2)T∼P∏∞T.The transformation matrix T is obtained by stacking a 3×4 matrix, arbitrarily chosen in the set of camera matrices Pi,s, and the row homogeneous coordinate vector ∏∞T of the plane at infinity. While every scene point Q is mapped by T to its new location TQ, the camera matrices in the affine frame are given by Pi,sT−1. In particular, the 3×3 matrixHi,s=Pi,sT−1I0Trepresents the inter-image homography induced by the plane at infinity and relating the reference image (whose projective camera matrix is P) and the image captured by the ithcamera at the setting s of its zoom. Note that I and 0 respectively denote the 3×3 identity matrix and the null 3-vector. When known, the matrices Hi,s allow the self-calibration of the imaging system and hence to upgrade the scene's structure and cameras into a metric frame. Indeed, these matrices satisfy the relationship(3)Hi,s−TωHi,s−1∼ωi,sbetween the Image of the Absolute Conic (IAC) ω in the reference image and its corresponding conic ωi,sin the image captured by camera i under the sthzoom setting. The IAC in each image, including the reference image, is solely dependent upon the intrinsic parameters of the imaging camera. It is represented by a 3×3 symmetric positive-definite matrix that can be factored into ωi,s~Ki,s−TKi,s−1 and whose inverse allows the recovery of the 3×3 upper-triangular intrinsic parameters matrix Ki,s,(4)Ki,s=τfi,sγui,s0fi,svi,s001through Cholesky factorization. While the focal length, denoted herefi,s, and the pixel coordinatesui,s,vi,sof the principal point may vary with every new camera or zoom change, all cameras will be assumed to have a known (unit) aspect ratio τ and zero skew γ. Note that as long as the aspect ratio is known for a camera,Ki,scan always be brought to unit τ with some adequate transformation of the pixel coordinates.In order to upgrade the scene and cameras to a metric frame, it only suffices to recover the intrinsic parameters matrix K of the reference camera or, equivalently, ω's entries. Under the zero skew and unit aspect ratio assumptions, each image provides two linear equations,(5)e1THi,s−TωHi,s−1e1−e2THi,s−TωHi,s−1e2=0ande1THi,s−TωHi,s−1e2=0in the unknown entries of ω. It is assumed throughout the paper that the element at the last row and last column of ω is fixed and set to 1. The vectors e1 and e2 are the canonical basis vectorse1=100Tande2=010T. At least three images (including the reference image) captured from distinct viewpoints are needed to recover all five unknown entries of ω.Note that some classes of motion sequences between cameras are critical for camera self-calibration and lead to its failure [26]. For instance, under the no-skew, known aspect ratio and known plane at infinity assumptions, camera sequences containing at most two viewing directions are critical and the underlying reconstruction ambiguity is affine. Cameras with parallel (or anti-parallel) optical axes share the same viewing direction. Hence, at least three distinct viewing directions throughout the sequence of cameras are necessary for the recovery of the intrinsic parameters when solving (5).The unfamiliar reader may refer to [6, p. 475–478] for more details regarding the affine and metric upgrades of projective reconstructions.The rows of a 3×4 projective camera matrix P are 4-vectors representing the homogeneous coordinates of three planes Π, Ψ and Φ. These planes can be inferred geometrically as the specific world planes depicted in Fig. 1and intersecting in the camera center C.In particular, the plane Φ, represented by the 3rd row of P, is the principal plane [6] (the term focal plane is also used in the literature). The principal plane is the plane containing the X and Y axes of the camera's reference frame, hence parallel to the image plane ℐ and containing the camera center. It is the plane of equation ϕTQ=0 representing the set of all points Q projected onto image points with coordinatesPQ∼uv0T, i.e. points at infinity on the image plane. The method proposed in this paper exploits the motion of the principal plane that accompanies the displacement of the camera center under zooming effect.It is well-known that parallelism is invariant under affine (hence metric) transformations. This property is often exploited to locate the plane at infinity by detecting and establishing correspondences of vanishing points or vanishing lines across images. The plane at infinity can be computed from 3 such vanishing points or from a single vanishing point and a vanishing line [23]. The most general way of locating vanishing points is by determining the intersection point of the images of lines that are parallel in the scene. Furthermore, as parallel planes intersect in a vanishing line, the latter can be located by reconstructing these planes in some projective frame and back-projecting their common line onto the images. It has recently been shown that the plane at infinity can also be located from scenes with 2 pairs of parallel planes determining two vanishing lines without the need for reconstruction [24]. This is achieved through a linear relationship between parallel scene planes and the plane at infinity. To briefly describe this, consider a 3D scene consisting of two distinct and parallel scene planes Π1 and Π2. Since these two planes are parallel to each other, they meet in a line on the plane at infinity Π∞. As a consequence, the coordinates of the plane at infinity and those of Π1 and Π2 are linearly dependent. Such dependency can be expressed by(6)∏∞∼α1∏1+α2∏2where α1 and α2 are non-zero scalars and Π1 and Π2 are the homogeneous coordinate vectors of the planes Π1 and Π2, respectively.In this section, we discuss the effect of zooming on the camera model and present the main ingredients used in our method for upgrading a projective scene structure and cameras to an affine frame. Consider a camera that is physically fixed in space, e.g. on a tripod, capturing two or more images at different settings of its zoom lens. At any given setting s∈Siof its zoom lens, a camera i is described by its image plane ℐi,sand by its optical center Ci,s(see Fig. 2). The optical center Ci,s, in which all light rays emanating from the scene intersect, is located at a focal distancefi,sfrom the image plane, along the optical axis of the camera. The optical axis of the camera perpendicularly intersects ℐi,sin the principal point ci,s.Under the effect of zooming, the optical center Ci,sundergoes a displacement to a new locationCi,s′at a focal distancefi,s′from the image plane. This repositioning of the lens, carried out by automated zooming hardware or by manual lens change, does affect not only the focal length of the pinhole camera model, but also other parameters. In fact, a change in the configuration of a camera lens due to zooming results in the repositioning of both the optical center and the image plane. In his design of an active model for zoom lenses [19], Willson has carried out a series of experiments in which a pattern-based calibration of a stationary camera is repeated at various zoom settings using several zoom lenses. This was to identify the camera parameters that must be allowed to vary with the zoom versus the ones that can be fixed in the zooming camera model. The results of Willson's experiments show that the optical center is dominantly shifted (possibly by several tens of millimeters) along the Z-axis of the camera (the optical axis) and that its displacements along the X- and Y-axes are small and hence neglected in his model. This does not imply that the position of the principal point remains stable since the image plane is also displaced under the effect of zooming. In particular, the image plane was found to undergo a mostly translational motion which is not necessarily parallel to the Z-axis and hence affecting the position of the principal point. In Willson's model, the displacement of the image plane is represented by the fact that both the optical center and the focal length are allowed to vary independently from one another and by allowing the principal point to shift as well. Note that the goal in Willson's work was to obtain a simple model involving only the most influential parameters.Based on this description, the zooming process incorporates displacing the camera center to a new location whilst preserving camera orientation (i.e. no rotation). This displacement of the camera center in general alters three out of the main five intrinsic parameters: the skew and aspect ratio remain very stable while the principal point and focal length vary. From this point of view, a stationary zooming camera can be viewed as a mechanism for obtaining images from pure translational motion. Obtaining two views from pure translation is hard in practice and requires a high degree of accuracy which might not be achievable except with special equipment and often in a laboratory setup. However, we should note that although an affine reconstruction is possible from two views of a camera with fixed parameters undergoing a pure translation, it is impossible to obtain an affine reconstruction from two views in the case of a zooming camera, i.e. varying intrinsic parameters with pure translation motion constraints as proved in [25]. Furthermore, the modulus constraints cannot be used in the case of zooming or varying camera parameters. The only remaining existing possibility is to rely on scene constrains such as parallel lines and planes. This paper introduces a new method which relies neither on scene constraints nor on explicit motion constraints.The affine self-calibration method we propose relies on less restrictive constraints on the geometry of a zooming camera than Willson's. Indeed, we rely on the fact that the optical center is mostly displaced along the Z-axis but, unlike Willson's model, we allow it to also shift along the X- and Y-axes by any amount. More importantly, we consider the image plane after zooming parallel to the one before zooming. This assumption includes the case in which the image planeundergoes a pure translation (as in Willson's model) but also allows the image plane to rotate around any axis parallel to the Z-axis. Note that our assumptions imply that all the intrinsic parameters are free to vary. Under these assumptions, we achieve our affine self-calibration goal by tracking the motion of the principal plane Φ (introduced in Section 2.3) of the camera which we denote hereafter Φi,s(Fig. 1.), i.e. the plane containing the optical center of camera i at zoom setting s and parallel to the image plane. Because Φi,scontains Ci,s, this plane is also displaced under the effect of zooming to overlapΦi,s′containing the new camera centerCi,s′. Since our assumption is that the image plane after zooming is parallel to the one before zooming, then so are the principal planes before and after zooming. Denoting ϕi,s andϕi,s′the homogeneous coordinate vectors of the principal planes at two distinct zoom settings of a camera i, the two planes represented by these coordinates are parallel if considered in any metric or affine frame and hence intersect the plane at infinity in a line. Although parallelism is not preserved under projective transformations, the linear relationship(7)αi,s,s′ϕi,s+αi,s′,sϕi,s′=∏∞still holds, for some non-zero scalarsαi,s,s′andαi,s′,s, regardless of the frame in which the coordinate vectors ϕi,s,ϕi,s′and ∏∞ may be expressed. Hence, the coordinate vectors of the principal planes may be provided by the last rows of the associated projective camera matrices Pi,s andPi,s′(see Section 2.3) whose calculation only requires feature correspondences across images. When considering a single camera i at two distinct zoom settings, the linear relationship (7) provides four independent equations in six unknowns:αi,s,s′,αi,s′,sand the coordinates of the plane at infinity Π∞. These equations define a one-parameter family of points describing the line on the plane at infinity at which Φi,sandΦi,s′intersect. All principal planes originating from the same camera meet in this line. In order to retrieve the plane at infinity, at least two distinct lines on this plane are necessary. Such lines can be obtained from two or more distinct zooming cameras in general position.In this section, we present and describe our camera self-calibration method for a set of two or more stationary non-rotating zooming cameras. Assuming a consistent set of projective camera matrices has already been recovered from image correspondences, the full camera self-calibration is carried out in three steps. The first step consists in linearly estimating the plane at infinity by employing two or more images captured by each camera at distinct settings of its zoom lens. This step exploits the assumption that the principal planes corresponding to distinct zoom levels of the same camera are parallel to one another. In the second step, an estimate of the intrinsic parameters is linearly calculated by assuming zero-skew and/or known (unit) aspect ratio. Finally, the optimal intrinsic parameters and coordinates of the plane at infinity are obtained through a nonlinear least-squares optimization procedure.The plane at infinity can be identified from at least two distinct lines lying on it. Using the zooming camera model described in the previous section, each zooming camera allows the identification of one such line lying at the intersection of its (parallel) principal planes. Two or more zooming cameras (n≥2), two of which not pointing in the same direction, allow the recovery of the coordinates of the plane at infinity by solving a linear system of Eq. (7) involving all cameras and zoom images. However, when relying on (7), each pair of parallel principal planes introduces two new unknownsαi,s′,sandαi,s′,sfor every given camera. For instance, two cameras, each capturing two zoom images, yield eight linear equations in eight unknowns and suffice in theory to retrieve the plane at infinity. In practice, more than two cameras may be available and each camera may very well capture more than two images at distinct zoom settings of its lens. In such case, in order to cope with image noise, it is highly desirable to employ all cameras and images at hand. However, n≥2 cameras and mi≥2 zoom images captured by camera i, give rise to a system of 4n linear Eq. (7) in4+∑i=1nmimi−1unknowns. Although such linear system can be solved (typically using SVD), the increase in the number of unknowns may render accurately solving such large systems, in particular in the presence of noisy image measurements, difficult to achieve.Fortunately, (7) can be brought to a system of equations solely involving the coordinates of the plane at infinity. This can be achieved by considering that neither the plane at infinity nor any of the principal planes contain the origin of the reference frame. Note that this is always possible either by arbitrarily choosing the world reference frame within the scene or by discarding the principal plane containing the origin of the frame should the latter be attached to one of the cameras. Under this assumption, the coordinate vector of the plane at infinity and that of any given principal plane are of the form ∏∞T=(π∞T1) and ϕi,sT=(ϕi,sT1) where π∞ andϕi,sare 3-vectors. Eq. (7) becomes(8)π∞T1=αi,s,s′ϕi,sT+αi,s′,sϕi,s′T1from which one can easily deduce thatαi,s,s′+αi,s′,s. Note thatαi,s,s′andαi,s′,scan be neither zero nor one since the plane at infinity is distinct from any of the principal planes. As a result, one of the unknown scalars, sayαi,s′,s, can be eliminated by substitution which simplifies the equation to(9)π∞=αi,s,s′ϕi,s−ϕi,s′+ϕi,s′.Let [v]× denote the skew-symmetric matrix induced by the cross-product of some 3-vector v. Since [v]×v=(0,0,0)T, the remaining unknown scalarαi,s,s′can be eliminated by multiplying both sides of (9) byϕi,s−ϕi,s′×which leads toϕi,s−ϕi,s′×π∞=ϕi,s×ϕi,s′,or, equivalently,(10)Mi,s,s′∏∞=000whereMi,s,s′=ϕi,s−ϕi,s′×ϕi,s×Tϕi,s′.For a given camera i and a pair of zoom settings, the rows of the 3×4 matrixMi,s,s′are the coordinate vectors of points lying on the plane at infinity. Note, however, that only two rows are linearly independent and more cameras are required to identify the plane at infinity. Let Mi be themimi−12×4matrix obtained by stacking allMi,s,s′matrices obtained from all pairs of zooming images of camera i. Considering n such cameras, the plane at infinity can be recovered by solving a homogeneous linear system of equations involving all cameras and zoom images:(11)∏∞TM1TM2T…MnT=0,0,0,…0.Retrieving the plane at infinity from (11) may work well for low levels of image noise. However, when using (11), cameras with more zoom images would carry more weight than the rest of the cameras and thus have more influence on the calculation of the plane at infinity. This may be a source of failure if the images obtained from such dominant cameras turn out to be particularly affected by noise. Furthermore, solving (11) allows the retrieval of the plane whose distance to all 3D points (given by the rows of all the Mi matrices) is minimal. However, in the presence of noise, such solution does not take into account the fact that the rows of each matrix Mi must define a line and that the sought plane ought to contain all such lines. Therefore, a more geometrically meaningful solution is to first define the line that best fits the points at infinity defined by each zooming camera (i.e. the rows of the associated Mi) before fitting a plane to those lines. Finding the line that best fits a set of 3D points is an orthogonal regression problem. The best fitting line can be parametrically defined as a set of pointsM¯i+λDicontaining the centroidM¯iT=m¯iT1and following a direction DiT=(diT0) [28]. The centroid can be obtained by re-scaling each row of Mi so its last entry is 1 and averaging the entries in each column of the resulting matrix. Denoting by (mi,rT1) the rthrow of the re-scaled matrix MiT, the di component of the direction of the line corresponds to the first principal component (i.e. the right singular vector associated with the largest singular value) of the matrix formed by stacking the vectorsmi,rT−m¯Tfrom all rows of Mi. Since∏∞TM¯i+λDi=0for all values of the parameter λ, the plane at infinity can then be obtained by solving the linear system of equations(12)∏¥TL=0000…0whereL=M¯1D1M¯2D2…M¯nDn.The plane at infinity corresponds to the right singular vector of the 4×2n matrix L associated with its smallest singular value. Retrieving the plane at infinity through (12) has proved much more accurate in practice and less sensitive to noise than when using (11).Once the plane at infinity is retrieved, the intrinsic parameters of the reference camera can be calculated by solving (5) either using SVD [7] or through Semi-Definite Programming (SDP) employing a Linear Matrix Inequality (LMI) formulation [21]. The advantage of solving (5) as a SDP problem is that, unlike when using SVD, the positive-definiteness of the sought IAC matrix ω can be enforced. Indeed, when using SVD in the presence of image noise, the retrieved ω may not be definite rendering the calculation of the camera parameters impossible. In the experiments presented in the present paper, we have tested both the SVD and the LMI-based SDP approaches.Through solving the linear system of Eq. (5), one would like to calculate all five entries of ω under the zero skew and unit aspect ratio assumptions for all cameras. Although the recovery of the plane at infinity requires two cameras, each capturing at least two images at different zoom settings, the calculation of the IAC requires at least three images captured from distinct viewpoints. We recall that motion sequences containing at most two viewing directions are critical for the no-skew and known aspect ratio case [26]. Note that the images obtained from the same camera at different zoom settings all share a unique viewing direction. Therefore, after the plane at infinity is retrieved, at least three images captured from cameras at different locations in space, and having different viewing directions, are required for the recovery of the reference IAC. In practice, because at least two of the cameras would provide two zoom images for the affine upgrade, at least five images will be available all of which will be used for calculating ω.Because solving (5) using SVD is straightforward and well-known [6], we briefly describe here the LMI-based approach. For instance, assuming n≥3 cameras are available, of which at least two are zooming (mi≥2 for at least two instances of i), the IAC ω of the reference image can be obtained by solving the following SDP:(13)minω,λi,s∑i=1n∑s=1miλi,ss.t.ω≻0,λi,se1THi,s−TωHi,s−1e2e1THi,s−TωHi,s−1e2λi,s≻0,λi,se1THi,s−TωHi,s−1e1−e2THi,s−TωHi,s−1e2e1THi,s−TωHi,s−1e1−e2THi,s−TωHi,s−1e2λi,s≻0.The symbol ≻0 means that the symmetric matrix on the left-hand side is positive definite. Problem (13) is a quasi-convex one that can be solved very efficiently using interior-point methods [27]. From a practical point of view, several solvers, such as SeDuMi (http://sedumi.ie.lehigh.edu/) and Matlab's LMI Control Toolbox, are available. The reader may refer to [21] for more details about this SDP formulation of the problem of retrieving the IAC.As in all camera self-calibration methods, the initial estimate of the plane at infinity and that of the intrinsic parameters need to be refined through a nonlinear optimization procedure as to retrieve the optimal parameters. While several cost functions have been proposed in the literature, the optimal results reported in the present paper have been obtained by minimizing the following objective function:(14)CKπ∞∑i=1n∑s=1mie1THi,s*TωHi,s*e22+e1THi,s*TωHi,s*e1−e2THi,s*TωHi,s*e2Hi,s*TωHi,s*F2where ‖.‖Frefers to the Frobenius norm of a matrix. Again, although at least three images captured from different viewpoints are needed, all available images are to be used in this optimization procedure. Note that, in (14), the matrix inverse Hi,s−1 has been replaced by its equivalent adjoint matrix Hi,s⁎ as to avoid inverting matrices during optimization and to make π∞ appear explicitly. We recall that the inverse of a matrix and its adjoint are related by(15)Hi,s*=detHi,sHi,s−1.The adjoint matrix Hi,s⁎ is defined as the transpose of the matrix of co-factors of Hi,s. It can thus be expressed numerically as well as symbolically. In particular, it has been recently demonstrated in [10] that Hi,s⁎ entries are affine functions of π∞ and is of the form(16)Hi,s*=Pi,sI0T*+π∞×I0Pi,sTpi,s×Twhere pi,s is the last column of Pi,s. It is this expression of Hi,s⁎ that we have employed in our cost function (14).

@&#CONCLUSIONS@&#
