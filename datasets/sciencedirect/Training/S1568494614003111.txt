@&#MAIN-TITLE@&#
Control of a nonlinear liquid level system using a new artificial neural network based reinforcement learning approach

@&#HIGHLIGHTS@&#
A neural network based reinforcement learning control strategy has been proposed.Neural network and reinforcement learning paradigms are synergistically combined.The proposed strategies are tested on two benchmark nonlinear control problem.Simulation results indicate good performance on nonlinear regulation problems.

@&#KEYPHRASES@&#
Reinforcement learning,Neural networks,Nonlinear control,Process control,

@&#ABSTRACT@&#
Most industrial processes exhibit inherent nonlinear characteristics. Hence, classical control strategies which use linearized models are not effective in achieving optimal control. In this paper an Artificial Neural Network (ANN) based reinforcement learning (RL) strategy is proposed for controlling a nonlinear interacting liquid level system. This ANN-RL control strategy takes advantage of the generalization, noise immunity and function approximation capabilities of the ANN and optimal decision making capabilities of the RL approach. Two different ANN-RL approaches for solving a generic nonlinear control problem are proposed and their performances are evaluated by applying them to two benchmark nonlinear liquid level control problems. Comparison of the ANN-RL approach is also made to a discretized state space based pure RL control strategy. Performance comparison on the benchmark nonlinear liquid level control problems indicate that the ANN-RL approach results in better control as evidenced by less oscillations, disturbance rejection and overshoot.system state vectorcontrol action or input to the systemstate transition probabilitiesreward for taking action ‘a’ in state ‘s’policy function or action to be taken in state ‘s’cumulative discounted reward for following policy π starting from state ‘s’optimal policy functionoptimal value function[h1h2h3]T, state vector for the liquid level systeminlet flow rate for the liquid level systemdiscount factor to discount future rewards and favor immediate rewardsnumber of discretization levels used for level variable hinumber of discretization levels used for inlet flow rate Q

@&#INTRODUCTION@&#
Control of liquid level in multiple interacting tanks by adjusting flow rates is a paradigmatic nonlinear control problem that is ubiquitous in many industrial processes. Conventional control strategies like PID control that use approximate linear models do not perform well while undergoing large changes in the operating point. In this paper a machine learning [1–3] based approach that uses a new reinforcement learning strategy to achieve state regulation of a nonlinear system is proposed and applied to a benchmark nonlinear liquid level control problem.Historically reinforcement learning (RL) has been applied in the fields of artificial intelligence and machine learning to solve optimal sequential decision making problems arising in game playing, scheduling and robotics [4–9]. Application of RL to enable autonomous agents to learn to make optimal decisions in real time is explored in [10–13]. Application of RL to a controller scheduling problem is considered in [14]. Application of RL strategies to tune ANN and fuzzy controllers is explored in [15–19]. Recently control of industrial processes using RL strategies was proposed [20–24].Reinforcement learning algorithms solve the very general problem of optimal policy choice in a sequential decision making process. Consider a controller that attempts to control the state ‘s’ of a plant by taking actions ‘a’ that depend on the state. When the controller performs an action ‘a’ on the plant in state ‘s’, it receives a reward R(s,a) that depends in general on both the action and the state. As a result of the action ‘a’ taken by the controller the controlled system transitions to the next state ‘s’ either probabilistically according to some probability distribution Psa(s′) or deterministically according to some state transition law: s′=f(s, a). The optimal policy π*(s) or action sequence is that which maximizes the expected cumulative discounted reward given by Eq. (1):(1)Vπ(s)=E[R(s0)+γR(s1)+γ2R(s2)⋯|s0=s,π]The expected cumulative discounted reward starting at some state s0 is denoted by Vπ(s) since it depends on that initial state and also on the sequence of actions performed in each state π(s). The constant γ in Eq. (1) is chosen in the set [0,1) to favor policies that provide immediate rewards. Also choice of γ ensures convergence of the infinite sequence in Eq. (1). This optimal policy choice problem is a Markov Decision Process (MDP) which is represented by a 5-tuple (S, A, Psa, γ, R) where:S – finite set of states (discretization necessary to deal with continuous state spaces),A – finite set of actions (discretization necessary to deal with continuous actions),Psa– probability distribution of the next state given the current state and action taken (includes deterministic transitions as a special case),γ∈[0, 1) – discount factor to discount rewards obtained in the futureR:S×A→ℝ– reward function.The policy π is a function π:S→A that maps the current state to the action to be taken by the controller. The optimal policy π* is the policy that maximizes the total payoff:π*(s)=maxπVπ(s). Vπ(s) is the expected cumulative discounted reward starting at some state s and following policy π and is known as the value function. The optimal value function is the value function obtained when the optimal policy is executed. The optimal value function satisfies Bellman equations:(2)V*(s)=R(s)+maxa∈Aγ∑s′∈SPsa(s′)V*(s′)The value iteration algorithm computes the optimal value function by iteratively using Eq. (2) starting with an estimate of all zeros. Once the optimal value function is known the optimal policy can be calculated from:(3)π*(s)=argmaxa∈A∑s′∈SPsa(s′)V*(s′)The RL strategy given above works with finite state and action spaces so to apply RL to the liquid level system we discretize the states and actions. Simulation results indicate that the approach proposed in this paper that exploits the generalization ability of artificial neural networks (ANNs) to minimize the effect of state discretization results in less oscillations and overshoot of the liquid level.The implementation of reinforcement learning begins with the definition of Markov Decision Process (MDP), which is a 5-tuple (H, Q1, Phq, γ, R) in the control problem considered.Here, H={[h1(m)h2(n)]T:m=1 to N1 and n=1 to N2}, where, h1 and h2 are the liquid heights in tanks 1 and 2 respectively. The continuous heights are discretized into N1 and N2 levels. Thus the set of states H has size N1×N2.Q1={q1(n):n=1 to Nf} is the set of all possible inlet flow rates to the first tank of the process. The inlet flow rate is taken as the action executed by the controller. This action is discretized into Nflevels.Phq– In the liquid level system the state transitions are deterministic so all probabilities except one are zero. A discretized version of the state space model of the system provided in Eq. (4) was used to find the next state h′∈H based on the current action q1∈Q1 taken and the present state h∈H.(4)dh1dt=(q1−r1h1−r3h1−h2)A1dh2dt=(q2−r2h2+r3h1−h2)A2γ∈[0, 1) – The discount factor used to give different weights to short term and long term rewards. γ was taken to be 0.99 in this paper.R – Reward function, which rewards the controller for being in a state.Possible reward functions for the control problem are given in Eqs. (5) and (6)(5)R(h)=−Chdesired−h(6)R(h)=−1,ifhdesired−h≥δ0,otherwisewhere, h – present state; hdesired – desired state; C>0 – a positive real number.The system starts in some state h(0)=[h1(0) h2(0)]T and the controller takes some action, q1(0)∈Q1. This selected action takes the system to a new state h(1)=[h1(1) h2(1)]T. From this state the controller takes the next action q1(1) and this process continues with successive action till the desired state is reached as given below.(7)h1(0)h2(0)⟶q1(0)h1(1)h2(1)⟶q1(1)h1(2)h2(2)⟶q1(2)⋯Upon visiting the sequence of states h(0), h(1), h(2),… with actions q1(0), q1(1), q1(2),… our total payoff is given by(8)V(h)=R(h(0))+γR(h(1))+γ2R(h(2))+⋯The goal of RL is to choose actions over time so as to maximize the value of payoff. The value function (Vπ(h)) defines the expected sum of discounted rewards the controller will receive upon executing a fixed policy π starting from state h(0) till reaching the desired state hdesired.(9)Vπ(h)=E[R(h(0))+γR(h(1))+γ2R(h(2))+⋯|h(0)=h,π]This relationship can also be represented by a Bellman equation as(10)Vπ(h)=R(h)+γVπ(h′)Here, the first term defines the immediate reward for the controller for being in this starting state. The second term represents the sum of future discounted rewards. This Bellman equation is used for finding the optimal value function for each of the N1×N2 states. The optimal value function, given below, is the value achieved when the optimal policy is followed by the controller.(11)V*(h)=maxπVπ(h)=R(h)+maxq1∈Q1γV*(h′)The policy that optimizes the value function of any state is known as the best policy, which is defined as(12)π*(h)=argmaxq1∈Q1V*(h′)The RL controller selects the best action from π*(h) based on the present state and executes it on the plant. Computation of the optimal value function and the optimal policy is possible through two algorithms, namely the value iteration and policy iteration algorithms. In this work, value iteration algorithm is used to make the controller learn the optimal control policy.The classical RL approach deals primarily with systems with finite state and action spaces. In order to apply RL to control systems with continuous state spaces, each state has to be discretized into finitely many levels. This discretization leads to control errors and is analogous to quantization errors in digital control systems. The effect of this discretization can be reduced by using a finer discretization, but this approach does not scale due to exponential increase in the number of discretized states.Consider an optimal policy that is correct only on a set of discretized states, if such a controller is applied to a system with continuous states the discretization error will lead to poor control performance like oscillations and overshoots around the set point. Effects of discretization on continuous time systems are very similar to the effects of measurement errors since in both cases only an approximate state is available for control.In this paper we alleviate the discretization error problem by exploiting the generalization ability of neural networks to guess the correct control action for continuous states from information available on discretized states. Two ANN based approaches are explored in this paper. In the first approach a neural network is used to directly learn the optimal policy function from the optimal policy function calculated for the discretized states.In the second approach the optimal value function is first learned by a neural network from function values at discretized states. Once the optimal value function on the continuous state space is available as an ANN model, the optimal policy (action to be taken in state ‘s’) can be calculated from Eq. (12). Simulation results indicate that optimal value function is in general smoother and easily approximated compared to the optimal policy function. In Fig. 1below the function F(h1,h2) can be either the optimal value function or the optimal policy function as discussed above.In this algorithm the optimal control actions for the discretized states are computed using the value iteration algorithm and a radial basis function (RBF) neural network is trained with the discretized values. The RBF network will take the continuous state variables as input and output an approximation to the control action (input flow rate). The algorithm for this approach is presented in Table 1.In this algorithm the optimal value function is computed using the value iteration algorithm and a radial basis function (RBF) neural network is trained with the values obtained for the discretized states. The optimal policy function is not learned directly in this approach. The RBF network will take the continuous state variables as input and output an approximation to the optimal value function.When the optimal action to be taken in a given state is needed it can be computed from optimal value function learned by the RBF network using Eq. (12). The algorithm for this approach is presented in Table 2.

@&#CONCLUSIONS@&#
