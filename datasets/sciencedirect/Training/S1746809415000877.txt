@&#MAIN-TITLE@&#
Comparative analysis of strategies for feature extraction and classification in SSVEP BCIs

@&#HIGHLIGHTS@&#
We apply 36 scenarios of signal processing techniques for a same database of EEG-SSVEP.Linear and nonlinear classifiers are analyzed, as LDA, SVM and ELM.Analysis of different spectral features extraction techniques.Comparison of distinct features selection strategies.Statistical considerations about electrodes location.

@&#KEYPHRASES@&#


@&#ABSTRACT@&#
Brain–computer interface (BCI) systems based on electroencephalography have been increasingly used in different contexts, engendering applications from entertainment to rehabilitation in a non-invasive framework. In this study, we perform a comparative analysis of different signal processing techniques for each BCI system stage concerning steady state visually evoked potentials (SSVEP), which includes: (1) feature extraction performed by different spectral methods (bank of filters, Welch's method and the magnitude of the short-time Fourier transform); (2) feature selection by means of an incremental wrapper, a filter using Pearson's method and a cluster measure based on the Davies–Bouldin index, in addition to a scenario with no selection strategy; (3) classification schemes using linear discriminant analysis (LDA), support vector machines (SVM) and extreme learning machines (ELM). The combination of such methodologies leads to a representative and helpful comparative overview of robustness and efficiency of classical strategies, in addition to the characterization of a relatively new classification approach (defined by ELM) applied to the BCI-SSVEP systems.

@&#INTRODUCTION@&#
A Brain–computer interface (BCI) is a device that aims to map brain signals onto commands for external devices, defining an alternative communication channel for users in different practical contexts, which can include applications from computer games to assistive technologies [1,2].BCIs, in general, make use of electroencephalography (EEG) [3], as a consequence of factors like portability, non-invasiveness and cost. EEG signals are acquired with the aid of an electrode cap positioned on the user's scalp, which is connected to pre-processing and sampling modules. The design of a BCI is determined by the chosen paradigm, the main trends in the field [4] being motor imagery, P300 and steady state visually evoked potentials (SSVEP). The last two are approaches based on event-related potentials (ERP). The first of these paradigms relies on the ability of the operator in modifying – by imagining the process of moving parts of both sides of his/her body (e.g. opening or closing the right or the left hand) – the activity of the motor cortex [5], while the second makes use of a specific event-related potential, the P300 wave, to characterize the interaction between the operator and a command interface [6]. Finally, the SSVEP paradigm, the subject of this study, is based on the analysis of oscillating EEG patterns that are generated in the cortex in response to certain visual stimuli. More specifically, when an individual is visually stimulated by a pattern that flickers repetitively within a certain range of frequencies, a synchronized SSVEP can be detected in his/her brain electrical activity. Hence, if light sources with different flickering rates are used to build a command interface, it is possible to identify on which light the subject focused his/her attention at a given period of time by suitably processing and classifying the EEG signal.In general, the structure of an SSVEP-based BCI can be roughly divided into four stages: data acquisition, signal processing, command generation and final application [7]. Fig. 1shows a block diagram of this structure highlighting the four stages of the signal processing module, which is the focus of this study. The first stage, pre-processing, is based on temporal and spatial filtering and is typically of a more general character. The second and third stages, on the other hand, have a stronger dependence with respect to the features of the selected paradigm. The classifier stage generates the control command based on input signal.In this study, we will perform a comparative analysis of methods for feature extraction, feature selection and classification in SSVEP BCIs. Three feature extraction approaches—spectral estimation using a bank of band-pass filters, Welch's method and the magnitude of the short-time Fourier transform (STFT) calculated at the evoked frequencies, three features selection – and three classifiers – a linear discriminant, an extreme learning machine (ELM) [8] and a support vector machine (SVM) [9] – will be considered. Furthermore, the performance of each structure will be analyzed under three feature selection approaches: an incremental wrapper [10], a filter using Pearson's method [11] and a strategy based on the Davies–Bouldin index [12], in addition to a case without feature selection. This repertoire of 36 scenarios applied on the same database defines interesting comparative elements: (1) since SSVEP engenders a well-defined spectral response, this study is relevant as a performance analysis of distinct frequency-domain feature extraction methods. (2) The robustness of nonlinear structures, as ELM and SVM, in handling the required SSVEP classification task is investigated. (3) The process of channel selection is analyzed adopting three strategies with distinct conceptual foundations. (4) Statistical considerations are made about the best configuration of electrodes according to different methods of feature selection.This study will be carried out using a database generated according to the experimental setup described in Section 3. In addition to the contribution that the study as a whole represents, we believe the analysis of the performance of an ELM in SSVEP systems can also be considered as a contribution per se, as an equivalent analysis, to the best of our knowledge, has not been reported so far in the literature.The remainder of this paper is organized as follows. Section 2 presents briefly the SSVEP paradigm. Section 3 describes the experimental setup and procedures of data recorder. Sections 4–7 discuss the four stages of signal processing, i.e., pre-processing, feature extraction approaches, features selection and the classification criteria, respectively. Section 8 presents the results, while Section 9 contains our conclusions and final remarks.The neurophysiology of the human visual system reports that the neuronal activity of the cells of the visual cortex is altered by visual stimulation, and it is possible identify variations of the brain response related to properties of the visual stimulus, such as luminance, contrast and frequency (between 1Hz and 100Hz [13]). Neurons in visual cortex synchronize their firing to the frequency of blinking of visual stimulus. The steady state visually evoked potentials occur when visual stimuli are presented repeatedly creating almost sinusoidal oscillations [14,15]. The EEG response presents an increase of energy in the same frequency of the blinking stimulus [16]. The strongest response occurs in the primary visual cortex, although other areas of the brain are activated in varying degrees. The SSVEP can be detected within narrow frequency bands (e.g., 0.1Hz) around the frequency of visual stimulation via signal processing methods that exploit specific characteristics of the signal, such as timing and rhythm.The SSVEP BCI systems use visual stimuli as a way to evoke a certain electrical pattern in the visual cortex. Unlike independent BCI systems, where the implementation is based on voluntary control of neural activity of the subject [17,18], the operation of SSVEP systems depends on the ability of the subject to focus on, fix and follow the visual stimuli according to an intended action, as also on the adopted signal processing strategies, which justifies the extensive scenarios analyzed in the present study.The stimulation interface (see Fig. 2) consists of two square checkerboards with sides of 3.8cm, displayed on the right and left centers of a black screen, blinking at 12 and 15Hz, respectively. A 14-in. monitor with refresh rate of 60Hz was used. The subject focused his/her gaze for 12s on each stimulus, repeating this process eight times with rest intervals. The EEG data were collected from seven healthy volunteers, with an average age of 26.3±3.3 years. The acquisition protocol was approved by the Ethics Committee of the University of Campinas (n. 791/2010). The database is composed of 1344s of EEG data recorded at a sample rate of 256Hz, using a g®.SAHARAsys dry-electrode cap with 16 channels and a g®.USBamp biosignal amplifier [19], and registered at the MATLAB® 2012b, using an Application Programming Interface (API) provided by the aforementioned device manufacturer. The acquisitions were only performed after the following proceedings regarding the EEG apparatus: channel impedance calibration; verification of the impedance electrode calibration (between 0.5 and 5.0kΩ); connection of the ground and reference channel sockets, respectively, to common ground and reference; and stabilization of the signal. The ground and reference are positioned on mastoids. Fig. 3shows the arrangement of electrodes at O1, O2, Oz, POz, Pz, PO4, PO3, PO8, PO7, P2, P1, Cz, C1, C2, CPz, FCz, according to the international 10–20 system [20].Several interferents are added to the EEG signal during the recording. These artifacts compromise the quality of the obtained signal, affecting the BCI performance. The main artifact sources are: EEG equipment and its connections to the scalp; electrical source (60Hz); the normal electrical activity of the subject as heart, eye blinking, eyes movement and muscles in general. Recognition and elimination of artifacts in EEG signals are complex tasks, but essential to the development of practical systems.In this study, the EEG signal was filtered by an analog Butterworth bandpass filter (5–60Hz) and a notch filter (58–62Hz) in order to remove the smooth displacement and electromagnetic artifacts. In the sequence, to remove other artifacts present in the band of, as eye blinking and neck movements, data are submitted to a spatial filtering using the Common Average Reference (CAR) [21] method, defined as:(1)ViCAR=ViER−1n∑j=1nVjERwhereVjERis the potential of i-th electrode measurement with respect to same reference and n is the number of electrodes in the array. The CAR uses the average value of the entire array to subtract this mean from each electrode, hence eliminating similar artifacts present in most electrodes. Although noise sources are deeply complex and vary across and within subjects, the temporal and spatial filtering have been demonstrated to be convenient to maximize the signal-to-noise ratio and to improve the accuracy of the SSVEP BCI system [22,23].Features are, in simple terms, elements of a compact and efficient data representation [24]. In the context of a BCI system, it is essential that the features extracted from the brain signals facilitate the discrimination task to be performed at the classification stage. As discussed in Section 1, the SSVEP paradigm is based on the detection of oscillating patterns within EEG waves, hence the use of spectral features is a natural choice [25]. Fig. 4shows the spectral characteristics of the SSVEP responses observed on channel O2 for the evoked frequencies 12 and 15Hz. It is noticeable that the spectral content is concentrated around the evoked frequencies.In fact, the standard technique for identifying the SSVEP response associated with an EEG signal is to analyze the signal in the frequency domain by calculating its power spectral density in all possibly evoked frequency bands. As each of these bands corresponds to the immediate vicinity of one of the interface blink rates, it is possible identify the desired BCI command. In this study, the underlying spectral content was estimated using three approaches: a filter bank, the short-time Fourier transform and Welch's method.An intuitive way to estimate the spectral power of an SSVEP signal is to focus on the frequency range of interest to assess the spectral content of this interval. The filter bank uses this idea combining a set of bandpass filters that separates the input signal into multiple components [26], each one carrying a single frequency sub-band of the original signal, as shown in Fig. 5.In our study, the filter bank is designed with two equiripple bandpass filters centered at the evoked frequencies, with 2Hz bandwidth, attenuation of 40dB in the stop band and 1Hz of transition range (see Fig. 6). The output power of the elements of the bank is considered as an estimate of the power spectrum at the central frequencies.The short-time Fourier transform allows the estimation of the power spectrum via the computation of the Fourier transform on segments of the signal, normally with an overlap to reduce artifacts at the boundary [26]. The obtained complex values provide information concerning the magnitude and phase of each point in time and frequency. The STFT is given by(2)X(m,ω)=∑n=−∞∞xnwn−mexp−jωnin which x[n] is the signal, w[n] is the window, m is the segment length and ω is the angular frequency. The squared magnitude of the STFT is given by the spectrogram as:(3)spectrogram≡X(m,ω)2and provides an estimate of the power spectrum of the signal.In our study, the spectrogram is computed around the two evoked frequencies (12 and 15Hz), using Hamming windows of 3s with 1s of overlap.Welch's method estimates the power spectral density (PSD) applying the fast Fourier transform (FFT) algorithm [26,27]. The method splits the input data into N segments, computes modified periodograms of segments via FFT and estimates the PSD by the average of the resulting periodograms. The mathematical formulation of the PSD can be expressed by(4)Sˆ(ω)=1KNU∑k=1K∑k=1KW(n)x(n+kD)exp(−jωn)2in which the signal is divided into K segments of length N and shifted of D points. W is a window function and U is a constant given by:(5)U=1N∑n=1NW(n)2In the present study, the data was windowed by Hamming windows with 3s and 1s of overlap. The PSD was estimated for each visual stimulus using 1Hz bands centered on frequencies of 12 and 15Hz and with a step of 0.01Hz.The amount of features available to design a classification system is usually large, when compared to the restricted number of features required to ensure suitable generalization properties of the classifier, reasonable computational complexity and processing time.In order to find the most relevant features for designing the classification system, feature selection is usually applied. This technique exploits the mutual (linear and/or nonlinear) correlation among features selecting those that retains more class discriminatory information. Strategies for performing this selection follow two approaches: filters or wrappers [10,11]. The first uses statistical measures to quantify the relevance of each feature and are probably the simplest techniques to operate on the feature space [11,28]. Filters operate with metrics directly obtained from features, being, therefore, independent of the classifier to perform the choice. The filters usually outline statistic functions that return a relevance index matching each attribute and label. This approach tacitly assumes independence between features and, therefore, ignores the correlation between variables, which can affect the prediction performance. The second approach takes into account the performance of the trained classifier to rank the features. In the following, two filter techniques are described – Pearson and Davies Bouldin –, as well as the forward wrapper algorithm used in this study.The Pearson correlation coefficient [28,29] defines a kind of filter strategy in which an input vector xiis associated with a feature and its label y in the form:(6)Ri=cov(xi,y)var(xi)var(y)being cov(.) is the covariance and var(.) is the variance.This strategy firstly evaluates Rifor i=1, …, M, being M the number of attributes, and, afterwards, ranks the K features using the criterion of maximum values of Ri. As correlation defines a second-order statistical measure, this coefficient is able to capture only linear dependency between the features. However, due to its computational simplicity, it can be suitably used as a basic metric to understand the feature space.The Davies Bouldin (DB) index is a cluster measure that attempts to quantify the separability of of different classes considering two main relevant aspects of data clustering: the minimization of the distance within a class and the maximization of the distance between the classes. For classes wiwith i=1, 2, …,m, the DB index can be described by the ratio:(7)DB=1m∑i=1mmaxj=1,…,mj≠1si+sjdijin which siis the average distance between each point of the class i and the centroid of this class, and sjis the same for the class j. The parameter dijis the Euclidean distance between the centroids of classes i and j.Taking Fig. 4b as an example of a two-dimensional attribute space, it is not difficult to realize that a low class dispersion with far apart centroids contributes to a desirable separable configuration, which implies in small DB values and in an interesting ranking measure. In this case, the inverse of this index (DBinv) was used to in order to seek the best channels (electrodes) at stimulation frequencies, and, consequently, to define the feature vector. A detailed description of the DB index can be found in [12].The wrapper methodology [10,11] performs feature selection in terms of the performance of the classifier. In simple terms, there are three aspects to define its implementation [10]: (i) the search strategy employed at the feature space, (ii) the stopping criterion and (iii) the classifier structure.The first step relies on performing an efficient search on the feature space due to the large number of possibilities in order of 2M−1, being M the number of features. There are many possibilities to realize such search as genetic algorithms, simulated annealing or greedy heuristics. In the study, the greedy heuristic based on forward selection was chosen, once it is supposed that the attributes are better correlated by a progressive incorporation. The simplest stopping criterion consists of the rule “if no improvement, so stop”. This approach can, however, lead to local convergence. A more robust stopping criterion considers k consecutive steps without performance gain. In this study k=2 was adopted. The third aspect, the classifier structure, has a strong influence on feature selection, since the performance of classifier is constantly evaluated, as described in the algorithm presented on Table 1. It is important to note that wrappers do not guarantee global convergence.The classifier structure is responsible for mapping each input feature vector onto a label corresponding to an element of a discrete set of classes. In simple terms, the mapping performed by a classifier can be understood as engendering a set of partitions of the input space that are delimited by decision boundaries [28,29]. Classifiers can be either linear or nonlinear, depending on the nature of the performed mapping. In the following, we will discuss three classifiers that are interesting options in the BCI context, and shall be, accordingly, adopted for further analysis.The LDA is one of the most used strategies in BCIs systems due to its simplicity and low computational cost. In simple terms, it consists in finding the linear combinationwthat better separate the classes, which implies in establishing a decision surface in the formwTx+c=0, for a constant threshold value c. For instance, if we assume two normal multivariate distributions with meansμ1 andμ2 and correlation matrices C1 and C2, respectively, the LDA approach aims to establishwthat maximize the ratio between the inter-class and intra-class variance, which can mathematically described by:(8)S=σbetween2σwithin2=(wT(μ1−μ2))2wT(C1−C2)wIt is possible to show that maximization of S is satisfied forw∝(C1+C2)−1(μ1+μ2)andc=1/2wT(μ1+μ2)[28]. There are also different criteria than can be used to setwfor obtaining linear decision surfaces, as the one provided by support vector machines strategies with linear kernel functions. When a Gaussian distribution is assumed, the covariance and the mean fully describe the model. However, non-Gaussian random variables can be assumed in this model, as the use of their statistical structure up to second order might be enough to solve the problem at hand.Structurally, an ELM can be defined as a multilayer perceptron neural network with a single hidden layer and a linear output layer (see Fig. 7). The parameters of the neurons that form the hidden layer are randomly chosen [8], and the process of training the output layer is essentially equivalent to the adaptation of a linear classifier. The choice of the number of neurons in the intermediate layer can be made by cross-validation methods.The model evokes elements of biological neuron operation—input data are weighted representing the synaptic efficiency and the activation function determines the firing (returns output +1) or the absence of firing (output returns −1) of the neuron. A typical activation function is the hyperbolic tangent, which presents exactly a nonlinearity of this kind.In simple terms, the hidden layer generates a number of nonlinear random projections that map the input vector space onto a feature space over which the output layer operates as a linear regressor. The canonical approach is to use the method of least squares, presented in Section 7.3. The ELM is an interesting option in the context of BCI in view of the simplicity of its associated training process and of its inherent regularization properties [30,31].In our analyses, the number of neurons in the hidden layer of the ELM was fixed at 20 after preliminary tests. The hyperbolic tangent was used as activation function. The weights of hidden layer were generated using a random Gaussian function. The performance of ELM was defined in terms of the average of 20 runs for each subject to account for the random character of the network.The method of least squares is often used in regression analysis. In this study, the least squares were used in two approaches of classification methods: the LDA and the output layer of the ELM.Considering that in a classifier problem we have a set of N samples labeled for training and the vector of the output layer weights isw, the main criterion underlying such strategy is the following:(9)minw||Hw−d||2beingHis the feature matrix,dthe label vector used to train the classifier andwthe weight vector. The solution to this problem can be calculated as a projection of the label vectordcarried out with the aid of an operator based on the Moore–Penrose pseudo-inverse [28]. In the case of an ELM, if the number of neurons in the hidden layer (M) is larger than the number of available data samples, there will be multiple optimal solutions to the problem shown in (9), and the pseudo-inverse has the desirable property – from a regularization perspective – of generating a minimal norm solution. In this study, as already mentioned, the value of M was chosen in accordance with a cross-validation criterion. For the case in which the number of data samples (N) is larger than M, the solution is:(10)w=(HTH)−1HTdIf M>N, the solution is given by:(11)w=HT(H⋅HT)−1dIf M=N,wis the same for both equations once the matrixHbecomes square.The SVM [9] is a learning structure that can be used to solve classification and regression tasks. In the context of classification, it can be understood as a maximal margin classifier whose linear/nonlinear structure is defined by a kernel function. The design of a classifier of this kind gives rise to a quadratic constrained optimization task that can be solved using a number of efficient computational tools. In a classification system, the SVM follows two stages: training and classification.In the training, labeled data are used in order to determine the hyperplane in a high-dimensional feature space that distinguish the classes with maximal margin. In practice, the training can be performed in the original data space using different kernel functions, as linear, quadratic, polynomial, multilayer perceptron (MLP) or Gaussian radial basis (RBF) [32]. In this study, the MLP kernel was selected after preliminary tests with all the methods, in view of its stability for multiple trials. The MLP kernel is defined as:(12)k(x,xi)=tanh(P1xiTx+P2)where xiis the input data and the kernels parameters were P1=1 and P2=−1.The machines found in the training phase are then used to classify new data on the classification stage.

@&#CONCLUSIONS@&#
The results revealed that, for the two-class SSVEP problem, the best structure was the linear classifier using the Welch method for feature extraction and incremental wrappers to carry out feature selection. This configuration obtained average accuracy around 95%, with windowing of 3s, for the 7 subjects, reaching 100% for some, which is very satisfactory. The feature extraction techniques showed to be equivalent to estimate the spectral power. The Welch and the STFT methods presented a similar performance and a slightly better performance (6%, approximately) was attained using filter banks, although this seems to be within the margin of error of the subjects. Feature selection proved itself to be an extremely important step, indicating the presence of relevant information in the parietal, motor and central zones, in addition to the occipital lobe. The results show that the three classifiers can be efficiently used to build an SSVEP-based BCI. However, the SVM classifier is very sensitive to the feature selection strategy, especially when associated with filter bank feature extracting. The ELMs are promising classifiers in the context of SSVEP, deserving to be considered as part of the current repertoire of BCI system classifiers, as they exhibit a good generalization performance. The obtained results support the use of ELMs, which can be even more efficient and promising when more classes are considered.