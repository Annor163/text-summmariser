@&#MAIN-TITLE@&#
Multivariate versus univariate Kriging metamodels for multi-response simulation models

@&#HIGHLIGHTS@&#
Applying nonseparable dependence models ensures positive-definite covariance matrix.We develop a Monte Carlo laboratory that satisfies all Kriging assumptions.We find that simple univariate Kriging has smaller MSE, so it is more practical.With known parameters, multivariate Kriging is not better than univariate Kriging.

@&#KEYPHRASES@&#
Simulation,Stochastic processes,Multivariate statistics,

@&#ABSTRACT@&#
To analyze the input/output behavior of simulation models with multiple responses, we may apply either univariate or multivariate Kriging (Gaussian process) metamodels. In multivariate Kriging we face a major problem: the covariance matrix of all responses should remain positive-definite; we therefore use the recently proposed “nonseparable dependence” model. To evaluate the performance of univariate and multivariate Kriging, we perform several Monte Carlo experiments that simulate Gaussian processes. These Monte Carlo results suggest that the simpler univariate Kriging gives smaller mean square error.

@&#INTRODUCTION@&#
In operations research (OR) practice, simulation is often applied. Simulation may be either deterministic or random (stochastic). Applications of deterministic simulation abound in engineering such as computer aided engineering (CAE), but there are also applications in OR as demonstrated by the following two examples. Example 1 concerns the management of fisheries at the French Research Institute for Exploitation of the Sea (IFREMER); see Mahevas and Pelletier (2004). Example 2 is the case study on the CO2 greenhouse effect by Kleijnen, Van Ham, and Rotmans (1992). Applications of random simulation are plentiful in OR, especially in queueing and inventory management; see the references in Kleijnen (2008, pp. 3–6).Kriging model may be used to analyze the input/output (I/O) behavior of a given simulation model; this analysis may serve validation, sensitivity analysis, and optimization, as discussed by Kleijnen (2008). This Kriging gives a metamodel; i.e., it approximates the I/O function defined by the underlying simulation model. There are different types of metamodels; most popular is a polynomial of either first or second order; see Kleijnen (2008). We, however, focus on Kriging, which has already become popular in engineering and is gaining popularity in OR; see the many references in Chen, Ankenman, and Nelson (2012) and Kleijnen (2008). Most of this Kriging literature, however, ignores multivariate Kriging; also see our literature summary below.In practice, a given simulation model has multiple outputs—also called responses or performance criteria. For example, Kleijnen (1993) discusses a case study on the production planning of steel tubes of different types, using a simulation model with 28 outputs which—after a discussion with management—were reduced to two outputs. Kleijnen and Smits (2003) discusses multiple performance metrics in supply chain management. The literature on metamodels, however, often reduces these multiple outputs to a single output—either ignoring all the other outputs or combining all outputs through a weighting function; in our Monte Carlo experiments (detailed In Section 4) we shall briefly discuss results for the sum and the product of two outputs. Other publications present metamodels per individual output ignoring the correlations between outputs; e.g., Kleijnen, Van Beers, and Van Nieuwenhuyse (2010) fit univariate Kriging models for each of the two outputs—namely, cost and service—of a call-center simulation. In all our Monte Carlo experiments we also apply such univariate Kriging—besides multivariate Kriging.Intuitively, it may seem that multivariate Kriging gives a lower mean squared error (MSE) than univariate Kriging, because the former accounts for the cross-correlations between different output types, whereas the latter accounts only for the auto-correlations between outputs of the same type for different input combinations—as we shall explain in Sections 2 and 3. However we think this intuition may be misleading. In practice the Kriging parameters are unknown so they must be estimated, which increases the MSE; multivariate Kriging requires the estimation of additional parameters—namely, the cross-correlations—which further increases the MSE. Note that Hernandez and Grover (2013) also use the MSE criterion in their article on Kriging.To empirically compare univariate and multivariate Kriging, we use Monte Carlo experiments that guarantee the validity of the Kriging metamodel. The literature usually experiments with realistic simulation models, but these experiments imply approximation errors (bias) of the Kriging metamodels. Moreover, these simulation models may be computationally expensive. We limit our investigation to Kriging in deterministic simulation, which is also the basis for Kriging in stochastic simulation.Furthermore, we limit our first Monte Carlo experiments to situations with a single input and two outputs. Many OR problems do have a single input; examples are queueing simulation models with the traffic rate as the single input and inventory models such as the newsvendor problem with the order quantity as the single input. Moreover, Kriging in simulation usually assumes that in case of multiple inputs the correlation function is the product of the correlation functions per individual input; see Eq. (2). In this example we limit the number of multiple outputs to two; in case of more outputs, the cross-correlations are correlations between all pairs of outputs. We do vary the magnitudes of the cross-correlation between the two outputs. In the second example we base our Monte Carlo experiment on a climate simulation with five inputs and three outputs.To provide some background for our study, we summarize the rather limited number of publications that explicitly discuss multiple outputs. This literature assumes different types of multivariate models; we distinguish the following three types:1.In practice, simulation models may have (say) n types of output; each type is a specific transformation of the same input combination and the same pseudorandom number stream; in deterministic simulation, this stream vanishes. Software (such as Arena) for building and running discrete-event simulation models permits the automatic collection of multiple outputs. Not only simulation may give multiple outputs; real-life systems may too. This type is the focus of our study.A given real system may be represented by n different simulation models with different degrees of realism (detail); so-called multi-fidelity simulation. We claim that this situation is extremely rare in OR. The simulation model with few details is run for many input combinations, whereas the detailed type is run for fewer combinations. Obviously, the most detailed simulation is the real system itself. See Santner, Williams, and Notz (2003), Forrester, Sobester, and Keane (2008), Goh et al. (2013), Tuo, Wu, and Yu (2013), and also “partially heterotropic” situations in Wackernagel (2003, p. 158).Besides the output of interest, the modelers collect information on the gradient of this output. In discrete-event simulation, this type is rare, because the estimation of this gradient is complicated (it typically uses either “perturbation analysis” or the “score function” method). An example is Chen, Ankenman, and Nelson (2013). Obviously, the output and its gradient are estimated for the same input combinations.For type-1 real-life systems, Cressie (1991, pp. 138–142) speaks of cokriging in his book on spatial data analysis. Wackernagel (2003, pp. 143–209) also discusses geostatistics, so he restricts the input data to one, two, or three dimensions (whereas simulation implies an arbitrary number of dimensions). Gneiting, Kleiber, and Schlather (2010) also discuss cokriging in geostatistics assuming so-called Matérn correlation functions. Santner et al. (2003, pp. 101–116) do discuss simulation or computer experiments, assume type-3 simulations. Higdon, Gattiker, Williams, and Rightley (2008) discuss the combination of real-life “field data” and simulation data, where both types of data concern the same real-life system so it concerns type-2 situations; they allow for very many types of output. Forrester (2010) also discusses type-2 situations; i.e., the combination of (i) scarce and expensive real-life data with abundant and inexpensive simulation data, or (ii) scarce and expensive data from a detailed simulation model with abundant and inexpensive data from a quick-and-dirty simulation model. Williams, Santner, Notz, and Lehman (2010) discuss multivariate Kriging in constrained optimization in simulation with multiple outputs—but they follow Santner et al. (2003). Altogether we recommend Santner et al. (2003) and Wackernagel (2003) for an introduction to multivariate Kriging. Note that Li, Azarm, Farhang-Mehr, and Diaz (2006) also recognize that in practice simulation models have multiple outputs and that Kriging is an important type of metamodel, but those authors use a completely different approach (they do not use cokriging with estimated cross-correlations).Besides the areas of operations research (our focus), geostatistics, and engineering there is another area with major contributions to Kriging or Gaussian process (GP); namely machine learning; see Rasmussen and Williams (2005). Multivariate GPs are investigated in machine learning in multi-task learning, multi-sensor networks or structured output data. To obtain positive definite (PD) covariance matrixes, this community uses either so-called separable models or nonseparable models. The nonseparable models are based on either convolution method or the linear model of coregionalization (LMC). We define these different models in Section 3. Separable models for multi-task learning are used by Bonilla, Chai, and Williams (2007). Álvarez, Rosasco, and Lawrence (2011) show how several models in machine learning are special cases of LMC. In LMC, they use Cholesky’s decomposition of the cross-covariance matrix to construct a PD covariance matrix, and they show that the convolution method gives lower standardized mean square errors than LMC. Fricker, Oakley, and Urban (2010) present an LMC variant that uses eigendecomposition of the cross-covariance matrix to construct a PD covariance matrix. They show that their LMC variant gives a lower mean squared error than the convolution method. The convolution method is introduced to this community by Boyle and Frean (2005). The main disadvantage of this method are the computational and storage requirements. Álvarez and Lawrence (2011) propose a more efficient approximation for multivariate GPs constructed through the convolution method. This method does not spend much effort on accurate modeling of cross-covariance. To improve the accuracy in convolution method, more parameters are needed; Fricker et al. (2010) propose a new method that introduces such parameters. Note that Constantinescu and Anitescu (2013) specify the covariance matrixes imposing constraints originating from the physics laws that determine relationships among the outputs of their application; in OR, however, such knowledge is usually not available.We summarize our article as follows. We consider multivariate Kriging model constructed through LMC which is proposed by Fricker et al. (2010). Furthermore, we interpret this novel Kriging model. We also present Monte Carlo results for the performance of this multivariate model and univariate Kriging per output. Using this Monte Carlo laboratory, we confirm previous results showing that multivariate Kriging does not provide improvements compared with univariate Kriging—even under ideal conditions. Svenson and Santner (2010) use Fricker et al. (2010)’s LMC for their multi-objective optimization problem; unlike we, they do not compare univariate and multivariate Kriging. Fricker et al. (2010) find that univariate Kriging always gives smaller RMSEs than multivariate Kriging. Fricker et al. (2010) suggest that if the output is a function of other outputs, then multivariate Kriging outperform univariate Kriging. We use the data in Fricker et al. (2010) only to select the parameters in our Monte Carlo experiment with a multivariate Kriging metamodel that has no specification errors; i.e., their Kriging metamodel is only an approximation of the true I/O function of their underlying simulation model, whereas multivariate Kriging in our Monte Carlo laboratory gives a metamodel without any bias. So, instead of selecting some arbitrary data that might accidentally favor or “bias” our methodology, we base our experiments on Fricker et al. (2010)’s data. Note that in an Appendix we give details, including several statistical tests for verifying the correctness of Monte Carlo experiments with Kriging; such tests are necessary because computer codes may contain unintended programming errors and peers should be enabled to reproduce results.We organize the remainder of this article as follows. Section 2 summarizes the basics of univariate Kriging, including references to computational issues. Section 3 extends this Kriging to multivariate Kriging with nonseparable dependence structure, including technical details. Section 4 describes our Monte Carlo laboratory with GP models so the Kriging assumptions are guaranteed and we can use this laboratory to empirically compare univariate and multivariate Kriging. Section 5 presents conclusions and topics for future research. The references at the end of this article enable the reader to study more aspects of this challenging topic.The various disciplines that apply Kriging, use different terminologies. We have already observed that geostatisticians speak of “sites”, whereas simulationists speak of “points” or “combinations”. In machine learning, the “old” points are called the “training set”. Simulationists use correlation functions, whereas geostatisticians use the related concept of variograms.Our notation remains close to the notation in DACE—the free univariate MATLAB Kriging toolbox developed and well-documented by Lophaven, Nielsen, and Sondergaard (2002), assuming deterministic simulation. For the reader’s convenience, Appendix A includes Table A.1, summarizing the symbols used. Note that alternative free software is mentioned by Frazier (2010), Kleijnen (2008, p. 146), Roustant, Ginsbourger, and Deville (2012). Commercial software called JMP is offered by SAS. Several authors present a Bayesian interpretation of the Kriging model, but we follow a frequentist approach.Suppose the given simulation model is run for m combinations of the k simulation inputsx=(x1,…,xk)⊤. These combinations are also called “locations” or “scenarios”; Lophaven et al. (2002) call them “sites”, which stems from the origin of Kriging; namely, geostatistics (Daniel Krige was a mining engineer in South Africa). Simulating these m input combinations gives the outputsy=(y1,…,ym)⊤.Like most authors on Kriging in simulation, we assume Ordinary Kriging:(1)y=μ+z,whereμdenotes the mean output, and z a stationary GP with zero mean. Because z is stationary, z has a constant—but unknown—varianceσz2, and its covariancesci;i′(i,i′=1,…,m)between the outputs of the input combinationsxiandxi′are determined by the distance betweenxi=(xi;1,…,xi;k)⊤andxi′in the k-dimensional input space (we use the symbolci;i′instead ofσi;i′because in multivariate Krigingσg;g′refers to the covariances between the outputs of type g andg′). Kriging of simulation models with their possibly high-dimensional input space assumes that these covariances are the products of the k individual correlation functions; e.g., a so-called Gaussian correlation function implies(2)ci;i′=σz2Πj=1kexp[-θj(xj;i-xj;i′)2],whereθj⩾0measures the importance of inputj;xj;iis the ith entry of the jth simulation input; andxj;i-xj;i′measures the distance in the input dimension j between the combinations i andi′. Note that ifθj=0, then changes in input j have no effect at all on the covarianceci;i′. Ifθj=∞, then the covarianceci;i′reduces to zero, so the outputs at the locations i andi′are independent. The covariancesci;i′are gathered in the symmetric and positive-definitem×mcovariance matrixΣ, and the corresponding correlationsci;i′/σz2are collected inRsoΣ=σz2R. The two extreme values for the correlation coefficient (θj=0orθj=∞) give a singular covariance matrix, because this matrix has identical columns. Note that Simpson, Poplinski, Koch, and Allen (2001) claim that Ordinary Kriging defined in (1) with a Gaussian correlation function defined in (2) is the most common Kriging model in engineering.The classic Kriging predictor assumes known (hyper) parametersμandΣ. Requiring the predictor to be linear and unbiased and using the mean squared prediction error (MSPE) criterion, the best linear unbiased predictor (BLUP) for the outputy0ofx0is(3)y0^=μ+c0⊤Σ-1(y-μ1),where1denotes the m-dimensional vector with ones;c0=(c0;1,…,c0;m)⊤the vector with the covariances between the outputs at the new and the m old input combinations (soΣ=(c1,…,cm)with vectorsci=(ci;1,…,ci;m)⊤); and(y-μ1)the vector with residuals. Note thatx0denotes the “new” input combination; an alternative notation replaces the subscript 0 bym+1. Ifx0is actually one of the old pointsxi(i=1,…,m), then the predictoryi^equals the observed outputyi; i.e., Kriging gives an exact interpolator.In practice, however, the parametersμandΣare unknown, so a major problem is their estimation. Note thatΣincludes the varianceσz2on its main diagonal and the k parametersθjassuming the Gaussian correlation function (2). Santner et al. (2003) use maximum likelihood estimation (MLE). Because z in (1) follows a GP, the density function (say) f ofyis(4)f(y)=1(2π)m/2(Σ)1/2exp-12(y-μ1)⊤Σ-1(y-μ1),whereΣdenotes the determinant ofΣ. This density function is denoted byNm(μ,Σ)withμ=μ1.MLE minimizes the log-likelihood functionl(Σ,μ|y), while ignoring terms that do not depend on the parametersμandΣto be estimated; i.e., (4) implies(5)l(Σ,μ|y)=lnΣ+(y-μ1)⊤Σ-1(y-μ1).The resulting MLE estimators are denoted byΣ^andμ^. This minimization is a difficult mathematical problem. The classic solution in Kriging is to “divide and conquer” through the application of mathematical statistics, as follows.We useΣ=σz2Rto replaceΣbyR(σz2)mandΣ-1byR-1/σz2, and obtain(6)l(R,μ|y)=mlnσz2+lnR+(y-μ1)⊤R-1(y-μ1)σz2.Following Santner et al. (2003) and also Gano, Renaud, Martin, and Simpson (2006), we minimize this function in the following steps:1.Initialize; i.e., select preliminary values forθ^=(θ1^,…,θk^)⊤which together defineR^.Compute the generalized least squares (GLS) estimator of the mean:(7)μ^=(1⊤R^-11)-11⊤R^-1y.Substituteμ^resulting from Step 2 andR^resulting from Step 1 into the variance estimate(8)σz2^=(y-μ^1)⊤R^-1(y-μ^1)m.Note thatσz2^uses the denominator m, whereas the classic unbiased estimator assumingR=Iwould usem-1.Solve the remaining problem in (6):(9)minθ^mlnσz2^+lnR^.Use theθ^that solves (9) to updateR^, and substitute the resultingR^into (7) and (8).These estimated Kriging parameters result in the estimated MSPE or variance of the Kriging predictor (3):(10)MSPE^=σz2^+1⊤Σ^-1c0^⊤1⊤Σ^-11-11⊤Σ^-1c0^-c0^⊤Σ^-1c0^.Minimization problem defined in (9) is difficult because of “the multimodal and long near-optimal ridge properties of the likelihood function”; i.e., this problem is not convex; see Gano et al. (2006), Jones, Schonlau, and Welch (1998, p. 486), and Marrel et al. (2010, p. 5). The problem of a flat likelihood function leading to highly variable MLE is tackled by Li and Sudjianto (2005), adding a penalty function to the likelihood function.In this section we considern⩾1outputs for each of the m input combinations (type-1 model in Section 1); i.e., the simulation outputs becomeyi;g(i=1,…,m)(g=1,…,n). For the reader’s convenience, we collect symbols in the Appendices A and B including Tables A.1 and B.1. In multivariate Kriging, we can still use the multinormal density defined for the univariate case in (4)—provided we define the stacked vector (say)Ywith mn elements such that we first gather the n outputs at the first input combinationy1=(y1;1,…,y1;n)⊤(first row of Table B.1), then the n outputs at the second input combinationy2=(y2;1,…,y2;n)⊤, etc., until finally the n outputs at the mth input combinationym=(ym;1,…,ym;n)⊤(last row of Table B.1). Note thatyg(output of type g withg=1,…,n) has the constant meanμg(see Table A.1). The resulting vectorYhas the multivariate normal density functionNmn(μ,ΣY)whereμdenotes the mean vector with mn elements andΣYdenotes themn×mncovariance matrix ofY:(11)f(Y)=1(2π)mn/2(ΣY)1/2exp-12(Y-μ)⊤ΣY-1(Y-μ).ForΣYwe offer the following comments.In this section we discuss the general case with k inputs and n outputs (in Appendix C we detail the simplest multivariate Kriging example; namely,k=1input andn=2outputs). A stationary GP for each type of output implies that the outputyghas the constant varianceσg2and (auto)covariances that decrease with the distance between its input combinations; again see (2). Moreover, in multivariate Kriging different output typesyg(xi)andyg′(xi′)withg;g′=1,…,nandg≠g′have cross-covariances, when simulated for the same or for different input combinations; i.e.,ximay be the same asxi′or may be different. For example, ifn=2(bivariate output), thenCov(Y(xi),Y(xi′))=cov(y1(xi),y1(xi′))cov(y1(xi),y2(xi′))cov(y1(xi),y2(xi′))cov(y2(xi),y2(xi′)).If we havexi=xi′in this example with n=2, then the2×2matrix (say)Σ0does not vary with the input combinationxand becomes(12)Σ0=σ12σ1;2σ1;2σ22,whereσ1;2=cov(y1,y2). In the general case with n outputs, the (symmetric) covariance matrix at input combination i(i=1,…,m)is(13)Σ0=σ12σ1;2…σ1;nσ22…σ2;n⋱⋮σn2.So in the general case,Yhas themn×mncovariance matrix(14)ΣY=Σ0Cov(Y(x1),Y(x2))…Cov(Y(x1),Y(xm))Cov(Y(x1),Y(x2))Σ0…Cov(Y(x2),Y(xm))⋮⋮⋱⋮Cov(Y(x1),Y(xm))Cov(Y(x2),Y(xm))…Σ0.To predict the n outputs at input combinationx0, we define—analogously toc0defined below (3)—then×mnmatrixΣ0;m;n=(Cov(Y(x0),Y(x1)),…,Cov(Y(x0),Y(xm))),and obtain—analogously to (3)—the multivariate BLUP(15)y^(x0)=μ^+Σ0;m;nΣY-1(Y-Fμ^),where the vectorμ^—analogously to (7)—denotes the GLS estimator(16)μ^=(F⊤ΣY-1F)-1F⊤ΣY-1Y,withF=1m⊗Inwhere1mdenotes an m-dimensional vector with ones,⊗the Kronecker operator, andInthen×nunity matrix; also see Svenson and Santner (2010).The estimated MSPE of the multivariate Kriging predictor (15) is(17)MSPE^[y^(x0)]=Σ0^-Σ0;m;n^(ΣY^)-1Σ0;m;n^⊤+UF⊤(ΣY^)-1F-1U⊤,withU=In-Σ0;m;n^(ΣY^)-1F.Now we consider one particular way to obtain aΣYthat is PD. Our formalization follows the nonseparable dependence model in Svenson and Santner (2010), who in turn follow Fricker et al. (2010), who precede Fricker, Oakley, and Urban (2013). Fricker et al. (2010) explain both nonseparable models and separable models. Separable models assumeΣY=Σ0Rwhere we defined the cross-covariance matrixΣ0in (13) and the auto-covariance matrixRbelow (2); i.e., separable models assume that the matrixΣYcan be separated into two components with the second component implying that all outputs have the same auto-correlation matrix. Moreover, Fricker et al. (2010) show that separable models have an undesirable so-called Markov property; see Eq. (6) in Fricker et al. (2010) or Eq. (4) in Fricker et al. (2013). An example of a separable model is Zhang (2007). Furthermore Fricker et al. (2010, 2013) discuss how nonseparable models may be created through either convolution method or LMC. These former methods convolve a Gaussian white noise process with a smoothing kernel; see Ver Hoef and Barry (1998) and Higdon (2002). Moreover, Fricker et al. (2010) present empirical results that suggest that convolution method gives worse results than LMC, so we limit our research to LMC. Originally, geostatistics uses LMC to model spatial multivariate processes, see Wackernagel (2003, pp. 194–200). In LMC the output process is a linear combination of building-block processes. Fricker et al. (2010) use an LMC with n blocks. Here we detail LMC following Fricker et al. (2010).Obviously, an n-variate Gaussian variable with mean vectorμand covariance matrixΣmay be generated from a vectorZwith n normally independently identically distributed (NIID) “standard” variables (which have zero means and unit variances) throughμ+AZwithΣ=AA⊤whereAis a symmetric matrix. Svenson and Santner (2010) extend this idea, and consider(18)Y=μ+AZ,whereYdenotes the n-variate output at any input combination,μ=(μ1,…,μn)⊤is the vector of GP means,A=(ag;g′)is a symmetric and PD matrix, andZis a vector of mutually independent stationary GPs with zero mean and unit variance. More precisely,Zhas the Gaussian correlation function defined in (2). It is simple to derive that (18)—together withRdefined below (2) and definingθ(l)=(θ1(l),…,θk(l))⊤with l=1,…,n—implies(19)Cov(Y(xi),Y(xi′))=Adiag[R(xi-xi′;θ(1)),…,R(xi-xi′;θ(n))]A⊤.We point out that we stack the covariance matrixes per input combination, not per output. Ifxi=xi′, then (2) givesexp[-θj(xj;i-xj;i′)2]=1so (19) implies thatΣ0—defined in (13)–becomes(20)Cov(Y(xi),Y(xi))=Σ0=AA⊤.Hence,σg;g′(covariance betweenygandyg′) andσg;g=σg2(variance ofyg) are(21)σg;g′=∑l=1nag;lag′;l(g,g′=1,…,n).For example, forn=2we get (remember thatAis symmetric soag;g′=ag′;g)(22)Σ0=a1;12+a1;22a1;1a2;1+a1;2a2;2a2;1a1;1+a2;1a2;2a2;12+a2;22.Note that each elementag;g′(=ag′;g) affects the two variances and the covariance; we shall detail this characteristic in the next section. If we assume the Gaussian correlation function (2) and a single input x, then (19) becomesR(xi-xi′;θ(g))=exp[-θ(g)(xi-xi′)2]=exp[-θ(g)di;i′2]withdi;i′=xi-xi′so we getCov[Y(xi),Y(xi′)]=AR(di;i′;θ(1))00R(di;i′;θ(2))A⊤,soCov[Y(xi),Y(xi′)]is(23)a1;12R(di;i′;θ(1))+a1;22R(di;i′;θ(2))a1;1R(di;i′;θ(1))a2;1+a1;2R(di;i′;θ(2))a2;2a1;1R(di;i′;θ(1))a2;1+a1;2R(di;i′;θ(2))a2;2a2;12R(di;i′;θ(1))+a2;22R(di;i′;θ(2)).Following Fricker et al. (2010) and Svenson and Santner (2010), we selectAas the eigendecomposition ofΣ0while guaranteeing thatAis PD. We therefore use the Cholesky transformation,A=LL⊤. We should ensure that all the elements on the main diagonal ofLare non-negative; i.e., we should impose the constraintli;i⩾0 (i=1,…,n) in the MLE optimization.Actually, Svenson and Santner (2010) apply Restricted MLE (RMLE) instead of MLE (for details on RMLE see Santner et al. (2003, pp. 66–67)). The RMLEA^(which must be PD) andΘ^(the multivariate analogue ofθ^defined below (6) in Step 1) minimize the following analogue of (6):(24)l(ΣY,μ|Y)=lnΣY+lnF⊤ΣY-1F+(Y-Fμ^)⊤ΣY-1(Y-Fμ^).In our next Monte Carlo experiments we shall use RMLE for univariate and multivariate Kriging for better comparison of the two methods. RMLE for univariate Kriging requires replacing m bym-1in (6), (8), and (9).First we explain why we need a laboratory instead of real applications. Kriging is based on specific assumptions; e.g., Kriging assumes a GP. To analyze the performance of the resulting Kriging procedure, we should start with situations that satisfy these assumptions; a “laboratory” can fully satisfy all our assumptions. Real applications enable us to study the “robustness” of the Kriging method; i.e., how well does the method perform if not all its assumptions are completely satisfied? However, before we perform such robustness studies, we should examine the performance of Kriging in the case where all assumptions do hold. Moreover, real applications may be extremely expensive; i.e., a single simulation run may take hours or days, whereas in our lab a “simulation” run takes only (micro) seconds (depending on the computer hardware and software).Kriging literature derives formulas for the estimated variance of the predictor in univariate and multivariate Kriging respectively. These formulas are popular, but we do not use them to compare univariate and multivariate Kriging because we estimate the MSE from the known I/O function for the simple systems that we simulate in our lab. Moreover, these formulas are biased because they ignore the variability caused by the estimation of the parameters of the GP; see Den Hertog, Kleijnen, and Siem (2006) and Kleijnen and Mehdad (2013).To compare multivariate and univariate Kriging, we use the MSE criterion; this criterion gives the “optimal” Kriging predictor defined in (3), and is relevant in sensitivity analysis (not optimization) of simulation models. Furthermore, in our first example (Section 4.1) we briefly consider a second criterion; namely, the coverage of the 90% confidence interval for the predictor in univariate versus multivariate Kriging. Fricker et al. (2010) also use criteria closely related to our two criteria.We wish to guarantee that the Kriging metamodel itself is a valid metamodel of the I/O function implied by the underlying simulation model. Therefore we generate the “simulation” observationsYfromNmn(μ,ΣY)defined in (11), to obtain the I/O data; these data are detailed in Table B.1 in Appendix B. To these I/O data we apply univariate and multivariate Kriging respectively, and compare their MSEs. Note that a similar Monte Carlo lab is used by Chen et al. (2012) for the “empirical evaluation” of their stochastic Kriging. In Section 4.1 we present a simple Monte Carlo example; in Section 4.2, we present a more complicated example.After we specify that our lab consists of a GP, we must select values for the parametersμandΣYinNmn(μ,ΣY). In our simple example we specify a bivariate output son=2and a single input so k=1. To generate “simulation” data, we select m=10 “old” I/O combinations; m=10 agrees with the value10koften recommended in the literature; also see the “practical guidelines” in Loeppky, Sacks, and Welch (2009). Because space-filing designs are most popular in Kriging, we select these m values equi-spaced in the standardized experimental domain 0⩽xi⩽1. Consequently, we getx⊤=(0,1/9,2/9,…,8/9,1)⊤. We decide to predict the simulation outputs form0“new” input values each halfway its two immediate neighbors, so we getm0=9andx0⊤=(1/18,3/18,…,17/18)⊤. When we sample the GP, we should also sample the “true” simulation outputs at these new input valuesx0⊤; i.e., we sample 10+9=19 bivariate outputs:(25)Y(38×1)=(y1;1,y1;2)⊤⋮(y19;1,y19;2)⊤∼N(38×1)μ(38×1),Σ(38×38).Furthermore, we select all n=2 means equal to zero so in (25) we haveμ(38×1)=(0,…,0)⊤. We wish to experiment with “high” and “low” values for the variancesσ12andσ22. A problem, however, is that in the nonseparable dependence model, the variances—and cross-covariances and auto-covariances—depend on A in (19). In our experiments with two outputs, the values we select for the variances and the cross-covariance together with (22) imply(26)Σ0=a1;12+a1;22a1;1a2;1+a1;2a2;2a2;1a1;1+a2;1a2;2a2;12+a2;22=σ12σ1;2σ1;2σ22,so the three variablesa1;1,a1;2(=a2;1), anda2;2must satisfy three equations. We selectσ12=1,σ22=25(soσ2=5, which quantifies variability better than its square,σ22), andσ1;2=ρ(1;2)σ1σ2=1 soρ(1;2)=0.2 andσ1;2=4 soρ(1;2)=0.8; i.e., in our experiments we keepσ12andσ22fixed, while we experiment with a low and a high cross-correlation (the cross-correlation remains constant across input combinations). For the auto-covariances we assumed a Gaussian auto-correlation function and a single input, as derived in (23). In (23) we have already selected all elements ofA(namely,a1;1,a1;2=a2;1, anda2;2) when selecting the variances and the cross-correlation. To further simplify our selection, we select equal Kriging parametersθ(1)=θ(2)=θfor the two outputs; this changesCov[Y(xi),Y(xi′)]in (23) into(a1;12+a1;22)R(di;i′;θ)(a1;1a2;1+a1;2a2;2)R(di;i′;θ)(a1;1a2;1+a1;2a2;2)R(di;i′;θ)(a2;12+a2;22)R(di;i′;θ).We wish to experiment with low and high auto-correlations. The Gaussian auto-correlation function (2) implies thatcor(yi;1,yi′;1)=exp[-θ(xi-xi′)2]. Obviously, these correlations decrease with the distancedi;i′=xi-xi′. These distancesdi;i′vary with m (number of old equidistant input values in the experimental range) and them0new input values to be predicted (which we selected halfway the old values). Given the input range0⩽x⩽1, these distances range between1/[(m0)/2]=1/18(closest neighbors) and 1 (neighbors farthest apart). We decide to focus on the strongest auto-correlation between old input valuesρ(1)(dmin)=exp[-θ/92]; e.g.,ρ(1)(dmin)=0.2 implies—after rounding—θ=131 andρ(1)(dmin)=0.8 impliesθ=18, which are far away from the two extreme values 0 and∞discussed below (2). Altogether, Table 1displays our four experiments combining “low” and “high” values for the cross-correlationρ(1;2)and the maximum auto-correlation for output g denoted byρ(g)(dmin)with g=1, 2. Note that Table 1 excludes zero or negative cross-correlations. We exclude zero correlation because discrete-event simulation with multiple outputs always gives correlated outputs as these outputs are driven by the same pseudorandom numbers. We exclude negative correlations, because the OR analysts usually know the signs of the correlations between the outputs of their simulation models; e.g., in queuing simulation, the average of the waiting time distribution and (say) the 90% quantile of that distribution are obviously positively correlated. If the analysts know that the correlation between two specific outputs is negative, then they simply take the negative values of one of these two outputs to make the correlation positive.We decided to obtain M=100 macro-replicates; i.e., we repeat our sampling—from the four different GPs in Table 1—100 times using non-overlapping pseudo-random number streams. We use these macro-replicates, to verify our computer code; i.e., we statistically test various intermediate results; namely, the means, variances, auto-correlations, and cross-correlations of the simulated bivariate GP output (25). This verification is detailed in Appendix D.In practice, the simulation analysts have only “a single macro-replicate” to compute the RMLEsμ^andΣ^. In nonseparable multi-variate Kriging, these RMLEs are based on the likelihood function (24). This function may have many local maxima so the search for these RMLEs may get stuck on a local hill. To get initial estimates ofA^andΘ^, Svenson (2011, pp. 314–315) uses a global optimizer; namely, the genetic algorithm (GA) in Forrester (2010); to get final estimates, he uses MATLAB’s “fmincon”. In our first Monte Carlo example, we use DACE for univariate Kriging; we combine DACE with the same GA and bounds that Svenson uses.Given these RMLEs computed from the “old” I/O data, we can predict the output for one of the new points (say)xt′witht′=1,…,m0; i.e., in (3) we replace the unknown parameters by their RMLEs, which gives (say)yt′^^. We compare this predicted valueyt′^^with the “observed” valueyt′; the latter value is conditional on the m old observed valuesyibecause of (25), which uses the true parameters instead of their RMLEs. This gives the squared error (SE) at the new input valuet′for output g in macro-replicate r where in this example n=2, m=10, M=100:(27)SEt′;r(g)=yt′;r(g)^^-yt′;r(g)2with(g=1,…,n)(t′=1,…,m0)(r=1,…,M).Because we predict the output form0new input combinations, we use this equation to compute the estimated integrated MSE (IMSE) for output g:IMSEr(g)^=∑t′=1m0SEt′;r(g)m0.Next we use thisIMSEr(g)^to compute its average over the M macro-replicates:(28)IMSE(g)^‾=∑r=1MIMSEr(g)^M.Table 2gives this average for univariate and multivariate Kriging, for the two outputs of our four experiments defined in Table 1. The standard errors—displayed in parentheses below the averages—show that the reported averages are quite accurate after M=100 macro-replicates. All eight differences between these averages for univariate and multivariate Kriging—denoted by Difference in the table—are non-significant, where our null-hypothesis is that there is no difference between the MSEs of univariate and multivariate Kriging, for both responses and all four experiments. This significance we test through the t-statistic for differences estimated from correlated observations (caused by common random numbers used by the four experiments). We use an experimentwise type-I error rateα=0.20 combined with Bonferroni’s inequality so the “per comparison” error rate is(α/2)/8=0.0125. Furthermore, the point estimates show that univariate Kriging gives a smaller MSE for output 1 in all four experiments; for output 2 with its higher variance, univariate Kriging gives smaller MSE in two out of four experiments.Comparisons of rows instead of columns in Table 2 shows that the averages are much bigger in rows 2 and 4. Table 1 shows that rows 2 and 4 have a lower value for the auto-correlation; obviously, such a low auto-correlation implies that the Kriging predictor for a new point is less accurate when taking a weighted average of the old simulation outputs (the Kriging predictor is indeed a weighted average with weights determined by the distances between the new point and the old points). These differences between rows are so big that we do not need a statistical test to conclude that these differences are important. To explain the results in Table 2, we compute t-statistics from the M=100 RMLEs for the Kriging parameters(μg,σg2,θ(g),σg;g′)whereσg;g′=0 in univariate Kriging. Our null-hypothesisH0states that the expected value of a RMLE equals the true value of the corresponding Kriging parameter; e.g.,H0:μg^=μg. Table 3shows whether such a t-test rejectsH0; i.e., the superscript∗denotes that the t-statistic is significant at the 5% significance level. This table implies that for univariate Kriging we should rejectH0only forθ(g), whereas for multivariate Kriging we should rejectH0forθ(g),σg2, andσg;g′. So multivariate Kriging gives inaccurate estimates of the Kriging parameters. These inaccurate estimates may result from the search in a space with more dimensions when solving a nonconvex problem. Svenson and Santner (2010) also mention that RMLE in multivariate Kriging requires a search in higher dimensions than univariate Kriging does, because the latter assumes zero cross-correlations; so the former search might actually result in poor estimates of the Kriging parameters. Svenson and Santner (2010) and Fricker et al., 2010 give numerical results for several examples suggesting that multivariate Kriging may not improve MSE dramatically relative to application of univariate Kriging to multiple outputs. To further investigate these numerical results, we run univariate and multivariate Kriging with the true Kriging parameters—which is easy in Monte Carlo experiments and impossible in real experiments. We point out that univariate Kriging uses 100% accurate information onσg2^andθj(g)^, but that information is incomplete because it ignores the cross-correlationsσg;g′^and consequently univariate Kriging uses the wrongμg^. The “old” and “new” outputs vary over the M macro-replicates, because they are sampled from (25). This gives Table 4, which displays the average performance defined in (28); this performance is the same for univariate and multivariate Kriging. Comparing this table with Table 2 shows that the performance is better when using the true Kriging parameters instead of their RMLEs, as we expected. However, IMSEs in Table 4 are not significantly better than IMSEs in Table 2. To explain that in this example univariate and multivariate Kriging give the same performance when they use the true Kriging parameters, we study the only difference between univariate and multivariate Kriging; namely,Σ0;m;nΣY-1in (15). Univariate Kriging assumes zero cross-covariances. We find that the corresponding elements in multivariate Kriging—using the true Kriging parameters—are virtually zero. For example, when predicting the output for the first element ofx0⊤=(1/18,3/18,…,17/18)⊤in macro-replicate 1 of experiment 1 (withρ(1:2)=0.8), multivariate Kriging gives values between10-16and10-13. We also compute these values forρ(1:2)=0.95 and again find virtually zero values; detailed results are given in Table E.1 of Appendix E. These results are typical for this example; i.e., in the more realistic example of Section 4.2 we shall find different results.Our conclusion is that univariate Kriging is simpler than multivariate Kriging, and that multivariate Kriging does not perform better than univariate Kriging, even when multivariate Kriging would know the true GP parameters.Besides the MSE, Fricker et al. (2010) study the coverage of the confidence interval of the Kriging predictor. A popular 90% two-sided confidence intervals foryt′;r(g)^^isyt′;r(g)^^±1.64MSPEt′;r(g),where 1.64 is the 0.95 quantile of the standard Gaussian density andMSPEt′;r(g)follows from (10) and (17). Obviously, for givent′and g values, macro-replicate r gives an interval that does or does not cover the true valueyt′;r(g); from the M macro-replicates we compute the estimated coverage. The coverage of this 90% confidence interval turns out to be too low, for any t and g (box plots are available from the authors). This low coverage may be caused by the classic variance of the Kriging predictor, which ignores consequences of estimating the Kriging parameters; see again Den Hertog et al. (2006) and Kleijnen and Mehdad (2013).Finally, Fricker et al. (2010) suggest that the relative performance of multivariate Kriging may improve when “the” output is a function of the individual cross-correlated outputs. Therefore we also experiment with the sum and the product,y(3)=y(1)+y(2)andy(4)=y(1)y(2). Appendix F (Table F.1) suggests that univariate Kriging gives smaller MSE for all n=4 outputs and all four experiments in Table 1 except fory(4)in experiment 1. Univariate Kriging gives better coverage than multivariate Kriging, but still below the nominal value. Note that we do know the true values of the Kriging parameters fory(1)andy(2), but not fory(3)andy(4). Furthermore, there are more Kriging parameters to be estimated; namely,σg,g′(g,g′=1,…,n=4)andθ(g). The Appendix (Table F.2) suggests that multivariate Kriging has more significant differences between the estimated and the true parameter values.In this subsection we summarize our second type of Monte Carlo example; namely, an example with d=5 inputs and n=3 outputs that is inspired by the simple climate model (SCM) case study in Fricker et al. (2010). For this example we select m=57 old I/O data(X,W)andm0=93 new data(X0,W0); these data we received from one of the coauthors (namely, Urban). For our Kriging computations we use Svenson’s code. So, from the old data(X,W)we computeψ^, the RMLE of the GP parameters. Thisψ^has 24 elements; namely, the three meansμg^(g=1, 2, 3), the three variancesσg2^, the5×3=15auto-correlationsθj(g)^(j=1,…,d=5), and the three cross-covariancesσg;g′^.We observe that the three outputs have indeed positive cross-correlations; i.e., from the simulation outputsWwe compute the classic estimates which do not assume a GP:(29)r(g;g′)=∑t=1m(wt(g)-w(g)‾)(wt(g′)-w(g′)‾)m.This givesr(1;2)=0.47,r(1;3)=0.55, andr(2;3)=0.80; these estimates are scale-free. The estimates (29) should be distinguished fromρ(g;g′)^, which denote the RMLE computed for the multivariate GP with scaled old data. These computations giveρ(1;2)^=0.50,ρ(1;3)^=0.53, andρ(2;3)^=0.82, which agree very well with the classic estimates.Because we scale the I/O data, the simulation outputs have zero means. The n=3 estimated GP meansμg^turn out to be virtually zero, in both multivariate and univariate Kriging. For the computations of univariate Kriging we apply Svenson’s code per output, ignoring cross-correlations; i.e., assuming these correlations are zero. The estimated variances in multivariate Krigingσg2^are 5.98, 3.48, and 3.62; i.e., output 1 has a bigger variance. We point out that univariate Kriging gives different estimates; namely, 6.38, 4.29, and 7.09; we emphasize that univariate Kriging assumes zero cross-variances so it is to be expected that its variance estimates are different. The 15 estimated auto-correlation coefficientsθj(g)^differ in multivariate Kriging and univariate Kriging.In our second Monte Carlo example, the true GP is the GP with the parameters that we estimated for the SCM case study using multivariate Kriging; e.g.,ρ(1;2)^=0.50. Analogous to (25) we sampleYfrom the multivariate normal distribution; thisYhas(57+93)×3=450 elements. This gives the IMSE defined analogously to (28). We use M=30 macro-replicates, which require approximately 15hours of computer time on our PC. Comparing columns 2 and 4 of Table 5shows that univariate Kriging gives a much smaller MSE than multivariate Kriging. Like we do in our first Monte Carlo experiment, we also compute the IMSE assuming true GP parametersσg2^,θj(g)^, andσg;g′^; notice thatμg^is computed through (16). Comparing columns 2 and 3 shows that using these true parameters drastically decreases the IMSE of multivariate Kriging. Comparing columns 3 and 4 shows that multivariate Kriging with true GP parameters does not give a smaller IMSE than univariate Kriging with estimated parameters. Comparison of these two columns shows that multivariate Kriging’s inferior performance is not due to the estimation of more parameters. We conjecture that multivariate Kriging suffers from an inherent bad property—like separable models, which have the Markov property, which implies that the cross-covariance between the outputs does not help. Comparing columns 3 and 5 shows that using the true parameters gives univariate Kriging with higher IMSE than multivariate Kriging; in Section 4.1 we have already pointed out that univariate Kriging uses 100% accurate but incomplete information on the GP parameters. We point out that the first example used assumptions so simplistic that univariate Kriging was not affected by the incompleteness of the information on the RMLE of the GP parameters.In this article we compare univariate and multivariate Kriging metamodels for simulation models with multiple outputs. A major problem of multivariate Kriging is ensuring that the (symmetric) covariance matrix of all the observed simulation outputs remains positive-definite; to solve this problem, we apply a nonseparable dependence model that was originally proposed by Fricker et al. (2010). We compare the resulting multivariate Kriging with univariate Kriging per type of simulation output; univariate Kriging ignores the cross-correlations between the multiple simulation outputs. To compare these two Kriging types, we perform some Monte Carlo experiments in which we guarantee that all the assumptions of multivariate Kriging are satisfied. We use these experiments to estimate the MSEs of multivariate and univariate Kriging. These experimental results suggest that the simpler univariate Kriging gives lower MSE than the more complicated multivariate Kriging; one explanation is that multivariate Kriging requires the estimation of additional Kriging parameters—namely, the cross-correlations between the simulation outputs—which affects the estimates of all parameters. To check this explanation, we run additional Monte Carlo experiments replacing the estimated Kriging parameters by their true values, which are known in a Monte Carlo experiment. We then find that multivariate Kriging with the true GP parameters still does not perform better than univariate Kriging. We conjecture that nonseparable models created through LMC have some inherent property that causes inferior performance. In Example 1 we briefly examine the coverage, and conclude that both multivariate and univariate Kriging give coverages lower than the nominal (90%) value; multivariate Kriging does not improve this coverage.Future research may address the following topics.•Multivariate Kriging may serve as a metamodel not only for simulation models with multiple outputs but also for multi-fidelity simulation, discussed in Section 1 (type 2). Our Monte Carlo lab can be easily adapted to model such types of simulation. Besides the output of interest, the modelers may also estimate its gradient; also see Section 1 (type 3).The goal of simulation may be optimization instead of sensitivity analysis. This optimization might replace MSE by a criterion such as used in efficient global optimization (EGO), but adapted for multivariate optimization. This optimization may use constrained optimization, selecting one output as the goal variable and satisfying constraints on the(n-1)remaining outputs. An alternative for this constrained optimization is multi-objective Pareto optimization.In our Monte Carlo experiments we guaranteed that Kriging gives a valid metamodel, but in practice we may improve the validity of the metamodel by replacing ordinary Kriging by “universal” Kriging which uses a linear regression model instead of the constantμin (1); see Fricker et al. (2010).In our Monte Carlo experiments we may replace the Gaussian correlation function by some other correlation function to generalize our results; in practice, such a function may improve the validity of the Kriging metamodel.If indeed multivariate Kriging does not outperform univariate Kriging in deterministic simulation, then it does not seem interesting to extend multivariate Kriging from deterministic simulation to more complicated (random) discrete-event simulation.

@&#CONCLUSIONS@&#
