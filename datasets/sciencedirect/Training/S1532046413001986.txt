@&#MAIN-TITLE@&#
Learning Bayesian networks for clinical time series analysis

@&#HIGHLIGHTS@&#
An algorithm to learn models from sparse time series data is formulated.Augmented temporal naive Bayes networks are proposed as an effective model.Predictive models for chronic obstructive pulmonary disease are built.Extensive evaluation results indicate the predictive power of the models.

@&#KEYPHRASES@&#
Chronic disease management,Bayesian networks,Machine learning,Temporal modelling,Clinical time series,Chronic obstructive pulmonary disease,

@&#ABSTRACT@&#
IntroductionAutonomous chronic disease management requires models that are able to interpret time series data from patients. However, construction of such models by means of machine learning requires the availability of costly health-care data, often resulting in small samples. We analysed data from chronic obstructive pulmonary disease (COPD) patients with the goal of constructing a model to predict the occurrence of exacerbation events, i.e., episodes of decreased pulmonary health status.MethodsData from 10 COPD patients, gathered with our home monitoring system, were used for temporal Bayesian network learning, combined with bootstrapping methods for data analysis of small data samples. For comparison a temporal variant of augmented naive Bayes models and a temporal nodes Bayesian network (TNBN) were constructed. The performances of the methods were first tested with synthetic data. Subsequently, different COPD models were compared to each other using an external validation data set.ResultsThe model learning methods are capable of finding good predictive models for our COPD data. Model averaging over models based on bootstrap replications is able to find a good balance between true and false positive rates on predicting COPD exacerbation events. Temporal naive Bayes offers an alternative that trades some performance for a reduction in computation time and easier interpretation.

@&#INTRODUCTION@&#
Clinical data often takes the form of time series, which, when interpreting all the variables concerned in their mutual context, offer a description of the progression of a disease over time. While insight into the evolution of a disease is an important aspect of the management of any disease, whether acute or chronic, for patients with a chronic disease the evolution is of even more importance, as often the disease will not disappear. In this context it is in particular important to be able to detect when the disease becomes worse, i.e., to detect and possibly prevent exacerbations. For any chronic disease it is therefore of clinical interest to study the interaction between different variables, ranging from signs and symptoms to environmental factors, in terms of both static and temporal relationships. If we can capture this knowledge in a model, predictions regarding health status made by means of such models can be used to assist in chronic disease management, for example by advising on therapy adjustment. Furthermore, disease models are important for epidemiological purposes, for example for survival analysis, as well as for cost-effectiveness analysis and policy planning.A complication that arises when analysing clinical time series is that it is often hard to obtain sufficient data, for example because the event of interest is relatively low frequency or because taking measurements is costly, time consuming or inconvenient for the patient. In addition, the reality of gathering clinical data is that observations are made at irregular time intervals and the data will contain missing values. Patients will sometimes forget to provide data, or omit some evidence for unknown reasons. Also measuring devices may sometimes fail or readings may not be recorded. These observations pose a challenging research question, which we seek to answer in this paper, namely, whether we can learn useful predictive models from clinical data with the combined characteristics of missing values and limited availability.The research methods we propose draw their inspiration from various existing methods, which have proven to be successful in machine learning applications. Yet the combination of these methods has not been applied to clinical time series analysis. Temporal variants of Bayesian networks are our main tools to reason about causal and temporal processes in a probabilistic manner. In particular we use dynamic Bayesian networks (DBNs), where ‘dynamic’ should be interpreted as modelling the temporal dynamics of the process. They provide interpretable and versatile models to describe time series data and can be used to classify states and make predictions about future states. To learn network structures in the presence of missing observations we make use of the structural expectation–maximisation (EM) algorithm [1], which iteratively completes the data and performs a search for the best network structure to explain the data. Here we employ a variant of structural EM, with a different approach to filling in values for the missing data. Finally, to tackle the problem of data sparsity, we consider bootstrap methods originally developed in statistics [2]. In the context of Bayesian networks these methods have been applied to the analysis of micro-array data and we extend them here to the learning of temporal models from clinical time series.To provide a concrete clinical context for this research, we focus on chronic obstructive pulmonary disease (COPD) as an application area. This disease has many characteristics that are typical for any chronic disease, although symptoms and signs will be mostly different from those other diseases. COPD is a major chronic disease in terms of morbidity and mortality; it affects the respiratory system, decreasing lung capacity and obstructing airways, thus interfering with normal breathing. An important aspect of COPD which is particularly relevant in the present context is the progressive nature of the disease. Specifically episodes of acute deterioration have a profound impact on patient well-being and on health-care costs [3]. These exacerbations are mainly caused by airway infections resulting in symptom worsening [4]. Important to note is also that patients with frequent exacerbations usually have faster disease progression, which makes exacerbation prevention a particularly relevant goal. Additionally, a faster treatment response to exacerbations appears to lead to better recovery [5].The main contributions of the paper are as follows:•We formulate an algorithm to learn temporal probabilistic models from limited clinical time series with missing values. The main novelty of the algorithm lies in combining learning of dynamic Bayesian networks from clinical data using structural EM with block bootstrapping for small data samples.We propose a variant of our learning algorithm based on naive Bayes networks, which has the attractive properties of reduced computational complexity, thus easy construction, while offering good prediction performance.The proposed learning algorithms are used to build predictive models of COPD patient’s health status, focussing on day to day progress of signs and symptoms that can rapidly change during exacerbation events. These predictive models are novel in the context of COPD as they can handle both the dynamic nature and uncertainty inherent in the disease progression. As such, these models can be embedded within clinical or home-based applications for chronic disease management.We evaluate the learning procedure on COPD synthetic and patient data and show that it is effective in terms of structure discovery of interesting variable relationships, interpretability and prediction performance of the models learned.The results from this research demonstrate important clinical implications not only for the prediction of COPD exacerbations but also for the clinical relevance of the methods proposed for chronic disease management applications in general.Survival analysis is a popular clinical application of time series analysis and here the technique of Cox-regression is normally used for model construction [6]. However, Cox-regression has important limitations; in particular, it is not suitable to model independence assumptions. Our work is more closely related to the research described in [7], which argues for using DBNs for prognostic models in medicine. Variants of DBNs have also been used to model temporal dynamics of organ failure in patients in intensive care units (ICU) [8], although there no structure learning was used. Further, a Bayesian network has been developed on the basis of electronic health record data to predict the onset of COPD in asthma patients; however, temporal information was not explicitly taken into account [9]. In the specific context of long term disease management for COPD, related research has focussed on facilitating remote communication [10,11] and automatic data interpretation is still uncommon. In [12] a telehealth system is described that has been applied to COPD and contains a decision support component. However, currently the decision support is limited to rule based detection of abnormal values and trend detection. Automatic interpretation of monitoring data using machine learning while taking time and uncertainty into account, therefore, appears to be a useful contribution to the area of chronic disease management.Early work on using Bayesian networks for prediction includes dynamic network models [13], with, for example, a clinical application to predicting sleep apnea [14]. In our work, we used and extended techniques for learning Bayesian networks from data with missing values [1] and learning from small samples using bootstrapping [15]. These methods are used extensively in bioinformatics [16], but application to the domain of clinical time series analysis constitutes a new and interesting challenge. The bootstrap methods used for small data samples are related to what is known as bagging in the machine learning literature [17], but are usually applied to learning decision trees instead of Bayesian networks. Our augmented temporal naive Bayes model is an extension of the TAN classifier from [18] to a prediction model that takes time dependencies into account.In [19] a method is proposed to learn DBNs with changing dependency structures. The models include hidden variables that influence the structure of the model depending on the value of a particular variable that controls the structure change. This method is of interest when sufficient data is available to learn changing dependencies. Finally, in [20] an approach is described to use steady state information in addition to time series data to learn DBNs. Steady state data from the limiting distribution of the Markov chain describing the process is used as an additional source of information.A different approach to modelling temporal processes is used in temporal nodes Bayesian networks (TNBNs), which model events instead of dynamics [21]. TNBNs are similar to Bayesian networks, but temporal nodes take time intervals as values. The intervals represent the time since a parent event. We made a TNBN model for our COPD domain to compare to the dynamic models, which will be discussed in Sections 5.3 and 6.3.The methods we developed and used in this paper were needed as part of a research project, called Aerial, aimed at the detection of worsening in patients with COPD, i.e., exacerbations. Here we briefly describe the system and the design of the study to sketch the practical clinical context of the work.In order to facilitate self-management of COPD by patients we developed a system with the capability to gather patient-specific information, to interpret the gathered data automatically and to offer feedback and advice to the patient. The general architecture of the Aerial system is shown in Fig. 1. The system consists of a smartphone as the main component taking care of communication and computation. Questionnaire data is collected from the patient on the smartphone, which also communicates wirelessly with the sensors used to obtain objective information on the patient’s health status. A web-based system allows scheduling tasks and collecting patient data centrally. The web-centre receives the data from the smartphone and provides data access for health-care workers. Patient data are interpreted in the smartphone by means of a disease-specific probabilistic model, different variants of which are studied in this paper. A more comprehensive description of the system can be found elsewhere [22].The aim of the study was to investigate the prerequisites to facilitate self-management of COPD-patients in a home-care setting. We tested the Aerial system at home to see whether this would result in usable data with the goal of gathering a data set from which to learn models for COPD-exacerbation prediction. Ten participants were recruited from hospitals and general practices in the Netherlands, 7 male and 3 female, between 53 and 76years of age (mean (sd): 65.6 (6.8)). All participants gave written informed consent. Inclusion criteria were GOLD II or III (disease severity classification on a scale from I to IV) [23] and sufficient cognitive capability to operate the system. There were two possible inclusion paths: stable patients, although having had exacerbations in the past, and patients that reported to a physician because of an exacerbation. The first path provides information on stable patients, and possibly on exacerbation onset; the second on exacerbation recovery and possibly relapse.Patients were monitored daily for a duration of approximately 4weeks. Each day they answered a set of binary questions about their symptoms and performed spirometry and pulse-oximetry measurements. All questions were formulated such that a ‘yes’ answer indicates a symptom worse than the baseline condition for that patient. The set of recorded variables consists of the symptoms dyspnea (D), sputum volume (SV), sputum purulence (SP), cough (C), wheeze (W), temperature (T), malaise (M) and activity (A); plus the measurements SpO2 (SO) (blood oxygen saturation) and FEV1 (F) (forced expiratory volume in 1s). Exacerbations (E) were defined as at least two consecutive days of an increase in at least two major respiratory symptoms (dyspnea, sputum volume or sputum purulence) or one major and at least one minor respiratory symptom (see e.g. [24]). A total of 189 data records were collected, which when regularised by adding missing values for days that were not recorded resulted in a data set of 250 records. The reason for explicitly representing missing values is that the learning algorithms assume regularly spaced data. Missing data was partly a consequence of technical issues and partly due to patients omitting data for unknown reasons, resulting in 30% missing data. Of all the collected data records 60 were collected during an exacerbation.As the research aim was to develop models that were able to interpret temporal clinical data, in particular revealing the progression of COPD including its stochastic variation, especially probabilistic models were considered to be attractive for that purpose. Probabilistic models can express temporal trends and handle missing values when data of a specific patient must be interpreted. The successful use of dynamic Bayesian networks in the context of micro-array analysis [16], where sparsity of data is also a problem, made us wonder whether similar methods might also be successfully applied to clinical problems.We start with a brief summary of basic methods used in the research, leading up to a method to learn models from sparse clinical time series data.A Bayesian network[25,26], is a probabilistic graphical model represented as a pairBN=(G,P). Here,G=(V,A)is a directed acyclic graph consisting of vertices V, corresponding one-to-one to random variables of interest, andA⊆V×Vare arcs, representing probabilistic dependencies between variables. Furthermore, P is a joint probability distribution defined by a family of conditional probability distributions of the formP(V∣pa(V)), that is, the probability that V takes on a specific value given the values of its parent variables,pa(V). The network represents the joint distribution over the random variables, which can be factored according to the dependencies represented in the graph, resulting in:P(V1,V2,…,Vn)=∏i=1nP(Vi∣pa(Vi)),whereVi∈Vis the representation of a random variable in the graph G. Any probability of interest can be computed from this joint probability distribution.A dynamic Bayesian network[27,28], DBN for short, is an extension of a Bayesian network to a distribution over a sequence of random variables. It is particularly well suited to represent a Markov processX1→X2→⋯→Xt→⋯whereXtrepresents a random variable at a particular moment in time t. The joint distribution can be decomposed using the chain rule, writingX1:TforX1,…,Xt,…,XT:P(X1:T)=P(XT∣X1:T-1)P(X1:T-1).In general a DBN is a factorisation of a probability distribution, like its atemporal counterpart. In addition, whenXtis composite it can be represented as a BN. This BN is called a time slice and relations between time slices can be modelled by introducing arcs in the graph between random variables in different time slices. A DBN factorisation can then be written as:P(X1:T)=∏t∏iP(Xt,i∣pa(Xt,i))where i indexes variables within a time slice andpa(X)denotes the parents of X in the graph. A hidden Markov model, a popular stochastic model used for pattern recognition, is a special case of a DBN.A common assumption is that there is only a limited time frame that influences the current state of the process, as opposed to the complete history, which simplifies model learning. When assuming an nth-order Markov process we obtain:P(X1:T)=P(XT∣XT-n:T-1)P(XT-n:T-1),recursively. In the context of clinical data analysis, this assumption makes sense for two reasons: first, as time passes physiological and disease processes will change and older information will be less informative about the current state of the patient; second, it will often not be possible to obtain reliable information about the past. For smaller n more temporal independence is introduced and when only sparse data is available, it is common to make the most restrictive version of the Markov assumption, first-order, such that the future state of the process only depends on the present:P(Xt+1∣X1:t)=P(Xt+1∣Xt).Hence, all parents of a variable X will be in the same time slice or in the previous time slice. From a medical point of view this means that the current health status provides the most information about the future. Given that clinical data is often sparse, an important practical consequence of this assumption is that it simplifies the model, and hence reduces the amount of data we need. When we now also assume that the process is stationary, that isP(Xt,i∣pa(Xt,i))=P(Xt′,i∣pa(Xt′,i))for allt,t′, we obtain a two-slice DBN consisting of an initial networkBN0and a transition networkBN→. A process through time can now be modelled by a sequence of repetitions of transition networks. When modelling a chronic disease over a long period, it may be that stationarity is not a reasonable assumption. For COPD one might argue that it is also useful to consider separate models for the disease stages (GOLD I-IV [23]). However the COPD data we study here is limited to GOLD II and III by the inclusion criteria, and the amount of available data is too limited to make a distinction. Yet, in general, it appears useful to consider recent techniques to learn non-stationary DBNs [29].Given a data set we can use machine learning techniques to learn a model from the data. Two main tasks are usually distinguished: (i) finding a network structure, and (ii) finding the parameters that best describe the data given a network structure. See [30,31] for fairly comprehensive overviews of probabilistic learning.Parameter learning entails finding the optimal parametersθˆfor a given network structure G, that explain the data D:θˆ=argmaxθP(D;θ,G).This is the maximum likelihood estimate, parametrised byθ(the semicolon indicates that it is a parameter, not a conditional). The maximum likelihood estimate is straightforward to compute, but may suffer from overfitting; especially for sparse data, which is the case that we are considering here. A Bayesian approach makes explicit the uncertainty in the parameters by takingθto be a random variable, leading to:P(θ∣D,G)=P(D∣θ,G)P(θ∣G)/P(D∣G),from Bayes’ theorem, where we compute a distribution over parameters, with some prior distributionP(θ∣G). Assuming independent and identically distributed data, global and local parameter independence, the likelihood can be decomposed in local likelihood terms according to the graph:P(D∣θ,G)=∏d∈D∏i=1nP(Xi=k∣pa(Xi)=j,θ).Since we are using discrete variables in our COPD model, we use multinomial distributions for the local likelihood termsP(Xi=k∣pa(Xi)=j,θ). The conjugate prior distribution of the multinomial is a Dirichlet distribution, where conjugacy implies that the posterior is also a Dirichlet distribution. We can interpret the hyperparametersαof the Dirichlet distribution as pseudo-counts which leads to a parameter estimateθijk=P(Xi=k∣pa(Xi)=j)=∣D[k,j]∣+α[k,j]∣D[j]∣+α[j],whereD[k,j]is an index expression selecting the data cases whereXi=kandpa(Xi)=jandD[j]=∑kD[k,j]. In principle it is possible to specify different prior pseudo-counts for different cases, leading to analogous index expressions forα, in practiceαis often constant.Parameter estimation using likelihood decomposition properties works well for complete data, which is unrealistic for clinical models. The expectation–maximisation (EM) algorithm can be used to learn parameters when we have missing values [32].LetD=〈O,H〉be a data set consisting of observed values O and hidden valuesH;Oi,Hiindicate a single data point. The EM algorithm iteratively adjusts the parameters in two steps:E-step:CompleteDaccording toP(Hi∣Oi,θs)M-step:θs+1=argmaxθP(D∣θ)where s is the step counter. The E-step completes the data by filling in values for the hidden variables given the current parameters (usually implemented by computing expected sufficient statistics); and the M-step computes the maximum likelihood parameters based on the completed data. This algorithm provably improves the likelihood in each step until some maximum is reached, although this could be a local maximum [32]. EM gives us no guarantee of finding a global maximum; however, its results are normally sufficiently good.Hand-crafting a network structure is difficult and time consuming and hence it is useful to also try to learn the network structure from data. Even for a limited number of variables as we have in our case, the search space is of such a size that exhaustive search is infeasible. Alternative methods include constraint based and score based techniques. See [33] for a comparison on a large set of hospital management data, where constraint based methods appear to perform somewhat better. However, an important disadvantage of constraint based methods, that rely on independence tests, is that early errors propagate and can have large effects. Additionally, independence tests tend to be less reliable especially on small samples, for example because distribution assumptions are not met. In [34] arguments for and against constraint based methods are examined and a hybrid approach is proposed, which is, however, not directly applicable to time series. We opt here for a standard score-based search procedure that tests local changes to the graph, greedily maximising the scorelogP(G∣D), which by Bayes’ rule can be written aslogP(G∣D)∝logP(D∣G)+logP(G).Taking a Bayesian approach the probabilityP(D∣G)can be written as the integralP(D∣G)=∫θP(D∣θ,G)f(θ∣G)dθ,withf(θ∣G)a probability density function, marginalising out the parameters. For complete data the likelihood again decomposes according to the graph and assuming conjugate priors there is a closed form solution for the marginal likelihoodP(D∣G)(see for example [30]). A common choice for the structure priorP(G)results in the BDe-score [35], which uses a prior such that we have score equivalence. That is, networks that encode the same independence information will have the same score.Unfortunately, the task becomes increasingly difficult with missing data. Finding optimal parameters when we have missing values depends on the structure, while structure search clearly depends on the data, including the missing values. There is no particularly efficient solution to this problem, but we can use the same general principle as we did to learn parameters from partially observed data. This idea is called structural expectation maximisation [1]. In essence the structural EM algorithm alternates between completing the data based on the current structure and parameters and finding a model that has a better score. The same caveat as mentioned for the EM algorithm applies here as well, structural EM does not guarantee finding the global maximum likelihood solution. When the number of missing values increases the search space becomes larger and we are less likely to find a globally optimal model.With the model learning techniques described above we can try to find models that reasonably fit our data, even when some of the data is missing. However, as we are learning statistical models there is a strong dependence on the amount of available data. The more data we have the better we can learn relations. Unfortunately, gathering data is often a difficult, time consuming process. For the COPD self-management application the demanding logistics of studying a new system in a home-care setting, made data collection hard. Furthermore, sometimes the model space is so large that obtaining sufficient data to learn models directly may be infeasible. Previous research in determining gene interaction patterns from micro-array data [15,16], is another example of research that stumbled across the mentioned problems. The clinical data we study here is, however, of a different nature than micro-array data, as signs and symptoms will clearly produce other temporal dynamics. Although the number of variables was fairly limited in our case, we had a combination of missing values within records and a limited number of records in total. Hence we needed to combine the EM procedure with some technique to deal with the small data sample.The data sparsity in modelling gene data led to using bootstrapping [2] as a way to estimate the uncertainty of relations in Bayesian networks [15]. The idea is as follows: bootstrapping can be used as a nonparametric estimate of a statistic of interest, for which we can take the presence of arcs in the Bayesian network graph. The intuition behind bootstrap replications of a data set is that if we assume a generative model, the actual data we see are just a particular realisation which may be unrepresentative of the underlying process. Bootstrapping a number, say m, of new realisations by resampling (with replacement)∣D∣samples from the original data and learning models from those data sets, we can estimate whether an arc a should be present in our model:P(a)=1m∑i=1mI(a,Gi),whereI(a,Gi)is an indicator function that is 1 when a is present in the graph learned from bootstrap realisation i.Since we are analysing temporal data, the situation is somewhat more difficult. Simply resampling from the original data will discard all the ordering information, which is clearly undesirable. Instead we have to apply a kind of bootstrapping that preserves the correlation between different time points. In the statistics literature a number of methods have been developed to do so, and we opted to use fixed-length block bootstrapping. As the name suggests this entails resampling blocks of data points, which preserves the time relations within the block. The moving block bootstrap splits the data in∣D∣-l+1blocks, where l is the block length. Block i contains the data points from i toi+l-1. A bootstrap time series is the concatenation of∣D∣/lsampled blocks. See e.g. [36] for a theoretical comparison of block bootstrap methods.From the bootstrap results we can construct a model that includes the features (arcs) that have a high probability. Another possibility is to use model averaging, where we compute the prediction probability on validation dataDvalas the average over the predictions of the bootstrap models(1)P(Dval)=1m∑iP(Dval∣Gi).However, each of the networksGihas some score attached to it, so a better approximation to the real probability can be obtained from the weighted average(2)P(Dval)=1m∑iP(Dval∣Gi)P(Gi∣Di),whereDiindicates the ith bootstrapped data set.With all the machinery described above, we can now start putting together the method to learn models from time series. The main idea is a variant of structural EM applied to bootstrap resampled data. The new features of this procedure are that it combines sparse data techniques with structure learning on small samples of time series data, which is typical for clinical data.Algorithm 1This algorithm generalises the bootstrap method for sparse data [15], to temporal models. In order to learn the structure of a temporal model, we need a generalisation of the structural EM algorithm [38]. In each iteration we learn the structure of a two-slice network consisting ofBN0andBN→. There are, however, some differences with the algorithm in [38]. In particular, we take a different approach to completing the data. On line 5 of our algorithm, we sample values from the posterior distributionP(H∣O,θj)instead of computing fractional sufficient statistics. That is we sample values for the missing data H from the posterior given the observed values O and the current parametersθj. This allows us to decouple the parameter learning and the structure search. The result is a complete data set without (fractional) expected values filled in for the missing values, which lets us use structure search methods developed for complete data. Additionally, sampling from the posteriorP(H∣O,θj)has the interesting property that it results in automatic small data perturbations. Because structural EM is not guaranteed to find a global maximum, the data perturbations help in starting the search from slightly different departure points. The posterior sampling provides a way to let the search reach different parts of the search space. In [39] some of the differences between hard-assignment, EM (soft-assignment) and posterior assignment are studied in the context of clustering, which is closely related to filling-in missing values.Although a full model is useful to gain insight into the domain, for our application it is important that the performance on predicting exacerbations is good. Therefore, it appears useful to emphasise this goal during model construction. To do so we start from the concept of a naive Bayes classifier, with exacerbation, E, as class variable. The rationale is that naive Bayes is a good baseline classifier that can be extended with the information obtained from structure learning. The idea of modelling dependencies between naive Bayes feature variables has been used to construct tree augmented naive Bayes (TAN) classifiers [18]. We here propose an augmented temporal naive Bayes classifier. The presence of arcs from our class variable exacerbation to each of the other variables in the same time slice is enforced, ensuring the naive Bayes structure. Structure search can then identify dependencies between the other variables, also through time. We retain the inner loop of Algorithm 1 to find the dependencies that best explain the data, but instead of bootstrapping we restrict the model search space by predefining part of the network structure. The acyclicity of the graph implies thatXt→Etis not present, whereas we need to explicitly blacklist arcs of the typeXt→Et+1for all X. The structure search will identify temporal dependencies but one could argue that since we are interested in the temporal behaviour of the class variable that we should also enforce the presence of the arcEt→Et+1. It turns out that in this specific case of a COPD model, when the arc is not enforced, it is found by the search procedure.In the previous section we described a general method to learn DBNs from small sample time series data. Here we describe the experimental evaluation of the methods for the clinical problem of predicting exacerbations of COPD. The experiments serve to evaluate, first, whether the models that result from the learning procedure are accurate; second, to explore what models can be learned from the COPD data. For the first goal we use synthetic data from a synthetic model, described below. The second goal can be further refined, as we are interested in how the learned models compare to the model constructed in cooperation with a pulmonologist; and we want to find out what the performance of these models is on predicting exacerbation events.To test our model learning procedure we first study the results with synthetic data from a known model. The model contains the same variables as the Aerial data (Section 3) to stay as close as possible to the real data context. The arcs were not chosen to be clinically correct; however, to make the results transferable to the real data the model resembles a clinical model. Analogous to the other models both atemporal and temporal arcs are present. The structure of the model is shown in Fig. 3a. The parameters were assigned by hand, without attempting to capture the real relations. From this model we generated four data sets of the same length as the Aerial data, one without missing values and the others with 10%, 20% and 30% missing values respectively. The latter is similar to the percentage of missing values in the Aerial data. We then applied our model learning procedure to each of these data sets in order to be able to compare the learned models to the known source model.The Aerial data consists of time series from 10 patients, gathered during the pilot study of our disease management system. The characteristics of this data set have been described in Section 3 as part of sketching the background for this work. The data set is used as the input for the learning procedure, to learn models that explain the COPD-monitoring time series.An independent data set [42] was used for validation. It provides time series of COPD-exacerbation related variables, used solely for external validation, so no model parameters were learned from this set. As the data is used for testing our model retrospectively, the set of variables is incomplete and contains 8 out of the 11 variables in the Aerial data. The data consists of time series from 13 patients of the London COPD cohort who had an exacerbation, with a total of 2849 data entries, of which 406 were during an exacerbation. The data contains values for the variables dyspnea, sputum volume and purulence, wheeze, cough, temperature, and oxygen saturation (SpO2). We consider two variants of this data set, the complete validation set, denoted by Dval, and a deduplicated version Ddedup. The latter consists of the same data, but with consecutive identical entries removed, resulting in 605 entries, of which 128 during an exacerbation. The idea behind removing duplicates is that we are interested in predicting relevant state changes instead of finding models that are often correct by predicting that nothing changes. By removing repeating sequences we ensure that we make predictions with data that frequently changes state. Note that we only remove exact duplicates, so a change in the observations without a change in exacerbation label still constitutes a change.To evaluate the models and their performance we will use standard metrics from classification. Classification evaluation is often based on measuring true positives (TP), cases that are correctly classified as positive; false positives (FP), incorrectly classified as positive; and analogously for true/false negatives (TN; FN). The true positive rate (TPR) is then defined asTP/(TP+FN)and the false positive rate (FPR) asFP/(FP+TN). Plotting a curve of TPR-FPR for different cut-off points results in an ROC-curve, which is often summarised in a single number by computing the area under the curve or AUC.We distinguish two situations: (i) the performance of network structure discovery and (ii) the prediction performance of different models. For structure discovery, a true positive is an arc present in both the source and learned network graph and a false positive is an arc not present in the original network. True and false negatives are defined analogously. For prediction, the usual interpretations in terms of data records is used.To evaluate the prediction results we also compute the Brier score [43], which is defined asBS=1∣D∣∑i=1∣D∣(pi-li)2,where p is the predicted probability and l the correct label. A Brier score of zero indicates perfect prediction.The model constructed in cooperation with pulmonologists from the Radboud University Medical Centre, described in [22], is used as a baseline model. This model is static, in the sense that it does not explicitly model time effects. To make temporal predictions we simply interpret the exacerbation probability as the prediction probability at a future time point.Additionally we constructed a dynamic version of the expert model by adding identity arcs to each variable. The graph is shown in Fig. 2. For the additional parameters we choose values heuristically, assuming that remaining in the same state is more likely than switching state. It should be noted that although we refer to this model as dynamic expert model, the pulmonologist was only involved in the construction of the static model and the dynamic version should therefore be seen only as a naive extension for comparison purposes.As a further comparison we also constructed a temporal nodes Bayesian network [21]. This model is based on the static expert model structure described above, where the exacerbation variable is replaced by a temporal node that takes as values whether an exacerbation occurs in the intervals today or tomorrow or does not occur. The intervals are relative to the observations, i.e., the exacerbation temporal node models the probability of an exacerbation on the same day and the day after the measurements have been taken. The other variables are instantaneous nodes, as they model the health state at the time of measurement. Parameters for this model were learned from the Aerial data. Performance was tested using a one against all scheme. For example the prediction performance for tomorrow was tested with tomorrow as the positive class and the other two values as the negative class.

@&#CONCLUSIONS@&#
