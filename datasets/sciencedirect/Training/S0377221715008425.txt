@&#MAIN-TITLE@&#
Hidden Markov model for municipal waste generation forecasting under uncertainties

@&#HIGHLIGHTS@&#
A model-driven learning method is proposed to trace fluctuations dynamically.The fluctuation forecasting under uncertainties is made possible using the HMM.Case studies of different scenarios are presented to show the validity of the HMM.Two forecasting formulas are proposed to cope with two scenarios separately.

@&#KEYPHRASES@&#
Forecasting,Waste generation,Wavelet de-noising,Hidden Markov model,Gaussian mixture model,

@&#ABSTRACT@&#
Waste generation forecasting is a complex process that is found to be influenced by some latent influencing parameters and their uncertainties, such as economic growth, demography, individual behaviors, activities and events, and management policies. These hidden features play an important role in forecasting the fluctuations of waste generation. We therefore focus on revealing the trend of waste generation in megacities which face significant influences of social and economic changes to achieve urban sustainable development. To dynamically trace fluctuations caused by these uncertainties, we propose a probability model-driven statistical learning approach which hybridizes a wavelet de-noising, a Gaussian mixture model, and a hidden Markov model. First, to gain the actual underlying trend, wavelet de-noising is used to eliminate the noise of data. Next, the Expectation–Maximization and the Viterbi algorithms are employed for learning parameters and discerning the most probable sequence of hidden states, respectively. Subsequently, the state transition matrix is updated by fractional predictable changes of influencing parameters to perform non-periodic fluctuation problem forecasting, and the forward algorithm is utilized to search the most similar data pattern for the current pattern from historical data in order to forecast the future trend of the periodic fluctuation problem. Finally, we apply the approaches into two kinds of case studies that test both a small dataset and a large dataset. How uncertainty factors influence forecasted results is analyzed in the subsection of results and discussion. The computational results demonstrate that the proposed approaches are effective in solving the municipal waste generation forecasting problem.

@&#INTRODUCTION@&#
Accurate forecasting of municipal waste generation can provide theoretical guidance in disposal capacity design and capital budget planning for waste management systems (Beigl, Lebersorger, & Salhofer, 2008; Cherian & Jacob, 2012; Intharathirat, Salam, Kumar, & Untong, 2015; Rimaityte, Ruzgas, Denafas, Racys, & Martuzevicius, 2012). Waste generation forecasting in megacities is a complex process that is found to be influenced by some latent influencing factors and their uncertainties, such as rapid economic growth or economic crisis, relentless demographic forces, ever-changing individual behaviors, special activities and events, and complex waste management measures (Cherian & Jacob, 2012; Denafas et al., 2014; Ghiani, Laganà, Manni, Musmanno, & Vigo, 2014; Pires, Martinho, & Chang, 2011). For example, the annual waste generation trend in a megacity could be altered by large-scale activities (such as the Olympic Games and the World EXPO), long-term epidemic diseases (such as SARS), or a new regulation policy for municipal waste. The fluctuation of waste generation comes from unmeasured uncertainties of the economy, individual behaviors, population mobility, and unexpected disasters. The latent factors and their uncertainties are difficult to quantify, and they play an important role in tracing the fluctuation of waste generation dynamically and forecasting the consequences of changes. The fluctuation and nonlinearity of both municipal solid waste (MSW) and wastewater generation is a huge obstacle for accurate forecasting. It is difficult to forecast fluctuation of waste generation accurately via traditional statistical models, such as the moving average and linear regression models.In this study, we focus on fluctuation forecasting problem. Three major contributions are made in this paper: (1) a probability model-driven statistical learning approach, that hybridizes a wavelet de-noising (WDE), a Gaussian mixture model (GMM), and a hidden Markov model (HMM), is proposed to trace waste generation fluctuations under uncertainties dynamically; (2) the way of weighted average Gaussian mixture and the way of searching the most similar pattern are proposed as two concrete HMM-based forecasting ways to cope with the periodic and non-periodic fluctuation problems, respectively; and (3) two kinds of case studies of different scenarios are presented in this study. They correspond to a small sample and a large sample datasets to show the effectiveness of the proposed approaches. The first case uses annual MSW generation in Shanghai and Beijing from 1978 to 2010, and the second case utilizes the daily wastewater level of a Spanish urban sewage treatment plant from 1990 to 1991.The paper is organized as follows: Section 2 summarizes the details of related studies. Problem description and forecasting modelling are presented in Section 3. Section 4 provides two kinds of case studies to test the effectiveness of the proposed approaches. Finally, we close this paper with some conclusions and guidelines for future work.So far, a number of forecasting techniques have been applied to waste generation forecasting. Mainstream methods include traditional statistical models, the gray and fuzzy models, simulation models, and non-probabilistic statistical learning models. In this section, we summarize the application scope, advantages, and weaknesses of these mainstream methods. Subsequently, literature of probability model-driven statistical learning approaches as a comparison is introduced briefly.Traditional statistical models include linear regression models (LRM), autoregressive moving average (ARMA), autoregressive integrated moving average (ARIMA), seasonal ARIMA (sARIMA), exponential smoothing (ES), and seasonal ES (sES). LRM aims to discover a linear function to quantify waste generation. Essentially, the problem of waste generation is nonlinear, so LRM is just an approximate fitting method for waste generation forecasting. The ARMA, a mixture of autoregressive and moving average models, can be employed to forecast waste generation of stationary time series (Katsamaki, Willems, & Diamadopoulos, 1998). ARIMA stochastic models are effective time series forecasting tools which can take influence factors into consideration, and can also process periodic time series data effectively. ARIMA model has been applied to a range of forecasting fields, such as portfolio optimization (Ustun & Kasimbeyli, 2012), and MSW generation (Owusu-Sekyere, 2013). The ES model is performed by utilizing the weighted mean of all values in time series, which assigns exponentially decreasing weights over time. Both sARIMA and sES can take into account seasonal changes and trends (Denafas et al., 2014; Rimaityte et al., 2012; Song & He, 2014; Xu, Gao, Cui, & Liu, 2013). Traditional statistical models can forecast the moving average or discover smooth trend effectively; however, fluctuations are hard to be traced accurately.The gray and fuzzy models are based on fuzzy set theory. The fuzzy mathematic method can solve problems with uncertainty, and can also obtain reliable models given poor data. Chen and Chang (2000) applied a gray fuzzy dynamic prediction technique to forecast MSW generation when the sample numbers were low. Karavezyris, Timpe, and Marzi (2002) employed fuzzy logic to enhance confidence in the validity of the system dynamics model. Fuzzy logic is an attractive method; however, the knowledge base of the fuzzy model is founded on expert experience. Gray model is suitable to study the uncertainty of systems and handle small sample dataset (Intharathirat et al., 2015); however, the conventional gray prediction model is sensitive to initial values (Guo, Liu, Wu, & Tang, 2014).Simulation models are suitable for simulating complex systems which are difficult to express via mathematic formula. System dynamics (SD) is one of the promising of these methods for studying complex feedback systems. SD models have been widely used for predicting waste generation (Ahmad, 2012; Dyson & Chang, 2005; Karavezyris et al., 2002; Kollikkathara, Feng, & Yu, 2010). SD models use a multivariate method and are far more complex due to the multifarious interactions between the selected parameters (Beigl et al., 2008). Therefore, it is hard to achieve model validation.Non-probabilistic statistical learning models for waste generation forecasting mainly include artificial neural networks (ANN) and support vector machines (SVM). ANN, one of the most widespread of intelligent statistical-learning models, has powerful ability to deal with nonlinear forecasting problems (Setzler, Saydam, & Park, 2009) and has been applied to generation forecasting fields (Ali Abdoli, Falah Nezhad, Salehi Sede, & Behboudian, 2012; Antanasijevic, Pocajt, Popovic, Redzic, & Ristic, 2013; Jalili Ghazi Zade & Noori, 2008). Although ANN is promising, the disadvantages, such as over-fitting and local minimum, limit its application. SVM is a nonlinear and kernel-based intelligent technique. Abbasi, Abduli, Omidvar, and Baghvand (2013), Noori, Abdoli, Ghasrodashti, and Jalili Ghazizade (2009), and Abbasi, Abduli, Omidvar, and Baghvand (2014)) proposed SVM models to implement weekly prediction of waste generation in the Iranian cities of Mashhad and Tehran, respectively. Compared with ANN, SVM overcomes the problem of over-fitting training data and local minimum. However, SVM is still a black box technique which cannot establish a mathematical functional mapping between the input variables and the drifting parameters (Dong et al., 2009).Recently, probability model-driven statistical learning approaches have been effectively applied to perform forecasting under uncertainties. Compared with ANN and SVM, they are white box methods which focus more on the components of the mathematical model. For example, Bayesian network (BN) is a combination of graph theory and probability theory which has unique advantages for handling problems with uncertainty, and mining causality among variables. BN has been introduced into forecasting areas, such as traffic flow forecasting (Sun, Zhang, & Yu, 2006) and wastewater inflow monitoring (Cheon, Kim, Kim, & Kim, 2008). The quality of results produced by the BN method largely depends on the amount of evidence (Dong et al., 2009); however, they are not available in most cases. Another example is HMM which became popular for classification and pattern recognition problems after its appearance. The HMM has been applied to a range of fields, such as stock market forecasting (Hassan, 2009), PM2.5 concentration prediction (Dong et al., 2009), failure prognosis (Kim, Jiang, Makis, & Lee, 2011; Zhou, Hu, Xu, Chen, & Zhou, 2010), and manpower planning (Guerry, 2011). However, to the best of our knowledge, no study has previously forecasted waste generation via an HMM.Although the first four stream methods have respective merits for various applications, there are still some difficulties using these methods for measuring and tracing fluctuations of waste generation dynamically. The advantages of the HMM lie in its theoretical basis, rigorous mathematical structure, and its proven fitness for modelling dynamic systems under uncertainties.In this section, the study problems and their characteristics are first introduced. Then, models are constructed to forecast MSW and wastewater generation.Municipal waste generation in megacities is a complex process of the interaction of intertwined systems, which is found to be influenced by latent influencing factors and their uncertainties. So waste generation has the characteristics of nonlinearity, fluctuation, and strong interference. The key challenges for waste generation forecasting problem of both MSW and wastewater lie in generation fluctuations which include non-periodic and periodic fluctuation. Annual MSW generation is a non-stationary time series. The increment rate, transformed from MSW generation, shows an obvious non-periodic fluctuation (Fig. 1). Daily wastewater generation level is an approximate stationary time series with periodic fluctuation (Fig. 2).Two techniques based on HMM are proposed to deal with above challenges of fluctuations:(1)For the periodic fluctuation problem, all data from each period are selected to set up data patterns. For instance, we will obtain 9 patterns as shown in Fig. 2. Once two similar patterns are revealed (e.g., patterns 3 and 6, or patterns 4 and 7), we employ the similar pattern to forecast waste generation trend in the coming period.For the non-periodic fluctuation problem, the variable to be forecasted and other observable factors can be integrated to constitute data patterns. Then, we can use the approach as above to forecast waste generation. However, if the number of searchable patterns is too small, the error of forecasted results becomes unacceptable. In this situation, one alternative forecasting way (weighted average Gaussian mixture) is employed to solve this problem.A probability model-driven statistical learning approach, which hybridizes a wavelet de-noising, a Gaussian mixture model, and a hidden Markov model, is constructed to dynamically trace fluctuations of waste generation. An HMM is a double embedded stochastic process which includes a Markov process and a general stochastic process. Fig. 3 shows a simple HMM component structure sketch with a hidden state sequence and a directly visible observation sequence.In this study, we define the total uncertainty degree of waste generation as a hidden variable. The hidden state sequence is assumed to follow a discrete-time, finite-state, and first-order Markov chain which implies that the current state is dependent only on the previous state. We also regard the variable to be forecasted as an observation which is a continuous random variable. The observation is assumed to be conditionally independent of all others given the state that generated it. Uncertainties do not need to be measured directly; instead, we let historical data tell us what the total uncertainty degree is most likely to be through data analysis.Fig. 4 illustrates the process of HMM-based forecasting which includes three parts: Part I: data preprocessing, Part II: HMM modelling, and Part III: forecasting. There are two scenarios which follow different processes in Part III. Scenario I is for the non-periodic fluctuation problem with small sample sizes, and Scenario II is for the periodic fluctuation problem with large sample sizes. In this paper, we present two kinds of case studies that represent the different scenarios. MSW generation forecasting for Shanghai and Beijing (Scenario I) is carried out via the way of weighted average Gaussian mixture (WAGM) based on an HMM, while wastewater generation forecasting for the Spain Manresa sewage treatment plant (Scenario II) is implemented via the way of searching the most similar pattern (SMSP) based on an HMM.This subsection mainly focuses on the preprocessing process for the whole model. This involves two steps: variable selection and data preprocessing.Step 1. Variable selectionThe total uncertainty of latent influencing factors (non-observable variables) is taken as an integrated hidden variable which is called the total uncertainty degree of waste generation. The variable to be forecasted is regarded as a manifest variable (i.e., observation variable). In Fig. 3, hidden state sequence is composed of different total uncertainty degrees of waste generation, and observation sequence is consists of the actual values of the manifest variable.Step 2. Data preprocessingThis section includes wavelet de-noising and data pattern setup. The former is a tool for removing noise in the data series, and the latter is the basis step of pattern recognition in an HMM.(I) Wavelet de-noisingThe municipal waste generation is fluctuant and nonlinear due to uncertainties. The statistic data of waste generation is not a true reflection of the underlying trend caused by sources of noises. Wavelet de-noising, that uses scaled and translated versions of the mother wavelet ψj, k(x) to build an approximationf˜(x)of an original function f(x), is a reliable method to eliminate the data noise of a time series signal (De Souza e Silva, Legey, & de Souza e Silva, 2010). Wavelet translation can be expressed as follows:(1)f˜(x)=∑∀j,kcj,kψj,k(x)(2)ψj,k(x)=δψ(2jx−k)(3)cj,k=∫−∞+∞f(x)ψj,k(x)dxwhere cj, kis wavelet coefficients; δ is a constant; k decides the translated versions of a wavelet; and 2jis its scale parameter.Ingrid Daubechies invented compactly supported orthonormal wavelets, which makes the analysis of discrete wavelets practicable. The families of orthonormal wavelets are also called Daubechies wavelets. The Symlets are nearly symmetrical wavelets that were proposed as modifications to the Daubechies wavelets (Misiti, Misiti, Oppenheim, & Poggi, 2011, chap. 1). In this study, the Daubechies and Symlets wavelets were employed to carry out wavelet decomposition and synthesis-reconstruction. After wavelet decomposition, data signal could be decomposed into two categories of filters: low-pass filter (the approximation part) and high-pass filter (the detailed parts). The underlying trend of the all factors can be captured by the approximation part, and the noise features can be recorded by the detailed parts. Eq. (4) reconstructs the approximation part (AJ) and the detailed parts (D1,D2,…,DJ) into a new time series data signal (Misiti et al., 2011, chap. 1):(4)S=AJ+D1+D2+⋯+DJwhere S is new signal, and J is scale parameter. After reconstruction, the new time series will be regarded as updated data for training and testing of the WDE-HMM approach. This will be illustrated by a specific example in Section 4.(II) Data pattern setupData patterns that can reflect the characteristics of fluctuation refer to specific and similar data vectors which follow a defined rule. In order to forecast fluctuations of waste generation in a large sample via an HMM, datasets of observations are usually arranged in a specific order so that every data vector forms a pattern which is the basis for data pattern recognition in an HMM. For example, the wastewater generation data for each week (no data on Saturday) of the Manresa sewage treatment plant is arranged in a data pattern [Monday, Tuesday, Wednesday, Thursday, Friday, and Sunday]. This pattern can interpret the wastewater generation behavior of each week. Waste generation in historical municipal activities or incidents can provide reference for future forecasting. When a similar situation reoccurs, this historical pattern can be used to forecast.The main difference between continuous HMMs and discrete HMMs lies in the statistic property of observations. In this study, the observation of each state is a vector of continuous random variables. This subsection includes four steps: continuous HMM modelling, parameter initialization, training and parameter determination, and Viterbi decoding.Step 1. Continuous HMM modellingFig. 5shows the translation relationship of hidden states and observations in a continuous HMM. For each hidden state, a probability exists for the current state to transform into any possible state, while each observation vector (i.e., data pattern) can be observed with a probability in its corresponding hidden state.The primary notations that will be used for the rest of this paper are listed as follows:N: The number of the hidden states of this model;T: Length of the state sequence or observation sequence.t=1,2,…,T;S: A set of states.S={s1,s2,…,si,…,sN}andi=1,2,…,N;Q: State sequence{q1,q2,…,qT}, whereqt=sirepresents the hidden state at time t;O: Observation vector sequence,O={O1,O2,…,Ot,…,OT};π: The prior probability of initial state {πi};A: Transition matrixA={aij}, aijrepresents state transition probability from the hidden state i to state j,j=1,2,…,N;B: A family of probability density functionsB={bj(O)};bj(O): The occurrence probability of observation at state j. bj(O) is illustrated by a widely used multi-dimensional Gaussian distribution:(5)bj(O)=∑kP(Kj=k|st=j)bjk(O)=∑kP(Kj=k|st=j)Φ[O,μjk,Σjk]=∑kcjkΦ[O,μjk,Σjk],1≤j≤N,1≤k≤Kλ: The overall parametersλ=(π,A,B), and∑iπi=1,∑jaij=1,∫−∞+∞bj(x)dx=1.Step 2. Parameter initializationPrior knowledge about probabilities of parametersλ=(π,A,B)is extremely useful for parameter initialization, but it is often not available. For case with prior information, parameters can be initialized easily. For case without prior knowledge, we only consider the basic constraints above to initialize parameters (λ) randomly. Note that the proper number of hidden states is unknown in this step which will be eventually decided in parameter determination of Step 3.Step 3. Training and parameter determinationIn this step, parameters are updated through training the HMM. The most suitable number of hidden states and the proper number of mixture components of a GMM are determined by Akaike's information criterion (AIC).(I) TrainingIn the training step, once we obtain the observation sequenceO={O1,O2,…,Ot,…,OT}the parameters (λ) of an HMM can be trained by the Expectation–Maximization (EM) algorithm (Bilmes, 1998; Rabiner, 1989) which iterates ceaselessly until it converges to the maximum likelihood. After updating all parameters, the new parameter set isλ¯ML=(π¯,A¯,B¯)=argmaxλP(O|λ), whereπ¯={π¯i},A¯={a¯ij}, andB¯={b¯j(O)}. The updated initial state probability (π¯i), transition probability (a¯ij), mixture coefficients (c¯jk), mean vector (μ¯jk), and covariance matrix (Σ¯jk) are expressed as follows:(6)π¯i=ξ1(i)(7)a¯ij=∑t=1T−1ξt(i,j)/∑t=1T−1ξt(i)(8)c¯jk=∑t=1Tξt(j,k)/∑t=1T∑k=1Kξt(j,k)(9)μ¯jk=∑t=1Tξt(j,k)·Ot/∑t=1Tξt(j,k)(10)Σ¯jk=∑t=1Tξt(j,k)·(Ot−μjk)(Ot−μjk)T/∑t=1Tξt(j,k)where ξt(i) and ξt(j) represent the probability of being in siand sjat time t, respectively; ξt(i, j) represents the probability of being in siat time t and in sjat timet+1; ξt(j, k) represents the probability of being in sjat time t with the kth mixture component; andOtis the observation at time t in the observation sequence. The forward–backward variables α and β can be used to illustrate these probabilities:(11)ξt(i)=αt(i)βt(i)/∑i=1Nαt(i)βt(i)(12)ξt(j)=αt(j)βt(j)/∑j=1Nαt(j)βt(j)(13)ξt(i,j)=αt(i)aijbj(Ot+1)βt+1(j)∑i=1N∑j=1Nαt(i)aijbj(Ot+1)βt+1(j)(14)ξt(j,k)=[αt(j)βt(j)∑j=1Nαt(j)βt(j)][cjkΦ[Ot,μjk,Σjk]∑k=1KcjkΦ[Ot,μjk,Σjk]]Since the EM algorithm easily converges to a local optimum or a saddle point, the log-likelihood estimator of the EM algorithm may less than from the maximum log-likelihood estimator. To improve the quality of parameter estimation, in this study, the model is trained by varied initial parameters (λ) with multiple runs to obtain the updated parameters (λ¯) by choosing the biggest log-likelihood estimator.(II) Parameter determinationIt is difficult yet crucial to determine the number of hidden states and components of the GMM in an HMM. Tradeoff always exists between a more accurate result and a smaller computing complexity for this problem. In this paper, AIC (Akaike, 1973), which is a simple and useful model selection criterion based on K–L information, is employed to determine the numbers, as the method attempts to optimize the tradeoff between complexity and goodness of fit (Eq. (15)). We consider the number combination of hidden states and mixture components with the smallest AIC as the best parameter for modeling.(15)AIC=−2ln(L)+2p(16)p=N+N(N−1)+N(K+K(K+1)/2)where L is the likelihood function; p is the number of parameters to be estimated which include state, transition, and emission parameters; N is the number of hidden states; and K is the number of components of the GMM.Step 4. Viterbi decodingIn the decoding step, the new parameter setλ¯and the observation sequenceOare available. The most probable sequence of statesQ*={q1*,q2*,...,qT*}can be calculated by the Viterbi decoding (Rabiner, 1989). Q* can be expressed as:(17)Q*=argmaxQ{P(Q|O,λ_)}Let δt(i) be the maximal probability of state sequences {q1,q2,…,qt,qt=si} at time t which generate the observations {O1,O2,…,Ot} for the given model.(18)δt(i)=maxq1,q2,…,qt−1P(q1,q2,…,qt,qt=si,O1,O2,…,Ot|λ_)The Viterbi decoding is a dynamic programming method.Q*={q1*,q2*,…,qT*}can be derived when achieving recursive results as max[δt(i)].This section includes two steps: forecasting waste generation for the two scenarios in Fig. 4 and data post-preprocessing by the bootstrapping.Step 1. ForecastingThe way of weighted average Gaussian mixture and the way of searching the most similar pattern (Hassan, 2009) can be employed to forecast non-periodic and non-periodic fluctuations via the HMM, respectively. After repeated experiments, we found that the former is suitable for both small sample (approximately 25–30 data points with non-periodic fluctuation) and large sample forecasting, and that the latter is proper for large sample forecasting.(I) Forecasting way for Scenarios IConventionally, a sample size less than 30 is considered small, making it difficult to detect similar patterns from historical data with such limited training data. In this situation, the way of weighted average Gaussian mixture, which integrates states transition and observations probability, is a better choice than the way of searching the most similar pattern.The state transition matrix A(i, j), learned by the training dataset, can reflect historical relations among hidden states. Some fresh information about changes of fractional latent influencing parameters is available at some forecasting points beforehand, such as a new issuing management policy on controlling MSW generation, a deteriorating economic crisis, or an expected large-scale activity. Although the controlling effects of the policy, real deteriorating degree of the crisis, and concrete participation degree of residents in the activity are also uncertain, they can still provide useful information to update the state transition matrix A(i, j) to a new oneA˜(i,j). The forecasted amount Fτat timeT+τis given by:(19)Fτ=Fτ−1(1+rτ)(20)rτ={∑j=1NA˜τ(qT,sj)Ej(X),iffreshinformationexists∑j=1NAτ(qT,sj)Ej(X),ifnofreshinformationwhere rτis the MSW increment rate at timeT+τ, forecasted via the proposed model;F0=OTwhenτ=1; X is a continuous random variable which represents the MSW increment rate; Ej(X) is the expectation of X under hidden state j; qTis the hidden state at time T; sjrepresents one state in the set of statesS={s1,s2,…,sN}; and updatedA˜τ(qT,sj)is the τ-step state transition probability from the state of time T to the state sj.(II) Forecasting way for Scenarios IIFor a large data sample, once the HMM is trained, the log-likelihood estimator of each data pattern can be calculated by the forward algorithm (Rabiner, 1989). Then, we use Eq. (21) to discover the time point with the most similar pattern in historical data to the current pattern (i.e., the pattern at time T).(21)t*=argmint{|LLET−LLEt|}1≤t≤T−1where LLETand LLEtare log-likelihood estimators at current time (T) and historical time (t), respectively.Theoretically, the nearest log-likelihood estimators can be discerned at time point t* from the historical data. The waste generation behavior of time point t* is most similar to that of current time point T. We assume that the increment rate that follows the reference point is the same as the point we are attempting to forecast. Thus, the ith forecasted amount in the data pattern at timeT+1isFi:(22)Fi=OTi[(Ot*+1i−Ot*i)/Ot*i+1]1≤i≤Iwhere I is the element number of the data pattern defined in a specific case.Step 2. Data post-processingDiscovering the most similar pattern is crucial for the second forecasting way. Due to the local optimum characteristic of EM algorithm, the HMM would find several similar patterns to the current one from the historical data. The local optimum may be a better predictor of behavior than a temporally distant global optimum as similar patterns that are closer in time should have more impact on one another. Here we make use of this fact by training the model via multi-simulation of K runs. After each multi-simulation training, we extract the most similar pattern and calculate a forecasted amount Fk(k=1,2,…,K). Then, the bootstrapping method (Efron, 1979) was utilized to estimate 95 percent confidence intervals of the forecasted amount{F1,F2,…,FK}, which is an especially proper statistical resampling algorithm for small sample dataset with unknown distribution.This section illustrates two kinds of cases, that are yearly MSW generation with non-periodic fluctuation for both Shanghai and Beijing (case I), and daily wastewater generation with periodic fluctuation for the Manresa sewage treatment plant (case II). The original datasets were extracted from the Shanghai Statistical Yearbook 2011 (Shanghai Bureau, 2011), the Beijing Statistical Yearbook 2011 (Beijing Bureau, 2011), and the UC Irvine Machine Learning Repository (Bache & Lichman, 2013), respectively.To assess the forecasting accuracy, four frequently used criteria were applied for comparisons of the proposed models and other models. The mean absolute error (MAE), the mean square error (MSE), the coefficient of determination (R2), and the adjusted coefficient of determination (adjustedR2) are given by:(23)MAE=1n∑i=1n|Ai−Fi|(24)MSE=1n∑i=1n(Ai−Fi)2(25)R2=1−SSE/SST=1−∑i=1n(Ai−Fi)2/∑i=1n(Ai−A¯)2(26)adjustedR2=1−n−1n−k−1SSE/SSTwhere Aiis the actual amount; Fiis the forecasted amount; n is the testing sample size; k is the number of explanatory variables.ARIMA and sARIMA, ES and sES, ANN, and SVM, introduced in Section 2, are all widely used and effective models to perform waste generation forecasting. Therefore, they were also implemented to compare and assess the effectiveness of the proposed models. For building ARIMA and sARIMA models, the autocorrelation function plot and AIC were both considered to discover the best-fitted model from the profile of different parameters (p, d, q). To build ES model for non-seasonal trend forecasting, the best suitable model is selected out among simple ES, double (Brown) ES, linear (Holt) ES, and damped-trend linear ES. To analysis seasonal trend by sES model, simple sES, winters additive algorithm based ES, and winters multiplicative algorithm based ES were tested on the formatted time series. The most suitable ES or sES models were selected by the Expert Modeler in the SPSS 19.0 software. The BP neural network, the most frequently used architecture of ANN, was utilized in this study, which is a three-layer network including input layer, hidden layer, and output layer. The classical Levenberg–Marquardt backpropagation algorithm is employed to update weights and bias states. Optimized parameters of ANN are given in Sections 4.1.3 and 4.2.3. The SVM regression was performed using the LIBSVM software (Chang & Lin, 2011). SVM regression with four types of kernel functions (i.e. linear, polynomial, radial basis function, and sigmoid) were tested to find out the most appropriate kernel function for the given application, according to criteria of MSE and R2 during training and testing stages. A two-step grid search method consisted of coarse grid search and finer grid search was used to select the best parameter of gamma function and loss function. In the process of parameters selection, the cross validation was employed to avoid over fitting problem.

@&#CONCLUSIONS@&#
In this study, we have proposed a probability model-driven statistical learning approach to forecast the fluctuant generation of municipal waste under uncertainties. The proposed approaches, which hybridizes a wavelet de-noising, a Gaussian mixture model, and a hidden Markov model, were tested by two kinds of case studies. They are Shanghai and Beijing annual MSW generation with non-periodic fluctuation, and daily wastewater generation with periodic fluctuation of the Manresa sewage treatment plant. The results have indicated that the HMM approach generally outperforms the ARIMA, sARIMA, ES, sES, ANN, and SVM approaches in both MAE, MSE, R2, and adjusted R2 criteria. The WDE-HMM approach performs better after the wavelet de-noising. The computational results have demonstrated that the proposed approaches are effective in solving the municipal waste generation forecasting problem.We presented two concrete forecasting ways, the way of weighted average Gaussian mixture (WAGM) and the way of searching the most similar pattern (SMSP), to forecast the MSW generation with non-periodic fluctuation, and the wastewater generation with periodic fluctuation, respectively. Actually, different steps of state transition in the state probability matrix can be chosen in the way of WAGM. Therefore, the way of WAGM is suitable for short or medium term forecasting of both small and large samples; while the way of SMSP is advantageous for short term forecasting with large samples.In this study, we have implemented the fluctuation forecasting of waste generation which is found to be influenced by some latent influencing factors and their uncertainties. How uncertainty factors influence forecasted results was illustrated in the subsection of results and discussion. In MSW generation cases, the fresh information is effectively utilized to update the state transition matrix to better foresee future trend.Future work is needed to improve the EM algorithm. Due to the local optimum characteristics of the EM algorithm, the model was trained repeatedly. This required a significant computing effort. In addition, one can further attempt to forecast middle-term waste generation via an improved hidden Markov model. Furthermore, in this study, the transition matrix is time homogeneous in each forecasting process although it can be updated online when new data is available, and can be updated once fresh information is obtained. However, non-homogeneous hidden Markov models are likely to achieve higher forecasting accuracy.