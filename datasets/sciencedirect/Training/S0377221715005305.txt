@&#MAIN-TITLE@&#
A new algorithm for linearly constrained c-convex vector optimization with a supply chain network risk application

@&#HIGHLIGHTS@&#
Vector optimization is studied.A proximal point algorithm is proposed for vector optimization.The global and local convergence results for the new algorithm are presented.The efficiency of the new algorithm is shown by an application to a supply chain network risk management problem.

@&#KEYPHRASES@&#
Multiple objective programming,Pareto optimum,C-convex,Proximal point algorithm,Supply chain network risk management,

@&#ABSTRACT@&#
We study a class of vector optimization problems with a C-convex objective function under linear constraints. We extend the proximal point algorithm used in scalar optimization to vector optimization. We analyze both the global and local convergence results for the new algorithm. We then apply the proximal point algorithm to a supply chain network risk management problem under bi-criteria considerations.

@&#INTRODUCTION@&#
Vector-valued optimization stems from multi-objective programming, multi-criteria decision making, statistics, and cooperative game theory (Handi, Kell, & Knowles, 2007; Jahn, 2003). Such optimization problems have been extensively studied and applied in various decision-making contexts. Let C ⊆ Rmbe a convex, closed and pointed cone with int(C) ≠ ∅. We define the partial orders ⪯ and ≺ as follows, for any u, v ∈ Rm,u⪯v⟺v−u∈C;u≺v⟺v−u∈int(C). In this paper, we consider the class of linearly constrained C-convex vector optimization problems,(1.1)minCf(x)s.t.Ax=b,x∈S,wheref:Rn→Rmis C-convex,A∈Rl×n,b∈RlandS⊆Rnis the constraint set, which is assumed to be compact and convex. Our goal is to propose an implementable proximal point algorithm to find a weak Pareto optimum (resp. Pareto optimum) for (1.1), namely a pointx*∈X:={x∈Rn|Ax=b,x∈S}such that ∄ x ∈ X satisfying f(x) ≺ f(x*) (resp. f(x) ⪯ f(x*)). Note that “minC” denotes the weak Pareto or Pareto optimum with respect to the cone C. It is known that many practical problems can be cast in the format of (1.1). The interested reader is referred to Pardalos and Hearn (1999, Chap. 10), and Ehrgott (2000) for the detailed modeling in this regard.Since the seminal work by Moreau (1965), proximal point algorithms have been successfully applied to scalar optimization problems (Liu, Sun, & Toh, 2012). Since then, they have well used in the optimization literature (Martinet, 1970; Meng, Zhao, Goh, & De Souza, 2008; Rockafellar, 1976b). Recently, proximal point algorithms have also been extended to multi-criteria and vector optimization problems (Villacorta & Oliveira, 2011), albeit without the numerical tests and applications. In this paper, we propose a proximal point algorithm for vector optimization, with a specific application to a supply chain network risk problem with bi-criteria considerations.The classical iterative methods for solving scalar optimization problems proposed for solving multi-criteria optimization problems seek a Pareto optimum by starting from a non-Pareto optimal point. Further, to compute a point satisfying the first-order conditions for Pareto optimality, descent-type methods that are independent of the scalarization approaches have been proposed for solving multi-criteria optimization (Bello Cruz, 2013; Bello Cruz, Lucambio Perez, & Melo, 2011; Qu, Goh, & Chan, 2011; Qu, Liu, Goh, Li, & Ji, 2014b). Bello Cruz et al. (2011) discuss the convergence of the gradient method for differentiable quasiconvex multi-objective optimization. Bello Cruz (2013) presents a subgradient method for vector optimization problems without using scalar-valued objectives. Different from the descent-type methods, our method aims at extending the proximal point methods to vector optimization problems with linear constraints. Proximal point algorithms are already widely used in the literature. To obtain a weak Pareto optimum, Bonnel, Iusem, and Svaiter (2005) extend proximal point algorithms to vector optimization and present both exact and inexact versions, in which the subproblems are only solved approximately. Villacorta and Oliveira (2011) propose an interior proximal method based on the generalized distance function for vector optimization. Qu, Goh, Souza, and Wang (2014a) propose a proximal algorithm for solving convex multi-criteria optimization with linear constraints. Bento, Cruz Neto, and Soubeyran (2014) present a proximal point-type method for multicriteria optimization with the help of a variable scalarization function. Our method is in line with the two methods in Bento et al. (2014); Bonnel et al. (2005) that we seek to extend the classical iteration methods for scalar optimization to vector optimization. The main difference between our method and that in Bonnel et al. (2005) is that we focus on finding weak Pareto or Pareto optimal solutions for linearly constrained vector optimization, while the method in Bonnel et al. (2005) aims at finding weak Pareto solutions for vector optimization without constraints. There are two differences between our method and the method in Bento et al. (2014). The authors Bento et al. (2014) establish the variable scalarization function withC=R+m. We present the algorithm by assuming that C ⊆ Rmis a convex, closed and pointed cone with int(C) ≠ ∅. Moreover, Bento et al. (2014) assume the problem is unconstrained. Our method allows for linear constraints.The primary contributions of this paper are as follows. We extend the proximal point algorithm to a vector optimization setting and demonstrate the power of the new algorithm in solving vector optimization problems. We prove global convergence for the new algorithm and analyze its convergence rate. We apply the new method to supply chain network risk management, where conflicting bi-criteria considerations can affect performance.This paper is thus organized as follows. Section 2 presents some preliminaries on the notations, and proximal point algorithms. Section 3 extends the proximal point algorithm to a vector optimization setting and presents the convergence of the new algorithm. The convergence rate for the new algorithm is also presented in this section. An application to a supply chain network risk management problem is presented in Section 4. Section 5 concludes.We say that functionf:Rn→RmisC−convexiff for anyx,y∈Rnand any λ ∈ [0, 1],f(λx+(1−λ)y)⪯λf(x)+(1−λ)f(y).We say that function f is strictlyC−convexiff for anyx,y∈Rnand any λ ∈ (0, 1),f(λx+(1−λ)y)≺λf(x)+(1−λ)f(y).The subdifferential of f at x is defined as follows:∂f(x):={V∈Rm×n:f(x)+V(y−x)⪯f(y),∀y∈Rn}.This definition follows from Bello Cruz (2013), and ∂f is nonempty convex and compact (see Theorem 4.12 of Luc, Tan, and Tinh, 1998). Whenm=1andC=R+,∂f collapses to the usual classical subdifferential (Rockafellar, 1970). The indicator function of a setS⊂Rnis defined as δ(x; S) ≔ 0 if x ∈ S andδ(x;S):=+∞,if x∉S. The normal cone to a convex set S atx¯∈Sis defined byNS(x¯):=∂δ(x¯;S)={x*∈Rn∣<x*,x−x¯>≤0,∀x∈S}.Define the positive polar cone of C,C*:={y∈Rm:yTx≥0,∀x∈C}and let the quasi-interior of C* beC♯:={y∈Rm:yTx>0,∀x∈C∖{0}}. Given any closed setU⊂{y∈C*:∥y∥=1},we show that a weak Pareto optimum (or Pareto optimum) to (1.1) is equivalent to finding an optimal solution to the following convex optimization problem,(2.1)minxmaxy∈UyTf(x)s.t.Ax=b,x∈S.The next theorem follows directly from Theorem 2.2 of Jahn (1984) (see also Jahn, 2003; Luc, 1987). In Theorm 2.2 of Jahn (1984), the scalarization function is implicitly defined. In this paper, as we explicitly present the scalarization problem (2.1), for the sake of completeness we provide the proof.Theorem 2.1LetU⊂{y∈C*:∥y∥=1}be a closed set and x*be a solution of (2.1). Then,1.x*is a weak Pareto optimum to problem (1.1);if U is a subset of the quasi-interior of C*, i.e., U⊂C♯, then x*is a Pareto optimum of (1.1);if x*is a unique optimal solution of (2.1), then x*is a Pareto optimum of (1.1);if function f is strictly C-convex, then x*is a Pareto optimum of (1.1).1.If x* is not a weak Pareto optimum, then∃x¯∈Xsuch thatf(x¯)≺f(x*),which implies thatyTf(x*)>yTf(x¯),∀y ∈ U. Therefore, the following inequalities hold,(2.2)minx∈Xmaxy∈UyTf(x)≤minx∈Xmaxy∈UyTf(x¯)<minx∈Xmaxy∈UyTf(x*)≤minx∈Xmaxy∈UyTf(x)which is a contradiction. Hence the conclusion is true.If x* is not a Pareto optimum, then∃x¯∈Xsuch thatf(x¯)⪯f(x*),which together with the assumption that U ⊂ C♯ and the definition for C♯, implies thatyTf(x*)>yTf(x¯),∀y ∈ U. A similar contradiction as (2.2) can be derived. Therefore, the assertion holds.If x* is not a Pareto optimum, then∃x¯∈Xwithx*≠x¯such thatf(x¯)⪯f(x*),which means thatyTf(x¯)≤yTf(x*),∀y ∈ U. Therefore,x¯is also a solution to (2.1), contradicting the uniqueness of the solution of (2.1). Hence, x* is a Pareto optimum of (1.1).Suppose x* is not a Pareto optimum of (1.1), i.e.,∃x¯∈Xsuch thatf(x¯)⪯f(x*),which impliesyTf(x¯)≤yTf(x*),∀y ∈ U. Define the Lagrangian of (2.1) as,L(x,γ):=maxy∈UyTf(x)+γT(Ax−b)+δ(x;S).It follows from the optimality of (2.1) that∃γ∈Rlsuch that 0 ∈ ∂Lx(x*, γ). Since f is strictly C-convex, so function L is strictly convex about x, which implies thatL(x¯,γ)>L(x*,γ)+vT(x¯−x*),∀v ∈ ∂Lx(x*, γ). Settingv=0yieldsmaxy∈UyTf(x¯)>maxy∈UyTf(x*),which contradictsyTf(x¯)≤yTf(x*),∀y ∈ U. Hence, x* is also a Pareto optimum to (1.1).□Remarks1.In Theorem 2.1, we only assume the closedness of U without convexity. Actually, we can use the convex hull of U in (2.1) without changing our problem, that is, an optimal solution of (2.1) is equivalent to finding an optimal solution of the following problem,(2.3)minx∈Xmaxy∈Conv(U)yTf(x),where Conv( · ) denotes the convex hull. To prove this, we first cast (2.1) in the following form, min{r| (r, x) ∈ Ω}, withΩ:={(r,x)∈Rn+1|x∈X,yTf(x)≤r,∀y∈U}. DefineΩ˜:={(r,x)∈Rn+1|x∈X,yTf(x)≤r,∀y∈conv(U)}. Now the proof reduces to showing thatΩ=Ω˜. Clearly,Ω˜⊂Ω. We show thatΩ⊂Ω˜. IfΩ=∅,the assertion is obvious. Now assume that Ω ≠ ∅ and for any (r, x) ∈ Ω, we show that(r,x)∈Ω˜. Given any y ∈ Conv(U), ∃yi,i=1,…,k,such thaty=∑i=1kλiyi,where λi≥ 0(i=1,…,k)and∑i=1kλi=1with somek≤m+1. It follows from the definition of Ω that (yi)Tf(x) ≤ r,i=1,…,k. Therefore,yTf(x)≤∑i=1k(yi)Tf(x)≤r,which implies that(r,x)∈Ω˜. Then,Ω⊂Ω˜. This implies that we can always assume that U is closed and convex. For the rest of this paper, we assume thatU⊂{y∈C*:∥y∥=1}is closed and the cone generated by its convex hull is C*.WhenC=R+m,U⊂{y∈R+m:∥y∥=1}and problem (2.1) reduces tominx∈Xmaxj∈Ifj(x)sinceC*=R+mand the convex hull of U isR+m. Theorem 2.1 (1) and (4) collapse to Theorem 2.1 (1) and (2) proposed by Qu et al. (2014a), respectively. In this case, the proximal point algorithm presented in this paper collapses to Algorithm 1 given in Qu et al. (2014a). Therefore, the method in this paper is a generation of that in Qu et al. (2014a).Theorem 2.1 shows that a Pareto optimum or weak Pareto optimum of (1.1) is equivalent to finding an optimum of (2.1). In the following section, we present a proximal point algorithm for solving the vector optimization problem (1.1) based on the scalarization optimization problem (2.1). We note that this method mainly focuses on finding one solution. Recently, there have been many methods proposed to obtain a solution of vector problems by extending the classical iteration methods (for example, gradient-type methods, Newton-type methods, proximal point-type algorithms) for scalar optimization (Bello Cruz, 2013; Bello Cruz et al., 2011; Bonnel et al., 2005; Qu et al., 2011; Qu et al., 2014a, 2014b). Our method is in line with these ideas. However, when there is a need to obtain the Pareto surface (the Pareto optimal solution set), our method may fail. This can be shown by the following example proposed by Antoni and Giannessi (2014).ExampleConsider the following bi-level optimization problem (upper level):(2.4)minxx12+(x2−12)2,s.t.x∈K0,where K0 is the Pareto optimal solution set of the following vector optimization problem,(2.5)minR+2f(x),s.t.x∈K:={x∈R2:g(x)≥0},withf(x):=(f1(x):=2x1−x2f2(x):=−x1+2x2),g(x):=(g1(x):=2x1+x2−1g2(x):=x1+2x2−1).“minR+2” marks the Pareto optimum with respect toR+2. By Theorem 2.1 of Qu et al. (2014a) and Proposition 13 of Giannessi, Mastroeni, and Pellegrini (2000), the Pareto optimum x* of the lower level problem satisfiesx*=(13,13),orx1*=1−2x2*,0≤x2*<13,orx2*=1−2x1*,0≤x1*<13. Then, the optimal solution to the corresponding bi-level problem (2.4) isx*=(15,35). As the original bi-level problem is cast in the form of a mathematical optimization problem with complementary constraints, we use the numerical solver PATH of Ferris and Munson (2000) to solve the resulting problem. By using PATH, we can also find the optimal solutionx*=(15,35)of (2.4). We reformulate the lower level problem as (2.1) and it is obvious that the optimal solution of the reformulated problem is equivalent to finding the optimal solution of the following linear optimization problem,min(x,t)∈R+2×Rt,s.t.fj(x)≤t,j=1,2,g(x)≥0.It can be found that(x*,t*)=(13,13,13)is the unique optimal solution andx*=(13,13)is a Pareto optimum to the lower problem from Theorem 2.1(3). Therefore, in this case, the optimal solution to the corresponding bi-level problem (2.4) isx*=(13,13). Since the last method fails to find the entire solution set K0, we cannot find the optimal solution to the bi-level problem.We now present a proximal point algorithm for solving (1.1).Step 0. Select an initial pointγ0∈Rland a sufficiently small numberϵ>0; set a constant θ > 0; set k ≔ 0;Step 1. At the kth iteration,xk+1is an optimal solution of the following subproblem,(3.1)minx∈S{maxy∈UyTf(x)−(γk)T(Ax−b)+θ2∥Ax−b∥2};Step 2.γk+1can be generated as follows,(3.2)γk+1:=γk−θ(Axk+1−b);Step 3. If∥(xk+1,γk+1)−(xk,γk)∥≤ϵ,stop; else, setk:=k+1and go to Step 1.Remarks1.Let us consider our algorithm in the linearly constrained scalar case, i.e.,U={1}. In this case, (3.1) becomesminx∈S{f(x)−(γk)T(Ax−b)+θ2∥Ax−b∥2}.This together with (3.2) is the augmented Lagrangian method in the literature for solving the linearly constrained scalar optimization (Hestenes, 1969; Powell, 1969), where γkis the Lagrange multiplier and θ > 0 is the penalty parameter for the violation of the linear constraints. In Rockafellar (1976a), it was shown that this augmented Lagrangian method is exactly the application of proximal point algorithm to the dual problem of the linearly constrained scalar optimization. Therefore, in Gu, He, and Yuan (2014); Rockafellar (1976a), this augmented Lagrangian method for solving linearly constrained scalar optimization is also named as the proximal point algorithm.WhenC:=Rm+,the algorithm in this paper reduces to Algorithm 1 of Qu et al. (2014a).Since f is C-convex,maxy∈UyTf(x)is convex for x ∈ S. Therefore, at every iteration, a convex subproblem needs to be solved. Note that even if the f is linear,maxy∈UyTf(x)is not necessarily a differentiable function. Therefore, it is difficult to directly solve subproblem (3.1). In this paper, to effectively resolve this subproblem, we focus on the special case ofC:=R+m. WhenC:=R+m,this subproblem is equivalent tominx∈St−(γk)T(Ax−b)+θ2∥Ax−b∥2s.t.fj(x)≤t,j=1,…,m,wheref:=(f1,…,fm)andfj:Rn→R,j=1,…,m. As f is C-convex andC=R+m,it is obvious that fj,j=1,…,m,are all convex. Therefore, the above problem is convex. When fj,j=1,…,m,are all continuously differentiable, there is a variety of methods to solve this problem, for example, SQP methods, trust region methods and the package CVX for specifying and solving convex programs (Grant & Boyd, 2011).When matrix A is column full rank, thenθ2∥Ax−b∥2is strictly convex with θ > 0 and the subproblem in Step 1 is strictly convex (sincemaxy∈UyTf(x)is convex). For this subproblem, if x is an optimal solution and satisfiesAx=b,then it is an optimum to Problem (2.1) and similar to the proof of Theorem 2.1(4), it is also a Pareto optimum to (1.1).The next theorem presents the convergence results for our algorithm in Section 3.1. WhenC:=Rm+,this theorem reduces to Theorem 3.1 of Qu et al. (2014a).Theorem 3.1Suppose that the sequence {(xk, γk)} is generated by the proximal point algorithm inSection 3.1, then1.any accumulation point of {xk} is a weak Pareto optimum to (1.1);if U⊂C♯, then any accumulation point of {xk} is a Pareto optimum to (1.1);if function f, is strictly C-convex, then any accumulation point of {xk} is a Pareto optimum to (1.1);if matrix A is column full rank, then any accumulation point of {xk} is a Pareto optimum to (1.1).1.Recall that a weak Pareto optimum to (1.1) is the point x* ∈ S such that for some y ∈ C* with∥y∥=1,∃γ*∈Rl,the following conditions hold0∈∂f(x*)Ty−ATγ*,Ax*−b=0.From the definition for U that the cone generated by its convex hull is C*, then there exists a positive integer q, yi∈ U and λi> 0 withi=1,…,qsuch thaty=∑i=1qλiyiand∑i=1qλi=1. Therefore, a weak Pareto optimum x* ∈ S satisfies0∈∑i=1qλi∂f(x*)Tyi−ATγ*,Ax*−b=0,∑i=1qλi=1,γ*∈Rl,yi∈U,λi>0,i=1,…,q.Given y ∈ C* with∥y∥=1andγ∈Rl,define the Lagrangian to (1.1) as follows,L(x,y,γ):=yTf(x)−γT(Ax−b),x∈S.We show that the weak Pareto optimum of (1.1) is equivalent to finding a saddle point of the above Lagrangian.The point(x*,y*,γ*)∈S¯withS¯:={(x,y,γ)∈S×C*×Rl|∥y∥=1}is a saddle point of L iff the following inequalities hold,(3.3)(x−x*)T((V*)Ty*−ATγ*)≥0(3.4)(γ−γ*)T(Ax*−b)≥0(3.5)∀(x,γ)∈S×Rl,V*∈∂f(x*).Inequalities (3.3) and (3.4) are the first order weak Pareto optimum conditions of (1.1). Similar to the above discussion, there exists a positive integer q* andyi*∈Uandλi*>0withi=1,…,q*such thaty*=∑i=1q*λi*yi*,∑i=1q*λi*=1,and the point(x*,y*,γ*)∈S¯is a saddle point of L iff the following inequalities hold,(3.6)(x−x*)T(∑i=1q*λi(V*)Ty*i−ATγ*)≥0,yi*∈U,λi*>0,i=1,…,q*,(3.7)(γ−γ*)T(Ax*−b)≥0,(3.8)∀(x,γ)∈S×Rl,V*∈∂f(x*),∑i=1q*λi*=1.According to (3.1) and (3.2), the following inequalities hold(3.9)(x−xk+1)T(vk+1−ATγk+1)≥0(3.10)(γ−γk+1)T(Axk+1−b+1θ(γk+1−γk))≥0(3.11)∀(x,γ)∈S×Rl,vk+1∈∂F(xk+1),whereF(x):=maxy∈UyTf(xk+1). It follows from the formula for the subdifferential of a maximum of convex functions and the assumption about U that there exists a positive integer qkandyik∈Uandλik>0with 1 ≤ i ≤ qksuch that(3.12)∑i=1qkλik=1,yikf(xk+1)=maxy∈UyTf(xk+1),i=1,…,qk,(3.13)vk+1=∑i=1qkλik(Vk+1)Tyik,Vk+1∈∂f(xk+1).Therefore, (3.9) can be rewritten as,(3.14)(x−xk+1)T(∑i=1qkλik(Vk+1)Tyik−ATγk+1)≥0,whereyik∈Uandλik>0with 1 ≤ i ≤ qkand satisfying (3.12). This implies that(xk+1,γk+1)generated by the proximal point algorithm satisfies (3.9), (3.11) and (3.14).As S is compact and convex, without loss generality, we assume that the proximal point algorithm does not stop finitely andxk+1→x*,k → ∞. It follows from the boundedness of{xk+1}and Lemma 2 (Bello Cruz, 2013) that{Vk+1}withVk+1∈∂f(xk+1)is bounded. Therefore, it can be assumed thatVk+1→V*,k → ∞. From (3.12) and (3.13), there exists a positive integer q* andyi*∈Uandλi*>0with 1 ≤ i ≤ q* such that∑i=1q*λi*=1,yi*f(x*)=maxy∈UyTf(x*),i=1,…,qk,v*=∑i=1q*λi*(V*)Tyi*,v*∈∂F(x*),V*∈∂f(x*).Therefore, letting k → ∞ in (3.14), we can show that (3.6) holds for any x ∈ S. Further, if{γk+1}is convergent, then taking k → ∞ in (3.10), (3.7) holds for anyγ∈Rl. Hence, the limit point (x*, γ*) of{(xk+1,γk+1)}satisfies (3.6)–(3.8) and is a weak Pareto optimum of (1.1). Thus, in the rest of the proof of this assertion, we show that{γk+1}is convergent.The proof of{γk+1}being convergent is equivalent to provingγk+1−γk→0,as k → ∞. To this end, we first show that(3.15)(γk−γ*)T(γk−γk+1)≥∥γk−γk+1∥2.From (3.9) and (3.14),∀(x,γ)∈S×Rl,Vk+1∈∂f(xk+1),the following inequality holds(3.16)(x−xk+1γ−γk+1)T{∑i=1qkλikΓik+1+(01θ(γk+1−γk))}≥0,which implies that(3.17)1θ(γ−γk+1)T(γk+1−γk)≥(xk+1−xγk+1−γ)T∑i=1qkλikΓik+1,whereΓik+1:=((Vk+1)Tyik−ATγk+1Axk+1−b). By setting (x, γ) ≔ (x*, γ*) in (3.17) as the weak Pareto optimum for (1.1), then the right side of (3.17) is nonnegative from the monotonicity of ∂f(x). This together with (3.17) shows that the left side of (3.17) is also nonnegative, i.e.,(γk−γ*)T(γk−γk+1)≥∥γk−γk+1∥2. Therefore we have(3.18)∥γk+1−γ*∥2=∥γk−γ*∥2+∥γk+1−γk∥2+2(γk+1−γk)T(γk−γ*)≤∥γk−γ*∥2,which implies that {γk} is convergent. Hence, the first assertion of Theorem 3.1 is true.The above conclusion means that any accumulation point of {xk} is a feasible solution of (1.1). Therefore, it follows from the above proof and Theorem 2.1(2) that this conclusion holds.Similar to the proof of Theorem 2.1(4), we can show that this conclusion is true.It follows from Remark (2) of the proximal point algorithm and Theorem 2.1(3) that this assertion holds.□Before we analyze the convergence rate for the proximal point algorithm in Section 3.1, we state the next lemma.Lemma 3.2Suppose the sequence {(xk, γk)} is generated by the proximal point algorithm inSection 3.1, then there exists a positive integer qk,yik∈Uandλik>0with∑i=1qkλik=1such that the following inequality holds,(3.19)(x−xk+1γ−γk+1)T∑i=1qkλikΓik+1+12θ[∥γ−γk∥2−∥γ−γk+1∥2]≥12θ∥γk+1−γk∥2;whereΓik+1is defined as in (3.16),i=1,…,qk.It is evident from (3.17) that(3.20)(x−xk+1γ−γk+1)T∑i=1qkλikΓik+1≥1θ(γ−γk+1)T(γk−γk+1).The right side of (3.20) satisfies(3.21)1θ(γ−γk+1)T(γk−γk+1)=12θ[∥γ−γk+1∥2+∥γk−γk+1∥2−∥γ−γk∥2].Therefore, it follows from (3.20) and (3.21) that Lemma 3.2 holds.□From the convergence of the proximal point algorithm, for sufficiently large k, there exists a positive integer q*,yi*∈Uandλi*>0with∑i=1q*λi*=1,such that inequalities (3.20) and (3.21) hold in the above lemma withqk=q*,λik=λi*,andyik=yi*,i=1,…,q*. In the following theorem, without any loss of generality, we assume that, for any k, (3.20) and (3.21) hold in the above lemma withqk=q*,λik=λi*,andyik=yi*,i=1,…,q*.Theorem 3.3Assume that the sequence {(xk, γk)} is generated by the proximal point algorithm inSection 3.1. For an integer κ > 0, define(3.22){(x¯κ,γ¯κ)}:=1κ+1∑k=0κ{(xk+1,γk+1)},then the following inequality holds, for all(x,γ)∈S×Rl,(3.23)(x¯κ−xγ¯κ−γ)T∑i=1q*λi*Γi(x,γ)≤12θ(κ+1)∥γ−γ0∥2,whereΓi(x,γ):=(VTyi*−ATγAx−b),for any given V ∈ ∂f(x).It is obvious that for any integer κ > 0,{(x¯κ,γ¯κ)}=1κ+1∑k=0κ{(xk,γk)}∈S×Rl. It follows from (3.19) that,∀(x,γ)∈S×Rl,(3.24)(x−xk+1γ−γk+1)T∑i=1q*λi*Γik+1+12θ[∥γ−γk∥2−∥γ−γk+1∥2]≥0,which is equivalent to,∀(x,γ)∈S×Rl,(3.25)∑i=1q*λi*(x−xk+1γ−γk+1)TΓik+1+12θ[∥γ−γk∥2−∥γ−γk+1∥2]≥0,whereΓik+1is defined as in (3.16),i=1,…,q*. Then from the above inequality and the monotonicity ofΓi(·,·),∀i=1,…,q*,we have(3.26)∑i=1q*λi*(x−xk+1γ−γk+1)TΓi(x,γ)+12θ[∥γ−γk∥2−∥γ−γk+1∥2]≥0.Summing the above inequality overk=0,1,…,κ,we obtain,∀(x,γ)∈S×Rl,(3.27)((κ+1)x−∑k=0κxk+1(κ+1)γ−∑k=0κγk+1)T∑i=1q*λi*Γi(x,γ)+12θ[∥γ−γ0∥2−∥γ−γκ+1∥2]≥0,which implies that∀(x,γ)∈S×Rl(3.28)((κ+1)x−∑k=0κxk+1(κ+1)γ−∑k=0κγk+1)T∑i=1q*λi*Γi(x,γ)+12θ∥γ−γ0∥2≥0.The above inequality is equivalent to∀(x,γ)∈S×Rl(3.29)(1κ+1∑k=0κxk+1−x1κ+1∑k=0κγk+1−γ)T∑i=1q*λi*Γi(x,γ)≤12θ∥γ−γ0∥2.Therefore it follows from (3.22) and the above inequality that Theorem 3.3 holds.□We now apply the proposed algorithm to supply chain network risk management. We consider a bi-criteria model, i.e.,C:=R+2andU={y∈R+2:∥y∥=1}. We propose two bi-criteria optimization approaches which considers three statistics: the expected value, variance and conditional value-at-risk measure at a specified confidence level.Consider a supply chain network comprising a producer, multiple suppliers, and multiple customers. The producer purchases parts from the suppliers in the network and assembles several families of products to meet customer demand. Let ρibe the disruption probability for supplier i, that is the parts ordered from supplier i cannot be delivered due to disruptions with probability ρi. Denote Psas the probability when a disruption scenario s is realized. Suppose each scenario s ∈ S comprises a unique subset Is⊂ I of suppliers who can deliver parts without disruption, where I is the index set of the suppliers andS:={1,…,2|I|}is the index set of all scenarios with |I| indicating the number of suppliers. Then Psin the presence of independent disruptive events can be expressed as follows,Ps:=∏i∈Is(1−ρi)·∏i∉Isρi.Let i and j represent the index of the m suppliers and n customer orders, respectively. Let Mibe the capacity of supplieri∈I:={1,…,m}. Let ciand qirepresent the cost of ordering parts from supplier i and the expected defect rate of supplier i respectively. For eachj∈J:={1,…,n},letd˜jand scjbe the number of parts to be purchased and the per unit shortage cost for customer order j respectively. Denote by pijthe price of a part for customer order j purchased from supplier i. Denote the expected total demand for parts asD:=E[∑j∈Jd˜j]. Let the decision variable zidenote an order for parts placed on supplier i withzi=1or 0 and xidenote the total demand allocation variable which is a fraction of the total demand for parts ordered from supplier i. Let yijbe the customer order allocation variable which is the fraction of parts required for customer order j ordered from supplier i.1.Model 1: bi-criteria optimization with variance. We now present a bi-criteria optimization problem with covariance for a single-period supplier selection and order allocation problem that determines how to allocate a static supply portfolio based on the model of Sawik (2011). The static supply portfolio is defined asX:={x∈Rm:∑i∈Ixi=1,0≤xi≤1,∀i∈I},where xiis the fraction of the total demand for parts ordered from supplier i. The supply portfolio can be measured by the expected costE[R˜(x)]withR˜(x):=∑i∈IciziD+∑i∈I∑j∈Jpij(1+qi)d˜jyijD+∑s∈S∑i∉Is∑j∈JPs(scj−pij(1+qi))d˜jyijDwhere the cost has three parts: ordering, purchasing and defects, and the cost of a parts shortage following a supply disruption. The mean-variance model can then be presented for selecting a supply portfolio by minimizing the expected cost as well as minimizing the variance ofR˜(x)subject to some constraints, that is, the following bi-criteria optimization problem,(4.1)minx∈Rnf(x):=(E[R˜(x)],σ2(R˜(x)))s.t.∑i∈Iyij=1,∀j∈J,E[∑j∈Jd˜jyij]≤Mizi,∀i∈I,zi≤∑j∈Jyij,∀i∈I,zi∈{0,1},∀i∈I,yij∈[0,1],∀i∈I,j∈J,whereE[R˜(x)]andσ2(R˜(x)):=E[(R˜(x)−E[R˜(x)])2]are the expectation and variance ofR˜(x)respectively. If the portfolio returnR˜(x):=∑i=1mxiRifrom choicex:=(x1,…,xm),thenσ2(R˜(x))=∑i=1m∑j=1mxixjσij,where σijis the covariance of Riand Rj, and thus the variance is expressed as a quadratic function ofx1,…,xm. Based on the solution of the customer order allocation {yij} to the above problem, the resulting supply portfolio(x1,…,xm)can be stated asxi=E[∑j∈Jd˜jyijD],∀i ∈ I. For simplicity, we denote by Y the constraints for the above problem. Since R is a linear function about x, the varianceσ2(R˜(x))is convex about x, making the above bi-criteria optimization problem convex.Model 2: bi-criteria optimization with conditional value-at-risk measure. Given a specified level α ∈ (0, 1], define the following functionFα(x,v):=1αE{[−R˜(x)+v]+}−v,where[u]+=uif u ≥ 0, otherwise[u]+=0,then the conditional value-at-risk measure at a specified confidence level about the random variableR˜(x)can be given asCVaRα(R˜(x))=minv∈RFα(x,v). The optimization problem about the conditional value-at-risk measure with respect to x ∈ X is equivalent to the optimization problem about Fαwith respect to(x,v)∈X×R,that isminx∈XCVaRα(R˜(x))=min(x,v)∈X×RFα(x,v). WhenR˜(x)is a discrete random variable, calculating and optimising the conditional value-at-risk measure are linear optimization problems (Meng, Sun, & Goh, 2010). SupposeR˜(x)has T possible outcomesR1(x),…,RT(x)with probabilitiesγ1,…,γT,then we have the following conclusionFα(x,v)=1α∑t=1Tγt{[−Rt(x)+v]+}−v. Thus, under a specified confidence level α, we can present the bi-criteria optimization problem with conditional value-at-risk measure for a supply selection portfolio,minx∈Rnf(x):=(E[R˜(x)],CVaRα(R˜(x)))s.t.((y11,y12,…,ymn),(z1,…,zm))∈Y,where Y is the feasible set defined in (4.1).We present some numerical tests using the proposed algorithm in Section 3.1 to solve the above two models. All codes are written in Matlab 7.10 with built-in solver “fmincon” to solve the approximation problems. The tests are conducted on a DELL computer with Intel(R)Core(TM)i5-2400 processor (3.10 gigahertz) and 4.00 gigabyte of memory on a Windows 7 platform. We discuss the implementation details for the proximal point algorithm in Section 3.1 that is the solution to the optimization subproblem and the stopping criterion. For this purpose, we rewrite the integrality constraints by introducing m strictly quadratic convex constraints, that is replacing the constraints zi∈ {0, 1}, \,∀i ∈ I withzi2+(1−zi)2=1,∀i∈I. Through the assumptions, the objective functions are all convex, so the above problems can be solved by the methods proposed so far. Define(4.2)A=(1,…,1︸m0,…,…,00,…,0,1,…,1︸m,0,…,0⋮⋮⋮0,…,…,0,1,…,1︸m)∈Rn×mn,y:=(y11,y21,…,ym1,y12,…,ym2,…,ymn)T∈Rmn,z:=(z1,…,zm)T∈Rm. Then the first constraint in (4.1) can be rewritten asAy=1. We present the implementation details for the proximal point algorithm in Section 3.1: θ ≔ 20, the stopping criterion is∥(yk+1,zk+1)−(yk,zk)∥∞+∥γk+1−γk∥∞≤10−6.For the numerical tests of Models 1 and 2, the definitions for the parameters can be found in the literature of Sawik (2011), where djis uniformly generated from the set [100, 500]. Therefore, from the above discussion, the variance is equivalent to a convex quadratic function and optimising the conditional value-at-risk measure is equivalent to solving a linear optimization problem. To show the efficiency of the proposed algorithm in this paper, we compare the performance of the algorithm of this paper with Algorithm 2 of Qu et al. (2014a). With m suppliers and n customer orders, Table 1lists the numerical results for the applications of the algorithm on this paper and Algorithm 2 of Qu et al. (2014a) to Models 1 and 2 respectively. In the numerical tests, the maximum number of customer orders n is set at 100. The specified confidence level α under the conditional value-at-risk measure is set at 0.99. The number of iterations (“Iter”) and computing time in seconds (“CPU”) are reported in Table 1. Generally, the proposed algorithm in this paper outperforms Algorithm 2 of Qu et al. (2014a) on CPU time, suggesting that the algorithm is efficient and the theoretical assertions about the new algorithm are verified.

@&#CONCLUSIONS@&#
