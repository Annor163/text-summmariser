@&#MAIN-TITLE@&#
Knot calculation for spline fitting via sparse optimization

@&#HIGHLIGHTS@&#
We reduce the computation time dramatically by solving convex optimization problem.We can simultaneously find a good combination of the knot number and knot locations.The algorithm has less knots with good fitting performance compared to other methods.We can recover the ground truth knots when data is sampled enough from a B-spline.

@&#KEYPHRASES@&#
Spline fitting,Knot calculation,Sparse optimization,

@&#ABSTRACT@&#
Curve fitting with splines is a fundamental problem in computer-aided design and engineering. However, how to choose the number of knots and how to place the knots in spline fitting remain a difficult issue. This paper presents a framework for computing knots (including the number and positions) in curve fitting based on a sparse optimization model. The framework consists of two steps: first, from a dense initial knot vector, a set of active knots is selected at which certain order derivative of the spline is discontinuous by solving a sparse optimization problem; second, we further remove redundant knots and adjust the positions of active knots to obtain the final knot vector. Our experiments show that the approximation spline curve obtained by our approach has less number of knots compared to existing methods. Particularly, when the data points are sampled dense enough from a spline, our algorithm can recover the ground truth knot vector and reproduce the spline.

@&#INTRODUCTION@&#
Curve fitting with splines is a traditional and fundamental problem in many engineering practices. In Computer Aided Design (CAD) and Geometric Modeling, curves are fitted with splines to reconstruct geometric models from measurement data  [1–4]. In signal processing and image processing, splines are often adopted to process noisy signals or to approximate complicated functions  [5,6].The intuitive idea of curve fitting with splines is to formulate it as a least-square problem when knots are fixed. However, the fitting result is not always satisfactory. Actually, it has long been known that freeing knots in fitting improves the result dramatically  [7–10]. But spline fitting with free knots is still a challenging problem. The reasons are as follows. First, analytic expressions for optimal knot locations, or even for general characteristics of optimal knot distributions, are not easy to derive  [11]. Second, the unknown number and position of knots result in a large and nonlinear optimization problem, which is computationally very difficult.In the literature many methods have been proposed to optimize knots with a given number of knots. The problem of knot placement is formulated as a nonlinear optimization problem with the constraint that knots should form a nondecreasing sequence. The first type of techniques transforms the constrained optimization problem into an unconstrained problem, then local gradient-based method or Gauss–Newton method are employed for minimization  [11–13]. However, local optimization methods require a good initial guess and cannot guarantee global optimality. The second type of techniques applies global optimization to avoid the drawbacks of local methods, but it is computationally more expensive  [14–16]. There are also some works which utilize the underlying feature information of the data to select knots, instead of solving a nonlinear optimization problem  [1,17,18]. However, in such methods the number of knots is determined beforehand and the results are sensitive to measurement noises.Another approach for knot calculation is based on knot-removal strategy which is to reduce the number of knots of a given spline by perturbing the spline within a given tolerance  [19,20]. The main idea of the technique is to remove interior knots according to assigned weights. For data approximation, a piecewise linear approximation of the data is computed, then knot removal strategy is performed on the linear approximation, and finally the data is approximated by a smooth spline with computed knots.A new development in recent years for knot calculation is based on sparse optimization  [21,22]. Sparsity is the core of compressed sensing which is widely used in computer vision and signal processing  [23–27]. Sparsity means that a signal can be represented in a linear combination of some bases or dictionaries such that most of the combination coefficients are zero. In  [21], the authors formulated the spline fitting problem as a convex optimization problem, where thel1norm of jump of third order derivatives ofC2cubic splines is minimized; while in  [22], the authors first selected a subset of basis functions from the pre-specified multi-resolution basis set using the statistical variable selection method-Lasso, then identified a concise knot vector that is sufficient to characterize the vector space spanned by the selected basis functions to fit the data. These two methods can compute the number and positions of the knots simultaneously, yet they still produce a lot of redundant knots.Targeting on the limitations of existing methods, we propose a computationally efficient framework to calculate knots for splines fitting via sparse optimization. The framework is composed of two stages: firstly we solve a convex sparse optimization model starting from a dense initial knot vector. The output is those knots (which we call active knots) at which a certain order derivatives of the fitting spline is discontinuous. The idea to formulate the optimization model in this step is the same as that in  [21] but with a distinct formulation. Secondly, we adjust the active knots in the first stage by certain rules to remove redundant knots. Furthermore, several theoretical results about the algorithm are established in this paper. In particular, when the data points are sampled dense enough from a spline, the knots of this spline can be recovered by the proposed framework in any given precision.The remainder of the current paper is organized as follows. In Section  2, we review some preliminary knowledge about B-splines and least-square fitting with B-splines. In Section  3, a two-stage framework of curve fitting with B-splines is described. Some related theoretical results are also presented. In Section  4, we illustrate the effectiveness of the proposed method through numerical experiments and comparisons with existing methods. Finally, in Section  5, we conclude the paper with discussions on future research problems.We refer to the fundamental book  [1] for a complete treatment of splines. Here we simply introduce the adopted notations which are needed for presenting our results.Let{ci}i=0n∈Rdben+1control points, andNip(t)be the B-spline basis functions of degreepdefined on a knot vectorU={t0,t1,…,tn+p+1}withti≤ti+1,i=0,1,…,n+p, then a B-spline curve of degreepis defined by(1)c(t)=∑i=0nciNip(t),whereNip(t)is defined recursively as follows:(2)Ni0(t)={1,t∈[ti,ti+1)0,otherwise(3)Nip(t)=t−titi+p−tiNip−1(t)+ti+p+1−tti+p+1−ti+1Ni+1p−1(t),p≥1.Whend=1,c(t)is called a B-spline function and the control points are called spline coefficients. In this paper, we only consider data fitting with B-spline functions.Uis often chosen as an open knot vector, namely boundary knots are set toa=t0=t1=⋯=tp,tn+1=⋯=tn+p+1=b. The knotsti,i=p+1,…,nare called interior knots ofU. The multiplicity of an interior knottiis denoted bymi(mi≤p+1). An interior knottiis called an active knot ofc(t)if the(p+1−mi)th order derivative ofc(t)is discontinuous atti, otherwise it is called an inactive knot ofc(t). Fig. 1shows aC2continuous cubic B-spline function and its first three derivatives. The spline has 9 interior knots (marked by crosses) and 3 active knots (marked by black crosses) where the third order derivatives are discontinuous.Thekth order derivative ofc(t)is a B-spline of degreep−k:(4)c(k)(t)=Πi=1k(p+1−i)∑i=knci(k)Nip−k(t),with(5)ci(k)={ci,ifk=0,ci(k−1)−ci−1(k−1)ti+p+1−k−ti,ifk>0.The Fourier transform of thejth basis functionNjp(t)is defined as(6)N̂jp(t)=∫−∞−∞Njp(t)eiwtdt=(p+1)!(iw)p+1∑k=jp+1+jeiwtkθ′(tk),whereθ(t)=Πk=jp+1+j(t−tk),w∈Rrepresents frequency. For uniformly distributed knots, the Fourier transform can be simplified as:(7)N̂jp(t)=(ei|τ|w−1i|τ|w)p+1,where|τ|is defined as|τ|=maxi(ti+1−ti).Given a set of data{Pi}i=1N, and corresponding parameter values{si}i=1N, the least square approximation with splines is defined(8)minc(t)∑i=1N(c(si)−Pi)2,wherec(t)is a spline function defined by (1). When the knot vectorUof the spline functionc(t)is fixed, problem (8) is reduced to(9)minC∈Rn+1‖P−AC‖2,whereP=(P1,…,PN)T,A=(aij)N×(n+1)withaij=Njp(si), andC=(c0,c1,…,cn)Tis the coefficient vector.IfAhas rankn+1, thenATAis nonsingular, thus the solutionCof problem (9) is obtained uniquely by(10)ATAC=ATP.The sufficient and necessary conditions forAto have rankn+1are stated by Schoenberg and Whitney  [28]. IfAdoes not have full rank, the solutionCis defined as the solution which minimizes‖C‖2among all the solutions of (10).In this section, we will present a two-stage framework of knot calculation for spline fitting in detail. We start with an outline of the algorithm. Then the sparse optimization model and knot adjustment strategy are described respectively.For data fitting with spline, it is often expected that the number of knots should be as few as possible under the condition that the fitting performance is good enough. Supposec(t)is defined as in (1) with an initial knot vectorU={t0,t1,…,tn+p+1}, then the number of valid knots can be described by the number of nonzero elements in the vectorv=(|c(p)(tj+)−c(p)(tj−)|)j=p+1j=nwhich describes the jumps ofpth order derivatives ofc(t)at the interior knots, becausec(p)(t)now is a piecewise constant function and the breakpoints are interior knots. Thus to minimize the number of knots, one can applyl0-norm minimization technique to vectorv, which has been widely used in computer vision and signal processing.Let{Pi}i=1Nbe given data points with corresponding parameters{si}i=1N, andc(t)be a B-spline function defined by Eq. (1). Then the fitting problem is formulated as(11a)minnci∈R,i=0…ntj,j=p+1,…,n‖Jc(p)‖0(11b)s.t.∑i=1N(c(si)−Pi)2≤Nεwhere thel0norm‖⋅‖0indicates the number of nonzero elements in the vector,ε>0is a fixed tolerance to control the quality of fit andJc(p)is the jump vector ofpth order derivatives ofc(t)at the interior knots, defined asJc(p)=(Jc(p)(tp+1),…,Jc(p)(tn))withJc(p)(tj)=c(p)(tj+)−c(p)(tj−),j=p+1,…,n. For smallp, an explicit formula forJc(p)can be written down based on (5). In this paper, we will restrict our experiments on fitting with cubic spline functions.Problem (11) has three parts of unknowns: spline coefficients{ci}i=0n, knot numbernand interior knot vector{ti}i=p+1n. Since a spline is a non-convex and nonlinear function of knot values, problem (11) is a non-convex and nonlinear optimization problem which is very difficult to compute. To find an optimal solution, we start from a dense knot vector to search for the best possible number of knots and their locations simultaneously by solving a convex optimization problem, followed by a further knot adjustment strategy, see Fig. 2for reference.For the first step, an initial knot vector containing enough knots for representing the details of data is provided. We then find a spline function with provided initial knots to fit the data with least square approximation by solving problem (11) except that the knots are fixed. That means we find a spline approximation with fewest number ofpth order derivative jumps. The nonzero jumps correspond to some active knots which are used as the input for the second step.For the second step, our goal is to further decrease the knot number while keeping the fitting quality. Only pruning knots from the input knot vector cannot fix redundant knot problem. So here we adopt the following adjustment strategy. For each interval[tik,tik+1]of the input active knot vectorŨ={tik}k=1m, we insert the middle pointt∗in the interval to obtain a new vectorU∗and compute the approximated degreepspline functionc∗(t)by solving problem (11) withU∗. Ift∗is an active knot ofc∗(t), then we replace the interval[tik,tik+1]with one of the following two intervals[tik,t∗]and[t∗,tik+1]. This process is repeated until the length of the interval is small enough or the interval does not contain any parameterssi. We then merge the interval to a single knot.As an illustration example shown in Fig. 2, data points are uniformly sampled in[0,1]withN=101from the functionf(t)=1.0/((x−0.5)2+0.02). The toleranceε=0.005and the initial knots are chosen as 11 equidistant points in[0,1]. Fig. 2(a) shows the approximated B-spline (red curve) and the corresponding active knots (blue triangles) obtained in the first step. Fig. 2(b) depicts the approximated B-spline together with the final five interior knots by the second step. The five interior knots are (0.2000, 0.3941, 0.5000, 0.6066, 0.8000), where the knots 0.3941 and 0.6066 are the results of performing adjustment strategy on the two intervals (0.3, 0.4) and (0.6, 0.7) respectively.In this section, we firstly present details of formulating and solving the sparse optimization model, then we provide some theoretic results about the properties of such a problem, which are helpful for understanding and designing strategy in second stage.Suppose we have fixed an initial knot vectorU={t0,t1,…,tn+p+1}, then problem (11) can be reformulated as(12a)minci∈R,i=0…n‖Jc(p)‖1(12b)s.t.∑i=1N(c(si)−Pi)2≤Nε.Here we changel0norm of objective function tol1norm for the ease of solve:‖Jc(p)‖1≡∑j=p+1n|c(p)(tj+)−c(p)(tj−)|.We call (12) as sparse optimization model. Since it is a convex optimization problem, it can be solved efficiently. For initial knots inU, they are usually chosen uniformly in domain[a,b]. The authors  [22] proposed a way of estimating the number of knots for given sample points according to the Fourier transformation of B-splines and Nyquist–Shannon sampling theorem  [29]. The idea is as follows. A basis function can be treated approximately as a band limited signal with band width[−2π/|τ|,2π/|τ|]from Eq. (7). Additionally, there areNdata points in the interval[a,b], so the sampling rateωequalsN(b−a). If we want to capture all the information contained in the given data, then the basis functions should contain frequency components with frequency at leastω/2according to Nyquist–Shannon sampling theorem, that is2π/|τ|≥ω/2, hence|τ|≤4π/ω. Then the number of interior knots of the knot vector should be at least1/|τ|≥N4π(b−a). For example, supposea=0,b=1,N=1001, then there should be at least 80 interior knots.It should be pointed out that the above estimate only provides a lower bound for initial knot number. In practice, initial knots are often selected more than the lower bound to capture the fine features of data points.When the jump|c(p)(tj+)−c(p)(tj−)|is close to zero, the corresponding knottjis pruned. The remaining knots are called active knots. When problem (12) is solved, the active knots ofc(t)can be consequently determined. The objective function in problem (12) favorsc(t)has few number of active knots. Consequently most of the initial knots are pruned. Notice thatc(t)can be represented as a spline defined over the active knots, that is, inactive knots can be deleted without changing the spline function.In this section, we study the solution of problem (12) when data points are sampled dense enough from a spline function. We illustrate it withp=0. Suppose data points{Pi}i=1Nwith corresponding parameter values{si}i=1Nare sampled from a piecewise constant function with breakpointsk1<⋯<kl, and a B-splinec(t)of degreep=0defined over an initial knot vectorU={t0,t1,…,tn,tn+1}is used to fit the data points via problem (12). We are interested in the characteristic of splinec(t)at the intervalIi0=[ti0,ti0+1]which contains a breakpoint.Without loss of generality, supposeki∈[ti0,ti0+1]and the value sampled atkiis shown in Fig. 3(a). For convenience,ci−1<ciis assumed. The value ofc(t)at the three intervalsIi0−1,Ii0,Ii0+1are denoted byd1,d2,d3respectively. We also suppose the data points in the two adjacent intervals ofIi0have been fitted well, that isd1=ci−1,d3=ci, see Fig. 3(b) for reference. Notice that the optimal value of problem (12) does not depend ond2as long asci−1≤d2≤cisince the sum of the jump inc(t)atti0andti0+1equals|d2−d1|+|d3−d2|=d3−d1=ci−ci−1. But the fitting errorE=∑i=1N(c(si)−Pi)2depends ond2. Particularly, the fitting error at an intervalIi0is defined byEi0=∑si∈Ii0(c(si)−Pi)2. We are going to study how the fitting errorEi0, thusE, changes when a midpoint is inserted inIi0. There are two cases.(1) First case: none of the parameters{si}i=1Nlies inIi0. Fig. 3(b) shows this case, where the horizontal dashed (solid) line segments are marked for the value of true solution (approximated solution) onIi0. For this case, we merge the endpointsti0andti0+1as the midpointt∗=0.5(ti0+ti0+1), and the resulting knot vector is denoted byU∗={t0,…,ti0−1,t∗,ti0+2,…,tn+1}.A splinec∗(t)defined onU∗is constructed as shown in Fig. 3(c). Substitutingc∗(t)in problem (12), it can be seen that both the objection value and fitting error do not change comparing to that ofc(t). But the number of knots ofc∗(t)is one less than that ofc(t).(2) Second case: amongsi,i=1,2,…,N, there aren1parameters in[ti0,ki]andn2parameters in[ki,ti0+1]. Then the valued2should beci−1n1+cin2n1+n2to minimize the fitting errorEi0and consequently we haveEi0=n1n2(ci−ci−1)2n1+n2. For this case, we insert the midpointt∗=0.5(ti0+ti0+1)inIi0and the resulting knot vector is denoted byU∗={t0,…,ti0,t∗,ti0+1,…,tn+1}. A splinec∗(t)defined onU∗is constructed as shown in Fig. 3(d), whered2∗=ci−1n0+cin2n0+n2and the corresponding fitting error atIi0is denoted byEi0∗=n0n2(ci−ci−1)2n0+n2, wheren0is the number of parameters ofsilying in[t∗,ki](or[ki,t∗]). Comparing toc(t), the objection value of problem (12) do not change, but the fitting errorEi0∗<Ei0ifn0<n1.The above analysis inspires us to subdivideIi0into two subintervals[ti0,t∗]and[t∗,ti0+1], then replaceIi0by one of the subintervals to make the fitting error decrease. This process can be repeated until the length of the interval is small enough or the interval does not contain any parametersi. The two end points of the interval is then merged into one single knot.For general casep≠0, when the given data points are sampled from a B-spline of degreep, we have a similar conclusion. We summarize the observation in the following theorem.Theorem 3.1Suppose the data points are sampled dense enough from a B-spline of degreepwith true knotsk1<⋯<kl,ε>0is small enough, and the initial knot vector isU={t0,t1,…,tn+p+1}. The active knots selected by problem   (12)   are collected increasingly in a vectorU˜={ti1,ti2,…,tim}. Then for eachkj,j=1,…,l, there existsj1,1≤j1≤m−1such thatkj∈[tij1,tij1+1]andij1+1=ij1+1.ProofWe prove the conclusion by contradiction. Without loss of generality, supposek1∈[ti1,ti2], buti1+1≠i2.c(t)is the approximated B-spline solved by problem (12) with knot vectorUand parameterε. The optimal value of problem (12) with respect toc(t)is denoted asTV. As assumedi1+1≠i2, then there exists a knott∗inUsuch thatti1<t∗<ti2. Andt∗must be an inactive knot ofc(t).Now we consider a new B-splinec∗(t)of degreepdefined on a knot vectorU∗which is defined as follows: Ifk1∈[t∗,ti2],U∗={t∗,ti2,…,tim}; otherwiseU∗={ti1,t∗,ti3,…,tim}(here for simplicity, we ignore the boundary knots ofU∗). LetTV∗=‖Jc∗(p)(t)‖1as defined in problem (12), thenc∗(t)is determined by solving a least square approximation of the given data points with the constraintTV∗=TV.Notice that the sampled B-spline function is a piecewise polynomial in[ti1,ti2]since there is an active knotk1∈[ti1,ti2]. The constructed new splinec∗(t)happens to be a piecewise polynomial in the interval[ti1,ti2], whilec(t)is a continuous polynomial. This meansc∗(t)has one more degree of freedom thanc(t)when they approximate the sampled data points. So the fitting error ofc∗(t)is less than or equal to the one ofc(t), that is∑i=1N(c∗(si)−Pi)2≤∑i=1N(c(si)−Pi)2≤Nε.Firstly, it can be asserted that the fitting error ofc∗(t)is less than that ofc(t), otherwise there will be two different solutions with the same optimal value for the problem (12) which contradicts with the uniqueness of solution of problem (12).Then, because problem (12) is a convex optimization, so the fitting error of the optimal solution must be equal toNε. ThusTV∗cannot be an optimal value, thenTVis not an optimal value becauseTV=TV∗, thenc(t)is not the optimal solution of problem (12) which contradicts with the assumption.Above all, ifk1∈[ti1,ti2]withi1+1≠i2, thenc(t)is not the optimal solution to the problem (12). In other words, ifk1∈[ti1,ti2], then it must havei1+1=i2. Hence the theorem is proved.□Remark 3.11.Forp=0, we haved2=ci−1n1+cin2n1+n2. Suppose|ci−ci−1|<η, then|d2−ci|=|n1(ci−ci−1)n1+n2|<ηn1n1+n2<η. Analogously, we have|d2−ci−1|<η. Ifηis small enough, thend2≈ciord2≈ci−1. Additionally, whenn1≪n2orn2≪n1, we also haved2≈ciord2≈ci−1. For such two cases, we may not have the conclusion in Theorem 3.1, that is the two knotsti0,ti0+1which are adjacent tokimay not be contained inU˜. For general degreep, when the jump atkjis too small or the parameters of data points aroundkjare distributed unevenly, it may haveij1+1≠ij1+1, that istij1ortij1+1may not be inU˜. Thus in the second step the true knots cannot be recovered precisely or even omitted in some cases. Fortunately such kinds of knots always have little effect on fitting performance and there is no redundancy phenomenon around them from the experiments.If some true knots are contained in several adjacent intervals, such aski∈[ti0,ti0+1],ki+1∈[ti0+1,ti0+2],…, then the conclusion in Theorem 3.1 may not be true. So the initial knots are always chosen dense enough to avoid this case.We take Fig. 4as an illustration for Theorem 3.1. Data points are randomly sampled in[0,1]withN=101from a B-spline associated with three interior knots 0.21, 0.5, and 0.93. The toleranceε=10−6and the 201 initial knots are chosen uniformly from[0,1]. Fig. 4(a) shows the approximated B-spline (red curve) and the associated active knots (blue triangles) selected in the first step. For each true knot, the two initial knots adjacent to it are selected as active knots. It should be pointed out that the requirement (εshould be small enough) in Theorem 3.1 is necessary. Fig. 4(b) shows the active knots and the corresponding approximated B-spline for the caseε=0.001, where the two knots adjacent to knot 0.93 are not included in the active knots set.Fig. 4 also illustrates that the knots of the approximated splines only by the first step are still much more than expected. Fortunately these active knots are clustered into three groups cleanly and are distributed intensively around a true knot. Such characteristic is not special when the data are sampled from a spline function. The following subsection will discuss how to design the knot adjustment strategy on account of this characteristic.LetUbe the initial knot vector,U˜be the set of active knots selected by problem (12) andk1<k2<⋯<klbe the real knots of a given B-spline functionf(t)as defined in Theorem 3.1. ObviouslyU˜is a subset ofU. And if[tj,tj+1]contains a knot of the given B-spline, thentjandtj+1must be inU˜based on Theorem 3.1. And such interval can be narrowed down to a single knot of the B-spline function. In this subsection, we will discuss how to find such interval and how to merge such interval to a single knot.For each interval[tik,tik+1], we insert the middle pointt∗of the interval inU˜to get a new knot vectorU∗={ti1,…,tik,t∗,tik+1,…,tim}.And the sparse optimization problem (12) is solved again withU∗. According to Theorem 3.1, if there is a real knot off(t)in[tik,tik+1], thent∗will be selected as an active knot, and,tikortik+1will become an inactive knot. For convenience and numerical stability, we usually find the minimal one of three jumps of thepth derivative of the approximated B-spline at three knotstik,t∗,tik+1and if the jump attikortik+1is the minimal one, then the interval[tik,tik+1]is considered as the candidate interval which will be narrowed down to a single knot. We conclude this idea in the functionflag=IsCandidateInterval (i,t,Err).For each candidate interval[tik,tik+1], we replace interval[tik,tik+1]with either[tik,t∗]or[t∗,tik+1]depending on the local fitting error, wheret∗is the middle point of the interval[tik,tik+1]. Or in other words, we replace either knottikortik+1witht∗. This process is repeated until the interval does not contain any parameterssior the length of the interval is small enough. We then merge the two end points of the interval into a single knot.The detailed procedure for adjusting knots is summarized in Algorithm 1.In general, for data points sampled from a spline function, the strategy of finding a candidate interval can be simplified. The simplification is based on the observation that the selected active knots by the first step are separated into several groups naturally. So firstly we separate the input knotsU˜by the functionG=ClusterClassify(δ,U˜) according to knot spacingδ. In detail, supposeU˜(i)belongs to thejth groupGj, ifU˜(i+1)−U˜(i)>δ, thenU˜(i+1)belongs to the(j+1)th groupGj+1. The knot spacingδequals the length of two adjacent interior knots in the initial knot vectorU. With this function, the candidate intervals are found without solving problem (12) again.In addition, in order to allow inserting multiple knots in an interval, a comparison betweenEdandEpis added, whereEdandEpare the fitting errors of the least square approximated B-spline defined onMdtandMptrespectively. HereMdtandMptare the knot vectors obtained by inserting a single knot and a double knot inU˜, which are defined in Algorithm 4. We summarize this adjustment strategy in Algorithm 4.Now we are ready to describe the algorithm to fit data points by a B-spline.Inputdata points{(si,Pi)}i=1N, degreep, initial knot vectorU, toleranceε.approximated B-splinec∗(t).Solve optimization problem (12) and the resulting active knots are denoted byU˜.Locally adjust the knots inU˜by Algorithm 1 or Algorithm 4. The resulting knot vector is regarded asU∗.Solve least squares problem (9) and the approximated spline is the outputc∗(t).The proposed Algorithm of fitting with B-splines have the following convergence property.Theorem 3.2Suppose the data points are sampled dense enough from a B-spline of degreepwith true knotsK={k1≤⋯≤kl}, then the final knot vectorU∗obtained by the above algorithm is an approximation of the true knotsKwith the given tolerancetol. More precisely, for every true knotki, there is a knottik∈U∗satisfying|tik−ki|<tol, wheretolis the input parameter in Algorithm  1.ProofThis is obviously true based on Theorem 3.1 and Algorithm 1. □Remark 3.21.When the data points are sampled from spline function, Algorithm 4 is much faster and more effective than Algorithm 1. For general given data points, the selected knots by the first step do not have obvious phenomenon of cluster, so it is better to use Algorithm 1. Above all, if the knots selected by the first step are separated into several groups obviously, then Algorithm 4 is preferred.For all numerical examples in this section, we use cubic B-splines (p=3) for least square fitting. The mean squared error (MSE for short) and the maximum error (ME for short), defined asMSE=∑i=1N(c(si)−Pi)2/NandME=maxi=1,…,N‖c(si)−Pi‖respectively, are used to measure the fitting equality.In Section  4.1, the Titanium heat data are fitted by our method to show the advantage of our method which favors fewer knots. In Section  4.2, the data points sampled from a Chebyshev polynomial of degree 10 are approximated by our method to demonstrate the effectiveness of approximation of functions. In Section  4.3, the proposed method is applied to the data points which are sampled from a B-spline. This example shows the exclusive property of recovering knots of a sampled spline with enough sampling by our method. We have implemented our algorithm on a PC with four-core Intel i5 CPU and 4GB RAM. All computations are done in Matlab and the CVX  [30] package is called for solving sparse optimization problem (12).Firstly we consider the Titanium heat data with 49 observations  [31]. The data are hard to fit well by traditional techniques due to the sharp peak in the data, and have therefore often been used to test spline fitting algorithms  [11,12,31,32]. The data originally ranged from[595,1075]and are normalized to[0,75]in this paper. The normalization using linear change of scale has no effect on the optimal knot distribution.The initial knots are chosen as 101 equidistant knots in[0,75]andε=0.0017. Fig. 5(a) shows the eight active knots (triangles) selected in the first step and the corresponding fitted spline (dashed line). Fig. 5(b) shows the approximated B-spline together with five interior knots by the second step. The fitting errorMSEof our fit is 0.014128.Table 1lists the interior knots and residual error of the approximated spline using our method and other two works  [21,22]. The residual error is defined by(∑i=1Nwi(c(si)−Pi)2N−1)1/2withw1=wN=1/2,wi=1,i=2,…,N−1. From the comparisons in Table 1, we can see that the second step of our method indeed reduces redundant knots and favors one less knot comparing to the other two methods  [21,22], which indicates the significance of the second step. Additionally, the five knots calculated by our method are very close to the five knots selected by Jupp’s method  [11] in which it has been identified that the five interior knots are optimal when the knots number is 5. However, the knots number is fixed in advance by Jupp’s method while is found automatically by our method. The last line in Table 1 shows the five optimal knots selected by Jupp’s method.In this subsection, we approximate the Chebyshev polynomialT10of degree 10 on the interval[−1,1]which has been considered by T. Lyche and K. Mørken  [20]. The data points are sampled fromT10at 401 uniformly spaced points. In the paper  [20], firstly the knot removal strategy was used for the linear interpolate of the data points, then the knot removal strategy was used again for the cubic spline which was transformed from the reduced linear interpolate. We recompute the approximated cubic spline by the method in  [20] which has built in SISL package(https://github.com/SINTEF-Geometry/SISL) and the approximated spline together with 13 interior knots is shown in Fig. 6(b), where the knots are marked by triangles.Fig. 6(a) shows the approximated spline together with 14 interior knots calculated by our method. The 14 interior knots are marked by triangles and the fitting error isMSE=3.4745e−5. The initial knots are chosen uniformly 25 equidistant points in[−1,1]and the parameter is chosen asε=0.003. By the way, for this function, the second step is not performed. The final fourteen interior knots of the approximated spline are selected automatically only by the first step.The approximated errorMEcomputed by our algorithm with 14 interior knots is 0.017258 compared with 0.037530, which is computed by the knot removal algorithm  [20] with 13 interior knots. As seen from Fig. 6(a), the one more knot by our algorithm is around zero and contributes to the smaller approximated errorMEcomparing to the algorithm in  [20]. And the interior knots calculated by our method distribute regularly in the domain[−1,1]and are selected adaptively according to the underlying curvature structures of the unknown function. By the way, as the SISL algorithm was developed with a great deal of optimization, the SISL implementation is faster than our algorithm.In this subsection, we consider a B-spline function with the following knots:(0.0439,0.0653,0.2293,0.2367,0.4821,0.4907,0.5408,0.5408,0.6209,0.7051,0.9407),where 0.5408 is a double knot. Fig. 7(a) shows this spline function. The data points are sampled uniformly 1001 points from this B-spline function. The parameterεis set to be 2.0e−5 and the initial knots are chosen uniformly 501 equidistant points in[0,1].Fig. 7(b) shows the approximated B-spline function (red curve) together with the active knots (blue triangles) selected by the first step. Notice that the active knots are clustered into several groups and especially around the double knot 0.5408. Fig. 7(c) shows the knots of the approximated B-spline function after the second step. TheMSEof our fit is 3.7596e−6. The obtained interior knots are(0.043969,0.065281,0.229406,0.236594,0.482094,0.490906,0.496031,0.540859,0.540859,0.620969,0.705031,0.940844).If we remove the knot 0.496031 which is not in the true knot vector, the fitting errorMSEincreases to 1.469837e−4.We compared our method with the three methods in the work  [20–22]. Fig. 7(d) shows the approximated B-spline function together with the knots using the method in  [20]. The resulting spline has 28 interior knots (0.49 is a double knot and 0.54 is a triple knot) and the maximum fitting errorMEis 1.49802e−2. The fitting error of our fit isME=1.2769e−2which is smaller than that of the method in  [20] and also the knots number determined by our method is fewer than that of the method in  [20]. Fig. 8illustrates the fitting result using the method in  [21,22]. It can be seen that there are still much redundant knots in the approximated spline function, especially the redundant knots around the double knot 0.5408.From the above comparison, for the data points sampled from a spline function, our method shows a significant superiority of removing redundant knots and recovering the true knots of the given spline.Finally we discuss the effect of non-uniform sampling on recovering the true knots of a given spline function. We take the above ground truth as an illustration. Fig. 9shows the fitting result when the data points are sampled randomly. The final interior knots are(0.043969,0.065282,0.229406,0.236598,0.482031,0.490906,0.531969,0.537969,0.537969,0.620099,0.705406,0.940531).It can be seen that now the true double knot 0.5408 is recovered with a larger error than that of the uniform sampling. The worst case of non-uniform sampling is some of the true knots especially the multiple knots cannot be recovered. The reason can be explained as in Remark 3.1. The random sampling results in nonuniform distribution of data points in an interval, consequently some of the candidate intervals cannot be found exactly in the second step, leading to some true knots omitted.Remark 4.11.The code of our algorithm is not optimized thoroughly. Therefore the time cost is expensive. The time cost of the first step is about 0.22 s while the time cost of the second step is about 40 s for the above three examples. It can be seen that the time cost of first step is much smaller than that of the second step. Generally speaking, the time cost of the first step depends on the given data points and the time cost of the second step depends on the number of input active knots.

@&#CONCLUSIONS@&#
In this paper, we propose a framework for computing knots number and position in curve fitting with splines based on a sparse optimization model. The knots number and position can be found automatically by two steps. In the first step, several knots are selected from the initial knots by solving a sparse optimization problem. There are still much more redundant knots after the first step. We further remove redundant knots and adjust the positions of active knots to obtain the final knot vector. So the resulting knot vector in the second step is not a subset of the initial knot vector given in the first step any more. This is the main difference between our method and the two methods in  [21,22]. Our experiments show that the approximation spline curve obtained by our approach has less number of knots compared to existing methods. Particularly, when the data points are sampled dense enough from a spline, our algorithm can recover the ground truth knot vector and reproduce the spline.There are a few problems worthy of further investigation. First, the time cost now is expensive and the method can only handle data with small noise. We will investigate how to optimize the code to decrease the time cost and how to add penalty term in the sparse optimization model to deal with noisy data. Second, how to design a much more effective strategy to deal with multiple knots? A possible way is to estimate the derivatives information from given data points to locate the multiple knots. Third, how to extend the idea to parametric curves and surfaces will be an interesting but a challenging problem.