@&#MAIN-TITLE@&#
A highly accurate protein structural class prediction approach using auto cross covariance transformation and recursive feature elimination

@&#HIGHLIGHTS@&#
Prediction performance of protein structural class has been improved.A high-quality feature extraction technique has been designed.A recursive feature selection has been used to reduce feature abundance.

@&#KEYPHRASES@&#
Low-similarity,Position-specific score matrix,Auto cross covariance,Support vector machine,Recursive feature elimination,

@&#ABSTRACT@&#
Structural class characterizes the overall folding type of a protein or its domain. Many methods have been proposed to improve the prediction accuracy of protein structural class in recent years, but it is still a challenge for the low-similarity sequences. In this study, we introduce a feature extraction technique based on auto cross covariance (ACC) transformation of position-specific score matrix (PSSM) to represent a protein sequence. Then support vector machine-recursive feature elimination (SVM-RFE) is adopted to select top K features according to their importance and these features are input to a support vector machine (SVM) to conduct the prediction. Performance evaluation of the proposed method is performed using the jackknife test on three low-similarity datasets, i.e., D640, 1189 and 25PDB. By means of this method, the overall accuracies of 97.2%, 96.2%, and 93.3% are achieved on these three datasets, which are higher than those of most existing methods. This suggests that the proposed method could serve as a very cost-effective tool for predicting protein structural class especially for low-similarity datasets.

@&#INTRODUCTION@&#
Knowledge of protein structural class plays an important role in the prediction of secondary structure and function analysis from the amino acid sequence information (Anand et al., 2008). Nowadays, the most frequently used classifications of protein structural classes can be found in the Structural Classifications of Protein (SCOP) database (Murzin et al., 1995). There are 110,800 protein domains with known structural class in SCOP, and about 90% of them belong to the four major classes: all-α, all-β, α/β and α+β. With the rapid development of genomics and proteomics, the newly discovered protein sequences are growing exponentially, which has made a large gap between the number of sequence-known and structure-known proteins. The current experimental determination of protein structure is costly and time-consuming and thus cannot cope with the demand for rapid classification. Hence there exists a great challenge to develop reliable and accurate computational methods to determine protein structural class.As a typical pattern recognition problem, computational methods for predicting protein structural class generally consist of two main steps: protein feature representation and algorithm design for classification. For the first step, previous studies have shown that sequence features can be represented in many different ways, including amino acids composition (Chou, 1999Zhou, 1998), pseudo amino acid (PseAA) composition (Chou, 2001; Li et al., 2009), polypeptide composition (Luo et al., 2002; Sun and Huang, 2006), functional domain composition (Chou and Cai, 2004), amino acid sequence reverse encoding (Yang et al., 2009), position-specific score matrix (PSSM) (Chen et al., 2008; Ding et al., 2014; Liu et al., 2010; Liu et al., 2012), and predicted secondary structure information (Dai et al., 2013; Dehzangi et al., 2014; Kong et al., 2014; Kurgan et al., 2008; Mizianty and Kurgan, 2009; Yang et al., 2010). It is worth mentioning that through quantitative analysis, Dai and his coauthors verify that exploring the position information of predicted secondary structural elements is a promising way to improve the abilities of protein structural class prediction (Dai et al., 2013). For the later step, a wide range of classification algorithms have been used to perform the prediction, such as neural network (Cai and Zhou, 2000), support vector machine (SVM) (Cai et al., 2001; Kong and Zhang, 2014; Li et al., 2008; Nanni et al., 2014), fuzzy clustering (Shen et al., 2005), fuzzy k-nearest neighbor (Zhang et al., 2008; Zheng et al., 2010), Bayesian classification (Wang and Yuan, 2000), Logistic regression (Jahandideh et al., 2007; Kurgan and Chen, 2007; Kurgan and Homaeian, 2006), rough sets (Cao et al., 2006), and classifier fusion techniques (Cai et al., 2006; Chen et al., 2006; Chen et al., 2009; Dehzangi et al., 2013). Early methods can achieve prediction accuracies more than 90% when tested on datasets with high sequence identities. However, they perform poorly on low-similarity datasets, with accuracies between 50% and 70% (Kurgan et al., 2008). To solve this problem, by incorporating various features such as PSSM, predicted secondary structure and physical-chemical properties, several methods have been proposed to improve prediction accuracies on low-similarity datasets (Dehzangi et al., 2013; Dehzangi et al., 2014; Kurgan et al., 2008; Liu et al., 2010; Wang et al., 2015; Yang et al., 2010). Nevertheless, most studies which rely only on predicted secondary structure to enhance the accuracy could not reach too far better results than 80% (Kurgan et al., 2008; Yang et al., 2010). This may be due to limited prediction accuracy (about 80%) of protein secondary structure by PSIPRED (Jones, 1999). On the other hand, since the performance of PSIPRED algorithm relies mainly on PSSM, PSSM profile provides more important and original discriminatory information for protein structural class prediction. In our previous study, we extracted auto-covariance variables from the PSSM profile and also obtained favorable prediction accuracy when the predicted secondary structure was not utilized (Liu et al., 2012).In this study, in order to further improve the prediction accuracy of protein structural class, we extract both auto-covariance variables and cross-covariance variables from the PSSM profile by auto cross covariance (ACC) transformation. The flowchart of the proposed method is depicted in Fig. 1, which presents the pipeline that goes from the query sequence to the final output as well as intermediate steps. Firstly, the PSSM profile generated by PSI-BLAST program (Altschul et al., 1997) is transformed into a fixed-length feature vector by ACC transformation. Secondly, support vector machine-recursive feature elimination (SVM-RFE) is applied for feature selection and reduced vectors are input to an SVM classifier to perform the prediction. Finally, results by the jackknife test on three widely used benchmark datasets suggest that the proposed method yields substantial improvements in prediction accuracies compared with most published results.In order to evaluate the prediction accuracy of the proposed method and compare it with those of existing methods, three widely used datasets are adopted in our work: D640 (Chen et al., 2008), 1189 (Wang and Yuan, 2000) and 25PDB (Kurgan and Homaeian, 2006), with sequence similarity lower than 25%, 40% and 25% respectively. The D640 dataset contains 640 protein sequences, which consists of 138 all-α proteins, 154 all-β proteins, 177 α/β proteins and 171 α+β proteins. The 1189 dataset includes 1092 protein domains of which 223 belong to the all-α class, 294 belong to the all-β class, 334 belong to the α/β class, and 241 belong to the α+β class. The 25PDB dataset includes 1673 protein domains of which 443 are all-α proteins, 443 are all-β proteins, 346 are α/β proteins and 441 are α+β proteins.To develop a powerful predictor for a protein system, one of the key tasks is to formulate the protein samples with an effective mathematical expression that can truly reflect their intrinsic correlation with the target to be predicted (Chou, 2011). In this section, we combine PSSM and ACC transformation to represent a protein sequence by a fixed-length feature vector.PSSM which can represent the evolutionary information of each sequence is generated by running PSI-BLAST (Altschul et al., 1997) program against the NCBI’s non-redundant (NR) dataset (ftp://ncbi.nih.gov/blast/db/nr). The parameters are defaulted except that parameter h and j are set to 0.001 and 3 respectively. The PSSM profile for each protein sequence is an L×20 matrix, where L is the length of the corresponding sequence. The (i, j) th element in the matrix reflects the score of amino acid in the ith position of the query sequence being mutated to amino acid type j during the evolution process.For convenience, the PSSM of the query sequence S can be described as follows:P=(P1,P2,...,P20)wherePj=(p1,j,p2,j,...,pL,j)T(j=1,2,...,20),L is the length of the query sequence S, and T is the transpose operator.Next, we adopt ACC transformation to convert the PSSMs of different lengths into uniform equal-length vectors. ACC is a powerful protein sequence analysis method developed by Wold and his colleagues (Wold et al., 1993), which has been widely applied to the field of bioinformatics such as protein family classification and protein interaction prediction (Dong et al., 2009; Guo et al., 2006; Guo et al., 2008; Liu et al., 2011). Since each residue has many physical-chemical properties such as hydrophobicity, hydrophilicity, etc., ACC can measure the correlation of two properties (or the same property) along the protein sequence. Two kinds of variables, i.e., auto-covarianceA(j,g)and cross-covarianceC(j,k,g), are calculated according to the following two equations:A(j,g)=∑i=1L-g(pi,j-Pj¯)(pi+g,j-Pj¯)(L-g),C(j,k,g)=∑i=1L-g(pi,j-Pj¯)(pi+g,k-Pk¯)(L-g),where j, k are two different amino acids, g is the lag, andPj¯(Pk¯) is the average score for amino acid type j (k) along the whole sequence.In this way, the number of auto-covariance variables is 20×G, and the number of cross-covariance variables is 20×19×G=380×G, where G is the maximum of g (g=1, 2, …, G) and G should be smaller than the length of the shortest protein sequence in our datasets.In view of ACC transformation of PSSM, the sequence-order effect and evolutionary information can be indirectly and partially, but quite effectively reflected. In this study, each protein sequence is characterized as a 400×G-dimensional vector by combining auto-covariance and cross-covariance variables.The proposed representation model includes relatively large number of features, which leads to a high computation cost in machine learning. Moreover, the noise and redundant information in the data could also affect the prediction results. Therefore, an R script from SVM-RFE algorithm package (Hu et al., 2013) is introduced to highlight the actual informative features and enhance computation speed. It is realized as follows: (1) all feature vectors of proteins in each set are trained using SVM with a linear kernel; (2) the variables are ranked with decreasing order according to their weights which reflect the relevance to carry out the protein structural class prediction; (3) the feature with the lowest weight is eliminated; (4) the process is repeated until classification accuracy decreases. According to this method, we get top K features with the most relevant ranks and apply them to represent a protein in the rest of this study.SVM, proposed by Cortes and Vapnik (1995), is a very effective technique for pattern classification that often achieves superior performance in comparison with other classification algorithms. The basic idea of SVM is to map the data of samples into a high dimensional Hilbert space and to seek a separating hyperplane in this space. Given a new test protein, the SVM first maps the labeled training vectors into one feature space (perhaps with a higher dimension). Then within the space mentioned above, it finds an optimized linear division to solve two-class or multi-class problem. Finally, a prediction label to the test protein is assigned according to this way. A more detailed description of SVM is in Vapnik’s book (Vapnik, 1995).In this study, LIBSVM package (Chang and Lin, 2011) is taken as an implementation tool to perform the SVM prediction. It provides multiple kernel types such as radial basis function (RBF), linear, polynomial and sigmoid. However, the linear kernel is selected because it is time-saving and can avoid the process of parameter optimization.In statistical prediction, the jackknife test is supposed to be the most prevalent for examining the power of predictors among three cross-validation methods (i.e., independent dataset test, sub-sampling test and jackknife test) (Chou and Shen, 2007). Here, we use it to examine the reliability and stability of the proposed method. Four standard performance measures are adopted to evaluate the performance of our method, i.e., sensitivity (Sens, or accuracy), specificity (Spec), overall accuracy (OA) and Matthew’s Correlation Coefficient (MCC). They are defined by the following formulas:Sensj=TPjTPj+FNj=TPj|Cj|,Specj=TNjTNj+FPj=TNj∑k≠j|Ck|,OA=∑jTPj∑j|Cj|,MCCj=TPjTNj-FPjFNj(TPj+FPj)(TPj+FNj)(TNj+FPj)(TNj+FNj),whereTPj,TNj,FPj,FNjis the number of true positives, true negatives, false positives, false negatives respectively, and|Cj|is the number of proteins in each structural classCj(all-α, all-β, α/β, α+β).

@&#CONCLUSIONS@&#
Though many efforts have been made so far, prediction of protein structural class for low-similarity sequences still remains a challenging problem in bioinformatics. In this regard, we apply ACC and SVM-RFE to further improve prediction accuracy. In this paper, ACC transformation is used to convert the PSSM profile into a fixed-length feature vector. Then, these features are ranked by SVM-RFE based on their importance and the optimal top K features are input to an SVM classifier to perform the prediction. To evaluate the performance of the proposed method, jackknife tests are performed on three low-similarity benchmark datasets. Our method obtains overall accuracies of 97.2%, 96.2% and 93.3% on D640, 1189 and 25PDB datasets, respectively. According to the comparison results, our method substantially outperforms most of the existing methods. Thus we think this method could be a valuable tool for protein structural class prediction especially for low-similarity datasets and may at least play an important complementary role to existing methods. Since user-friendly and publicly accessible web-servers represent the direction for developing practically useful models or methods (Chou and Shen, 2009), we shall make efforts in our future work to provide a web-server for the method presented in this paper.