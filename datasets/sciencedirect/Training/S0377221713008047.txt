@&#MAIN-TITLE@&#
Learning from discrete-event simulation: Exploring the high involvement hypothesis

@&#HIGHLIGHTS@&#
We explore client learning when involved in simulation model building and reuse.Analysis is conducting using a laboratory experiment setting with three conditions.Model builders explored a larger variety of variables compared to model reusers.Experimentation with a model aided understanding about resource utilisation.Results show a trade-off in learning between model building and reuse.

@&#KEYPHRASES@&#
Psychology of decision,Learning,Model building,Model reuse,Generic models,Simulation,

@&#ABSTRACT@&#
Discussion of learning from discrete-event simulation often takes the form of a hypothesis stating that involving clients in model building provides much of the learning necessary to aid their decisions. Whilst practitioners of simulation may intuitively agree with this hypothesis they are simultaneously motivated to reduce the model building effort through model reuse. As simulation projects are typically limited by time, model reuse offers an alternative learning route for clients as the time saved can be used to conduct more experimentation. We detail a laboratory experiment to test the high involvement hypothesis empirically, identify mechanisms that explain how involvement in model building or model reuse affect learning and explore the factors that inhibit learning from models. Measurement of learning focuses on the management of resource utilisation in a case study of a hospital emergency department and through the choice of scenarios during experimentation. Participants who reused a model benefitted from the increased experimentation time available when learning about resource utilisation. However, participants who were involved in model building simulated a greater variety of scenarios including more validation type scenarios early on. These results suggest that there may be a learning trade-off between model reuse and model building when simulation projects have a fixed budget of time. Further work evaluating client learning in practice should track the origin and choice of variables used in experimentation; studies should also record the methods modellers find most effective in communicating the impact of resource utilisation on queuing.

@&#INTRODUCTION@&#
It is often assumed that clients of simulation experience significant learning through involvement in model building (Robinson, 2004; Rouwette, Korzilius, Vennix, & Jacobs, 2011). Although this high involvement hypothesis is plausible, it is difficult to measure client learning in practice and little empirical evidence exists to validate the theory. Exploration of the role involvement in model building plays in client learning is important. Particularly as the effort needed to build a discrete-event simulation (DES) model may affect study feasibility or limit scope due to a fixed budget of time (Cochran, Mackulack, & Savory, 1995; Law, 2007; Pidd, 2004; Robinson, Nance, Paul, Pidd, & Taylor, 2004). In fact, given a fixed budget of time, a modeller may choose not to involve clients in model building, but instead reuse an existing or generic simulation model (Bowers, Ghattas, & Mould, 2012; Fletcher & Worthington, 2009; Robinson et al., 2004). For example, the time saved by reusing a model of a whole hospital could instead be used for experimentation (Günal & Pidd, 2011) or, where no time is available for model building, pre-built models could be used to rapidly educate clients in approaches to improvement, for instance in lean (Robinson, Radnor, Burgess, & Worthington, 2012). Given that reuse is occurring in practice, it is important to understand how client learning is influenced by the reduced involvement in model building and the increased opportunity for experimentation offered by model reuse.To address this issue, this paper details a laboratory experiment where learning is measured using Argyris and Schön’s (1996) theory of action and learning loop framework. We seek to investigate if the effect can be demonstrated empirically; to understand the mechanisms that aid client learning from involvement in building and reuse; and to explore the factors that inhibit learning from simulation. Comparisons are made between the learning novice simulation clients (undergraduate business students) experience in an emergency department (ED) setting, given different degrees of involvement in model building, reuse and experimentation time. Participants are explicitly tasked with learning how to increase the proportion of patients meeting the four hour wait time target within UK EDs and also satisfying their own definitions of effective performance (typically aiming for resource utilisation to be close to 100%). Measurement focuses on single-loop learning: participants’ learning of strategies to meet these objectives within the ED; their attitudes towards the management of resource utilisation; and their choice of variables in experimentation.The paper is structured as follows. Firstly, the learning themes from the simulation literature are briefly summarised and classified using formal definitions of learning taken from Argyris and Schön’s (1996) theory of single and double-loop learning. Secondly, the design, materials and predictions for the experiment are detailed. After presenting the results there is a discussion of the possible learning mechanisms in the experiment and how evaluation studies might incorporate the results.This section provides a discussion of the conceptual and theoretical background for understanding our experiment and its results. This begins with a discussion of the overarching high involvement hypothesis and how it is typically expressed in the simulation literature. This is followed by an overview of Argyris and Schön’s framework of single and double-loop learning and a review of studies of learning from simulation and the complementary field of behavioural operations.Detailed studies of client learning in DES and the wider simulation community are relatively rare compared to publications on models and their results. Of those that do tackle learning and practice, discussions often refer to the hypothesis that involving the clients in model building provides much of the learning useful for aiding decisions (Alessi, 2000; Andersen, Richardson, & Vennix, 1997; Paich & Sterman, 1993; Robinson, 2004; Rouwette et al., 2011; Rouwette, Vennix, & Mullemkom, 2002; Thomke, 1998; Ward, 1989). The outcomes of client involvement in model building might take an anecdotal form in general discussion; for example, 50% of benefit of modelling is gained simply by building the model with client involvement (Robinson, 1994). More usefully, it might take a more testable form by referring to specific learning outcomes, such as those listed in Table 1.Common to all of the formulations listed in Table 1 is the theory that a simulation client has a simple predictive mental model of how the system under study behaves. Involvement in model building is hypothesised to aid clients to recognise their own implicit assumptions (Andersen et al., 1997), refine and change mental models (Rouwette et al., 2011; Thomke, 1998), enhance creativity in problem solving (Robinson, 2004), and generalise knowledge so that it can be transferred to other similar problems (Alessi, 2000; Lane, 1994; Thomke, 1998).To illustrate the high involvement hypothesis using these definitions, consider a manager in an ED that aims to simultaneously reduce patient waiting times and increase the utilisation of ED resources to, her definition of optimality, 100%. Implicitly the manager believes that achieving these aims is just a question of resources working hard(er) to meet the targets and hence does not recognise any trade-off between the two objectives. Under the assumptions of the high involvement hypothesis the manager, through her involvement in model building, would recognise the limitations of her mental model and refine it. One way to conceptualise this refinement is as the change in an individual’s attitude(s) towards controllable variables (Thomke, 1998) or competing implementation options (Rouwette et al., 2011). For example, our ED manager may now realise that she should consider the trade-off between utilisation and waiting times when making resource decisions. If the manager has learnt correctly her attitude towards 100% utilisation of resources will decrease in strength while her attitude towards allowing a reduction in resource utilisation to achieve lower waiting times will have increased.A well-known framework for learning is Argyris and Schön’s (1996) theory of single and double-loop learning. The starting point to understanding the framework is to assume that an individual’s mental model comprises a set of variables that govern what to do or how to act in relevant situations. These governing variables constrain the actions (or decisions) individuals will take given a particular situation.Argyris and Schön’s empirical work illustrates that most individuals will attempt to keep their governing variables within acceptable limits. For example, assume our healthcare manager finds that her new ED management policies are achieving 95% utilisation of nurses, but very long waiting times in ED. As our manager’s expectations have not been met, her attitude towards the management policy will be less favourable and she will attempt to find new policies that do keep governing variables within acceptable limits. Learning of this type is called single-loop learning: a change in attitudes towards various management actions to keep governing variables within acceptable limits. When a mismatch in expectations prompts the manager to reflect on her own mental model she undertakes a double learning loop: a change in governing variables and a change in actions to keep them in acceptable limits.The learning framework set out by Argyris and Schön’s framework applies not just to business management situations, but also the approach to learning or more formally the learning systems individuals employ. Bakken, Gould, and Kim’s (1994) experiment, using System Dynamics models, illustrates the impact of learning systems. In the experiment students and managers (executive MBAs) used a training simulation model, set in the same domain as the managers worked, followed by a second model with the same underlying behaviour but set in a different domain. All participants had to achieve a high profit with both models. In the training game, the managers followed the management approaches they used in real life. The students, having no experience of the real world system, used many alternative approaches and were rewarded with many negative (bankruptcy) as well as positive outcomes. Surprisingly, the students substantially outperformed managers in the second model.To explain this result, the difference in learning systems between the managers and students must be examined. The governing variable the managers were attempting to satisfy was ‘maximise winning and minimise losing’. Indeed they did find strategies that achieved this outcome in the training model and hence were constrained within a single-loop learning system. This is not necessarily a problem; however, it did mean that the managers failed to grasp the deeper transferable knowledge about system structure. Double-loop learning systems involve meta-learning where individuals must reflect on and correct their governing variables for learning. For example, the managers could have reassessed their need to maximise winning and adopt an approach similar to the students. To do this they would need to firstly recognise their confirmation bias and overcome any reluctance to produce negative results.The majority of studies that have explored learning from models have done so from an experimentation perspective using simulation gaming (e.g. Bakken et al., 1994; Bell & O’Keefe, 1995; Langley & Morecroft, 2004; Paich & Sterman, 1993; Rouwette, Größler, & Vennix, 2004; Sterman, 1989). These have explored aspects such as transfer of learning from one game to the next (Bakken et al., 1994), compared Visual Interactive Simulation (VIS) with traditional statistical analysis (Bell & O’Keefe, 1995) and identified factors such as model transparency that are important for learning (Rouwette et al., 2004).Unfortunately the evidence of successful learning from experimentation and gaming is mixed (Lane, 1995; Neuhauser, 1976; Rouwette et al., 2004; Van der Zee & Slomp, 2009) and provides little insight into how clients learn in a model reuse project. One simple argument is that model reuse may actually aid learning, as time saved can be used to run more experimentation and more VIS with clients. More VIS, for example, would be beneficial as it seemingly aids discovery, clarification and change of clients’ views and ideas about system management (Belton & Elder, 1994). Moreover, efficient experimental designs provide detailed information to clients on controllable variables and competing implementation options (Law, 2007). The other side of this argument is that client learning may be affected by not invented here syndrome (Pidd, 2002; Robinson et al., 2004), that is, lack of stakeholder trust in the model, in which case at least some project time needs to be allocated to familiarising the clients with a reused model and building credibility.Outside of gaming only a small number of rigorous studies exist that explore client learning in practice (Rouwette et al., 2011; Thomke, 1998). Although these studies demonstrate a general change in simulation client attitudes towards implementation options (Rouwette et al., 2011) and sudden changes in client attitudes towards controllable variables (Thomke, 1998), these studies do not separately analyse model building and experimentation and provide little insight into and evidence of the high involvement hypothesis.Further to the individual of level of learning, it is worth briefly mentioning that outcomes from involvement in model building have also been studied and evaluated at the group level (Andersen et al., 1997; Franco & Lord, 2010; Franco & Montibeller, 2010; Rouwette et al., 2002). Evaluation at the group level refers primarily to the effect involvement has on the interaction between group members and group project performance. Key contributions are that a modelling framework provides a shared language to improve communication within a group (Franco & Lord, 2010); social construction of a model aids management teams make sense of the organisation in which they are working (Andersen, Vennix, Richardson, & Rouwette, 2007); and the degree to which a group understands system dynamics is a predictor of perceptions of information sharing quality and psychological safety (Bendoly, in press).In recent years there has been much interest in behavioural operations management (Bendoly, Croson, Goncalves, & Schultz, 2010). This field re-examines the traditional assumptions built into operational models, such as ‘equal worker skills’ on a production line or ‘managers following optimal ordering policies’ for inventory management, and studies how this affects operational performance. A common approach to capture this behavioural information is a controlled experiment set in a specific context (Bendoly, Donohue, & Schultz, 2006). For instance, there has been a stream of work investigating the behaviour of boundedly rational decision-makers in the supply chain context (Dimitriou, Robinson, & Kotiadis, 2009; Kalkanci, Chen, & Erhun, 2011; Katok & Wu, 2009; Loch & Wu, 2008; Su, 2008). In general this work asks students to act as suppliers or retailers working in a newsvendor setting under a different contractual arrangements e.g. the wholesale price contract, buyback contract or revenue sharing contract. Observations in a laboratory setting demonstrate that these human actors behave very differently to a rational-optimising decision-maker, such as is assumed by analytical models. Indeed, there are examples where humans perform better than expected from the assumptions of analytical models e.g. Katok and Wu (2009), Dimitriou et al. (2009).Our work has a similar interest in observing human actors in a decision-making environment, but our focus is on the use of models to aid learning and decision-making rather than on the decision-making itself. This places our work in the field of behavioural operational research which ‘focuses on the psychological and social interaction-related aspects of model use in problem solving’ (Hämälläinen, Luoma & Saarinen, 2013).Although the high involvement hypothesis appears credible there are few empirical studies providing evidence of its existence. Given the difficulties of exploring learning in simulation practice, there is relatively little insight into the mechanisms that benefit client learning when they are closely involved in model building or understanding of how model reuse may alter the learning approach. This lack of knowledge prevents a focussed exploration of learning in practice and further development of the theory and evidence base for the effectiveness of simulation as a decision aid. The remainder of this paper describes a laboratory experiment to address this area. The experiment tests single-loop learning within studies where a participant is involved in different levels of model building, experimentation and reuse. Discussion of results also considers the participants use of single and double-loop learning systems.Observation and measurement of client learning from modelling is difficult in practice, as it is often uncertain if other factors affected learning or if the same learning would have been achieved from a different decision making approach. An experimental approach such as ours provides the opportunity to study learning within a controlled and simplified environment and provides insight into where evaluation of learning in fieldwork might best be focussed. We sought to answer three research questions:Q1.Can the high involvement hypothesis be demonstrated empirically?What mechanisms aid client learning from involvement in model building and reuse?Is there any evidence that single-loop learning systems interact with learning from DES models?This section details a methodology for answering these questions using a laboratory experiment. Details are presented for the independent variables, dependent variables, predictions, participants, experimental materials and procedure.To answer the research questions the experiment manipulates one independent variable (IV): the simulation study process. We include three levels of IV, as illustrated in Fig. 1. The first two levels provide a comparison of model building and model reuse using a fixed budget of time, labelled as model building with limited experimentation (MBL) and model reuse (MR) respectively. Experimentation is limited in MBL over MR as more time is required to build the model. Superior performance in MBL over MR would provide evidence of the high involvement hypothesis in action. To explore the interaction of experimentation with the learning mechanisms of model building we include a third condition with an extended time budget labelled as MB. In this condition participants spend as much time on experimentation as in MR, but they are also involved in model building as in the MBL condition. A comparison between MR and MB provides the opportunity to study how involvement in model building influences the participant’s behaviour in experimentation.There are two measures of learning within the experiment: the participant’s selection of variables in experimentation and the change in a participant’s attitude to managing resource utilisation in the ED. We chose resource utilisation as the focus of our measurement because a common task of a DES modeller is to aid clients’ understanding of the trade-off between resource utilisation and time in the system. Other research has also indicated that experienced managers do not always appreciate the trade-off, for instance in manufacturing (Suri, 1998).We operationalised measurement of single-loop learning by using the attitude measurement procedures of the Theory of Planned Behaviour (Ajzen, 1991): a well-known and substantiated theory (see Ajzen, 2001) from social psychology that has been used elsewhere in the simulation literature (Rouwette et al., 2011). Two attitude variables were developed during a pilot phase. Firstly, MaxUtil that represents the attitude towards attempting to achieve 100% utilisation of a resource. Secondly, TradeUtil that represents the attitude towards allowing some spare capacity, on average, to improve performance (i.e. time in system).The two attitudes are measured pre- and post-test using identical questionnaires (see Supplementary material). The participant reads a series of statements in the form of beliefs about the advantages or disadvantages of management actions at the hospital. Participants then rate the likelihood that a belief is true (b) on a seven point scale (1=Extremely Unlikely; 7=Extremely Likely), and how desirable the outcome of this belief is (e) on a bipolar seven point scale (−3=Extremely Bad, +3=Extremely Good). The measure of an attitude (Ai) is constructed from the summation of the j products of the subjective probability and outcome evaluations (Ajzen, 1991). This is summarised in Eq. (3.1) where k is the number of attitude variables.(3.1)At=∑j-1kbjejMaxUtil is constructed from three beliefs and TradeUtil from five, giving scale ranges of ±63 and ±105 respectively. Single-loop learning is inferred from a reduction in the strength of MaxUtil and an increase in the strength of TradeUtil.The experiment tests three hypotheses to answer Q1; these are summarised in Table 2. Firstly, if the high involvement hypothesis holds true then MB participants should show more creativity in experimentation than MR participants. We test this by comparing the number of new variables MB and MR select for analysis (hypothesis 1). Secondly, participants in the model building (MB and MBL) conditions will experience more attitude change towards queue management than MR (hypothesis 2). Finally, we analyse a subgroup where participants involved in model building (MB and MBL) should typically have more attitude change in the correct direction than MR. Note that in a real simulation study one might argue that a decision maker is neither right nor wrong, but instead makes a decision based on his/her own worldview. In the simplified world of the laboratory experiment it is possible to determine the direction of attitude change that improves the performance of a system and the direction of attitude change that does not improve performance. For the sake of clarity and understanding these directions will be labelled as ‘the correct direction’ (hypothesis 3) and the ‘incorrect direction’ of attitude change.For MaxUtil the correct direction of attitude change is represented by a decrease in score. I.e. participants will no longer seek to work resources at their maximum utilisation. In contrast, high scores on TradeUtil represent a favourable attitude towards allowing spare capacity in order to achieve lower queuing times. Therefore the correct direction subgroup is identified by an increase in attitude strength.Sixty-four business undergraduates volunteered to take part in the research (first year undergraduates=41[64%], second year undergraduates=23[36%], male=35[55%] female=29[45%], age range=18–22). All students were registered on modules taught by Warwick Business School, but none had prior experience of simulation. After sign-up was completed students were anonymised and randomised to a condition. To encourage participation the students were paid a small fee for their time. To improve participant motivation an additional cash prize was available for the best performance. Three participants were excluded from the analysis due to unusual multivariate profiles. In particular, these participants demonstrated good understanding of the queuing concepts tested in the experiment during a post-test interview, but provided contradictory and extreme scores all attitude measures.The experiment is based around a case study problem. This takes the form of a fictional Emergency Department (ED) – St. Specific’s. The objective is to reduce the percentage of patients that are in ED for longer than four hours over a six month time horizon as well as achieve participants’ own definitions of effective performance, such as acceptable resource utilisation targets. Embedded in the case are several contrived learning outcomes for participants. For example, a participant may discover:•Maximising resource utilisation of doctors and nurses over the six month period leads to a large number of performance target breaches.There is always a trade-off between the mean time patients spend in the ED and the utilisation of ED resources.Participants are provided with a default set of scenarios to investigate; for example, the reallocation of nurses across shifts. Participants in the MB and MR conditions are also able to suggest new scenarios which may investigate the default scenarios further or something else entirely. For example, a participant may investigate the effect of patient prioritisation on performance.The case study is based around a simplified version of the generic ED models described in Fletcher, Halsall, Huxham, and Worthington (2007) and Günal and Pidd (2011). The model is implemented in Simul8 Education Edition 2009 (Simul8., 2009) and the same model is used by all participants during experimentation. We also created 11 sub-model versions of the simulation that are tested and explored by MB and MBL participants during their involvement in model building.

@&#CONCLUSIONS@&#
It is often assumed that simulation clients and decision makers experience much of their learning during involvement in model building. This study provides a simple test of this high involvement hypothesis: what happens to learning if we build and limit experimentation versus if we reuse a model. The simulation clients (students) still learnt about aspects relevant to the queuing problem (resource utilisation) when they reused a model; in fact, these participants learnt more than those that were involved in model building, but had very limited time for experimentation. However, this came at a cost of less variety in the choice of scenarios for experimentation. This latter result may be general to other modelling approaches as well i.e. involvement in modelling broadens the scope of a client’s mental model of a problem. It may be less easy, however, to involve clients in model building and experimentation with some other methods, for example, analytical queuing models. Hence, this result may be specific to approaches which lend themselves to visual interactive modelling and live experimentation, such as system dynamics and discrete-event simulation.The results of this experiment should, of course, be subject to further empirical testing; however, they do illustrate that learning in simulation studies is not as clear cut as is often assumed. In time constrained simulation studies, practitioners may wish to consider the learning needs of their clients when deciding whether to build the model and so limit experimentation, or whether to reuse a generic model enabling more time for experimentation.Supplementary data associated with this article can be found, in the online version, at http://dx.doi.org/10.1016/j.ejor.2013.10.003.Supplementary data 1