@&#MAIN-TITLE@&#
Hybrid PSO–SVM method for short-term load forecasting during periods with significant temperature variations in city of Burbank

@&#HIGHLIGHTS@&#
Hybrid forecast model that includes recent history and similar day.Day tagging based on temperature integral.We detect switching events when recent history does not provide quality data for training.Real life data from Burbank utility.

@&#KEYPHRASES@&#
Load forecasting,Support vector machines,Particle swarm optimization,

@&#ABSTRACT@&#
This paper proposes a practical new hybrid model for short term electrical load forecasting based on particle swarm optimization (PSO) and support vector machines (SVM). Proposed PSO–SVM model is targeted for forecast load during periods with significant temperature variations. The proposed model detects periods when temperature significantly changes based on weather (temperature) forecast and decides whether the model can be trained just on recent history (typically 4 weeks ago) or such history has to be modified with data for similar days taken from history beyond recent history when such weather conditions were detected. Architecture of the solution consists of three modules, preprocessing module, SVM module and PSO module. The algorithm has been tested in city of Burbank utility, USA and obtained results show better accuracy comparing to results generated with classical methods of training on recent history only or similar days only.

@&#INTRODUCTION@&#
The electric power industry has been intensively restructuring in last years and competitive pressure creates a need for an intelligent approach to energy management and operation. Forecasting the electrical load from one to several days ahead is usually referred as short term load forecasting (STLF). It is one of the key parameters that help electric utility operators make decisions regarding purchasing and selling electric power, maintenance, load switching and maintenance planning, including some sophisticated areas, e.g. as relay protection, etc. The errors in STLF have direct impact on costs.This paper introduces a new hybrid method to solve the problem of STLF especially in future days when significant change in temperature can be expected comparing to days in recent history which can be completely different. Such temperature changes are one of the biggest challenges in the field of load forecasting because utility operators need accurate load forecasts for those days. The classical (standard) solution means using data from recent history (typically last 4 week period) for training and it can happen that in that period there are not enough similar patterns that can be used for high quality training keeping in mind that forecasted and recently historized temperatures are significantly different. For example, on summer beginning/end, a significant change in daily load curve shape could occur due to a different behavior of consumers caused by increased/decreased need for air-conditioning. This means that appropriate historical data for training cannot be found in recent history but beyond recent history. Expanding the training period enough to encompass enough days where such temperature exists is not a real possibility keeping in mind the memory and processing power required for forecasting. Additionally, this approach would introduce days from other periods that are not relevant for training, so this approach can be very questionable. The other reasonable idea is to identify future days according to temperature behavior and then select (tag) only similar days in history to use for training. This approach is also considered a classical method. It would seem that using the “similar day” approach would always give better results, but our testing examples do not confirm this. Comprehensive testing based on training using only the “similar day” method confirm that results can be even less accurate than comparative results using recent history data compared with similar temperature conditions. Two conditions are not completely taken into consideration when utilizing the “similar day” approach. First, the effects of recent history (for practical purposes, a kind of thermal inertia) and the fact that load vs. temperature variations predominantly depend on a period of couple days, not on 1 day. Second, it is necessary to mention that some deviation in the “similar day” approach could happen as a consequence of changes in customer features/behavior evolution over time, e.g. social conditions like number of residents, industry status, energy price, some social events, etc. In this paper, the second condition is not a subject of consideration as this condition does not exist in the Burbank area.Keeping in mind all previously mentioned facts, this paper proposes a hybrid method where a training set is basically taken from recent history but specifically combined with similar days. The hybrid method is based on two very important procedures: (1) defining and identifying significant changes in temperature during the forecasted period and then (2) updating the training set regarding identified temperature conditions. Establishment of these two procedures in a forecasting method is the main contribution of this paper.The proposed method is tested on a data set gathered from an actual utility – Burbank Water and Power (BWP) – and the results utilizing this hybrid method have proven more accurate when compared to both classical forecasts methods, based either on historical data or on similar days only.The field of load forecasting attracts significant attention and there are a large variety of methods which have been proposed. Those methods are: autoregressive [1,2], artificial neural networks [3,4], fuzzy logic [5,6], recursive digital filters [7], expert systems [8], support vector regression [9,10] and much more. A support vector machine (SVM) is a relatively new and promising technique which can be used for load forecasting. There are some very good properties of SVM which lead to a significant number of applications and methods in the field of load forecasting – good generalization performance, no problem with local minima and a sparse solution representation. The application of SVM in various [11] regression problems shows good results with small error and a fast training speed, and is commonly used in load forecasting problems [12].There is a very strong correlation between temperature and load (customer behavior) [13,14]. Thermal inertia introduces complexities in correlation between temperature and load when using forecasting algorithms [15]. There is variety of proposed methods how to include phenomenon of thermal inertia in forecasts. In [16], the authors propose a formula to calculate thermal inertia based on the last 5 days of temperature measurements. In [17], the authors implicitly include thermal inertia by including average, maximum, and minimum temperatures from previous days. In [15], a double forecast is proposed, with forecasted and previous day temperature and a combination of two results. In [18], thermal inertia is modeled by lagging temperature signal.The problem of seasonal variation of the load curve is also very important in forecasting theory. It is shown in [19] that load curve patterns can be more influenced by weather conditions depending on the season. Authors in [20] pay special attention to model accuracy in different seasons of the year and train the forecasting solution on similar days. Seasonal effect which is responsible for the relationship between weather and load, limits the usage of historical data to 30 days as defined in [21]. Authors in [22] detect four seasons and created different forecasting methods for different seasons. Only data from one season was used for that seasons training and forecast, and that is a typical problem with forecasting seasons as the transition periods between seasons typically contain a lot of temperature variations.It is necessary to emphasize one important conclusion from all above mentioned references: (1) there is no universal forecast method – some methods are better for some conditions, (2) there is always a problem with a training data set that is not similar enough with the forecasted period and (3) the forecast has to be tuned to incorporate specific behavior and specifics of load vs. temperature for concrete forecasting cases and conditions. In this sense, the proposed hybrid solution is one new additional method in a group of forecasting methods which is proposed as providing a high quality solution for load forecast (vs. temperature) in periods up to 7 days.The proposed method goes one step further in encompassing temperature variations and temperature inertia by combining the classical methods mentioned in Section 1 into one hybrid method which will make forecasts based on a combination of recent history data and similar days data. The basic idea of the hybrid method is to check if recent history data can be used for training for forecasted periods and if it is not a case, to “fix” training data set. So a few tasks have to be executed in order to provide an appropriate training data set: (1) check if recent history data can be used for forecasting by detecting if there is no significant temperature deviation; if there are significant temperature deviations during the forecasted period, then a so called “switching event” may be occurring, (2) if a switching event exists, it is then necessary to identify the days which are encompassed within the switching event – this is day tagging, (3) if a switching event exists, it is also necessary to fix the training data set. All of these tasks are then coordinated and an appropriate solution is proposed which provide very accurate model for load forecast and high quality results for practically all power applications in utility (planning, maintenance, eventual relay protection re-setting, etc.). Considerations start with a load vs. temperature analysis.To get more information about load vs. temperature behavior, the initial step is to show this dependence. There are 2 possibilities regarding temperature vs. load display – to use peak daily temperature or average daily temperature (daily average temperature is estimated as more relevant for identification of the significant change of temperature because high daily peak temperature is not always followed by a load increase, especially if nights are relatively cold; average temperature and average load indirectly take into consideration a whole day's load and temperature and in this way provide a more comprehensive description of the whole phenomenon).Distribution load vs. average daily temperature is given in Fig. 1. Three years of historical load and historical daily average temperature from Burbank has been used for this statistical analysis. Average values were calculated from hourly values for all days. The representative load is mostly of a residential/commercial type.Using Fig. 1, 3 temperature ranges can be identified: (1) average, (2) hot and (3) very hot. Average range is up to 19.5°C, hot starts when daily average temperature exceeds 19.5°C and very hot starts at 25°C. This classification is based on load vs. temperature analysis: average corresponds to consumption without air-conditioning and it can be referred as a period when a customer is not temperature sensitive, hot corresponds to consumption with air-conditioning and very hot corresponds to very intensive air-conditioning. Previously identified temperature ranges are not universal ones and they have to be generated through analysis of historical data (load vs. temperature).The nature of load vs. temperature dependence encompasses numerous factors: meteorological (e.g. combination of temperature and humidity, duration of temperature variation), thermal inertia of buildings, behavior of customers depending on value of temperature change as well as behavior of consumers during significant variation of temperature, etc. All the above mentioned factors can differ for various geographical locations and customer behavior, and there is a need to tune the forecasting system to respond to the load behavior vs. temperature change. E.g. from Fig. 1, two swarms of loads are identified in the average range marked with red dotted rectangles. Additional analyses points out that the upper rectangle corresponds to loads for working days while the lower corresponds to weekends and holidays. This distinction is very important and it means that beside temperature range classification there is a necessity to further classify the data by day type.Comprehensive testing was done in order to define the event (switching event) when temperatures in forecasted periods are of such values that the recent history data set is not appropriate for training. This paper proposes a simple rule to recognize this switching event. It is determined initially by checking the period defined by calculating the temperature integral for a period of 7 days – the integral is calculated from 168 hourly values and divided with 24 (elaboration why 7 day period is selected is given in addition).Temperature integrals for checking periods are ranked as:-Average: Temperature integral is up to 136.5 (7×19.5)°C.Hot: Temperature integral is from 136.5°C up to 175 (7×25)°C.Very hot: Temperature integral is from 175°C and above.The temperature integral will be calculated for each of 4 weeks in recent history and consequently each week will be tagged as average, hot or very hot depending on the temperature integral value. Then, the same temperature integral will be calculated for forecasted period of 7 days (it is necessary to emphasize that even if forecasted period can be shorter (e.g. 1 or 2 days), it is necessary to provide the forecast for the next 7 days to avoid a short and/or incomplete vision of the whole future period). Depending on the integral value, the forecasted week will be tagged as average, hot or very hot. If the forecasted week tag is different from all other week tags in recent history, then the switching event is detected by definition. This means that recent history cannot be used for training (at least 1 week from recent history has to have the same tag as forecasted week).Different durations of checking periods were tested. It was concluded that the checking periods from 1 or 2 days cannot take into account the proper inertia (significant load change is not detected after 1 or 2 days with average temperature from different ranges). Analyzing a 3-year period, it was concluded that a checking period of at least 3 days should be considered. The checking period was then expanded to that of 5 and 7 days, both of which gave the same results. Longer checking periods can eliminate short and temporary temperature variations as it was determined that load change is better defined with average temperature for longer periods. A consideration of longer checking periods than 7 days was not of interest as the definition of STLF (up to 7 days) and longer periods would annul any eventual variation of temperature inside that checking period. The possibility to use the same duration of checking and forecasting periods (7 days) is very useful as it definitively reduces the calculation effort.When a switching event is detected, it is necessary to fix the training set initially established from the recent history data set. The next step is identification of the appropriate day types for fixing the training set.When a switching event is detected, a calculated daily average temperature is determined for each day in the recent history and forecasted periods: an integral of 24 hourly temperature values is divided with 24 for a day. Then the 28+7 calculated values are ranked: (1) average (up to 19.5°C), (2) hot (between 19.5 and 25°C) and (3) very hot (above 25°C).Additionally, all 28+7 days are marked with a day type such as Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, Sunday or holiday.After that, the 7 days from the forecasted period are compared with the 28 days from recent history. If a day in the forecasted period is not found to have a similar day in recent history, that day is tagged. This means that the outcome of this procedure will generate a certain number of tagged days. For each tagged day, data will be taken from the first similar day beyond recent history. Once these tagged days are identified, the next step will be to fix the training set.In order to fix the training data set, the data initially populated from recent history will be overwritten with data taken from the tagged day type within the appropriate temperature range (average, hot, very hot) beyond the recent history period.The proposed idea for “fixing” the training set is explained in one simple example using the temperature signal shown in Fig. 2. One can see that the average temperature in recent history (here identified as the last 3 weeks) was 18.5° and that it obviously is in average range. There is a switching event during the forecasting period because the temperature integral for the whole week falls within the hot range.The temperature integral for the days during the forecasted period identify Wednesday, Thursday and Friday as hot days. It is obvious that these days will not have similar days in recent history to compare to. Therefore data for all four Wednesdays, Thursdays and Fridays should be taken from history beyond the recent history period and used to overwrite the recent historical day data. Data for the Mondays, Tuesdays, Saturdays and Sundays will remain unchanged. Thus, the training set is “fixed”.Having defined all necessary inputs, the architecture of the solution can be proposed. The proposed solution consists of three sub modules, pre-processing unit, SVM unit and PSO (particle swarm optimization) unit, which are shown in Fig. 3.Weather forecast is not part of this solution and this research. Weather forecast is received using web service from weather forecast provider.The pre-processing unit (PPU) receives a weather forecast for a selected period from the weather service and a historical temperature from the historical database. These are used for the detection of the switching event and for fixing the training set. The PPU also communicates with the historical database to obtain historical load data used for training and calculates the day type.A support vector machine (SVM) is new and promising technique for classification and regression problems first proposed by Vapnik. Support vector regression is common term for implementation of SVM in regression problems. SVR is an optimization problem:(1)minω,b,ξ,ξ*12ωTω+C∑i=1n(ξi+ξi*),subject to:(2)yi(ωTϕ(xi)+b)≤ε+ξi,(ωTϕ(xi)+b)yi≤ε+ξi*,ξi,ξi*≥0,i=1,…,nwhere (x1, y1),…, (xn, yn) are pair of input and output vectors, n is the number of samples, ω is weight factor, b is the threshold value, C is error cost, input samples are mapped to higher dimensional space by using kernel function ϕ, ξiis the upper training error and the ξi* is the lower training error subject to ɛ-insensitive tube |y−(ωTϕ(x)+b)|≤ɛ. Kernel function is responsible for non-linear mapping between input and feature space. There are three typical kernel functions: Gaussian (RBF), Polynomial and Linear [24,26,27].The particle swam optimization (PSO) is a population based stochastic optimization method proposed Kennedy and Eberhart. Inspiration for optimization procedure is found in social behaviors of bird groups and fish schools [25]. PSO searches for optimal solution in solution space using population of particles. Each particle is considered as a solution of the problem. Particle with smallest error at the end of optimization procedure is defined as a solution.PSO is defined in following statements:(a)Each particle i is defined in search space by its position Xi, velocity Viand personal best position yi.Initial particles position and velocity are random.Particle best position yirepresents position in search space where particle i presents smallest error in the objective function f. This value is called fitness value.Global best position y represents position in the search space for which lowest error of all yiexists.Search space has as many dimensions as many degrees of freedom particles have. Each degree of freedom represents one variable in optimization function.Velocity and position of particle in search space is continuously adjusted depending on the actual velocity and the distance from the particle best and global best position:The SVM module consists of 192 SVM networks where each network is responsible for forecasting one hour of a next 8 days. 192 networks are chosen because of parameter optimization procedure. Because every network is responsible for forecasting 1h in 192h array and that output correlation with hour, day type and temperature is different, it is very important to find optimal SVM parameters for every particular hour. There are three main parameters of SVM that are optimized: (1) error cost C, ɛ as a size of a tube and γ parameter of the Gaussian (RBF) kernel:(5)k(xi,xj)=exp−xi−xj22γ2.Inputs into the SVM module are organized per single SVM network. Every SVM network for training receives the following inputs: (1) day type, (2) load for the same hour of the same day last week, (3) temperature for the same hour of the same day last week. For training, inputs are delivered for the last 4 weeks. Outputs from the SVM module consist of 1 outputs of every SVM network where a single SVM network outputs forecast values for a single hour. The proposed model forecasts based on a “sliding window” principle which means that the forecast starts from the next round hour and goes up to 7 days. To reduce the need for additional inputs which define what data [load, temperature] belongs to what day type (necessary for cases when the forecast begins at a time other than 00h and causes the 24 period to span two days) causing the forecasting time to increase, the forecast is run for next 8 days, starting from 00h current day and 168h are cropped from the forecasted array.To optimize the SVM parameters, the PSO module is used. The PSO searches for the best C, kernel and ɛ parameters by comparing the forecasting error in every iteration. Search space is organized in three dimensions, one for each parameter. Main fitness factor is Mean Absolute Percentage Error (MAPE).The PSO module consists of 15 particles and optimizes in 20 iterations. Optimization starts with random values for each parameter for SVM in next range: (1) 1<C<2, (2) 0.1<γ<0.5 and 0.001<ɛ<0.011. The forecast is run with a combination of C, kernel and ɛ parameters for that particle in every iteration. Initial values for c1 and c2 in PSO are 2 [28]. χ in PSO is calculated by:(6)χ=22−(c1+c2)−(c1+c2)2−4(c1+c2)as proposed in [29].The Mean Absolute Percentage Error (MAPE) is recorded. In the next iteration, all particles go toward the particle with the smallest MAPE.

@&#CONCLUSIONS@&#
Load forecasting is one of the key components in the modern management of a utility. There is wide range of tasks where such results can be applied: operation management, maintenance, relay protection. To improve the accuracy of load forecasting in those periods of the year when system operators need it most, during periods with intensive change in temperature, a new approach based on PSO–SVM has been proposed.The proposed method consists of three units where the first unit is called a preprocessing unit and is responsible for detecting that a season has changed, for tagging days, and for searching for similar days. The second unit is a SVM based hourly predictor, while the third unit is for optimizing the SVM parameters based on the PSO.The proposed method gives better results compared to the classical method which uses just recent history data or just similar days data. The key element is the preprocessing module which detects which data should be used for training by detecting the switching event and tagging the similar days. Traditional approach was unable to give accurate forecast because training set was not proper for forecasting. We have showed that at the beginning of the season (for example summer) traditional approach (which is recent history training in this case) do not give accurate results since training set is from Spring and has different load–temperature correlation and at the middle of the season when there is no significant change in temperature traditional approach (which is similar day in this case) do not give accurate results since training set is obsolete and loses trend and local thermal inertia of the system. Proposed hybrid solution which detects weather to use recent history or similar day adapts to current situation and gives more accurate results.It needs to be stressed that a universal forecasting solution cannot be made that will give the most accurate results for all areas and weather conditions. Different weather conditions, customer habits and behavior, temperature sensitivity of load, energy price and availability, etc. are crucial elements which create daily load curves, and this implies that forecasting solutions needs to be tuned for every system based on the number of years of historical data and a lot of tests, although this forecasting solution can be implemented in other areas where there exists temperature–load correlation in respect to generalization nature of SVM and PSO in this solution.Finally, it is worth mentioning that this solution tackles problem of defining input space for training during the period of significant temperature variations and that there is space for improvement to create more universal solution, i.e. consider change of population and electric power price as an input, test different algorithms and parameter combinations for different types of consumers, etc.