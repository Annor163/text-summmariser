@&#MAIN-TITLE@&#
‘Horses for Courses’ in demand forecasting

@&#HIGHLIGHTS@&#
We try to answer the question: “what is the best forecasting method for my data?”.We simulate seven time series features and one strategic decision.Cycle and randomness have the biggest (negative) effect for fast-moving data.Inter-demand interval has the biggest (negative) effect for intermittent data.Increasing length of a series has a small positive effect on forecasting accuracy.

@&#KEYPHRASES@&#
Forecasting methods,Time series methods,Forecasting accuracy,M-Competitions,Simulation,

@&#ABSTRACT@&#
Forecasting as a scientific discipline has progressed a lot in the last 40years, with Nobel prizes being awarded for seminal work in the field, most notably to Engle, Granger and Kahneman. Despite these advances, even today we are unable to answer a very simple question, the one that is always the first tabled during discussions with practitioners: “what is the best method for my data?”. In essence, as there are horses for courses, there must also be forecasting methods that are more tailored to some types of data, and, therefore, enable practitioners to make informed method selection when facing new data. The current study attempts to shed light on this direction via identifying the main determinants of forecasting accuracy, through simulations and empirical investigations involving 14 popular forecasting methods (and combinations of them), seven time series features (seasonality, trend, cycle, randomness, number of observations, inter-demand interval and coefficient of variation) and one strategic decision (the forecasting horizon). Our main findings dictate that forecasting accuracy is influenced as follows: (a) for fast-moving data, cycle and randomness have the biggest (negative) effect and the longer the forecasting horizon, the more accuracy decreases; (b) for intermittent data, inter-demand interval has bigger (negative) impact than the coefficient of variation; and (c) for all types of data, increasing the length of a series has a small positive effect.

@&#INTRODUCTION@&#
Forecasts are important for all decision-making tasks, from inventory management and scheduling to planning and strategic management. Makridakis and Hibon (2000) advocated: “predictions remain the foundation of all science”. To that end, identification of the best forecasting techniques for each data set, or, even, for each series separately, is still the ‘holy grail’ in the forecasting field, and, as a result, empirical comparisons to this direction are considered very important (Fildes & Makridakis, 1995). Advanced, sophisticated and simpler extrapolation methods could be associated with specific features of data. The development of a protocol for automatic selection of the best tools for resolving each problem, a protocol that would guarantee minimum out-of-sample forecasting error and therefore have a substantial impact on decision making, is the ultimate challenge for researchers and practitioners in the field.As early as the late 1960s and most of the 1970s, several researchers (Cooper, 1972; Groff, 1973; Kirby, 1966; Krampf, 1972; Levine, 1967; Makridakis & Hibon, 1979; Naylor & Seaks, 1972; Newbold & Granger, 1974) sought to determine the accuracy of various forecasting methods in order to select the most appropriate one(s). In addition, psychologists have been concerned with judgmental predictions and their accuracy, as well as the biases that affect such predictions, for more than half a century (Dawes, 1979; Hogarth, 1987; Kahneman & Tversky, 1973; Meehl, 1954, 1986; Slovic, 1972; Tversky & Kahneman, 1982). Amongst these biases, those affecting forecasting include over-optimism and wishful thinking, recency, availability, anchoring, illusory correlations and the underestimation of uncertainty. In a recent book, Kahneman (2011) describes these and other biases whilst also discussing what can be done to avoid, or minimize their negative consequences and emphatically states: “the research suggests a surprising conclusion: to maximize predictive accuracy, final decisions should be left to formulas, especially in low-validity environments” (Kahneman, 2011, p. 225). Moreover, the growing demand for forecasting big data (e.g. more than 200,000 time series for major retailers) renders the use of automatic statistical procedures necessary.The purpose of this study isto measure the extent to which each of seven time series features (seasonality, trend, cycle, randomness, number of observations, inter-demand interval and coefficient of variation) and one strategic decision (the forecasting horizon) affect forecasting accuracy. In order to do this, we measure the impact of each of these eight factors1We use the term ‘factor’ to refer to both the data features as well as the strategic decision (forecasting horizon) which impact on forecasting accuracy will be examined through this study.1by generating a large number of time series – as well as using real data, and measuring the accuracy of the forecasts derived from 14 methods and five combinations of them. Furthermore, a multiple regression analysis is performed to measure the extent to which each of the factors affects the accuracy of each of the time series methods/combinations. The findings of this research could be very useful for practitioners if used for the appropriate selection of the best statistical forecasting practices based on an ex-ante analysis of their data (and their respective features).This paper is structured as follows: after the literature review (Section 2), the simulation design for fast-moving and intermittent demand data is discussed in Section 3. In Section 4 the accuracy results are presented. Section 5 discusses the findings and Section 6 presents the practical implications for decision makers. Finally, Section 7 concludes and suggests possible avenues for future research.Extrapolation models are used very often when facing large amounts of data. Among them, exponential smoothing forecasting approaches were developed in the early 1950s and have become very popular amongst practitioners. Their main advantages are simplicity of implementation, relatively low computational intensiveness and no requirement for lengthy series, whilst being appropriate for short-term forecast horizons over a large number of items. Single Exponential Smoothing (SES – Brown, 1956) uses only one smoothing parameter and is forecasting quite accurately stationary data. Holt’s two parameters approach (1957) expands the Single method with a smoothing parameter for the slope, making the method more appropriate for trended data. The Holt-Winters approach (Winters, 1960) is an expansion upon the Holt trended model, which assumes an additive or multiplicative seasonality in the data. Gardner and McKenzie (1985) added a dampening factor (0<φ<1) applied directly on the trend component, resulting in a very successful approach that is often considered the benchmark in many empirical evaluations. Assimakopoulos and Nikolopoulos (2000) proposed the Theta model – a prima facie variation of SES with drift, with the full theoretical underpinnings presented by Thomakos and Nikolopoulos (2014), a method that topped the M3-Competition, the largest empirical forecasting competition to date (Makridakis & Hibon, 2000, Appendix B).On the other hand, the more complex but quite popular Box–Jenkins methodology (Box & Jenkins, 1970) uses an iterative three-step approach (model identification, parameter estimation and model checking) in order to find the best-fit ARIMA model. To date ARIMA models are still considered the dominant benchmark in empirical forecasting evaluations, and find great popularity among OR researchers in applications spanning from hospitality and production to healthcare and climate forecasting (for e.g. see Broyles, Cochran, & Montgomery, 2010; Cang & Yu, 2014; Cao, Ewing, & Thompson, 2012).One result that stands for fast-moving data is that combining improves predictive accuracy (Clemen, 1989; Makridakis & Winkler, 1983; Surowiecki, 2005). In addition to this, combining reduces the variance of forecasting errors and therefore the uncertainty in predictions, rendering the selection of combinations less risky than individual methods (Hibon & Evgeniou, 2005). Many recent studies have verified that the combination of methods leads to more accurate forecasts, whilst, at the same time proposing more sophisticated weightings such as the trimmed and Winsorized means (Jose & Winkler, 2008), and the use of information criteria (Kolassa, 2011; Taylor, 2008).For count data/intermittent data, Croston (1972) proposed decomposing the data into two subseries (demands and intervals) with Syntetos and Boylan (2005) proposing a bias-correction to the Croston’s method (Syntetos and Boylan Approximation or SBA). More recently, Teunter, Syntetos, and Babai (2011) suggested a decomposition method that relies on the separate extrapolation of the non-zero demands and the probability to have a demand. This method is very useful in cases of obsolescence. Lastly, simpler approaches, such as Naïve, Moving Averages and SES, have also been quite popular for such data especially among practitioners.An interesting spin-off from the later intermittent demand literature came from Nikolopoulos, Syntetos, Boylan, Petropoulos, and Assimakopoulos (2011) with the ADIDA non-overlapping temporal aggregation forecasting framework, that although designed and successfully evaluated empirically on count data (Babai, Ali, & Nikolopoulos, 2012), the implications pretty fast span out for fast-moving data as well (Kourentzes, Petropoulos, & Trapero, 2014; Spithourakis, Petropoulos, Babai, Nikolopoulos, & Assimakopoulos, 2011). The proposed framework soon was perceived as a forecasting method “self-improving” mechanism that by changing the data series features through frequency transformation, can help extrapolation methods achieve better accuracy performance. The first theoretical results for the ADIDA framework appeared recently in the literature (Rostami-Tabar, Babai, Syntetos, & Ducq, 2013; Spithourakis, Petropoulos, Nikolopoulos, & Assimakopoulos, in press).Given the plethora of the aforementioned methods, it is now even more unclear: when should each method be used? Many researchers compared the performance of aggregate and individual selection strategies (Fildes, 1989; Shah, 1997; Fildes & Petropoulos, in press). While selecting a single method for an entire data set would make sense for homogeneous data, model selection should be done individually (per series) when we deal with heterogeneous data, as to capture the different features met in each series.Pegels (1969) presented the first graphical classification for exponential smoothing methods, separating trend from cycle patterns, and also as additive from multiplicative forms. In a simulation study, Adam (1973) evaluated several forecasting models across five different demand patterns, including constant, linear trend, seasonal and step function. His findings indicate that no single model is consistently better than the others, and their performance depends primarily on the demand pattern, the forecasting horizon and the randomness, and secondarily on the selected accuracy metric. Gardner and McKenzie (1988) provided a procedure for model identification in the case of large forecasting applications. Their selected course of action involved the calculation of variances at various levels of differences in data, and using those for classifying the underlying pattern of the time series (constant or trended, seasonal or not seasonal, and so on).A first attempt for a rule-based selection procedure of the best model derived from Collopy and Armstrong (1992). They proposed a framework that combines forecasting expertise with domain knowledge in order to produce forecasts based on the characteristics of the data. Their procedure consisted of 99 rules and four extrapolation techniques, while 18 time series features were used. A simplified domain knowledge-free version of this rule-based procedure was presented by Adya, Armstrong, Collopy, and Kennedy (2000), using just 64 rules, three forecasting methods and six time series features. In order to render the procedure fully automated, Adya, Collopy, Armstrong, and Kennedy (2001) presented an automatic identification of time series features for rule-based forecasting, which reduces significantly the forecasting cost of large data sets without serious losses in accuracy.Shah (1997) proposed a seven-step model selection procedure for univariate series forecasting, using an individual selection rule based on 26 features. In a later study, Meade (2000) used 25 simple statistics as explanatory variables in order to predict the forecasting accuracy performance of nine extrapolation methods and, thus, select the most promising one. His results were evaluated on two empirical data sets.A different path for model selection focuses on the application of families of methods (for example, Exponential Smoothing or ARIMA) and subsequently the selection of the single method that has the best trade-off between the goodness-of-fit and the complexity of modeling (number of parameters). To this end, the use of information criteria (Hyndman, Koehler, Snyder, & Grose, 2002) has been very popular. At the same time, there is little to distinguish from the application of different information criteria (Billah, King, Snyder, & Koehler, 2006). A disadvantage of model selection with information criteria is the inability to compare across different families of methods. An alternative to the use of information criteria is the performance evaluation of methods in a hold-out sample where forecasts are calculated for single or multiple origins and lead times (Fildes & Petropoulos, in press). Depending on the specific experimental design, this strategy is known as “validation” or “cross-validation”.For count data, Bacchetti and Saccani (2012) provided a comprehensive literature review of the classification methods, whilst a demand-based classification for intermittent demand was proposed by Syntetos, Boylan, and Croston (2005), later revised by Kostenko and Hyndman (2006).Lastly, it is worth emphasizing that various similar attempts to identify suitable methods for forecasting cross-sectional data had been presented over the years; as for example in Nikolopoulos, Goodwin, Patelis, and Assimakopoulos (2007) in a marketing application and Bozos and Nikolopoulos (2011) in a strategic financial decision-making application, where a series of economics, econometrics, time series, artificial intelligence and computational intensive approaches as well as human judgment were compared within the context of the respective investigations.Forecasting competitions have evaluated the performance of time series methods (Makridakis & Hibon, 2000; Makridakis et al., 1982, 1993) in order to better understand their relative accuracy and improve their usefulness. Since then, a large number of studies have compared the accuracy of various methods in different forecasting settings. For example, Franses and van Dijk (2005) concluded that simpler (linear) models for seasonality perform better for short horizons. At the same time, more complex (non-linear) models should be preferred for longer forecasting horizons.Furthermore, many researchers focused on the performance of exponential smoothing methods. Gardner (2006) compared the performance of damped trend to the class of state-space models. Using the data sets from the M and M3 forecasting competitions, he concluded that the damped approach is more robust and accurate than the individual selection of models through information criteria in almost every case, except for the short horizons of the M3-Competition monthly data. Gardner and Diaz-Saiz (2008) explored the performance of exponential smoothing methods using telecommunications data. An analysis of the results suggested that SES with drift, a simplification of the Theta method (Assimakopoulos & Nikolopoulos, 2000) as proposed by Hyndman and Billah (2003), provided the most accurate forecasts compared to any other smoothing method for every horizon.Crone, Hibon, and Nikolopoulos (2011) conducted a forecasting competition for Computationally Intensive approaches, most notably Artificial Neural Networks (ANN). One of their main findings was that only one ANN method outperformed the damped trend. Lastly, Athanasopoulos, Hyndman, Song, and Wu (2011) found that tourism data are best extrapolated using time series approaches rather than causal models, whilst the forecasting performance of Naive for annual data was “hard to beat”. A similar insight was also offered by another study using tourism data (Gil-Alana, Cunado, & De Garcia, 2008), in which a simple seasonal model outperformed more complex ones for short horizons.Having revisited all this literature, we believe there is still scope for studies investigating what makes some methods more (or less) accurate, and under what conditions; having that said, the main Research Question (RQ) of this study is as follows:RQ: How various factors affect, if at all, the forecasting accuracy of time series extrapolation methods?To address this question, we design two extensive simulations for fast-moving and intermittent data respectively, as well as empirical evaluations in real data, involving 14 univariate forecasting methods and five combinations of them. Consequently, the extent of the influence of each factor is calculated through regression analysis.Before we start elaborating on the empirical investigations, we need to formally introduce the data features that we will simulate in this study.To that end we were inspired originally by the work of Adam (1973) that he identified the importance of trend, seasonality, randomness and the forecasting horizon. We were further influenced by the work of Collopy and Armstrong (1992) where they proposed their Rule-Based Forecasting (RBF) framework, the very essence of which is dominated by the identification of data features. Among the selected time series features, trend, cycle, seasonality and length of the series were of key importance. Finally, the work of Nikolopoulos and Assimakopoulos (2003) where many of these data features were used as key elements in the object-oriented architecture of a prototype Forecasting Support System – TIFIS, gave us more firm evidence on the importance of the aforementioned data features.The level of temporal aggregation (frequency) and the level of cross-sectional aggregation (level in hierarchy) of the data were not considered in this study. We focus on the forecasting performance of a specific level of temporal/cross-sectional aggregation. Most of the patterns described above may be observed at any level of aggregation and, thus, this latter feature was not simulated. The only exception is the seasonality. The effects of temporal and cross-sectional aggregation on seasonality have been addressed elsewhere. Nikolopoulos et al. (2011) and the ADIDA framework indicate that forecasters may, via temporal aggregation, switch to other different frequencies (than the ones the data are observed), while Kourentzes et al. (2014) present a framework to efficiently combine forecasts derived from multiple frequencies. Lastly, Athanasopoulos, Ahmed, and Hyndman (2009) explore different approaches to hierarchical forecasting, proposing an “optimal” approach, which provides optimally reconciled forecasts at every level.So the RQ can now be narrowed for fast-moving data as follows:RQ1: How six factors (seasonality, trend, cycle, randomness, the number of observations and the forecasting horizon) affect, if at all, the forecasting accuracy of time series extrapolation methods on fast-moving data?To generate simulated series for testing the above-mentioned research question, each of the first five factors (seasonality, trend, cycle, randomness and the number of observations) was varied around six levels (see Table 1) while 10,000 series were randomly generated at each level (ceteris paribus). Since there are six levels and five factors to vary, there is a total of 7776 (65) combinations, resulting in 77,760,000 generated time series covering every possible combination. For each of the generated time series, 18 forecasts were produced. The values and variation of each of the six levels was selected based on the respective ranges of the 1428 real monthly series of the M3-Competition.The generation procedure assumes a deterministic, multiplicative model where each component is applied individually as suggested by Miller and Williams (2003), but in addition we introduce a cycle component as well. Thus:(1)Xt=St·Tt·Ct·Rtwhere Xtis the series, Stis the seasonal component, Ttis the trend component, Ctis the cycle component and Rtis the random component.The procedure of simulating fast-moving series is described below. First, a vector of length equal to a selected number of observations plus 18 (out-of-sample) is defined, with all values being set equal to a randomly selected initial level (L). This vector is multiplied by the respective seasonal indices, which, given a seasonality level (SL), are defined as:(2)St=SIkSLMAP(FD)where SI is a single dimensional array of 12 values, containing the mean seasonality curve of the monthly seasonal time series of M3-Competition and MAP(FD) represents a normalization factor calculated as the mean absolute percentage of first differences of the SI values. Then, the trend component is applied to each observation. This component is equal to:(3)Tt=(1+TL)tpwhere TL is the selected trend level and p represents the number of periods within a year. The cyclical component, Ct, is introduced as:(4)Ct=Ct-1+Nt(CL,CV=1/3)where C0=0 and Ntis a normally distributed random variable with mean value CL (the selected level of the cycle component) and a standard deviation so that CV=1/3. A new value of Ntis generated for each data point. Lastly, for the randomness component, a normally distributed and randomly selected variable with mean value RL (level of randomness) and CV=1/3 is generated, or:(5)Rt=Nt(RL,CV=1/3)So, Xtis calculated as follows:(6)Xt=L·St·Tt·Ct·RtThe forecasting methods used in the study are (for more details on these methods, see Appendix A):Naïve 1, Naïve 2, four exponential smoothing methods (Single, Holt, Damped, Holt-Winters), Theta (Assimakopoulos & Nikolopoulos, 2000), Linear Trend and two commercial packages (Autobox and Forecast Pro).Moreover, the following five combinations of the above methods were constructed:•Single-Damped (SD).Single-Holt-Damped (SHD).Single-Theta (ST).Single-Damped-Theta (SDT).Single-Holt-Damped-Theta (SHDT).With the six methods (Naïve 2, Single, Holt, Damped, Linear Trend and Theta) not suitable to handle seasonality, the forecasts were produced following a three-step procedure: firstly the data was deseasonalized via a Classical Decomposition approach. Secondly, 18 forecasts were computed using the deseasonalized data. Finally, these 18 forecasts were reseasonalized using the same seasonal indices as the Classical Decomposition. The remaining methods (Naïve 1 and Holt-Winters) and the methods implemented in the commercial packages (Autobox and Forecast Pro) were applied directly to the original data. The extrapolation of each series was always carried out without using the last 18 out-of-sample observations which were being kept for evaluating the forecasting accuracy of each method. For all forecasting methods, except the two commercial packages, 10,000 series were generated for each of the 7776 permutations; in contrast, for the two commercial packages only 300 series were generated for each of the 7776 permutations (i.e. 2,332,800 series in total) because of the time required in order to run them.The forecasting accuracy was measured by comparing the 18 hold out data points with the 18 point forecasts. Three accuracy metrics were calculated:•The Symmetric Mean Absolute Percentage Error (sMAPE), the main metric of the M3-Competition (Makridakis & Hibon, 2000).The Percentage Better, where the accuracy of each method was benchmarked against Naïve 2.The Mean Absolute Scaled Error (MASE), introduced by Hyndman and Koehler (2006).The overall evaluation involves the calculation and comparison of over 55billion forecast errors.We also considered the case of intermittent demand data, where two main factors were considered, namely average inter-demand interval (IDI) and squared coefficient of variation (CV2) of the non-zero demands as it is dictated by the work of Syntetos et al. (2005). On top of that, we examined also the effect of the length of the series (number of available observations) that is quite ignored in the respective literature as usually in practice these series are short. Lastly, in line with the investigation on fast-moving data, we study the effects of forecasting horizon. So the basic research question may now be narrowed to:RQ2: How four factors (inter-demand interval, coefficient of variation, the number of observations and the forecasting horizon) affect, if at all, the forecasting accuracy of time series extrapolation methods on intermittent data?To generate simulated series for testing the above-mentioned research question, each of the first three factors (IDI, CV2 and the number of observations) was varied around six levels (see Table 2) while 10,000 series were generated at each level (ceteris paribus). Given the number of different combinations (216=63) considered, we examine in total 2,160,000 simulated time series. For each series, we produce forecasts for the next 12 periods.The procedure followed to generate the intermittent demand data is given below. For a selected level of number of observations (l) and level of IDI, we generate a vector which specifies the occurrence of non-zero demands as a Bernoulli distribution (Croston, 1972; Syntetos & Boylan, 2001), where p=1/IDI. The output of this step is a binary vector I of length l. For the demand sizes we use randomly generated numbers following a negative binomial distribution (see Syntetos, Babai, Lengu, & Altay, 2011) and increase them by one (1) as not to generate zero demands. The number of successful trials (n) and the success probability (p) can be easily derived as:(7)n=μp(1-p)p=μcv2(μ+1)2where cv>0 is the required coefficient of variation (square root of CV2) and μ is the mean, selected randomly in [10,50]. Letting the l generated values of the negative binomial distribution be the vector d, the required non-zero demand sizes can be derived as D=d+1. Finally Xtis calculated as Xt=It· Dtfor t=1,…,l. If cv=0, then d=round(μ) for every t, where round denotes the rounding function.The forecasting methods used in this simulation are (for more details on these methods, see Appendix A):Naïve, Simple Moving Averages (SMA) of length 4, 8 and 12, SES with a prefixed smoothing parameter equal to 0.1 (SES(0.1)), SES with optimized smoothing parameter (SES(auto)), Croston’s method, Syntetos and Boylan Approximation (SBA) and Teunter, Syntetos and Babai method (TSB).For the methods designed specifically for intermittent demand (Croston, SBA, TSB), small values for the smoothing parameters were applied, as suggested by the literature (Syntetos & Boylan, 2005). In more detail, the smoothing parameter for estimating both demands and intervals in Croston’s method and SBA and demands in TSB method is set equal to 0.1. Moreover, the smoothing parameter for estimating the probability of the occurrence of a non-zero demand in TSB method is set equal to 0.02.The forecasting accuracy was measured by comparing the 12 hold out data points with the 12 point forecasts of each method. Due to the presence of zero demands, the use of percentage errors is not appropriate. We, therefore, use a scaled version of the Mean Absolute Error (sMAE), where the scaling is performed through dividing with the in-sample mean demand. The point forecast error (scaled absolute errors or sAE) for the h-step-ahead forecast can be calculated as follows:(8)sAEh=|XN+h-Fh|1N∑t=1NXtwhere X is the vector of observations generated previously, F is the vector of point forecasts and N is the length of the in-sample. The sMAE is simply derived by averaging across horizons and series.In this section we present results from simulated data for fast-moving and intermittent series, as well as for real fast-moving data from the M3-Competition.The accuracy results for the entire dataset are summarized in Table 3.We note:•The very good performance of the five combinations, a finding consistent with the conclusions from the M-Competitions.The strong similarities in the performance of methods (in terms of sMAPEs) with comparison to the ones in the M3-Competition, as depicted by the close values of the respective ratios.There are three single methods that consistently perform better: Single Exponential Smoothing,2Also referred in the literate as ‘Simple’ Exponential Smoothing, or just abbreviated as SES and is equivalent to an ARIMA(0,1,1) without constant model.2Damped Exponential Smoothing and the Theta method.The quite similar average results for MASE and Percentage Better as presented in the last two columns of Table 3.Table 4shows the results of the regression analysis. It lists the standardized beta coefficients for each factor, the corresponding t-tests as well as the overall R2 and standard errors. Table 4 suggests that for all variables and methods the regression coefficients are statistically significant (at 0.01) with Naïve 1 being the only exception. Moreover the R2 values range from 0.850 to 0.932, indicating a very good fits as expected from such a rich dataset.A positive beta coefficient means less accuracy whilst a negative one means improvement. Furthermore, the bigger the absolute value of the coefficient the greater the deterioration or improvement in forecasting performance. The signs of most regression coefficients are positive (seasonality, cycle, randomness and the forecasting horizon) and this means that when these following factors increase, the forecasting accuracy for all methods and combinations decreases.Notable exceptions are the negative betas for: (a) the number of observations factor for all methods with the exception Naïve 1, thus when the length of the series increases the accuracy increases as well even if it is a marginal improvement and (b) the trend factor for Holt, Holt-Winters and Linear Trend methods as these methods capture the trend in the data (see Table 4, column 4), and therefore marginally improve accuracy.Randomness is the variable that most affects forecasting accuracy. Moreover, the values of these coefficients for the majority of methods are similar (ranging from 0.823 to 0.878); this means that the accuracy of all methods rapidly decreases as the randomness in the data increases, but also that practically all methods are equally capable of dealing with increasing levels randomness. The variable with the least influence is trend, in particular for the Holt, Holt-Winters and Linear Trend methods as the values of the corresponding regression coefficients are small. Cycle and the forecasting horizon variables appear to have less influence compared to randomness in terms of the extent to which they affect forecasting accuracy. Furthermore, the seasonal fluctuations in the data are captured in a similar way by practically all methods, as their regression coefficients are small, ranging from 0.026 to 0.063 (the exceptions are Naïve 1 and one of the commercial packages).One way to apply the findings of the previous section in real data is as follows: when decomposing a time series we can estimate the levels of seasonality, cycle, trend, and randomness while we also do know exactly the number of available observations of the series and we decide on the forecasting horizon. This information allows us to estimate the percentage error for each forecasting horizon by utilizing the corresponding regression equation for each single method shown in Table 4. Consequently we can identify the method with the smallest error as well rank all methods according to their respective errors. We can, therefore, select for each series and forecasting horizon the method with the minimum expected error.Often, the difference between the best method identified and the second, third and fourth are small and to account for this we consider the simple combination of these methods for which a specific criterion – hereafter called Threshold Ratio – is more than a pre-defined value. The Threshold Ratio may be calculated by dividing the smallest expected sAPE of the ‘optimal’ method for a given forecasting horizon with the expected sAPE of the second, third and fourth method respectively. The maximum number of methods to be combined could be limited and in this example we use the ad hoc value of 6, as combinations of large pools of methods is not regarded as beneficial (Fildes & Petropoulos, in press).We apply this method selection and combination approach to the monthly series of the M3-Competition (1428 time series, Makridakis & Hibon, 2000). The results are presented in Table 5illustrating that the proposed method selection protocol, based on the regression estimation in Table 4, results in improved forecasting performance of 9.9% and 5.4% compared to SES and Damped respectively. For low values of the threshold so as to allow more methods to be included in the simple combination, the performance gets even better than that of the Theta model (Fig. 1).The sMAE results for the entire dataset are shown in Table 6.There are a few interesting findings in Table 6:•TSB performs slightly better than the other methods for all horizons.The improvement in forecasting accuracy as the length of the Simple Moving Average increases.Forecasting horizon does not affect accuracy.In Table 7we present the results of the multiple regression analysis where the standardized beta coefficients and their corresponding values of t-test are listed for each factor. Moreover, the overall goodness of fit (R2) and the standard error estimates are provided. The reported R2 values indicate very good fit for all equations as in the case of fast-moving data.Forecasting performance for all methods is heavily affected by the increase of intermittence of the data (IDI) as well as the coefficient of variation. As the values of these two factor increase, the respective accuracy of all methods decreases. Even if the differences are very small, Croston’s method and SBA are the two methods most affected by the increase of the average IDI, despite the fact that both methods are specifically designed for intermittent demand data. On the other hand, Naive is the method which is least affected by the IDI, as increase of intermittence is likely to result in spot-on forecasts (for zero demand). The exact opposite is true for the coefficient of variation. SBA and Croston are the two methods with the lowest effect of on their forecasting accuracy. On the contrary, Naïve, SMA(4) and TSB are the three methods affected the most by variability in demand, according to the standardized beta coefficients.The number of available observations has a very small positive impact on the forecasting accuracy of SES(auto), Croston and SBA. Finally, forecasting horizon has practically no impact on the predictive power of the alternative methods (non-statistically significant beta coefficients).

@&#CONCLUSIONS@&#
