@&#MAIN-TITLE@&#
Multi-way PLS regression: Monotony convergence of tri-linear PLS2 and optimality of parameters

@&#HIGHLIGHTS@&#
It is shown that the tri-linear PLS2 procedure is convergent.The sequences generated by the tri-linear PLS2 can be described as increasing or decreasing two specific criteria.A hidden tensor is described allowing tri-linear PLS2 to search its best rank one approximation.A link between multi-way PLS regression and the well-known PARAFAC model is highlighted.

@&#KEYPHRASES@&#
Multi-way data,Tensors,Multi-way PLS regression,Best rank-one approximation,Monotony sequences,

@&#ABSTRACT@&#
The tri-linear PLS2 iterative procedure, an algorithm pertaining to the NIPALS framework, is considered. It was previously proposed as a first stage to estimate parameters of the multi-way PLS regression method. It is shown that the tri-linear PLS2 procedure is convergent. The procedure generates a sequence of parameters (scores and loadings), which can be described as increasing or decreasing two specific criteria. Furthermore, a hidden tensor is described allowing tri-linear PLS2 to search its best rank-one approximation. This tensor highlights the link between multi-way PLS regression and the well-known PARAFAC model. The parameters of the multi-way PLS regression method can be computed using three alternative procedures.

@&#INTRODUCTION@&#
Regression methods constitute widely used tools in chemometrics. Usually, the data set to be analyzed is composed of two matrices,XandY. The rows ofYcorrespond to the responses, which have to be predicted according to the values contained inX. A very popular method to implement this prediction is the Partial Least Squares method (PLS), and numerous papers have previously discussed this method from a geometrical, mathematical and statistical point of view (Bhupimder and McGregor, 1997; Helland, 1988; Martens and Naes, 1989; Phatak and de Jong, 1997). Several reviews illustrating the interest in PLS regression for various applications are also available (Clayton et al., 2006; Martin et al., 2008).The amount of data currently generated by modern instrumentation is very large, and collecting data has become easier due to more efficient computers, technological advances in data storage, and automatic instrumentation. In many situations, the data contained inXhave three modes and can be meaningfully arranged in a three-way table (see Fig. 1). For example, if a batch process is running, measurements of the process variables are made at regular time intervals. For each batch, the resulting data matrix has a size equal to the number of process variables∗the number of time points. When several batches are simultaneously considered, the data set becomes dimensioned as third-order data (number of batches∗number of process variables∗number of points), which are also called third-order tensors.The purpose of multi-way regression is to build a model between a high-order arrayXof independent variables and a response arrayY. Similar to PLS, such a model might be useful for predicting futureYvalues from futureXvalues or for studying the relation betweenXandY. A well-known method for addressing the multi-way regression problem is multi-way PLS (NPLS), a modeling strategy introduced by Bro (1996) and further studied by Smilde (1997). NPLS can be seen as an extension of the ordinary PLS regression model to the case where theXdata consists of a multi-way array. It has been used in different situations, including multivariate time-series analysis, e.g., metabolomics longitudinal data. For instance, NPLS was proposed for the quantification of lipoprotein fractions obtained by 2D diffusion-edited NMR spectra (Dyrby et al., 2005) to model longitudinal microbial metabolic data (Rubingh et al., 2009) and to analyze LC-MS time-series data (Boccard et al., 2010, 2011).As in PLS regression, NPLS can be described as essentially consisting of two stages. In the first stage, the components (Xscores andYscores) and the vector of weights (Xloadings andYloadings) are computed according to an iterative procedure, which pertains to the NIPALS framework. This procedure allows one to compute the scores and loadings related to theXandYdata.The second stage in NPLS aims at computing subsequent components (scores and loadings of order higher than 1). This is achieved by deflating theXandYdata with respect to the previously computed parameters (Xscores andXloadings).The case where the response dataYis a vector or a matrix is considered. The independent dataXis a three-way array (third-order tensor). WhenYis a vector (single variabley), the NPLS follows the so-called tri-linear PLS1 procedure (see below). WhenYis a matrix, the NPLS regression follows the tri-linear PLS2 procedure. The main difference between tri-linear PLS1 and tri-linear PLS2 resides in the fact that the latter is iterative.The computation of scores and loadings by the tri-linear PLS2 procedure is considered. Because tri-linear PLS1 is not an iterative procedure, it does not exhibit any computational difficulty. The proof of convergence of the Tri-linear PLS2 procedure has not been provided and a criterion optimized by scores and loadings of the NPLS regression has not been established. Indeed, no apparent optimization properties have been proposed to characterize the parameters (scores and loadings) of the multi-way PLS regression method. This remains problematic because the solution is not guaranteed to be optimal and, therefore, the output interpretation can be erroneous.The contributions of the present paper can be summarized using the following three results:(i)Iteratively, the tri-linear PLS2 procedure generates sequences of scores and loadings vectors for which two criteria have been found to be monotone. As a consequence, the monotony convergence of tri-linear PLS2 is guaranteed.A hidden tensor allowing tri-linear PLS2 to be considered as searching its best rank-one approximation is described. This tensor highlights the link between multi-way PLS regression and the well-known PARAFAC model.As an alternative to the tri-linear PLS2 procedure, scores and loadings can be computed using three other procedures, which are available in the literature.The organization is as follows. Section  2, summarizes the notations and basic definitions from tensor algebra. Section  3 gives a brief presentation of the multi-way PLS regression model and details the stage of the computation of scores and loadings by providing a complete description of the tri-linear PLS1 and tri-linear PLS2 procedures. Section  4 focuses on the results and their proofs, and contribution (i) mentioned above is presented. Contributions (ii) and (iii) are presented in Section  5, where three alternatives procedures to tri-linear PLS2 are briefly discussed.Various notations have been used to describe the multi-way PLS regression method. Bro’s presentation (Bro, 1996) is based on matrix notation, whereas (Smilde, 1997) uses Kronecker product notation (Van Loan, 2000). This paper follows a description of the multi-way PLS regression method based onn-mode product notation (De Lathauwer et al., 2000; Kolda, 2006). This choice appears to be more rigorous and uses fewer indexes.This section summarizes the basic definitions and notations required in subsequent developments. Matrices and third-order tensors are in uppercase bold, and vectors are in lowercase bold.•Leta,b,cbe three real vectors of size(I×1),(J×1)and(K×1), respectively, and a third-order tensorXis rank one if it can be written as the outer product of three vectors, i.e.,X=a∘b∘c. The symbol “∘” represents the vector outer product. This means that each element of the tensor is the product of the corresponding vector elements:xijk=aibjck.LetXandX̃be two tensors of size(I,J,K). The scalar product of two third-order tensors is denoted by〈X,X̃〉and is computed as a sum of element-wise products over all indices, that is,〈X,X̃〉=∑i=1I∑j=1J∑k=1Kxijkx̃ijk. The scalar product allows the norm of a third-order tensorXto be defined as‖X‖=〈X,X〉=∑i=1I∑j=1J∑k=1Kxijk2.To multiply a third-order tensor by a vector or a matrix, it is necessary to specify the corresponding tensor mode (or way).1—The 1-mode productX×̄1uof a third-order tensorXof size(I,J,K)and anI-vectoruis a matrixMof size(J,K)with elementsmjk=∑i=1Ixijkui(1≤j≤J)and(1≤k≤K).In the same manner, the 2-mode productX×̄2uof a third-order tensorXof size(I,J,K)and aJ-vectoruis a matrixMof size(I,K)with elementsmik=∑j=1Jxijkuj. Similarly, the 3-mode productX×̄3uof a third-order tensorXof size(I,J,K)and aK-vectoruis a matrixMof size(I,J)with elementsmij=∑k=1Kxijkuk(see Fig. 2for a graphical representation of then-mode product).2—The 1-mode productX×1Uof a third-order tensorXof size(I,J,K)with a matrixUof size(L,I)is a third-order tensorXˆ1of size(L,J,K)with elementsxˆljk=∑i=1Ixijkuliwith(1≤l≤L),(1≤j≤J)and(1≤k≤K).In the same manner, the 2-mode productX×2Uof a third-order tensorXof size(I,J,K)and a matrixUof size(L,J)is a third-order tensorXˆ2of size(I,L,K)with elementsxˆilk=∑j=1Jxijkulj. Similarly, the 3-mode productX×3Uof a third-order tensorXof size(I,J,K)and a matrixUof size(L,K)is a third-order tensorXˆ3of size(I,J,L)with elementsxˆijl=∑k=1Kxijkulk(see Fig. 3for a graphical representation of then-mode product).Matricization (also known as unfolding or flattening) attempts to reorganize the elements of a third-order tensor into a matrix.(a)The mode-1 matricization of a third-order tensorXis denotedX(1)and is obtained as follows: the elementmilof a matrixX(1)of size(I,K×J)withl=1+(j−1)J+(k−1)JKcorresponds to the elementsxijkof the third-order tensorXof size(I,J,K)(Fig. 4(a)).The mode-2 matricization of a third-order tensorXis denoted asX(2)and is obtained as follows: the elementmjlof a matrixX(2)of size(J,I×K)withl=1+(i−1)I+(j−1)I+(k−1)IKcorresponds to the elementsxijkof the third-order tensorXof size(I,J,K)(Fig. 4(b)).The mode-3 matricization of a third-order tensorXis denoted asX(3)and is obtained as follows: the elementmklof a matrixX(3)of size(K,J×I)withl=1+(i−1)I+(j−1)IJ+(k−1)IJcorrespond to the elementsxijkof the third-order tensorXof size(I,J,K)(Fig. 4(c)).The following definition is necessary to explore the systematic variation patterns in a three-way data setXof size(I,J,K), which are likely to predict the systematic variation patterns in the response data matrixYof size(I,Q). The multi-way PLS regression model for the mean-centered dataXandYis defined as(1)X=∑h=1HtX,h∘aX,h∘bX,h+RX(H).(2)Y=TX(H)B(H)+RY(H).In Eq. (1),tX,hdenotes the so-calledXscores,aX,handbX,hindicates theXloadings associated with the second and third mode, andRX(H)denotes the residual part.In Eq. (2),TX(H)=[tX,1tX,2⋯tX,H]is anI×Hmatrix obtained by combining all theXscore vectorstX,h(h=1,2…H).To estimate model (1), the parameters of theXscorestX,hand theXloadings(aX,h,bX,h)(h=1,2…H)are computed according to the iterative procedure detailed in 3.1.The matrixB(H)in model (2) is estimated asB(H)=(TX(H)TTX(H))−1TX(H)Y. The columns ofB(H)are obtained from multiple regressions of each column ofYonTX(H).RY(H)denotes the residual part.The parameter (H), i.e., the rank of the tensorXˆ(H)=∑h=1HtX,h∘aX,h∘bX,hto be retained in Eqs. (1) and (2), is usually evaluated using cross-validation (CV) techniques (Kemsley, 1996; Kiers, 2000).Given a fixedH, N-PLS proceeds in two stages to compute theXscores andXloadings. In the first stage, the parameters (Xscores andXloadings) are computed according to the tri-linear procedure detailed in 3.1. The second stage in N-PLS aims at computing subsequent components (scores and loadings of order higher than 1). This is performed by deflating with respect to the parameters previously computed, as will be detailed in 3.2.When theYresponse is anI-vector, the multi-way PLS regression follows the tri-linear PLS1 procedure to compute the scores and loadings. This procedure is described in Table 1.As can be observed in Table 1, the procedure starts with the computation of a matrixZ. Thereafter, Singular Value Decomposition (SVD) is performed onZ. The dominant left and right singular vectors (associated with the largest singular value) are extracted. Then, theXscore is directly computed.The elements of the matrixZhave a simple interpretation in terms of covariance between a third-order tensorXand the single variabley. This matrix can be written inn-mode product notation asZ=X×̄1yT. First, the third-order tensorXis considered as a collection of(J,K)centered columns denotedcXjk. Second the covariance of eachcXjkand that ofyare computed, leading to the following(J,K)matrix:(3)zjk=∑i=1Iyixijk=(I−1)∑i=1Iyixijk(I−1)=(I−1)cov(y,cXjk)wherecov(y,cXjk)denotes the covariance betweenyandcXjk. The equality (3) holds becauseXandyare centered. The matrixZcontains the covariance betweenXandy. Fig. 5presents how the matrixZis calculated.On other hand, theXscoretXi=∑j=1J∑k=1KxijkaXjbXk;(1≤i≤I), which can be equivalently written astX=X×̄2aX×̄3bX, can also be seen as a component or latent variable or projection to the latent structure, which is built using a linear combination from the third-order tensorX. TheXscoretXhas the same meaning as scores in the case of ordinary PLS Regression.When theYresponse is a matrix, N-PLS follows the tri-linear PLS2 procedure. The parameters (Xscores,Xloadings,Yscores andYloadings) are computed according to an iterative procedure as follows:In Table 2,sdenotes the current iteration. Steps 2–4 can be viewed as a tri-linear PLS1 procedure (Table 1) applied toXandtY(s).The tri-linear PLS2 procedure iterates in the following way: The procedure starts with a normalized vectoraY(s); then, theYscoretY(s)=Y aY(s)is computed. Thereafter, the matrixZ(s)=[zjk(s)]zjk(s)=∑i=1ItY(s)ixijkand the pair(aX(s),bX(s))corresponding to the singular vectors associated with the largest singular value ofZ(s)are extracted. Therefore, theXscoretX(s)is computed. Finally, theYloading vector is updated usingaY(s+1)=YTtX(s)/tX(s)TY YTtX(s).One can observe that steps 2 and 4 of the tri-linear PLS2 procedure can be written in then-mode product notations, considering thatZ(s)=X×̄1tY(s)and thattX(s)=X×̄2aX(s)×̄3bX(s).The steps described in Table 2 are iterated oversuntil convergence is achieved. After convergence,tX,1,aX,1andbX,1are obtained, and Eqs. (1) and (2) can be evaluated. It should be noted that the convergence of the tri-linear PLS2 procedure is often observed in practice, but to our knowledge, no formal mathematical proof is available.The second stage in NPLS regression attempts to compute subsequent components (scores and loadings of order higher than 1). This is performed on the residual part with respect to theXscores andXloadings previously computed.Assume that at the current levelH0,XscorestX,handXloadingsaX,h,bX,h(h=1,2…H0)have already been computed. Then, in the next level, the tri-linear PLS2 procedure is repeated by replacing the current data(X,Y)with(RX(H0),RY(H0)), which is defined as(4)RX(H0)=X−Xˆ(H0)(5)RY(H0)=Y−TX(H0)B(H0)whereXˆ(H0)=∑h=1H0tX,h∘aX,h∘bX,hin (3); the columns ofB(H0)=(TX(H0)TTX(H0))−1TX(H0)Yin (4), which is obtained from multiple regression on each column ofYonTX(H0)=[tX,1tX,2⋯tX,H0]; andtX,h(1≤h≤H0)are theXscores.From the computational point of view, it is clear that, whereas the first stage (i.e., the computation of scores and loadings by the iterative Tri-linear PLS2) needs further investigation, the second stage of the multi-way PLS regression method (i.e., the deflation stage) does not present any computational issues (while(TX(H0)TTX(H0))−1exists, or, equivalently,H0≤rank(TX)).Relevant algorithmic properties related to the convergence of the solution and the optimality of parameters (scores and loadings can be considered as solutions to the optimization problem) are reported in Table 3.WhenYis a single variable, multi-way PLS regression does not pose any computational problems because the tri-linear PLS1 procedure is not iterative. Thus, as clearly presented by Bro (1996),Xscores can be characterized as optimal because they are solutions of the following optimization problem:Maximize cov2(tX,y)under the constrainttX=X×̄2aX×̄3bXand‖aX‖=‖bX‖=1.WhenYis a matrix, proof of convergence of the tri-linear PLS2 procedure is not available. In contrast, no apparent optimization is provided to characterize the parameters (scores and loadings) of the method. Therefore, multi-way PLS regression needs further investigation, which is presented in the following section.Iteratively, the tri-linear PLS2 procedure, as described in Section  3.2, generates five sequences denoted as follows:(i)(tX(s))s≥0is the sequence ofXscores.(tY(s))s≥0is the sequence ofYscores.(aX(s))s≥0,(bX(s))s≥0are the two sequences ofXloadings.(aY(s))s≥0is the sequence ofYloadings.The following section is dedicated to the analysis of these sequences. Two monotony properties for the sequences (i)–(iv) generated by the tri-linear PLS2 procedure will be established. As a consequence, the monotony convergence of this procedure is proven.In this section, two additional sequences are introduced, in order to better discuss the monotony convergence of tri-linear PLS2 as well as the optimality properties of scores and loadings of multi-way PLS. These two real sequences are defined as(6)α(s)=cov2(tX(s),tY(s))(7)β(s)=‖Z−λ(s)aY(s)∘aX(s)∘bX(s)‖2where in (7),λ(s)=cov(tX(s),tY(s)), and the third-order tensorZis defined asZ=X×1YT. Here, the tensorZis called the covariance tensor betweenXandY(Fig. 6).The following property relates the two sequencesα(s)andβ(s)as always summing to a constant.Property 1At each iterations, the following equality holds:(8)β(s)+α(s)=‖Z‖2.The proof can be found inAppendix  A.1.Property 2At each iterations, the following inequality holds:(9)α(s)=cov2(tX(s),tY(s))≤cov2(tX(s+1),tY(s+1))=α(s+1).The proof ofProperty  2   can be found inAppendix  A.2.Property 2 is the core of the monotony results presented in this section. For a better understanding of its meaning, it is illustrated with simulated data. Five couples of centered data (one couple = one third-order tensor setXand one matrixY) were randomly generated and a tri-linear PLS2 procedure performed to compute scores from each of them. For each iterations,α(s)=cov2(tX(s),tY(s))(the square of the covariance between scores) is computed. The evolution ofα(s)is shown in Fig. 7.Whensincreases, it can be observed that the sequenceα(s)=cov2(tX(s),tY(s))regularly increases until reaching a maximum value. This rise is found for every initial value of the covariance defined by the initialization step of the procedure.Property 2 helps one understand how theXscores andYscores are related when the tri-linear PLS2 is iterated. The interpretation is straightforward because tri-linear PLS2 entails finding anXscore and aYscore recovering as much covariance (relation) as possible betweenXandY.Property 2 focused only on the monotony of the sequences of scores generated by the tri-linear PLS2 procedure through the sequenceα(s)=cov2(tX(s),tY(s)). The interest now is to analyze the three sequences of loadings. This analysis requires the covariance tensorZbetweenXandY. This tensorZand the sequenceβ(s)provide an interesting description of the sequences of loadings generated by tri-linear PLS2, as summarized in the following monotony property.Property 3At each iterations, the following inequality holds:(10)β(s+1)=‖Z−λ(s+1)aY(s+1)∘aX(s+1)∘bX(s+1)‖2≤‖Z−λ(s)aY(s)∘aX(s)∘bX(s)‖2=β(s).Property  3   directly results from the combination ofProperties  1 and 2. Substitutingβ(s)=‖Z‖2−α(s)(Property  1) inα(s)≤α(s+1)(Property  2) leads to(10).At iterations,β(s)can be interpreted as the quadratic error approximation ofZby the rank-one tensorλ(s)aY(s)∘aX(s)∘bX(s). By stating Property 3, it is shown that the tri-linear PLS2 procedure attempts to extract the best rank-one approximation of the covariance tensorZ. This property is also interpreted in a manner whereby the tri-linear PLS2 iterative procedure tries to estimate the so-called PARAFAC model, as applied to the covariance tensorZ. This is an important bridge, enabling the connection between these two models.Property 4The two sequencesα(s)andβ(s)are convergent.This property is a direct consequence ofProperties  2 and 3. The convergence of the sequence(α(s))s≥1comes from a well-known theorem in real analysis, which states that each monotonic and bounded real sequence is convergent (Sudhir and Balmohan, 2010).The sequenceα(s)=cov2(tX(s),tY(s))(withβ(s)=‖Z−λ(s)aY(s)∘aX(s)∘bX(s)‖2) is monotonously increasing (Property 2) (respectively monotonously decreasing (Property 3)). Property 1 shows that both sequencesα(s)andβ(s)are bounded. Therefore, the sequencesα(s)andβ(s)converge. It follows that a stopping rule in the Tri-linear PLS2 procedure can be stated by checking that the sequenceα(s)ceases to increase (respectivelyβ(s)ceases to decrease) by more than a pre-specified (small) threshold.Using Property 3, it is shown that Tri-linear PLS2 entails finding a best rank-one approximation of the covariance tensorZ. Three procedures can be found in the literature for computing the best rank-one approximation of a tensor. Therefore, these methods can be seen as alternatives to computing loadings for multi-way PLS regression. In the present section, the three procedures are briefly presented.The first alternative to tri-linear PLS2, Alternative Least-squares (ALS) was reported by Kolda and Bader (2009). ALS has been proposed in the general case to compute the best rank-R(R≥1)approximation of a tensor. This procedure was applied here to the tensorZin the particular caseR=1. As a consequence, some simplifications of the original ALS procedure are possible, leading to a simple form, as described in Table 4.The two other alternative procedures, the Update Higher-order Power Method (UPM) and the Joint Update Higher-order Power Method (JUPM), were proposed by De Lathauwer et al. (2000). These procedures have been proposed for tensors of order possibly greater than 3. Here, these two methods were applied to the tensorZ, as described in Table 4.First, the covariance tensorZis computed, and subsequently, each procedure is initialized and iterated until convergence is obtained leading toXloadings andYloadings. Thereafter, the scores are computed astX=X×̄2aX×̄3bX.To visualize the behavior of the four procedures, a simulation was performed. The aim of this simulation was to highlight that the sequences generated by the four procedures are different, showing that the four procedures can lead to different results. Two data setsXandY(Xbeing a third-order data set, andYbeing a matrix having the same number of rows as the first mode ofX) were randomly generated. The procedures were initialized with the same vectors at the first iteration (s=0). Next, each procedure was iterated five times, and the least-squares criteria(β(s))was evaluated at each iteration. The results obtained are shown in Fig. 8, each curve representing one procedure, as variations inβ(s)over the five iterations.It can be observed in Fig. 8, that the four procedures (Tri-linear PLS2, ALS1, UPM and JUPM) are quite different because they generate different sequences of loadings; even if the four curves have the same starting point (i.e., identical initialization values), differences in the solutions are visible starting from the second iteration. Therefore, these four procedures cannot be regarded as equivalent from a computational point of view.Because the algorithms behave differently between iterations (they generate different sequences), they should not necessarily converge to the same end points. It may seem surprising that algorithms proposed to solve the same problem produce different results, but this phenomenon is a result of each algorithm depending on randomly chosen starting vectors. Two simulations based on different initialization conditions can therefore produce slightly different solutions. In this respect, these algorithms can only guarantee a local solution, and they cannot guarantee a globally optimal solution. Moreover, the trilinear PLS2, as with the other algorithms, does not minimize the least-squares criterionβ, but it does monotonically decreaseβ, until convergence. Similarly, the ALS procedure does not minimizeβ; therefore, it cannot always provide the lowest function value, as shown in Fig. 8. As a consequence, the ALS procedure is not guaranteed to find the best solution.

@&#CONCLUSIONS@&#
