@&#MAIN-TITLE@&#
Recognizing activities in multiple views with fusion of frame judgments

@&#HIGHLIGHTS@&#
Data fusion based method for activity recognition using multiple viewsStraightforward architecture to incorporate new cameras or new featuresPerformance generally increases when there are more cameras and features.Comparable performance with that produced by reconstructionDetailed experiments to answer different system considerations

@&#KEYPHRASES@&#
Video analysis,Human activity recognition,Multiple views,Multiple camera,

@&#ABSTRACT@&#
Graphical abstract

@&#INTRODUCTION@&#
There is a broad range of applications for systems that can recognize human activity in video. Medical applications include methods to monitor patient activity for keeping track of progress in stroke patients; or for keeping demented patients secure. Safety applications include detecting unusual or suspicious behavior, or detecting pedestrians to avoid accidents. The problem remains difficult due to important reasons. There is no canonical taxonomy of human activities. Changes in illumination direction and viewing direction cause massive changes in what people look like. Individuals can look very different from one another, and the same activity performed by different people can vary widely in appearance.Generally, we expect that having multiple views makes recognizing human activity easier. There is support for this viewpoint in the literature (e.g., see Section 2). However, these results tend not to take into account various desirable engineering features for distributed multi-camera systems. In such systems, we may not be able to get accurate geometric calibrations of the cameras with respect to one another (e.g., if the cameras are dropped into a terrain). Cameras might drop in or out at any time, and we need a simple architecture that can opportunistically exploit whatever data is available. We will not be able to set cameras at fixed locations with respect to the moving people, meaning that training data might be obtained from different view directions than test data.In this paper, we describe an architecture to label activities using multiple views. Fig. 1shows the main structure of our architecture. We assume that there are one or more cameras, and that each camera can compute one or more blocks of features representing each frame. Breaking features into blocks allows us to insert new sets of features without disrupting the overall architecture. In the first step, each block of features for each frame of each camera is used for a nearest neighbor query, independent of all other cameras, frames or blocks (Section 4).In the second step, the resulting matches are combined with a weighting scheme. Because the viewing direction of any camera with respect to the body is unknown, some frames (or feature blocks) might be ambiguous. We expect that having a second view should disambiguate some frames, so it makes sense to combine matches over cameras. However, close matches are very likely to be right. This suggests using a scheme that allows (a) several weakly confident matches that share a label to support one another and (b) strongly confident matches to dominate (see Fig. 2). This stage reports a distribution of similarity weights over labels, but conceals the number of cameras or of features used to obtain it, so that later decision stages can abstract away these details (Section 4.1). Finally, we use temporal smoothing, to estimate the action in a short sequence (Section 4.2).Our architecture requires no volume reconstruction and makes engineering easy in new sets of features. When a set of features in a camera is confident, it dominates the labeling process for that frame. Similarly, the frames in a sequence that are confident dominate the decision for a sequence. Our experiments (Section 5) demonstrate that our method performs at the state of the art. We show results for several types of features. It is straightforward to incorporate new cameras or new features into our method. Performance generally improves when there are more cameras and more features. Our method is robust to differences in view direction; training and test cameras do not need to overlap. Discriminative views can be exploited opportunistically. Performance degrades when the test and training data do not share viewing directions. Camera drop in or drop out is handled easily with little penalty. There is no need to synchronize and calibrate cameras.The main point of our paper is to show that, when one has multiple views of a person, straightforward data fusion methods give comparable recognition performance with that produced by 3D reconstruction in the context of a radically simpler system architecture with significant advantages.

@&#CONCLUSIONS@&#
