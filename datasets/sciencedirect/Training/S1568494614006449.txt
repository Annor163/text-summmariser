@&#MAIN-TITLE@&#
Choice function based hyper-heuristics for multi-objective optimization

@&#HIGHLIGHTS@&#
Selection learning hyper-heuristics is proposed for multi-objective optimization.GDA and LA are utilized as move acceptance within the hyper-heuristic framework for multi-objective optimization.The D metric is integrated into the move acceptance methods to enable the approaches to deal with multi-objective problems.The experimental results demonstrate the effectiveness of non-deterministic move acceptance strategy based methodology.The proposed methods are tested on a generic benchmark and a real-world problem.

@&#KEYPHRASES@&#
Hyper-heuristic,Metaheuristic,Great deluge,Late acceptance,Multi-objective optimization,

@&#ABSTRACT@&#
A selection hyper-heuristic is a high level search methodology which operates over a fixed set of low level heuristics. During the iterative search process, a heuristic is selected and applied to a candidate solution in hand, producing a new solution which is then accepted or rejected at each step. Selection hyper-heuristics have been increasingly, and successfully, applied to single-objective optimization problems, while work on multi-objective selection hyper-heuristics is limited. This work presents one of the initial studies on selection hyper-heuristics combining a choice function heuristic selection methodology with great deluge and late acceptance as non-deterministic move acceptance methods for multi-objective optimization. A well-known hypervolume metric is integrated into the move acceptance methods to enable the approaches to deal with multi-objective problems. The performance of the proposed hyper-heuristics is investigated on the Walking Fish Group test suite which is a common benchmark for multi-objective optimization. Additionally, they are applied to the vehicle crashworthiness design problem as a real-world multi-objective problem. The experimental results demonstrate the effectiveness of the non-deterministic move acceptance, particularly great deluge when used as a component of a choice function based selection hyper-heuristic.

@&#INTRODUCTION@&#
Hyper-heuristics perform a search over the space of heuristics when solving problems. In a hyper-heuristic approach, different heuristics or heuristic components can be selected, generated or combined to solve a given computationally difficult optimization problem in an efficient and effective way. A selection hyper-heuristic, which is the focus of this study, manages a predetermined set of low level heuristics with the goal of choosing the best one at any given time using a performance measure maintained for each low level heuristic. This type of hyper-heuristic comprises two main stages: heuristic selection and move acceptance strategy. A selection hyper-heuristic is often described as heuristic selection-move acceptance. Hyper-heuristics are sufficiently general and modular search methods enabling reuse of their components for solving problems from different domains [1]. The task of heuristic selection, also referred to as the high level strategy, is to guide the search intelligently and adapt taking into account the success/failure of the low level heuristics or combinations of heuristic components during the search process.The low level heuristics in a selection hyper-heuristic framework are in general human designed heuristics which are fixed before the search starts. An initial solution (or a set of initial solutions) is iteratively improved using the low level heuristics until some termination criteria are satisfied. During each iteration, the heuristic selection decides which low level heuristic will be employed next. After the selected heuristic is applied to the current solution(s), a decision is made whether to accept the new solution(s) or not using an acceptance criteria. Usually, in a selection hyper-heuristic framework, there is a clear separation between the high level strategy and the set of low-level heuristics or heuristic components. It is assumed that there is a domain barrier between them [2]. The purpose of domain barrier is increase the level of the generality of hyper-heuristics by being able to apply it to a new of problem without changing the framework. Only a new set of problem-related low-level heuristics need to be supplied. The barrier allows only problem domain independent information to flow from the low level to the high level, such as the fitness/cost/penalty value measured by an evaluation function, indicating the quality of a solution [3]. Low level heuristics, or heuristic components, are the problem domain specific elements of a hyper-heuristic framework. Hence they have access to any relevant information, such as candidate solution(s).Many real-world optimization problems are multi-objective requiring improvement of more than one objective, simultaneously. Often, there is some trade-off between multiple conflicting objectives [4–7]. Hence, the multi-objective approaches provide a set of improved solutions (not a single solution as in single objective optimization) capturing the trade-off between those objectives for a given problem at the end of the search process. There is a variety of population based approaches for multi-objective optimization in the scientific literature, such as NSGAII [8], SPEA2 [9], and MOGA [10]. However, there are a few studies on multi-objective selection hyper-heuristics. To the best of the authors’ knowledge, this paper is one of the first studies that investigate the influence of the move acceptance component on the performance of a selection hyper-heuristic for multi-objective optimization. In this study, we extend our previous work in [11] which describes a HHMO_CF_AM multi-objective hyper-heuristic controlling a set of low level (meta-)heuristics (NSGAII [8], SPEA2 [9], and MOGA [10]). We have adopted the great deluge algorithm (GDA) and late acceptance (LA) separately as a non-deterministic move acceptance component of a selection hyper-heuristic for multi-objective optimization and we have tested the performance of the overall algorithm using the same set of low level heuristics as in our previous study on the well-known Walking Fish Group (WFG) benchmark instances [12]. Moreover, we have applied the proposed selection hyper-heuristics with embedded GDA and LA, on a multi-objective real-world problem of vehicle crashworthiness[13] for which a solution aims to provide a vehicle design satisfying multiple objectives reducing different types of injuries as much as possible for the passengers within the vehicle during a crash. The empirical results are aligned with the previous observations for single objective optimization [14] that different combinations of heuristic selection and move acceptance under a selection hyper-heuristic framework yield different performances. Move acceptance components could be extremely influential on the overall performance of a selection hyper-heuristic. Moreover, the proposed multi-objective hyper-heuristic, embedding GDA, turns out to be an effective, reusable and general approach for multi-objective optimization. The empirical results show that it is the best option as a multi-objective selection hyper-heuristic move acceptance component, outperforming each individual low level (meta-)heuristic run on their own for the WFG instances and NSGA II for the vehicle crashworthiness design problem.The rest of the paper is organized as follows. In Section 2, a broad overview of the scientific literature on move acceptance methods, in particular the great deluge and late acceptance algorithms, is provided. An overview of existing studies on multi-objective selection hyper-heuristics and a selection hyper-heuristic framework supporting the use of great deluge and late acceptance move acceptance methods for multi-objective optimization are covered in Section 3. The experimental results for the proposed hyper-heuristics to the WFG benchmark and vehicle crashworthiness problem instances are provided in Sections 4 and 5, respectively. Finally, the conclusions are presented in Section 6.The choice of heuristic selection and move acceptance methods in selection hyper-heuristics influences the performance of a hyper-heuristic [14]. A move acceptance criterion can be deterministic or non-deterministic. A deterministic move acceptance criterion produces the same result given the same initial solutions. A non-deterministic move acceptance criteria may generate a different result even when the same solutions are used. This could be because the move acceptance criterion depends on time or it might have a stochastic component while making the accept/reject decision. Examples of deterministic move acceptance criteria are All-Moves, Only-Improving and Improving & Equal. In All-Moves, the candidate solution is always accepted whether a move worsens or improves the solution quality. The candidate solution in Only-Improving criteria is accepted only if it improves the solution quality, while in Improving & Equal criteria, the candidate solution is accepted only if it improves or it is equal to the current solution. For a non-deterministic move acceptance criteria, the candidate solution is always accepted if it improves the solution quality, while the worsening solution can be accepted based on an acceptance function some of which include the great deluge algorithm [15], simulated annealing [16] and Monte Carlo [17].The choice function (CF) is introduced as a heuristic selection method as part of a selection hyper-heuristic in Cowling et al. [18]. The choice function maintains a score for each low level heuristic and chooses the one with the highest score at each decision point during the search process. A low level heuristic's score depends on whether or not the heuristic generates improvement when used individually, when used in cooperation with another heuristic and how much time has been passed since its last invocation. This initial study has been followed by many other studies indicating the success of choice function based hyper-heuristics using different move acceptance methods on different problems. Cowling et al. [19] developed an approach using several proposed hyper-heuristic components in order to solve a real-world scheduling problem; namely project presentations. The approach employed deterministic move acceptance strategies {All-Moves, Only-Improvements} and seven heuristic selection methods {Simple Random, Random Gradient, Random Permutation, Random Permutation-Gradient, Greedy, Reinforcement Learning, Choice Function}. The experimental results show that choice function all-moves performs better than simple random moves over the given problems, and produced better solutions than those produced by humans.There are a few comparative studies which evaluate the performances of different heuristic selection and move acceptance methods. A set of seven different heuristic selection strategies (Simple Random, Random Descent, Random Permutation, Random Permutation Descent, Greedy, Choice Function, Tabu Search) are combined with a set of five acceptance strategies {All-Moves, Only-Improving, Improving & Equal, Exponential Monte Carlo with Counter, GDA}. The combination set is tested on fourteen benchmark functions against genetic and mimetic algorithms. Choice Function-Improving & Equal performs the best [14]. Another study was conducted by Bilgin et al. [20] using a set of eight heuristic selection strategies {Simple Random, Random Gradient, Random Permutation, Random Permutation Gradient, Greedy, Choice Function, Reinforcement Learning, Tabu Search} and five move acceptance strategies {All-Moves, Only-Improving, Improving &Equal, GDA, EMCQ} which were tested on different timetabling benchmark problems. The study showed that there is no one strategy that dominates. In the scientific literature, a wide variety of hyper-heuristics have been proposed that use different heuristic selection and acceptance strategies in different domains: packing, vehicle routing, timetabling, channel assignment, component placement, personnel scheduling, planning and shelf space allocation (see Ref. [21]). The choice function simulated annealing hyper-heuristic performed better than a simple random great deluge hyper-heuristic over a set of examination timetabling problems as presented in [20]. In [22] different heuristic selection methods {Simple Random, Greedy, Reinforcement Learning, Reinforcement, Tabu Search, Choice Function} were combined with a Late Acceptance methodology. The results show that the random heuristic selection with late acceptance obtained the best results on the examination timetabling problem.All these previous studies focus on single-objective optimization problems. To the best of the authors’ knowledge, this paper is one of the first studies that investigate the influence of the move acceptance component of selection hyper-heuristics for multi-objective optimization. In the following subsections we describe the move acceptance methods that we have used in our experiments, as well as related work.The great deluge algorithm (GDA) is a metaheuristic proposed by Dueck [15] using a threshold move acceptance criterion as illustrated in Algorithm 1. This algorithm is adopted from [15], assuming a maximization problem. GDA always accepts improving moves, while a worsening move is accepted only if it is better than a threshold (target improvement denoted as LEVEL) at a given step. The algorithm starts with an initial water level, which is often taken as the quality of the initial solution (step 4). At each step, a solution in hand (s) is modified and a new solutions (s*) is obtained from its neighbourhood using a move operator (step 6). The water level is increased gradually (usually linearly) at each iteration, during the search, according to a predefined rate referred to as Rain Speed (UP). A worsening solution (s*) is accepted if the quality of the solution (measured by (f(s*)) is greater than or equal to the water level (steps 7 and 8) and then the water level is updated (step 9). The algorithm terminates when there is no change in the solution quality within a predefined time or when the maximum number of iterations is exceeded.Algorithm 1Great deluge algorithm1:procedure GDA2:Produce an initial solution s3:Choose an initial rain speed UP>04:Choose an initial water level LEVEL>0 ▷ LEVEL=f(s)5:repeat6:Obtain a new solution s*∈N(s, Q) from s using a move operator Q7:if (f(s*)>LEVEL) then8:s=s*9:LEVEL=LEVEL+UP10:end if11:until (termination criteria are satisfied)12:end procedureThe main advantage of GDA is that it is simple and easier to implement when compared to many other metaheuristics, such as, simulated annealing [16] and Tabu search (TS) [23]. Moreover, better quality solutions could be produced with an increased search time [24]. GDA requires fewer input parameters; in fact it only has one parameter, rain speed (UP) [25]. The value of UP is usually a small value greater than 0, and less than 0.03 [26]. Dueck [15] provided various recommendations regarding UP. For example, a suggestion is that UP should be on average smaller than 1% of the average distance between the quality of the current solution and the water level. So the water level can be calculated for a solution s* using:(1)LEVEL=LEVEL+UP(LEVEL+f(s*))The value of UP can also be calculated based on the time allocated for the search and by defining upper/lower bounds of an estimated quality of solution [27]. However, both of those parameters depend on the problem dimensions and can affect the quality of final solution for a given problem [28]. An extended GDA with reheating was proposed by McMullan and McCollum [29]. The idea is similar to the reheating scheme utilized in simulated annealing. The reheating (re-levelling in the GDA context) aims to widen the boundary condition, via improving the rain speed, in order to allow a worsening move to be accepted and avoid becoming trapped in a local optimum. If there is no improvement, water level is reset and re-levelling strategy is applied using a new rain speed value based on the number of total moves in the process.GDA has been used in many hyper-heuristic approaches as an acceptance move strategy. Özcan et al. [30] proposed a reinforcement learning great deluge hyper-heuristic. It was applied to examination timetabling, producing good quality solutions when compared to some other approaches in the literature. Kendall and Mohamad [31] presented a variant of a GDA based hyper-heuristic. It was applied to channel assignment benchmarks. The experimental results show that a Simple Random GDA hyper-heuristic produced good results when compared to a constructive heuristic and a genetic algorithm. In addition, a variant of the GDA hyper-heuristic approach including flex deluge, non-linear and extended great deluge is proposed in [32]. These approaches were applied to large scale and highly constrained timetabling problems and tested on exam timetabling benchmark problems. The experimental analysis demonstrates that non-linear great deluge produced the best results compared to other approaches.In the scientific literature, there are many other studies that investigate GDA and its variants in tackling various optimization problems. However, the majority of them are applied to optimization problems with a single-objective. In fact, there is only one study that proposed the GDA for multi-objective optimization [33] in which weighted sum of the objectives is used for multi-criteria examination timetabling. GDA guides the search process via a trajectory, determined by adaptively changing weights. In this study, we employ a different method rather than reducing the multiple objectives into a single objective.Late acceptance local search (LALS) is an iterative method, proposed recently by Burke and Bykov [34]. This approach won an international competition to automatically solve the Magic Square problem which was later beaten by a selection hyper-heuristic [35]. It is based on a hill-climbing framework as illustrated in Algorithm 2 (adopted from [34]) which embeds a new move acceptance strategy. The idea is to delay the comparison between the cost of the current solution and the previous solution, hence the move acceptance method is referred to as late acceptance (LA) at the core of the local search algorithm. LA is simple and easy to implement, requiring implementation of a list C of size L. Each item Cl(0≤l<L) in the list contains the cost (fitness/objective) value of a solution visited in the lth previous iteration. At the beginning of the search, C list is filled by the initial cost value. During the search process, a new solution s* is obtained by applying the move operator to the current solution s. The cost of the new solution f(s*) is compared to the cost of a previously visited solution. This is done via the use of the list C. The last element indicating the cost of a solution visited L steps prior to the current step (i) is maintained at Cl. Hence, Clis compared to the cost of the new solution f(s*) for the accept/reject decision. If this cost is better than or equal to the cost of the last element, then the new solution is accepted (s is set to s*). The cost of s is inserted into the beginning of the list, while the last element is removed from the list. This process is repeated until a set of stopping criteria is met. In order to avoid shifting the whole list at each iteration and reduce the processing time of LA, a circular queue is employed as suggested and l is calculated using the following formula:(2)l=imodLwhere mod represents the remainder of integer division, ith is the current iteration, L is the length of the cost list C.Algorithm 2Late acceptance local search algorithm1:procedure LALS2:Begin3:Produce an initial solution s4:Calculate cost of s using f(s)5:for ∀l∈{0, …, L−1} do6:Cl=f(s)7:end for8:Initialize the iteration counter, i=09:repeat10:Create a new solution s* from s using a move operator11:Calculate its cost by f(s*)12:l=i mod L13:if (f(s*)≤Clor f(s*)≤f(s)) then14:Accept candidate (s=s*)15:end if16:Cl=f(s)17:i=i+118:until (a chosen stopping condition)19:end procedureIntuitively, in order to be able to exploit the unique features of LA, L should be set to a value less than the number of iterations and greater than or equal to two. If L is equal to one, LALS becomes a greedy hill-climber [34]. If it is equal to the number of iterations, the search could turn into random walk depending on the move operator. Since LA is a fairly new methodology, there is only a limited number of studies in the scientific literature and none of those previous studies has dealt with a multi-objective optimization problem. In [22], the late acceptance strategy was combined with different heuristic selection methods (Simple Random, Greedy, Reinforcement Learning, Tabu Search and Choice Function) and applied to examination timetabling problems. The experiments show that the random heuristic selection with late acceptance performs well among other combination methods. In [36], an experimental comparison of LALS with well-known search methods (simulated annealing (SA), threshold accepting (TA) and GDA were carried out on the traveling salesman and exam timetabling problems. The results show the success of LALS when its performance is compared to the others.The interest in selection hyper-heuristics has been growing in recent years. However, the majority of research in this area has been limited to single-objective optimization. To date, only a limited number of studies have been identified that address/deal with selection hyper-heuristics for multi-objective problems. Section 3.1 discusses some of those selection hyper-heuristics. Section 3.2 describes the proposed choice function based hyper-heuristic which embeds non-deterministic acceptance methods.

@&#CONCLUSIONS@&#
