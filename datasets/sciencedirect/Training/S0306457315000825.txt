@&#MAIN-TITLE@&#
Associative topic models with numerical time series

@&#HIGHLIGHTS@&#
Introduce a probabilistic graphical model extracting topics with numerical guidance.Enhance the regression performance with unified PGM of text and numbers.Tightly links the analysis on numeric and text data over time.

@&#KEYPHRASES@&#
Time series analysis,Topic models,Text mining,

@&#ABSTRACT@&#
A series of events generates multiple types of time series data, such as numeric and text data over time, and the variations of the data types capture the events from different angles. This paper aims to integrate the analyses on such numerical and text time-series data influenced by common events with a single model to better understand the events. Specifically, we present a topic model, called an associative topic model (ATM), which finds the soft cluster of time-series text data guided by time-series numerical value. The identified clusters are represented as word distributions per clusters, and these word distributions indicate what the corresponding events were. We applied ATM to financial indexes and president approval rates. First, ATM identifies topics associated with the characteristics of time-series data from the multiple types of data. Second, ATM predicts numerical time-series data with a higher level of accuracy than does the iterative model, which is supported by lower mean squared errors.

@&#INTRODUCTION@&#
Probabilistic topic models are probabilistic graphical models used to cluster a corpus with semantics (Blei, Ng, & Jordan, 2003; Griffiths & Steyvers, 2004; McCallum, Corrada-Emmanuel, & Wang, 2005), and they have been successful in analyzing diverse sources of text as well as image data (Gupta & Manning, 2011; Hörster, Lienhart, & Slaney, 2007; Khurdiya, Dey, Raj, & Haque, 2011; Ramage, Dumais, & Liebling, 2010). The essence of the topic models is the probabilistic modeling of how to generate the words in documents, given the prior knowledge of word distributions per key ideas, called topics, in the corpus. Specifically, a topic is a probability distribution over a vocabulary, and the topic models assume that a document is a mixture of multiple topics. These ideas are implicit because they cannot be directly observed, so some describe these topics as latent topics (Blei, 2012). The latent Dirichlet allocation (LDA) (Blei et al., 2003), one of the popular topic models, is the Bayesian approach to modeling such generation processes.Given the success of LDA, many expansions of LDA are introduced, and they strengthen the probabilistic process of generating topics and words with additional information. One type of such additional information is the meta-data of documents. For instance, the author-topic model (Rosen-Zvi, Griffiths, Steyvers, & Smyth, 2004) includes the authorship model to produce additional estimations on the authorship as well as better topic modeling results. Another type of information is the prior information on the corpus characteristics. For instance, the aspect-sentiment unification model (Jo & Oh, 2011) utilizes the sentiment information of words as priors to estimate the sentiment of topics. While the expansion is motivated by the additional information, such expansion is often realized by either adding additional variables in the graphical model or calibrating prior settings in the inference process.While most variations of LDA augment the data, such as sentiment lexicons and authorships, from either text data or the meta-data of the corpus, several models relate text data to other types of data, such as geospatial data (Sizov, 2012) and document-level labels (Blei & McAuliffe, 2007; Chen, Zhu, Kifer, & Lee, 2010; Ramage, Hall, Nallapati, & Manning, 2009). This paper considers integrating texts and numerical data over time. We assume that there are events that become the common cause of dynamics in the text and the numeric data. Then, we are interested in identifying the representations of such events which are not directly observable from the corpus and the numbers. For instance, there are latent events, such as interest rate changes or policy changes, in the stock markets that cause the generation of news articles and numerical data. Our goal is to represent such latent events with word distributions per events. The key to identifying such events is the correlation between texts and numeric variables associated with the events. Such correlations between numerical and textual data are common in product sales (Liu, Huang, An, & Yu, 2007), candidate approval (Livne, Simmons, Adar, & Adamic, 2011), and box-office revenues (Asur & Huberman, 2010). In these domains, understanding why we see such numerical fluctuation with the textual context can provide a key insight, i.e., finding a topic that resulted in the huge drop of a stock index.This paper introduces a new topic model called an associative topic model (ATM), which correlates the time-series data of text and numerical values. We define an associative topic as a soft cluster of unique words whose likelihood of cluster association is influenced by the appearances on documents as well as the fluctuation of numeric values over-time. The association likelihood of words to clusters, or associative topics, is inferred to maximize the joint probability including factors of text generation and numeric value generation by the proportion of topics over-time. The topic proportion is the likelihood of the cluster, or topic, appearance on a certain time, which means that our data consists of timed batch datasets of texts and numbers over the period. In other words, The model assumes that the text and the numerical value at the same timed-batch are generated from a common latent random variable of topic proportions, which indicate the relative ratio of topic appearances at the time (Blei & Lafferty, 2006; Putthividhy, Attias, & Nagarajan, 2010). For instance, a topic on tax could be strongly related to and influencing to the generation of economic articles as well stock prices, for example. Then, we interpret that increasing the proportion of tax topic is the event influencing the numerical-data and text-data generation.This assumption enables the numerical value to adjust the topic extraction from texts through its movements. Fig. 1describes an example of an associative topic. The associative topic provides summarized information about text-data, and its appearance over time is highly related to the numerical time-series data. Such associative topics are useful in interpretations and predictions. For the interpretation, the associative topics hold more information about the numerical context than do the topics from only texts. Also, the associative topics enable the prediction of the numerical value of the next time step with high accuracy. Fig. 2explains the inputs and the outputs of ATM. A time-series numerical datum,y1:T, and time-series text datum,D1:Tare input data for ATM. ATM provides associated topics with correlation indexes and topic proportions over time indicating the dynamics of topic appearances in text data. Additionally, ATM can predict the next time numerical value,yT+1, with the next time text data,DT+1.Inherently, the proposed model infers associative topics that are adjusted to better explain the associated numerical values. When this paper applies the model to the three datasets, the economic news corpus and the stock return from the Dow Jones Industrial Average (DJIA), the same news corpus and the stock volatilities from the DJIA, and the news corpus related to the president and president approval index, we observe that the model provides adjusted topics that explain each numerical value movement. Associative topics extracted from the proposed model have a higher correlation with time-series data when there is a relationship between the text and the numerical data. Also, the model becomes a forecasting model that predicts the next time variable by considering the past text and numerical data. We experimented with the model to predict the numerical time-series variable, and we found that the proposed model provides topics better adapted to the time-series values and better predicts the next time numerical value than do the previous approaches. The contributions of the paper are summarized as follows:–We introduced a model to analyze a bi-modal dataset including text and numerical time-series data, which have different characteristics. This is the first unified topic model for the numerical and text bimodal dataset without batch-processing two different models for each data type.In the parameter inference of the presented model, we introduced a method for estimating the expectation of the softmax function with Gaussian inputs, which was inevitable in the process of relating the numerical and the text data. This parameter inference technique is applicable to diverse cases of joining two datasets originating from the continuous and discrete domains.There are several attempts to integrate side information with topic models. Basically, three kinds of side information have been considered: document-level features (i.e., categories, sentiment label, hits, etc.), word-level features (i.e., term volume), and corpus-level features (i.e. product sales, stock indexes). For document-level features, several topic models (Blei & McAuliffe, 2007; Ramage et al., 2009; Zhu, Ahmed, & Xing, 2012) are suggested to incorporate features of a document with the topic information of the document. In order to incorporate them, either the conditional latent topic assignment or the hyper-parameters of conditional distribution is used. For word-level features, a fully generative model (Hong, Yin, Guo, & Davison, 2011) was developed to incorporate features of a word with the significance of the word in topics. More specially, the model integrates a temporal topic model and temporal term-volumes, which is a sort of time series. This work is similar to our work in modeling time-series variables. However, they focused on the prediction of word volumes in text, while we considered time-series numerical variables from a different source not directly related to text data. Also, they assumed topic representation would be helpful for predicting word volumes, while we assumed that topic proportions in a period of time are related to time-series numerical values of the different source. For corpus-level features, which are our viewpoints of external time series, there were several attempts (Bejan & Harabagiu, 2008; Chen, Li, Xu, Yang, & Kitsuregawa, 2012; Kim et al., 2013) to analyze relationships between features in a corpus at time t and topic proportions of the corpus. Unlike the works for document-level and word-level features, they proposed no fully generative models, and therefore, these feature values cannot be easily inferred.While there is no unified model of corpus-level numerical values and texts over time, there have been efforts to estimate topics related to time series variables by iteratively utilizing multiple statistical models. Iterative Topic Modeling Framework with Time Series Feedback (ITMTF) (Kim et al., 2013) creates a feedback loop between a topic model and a correlation model by utilizing the prior distributions of the topic model as bridging variables. However, the batch style of applying multiple models may have two shortcomings. First, the topic model and the correlation model have two different joint probabilities that are not shared in their inference processes, so the inference to increase one joint probability might not be the right direction to increase the other joint probability. Second, the underlying assumptions might conflict. Each statistical model has underlying assumptions of probability distributions, and the statistical models set up the inference process based upon the assumptions. When chaining these models, the two assumptions regarding the shared variable can be different.Because of these potential shortcomings, we created a unified model of numerical values and texts as a single probabilistic graphical model instead of chaining two different statistical models. Some previous studies illustrate a prediction of a numerical time-series data with text topics. For example, some researches (Chen et al., 2012; Mahajan, Dey, & Haque, 2008; Si et al., 2013) used a topic model to analyze themes in corpus, and then they inferred a prediction model, i.e., a linear regression, by applying the identified topics as observed variables. These methods held similar shortcomings to those we described above.This section provides the description of our proposed model, ATM, used to infer the associative topics jointly influenced by the texts and the numeric values. Fundamentally, ATM is a model of Bayesian network, so the first subsection provides its probabilistic graphical model and its independence/causality structure between modeled variables. The first subsection compares and contrasts ATM to its predecessor model, DTM; and the subsection provides a description on the generative process of texts and numeric values from the topic proportion, which is a standard model description in the generative modeling community. The second subsection explains the inference of modeled latent variables with the observed data by following the structure of ATM. We used the variational inference to optimize the parameters of the latent random variables. The third subsection shows the formula to calculate the parameters which were not inferred in the variational inference process. The fourth subsection shows how to turn ATM, which is intrinsically a soft cluster model, into a model predicting the numerical value of the next time.Here, we describe an ATM that learns associative topics from time-series text data regarding time-series numerical values. Our model captures the trajectory of topic proportions over time, and topics in the trajectory are correlated with a numerical variable over time. The model can be considered a combination of a Kalman filter and LDA. A similar model, which motivated ATM, is the dynamic topic model (DTM) (Blei & Lafferty, 2006), which explores the dynamics of topics,β1:T, and topic appearances over time,α1:T. Fig. 3presents DTM and ATM in graphical representations. The difference between DTM and ATM lies in: (1) adding the numerical time-series variable,y1:T, and (2) simplifying the word distribution by topics, β.The first difference is the existence of numerical time-series values influenced by the latent variables of the corpus. The objective of the proposed model requires the inclusion of the numerical time-series variables, so we made a causal link from the topic proportions,α1:T, to the numerical variables,y1:T, by assuming that the latent topic from the corpus would generate the numerical values as well as the words in the corpus.The second difference is the simplified word distributions of topics, β, compared to DTM because we wanted to see a single set of topic words over the period for a simplified insight. If we utilize ATM in the practical field, i.e., stock analyses, the presentation of the dynamic changes in the topic words might not be intuitive to non-experts. If we support the dynamic changes of topic words, it might increase the accuracy on the prediction result of the numerical values, but this would decrease the clarity of the topic interpretation representing the overall period. Additionally, there is a potential risk of overfitting topics to the numeric values.From now, we describe the details of the assumptions of ATM in Fig. 3. ATM has additional y variables from DTM. In DTM and ATM, α is modeled by the Gaussian distribution to express the dynamics of topic appearances over time. Then, α is linked to the topic proportions of documents, θ, that have the same distribution type:(1)αt|αt-1∼N(αt-1,σ2IK)(2)θt,d|αt∼N(αt,a2IK)whereIKis K-dimensional identity matrix. In the above, each co-variance matrix is modeled as a scalar matrix to reduce the computational cost. A topic of each word, z, in a document is assigned in accordance with the topic proportion of the document:(3)zt,d,n|θt,d∼Multi(π(θt,d))In Formula (3), n is the number of words in the d document at time t. To turn the Gaussian distribution into a prior of multinomial distribution, we used a softmax function, π(θ), as defined below.(4)πk(θ)=exp(θk)∑i=1Kexp(θi)Another interpretation of using the softmax function is that the scale of θ is ignored and relative differences within θ are used to decide a topic for a word. Finally, each word is generated from topic distributions,β1:K, over words from a vocabulary of size V. Up to this point, the modeling approach of ATM is very similar to that of DTM.The main idea of ATM is on generating the numerical time-series variables, y, from the prior information of α. In order to use the same information of the topic assignments on words for predicting y, the numerical time-series variables are generated from α through the same softmax function in Formula (4). The equation below specifies the modeled causality between y and α.(5)yt|αt∼N(bTπ(αt),δ2)where b is a vector of K-dimensional linear coefficient parameters.Formula (5) is the key mechanism that propagates the influence from the topic proportion,αt, to the numeric values,yt. We model the relation between the topic proportion and the numeric values as a linear combination, fundamentally. However, our model is different from a typical Gaussian distribution with conditional information, i.e.N(bTαt,δ2), because we apply a softmax function,π(αt), to the random variable of the topic proportion,αt. Therefore, this paragraph clarifies the application of the softmax function in the generation process of numeric values. The topic proportion,αt, itself is modeled as a multivariate Gaussian distribution as Formula (1). This suggests that the value has an infinitely long tail in a direction of the modeled dimension. Hence, there is an overfitting risk to infer a value, or an element ofαt, from such long tail while learning the parameters from the text side. Then, this overfitted part of,αt, will distort the inference ofb, because both will be combined as a linearly weighted sum if there is no other precaution made. To limit such extreme influence from the text to the numeric values, we use a soft-maxed topic proportion, which isπ(αt), instead of the simple linear combination. The soft-max function has a finite range with continuous activation functionality. Thus, the topic proportion will activate the influence to the numeric value, but the influence will be limited, or squashed in a casual statistics term, to the level of the activation.Additionally, Formula (5) has an interesting variable to control,δ2, in its variance term. The variance ofδ2affects the equation as a strength factor for fitting the numerical values to the topic proportions,αt. Learningδ2means that ATM will appreciate and admit such amount of variance inytfrom the expectation ofytdetermined by the linear combination ofbandαt. If we limit the variance level,δ2to the small amount, the inferred coefficients corresponding to each associative topic will be forced to over-fit the numeric value movement, and this will eventually provide too strong bias in extracting the parameters in the topic extraction of the text modeling part. On the other hand, the high variance ofδ2will provide no guidance to the extracted topics, so this will turn the resulted topics to the ordinary topics without the guidance from the numeric values. Therefore, we experiment two versions of ATM. The first version is fixed-ATM which fixes the varianceδ2as a very small value to maximize the fitness to the numeric values by accepting the possibility of over-fitted associative topics. The second version is an ordinary ATM which infers the varianceδ2without any fixed point, so the extracted topics could be more meaningful by avoiding the over-fitness to the numeric values (see Formula (23)). Such adjustment of the influence strength from the topic proportion to the numeric value is experimented by comparing the result of fixed-ATM and ATM in the result section.Fig. 4is the generative process summarizing the assumptions of ATM. As we mentioned, the differences from DTM are (1) the generative process on numerical time-series value,yt, and (2) the unified topics, β ignoring dynamic changes by time.Due to the non-conjugacy between distributions in ATM, the posterior inference becomes intractable. We employed the variational method (Jaakkola, 2001; Jordan, Ghahramani, Jaakkola, & Saul, 1998) to approximate the posterior in the ATM. The idea behind the variational method is to optimize the free parameters of a distribution over the latent variables by minimizing the Kullback–Leibler (KL) divergence.In ATM, the latent variables are: (1) corpus-level latent states of topic proportions,αt, (2) document-level latent states of topic proportionsθt,d, and (3) topic indicatorszt,d,n. Below are our factorized assumptions of the variational posterior,q(α,θ,z).(6)q(α,θ,z)=q(α1:T|α̂1:T)∏t=1T∏d=1Dtq(θtd|γtd,Λtd)∏n=1Ntdq(ztdn|ϕtdn)whereα̂1:T,γtd,Λtdandϕtdnare variational parameters of the variational posterior over latent variables. Factorized variational distributions ofθtdandztdnare simply modeled,θtd|γtd,Λtd∼N(θtd;γtd,Λtd)andztdn|ϕtdn∼Multi(ϕtdn). However, in the variational distribution ofα1:T, we retained the sequential structure of the corpus topic appearance by positing a dynamic model with Gaussian variational observations,α̂1:T. In DTM, the variational Kalman filter model (Blei & Lafferty, 2006) was used to express the topic dynamics, and we adopted the model for estimating the dynamics of the topic proportions in the corpus over time. The key idea of the variational Kalman filter is that observations in the standard Kalman filter model are regarded as the variational parametersα̂and the posterior distribution of the latent state in the standard Kalman filter as the variational distributionq(α|α̂). Our variational Kalman filter is as follows:(7)α̂t|αt∼N(αt,v̂Ik)(8)αt|αt-1∼N(αt-1,σ2Ik)Using the standard Kalman filter calculation, the forward mean and variance are given by(9)αt|α̂1:t∼N(αt;μt,Vt)μt=v̂tVt-1+σ2+v̂tμt-1+1-v̂tVt-1+σ2+v̂tα̂tVt=v̂tVt-1+σ2+v̂t(Vt-1+σ2)with an initial condition of fixedμ0and fixedV0. The backward mean and variance are given by(10)αt|α̂1:T∼N(αt;μ̃t,Ṽt)μ̃t=σ2Vt+σ2μt+1-σ2Vt+σ2μ̃t+1Ṽt=Vt+1-σ2Vt+σ22Ṽt+1-(Vt+σ2)In Formula (10),μ̃tandṼtare drawn from variational Kalman filter calculations. We approximate the posteriorp(α1:T|w1:T,y1:T)using the state space posteriorq(α1:T|α̂1:T).Using these variational posteriors and Jensen’s inequality, we can find the lower bound of the log likelihood as follows:(11)logp(w1:T,y1:T)⩾∬∑zq(α,θ,z)logp(w,y,α,θ,z)q(α,θ,z)dθdα=Eq[logp(α1:T)]+∑t=1T∑d=1DtEq[logp(θtd|αt)]+∑t=1T∑d=1Dt∑n=1NtdEq[logp(ztdn|θtd)p(wtdn|ztdn)]+∑t=1TEq[logp(yt|αt)]+H(q(α,θ,z))This bound contains four expectation terms related to the in-hand data. The first term is related to the latent states from both sources of data. The second and third expectation terms are related to the text data and easily found by the same expectation terms in DTM. The fourth term,Eq[logp(yt|αt)], is related to the continuous time-series data that we introduce. The first term on the right-hand side of Formula (11) is:(12)Eq[logp(α1:T)]=∑tEq[logp(αt|αt-1)]∝-TK2logσ2-12σ2∑tμ̃t-μ̃t-1-1σ2∑tTr(Ṽt)-12σ2(Tr(Ṽ0)-Tr(ṼT))using the Gaussian quadratic form identity(13)Em,V[(x-μ)TΣ-1(x-μ)]=(m-μ)TΣ-1(m-μ)+Tr(Σ-1V)The second term of the right-hand side of Formula (11) is:(14)Eq[logp(θtd|αt)]∝-K2loga2-12a2γtd-μ̃t-12a2(Tr(Ṽt)+Tr(Λtd))The third term is:(15)Eq[logp(ztdn|θtd)p(wtdn|ztdn)]⩾∑kϕtdnkγtdk-∑kϕtdnklogζ̂td+1ζ̂td∑ieγtdk+Λtdk/2+∑kϕtdnklogβk,wtdnintroducing additional variational parametersζ1:T,1:Ddue to the softmax function whose input variables are drawn from the Gaussian distribution. The closed form of expectation cannot be calculated, but the lower bound of the expectation can be found. This treatment of the lower bound of the expectation keeps the lower bound of log likelihood. The fourth term is:(16)∑t=1TEq[logp(yt|αt)]∝1δ2∑kbk∑tytEq[πk(αt)]-12δ2∑i,jbibj∑tEq[πi(αt)πj(αt)]It should be noted that the we applied the softmax function π to α because we modeled the regression from the discretized topic proportions to the numerical variables. The rationale behind the discretization is that what we see as topics from text should influence the regression, so we applied the same softmax treatment to θ and α. While such discretization for topic extraction occurred in DTM to join the multinomial and the Gaussian distributions, this treatment is joining two Gaussian distributions, which is a difference between ATM and DTM. While this difference might seem trivial, this linkage of two Gaussian distributions with the softmax function invokes a non-trivial calculation of the expectation. In Formula (16), finding the lower bound with the variational parameters of the expectation terms is intractable due to non-concavity caused by opposite signs of the expectations (Bouchard, 2007). Additionally, the closed form of the expectation terms cannot be calculated exactly because of the difficulties in log-normality and the rate of two random variables. Hence, we used an approximation approach to calculate the local expectations using the Taylor expansions for rate expectation (Kendall, Stuart, & Ord, 2010; Rice & Papadopoulos, 2009). To the best of our knowledge, this is the first case of inferencing a probabilistic graphical model with approximate expectations of the softmax function with a Gaussian prior. The expectation of the simple softmax function is shown below.(17)Eq[πi(αt)]=Eqexp(αti)∑kexp(αtk)≈mti+mti(eṼt-1)∑kmtk2-mtiIn Formula (17), a new notation,mti=πi(μ̃t), is introduced. The joint expectations of the two softmax functions are finally approximated as follows:(18)Eq[πi(αt)2]=Eqexp(αti)∑kexp(αtk)2≈eṼtmti2+mti2(eṼt-1)3∑kmtk2-4mti(19)Eq[πi(αt)πj(αt)]=Eqexp(αti+αtj)(∑kexp(αtk))2≈mtimtj+mtimtj(eṼt-1)3∑kmtk2-2mti-2mtjIn Appendix A, we described the approximate method for finding the expectations of softmax functions. The last term of Formula (11) is the entropy term:(20)H(q)=12∑tlog|Ṽt|+12∑t,dlogΛtd-∑t,d,n,kϕtdnklogϕtdnkWith the above expectations terms, we can find an approximated lower bound of the log likelihood. The learning equations for variational parameters are described in Appendix B.Using variational distributions that are approximate posterior distributions over latent variables, we can find update equations for model parameters. When it comes to topic representations, β can be updated as follows:(21)βkv∝∑t,d,nϕtdnkI(wtdn=v)whereI(wtdn=v)is a binary function that is equal to 1 whenwtdn=vor 0 otherwise. Variances of Gaussian noises to text data (a2) and numerical time-series data (δ2) can be updated as follows:(22)a2=1DK∑t,d(γtd-μ̃t2+Tr(Λtd)+Tr(Ṽt))(23)δ2=1T∑t(yt-μ̃tTb)+∑tbTṼtbWe assumed that each document-level latent state of topic proportion θ are drawn from the time-series latent state of topic proportions, α, with the scalar variance,a2, while the numerical time-series variables are drawn from a linear combination of α with a variance,δ2. By learning these variations, we can find the degree of correlation between text data and numerical time-series data. A low value ofδ2means two sources of data are strongly correlated and text data can be helpful to predict time-series variables. Conversely, a high value ofδ2means two sources of data are not closely related to each other. If we set fixed variances at a low value, ATM tends to learn topics that have a high explanatory power about the trajectory of numerical time series in the training set.As noted before, the linear combination of rescaled α and b becomes the estimation of y. To infer b, we maximize the lower bound of its log-likelihood. The function below is the softmax function to infer the coefficient vector.(24)b=∑tEq[π(αt)Tπ(αt)]-1Eq[π(α)]TYIn Formula (24),Eq[π(αt)Tπ(αt)]is aK×Kmatrix whose elements areEq[πi(αt)Tπj(αt)]∀i,j,Eq[π(α)]is a T×K matrix whose elements areEq[πk(αt)], and Y is a numerical value vector over the period T. This update equation is similar to the Gaussian response of Supervised LDA (Blei & McAuliffe, 2007).After all of the parameters are learned, ATM can be used as a prediction model of a future time-series variable (yT+1) by observing new text data (wT+1). The expectation ofyT+1is formalized in the function below.(25)Eq[yT+1|y1:T,w1:T+1]=∑kbkEq[πk(αT+1)|y1:T,w1:T+1]In order to calculate the expectations of the softmax function for the new time-step, we infer the posterior distribution ofαT+1utilizing the posterior distribution ofαT, documentsDt+1, and learned model parameters. This particular inference is based upon Formula (11) except for the fourth expectation terms. After enough iterations of variational inferences, the posterior distribution of (αT+1) can be used to predict the numerical value of the next time step.This section demonstrates the utility of the ATM in both explaining and predicting the time-series values. We applied ATM to a financial news corpus and stock indexes as well as to a news corpus related with the president and president approval index. Section 4.1 shows the detailed description of the datasets. Sections 4.2 and 4.3 describe the overview of our experiments and the baseline models, such as autoregressive models (AR), LDA, DTM, ITMTF. Through Sections 4.3, 4.4, 4.5 and 4.6, we discuss the results of the experiments.We applied ATM in two domains, finance and presidential popularity. For the financial datasets, we used financial news articles from the Bloomberg and the stock indexes of the DJIA from January 2011 to April 2013 (121weeks). Due to the different roles of the indexes, we chose the return, which indicates a profit on an investment, and the volatility, which is a measure for variation of price of a financial instrument. The 60,500 news articles were selected randomly; 500 articles were selected for each week. That is, we used two coupled datasets in the finance domain; one includes Bloomberg articles and the DJIA stock return, and the other includes the same Bloomberg articles and the DJIA stock volatility. For the presidential datasets, we used news articles searched with the name of the president, ‘Obama’ from The Washington Post and the weekly president approval index from January 2009 to June 2014 (284weeks). The 27,582 news articles were selected randomly; fewer than 100 articles were selected for each week. We pruned vocabularies by removing stop-words and terms, which occurred in less than 2% of the documents. We set a time step in the experiment as a week. Table 1describes the summarized information of the datasets.Our experiments are categorized into qualitative and quantitative analysis from a large perspective, see Fig. 5. The qualitative analysis examines the outputs of ATM and fixed-ATM from three datasets with texts and numeric values over-time.Section 4.4 enumerates the associative topics extracted by ATM for a qualitative analysis, and the section contrasts the associative topics to the topics without any such guidance, i.e. the topics from LDA and DTM. Section 4.5 shows the results of the fixed-ATM, and the section shows the associative topics which are biased to overfit the numeric values. Since the extracted topics of fixed-ATM are meant to be only dependent variables for the numeric value prediction, we omit the further analysis on the quality of the fixed-ATM topics. If users values the interpretation of extracted associative topics guided by the numeric values, they are recommended to use ATM instead of fixed-ATM. Section 4.6 provides the quantitative evaluations on the numeric value prediction by measuring the mean squared error and the mean absolute error, and the section includes both models for texts and numeric values as well as models for numeric values only, i.e. AR. Section 4.7 investigated the predictive log-likelihoods that is a quantitative measure in explaining unseen data with a model and a historic dataset.To compare performances of the proposed models, we applied four baseline models. The autoregressive model (AR) is only for analyzing numerical time series, so we compared its numerical value prediction performance with ours. Since other topic models are only for analyzing text data, we applied linear regression models to joint numerical time-series values. The short descriptions of baselines are below.We could analyze continuous time-series values with the AR, which is a traditional time-series analysis tool. In a univariate autoregressive model AR(p), the variableytdepends on its previous values fromyt-1toyt-p. In this model, selecting lag values, p, is important regarding causalities with past information, and we optimized p empirically.The basic topic model, LDA, was applied. We firstly analyzed topics in the total corpus regardless of time steps, then extracted a topic in each document,θk(d). We could compute time-level topic proportions,tckt, from them as normalizing document topic proportions over time:tckt=1N(Dt)∑d∈Dtθk(d), whereN(Dt)is the number of documents at time t. Then, a simple linear regression was applied to evaluate topic association with numerical time-series values.The time-series topic model, DTM, catches the dynamics of the latent state of topic proportionsαt. By utilizing DTM, we could extract the posterior mean ofαtdynamics,μ̃t. Considering the softmax treatment for topic assignments, we extracted topic proportions over time by simply applied the softmax function toμ̃t. After defining topic proportions, we utilized a simple linear regression.Iterative Topic Modeling with Time Series Feedback (ITMTF) is a general text mining framework that combines probabilistic topic modeling with time-series causal analysis to uncover topics that are both coherent semantically and correlated with time-series data. ITMTF consists of the following iterative steps: (1) Find topics with topic modeling techniques, (2) identify causal topics using correlation tests, and (3) make prior topic distribution using topic-level and word-level causality analysis. In (2) and (3) processes, there should be a threshold of confidence. In this paper, we applied LDA(ITMTF-LDA) and DTM(ITMTF-DTM) as topic models and a linear regression as a causal analysis tool with a 70% confidence threshold.ATM has two variations by the setting of δ. The first variation is learning δ just like the other parameters from the data. This is an ordinary version of ATM, and this model limits the fitting to the past numeric data by allowing the deviation modeled by the variance of δ. This model balances the influence of the numeric and the text data in the parameter inference process. We utilize this ordinary version of ATM in (1) Section 4.4 for qualitative topic evaluation, (2) Section 4.6 for numeric value prediction evaluation, and (3) Section 4.7 for quantitative topic evaluation.The second variation, or fixed-ATM, is fixing δ as the small value, so the numeric fitting can be maximized, or overfitted from a certain perspective. This would not be a better way to predict the numeric time-series in the future, but this does maximize the influence from the numeric data to the text data in the topic extraction process. Therefore, the model would result in topics that are the best in describing the past numeric value movements, and Section 4.5 shows such retrospective analysis of texts influenced by the numeric values.To investigate the influence of time-series for identifying associative topics, we applied ATM to the same Bloomberg articles over 121weeks as the text input with different time-series, the stock returns and the volatilities of DJIA. We initializedβ1:Kto randomly perturbed topics from uniform topics,δ2to the sample variance of the time-series data,σ2to 0.1,a2to 0.1, and b to zero vector over the number of topics 10. The top and middle of Fig. 6show the learning results with stock returns and volatilities. The results show that some associative topics from different time series are similar topics, e.g., European crisis, tax cuts, and economic reports, because of the property of the financial domain. However, some associative topics are different: Topics about energy and the economy of Asia are revealed from ATM with stock returns, and the topics about federal lows are revealed from ATM with stock volatilities. When we show two numerical time-series cases, the volatility is better predicted than the return is. This result is expected because of the efficient market hypothesis, which states the difficulty in predicting stock prices and returns (Poon & Granger, 2005). In the financial domain, the volatility often becomes the target of prediction with a general sentiment.The numeric values over-time drive ATM to extract different topics per the types of the numeric values, i.e. return and volatility. Subsequently, the different topics induced the different dynamics of topic proportions though two dynamics were captured from the same text data. This result qualitatively demonstrates that ATM identifies different associative topics from different time-series values, although the text data and the initial settings are the same. To confirm this influence from the numeric values in extracting topics, we calculated the similarities between (1) topics without numeric information and temporal information (LDA), (2) topics without numeric information (DTM), (3) topics with over-time numeric values, volatilities (ATM(Volatilities)), and (4) topics with over-time numeric values, returns (ATM(Returns)).Specifically, we investigated the cosine similarities of the topics from different topic extraction models utilizing different information. We examined ten topics from LDA and DTM with the same period text data to compare two sets of topics from ATM with two different numeric values. Fig. 7shows the block diagrams of the results. The color strength of each block indicates the degree of the topic similarities (a black cell means that the two topics are same, and a white cell means that two are quite different). In the case of LDA and DTM, each topic from LDA has a strongly similar topic in DTM. This indicates that DTM extracts similar topics from LDA although DTM takes the time information of text data into account. On the contrary, in the cases of DTM and ATM with returns as well as DTM and ATM with volatilities, ATM was able to extract different topics by considering numerical time-series values, i.e. the topic 4, 7, and 10 from ATM(Return) are matched to none of the topics from DTM. Such difference were observed between the pair of DTM and ATM(Return); DTM and ATM(Volatilities); and ATM(Return) and ATM(Volatilities). This numeric value guided topic extraction is a major objective in developing ATM, and this correlation analysis demonstrates such capability of ATM.In order to explore The Washington Post corpus and its themes associated with president approval indexes, we inferred the parameters of ATM with 284weeks of data. For learning ATM, we initialized parameters in the same way of the stock index case. ATM identifies two sets of associative topics corresponding to the president approval indexes. The bottom of Fig. 6 shows the results from ATM. The left side lists the estimated words with the top eight probabilities of ten topics and the associative factors that are standardized coefficients b. The results show that an associative topic about family life is positively related with the approval indexes. However, some associative topics are negatively related, e.g. Romney party, war, and policy. In addition, some topics are not highly related, e.g. education, tax, and agency. These results qualitatively demonstrate that ATM identifies plausible associative topics.Our proposed topic model, ATM, shows how text data and numerical time-series data are related, and ATM identifies associative topics that have explanatory power regarding the relation between two different types of data from two different sources.ATM can be used for retrospective analysis, not for prediction. In the modeling process of ATM, we integrated numerical time-series data and dynamic topic models. At modeling time-series, we assumed numerical hidden states influence generating time-series variables with Gaussian errors (δ2). If we set the error (δ2) as the small fixed number, such as10-10, then the ATM catches associative topics only explaining the exact trajectories of time-series in the training period. As we mentioned, we label ATM with this treatment as fixed-ATM.To explore the results of fixed-ATM, we initializedβ1:Kto randomly perturbed topics from uniform topics,σ2to 0.1,a2to 0.1, and b to zero vector over the number of topics. We set the number of the topics as 10 for the return and approval index cases and 5 for the volatility case. The different number of topic setting is decided by empirical experiment; the proportions of some topics among 10 with fixed-ATM become close to 0.Fig. 8shows total results from fixed-ATM. When investigating dynamics of topic proportions, all dynamics are dramatically changed to contain errors of time-series. Especially, a few topics are highly correlated in the volatility case. This means most of text data are not effectively related with volatilities by ignoring the error of time-series. In figures of estimations, as we said, the fixed low value ofδ2made the estimation exact in the training period. This treatment is very similar with ITMTF (Kim et al., 2013) because the trajectories of time-series influence directly topics ignoring errors in time-series. In order to analyze highly associative topics over the training periods, fixed-ATM can be useful. As can be seen, there are some different topic effects from ATM results. For example, in the return case, the last topic appears to have the most negative effects from a fixed-ATM, while it was affected in a small way by ATM. This is caused by including the errors of time-series themselves into topic proportions. These results may be used for a retrospective understanding.To quantitatively evaluate the prediction performance of ATM, we considered the task of predicting the next time value by assuming the model was trained by prior data. We repeated this prediction from the 101th to 121th week (21 different period) for the stock indexes cases and from the 270th to 284th week (25 different period) for the approval index case, using the trained model with historically cumulated data. For instance, to predict y value at 270th week, the model was trained with 1–269th weeks data. The baseline models and ATM infer five topics, and the models had the same initial parameters and the same criteria to stop the approximations. For AR(p), we select p of the best performers on the test data. For ITMTF models, we iterated three times of prior feedback loops with a 70% confidence threshold. After the estimation, we measured the model performance of experimental models with mean squared errors (MSE),1M∑m(ym-ŷm)2, and mean absolute errors (MAE),1M∑m|ym-ŷm|where M is the number of test points;ymis the true value of the prediction; andŷmis the predicted value. Although the same training data are not used for prediction, MSE and MAE can be used to evaluate the model quality.Table 2displays MSE and MAE from the baseline and the proposed models. The bold font indicates the best model generating both interpretation and prediction in other words, analyzing text and metrics simultaneously while the underlined font indicates the best performer including only numerical model, such as AR. When we focus only on the numerical value prediction, ATM is the best performance model in general. Exceptions are the MSE and the MAE of AR in the volatility prediction; and the MAE of AR in predicting the approval rate. This shows that simple regression model might beat sophisticated metric-text models, but given the result, ATM is the best solution that provides both prediction and interpretation. Additionally, some cases, i.e., predicting stock price, suggests that ATM would be better for the numerical value prediction. Other sophisticated methods, i.e., the generalized linear model, that can replace AR exist, but these models do not provide the interpretation of the context, either.After measuring the accuracy of the numeric value predictions, we compare the quality of the extracted topics from the texts with the predictive log likelihoods (Blei & Lafferty, 2006; Chang, Gerrish, Wang, Boyd-graber, & Blei, 2009; Wallach, Murray, Salakhutdinov, & Mimno, 2009; Wang, Blei, & Heckerman, 2008). The predictive log-likelihood is frequently utilized and widely accepted in the topic modeling community, and this metric represents how clearly the extracted topics are able to explain the future document distribution from the topic perspective. Since this evaluation only handles the topic modeling, we excluded the evaluation on the numeric only model, i.e. AR.The predictive log likelihoods in the topic models indicate the probabilities of generating unseen text data with past learned parameters. In the proposed models, the predictive likelihood for next-time text data,wT+1, is specified in the follow:(26)p(wT+1|w1:T,y1:T)=∬p(wT+1|αT+1)p(αT+1|αT)p(αT|w1:T,y1:T)dαT+1dαTDue to the same reason of the intractability of the inference, we calculated the lower bound of the likelihood in the same way described in Formula (11). Because the baseline topic models, which are LDA, DTM, ITMTF-LDA, and ITMTF-DTM, are not unified probabilistic models containing numerical variabley1:T, their predictive log-likelihoods do not contain the historical condition of numerical time series. The formula for the baseline topic models is described in the below:(27)p(wT+1|w1:T)=∫p(wT+1|Θ1:T)p(Θ1:T|w1:T)dΘ1:TwhereΘ1:Tindicates latent variables, which are included in the baseline topic models. Due to the intractabilities, the lower bounds ofp(wT+1|w1:T)are used. These lower bounds are not exactly comparable values because each probability is derived from different hyper-parameters as well as different condition, such as the existence ofy1:T. However, these comparisons are still be worth to measure model performances (Asuncion, Welling, Smyth, & Teh, 2009) due to the difficulty on calculating the exact likelihood.The upper graphs in Fig. 9display the log likelihoods on test datasets over time from the ATM and the baseline models. The dramatic differences over times are caused by the difference of the period. All models show similar performance from the predictive log likelihood aspect. This is encouraging for ATM because ATM has the similar likelihood performances in spite of the strong assumption that two different sources are generated from the same hidden states. In order to show details, the lower graphs in Fig. 9 show the log likelihood of the last period on the upper graphs in Fig. 9.

@&#CONCLUSIONS@&#
