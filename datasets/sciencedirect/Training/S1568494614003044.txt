@&#MAIN-TITLE@&#
Signal processing and time series description: A Perspective of Computational Intelligence and Granular Computing

@&#HIGHLIGHTS@&#
We discuss a general framework of Computational Intelligence for signal processing and time series analysis.Synergistic functionalities of Computational Intelligence are elaborated on.An overall architecture of processing of time series is discussed.Interpretability issues and transparency of models are addressed.

@&#KEYPHRASES@&#
Computational Intelligence,Biomedical signals,Neurocomputing,Fuzzy sets,Information granules,Granular Computing,

@&#ABSTRACT@&#
This study provides a general introduction to the principles, algorithms and practice of Computational Intelligence (CI) and elaborates on their use to signal processing and time series. In this setting, we discuss the main technologies of Computational Intelligence (namely, neural networks, fuzzy sets or Granular Computing, and evolutionary optimization), identify their focal points and stress an overall synergistic character, which ultimately gives rise to the highly synergistic CI environment. Furthermore, the main advantages and limitations of the CI technologies are discussed. In the sequel, we present CI-oriented constructs in signal modeling, classification, and interpretation.

@&#INTRODUCTION@&#
There have been a significant number of various information technologies applied to biomedical signal analysis, interpretation, and classification. Along with the steady progress of hardware, new more advanced algorithmic developments have been reported upon. There are strong motivating factors and several compelling reasons behind this progress, which mainly results from the exposure to the challenges inherently associated with the domain of signal processing, analysis, and interpretation:•Biomedical signals are one of the most important sources of diagnostic information. Their proper acquisition and processing provide an indispensible vehicle to support medical diagnosis. Acquired signals are affected by noise and their further processing calls for the use of advanced filtering techniques.A description and classification of signals call for nonlinear mechanisms producing a suitable set of features (descriptors) of the signal so that the ensuing classifiers come with significant discriminatory capabilities. We witness a great deal of various ways used in signal description being followed by a numerous classifiers.It is anticipated that any advanced computerized interpretation of signals has to be user-friendly, meaning that the results of classification/interpretation could be easily comprehended by a human user and any queries along the “what-if” investigations could be fully attended to. Likewise it becomes highly beneficial to endow the results of classification or prediction with confidence levels, expressed either numerically or linguistically. This requirement calls for an effective way of dealing with knowledge acquisition and knowledge processing when working with numeric signals.Quite often these problems are intertwined and need to be dealt with in a holistic manner. We notice that some of them (preprocessing, filtering) require advanced nonlinear processing techniques while the others (interpretation) call for knowledge-oriented techniques. Altogether, a comprehensive methodological and algorithmic environment, which is offered by Computational Intelligence, comes as a viable alternative.In this study, we discuss the main conceptual, methodological and algorithmic pillars of Computational Intelligence (CI), identify their main features and elaborate on their role in biomedical signal processing. To a significant extent, the content of this study is self-contained and the most essential ideas are elaborated on from scratch. The reader can benefit from some introductory knowledge of the subject matter on neural networks, fuzzy sets and evolutionary computing; see, for instance [30,31]. The presentation is provided in a top-down approach. We start with a concise introduction to Computational Intelligence (CI) viewed as a highly synergistic environment bringing a number of highly visible technologies of Granular Computing, neural networks, and evolutionary optimization (Section 2). In a sequence of sections, Sections 3–5, we discuss neurocomputing, evolutionary optimization, and Granular Computing. Furthermore we show that the three main technologies of CI are naturally inclined to form useful synergistic linkages. Formal platforms of information granularity are discussed in Section 6. Practical insights identifying advantages and possible limitations of these technologies are carefully addressed. We elaborate on information granularity and its role in signal representation in Section 7. The concept of information granulation–degranulation is discussed in Section 8. The design of information granules regarded as semantically sound abstractions is covered in Sections 9 and 10. Here we discuss ways where not only numeric data – experimental evidence is taken into account but various tidbits of domain knowledge are also used in the formation of information granules. Ways of building information granules of higher type and higher order are discussed in Section 11. In Section 12, we relate key aspects of information granularity with time series and offer a hierarchical architecture supporting a realization of description and prediction tasks.Fig. 1shows a general scheme of processing (classification, description, prediction, etc.) with several with well-identified key modules; in the subsequent discussion we will be associating their realization in the setting of the fundamental technologies of Computational Intelligence. As far as the notation used in this study is concerned we follow the symbols being in common usage. Patterns (data) x1, x2, …, xNare treated as vectors located in n-dimensional space Rn, ||.|| is used to denote a distance (Euclidean, Mahalanobis, Hamming, Tchebyshev, etc.). Fuzzy sets will be described by capital letters; the same notation is being used for their membership functions.Computational Intelligence can be defined in many different ways. Let us start by recalling two definitions or descriptions, which are commonly encountered in the literatureA system is computationally intelligent when it: deals with only numerical (low-level) data, has pattern recognition components, does not use knowledge in the AI sense; and additionally when it (begins to) exhibit (1) computational adaptivity; (2) computational fault tolerance, (3) speed approaching human-like turnaround, and (4) error rates that approximate human performance[6,7]The description provided by W. Karplus comes as followsCI substitutes intensive computation for insight how the system works. Neural networks, fuzzy systems and evolutionary Computation were all shunned by classical system and control theorists. CI umbrellas and unifies these and other revolutionary methodsThe first description captures the essence of the area. Perhaps today such a definition becomes slightly extended by allowing for some new trends and technologies, which are visible in the design of intelligent systems. Nevertheless the essence of CI is well-captured.The comprehensive monograph on CI [31] emphasizes the importance of synergy of the contributing and very much complementary technologies of fuzzy sets, neurocomputing and evolutionary optimization. In a nutshell, CI is about effective and omnipresent mechanisms of synergy exploited in a variety of tasks of analysis and design of intelligent systems. The reader may refer to [9,19], including fuzzy automata [23], which serve as comprehensive sources of updated material on CI. Some interesting more detailed studies visualizing a role of CI and showing how the optimization mechanisms (including Particle Swarm Optimization) are effectively utilized in the design of advanced models are reported [34–36,38].In signal processing, especially, with regard to ECG signal processing and interpretation, the reader is referred to [1,10,11,17].The emergence of CI is justifiable, and in some sense, unavoidable. Over time, being faced with more advanced problems, increased dimensionality and complexity of systems one has to deal with, neural networks, fuzzy sets and evolutionary computing started to exhibit some clear limitations. This is not startling at all as their research agendas are very much distinct and they focus on different aspects of the design of intelligent systems. The synergistic environment, in which the key pursuits of knowledge representation, learning and global optimization go hand in hand, becomes highly desirable.Let us elaborate in more detail on knowledge representation as being captured by fuzzy sets. Fuzzy sets offer a unique and general opportunity to look at information granules as semantically meaningful entities endowed with detailed numeric description. For instance, consider an information granule termed high amplitude of signal. On the one hand, high is just a single symbol and as such could be processed at the level of symbolic processing encountered in Artificial Intelligence (AI). For instance, it could be one of the symbols used in syntactic pattern classifier captured by a collection of syntactic production rules or automata. On the other hand, the same granular entity high is associated with the detailed numeric description, which calibrates the concept in presence of available numeric evidence. A suitable level of abstraction helps us establish the most promising tradeoff between detailed and abstract view at the problem/data. Of course, the choice of the tradeoff is problem driven and depends upon the task and the main objectives specified therein. Likewise, the same information granule high can be looked at in less detail and through the use of some partially specified numeric content (that is in the form of higher type information granules, say fuzzy sets of type-2) could be processed in a semi-numeric fashion. In this way, the granularity of information and a formal mechanism used in granulation itself offers a way to position anywhere in-between symbolic view and numeric perception or quantification of the reality.One may emphasize an important and enlightening linkage between Computational Intelligence and Artificial Intelligence (AI). To a significant extent, AI is a synonym of symbol-driven processing faculty. CI effectively exploits numeric data however owing to the technology of Granular Computing, it may invoke computing based on information described at various levels of granularity by inherently associating such granules with their underlying semantics described in a numeric or semi-numeric fashion (such as e.g., membership functions, characteristic functions or interval-valued mappings). The granularity of results supports the user-friendly nature of CI models. They can also form an important construct to be further used in facilitating interaction with the user as well as forming linkages with symbolic processing of AI constructs. The three fundamental components of CI along with an emphasis on their synergy of the main function give rise to a plethora of architectures in which various technologies assume a dominant role. This comes with various names such as neurofuzzy systems, evolutionary neural networks, genetic neural classifiers, etc. Let us recall that knowledge representation associated with the component of information granularity (along with its abstraction facet), learning (adaptive) abilities and global structural optimization (supported by evolutionary methods).The results of the queries on Computational Intelligence and biomedical signal (completed with the use of Google Scholar – accessed on January 15, 2014) clearly indicate a substantial interest in the area:Signal processing & fuzzy sets 374,000Signal processing & neural networks 1,280,000Signal processing & particle swarm optimization 34,900Signal processing & genetic algorithms 384,000Signal processing & Computational Intelligence 666,000Signal processing & Granular Computing 34,100There exists an immensely vast body of literature on neural networks. Neural networks are viewed as highly versatile distributed architectures realizing a concept of universal approximation [13,37], which offers a very much attractive feature of approximating nonlinear (continuous) mappings to any desired level of accuracy and in this way supporting various classification tasks.The two main taxonomies encountered in neurocomputing can be established centered around: (a) topologies of networks and (b) a variety of ways of their development (training) schemes. With regard to the first coordinate of the taxonomy, one looks at a way in which individual neurons are arranged together into successive layers and a way in which processing is realized by the network, namely if this is of feedforward nature or there are some feedback linkages within the structure. Typically, within the spectrum of learning scenarios one distinguishes between supervised learning and unsupervised learning however there are a number of interesting learning schemes, which fall in-between these two extreme positions (say, learning with partial supervision, proximity-based learning, etc.).One needs to be aware of some limitations of neural networks that start manifesting in practical scenarios (those drawbacks might be alleviated to some extent but it is unlikely they will vanish completely). From the perspective of practice of neural networks in the context of biomedical signal processing, we can list a number of advantages. The main of those include: universal approximation capabilities (neural networks are universal approximators), significant learning abilities with a large repository of algorithms, well-developed and validated training methods. Neural networks support distributed processing, which as a result exhibit high potential for endowing them with significant fault tolerance capabilities. Just recently, there is a visible interest in the efficient realizations of networks, especially when considering their usage in portable medical devices. Along with these evident advantages, one has to be aware of several limitations of which neural networks are not free from. Neural networks exhibit black-box architectures (in other words, there is some effort to interpret constructed networks). Gradient-based optimization exhibits all limitations associated with this type of learning.We witness non-repetitive results of learning of the networks (depending upon initial learning condition, parameters of the learning algorithm, etc.) while the learning realized in the presence of high-dimensional and large data sets could be slow and inefficient.From the perspective of signal processing applications, we need to be aware that neural networks could offer a highly competitive solution however one has to proceed very prudently with the learning process. Most importantly, the learning results might not be repetitive: running the same method while starting from a slightly different initial configuration (say, a different random initialization of the connections of the neurons) may result in quite substantial differences in the performance of the constructed network. Likewise setting different numeric values of the learning environment (say, a learning rate) could lead to a different solution. A formation of the input space, which becomes of a genuine challenge, when dealing with highly dimensional data and a large number of data themselves, requires attention. Ignoring this problem may result in a highly inefficient learning producing quite poor, non-competitive results lacking generalization abilities.We should highlight that by no means neural networks can be sought as a plug-and-play technology. To the contrary: its successful usage does require careful planning, data organization and data preprocessing, a prudent validation and a careful accommodation of any prior domain knowledge being available. The black box nature of neural networks can bring some hesitation and reluctance to use the neural network solution and one has to be prepared for further critical evaluation of the obtained results.The attractiveness of this paradigm of computing stems from the fact that all pursuits are realized by a population of individual – potential solutions so that this offers a very much appealing opportunity of exploring or exploiting a search space in a holistic manner [12]. The search is realized by a population – a collection of individuals, which at each iteration (generation) carry out search on their own and then are subject to some processes of interaction/communication.In case of genetic algorithms, evolutionary methods, and population-based methods (say, genetic algorithms, evolutionary strategies, particle swarm optimization), in general, a population undergoes evolution; the best individuals are retained, they form a new population through recombination. They are subject to mutation. Each operator present in the search process realizes some mechanism of exploration or exploitation of the search space. A general processing scheme can be outlined as follows{evaluate population (individuals);select mating individuals (selection process);recombination;mutation;}The above basic sequence scheme is repeated (iterated).In contrast to evolutionary methods, in the swarm-based methods [8], we encounter an interesting way of sharing experience. Each particle relies on its own experience accumulated so far but it is also affected by the cognitive component where one looks at the performance of other members of the population as well as an overall behavior of the population.The essential phase of any evolutionary and population-based method (directly affecting its performance) is a representation problem. It is concerned about a way how to represent the problem in the language of the search strategy so that (a) the resulting search space is made compact enough (to make the search less time consuming) and (b) is well reflective of the properties of the fitness function to be optimized. By forming a suitable search space we pay attention to avoid forming extended regions of the search space where the fitness function does not change its values.The forte of the methods falling under the rubric of these population-based optimization techniques is the genuine flexibility of the fitness function – there is a great deal of possibilities on how it can be formulated to capture the essence of the optimization problem. This translates into an ability to arrive at a suitable solution to the real-world task.The inevitable challenges come with the need to assess how good the obtained solution really is and a formation of the feature space itself.Information granules permeate numerous human endeavors [2,4,24,40,41]. No matter what problem is taken into consideration, we usually express it in a certain conceptual framework of basic entities, which we regard to be of relevance to the problem formulation and problem solving. This becomes a framework in which we formulate generic concepts adhering to some level of abstraction, carry out processing, and communicate the results to the external environment. Consider, for instance, image processing. In spite of the continuous progress in the area, a human being assumes a dominant and very much uncontested position when it comes to understanding and interpreting images. Surely, we do not focus our attention on individual pixels and process them as such but group them together into semantically meaningful constructs – familiar objects we deal with in everyday life. Such objects involve regions that consist of pixels or categories of pixels drawn together because of their proximity in the image, similar texture, color, etc. This remarkable and unchallenged ability of humans dwells on our effortless ability to construct information granules, manipulate them and arrive at sound conclusions. As another example, consider a collection of time series. From our perspective we can describe them in a semi-qualitative manner by pointing at specific regions of such signals. Specialists can effortlessly interpret various diagnostic signals including ECG recordings. They distinguish some segments of such signals and interpret their combinations. Experts can interpret temporal readings of sensors and assess the status of the monitored system. Again, in all these situations, the individual samples of the signals are not the focal point of the analysis and the ensuing signal interpretation. We always granulate all phenomena (no matter if they are originally discrete or analog in their nature). Time is another important variable that is subjected to granulation. We use seconds, minutes, days, months, and years. Depending upon a specific problem we have in mind and who the user is, the size of information granules (time intervals) could vary quite dramatically. To the high-level management time intervals of quarters of year or a few years could be meaningful temporal information granules on basis of which one develops any predictive model. For those in charge of everyday operation of a dispatching plant, minutes and hours could form a viable scale of time granulation. For the designer of high-speed integrated circuits and digital systems, the temporal information granules concern nanoseconds, microseconds, and perhaps microseconds. Even such commonly encountered and simple examples are convincing enough to lead us to ascertain that (a) information granules are the key components of knowledge representation and processing, (b) the level of granularity of information granules (their size, to be more descriptive) becomes crucial to the problem description and an overall strategy of problem solving, and (c) there is no universal level of granularity of information; the size of granules is problem-oriented and user dependent.What has been said so far touched a qualitative aspect of the problem. The challenge is to develop a computing framework within which all these representation and processing endeavors could be formally realized. The common platform emerging within this context comes under the name of Granular Computing. In essence, it is an emerging paradigm of information processing. While we have already noticed a number of important conceptual and computational constructs built in the domain of system modeling, machine learning, image processing, pattern recognition, and data compression in which various abstractions (and ensuing information granules) came into existence, Granular Computing becomes innovative and intellectually proactive in several fundamental ways•It identifies the essential commonalities between the surprisingly diversified problems and technologies used there, which could be cast into a unified framework known as a granular world. This is a fully operational processing entity that interacts with the external world (that could be another granular or numeric world) by collecting necessary granular information and returning the outcomes of the granular computing.With the emergence of the unified framework of granular processing, we get a better grasp as to the role of interaction between various formalisms and visualize a way in which they communicate.It brings together the existing formalisms of set theory (interval analysis) [18], fuzzy sets [39,41], rough sets [20–22] under the same roof by clearly visualizing that in spite of their visibly distinct underpinnings (and ensuing processing), they exhibit some fundamental commonalities. In this sense, Granular Computing establishes a stimulating environment of synergy between the individual approaches.By building upon the commonalities of the existing formal approaches, Granular Computing helps build heterogeneous and multifaceted models of processing of information granules by clearly recognizing the orthogonal nature of some of the existing and well established frameworks (say, probability theory coming with its probability density functions and fuzzy sets with their membership functions).Granular Computing fully acknowledges a notion of variable granularity whose range could cover detailed numeric entities and very abstract and general information granules. It looks at the aspects of compatibility of such information granules and ensuing communication mechanisms of the granular worlds.Interestingly, the inception of information granules is highly motivated. We do not form information granules without reason. Information granules arise as an evident realization of the fundamental paradigm of abstraction.Granular Computing forms a unified conceptual and computing platform. Yet, it directly benefits from the already existing and well-established concepts of information granules formed in the setting of set theory, fuzzy sets, rough sets and others. While Granular Computing offers a unique ability to conveniently translate the problem into the language of information granules, it also exhibits some limitations associated with the lack of effective learning schemes, and quite commonly prescriptive nature of granular constructs (so there might be some danger of not carefully considering experimental evidence).While all the three classes of technologies discussed so far offer tangible benefits and help address various central problems of intelligent systems, it becomes apparent that they are very much complementary. The strength of one technology is a quite visible limitation of some other. It is not surprising that there have been various ways of forming hybrid approaches dwelling upon the complementarity of neurocomputing, fuzzy sets (Granular Computing), and evolutionary methods, out of which a concept of Computational Intelligence (CI) has emerged.There exists a plethora of formal platforms in which information granules are defined and processed.Sets (intervals) realize a concept of abstraction by introducing a notion of dichotomy: we admit element to belong to a given information granule or to be excluded from it. Sets are described by characteristic functions taking on values in {0,1}. A family of sets defined in a universe of discourse X is denoted byP(X).Fuzzy sets ([39,41]) offer an important generalization of sets. By admitting partial membership to a given information granule we bring an important feature which makes the concept to be in rapport with reality. The description of fuzzy sets is realized in terms of membership functions taking on values in the unit interval. A family of fuzzy sets defined in X is denoted byF(X).Probability-based information granules are expressed in the form of some probability density functions or probability functions. They capture a collection of elements resulting from some experiment. In virtue of the concept of probability, the granularity of information becomes a manifestation of occurrence of some elements.Rough sets[20–22] emphasize a roughness of description of a given concept X when being realized in terms of the indiscernibility relation provided in advance. The roughness of the description of X is manifested in terms of its lower and upper approximations of a certain rough set. A family of fuzzy sets defined in X is denoted byR(X).Shadowed sets[26] offer a description of information granules by distinguishing among elements, which fully belong to the concept, are excluded from it and whose belongingness is completely unknown. Formally, these information granules are described as a mapping X: X→{1, 0, [0,1]} where the elements with the membership quantified as the entire [0,1] interval are used to describe a shadow of the construct. Given the nature of the mapping here, shadowed sets can be sought as a granular description of fuzzy sets where the shadow is used to localize partial membership values, which in fuzzy sets are distributed over the entire universe of discourse. A family of fuzzy sets defined in X is denoted byS(X).Probability-grounded sets are defined over a certain universe where the membership grades are represented as some probabilistic constructs. For instance, each element of a set comes with a truncated to [0,1] probability density function, which quantifies a degree of membership to the information granule. There are a number of variations of these constructs with probabilistic sets [14] being one of them.Other formal models of information granules involve axiomatic sets, soft sets, and intuitionistic sets.In general, we distinguish between information granules of higher type and higher order.Higher type information granules. The quantification of levels of belongingness to a given information granule is granular itself rather than numeric as encountered in sets or fuzzy sets. This type of quantification is of interest in situations it is not quite justifiable or technically sounds to quantify the membership in terms of a single numeric value. These situations give rise to ideas of type-2 fuzzy sets or interval-valued fuzzy sets. In the first case the membership is quantified by a certain fuzzy set taking on values in the unit interval. In the second case we have a subinterval of [0,1] representing membership values. One can discuss fuzzy sets of higher type in which the granular quantification is moved to the higher levels of the construct. For instance, one can talk about type-3, type-4, … fuzzy sets. Albeit conceptually sound, one should be aware that the computing overhead associated with further processing of such information granules becomes more significant. In light of the essence of these constructs, we can view probabilistic granules to be treated as higher type information granules as we admit membership values to be granulated in a probabilistic manner.Higher order information granules. The notion of higher order of information granules points at a space in which an information granule is defined. Here the universe of discourse is composed of a family of information granules. For instance, a fuzzy set of order 2 is constructed in the space of a family of so-called reference fuzzy sets. This stands in a sharp contrast with fuzzy sets of order 1, which are defined in individual elements of the universe of discourse. One could remark that fuzzy modeling quite often involve order 2 fuzzy sets.The illustration of these concepts is included in Fig. 2.These types of construct could be generalized by invoking a number of consecutive levels of the structure. In all situations, we could assess whether moving to the higher level or order constructs is legitimate from the perspective of the problem at hand.Information granules can embrace several granulation formalisms at the same time forming some hybrid models. This constructs become of particular interest when information granules have to capture a multifaceted nature of the problem. There are a large number of interesting options here. Some of them, which have been found convincing concern(a)Fuzzy probabilities. Probability and fuzzy sets are orthogonal concepts and as such they could be considered together as a single entity. The concepts of a fuzzy event and fuzzy probabilities (viz. probabilities whose values are quantified in terms of fuzzy sets, say high probability, very low probability) are of interest here.Fuzzy rough and rough fuzzy information granules. Here the indiscernibility relation can be formed on a basis of fuzzy sets. Fuzzy sets, rather than sets are also the entities that are described in terms of the elements of the indiscernibility relation. The original object X for which a rough set is formed might be a fuzzy set itself rather than a set used in the original definition of rough sets.In signal characterization, before proceeding with its detailed processing, a careful, prudently thought out representation is an essential prerequisite directly impacting the effectiveness of all ensuing algorithms, in particular influencing classification quality of pattern classifiers built on the basis of such information granules.Granularity of information plays a primordial role in all these characterization pursuits. Numeric data are represented in the form of information granules and the manifestation of such granules (e.g., as the values of membership functions) is used afterwards in a more detailed system design.Let us highlight the main role information granules play in the constructs of CI such as e.g., neural networks. This role is visible in the formation of data interfaces. The essence of the underlying construct and its role vis-à-vis processing realized by the neural network is profoundly visible in the realization of a new feature space based on information granules. Consider a general scheme portrayed in Fig. 3.Formally speaking, the original data space, typically, n-dimensional space of real number vectors, Rn, is transformed via a finite collection of information granules, say, A1, A2, …, Ac. We say that the input space has been granulated. Each input x is perceived by the following classifier/analyzer through the “eyes” of the information granules, meaning that the following relationship holds, see also Fig. 3,(1)G:Rn→[0,1]cwhere the result of the mapping is a c-dimensional vector positioned in the [0,1] hypercube.There are at least three important and practically advantageous aspects of the mapping realized by information granules:Nonlinear mapping of the data space with an intent of forming information granules in such a way that the transformed data G(x1), G(x2), …G(xN) are more suitable to construct an effective classifier. We rely on the nonlinearity effect that can be carefully exploited.The tangible advantage results from the nonlinear character of membership functions. A properly adjusted nonlinearity could move apart patterns belonging to different classes and bring closer those regions in which the patterns belong to the same category. For instance, patterns belonging to two classes and distributed uniformly in a one-dimensional space, become well separated when transformed through a sigmoid membership function A, A(x)=1/(1+exp(−(x−a))) and described in terms of the corresponding membership grades where “a” is a translation parameter of the membership function. In essence, fuzzy sets play a role of a nonlinear transformation of the feature space.Reduction of the dimensionality of the feature space. While the original feature space could be quite high (which is common in many classification problems), the dimensionality of the space of information granules is far lower, c≪n. This supports the developments of the classifiers, especially neural networks and reduces a risk of memorization resulting in poor generalization capabilities. We often witness this role of information granules in the construction of neuro-fuzzy systems.Information granules as essential constructs supporting the development of interpretable models. For instance, in rule-based systems (classifiers, analyzers), the condition parts (as well as conclusions) comprise information granules – interpretable entities, which make rules meaningful.When it comes to numeric information x forming a vector in a certain multidimensional space, we can develop an interesting granulation–degranulation scheme [30]. We assume that the information granules forming a collection (codebook) A are described by their prototypes v1, v2, …, vc. Such prototypes can be formed as a result of fuzzy clustering [5,29,15]. The granulation–degranulation task is formulated as a certain optimization problem. In what follows, we assume that the distance used in the solutions is the Euclidean one. The granulation of x returns its representation in terms of the collection of available information granules expressed in terms of their prototypes. More specifically, x is expressed in the form of the membership grades uiof x to the individual granules Ai, which form a solution to the following optimization problem(2)Min∑i=1cuim(x)||x−vi||2subject to the following constraints imposed on the degrees of membership(3)∑i=1cui(x)=1ui(x)∈[0,1]where “m” stands for the so-called fuzzification coefficient, m>1 [5]. The derived solution to the problem above reads as follows(4)ui(x)=1∑j=1c(||x−vi||/||x−vj||)2/(m−1)For the degranulation phase, given ui(x) and the prototypes vi, the vectorxˆis considered as a solution to the minimization problem in which we reconstruct (degranulate) original x when using the prototypes and the membership grades(5)∑i=1cuim(x)||xˆ−vi||2Because of the use of the Euclidean distance in the above performance index, the calculations here are straightforward yielding the result(6)xˆ=∑i=1cuim(x)vi∑i=1cuim(x)It is important to note that the description of x in more abstract fashion realized by means of Aiand being followed by the consecutive degranulation brings about a certain granulation error (which is inevitable given a fact that we move back and forth between different levels of abstraction). While the above formulas pertain to the granulation realized by fuzzy sets, the granulation–degranulation error is also present when dealing with sets (intervals). In this case we are faced with a quantization error, which becomes inevitable when working with A/D (granulation) and D/A (degranulation) conversion mechanisms.Any granular processing starts with a formation of information granules. There are numerous approaches to construct them. Clustering (including fuzzy clustering, rough clustering) are representative algorithmic means in this setting. The principle of justifiable granularity is another essential vehicle to build an information granule on a basis of available experimental evidence.Clustering is a commonly encountered way of forming information granules. The essence of the process is underlined by (7). In objective function-based clustering there is usually a certain constraint imposed on the relationship between the resulting information granules. For instance, one requires that the union of information granules “covers” the entire data set, that is⋃i=1cAc=D.Obviously the union operation has to be specified in accordance to the formalism of information granules used there. There are a large number of clustering methods and depending on the formalism being used we end up with the granules expressed in the language of sets,P(X), fuzzy setsF(X), rough setsR(X), shadowed setsS(X) and others. The form of the granules depends on the clustering algorithm and the formulation of the objective function (and partition matrix, in particular). The number of information granules has been another point of comprehensive study as this pertains to the problem of cluster validity.By having a quick look at the plethora of clustering methods one can conclude that they predominantly realize the concept of closeness between elements: data, which are close to each other form the same information granule. There is another aspect of functional resemblance and this facet is captured through so-called knowledge-based clustering, cf. [29]Unsupervised learning, quite commonly treated as an equivalent of clustering is aimed at the discovery of structure in data and its representation in the form of clusters – groups of data.In reality, clusters, in virtue of their nature, are inherently fuzzy. Fuzzy sets constitute a natural vehicle to quantify strength of membership of patterns to a certain group. An example shown in Fig. 4clearly demonstrates this need. The pattern positioned in-between the two well structured and compact groups exhibit some level of resemblance (membership) to each of the clusters. Surely enough, one could be hesitant to allocate it fully to either of the clusters. The membership values such as e.g., 0.55 and 0.45 are not only reflective of the structure in the data but they flag (highlight) the distinct nature of this data–and maybe trigger some further inspection of this pattern. In this way we remark a user-centric character of fuzzy sets, which make interaction with users more effective and transparent.Fuzzy sets can be formed on a basis of numeric data through their clustering (groupings). The groups of data give rise to membership functions that convey a global more abstract and general view at the available data. With this regard Fuzzy C-Means (FCM, for brief) is one of the commonly used mechanisms of fuzzy clustering (Bezdek [29]).Let us review its formulation, develop the algorithm and highlight the main properties of the fuzzy clusters. Given a collection of n-dimensional data set {xk}, k=1,2,…,N, the task of determining its structure – a collection of “c” clusters, is expressed as a minimization of the following objective function (performance index) Q being regarded as a sum of the squared distances between data and their representatives (prototypes)(7)Q=∑i=1c∑k=1Nuikm||xk−vi||2here vis are n-dimensional prototypes of the clusters, i=1, 2,…, c and U=[uik] stands for a partition matrix expressing a way of allocation of the data to the corresponding clusters; uikis the membership degree of data xkin the i-th cluster. The distance between the data zkand prototype vi is denoted by ||.||. The fuzzification coefficient m (>1.0) expresses the impact of the membership grades on the individual clusters. It implies as certain geometry of fuzzy sets. A partition matrix satisfies two important and intuitively appealing properties(8)(a)0<∑k=1Nuik<N,i=1,2,...,c(b)∑i=1cuik=1,k=1,2,...,NLet us denote by U a family of matrices satisfying (a)–(b). The first requirement states that each cluster has to be nonempty and different from the entire set. The second requirement states that the sum of the membership grades should be confined to 1.The minimization of Q completed with respect to U∈U and the prototypes viof V={v1, v2,…vc} of the clusters. More explicitly, we write it down as follows(9)minQwith respect toU∈U,v1,v2,…,vc∈RnThe successive entries of the partition matrix are expressed as follows(10)ust=1∑j=1c(||xt−vs||/||xt−vj||)2/(m−1)Assuming the Euclidean form of distance, the prototypes v1, v2, …, vccome in the form(11)vs=∑k=1Nuikmxk∑k=1NuikmIn addition to the Euclidean distance studied are different distances, say the Hamming or the Tchebyschev one [25]. Overall, the FCM clustering is completed through a sequence of iterations where we start from some random allocation of data (a certain randomly initialized partition matrix) and carry out the following updates by adjusting the values of the partition matrix and the prototypes. The iterative process is continued until a certain termination criterion has been satisfied. Typically, the termination condition is quantified by looking at the changes in the membership values of the successive partition matrices. Denote by U(t) and U(t+1) the two partition matrices produced in the two consecutive iterations of the algorithm. If the distance ||U(t+1)−U(t)|| is les than a small predefined threshold ɛ say, ɛ=10−5 or 10−6, then we terminate the algorithm. Typically, one considers the Tchebyschev distance between the partition matrices meaning that the termination criterion reads as follows(12)maxi,k|uik(t+1)−uik(t)|≤εClustering and classification are positioned at the two opposite poles of the learning paradigm. In reality, there is no “pure” unsupervised learning as usually there is some limited amount of domain knowledge. There is no fully supervised learning as some labels might not be completely reliable (as those encountered in case of learning with probabilistic teacher).There is some domain knowledge and it has to be carefully incorporated into the generic clustering procedure. Knowledge hints can be conveniently captured and formalized in terms of fuzzy sets. Altogether with the underlying clustering algorithms, they give rise to the concept of knowledge-based clustering – a unified framework in which data and knowledge are processed together in a uniform fashion.We can distinguish several interesting and practically viable ways in which domain knowledge is taken into consideration:A subset of labeled patterns. The knowledge hints are provided in the form of a small subset of labeled patternsK⊂N. [27,28]; For each of them we have a vector of membership grades fk, k∈Kwhich consists of degrees of membership the pattern is assigned to the corresponding clusters. As usual, we have fik∈[0,1] and∑i=1cfik=1.Proximity-based clustering.Here we are provided a collection of pairs of patterns [16] with specified levels of closeness (resemblance) which are quantified in terms of proximity, prox(k, l) expressed for xkand xl. The proximity offers a very general quantification scheme of resemblance: we require reflexivity and symmetry, that is prox(k, k)=1 and prox(k, l)=prox(l, k) however no transitivity is needed.“Belong” and “not-belong” Boolean relationships between patterns.These two Boolean relationships stress that two patterns should belong to the same clusters, R(xk, xl)=1 or they should be placed apart in two different clusters, R(xk, xl)=0. These two requirements could be relaxed by requiring that these two relationships return values close to one or zero.Uncertainty of labeling/allocation of patterns. We may consider that some patterns are “easy” to assign to clusters while some others are inherently difficult to deal with meaning that their cluster allocation is associated with a significant level of uncertainty. Let F(xk) stands for the uncertainty measure (e.g., entropy) for xk(as a matter of fact, F is computed for the membership degrees of xkthat is F(uk) with ukbeing the k th column of the partition matrix. The uncertainty hint is quantified by values close to 0 or 1 depending upon what uncertainty level a given pattern is coming from.Depending on the character of the knowledge hints, the original clustering algorithm needs to be properly refined. In particular the underlying objective function has to be augmented to capture the knowledge-based requirements. Below shown are several examples of the extended objective functions dealing with the knowledge hints introduced above.When dealing with some labeled patterns we consider the following augmented objective function(13)Q=∑i=1c∑k=1Nuikm||xk−vi||2+α∑i=1c∑k=1N(uik−fikbk)2||xk−vi||2where the second term quantifies distances between the class membership of the labeled patterns and the values of the partition matrix. The positive weight factor (a) helps set up a suitable balance between the knowledge about classes already available and the structure revealed by the clustering algorithm. The Boolean variable bkassumes values equal to 1 when the corresponding pattern has been labeled.The proximity constraints are accommodated as a part of the optimization problem where we minimize the distances between proximity values being provided and those generated by the partition matrix P(k1, k2)(14)Q=∑i=1c∑k=1Nuikm||xk−vi||2||prox(k1,k2)−P(k1,k2)||→Mink1,k2∈KwithKbeing a pair of patterns for which the proximity level has been provided. It can be shown that given the partition matrix the expression∑i=1cmin(uik1,uik2)generates the corresponding proximity value.For the uncertainty constraints, the minimization problem can be expressed as follows(15)Q=∑i=1c∑k=1Nuikm||xk−vi||2||F(uk)−gk||→Mink∈KwhereKstands for the set of patterns for which we are provided with the uncertainty values gk.Undoubtedly the extended objective functions call for the optimization scheme that is more demanding as far as the calculations are concerned. In several cases we cannot modify the standard technique of Lagrange multipliers, which leads to an iterative scheme of successive updates of the partition matrix and the prototypes. In general, though, the knowledge hints give rise to a more complex objective function in which the iterative scheme cannot be useful in the determination of the partition matrix and the prototypes. Alluding to the generic FCM scheme, we observe that the calculations of the prototypes in the iterative loop are doable in case of the Euclidean distance. Even the Hamming or Tchebyshev distance brings a great deal of complexity. Likewise, the knowledge hints lead to the increased complexity: the prototypes cannot be computed in a straightforward way and one has to resort himself to more advanced optimization techniques. Evolutionary computing arises here as an appealing alternative. We may consider any of the options available there including genetic algorithms, particle swarm optimization, ant colonies, to name some of them. The general scheme can be schematically structured as follows:repeat{EC (prototypes);compute partition matrix U;}One can recall also studies in the formation of granular prototypes [3].Here one of the fundamental ideas of Granular Computing, namely a principle of justifiable granularity [33] is of interest. Using it we construct an information granule. In what follows, we consider information granules coming in the form of intervals or fuzzy sets.The principle guides a formation of an interval information granule by involving two intuitively appealing criteria, namely a criterion of sufficient experimental evidence and specificity.The requirement of experimental evidence is quantified by counting the number of data falling within the bounds of a certain interval Ω. More generally, we may consider an increasing function of this cardinality, say f1(card{zk|zk∈Ω}) where f1 is an increasing function of its argument. The simplest example is a function of the form f1(u)=u. The specificity of the information granule Ω associated with its well-defined semantics (meaning) can be articulated in terms of the length of the interval. In case of Ω=[a, b], any continuous non-increasing function f2 of the length of this interval, say f2(m(Ω)) where m(Ω)=|b−a| can serve as a sound indicator of the specificity of the information granule. The shorter the interval (the higher the value of f2(m(Ω))), the better the satisfaction of the specificity requirement. It is evident that two requirements identified above are in conflict: the increase in the values of the criterion of experimental evidence (justifiable) comes at an expense of a deterioration of the specificity of the information granule (specific). As usual, we are interested in forming a sound compromise between these requirements.Having these two criteria in mind, let us proceed with the detailed formation of the interval information granule. We start with a numeric representative of the set of data D around which the information granule Ω is created. Denote the corresponding numeric representative of the data as rep(D). Once this representative has been determined, Ω (the interval [a,b]) is formed by specifying its lower and upper bounds, denoted here by “a” and “b”, respectively. The determination of these bounds is realized independently. Let us concentrate on the optimization of the upper bound (b). The optimization of the lower bound (a) are carried out in an analogous fashion. For this part of the interval, the length of Ω or its non-increasing function, as noted above. In the calculations of the cardinality of the information granule, we take into consideration the elements of D positioned to the right from the median, that is card {zk∈D|rep(D)≤zk≤b}. As the requirements of experimental evidence (justifiable granularity) and specificity (semantics) are in conflict, we resort ourselves to a maximization of the composite index in which a product of the two expressions governing the requirements. This is done independently for the lower and upper bound of the interval. In light of the evident conflicting requirements elaborated on above, we form a multiplicative form of the optimization criterion(16)V(b)=f1(card{zk∈D|rep(D)≤zk≤b})*f2(|rep(D)−b|)We obtain the optimal upper bound bopt, by maximizing the value of V(b), namely(17)V(bopt)=maxb>rep(D)V(b)Among numerous possible design alternatives regarding functions f1 and f2, we consider the following alternatives(18)f1(u)=u(19)f2(u)=exp(−αu)where α is a positive parameter delivering some flexibility when optimizing the information granule Ω. Under these assumptions, the optimization problem takes on the following form(20)V(b)=card{zk∈D|rep(D)≤zk≤b}*exp(−α|rep(D)−b|)Its essential role of the parameter α is to calibrate an impact of the specificity criterion on the constructed information granule. Note that if α=0 then the value of the exponential function is 1 hence the criterion of specificity of information granule is completely ruled out (ignored). In this case, b=zmax with zmax being the largest element in D. Higher values of α stress the increasing importance of the specificity criterion.In the extreme case, we may request that the intervals “cover” all the data meaning that there is no concern about the low specificity of the information granule however we require that it exhibits the highest experimental evidence.Interestingly the principle of justifiable granularity gives rise to quantiles and quartiles commonly encountered in probability. Ignoring the requirement of information specificity of the resulting information granularity, we assume that f1 takes on the form u/N+ or u/N− where N+ (N−, respectively) is the number of data positioned to the right (left) from the numeric representative. We determine the value of “b” (“a”, respectively) such that V(b)=p0 or V(a)=p0 where p0 is a certain predetermined probability value.To put the discussion above in a broader and systematically structured view, there are some relationships between clustering and the principle of justifiable granularity, Fig. 5.It is apparent that the principle of justifiable granularity helps refine information granules produced by clustering data. For instance, one can form granular prototypes around numeric prototypes initially formed through clustering. We look into this issue in more details in the next section.Clustering objects, which are non-numeric (viz. they come in the form of intervals, fuzzy sets, rough sets, etc.) requires a careful formulation of the problem and leads to some changes of the algorithm and produces different way of looking and interpreting the results. From a conceptual point of view, the results of clustering are again information granules of the same nature as the objects (data) we have started with. From the algorithmic perspective crucial is a formulation of the space in which the data are cast. In particular, a character of this space implies how the clustering is being realized.In some initial studies in this realm, considered were parametric and non-parametric approaches to carry out clustering of granular data. In the parametric approach we assume that fuzzy sets come from the same family of fuzzy sets (say, triangular, trapezoidal, Gaussian, etc.) so that they share the same parametric description. The clustering is carried out in the space of parameters of fuzzy sets. More formally, the following processing takes place Considering that we encounter multidimensional fuzzy sets defined in the s-dimensional space of real numbers, Rs, whose description in each coordinate is realized with the use of “r” parameters, we end up with s*r-dimensional space of parameters. For instance, in case of triangular fuzzy numbers, we have r=3. Subsequently clustering takes place in this space. In other words, the parametric representation of information granules follows the mapping G(Rs)→Rrswhile the obtained results of clustering positioned in the same space of the parameters are transformed back into information granules, Rrs→G(Rs).Subsequently the prototypes are fuzzy sets. In case the data are intervals (or their multivariable versions – hyperboxes), the result of clustering comes in the form of hyperboxes. It could be noted that the clustering realized in the space of parameters of the information granules treats the entities as numeric entries. The granular character of the results starts manifesting once we transform the findings into the corresponding information granules.A careful view at the granular representation of the time series and a way in which the vectors of the parameters were processed by the FCM algorithm, indicates that within the clustering process there was no distinction between the triples of coordinates describing successive coordinates of the information granules but instead the overall 3N-dimensional vector was treated en block. This treatment was afterwards reflected in a way how the reconstruction criterion was designed and used. To account for the granular viz. interval-valued nature of the clustered objects, we devise a suitable criterion, which concentrates on the comparison and quantification of the reconstructed information granules vis-à-vis the original interval data.One could legitimately argue, and do rightly so, that representatives (descriptors) of information granules (clusters) are numeric prototypes rather than information granules. As such they might not be fully reflective of the nature of the original data they capture. To alleviate the problem and make the prototypes more in rapport with the nature of the data, we can express the essence of the problem as follows: admit a granular character (generalization) of the prototypes to underline the nature of the granulation mechanism, stress an inherently granular character of the representatives and quantify their capabilities to describe the data.In a schematic form, we envision an underlying flow of reasoning in Fig. 6.The crux of the overall processing can be summarized as follows(21)numeric data→numeric prototypes→granular prototypeswhere the granular prototypes are formed on a basis of the originally formed numeric prototypes.Let us recap the discussion by highlighting the evident point: while numeric data are clustered, the prototypes (representatives) endowed with the representation capabilities become granular. In particular, these prototypes could be intervals (in the simples scenario), fuzzy sets, rough sets or probabilistic granules. The choice of the specific form of the granules becomes of secondary nature. Let us observe that the results of degranulation applied to the original data granulated with the aid of numeric prototypes and the membership degrees do not coincide with the original data that is G−1(G(xk, v1, v2, …, vc)) does not return xk. We form granular prototypes V1, V2, …, Vcso that the result of the degranulation-granulation G−1(G(xk, V1, V2, …, Vc)) is also granular, say Xk, and it is quite likely that xkbelongs to Xk. Proceeding with the optimization details, the prototypes are made granular in a way so that the coverage expressed as the sum of Boolean predicates taken over all data, namely∑k=1Nincl(xk,Xk)becomes maximized whereincl(xk,Xk)=1ifxk∈Xk0otherwise.In the ensuing discussion, we take an immediate advantage of this observation and develop its generalized counterpart formed in the presence of interval-valued data.By noting that numeric data legitimize the formation of granular prototypes, granular data would legitimize a creation of granular granular prototypes or granular2 prototypes, for short. The pictorial view of the construct is presented in Fig. 7.Again as before a flow of processing involves information granules and their generalizations:(22)granular data→granular prototypes→granular2prototypesThe quality of the granular2 prototypes has to be carefully evaluated and quantified by taking into account a nature of the granular data themselves, the nature of granular expansion of the original prototypes. In the development of the granular generalization one has to look for a suitable performance index, which can be engaged in the evaluation of the performance of information granules of higher type.Time series treated as a sequence of numbers (ordered in time) are a subject of numerous studies in the realm of various tasks of analysis, design, prediction, and classification. There is a profound diversity of methods. In spite of their variety, the methods share some profoundly visible commonality: the processing is realized at the numeric level. The ensuing models are numeric, prediction results are numeric, interpretation has to resort itself to numeric relationships. In all these pursuits, while precision is retained, interpretation aspects are not considered at all.The pursuits of perception, analysis and interpretation of time series, as realized by humans, are realized at a certain, usually problem-related, level of detail. Instead of single numeric entries – successive numeric readings of time series, developed are conceptual entities over time and feature space – information granules using which one discovers meaningful and interpretable relationships forming an overall description of time series. The granularity of information is an important facet being imperative to any offering of well-supported mechanisms of comprehension of the underlying temporal phenomenon. In all these pursuits, information granules manifest along the two main dimensions (as noted above). The first one is concerned with time granularity. Time series is split into a collection of time windows – temporal granules. One looks at time series in temporal windows of months, seasons, years. Time series is also perceived and quantified in terms of information granules being formed over the space of amplitude of the successive samples of the sequence; one arrives at sound and easily interpretable descriptors such as low, medium, high amplitude and alike. One can also form information granules over the space of changes of the time series. Combined with the temporal facets, the composite information granules arise as triples of entities, say long duration, positive high amplitude, approximately zero changes, etc. Once information granules are involved, long sequences of numbers forming time series are presented as far shorter, compact sequences of information granules-conceptual entities. As noted above, those are easier to understand and process. Let us note that while temporal granules are quite common (and they are of the same length), forming and processing composite information granules built over variable length time intervals call for detailed investigations.As commonly encountered in the investigations on time series, a starting point is a collection of numeric data (samples recorded in successive time moments) – time series {x1, x2…xN}. For the purpose of further analysis, we also consider the dynamics of the time series by considering the sequences of differences (changes) observed there, namely {Δx2…ΔxN} where Δxi=xi−xi−1. Quite often a smoothed version of these differences is considered. The space in which time series are discussed comes as a Cartesian product of the space of amplitude X and change of amplitude, namely X×ΔX.The bird's eye view of the overall architecture supporting processing realized at several layers which stresses the associated functionality is displayed in Fig. 8[32]. Let us elaborate in detail on the successive layers at which consecutive phases of processing are positioned:Formation of information granules: the granules are formed over the Cartesian product of amplitude and change of amplitude, viz. the space X×ΔX. Here for each time slice Ti(note that the time variable is also subject to granulation as well; in addition to the same length time intervals, their size could be eventually optimized), we form an interval information granule (Cartesian product) Xi×ΔXi. These information granules are constructed following the principle of justifiable granularity.Visualization of information granules: The results produced at the previous processing phase are visualized. In essence, for each time slice (segment) Ti, one can visualize a collection of Cartesian products of the information granules in X×ΔX obtained in successive time slices.Linguistic description of granular time series: While the visualization of the granular time series could be quite appealing emphasizing the main temporal tendencies observed in the time series, it is also worth arriving at the linguistic description of the time series, which is articulated in terms of some linguistic (granular) landmarks and levels of the best matching of these landmarks with the constructed information granules Xi×ΔXi. Having a collection of the landmarks Ai, i=1, 2, …, c where typically c≪p (where “p” stands for the number of time slices of the time series), the linguistic description comes as a string of the landmarks (for which the best matching has been accomplished) along with the corresponding matching levels μ1, μ2, …, μp. For instance, the linguistic description of the time series can read as the following sequence of descriptors,{(positive small,negative medium)(0.7)}{(negative large,around zero)(0.9)}In the above, each granular landmark comes with its own semantics (e.g., positive small, negative medium, etc.). The triples linguistically describe the amplitude and its change (expressed in terms of linguistic terms), and the associated matching level.To arrive at the linguistic description of this nature, two associated tasks are to be completed, namely (a) a construction of meaningful (semantically sound) granular landmarks, and (b) invocation of a matching mechanism, which returns a degree of matching achieved. The first task calls for some mechanism of clustering of information granules while the second one is about utilizing one of the well-known matching measures encountered in fuzzy sets, say possibility or necessity measures.Linguistic prediction models of time series: The descriptions of the time series are useful vehicles to represent (describe) time series in a meaningful and easy to capture way. Per se, the descriptors are not models such as standard constructs reported in time series analysis. They, however, deliver all components, which could be put together to form granular predictive models. Denoting by A1, A2, …, Acthe linguistic landmarks developed at the previous phase of the overall scheme, a crux of the predictive model is to determine relationships present between the activation levels of the linguistic landmarks present for the current time granule Tkand those levels encountered in the next time granule Tk+1. The underlying form of the predictive mapping can be schematically expressed in the following way,(23)A1(Xk),A2(Xk),…,Ac(Xk)→A1(Xk+1),A2(Xk+1),…,Ac(Xk+1)where Ai(Xk) stands for a level of activation (matching) observed between Aiand the current information granule Xk. The operational form of the predictive model can be realized in the form of a fuzzy relational equation(24)A(Xk+1)=A(Xk)∘Rwith “ο” being a certain composition operator used in fuzzy sets (say, max–min or max-t composition where “t” stands for a certain t-norm) completed over information granules. Ai(Xk+1) is the activation levels of the linguistic landmarks Aicaused by the predicted information granule Xk+1. Some other types of relationships could be envisioned here as well such as neural networks, A(Xk+1)=NN(Xk, W) with W being a collection of connections.Overall, the vector A(Xk) has “c” entries, namely [A1(Xk) A2(Xk) …. Ac(Xk)]. The granular relation R of dimensionality c×c captures the relationships between the corresponding entries of A(Xk) and A(Xk+1). Note that as a result of using the model (23), Xk+1 is not specified explicitly but rather through the degrees of activation of the linguistic landmarks. In other words, the predictive granular model returns a collection of quantified statements (the corresponding entries of A(Xk+1))(25)–predicted information granule isA1with degree of activationλ1(=A1(Xk+1))–predicted information granule isA2with degree of activationλ2(=A2(Xk+1))–predicted information granule isAcwith degree of activationλc(=Ac(Xk+1))which offer a certain intuitive view at the prediction outcome. Obviously, one can easily choose the dominant statement for which the highest level of matching has been reported, namely predicted information granule is Ai with degree of activation λi(=Ai(Xk+1)) where λi=arg maxj=1,2,…,cAj(Xk+1)).Note that there might be also another relational dependency as to the predicted length of the next time granule Tk+1 (in case the sizes of temporal information granules vary across the description of the time series)(26)A1(Xk),A2(Xk),…,Ac(Xk),Tk→Tk+1which can be regarded as a certain granular relational equation describing dependencies among time granules Tkand Tk+1(27)Tk+1=A(Xk)∘Tk∘GIn the above expression, G stands for a fuzzy relation, which links the input and output variables while “ο” denotes a sup-t composition operator. Alluding to the above formulas we can view these predictive models as granular models of order-2 as they are described over the space of information granules rather than numeric entries.In a general sense, we can note that the granular time series offer a different, more synthetic view at temporal relationships where both time and feature space in which data are characterized are subject to granulation, see Fig. 9. Here the notation G(T) and G(X) refers to the granulation of the time framework and the feature space of the time series. The elevation to the level of information granules is carried out with regard to the feature space (meaning that further processing is completed in G(X)) or both the time and feature space, viz. G(T) and G(X).The quality of processing (say prediction or classification) is then assessed in terms of produced information granules.

@&#CONCLUSIONS@&#
A wealth of problems of signal processing (filtering, discrimination, interpretation) can be effectively formulated and solved in the setting of Computational Intelligence. CI provided new, attractive opportunities by bringing a facet of nonlinear processing (supported by neural networks) and delivers a realization of a variable perspective at the problem description through information granularity. Furthermore evolutionary computing helps approach the system design from the perspective of structural optimization – a unique opportunity not commonly available when dealing with the “standard” methods of signal processing or classification.We outlined the fundamentals of Computational Intelligence showing that the synergy of the technologies of fuzzy sets becomes a vital component of the design of intelligent systems.With this regard, fuzzy sets or being more general, information granules, form an important front- and back-end of constructs of CI. By forming the front end, they help develop a suitable view at temporal data, incorporate available domain knowledge and come up with a feature space that supports the effectiveness of ensuing processing, quite commonly engaging various schemes of neurocomputing or evolutionary neurocomputing. Equally important role is being played by fuzzy sets in the realization of the back end of the overall processing scheme: they strengthen the interpretability of classification results as well as provide useful interpretation faculties to neural networks or help develop logic mappings in the form fuzzy logic neural networks.