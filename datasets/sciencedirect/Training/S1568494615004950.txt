@&#MAIN-TITLE@&#
Optimization of neural network model using modified bat-inspired algorithm

@&#HIGHLIGHTS@&#
A modified bat algorithm with a new solution representation for both optimizing the weights and structure of ANNs is proposed.To improve the exploration and exploitation capability of bat algorithm some modifications based on chaotic map on bat algorithm are studied.The Taguchi method is used to tune the parameters of the algorithm.Six classifications and two time series benchmark datasets are used to test the performance of the proposed approach in terms of classification and prediction accuracy.Finally, our best method is applied to a real-world problem, namely to predict the future values of rainfall data in Selangor at Malaysia.

@&#KEYPHRASES@&#
Bat-inspired algorithm,Artificial neural network,Chaotic map,Time series prediction,Classification,Real-world rainfall data,

@&#ABSTRACT@&#
The success of an artificial neural network (ANN) strongly depends on the variety of the connection weights and the network structure. Among many methods used in the literature to accurately select the network weights or structure in isolate; a few researchers have attempted to select both the weights and structure of ANN automatically by using metaheuristic algorithms. This paper proposes modified bat algorithm with a new solution representation for both optimizing the weights and structure of ANNs. The algorithm, which is based on the echolocation behaviour of bats, combines the advantages of population-based and local search algorithms. In this work, ability of the basic bat algorithm and some modified versions which are based on the consideration of the personal best solution in the velocity adjustment, the mean of personal best and global best solutions through velocity adjustment and the employment of three chaotic maps are investigated. These modifications are aimed to improve the exploration and exploitation capability of bat algorithm. Different versions of the proposed bat algorithm are incorporated to handle the selection of the structure as well as weights and biases of the ANN during the training process. We then use the Taguchi method to tune the parameters of the algorithm that demonstrates the best ability compared to the other versions. Six classifications and two time series benchmark datasets are used to test the performance of the proposed approach in terms of classification and prediction accuracy. Statistical tests demonstrate that the proposed method generates some of the best results in comparison with the latest methods in the literature. Finally, our best method is applied to a real-world problem, namely to predict the future values of rainfall data and the results show satisfactory of the method.

@&#INTRODUCTION@&#
An artificial neural network (ANN) is an imitation of a biological natural neural network. Each ANN is structured into several interconnections of simple processing units which are called nodes or neurons. An ANN can have one or more layers of nodes between the input layer and output layer. Each of these interlayers is called a hidden layer, and they can be completely or partially connected. Each connection between two nodes has a specified weight. An ANN can be taught by training it using on hand cases with inputs and expected outputs. Due to the learning ability and nonlinearity method of ANNs [1], they have been used extensively in many applications of data mining such as classification, associate rule mining and clustering [2,3]. ANN is regarded as one of the computational intelligence techniques and an effective machine learning tool capable of data mining applications.Although many researches have been performed to generate an accurate ANN model in the literature, the automatic creation of a near-optimal structure for an ANN for a given task is still considered to have potential. Several researchers have used evolutionary algorithms in the design of ANNs. For example, Palmes, Hayasaka, and Usui [4] applied a mutation-based genetic algorithm for neural network training, while Gepperth and Roth [5] proposed an evolutionary multiobjective procedure to optimize the feedforward neural network. Hung and Du [6] applied particle swarm optimization (PSO) to optimize an ANN. Hervás-Martínez, Martínez-Estudillo, and Carbonero-Ruz [7] designed the structure and weights of an ANN using an evolutionary algorithm and a backward stepwise procedure. Chi-Keong, Eu-Jin, and Kay Chen [8] proposed a hybrid multiobjective evolutionary approach in their design for an ANN. A Taguchi-based parameter designing of genetic algorithm for ANN training was proposed in [9]. However, most of these strategies need the chromosome length to be predefined. Since this user-defined length is problem-dependent, it can affect the efficiency of the method. Besides this issue, the input variables are fixed (no feature selection), but the selection of features is important especially for the classification problem [10–13]. Even selecting all features as the input does not guarantee that the best accuracy in classification will be achieved [14].The use of merging and growing algorithms in the design of ANNs was proposed by Islam, Amin, Ahmmed, and Murase [15] and later on in another approach by Islam, Sattar, Amin, Yao, and Murase [16]. Kaylani, Georgiopoulos, Mollaghasemi, and Anagnostopoulos [17] employed a prune operator for a genetic algorithm to design the ARTMAP architecture. Later on, they extended their study for multiobjective approach [18]. Carry and Morgan [19] used a modified and pruned neural network for seasonal data. Mantzaris, Anastassopoulos, and Adamopoulos [20] applied a pruned probabilistic neural network using a genetic algorithm to minimize the structure of an ANN. The important problem with merging and growing algorithms is that when to add or delete the nodes need to be carefully predefined.In an attempt to address designing ANN, a family of multi-layer self-organizing neural networks has been studied in recent years [21–24]. Their method starts with one hidden layer and then the next hidden layers are added until the stopping criteria are met. Masutti and de Castro [25] used a combination of self-organizing networks and an artificial immune system to minimize the neurons in an ANN. However, the main disadvantage of the self-organizing method is that by growing the ANN during the process, the time will also incrementally grow so the stopping criteria need to be carefully predefined.Other metaheuristic algorithms have been applied for ANN model optimization in recent years. Ludermir, Yamazaki and Zanchettin [26] used the advantages of combined simulated annealing, tabu search and backpropagation to optimize an ANN model during the training process. Yu, Wang, and Xi [27] evaluated an ANN using modified PSO and discrete PSO (DPSO). A PSO-based single multiplicative neuron model for time series prediction was proposed by Zhao and Yang [28]. In 2011, a hybrid approach has been proposed to optimize the operation of the ANN in terms of weights and architecture [29].In the literature a few number of methods [1,26,29] have been applied for optimizing both weights and structure of neural network simultaneously. It is the significance of the research in this area to find a superior solution for the neural network model.In our method presented in this paper, both the weights and the structure of ANN are varied during optimization procedure. The variation of weights and structure of ANN is provided using a solution representation proposed in this paper. With aid of this solution representation, the troubles with user-defined length and feature selection in evolutionary algorithms, time growing with self-organizing neural networks and predefined period for adding and deleting process in merging and growing algorithms are overcome. In order to have more accurate model compared to proposed methods in the literature, optimization technique in our method is provided derived from bat algorithm. The bat algorithm as recent and powerful algorithm has been proposed based on the echolocation behaviour of bats by Yang [30]. The advantage of bat algorithm is for gaining the combination of population based algorithm and local search. This combination initially provides the algorithm capability of global diverse exploration and local intensive exploitation which is the key point in metaheuristic algorithms. This interesting metaheuristic optimization technique behaves as a group of bats looking for prey using their ability of echolocation [31–33]. This new metaheuristic algorithm very quickly revealed its superior ability in many areas of optimization. A work comparing the bat algorithm and many other algorithms was recently published [34]. Also, a solution for a scheduling problem using a bat algorithm was presented by Musikapun and Pongcharoen [35], while a fuzzy bat clustering method was proposed by Khan, Nikov, and Sahai [36], and many other applications of the bat algorithm have been put forward [37–40].In this paper, the basic bat algorithm and several modified versions which are towards enhancing exploration and exploitation capability of bat algorithm are evaluated and tested using six classifications and two time series prediction problems. Among these, the ability of three versions using the chaotic map approach to generate a chaotic sequence instead of a random sequence is investigated. Chaos is a random-like procedure set up in nonlinear and dynamic systems. Most of the metaheuristic algorithms in the literature use uniform probability to generate random numbers. When a random number is needed by the algorithm, it can be produced by iterating one action of the selected chaotic map that is started from a random initial value in the first step. In recent years, new approaches using the chaotic map have been presented [41–43]. In some works, chaotic sequences have been assumed instead of random sequences and to some extent good results have been achieved in many applications [44–46]. Following this same line of study in the literature, we combine three versions of chaotic maps with our method to improve the quality of the convergence in bat algorithm. The Taguchi method is then applied to the best method to adjust the parameters of the algorithm. Then statistical tests are undertaken to determine the efficacy of our proposed algorithm.The rest of this paper is organized as follows. Section 2 provides a brief description of classification and time series prediction problems. Section 3 explains the proposed method in detail. Section 4 reports the experimental results and discusses the outcomes of the statistical tests, and finally, Section 5 presents the conclusion of this work.In this work we solve two different data mining tasks: classification and time series prediction. Many algorithms have been widely applied to solve these kinds of problems which are significant in the field of data mining.The task of assigning an object to a proper group (based on a number of attributes describing that object) is defined as a classification problem. The classification process is as follows:•Given a set of records called a training set, each record contains a set of attributes; one of the attributes is selected as the class attribute.A proper model for the class attribute as a function of other attributes is sought.Earlier unseen records are allocated to a class as accurately as possible and a testing set is employed to determine the accuracy of the model.Generally, the given dataset is divided into training and testing sets, where the training set is employed to build the model and the testing set is applied to validate the model.Time series prediction is the use of a specified model to forecast future values based on earlier values. The time series prediction procedure is as follows:•Given a set of points in the past as a training set, each point is considered as input for the model.A proper model for the future values as a function of past values is sought.For every point in the past, the model is trained using past data as the inputs and what follows is the desired output.The training is referred to when building the model and the model is then tested using test data.

@&#CONCLUSIONS@&#
