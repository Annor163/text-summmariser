@&#MAIN-TITLE@&#
A rapid fuzzy rule clustering method based on granular computing

@&#HIGHLIGHTS@&#
A rapid fuzzy rule clustering method based on granular computing is proposed.Exemplar descriptions are selected from sample's descriptions by relative frequency.Data granulation is guided by the selected exemplar descriptions.

@&#KEYPHRASES@&#
Fuzzy clustering,Granular computing,Fuzzy rule,Fuzzy number,Sample's description,

@&#ABSTRACT@&#
Traditionally, clustering is the task of dividing samples into homogeneous clusters based on their degrees of similarity. As samples are assigned to clusters, users need to manually give descriptions for all clusters. In this paper, a rapid fuzzy rule clustering method based on granular computing is proposed to give descriptions for all clusters. A new and simple unsupervised feature selection method is employed to endow every sample with a suitable description. Exemplar descriptions are selected from sample's descriptions by relative frequency, and data granulation is guided by the selected exemplar fuzzy descriptions. Every cluster is depicted by a single fuzzy rule, which make the clusters understandable for humans. The experimental results show that our proposed model is able to discover fuzzy IF–THEN rules to obtain the potential clusters.

@&#INTRODUCTION@&#
Clustering is one of the most significant research fields in data mining. It aims at partitioning the data into groups of similar objects (samples, patterns). From a machine learning perspective, what clustering does is to find the hidden patterns of the dataset in an unsupervised way, and the resulting system is usually referred to as a data concept. From a practical perspective, clustering plays an outstanding role in data mining applications such as image segmentation [1], computational biology [2,3], web analysis [4], text mining [5], graph clustering [6], and many others.Clustering methods can be classified into two types: partitional and hierarchical [7]. The partitional approach produces a single partition of the data points, such as the well-known K-means [8] clustering method. And while the hierarchical approach gives a nested clustering result in the form of a dendrogram (cluster tree), from which different levels of partitions can be obtained such as single-link [9], complete-link [10] and average-link [11].Recently, Frey and Dueck devised a method called Affinity Propagation (AP) [12], which is an unsupervised clustering algorithm based on message-passing techniques. Zhang et al. presented a clustering method called KAP to generate specified K clusters based on AP. Fowlkes et al. proposed a spectral grouping method Normalized Cuts [13] to use the Nyström approximation to extend normalized cut. Zelnik-Manor and Perona proposed a method named Self-Tuning spectral clustering (STSC) [14] in which a “local” scale should be used to compute the affinity between each pair of points. Agarwal and Mustafa presented an extension of the K-means clustering algorithm for projective clustering in arbitrary subspaces (KMPC) [15]. Steinley and Hubert proposed an order-constrained K-means cluster analysis through an auxiliary quadratic assignment optimization heuristic (OCKC) [16]. Two distance based clustering methods al-SL [17] are proposed by Patra et al. for arbitrary shaped clusters. DBCAMM [18] is an approach to merge the sub-clusters by using the local sub-cluster density information.Relaxing this rigidity (crispness) of the partition has constituted in the past a domain of research in the framework of cluster analysis [19]. Many authors have proposed a fuzzy setting as the appropriate approach to cope with this problem. In fuzzy clustering, the well-known fuzzy c-means (FCM) clustering algorithm, which was first proposed by Dunn [20] and then extended by Bezdek [21], is the best-known and has been extensively used in data clustering and related applications. Some new fuzzy clustering methods are proposed, Honda et al. [22] used the fuzzy principal component analysis to obtain the cluster indicator, as well as the responsibility weights of samples for the K-means process in order to develop a robust K-means clustering scheme. Tan et al. proposed an improved FCMBP fuzzy clustering method [23] based on evolutionary programming. A proximity fuzzy framework for clustering relational data [24] is presented by Graves et al. A certain knowledge-guided scheme of fuzzy clustering in which the domain knowledge is represented in the form of viewpoints is introduced [25] by Pedrycz et al., the users point of view at data, which are represented in a plain numeric format or through some information granules, is included in the clustering process.Nevertheless, samples are assigned to clusters by these traditionally clustering techniques, users need to manually give descriptions for all clusters. In addition, users sometimes have no specific idea regarding how to explain the clustering results, thus, they might give inappropriate descriptions. A clustering technique is proposed in this study to discover fuzzy IF–THEN rules and provide description of clusters. Each cluster is depicted by a single fuzzy IF–THEN rule. A fuzzy rule-based clustering system is a special case of fuzzy modeling, the acquired knowledge with these system may be more human understandable [26]. The proposed clustering method named FRCGC is based on granular computing. The obtained clusters are specified with some interpretable fuzzy rules, which make the clusters understandable for humans. The experimental results show that our proposed model is able to discover fuzzy IF–THEN rules to obtain the potential clusters.The remainder of this paper is organized as follows. The following section, we provide an overview of data representation and preliminary notions. The procedure of our clustering algorithm is presented in section “Our proposed rapid fuzzy rule clustering method”. Section “Illustrative experiments” provides the detailed analysis of the experiments on a synthetic data. In section “Experimental evaluation”, we thoroughly evaluate the efficacy of the proposed model through a number of experiments, using publicly available data. Finally, in the concluding section, we summarize and discuss our results.This section briefly reviews some background concepts regarding data representations as well as notions. Let X={x1, x2, …, xn} be a set of n samples, where xi∈Rd(i=1, 2, …, n) denotes the ith sample. fj(j=1, 2, …, d) denotes the jth column (feature) of X. Thus, X=(xij) is an n×d matrix representing data, each column of X corresponds to a feature, whereas each row corresponds to a sample.The non-symmetric trapezoidal and triangular forms of fuzzy numbers [27] are used (Fig. 1) in this paper to construct fuzzy rules. In Fig. 1, feature value is divided into four fuzzy subspaces, “big”, “medium big”, “medium small”, and “small”. The fuzzy numbers: trapezoidal for the linguistic terms “big”, “small”, and triangular for the linguistic term “medium big”, “medium small” are used. It is often possible, on the basis of expert experience, to define the parameters of fuzzy number ϕ by means of linguistic variables.The parameters of fuzzy numbers are defined by the method of Equal Interval Width/Uniform Binning (EIB) [28]. This method relies on sorting the jth feature value and dividing the fjvalues into equally spaced bin ranges. A seed K (the number of cluster) supplied by the user determines how many bins are required. With this seed K, it is just a matter of finding the maximum and minimum values of fjto derive the range and then partition the data into K bins. The bin width is computed by: ɛ=(max(fj)−min(fj))/K, and bin thresholds (cut points) are constructed at cpi=min(fj)+iɛ, where i=1, 2, …, K−1. Let mskdenote the mean of all of the samples that fall into the kth bin.Example 1To illustrate the process of generating fuzzy numbers, let us consider the dataset that is shown in Table 1. Assume that K=2, for feature f1, min(f1)=1.4, max(f1)=4.9, ɛ=(max(fj)−min(fj))/K=(4.9−1.4)/2=1.75, cp1=min(fj)+ɛ=1.4+1.75=3.15, ms1=mean(1.4, 1.6, 1.5, 1.4, 1.5, 1.6)=1.5, ms2=mean(4.6, 4.7, 4.9, 4.6)=4.7, and the fuzzy numbers are shown in Fig. 2respectively corresponding to “big” and “small”. The membership value of sample belonging to fuzzy number is shown in Table 2, where, ϕ11 stands for the value of feature f1 is “small”, ϕ21 stands for the value of feature f1 is “big”,ϕ12 stands for the value of feature f2 is “small”, ϕ22 stands for the value of feature f2 is “big”.IF–THEN clustering rules are intuitively comprehensible for most humans since they represent knowledge at a high level of abstraction involving logical conditions rather than point-based cluster representations. In this paper, a clustering rule R defined in the continuous space Rdis knowledge representation in the form:R:IFxiisϕ11and,…,andxiisϕddTHENcluster_labelThe antecedent (IF) part of R consists of a logical conjunction of d conditions, one for each feature, whereas the conclusion (THEN) part contains the cluster label. The semantics of this kind of clustering rule is: if all the conditions specified in the antecedent part are satisfied by the corresponding feature values of a given data point, then this point is assigned to (or covered by) the cluster, identified by the consequent.Let ϕj={ϕkj|k=1, 2, …, K} be the set of all fuzzy numbers one defined on the jth feature, Φ={ϕkj|k=1, 2, …, K, j=1, 2, …, d} be the set of all fuzzy numbers one defined on the whole features f1, f2, …, fd.Definition 1fuzzy spaceIf Fs∈Ω, then Fs is a fuzzy subspace, where Ω can be calculated using:(1)Ω={Πj=1dϕ|{ϕ∈ϕj}}In fact, the antecedent part of fuzzy rule R is corresponding to a fuzzy subspace Fs, fuzzy numbers Φ divides the universe of discourse Rdinto Kdfuzzy subspaces (fuzzy rules).Example 2Let us consider Example 1, due to K=2, the universe of discourse R2 into 4 fuzzy subspaces is shown in Fig. 3. The 4 fuzzy subspaces can be respectively represented by the following 4 antecedent part of fuzzy rules.R1:IFxiisϕ11andxiisϕ12THENcluster_label,R2:IFxiisϕ11andxiisϕ22THENcluster_label,R3:IFxiisϕ21andxiisϕ12THENcluster_label,R4:IFxiisϕ21andxiisϕ22THENcluster_label.Definition 2sample's description [29–33]For a sample xi∈Rd, the fuzzy description Des(xi)∈Ω of xiis defined as:(2)Des(xi)=∏j=1dargmaxϕ∈ϕjϕ(xi)Each sample's fuzzy description consists of a fuzzy subspace, is also an interpretable fuzzy set. LetμDes(xi)(xj)denote the membership value of a sample xjbelonging to the fuzzy description Des(xi),μDes(xi)(xj)can be calculated using the following formula:(3)μDes(xi)(xj)=1d∑ϕ∈Des(xi)ϕ(xj)Example 3Let us consider Example 2, Let A1, A2, A3, A4 respectively denote the antecedent part of fuzzy rules R1, R2, R3, R4. Every samples fuzzy descriptions is shown in Table 3, and the membership value of every sample belonging to the four fuzzy rules is also shown in Table 3.Definition 3similarity between two fuzzy descriptionsFor two fuzzy descriptions Des(xi) and Des(xj), the similarity Sim(Des(xi), Des(xj)) between two descriptions Des(xi) and Des(xj), is defined as:(4)Sim(Des(xi),Des(xj))=1d|{ϕ|ϕ∈Des(xi)∧ϕ∈Des(xj)}|The dissimilarity DSim(Des(xi), Des(xj)) between two fuzzy descriptions Des(xi) and Des(xj) is measured by:(5)DSim(Des(xi),Des(xj))=1−Sim(Des(xi),Des(xj))where, the symbol |·| represents the cardinality of a set.The dissimilarity is a metric distance function, ∀DesA, DesB, DesC∈Ω, one can easily verify that:(1)DSim(DesA, DesB)=DSim(DesB, DesA),DSim(DesA, DesA)=0,0≤DSim(DesA, DesB)≤1,DSim(DesA, DesB)≤DSim(DesA, DesC)+DSim(DesB, DesC).Example 4Let us consider the fuzzy descriptions in Example 3, Sim(A1, A2)=0.5, Sim(A1, A3)=0.5, Sim(A1, A4)=0, Sim(A2, A3)=0, Sim(A2, A4)=0.5, Sim(A3, A4)=0.5.Definition 4probability spaceThe 3-tuple (Ω, 2Ω, P) is a finite probability space, if P is a probability measure on Ω satisfies:(1)P(Ω)=1;∀B∈2Ω, 0≤P(B)≤1;For every countable sequence of mutually disjoint events {Bl∈2Ω}, l=1, 2, …, n,P(⋃l=1nBl)=∑l=1nP(Bl);(6)P(B)=1n|{xi|μB(xi)≥μBc(xi)}|where, Bcdenotes complement of a set B, μB(xi) denotes the membership value of xibelonging to B can be calculated using:(7)μB(xi)=maxFs∈BμFs(xi),(8)μBc(xi)=maxFs∈BcμFs(xi).Example 5Let us again consider the fuzzy descriptions in Example 3, P(A1)=(1/10)|{x1, x4, x5, x7}|=0.4, P(A2)=(1/10)|{x3, x9}|=0.2, P(A3)=(1/10)|{x2}|=0.1, P(A4)=(1/10)|{x6, x8, x10}|=0.3.Granular computing [34,35] offers a simple and effective way of extracting information out of datasets, inspired by the human perception of grouping similar featured items together [36,37]. By using granular computing, it is possible to group data together not only based on similar mathematical properties such as proximity, but also it considers the raw data as conceptual entities that are captured in a compact and transparent manner.Data granulation [38–41] is an algorithmic process which is achieved by a simple two step iterative process involving the following two steps:•Find the two most compatible information granules and merge them together as a new information granule containing both original granules.Repeat the process of finding the two most compatible granules until a satisfactory data abstraction level is achieved.In this study, a fuzzy rule consists of only a single fuzzy number B∈Φ is viewed as an atomic granule denoted by Gar[40]. Data granulation is accomplished by merging these atomic granules.Our proposed rapid fuzzy rule clustering method is based on granular computing, and named FRCGC. The FRCGC work flow is summarised in the top level overview of the process shown in Fig. 4. The idea of FRCGC is described as follows: firstly, some features are selected in order to control the complexity. After that, calculating every samples description (fuzzy rule) by Definition 2 on the remaining features. Lastly, exemplar descriptions are selected from samples descriptions, and data granulation (the clustering procedure) is guided by the selected exemplar fuzzy descriptions.A. Ferreira and M. Figueiredo proposed an unsupervised feature selection method RFS [42], the key idea is that features with higher variance are more informative than features with lower variance. In order to choose an adequate number of features, A.J. Ferreira and M. Figueiredo proposed to use a cumulative measure as follows. Let {ri, i=1, …, d} be the variance values as given by {ri=var(fi)/bi}, where biis the number of bits allocated to feature fiby U-LBG1 method [42] which is a method of feature discretization, and {r(i), i=1, …, d} is the same values after sorting in descending order, choosing m as the lowest value that satisfies:∑j=1mr(j)/∑j=1dr(j)≥L, where L is some threshold (such as 0.5). In order to decrease the computational cost, in this paper {ri, i=1, …, d} is computed by {ri=var(fi)} and do not consider the number of bits bi, that means the procedure of feature discretization is not employed. The modified method called mRFS is shown in Algorithm 1.Algorithm 1mRFS: modified Relevance Feature Selection.Input: X: n×d matrix, d dimensional dataset with n samples; L∈[0, 1]: threshold to choose an adequate number of features to keep;Output: FeatKeep: an m-dimensional array (with m<d) containing the indexes of the selected features;Xˆ: n×m matrix, reduced dimensional dataset, with features sorted by decreasing relevance;1:Compute the relevance rj=var(fj) of each feature, j=1, 2, …, d;2:Sort features by their relevance rjin decreasing order obtaining r(j);3:Compute m satisfies:∑j=1mr(j)/∑j=1dr(j)≥L;4:Fill the FeatKeep array with the indexes of the m top-ranked features;5:BuildXˆfrom X using FeatKeep, by keeping only the m features with largest relevance r(j);6:returnXˆ, FeatKeep, m;Now suppose that, the number of clusters is K, the triangular and trapezoidal membership function are adopt to generate fuzzy number, and the number of fuzzy numbers on every feature is set to K. Let Φ={ϕkj|k=1, 2, …, K, j=1, 2, …, d} be the set of all fuzzy numbers. Fuzzy numbers Φ divide the universe of discourse Rdinto Kdfuzzy rules. Each sample's fuzzy description is calculated by Definition 2.Lastly, exemplar fuzzy descriptions are selected from samples descriptions by relative frequency, and data granulation is guided by the selected exemplar fuzzy descriptions. The clustering pseudocode is shown in Algorithm 2.Algorithm 2FRCGC: Proposed rapid fuzzy clustering method based on granular computing.Input:Xˆ: n×m matrix, reduced dimensional dataset, with features sorted by decreasing relevance; K the number of clusters;Output: Grak(k=1, 2, …, K): clustering granule set; Exek(k=1, 2, …, K): is the kth exemplar of fuzzy descriptions;1:Gra=∅, Λ=∅, λ=∅, p=0;2:For each sample xi(i=1, 2, …, n);3:ComputeDes(xi)=∏j=1dargmaxϕkj∈ϕjϕkj(xi);4:if Des(xi)∉Λ;5:p=p+1;6:Λp=Des(xi);7:compute λp=P(Des(xi));8:end if;9:end for;10:For k=1, 2, …, K;11:compute id=argmaxq,(q=1,2,…,p)λq;12:Exek=Λid;13:For q=1, 2, …, p;14:compute λq=λq×Dsim(Λq, Exek);15:end for;16:end for;17:For each sample xi(i=1, 2, …, n);18:computeid=argmaxk,(k=1,2,…,K)μExek(xi);19:Graid=Graid⋃xi;20:end for;21:returnGrak, Exek, Des(xi);A weather data in Table 4is used as an illustrative example. It consists of 10-day observations and 3 features which are Temperature (f1), Humidity (f2), and Wind (f3). Let X={x1, …, x10} be a set of 10 day observations, and xi∈R3 (i=1, 2, …, n) denotes the ith sample, fj(j=1, 2, 3) denotes the jth feature of X. Thus, X=(xij) is a 10×3 matrix representing data, xijis the jth feature value of xi.Firstly, data should be firstly normalized by the following formula:(9)f˜j=fj−min{fj}max{fj}−min{fj}where, fjrepresents the jth column of X corresponds to the value of the jth feature. The normalized weather data is shown in Table 5. Due to the variance ofr1=var(f˜(:,1))=0.11,r2=var(f˜(:,2))=0.12, andr3=var(f˜(:,3))=0.08, according to Algorithm 1 (L=0.5), r(1)=0.12, r(2)=0.11, r(3)=0.08,r(1)/∑j=13r(j)=0.39<L,(r1+r2)/∑j=13r(j)=0.75≥L, thus, features f1 and f2 are selected.Given the number of clusters K=2, the parameters of fuzzy numbers can be calculated as follows. For feature f1, the bin width is computed by: ɛ=(max(f1)−min(f1))/K=(100−25)/2=37.5, and cut points are constructed at cp1=min(f1)+ɛ=62.5, and ms1=mean{25, 43, 47, 45}=40, ms2=mean{75, 73, 70, 94, 100, 91}=84. For feature f2, the bin width is computed by: ɛ=(max(f1)−min(f1))/K=(0.9−0)/2=0.45, and cut points are constructed at cp1=min(f1)+ɛ=0.45, and ms1=0.16, ms2=0.85, and the obtained fuzzy numbers are shown in Fig. 5respectively corresponding to “big” and “small”.The membership value of sample belonging to fuzzy numbers is shown in Table 6, where, ϕ11 stands for the value of feature f1 is “small”, ϕ21 stands for the value of feature f1 is “big”,ϕ12 stands for the value of feature f2 is “small”, ϕ22 stands for the value of feature f2 is “big”. The membership value of samples belonging to fuzzy number is shown in Table 6. According to Table 6 and Definition 2, we can calculate every sample's description as follows: Des(x1)=ϕ21ϕ22, Des(x2)=ϕ21ϕ12, Des(x3)=ϕ21ϕ12, Des(x4)=ϕ21ϕ12, Des(x5)=ϕ21ϕ12, Des(x6)=ϕ21ϕ12, Des(x7)=ϕ11ϕ12, Des(x8)=ϕ11ϕ12, Des(x9)=ϕ11ϕ12, Des(x10)=ϕ11ϕ22,Firstly, we need choose only a single sample's description as exemplar to describe the first cluster by Algorithm 2. According to Eq. (6), the importance of sample's description can be obtained by the frequency of occurrence: λ1=P(ϕ21ϕ22)=0.1, λ2=P(ϕ21ϕ12)=0.5, λ3=P(ϕ11ϕ12)=0.3, λ4=P(ϕ11ϕ22)=0.1. Thus, ϕ21ϕ12 is chosen as the first cluster's description. Next, we should choose another sample's description as the second cluster's description. According to Algorithm 2, the importance of sample's description should be updated by considered the first cluster's description, λ1=λ1*Dsim(ϕ21ϕ22, ϕ21ϕ12)=0.1*0.5=0.05, λ2=λ2*Dsim(ϕ21ϕ12, ϕ21ϕ12)=0.5*0=0, λ3=λ3*Dsim(ϕ11ϕ12, ϕ21ϕ12)=0.3*0.5=0.15, λ4=λ4*Dsim(ϕ11ϕ22, ϕ21ϕ12)=0.1*1=0.1, the biggest one is λ3, thus ϕ11ϕ12 is selected as the second cluster's description.The first cluster's description is “the value of feature 1 is large and the value of feature 2 is small”. The second cluster's description is “the value of feature 1 is small and the value of feature 2 is small”.Data granulation is guided by the cluster's description, according to the membership degree of samples belonging to clusters’ description which is shown in Table 7, the first cluster is {x1, x2, x3, x4, x5, x6}, the second cluster is {x7, x8, x9, x10}.In this section, we evaluate the performance of FRCGC algorithm with comparison to OCKC [16], KAP [43], KMPC [15], K-means++ [44], FCM [21], K-means [8], and STSC [14], and conduct a number of experiments to verify the properties of our proposed FRCGC such as: how to choose the threshold value L and the computational complexity of FRCGC.All experiments are carried out on a personal computer with Intel (R) Core (TM) i5-2520M CPU (2.50GHz) processor, 4.00GB (2.94GB usable) memory, and Windows 7 64-bit Operating system. All algorithms are implemented as a computer program in the Matlab 7.13.0.564 (R2011b) environment.The classification error rate is a common measure used to determine how well clustering algorithms perform on a data with a known structure (i.e., classes). The classification error rate is determined first by transforming the fuzzy partition matrix into a Boolean partition matrix and by selecting the cluster with the maximum membership value for each sample. Class labels are assigned to each cluster according to the class that dominates that cluster. The classification error rate is the percentage of samples that belong to a correctly labeled cluster. The classification error rate can be determined by building a contingency matrix [45]. Higher classification error rates indicate better clustering results. The classification error rate is quite often used to measure the performance of fuzzy clustering algorithms.To assess the ability of the FRCGC in real-world data, 21 classification datasets (i.e., data with labeled samples) with numerical attributes are chosen from the University of California at Irvine Machine Learning Repository [46]. ALL-AML data [47] is a collection of 72 Leukemia patient samples, can be download from (http://www-genome.wi.mit.edu/cgi-bin/cancer/publications/pub_paper.cgi?mode=view&paper_id=43). Although more clustering data should be prepared for the evaluation of the FRCGC, the classification data are more appropriate here since the accuracy of the algorithm is more critical, provided the class labels of data are removed before applying the FRCGC. Table 8briefly describes the datasets name, the number of features, classes, and samples used in the experiments. These datasets have several types of data and represent many different learning problems.The classification error rate of the FRCGC on 21 classification datasets are shown in Table 9, the threshold L for the number of selected features is set to 0.5 for all these datasets. We have executed all algorithms 100 times independently with random initialization (different sort of samples) on each dataset. In Table 9, the best performance among these datasets is identified in bold case, and “–” denotes this clustering method cannot obtain the corrected number of classes, “*” means out-of-memory errors, “&” denotes this clustering method cannot work on this data, “#” means the running time more than 3000s. According to the results in this table, FRCGC is the best in 9 out of 21 datasets with five ties (Table 5), the performance of the FRCGC is comparable with other methods, although the clustering accuracy is not its essential issue. Standard deviation for the experiments on different datasets are summarized in Table 10.The results presented in Table 9 offer some insight in the performance of the algorithms. However, those results do not provide enough support for drawing a strong conclusion in favor or against any of the studied methods. To arrive at strong evidence, we resort ourselves to statistical testing of the result. The Holm test [48] is based on the relative performance of clustering method in terms of their ranks: for each dataset, the methods to be compared are sorted according to their performance, i.e., each method is assigned a rank (in case of ties, average ranks are assigned [48]). The test statistics for comparing the two clustering methods is expressed as:(10)z=Rankj−RankkSEwhere, k is the number of clustering methods, N is the number of dataset,Ranki=(∑i=1Nrij)/N,SE=k(k+1)/(6*N),rijis the rank of the clustering method j on the ith dataset. The z value is used to find the corresponding probability (p) from the table of normal distribution, which is then compared with an appropriate α. We denote the ordered p values by p1, p2, …, so that p1≤p2≤…≤pk−1. The Holm's step-down procedure compares each piwith α/(k−i), but differ in the order of the tests, and it starts with the most significant p value. If p1 is below α/(k−1), the corresponding hypothesis (the two classifiers have the same performance) is rejected and we are allowed to compare p2 with α/(k−2). If the second hypothesis is rejected, the test proceeds with the third one, and so on. As soon as a certain null hypothesis cannot be rejected, all the remaining hypotheses are retained as well.In this studies, RankOCKC=5.52, RankKAP=5.43, RankKMPC=4.43, RankK-means++=2.9, RankFCM=2.86, RankK-means=2.76, RankSTSC=2.48, RankFRCGC=2.33, and with α=0.05, k=8 and N=21, the standard error isSE=(8×9)/(6×21)=0.76. The Holm procedure rejects the first, the second, and the third hypothesis since the corresponding p values are smaller than the adjusted α's (see Table 11). This shows that FRCGC performs significantly better than OCKC, KAP, and KMPC at the significance level α=0.05. FRCGC is not significantly better than K-means++, FCM, K-means and STSC, however, FRCGC obtains the best ranks in the Friedman test and the knowledge represented by fuzzy rules is more human readable which will be shown in the following subsection.Compared with the traditional clustering methods such as OCKC, KAP, KMPC, K-means++, FCM, K-means, and STSC, our proposed FRCGC can automatically mine fuzzy IF–THEN rules to describe each class. We illustrate the obtained descriptions on the Iris data. There is a 150×4 data matrixX=(xij)150×4with data evenly distributed across three classes: iris-setosa, iris-versicolor, and iris-virginica. There are four features: sepal length and width, and petal length and width. Let M={f1, f2, f3, f4} be the set of features, xi=(xi1, xi2, xi3, xi4) be the ith sample.First of all, data should be normalized by Eq. (9), and computing the variance of normalized featuresf˜1,f˜2,f˜3, andf˜4, thus,r1=var(f˜(:,1))=0.05,r1=var(f˜(:,1))=0.03,r1=var(f˜(:,1))=0.09,r1=var(f˜(:,1))=0.1. According to Algorithm 1 with L=0.5, r(1)=0.1, r(2)=0.09, r(3)=0.05, r(4)=0.03,r(1)/∑j=14r(j)=0.37<L,(r1+r2)/∑j=13r(j)=0.69≥L, thus, features f4 and f3 are selected.Given the number of clusters is 3, the fuzzy numbers on features f4 and f3 is shown in Fig. 6. The obtained descriptions by FRCGC as follows: The first cluster's description is “the value of petal length is small and the value of petal width is small”. The second cluster's description is “the value of petal length is medium and the value of petal width is medium”. The third cluster's description is “the value of petal length is large and the value of petal width is large”.From our proposed FRCGC, we can notice that the threshold value L can decide the number of selected features, and control the length of fuzzy rules. To further verify the clustering performance of FRCGC, we analyze the relationship between the threshold value L and classification error rate. FRCGC is executed on the whole 21 datasets by given threshold value L equal to 0.01, 0.02, …, 0.99, the mean classification error rate on the whole 21 datasets is shown in Fig. 7. Fig. 7 also shows the mean classification error rate of OCKC, KAP, KMPC, K-means++, FCM, K-means, and STSC obtained from Table 9. One can easily see the mean classification error rate of FRCGC is always higher than other's. From Fig. 7, the best threshold value L should be set to 0.5 according to experimental studies.FRCGC is a non-iteration linear-time algorithm to find the best clustering solution, only needs to scan the dataset one time. The computational complexity is O(n)+O(d), where, O(n) is the computational complexity of mRFS, and O(d) is the computational complexity of FRCGC. The computational complexity of original K-means algorithm is O(ndK) at each iteration [49], and the computational complexity of original FCM algorithm is also O(ndK) at each iteration [50]. Table 12shows the average running times (s) of 100 runs of the proposed FRCGC and other clustering methods. From this table, one can see that KMPC, K-means, FCM, KAP, OCKC, and STSC always take more time than FRCGC on the 21 datasets, the speed of K-means++ and FRCGC is similar.

@&#CONCLUSIONS@&#
