@&#MAIN-TITLE@&#
Image super-resolution via 2D tensor regression learning

@&#HIGHLIGHTS@&#
Presenting a novel framework on 2D tensor regression learning model.Proposing three types of regularization to assist the model learning.Implementing the optimization algorithms for each specific model.Conducting comprehensive experiments on testing images.Demonstrating the high performance & efficiency of reconstruction.

@&#KEYPHRASES@&#
Image super-resolution,Tensor regression,Multi-task regression,Non-negative constraint,Orthogonal constraint,Frobenius norm,

@&#ABSTRACT@&#
Among the example-based learning methods of image super-resolution (SR), the mapping function between a high-resolution (HR) image and its low-resolution (LR) version plays a critical role in SR process. This paper presents a novel framework on 2D tensor regression learning model to favor single image SR reconstruction. From the image statistical point of view, the statistical matching relationship between an HR image patch and its LR counterpart can be efficiently represented in tensor spaces. Specifically, in this paper, we define a generalized 2D tensor regression framework between HR and LR image patch pairs to learn a set of tensor coefficients gathering statistical dependency between HR and LR patches. The framework is imposed by different constraint terms resulting in an interesting interpretation for the linear mapping function relating the LR and HR image patch spaces for image super-resolution. Finally, the HR image is then synthesized by a set of patches from one LR image input under the learned tensor regression model. Experimental results show that our algorithm generates HR images that are competitive or even superior to images produced by other similar SR methods in both PSNR (peak signal-to-noise ratio) and visual quality.

@&#INTRODUCTION@&#
As one of software resolution enhancement techniques, image super-resolution aims to recover high-resolution (HR) images from low-resolution (LR) input images [27]. Actually, image super-resolution (SR) processing is often desirable for low-cost imaging devices with resolution limitations [13], such as mobile phones, satellite imaging, video surveillance, microscopy, and digital mosaicing. With the assistance of SR, we can offer high quality images for the growing capability of modern HR displays. The basic idea behind SR is the fusion of a single or a sequence of LR noisy blurred images to produce an HR image or sequence. In many cases, multiple LR images of the same scene offer more information to recover a HR image of the scene. However, sometimes we only acquired few even a single LR input image in real world, thus an SR algorithm or technique using a single LR input image to recover HR image is more practical [25,39]. In this paper, we only consider the case that the input is a single LR image.In general, SR task is usually cast as the inverse problem [7] of recovering original HR image by fusing the observed LR images. The inverse problem is formulated under the following generic model,(1)y=Hxwhereyis the observed LR image (vectorized) andxis the unknown HR image (vectorized). The matrix H represents the imaging system, consisting of several processes, such as blurring and down-sampling operations. However, findingxfromybased on the above model is severely ill-posed because of the insufficient information from LR images thus the solution from the reconstruction constraint is not unique.Generally speaking, the existing super-resolution methods can be roughly categorized into three types, which are interpolation based [5], reconstruction based [22] and learning-based [3] approaches. Interpolation based techniques have their roots in sampling theory and the HR image is directly recovered through an interpolation from the LR input. These approaches tend to blur high frequency details resulting in noticeably smooth images with ringing and jagged artifacts particularly along edges, however, they remain popular due to their computational simplicity. In reconstruction based approaches, SR problem is cast as an inverse problem [7] of recovering an HR image based on a reasonable observation model assumption that maps HR image to the LR image(s) together with prior knowledge [6] about HR images. In this process, many kinds of regularization [24] are incorporated into the model as prior knowledge to stabilize the inversion procedure of this ill-posed problem, see [11,30]. However, the performance of the mentioned approaches is only acceptable for small upscaling factors, leading to the development of example-based learning approaches [12], which aim to learn the co-occurrence prior between local HR and LR image structures from an external training database [3,29].As the contemporary super-resolution approach, learning-based methods provide some promising results according to reported experiments. In [3], Chang et al. adopted the philosophy of Locally Linear Embedding (LLE) from manifold learning to recover the high-resolution image given its low-resolution counterpart as input. It is worth mentioning that Yang et al. [37] proposed a learning based super-resolution scheme based on the sparse representation and Gao et al. [13] implemented the sparse dictionary learning for image super-resolution by using the-state-of-the-art Restricted Boltzmann Machine (RBM). Fundamentally all the mentioned approaches rely on the assumed linear (maybe locally linear) relationships between the LR and HR pairs. For better exploiting the underlying context information of image, Yang et al. [39] modeled the relationship between the HR and LR patches by learning similar textural context for image sparse representation. In order to enhance the performance of image restoration (IR), Dong et al. [10] introduced two adaptive regularization terms, i.e., piecewise autoregressive (AR) model and non-local (NL) self-similarity, into the adaptive sparse representation framework. To further improve the capability of sparse representation based IR, the authors [9] proposed the concept of sparse coding noise and recast the IR goal into how to suppress the sparse coding noise.On the other hand, other researchers also proposed SR techniques by exploiting nonlinear relations, for example, the Gaussian process (GP) based regression models have been utilized to learn such nonlinear mappings at pixel levels [16]. Although the resulting HR image is pleasing, there exists huge computational overhead because all the Gaussian models have to be re-calculated for each pixel of each input image. To extend their work of [37], the authors proposed a bilevel optimization model for the coupled dictionary training under sparse representation framework [36]. For image SR, they considered the case where the mapping function may take nonlinear forms. The experimental results showed that the new learning method outperform their old one, i.e., joint dictionary training method, both quantitatively and qualitatively.In the above techniques for learning the mapping function, the HR/LR image patches are all manually vectorized. Thus some important spatial information among pixels tends to lose in the vectorization process. To effectively exploit such spatial information, appropriate feature representation for image patches are desired. 2D tensor [1] is an effective representative for images without damaging pixel spatial relationships. Jia et al. [18] proposed a Bayesian framework to perform face image super-resolution for recognition in tensor space. Furthermore, to effectively explore the spatial local information and avoid the curse of dimensionality dilemma, Wu et al. [34] proposed a regression model in the tensorPCA subspace for face super-resolution reconstruction. They separately learn the tensor subspaces for the high-resolution images and low-resolution counterparts, however it is more desired to learn the matching relations between LR and HR images.Currently, tensor learning methods are drawing considerable attentions [15,41]. Motivated by the idea of using this popular tool, in this paper, we propose a generalized 2D tensor regression learning framework for single image super-resolution via learning tensor coefficients for HR and LR image patch pairs simultaneously. In light of the importance of the mapping function in the learning-based SR, the intent of our work will be understood explicitly since the SR quality largely depends on whether the mapping function can represent well the underlying relation between HR and LR pairs. Meanwhile it is well known that some patches in a natural image may redundantly occur many times not only within the same scale, but across different scales. This observation motivates us to better exploit the relationship between HR/LR patch pairs for image SR task. Different from the existing vectorizing-based methods, the pixel spatial information can be preserved well when image patches are represented as tensorial data. By imposing different constraint terms, we can obtain three kinds of 2D tensor regression learning task for image SR.Our main contributions are summarized as follows. Firstly, by taking advantage of tensorial representation, we propose a general 2D tensor regression learning framework to learn a mapping function for image super-resolution; and secondly, to stabilize the solution of the 2D tensor learning task, we further impose three different regularization terms, i.e., Orthogonal constraint, Squaredℓ2-norm constraint and Non-negative constraint, on the generic model, which can leverage the power of this combination for image super-resolution.The remainder of the paper is organized as follows. In Section 2, a generalized 2D tensor regression learning framework is proposed to learn a mapping function for single image SR problem followed by a detailed description of algorithm in Section 3. In Section 4, we discuss how to apply the learned mapping function to the single image SR. The extensive experimental results for image SR and their analysis are reported in Section 5, where our results show the proposed methods are quantitatively and qualitatively competitive or even superior to the existing interpolation and learning-based SR approaches. Finally, the conclusion is drawn in Section 6.To review some related existing learning-based SR techniques and introduce our proposed model, we first borrow some useful notations for tensorial algebra that will be used throughout this paper. More specifically, matrices will be denoted by capital letters, e.g., X, vectors by boldface lowercase letters, e.g.,x, and scalars by lowercase letters, e.g., x. As for tensors, we denote it by Euler script calligraphic letters, e.g.X.In [15], Guo et al. considered the following linear regression model based on the tensor representation,y=X,W+b.Similar to the vectorial case, the inner product〈X,W〉is defined as the sum of elementwise products of two tensorsXandWin the same size. In least square learning, we may assumeb=0if we centralize all the training data by removing their mean. In this paper, we are particularly interested in the case of 2D matrix tensorsXandW. In particular, we consider the following bilinear modely=uXvT+bwhere u and v are two parameter vectors. To apply this model in our SR setting, we need generalize the above bilinear model to suit image patch outputs.To efficiently utilize 2D visual data in the SR setting, Wu et al. [34] proposed a regression model over the tensorPCA subspaces [1]. In their approach, an HR tensor subspace and an LR tensor subspace are learned from the training HR tensorsX={Xi}i=1Nand their LR tensorsY={Yi}i=1N, respectively, under the PCA criterion, i.e., maximizing the tensor variances. In fact, each defines a mapping from a higher dimensional 2D tensor (an image patch in HR) to a lower dimensional 2D tensor given byY=UXVT, where U and V are the left projection matrix and the right projection matrix, respectively. After two sets of tensorPCA subspace projection for HR tensor and LR tensor pairs are obtained, a co-occurrent linear model for the relationship between the HR and its LR tensorPCA subspaces is trained [34]. More specifically, an approximate conditional probability model was applied for the tensor subspace coefficients and the maximum-likelihood (ML) estimator gives an ordinary linear regression model.Motivated by [15,34], in this section, we focus on how to train the mapping function between LR and HR patch pairs by using tensorial data and then propose a generalized 2D tensor regression learning framework. Without loss of generality, we assume that there exists a set of HR and LR image patch pairs, denoted by{(Xi,Yi)}i=1N, where eachXi∈Rm×nandYi∈Rp×q. The dimensions satisfym>pandn>q.The traditional learning-based super-resolution methods usually focus on modeling the relationship between HR image patch{Xi}i=1Nand its counterpart LR{Yi}i=1Nby exploiting priors of specific images [6,7,24,37]. However, these models often treat each image patch as a single feature vector, thus the pixel spatial relations tend to lose in this conversion. As referred in [34], the vectorization of data degrades the underlying structural information due to losing spatial localization ability. Moreover, vectorizing a tensor data often leads to an algorithm suffering from the curse of dimensionality dilemma. Actually, tensors can often be considered as a more natural representation of visual data, as observed in [41].Different from the method [34] assuming a linear regression model on the learned tensor subspace, we directly regress HR image patches{Xi}i=1Nover LR inputs{Yi}i=1Nwith a multiple tensor regression learning model. Our proposed model can be formulated as follows,(2)Xi=UYiVTwhere U and V are the left and the right parameter matrices, respectively. It seems that the proposed model is contradictory to intuition by using less information (LR) to prediction more information (HR), however there exists an implicit relation betweenXiandYiwhich admits a possible recovery of the model. Meanwhile, the model still defines a linear relation between{Xi}(output) and{Yi}(input) and the coefficients/weights are constrained though. It is this constrained condition that enforces the model to learn the information among the spatially related pixels. We can see this from the following example. Consider a pixelxijof HR patch X. Under model (2), we can writexijas followsxij=uiYvj=∑kluikvljyklwhereui=(ui1,…,uip)is the ith row ofU,vj=(v1j,…,uqj)Tis the jth column of V andykls are pixels of image patch Y. In the above bilinear relation, the coefficient ofyklhas been decomposed into two partsuikandvljwhich capture row and column relation information independently.In fact, if we vectorize bothXiandYi, then (2) is equivalent to the following structured linear model,vec(Xi)=(U⊗VT)vec(Yi)where the overall coefficient matrixW=U⊗VTis composed of two separate matrices U and V.⊗means the matrix Kronecker product. In other words, the generic LR image formulation model (1) is defined withH=W+=(U⊗VT)+whereW+is pseudo-inverse of W.Inspired by the advance of tensor regression learning [15], we consider a generalized 2D tensor learning task below. Given a training set{Xi,Yi}i=1N, our purpose is to find two appropriate parameter matrices U and V such that the following criterion is satisfied(3)minU,V∑i=1NXi-UYiVTF2+JU(U)+JV(V),where·Fis the matrix Frobenius norm defined asXF2=∑i=1m∑j=1nxij2. FunctionalsJU(U)andJV(V)are regularization terms, which are applied to enforce certain application-dependent characteristics of the optimal solution.To obtain stable parameter matrices U and V, we propose the following regularized version, named as the Orthogonally Constrained Learning Model:(4)minUTU=I,VTV=I∑i=1NXi-UYiVTF2,where I is the identity matrix. The explicit motivation for this regularization is to regard U and V as projection operator (or bases) along the row and column directions.In the next section, we will propose an algorithm to solve problem (4). Once U and V have been learnt, the HR resolution image patch X can be easily worked out from the low-resolution image patch Y. Finally the whole HR image can be recovered by averaging over those corresponding HR patches given by the model.Remark 1Here we would like to stress the difference between our learning model and the tensor regression learning presented in [15]. In our case, rather than the scalar output in [15,41], the regression output is an image patch. Indeed, our new model can be regarded as a multi-task tensor regression learning [35] as all the pixels will share the same set of 2D tensors. Under such a constraint, it is likely for a model concerning 2D spatial information to perform better, thus as a consequence in our case it is likely that a better model relationship between HR patches and LR inputs can be learned.The above orthogonally constrained SR model places strong regularized conditionsUTU=IandVTV=Iover U and V, respectively. Thus (4) is an optimization problem over Stiefel manifold [31]. We can relax the orthogonal conditions by regularizing the norms of U and V. Here, we setJU(U)=∑i,juij2,JV(V)=∑i,jvij2. This can be implemented with an unconstrained regularized version by introducing Lagrangian multipliers,(5)minU,V∑i=1NXi-UYiVTF2+λ1‖U‖F2+λ2‖V‖F2.We call model (5) the Regularized Learning Model. In this model, we impose the squaredℓ2-norm onto the variables in order to find a smooth solution for (3). Since the regularized SR model (5) is an unconstrained optimization problem, it is relatively easy to solve it by using a standard optimization algorithm, such as the gradient descent algorithm [4]. We discuss algorithms for this model in the next section.As both U and V in (2) can be considered as up-sampling and filtering, we can propose to find out U and V with only positive elements by imposing non-negative constraints on both of them.(6)minU⩾0,V⩾0∑i=1NXi-UYiVTF2,whereU⩾0andV⩾0mean all the matrix elements are non-negative. The optimization problem (6) is termed as the Non-negative Learning Model. In fact, the non-negative SR model gives a more natural interpretation that an HR image can be recovered by a positively additive combination of low-resolution image patches.Non-negative Matrix Factorization (NMF) [20,26] is a useful tool to find a suitable representation of data, which typically makes latent structures for the given data. Applications of NMF are broad, including image processing [40], text data mining [2,23], face recognition [28], subspace learning [38], etc. To find such a part-based subspace, NMF is formulated as the following optimization problem,minU⩾0,V⩾0X-UVF2,where X is the data matrix, U is the basis matrix and V is the coefficient matrix. The inequalities are referred to as bound constraints since the variables are lower-bounded in this case. In our context here, model (6) imposes non-negative constraints on the parameter matrices U and V, which leads to a part-based HR image representation since they allow only additive, not subtractive, combinations of the LR patches data. To solve the NMF problem, researchers proposed many algorithms and variants in recent years [8,19,20,26]. The optimization problem for NMF is convex with respect to one matrix U or V while fixing the other, however, it is not convex in both simultaneously. Paatero et al. [26] presented a gradient algorithm for this optimization, whereas Lee and Seung [20] provided a multiplicative update rule that is somewhat simpler to implement and also showed good performance. Although it is simple to implement, this algorithm even does not guarantee to converge to a stationary point. To tackle this drawback, Kim and Park [19] proposed the alternating non-negative least squares (NNLS) algorithm for which the convergence to a stationary point has been proved. Meanwhile, Ding et al. [8] extended the traditional NMF to semi-NMF which removes the non-negative regularization on the data X and basis matrix U. The authors applied a gradient-descent-based update rule to efficiently solve the semi-NMF problem.Similarly, model (6) can also be tackled by using projected gradient descent methods [21]. In this paper, we propose to use alternative coordinate algorithms to solve the problem. Details of the algorithm for model (6) is presented in the following section.In this section, we detail how to solve the 2D tensor-learning SR optimization problems along with different constraints on the parameter matrices U and V.First we re-write the objective function (4) as follows,(7)∑iXi-UYiVTF2=∑itrXi-UYiVTXi-UYiVTT=∑itrYiTYi+trXiTXi-2trUYiVTXiwheretr·is the trace function. Since{Xi}and{Yi}are constants, the above problem is equivalent to(8)maxUTU=I,VTV=I∑itrUYiVTXiTNote that problem (8) is not jointly convex over both variables U and V, but it is convex with respect to either one variable while the other is fixed. For example, when V is known and fixed, optimization problem (8) is actually a well-posed problem that could be efficiently solved [32],(9)maxUTU=I∑itrUYiVTXiT=maxUTU=ItrU∑iYiVTXiTLetM=∑iYiVTXiT. Suppose M has its SVD computed bysvdM=UˆΣVˆT. Then (9) can be written astrU∑iYiVTXi=trUUˆΣV^T=trV^TUUˆΣDenote byZ=V^TUUˆ, then problem (9) is finally recast as follows,(10)maxUTU=ItrZΣAsΣis diagonal and Z is orthogonal, we havetr(ZΣ)=∑iziiσi. Hence the solution to the above problem in terms of Z should be given byZ=I. Therefore, the solution U is given byU∗=VˆIU^T, where[U^,Σ,V^]=svd∑iYiVTXiT.Alternatively, fixing the variable U, we can obtain the solution ofV∗=V^IU^T, where[U^,Σ,V^]=svd∑iYiTUTXi. Finally, the main procedure of the optimization algorithm for Orthogonally Constrained Learning Model can be summarized in Algorithm 1.Algorithm 1Training orthogonally constrained learning model.Require:{Xi,Yi}i=1NEnsure:U∗,V∗1: Initialization:V0is set as a rand matrix;tol= 1e-5;2: While not convergedk=1,2,…do3:[U^,Σ,V^]=svd∑iYiVk-1TXiT,4:Uk=VˆIU^T,5:[U^,Σ,V^]=svd∑iYiTUkTXi,6:Vk=V^IU^T.7: check convergence:8:if∑iXi-UkYiVkTF2<tolthen9:break;10:end if11: end whileNext, we shift our attention to the Regularized Learning Model(5) and give its algorithm to find the optimal solution. The problem is an unconstrained optimization problem and the objective function is quadratic in one variable while fixing the other. Hence it is easy to use an alternative coordinate method to solve it.Supposed that U is known, we letY∼i=UYiand the problem (5) is recast as,L(V)=argminV12∑iXi-Y∼iVTF2+λ22VF2Taking the derivative ofL(V)w.r.t V and setting the derivative to zero, we have,∂L(V)∂V=∑iY∼iVT-XiTY∼i+λ2V=0Then the solution forV∗is given by,(11)V∗=∑iXiTY∼i∑iY∼iTY∼i+λ2·I-1whereIis an identity matrix with appropriate dimensions.Alternatively, fixing V and lettingY∼i=YiVT, we can obtain the following formulation as well.L(U)=argminU12∑iXi-UY∼iF2+λ12UF2Similarly, we take the derivative ofL(U)w.r.t U and set the derivative to zero, then we have∂L(U)∂U=∑iUY∼i-XiY∼iT+λ1U=0The optimal solution forU∗is obtained as,(12)U∗=∑iXiY∼iT∑iY∼iY∼iT+λ1·I-1Alternatively, after updating U, one may iterate Eq. (11) and then Eq. (12) until a convergence criterion is satisfied. In summary, the algorithm for Regularized Learning Model(5) is described in Algorithm 2.Algorithm 2Training Regularized Learning Model.Require:{Xi,Yi}i=1NEnsure:U∗,V∗1: Initialization:U0is set as a rand matrix;tol= 1e-5;2: While not convergedk=1,2,…do3: fixing U and settingY∼ik=Uk-1Yi, update V by4:Vk=∑iXiTY∼ik∑i(Y∼ik)TY∼ik+λ2·I-1,5: fixing V and settingY∼ik=YiVkT, update U by6:Uk=∑iXi(Y∼ik)T∑iY∼ik(Y∼ik)T+λ1·I-1,7: check convergence:8: if∑iXi-UkYiVkTF2<tolthen9:break;10:end if11: end whileFor problem (6), it is well known that the objective is non-convex. Therefore it is impractical to provide an algorithm of seeking the global minimum solution of the problem. Here we propose an iterative algorithm which can achieve a local minimum.Firstly, initializing V with non-negative values, we letY∼i=YiVT. Then, problem (6) is reformulated as follows,L(U)=argminU⩾0∑i=1NXi-UY∼iF2,Alternatively, fixing U and lettingY∼i=UYi, then let V be updated by,L(V)=argminV⩾0∑i=1NXi-Y∼iVTF2Now, our task is to iteratively solve these two subproblems, termed as alternating non-negativity least squares (ANLS) problems, until a convergence criterion is satisfied. The solutions are represented asU∗=∑iXiY∼iT∑iY∼iY∼iT-1whereY∼iis equal toYiVT, andV∗=∑iXiTY∼i∑iY∼iY∼iT-1whereY∼iis equal toUYi.In the cases that the normal matrices∑iYiVTVYiTor∑iUYiYiTUTbecome very ill-conditioned, the above two solutions can be achieved by using the regularized pseudo-inversion, that is,(13)U∗=max0,∑iXiY∼iTα·I+∑iY∼iY∼iT+whereY∼i=YiVT. Similarly(14)V∗=max0,∑iXiTY∼iα·I+∑iY∼iY∼iT+whereY∼i=UYiand(·)+is pseudo-inverse operator.αin (13) and (14) is a regularization parameter introduced for the algorithm to avoid from getting stuck in local minima. The value ofαis adjusted in the k-th iterative step in our algorithm asαk=α0·exp-k/ρIn our paper,α0andρare empirically set 20 and 10, respectively.In optimization procedure, both U and V are updated alternatively. That is, one may iterate Eq. (14) and then Eq. (13) until a convergence condition is met. In summary, the algorithm for Non-negative Learning Model(6) is described in Algorithm 3.Algorithm 3Training Non-negative Learning Model.Require:{Xi,Yi}i=1NEnsure:U∗,V∗1: Initialization:U0is set as a rand matrix with positive value;tol= 1e-5;α0=20;ρ=10.2: While not convergedk=1,2,…do3: fixing U and settingY∼ik=Uk-1Yi, update V by4:Vk=max0,∑iXiTY∼ikαk·I+∑iY∼ikTY∼ik+,5: fixing V and settingY∼ik=YiVkT, update U by6:Uk=max0,∑iXiY∼ikTαk·I+∑iY∼ikY∼ikT+,7: updatingαk+1,αk+1=α0·exp-k/ρ.8: check convergence:9:if∑iXi-UkYiVkTF2<tolthen10:break;11:end if12: end whileIn this section, we discuss how to perform the patchwise SR recovery by the learned mapping function. At the training stage, we collect a large number of HR and LR image pairs to efficiently learn the relationship between them. In order to accurately represent the mapping, we first classify each training patch pairs into a certain cluster. Specifically, we apply a high-pass filter to each HR patch to output the feature for clustering similar to [9,10]. Given{Xi,Yi}i=1Npatches, we categorize the dataset into K clusters. And the mapping function for each of the K clusters is learned by using the above three 2D tensor learning model (4)–(6).After training each 2D tensor learning model, we obtain the parameter matrices U and V for subsequent SR processing. For simplicity, we first interpolate the input LR image into the size of desired HR one with Bicubic method, and then separate the initial HR image into a set of overlapping patches with the same size as in the phase of learning the mapping function. Then, these image patches are also categorized into K clusters. In terms of the mapping function belonging to each cluster, the most suitable mapping function can be selected for each given low-resolution patch. Given each LR image patchYi, we remove its dc component and then recover its HR counterpartXiby computingXi=UYiVT. And then the mean value is added into the HR counterpart. Then, we can efficiently recover HR image via tiling those HR patches together, where the average of multiple estimates is taken for each pixel in the overlapping region. Since the obtained HR image may not satisfy the reconstruction constraint as (1), a back-projected process is required to be carried out to further refine the recovered HR image. In particular, the recovered HR image should be projected onto the solution space of (1) to correct the high-resolution image pixels in a back-projected way [17]. This process validates that the final recovered HR image is consistent with the input LR image.

@&#CONCLUSIONS@&#
