@&#MAIN-TITLE@&#
Predicting memorability of images using attention-driven spatial pooling and image semantics

@&#HIGHLIGHTS@&#
We examine the role of visual attention and image semantics in understanding image memorability.We propose an attention-driven spatial pooling strategy for image memorability.Considering image features from the salient parts of images improves the results of the previous models.We also investigate different semantic properties of images.Combining attention-driven pooling with semantic features yields state-of-the-art results.

@&#KEYPHRASES@&#
Image understanding,Image memorability,Visual saliency,Spatial pooling,Semantic features,

@&#ABSTRACT@&#
In daily life, humans demonstrate an amazing ability to remember images they see on magazines, commercials, TV, web pages, etc. but automatic prediction of intrinsic memorability of images using computer vision and machine learning techniques has only been investigated very recently. Our goal in this article is to explore the role of visual attention and image semantics in understanding image memorability. In particular, we present an attention-driven spatial pooling strategy and show that considering image features from the salient parts of images improves the results of the previous models. We also investigate different semantic properties of images by carrying out an analysis of a diverse set of recently proposed semantic features which encode meta-level object categories, scene attributes, and invoked feelings. We show that these features which are automatically extracted from images provide memorability predictions as nearly accurate as those derived from human annotations. Moreover, our combined model yields results superior to those of state-of-the art fully automatic models.

@&#INTRODUCTION@&#
We humans have an astonishing ability to rapidly perceive and understand complex visual scenes. When exploring parts of a city that we have never visited before, glancing at the pages of a magazine or a newspaper, watching a film on television, or the like, we are constantly bombarded with a vast amount of visual information, yet we are able to process this information and identify certain aspects of the scenes almost effortlessly [1,2]. We also have an exceptional visual memory [3,4] that we can remember particular characteristics of a scene with ease even if we look at it only a few seconds [5]. Here, what is being remembered is considered nothing like an identical representation of the scene itself but the gist of it [6,7]. Although there is no general agreement in the literature about the contents of this “gist”, the most common definitions include statistical properties of the scene such as the distributions of basic features like color and orientation, the structural information about the scene layout like the spatial envelope of Torralba and Oliva [8], and the image semantics such as existing objects and their spatial relationships.Interestingly, we can recall some images surprisingly well while some are lost in our minds. Put simply, not all images are equally memorable. Isola et al. [9] were the first to carry out a computational study about this phenomenon, the so-called intrinsic memorability of images. They devised a Visual Memory Game experiment and utilized Amazon's Mechanical Turk service to quantify the memorability of 2222 natural images (see Fig. 1). In the course of these experiments, a total of 665 participants were shown a sequence of images, each of which was displayed for 1s with a short gap in between image presentations. These subjects were then asked to provide a feedback any time whenever he/she thinks an identical image is displayed. By this setup, a memorability score for each image is calculated by the rate at which the subjects detect a repeated presentation of it. The authors showed that the memorability of an image is pretty consistent across subjects and under a wide range of contexts, which indicates that image memorability is in fact an intrinsic property of images. In addition, the authors explored the use of different visual features and interestingly showed that the intrinsic memorability of an image can indeed be estimated reasonably well by a machine. Since that seminal work, there has been only a few works that explore this difficult and interesting problem [10–14].Our first goal in this study is to explore the role of visual attention in understanding image memorability. We humans use attentional mechanisms to efficiently perform higher level cognitive tasks by focusing on small and relevant bits of the visual stimuli. Our intuition is that we are perhaps more likely to remember or forget an image depending on which parts of the image we focus more. To give an example, Fig. 2illustrates the function of visual attention in selecting important features from images. Suppose that we are exposed to these three natural images, each having different visual contents, i.e. different objects, scene characteristics. Our visual system focuses on certain regions that attract our attention as modeled here by a bottom-up saliency model. In this work we propose a visual attention-driven spatial pooling strategy to select important features from images. Our approach makes use of two complementary feature pooling schemes related to visual attention. First, we investigate selecting features from the most salient regions of the images determined according to a recently proposed bottom-up visual saliency model [15]. Our second scheme, on the other hand, considers a top-down definition of visual attention and employs an object-centric spatial pooling scheme. To our interest, a body of research in cognitive sciences promotes that attention plays an important role in understanding natural scenes and enhancing visual memory [7,16–19]. However, none of the previously proposed memorability models make use of any attentional mechanisms for feature selection, and only [11,13] use saliency maps but as additional image features.Apart from the global dense image features, some previous studies on image memorability [9–12] have also investigated the use of high-level semantic information about images. They consider objects-related features [9,12], presence of certain object and scene categories [9–11], and their attributes [10], which are all based on manual annotations produced by humans. Fig. 3illustrates some sample images from the MIT memorability dataset along with the semantic features that are manually collected from the human subjects [10]. As illustrated here, an image can be semantically represented in terms of objects, scene information and related attributes.In addition to our attention-driven feature selection strategy, our second focus in this study is to investigate the use of a diverse set of recently proposed semantic features which encode meta-level object categories [20], scene attributes [21], and invoked feelings [22] for predicting image memorability. Compared to the features considered in the former studies [9,10,12], these semantic features can be directly extracted from the images, eliminating the need for manual annotations. Using these features thus decreases the complexity of the prediction process and makes the prediction model to work in a fully automatic manner. Moreover, compared to prior work, these features encode semantic properties of images from a perspective or scale that has not been investigated before. The Meta-class descriptor [20] encodes image semantics based on a hierarchical structure of object categories (concepts) by capturing the relationships among them. The SUN Scene Attributes [21] represents an image by means of responses of a comprehensive list of attribute classifiers that relates to different scene characteristics such as affordances, materials and surface properties. The SentiBank features [22] are the responses of a set of classifiers trained to detect adjective–noun pairs (attributes—objects), and used to associate certain sentiments with images.In order to validate our approach, we performed a series of experiments on the MIT memorability dataset. To show the effectiveness of the attention-driven pooling strategy, we used the dense global features employed in [9], namely SIFT [23], HOG [24], SSIM [25] and we analyzed the gain when the features pooled over the salient regions are concatenated to the feature vectors obtained with spatial pyramid pooling [26]. Moreover, regarding our second goal, we performed experiments with the high-level semantic features [20–22] and tested their performances on predicting image memorability. Lastly, we compared our combined model, which uses both semantic features and dense global features pooled over salient regions and spatial pyramids, to the state-of-the-art models in the literature.Our main contributions are: (1) an attention-driven pooling approach to put special emphasis on the interesting parts of the images in the computations, (2) a systematic analysis of a diverse set of semantic features on predicting image memorability, and (3) experiments demonstrating that the combination of these ideas provides significant improvement over the existing fully-automatic models.

@&#CONCLUSIONS@&#
