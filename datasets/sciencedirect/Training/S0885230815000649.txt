@&#MAIN-TITLE@&#
Comparing human and automatic speech recognition in a perceptual restoration experiment

@&#HIGHLIGHTS@&#
Missing-data methods are evaluated in a perceptual restoration task.Human and automatic speech recognition performance are compared.Methods include a novel approach to cepstral-domain bounded marginalisation.

@&#KEYPHRASES@&#
Automatic speech recognition,Missing data,Observation uncertainties,Perceptual restoration,Uncertainty propagation,

@&#ABSTRACT@&#
Speech that has been distorted by introducing spectral or temporal gaps is still perceived as continuous and complete by human listeners, so long as the gaps are filled with additive noise of sufficient intensity. When such perceptual restoration occurs, the speech is also more intelligible compared to the case in which noise has not been added in the gaps. This observation has motivated so-called ‘missing data’ systems for automatic speech recognition (ASR), but there have been few attempts to determine whether such systems are a good model of perceptual restoration in human listeners. Accordingly, the current paper evaluates missing data ASR in a perceptual restoration task. We evaluated two systems that use a new approach to bounded marginalisation in the cepstral domain, and a bounded conditional mean imputation method. Both methods model available speech information as a clean-speech posterior distribution that is subsequently passed to an ASR system. The proposed missing data ASR systems were evaluated using distorted speech, in which spectro-temporal gaps were optionally filled with additive noise. Speech recognition performance of the proposed systems was compared against a baseline ASR system, and with human speech recognition performance on the same task. We conclude that missing data methods improve speech recognition performance in a manner that is consistent with perceptual restoration in human listeners.

@&#INTRODUCTION@&#
Human listeners have a remarkable ability to perceptually restore sounds that have been masked by noise. This ability is particularly evident in the perception of speech sounds. If spectral or temporal sections of a speech signal are removed, listeners perceive the resulting stimulus as distorted. However, when the removed speech regions are filled with loud additive noise, listeners commonly perceive the removed acoustic content as being present (Warren, 1970; Miller and Licklider, 1950). It therefore appears that perceptual restoration only occurs if there is sufficient evidence that the speech has been masked by additive noise (Bregman, 1990). Furthermore, perceptual restoration is accompanied by a benefit in terms of speech intelligibility; listeners are better able to repeat speech content when additive noise is introduced in the removed portions (Warren, 1984; Verschuure and Brocaar, 1983; Powers and Wilcox, 1977; Warren et al., 1997).Repp (1992) proposed that perceptual restoration theories could be categorised into those based on segregation or top-down completion. These two theories differ in the role that they ascribe to the additive noise. Top-down completion theories propose that restoration is an auditory illusion; the additive noise provides evidence that a speech sound has been masked, therefore activating an (illusory) phonetic percept that depends on the speech context (Bregman, 1990). In contrast, segregation theories explain restoration in terms of sound separation; some of the noise energy is used to reconstruct the missing speech at the acoustic level, with the remainder being attributed to an extraneous sound (Warren, 1984).Motivated by these findings, several authors have proposed computational models of perceptual restoration. Such models are useful both as a means of clarifying the underlying perceptual mechanisms, and also as components of automatic speech recognition (ASR) systems that are robust in noise. Perceptual restoration has been modelled within source separation systems using continuity of spectral change and fundamental frequency (Cooke and Brown, 1993; Masuda-Katsuse and Kawahara, 1999) and using a prediction-driven approach (Ellis, 1999; Srinivasan and Wang, 2005). These systems were evaluated by assessing the extent to which they could segregate acoustic mixtures and restore acoustic content that was masked by noise. Perceptual restoration has also motivated missing data approaches to noise-robust ASR (Cooke et al., 2001). These include techniques that recognise noise-corrupted speech based on observed, incomplete information, and approaches that restore unobserved clean speech information prior to recognition (Barker, 2012; Gemmeke and Remes, 2012).Despite the close link between missing data ASR and perceptual restoration, we are not aware of any previous work that directly compares human performance with a missing data system. Cooke (2006) compared human and missing data ASR performance in a consonant recognition task, but noise was added to the speech without prior deletion of speech information; hence his task was different to that used to demonstrate perceptual restoration, in which spectral or temporal gaps are filled by noise. A comparison of human and machine performance using stimuli that induce perceptual restoration is therefore the aim of the current paper. Such a comparison can suggest new developments of missing data ASR systems that might bring them closer to human performance; indeed, in the current study it motivated a particular approach to modelling the uncertainty of observed features.The system proposed here models unobserved clean speech information as a random variable whose full posterior distribution is used in the ASR task. This can be regarded as a top-down completion model, in that speech is recognised based on partial information and unobserved clean speech information is not restored prior to recognition. Two approaches are used to derive information from the occluding noise about the uncertainty in the underlying observations, namely bounded marginalisation (Cooke et al., 2001) and bounded conditional mean imputation (Faubel et al., 2009) with uncertainty propagation.We evaluate our missing data ASR system on a perceptual restoration task, in which the goal is to recognise speech utterances in which acoustic content has either been removed, or substituted with additive noise. First, we confirm that perceptual restoration occurs for human listeners using the speech material selected for our study, and that human speech recognition performance is higher in conditions where perceptual restoration occurs. Subsequently, we show that perceptual restoration also occurs in our missing data ASR system, and that this gives it a performance advantage compared to a baseline system when the speech signal is distorted. Finally, we report a direct comparison between the performance of human listeners and our missing data ASR system, on a subset of the speech material that was used in our listening test.The remainder of the paper is structured as follows. First, we present the missing data system in Section 2 and the perceptual restoration task in Section 3. The experiments that we conducted are then presented in three sections. Transcription tests conducted to evaluate listener performance, and ASR tests conducted to determine whether the proposed system exhibits perceptual restoration, are reported in Sections 4 and 5, respectively. A direct comparison between human and ASR performance is reported in Section 6. Our results are discussed in Section 7, and conclusions are presented in Section 8.Missing data processing includes components for mask estimation and missing data compensation. The former divides noise-corrupted speech data into reliable and unreliable features, whereas the latter compensates for unreliable information. The approach for mask estimation used in the current study is discussed in Section 2.1. The proposed system does not restore the unobserved clean speech features; instead, the unobserved clean speech features are modelled as a random variable whose posterior distribution is used in the acoustic model likelihood calculation. We calculate the distributions with bounded marginalisation and bounded conditional mean imputation which are introduced in Sections 2.2 and 2.3, respectively. Calculation of acoustic model likelihoods based on full distributions is discussed in Section 2.4.Missing data methods compensate for environmental noise that is additive in the time domain. The methods operate on observed speech and additive noise mixtures in a magnitude-compressed spectral domain, where additive noise can be modelled as an occluder (Nádas et al., 1989). The missing data front-end used in the current work operates on log-magnitude-compressed mel-spectral features. We denote the observed speech and additive noise mixture in channel d of time frame τ in the log-mel-spectral domain by Y(τ, d). According to the so-called log-max approximation (Nádas et al., 1989; Varga and Moore, 1990) the log-magnitude-compressed observations can be approximated as(1)Y(τ,d)≈max{X(τ,d),N(τ,d)},where X(τ, d) denotes the speech and N(τ, d) is the additive noise component. Since Y(τ, d)≈N(τ, d) when N(τ, d)>X(τ, d), additive noise behaves like an occluder. The additive-noise-dominated components are therefore referred to as unreliable observations and speech-dominated components as reliable observations.The current work focuses on top-down completion modelled with clean speech posterior distributions and observation uncertainties, and does not propose a solution to mask estimation. Instead, we assume access to the speech stimuli and additive noise stimuli used to construct the observed data, so that the reliable and unreliable components can be determined by comparing the speech and noise energy in each time-frequency region. Of course, separate speech and noise components would not be available in practice, and therefore a technique would be required to estimate the mask from the observed noisy speech only. A review of such mask estimation techniques is given by Cerisara et al. (2007).The baseline approaches in missing-data based noise-robust ASR are marginalisation and bounded marginalisation (Cooke et al., 1994, 2001). The present work uses bounded marginalisation. We assume that the observed features are represented in the log-mel-spectral domain and have been divided into reliable and unreliable components. The unobserved clean speech feature that corresponds to an observed feature component Y(τ, d) is modelled as a random variable ξ. Since the log-mel-spectral features used are non-negative, we know a priori that the unobserved clean speech features ξ≥0. Then, the observations provide information as follows. Since reliable observations are assumed to represent clean speech, for a reliable observation we set ξ=Y(τ, d). In contrast, an unreliable observation represents additive noise. Since the noise is assumed to be more intense than speech, the unobserved clean speech feature cannot exceed the observed value, ξ<Y(τ, d).A posterior distribution encompasses the observed and prior information: when Y(τ, d) is assumed unreliable, the clean speech posterior distribution in channel d in time frame τ is a continuous uniform distribution between 0 and Y(τ, d). Since a continuous uniform distribution between a lower bound a and an upper bound b has mean12(b−a)and variance112(b−a)2, the clean speech feature ξ corresponding to an unreliable feature component Y(τ, d) has mean and variance(2)υ(τ,d)=12Y(τ,d),(3)Δ(τ,d)=112Y(τ,d)2,where υ(τ, d) denotes the posterior mean and Δ(τ, d) the posterior variance in mel-spectral channel d in time frame τ. Since the acoustic models used in the current work are trained on normalised cepstral features, the clean speech posterior cannot be compared to acoustic models in the magnitude-compressed spectral domain as described by Cooke et al. (2001). Instead, the clean speech posterior in the log-mel-spectral domain is used to calculate the clean speech posterior in the acoustic model domain, and acoustic model likelihoods are calculated as discussed in Section 2.4. Previous work on cepstral-domain marginalisation has proposed a similar approach, in which cepstral-channel weights were used to suppress information from unreliable components (Häkkinen and Haverinen, 2001).Bounded marginalisation and bounded conditional mean imputation (BCMI) assume that the unobserved clean speech features ξ are constrained between 0 and Y(τ, d). The difference between bounded marginalisation and BCMI is that the latter utilises prior information about statistical dependencies between the clean speech features. The statistical dependencies are represented as a Gaussian mixture model (GMM) whose parameters are trained on clean speech data. A model trained on D-dimensional log-mel-spectral feature vectors captures dependencies between spectral channels, but does not model temporal dependencies between clean speech components. However, a GMM can capture short-term temporal dependencies if the features are processed in windows that span multiple time frames, as demonstrated in Remes et al. (2011) and González et al. (2013).The current work evaluates frame and window-based approaches to BCMI. When speech data is processed in multi-frame windows, we concatenate the D-dimensional feature vectors in T consecutive time frames into TD-dimensional vectorsy(t). The reliable feature components in window t are represented as subvectoryr(t) and the unreliable components as subvectoryu(t). The unobserved clean speech information that corresponds toy(t) is modelled as a TD-dimensional random variable ζ. Prior information on ζ is represented in the prior distribution p(ζ), which is a GMM trained on clean speech data, and an approximate clean speech posterior is calculated as follows. First, the reliable observationsyr(t) are used to calculate a conditional distribution p(ζ|yr(t)). This is a clean speech posterior distribution which does not take into account unreliable observations. The distribution is approximated with a normal distribution N(ζ|yr(t)) that has a diagonal covariance matrix. The calculations needed to determine p(ζ|yr(t)) and N(ζ|yr(t)) are described in (Remes et al., 2015).Since our acoustic model processes speech data in frames τ, rather than windows t, window-based distributions must be converted to frame-based distributions prior to recognition. In the current work, approximate frame-based clean speech posterior distributions are calculated based on window-based distributions N(ζ|yr(t)) as follows. The approximate posterior distribution N(ζ|yr(t)) models each feature component ζ(k) in window t as an independent random variable with mean m(t, k) and variance S(t, k), where k indexes the component. When features are processed in windows that overlap in time, a clean speech feature ξ in mel-spectral channel d and time frame τ is associated with several window-based posterior distributions that arise from the consecutive windows. Here, the posterior distribution is calculated as an average of the window-based posterior distributions. The distribution is normal, with a mean m′(τ, d) and variance S′(τ, d) that are calculated from the means m(t, k) and variances S(t, k) corresponding to the clean speech component in mel-spectral channel d and time frame τ.The posterior distribution associated with an unobserved clean speech feature ξ is now a normal distribution with mean m′(τ, d) and variance S′(τ, d). To encode the information that ξ is constrained between 0 and Y(τ, d), we box-truncate the approximate posterior distribution. Thus, the approximate posterior distribution associated with ξ becomes a truncated normal distribution. The distribution mean υ(τ, d) and variance Δ(τ, d) are calculated as(4)υ(τ,d)=m′(τ,d)+f(L)−f(U)F(U)−F(L)S′(τ,d),(5)Δ(τ,d)=S′(τ,d)1+L·f(L)−U·f(U)F(U)−F(L)−f(L)−f(U)F(U)−F(L)2,whereL=−m′(τ,d)/S′(τ,d)andU=(Y(τ,d)−m′(τ,d))/S′(τ,d). Here, m′(τ, d) denotes the posterior mean and S′(τ, d) the posterior variance prior to truncation. The probability density function of the standard normal distribution is denoted by f(Z), whereas F(Z) represents its cumulative distribution function. The means and variances are used to calculate the clean speech posterior in the acoustic model domain, and acoustic model likelihoods are calculated as described in the next section.Our approach models perceptual restoration as a top-down completion process. This means that the clean speech features are not restored at the acoustic level, but the available information is modelled as a clean speech posterior distribution. The information encoded in posterior distributions is communicated to the ASR system as proposed by Arrowood and Clements (2002) and Deng et al. (2005). We assume that the acoustic model states are modelled as GMMs whose components are indexed by m. A clean speech feature vector in time frame τ in the acoustic model domain is modelled as a random variablexthat follows a multivariate normal distribution with meanμˆ(τ)and diagonal covarianceΣˆ(τ). The likelihood that random variablexpertains to the acoustic model component m is calculated as(6)E{L(τ,m)}=∫N(x|μˆ(τ),Σˆ(τ))N(x|μ(m),Σ(m))dx,whereN(x|μˆ(τ),Σˆ(τ))is the clean speech posterior distribution associated with time frame τ and N(x|μ(m),Σ(m)) is the clean speech distribution associated with the acoustic model component m. We further assume that the covariance matricesΣˆ(τ)andΣ(m) are diagonal, in which case the convolution has a closed-form solution,(7)E{L(τ,m)}=N(μˆ(τ)|μ(m),Σ(m)+Σˆ(τ)).The modified likelihood calculation compares the mean of the clean speech posterior distribution to the acoustic model distribution with meanμ(m) and varianceΣ(m)+Σˆ(τ). Thus, the clean speech posterior mean is interpreted as a feature estimate, and the clean speech posterior variance is introduced into the acoustic model parameters as a dynamic variance component.The ASR system used in the current work operates on normalised cepstral features (see Section 5.1), whereas the clean speech posterior distributions discussed in the previous sections represent information in a log-mel-spectral domain. Therefore, in order to employ a posterior distribution given by Equations (2)–(3) or (4)–(5) in the ASR system, the distribution is used to calculate a posterior distribution for the clean speech features in the acoustic model domain. The clean speech posterior distribution in the acoustic model domain is modelled as a multivariate normal distribution with meanμˆ(τ)and varianceΣˆ(τ). The distribution parameters are calculated with piecewise uncertainty propagation (Astudillo et al., 2010), with system-related details as presented in our previous work (Remes et al., 2015).While the clean speech feature components are assumed to be independent in the log-mel-spectral domain, the feature transformations introduce correlation between components. Distribution parameters needed to model the correlations increase the computational cost in each successive posterior transformation, and thus make observation uncertainties an unattractive alternative compared to other noise-robust speech recognition frameworks. Here, we model the correlation introduced in the cepstral transformation but do not model the inter-frame correlation introduced by cepstral mean subtraction or the inter-frame and intra-frame correlation introduced by differential transformations. The correlations introduced by cepstral mean subtraction and differential transformations that operate on multi-frame windows are ignored to limit the increase in computational cost.The system described in the previous section was evaluated using a perceptual restoration task in which stimuli were constructed from clean speech utterances and additive noise. Perceptual restoration studies indicate that the restoration effect depends on acoustic cues that relate to the additive noise (Powers and Wilcox, 1977; Samuel, 1981; Warren et al., 1997) and context cues that relate to the speech material (Warren and Sherman, 1974; Verschuure and Brocaar, 1983; Bashford et al., 1992). The current section describes the stimuli and evaluation metric used in the present work. The stimuli include (i) read sentences in which certain spectral or temporal portions have been removed and (ii) read sentences in which additive noise has been introduced in the removed portions. Sections 3.1 and 3.2 discuss the characteristics of the additive noise, specifically the noise type and level. The stimuli used in the perceptual restoration experiments are then described in Sections 3.3 and 3.4, and the evaluation approach is discussed in Section 3.5.To construct noisy speech stimuli, we paired each clean speech utterance with a speech-shaped noise sample constructed as described below. Since similarity of spectral shape is known to enhance perceptual restoration (Samuel, 1981), a noise sample was produced for each utterance that matched its spectral shape. Our approach was motivated by the stimuli used in Warren et al. (1997), where the speech stimuli were presented in two narrow spectral channels that were equalised so that the peaks in each spectral band were within ±2dB(A) of the overall presentation level. This procedure effectively whitens the speech spectrum, so that the removed part between the narrow spectral channels can be replaced with bandpass-filtered white noise. In the present study, we chose to shape the frequency content of the noise to match the speech, rather than to equalise the spectral gains of the clean speech utterances. This ensured both that the speech retained its naturalness, and also that the spectral shape of the speech matched the acoustic models in our ASR system.In practice, white noise was shaped to each utterance as follows. First, the clean speech utterances were processed with a simple voice activity detection (VAD) algorithm to remove silence frames. The approach used an estimate of the speech energy envelope that was calculated as the absolute value of the speech waveform smoothed with a moving average filter. The moving average was calculated using a 380 ms triangular window. Envelope values which exceeded a threshold of 0.2 times the standard deviation of the envelope were considered active, and values below the threshold were discarded as silence. Active speech frames were used to estimate the spectral shape of the utterance, which was modelled as a second-order linear prediction (LP) filter. The model order was considered sufficient for capturing the broad spectral shape, while also guaranteeing that the utterance-dependent filters did not capture local spectral components such as formants that could bias the speech recognition results. The utterance-dependent filters were applied to white noise to construct utterance-dependent noise samples, which were then combined with the corresponding speech utterance as described below.Experiments conducted on human listeners indicate that the perceptual restoration effect depends on the level difference between the speech and additive noise stimuli (Powers and Wilcox, 1977; Warren et al., 1997). For this reason, we determined the average level in each clean speech utterance and scaled the additive-noise levels according to the estimated speech levels. The levels were calculated in MATLAB with a virtual sound level meter (Lanman, 2006). The levels were determined on the dB(A) scale, and the average speech level in each utterance was calculated based on the active speech frames, detected as proposed in the previous section. Then, the speech and noise pairs were processed as follows. To prepare data for human listeners, we determined the sound pressure level of a reference noise sample and scaled each clean speech utterance to the same level. Noise samples were then scaled with respect to the speech level. The utterances used in ASR experiments, on the other hand, were not normalised to a specific sound pressure level, but the noise samples were scaled with respect to the speech level in each utterance. This means that the level difference between each speech and noise pair was fixed, and the exact same level differences were used in human and ASR experiments when experiments were conducted on the same speech and noise pairs. The only difference was that the speech level in each utterance was not fixed in the ASR experiments. The exact noise levels used in the human and ASR experiments are provided in Sections 4.3 and 5.3.The perceptual restoration experiments conducted in the current study include both spectral and temporal restoration. Experimental data was prepared based on the clean speech and noise pairs constructed and adjusted as described in the previous sections. For the spectral restoration experiments, the clean speech utterances were filtered with a 1458-point finite impulse response (FIR) bandstop filter with linear phase and >1000dB/octave attenuation in the transition bands. The 3dB cutoff frequencies were 472Hz and 5680Hz, and minimum attenuation in the stopband between 505Hz and 5650Hz was 80dB. The additive noise used in the spectral restoration task was constructed in a similar manner. The noise samples paired with the utterances were filtered with a 1457-point FIR bandpass filter with a linear phase and >1000dB/octave attenuation in the transition bands. To avoid masking between the speech and noise stimuli, the bandstop and bandpass filter cutoffs were separated at 6dB cutoff frequencies by one equivalent rectangular bandwidth (ERB) (Edmonds and Culling, 2005). The 3dB cutoff frequencies in the bandpass filter were 560Hz and 5030Hz, and the minimum attenuation in the stopbands below 526Hz and above 5060Hz was 80dB. To construct the speech and noise stimuli used in perceptual restoration experiments, the filtered speech and noise stimuli were added as illustrated in Fig. 1.Interrupted speech stimuli used in temporal restoration experiments were constructed from alternating 200 ms speech and silence segments or 200 ms speech and noise segments. Raised cosine ramps of 10 ms duration were applied to the onset and offset of each segment, and the interrupted speech and noise stimuli were added to construct interrupted speech with additive noise (Fig. 2). We also informally tested 50 ms and 100 ms interruptions, but listening to the stimuli indicated that – while the interrupted speech sounded discontinuous without additive noise – there was no reduction in human speech recognition performance. The same observation has been reported in previous studies (Miller and Licklider, 1950; Powers and Speaks, 1973).

@&#CONCLUSIONS@&#
Previous studies indicate that listener performance in perceptual restoration tasks improves as additive-noise level increases, and reaches an optimal level when the noise is not too intense. In the current work, the same trends were observed in the performance of a missing data ASR system. In addition, direct comparison between the missing data system and human listeners on the same utterances indicated that the performance of the two was correlated. A baseline ASR system performance did not correlate with listener performance evaluated in the current or previous studies; we therefore conclude that the missing data methods and observation uncertainties improved the ASR system performance in the perceptual restoration task.While the proposed missing-data approach improved system performance in perceptual restoration task, our results also suggested that the missing data system does not utilise available semantic and syntactic context to their full extent. Future experiments are needed to evaluate system performance on isolated words, and the performance on isolated words and complete sentences must be compared in order to determine whether the additional context information available in complete sentences improves system performance. Since previous studies indicate that semantic context is important to human listeners in the perceptual restoration task, future experimenters may wish to evaluate top-down completion approaches with a whole-sentence language model (Rosenfeld et al., 2001).