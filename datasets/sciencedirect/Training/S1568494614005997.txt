@&#MAIN-TITLE@&#
Modeling glass-forming ability of bulk metallic glasses using computational intelligent techniques

@&#HIGHLIGHTS@&#
Various intelligence approaches are employed to model the Dmax for GFA.The improved R2-values are found using the proposed CI approaches.The performances of CI approaches are compared to the existing modeling approaches.

@&#KEYPHRASES@&#
Glass forming alloys,Bulk metallic glasses,Maximum section thickness,Support vector regression,Artificial neural network,General regression neural network,

@&#ABSTRACT@&#
Modeling the glass-forming ability (GFA) of bulk metallic glasses (BMGs) is one of the hot issues ever since bulk metallic glasses (BMGs) are discovered. It is very useful for the development of new BMGs for various engineering applications, if GFA criterion modeled precisely. In this paper, we have proposed support vector regression (SVR), artificial neural network (ANN), general regression neural network (GRNN), and multiple linear regression (MLR) based computational intelligent (CI) techniques that model the maximum section thickness (Dmax) parameter for glass forming alloys. For this study, a reasonable large number of BMGs alloys are collected from the current literature of material science. CI models are developed using three thermal characteristics of glass forming alloys i.e., glass transition temperature (Tg), the onset crystallization temperature (Tx), and liquidus temperature (Tl). The R2-values of GRNN, SVR, ANN, and MLR models are computed to be 0.5779, 0.5606, 0.4879, and 0.2611 for 349 BMGs alloys, respectively. We have investigated that GRNN model is performing better than SVR, ANN, and MLR models. The performance of proposed models is compared to the existing physical modeling and statistical modeling based techniques. In this study, we have investigated that proposed CI approaches are more accurate in modeling the experimental Dmax than the conventional GFA criteria of BMGs alloys.

@&#INTRODUCTION@&#
Due to the superior physical and chemical properties compared to their crystalline counterparts, bulk metallic glasses (BMGs) have attracted a lot of attention of the researchers and engineers. Owing to their excellent properties, BMGs find a number of important and exciting applications in sporting goods, jewelry, medical, surgical instruments, electronics, defense, and aerospace [1]. In the last two decades, different families of amorphous alloys with excellent glass forming ability have been presented in the literature. For example, La-, Zr-, Pd-, Mg-, Cu-, Fe-, Ti-, Pr-, Co-, Ca-, Y-, Au-, Hf- and Gd-based metallic glasses have shown superb glass-forming ability [2]. However, there exist no scientific rules or justified theories to design bulk metallic alloys with excellent GFA. To date, GFA is one of the hot issues ever since metallic glasses are discovered [3]. In the past few years, the glass-forming ability (GFA) of these alloy systems, which is often described in terms of maximum section thickness (Dmax), has been increased from 1mm to several centimeters. The accurate estimation of Dmax highly depends on the fabrication conditions. The critical cooling rate (Rc) is another quantitative measure of GFA, which is the minimum cooling rate to get completely amorphous solid from melts. The larger GFA of alloys can be expected for the smaller Rc or higher Dmax. In other words, the accurate values of Dmax and Rc of each metallic glass could only be obtained after the fabrication of metallic glass through the laborious experimentation. Therefore, it is imperative to uncover a more effective criterion for evaluating the GFA of BMGs. In order to make BMGs cost effective for various engineering and industrial applications, it would be very useful if we could model Dmax prior to its fabrication [4].The scientist and engineers have proposed various physical and statistical modeling based approaches to predict GFA. They developed various GFA criteria using thermal parameters which highly effect the GFA of metallic glasses [2]. These criteria are mostly based on three thermal characteristics of glass forming alloys i.e., glass transition temperature Tg, the onset crystallization temperature Tx, and liquidus temperature Tl. These parameters can easily be determined from differential thermal analysis (DTA) and differential scanning calorimetry (DSC). These physical and statistical techniques model experimental Dmax with thermal parameters/characteristics of the glass-forming alloys. For example, the reduced glass transition temperature Trg=Tg/Tl[5], the supercooled liquid range ΔTx=Tx−Tg[6], γ=Tx/(Tg+Tl) [7], γm=(2Tx−Tg)/Tl[8], ΔTrg=(Tx−Tg)/(Tl−Tg) [9], δ=Tx/(Tl−Tg) [10], Φ=Trg(ΔTx/Tg)0.143[11], α=Tx/Tl[12], β=Tx/Tg+Tg/Tl[13], β1=Tx×Tg/(Tl−Tx)2, and ω=Tg/Tx−2Tg/(Tg+Tl) [14]. However, due to the simple foundation of physical/statistical models [5–14], it is difficult to model Dmax accurately for the synthesized glass-forming alloy. In fact, there exists a complex nonlinear relationship between the GFA and its thermal parameters. These models compromise on accuracy for correlating Dmax of BMGs. Keeping in view this fact, there is ever increasing demand for the advanced computational intelligent (CI) techniques. We have employed these CI techniques to model the experimental Dmax (instead of Rc) in terms of thermal parameters. Because the values of Rc are difficult to be measured. The measurement of Rc involve a series of continuous cooling experiments involving heat transfer calculations that makes it quite tedious and a costly job. On the other hand, the values of Dmax can be measured easily. It is most widely used indicator of glass forming-ability of metallic glasses. We have prepared a dataset of thermal parameters of BMGs alloys along with Dmax values from the literature of material science. CI techniques can model a complex function, like Dmax, by selecting useful information from the input training alloys, and then predict Dmax for novel alloys.Previously, support vector regression (SVR), general regression neural network (GRNN), artificial neural network (ANN), and multiple linear regression (MLR) based CI techniques are used in predicting the lattice structure of perovskites compounds [15,16]. These techniques provide an efficient alternative in modeling material [17], monitoring structures [18], fabrication [19], chip designing [20], and vibration modeling [21]. For example, ANN based computing approach is tried to model undercooled liquid region of metallic glasses [4]. ANN approach is used to model the crystallization temperatures of Ni–P based amorphous alloys. Promising good results are obtained for the crystallization peak temperature [22]. The main advantage of CI techniques is that once an efficient model is built for training samples then they can be successfully used to predict the structure of novel samples. It is of practical interest to employ CI techniques in less resources/skill environment.These techniques have gained much importance in predicting various structural properties of materials [15,16]. These approaches have ability to reduce the experimental and temporal cost. ANN based computational models are inspired from the functional aspects of the biological neural networks. Sometimes, ANN based models are trapped in local minima [16,22] and their performance may be degraded as compared to statistical learning theory based SVR models. SVR models use the Lagrange constraints optimization and Mercer theorem to extract useful information from input data [23,24]. Due to simplicity, GRNN approach can extract suitable information for developing regression model.During model developing phase, the optimal parameters values are computed using the training data. Next, the developed model is used to predict Dmax of novel glass alloys of the same family. The performance of CI models is reported in terms of percentage relative absolute difference (PAD) error and correlation coefficient R. GRNN model has revealed an excellent tendency in modeling the current GFA problem. The mean PAD errors are computed to be 1.971% and 3.681% for training and novel BMGs, respectively. However, SVR model has given the mean PAD errors of 2.700% and 4.525% for training and novel BMGs, respectively. ANN model has given mean PAD values of 3.817% and 3.676% for training and novel alloys, respectively. MLR models have developed linear relationship of thermal parameters with experimental Dmax. This model has given relatively higher mean PAD error of 3.656% and 4.895% for training and novel alloys, respectively. The overall performance in terms of PAD error for GRNN, SVR, ANN, and MLR models are found to be 2.750, 3.531, 3.753, and 4.220, respectively. This highlights the best correlation of GRNN models among CI models. We also underlines improved performance of GRNN model in comparison to existing models based on the physical/statistical techniques.The rest of paper is organized such that in Section 2, a brief description of input data collection is given. In this section, it is explained how CI models are developed. In Section 3, simulated results of the proposed models are described and analyzed with the conventional GFA approaches. Finally, conclusions are provided in Section 4.Glass formation is a competing process between liquid phase and the resulting crystalline phases. The glass formation of the molten alloy would be favored if the liquid phase is stabilized on cooling and the competing crystalline phases become difficult to precipitate out. Since it is quite hard to study the cooling process of metallic glasses, few characteristic temperatures upon heating are often used for predicting GFA. These temperatures include the glass transition temperature Tg, crystallization temperature Tx and liquidus temperature Tl which reflect both thermodynamic as well as kinetic aspects of the alloy. It is well known that the glass transition and the crystallization procedure are strongly related to characteristic temperatures Tg, Tx and Tl. These thermal parameters are building blocks almost all the mathematically driven GFA criteria. The functional dependency of Dmax is developed using three thermal characteristics Tg, Tx, and Tl of metallic glass compounds. The experimental values of Dmax are used as dependent variables along with three independent variables as follows:(1)Dmax-pred=fCI(Tg,Tx,Tl)Block diagram for developing CI models is shown in Fig. 1. Detail description of developing CI models is given in Section 2.2. The performance of models is measured in terms of percentage relative absolute error (PAD) between experimental Dmax and predicted Dmax, defined as:(2)PAD=|Dmax-expt−Dmax-pred|Dmax-expt×100We retrieved input dataset of 349 BMGs containing La-, Zr-, Pd-, Mg-, Cu-, Fe-, Ti-, Pr-, Co-, Ca-, Y-, Au-, Hf- and Gd-based metallic glasses. The majority of the characteristic temperatures of glass alloys were measured using DSC and/or DTA at a heating rate of 20K/min [12]. On the other hand, experimental values of Dmax are obtained by measuring cross section of Cu mold cast samples. The detail information of thermal parameters Tg, Tx, Tl, and experimental Dmax is given in the supplementary Table 1. Out of 349 BMGs, 190 alloys are randomly picked for model selection and the remaining 159 alloys are used to report the testing performance of CI models. For comparative analysis, the CI models are estimated by forming examination data (combining training and novel samples) of 349 BMGs. SVR, ANN, GRNN, and MLR models are developed by normalizing the input and output parameters of BMGs in the range [0–1].SVR, ANN, GRNN, and MLR models are developed using four diverse types of techniques. During model development, the error between experimental Dmax and predicted Dmax is minimized. The description of each prediction models developed is given below.SVR models are based in the statistical learning theory [24]. The theoretical description of SVR models is well documented in the literature. Here, we will describe using the same conventional notations as reported in the literature. SVR models develop functional form by mapping the input data space x∈Rnto the output data prediction space y∈R using a pair of N data samples, S=(xi, yj), ∀i=1, …, N the input dataset is mapped into higher dimensional feature space through a nonlinear mapping Φ. The functional form of SVR model is estimated by mapping input features Φi(x) as:(3)y=∑i=1lwiΦi(x)+b,where i=1, …, l and l≤N indicates the number of support vectors required to model. The above equation can be solved by selecting suitable kernel function K(x, xi) according to the complexity in the input data i.e.,(4)f(x,α,α*)=∑i=1l(αi*−αi)K(x,xi)+b,where the coefficientsαi*and αirepresents the Lagrange multipliers. SVR model pick support vectors from training data in order to minimize the following regularized risk function R(w):(5)R(w)=1l∑i=1l|yi−f(xi,w)|ε+γ〈w,w〉,where(6)|yi−f(xi,w)|ε=0,if|y−f(x,w)|<ε|y−f(x,w)|−ε,otherwise,Here 〈w, w〉 and ɛ represent the dot product of two weight vectors and the insensitivity parameter, respectively. During training phase, the values of Lagrange multipliersαi*and αi, are determined by maximizing the following functional:(7)w(α*,α)=−ε∑i=1l(αi*+αi)+∑i=1ly(αi*−αi)−12∑i,j=1l(αi*−αi)(αj*−αj)K(xi,xj)with constraints(8)∑i=1lαi*−αi=0,0≤αi*,αi≤C,i=1,…,lThe trade-off parameter C represents the cost of constraint violation. The optimal value of this parameter was empirically determined. The supporting data points correspond to non-zero values ofαi*−αwere selected to construct the nonlinear prediction boundary of the SVR model. The most popular Gaussian kernel was selected using the LIBSVM program [25]. The values of cost function C, error function ɛ and the kernel width γ are optimized by using the grid search technique [26]. The optimal parameter values for three characteristic temperatures (i.e., Tg, Tx, and Tl) are found to be C=230, γ=9.5 and ɛ=0.02. The predicted values of SVR model, for 349 BMGs alloys, are given in supplementary Table 2.MLR model is developed using maximum section thickness, Dmax as a linear function of three characteristic temperatures Tg, Tx, and Tl. The coefficients of independent variables are computed for training data using ordinary least squares criterion, as follows:(9)yMLR=0.0626−0.4745Tg+1.0132Tx−0.5134TlThe numerical values of coefficients are slopes of the regression surface and the constant term indicates the intercept of the regression surface. Using this relationship, predicted values of Dmax are computed for novel and combined datasets, respectively.GRNN approach is an improved variant of feed forward neural network. The architecture is the combination of multilayer perceptrons and radial basis functions. GRNN is widely used in prediction problems due to its nonlinear mapping ability that enhances its approximation capabilities [27]. Here this approach is selected due to its nonlinear network structure, which is one pass learning network with high parallel structure. This model has four layer of processing units. To perform nonlinear regression specific computational functions are used. The first layer of the network, in which each predictor variable has its unique input neuron, collects the input information. The second layer gets the data from first layer and the number of neurons in this layer is same as those of training set due to which neurons in the second layer are known as pattern neurons. By processing the data in a systematic way, pattern neurons are used to memorize the relationship between input and the proper response. In third layer neurons are called summation neurons, receives the output of the pattern neurons. Third layer performs simple and weighted summations. The output of the network is the summation of the products of pattern neuron to the weights of the third layer.To minimize the prediction error an optimal trade-off is adjusted for the training data and the novel data. Over-training and under-training of the network is dependent on the value of Gaussian function parameter σ. For low values of parameter σ, the network is over-trained and prediction error would be higher for novel data, whereas for large value, the network is under-trained and the prediction error would increase for training data. The network reports its best results for the optimal value of spread parameter σ=0.02. The predicted Dmax values of GRNN model, for 349 BMGs alloys, are given in supplementary Table 2.A neural network consists of interconnected neurons and these neurons are arranged in layers and are differentiated as input, hidden and output neurons depending upon the layer in which these are located. The synaptic weights are adjusted according to the target. ANN modeling is based on the lining up of input and output data via a nonlinear basis functions. The procedure of ANN modeling involves collection of data, analysis and pre-processing of dataset, training of the neural network, testing of the trained neural network and using the trained network for simulation and prediction.Back propagation neural network (BPNN) has complicated multidimensional mapping property. Here, we have initialized BPNN with one input layer, two hidden layers and one output layer. The first and second hidden layers contain two and three neurons, respectively. In training phase, back-propagation learning algorithm computes the error for the output neuron j as:(10)ej(t)=zj(t)−yj(t)where zjand yjare the actual and target output for neuron j during iteration t. Average squared error of the network is then computed as:(11)ξavg(t)=1n∑t=1nξ(t)whereξ(t)=(1/2)∑j∈pej2(t)shows the instantaneous sum of squared error and p indicates the number of neurons in the output layer. The average error ξavg is a cost function of the network. The purpose of the learning process is to minimize ξavg. The structure of neural network initialized for ANN model development is shown in Fig. 2.For better performance, the ANN model is development in two stages using Neural Network Toolbox of Matlab R2007 [28]. First, the network is trained using “trainlm” function that updates weights and bias values according to Levenberg–Marquardt optimization algorithm. This algorithm trains neural networks nearly 10–100 times faster than the usual gradient descent method. In the second stage, this network is further trained using training function “trainbr”. This function uses the Bayesain Regularization method to enhance the generalization of the network. This training algorithm updates the weights and bias values according to the Levenberg–Marquardt optimization. The data parameters normalized in the range 0–1. The predicted values of Dmax for ANN model are given in supplementary Table 2.

@&#CONCLUSIONS@&#
