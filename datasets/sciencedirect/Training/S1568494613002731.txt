@&#MAIN-TITLE@&#
A parallel hybrid optimization algorithm for fitting interatomic potentials

@&#HIGHLIGHTS@&#
Hybridization of PSO and Torczon's simplex algorithms.Supporting dynamic, irregular, nested parallelism.Parallel implementation on both shared memory systems and clusters of multicores.Application on an interatomic potential fitting problem.

@&#KEYPHRASES@&#
Interatomic potential,Hybrid global optimization,Particle swarm,Multidimensional search,Irregular task parallelism,Cluster programming,

@&#ABSTRACT@&#
In this work we present the parallel implementation of a hybrid global optimization algorithm assembled specifically to tackle a class of time consuming interatomic potential fitting problems. The resulting objective function is characterized by large and varying execution times, discontinuity and lack of derivative information. The presented global optimization algorithm corresponds to an irregular, two-level execution task graph where tasks are spawned dynamically. We use the OpenMP tasking model to express the inherent parallelism of the algorithm on shared-memory systems and a runtime library which implements the execution environment for adaptive task-based parallelism on multicore clusters. We describe in detail the hybrid global optimization algorithm and various parallel implementation issues. The proposed methodology is then applied to a specific instance of the interatomic potential fitting problem for the metal titanium. Extensive numerical experiments indicate that the proposed algorithm achieves the best parallel performance. In addition, its serial implementation performs well and therefore can also be used as a general purpose optimization algorithm.

@&#INTRODUCTION@&#
Computer simulation at the atomic level is a valuable tool in several scientific disciplines such as physics, chemistry and materials science. It has been successfully used for the understanding of various phenomena, the interpretation of experimental results and the prediction of physical properties. Although first principles methods have advanced considerably in speed and accuracy during the last decades, computations based on classical interatomic potentials is still the only feasible way for molecular dynamics or Monte Carlo simulations for systems with a very large number of atoms or for long simulation times.A variety of interatomic potentials for metals is available in the literature such as the Embedded Atom Method, the Modified Embedded Atom Method, the Finnis–Sinclair potential, and potentials based on the Second Moment Approximation of the Tight Binding theory (SMATB). For a review of the various potential models for metals and alloys see [1]. Here we describe the procedure for obtaining an interatomic potential for the metal titanium, using the SMATB formalism. Computing an interatomic potential reduces into a data fitting problem where the potential model is fitted against experimentally known physical properties. The resulting optimization problem is characterized by small dimensionality, large and varying runtime, discontinuities in the objective function and lack of derivative information.Recently the tremendous growth of parallel computing systems and programming techniques sparked the research on parallel global optimization algorithms, specifically for molecular biology and computational chemistry problems. The main reason is that the optimization of such complex systems involves many numerical calculations and a single function evaluation may be computationally expensive. In [2] the authors calculate minimum energy clusters of atoms using a parallel build-up algorithm based on simulated annealing. A similar problem is tackled in [3], where the authors present a parallel algorithm based on local searches. In [4,5] a parallel algorithm is proposed for the three-dimensional protein conformation problem. Both algorithms apply local searches inside a multistart framework, where random points are uniformly sampled in order to explore the domain. Parallel schemes involving a Particle Swarm Optimization (PSO) [6] variant and the Nelder–Mead Simplex method [7] were presented in [8]. In that article parallelization is achieved by using two separate processes that perform function evaluations and local search applications, upon request of a main process. In [9] the authors present a hybrid between a Genetic Algorithm (GA) and PSO. The hybridization is achieved by assigning the best half of the population to GA iterations and the worst half to PSO iterations. Since no local optimization algorithm is involved the parallelization is quite straightforward using a master–slave model. In the same spirit the authors in [10] present a hybridization of a tabu search and PSO that utilizes the analogy between the algorithm and the master–slave paradigm.In this work we present a hybrid Global Optimization (GO) algorithm designed to solve interatomic potential fitting problems that follow the SMATB formalism. The algorithm under study falls into the category of memetic algorithms and combines a Particle Swarm Optimization (PSO) variant [11] (global component) with Multidimensional Search (MDS) [12] (local component). The efficiency of this hybridization scheme was demonstrated in [13,14]. We decided to incorporate MDS since it is suitable for discontinuous functions and a perfect candidate for parallelization. Our basic incentive is to create a powerful hybrid optimization scheme with parallelizable components so that we can handle the large runtime of the potential fitting problem at hand.We describe in detail the parallelization capabilities and implementation details of our algorithm, using a task-based programming and runtime environment. Here, two levels of parallelism are being exploited in order to accelerate the method. The first parallelism issue is raised by the stochastic and irregular nature of task spawning. The GO algorithm is an iterative procedure but since we cannot foresee how many computational tasks may emerge at each iteration, the solution must be adaptive on the specific instance of the algorithm. The second issue comes from the fitting problem at hand. The interatomic potential involves many complex calculations and hence the execution time of the basic task varies greatly. By means of the recent OpenMP tasking model, we effectively tackle these issues on shared-memory systems. In order to extend this work on distributed memory cluster environments, we introduce a runtime system that implements a programming and execution environment for irregular and adaptive task-based parallelism on multicore clusters. This approach allows for a parallel implementation of the hybrid GO algorithm that runs efficiently on both shared and distributed memory architectures.The rest of this paper is organized as follows: Section 2 describes in detail the fitting procedure of the interatomic potential and introduces the objective function to be minimized. Section 3 gives a description of the hybrid GO algorithm. Section 4 discusses the parallelism issues of the hybrid GO algorithm and outlines the runtime system that will support it. In Section 5 we present an experimental evaluation of the proposed scheme. Finally, we conclude with a discussion in Section 6.In the SMATB formalism the total potential energy Epot of a system consisting of Nainteracting atoms is:Epot=∑i=1NaEiwhere the partial energies Eiare given by:Ei=A∑j=1j≠iNae−p((rij/r0)−1)−ξ∑j=1j≠iNae−2qrijr0−1In this expression rijis the distance between atoms i and j while r0 is set to the nearest neighbor distance (r0=3.1946Å in the case of Titanium). There are four adjustable parameters p, q, A and ξ that need to be determined using a least squares fitting procedure.For the fitting a set of M=10 properties yiis chosen, whose values are experimentally known and the error function that determines the agreement with the model is defined as:f(p,q,A,ξ)=∑i=1Mwiyicalc−yiexpyiexp2Here,wiare weights whose values reflect the importance of the property being reproduced. For the set of properties we have used the lattice parameter a, c/a ratio, atomic volume Ω0, cohesive energy Ec, five elastic constants and the vacancy formation energy Evf. We have opted to reproduce the atomic volume and cohesive energy as closely as possible by assigning appropriate weights. All propertiesyicalcare calculated using numerical methods. More specifically the lattice parameter a, and the c/a ratio are determined as the values that provide minimum energy in an HCP titanium lattice. The five elastic constants are determined by the method described in [19]. Finally the vacancy formation energy is determined by removing an atom from a perfect HCP lattice and allowing all neighboring atoms to relax. A numerical optimization procedure is then applied to the error function in order to determine the set of parameters that produce the best agreement with the experimental quantities.The error function f(p, q, A, ξ) has some important characteristics: Since all properties are numerically calculated the error function is quite noisy, has discontinuities and in addition there exist no analytic derivatives with respect to the parameters p, q, A and ξ. Hence application of numerical optimization methods that utilize analytic or numerically calculated first derivatives are not applicable. Table 1summarizes the values of the properties used in the fit, while Table 2 provides information for the error function.The hybrid GO algorithm explored in this paper, belongs to the class of hybrid meta-heuristics that combine population-based optimization algorithms with local search procedures [20,21,13]. The rationale behind their development was the necessity for powerful algorithms where the global exploration capability of population-based approaches (exploration phase) would be complemented with the efficiency and accuracy of classical local optimization techniques (exploitation phase). The most common cooperation scheme keeps the main exploratory part of a population-based strategy and stochastically selects a subset of individuals on which a local search is applied.Population-based algorithms draw their inspiration from physical systems. Based on natural selection they exhibit a remarkable capability of producing populations with fitter individuals. Genetic algorithms [22], particle swarm optimization (PSO) [6], differential evolution [23], harmony search [24], all fall into the category of population-based algorithms for global optimization.On the other hand local search (LS) procedures are deterministic and iterative procedures that aim to converge to a local minimizer of a given objective function, using mostly local information (function values). They enjoy guaranteed convergence but provide no assurance that the best minimum will be found.In this work, we have chosen PSO for the population-based exploratory part and Multidimensional Search (MDS) [12] for the exploitation part. The high degree of parallelization capabilities of both algorithms renders them perfect candidates for our parallel hybrid algorithm. PSO popularity has been gradually increased due to implementation simplicity that made it accessible to researchers in diverse fields, as well as due to the increased efficiency of recent variants. Today, PSO has been distinguished as one of the most promising population-based approach. Moreover it does not require smooth functions and it is robust to noise. Following the same reasoning: (a) easy implementation, (b) natural parallelization, (c) no requirements of derivative information and (d) robustness to noise, the Multidimensional Search algorithm is chosen.Many authors in the past used the same reasoning to combine two optimization methods together [25–27]. We assume the n-dimensional continuous optimization problem:(1)minx∈X⊂ℝnf(x),where the search space X is an orthogonal hyperbox inℝn:X≡[l1,r1]×[l2,r2]×⋯×[ln,rn].Some of the algorithms presented in this section are described thoroughly in [13,14].The PSO algorithm was introduced by Eberhart and Kennedy [6,28]. The main concept of the method includes a population, also called swarm, of search points, also called particles, searching for optimal solutions within the search space, concurrently. The particles move in the search space by assuming an adaptable position shift, called velocity, at each iteration.A swarm of N particles is a set of search points:S={x1,x2,…,xN},where the i-th particle is defined as:xi=(xi1,xi2,…,xin)⊤∈X,i=1,2,…,N.The velocity (position shift) of xiis denoted as:vi=(vi1,vi2,…,vin)⊤,i=1,2,…,N,and its best position as:pi=(pi1,pi2,…,pin)⊤∈X,i=1,2,…,N.Let gidenote the particle which attained the lowest function value so far i.e.:gi=argminj∈1…Nf(pj),and t the algorithm's iteration counter. Then, the particle positions and velocities are updated at each iteration according to the equations [29]:(2)vij(t+1)=χvij(t)+c1r1pij(t)−xij(t)+c2r2pgij(t)−xij(t),(3)xij(t+1)=xij(t)+vij(t+1),i=1,2,…,N,j=1,2,…,n,where χ is the constriction coefficient; c1 and c2 are positive constants called cognitive and social parameter, respectively; r1, r2, are random numbers drawn from a uniform distribution in the range [0, 1]. The best position of each particle is updated at each iteration:(4)pij(t+1)=xij(t+1),iff(xi(t+1))<f(pi(t)),pij(t),otherwise.The function evaluationsfxi(t+1))used for the update of best position (Eq. (4)) can be performed independently for all members of the swarm.In early 90s, Torczon [12] introduced a novel local optimization algorithm, called Multidimensional Search (MDS), that operates on a simplex of points defined in the search space. The proposed deterministic algorithm uses a sequence of reflections, expansions and contractions of a simplex to guaranty convergence to a local minimum. It was devised to operate without any derivative information for the objective function, using function evaluations that can be executed concurrently.At any iteration t an n-dimensional simplexS(t)={x0(t),x1(t),…xn(t)},xi(t)∈ℝnis maintained. The center of the simplexxc(t)=1n∑i=0nxi(t)is considered as an approximation to the minimum. The objective function is evaluated at n vertices of the simplex and the best vertex is defined as the one with the smallest function value. The vertices are rearranged so thatf(x0(t))=mini=0,…,nf(xi(t))The transition to the next iteration is performed by pivoting the simplex aroundx0(t)and attempting a reflection. The objective function is then evaluated at the n vertices of the reflected simplex. If the function value decreases, then an expansion step is attempted, in order to produce an even larger reflected simplex. If the reflection fails to produce a smaller function value (than the one of the pivot) then a contraction step that reduces the size of the simplex is attempted. The simplex transformations are illustrated in Fig. 1. The procedure is repeated until a termination criterion is satisfied. This criterion may be the maximum number of iterations, function evaluations or the detection of no further progress in one iteration. The overall MDS procedure is presented in Algorithm 1. It is clear that the function evaluations in lines 7, 12 and 22 are independent and can be performed in parallel.Algorithm 1MDS algorithmThe design of the hybrid global optimization algorithm is based on [13] and provides answers to the following questions: (a) Where should the LS procedures be applied, (b) When should the LS procedures be applied, (c) How much of the budget should the LS procedures spend. The cooperation strategy chosen for this work states:At each PSO iteration (when), a fixed budget (how much) LS can be applied on each best position vector, pi, i=1, 2, …, N, with a prescribed fixed probability, ρ∈(0, 1] (where). The overall best should always be included as a local search candidate.Of course, many other strategies can be considered. The one above is easy to implement and resulted very good performance in [13] and in [14,30,31]. The general procedure of the hybrid algorithm discussed in our study is given in Algorithm 2.Algorithm 2Hybrid GO algorithmAt the beginning of its main iteration loop, the algorithm determines for each particle whether a simple function evaluation (FE) will take place or a local search (LS) will originate. In line 17 the best position giis detected and in line 18 the update to the new particle position is calculated. Then in line 24 the corresponding tasks are executed. Finally the results are gathered and the new best positions are determined in line 31.The stochastic nature of the GO algorithm renders it, at a first glance, a cumbersome case of effective parallelization. The basic loop in line 24 of Algorithm 2 spawns N independent tasks, each one associated with a specific particle. Some of these tasks are simple function evaluations (FE) while the rest of them are MDS local searches. The probability ρ at line 11 of the same algorithm controls which particles will execute FE (approximately (1−ρ)·N particles) and which MDS (approximately ρ·N particles). MDS calls include an iterative procedure with a second level of parallelism. At this inner level, n independent calls to the objective function are made for the calculation of the reflection simplex and subsequently n for expansion and n for contraction. The iterations of the MDS procedure are not known beforehand unless we choose a single termination criterion that is based on the maximum number of iterations or function evaluations. However in practice, termination criteria based on the proximity to the solution, are always used. In this way wasting valuable resources is avoided.Fig. 2.According to the above, the parallelism of the GO algorithm is highly irregular and depends on several factors: the swarm size and the probability of performing local searches, the time steps required by the local optimization for finding a minimum, the dimensionality and the execution time of the objective function. An efficient parallel implementation of the GO algorithm requires flexible management of this dynamic and highly irregular nested parallelism. To achieve that on shared-memory platforms, we have used the recent OpenMP [32] tasking model, which extends the expressiveness of OpenMP beyond loop-level and coarse-grain parallelism. Both function evaluations and MDS calls are expressed with OpenMP tasks, with the latter dynamically spawning further function evaluation tasks. Specifically, we create only a single team of threads for all levels of parallelism. The master thread runs an implicit task, the primary task, that executes the main loop of the algorithm and iteratively spawns first-level tasks using the task construct. The rest of the threads reach the end of the parallel region and participate in their execution of these tasks. The primary task then encounters a taskwait construct and waits for its child tasks to complete, while the master thread participates in the their execution. Any OpenMP thread that executes an MDS task, dynamically spawns additional tasks at the innermost level of parallelism following the same fork-join approach. The underlying OpenMP runtime library is responsible for the scheduling of all these tasks across the available cores. The OpenMP tasking model allows us to instantiate the task-graph of the hybrid GO algorithm in a straightforward way and effectively exploit the multiple levels of parallelism. In addition, due to the single team of threads, our parallel application avoids the overheads and performance implications of OpenMP nested parallel regions.Despite the advantages of the OpenMP tasking model, the above implementation is not supported on distributed memory systems. In order to exploit multicore clusters, we implemented the parallel GO algorithm with TORC [5,33], a custom task-parallel library that offers a programming and runtime environment where parallel programs can be executed unaltered on both shared and distributed memory platforms.According to TORC, a parallel application consists of multiple MPI processes running on cluster nodes and having one or more kernel-level POSIX threads that share the process memory. Each kernel thread is a worker that continuously dispatches and executes ready-to-run tasks, submitted for execution to a set of ready queues. Due to the decoupling of tasks and execution vehicles, arbitrary nesting of tasks is inherently supported and any child task can become a parent.In accordance with OpenMP, a parent–child relationship is supported between the tasks of the library. Furthermore, similarly to Remote Procedure Calls [34], a task results in the asynchronous execution of the user specified function and its parameters.All data transfers in the library are performed with explicit, transparent to the user, messaging. This is achieved with a server thread in each MPI process, which is responsible for the remote queue management and the transparent and asynchronous data movement. There are private and public worker specific and node specific ready queues where tasks can be submitted for execution. When running on a single node, TORC operates as a two level threading library that seamlessly exploits intra-node task parallelism and completely avoids explicit messaging. An overview of the architecture of TORC on a single cluster node is depicted in Fig. 3.The user can query the execution environment (number of nodes and workers) and then specify the node or worker where each task will be submitted for execution. As task stealing is inherently supported by TORC, the programmer has only to decide about the task distribution scheme. Upon termination or suspension of the current task, the underlying worker runs the scheduling loop: it first visits its local ready queue and tries to extract the task at the front of it. If the queue is empty, it tries to steal tasks from the rest of the intra-node ready queues. If inter-node task stealing is enabled, it will issue requests for work to remote nodes in sequential order. Task stealing is always performed from the back of the ready queues.The parallel implementation of the hybrid GO algorithm on top of the TORC library is equivalent to the OpenMP-based one. The main application task is executed by one of the workers of the MPI process with rank 0. The first level tasks are distributed cyclically to the workers and inserted at the back of their ready queues. At the second level of parallelism, however, the spawned tasks are inserted at the front of the queue that belongs to the worker thread where the parent (MDS) task runs on. The combination of the described task distribution scheme with the stealing mechanism of the library favors the local computation of function evaluation tasks and gives priority to the remote stealing of MDS tasks.In this section we present experimental results designed to:(a)analyze the objective function execution time,explore the efficiency of the hybrid algorithm in its serial implementation,measure the parallel performance of the proposed hybrid algorithm against parallel implementations of other global optimization methods,explore the scalability of the parallel implementation.The parallel algorithm implementation was tested on two different hardware platforms:P1A multicore server with two 12-core AMD Opteron 6174 CPUs (2.2GHz, 512KB private L2 cache/core) and 32GB of RAM.A 12 node Sun Fire x4100 cluster interconnected with Gigabit Ethernet. Each node has 2 dual-core AMD Opteron-275 processors (2.2GHz, 1MB cache/core) and 4GB of RAM.The software was compiled under Linux 2.6 with GNU gcc 4.6 and mpich2. Moreover, the objective function is written in Fortran and compiled with the corresponding GNU gfortran compiler.In order to analyze the time distribution of the objective function a series of functions evaluations were performed inside search space X, at points generated by the hybrid GO algorithm. Fig. 4depicts the probability distribution of the execution time of 5372 total function evaluations for an indicative experimental setup, executed on a single-core of the shared-memory server. The average time is 18.97s with standard deviation 4.67, while the minimum and maximum evaluation time is 10.62 and 90.85s, respectively. The observed variation of the function time corresponds to an additional source of irregularity in the GO algorithm and indicates the required support of adaptive load balancing from the parallel execution environment.In order to establish the efficiency of the chosen hybrid scheme, we have tested our serial implementation against the Black-Box Optimization Benchmarking (BBOB) 2009 test set [35]. Following the experimental setup instructions of [36] we performed a series of experiments using the parameters shown in Table 3.The algorithms in BBOB 2009 include the multistart method with various local searches (BFGS, Nelder–Mead simplex, NEWUOA, Rosenbrock's method), clustering multistart (GLOBAL), single point deterministic algorithms (DIRECT, MCS), hybrid algorithms (MA-LS-Chain, DEPSO) and population based methods (GA variants, DE variants, PSO variants, CMA-ES variants, ant stigmetry). For a detailed description of the above algorithms please refer to [35]. The test set includes 24 test function categories with variable dimensionality and a complete set of postprocessing scripts to generate insightful charts. We have chosen to present results as they are produced by the automatic postprocessing scripts so that the comparison would be on fair grounds. Here we show 3, 5 and 10-dimensional results since our potential fitting application is of similar dimensionality. A total of 360 test function instances for each of the three dimensionalities were used in our benchmark.The comparison is made through the Empirical Cumulative Distribution Function (ECDF) of the number of function evaluations. The ECDF represents the fraction of test instances solved within the prescribed accuracy and is described in detail in [37]. In Fig. 5 we present the ECDF of the number of function evaluations needed to achieve various levels of accuracy (10, 10−1, 10−4, 10−8). The results presented correspond to the 3, 5 and 10-dimensional instances of the test functions. The thick red line shows the percentage of trials that reached the global minimum with accuracy of 10−8 as a function of running time. The light brown lines in the background show ECDFs for the same accuracy for all algorithms benchmarked during BBOB 2009. By inspecting Fig. 5an immediate qualitative comparison is possible. The prosed hybrid algorithm solves all 3-dimensional cases, almost 90% of the 5-dimensional and 60% of the 10-dimensional to an accuracy of 10−8. Only 4 other competitive serial algorithms achieved the same percentage. The above results, establish the efficiency of our hybrid scheme regarding low dimensional optimization cases.For a direct comparison of our hybrid to competitive algorithms from BBOB 2009, we present in Table 4 the percentage of trials that reached accuracy up to 10−8 using a budget of 100, 000×n function evaluations. Detailed description of the algorithms can be found in [35]. Table 4confirms that the proposed algorithm is very competitive in solving small dimensional problems.The purpose of the parallel implementation experiments is twofold. First to compare the performance of our parallel hybrid algorithm against some well established parallel global optimization implementations. The second goal is to investigate the effect of the input parameters to the overall speedup.Comparison to other parallel algorithms. Despite the multitude of parallel global optimization algorithms presented in the literature only a handful of real systems is actually implemented and distributed freely. Among them, we distinguish PAGMO [38] that offers an island model paradigm able to parallelize hybrids between global and local optimization algorithms, VTDIRECT95 [39] that provides a parallel implementation of a deterministic global optimization algorithm called DIRECT [40] and CMA-ES [41] which evolves a population using a multivariate normal distribution.All algorithms were tested on platform P1. For the PAGMO algorithm we established an archipelago of 24 fully connected MPI islands, each one running a DE algorithm with population size equal to 6. This gives us a total of 144 particles. For the VTDIRECT implementation we used in total 24 processes that were split in both 8 subdomains and 24 subdomains. For the CMA-ES we implemented a simple MPI scheme that circularly assigns function evaluation tasks to processes and used swarm sizes of 24, 48 and 144. Finally in our hybrid implementation we also used a swarm size of 144, probability ρ=0.0 and ρ=0.5 and a maximum of 2000 function evaluations per local search. We measured the total running time and the parallel efficiency of all schemes. We present speedup and efficiency results in Table 5. An illustration of the efficiency is also given in Fig. 6. It is clear that our parallel implementation achieves the best speedup and efficiency score.Speedup calculation. The second experiment studies the scalability of the hybrid GO algorithm for a fixed set of control parameters (probability ρ=0.5, swarm size N=128) and a specific number of total function evaluations. In all experiments the target parameters in Table 2where approximated by an order of 10−4.Fig. 7(a) and (b) shows the speedup for the particular test case on the shared-memory and the cluster system, respectively. We observe that the application scales well on both architectures. On the 24-core system, the maximum speedup for the OpenMP version is 21.30 (88.7% efficiency), while the TORC-based version attains comparable performance (20.69 speedup, 86.2% efficiency). Moreover, the latter version obtains a speedup of 36.12 on the 48 cores of the cluster (75.3% efficiency). The decrease in the performance with the number of cores is mostly attributed to the inherent load imbalance of the algorithm, as the computational work (tasks) cannot be distributed evenly. Furthermore, the objective function time increases due to the higher contention on the memory subsystem.Varying swarm sizeN. In this experiment we study the parallel performance of the GO algorithm with respect to the swarm size. We have used a fixed number of processing elements for each platform: all the 24 cores of the shared-memory server and 8 nodes (32 cores) of the cluster. In addition, the probability of local optimization is set to 0.1 in all cases.In Fig. 8we present the obtained efficiency of the parallel GO algorithm for swarm size 32, 64, 128 and 256. We observe that the parallel performance improves as the swarm size increases. This is attributed to the higher number of available tasks, which are distributed more evenly to the processors and thus better load balancing and hardware utilization is achieved. In the same figure, we also observe that the OpenMP and the library-based implementation of the GO algorithm exhibit comparable performance.Varying probabilityρ. Similarly to the previous one, our last experiment studies how the GO algorithm behaves with respect to the probability ρ. Fig. 9shows the parallel efficiency for a swarm of 128 particles and probability ρ equal to 0, 0.05, 0.1, 0.2 and 0.5. For this particular configuration, we observe that the pure PSO approach (ρ=0) attains good load balancing and, thus, the highest efficiency. When local optimizations are used, the performance is improved as the probability increases, because more function evaluations tasks become available for execution at each step of the algorithm.

@&#CONCLUSIONS@&#
We presented a parallel global optimization algorithm suitable for the specific class of time consuming potential fitting problems that follow the SMATB formalism. The proposed algorithm is a hybrid between PSO, for the global exploration, and the MDS algorithm that locates local minimizers. Both algorithms are parallelizable and the hybridization strategy results in a highly irregular and dynamic task graph. We introduce a runtime environment suitable to support irregular and dynamic tasks on both shared and distributed memory platforms.We have performed extensive experiments using the serial and parallel implementations of the proposed algorithm. The parallel implementation scores high efficiency on both cluster and shared memory systems. Furthermore, it achieves the best parallel performance when compared to other parallel algorithms. Experiments using the serial implementation of the algorithm demonstrate its applicability on a wide range of general optimization problems.Concerning future work a different parallelization scheme of MDS algorithm can be explored. In this new scheme reflection, expansion and contraction take place in a single parallel loop resulting in 3·n parallel function evaluation tasks. In this way we retain good efficiency in even larger parallel systems without compromising the properties of the algorithm. Furthermore, following Torczon's guidelines in [12] a full lookahead step of the MDS algorithm will increase exponentially the number of parallel tasks. On small sized problems, launching a large number of parallel function evaluations increases the parallel efficiency.It would be also interesting to implement different hybrid schemes combining new global and local components. Note that various new derivative-free local search methods have been developed that would be excellent candidates for the local component. Extending the above, a new class of adaptive hybrid optimization schemes that are not limited to the use of a single local search but choose a proper one from a rich has been proposed and it indeed exhibits encouraging results [42].Considering the potential fitting application one can extend the SMATB formalism to include alloys of metals. The resulting optimization problem is of higher dimensionality and complexity and hence requires longer execution times. Predicting the structural behavior of metallic alloys is an application of huge practical importance.