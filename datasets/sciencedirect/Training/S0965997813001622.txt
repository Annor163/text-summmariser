@&#MAIN-TITLE@&#
The use of partially converged simulations in building surrogate models

@&#HIGHLIGHTS@&#
We propose here to use partially converged data to construct a surrogate model in solid mechanics.We use a correction principle (evofusion) of partially converged data to improve the quality of the surrogate model.Two strategies are presented, one to construct a complete precise surrogate model, the other one to localize the minimum area.It is shown that for the two strategies, the use of partially converged data enables increases in computing performance.

@&#KEYPHRASES@&#
Partially converged data,Evofusion,Metamodel,Global optimization,LATIN method,

@&#ABSTRACT@&#
The main objective of this paper is to propose an optimization strategy which uses partially converged data to minimize the computational effort associated with an optimization procedure. The framework of this work is the optimization of assemblies involving contact and friction.Several tools have been developed in order to use a surrogate model as an alternative to the actual mechanical model. Then, the global optimization can be carried out using this surrogate model, which is much less expensive. This approach has two drawbacks: the CPU time required to generate the surrogate model and the inaccuracy of this model.In order to alleviate these drawbacks, we propose to minimize the CPU time by using partially converged data and then to apply a correction strategy. Two methods are tested in this paper. The first one consists in updating a partially converged metamodel using global enrichment. The second one consists in seeking the global minimum using the weighted expected improvement. One can achieve a time saving of about 10 when seeking the global minimum.

@&#INTRODUCTION@&#
The development of numerical tools and mechanical models over the past few years has helped improve the design of complex systems. The ability of modeling and simulation softwares to handle nonlinear problems, which are sources of numerical difficulties, has increased continuously. Aside from resolution difficulties, the computation costs associated with systematic calls to the simulation routines make direct structural optimization unthinkable in an industrial context. In this work, we focus on parameterized assemblies. LetDbe the design space and x a vector of dimension p which contains the p design parameters: x=[x1…xp].As recalled in [1], the most efficient optimization methods rely on multilevel optimization concepts, which can be divided into three categories: parallel model optimization based on domain decomposition methods [2,3], multilevel parameter optimization, which consists in replacing an optimization problem by several subproblems, each with a reduced set of parameters [4–7], and multilevel model optimization, which introduces several modeling levels [8–10].In the context of multilevel optimization, the use of a tool called “metamodel” has become predominant. A metamodel (or surrogate model [11]) is an approximate model of the objective function F(x). This reduced model, denotedF^(x), defined over the whole domain being studied, requires one to simulate a large number of points (which can be very expensive), then associate a mathematical interpolation or regression model with these points [11,12].In more general case, surrogate models can be divided according to [13] in three categories:•Response surfaces: a response surface is a functional mapping of several input parameters to a single output feature. It could be of polynomial form where regression coefficients are determined by the method of least squares. Typically model properties of interest are those that characterize model fit quality, contribution of an individual variable to total model variance, model resolution, etc. [11] provides a taxonomy of the different processes used to create a surface.Reduced models derived from the Proper Orthogonal Decomposition (POD) or Proper Generalized Decomposition (PGD). Readers can find details in [14,15]. More recently PGD was used on parametric models and this approach is initiated the use of the PGD to deal with parametric problems such as parametric optimization problems, inverse identification problems or real time simulation. Some examples of the parametric modelling can be found in [16,17]. PGD was also carried out in the context of Multiparametric Strategy [18].Hierarchical models, also called multifidelity, variable-fidelity or variable-complexity. In the context of two levels of fidelity, corrections between the two models can be lead, for example [19] uses a kriging model to correct the low-fidelity model on the high-fideliy model. This correction is called scaling model or correction response surface [20,21].In this general case, the main stumbling block is the computation time of the points of the design domain. In order to overcome this problem, we undertake to develop strategies, inspired by previous works in fluid mechanics, based on partially converged data.This paper focuses on the construction of metamodels based on partially converged data. Two strategies have been implemented: one to localize the zone of interest (the zone in which the optimum is to be found), and the other to build the complete metamodel with a given level of accuracy.Classically, optimization techniques begin with a metamodel-based optimization (which is much less time-consuming) in order to locate the zone of interest. Then, the mathematical model associated with the metamodel plays an important role. There are several possible construction approaches, such as kriging [22,12], co-kriging [23–25], the RBF method, etc. In this paper, we use a universal kriging built using the DACE toolbox [26,27]. Kriging was introduced in 1951 by D.G. Krige and later developed, among others, by Matheron [12], who laid out its mathematical foundations.The kriging technique enables one to determine an approximate function using statistical reasoning. It consists in considering the objective function in the approximate form:(1)F^(x)=μ(x)+z(x)where μ(x) is a regression function of the known data, and z(x) is a stochastic process which represents the prediction error of the regression model. This function z has zero expectancy and constant variance:(2)E(z(x))=0E(z(x)2)=σkrige2In the case of universal kriging, μ(x) is an approximation by regression over a basis of functions which, usually, are polynomials. It is also important to define a correlation function, which is required for the evaluation of z(x). We use a generalized exponential correlation function similar to that defined in the following equation:(3)Corr[(xi),xj)]=exp-∑l=1pθl|xil-xjl|tlThe various parameters are obtained through the calculation of a maximum likelihood. θlis a parameter which is associated with the correlation drop-off rate in the lth dimension, and t is a smoothing parameter of the metamodel.After solving a problem of minimization of the mean square error (MSE), one finds, for any x, bothF^(x)and an estimate of the associated MSE, denoted σ(x)2 and defined by:(4)σ(x)2=E(F^(x)-F(x))2Thus, the optimization, being carried out using this metamodel, is relatively inexpensive. However, such a metamodel, besides being costly to generate, can be inaccurate, which leads to problems in terms of its resolution. In order to alleviate this drawback, there exist metamodel enrichment strategies [28,29], especially the “expected improvement” method [30], which enables one to target the zones of interest more accurately (and, thus, accelerate and improve the optimization). The expected improvement is a probabilistic criterion which defines the improvement which can be achieved in terms of seeking an optimum. Then one carries out the enrichment at a point to the highest expected improvement in order to seek the minimum of the objective function (see Section 3.3).Strategies based on the calculation of partially converged data have been implemented in order to accelerate the optimization even further. These strategies, which do not necessarily rely on the metamodel tool, can be divided into three main categories.(1)The first category uses different levels of fidelity of the objective function F(x) ([31,19,32]). The techniques developed in the first two references use two levels of fidelity of the objective function whose optimum is being sought. Local optimizations are carried out on the low-fidelity model (LF) using a confidence-region-based algorithm. Then, the high-fidelity model (HF) is evaluated for each resulting optimum in order to guarantee the convergence of the method. This type of algorithm is illustrated in Fig. 1, where fhfand flfdenote respectively the HF and LF representations of the objective function.We can use a correction between the two models to be more accurate.Another category of methods using metamodels is comparable to the processing of noisy data [33–35]. Indeed, to a certain extent, partially converged data can be viewed as noisy data. One possible approach consists in transforming the universal kriging method into a modified regressive kriging ([35]). One can also use enrichment criteria which are different from those which are generally applied. [33] proposed a modification of the expected improvement criterion [30]. However, we did not use such methods in our work because the results obtained with noisy data and with partially converged data are not comparable.The last category of methods, which includes our work, makes full use of partially converged data (i.e. points in the design space) [36–39]. The earliest works in that group ([38]) were based on a progressive algorithm, i.e. a local optimization method based on a sequential refinement of the mesh. The main idea is to use an adjoint problem whose solution is the gradient of the objective function, leading to the directions of descent. Two solvers are used: one for the direct problem and the other for the adjoint problem. Partially converged solutions of these problems are sought. Then, the optimization variables are updated along the direction of the gradient and the process is repeated until convergence, i.e. until the estimated gradient of the function reaches a certain threshold. Then, the following step consists in refining the mesh and repeating the various operations down to the finest mesh level (see Algorithm 1).The progressive algorithm1: Set up the design variables2: Begin the resolution using a coarse mesh3: Proceed with the direct resolution of the problem4: Solve the adjoint problem5: Evaluate the function of interest F and its gradient ∇F6: Update the design variables according to the relation(5)xjl+1=xjl-aj∂F∂xj7: Repeat Steps 3 to 6 until the gradient of the objective function has decreased sufficiently8: Refine the mesh by doubling the number of intervals in each direction, and interpolate the previous solution over this refined mesh9: Repeat 3 to 610: Repeat 8 and 9 down to the most refined mesh11: Repeat 3 to 6 until∂F∂xj≃0The results obtained have led to the conclusion [36] that by calculating the evaluation points of the objective function iteratively one rapidly ends up with a good shape of the metamodel after only a few iterations (with a correlation coefficient equal to about 0.95 after 80 iterations instead of 250). Since partially converged data can reproduce the proper shape of a reference metamodel, a correction and enrichment strategy for the partially converged metamodel, called evofusion [37], was implemented in order to improve the available data and target the zones of interest more accurately.This bibliography emphasizes the fact that there can be no global optimization method based solely on partially converged data. Either a method is suitable for local optimization [38,39], or it uses partial convergence as the first step of an optimization technique [37]. The evofusion method seems to be suitable for our goal of creating a metamodel based on partially converged data. The calculation of partially converged data is the preamble to this strategy. In addition, this type of data seems to have never been used before in the context of the optimization of a solid mechanics problem. This work appears to be the very first one to do so. Here, we will use sample problems involving assemblies with frictional contact [40] to illustrate the results of this approach consisting in building a partially converged, then evofused, metamodel.The evofusion defined by [37,41] will be customized in order to determine the zone of interest for the subsequent optimization. The advantage of this strategy is that most of the computational effort is focused on seeking the zone of interest rather than wasted on possibly distant zones. We will also use this approach (i.e. a partially converged metamodel corrected through evofusion) to generate a metamodel which is accurate throughout the design space. This is equivalent to a global enrichment technique. The quality of this metamodel will be assessed by comparison with a reference metamodel. This technique is called model updating. Two definitions of an improvement factor will be defined for the two strategies.In order to build the metamodel using data from partially converged simulations, one must have a tool which is capable of generating a response of the structure over the whole loading interval at a prescribed level of accuracy. Classical incremental algorithms are unsuitable because they work iteratively with successive loading steps and their convergence is prescribed for each loading step instead of globally. Thus, in the case of classical, Newton-type incremental algorithms used in structural analysis, the concept of partially converged calculation is meaningless. Indeed, only the calculation associated with the last loading increment can be partially converged (if the other calculations are not fully converged, there is a high risk of divergence of the algorithm). Therefore, we will rely on the LATIN method [42], which has the advantage of enabling the definition of an approximate solution through a series of iterations over the whole loading interval. A fully converged quantity is a computed quantity whose error indicator (which controls its convergence) falls below a certain reference level. Conversely, a partially converged quantity is a quantity calculated with a higher error level.Section 2 gives an overview of the LATIN method and presents the error indicator used, which is necessary in order to determine full or partial convergence.Section 3 presents correlation indices which enable one to quantify the quality of a metamodel with respect to a reference. The evofusion methods used for seeking the zones of interest and for updating are explained. Then, we introduce the expected improvement, which will be used as the enrichment criterion for the evofusion method of seeking the zone of interest.Finally, Section 2. illustrates the efficiency of the combined use of these main tools (evofusion and the LATIN iterative method) based on two test cases involving contact with friction.The first idea underlying the LATIN method is domain decomposition, in which the interfaces are mechanical entities in their own right, with their unknowns and constitutive relations. An evolution law is associated with each interface depending on the problem being studied (friction, contact, etc.). The second idea is that the difficulties are separated by considering two sets of equations: the local (possibly nonlinear) equations, and the linear (possibly global) equations.In order to simplify the presentation, let us consider only two substructures ΩEand ΩE′ connected by an interface ΓEE′. The interface variables are two force fields (fE,fE′) and two dual velocity fields (ẇE,ẇE′) (Fig. 2). By convention, (fE,fE′) are the actions of the interface on the substructures and (ẇE,ẇE′) are the velocities of the substructures observed from the interface. In the case of an assembly, each part is considered to be a substructure. Therefore, the interface describes the behavior of the assembly (friction, gaps, etc.).Let uE(M,t) be the displacement field at any point M of ΩEand at any time t in [0,T], and letUad[0,T]be the associated space. Then, the problem to be solved in each substructure is: find the evolutions of the displacement field uE(M,t) and of the stress field σE(M,t). One introduces the kinematic variable wE(M,t) defined byuE(M,t)|∂ΩE=wE(M,t). In our case, since the problem is quasi-static, we use the quantityẇE(M,t).The solution s is described a priori as a set of time-dependent fields relative to the interface as well as to the substructures. Here, the substructures have linear elastic behavior and the interior solution (i.e. the displacement uE(M,t) and the stress σE(M,t)) can be easily calculated from the boundary quantities (ẇE(M,t)and fE(M,t)). The solution s can be represented using only the force and velocity fields on both sides of the interface:(6)s=∑EsE,sE=ẇE(M,t),fE(M,t),∀t∈[0,T]Considering that the substructures are elastic and that all the nonlinearities are concentrated at the interface, the equations are divided into two groups:•the setAdof the solutions sEwhich satisfy the linear equations relative to the substructures, andthe set Γ of the solutions sEwhich satisfy the local (possibly nonlinear) equations relative to the interface.Then, the solution of the problem is determined iteratively by seeking successive approximations s which satisfy the two groups of equations alternatively, using search directions E+ and E− (Fig. 3). Thus, the two steps of the iterative algorithm are:•(local step) givensn∈Ad, finds^such that:(7)s^∈Γ(interfaces)(8)s^-sn∈E+(searchdirection)(global step) givens^∈Γ, find sn+1 such that:(9)sn+1∈Ad(substructures)(10)sn+1-s^∈E-(searchdirection)Here, we use conjugate search directions which depend on a single scalar parameter k0:(11)s^-sn∈E+≡f^E-fnE=k0(ẇ^E-ẇnE)(12)sn+1-s^∈E-≡fn+1E-f^E=-k0(ẇn+1E-ẇ^E)The solution of the problem is independent of the value of parameter k0, which affects only the convergence rate of the algorithm. In our case of quasi-static calculations, k0 is given by:(13)k0=ETLcwhere E is the Young’s modulus, [0,T] the time interval being considered and Lcthe largest dimension of the structure.Since the LATIN algorithm is iterative, it is important to have an error estimator ([42]) which can be used to identify full or partial convergence. This estimator is defined by:(14)η=∑‖snE-s^E‖2∑‖snE‖2+‖s^E‖2with:(15)‖s‖2=∫∂ΩEfTk0fdS+∫∂ΩEẇTk0ẇdSThis is a global estimator, which is what one uses to determine the accuracy of a solution. The error η is a relative distance between spacesAdand Γ (see Fig. 4).Definition 1A fully converged quantity is an approximate quantity whose estimated error is smaller than a reference value; a partially converged quantity is a quantity whose error is greater than the reference. Thus, the choice of this reference level is important. In our study, the reference levels were set, based on engineering practice, at 1×10−4 for plane or axisymmetric 2D problems and at 1×10−5 for 3D problems.As shown in Fig. 5, the use of partially converged data can vastly improve computation time. In the example of a shrink disk discussed in Section 4, if Tpcvand Tfcvdenote respectively the computation time of a partially converged quantity and a fully converged quantity, one can have ratios Tfcv/Tpcvas high as 80, i.e. 80 partially converged quantities can be calculated in the same time as a single fully converged quantity.In this study, for both strategies, we are interested in the initial correlation between the partially converged metamodel and a reference metamodel. In the case of the metamodel developed for the purpose of finding the minimum, we use the determination coefficient defined by [43] and also used in [41,37]:(16)r2=Ns∑F^refF^app-∑F^ref∑F^appNs∑F^ref2-∑F^ref2Ns∑F^app2-∑F^app22where Nsis the number of points to be correlated,F^refare the values of the objective function from the independent test set andF^appare approximate values of the objective function taken from the surrogate model at the locations ofF^ref. This coefficient gives an indication of the shape similarity between the two metamodels. When seeking the minimum, the shape alone is interesting because it enables one to localize the zone of interest.Conversely, for updating purposes, one needs to have a correlation coefficient which takes into account both the amplitude and the shape of the metamodels. Therefore, one uses the concordance correlation coefficient defined by [44] in the case of two observers for one experiment:(17)rccc=2.SF^appF^refSF^app2+SF^ref2+F^¯app-F^¯ref2where(18)F^¯=1Ns∑n=1NsF^,SF^2=1Ns∑n=1Ns(F^-F^¯)2SF^appF^ref=1Ns∑n=1Ns(F^app-F^¯app)(F^ref-F^¯ref)The concept of evofusion was developed in order to be able to use a metamodel to find the global optimum of an objective function [37,41]. This metamodel is a combination of two metamodels, one built using partially converged data and the other using fully converged data. The main idea is that with partially converged data it is possible to obtain the correct shape of the metamodel and, thus, the right tendency [36]. In addition, in our study, the concept of evofusion was used to generate an accurate metamodel throughout the design space. This approach, which is equivalent to global enrichment, is called model updating.Evofusion is easy to implement (even though there are some subtleties) and it is very effective. Essentially, it consists in correcting a partially converged surrogate model at some specific points in the region of interest using a surrogate model of the error defined between some other partially converged data and fully converged data. Evofusion strategies start with the creation of a first metamodel,F^pcv, using partially converged data xpcv. Subsequently, one creates an error metamodelF^erras the difference between partially converged data and fully converged data at a set of points xfcvissued from an enrichment criterion. This error metamodel corrects the partially converged metamodel (F^pcv), leading to the evofused metamodelF^evof. Then one repeats the algorithm at a new point which is added to xfcv. This point corresponds to the maximum mean square error of the error metamodel in the case of an updating strategy (see Section 1), or to the maximum expected improvement if one seeks to localize the zone of interest (see Section 3.3).The equation of the evofused metamodel is given in [37]:(19)F^evof=F^pcv(xpcv)+F^err(F^fcv(xfcv),F^pcv(xfcv))An illustration of evofusion in the case of the localization of a zone of interest is given in [37]. Fig. 6shows the evofusion during the updating process. The squares and the circles represent fully converged data and partially converged data respectively.The concept of expected improvement, described in [45,30], is used when a metamodel is constructed by means of a kriging process. Indeed, expected improvement is a statistical approach.Expected improvement is a probabilistic means of finding the global minimum of a function which is known by means of a metamodel. By definition, the expected improvement is the degree of improvement which one can hope to achieve at a given point. Two aspects are taken into account: one can expect an improvement around a minimum which is known to the metamodel or, on the contrary, in a zone in which only little information is available. Assuming a normal distribution law, one can express the expected improvement as:(20)E(I)=(Fmin-F^(x))ΦFmin-F^(x)σ(x)+σ(x)ϕFmin-F^(x)σ(x)ifσ>00ifσ=0where Φ is the normal cumulative distribution function and ϕ is the density function. Obviously, when σ=0, there can be no improvement: the calculation is deterministic and, therefore, when a point is calculated, one assumes that the value obtained is the right one (See Fig. 7).Remark 1More recent works ([46]) have shown that one can assign weights to the known minimum and to the unexplored area. The first term of the expected improvement favors the search around a known calculated minimum, whereas the second term favors the unexplored area. The weighted expected improvement can be expressed as:(21)WE(I)=ω(Fmin-F^(x))ΦFmin-F^(x)σ(x)+(1-ω)σ(x)ϕFmin-F^(x)σ(x)ifσ>00ifσ=0The difference between the expected improvement and the weighted expected improvement is illustrated in Fig. 8with ω=0.2.One can clearly see that for the second iteration more importance was given to the unexplored area. Throughout our work, we used ω=0.2.[41] describes a reinforcement learning algorithm [47] which leads to the most appropriate value of ω for each iteration. An illustration of our complete algorithm is presented on Fig. 9.

@&#CONCLUSIONS@&#
