@&#MAIN-TITLE@&#
On stepwise pattern recovery of the fused Lasso

@&#HIGHLIGHTS@&#
We provided necessary and sufficient conditions such that fused Lasso consistently recovers the piecewise constant pattern.We found that in general the fused Lasso is not consistent.We proposed a preconditioned fused Lasso to overcome the non-consistent issue.Simulation studies support our findings.

@&#KEYPHRASES@&#
Fused Lasso,Non-asymptotic,Pattern recovery,Preconditioning,

@&#ABSTRACT@&#
We study the property of the Fused Lasso Signal Approximator (FLSA) for estimating a blocky signal sequence with additive noise. We transform the FLSA to an ordinary Lasso problem, and find that in general the resulting design matrix does not satisfy the irrepresentable condition that is known as an almost necessary and sufficient condition for exact pattern recovery. We give necessary and sufficient conditions on the expected signal pattern such that the irrepresentable condition holds in the transformed Lasso problem. However, these conditions turn out to be very restrictive. We apply the newly developed preconditioning method — Puffer Transformation (Jia and Rohe, 2015) to the transformed Lasso and call the new procedure the preconditioned fused Lasso. We give non-asymptotic results for this method, showing that as long as the signal-to-noise ratio is not too small, our preconditioned fused Lasso estimator always recovers the correct pattern with high probability. Theoretical results give insight into what controls the ability of recovering the pattern — it is the noise level instead of the length of the signal sequence. Simulations further confirm our theorems and visualize the significant improvement of the preconditioned fused Lasso estimator over the vanilla FLSA in exact pattern recovery.

@&#INTRODUCTION@&#
Assume we have a sequence of signals(y1,y2,…,yn)and it follows the additive model(1)yi=μi∗+ϵi,i=1,2,…,n,whereY=(y1,…,yn)T∈Rnis the observed signal vector,μ∗=(μ1,…,μn)T∈Rnthe expected signal vector, andϵ=(ϵ1,…,ϵn)Tthe white noise such thatϵ1,…,ϵnare assumed to be i.i.d. Gaussian random variables with mean 0 and varianceσ2. The model is assumed to be blocky in the sense that the signals come in blocks and have only a few change-points. To be exact, there exists a partition of{1,2,…,n}=∪j=1J{Lj,Lj+1,…,Uj}withL1=1,UJ=n,Uj≥Lj,Lj+1=Uj+1, and the following stepwise function holds:μi∗=∑j=1Jνj∗1Lj≤i≤Uj,withνj∗,Lj,Ujfixed but unknown. We also assume that the vectorν=(ν1,ν2,…,νJ)is sparse, meaning that only a few ofνj’s are nonzeros. We point out that the Gaussian noise is not necessary, but we still use it to get insight of the fused Lasso. The varianceσ2ofϵiis the measure of noise level and does not have to be a constant here. In many cases, each observation ofyican be an average of multiple measurements and soσ2decreases when the number of measurements increases. Rinaldo (2009) considers the model whenσ2=σ02/n, whereσ0is a constant. We do not make this specific assumption in the development of our theory.Featured by blockiness and sparseness, this model has many applications. For example, in tumor studies, based on the Comparative Genomic Hybridization (CGH) data, it can be used to automatically detect the gains and losses in DNA copies by taking the “signal” above as the log-ratio between the number of DNA copies in tumor cells and that in reference cells (Tibshirani and Wang, 2008). For more applications, see Tibshirani and Taylor (2011), Friedman et al. (2007) and Hoefling (2010).One way to estimate the unknown parameters is via the Fused Lasso Signal Approximator (FLSA) defined as follows (Tibshirani et al., 2004; Friedman et al., 2007):(2)μˆ(λ1,λ2)=argminμ12‖Y−μ‖22+λ1‖μ‖1+λ2‖μ‖TV,where‖μ‖1=∑i=1n|μi|,‖μ‖22=∑i=1nμi2and‖μ‖TV=∑i=1n−1|μi+1−μi|. TheL1-norm regularization controls the sparsity (number of zeros) and the total variation seminorm(‖μ‖TV)regularization controls the blockiness (number of blocks or partitions).Fig. 1gives some CGH data, a typical example of signals with such features and a proper FLSA estimate on the data. More details and examples can be seen in Tibshirani and Wang (2008).One important question for the FLSA is how good the estimator defined in Eq. (2) is. We analyze in this paper if the FLSA can recover the “stepwise pattern” or not. We also try to answer the following question: what do we do if the FLSA does not recover the “stepwise pattern”? To measure how good an estimator is, we introduce the following definition of Pattern Recovery.Definition 1Pattern RecoveryAn FLSA solutionμˆ(λ1n,λ2n)recovers the signal pattern if and only if there existλ1nandλ2n, such that(3)sign(μˆi+1(λ1n,λ2n)−μˆi(λ1n,λ2n))=sign(μi+1∗−μi∗),i=1,…,n−1.We useμˆ=jsμ∗to shortly denote (3) (jsis the acronym forjumpsign). The FLSA with the property of pattern recovery means that it can be used to identify both the groups and the jump directions (up or down) between groups.The concept of pattern recovery of the FLSA is very similar to the sign recovery of the Lasso (Zhao and Yu, 2006). In fact, we will see in Section  2 that the pattern recovery property of the FLSA is equivalent to the sign recovery property of the Lasso after transformation.For observation pairs(xi,yi),i=1,2,…,nwithxi∈Rpandyi∈R, the Lasso estimator is defined as follows (Tibshirani, 1996):βˆ(λ)=argminβ12∑i=1n(yi−xiTβ)2+λ‖β‖1,or equivalently, in the matrix form,(4)βˆ(λ)=argminβ12‖Y−Xβ‖2+λ‖β‖1,whereY=(y1,…,yn)T∈RnandX∈Rn×pwithxiTas itsith row. We useXjto denote thejth column ofX. Lasso is a regularized method that gives sparse solutions. TheL1term in the objective function makes the solution sparse. For different purposes, other loss functions could be used. Xu et al. (2015) used Cauchy loss in the objective function to get robust estimators. For matrix parameters, nuclear norm and its variations could be used to get low rank estimators (Cai et al., 2010; Xu et al., 2014).Sign recovery of the Lasso is defined as follows.Definition 2Sign Recovery (Zhao and Yu, 2006)Suppose that data(X,Y)follow a linear model:Y=Xβ∗+ϵ, whereY=(y1,…,yn)T,X∈Rn×pwithxiTas itsith row,β∗∈Rp×1andϵ=(ϵ1,…,ϵn)T∈Rn×1withE(ϵi)=0. A Lasso estimatorβˆ(λn)has the sign recovery property if and only if there existsλnsuch that(5)sign(βˆj(λn))=sign(βj∗),j=1,…,p.We will useβˆ=sβ∗to shortly denote this property. In particular, this property, if satisfied, means that the Lasso selects the correct set of predictors. Asymptotically, we say the Lasso estimatorβˆ(λn)is sign consistent if there exists a sequence ofλnsuch thatP(βˆ(λn)=sβ)→1, as the sample sizen→∞.A rich theoretical literature has studied the consistency of the Lasso, highlighting several potential pitfalls (Knight and Fu, 2000; Fan and Li, 2001; Greenshtein and Ritov, 2004; Donoho et al., 2006; Meinshausen and Bühlmann, 2006; Tropp, 2006; Zhao and Yu, 2006; Zhang and Huang, 2008; Wainwright, 2009). The sign consistency of the Lasso requires the irrepresentable condition, a stringent assumption on the design matrix (Zhao and Yu, 2006). Now it is well understood that if the design matrix violates the irrepresentable condition, the Lasso will perform poorly in sign recovery, even with increased sample size.The analyses of the FLSA are fairly recent. Tibshirani and Taylor (2011) consider the FLSA as a special case of the generalized Lasso problem and give the boundary lemma, a remarkable property of the FLSA saying that along the solution path, the coordinates on the boundary will always stay on the boundary asλdecreases. Rinaldo (2009) considers the sign consistency of the FLSA and proposes the adaptive fused Lasso. It shows that under some conditions, the FLSA can be consistent in both block reconstruction and model selection. The fused Lasso applied to general graphs, called the edge Lasso, is considered in Sharpnack et al. (2012). The ability of exact pattern recovery depends heavily on the structural properties of the graph. In particular, the author mentions the sign inconsistency of the 1D fused Lasso, the FLSA here, but does not attempt at addressing this issue.There is a very close connection between the fused Lasso and change point estimation. In Harchaoui and Lévy-Leduc (2008) and Harchaoui and Lévy-Leduc (2010), the authors study the change point estimation problem using the total variation penalty, which is the fused Lasso whenλ1is taken as 0. Harchaoui and Lévy-Leduc (2010) have a very nice result for consistency on the estimation of the signal mean. They also point out that the total variation penalized least squares cannot consistently estimate the locations of change points. Instead, they show that under a few regularity conditions, the estimated locations of the change points are close to the true locations. In this paper, we give deeper analysis of the total variation penalized least squares. We give a necessary and sufficient condition on consistently recovering the piecewise constant pattern, which is also called consistent recovery of change points.We borrow analytical tools used to study the LASSO in the paper. There are several differences between the fused Lasso and Lasso that should be highlighted, though the former can be transformed to the latter by reparametrization. First, though could be seen as a specific kind of Lasso problem under reparametrization, the recovery of piecewise constant signals, or detection of multiple change points itself is an important class of problems and has very wide applications. So it is worthwhile to gain more insight into this problem beyond general Lasso’s and to consider fast and consistent algorithms as we do here. Second, the analysis of sign consistency is different than classic Lasso problems in that the attempt to transform to Lasso introduces an intercept term that is not penalized as other coefficients. The intercept can be replaced by the mean of observations, however, extra care should be taken before directly applying the Lasso theories. We adapt the argument for standard Lasso to general noise structures other than independent noise.In this paper, we not only study the sign recovery of the FLSA using the irrepresentable condition for the Lasso, but more importantly, by preconditioning the design matrix, our newly proposed method significantly improves the performance of pattern recovery. For preparation, we first prove that even for the linear model with correlated noise, the irrepresentable condition is still necessary for sign consistency. We then analyze the design matrix in the transformed Lasso problem. We give necessary and sufficient condition such that the design matrix in the transformed Lasso problem complies with the irrepresentable condition. We show that, only for a special class of models (with special designed stepwise function onμi∗), the irrepresentable condition holds. For other signal patterns, the irrepresentable condition does not hold and thus the FLSA may fail to keep consistent. A recent paper “Preconditioning to comply with the irrepresentable condition” by Jia and Rohe (2015) suggests a Puffer Transformation that will improve the Lasso and make the Lasso estimator sign consistent under some mild conditions. We apply this technique, propose the preconditioned fused Lasso and show that it significantly improves the FLSA and recovers the signal pattern with high probability. We also point out that we did not fully solve the change point estimation problem. Our results show that if the signal is strong and the noise is small, then no matter what the pattern is, the preconditioned fused Lasso gives consistent estimation of the change point locations. In change point literature, people also study some special patterns — say when the block size is big enough. With more information on the signal pattern, one may have stronger results. This is now out of our research scope and will be our further study. In this paper, we try to understand the fused Lasso (equivalently the total variation penalized least squares) for one dimensional signals.The rest of the paper is organized as follows. In Section  2, we establish connection between the FLSA and a Lasso problem via proper transformation. Section  3 discusses when the FLSA can recover the signal pattern and when it cannot. In Section  4, we propose a new algorithm called the preconditioned fused Lasso that improves the FLSA by using the preconditioning technique. We show that for a wide range of designs of the stepwise function onμ∗, this algorithm can recover the signal pattern with high probability. In Section  5, simulations are implemented to compare the performances between the preconditioned fused Lasso and the vanilla FLSA. Section  6 concludes the paper. The proofs are given in the Appendix.In this section, we transform the FLSA problem into a Lasso problem by change of variables. Define the soft thresholding functionSHλ(x)asSHλ(x)={x+λx<−λ0−λ≤x≤λx−λx>λ.Letμˆ(λ1,λ2)be the fused Lasso estimator defined in (2). We have the following result.Lemma 1Friedman et al., 2007μˆ(λ1,λ2)=SHλ1(μˆ(0,λ2)).From Lemma 1, to study the properties ofμˆ(λ1,λ2), we can setλ1=0first. Since pattern recovery is our main concern here, we only consider the caseλ1=0in this paper. Whenλ1=0, we can solve the FLSA by change of variables. Letθ1=μ1,θi=μi−μi−1,i=2,…,n, or in the matrix form,μ=Aθ, whereA∈Rn×nis the lower triangular matrix with nonzero elements equal to one. So by usingθinstead ofμ, we have an equivalent solution ofμˆ(0,λ2)via the followingθˆ(λ2):(6)θˆ(λ2)=argminθ12‖Y−Aθ‖22+λ2‖θ̃‖1,whereθ̃=(θ2,θ3,…,θn)T∈Rn−1. Once we obtainθˆ(λ2), we haveμˆ(0,λ2)=Aθˆ(λ2). With the special form of the design matrixA, (6) is a Lasso problem with intercept. In fact, (6) can be rewritten as(7)θˆ(λ2)=argmin(θ1,θ̃)12‖Y−1⋅θ1−Xθ̃‖22+λ2‖θ̃‖1where1=(1,…,1)T∈Rn,θ̃=(θ2,…,θn)TandX=(xij)∈Rn×(n−1):xij={1i>j0i≤j.Define the centered version ofX∈Rn×(n−1)andY∈Rnas follows:(8)X̃=[X1−X1̄,…,Xn−1−X̄n−1]andỸ=Y−Ȳ,whereūis the vector with all elements equal to the average ofu. It is easy to see that (7) is equivalent to the following standard Lasso problem without intercept:(9)θ̃ˆ(λ2)=argminθ̃12‖Ỹ−X̃θ̃‖22+λ2‖θ̃‖1,andθˆ1(λ2)=Ȳ−X̄θ̃ˆ(λ2).Defineθ∗=A−1μ∗; that is,θ1∗=μ1∗,θi∗=μi∗−μi−1∗,i=2,…,n. Letθ̃∗=(θ2∗,θ3∗,…,θn∗)T∈Rn−1. Since the observationY=(y1,…,yn)follows the model defined in (1), we have that(X,Y)satisfies the linear model:Y=Aθ∗+ϵ=θ1∗+Xθ̃∗+ϵ,whereXis defined at (2). Thus the centered version of(X,Y)satisfies the following linear model:(10)Ỹ=X̃θ̃∗+ϵ̃,whereϵ̃=ϵ−ϵ̄, the centered version. Now we see thatθ̃ˆ(λ2)defined in (9) has the sign recovery property if and only ifθ̃ˆ(λ2)=sθ̃∗. By the relationship betweenμandθ(μ=Aθ),θ̃ˆ(λ2)=sθ̃∗is equivalent toμˆ(0,λ2)=jsμ∗. In other words, the pattern recovery property of the FLSA is equivalent to the sign recovery of the corresponding Lasso estimator.Property 1The pattern recovery of the FLSAμˆ(0,λ2)defined in   (2)   is equivalent to the sign consistency of the Lasso estimatorθ̃ˆ(λ2)defined in   (9).Note that the main purpose of the change of variables here is for theoretical analysis rather than computational considerations. Although there are many efficient algorithms for the Lasso such as LARS and coordinate descent, it is not recommended in practice to solve the FLSA by transforming to the Lasso and then applying these algorithms. The transformation makes the design matrix in (9) dense and is computationally unfavorable. Instead, Friedman et al. (2007) develop specialized algorithm for the FLSA based on the coordinate-wise descent. Hoefling (2010) proposes the path algorithm and extends that to more general fused Lasso problems. In our theoretical analysis, however, such transformation helps since we can use well understood techniques for the Lasso to analyze the properties of the FLSA.From the above argument, the study of the FLSA is reduced to analyzing the Lasso problem defined in (9). It is now well understood that in a standard linear regression problem the Lasso is sign consistent when the design matrix satisfies some stringent conditions. One such condition is the irrepresentable condition (Zhao and Yu, 2006) defined as follows:Definition 3Irrepresentable ConditionSuppose that data(X,Y)follows a linear model:Y=Xβ∗+ϵ, whereY=(y1,…,yn)T,X∈Rn×p,β∗∈Rp×1andϵ=(ϵ1,…,ϵn)T∈Rn×1withE(ϵi)=0. The design matrixXsatisfies the Irrepresentable Condition forβ∗with supportS={j:βj∗≠0}if, for someη∈(0,1],(11)‖XScTXS(XSTXS)−1sign(βS∗)‖∞≤1−η,where for a vectorx,‖x‖∞=maxi|xi|, and forT⊂{1,…,p}with|T|=t,XT∈Rn×tis a matrix which contains the columns ofXindexed byT.The irrepresentable condition is a key condition for the Lasso’s sign consistency. A lot of researchers noticed that the irrepresentable condition is a necessary condition for the Lasso’s sign consistency (Zhao and Yu, 2006; Wainwright, 2009; Jia et al., 2013). We also state this conclusion under a more general linear model with correlated noise terms.Theorem 1Suppose that data(X,Y)follows a linear modelY=Xβ∗+ϵ, with Gaussian noiseϵ∼N(0,Σϵ). The irrepresentable condition   (11)   is necessary for the sign consistency of the Lasso. In other words, if(12)‖XScTXS(XSTXS)−1sign(βS∗)‖∞≥1,we haveP(βˆ(λ)=sβ∗)≤12.The proof of Theorem 1 is postponed to Appendix. This theorem says if the irrepresentable condition does not hold, it is very likely that the Lasso cannot correctly recover the signs of the coefficients.With the above theorem, we come back to the transformed Lasso problem defined in (9) and examine if the irrepresentable condition holds or not in this case. Recall that for the Lasso problem induced from the FLSA, we have the design matrixX̃=[X1−X1̄,…,Xn−1−X̄n−1].LetS={j:θ̃j∗≠0}denote the index set of the relevant variables in the true model. Then (11) can be written as|Xj̃TXS̃(XS̃TXS̃)−1sign(θ̃∗)|<1,∀j∉S.This is equivalent to|bˆjTsign(θ̃∗)|<1,∀j∉S,wherebˆj∈R|S|is the OLS estimate ofbjin the following linear regression equation:(13)Xj̃=bjTX̃S+ϵ.SinceX̃is the centered version ofX, it can be easily shown thatbˆjis also the OLS estimate ofbjin the following linear regression equation:(14)Xj=b0+bjTXS+ϵ,whereb0∈Ris the intercept term. Recall the irrepresentable condition defined in (11):‖XScTXS(XSTXS)−1sign(βS∗)‖∞≤1−η.We can thus find all the signal patterns in (1) such that the irrepresentable condition holds for the corresponding transformed Lasso.Theorem 2Assumey=(y1,…,yn)satisfies model   (1), the collection of the indices of jump points areS={j1,j2,…,js}withjk(1≤k≤s)increasing. Formally,S={j:μj∗≠μj−1∗,j=2,…,n}. Then the irrepresentable condition   (11)   holds for the transformed Lasso if and only if one of the following two conditions holds.(1)The jump points are consecutive. That is,s=1ormax1≤k<s(jk+1−jk)=1.If there exists one group of data points (with more than  1  point) between some two consecutive jump points and these data points have the same expected signal strength, then the two jumps are of different directions (up or down). Formally, letjkandjk+1be two jump points andμjk∗=⋯=μjk+1−1∗, then(μjk∗−μjk−1∗)(μjk+1∗−μjk+1−1∗)<0.The proof is given in the Appendix. Theorem 2 implies that only a few configurations ofμ∗can make the transformed Lasso comply with the irrepresentable condition. In applications, most signal patterns do not satisfy either of the two conditions in Theorem 2. Harchaoui and Lévy-Leduc (2010) also provide a result considering the irrepresentable condition. They show that the irrepresentable condition never holds for a slightly modified total variation penalty. Note that the penalty term for original total variation penalty term is∑i=2p|βi+1−βi|, while Harchaoui and Lévy-Leduc (2010) give a result for a slightly different penalty∑i=2p[|βi+1−βi|]+β1. With the small modification, the result is also different.We noticed that Rinaldo (2009) gave a result (Theorem 2.3 on page 2930) saying that under some regularity conditions, the fused Lasso consistently recovers the signal pattern. This is a contradiction with Theorem 2, since those regularity conditions only depend on the signal strengths, the number of blocks and the minimal size of the blocks. Because these conditions do not guarantee the irrepresentable condition, that result missed this irrepresentable condition.For sign recovery defined in (5), Jia and Rohe (2015) proposed a Puffer Transformation that preconditions the design matrix in order to comply with the irrepresentable condition. The connection between sign recovery and pattern recovery defined in (3) enables us to apply the same technique and thus improve the performance over the vanilla FLSA in pattern recovery.Jia and Rohe (2015) introduce the Puffer Transformation to the Lasso when the design matrix does not satisfy the irrepresentable condition. They showed that whenn≥p, even if the Lasso is not sign consistent, after the Puffer Transformation, the Lasso is sign consistent under some mild conditions.We assume that the design matrixX∈Rn×phas rankd=min{n,p}. By the singular value decomposition, there exist matricesU∈Rn×dandV∈Rp×dwithUTU=VTV=Idand a diagonal matrixD∈Rd×dsuch thatX=UDV′. Define the Puffer Transformation  (Jia and Rohe, 2015),(15)Fn×n=UD−1UT.The preconditioned design matrixFXhas the same singular vectors asX. However, all of the nonzero singular values ofFXare set to unity:FX=UV′. Whenn≥p, the columns ofFXare orthonormal. Whenn≤p, the rows ofFXare orthonormal. Jia and Rohe (2015) have a non-asymptotic result for the Lasso on(FX,FY)stated in Theorem 4 in the Appendix. We see that with the Puffer Transformation, the Lasso does not need the irrepresentable condition any more.As shown previously, the FLSA can be transformed to a standard Lasso problem. We have already shown that for most configurations ofμ∗, the design matrixX̃does not satisfy the irrepresentable condition. Now we turn to the Puffer Transformation and obtain a concrete non-asymptotic result for the preconditioned fused Lasso.Theorem 3Assumey=(y1,…,yn)satisfies model   (1).X̃andỸare defined in   (8). Letθ∗=A−1μ∗(equivalently,θ1∗=μ1∗,θi∗=μi∗−μi−1∗,i=2,…,n), whereAis defined to be the lower triangular matrix with nonzero elements equal to one. Letθ̃∗∈Rn−1=(θ2∗,θ3∗,…,θn∗)T. Define the singular value decomposition ofX̃asX̃=UDVT. Denote the Puffer Transformation byF=UD−1UT. LetZ=FX̃anda=FỸ. Define(16)β̃(λ)=argminb12‖a−Zb‖22+λ‖b‖1.Ifminj≥2,θj∗≠0|θj∗|≥2λ, thenP(β̃(λ)=sθ̃∗)≥1−2nexp{−λ28σ2}.The proof is given in the Appendix. By the relationship betweenθ∗andμ∗, ifβ̃(λ)— the estimate ofθ̃∗has the sign recovery property, then the estimate ofμ∗defined as follows has the property of pattern recovery.(17)μˆ∗=Aθˆ∗withθˆ∗=[θˆ1,β̃(λ)]andθˆ1=Ȳ−X̄β̃(λ).Theorem 3 shows that the pattern recovery of the preconditioned fused Lasso depends on the signal jump strength(minj≥2,θj∗≠0|θj∗|)and the noise levelσ2. To get a pattern-consistent estimate, we needσsmall enough andminj≥2,θj∗≠0|θj∗|big enough. To think about the smallσ2issue, we can treat eachyias an average of multiple Gaussian measurements. If the number of measurements ism, thenσ2=σ02mwith some constantσ02. Ifm≫log(n), we can find a very smallλto make the estimator defined in (17) have the pattern recovery property. One choice ofλis such thatλ2=log(n+1)m. For this choice ofλ, the probability ofμˆ∗=jsμ∗is greater than1−2exp(−[m8σ02−1]log(n+1)), which goes to 1 asngoes to∞.We use several examples to illustrate our theory. Recall the model is set to beyi=μi∗+ϵi,where the errorsϵi’s are i.i.d. Gaussian variables with mean 0 and standard deviationσ. In the following simulations, the length of the signal is set to be 430, not for others, but is just the same as the signal length in the example in Rinaldo (2009) and more convenient for comparison.μi∗will be specified case by case, reflecting the characteristics of the signal pattern.σwill generally vary between 0.05 and 0.5 to illustrate the recovery ability as a function of the noise level. Here are some implementation details of the two procedures.FLSA When calculating the FLSA solution, we use a path algorithm proposed by Hoefling (2010) which is very efficient to give the entire solution path of the FLSA. An R package (“flsa”) for this algorithm is available in http://cran.r-project.org/web/packages/flsa/index.html. In fact, the entire solution path is piecewise linear inλ. “flsa” only stores theλ’s corresponding to the breakpoints at which the directions of the linear function change.Preconditioned fused Lasso We calculate the solution defined in (16). After the SVD and the Puffer Transformation, the task becomes much easier. It suffices to do soft-thresholding to obtain the entire solution path. This is becauseZTZ=XTFTFX=(VDUT)(UD−1UT)(UD−1UT)(UDVT)=Inand the property of the Lasso allows us to solve it directly by soft-thresholding (Tibshirani, 1996):bˆ(λ)=SHλ(ZTa).Obviously,bˆ(λ)is piecewise linear inλand the breakpoints areλi=|ZTa|(i),i=1,2,…,n, wherex(i)denotes theith largest value in vectorxandnis the dimension of the vectorZTa.By a little further algebra analysis, we see that the Preconditioned Fused Lasso estimator can be calculated via soft thresholding of the successive differences of the observed signals. This is becauseZTa=X̃TFTFỸ=(X̃TX̃)−1X̃TỸ=argminθ‖Ỹ−X̃θ‖2=argminθ‖Y−Xθ−θ1‖2={argminθ‖Y−Aθ‖2}[2:n]={A−1Y}[2:n],whereA∈Rn×nis the lower triangular matrix with nonzero elements equal to one, andx[2:n]stands for the vector consisting of the2ndtonthelements ofx. Sobˆi(λ)=SHλ(Yi+1−Yi),i=1,2,…,n−1.Despite equivalence to this simplified thresholding algorithm, we point out that the preconditioned fused Lasso can be extended easily to more general settings. For example, suppose we would like to recover the blocks in an unknown coefficient sequence by solving(18)βˆ=argminβ‖y−Xβ‖22+λ∑i=1n−1|βi+1−βi|.We can do the transformationβ=Aγ, whereAis the same as in (6). The objective can again be transformed to a Lasso-type, only no penalty onγ1. Then we can apply the preconditioned technique, in particular the Puffer Transformation as in Theorem 3, to the responseyand the new matrixXAafter reparametrization, and solve resulting problem. We do a few experiments for this problem and report one simulated result at the end of this section.There are many criteria for comparison. In the context of exact pattern recovery, our principle is to check the solution path and see if there is a solution that has exactly the same jump points as the true signals.For eachσselected, we draw 1000 sample sequences and definePFLSAandPPCDFLto be the proportion of samples that the recovered blocks (jump points) exactly match the true blocks (jump points) by using the FLSA and using the preconditioned fused Lasso, respectively.We will demonstrate that when irrepresentable condition holds, FLSA can recover the true blocks with high probability; when irrepresentable condition does not hold, FLSA performs poorly, and for this case Puffer Transformation helps a lot.We give specific examples of the signal patterns such that the irrepresentable condition of the transformed Lasso holds and thus the FLSA can recover the pattern under mild conditions. Theorem 2 provides the necessary and sufficient conditions for irrepresentable condition. Since the second condition in Theorem 2 is more commonly met, we focus on the signals that satisfy this condition in the following.We used two examples to show that when signal pattern follows the second condition in Theorem 2, both FLSA and preconditioned FLSA can recover the signal pattern when noise is small. The sample data and results are shown in Fig. 2. The top row gives the expected signal (plotted in lines) and the sampled noisy data (plotted in dots). The two signal patterns are slightly different — the left one is more regular in the sense that the block size is almost the same. It is not clear when transformed FLSA is better than vanilla FLSA when irrepresentable condition holds. But when irrepresentable condition does not hold, which happens more likely than it holds, preconditioned FLSA definitely outperforms the vanilla FLSA. We demonstrate this in the next subsection.When the irrepresentable condition does not hold, the FLSA cannot reliably recover the exact pattern. We analyze in more detail the numerical performances of the two procedures.We use the same example as in Rinaldo (2009) except for larger noise (σ=0.25here). Recall that the signal pattern is set to beμi∗={0,1≤i≤100−2,101≤i≤110−0.1,111≤i≤2102,211≤i≤2200.1,221≤i≤320−2,321≤i≤3300,331≤i≤430.Fig. 3shows the sample data and true signals.We apply the FLSA and the preconditioned fused Lasso to this sample data and compare the recovery performances. Fig. 4plots two solutions by different selection of tuning parameter. The solution in the left plot is the one with tuning parameter selected by recovering the same number of blocks as that ofμ∗, the true signals; Right panel plots the FLSA solution (in red lines) with tuning parameter selected by minimizing theℓ2error betweenμˆ∗andμ∗. Fig. 4 shows that the FLSA cannot locate the jump points correctly and the right subfigure illustrates that a good estimate under the Euclidean norm is not reliable in exact pattern recovery. In sharp contrast, the preconditioned fused Lasso applied shown in Fig. 5precisely locates all the jump points and recovers the pattern.Note that the reported preconditioned FLSA estimate in Fig. 5 is very biased from the expected value. There is a tradeoff between the unbiasedness and the quality of pattern recovery. One possible solution for the unbiasedness is via a two-stage estimator — for the first stage the signal patten is recovered and for the second stage an unbiased estimate is obtained.We further compare the recovery performances under different noise levels. We draw 1000 random sample sequences and compare the approximate exact recovery probability. Fig. 5 visualizes the result.The FLSA can hardly recover the signal pattern exactly even when the noise level is as small asσ=0.05. This is supported by the theory above. In contrast, the preconditioned fused Lasso has fairly satisfactory recovery performance tillσ=0.25.Finally, we provide one simulation result for the problem ofminβ‖Y−Xβ‖22+λ‖β‖TV.In this simulation study, we choose each row ofXi.i.d. fromN(0,Σ), whereΣij=0.4, fori≠jandΣii=1. We have a design matrixX∈Rn×pwithn=200andp=100. We takeβto have the pattern of piecewise constant. The true coefficients are plotted in Fig. 6(left).Yis designed asXβ+ϵ, whereϵ∼N(0,σ2). We varyσfrom 0.25 to 5. We apply both preconditioned method (denoted as PCFL) and the one without preconditioning (denoted as FLSA). For each noise levelσ, we do 500 repetitions and then we calculate the proportion of successful cases when one method could recover the piecewise pattern of the coefficients. The results are plotted in Fig. 6 (right), from which we see the necessity of preconditioning.In this paper we provided more understanding of the FLSA and shed some light on the insight into the FLSA. The FLSA can be transformed to a standard Lasso problem. The sign recovery of the transformed Lasso problem is equivalent to the pattern recovery of the FLSA problem. Theoretical analysis showed that the transformed Lasso problem is not sign consistent in most situations. So the FLSA might also meet this consistency problem when it is used to recover signal patterns. To overcome such problem, we introduced the preconditioned fused Lasso. We gave non-asymptotic results on the preconditioned fused Lasso. The result implies that when the signal-to-noise ratio is not so small, the preconditioned fused Lasso can recover the signal pattern with high probability. Through simulation studies, we also found that the FLSA, if the irrepresentable condition holds, is only more apt at recovering regular signals while our preconditioned fused Lasso is more robust to various kinds of signals. Some attempts also imply that the preconditioned fused Lasso does not work so well if all the signals are equally weak.A good pattern recovery will facilitate many things afterwards. The preconditioned fused Lasso is reliable for pattern recovery, and so it can be incorporated into other processes — such as the recovery of sparsity.

@&#CONCLUSIONS@&#
