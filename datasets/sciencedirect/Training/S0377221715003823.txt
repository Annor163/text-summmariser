@&#MAIN-TITLE@&#
A direct search method for unconstrained quantile-based simulation optimization

@&#HIGHLIGHTS@&#
A direct search method is developed for unconstrained quantile-based simulation optimization.Effective quantile estimation techniques and a sample size schedule are developed.Without requiring gradient estimation, SNM-Q can handle many practical problems.We prove that SNM-Q possesses global convergence guarantee.Numerical experiments show that the performance of SNM-Q is promising.

@&#KEYPHRASES@&#
Simulation,Quantile,Direct search method,Nelder–Mead simplex method,

@&#ABSTRACT@&#
Simulation optimization has gained popularity over the decades because of its ability to solve many practical problems that involve profound randomness. The methodology development of simulation optimization, however, is largely concerned with problems whose objective function is mean-based performance metric. In this paper, we propose a direct search method to solve the unconstrained simulation optimization problems with quantile-based objective functions. Because the proposed method does not require gradient estimation in the search process, it can be applied to solve many practical problems where the gradient of objective function does not exist or is difficult to estimate. We prove that the proposed method possesses desirable convergence guarantee, i.e., the algorithm can converge to the true global optima with probability one. An extensive numerical study shows that the performance of the proposed method is promising. Two illustrative examples are provided in the end to demonstrate the viability of the proposed method in real settings.

@&#INTRODUCTION@&#
When many real-world problems involve randomness and are too complex to be solved by analytical tools, stochastic simulation (henceforth called “simulation”), which relies on the internal generation of pseudorandom numbers to represent the randomness in relevant events, can provide useful insights into the system under study. The advantage of simulation is that it can account for any detail that is important and moreover, it is useful for answering “what-if ” questions. Over the past decade, due to the rapid advancement of computing power, an emerging field, called simulation optimization, which refers to a systematic methodology useful for locating the optimal parameter values of the system under study when the objective function can only be evaluated by simulation, has gained more popularity in the simulation community. In fact, many real-world problems can be cast in the framework of simulation optimization, for example, choosing the number of automatic guided vehicles (AGVs) and load per AGV to minimize the mean waiting time for an automated material handling system, and choosing redundancy and repair capability for components, subject to a budget constraint, to maximize long-run system reliability.In the literature, much research has been devoted to developing simulation optimization methodologies, which, in general, can be classified into three categories: stochastic approximation (SA), response surface method (RSM) and metaheuristics. SA, first developed by Robbins and Monro (1951) and later generalized by Kiefer and Wolfowitz (1952), is one of the most prevalent and extensively studied methods in stochastic optimization over the past decades, e.g., Benveniste, Metivier, and Priouret (1990), Andradóttir (1995), Kushner and Yin (1997), Wang and Spall (2008), Bhatnagar, Hemachandra, and Mishra (2011) and Andrieu, Cohen, and Vázquez-Abad (2011). The advantages of SA are that it is conceptually easy to implement and moreover, it has a provable convergence guarantee under regularity conditions (Kushner & Yin, 1997).RSM (Myers, Montgomery, & Anderson-Cook, 2009) iteratively builds metamodels to approximate the objective function and obtain approximate solutions. Chang, Hong, and Wan (2013) proposed an improved RSM-based framework, called Stochastic Trust-Region Response-Surface method (STRONG), which introduces the concept of “trust region” to eliminate the need for human involvements and to guarantee algorithm convergence. Chang, Li, and Wan (2014) further proposed an extended framework, called STRONG-S, that combined STRONG and efficient screen designs to solve large-scale simulation optimization problems. The numerical experiments show that STRONG-S can solve problems with more than 500 factors in reasonable computing time.One final important class of methods for solving simulation optimization problems is metaheuristics, for example, genetic algorithm, tabu search, Nelder–Mead simplex method (NM) and scatter search etc (Spall, 2003). In particular, NM is an efficient heuristic developed for deterministic, nonlinear optimization problems (Nelder & Mead, 1965). Some NM-based methods and the applications have been reported in, e.g., Barton and Ivey (1996), Rahami, Kaveh, Aslani, and Asl (2011), Lagarias, Poonen, and Wright (2012), Gao and Hao (2012). In particular, Barton and Ivey (1996) proposed modified Nelder–Mead simplex method (MNM) that significantly enhances the computational efficiency of NM. The disadvantage of MNM, however, is that it has no convergence guarantee. Chang (2012) therefore proposed an improved framework, called Stochastic Nelder–Mead simplex method (SNM), that has provable convergence guarantee.All of the methodologies mentioned above are focused on problems with mean-based performance metric, which, however, concerns only the average behavior of system performance. Other behaviors, such as downside risk and upside risk, are ignored. According to Batur and Choobineh (2010), the downside risk involves “the probability of obtaining outcomes smaller than a target value, while the upside risk involves the probability of obtaining outcomes larger than a target value”. For many practical problems, however, the downside or upside risks can be more important than means. Quantiles of a random system performance serve as important alternatives to the usual mean, allowing for the consideration of the downside and upside risks. The αth quantile of a continuous random variable Y, denoted by ξα, is defined by P[Y ≤ ξα] = α for prespecified α (0 < α < 1). The most well-known quantile is median where α = 0.5. Compared to mean, quantile provides additional and alternative information about the distribution of system performance. In particular, it is useful in describing tail behavior (Batur & Choobineh, 2010). For example, if Y is the delay of a wafer lot in semiconductor manufacturing, then 5 percent of the wafer lots have delays less than ξ0.05.In this paper, we modify the SNM framework to enable the solving of the unconstrained simulation optimization problems with quantile-based objective functions. The proposed method, called stochastic Nelder–Mead simplex method for quantile-based simulation optimization (SNM-Q), can be used, for example, to facilitate the determination of the staffing level in an emergency room of a hospital so as to maximize the service quality of an out-of-hospital system measured by the 90th percentile of the times taken to respond to emergency requests (Hong, 2009). SNM-Q utilizes the same search strategy as SNM’s but further includes effective quantile estimation techniques and algorithmic modifications to ensure algorithm convergence for the unconstrained quantile-based simulation optimization problems, i.e., the algorithm is guaranteed to converge to the true global optima with probability one (w.p.1).Having the same advantage as SNM, SNM-Q uses only function values to optimize the quantile-based objective function without requiring gradient estimation, thus it can solve problems where the gradient of objective function does not exist or is difficult to estimate. Complementary to SNM that is focused on problems with mean-based objective functions, SNM-Q is capable of solving the quantile-based simulation optimizations problems. An early version of SNM-Q is given in Chang and Lu (2014) where the algorithm compares a set of solutions by utilizing a ranking-and-selection procedure. This framework, however, suffers from two problems and thus is not easy to be applied: First, the sample size needed for each solution to be “correctly” compared is difficult to obtain; Second, the framework does not guarantee convergence. The SNM-Q proposed in this paper is an improved version that successfully addresses the two issues by incorporating an easy-to-follow sample size scheme and effective algorithmic modifications. We show that SNM-Q can achieve convergence for problems with quantile-based objective functions w.p.1. More details about the framework and the convergence proof will be presented in later sections.The rest of this paper is organized as follows. In Section 2, we mathematically define the unconstrained quantile-based simulation optimization problem. In Section 3, we discuss the quantile estimation methods employed in SNM-Q. In Section 4, we introduce the main framework of SNM-Q, followed by Section 5 where we prove the convergence of SNM-Q. In Section 6, we conduct an extensive numerical study to show the performance of SNM-Q. In Section 7, we provide two illustrative examples to demonstrate the viability of SNM-Q in real settings. We conclude with future direction in Section 8.Letxbe a p–dimensional vector of decision variables, ω be the randomness defined in the probability space(Ω,F,P),andG(x,ω)be the response of a simulation model evaluated atx.In this paper, we consider the following unconstrained quantile-based simulation optimization problem:(1)Minimizex∈Rpξα[G(x,ω)],α∈(0,1)It is assumed that for everyxthe objective function,ξα[G(x,ω)],is not analytically available and can only be estimated by stochastic simulation. In what follows, we useξα(x)to represent the fact that ξα(·) is a function ofx.In fact, Problem (1) differs from the formulation in the existing literature of simulation optimization, which is mainly focused on problems with mean-based performance metric. Moreover, the advantage of Problem (1) is that it allows for more flexibility when seeking the “best” decision of many practical problems. However, it can be a difficult task to solve Problem (1). The main point is that the existing gradient estimation techniques in simulation optimization, e.g., Fu (2006), are largely concerned with problems whose performance metrics are mean- rather than quantile-based. Without appropriate gradient estimation techniques, it is difficult for the algorithm to determine where to move in the parameter space for the next iteration.Even if the algorithm is gradient-free, i.e., the algorithm does not rely on gradient estimation to determine the moving direction, the algorithm is still confronted by an essential problem that whether the newly-found solution should replace the current best one. When the objective function is quantile-based, this problem becomes even more challenging than that of mean-based ones because the quantile properties are not as nicely behaved as those of means. As a result, the algorithm can replace a good solution with a poor one and finally fail to converge.For any fixedx,letFxn(t)be the empirical discrete c.d.f based on samples {Gi, i = 1, 2, … , n}. Further, let G(1) ≤ G(2) ≤ … ≤ G(n) be order statistics obtained by sorting the observations {Gi, i = 1, 2, … , n} in an ascending order. Then,Fxn(t)is defined as follows:(2)Fxn(t)={0ift<G(1),inifG(i)≤t<G(i+1)and1≤i≤n−1,1ifG(n)≤t,=1n∑i=1n1(Gi≤t)where 1(·) denotes the indicator function. One natural choice of quantile estimators would be(3)ψ1={G⌈np⌉ifα=0G⌈np⌉+1ifα>0where ⌈np⌉ denotes the largest integer less than or equal to np. It is known that ψ1 is strongly consistent forξα(x),as stated in Serfling (1980) and moreover, it is asymptotically normal, as stated in Theorem 1.Theorem 1(Serfling, 1980) Let 0 < α < 1. IfFxpossesses a density f in a neighborhood of ξα and f is positive and continuous atξα(x),then ψ1is asymptotically normal with meanξα(x)and varianceα(1−α)f2(ξα(x))n.As noted in Batur and Choobineh (2010), while ψ1 provides a very accurate and precise result, it is an inefficient estimator due to its large variance. One approach to remedy this shortcoming is to estimate quantiles using linear combinations of order statistics, termed “L-estimators.” Harrell and Davis (1982) proposed the following L-estimator for estimating quantiles:(4)ψ2=∑i=1nWiG(i)where Wi= Ii/n{α(n + 1), (1 − α)(n + 1)} − I(i−1)/n{α(n + 1), (1 − α)(n + 1)} and where Ix(a, b) denotes the incomplete beta function. Another alternative L-estimator is proposed by Kaigh and Lachenbruch (1982), called generalized quantile estimator, that is obtained by averaging an appropriate subsample quantile over all subsamples of a fixed size:(5)ψ3=∑i=uu+n−mWiG(i)whereWi=(i−1u−1)(n−im−u)(nm)−1and where u = [(m + 1)p] and subsample size m = [n/2]. It can be proved that ψ2, ψ3 are also asymptotically unbiased (Dielman, Lowrt, & Pfaffenberger, 1994).All of the quantile estimators ψ1, ψ2, ψ3 are single-sample-based. Two problems, however, prevent them from being directly applied to estimate quantiles in an optimization framework. Firstly, they require a large space in a computer to store a lot of samples for producing accurate quantile estimates. The situation is further exacerbated when the solutions under comparison have very close objective values (usually occurred when the algorithm is close to convergence). Secondly, the relation between the accuracy of quantile estimates and the sample size involves the unknown density value atξα(x). This makes it difficult to determine the sample size in order to achieve the required accuracy.To address these problems, we use the multiple-sample-based quantile estimator proposed by Avramidis and Wilson (1998), where the quantile is estimated by computing the sample mean of N independent single-sample quantile estimates based on sample size m. Specifically, letξ^α(i)(x;m)denote the ith single-sample quantile estimate based on m samples, i = 1, … , k. Then, the multiple-sample estimator is simply the mean of the single-sample estimators(6)ξ¯α(x)=N−1∑i=1Nξ^α(i)(x;m).The advantage of the multiple-sample estimator is that there is no need to store all samples. Instead, the algorithm simply needs to keep track ofξ¯α(x)and updates it when more independent single-sample quantile estimates are obtained. Also, as will be shown later, the multiple-sample estimator allows the algorithm to “correctly” compare a set of solutions when N is appropriately chosen.Following the search strategy of NM, SNM-Q first forms a simplex, which is a convex hull of p + 1 points inRpnot lying in the same hyperplane, and then reflects the point with the worst (estimated) objective value through the centroid of the remaining points. When the new solution is satisfactory, the simplex will be enlarged to search for a better solution. On the other hand, when the new solution is unsatisfactory, the simplex will be contracted to restrict the search in a smaller region. If all the previous steps fail, different from NM where a shrink step is performed, SNM-Q employs a global/local search framework, called adaptive random search (ARS), to find a satisfactory solution. ARS, constituted by a local search and a global search, is designed to ensure a satisfactory solution can be found when the contraction step fails to generate a satisfactory solution. The details about ARS are given in the appendix.Letξ^αk(x)=Nk−1∑i=1Nkξ^α(i)(x;m)be the quantile estimate ofξα(x)at iteration k, where Nkis the “sample size” at iteration k and Nk≥ Nk−1. For p + 1 simplex points, letxkmax,xk2max,andxkminrepresent the points that have the largest, second largest, and smallest estimates of function values, respectively at iteration k. For any iteration k, SNM-Q conducts the following four steps:Step 1.Generate a new solutionxk(Section 4.2).Take Nkand Nk− Nk−1 samples at the new solution and the solutions that have been visited, respectively.Calculate or updateξ^αk(x)for the new solution and the solutions that have been visited. Remove the solution that has the largestξ^αk(x).Stop if a prespecified terminating criterion is fulfilled or if the maximum number of function evaluations has been reached; returnx*=xkmin.Otherwise, go to Step 1.Similar as SNM, in order to ensure that the solutions can be correctly ranked, SNM-Q requires an appropriate sample size schedule, which is a sequence of positive integer numbers {N1, N2, … Nk} representing the sample size of the solutions at iterations 1, 2, … , k. The sample size schedule would guarantee the correct acceptance/rejection of the new solution and the identification of the best solution in the set of simplex points. Also, the comparison of the quantile estimates of all solutions allows SNM-Q to decide on where to move for next iteration.As it is mentioned earlier, for any iteration k, SNM-Q generates a new solution in a similar way as NM, except that when all previous steps fail SNM-Q employs ARS instead of performing the “shrink” step in order to avoid premature convergence. The detailed steps of generating a new solution is given as follows.Step 1.Findxkcent,the centroid of all vertices other thanxkmax.Rank all points and identifyxkmax,xk2max,andxkminaccording to their (estimated) objective values. Generate a new pointxkrefby reflectingxkmaxthroughxkcentaccording toxkref=(1+λ)xkcent−λxkmax(λ > 0). Calculate the value ofξ^αk(xkref).Ifξ^αk(xkmin)≤ξ^αk(xkref)<ξ^αk(xk2max),then letXk=Xk−1⋃{xkref}.Ifξ^αk(xkref)<ξ^αk(xkmin),then the reflection point is expanded usingxkexp=μxkref+(1−μ)xkcent,where μ > 1. Ifξ^αk(xkexp)<ξ^αk(xkref),then letXk=Xk−1⋃{xkexp}.Otherwise, letXk=Xk−1⋃{xkref}.Ifξ^αk(xkref)≥ξ^αk(xk2max),then the simplex contracts. If (i)ξ^αk(xk2max)≤ξ^αk(xkref)<ξ^αk(xkmax)the contraction point is determined byxkcont=βxkref+(1−β)xkcent,0≤β≤1(outside contraction). If (ii)ξ^αk(xkref)≥ξ^αk(xkmax),the contraction point is determined byxkcont=βxkmax+(1−β)xkcent(inside contraction). In case (i), ifξ^αk(xkcont)≤ξ^αk(xkref),the contraction is accepted. In case (ii), ifξ^αk(xkcont)≤ξ^αk(xkmax),the contraction is accepted. If the contraction is accepted, letXk=Xk−1⋃{xkcont}.Otherwise, perform Adaptive Random Search (Section 4.3) to find an appropriatexARSand letXk=Xk−1⋃{xARS}.To sum up, SNM-Q generates a solution with the reflection point, followed by the contraction point if the reflection point is not promising, and finally relies on the ARS when all previous steps fail. As is shown in Section 5, because ARS allows for a positive probability that the algorithm can find a satisfactory at each iteration, the algorithm can achieve convergence w.p.1.In this section, we prove that the proposed SNM-Q framework can converge to the true global optima w.p.1. The proving steps are essentially identical to SNM. First recall that a new solution is only accepted if its quantile estimate is smaller than that ofxkmax,and a solution is removed if it has the largest quantile estimate in the set of simplex points. For each iteration k, letxk*denote the solution that has the largest true quantile estimate inXk,and let the indicator process beI(k)={1ifξ^αk(xk*)≥maxx∈Xk∖xk*ξ^αk(x)0otherwise.Lemma 1(Dai, 1996) Ifξ^α(i)(x;m)is an i.i.d. random variable with (nondegenerate) normal distribution, there exists a positive number γ > 0 such that(7)Pr[I(k)=1]=1−O(Nk−1/2e−γNk),andPr[I(k)=0]=O(Nk−1/2e−γNk).Lemma 1 shows that the probability that the solution with the largest objective value also has the largest quantile estimate in the set of simplex points will converge to one as the sample size Nkgoes to infinity. Further, the rate of convergence for the indicator process in the case of averaging i.i.d. normal random variables is in fact exponential. Note that the bound given in Lemma 1 does not depend on the solutions in the parameter space. Also, the single-sample quantile estimator ψ1, ψ2, ψ3 given in Section 3 are asymptotically normal and thus fulfill the requirement of Lemma 1.While the rate of convergence for the indicator process is exponential, SNM-Q requires more to be shown to converge to the true global optima w.p.1. In fact, it is required that the sample size schedule employed in SNM-Q satisfy(8)∑k=1∞γNk<∞forallγ∈(0,1).Lemma 2Let{Nk}k=1∞be a sequence satisfying the sample size schedule andXkbe the set of simplex points at iteration k. Then,Pr{ω:xkmaxis not the one that has the largest quantile value inXk}=0;Pr{ω:an accepted point has the true quantile value not smaller thanxkmax}=0;Pr{ω:xkminis not the point that has the smallest true quantile inXk}=0.Lemma 2 shows that the employment of the sample size schedule governed by Eq. (8) can result in correct identification of the worse and best points in the set of simplex points asymptotically. The proof is similar to Lemma 2 of Chang (2012) and is omitted here. Now letXϵ={x∈Rp:ξα≤ξα*+ϵ},which denotes ε − neighborhood of the global optimaξα*. In Assumption 1, it is assumed that the probability that a point inXϵ,ε > 0 can be selected by ARS is strictly positive.Assumption 1For ε > 0,Pr(ARSselectsapointx∈Xϵ)>0.With the assumption above, we are ready to show the convergence of SNM-Q. Without loss of generality, suppose all steps taken to find an improved solution fail and SNM-Q is forced to perform ARS. We first prove that for any ε > 0 and a sufficiently large k, SNM-Q can find a solution whose true function value is within ε − neighborhood of the global optima w.p.1. Lemma 3 shows the probability that the algorithm cannot find a solution within ε − neighborhood of the global optimaξα*is equal to 0. The proof is similar to Lemma 3 of Chang (2012) and is omitted here.Lemma 3SupposeAssumptions 1holds. Then, for any ε > 0(9)Pr{ω:thereisnox∈Xksuchthatξα(x)<ξα*+ϵ}=0.A detailed examination of the proof of Lemma 3 shows that there is room for the design of local search without affecting the lemma. In fact, there is room for the design of the global search too, as long as Assumption 1 can hold. Therefore, both the local and the global search can be devised according to the problem at hand.We now show the convergence of SNM-Q. The proof is given in the appendix.Theorem 2SupposeAssumptions 1holds, and∑k=1∞γNk<∞forallγ∈(0,1).If SNM-Q generates a sequence of{xkmin}k=1∞,then we have(10)ξα(xkmin)→ξα*w.p.1.It can be observed that the ARS framework plays an essential role in the algorithm convergence. This is because it can ensure that the algorithm would have a positive probability to find a better solution at each iteration and, as a result, the algorithm will not get stuck in a non-optimal solution. Note that the sample size schedule can be determined by end users as long as Eq. (8) is satisfied.In this section, we conduct a numerical study to evaluate the performance of SNM-Q. In particular, we consider three algorithms, SNM-Q(a), SNM-Q(b), SNM-Q(c), that are essentially based on the SNM-Q framework but with the multiple-sample-based quantile estimator embedded with three types of single-sample-based quantile estimators, ψ1, ψ2, ψ3, respectively. For comparison purposes, we also consider the fourth algorithm NM, which is based on NM with the single-sample quantile estimator ψ1. The performance of the four algorithms are compared using 96 scenarios, constituted by eight types of test functions, three types of dimensions, two types of initial solutions and two types of distributions for the randomness.The test problem is composed of a deterministic function with added noise, i.e.,G(x)=g(x)+ϵx.For each test problem, we consider three types of dimensionality: 4D, 12D and 16D, which correspond to low-, moderate-, and high-dimensional problems respectively. The added noiseϵxis generated from Normal(0,0.5·g(x))and Uniform(−0.5·g(x),0.5·g(x)). Note that the setting of varying variance, when the initial solution is selected far from the true optima, the function valueg(x)will be large, making the response variable grossly noisy. This allows us to test the stability of algorithms. To evaluate the effect of initial solutions, the initial solution is generated by: (1) using a fixed initial solution,10·1p,where1p=[1,1,…1]Tand p is the dimension of the problem; and (2) randomly selecting an initial solution from the parameter space. The details about the test functions are given in the appendix.Following Chang (2012), two performance measures are used to evaluate the algorithm performance according to how the initial solution is generated: absolute and relative performance measures. In particular, when the initial solution is fixed, the absolute performance measure is defined as(11)log(ξ0.9(xk*)−ξ0.9*+1),wherexk*andξ0.9*correspond to the best solution found when the algorithm is terminated and the true globally optimal function value, respectively. On the other hand, when the initial solution is randomly generated, the relative performance measure is defined as(12)ξ0.9(xk*)−ξ0.9*ξ0.9(x0)−ξ0.9*.For each algorithm and each scenario, 30 macroreplications are run. For each macroreplication, we stop the algorithm when the maximum function evaluations 50,000 are reached and report the performance measure as well as the associated standard deviation (given in the parenthesis). In the case where at least one macroreplication fails to converge, we report the ratio of successful macroreplications. The fitness function used in ARS is set as1/ξ^αk(x).The sample size for the single-sample quantile estimate is 30; the sample size schedule is set asNk=[k],and the Psis set at 0.4.

@&#CONCLUSIONS@&#
