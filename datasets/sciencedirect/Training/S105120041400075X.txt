@&#MAIN-TITLE@&#
Source separation using regularized NMF with MMSE estimates under GMM priors with online learning for the uncertainties

@&#HIGHLIGHTS@&#
Single channel source separation using nonnegative matrix factorization (NMF).Using MMSE under GMM for regularizing the NMF gains, online estimation of uncertainty.Less sensitive to the regularization parameter.Efficient update rules for gain parameters.Improved results in speech–music separation experiments.

@&#KEYPHRASES@&#
Single channel source separation,Nonnegative matrix factorization,Minimum mean square error estimates,Gaussian mixture models,

@&#ABSTRACT@&#
We propose a new method to incorporate priors on the solution of nonnegative matrix factorization (NMF). The NMF solution is guided to follow the minimum mean square error (MMSE) estimates of the weight combinations under a Gaussian mixture model (GMM) prior. The proposed algorithm can be used for denoising or single-channel source separation (SCSS) applications. NMF is used in SCSS in two main stages, the training stage and the separation stage. In the training stage, NMF is used to decompose the training data spectrogram for each source into a multiplication of a trained basis and gains matrices. In the separation stage, the mixed signal spectrogram is decomposed as a weighted linear combination of the trained basis matrices for the source signals. In this work, to improve the separation performance of NMF, the trained gains matrices are used to guide the solution of the NMF weights during the separation stage. The trained gains matrix is used to train a prior GMM that captures the statistics of the valid weight combinations that the columns of the basis matrix can receive for a given source signal. In the separation stage, the prior GMMs are used to guide the NMF solution of the gains/weights matrices using MMSE estimation. The NMF decomposition weights matrix is treated as a distorted image by a distortion operator, which is learned directly from the observed signals. The MMSE estimate of the weights matrix under the trained GMM prior and log-normal distribution for the distortion is then found to improve the NMF decomposition results. The MMSE estimate is embedded within the optimization objective to form a novel regularized NMF cost function. The corresponding update rules for the new objectives are derived in this paper. The proposed MMSE estimates based regularization avoids the problem of computing the hyper-parameters and the regularization parameters. MMSE also provides a better estimate for the valid gains matrix. Experimental results show that the proposed regularized NMF algorithm improves the source separation performance compared with using NMF without a prior or with other prior models.

@&#INTRODUCTION@&#
Nonnegative matrix factorization [1] is an important tool for source separation applications, especially when only one observation of the mixed signal is available. NMF is used to decompose a nonnegative matrix into a multiplication of two nonnegative matrices, a basis/dictionary matrix and a gains/weights/activities matrix. The basis matrix contains a set of basis vectors and the gains matrix contains the weights corresponding to the basis vectors in the basis matrix. The NMF solutions are found by solving an optimization problem based on minimizing a predefined cost function. As most optimization problems, the main goal in NMF is to find the solutions that minimize the cost function without considering any prior information rather than the nonnegativity constraint. There have been many works which tried to enforce prior information related to the nature of the application on the NMF decomposition results. For audio source separation applications, the continuity and sparsity priors were enforced in the NMF decomposition weights [2]. In [3,4], smoothness and harmonicity priors were enforced on the NMF solution in Bayesian framework and applied to music transcription. In [5,6] the regularized NMF was used to increase the NMF decomposition weights matrix likelihood under a prior Gaussian distribution. In [7], Markov chain prior model for smoothness was used within a Bayesian framework in regularized NMF with Itakura–Saito (IS-NMF) divergence. In [8], the conjugate prior distributions on the NMF weights and basis matrices solutions with the Poisson observation model within Bayesian framework was introduced. The Gamma distribution and the Gamma Markov chain [9] were used as priors for the basis and weights/gains matrices respectively in [8]. A mixture of Gamma prior model was used as a prior for the basis matrix in [10]. The regularized NMF with smoothness and spatial decorrelation constraints was used in [11] for EEG applications. In [11,12], a variety of constrained NMF algorithms were used for different applications.In supervised single channel source separation (SCSS), NMF is used in two main stages, the training stage and the separation stage [13–17]. In the training stage, NMF is used to decompose the spectrogram of clean training data for the source signals into a multiplication of trained basis and weights/gains matrices for each source. The trained basis matrix is used as a representative model for the training data of each source and the trained gains matrices are usually ignored. In the separation stage, NMF is used to decompose the mixed signal spectrogram as a nonnegative weighted linear combination of the columns in the trained basis matrices. The spectrogram estimate for each source in the mixed signal can be found by summing its corresponding trained basis terms from the NMF decomposition during the separation stage.One of the main problems of this framework is that, the estimate for each source spectrogram is affected by other sources in the mixed signal. More prior information about the source signals besides their trained basis matrices is usually needed during the separation stage. The trained gains/weights matrix can also be used to incorporate more information about each source signal during the separation stage. The trained weights matrix contains valid weight combinations that their corresponding bases in the basis matrix can jointly receive for a certain source signal. The information in the trained weights matrix can be used to favor solutions with valid weight combinations during the separation stage. The trained weights matrix can be used to train a prior model that captures the statistics of the valid weight combinations for each source signal. The trained prior models can be used to guide the NMF solution of the weights matrix during the separation stage to find a better estimate for the source signals.In this work, we use the trained weights matrix columns to train a prior multivariate Gaussian mixture model (GMM) for each source signal. The GMM is a rich model which captures the statistics and the correlations of the valid weight combinations for the trained basis vectors for each source signal. GMMs are extensively used in speech processing applications like speech recognition and speaker verification. GMMs are used to model the multi-modal nature in speech feature vectors due to phonetic differences, speaking styles, gender, accents [18]. We are conjecturing that the weight vectors of the NMF gains matrix can be considered as a feature extracted from the signal in a frame so that it can be modeled well with a GMM.The trained basis matrix and its corresponding trained prior GMM for the weights matrix are used as representative models for the training data for each source signal. In the separation stage, the prior GMMs are used to guide the NMF to find a weights matrix that is more compatible with its corresponding trained weights matrix for each source. Since the prior GMMs are trained using training data for the source signals, we can see these GMMs as source dependent prior models. Using trained prior models is different than incorporating other intuitive priors such as continuity, smoothness, and sparsity [2–4,11], which are valid prior assumptions for a wide range of source signals. Using a powerful model like GMM which can capture the multi-modal structure of training data is better than using single modal prior models [5–8] that cannot capture the multi-modal nature in some source signals such as speech signals due to phonetic differences, speaking styles, gender, and accents.In [19], the prior GMM that models the valid weight combinations for each source was also used to guide the NMF solution for the gains/weights matrix during the separation stage. The priors in [19] are enforced by maximizing the log-likelihood of the NMF solution with the trained prior GMMs. The priors in [19] are enforced without evaluating how good the NMF solution is without using the priors. For example, if the NMF solution without prior is not satisfactory, we would like to rely more on the priors and vice versa.In this paper, we introduce a new strategy of applying the priors on the NMF solutions of the weights matrix during the separation stage. The new strategy is based on evaluating how much the solution of the NMF gains matrix needs to lean on the prior GMMs. The prior information about the gains matrix is incorporated on the NMF solution of the gains matrix during the separation stage in three main stages. In the first stage, NMF is used to decompose the mixed signal spectrogram as weighted linear combinations of the trained basis matrices without considering any prior information about the weight combinations in the gains matrix. In the second stage, the NMF solution of the gains matrix is then evaluated by measuring how far the weight combinations in the gains matrix is from the trained prior GMM for each source. The NMF solutions without using priors for the weights matrix for each source during the separation stage can be seen as a deformed image. The deformation operator parameters which measure the uncertainty of the NMF solution of the weights matrix are learned directly from the observed mixed signal. The uncertainty in this work is a measurement of how far the NMF solution of the weights matrix during the separation stage is from being a valid weight pattern that is modeled in the prior GMM. In the third stage, the learned uncertainties are used with the minimum mean square error (MMSE) estimator to find an estimate of the weights matrix. The estimated weights matrix should also consider the minimization of the NMF cost function. To achieve these two goals, a regularized NMF cost function is used. The uncertainties within MMSE estimates of the valid weight combinations are embedded in the regularized NMF cost function. The uncertainty measurements play a very important role in this work as we show in the following sections. If the uncertainty of the NMF solution of the weights matrix is high, that means the regularized NMF needs more support from the prior term. In case of low uncertainty, the regularized NMF needs less support from the prior term. Including the uncertainty measurements in the regularization term using the MMSE estimate makes the proposed regularized NMF algorithm decide automatically how much the solution should rely on the prior GMM term. This is the main advantage of the proposed regularized NMF compared to the regularization using the log-likelihood of the GMM prior or other prior distributions [19–21]. Incorporation of the uncertainties that measure the extent of distortion in the NMF weights in the regularization term is a main novelty of this work, which has not been seen before in the regularization literature.The remainder of this paper is organized as follows: In Section 2 we give a brief explanation about the SCSS problem, NMF, the conventional use of NMF in the SCSS problem, and our previous work of using GMM as prior model for regularizing the NMF cost function. In Section 3, we introduce the new proposed regularized NMF and how it is used in the SCSS problem, which is the main contribution of this paper. In the remaining sections, we present our observations and the results of our experiments.In this paper, we apply the proposed MMSE based regularization for the single channel source separation (SCSS) problem using NMF.In single channel source separation (SCSS) problems, the aim is to find estimates of source signals that are mixed on a single observation channely(t). This problem is usually solved in the short time Fourier transform (STFT) domain. LetY(t,f)be the STFT ofy(t), where t represents the frame index and f is the frequency-index. Assuming the number of sources is two and due to the linearity of the STFT, we have:(1)Y(t,f)=S1(t,f)+S2(t,f),whereS1(t,f)andS2(t,f)are the unknown STFT of the first and second sources in the mixed signal. Assuming independence of the sources, we can write the power spectral density (PSD) of the measured signal as the sum of source signal PSDs as follows:(2)σy2(t,f)=σ12(t,f)+σ22(t,f),whereσy2(t,f)=E(|Y(t,f)|2). We can write the PSDs for all frames as a spectrogram matrix as follows:(3)Y=S1+S2,whereS1andS2are the unknown spectrograms of the source signals, and they need to be estimated using the observed mixed signal and training data for each source. The spectrogram of the measured signalYis calculated by taking the squared magnitude of the STFT of the measured signaly(t)and the rows of the matrix correspond to frequencies and the columns correspond to frames.Nonnegative matrix factorization is used to decompose any nonnegative matrixVinto a multiplication of a nonnegative basis matrixBand a nonnegative gains or weights matrixGas follows:(4)V≈BG.The columns of matrixBcontain nonnegative basis or dictionary vectors that are optimized to allow the data inVto be approximated as a nonnegative linear combination of its constituent vectors. Each column in the gains/weights matrixGcontains the set of weight combinations that the basis vectors in the basis matrix have for its corresponding column in theVmatrix. To solve for matrixBandG, different NMF cost functions can be used. For audio source separation applications, the Itakura–Saito (IS-NMF) divergence cost function [7] is usually used. This cost function is found to be a good measurement for the perceptual differences between different audio signals [7,22,23]. The IS-NMF cost function is defined as:(5)minB,GDIS(V∥BG),whereDIS(V∥BG)=∑m,n(Vm,n(BG)m,n−logVm,n(BG)m,n−1).The IS-NMF solutions for Eq. (5) can be computed by alternating multiplicative updates ofBandG[7,22] as:(6)B←B⊗V(BG)2GT1BGGT,(7)G←G⊗BTV(BG)2BT1BG,where the operation ⊗ is an element-wise multiplication, all divisions and(.)2are element-wise operations. In source separation applications, IS-NMF is used with matrices of power spectral densities of the source signals [7,22].In conventional single channel source separation using NMF without regularization [15], there are two main stages to find estimates forS1andS2in Eq. (3). The first stage is the training stage and the second stage is the separation/testing stage. In the training stage, the spectrogramStrainfor each source is calculated by computing the squared magnitude of the STFT of each source training signal. NMF is used to decompose the spectrograms into basis and gains matrices as follows:(8)S1train≈B1G1train,S2train≈B2G2train.The multiplicative update rules in Eqs. (6) and (7) are used to solve forB1,B2,G1trainandG2trainfor both sources. Within each iteration, the columns ofB1andB2are normalized and the matricesG1trainandG2trainare computed accordingly. The initialization of all matricesB1,B2,G1trainandG2trainis done using positive random noise. After finding basis and gains matrices for each source training data, the basis matrices are used in the mixed signal decomposition as shown in the following sections. All the basis matricesB1andB2are kept fixed in the remaining sections in this paper.In the separation stage after observing the mixed signaly(t), NMF is used to decompose the mixed signal spectrogramYwith the trained bases matricesB1andB2that were found from solving Eq. (8) as follows:(9)Y≈[B1,B2]G,orY≈[B1B2][G1G2].LetBtrain=[B1,B2]. The only unknown here is the gains matrixGsince the matrixBtrainwas found during the training stage and it is fixed in the separation stage. The matrixGis a combination of two submatrices as shown in Eq. (9). NMF is used to solve forGin (9) using the update rule in Eq. (7) andGis initialized with positive random numbers.After computing the gains matrix, the initial spectrogram estimate for each source can be found as:(10)S˜1=B1G1,S˜2=B2G2.The solution of the gains submatrixG1in (9) is usually affected by the existence of the second source in the mixed signal. Also,G2is affected by the first source in the mixed signal. The effect of one source into the gains matrix solution of the other source strongly depends on the energy level of each source in the mixed signal. Therefore, the estimated spectrogramsS˜1andS˜2in Eq. (10) that are found from solvingGusing the update rule in (7) may contain residual contribution from each other and other distortions. To fix this problem, more discriminative constraints must be added to the solution of each gains submatrix. The columns of the gains submatrixG1(orG2) should form a valid/expected weight combination for its corresponding basis matrix of its corresponding source signal. The information about the valid weight combinations that can exist in the gains matrix for a source signal can be found in the gains matrix that was computed from the clean training data of the same source. For example, the information about valid weight combinations that can exist in the gains matrixG1in Eq. (9) can be found in its training gains matrixG1trainin Eq. (8). The columns of the trained gains matrixG1trainrepresent the valid weight combinations that the basis matrixB1can receive for the first source. Note that, the basis matrixB1is common in the training and separation stages. The solution of the gains submatrixG1in Eq. (9) should consider the prior about the valid combination that is present in its corresponding trained gains matrixG1trainin Eq. (8) for the same source.In our previous work [19], data in the training gains matrixGitrainfor source i was modeled using a GMM. The NMF solution of the gains matrix during the separation stage was guided by the prior GMM. The GMM was learned using the logarithm of the normalized columns of the training gains matrix.The gains matricesG1trainandG2trainin Eq. (8) were used in our previous work [19] to train prior models for the expected/valid weight patterns in the gains matrix for each source. For each matrixG1trainandG2train, we normalized their columns and then calculated their logarithm. The normalization is usually done using the Euclidean norm. The log-normalized columns were then used to train a prior GMM for each source. The GMM for a random variablexis defined as:(11)p(x)=∑k=1Kωk(2π)d/2|Σk|1/2exp{−12(x−μk)TΣk−1(x−μk)},where K is the number of Gaussian mixture components,ωkis the mixture weight, d is the vector dimension,μkis the mean vector andΣkis the diagonal covariance matrix of the kth Gaussian model. In training GMM, the expectation maximization (EM) algorithm [24] is used to learn the GMM parameters (ωk,μk,Σk,∀k={1,2,…,K}) for each source given its trained gain matrixGtrain. The suitable value for K usually depends on the nature, dimension and the size of the available training data. We used the logarithm because it has been shown that the logarithm of a variable taking values between 0 and 1 can be modeled well by a GMM [25]. Since the main goal of the prior model is to capture the statistics of the patterns in the trained gains matrix, we used normalization to make the prior models insensitive to the energy level of the training data. The normalization makes the same prior models applicable for a wide range of energy levels and avoids the need to train a different prior model for different energy levels. By normalization we are modeling the ratio and correlation between the combination of the weights that the bases can jointly receive.The NMF solution for the gains matrix during the separation stage was enforced in [19] to increase its log-likelihood with the trained GMM prior using regularized NMF as follows:(12)Cold=DIS(Y∥BtrainG)−Rold(G),whereRold(G)is the weighted sum of the log-likelihoods of the log-normalized columns of the gains matrixG.Rold(G)was defined as follows:(13)Rold(G)=∑i=12ηiΓold(Gi),whereΓold(Gi)is the log-likelihood for the submatrixGi, andηiis the regularization parameter for source i. The regularization parameter in [19] was playing two important roles. The first role was to match the scale of the IS-NMF divergence term with the scale of the log-likelihood prior term. The second role was to decide how much the regularized NMF cost function needs to rely on the prior term. The results in [19] show that, when the source i has higher energy level than the other sources, the value of its corresponding regularization parameterηibecomes smaller than the values of other regularization parameters for the other sources. This observation can be stated as follows: when the source has high energy level, the gains matrix solution of the regularized NMF in (12) relies less on the prior model and vice versa. The values of the regularization parameters in [19] were chosen manually for every energy level for each source.The conjugate prior models usually enforced on NMF solutions using a Bayesian framework [7,8,21]. The conjugate prior models are usually used to make the probabilistic inference tractable. There is no need to learn the parameters (hyper-parameters) of the conjugate prior models during the training stage. The hyper-parameters are usually chosen during the separation/testing stage. These hyper-parameters are usually multi-dimensional parameters and trying all possible combinations for their entries to find the suitable values for the given task is impossible. In the cases when the conjugate prior models of the NMF solutions were used [8,21], the hyper-parameters of the prior models were also chosen manually. In [19], it was also shown that, the hyper-parameter choices for the conjugate prior models can also depend on the energy level differences of the source signals in the mixed signal.In the following sections, we give the motivation of the proposed MMSE-based regularization. Then, we give more details about our proposed regularized NMF using MMSE estimate to find better solution for the gains matrix in (9). In Section 3.2, we present our proposed regularized NMF in a general manner. In Section 3.2, we assume we have a trained basis matrixB, a trained prior GMM for a clean gains matrix, and a gains matrixGthat inherited some distortion from the original matrixVfrom solving Eq. (5). We introduce our proposed regularized NMF in a general fashion in Section 3.2 to make the idea clearer for different NMF applications like: dimensionality reduction, denoising, and other applications. The update rules that solve the proposed regularized NMF are also derived in Section 3.2 in a general fashion regardless of the application. The GMM in Section 3.2 is the trained prior GMM that captures the statistics of the valid weights combinations that should have been existed in the gains matrixG. In Section 3.3, we show how we use the proposed regularized NMF to find better solutions for the gain submatrices in Eq. (9) for our single channel source separation problem.In this work, we use prior GMMs to guide the solution of the gains matrix during the separation stage using regularized NMF as in [19] but following a totally different regularization strategy. We try to find a way to estimate how much the solution of the regularized NMF needs to rely on the prior GMMs automatically, not manually, as in [19]. The way of finding how much the regularized NMF solution of the gains matrix needs to rely on the prior GMM is by measuring how far the statistics of the solution of the gains matrixGiin (9) is from the statistics of the solution of the valid gains matrix solutionGitrainin (8) for source i. Recall that, the matrixGitrainin (8) contains the valid weight combinations that the columns in the basis matrixBican jointly receive for the clean data of source i. The data inGitraincan be used as a prior information for what kind of weight combinations that should exist inGiin (9) since the matrixBiis the same in (8) and (9). The matrixGitrainin (8) is used to train a prior GMM for the expected (valid) weight combinations that can exist in the gains matrix for source i as in [19]. The solution of the gains submatrixGiin (9) can be seen as a deformed observation that needs to be restored using MMSE estimate under its corresponding GMM prior for source i. How far the statistics of the solution of the gains matrixGiis from the statistics of the solution of the valid gains matrix solutionGitraincan be seen as how much the gains submatrixGiis deformed. How much deformation exists in the gains matrixGican be learned directly and the logarithm of this deformation is modeled using a Gaussian distribution with zero mean and a diagonal covariance matrixΨi. When the deformation or the uncertainty measurementΨiof the gain submatrixGiis high, we expect our target regularized NMF cost function to rely more on the prior GMM for source i and vice versa. Based on the measurementΨi, the proposed NMF cost function decides automatically how much the solution of the regularized NMF needs to rely on the prior GMMs, which is a main advantage of the proposed regularized NMF over our previous work [19]. Applying the prior information on the gains matrixGiin (9) using MMSE estimate under a GMM prior using regularized NMF is the new strategy that we introduce in this paper.In this work, we need the solution of the gains matrixGto minimize the IS-divergence cost function in Eq. (5), and the columns of the gains matrixGshould form valid weight combinations under a prior GMM model. Gaussian mixture model is a very general prior model where we can see the means of the GMM mixture components as “valid templates” that were observed in the training data. Even, Parzen density priors [26] can be seen under the same framework. In Parzen density prior estimation, training examples are seen as “valid templates” and a fixed variance is assigned to each example. In GMM priors, we learn the templates as cluster means from training data and we can also estimate the cluster variances from the data. We can think of the GMM prior as a way to encourage the use of valid templates or cluster means in the NMF solution during the separation stage. This view of the GMM prior will be helpful in understanding the MMSE estimate method we introduce in this paper.We can see the solution of the conventional NMF (NMF without prior) as distorted observations of a true/valid template. We can find a way of measuring how far the conventional NMF solution is from the trained templates in the prior GMM and call this the error term. We can learn a probability distribution model for the distortion that captures how far the observations in the conventional gains matrix is from the prior GMM. The distortion or the error model can be seen as a summary of the distortion that exists in all columns in the gains matrix of the NMF solution. Based on this distortion evaluation, the proposed regularized NMF can decide automatically how much the solution of the NMF needs help from the prior model. By deciding automatically how much the regularized NMF needs to rely on the prior we conjecture that, we do not need to manually change the values for the regularization parameter based on the energy differences of the sources in the mixed signal11In this paper, the regularization parameters are chosen once and kept fixed regardless of the energy differences of the source signals.to improve the performance of NMF as in [19].Based on the prior GMM and the trained distortion model, we can find a better estimate for the desired observation for each column in the distorted gains matrix. We can mathematically formulate this by seeing the solution matrixGthat only minimizes the cost function in Eq. (5) as a distorted image where its restored image needs to be estimated. The columns of the matrixGare normalized using theℓ2norm and their logarithm is then calculated. Let the log-normalized column n namely (loggn‖gn‖2) of the gains matrix beqn. The vectorqnis treated as a distorted observation as:(14)qn=xn+e,wherexnis the unknown desired pattern that corresponds to the observationqnand needs to be estimated under a prior GMM,eis the logarithm of the deformation operator, which is modeled by a Gaussian distribution with zero mean and diagonal covariance matrixΨasN(e|0,Ψ). The GMM prior model for the gains matrix is trained using log-normalized columns of the trained gains matrix from training data as shown for example in Section 2.5.2. The uncertaintyΨis trained using maximum likelihood directly from all the log-normalized columns of the gains matrixq={q1,…,qn,…,qN}, where N is the number of columns in the matrixG. The uncertaintyΨcan be iteratively learned using the expectation maximization (EM) algorithm. Given the prior GMM parameters which are considered fixed here, the update ofΨis found in the M-step based on the sufficient statisticszˆnandRˆnas follows [27–29] [Appendix A]:(15)Ψ=diag{1N∑n=1N(qnqnT−qnzˆnT−zˆnqnT+Rˆn)},where the “diag” operator sets all the off-diagonal elements of a matrix to zero and the sufficient statisticszˆnandRˆncan be updated in the E-step usingΨfrom the previous iteration as follows:(16)zˆn=∑k=1Kγknzˆkn,and(17)Rˆn=∑k=1KγknRˆkn,where(18)γkn=[ωkN(qn|μk,Σk+Ψ)∑j=1KωjN(qn|μj,Σj+Ψ)],(19)Rˆkn=Σk−Σk(Σk+Ψ)−1ΣkT+zˆknzˆknT,and(20)zˆkn=μk+Σk(Σk+Ψ)−1(qn−μk).Ψis considered as a general uncertainty measurement over all the observations in matrixG.Given the GMM prior parameters and the uncertainty measurementΨ, the MMSE estimate of each patternxngiven its observationqnunder the observation model in Eq. (14) can be found similar to [27–29] as in Appendix A as follows:(21)xˆn=f(qn)=∑k=1Kγkn[μk+Σk(Σk+Ψ)−1(qn−μk)],where(22)γkn=[ωkN(qn|μk,Σk+Ψ)∑j=1KωjN(qn|μj,Σj+Ψ)].The value ofΨin the termΣk(Σk+Ψ)−1in Eq. (21) plays an important role in this framework. When the entries of the uncertaintyΨare very small comparing to their corresponding entries inΣkfor a certain active GMM component k, the termΣk(Σk+Ψ)−1tends to be the identity matrix, and MMSE estimate in (21) will be the observationqn. When the entries of the uncertaintyΨare very high comparing to their corresponding entries inΣkfor a certain active GMM component k, the termΣk(Σk+Ψ)−1tends to be a zeros matrix, and MMSE estimate will be the weighted sum of prior templates∑k=1Kγknμk. In most casesγkntends to be close to one for one Gaussian component, and close to zero for the other components in a large dimensional space. This makes the MMSE estimate in the case of highΨto be one of the mean vectors in the prior GMM, which is considered as a template pattern for the valid observation. We can rephrase this as follows: When the uncertainty of the observationsqis high, the MMSE estimate ofx, relies more on the prior GMM ofx. When the uncertainty of the observationsqis low, the MMSE estimate ofx, relies more on the observationq. In general, the MMSE solution ofxlies between the observationqand one of the templates in the prior GMM. The termΣk(Σk+Ψ)−1controls the distance betweenxˆnandqnand also the distance betweenxˆnand one of the templateμkassuming thatγkn≈1for a Gaussian component k.The model in Eq. (14) expresses the normalized columns of the gains matrix as a distorted image with a diagonal multiplicative deformation matrix. For the normalized gain columnsgn‖gn‖2ofGthere is a deformation matrixEwith log-normal distribution that is applied to the correct pattern to obtain the observation. We need to estimategˆnwhich satisfies the following equation:(23)gn‖gn‖2=Egˆn.The uncertainty forEis represented in the covariance matrixΨ. For the distorted matrixGwe find the corresponding MMSE estimate for its normalized columnsGˆ. Another reason for working in logarithm domain is that, the gains are constrained to be nonnegative and the MMSE estimate can be negative so the logarithm of the normalized gains is an unconstrained variable that we can work with. The estimated weight patterns inGˆthat are corresponding to the MMSE estimates for the correct patterns do not consider minimizing the NMF cost function in Eq. (5), which is still the main goal. We need the solution ofGto consider the pattern shape priors on the solution of the gains matrix, and also consider the reconstruction error of the NMF cost function. To consider the combination of the two objectives, we use regularized NMF. We add a penalty term to the NMF cost function. The penalty term tries to minimize the distance between the solution of log-normalized columns ofgnwith its corresponding MMSE estimatef(gn)as follows:(24)loggn‖gn‖2≈f(loggn‖gn‖2)orgn‖gn‖2≈exp(f(loggn‖gn‖2)).The regularized IS-NMF cost function is defined as follows:(25)C=DIS(V∥BG)+αL(G),where(26)L(G)=∑n=1N‖gn‖gn‖2−exp(f(loggn‖gn‖2))‖22,f(loggn‖gn‖2)is the MMSE estimate defined in Eq. (21), and α is a regularization parameter. The regularized NMF can be rewritten in more details as(27)C=∑m,n(Vm,n(BG)m,n−logVm,n(BG)m,n−1)+α∑n=1N‖gn‖gn‖2−exp(∑k=1Kγkn[μk+Σk(Σk+Ψ)−1(loggn‖gn‖2−μk)])‖22.In Eq. (27), the MMSE estimate of the desired patterns of the gains matrix is embedded in the regularized NMF cost function. The first term in (27), decreases the reconstruction error betweenVandBG. GivenΨ, we can forget for a while the MMSE estimate concept that leaded us to our target regularized NMF cost function in (27) and see Eq. (27) as an optimization problem. We can see from (27) that, if the distortion measurement parameterΨis high, the regularized nonnegative matrix factorization solution for the gains matrix will rely more on the prior GMM for the gains matrix. If the distortion parameterΨis low, the regularized nonnegative matrix factorization solution for the gains matrix will be close to the ordinary NMF solution for the gains matrix without considering any prior. The second term in Eq. (27) is ignored in the case of zero uncertaintyΨ. In case of high values ofΨ, the second term encourages to decrease the distance between each normalized columngn‖gn‖2inGwith a corresponding prior templateexp(μk)assuming thatγkn≈1for a certain Gaussian component k. For different valuesΨ, the penalty term decreases the distance between eachgn‖gn‖2and an estimated pattern that lies between a prior template andgn‖gn‖2.Ψhere has the effect of a multi-dimension regularization parameter which regularizes each row in the gains matrix differently. This multi-dimensional regularization parameter makes the idea of using the MMSE estimate based regularization more powerful than the log-likelihood based regularization in [19] where we have only scalar regularization parameters. The term (loggn‖gn‖2−μk) in (27) measures how far each log-normalized column in the gains matrix is from a valid templateμk. Under the assumptionγkn≈1for a certain Gaussian component k, the second term in (27) is also ignored when the observationloggn‖gn‖2form a valid pattern (loggn‖gn‖2=μk). How far each log-normalized column in the gains matrix is from a valid template decides how much influence the MMSE estimate prior term has to the solution of (27) for each observation.The multiplicative update rule forBin (27) is still the same as in Eq. (6). The multiplicative update rule forGcan be found by following the same procedures as in [2,4,19]. The gradient with respect toGof the cost function∇GCcan be expressed as a difference of two positive terms∇G+Cand∇G−Cas follows:(28)∇GC=∇G+C−∇G−C.We can apply diagonally weighted gradient descent on the cost function where the diagonal step sizes are chosen such that the result is a multiplicative update of the matrix entries:(29)G←G⊗∇G−C∇G+C,where the operations ⊗ and division are element-wise as in Eq. (7). We can write the gradients as:(30)∇GC=∇GDIS+α∇GL(G),where∇GL(G)is a matrix with the same size ofG. The gradient for the IS-NMF and the gradient of the prior term can also be expressed as a difference of two positive terms as follows:(31)∇GDIS=∇G+DIS−∇G−DIS,and(32)∇GL(G)=∇G+L(G)−∇G−L(G).We can rewrite Eqs. (28), (30) as:(33)∇GC=(∇G+DIS+α∇G+L(G))−(∇G−DIS+α∇G−L(G)).The final update rule in Eq. (29) can be written as follows:(34)G←G⊗∇G−DIS+α∇G−L(G)∇G+DIS+α∇G+L(G),where(35)∇GDIS=BT1BG−BTV(BG)2,(36)∇G−DIS=BTV(BG)2,and∇G+DIS=BT1BG.Note that, in calculating the gradients∇G+L(G)and∇G−L(G), the termγknis also a function ofG. The gradients∇G+L(G)and∇G−L(G)are calculated in Appendix B. Since all the terms in Eq. (34) are nonnegative, then the values ofGof the update rule (34) are nonnegative.In this section, we are back to the single channel source separation problem to find a better solution to Eq. (9). Fig. 1shows the flow chart that summarizes all stages of applying our proposed regularized NMF method for SCSS problems. Given the trained basis matricesB1,B2that were computed from solving (8), and the trained gain prior GMM for each source from Section 2.5.2, we try to apply the proposed regularized NMF cost function in Section 3.2 to find a better solution for the gain submatrices in Eq. (9). The bases matrixBtrain=[B1,B2]is still fixed here, we just need to update the gains matrixGin (9). The normalized columns of the submatricesG1andG2in Eq. (9) can be seen as deformed images as in Eq. (23) and their restored images need to be estimated. First, we need to learn the uncertainty parametersΨ1andΨ2for the deformation operatorsE1andE2respectively for each image as shown in learning the uncertainties stage in Fig. 1. The columns of the submatrixG1are normalized and their logarithm are calculated and used with the trained GMM prior parameters for the first source to estimateΨ1iteratively using the EM algorithm in Eqs. (15) to (20). The log-normalized columns “loggn‖gn‖2” ofG1can be seen asqnin Eqs. (15) to (20). We repeat the same procedures to calculateΨ2using the log-normalized columns ofG2and the prior GMM for the second source. The uncertaintiesΨ1andΨ2can also be seen as measurements of the remaining distortion from one source into another source, which also depends on the mixing ratio between the two sources. For example, if the first source has higher energy than the second source in the mixed signal, we expect the values ofΨ2to be higher than the values inΨ1and vice versa. After calculating the uncertainty parameters for both sourcesΨ1andΨ2, we use the regularized NMF in (25) to solve forGwith the prior GMMs for both sources and the estimated uncertaintiesΨ1andΨ2as follows:(37)C=DIS(Y∥BtrainG)+R(G),where(38)R(G)=α1L1(G1)+α2L2(G2),L1(G1)is defined as in Eq. (26) for the first source,L2(G2)is for the second source,α1, andα2are their corresponding regularization parameters. The update rule in Eq. (34) can be used to solve forGafter modifying it as follows:(39)G←G⊗∇G−DIS+∇G−R(G)∇G+DIS+∇G+R(G),where∇G+R(G)and∇G−R(G)are nonnegative matrices with the same size ofGand they are combinations of two submatrices as follows:(40)∇G−R(G)=[α1∇G−L(G1)α2∇G−L(G2)],∇G+R(G)=[α1∇G+L(G1)α2∇G+L(G2)],where∇G+L(G1),∇G−L(G1),∇G+L(G2), and∇G−L(G2)are calculated as in Section 3.2 for each source.The normalization of the columns of the gain matrices are used in the prior termR(G)and its gradient terms only. The general solution for the gains matrix of Eq. (37) at each iteration is not normalized. The normalization is done only in the prior term since the prior models have been trained by normalized data before. Normalization is also useful in cases where the source signals occur with different energy levels from each other in the mixed signal. Normalizing the training and testing gain matrices gives the prior models a chance to work with any energy level that the source signals can take in the mixed signal regardless of the energy levels of the training signals.The regularization parameters in (38) have only one role. They are chosen to match the scale between the NMF divergence term and the MMSE estimate prior term in the regularized NMF cost function in (37). There is no need to change the values of the regularization parameters according to the energy differences of the source signals in the mixed signal as in [19]. Reasonable values for the regularization parameters are chosen manually and kept fixed in this work. Another main difference between the regularized NMF in [19] that is shown in Eq. (12) and the proposed regularized NMF in this paper is related to the training procedures for the source models. In both works, the main aim of the training stage is to train the basis matrices and the prior GMMs for the gains for the source signals.In [19], to match between the way the trained models were used during training with the way they were used during separation, the basis matrices and the prior GMM parameters were learned jointly using the regularized NMF cost function in (12). The joint training for the source models was introduced in [19] to improve the separation performance. In joint training, after updating the gains matrix at each NMF iteration using the gain update rule for the regularized NMF in (12), the GMM parameters were then updated (retrained). Since, we needed to update (retrain) the GMM parameters at each NMF iteration, joint training slowed down the training of the source models in [19]. Another problem of using joint training is that, we had other regularization parameters during the training stage that needed to be chosen. Using joint training duplicates the number of the regularization parameters that need to be chosen. Choosing the regularization parameters in [19] was done using validation data. That means, in [19] we had to train many source models (basis matrix and prior GMM) for different regularization parameter values. Then, we chose the best combination for the regularization parameter values in training and separation stages that gave the best results during the separation stage. In the case of using MMSE estimate regularization for NMF, we do not need to use joint training. In this paper, we do not need to consider solving the regularized NMF in (27) during the training stage to solve (8). In the training stage, the training data for each source is assumed to be clean data. Since the spectrogram of each source training data represents clean source data, the NMF solution for the gains matrix cannot be seen as a distorted image. Therefore, the deformation measurement parameterΨtrainis a matrix of zeros. WhenΨtrain=0, the MMSE estimates prior term in (27) will disappear because∑k=1Kγkn=1. Then, the regularized NMF (27) becomes just NMF. That means, we do not need to use the regularized NMF during the training stage which is not the case in [19]. Here in the training stage, we just need to use IS-NMF to decompose the spectrogram of the training data into trained basis and gains matrices. After the trained gains matrix is computed, it is used to train the prior GMM as shown in Sections 2.3 and 2.5.2.After finding the suitable solution for the gains matrixGin Section 3.3, the initial estimated spectrogramsS˜1andS˜2can be calculated from (10) and then used to build spectral masks as follows:(41)H1=S˜1S˜1+S˜2,H2=S˜2S˜1+S˜2,where the divisions are done element-wise. The final estimate of each source STFT can be obtained as follows:(42)Sˆ1(t,f)=H1(t,f)Y(t,f),Sˆ2(t,f)=H2(t,f)Y(t,f),whereY(t,f)is the STFT of the observed mixed signal in Eq. (1),H1(t,f)andH2(t,f)are the entries at row f and column t of the spectral masksH1andH2respectively. The spectral mask entries scale the observed mixed signal STFT entries according to the contribution of each source in the mixed signal. The spectral masks can be seen as the Wiener filter as in [7]. The estimated source signalssˆ1(t)andsˆ2(t)can be found by using inverse STFT of their corresponding STFTsSˆ1(t,f)andSˆ2(t,f).We applied the proposed algorithm to separate speech signals from background music signals. We used two different experimental setups. In the first experiment we separate speech signals of male speakers from TIMIT database from background piano signals. In the second experiment, we separate male and female speech signals from TIMIT dataset from general background music signals where different musical instruments are used.In this experiment, we applied the proposed algorithm to separate a speech signal from a background piano music signal. Our main goal was to get a clean speech signal from a mixture of speech and piano signals. We simulated our algorithm on a collection of speech and piano data at 16 kHz sampling rate. For speech data, we used the training and testing male speech data from the TIMIT database. For music data, we downloaded piano music data from the piano society web site [30]. We used 12 pieces with approximate 50 minutes total duration from different composers but from a single artist for training and left out one piece for testing. The PSD for the speech and music data were calculated by using the STFT: A Hamming window with 480 points length and60%overlap was used and the FFT was taken at 512 points, the first 257 FFT points only were used since the conjugate of the remaining 255 points are involved in the first points. We trained 128 basis vectors for each source, which makes the size ofBspeechandBmusicmatrices to be257×128, hence, the vector dimensiond=128in Eq. (11) for both sources. The mixed data was formed by adding random portions of the test music file to 20 speech files from the test data of the TIMIT database at different speech-to-music ratio (SMR) values in dB. The audio power levels of each file were found using the “speech voltmeter” program from the G.191 ITU-T STL software suite [31]. For each SMR value, we obtained 20 mixed utterances this way. We used the first 10 utterances as a validation set to choose reasonable values for the regularization parametersαspeechandαmusicand the number of Gaussian mixture components K. The other 10 mixed utterances were used for testing.Performance measurement of the separation algorithm was done using the signal to noise ratio (SNR). The average SNR over the 10 test utterances for each SMR case are reported. We also used signal to interference ratio (SIR), which is defined as the ratio of the target energy to the interference error due to the music signal only [32].Table 1shows SNR and SIR of the separated speech signal using NMF with different values of the number of Gaussian mixture components K and fixed regularization parametersαspeech=αmusic=1. The first column of the table shows the separation results of using just NMF without any prior.As we can see from the table, the proposed regularized NMF algorithm improves the separation performance for challenging SMR cases compared with using just NMF without priors. Increasing the number of Gaussian mixture components K improves the separation performance untilK=16. From the shown results,K=16seems to be a good choice for the given data sets. The best choice for K usually depends on the nature and the size of the training data. For example, for speech signal in general there are variety of phonetic differences, gender, speaking styles, accents, which raises the necessity for using many Gaussian components.In this section we compared our proposed method of using MMSE estimates under GMM prior on the solution of NMF with two other prior methods. The first prior is the sparsity prior and the second prior is enforced by maximizing the loglikelihood under GMM prior distribution.In the sparsity prior, the NMF solution of the gains matrix was enforced to be sparse [10,13]. The sparse NMF is defined as(43)C(G)=DIS(Y∥BG)+λ∑m,nGm,n,where λ is the regularization parameter. The gain update rule ofGcan be found as follows:(44)G←G⊗BTY(BG)2BT1BG+λ.Enforcing sparsity on the NMF solution of the gains matrix is equivalent to model the prior of the gains matrix using exponential distribution with parameter λ[10]. The update rule in Eq. (44) is found based on maximizing the likelihood of the gains matrix under the exponential prior distribution.The second method of enforcing prior on the NMF solution is by using GMM gain prior [19,20]. The NMF solution for the gains matrix is enforced to increase its log-likelihood with the trained GMM prior as follows:(45)C=DIS(Y∥BG)−R2(G),whereR2(G)is the weighted sum of the log-likelihoods of the log-normalized columns of the gains matrixG.R2(G)can be written as follows:(46)R2(G)=∑i=12ηiΓ(Gi),whereΓ(Gi)is the log-likelihood for the submatrixGifor source i.In sparsity and GMM based log-likelihood prior methods, to match between the used update rule for the gains matrix during training and separation, the priors were enforced during both training and separation stages. In sparse NMF we used sparsity constraints during training and separation stages. In regularized NMF with GMM based log-likelihood prior we trained the NMF bases and the prior GMM parameters jointly as argued in [19].In the sparse NMF case, we obtained best results whenλ=0.0001for both sources in the training and separation stages. In the case of enforcing the gains matrix to increase the log-likelihood under GMM prior [19] we obtained the best results whenη=1in the training andη=0.1in the separation stage. The number of Gaussian components wasK=4for both sources. It is important to note that, in the case of using MMSE under GMM prior there is no need to enforce prior during training since the uncertainty measurements during training are assumed to be zeros since the training data are clean signals. When the uncertainty is zero, then the regularized NMF in case of MMSE under GMM prior is the same as the NMF cost function, then the update rule for the gains matrix in the training stage is the same as the update rule in regular NMF.Figs. 2 and 3show the SNR and SIR for the different type of prior models. The black line shows the separation performance when no prior is used. The red line shows the performance for the case of using sparse NMF. The green line shows the performance when enforcing the gains matrix to increase its likelihood with the prior GMM. The blue line shows the separation performance in the case of using MMSE estimate under GMM prior that is proposed in this paper. As we can see, the proposed method of enforcing prior on the gains matrix using MMSE estimate under GMM prior gives the best performance compared with other methods. The used MMSE estimates prior in this work gives better results than the GMM likelihood method [19] because of the measurements of the uncertainties in the MMSE under GMM case. The uncertainties work as feedback measurements that adjust the need to use the prior based on the amount of distortion in the gains matrix during the separation stage.Comparing the relative improvements in dB that we obtained in this experiment with the achieved improvements in other works [2,5,6,10] we can see that the improvements in this work can be considered to be high.In this experiment, we used general speech data (male and female speakers) from the TIMIT database. We also used general background music signals where different musical instruments are used. We downloaded these music signals from the website [33], the dataset which was collected for the purposes of music/speech discrimination. We used 20 minutes of these music files which are available at [34] for this experiment. For speech data, we used TIMIT dataset for male and female speakers to train general models for the speech signal. We trained a single basis matrix and a single GMM prior model for male and female speakers of the TIMIT database as general models for the speech signals. For the music signals, we used all the available music files for training. We trained the basis matrix of the speech and music data with 128 basis vectors which makes the size ofBspeechandBmusicmatrices to be257×128. For training the prior GMMs, we tried with different values for the number of Gaussian mixture components K and we achieved reasonable performance atK=256. The higher value for K in this experiment is because of the variety of the used speech and music signals.The mixed data was formed by adding random portions of the available 20 minutes music files to 20 speech files from the test data of the TIMIT database at different speech-to-music ratio (SMR) values in dB. The 20 speech files are 10 files from each gender. The audio power levels of each file were found similar to the first experiment. For each SMR value, we obtained 20 mixed utterances. In this experiment, we choseαspeech=αmusic=1. We also compared with using different prior models similar to the first experiment.Figs. 4 and 5show the SNR and SIR for the different type of prior models. The first observation is that in general the results are much lower than the first experiment which indicates the hardness of the problem in the general speech + music case. As can be seen, the proposed method of enforcing prior on the gains matrix using MMSE estimate under GMM prior gives the best SIR performance for all SMR cases compared with the other methods. For SNR values, at low SMR the proposed MMSE regularization gives better results but at high SMR it seems that just using NMF without any regularization works slightly better than using regularized NMF. However, we can see that the output SNR is between 4 and 4.5 dB for all methods when the input SMR is 5 dB in this case. This indicates that all the methods are decreasing the SNR a little bit. However, the SIR results indicate that the MMSE method decreases the contribution of the music in the reconstructed signal much more than the other methods. We believe that SIR is a more important performance measure than SNR when the SMR of the input signal is higher than 5 dB.

@&#CONCLUSIONS@&#
