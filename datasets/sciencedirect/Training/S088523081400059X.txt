@&#MAIN-TITLE@&#
Leveraging social Q&A collections for improving complex question answering

@&#HIGHLIGHTS@&#
The proposed approach leverages social Q&A collections to improve automatic complex QA system.There is no need to manually collect training Q&A pairs that are necessary for supervised machine learning approaches.Extensive comparison experiments are conducted, i.e., LexRank, question-specific and translation-based approaches are compared.Experiments on the extension of NTCIR 2008 test questions indicate that the proposed approach is more effective.

@&#KEYPHRASES@&#
Complex question answering,Web mining,Summarization,

@&#ABSTRACT@&#
This paper regards social question-and-answer (Q&A) collections such as Yahoo! Answers as knowledge repositories and investigates techniques to mine knowledge from them to improve sentence-based complex question answering (QA) systems. Specifically, we present a question-type-specific method (QTSM) that extracts question-type-dependent cue expressions from social Q&A pairs in which the question types are the same as the submitted questions. We compare our approach with the question-specific and monolingual translation-based methods presented in previous works. The question-specific method (QSM) extracts question-dependent answer words from social Q&A pairs in which the questions resemble the submitted question. The monolingual translation-based method (MTM) learns word-to-word translation probabilities from all of the social Q&A pairs without considering the question or its type. Experiments on the extension of the NTCIR 2008 Chinese test data set demonstrate that our models that exploit social Q&A collections are significantly more effective than baseline methods such as LexRank. The performance ranking of these methods is QTSM>{QSM, MTM}. The largest F3 improvements in our proposed QTSM over QSM and MTM reach 6.0% and 5.8%, respectively.

@&#INTRODUCTION@&#
Unlike conventional search engines, which find relevant documents on the web, question-answering (QA) systems are designed to return much more focused answers, for example:Q:In which city will the 2020 Olympic games be held?TokyoQA research attempts to deal with a wide range of question types, including factoid, list,11Example question: Name 20 countries other than the United States that have a McDonald's restaurant.and complex questions. In the past, substantial progress has been made with factoid and list questions. In February 2011, IBM's Watson QA system (Ferrucci et al., 2010) defeated human grand champions in the game of Jeopardy!22http://www.jeopardy.com/.In this paper, we focus on complex questions because – apart from definitional33Example questions: Who is Aaron Copland? What is a golden parachute?(Voorhees, 2003; Mitamura et al., 2008), reason (Higashinaka and Isozaki, 2008; Oh et al., 2013), and opinion (Dang, 2008) complex questions – many types remain largely unexplored.44Most complex questions have generally been referred to as what-questions in previous studies. This paper argues that it is helpful to treat them discriminatively.Compared to factoid and list questions that can be answered by short phrases, such as persons, organizations, locations and dates, complex questions, whose answers generally consist of a list of “nuggets” (Voorhees, 2003; Mitamura et al., 2008), are more difficult to answer. For instance, the factoid question, “In which city will the 2020 Olympic games be held?” asks for a city name, and thus it is easier to impose some constraints on the plausible answers for this type of question and significantly reduce the search space of plausible answers. However, complex questions such as “What are the hazards of global warming?” often seek multiple, different types of information simultaneously, making it difficult to screen plausible answers. Moreover, complex QA tasks require inferring and synthesizing information from multiple documents to provide multiple nuggets as answers (Dang, 2006; Chali et al., 2009). To answer complex questions, we often need to go through complex procedures.Many approaches have been proposed to answer factoid, definitional, reason, and opinion questions. Among them, machine learning techniques have proven to be effective in constructing QA components from scratch, but these supervised techniques require a certain quantity of question and answer (Q&A) pairs as training data. For example, Echihabi and Marcu (2003) and Sasaki (2005) constructed 90,000 English and 2000 Japanese Q&A pairs for their factoid QA systems, respectively. Cui et al. (2004) collected 76 term-definition pairs for their definitional QA system. Higashinaka and Isozaki (2008) used 4849 positive and 521,177 negative examples in their reason QA system. Stoyanov et al. (2005) required a known subjective vocabulary for their opinion QA system. To answer other types of complex questions using supervised techniques, we need to collect Q&A pairs for each type of complex question to train models, even though this is an extremely expensive and labor-intensive task. Fortunately, many user-generated Q&A pairs can be found in social QA websites such as Yahoo! Answers,55http://answers.yahoo.com/.Baidu Zhidao,66http://zhidao.baidu.com/.and Answers.com.77http://www.answers.com/.This paper explores the automatic learning of Q&A training pairs and the mining of needed knowledge from social Q&A collections such as Yahoo! Answers. We are interested in whether millions of typically noisy user-generated Q&A pairs can be exploited for automatic QA systems. If so, a plethora of Q&A training data is already readily available.Many studies (Riezler et al., 2007; Surdeanu et al., 2008; Duan et al., 2008; Wang et al., 2010) have retrieved similar Q&A pairs from social QA websites as answers to test questions; accordingly, answers cannot be generated for questions that have not previously been answered on such sites. Our study, however, regards social Q&A websites as knowledge repositories and exploits their knowledge to synthesize answers to questions that have not yet been answered. Even for questions that have been answered, it is necessary to perform answer summarization (Liu et al., 2008; Chan et al., 2012). Our approach can also be used for this purpose. To the best of our knowledge, very few works in the literature have addressed this aspect.Various kinds of knowledge can be mined from social Q&A collections to support complex QA systems. In this paper, we present a question-type-specific method (QTSM) to mine question-type-specific knowledge and compare it with the question-specific and monolingual translation-based methods proposed in related work. Given question Q, whose question type Qtis automatically recognized from Q, three kinds of methods can be applied here.•Our proposed QTSM collects Q&A pairs in which the question types are the same as Qtand extracts salient cue expressions that are indicative of possible answers to question type Qt. It uses the expressions and the Q&A pairs to train a binary classifier to remove noisy candidate answers.The question-specific method (QSM, introduced in Section 5.1) collects Q&A pairs that resemble Q from social Q&A collections and extracts question-dependent answer words to improve complex QA systems.The monolingual translation-based method (MTM) employs all of the social Q&A pairs and learns word-to-word translation probabilities, without considering question Q and question type Qt, to solve the lexical gap problem between question and answer. MTM will be discussed in Section 5.2.We evaluated three methods in terms of the extension of the NTCIR (NII Testbeds and Community for Information access Research) 2008 test data set. We employed the Pourpre v1.0c evaluation tool (Lin and Demner-Fushman, 2006), which was also adopted to evaluate the TREC (Text REtrieval Conference) QA systems. Experiments showed that our proposed QTSM is the most effective; for instance, the largest F3/NR improvements of QTSM over the baseline, QSM, and MTM models reached 8.6%/12.6%, 6.0%/6.7%, and 5.8%/7.1%, respectively. The performance ranking of these methods was QTSM>{QSM, MTM}. LexRank (Erkan and Radev, 2004), which is commonly used in query-based summarization tasks, was implemented as the baseline. Our experiments showed that the three models designed for exploiting social Q&A collections are significantly better than LexRank.Social QA websites such as Yahoo! Answers and Baidu Zhidao provide interactive platforms for users to post questions and answers. After users answer the questions, the best answer can be chosen by the asker or nominated by the community. Table 1demonstrates an example of these Q&A pairs, whose numbers have risen dramatically on such sites. Table 2shows the statistics of Q&A pairs crawled from Baidu Zhidao in December 2008. The pairs collectively form a source of training data, which is essential for supervised machine-learning-based QA systems.This paper exploits such user-generated Q&A collections to improve complex QA systems by the automatic learning of Q&A training pairs and the mining of needed knowledge from them. Social collections, however, have two salient characteristics: the textual mismatch between questions and answers (i.e., question words are not necessarily used in answers), and user-generated trolling or flippant answers, which are unfavorable factors in our study. Therefore, we only extract questions and their best answers to form the Q&A training pairs, where the best answers are longer than an empirical threshold (20 words).88The average length of the answer candidates is 29 words. Therefore, we set the threshold at 20 to filter out those Q&A pairs that are not likely to answer complex questions.Eventually, we extracted 15,678,739 Q&A pairs and used them as training data.A typical complex QA system is a cascade of three modules. The Question Analyzer analyzes the test question and identifies its type. The Document Retriever & Answer Candidate Extractor retrieves documents related to questions from the given collection (Xinhua and Lianhe Zaobao newspapers published 1998–2001 were used in this study) for consideration and segments them into sentences as answer candidates. The Answer Ranker applies sentence retrieval techniques (Balasubramanian et al., 2007) to estimate the “similarities” between the sentences (we used 1024 sentences) and questions, and ranks them based on their similarities. This paper simply employs a Kullback-Leibler (KL)-divergence language model approach. The top N sentences are deemed the final answers.As an example, consider Table 3, in which test question Q1 and three candidate answers extracted by the Document Retriever & Answer Candidate Extractor are given. For the Answer Ranker, which directly calculates the similarities between the question and the answer candidates, it is difficult to correctly select a2 as an answer because the three candidates contain the same question keywords. To improve this architecture, external knowledge must be incorporated. As introduced in Section 2, social Q&A collections are a good choice for mining the necessary knowledge.In this paper, we propose a question-type-specific technique of exploiting social Q&A collections (Section 4) to mine knowledge and compare it with question-specific (Section 5.1) and monolingual translation-based (Section 5.2) methods in the experimental section. We also compare the three models with LexRank, which has been widely used in query-based summarization tasks.Based on our observation that answers to certain types of complex questions usually contain question-type-dependent cue expressions that are helpful for answering such questions (Table 6), we propose a QTSM that learns these cue expressions for each type of question and utilizes them to improve complex QA systems. For each test question, QTSM performs the following steps:•Recognizes the type of test question.Learns the positive and negative Q&A training pairs for the particular type of question from social Q&A collections.Extracts salient question-type-specific cue expressions from the Q&A pairs and then utilizes them with the Q&A pairs to build a binary classifier for the type of test question.Employs the learned classifier to remove noise from the candidate answers before using the Answer Ranker to select the final answers to the test question.Fig. 1shows the architecture of the QTSM system.Previous work on factoid QA systems recognized question types by classification techniques (Li and Roth, 2002) that require the taxonomy of the question types and the training instances for each type. This algorithm may be inappropriate for complex QA systems since (a) hundreds of question types exist and (b) we have little prior knowledge for defining complex QA-oriented taxonomy. Therefore, such related studies as NTCIR (Mitamura et al., 2008) classify questions into a specific question type: the EVENT type. In this paper, we categorize complex questions into fine-grained question types by identifying their question focus.Question focus is defined as a short sub-sequence of tokens (typically one to three words) in a question that are adequate for indicating the question type. For example, consider Q1=“What are the hazards of global warming?” and Q2=“What were the casualties of the Indonesian tsunami?.” “Hazard” and “casualties” are the corresponding question focuses based on the above definition. Note that the question focus used here resembles the term “lexical answer type (LAT)” (Ferrucci et al., 2010) in which answers to such factoid questions as “In which city is the River Seine?” must be instances of LATs. This paper uses “question focus” because answers to complex questions are sets of information nuggets.To recognize the question type, we simply assume that the type of complex question is only determined by its question focus; we use question-type and question focus interchangeably in this paper. Based on this assumption, questions Q1 and Q2 belong to hazard and casualty-type questions, respectively. Krishnan et al. (2005) showed that (a) the accuracy of recognizing question types reached 92.2% using only question focuses and (b) the recognition accuracy of the question focuses was 84.6%. This indicates that most questions contain question focuses, and thus it is practical to use them in representing question types. This shifts the task of recognizing question types to recognizing question focuses in questions. Nevertheless, our approach to recognizing question types has the following potential disadvantages: (1) questions belonging to identical question types may variously use different question focuses and (2) questions do not necessarily contain a question focus. Identifying synonyms of question focuses can partially solve the first problem, and here this is introduced in Section 4.2. Patterns can be used to address the second problem, but that approach is beyond the scope of this paper.We regard question focus recognition as a sequence-tagging problem and employ conditional random fields (CRFs) (Lafferty et al., 2001) because many studies have proven their consistent advantage in sequence tagging. We manually annotated 4770 questions with question focuses to train a CRF model that classifies each question word into a set of tags O={IB, II, IO}, where IBis a word that begins a focus, IIis a word occurring in the middle of a focus, and IOis a word outside of a focus. In the feature templates used in the CRF model,wnand tnrefer to word and part-of-speech (POS), where n refers to the relative position from current word n=0. These feature templates have four types: unigrams ofwnand tn, where n=−2, −1, 0, 1, 2; bigrams ofwnwn+1and tntn+1, where n=−1, 0; trigrams ofwnwn+1wn+2and tntn+1tn+2, where n=−2, −1, 0; and bigrams of OnOn+1, where n=−1, 0. We employ an open source CRF tool, CRF++,99http://code.google.com/p/crfpp/.and set the parameters to default values.Among the 4770 questions obtained, 1500 were extracted for the test set, and the others were used for training. Our experiment shows that the precision of the CRF model on the test set was 89.5%. Offline, the CRF model was used to recognize question focuses in the questions of social Q&A pairs. The distributions of the identified question focuses are shown in Figs. 2 and 3. We recognized 103 question focuses for which the frequencies exceeded 10,000. The numbers of question focuses for which the frequencies exceeded 100, 1000, and 5000 are 4714, 807, and 194, respectively. Among the 4714 recognized question focuses, 87% were not included in the training questions. In the online phase, we used the CRF model to identify the question focuses of the test questions.Once question types are recognized from social Q&A pairs, positive and negative training Q&A pairs for the recognized question types can be automatically learned.For question-type X, the social Q&A pairs, for which the question focuses are the same as X, are regarded as basic positive Q&A pairs QAbasicof X-type questions. Formally, QAbasic={QAi|QFi=X}, where QAidenotes a Q&A pair and QFidenotes the question focus of QAi. Table 4gives the number of Q&A pairs for each type of question in an extension of the NTCIR 2008 test set (discussed in the experimental section). For example, 10,362 Q&A pairs are learned for answering hazard-type questions. Table 5lists some of the questions used with their best answers as basic positive training pairs of the corresponding type of complex questions.For question types such as casualty ()-type, for which only a few basic positive Q&A pairs are learned, it is possible to use Q&A pairs for similar question types like fatality ()-type. We adopted HowNet1010We used the version released in 2000.(Zhendong and Qiang, 1999), which is a lexical knowledge base with rich semantic information that serves as a powerful tool in meaning computation, for bootstrapping the basic positive Q&A pairs. In HowNet, a word may represent multiple concepts, each of which consists of a group of sememes. We demonstrate this with the following two examples:Here, word(casualty) represents one concept (c1) that is composed of four sememes, such as phenomena|. Word(damage) has two concepts (c1 and c2), each of which consists of a sememe.The similarity between the two words can be estimated bysim(w1,w2)=max1≤i≤|w1|;1≤j≤|w2|sim(ci,cj),sim(ci,cj)=∑1≤k≤|ci|max1≤z≤|cj|sim(sei,k,sej,z)|ci|,where ciand cjrepresent the ith and jth concepts of wordsw1andw2, respectively,|w1|is the number of concepts represented byw1, sei,kdenotes the kth sememe of concept ci, |ci| is the number of sememes of concept ci, and sim(sei,k, sej,z) is set to 1 if they are the same (otherwise, to 0). Using this algorithm, we get similarity sim(casualty|, damage|)=0.25.Accordingly, the bootstrapping positive Q&A pairs QAbootof X-type questions are composed of the Q&A pairs in which the question focuses resemble X. Formally, QAboot={QAj|sim(QFj, X)>θ1}, where QFjis the question focus of QAjand θ1 is the similarity threshold.For each type of question, we also randomly selected Q&A pairs that do not contain question focuses and their similar words in questions as negative Q&A training pairs.We preprocessed the training data, including word segmentation, POS tagging, named entity recognition (Wu et al., 2005), and dependency parsing (Chen et al., 2009). We also replaced each named entity1111We used five unique entities: person, location, organization name, time, and numeric expressions.with its tag type to extract the class/lexical-based cue expressions.In this paper, we extracted two kinds of cue expressions: n-grams at the sequential level and dependency patterns at the syntactic level. Cue expression mining extracts a set of frequent class/lexical-based and POS-based subsequences that are indicative of the answers to a type of question. n-gram cue expressions include the following:•Lexical unigrams, selected using formulaTFIDFw=tfw×log(N/(dfw)), wheretfwdenotes the frequency of wordw,dfwdenotes the number of documents where wordwappears, and N is the total number of Q&A pairs. Table 6shows the most frequent unigrams learned for each type of question.Class/lexical-based bigrams and trigrams that contain the selected unigrams. In theory, we could feed all features to the support vector machine (SVM) classifier and let it select the informative features, but we restricted the feature set for practical reasons: (1) the number of features would become tremendously large and (2) the feature vector would be highly sparse with a huge number of infrequent features, and thus the SVM learning would become very time-consuming. Therefore, we ranked the class/lexical-based bigrams and trigrams (including the following POS-based bigrams and dependency patterns) by their TFIDFs, and chose them at an appropriate percentage (in Section 6.2, we compare various percentages).All POS-based unigrams and POS-based bigrams at an appropriate percentage.A dependency pattern is defined as the relation among the words of a dependency tree. Fig. 4shows an example. Similarly, we selected the frequent class/lexical-based and POS-based dependency patterns at an appropriate percentage. Note that we used only a percentage for both n-gram and dependency cue expressions.We also assigned each extracted cue expression ceia weight calculated usingweightcei=c1cei/(c1cei+c2cei), where,c1ceiandc2ceidenote the frequencies in positive and negative Q&A training pairs, respectively. The weights are used as feature values in the SVM classifier.As mentioned above, we used the extracted cue expressions and collected Q&A pairs for each type of question to build a question-type-specific classifier for removing noisy sentences from answer candidates. Many studies indicated that SVM performed very well in such classification tasks as question classification (Zhang and Lee, 2003), factoid QA (Suzuki et al., 2002), biographical sentence classification (Biadsy et al., 2008), and text classification (Joachims, 1998). Finally, we employed multivariate classification SVMs (Joachims, 2005)1212http://svmlight.joachims.org.that can directly optimize a large class of performance measures like F1-Score, prec@k (precision of a classifier that predicts exactly k=100 examples to be positive), and error-rate (percentage of errors in predictions). Instead of learning a univariate rule that predicts the label of a single example in conventional SVMs (Vapnik, 1998), multivariate SVMs formulate the learning problem as a multivariate prediction of all the examples in the data set. Considering hypothesesh¯that map tuplex¯of n feature vectorsx¯=(x1,...,xn)to tupley¯of n labelsy¯=(y1,...,yn), multivariate SVMs learn a classifier(1)h¯w(x¯)=argmaxy¯′∈Y¯{wTΨ(x¯,y¯′)},by solving the following optimization problem:(2)minw,ξ≥012∥w∥2+Cξ,(3)s.t.:∀y¯′∈Y¯∖y¯:wT[Ψ(x¯,y¯)−Ψ(x¯,y¯′)]≥Δ(y¯′,y¯)−ξ,where w is a parameter vector, Ψ is a function that returns a feature vector describing the match between (x1...xn) and (y1′...yn′), Δ denotes the types of multivariate loss functions, and ξ is a slack variable.Our complex QA system is related to the query-based summarization task of DUC (Document Understanding Conferences) (Dang, 2006; Harabagiu et al., 2006), and in this regard LexRank (Erkan and Radev, 2004; Otterbacher et al., 2005) is commonly used in query-based summarization tasks. Therefore, we first re-implement LexRank as a baseline. In addition, previous studies proposed the QSM (Kaisser et al., 2006; Chen et al., 2006) and MTM methods (Berger and Lafferty, 1999; Riezler et al., 2007; Xue et al., 2008; Bernhard and Gurevych, 2009) for generating and retrieving answers to complex questions. In this paper, we also re-implement those approaches for performance comparison.The complex QA system discussed in this paper is also related to the query-based summarization of DUC (Dang, 2006; Harabagiu et al., 2006), which synthesizes a fluent, well-organized 250-word summary for a given topic description and a collection of manually generated relevant documents. The topic descriptions usually consist of several complex questions such as “Describe the theories on the causes and effects of global warming and the arguments against them.” LexRank is commonly used in query-based summarization tasks (Erkan and Radev, 2004; Otterbacher et al., 2005; Chali et al., 2009).The concept of LexRank is illustrated by the following model:(4)p(ai|Q)=d×rel(ai|Q)∑zrel(z|Q)+(1−d)×∑vsim(ai,v)∑zsim(z,v)×p(v|Q),where p(ai|Q) is the score of sentence aigiven question Q, which is determined as the sum of its relevance to the question and the similarity to the other sentences among the candidates, and d is “question bias,” which is a trade-off between relevance and similarity. For details, refer to the previous works (Otterbacher et al., 2005; Chali et al., 2010).Eq. (4) can be explained as follows. Candidate ai, which achieved a high relevance score, is likely to contain an answer; candidate aj, which may not resemble the question itself, is also likely to contain an answer if it resembles ai.QSM first learns the potential answer words to the question and then re-ranks the candidates by incorporating their “similarities” with the answer words. Fig. 5shows the architecture. For each submitted question, the following steps are performed. (1) In Retrieving Similar Q&A Pairs, an information retrieval (IR) algorithm retrieves the most similar Q&A pairs (the top 50 in our experiments) to the question from the social Q&A collection. (2) The Learning Answer Profile gives weights to all of the non-stop words from the retrieved Q&A pairs using the TFIDF scores and then selects the top M words to form an answer profile: Ap. (3) The Answer Ranker re-ranks the answer candidates based on a similarity formula sim(ai)=γrel(ai|Q)+(1−γ)sim(ai, Ap), where rel(ai|Q) denotes the relevance between question Q and candidates ai, sim(ai, Ap) means the similarity between candidates and answer profile Ap, and γ is the question bias weight that is empirically set to 0.95 in our experiments. (4) Finally, the top N candidates are selected as answers to question Q.Following the earlier work (Otterbacher et al., 2005; Chali et al., 2010), we computed rel(ai|Q) and sim(ai, Ap) using the following equations:(5)rel(ai|Q)=∑w∈Qlog(tfw,ai+1)×log(tfw,Q+1)×idfw,(6)sim(ai,Ap)=∑w∈ai,Aptfw,aitfw,Ap(idfw)2∑xi∈ai(tfxi,aiidfxi)2∑yi∈Ap(tfyi,Apidfyi)2,wheretfw,xis the number of timeswappears in x.QSM was first proposed for answering definitional questions and TREC “other” questions (Kaisser et al., 2006; Chen et al., 2006); however, it learns answer words from the most relevant snippets returned by a web search engine. Section 6 compares a QSM based on the 50 most relevant social Q&A pairs with a QSM based on the 50 most relevant snippets returned by Yahoo!.MTM learns the word-to-word translation probability from all social Q&A pairs without considering the question and the question type to improve complex QA systems. Fig. 6shows the architecture of an MTM-based system.A monolingual translation-based method treats Q&A pairs as a parallel corpus, where the questions correspond to the “source” language and the answers to the “target” language. Monolingual translation models have recently been introduced to solve the lexical gap problem in IR and QA systems (Berger and Lafferty, 1999; Riezler et al., 2007; Xue et al., 2008; Bernhard and Gurevych, 2009). A monolingual translation-based method for our complex QA system can be expressed by:(7)P(Q|ai)=∏w∈Q((1−γ)Pmx(w|ai)+γPml(w|C)),Pmx(w|ai)=(1−ζ)Pml(w|ai)+ζ∑t∈SP(w|t)Pml(t|ai),where Q is the question, aiis the candidate answer, γ is the smoothing parameter for the entire Q&A collection C,Pml(t|ai)=#(t,ai)|ai|, #(t, ai) denotes the frequency of term t in ai, andP(w|t)is the probability of translating answer term t to question termw, which is obtained using GIZA++ (Och and Ney, 2003).Adopting the common practice used in translation-based retrieval, we utilized IBM Model 1 to obtain word-to-word probabilityP(w|t)from 6.0 million social Q&A pairs. The preprocessing of the Q&A pairs only involved word segmentation (Wu et al., 2005) and stop-word removal.

@&#CONCLUSIONS@&#
