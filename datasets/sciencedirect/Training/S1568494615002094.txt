@&#MAIN-TITLE@&#
Composite artificial bee colony algorithms: From component-based analysis to high-performing algorithms

@&#HIGHLIGHTS@&#
31 components of ABC algorithm are tested with a systematic experimental study.Each component impact on algorithm performance is identified.Two benchmark sets, SOCO and CEC05, are used in experimental study.Two new variants of ABC algorithms are proposed for each of the two benchmark sets.The best components are selected for each step of the proposed ABC algorithms.

@&#KEYPHRASES@&#
Artificial bee colony,Continuous optimization,Component-based analysis,Integration of algorithmic components,

@&#ABSTRACT@&#
The artificial bee colony (ABC) algorithm is a swarm intelligence algorithm inspired by the intelligent foraging behavior of a honeybee swarm. In recent years, several ABC variants that modify some components of the original ABC algorithm have been proposed. Although there are some comparison studies in the literature, the individual contribution of each proposed modification is often unknown. In this paper, the proposed modifications are tested with a systematic experimental study that by a component-wise analysis tries to identify their impact on algorithm performance. This study is done on two benchmark sets in continuous optimization. In addition to this analysis, two new variants of ABC algorithms for each of the two benchmark sets are proposed. To do so, the best components are selected for each step of the Composite ABC algorithms. The performance of the proposed algorithms were compared against that of ten recent ABC algorithms, as well as against several recent state-of-the-art algorithms. The comparison results showed that our proposed algorithms outperform other ABC algorithms. Moreover, the composite ABC algorithms are superior to several state-of-the-art algorithms proposed in the literature.

@&#INTRODUCTION@&#
The artificial bee colony (ABC) algorithm is a recent swarm intelligence algorithm inspired by the foraging behavior of honeybee swarms. It was introduced by Karaboğa [1] who illustrated the algorithm applying it to continuous optimization problems. Following these early works, there has been a large number of follow-up researches and a large number of papers have proposed variants of the ABC algorithm. These variants often change only one single or at most two components of the original ABC algorithm. Examples of such modifications are changes in an equation that is defining the modifications applied to solutions or different ways of how to initialize the colony.In a previous study [2], a comprehensive, experimental analysis has been conducted to investigate the behavior of nine different ABC variants on the same benchmark function set. In particular, we have examined the impact the (automatic) tuning of ABC algorithm parameters and the introduction of a local search phase have on performance and the contribution specific ABC variants make. Therefore, discussion about the general advantages and disadvantages of ABC variants can be found in our previous study [2]. However, the impact individual proposals of modifications for ABC algorithm components have were not analyzed in that previous work. In particular, a systematic analysis of the individual contribution of each new ABC algorithm component on performance was not done.This article has two main contributions. The first is a systematic study of the impact that specific modifications of the ABC algorithm have on its performance. It is done by decomposing the ABC algorithm in a number of algorithm components (for example the initialization, the search equation used by onlooker / employed bees, the usage or not of a local search etc.). Then, each of these components is studied and it is tried to identify their impact on algorithm performance. This study is done on two well accepted benchmark sets in continuous optimization. One of these sets is the CEC 2005 benchmark set that comprises the benchmark functions that have been used in the IEEE CEC’05 competition on real-parameter optimization [3]; the other set is the SOCO benchmark set that was used for a special issue of the Soft Computing journal on large-scale continuous optimization problems [4]. These sets are used as they are rather large sets of functions with different characteristics and by using them, the results become comparable to those of other algorithms from the literature.Once the impact specific ABC algorithm components have on performance have been analyzed, a next natural step is to show that the obtained knowledge is useful not only for a better understanding of ABC algorithms, but also for the design of new algorithms. In particular, as a next step we try to exploit the knowledge gained for designing new, high performing algorithms. This step is the second, main contribution of the article. In particular, we aim at identifying for each of the two benchmark sets that have been used in the component-wise analysis to define new ABC algorithm variants. Two new variants of ABC algorithms are proposed. They are called “composite artificial bee colony algorithms” (CompABCcecand CompABCsocoin short for the composite ABC algorithms that are proposed for the CEC and the SOCO benchmark set, respectively). Obviously, putting together the algorithm components that had the best individual effect does not guarantee to obtain the best overall final algorithm. This is mainly due to interactions that may exist between these components. However, such an approach may be the one that is (and often has) been chosen by algorithm designers. Still, setting of the numerical algorithm parameters is helped by the use of automatic algorithm configuration techniques, which somehow alleviates the human algorithm designer from choosing specific algorithm parameters based on intuition and limited experiments.The proposed algorithms were compared to several ABC variants and other state-of-the-art algorithms to assess their performance. The encouraging results of the proposed algorithm indicates the importance of component-based analysis for further new variants of ABC algorithms. It also indicates that the approach of designing algorithms based on combining promising algorithm components, as it has been done here, is a feasible procedure.This paper is structured as follows. Section 2 briefly reviews the original ABC algorithm. The component-based view of ABC algorithms is presented in Section 3. Section 4 elaborates the experimental procedures and function suites, ABC variants and their components used in the experiments. Section 5 analyzes the impact the specific modifications have on ABC performance. In Section 6, the composite ABC Algorithms for two benchmark suites (CompABCcecand CompABCsoco) are introduced. Comparisons of the Composite ABC algorithms to other ABC variants and state-of-the-art algorithms are presented in Section 7. Section 8 concludes the article.Real bee colonies consist of three different types of bees: employed bees, onlooker bees and scout bees [1]. Each bee has a specialized task in the colony and their aim is to maximize the amount of nectar stored in the hive using division of labor and self-organization [5]. This organizational nature and the behavior of the bees has inspired the artificial bee colony (ABC) algorithm. It was proposed in 2005 [1] for bound-constrained continuous optimization problems, which can be summarized as follows:(1)minf(X)st:xj∈[xjmin,xjmax],j=1,2,...,Dwhere f(X) is the objective function, X={x1, x2, ..., xD} is the set of decision variables, D is the number of decision variables, which is also called dimension (of the search space), andxjminandxjmaxare the lower and upper bound values for each decision variable xj, respectively.In ABC, each food source is placed in the D-dimensional search space and presents a potential solution to the problem. The amount of nectar at the food source is assumed to be the fitness value of a food source. Generally, the population size of the bee colony is considered to be twice the number of food sources. Half of the population is considered to be the employed bees and the other half to be onlooker bees.Algorithm 1The pseudo-code of the original artificial bee colony algorithmStep 1. Initializationwhiletermination condition is not met doStep 2. Employed Bees StepStep 3. Onlooker Bees StepStep 4. Scout Bees Stepend whileThe ABC algorithm consists of four steps: initialization, employed bees step, onlooker bees step and scout bees step. In the initialization step of the algorithm, the initial population is generated and other parameters are set. Then, the other three main steps of the algorithms are executed repeatedly in a loop until the termination condition is met. The outline of the algorithm is given in Algorithm 1 and the detailed description of the main steps is the following.Step 1. Initialization In this step, SN food sources (candidate solutions), where SN denotes the size of population, are placed randomly on the D-dimensional search space as follows:(2)xi,j=xjmin+φi,j(xjmax−xjmin),where φi,jis a random number distributed uniformly in0,1for dimension j and food source i. Besides, a visit limit of each food value is initialized for each food source.Step 2. Employed bees There is a one-to-one correspondence between employed bees and food sources in this step. Each employed bee visits the food source to which it is associated and searches in the vicinity of the selected food source for better solutions. To find a new candidate food source location,vi, the following search equation is used by taking advantage of a randomly chosen second food source, xr1:(3)vi,j=xi,j+ϕi,j(xi,j−xr1,j),where j is a randomly selected dimension (j∈{1, 2, …, D}), ϕi,jis a uniform random number in−1,1, xi,jand xr1,jare the position of the reference food source xiand a randomly selected food source xr1 in dimension j, respectively. If the candidate food source,vi, is placed in a better location than the reference food source, xi, it replaces xiand becomes the new food source. The important point to note here is that the search equation changes only one dimension of the reference food source. On one hand, this may lead to improve the diversification behavior; on the other hand, it may lead to slow convergence.Step 3. Onlooker bees Onlooker bees select randomly food sources to visit and search for the new food source locations using Eq. 3 as the employed bees do. Unlike the employed bees, onlooker bees take the amount of nectar on food sources into account when selecting a food source to visit. The better the fitness value, the higher the probability of being selected by an onlooker. The selection probability, pi, of a food source xiis calculated as follows:(4)pi=fitnessi∑n=1SNfitnessn.In Eq. 4, fitnessiis the fitness value of food source xi. It is defined as(5)fitnessi=11+fi,fi⩾0,1+abs(fi),fi<0,where fiis the objective function value of food source xi. Using this probabilistic selection rule, onlooker bees usually visit better food sources and try to find new candidate locations around good food sources. The onlooker bees step plays an important role in the intensification behavior of the algorithm [6].Step 4. Scout bees If onlookers and employed bees cannot improve the location of a food source a number of times, which is controlled by the limit parameter, the food source is considered as exhausted and it is abandoned. In this case, a scout bee tries to find a new food source to replace the abandoned one. A new food source location is determined by Eq. 2 as described in the initialization step. This abandoning and scouting mechanism is designed to assist the algorithm to escape local optima.In this article, a component-based view on ABC algorithms are adopted. In this point of view, an algorithm in general, or an ABC algorithm more specifically, is seen as being composed of a number of algorithm components. In the ABC case, such algorithm components are on a high level of the four steps: initialization, employed and onlooker bees step, and scout bees step. Moreover, hybridization with local search or adaptive population size strategies can be seen as additional components of the ABC framework, which is presented in Algorithm 2 from the component-based view. Each of these components has some specific tasks (e.g. initializing the search, exploration of close-by solutions in the employed bees steps, biased exploration of close-by solutions (onlooker bees step) etc.). In turn, each of these components may further be composed of other components that influence its particular search behavior. For example, each possible way of how to initialize the search may be seen as one component. As another example, the particular search equation used by the employed bees step may be seen as being composed by several components, where one is the choice of the biasing solution (e.g. a random one in the original Eq. (3)) but where other choices (e.g. the global-best solution in gbest ABC, see later Eq. (7)) are possible. Taking this reasoning to a next level, also specific parameters may be seen as components that have to be appropriately fixed.In general, the component-based view sees algorithms as being composed of different components that may be interchangeable and that have to be appropriately chosen to reach high algorithm performance. From this point of view it is also clear that most proposals of alternatively ABC algorithms change one or few components of the original ABC algorithm. This behavior of the algorithms can be seen in Table 1. Therefore, with the component-based view, it is possible to obtain information about how much the strategies suggested in the same component contribute to the algorithm performance by comparing them to each other. By doing so, the best strategies can be determined for each component of the algorithm. Furthermore, for many ABC variants which propose one or two modifications of the original ABC algorithm, the component that dominates the algorithm performance can be identified. Another important advantage of the component-based view and analysis is that the results of the analysis will inform designers of future ABC algorithms on what is the individual impact of specific algorithm components and this information may guide the designer to generate high-performing algorithms.Algorithm 2The skeleton of artificial bee colony frameworkStep (Component) 1. Initialization while termination condition is not met doStep (Component) 2. Employed Bees StepStep (Component) 3. Onlooker Bees StepStep (Component) 4. Scout Bees StepStep (Component) 5. Apply Local Search (Optional)Step (Component) 6. Determine the Population Size (Optional)end whileTo determine actual effects of the considered modifications at each step on the performance of ABC, the component-based analyses were followed systematically. The procedures of empirical analysis conducted here is the following. First, the components (modifications) were categorized into groups according to the steps where they were applied. For each modification, a new ABC variant was implemented. These new variants operate the same way as ABC except the change that is introduced by the considered modification. Secondly, when necessary, the parameters of the algorithms were tuned by using an offline algorithm configuration tool, Iterated F-Race [7], to make an unbiased comparison between the ABC algorithms. An empirical analysis and a comparison of the resulting algorithms were performed on two different benchmark function suites. In the following subsections, the considered ABC variants and their modifications are reviewed, and give details on benchmark functions, parameter tuning and the statistical tests were used in the comparison.30 algorithmic components of the original ABC algorithm and 12 different ABC variants have been used in the component-based analysis. These components are listed in Table 1. These variants of ABC have been published in various journals and have been shown to outperform the original ABC algorithm. Therefore, it is expected that some components of these algorithms introduce significant positive effects on the original ABC algorithm. Unfortunately, the individual effect of each modification is often unknown and can be influenced by the problem type or problem dimension. To this aim, the most effective components at each step of the algorithm have been tried to identify. In total, 31 components of ABC algorithms (one component is newly proposed in this paper) have been analyzed and compared on two different benchmark suites. The short descriptions of the algorithms and their proposed components are given next.•Modified ABC (MABC): MABC [5] introduces two modifications to Eq. 3. The first one is a “modification ratio” (MR), which controls the number of variables that are modified in Eq. 3. The second modification is a “scaling factor” (SF), which adjusts the perturbation range by changing the range of ϕi,jfrom−1,1to−SF,SF. Consequently, for each dimension the modified search equation is applied as follows:(6)xi,j=vi,j=xi,j+ϕi,j(xi,j−xr1,j),Ri,j<MRxi,j,otherwisewhere Ri,jis a uniformly distributed random number between 0 and 1.Gbest-guided ABC (GABC): To enhance the intensification behavior, global-best information (xGbest) is used in GABC; here, xGbestis the position of the best solution found so far. The modified search equation of GABC for both employed and onlooker bees step is:(7)vi,j=xi,j+ϕi,j(xi,j−xr1,j)+ψi,j(xGbest,j−xr1,j),where xGbest,jis the jth element of the global-best solution, ψi,jis a uniform random number in0,C, where C is a nonnegative constant that is set to C=1 [8] or used as a controllable parameter that is set manually [9].Gbestdist-guided ABC (GDABC): In addition to GABC, Diwold et al. [8] have proposed GDABC, in which Eq. 7 is modified to select preferably a neighbor food source, xr1, according to a probabilistic selection rule, which is defined as(8)pk=1dist(loci,lock)∑n=1,n≠iSN1dist(loci,locn),where pkis the probability of neighbor xkbeing chosen, locxis the location of a food source, and the distance between two food source locations, locxand locy, is denoted by dist(locx, locy) [8]. The underlying idea of this modification is to prefer nearer neighbors because it is probable to find better locations by searching between two good solutions, which are probably close to each other in many types of optimization functions [8].Opposition-based ABC (OABC): This ABC variant [10–12] proposes embedding opposition-based learning into the initialization step of the original ABC. The basic idea underlying opposition-based learning is to improve the chance of starting with better solutions by simultaneously checking the opposite position. Let xi=(xi,1, xi,2, ..., xi,D) be a food source in the D-dimensional space. Then, the food source,xˇi, at the opposite location of xiis defined by(9)xˇi,j=xi,jmax+xi,jmin−xi,j.If the fitness of the opposite food source,xˇi, is better than the food source xi, then xiis replaced byxˇiat the initialization step.One-Point Inheritance ABC (OPIABC): OPIABC proposed a modification of the search equation of the employed bees step. In the employed bees step of OPIABC [13], selected food sources can inherit one decision variable information from a randomly selected food source by using the following equation:(10)vi,j=xi,j+ϕi,j(xi,j−xr1,j)ifj=j1xr2,jifj=j2xi,jotherwisewhere j1, j2∈{1, 2, …, D} are two random indices with j1≠j2. The authors claimed that promoting one-point inheritance (OPI) can balance the intensification and diversification behaviors leading to a speed-up of the convergence of the algorithm.Chaotic ABC (CABC): Alatas [14] proposed three variants of CABC algorithms. In the first version of CABC, seven different chaotic maps (logistic map, circle map, gauss map, henon map, sinusoidal iterator, sinus map, and tent map) are used as random number generators instead of canonical uniform random number generators. Initial values of each dimension of the food source locations are generated by iterating one of the selected chaotic random number generators. In the second version of CABC, a “chaotic” search procedure was embedded into the scout bees step. If a food source cannot be improved for limit/2 trials, then the chaotic search is triggered for limit/2 trials on the food source in the scout bees step. In the chaotic search procedure, for finding better food source locations, randomly selected decision variables of the food source are modified by a search equation which uses chaotic random numbers iteratively. The details of the chaotic search procedure can be found in supplementary material [15] and [14]. The third version of CABC combines these two modifications.Best-so-far ABC (BABC): BABC uses the information of the global-best solution (global-best solution is referred as the best-so-far solution by the authors) similar to GABC and GDABC but only in the search equation of the onlooker bees step:(11)vi,d=xi,j+fitness(xGbest)(ϕi,j(xi,j−xGbest,j)),where j is a randomly selected dimension and fitness(xGbest) is the latest fitness value of xGbestbefore onlooker bees step starting. The search equation is applied for each dimension d. This may cause that all dimensions of a candidate solution,vi, get closer to the global-best solution, xGbest. BABC introduces a modification for scout bees step as well. BABC tries to adjust the exploration of scout bees as the following modified equation(12)vi,j=xi,j+xi,jϕi,j(wmax−(itrcurrent/itrmax)(wmax−wmin)),whereviis a new feasible solution of a scout bee that is modified from the current position of an abandoned food source, xi,wmaxandwminare control parameters that define the minimum and maximum percentage of scout bee position adjustment, respectively; itrcurrentpresents the current iteration executed so far, and itrmaxpresents the maximum iteration number for the algorithm [16]. This approach is based on an assumption that when the abandoned food source will be far from the optimal solution in the early iterations and then the strategy enhances diversification, in later iterations it will converge closely to the optimal solution and this favors intensification.Improved ABC (ImpABC): ImpABC [17] modified initialization, the employed and the onlooker bees steps of the algorithm. It employs both chaotic maps and an opposition-based learning method at the initialization step. First, the initial population is generated using a chaotic random number generator (logistic map). Then, opposition-based initialization is performed duplicating the number of the solutions in the population. The final population is gathered by discarding half of the worst food locations from the environment. This initialization mechanism is used by the same authors for some later ABC variants [18,19] as well. For the employed and onlooker bees steps, ImpABC offers two modifications in the search equation. The first modification emphasizes controlling the frequency of perturbation by the MR parameter as in MABC. The second modification is inspired from differential evolution (DE) [20]. The algorithm comes with a probabilistic selection of two search equations, namely ABC/best/1 and ABC/rand/1 similar to DE/best/1 and DE/rand/1 in DE, respectively. ABC/rand/1 is the same equation as defined by Eq. 3 and ABC/best/1 is defined as:(13)vi,j=xGbest,j+ϕi,j(xr1,j−xr2,j).ABC/best/1 generates a new food source with the help of two neighbor food sources (xr1 and xr2) and the xGbestfood source. According to the authors, while ABC/best/1 improves the convergence speed (but may cause premature convergence), ABC/rand/1 is a robust method but results in low convergence speed [17].Enhanced ABC (EABC): EABC [21] proposes two different search equations for employed bees and onlooker bees, separately. The search equation of the employed bees step is(14)vi,j=xr1,j+αi,j(xGbest,j−xr1,j)+βi,j(xr1,j−xr2,j),where α is a random number in the range0,A, where A is a non-negative constant; β is B*rand(0, 1) where B is a random number generated by a normal distribution with mean μ and standard deviation σ. In the search strategy of the onlooker bees step, xGbestis used in the third term of the search equation instead of xr2 in order to enhance exploitation as:(15)vi,j=xr1,j+αi,j(xGbest,j−xr1,j)+βi,j(xr1,j−xGbest,j)Efficient and Robust ABC (ERABC): In ERABC [22], the default initialization is modified by a “chaotic” initialization using a logistic chaotic map. Then, search equation of employed bees step is replaced with the BABC search equation (Eq. (11)). Finally, to avoid getting trapped into local optima and to obtain more refined results, a chaotic search, (different from CABC) is incorporated into the scout bees step of the ERABC algorithm [22]. This chaotic search is triggered after a food source reaches the search limit and it tries to find better solutions around abandoned food sources by modifying the dimensions of a food source with iterated chaotic random numbers. A detailed description of the chaotic search mechanism of ERABCscan be found in the supplementary material [15] and [22].Incremental ABC (IABC): Aydın et al. [23,24] have proposed IABC that applies the incremental social learning (ISL) framework [25] and local search procedures to the ABC algorithm. The basic idea of ISL is that a new agent is added to the population every g iterations until a maximum population size (SNmax) is reached. A new agent is added using information from the global-best solution by modifying the initial random position of the new food source as follows:(16)x´new,j=xnew,j+φi,j(xGbest,j−xnew,j),where xnew,jis the randomly generated new food source location according to Eq. 2, andx´new,jis the updated location of the new food source. Apart from adapting ISL to ABC, two other modifications have been proposed for the ABC. For enhancing intensification behavior, a modified search equation guided by global-best solution information is used as in GABC but in a different way:(17)vi,j=xi,j+ϕi,j(xGbest,j−xi,j).As the second modification, a search strategy is defined to assist the scout bees discovering new food sources in the vicinity of the global-best solution. The search equation is formulated as(18)vi,j=xGbest,j+Rfactor(xGbest,j−xnew,j),whereviis the new food source location replacing the abandoned food source, xi, and Rfactoris the replacement factor around the global-best solution, xGbest. This modification boosts the exploitation behavior of the algorithm and helps to converge quickly towards good solutions; however, it may lead to early convergence. Furthermore, the IABC algorithm is also hybridized with two well-known and powerful local search routines. The first local search is Powell's direction set method [26] using Brent's technique [27] as the auxiliary line minimization algorithm. The second local search procedure is Lin-Yu Tseng's Mtsls1 [28]. xGbestis set as the initial position of the local search procedure and applied at each iteration. Local search is terminated after a number of local search iterations, LSItrmaxor once a very small difference between two solutions is detected, so-called FTol. It uses an adaptive step size and a parameter, MaxFailures, to overcome stagnation. More detailed descriptions about the local search procedures and the related parameters are given in [23,24,29].Dynamic population size ABC (ABCDP): Recently, Aydın et al. [29] proposed another variant of ABC in which the population size is changing dynamically at computation time. It is an improved version of IABC, which is designed for tackling the bi-objective power dispatch problem. ABCDP begins with a very small-sized population and dynamically changes the population size according to the success of the colony after each iteration. Then, the algorithm decides stochastically to add a new food source or to remove the worst food source from the environment. Except the population size mechanism, ABCDP uses the same mechanisms and local search procedures defined in IABC.The name of each variants and their components, together with their names in the component-based analysis, are listed in Table 1. As shown in Table 1, most the variants have suggested modifications on one or two components of the ABC algorithm. When conducting the component-based analysis, it has been observed that the contribution of each modification is not only on the original ABC but also on the algorithms which proposed the modifications. This behavior shows another contribution of the proposed component-based analysis.To analyze the performance and the behavior of the modifications in ABC, experiments were conducted on two well-known benchmark function suites that have different features. The first suite is provided by the special issue on “Scalability of Evolutionary Algorithms and other Metaheuristics for Large Scale Continuous Optimization Problems” of the Soft Computing (SOCO) journal. It includes 19 scalable shifted functions. The SOCO suite was proposed to identify the scalability of the algorithms and therefore the dimensions of each problem can be freely scaled from 1 to D dimensions. In the SOCO special issue, the largest value of D used was 1000. Detailed formulas and descriptions of these functions can be found in [4].The second suite includes 25 benchmark functions proposed for the special session on real-parameters optimization organized for the IEEE 2005 Congress on Evolutionary Computation (CEC 2005). CEC 2005 has different problems and many of them are “shifted-rotated” functions, which are very hard to solve. Each problem can be formulated as 10, 30 or 50 dimensional functions. Because rotation matrices have been defined for functions of 10, 30 and 50 dimensions and, thus, there are usually used in the experiments. A detailed description of the individual functions and their mathematical expressions can be found in [3]. One of the reasons of selecting these benchmark suites is to show that the behaviour of components may change according to the applied problem suites. The other reason is that they are well-known and the results become comparable to those of other algorithms from the literature. Table 2gives a short description of these functions with their specific peculiarities.All the experiments have carried out the following experimental procedures, which were also indicated in the test suites specification documents [3,4]:•Each algorithm is run 25 times for each test function. The error value is calculated by (f(x)−f(x*)), where x* is the known global optimum of the function. Median of the error achieved in the 25 runs are provided as the performance measure.Each run stops when the number of function evaluations reached is to 5000×D and 10000×D or an error value lower than 10−14 and 10−8 is achieved on SOCO and CEC 2005 functions, respectively. It is refered to 10−14 or 10−8 as optimum threshold in this paper. For consistency, 5000×D is used as the maximum number of function evaluations for the component-based analyses.In this paper, the benchmark functions have been studied with different dimensionality. For the parameter tuning, 10 dimensional functions have been used to reduce the time complexity of the tuning. On the other hand, depending on necessary component-based analyses, specific procedures are followed on different problem dimensions at each step. Finally, the 30 and 50 dimensional CEC 2005 functions, and the 100 and 500 dimensional SOCO functions are used in the comparison of the Composite ABC algorithms to other algorithms to understand the performance and the scalability of the algorithms.In the broad sense, algorithm design includes all decisions needed to specify an algorithm for solving a given problem. The main challenge of an algorithm designer is that algorithm components and the parameter values influence the performance of the algorithm. To determine appropriate parameter values for an algorithm is a difficult task [30]. Recent research efforts in the direction of automated algorithm configuration have shown that automatic approaches to this problem have clear advantages over manual tuning [7,30–32]. Therefore, a parameter configuration tool, Iterated F-Race [7,33], is used for finding the parameters values of the original ABC algorithm and values of parameters for the modifications in this study.The modifications introduced to the original ABC algorithm could be considered as tunable categorical parameters of an ABC algorithm and the tuning therefore decide whether to use the modification or not. However, the studied modifications are not examined as parameters to be tuned since one of the main objectives of this study is to understand the individual effect of each modification on the ABC algorithm. Instead, Iterated F-Race is used for tuning the parameters defined in each modification to achieve the real effect of each modification on ABC and to ensure a fair comparison. Note that from the methodological point of view, this is different from what is done in many papers where the modifications have been proposed. In those papers, the modifications are usually added to default parameter setting.For the tuning task, 10-dimensional versions of the two benchmark suites are given to Iterated F-race as input instances. The number of function evaluations of each run of the algorithm to be tuned by Iterated F-race is equal to 50,000 for each benchmark set. The other parameters of Iterated F-Race were set to the default values defined in the original implementation. The results were obtained after reaching 5000 trial runs of the algorithms. In the online supplementary material [15], the details of the tuned parameters and their values have been listed for each empirical analysis.Statistical tests are necessary to confirm whether a newly proposed method offers a significant improvement over existing methods. In recent years, using statistical tests to understand differences in performance of the algorithms has become a standard in metaheuristics. There are two kinds of statistical tests: parametric and nonparametric [34]. In our experimental framework, non-parametric tests have been used to analyze the results, because it has been shown that the assumptions for the application of parametric tests are not met for several of the functions used in the test suites [34]. Two different nonparametric tests, the Wilcoxon signed ranks test and the Friedman test, have been conducted. The Wilcoxon signed ranks test aims to detect significant differences between the behavior of two algorithms. On the other hand, the Friedman test is used for multiple comparisons test to find performance differences between more than two algorithms. In the case of the Wilcoxon test, adjusted p-value is computed as described in [34,35], to allow the comparison of multiple algorithms avoiding the family-wise error that multiple pairwise comparisons could have introduced. To reject the null hypothesis, which is that no significant difference exists between the algorithms, the statistical analyses are performed with a significance level α=0.05.The designers of several ABC variants suggested that creating an initial population, which is spread properly in the search space, is an important task as it can affect the convergence speed of the algorithm and the quality of the final solution [17–19,36–39]. However, there are some questions that need to be answered: (1) Is the effect of initialization strategies on the convergence speed of the algorithm indeed statistically significant? (2) For high-dimensional functions, is it possible to the initial population to spread properly in the search space? (3) Are there any efficient initialization strategy that contribute to the algorithm's search behavior better than other approaches?For the component-based analysis in this step, our objective is to find answers to these questions by comparing the initialization strategies. Four different initialization approaches defined in the previous subsection have been considered: default initialization, chaotic initialization [14,36,38], opposition-based initialization [10–12,37,39] and opposition-based chaotic initialization [17–19]. These possibilities were integrated in the ABC algorithm so that the variants are called ABC-i-def (it is original ABC), ABC-i-cha (it is also first version of CABC [14]), ABC-i-opp (it is equal to OABC [10]) and ABC-i-mix, respectively.The initialization strategies have been tested on 10 and 50 dimensional CEC 2005 functions, and on 50 and 500 dimensional SOCO functions. For a fair comparison, Iterated F-race was not used for population size and limit parameters. The population size was set to 20 for all algorithms with limit=D×SN. Fig. 1shows parallel coordinates plots between the four algorithms on CEC 2005 and SOCO functions. As seen in Fig. 1 no difference can be found between any strategies. Moreover, the Wilcoxon test doesn’t find a significant difference between the default and the considered initialization strategies. In any case, according to the detailed comparison results, which can be found in the supplementary document [15], for both CEC 2005 and SOCO functions, ABC-i-mix strategy gives better averages than other strategies on more functions (7 functions for SOCO and 10 functions for CEC 2005) which are, however, not statistically significant.To understand the effect of initialization strategies on convergence speed, the difference of mean error values of initialization strategies relative to the original initialization strategy after the different number of function evaluations for low (10 dimensional CEC 2005 functions) and high (500 dimensional SOCO functions) dimensional functions are examined. Morever, the convergence characteristics have been plotted in supplementary document [15] for some functions in terms of the mean error values of 25 trials over the number of function evaluations. In Fig. 2, the points above zero indicate that the average result of the compared algorithm is better than the default strategy, otherwise it is worse than the default strategy after given number of functions evaluations. As seen in Fig. 2, for the low dimensional functions, the modified initialization strategies converge slightly faster than ABC-i-def in early iterations. However, for the later iterations, the relative differences between the algorithms do not change related to the initial solutions. Therefore, there is no significant improvement on convergence speed or on final outcomes can be observed for the initialization strategies. Moreover, for high dimensional functions, the initial populations created by the strategies are very similar in most cases. This indicates that all initialization strategies spread initial population in the search space similarly.Results:(1) No initialization strategy can increase the algorithm performance on CEC 2005 and SOCO functions. Therefore, the hypothesis that the initialization strategy has an important place in ABC algorithm design can be rejected. (2) The ABC variants (such as OABC and first version of CABC), which proposed modifications only for the initialization step, are not significantly better than the original ABC algorithm. (3) When the problem dimension increases, the gap between the best and the worst strategies decreases. This indicates that when the number of dimensions increases, the effect of any initialization strategy is lost. (4) Opposition-based chaotic initialization (ABC-i-mix) gives better results than others although a significant improvement is not observed on both benchmark suites. Hence, this strategy is used in our proposed composite ABC algorithms.The exploration and exploitation behaviors of the population influences directly the performance of the algorithm. If an employed bee or onlooker bee strategy that balances well the exploration and exploitation for all functions could be found, it would be the best performing algorithm. Since the employed and onlooker bees choose food sources with different probabilities, the impact of modifications of the search Eq. 3 is examined for each of these two steps as different choices may be best. To determine which strategy controls better the movement of the population towards high quality food sources, the following eight aforementioned search strategies have been considered in the experiments:•ABC-e-def: Search equation of original ABC defined by Eq. 3.ABC-e-gb: Search equation defined in Gbest-guided ABC (GABC) (Eq. (7))ABC-e-gbd: Search equation defined in Gbest distance-guided ABC (GDABC) (Eq. (7) with Eq. (8))ABC-e-inc: Search equation defined in Incremental ABC (IABC)(Eq. (17))ABC-e-mod: Search equation defined in Modified ABC (IABC)(Eq. (6))ABC-e-imp: Search strategy of Improved ABC (ImpABC) (with using Eq. (3) and (13))ABC-e-enh: Search equation in employed bees step of Enhanced ABC (EABC) (Eq. (14))ABC-e-opi: Search equation in employed bees step of OPIABC (Eq. (10))Each strategy considered here has different features such as using global-best position, the amount of perturbation or adjusting search perturbation. Table 3summarizes the search strategies according to these features. To compare the strategies and find any correlation between the results and the features, these modifications were added to the employed bees step of the original ABC algorithm. Then these algorithms were run on both small and large scale test functions after tuning all parameters using Iterated F-race. Figs. 3 and 4show the results of the comparison. Moreover, Wilcoxon's signed-rank test for the algorithm comparisons were used. Detailed test results are presented in Table 4.On the 10 dimensional CEC functions, ABC-e-imp and ABC-e-mod are significantly better than other approaches on f2cecand f4cec. Moreover, these algorithms, which have the feature of high frequency of perturbation (more than one dimensions to be modified at each step), are the top ranked algorithms. On 50-dimensional test problems, ABC-e-gdb is better than others in general. Moreover, the overall performance of ABC-e-imp and ABC-e-mod decreases as the dimension increases. Based on the results obtained on CEC functions, it is hard to decide which strategy is better than the others for all dimensions. But it seems that the methods, that are based on a high frequency of perturbation or using xGbestin the search equation enhance the performance of the ABC algorithm. Therefore, a new ABC variant, that uses the search equation of ABC-e-gdb and controls the frequency of perturbation with the MR parameter, is added. This variant is refered as ABC-e-gdbm and is tested on the CEC functions. The comparison results are shown in Table 5. According to comparison results listed in Table 5, ABC-e-gdbm performs slightly better than the other strategies in general and ranks first.For SOCO functions, the results are shown in Fig. 4 and Table 4. Surprisingly, on the contrary to CEC 2005 results, ABC-e-imp and ABC-e-mod strategies are significantly worse than other strategies for the SOCO functions. This indicates that the strategies with MR parameter exhibit poor scaling behavior. On the other hand, the strategies that use xGbestin the search equation is better than the original ABC on SOCO functions as well as on CEC 2005 functions. Specifically, ABC-e-gdb outperforms all strategies. However, no significant difference is found by the Wilcoxon test. This is because the relatively small number of problems and because the original ABC algorithm reaches the optimum threshold for many functions, leading to for effects. On the other hand, for both CEC 2005 and SOCO benchmark suites, any positive effect of adjusting perturbation range is clearly observed on the results.To understand the difference of the convergence behavior of ABC-e-gdb and other approaches that useGbestin search equation, run-length distributions (RLDs) [40] were generated. The RLDs plots for four algorithms on benchmark functions (f9soco,f11soco, f12socoand f14soco) are given in Fig. 5. According to the RLDs plots, ABC-e-gdb has similar convergence behavior to ABC-e-gd, however, it can escape local optima better than ABC-e-gb. This indicates that, selecting a random neighbor, xr1, related to the Euclidean distance rather than in a uniform random manner directly may allow the algorithm to escape from local optima and to avoid strong stagnation behavior. An interesting case for ABC-e-enh is also observed in Fig. 5 in early function evaluations. It can be seen that in almost 70% of the runs it finds better results than the optimum threshold after 100000 function evaluations, but in the remaining, much longer runs, it hardly finds additional optima.Results:(1) Search equations should use xGbestinformation for enhancing convergence speed. (2) For shifted-rotated and hard (CEC 2005) problems, modifying more than one dimension in the search equation helps to control the balance between exploration and exploitation, thus, leading to a performance improvement. However, the control of the frequency of perturbation in the search equation is difficult and sensitive to the problem dimension. For high dimensional functions, if the MR value is set close to 1, then the performance of ABC decreases dramatically. (3) Related to the second result, MABC and ImpABC algorithms outperform original ABC for CEC 2005 or small-size functions. However, they are significantly worse than the original ABC for high-dimensional functions. This is maybe the reason that superiority of MABC and ImpABC was shown on the low-dimensional functions by their developers[5,18]. To overcome this problem, it may be better to keep a fixed number of dimensions to be changed rather than increasing the number of dimensions to be changed[2]. (4) No significant impact of adjusting the perturbation on the performance improvement is observed. Therefore, to reduce the number of parameters, the default perturbation range, [−1, 1], can be used in the search equation. (5) ABC-e-gdbm (ABC-e-gdb with MR parameter) outperforms all algorithms. Therefore, it shows that the combination of the best performing strategies allows better results to be obtained. (6) ABC-e-gdb is giving best and robust results for both benchmark suites.In almost all ABC variants, the modified search equations for employed bees are used also for the onlooker bees step. There are few variants that propose modifications in the onlooker bees step only. In the component-based analysis conducted in this step, two modifications have been considered. One of them is the onlooker bee strategy of BABC (Eq. (11)) and the other is the onlooker bee step of EABC (Eq. (15)). They are called as ABC-o-bsf and ABC-o-enh shortly in this step of the experiments. In addition to them, the strategies that performed well in the analysis of the employed bees step were tested because they were proposed for the onlooker bees step as well. However, these strategies have been renamed as ABC-o-gdbm (which proposed in this study), ABC-o-gbd, ABC-o-imp, ABC-o-mod, ABC-o-gb to indicate that the modifications are tested now in the onlooker bees step.Table 6shows median error values of each onlooker bees strategy on 50 dimensional functions. When Table 6 is examined for CEC functions, it can be observed that although ABC-o-imp reaches lower median errors for 10 functions, it performs worse than the other approaches on 5 functions. On the other hand, ABC-o-gbdm performs better than the other proposed modifications on average and in most cases it achieves the best or second best results. The results are slightly different when 50-dimensional SOCO functions are considered. In this case, the overall best performing onlooker bee strategy becomes ABC-o-gbd. ABC-o-enh also gives good results, and it is better than all other strategies on two functions. The most noteworthy result is the poor performance of ABC-o-bsf for both benchmark sets. The main reason of the poor performance of ABC-o-bsf can be explained by the updating of the strategy of the equation 11, in which each dimension becomes closer to each other. This design choice is good for the functions when the optimum are at the center, but it fails for shifted and shifted-rotated functions as shown in [2].Results: (1) For the considered strategies, similar results are obtained whether they are used in employed bees or onlooker bees step on the same benchmark suite. Therefore, the obtained results for the analysis of the employed bees step are also valid in this step. (2) In a search equation, if all dimensions are replaced by a guidance of the same dimension as in ABC-o-bsf, then a decrease of the performance is observed except the basic (non-shifted and non-rotated) functions. (3) As in the previous step, ABC-o-gdb and ABC-o-gdbm are the best strategies and are selected for the proposed composite ABC algorithms for the SOCO and the CEC 2005 functions, respectively.In ABC, if a food source can not be improved for a predetermined number of times, which is controlled by the limit parameter, it is abandoned. After that, a scout bee discovers a new food source following some search strategy. The underlying idea of this step is to propose a strategy to avoid the algorithm to stagnate. The selected scout bee step strategies, which are based on using xGbestinformation (ABC-s-bsf and ABC-s-inc) or chaotic search procedures (ABC-s-cha1 and ABC-s-cha2) are the following:•ABC-s-def: The scout bee mechanism of the original ABC defined in Section 2.ABC-s-bsf: This approach is the scout bees strategy of BABC defined in Eq. (12).ABC-s-inc: The strategy defined in Incremental ABC (IABC) and explained in Eq. (18).ABC-s-cha1: This scout bees strategy contains a chaotic search defined in CABC.ABC-s-cha2: This approach is based on chaotic search procedure proposed in ERABC.Unfortunately, the scout bees step is called infrequently during an algorithm. Thus, it is hard to determine the behavior of each strategy proposed in the scout bee step. Therefore, a different procedure is followed to analyze these strategies than in the previous section. First, the effect of strategies on the stagnation behavior has been investigated for different kinds of functions. Two functions with different features were selected. The first one is f5socoand the other is f9cec. f5socohas a wide search space with bounds (−600, 600), and doesn’t have many local optima. On the contrary, f9cechas a narrow search space with bounds (−5, 5) and has many local optima. On the other hand, the original ABC algorithm can find optimum results for these functions but it may get trapped in local optima for some cases. Therefore, it is possible to determine convergence characteristics of these scout bees step strategies with RLD plots.For each strategy, ABC algorithms with the modified scout bees step were implemented. Then, to understand the appropriate frequency of applying the scout bee strategy, the algorithms were run with different limitF values in 0.1, 0.5, 1.0, 1.5, 2.0 (limitF=limit/(SN*D), where SN is the number of food sources and D is the problem dimension). The parameter tuning task wasn’t performed for this experimental work but ABC default parameter values are taken. The population size was set to 20 for all strategies. For ABC-s-bsf, wMin and wMax parameters are set to 0.3 and 0.7, respectively, and Rfactoris set to 10−1 for ABC-s-inc. These values are the default values proposed by their authors.Stagnation behaviors of the strategies on f5socoand f9cecare shown in Table 7. It is clearly seen that the ABC-s-bsf strategy is robust to the change of the limitF values and that it gives good results. On the contrary, the ABC-s-def strategy is not robust and it is hard to find an appropriate limitF value for a problem. ABC-s-imp also gives good results except limitF=0.1. When limitF is 0.1, xGbestis used in the early stage of the algorithm which causes the algorithm to be quickly get trapped on local minima.When the chaotic-based approaches are applied frequently, they seize the algorithm control because they spend many function evaluations when they are invoked. Therefore, it is not surprising that chaotic-based scout bees strategies, ABC-s-cha1 and ABC-s-cha2, help algorithms to escape local minima when they are invoked infrequently but the algorithm gets trapped in local optima when they are invoked frequently.Results:(1) To obtain good results regardless of an appropriate limit value, the search region of the scout bee should be adjusted adaptively during the execution like as in ABC-s-bsf. (2) Using Gbest in scout bees’ search equation may enhance the convergence behavior. (3) Chaotic search procedures can be used only if the limit value is large, otherwise they lead to performance degradation because of unnecessary budget consumption. (4) The ABC-s-bsf strategy gives promising results for any limit value for several types of functions. Therefore, this strategy is used in the proposed composite ABC algorithms.Artificial bee colony algorithms can be hybridized with local search procedures to enhance their search intensification. In this step of the experiments, two well-known and powerful local search routines that can be used in ABC algorithms have been taken into account. The first local search is Powell's direction set method [26] using Brent's technique [27] as the auxiliary line minimization algorithm. The second local search procedure is Lin-Yu Tseng's Mtsls1 [28]. The hybrid algorithms have been referred shortly as ABC+powell and ABC+mtsls, and the original ABC algorithm without local search as ABC-noLS.The hybridization strategy proposed in incremental ABC with local search (IABC-LS) [23,24] have been followed for the experiments. In IABC-LS, the global-best position is set as the initial position of the local search procedure and applied at each iteration. Local search is terminated after a number of local search iterations, LSItrmaxor once a very small difference between two solutions is detected, so-called FTol. It uses an adaptive step size and a parameter, MaxFailures, for fighting stagnation.To determine the effect of the local search procedures on the ABC algorithm, the hybrid algorithms have been run on large and small sizes of the CEC 2005 and SOCO functions. The results are plotted in Fig. 6. As seen in Fig. 6, ABC+mtsls achieves consistently good results on f2cec, f5cec, f2soco, f4soco, f12socoand f16soco) over all tested dimensions. On other five functions of CEC 2005 (f3cec, f4cec,f7cec, f12cecand f15cec) and three functions of SOCO (f8soco,f14socoand f18soco), ABC+mtsls showed also very good performance on low-dimensional problems. Powell's conjugate direction set method is not working well with ABC for SOCO functions but it enhances the performance of ABC for f2cec, f4cec, f5cec, f6cecand f7cec.The Wilcoxon matched-pairs signed-rank test on median results are conducted for the obtained results. The test shows a significant improvement of ABC+mtsls over ABC-noLS for 50-dimensional CEC 2005 benchmark functions (p=0.0274). Moreover, ABC+mtsls ranks first for all cases. Consequently, the hybridization of the ABC algorithm with Mtsls1 is useful in general.Results:(1) Hybridization of ABC with local search procedures enhances the performance of the algorithm. However, the appropriate local search procedure should be carefully selected as otherwise, it may lead to a decrease in the algorithm performance. (2) Mtsls1 local search works good with ABC algorithms. Therefore our composite ABC algorithms were hybridized with Mtsls1 local search procedure.Finally, there are few variants of ABC (IABC [23] and ABCDP [29]) proposing modifications in the population size. IABC increases the population size iteratively while ABCDP changes the size of the population dynamically. In this stage of the component-based analysis, the adaptation of the population size modifications into the original ABC is denoted by ABC-p-inc and ABC-p-dyn, respectively. In the original ABC, the population size remains fix, therefore the default population size strategy was called ABC-p-fix in this experiments.In Fig. 7, the performance of each variant on CEC 2005 and SOCO functions are shown in the parallel coordinate plots. For 10 dimensional CEC 2005 functions, both ABC-p-inc and ABC-p-dyn often perform better than the original ABC with fixed population size (ABC-p-fix) on f1cec-f15cec. However, these strategies behave similarly with ABC-p-fix for the most difficult composite problems, f16cec-f25cec. Furthermore, the performance similarity between ABC-p-inc and ABC-p-fix is observed for 50 dimensional CEC 2005 functions. The conducted statistical tests do not find any significant difference between the original approach and the population size modification strategies.For the SOCO benchmark suite, a significant difference is detected for large scale instances of SOCO functions, specifically for 500-dimensional functions (f11soco-f18soco). On the other hand, the convergence behaviour of the strategies were analyzed on SOCO benchmark suite. The complete results are presented in supplementary document and some of the results are shown in Fig. 8. The results indicates that ABC-p-inc converges to high quality solutions much quickly than other approaches.Results:(1) Incremental social learning mechanism (ABC-p-inc) can assist to increase algorithm performance specifically for large scale optimization problems. Therefore, the incremental social learning approach was used for our composite ABC algorithm designed for SOCO functions. (2) Dynamic population size in ABC (ABC-p-dyn) can not help to improve performance although it shows a significant performance improvement for the economic power dispatch problem[29]. Therefore, dynamic population size strategy should be adopted and redesigned related to the problem domain.The component-based analyses give us information about the impact the algorithm components have at each step of the algorithm. As a next stage of our study, we now are interesting whether this knowledge can be exploited to define, for a given benchmark suite, in a rather straightforward way new ABC algorithms that outperform the considered ABC variants. Two new ABC algorithms are proposed by combining the best performing components of ABC algorithm. The proposed composite ABC algorithms for the SOCO and CEC 2005 benchmark suites are referred to shortly as CompABCsocoand CompABCcec, respectively. The pseudo-code of these composite algorithms are shown in Algorithm 3 and 4.Algorithm 3Composite ABC Algorithm for SOCO benchmark suite (CompABCsoco)Opposition-based initialization with chaotic random generator (ABC-i-mix)whiletermination condition is not metdoEmployed Bees Step using Eq. (7) and (8) (ABC-e-gbd)Onlooker Bees Step using Eq. (7) and (8) (ABC-o-gbd)Scout Bees Step using Eq. (12) (ABC-s-bfs)Apply Mtsls1 Local Search (ABC+mstls1)Increase Population Size using eq. (16) (ABC-p-inc)end whileAlgorithm 4Composite ABC Algorithm for CEC 2005 benchmark suite (CompABCcec)Opposition-based initialization with chaotic random generator(ABC-mix)whiletermination condition is not metEmployed Bees Step applying Eq. (7) and (8) with MR probability (ABC-e-gbdm)Onlooker Bees Step applying Eq. (7) and (8) with MR probability (ABC-o-gbdm)Scout Bees Step using Eq. (12) (ABC-s-bfs)apply Mtsls1 Local Search (ABC+mstls1)end whileFor Algorithms 3 and 4, the same components are selected for the initialization and the scout bees steps. Moreover, the algorithms are hybridized with the Mtsls1 local search procedure. It indicates that the selected components may improve the algorithm performance individually on several benchmark functions. While initialization with chaotic maps and opposition based approach help the population to spread a bit better than other approaches in the search space, the scout bee mechanism of BABC (ABC-s-bsf) controls the balance between escaping local optima for earlier iterations and increasing exploitation for later iterations by decreasing the search range of the scout bee adaptively. For CompABCsoco, incremental population size with local search invocation enhances intensification and diversification balance of the algorithm on large-scale functions.For employed and onlooker bees strategies, it has been determined that the employed bees and the onlooker bees strategies of GbestDist-guided ABC (GDABC) (ABC-e-gbd and ABC-o-gbd) are giving good results for both benchmark suites. In addition to this, for CEC 2005 functions, the importance of changing more than one dimension in the search equation has been determined. It enables the algorithm to escape easier local optima. Therefore, the search equation of GDABC has been modified by allowing to modify more than one dimension in the search equation. The proposed search equations are called ABC-e-gbdm and ABC-o-gbdm for the employed bees and the onlooker bees steps, respectively. The component-based-analyses showed the superiority of the individual effect of the proposed search strategy over other components on CEC 2005 functions. Therefore, this proposed method is used in CompABCcecalgorithm.In summary, the composite algorithms proposed in this paper are composed of the best components determined by the component-based analyses. It is important to note here that the algorithm designed by a combination of all best individual components does not guarantee to result in the best possible algorithm overall. This is because there might be interactions between individual components so that the resulting algorithm may be worse than the sum of the potential improvements. However, it is difficult to study the interactions of all components in the literature. Hence, it is not expected that the proposed algorithms give the best possible combination of the available ABC algorithm components. Instead, it is expected that the composite algorithms designed by the component-based analysis are better than other ABC variants from which the components were taken.In this section, the overall performance of the composite ABC algorithms are analyzed. In the following sections, the proposed algorithms are first compared to several variants of the ABC algorithm on our two benchmark sets. Then, the proposed algorithms are compared to state-of-the-art algorithms.CompABCsocoand CompABCcecare analyzed on two different benchmark function sets. The proposed algorithms were compared with the original ABC and nine ABC variants that were examined in the component-based analysis and from which algorithmic components are taken. These algorithms are Gbest-guided ABC (GABC) [8,9], GbestDist-guided ABC (GDABC) [8], Incremental ABC with local search (IABC-LS) [23,24], Improved ABC (ImpABC) [17], Modified ABC (MABC) [5], Best-so-far selection ABC (BABC) [16], Enhanced ABC (EABC) [21], One Point Inheritance ABC (OPIABC) [13] and Chaotic ABC (CABC) [14]. Prior to each comparison, all algorithms were tuned by Iterated F-Race for a fair comparison. The tuned parameter values can be found in the supplementary pages [15].Fig. 9presents the comparison results on 100 and 500 dimensional SOCO functions. As shown in Fig. 9, CompABCsocoreaches optimum values for 14 functions in both cases. In addition, it provides better results than the other approaches for almost all functions. When the statistical comparison results listed in Table 8are examined, CompABCsocois significantly better than ABC, ImpABC, BABC and MABC. Furthermore, it significantly outperforms GDABC and EABC for 500 dimensional problems. This indicates the good scalability of the algorithm to high dimensional functions.The comparion results on the CEC 2005 functions for CompABCcecare shown in Table 9and 10. In Table 9 and 10, the algorithm performs significantly better than almost all compared algorithms for f2cec, f3cec, f5cec, f7cecand f9cec. Moreover, CompABCcecis better than other ABC variants on several benchmark functions, as can be seen in the data on win, loss, and draw at the bottom of Table 9 and 10. According to the Friedman test, CompABCcecrank first on CEC 2005 functions. More specifically, the performance of CompABCcecis significantly better than the original ABC, CABC, BABC, and IABC-LS on 30 dimensional CEC 2005 functions, moreover the Friedman test finds significant difference between CompABCcecand original ABC, CABC, BABC, IABC-LS and MABC on 50 dimensional CEC 2005 functions. Therefore, it can be said that CompABCcecis the best performing algorithm ABC algorithm in terms of overall performance.In this section, the Composite ABC algorithms are compared to several algorithms from the literature. First, the CompABCsocoalgorithm was compared to sixteen algorithms for the SOCO functions. 13 of these algorithms were published in the SOCO special issue and the other three algorithms (CHC, DE, and G-CMA-ES) are reference algorithms of the special issue. In this comparison, the median results of the algorithms are taken directly from the website of the special issue: http://sci2s.ugr.es/eamhco/#LSCOP-special-issue-SOCO. A comparison of the results on 100 and 500 dimensional functions is shown in Fig. 10and the detailed results can be seen in Tables in the supplementary document [15]. As can be seen from Fig. 10, CompABCsocooutperforms all algorithms except MOS-DE and IPSO-Powell. Both Wilcoxon and Friedman tests clearly indicate that the difference between the CompABCsocoand the other algorithms is statistically significant in most cases. Moreover, it is important to note that performance differences between the algorithm increase with dimensionality. Related to this end, an additional significant difference is detected between CompABCsocoand DE-D40-Mm on 500 dimensional SOCO functions.On the CEC 2005 functions, the CompABCcecalgorithm was compared to seven well-known and high performing variants of Differential evolution (DE) and particle swarm optimization (PSO); HDDE [41], JADE [42,43], SaDE [43,44], BBPSO [45], CLPSO [46], GL-25 [47], jDE[43,48]. The mean error objective values of these algorithms were taken directly from their original papers because they have been run under the same conditions. The performance results are given in Table 11. For unimodal functions (f1cec-f5cec), CompABCcecis significantly better than all algorithms on f3cecand f5cecspecifically on 50 dimensional functions. For non-hybrid multimodal functions (f6cec-f12cec), CompABCcecoutperforms other algorithms on f7cec, f8cec, f9cec, and f11cec. For the rest of the functions (f13cec-f25cec), which are hybrid and much harder than the others, the solution the quality of the proposed algorithm decreases. Nevertheless, in most cases, CompABCcecis better than several algorithms on f13cec, f15cec, and f21cec. When the Friedman test was used in the experimental study, no algorithm showed a significant improvement over CompABCcec. Therefore, it is possible to say that CompABCcecperforms competitively with the state-of-the-art algorithms.

@&#CONCLUSIONS@&#
