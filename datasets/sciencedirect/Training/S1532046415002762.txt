@&#MAIN-TITLE@&#
An evaluation of patients’ experienced usability of a diabetes mHealth system using a multi-method approach

@&#HIGHLIGHTS@&#
Multi-method, structured and standardized approaches are needed in mHealth usability research.A novel application of the FA method and UPT was used to structure, code and classify problems.Clearly defined, classified usability problems proved the feasibility of the combined methods.

@&#KEYPHRASES@&#
Usability,mHealth,Diabetes,Multi-method evaluation,Framework analysis,Usability Problem Taxonomy,

@&#ABSTRACT@&#
ObjectivemHealth systems are becoming more common to aid patients in their diabetes self-management, but recent studies indicate a need for thorough evaluation of patients’ experienced usability. Current evaluations lack a multi-method design for data collection and structured methods for data analyses. The purpose of this study was to provide a feasibility test of a multi-method approach for both data collection and data analyses for patients’ experienced usability of a mHealth system for diabetes type 2 self-management.Materials and methodsA random sample of 10 users was selected from a larger clinical trial. Data collection methods included user testing with eight representative tasks and Think Aloud protocol, a semi-structured interview and a questionnaire on patients’ experiences using the system. The Framework Analysis (FA) method and Usability Problem Taxonomy (UPT) were used to structure, code and analyze the results. A usability severity rating was assigned after classification.ResultsThe combined methods resulted in a total of 117 problems condensed into 19 usability issues with an average severity rating of 2.47 or serious. The usability test detected 50% of the initial usability problems, followed by the post-interview at 29%. The usability test found 18 of 19 consolidated usability problems while the questionnaire uncovered one unique issue. Patients experienced most usability problems (8) in the Glucose Readings View when performing complex tasks such as adding, deleting, and exporting glucose measurements. The severity ratings were the highest for the Glucose Diary View, Glucose Readings View, and Blood Pressure View with an average severity rating of 3 (serious). Most of the issues were classified under the artifact component of the UPT and primary categories of Visualness (7) and Manipulation (6). In the UPT task component, most issues were in the primary category Task-mapping (12).ConclusionsMultiple data collection methods yielded a more comprehensive set of usability issues. Usability testing uncovered the largest volume of usability issues, followed by interviewing and then the questionnaire. The interview did not surface any unique consolidated usability issues while the questionnaire surfaced one. The FA and UPT were valuable in structuring and classifying problems. The resulting descriptions serve as a communication tool in problem solving and programming. We recommend the usage of multiple methods in data collection and employing the FA and UPT in data analyses for future usability testing.

@&#INTRODUCTION@&#
Diabetes is a condition affecting 29.1million people in the United States [1] with concomitant health care expenditures of an estimated $198billion [2]. The most common form of diabetes is Type 2 (T2DM) affecting a majority, 90%, of those with the condition [3]. A healthy diet, regular physical exercise, and maintaining a normal body weight in addition to medication treatment are seen as important [3] whereas poorly regulated glycemic measurements and poor self-management practices are contributors to a worsening condition [4,5]. Due to its complexity, Type 2 diabetes puts heavy demands on both patients and providers [6].Different types of support systems for diabetes self-management have been developed recently using Information and Communication Technology (ICT) [7,8]. Studies show that mobile health (mHealth) technology has been particularly successful in improving glucose management for these patients [9–11] and has aided in regulating lifestyle changes [11]. Recent interventions having beneficial effects are, for example, those focusing on self-coaching by allowing users to interact with algorithm-based systems that provide individually tailored messages [12] and feedback messages with automatically updated text messages based on clinical guidelines and patients’ lifestyles [13].Although mHealth applications have been beneficial in diabetes self-management, researchers mention the need for improved usability to allow patients to feel confident in their interactions with mHealth applications [14]. Unfortunately, relatively few studies exist on mHealth usability [15]. Of the available studies, most lack methodological rigor [16,17] and use a single method for data collection [17,18]. Also, qualitative data analyses in mHealth studies typically lack a structured approach, making study reproducibility difficult, especially when using multi-method approaches for large data sets with multiple researchers. No usability studies are yet available using the Framework Analysis (FA) method or the Usability Problem Taxonomy (UPT) to address this gap. Our approach acknowledges these current limitations and tests the feasibility of a multi-method approach for data collection and novel, structured techniques for data analyses.Methodological improvements in mHealth usability studies could result in more comprehensive identification of usability issues, more specific redesign recommendations based upon user-centered data, more reliable data analyses and they could potentially improve the reproducibility of results across studies. For users, improved mHealth designs could result in improved interactions, user performance, adoption of mHealth applications and perhaps even increased adherence to suggested interventions for chronic care conditions.Recent reviews of mHealth usability studies point to the need for more rigorous studies and the use of multiple methods to help validate findings [16,17]. Authors commented that each usability testing method makes its own unique contribution to the overall identification of usability issues [17]. Thus, a robust usability evaluation should employ a combination of methods [16,17]. Specifically, Zapata et al. emphasized the need for (1) employing two or more different types of usability methods, (2) using more standardized methods and tools in mHealth evaluations and (3) using open-ended inquiry methods and qualitative analysis to identify more comprehensive user concerns and recommended that future researchers conducting usability evaluations need to standardize processes and use a combination of more than one method [17,18].Multiple methods in data collection can increase the depth of inquiry while improving the reliability and validity of findings [19]. The use of multiple methods also assists with methodological triangulation because it can allow for a more comprehensive understanding of a phenomenon [20,21]. This type of triangulation was first identified by Patton who advocated data collection using observation, field notes and interviews [20,22]. These kinds of qualitative methods produce rich data [20,23] and are especially suitable in health informatics research [24]. Pertinent to mHealth, multiple methods in data collection could allow for a more comprehensive understanding of the mHealth user experience and pinpoint specific usability problems for redesign.In the current study, we test the feasibility of overcoming common methodological issues in data collection during a mHealth usability study. We completed a multi-method approach to data collection with user-based testing, Think Aloud protocol, open-ended interviews and a short answer questionnaire.The main shortcomings of qualitative research methods are a lack of standardization in data analytic techniques, meaning that replicating the results of analyses can be difficult [25]. Two methods can assist in overcoming this issue, the Framework Analysis (FA) method and the Usability Problem Taxonomy (UPT) classification scheme. Neither has yet been employed in mHealth or other health informatics usability evaluations to our knowledge.The FA, originally created by Richie and Spencer in 1994, is a standardized yet somewhat flexible framework for analyzing qualitative data. Its purpose is to assist with the analysis of descriptive, textual source data to produce reliable and valid qualitative results [26,27]. Importantly, the FA is seen as scientifically robust [28]; it has been used in social sciences research and to a lesser extent in health care research [28,29]. Recent applications of the FA have been mainly in health care (nursing research) and in multidisciplinary studies to manage large sets of qualitative data [28,29].The FA consists of five stages (1) familiarizing oneself with the data, (2) identifying a thematic framework to be used, (3) indexing and applying the framework to the data, (4) charting the data and (5) mapping and interpretation [26]. This analysis method is especially useful in organizing, reducing, and interpreting data because it provides clear steps to follow and it produces more structured output with summarized data [29]. Moreover, the process can be used by several researchers simultaneously [28] such as multi-disciplinary team members [29]. The FA promotes, in particular, data/decision transparency [28]. It is useful for both experienced and less experienced coders due to its available audit trail [28]. The framework can accommodate different analytic tools such as paper, post-it notes, Microsoft Word, or NVIVO [28]. Primary difficulties with this method are those usually inherent to qualitative analysis e.g., it is time consuming and requires a committed manner of analysis [29].Finding ways to structure and classify usability problems has been explored within the Human–Computer Interaction (HCI) field. Keenan et al. [30] found no framework for classifying usability problems on an individual problem level, particularly using various perspectives that would allow problems to be compared, analyzed and described. Such an analysis would provide better information for problem correction [30]. These authors thought a new approach would either have a trained individual examine the whole set of usability problems to look for trade-offs, contradictions, and consistency issues [31], or an expert might think about the problems from a more global perspective to look for problem clusters [32]. Keenan et al. considered using heuristic analysis [33,34], but found that technique lacked sufficient problem distinguishability, mutual exclusiveness and specificity [30,35]. Thus, the UPT was built empirically using over 400 usability problem descriptions collected on real-world development projects [30].UPT is a classification scheme and framework for characterizing usability problems according to their dimensions, providing a clear structure in usability problem definition. It was initially used to classify usability problems found on graphical user interfaces with textual components where usability problems were easily detected, classified, and analyzed [30]. The UPT is based on the notion that usability problems should be examined from two perspectives: the task-artifact approach proposed by Carroll et al. [36] and the Object-Action Interface Model by Shneidermann [37] to enhance problem definition. The artifact component defines usability problems arising when the user interacts with the interface while categories under task component focus on usability problems that surface when a user moves through a task [30]. Problem classification occurs from both an artifact and a task component perspective and is then divided into five primary categories: Visualness, Language and Manipulation, Task-mapping and Task-facilitation (see Fig. 1).The UPT classification could also aid developers in several ways (1) the categories are based on problem characteristics versus only problem type, meaning classifications occur on two levels (artifact and task), (2) UPT offers a method of identifying problem clusters using different ways and levels of categorizing problems, (3) it is beneficial for examining problem sets at varying levels of abstraction, and (4) problems distributed across UPT categories make visible the kinds of issues encountered most often. It can thus be used by developers to assess problem scope and frequency as well as defining the types of problems that both global and local solutions may be considered. The UPT has been used only sparingly since its creation, but its classification methods could provide an excellent description of the dimensions in detected usability problems. We concluded the UPT is a lesser known but robust technique.Our purpose was to test the feasibility of (1) using a multi-method approach for data collection during a mHealth usability evaluation and (2) applying structured approaches to data analyses by using the FA method and UPT. These comprise a novel, multi-method approach to data analyses and, to our knowledge, are the first applications of both FA and UPT in health informatics research.The system evaluated in this study is an interactive, SMS-based mobile intervention for patients with diabetes, designed as a personalized self-care management tool. It is a commercially available tool in current use in several organizations. The system consists of a combined mobile phone solution and web service. Patients interact by either sending in or being prompted to send in their Type 2 diabetes, self-management values, e.g., morning blood glucose via text message. Using the web service, they can enter and/or review their results across various parameters. Patients can, for example, obtain their glucose readings or blood pressure values visualized in a meter format, see medication adherence levels, exercise and weight progress and view scheduled medical appointments. When sending in their measurements patients receive personalized coaching and tailored responses from the system to track the progress of their disease. The web portal contains different features, main views and sub-views for patients to perform different actions and track results (see Fig. 2). In this study we concentrated our evaluation mainly on the web service part of the solution because of its inherent complexity and because it had no previous usability evaluation.Institutional Review Board approval was obtained (WIRB, Olympia, Washington, USA). A larger randomized controlled trial for a diabetes mHealth intervention study involving 18 primary care clinics in the Salt Lake City metropolitan area served as the study population for our usability study. A set of 2317 patients met the inclusion criteria for the larger study [38]. Ten patients were randomly selected from this larger randomized controlled trial on the mHealth intervention by computer randomization and invited to take part in our usability evaluation. Inclusion criteria for our usability study sample included (1) patients diagnosed with Type 2 diabetes (2) no cognitive impairment; (3) familiarity and some knowledge and use of computers, the Internet, and cell phone; and (4) the ability to speak and understand the English language. Patients had no previous exposure to the mHealth web system evaluated in our study. The evaluation sessions were conducted in a quiet laboratory setting at HealthInsight, a U.S. Beacon Community in Salt Lake City, Utah.Usability can be evaluated by several different methods. Inspection methods such as heuristic evaluation [39,40], and cognitive-walk through [41,42] are expert methods meaning that experts go through the system to identify usability issues. Think Aloud protocol [43,44] is a user-related method for evaluating usability where users express their perceptions out loud as they interact with the system. Other user-related methods include administering in-depth interviews and questionnaires about patients’ experiences with the system [24]. The main goal of user-related methods is to involve actual users in the evaluation and obtain their perspectives [45], to gain insight into how the intervention needs to be adapted for different users’ abilities and experience levels [46,47] and to identify usability problems for correction.Think Aloud is a usability assessment method commonly employed to determine users’ thoughts and opinions while they perform a list of specified tasks with a system. The method originated in 1984 in psychology when Simon and Ericsson thought of verbal reports as data. Revised in 1993 [48], the technique is well established within the Human Factors field [39]. Think Aloud asks users to talk aloud during their interactions, to express their reactions and thinking and to explain what they are doing as they perform specific, representative tasks [24]. The resulting data are normally audio- and/or video-recorded and/or an observer takes thorough, written notes [49]. Minimal intervention from the usability tester assures users’ thought processes are not interrupted except to remind them to keep talking [49]. The focus is on understanding users’ decision making processes and on how users experience the system in their own words [39,49]. Because the method provides extensive, detailed data, only a small sample of five to eight users is needed in usability testing to detect 80–85% of usability problems [50–52] to gain a thorough understanding of task behavior [48] and to identify the main usability problems [53]. Representative tasks for the specific domain are also essential, and they should be as realistic as possible [54]. Also, testing should be conducted in the actual user’s context or one as close to the natural environment as possible [49,55]. Authors indicate that Think Aloud provides complete and detailed descriptions of patients’ thought processes during system interactions and the technique generates many usability problems [56].Tasks for this study were based on common patient user interactions with the system; they were disease-specific and had varying levels of difficulty to simulate patient usage in a clinic or at home. Tasks were validated by a panel consisting of a physician and a nurse whose specialties were diabetes, a public health professional with chronic patient intervention systems expertise, and a diabetes patient. The specific tasks patients had to perform consisted of (1) uploading glucose values into the system, (2) interpreting a glucose measurement in a graph view, (3) correcting a recorded glucose measurement value, (4) exporting glucose measurement value trends to a PDF to simulate material to take to a provider visit, (5) interpreting a blood pressure measurement in a graph view, (6) setting personal tracking goals for exercise and weight, (7) setting medication reminders, and (8) setting a physician appointment reminder.Both interviews and questionnaires have been used extensively in usability research to determine users’ opinions about the difficulties they experience in an evaluated system [49]. In our study patients completed both an open-ended interview and a post-experience questionnaire [57]. The open-ended interview guide asked users to talk about aspects of the system with good or poor usability. This kind of interview is especially useful in uncovering comprehensive information from participants [58]. The three questions in the interview asked patients to comment on sections of the system they thought were well designed, sections that were inadequately designed and any further comments they might have about system usability (see Appendix A).Patients also completed a post-interaction questionnaire on the mHealth system (see Appendix B). The first section consisted of short answer questions about: patients’ IT/computer, mobile phone, and internet experience and use; their experience and perceptions about web and mobile service systems in health care; what they thought about the specific mHealth system evaluated in the session in terms of usability and any further comments they might have about these topics. Patients were also asked to rate their preferences in technology usage for work and leisure time using a Likert scale with 4 points ranging from strongly agree (1) to strongly disagree (4).The second part of the post-questionnaire included open-ended questions about the specific system patients used in this study. Patients specified in writing their thoughts about what they found difficult and easy about the system and its navigation, they listed usability/user experiences they found satisfying or dissatisfying and then a final question asked for any further comments about the system.Both the interview guide and post-experience questionnaire were assessed for face validity by a panel of three health care professionals, three usability experts and one patient. The resulting format was finalized via discussion and consensus.First, patients were asked for informed consent. Then, patients were walked through the different steps of the evaluation procedure and asked if they had any questions. The evaluation started with patients filling out a brief demographic questionnaire. Topics included age, gender, educational level, occupation, and how long they had been diagnosed with diabetes.Next, standardized training was performed to simulate an actual patient educational process in a health clinic. This was important to decrease individual variability and to ensure that patients all had the same information about the system [59] because none had used the system before. After the training was completed by the first author, users interacted with the system on their own to get familiar with it for an average of about 10min.The Think Aloud and usability evaluation session was conducted by the first author. Patients were given a booklet outlining the specific tasks to perform. During the session patients were asked to think aloud as they completed the prescribed tasks in the system. If they became silent, the researcher encouraged them by asking what they were thinking or by clarifying actions, but otherwise any other interference with patients’ thought processes was avoided. Interactions were digitally audio-and video recorded using Morae™ software [60]. The recording showed patients’ navigation on screens, their facial expressions and captured their voices. The researcher made observations and notations about the individual task performances directly into Morae™.Afterwards, patients were interviewed about their experiences and perceptions about the system, by the first author, using the in-depth interview guide with interview topics. Patients were provided the opportunity to express freely what they thought was easy and difficult with the system and add any further comments. This session was also digitally audio-recorded and lasted for 15–20min. Finally, patients completed the post-test questionnaire on their perceptions about the usability of the system.The complete testing procedure for all the steps averaged approximately two hours with a range of 1.5–2h. Patients received a gift card for $20 after completing the session.The audio-and video recordings from the sessions, Think Aloud comments and observations, post-test interview and post-test questionnaire usability data were transcribed, checked for accuracy and imported into Nvivo™ 10 Qualitative Data Analysis Software [61]. Data analyses included content analysis in addition to applying the FA and UPT.Coding and analysis included the five steps in the FA method [26] (1) data familiarization, (2) identifying themes and framework used and (3) classifying usability problems using the UPT, (4) the results were organized or charted into the place of occurrence within the mHealth application and then (5) mapped and interpreted. Descriptive statistics were used to summarize data.The first author transcribed, imported the data and completed the initial coding which were verified by the second author. Both authors conducted step 2–5 in the analysis together.See Fig. 3and the accompanying description of the analysis process below.(1)Familiarization with the dataThe first step was to become familiar with the transcribed textual data through immersion. This occurred by reading the uploaded textual documents several times.(2)Identifying the themes and/or framework to be usedThe second step was to identify themes and apply a framework to code the key issues. In the original FA method, researchers can select either an inductive or deductive content analysis approach to perform the coding. We modified the FA method slightly. We used FA to generate usability problems inductively across the various data collection methods, but we used the UPT deductively as well to classify the problems (see Fig. 3). The usability problems can be considered themes derived from the data. After a discussion and removal of duplicates, we consolidated the usability issues under their appropriate tasks. The application of the UPT is described in more detail below, and we provide examples of two coded usability problems in Table 1.(3)Indexing and applying the classification to the dataThe third step consisted in indexing and applying the UPT classification framework as well as assigning severity ratings to our final usability problem list. We classified each usability problem into an artifact component (Visualness, Language and Manipulation) and/or a task component (Task-mapping and Task-facilitation) proceeding in the classification scheme as far as possible. The categories and subcategories within the artifact and task components at any level are mutually exclusive resulting in one final categorization [30]. An example of a classified usability problem from our data is shown in Table 2.For the severity rating, we used a process defined by Travis that asks three questions about each usability problem (1) Does the problem occur on a red route; i.e., is it a frequent or critical task the system needs to support? (2) Is the problem difficult for users to overcome? and (3) Is the problem persistent and does it keep recurring [62]? Ratings were assigned using the scale (1) low, (2) medium, (3) serious or (4) critical for each problem [62]. The resulting severity scores were averaged per system view and for the whole system.(4)Charting the dataThis step consisted of abstracting our final list of usability problems back into their original context as is consistent with the FA method. We arranged the order according to the problem’s place of occurrence, classification and severity level. Descriptive statistics were used to summarize issues within each method and per patient.(5)Mapping and interpretation of the dataThe final step involved mapping and interpreting the usability problems. After the resulting list of problems were charted, we were able to identify the most prevalent problems and their severity ratings in their respective views. By doing this, we could determine the nature of the problems and what their classifications implied. The latter served as guidance for designers in correcting specific usability issues.

@&#CONCLUSIONS@&#
Recent systematic reviews of mHealth self-management tools in general and for diabetes in particular speak to the need for more studies on patient interaction and system usability. Finding standardized, structured and reproducible ways to work in usability evaluation is important for providing evidence. This study provides an example of a multi-method design for both data collection and data analyses. Multiple data collection methods resulted in a more comprehensive set of usability problems and helped triangulate data. The structured data analyses allowed reproducible steps and data validation (triangulation), a method of determining the most severe problems for users.Usability testing with Think Aloud was essential for surfacing usability issues. The in-depth interview and questionnaire allowed data triangulation for severe usability issues, but both uncovered a smaller volume of consolidated usability issues. For data analyses, the more structured method, using the FA, provided a more standardized and feasible way to derive usability problems from a large volume of qualitative data. The FA method can guide analyses across multiple researchers. The UPT was advantageous as an in-depth classification scheme and for determining severity ratings for usability problems. It also assisted in categorizing specific types of problems which could be useful for designers. We recommend the use of multiple data collection methods to uncover a variety of problem types and severity levels. We also recommend the use of the FA and UPT methods during data analyses.The authors declare that there are no conflicts of interest.Start-up topics/questions for in-depths interviews•What parts of the system did you think were well designed (?)Which parts of the system did you think were inadequately designed (?)Do you have any other comments about the system functions and regarding its usability (?)Usability problem descriptions and classifications (complete list)Usability problem descriptionPlace of occurrence% of pat. detecting per methodUPT ClassificationSeverityTAaIaQaArtifactTask(1) Difficulty knowing to exit the table view and click the “Add data” button to be able to add in a new valueGlucose Readings View7010050Manipulation-Cognitive aspects-Visual cues (FC)Task mapping-Navigation (FC)4(2) Difficult to understand and perform the exporting actionGlucose Readings View304040Manipulation-Cognitive aspects-Visual cues (FC)Task mapping-Interaction (FC)3(3) No support to specify where the exported file is to be savedGlucose Readings View20––Visualness-Non message feedback (FC)Task mapping-Functionality (FC)3(4) Difficult to know how to navigate within and adjust the table view to get to or show the right value or value rangeGlucose Readings View10––Visualness-Presentation of Information/results (FC)Task mapping-Navigation (FC)3(5) No message about the lack of capability to save an exported fileGlucose Readings View10––Visualness-Non-message feedback (FC)Task facilitation-Keeping the user on track (FC)3(6) Difficult to know to go to the “Export” tab to view, export, and/or print the data table or its valuesGlucose Readings View503030Manipulation-Cognitive aspects-Direct manipulation (FC)Task facilitation-Keeping the user task on track (FC)2(7) Difficult to know to choose the Action-tab for deleting a valueGlucose Readings View50––Manipulation-Cognitive aspects-Visual cues (FC)Task mapping-Navigation (FC)2(8) Difficult to find the Export button for the specific export commandGlucose Readings View20––Visualness-Object appearance (FC)(NC)2(9) Difficult to know to select “Delete data” for the table view to change, delete, export, or print the value listGlucose Diary View70100100Language-Naming/labeling (FC)(NC)4(10) Difficult to know how to adjust the range of values that are to be shown, retrievedGlucose Diary View90––Manipulation-Cognitive aspects-Visual cues (FC)Task mapping-Functionality (FC)3(11) Incorrect rendering of graph values for the last 30 or 90 days as it lacks the correct delimitersGlucose Diary View20––Visualness-Presentation of Information/results (FC)(NC)2(12) Difficult to find, access “Medication Reminder” as four different paths existed with similar but different namesDashboard4020–Language – Naming/labeling (FC)Task-mapping-Navigation (FC)2(13) Difficult to find, access “Appointment reminder” as it was listed under a pane with a different nameDashboard30–10Language-Naming/labeling (FC)Task-mapping-Navigation (FC)2(14) Difficult to find the “Tracking goal” (exercise, weight) item as three different paths existed with similar but different namesDashboard40––Language-Naming/labeling (FC)Task-mapping-Navigation (FC)2(15) The blood pressure graph shows only the systolic blood pressure value; the diastolic blood pressure is missingBlood Pressure View1020–Visualness-Presentation of Information/results (FC)(NC)3(16) Weights lack conversion to the metric systemSubmit Weight Progress View1010–(NC)Task-mapping-Functionality (FC)1(17) Difficult to detect and distinguish the Update and Submit buttonsSet Goals View––10Visualness – Object appearance (FC)(NC)2(18) Times are only available for whole hours, not minutes or half hoursSet Medication Reminders View1010–Manipulation (PC)Task-mapping-Functionality (FC)2(19) Only dates and not times are available for appointment remindersSet Appointment Reminders View201010(NC)Task-mapping-Functionality (FC)2aTA=Think Aloud usability test, I=Interview, Q=Questionnaire.