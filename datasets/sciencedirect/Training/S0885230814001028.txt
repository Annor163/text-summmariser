@&#MAIN-TITLE@&#
A generalised alignment template formalism and its application to the inference of shallow-transfer machine translation rules from scarce bilingual corpora

@&#HIGHLIGHTS@&#
New approach to infer shallow-transfer rules from scarce parallel corpora.New rule formalism permits strong generalisation over the parallel corpus.First approach in which rule learning is rewritten as a global minimisation problem.Translation quality improves over previous approach with a smaller number of rules.Translation quality outperforms hand-coded rules for some language pairs.

@&#KEYPHRASES@&#
Machine translation,Transfer rule inference,Hybrid machine translation,

@&#ABSTRACT@&#
Statistical and rule-based methods are complementary approaches to machine translation (MT) that have different strengths and weaknesses. This complementarity has, over the last few years, resulted in the consolidation of a growing interest in hybrid systems that combine both data-driven and linguistic approaches. In this paper, we address the situation in which the amount of bilingual resources that is available for a particular language pair is not sufficiently large to train a competitive statistical MT system, but the cost and slow development cycles of rule-based MT systems cannot be afforded either. In this context, we formalise a new method that uses scarce parallel corpora to automatically infer a set of shallow-transfer rules to be integrated into a rule-based MT system, thus avoiding the need for human experts to handcraft these rules.Our work is based on the alignment template approach to phrase-based statistical MT, but the definition of the alignment template is extended to encompass different generalisation levels. It is also greatly inspired by the work of Sánchez-Martínez and Forcada (2009) in which alignment templates were also considered for shallow-transfer rule inference. However, our approach overcomes many relevant limitations of that work, principally those related to the inability to find the correct generalisation level for the alignment templates, and to select the subset of alignment templates that ensures an adequate segmentation of the input sentences by the rules eventually obtained. Unlike previous approaches in literature, our formalism does not require linguistic knowledge about the languages involved in the translation. Moreover, it is the first time that conflicts between rules are resolved by choosing the most appropriate ones according to a global minimisation function rather than proceeding in a pairwise greedy fashion.Experiments conducted using five different language pairs with the free/open-source rule-based MT platform Apertium show that translation quality significantly improves when compared to the method proposed by Sánchez-Martínez and Forcada (2009), and is close to that obtained using handcrafted rules. For some language pairs, our approach is even able to outperform them. Moreover, the resulting number of rules is considerably smaller, which eases human revision and maintenance.

@&#INTRODUCTION@&#
Machine translation (MT) can be defined as the process carried out by a computer to translate a text in a natural language, the source language (SL), into another language, the target language (TL). According to the kind of knowledge used in their development, MT systems may be said to be corpus based or rule based, although hybrid approaches (Thurmair, 2009; Costa-Jussà and Farrús., 2014) are also possible.On the one hand, corpus-based approaches use large collections of parallel texts as the source of knowledge. A parallel text is a text that is placed alongside its translation into another language; a collection of parallel texts is usually referred to as a parallel corpus. Statistical machine translation (SMT) (Koehn, 2010) is currently the leading paradigm in corpus-based MT. SMT systems can be built with little human effort, provided that a parallel corpus of sufficient size, of the order of tens of millions of words in each language, is available (Och, 2005).Rule-based MT (RBMT) systems (Hutchins and Somers, 1992) meanwhile use linguistic resources, such as morphological dictionaries, bilingual dictionaries and structural transfer rules,11Structural transfer rules define the transformations needed to convert SL groups of words into their TL counterparts.to describe the translation process. Building an RBMT system usually implies a considerable investment in the development of these resources, some of which can only be developed by trained experts. As the availability of a parallel corpus is not necessary for RBMT systems, the RBMT approach is the approach of choice when building MT systems for the translation between under-resourced language pairs (e.g. Breton–French, Icelandic–English, Kazakh–Tatar) for which large parallel corpora are not readily available.RBMT systems usually work by analysing—at the morphological, syntactic or semantic level—the SL text in order to build an intermediate representation (IR), which is the basis for the generation of the translation into the TL. Depending on the nature of this IR, RBMT systems can be said to be interlingua based or transfer based. Interlingua-based RBMT systems use language-independent IR; this makes analysis and generation difficult—and almost impossible for broad domains—but avoids the need for transfer. Transfer-based RBMT systems use language-dependent IRs and include a transfer stage which transforms the SL IR into a TL IR by applying lexical and structural transfer rules. Since they are language-dependent, the IRs used in transfer-based RBMT are much easier to develop than those used by interlingua-based systems, thus making transfer-based RBMT the leading approach in RBMT.Transfer-based RBMT systems can in turn be classified according to the complexity of the IR used into shallow-transfer, syntactic-transfer, and semantic-transfer RBMT systems. In this paper, we focus on shallow-transfer RBMT systems, which are those that perform a shallow-syntactic analysis of the SL, i.e. they do not perform full syntactic parsing and do not build a parse tree. This signifies that the IR they use is as simple as a sequence of lexical forms (lemma, lexical category and morphological inflection information) of the words to be translated; transfer rules usually split this sequence into chunks (groups) of lexical forms whose elements are processed together.Shallow-transfer RBMT systems use bilingual dictionaries for lexical transfer, and shallow-transfer rules for structural transfer. These rules match chunks of lexical forms in the SL and produce TL lexical forms as their output. They operate on the lexical forms they have matched, regardless of the SL lexical forms matched by other rules, often with no interaction between the rules. The Apertium shallow-transfer MT engine (Forcada et al., 2011) has recently been used for the development of several language pairs, such as Breton–French (Tyers, 2010), Italian–Catalan (Toral et al., 2011) or Icelandic–English (Brandt et al., 2011), to name but a few.This paper presents a new approach with which to automatically learn shallow-transfer MT rules from small parallel corpora,22Barely a few hundreds or thousands of sentences: a small size compared to the amount of parallel corpora required to train competitive SMT systems.which, inspired by the work by Sánchez-Martínez and Forcada (2009), uses alignment templates (AT),33Alignment templates are a generalised version of the phrase pairs used in phrase-based SMT (Koehn, 2010) which use word classes rather than words and also include word alignment information.like those initially used in SMT (Och and Ney, 2004), and overcomes the main limitations of their method (see Section 2.1 for a thorough description of these limitations): (i) its conservative approach which prevents the appropriate generalisation level for the ATs from which rules are generated from being found; (ii) as a particular case of the latter, its inability to perform context-dependent lexicalisations to give a different treatment to those words that are incorrectly translated by more general ATs; and (iii) the deficient selection of the ATs to eventually be used for the generation of rules which finally results in rules that prevent the application of other, more convenient rules. The inferred rules are compatible with the formalism used by Apertium (Forcada et al., 2011) to code shallow-transfer rules, may be modified (post-edited) by human experts and can co-exist with hand-written rules.The remainder of the paper is organised as follows. The following section presents a brief description of the approach by Sánchez-Martínez and Forcada (2009), stressing its main limitations and summarising how they are overcome in our approach. Sections 3 and 4, respectively, introduce the AT formalism used in our approach and the method employed to extract ATs using this formalism. Section 5 then provides a review of the related approaches found in literature. The experiments conducted to test our approach are presented in Section 6, whereas the results obtained are reported and discussed in Section 7. The paper ends with some concluding remarks, two appendices containing low-level details to ensure the reproducibility of the experiments and a table of acronyms for the benefit of the reader.The approach by Sánchez-Martínez and Forcada (2009) is based on the alignment template (AT) approach (Och, 2002; Och and Ney, 2004) initially proposed in the context of SMT. An AT performs a generalisation of bilingual phrase44In order to use the same terminology as that used in SMT (Koehn, 2010) we refer to segments as phrases, despite the fact that they are not necessarily syntactic constituents.pairs (pairs of segments which are mutual translations) by using word classes rather than the words themselves. Sánchez-Martínez and Forcada (2009) adapted the AT approach for their application in RBMT by extending the ATs with a set of restrictions in order to control their application as shallow-transfer rules. An extended AT (henceforth, EAT) is defined as a tuple z=(S, T, A, R), consisting of a sequence S of SL word classes, the corresponding sequence T of TL word classes, a set A of pairs of word class indexes (i, j) with the alignment information between the word classes in the two sequences, and a set R of restrictions over the TL inflection information that the words to translate need to meet. These restrictions prevent, for example, an AT producing a TL masculine noun from an SL feminine noun from being applied to an SL noun whose translation, according to the bilingual dictionary of the system, does not have a feminine gender.The method by Sánchez-Martínez and Forcada (2009) needs a human-designed set of lexicalised units. This set is made up of both the SL and TL lexical forms (usually corresponding to closed lexical categories) involved in lexical changes and which should not be generalised. EATs are then learnt from a parallel corpus by using the following steps:1.Analyse and convert both sides of the parallel corpus into the IR used by the RBMT system to be used; in the case of Apertium, sequences of lexical forms.Apply classical statistical, word-translation models (Brown et al., 1993; Vogel et al., 1996) in order to obtain word alignments in both translation directions, and then symmetrise the alignments obtained using the refined intersection method proposed by Och and Ney (2003).Extract bilingual phrase pairs that are compatible with the set of alignments (Koehn, 2010, Sec. 5.2.3).Remove those bilingual phrase pairs that cannot be reproduced by the RBMT system in which the transfer rules will be used because, according to the bilingual dictionary, the translation equivalent of at least one lexical form not present in the set of lexicalised units differs from that observed in the bilingual phrase.Replace lexical forms with word classes. Word classes represent the lexical category and morphological inflection information of the corresponding words. They are obtained by removing the lemma from each lexical form that is not present in the set of lexicalised units provided by the user.Infer the set of restrictions R by looking up in the bilingual dictionary the lexical forms that do not belong to the set of lexicalised units.The resulting set of EATs is then used to generate structural shallow-transfer rules after removing those EATs whose frequency is below a threshold that is empirically determined on a development parallel corpus.During translation, the actions that need to be performed in order to build each TL lexical form depend on the type of word class:•if the TL word class includes a lemma (e.g. dePR, the Spanish preposition de), because the corresponding lexical form belongs to the set of lexicalised units, it is introduced unchanged.if the TL word class does not include a lemma (e.g. N-gen:m.num:sg; noun, masculine, singular), the lemma to be included in the TL lexical form is obtained by looking up the SL lexical form that is matched by the SL word class to which the TL word class is aligned in the bilingual dictionary. The TL lemma is then accompanied by the morphological inflection attributes in the TL word class.Fig. 1shows an English–Spanish bilingual phrase pair and the EAT obtained from it. This EAT matches any proper noun (PN) followed by a possessive ending (POS), and a singular (num:sg) noun (N). As an output, this EAT produces a masculine (gen:m) singular determiner (DT) with the lemma el, a masculine singular noun whose lemma is obtained by looking up the lemma of the noun that is matched in the SL in the bilingual dictionary, a preposition (PR) with the lemma de, and a proper noun whose lemma is retrieved from the bilingual dictionary by looking up the lemma of the proper noun that is matched in the SL. Restriction r3 prevents the EAT from being applied when the noun is not masculine in the TL, which would produce a TL translation with no gender agreement between the determiner and the noun. With this EAT, the translation into Spanish of the English phrase Fran's pen, with SL IRw1=FranPN, w2='s POS,w3=penN-num:sg, would be el bolígrafo de Fran, with TL IRw1′=elDT-gen:m.num:sg,w2′=bolígrafoN-gen:m.num:sg,w3′=dePR,w4′=FranNP.Although this method infers shallow-transfer rules capable of producing translations that are close to those produced with hand-written rules and provides better results than SMT systems trained on the same parallel corpus extended with the bilingual dictionary of the RBMT system (Sánchez-Martínez and Forcada, 2009, Sec. 5.2.1), it has three main limitations which we describe below. The first two limitations are inherent to the expressiveness of the EATs they use, whereas the third is a limitation of the aforementioned authors’ learning algorithm.The definition of word classes is not sufficiently flexible to permit EATs with different generalisation levels. The most general word classes are obtained by removing the lemma from the lexical forms and they therefore take into account the lexical category and all the inflection information (e.g. gender, number, verb tense, person, case, etc.). This often results in having to learn several EATs in order to describe the translation of the same linguistic phenomenon. For instance, adjectives in English are placed before the noun, whereas in Spanish they are usually placed after the noun. In order to properly translate an adjective followed by a noun into Spanish, Sánchez-Martínez and Forcada (2009) need to learn the four EATs shown in Fig. 2; these EATs only differ in the morphological inflection information (gender and number) of the lexical forms they match. Note that if more general word classes were used, so that all adjectives (or nouns) were assigned to the same word class regardless of the inflection information, the adjective–noun reordering could be described with the EAT shown in Fig. 3. In that figure, the morphological inflection attributes gen:* and num:* in the SL word classes mean that they match any gender and number, respectively. The morphological inflection attributenum:$s1in a TL word class means that the TL lexical form produced as a translation takes the value of the attribute with the same name in the first SL lexical form matched (more information on the word classes used is provided in Section 3).In general, we solve the partial generalisation limitation by using word classes with different levels of generalisation and exploiting the information contained in the bilingual phrase pairs to decide, in a context-dependent manner, the generalisation level of the EATs, that is, the morphological inflection attributes that contain the wildcard value (*) that matches any possible value. In our approach, multiple EATs, with different levels of generalisation, are generated from each bilingual phrase pair. The set of EATs to be used—and therefore the appropriate generalisation level to be used to describe the translation of the different linguistic phenomena found in the training corpus—is then automatically determined by selecting the minimum number of EATs that are needed to reproduce the bilingual phrase pairs from which the EATs are obtained. In order to deal with the complexity of choosing that minimum set of EATs when working with all the bilingual phrase pairs extracted from the corpus, the problem is posed as an optimisation problem by defining a set of inequations which are solved using integer linear programming methods (Garfinkel and Nemhauser, 1972). Our approach is the first in literature (see Section 5) in which the problem of automatically inferring transfer rules is reduced to finding the optimal value of a minimisation problem.Being able to use word classes with different generalisation levels implies that our method needs fewer examples to learn common structural transformations between the SL and the TL. In addition, having more general EATs makes it easier for linguists to revise the inferred rules and for these rules to be combined with hand-written rules.The way in which word classes are defined by Sánchez-Martínez and Forcada (2009), that is, by using a set of lexicalised units, not only prevents better generalisations from being created, as explained above, but also prevents context-dependent lexicalisation from taking place. Context-dependent lexicalisation would permit a different treatment to be given to those words that, in a given context, are not properly translated by more general EATs. For instance, in Spanish some adjectives—called prepositive adjectives55Although only a reduced set of adjectives are always prepositive in Spanish, all adjectives can be used prepositively in poetry and literature. Postpositive adjectives are unusual in English, but they can be found in phrases such as body politic, queen consort or time immemorial.—are usually placed before the noun, e.g. gran hombre,66Translated into English as great man.instead of after the noun as usual. In order to properly translate the English adjective–noun construction into Spanish when the Spanish equivalent of the English adjective is a prepositive adjective, prepositive adjectives need to be lexicalised. In the approach by Sánchez-Martínez and Forcada (2009) this would require knowing the set of the most frequent prepositive adjectives in Spanish in advance, adding them to the set of lexicalised units, and learning, in addition to the EATs in Fig. 2, EATs like those shown in Fig. 4for the adjective great.77We assume that if several EATs match the same sequence of SL lexical categories, the most specific EAT is applied. We also assume that the dictionaries of the RBMT system do not contain any information indicating whether or not an adjective is prepositive.Note that z1 from Fig. 4 is an exception to the general rule used to translate the adjective–noun constructions because it does not perform any reordering, as opposed to the EATs in Fig. 2, and that the translation rule encoded in z2 from Fig. 4 is equivalent to the general rule used to translate a determiner, followed by a singular noun, the verb to be in the past tense, 3rd person, singular, and a (predicative) adjective. It is therefore clear that the lexicalisation in z2 is not needed and performing such a lexicalisation leads Sánchez-Martínez and Forcada (2009) to generate more EATs than are really necessary, some of which may be useless.Our approach overcomes this limitation because the different generalisation levels explored for each word class include EATs in which the lemma of the lexical forms is kept unchanged. We then follow the approach outlined above to select the minimum number of EATs that are needed to reproduce the bilingual phrase pairs from which EATs are obtained. Consequently, these lexicalisations are only used when they are needed to encode an exception to a more general translation rule.Finally, Sánchez-Martínez and Forcada (2009) do not apply any method with which to discard those EATs that force SL lexical forms that should be processed together by the same rule—because they are involved in the same linguistic phenomenon—to be dealt with by different rules. This is a common situation when the bilingual phrase pairs, from which the EATs are obtained, are extracted by following the standard method in SMT (Koehn, 2010, Sec. 5.2.3), which is likely to separate words that should be processed together into different phrases. This is a problem in shallow-transfer RBMT because an SL lexical form can only be translated by a single rule. In the specific case of Apertium, the SL sentence to be translated is divided into chunks so that each chunk is matched and translated by a single rule in a left-to-right, longest-match fashion. Apertium starts from the first SL lexical form in the sentence, selects the longest applicable rule, applies it to the matched chunk, prints the result, and starts the process again from the next (unmatched) SL lexical form in the sentence. If no rule can be matched, the corresponding SL lexical form is translated in isolation and the process starts again with the next one.Not discarding the EATs that perform a segmentation of the SL sentence that is unsuitable for a shallow-transfer RBMT system may result in not applying other EATs that would perform a correct translation, despite having been learnt from the parallel corpus. This is illustrated in Fig. 5in which an EAT that matches a determiner, an adjective, a noun, a conjunction (CC) and another determiner is applied (top), rather than applying an EAT that matches a determiner, an adjective and a noun twice (bottom). In the first case, the translation of the determiner after the conjunction is not processed together with the noun and the adjective it has to agree with in gender and number, which may result in an incorrect translation into the TL.We have tackled this last problem by retaining in the final set of EATs only those that make the translation of the SL side of the training parallel corpus sufficiently close to its TL side when the EATs to be used are selected in a left-to-right longest match manner, as the RBMT engine will do. This is done by following a greedy approach in order to identify the set of sequences of lexical categories that should be translated by the same rule, and by removing those EATs that produce the same translation as a set of shorter EATs would produce.This section describes the notation that will be used in the remainder of the paper along with the improvement we have made to the EAT formalism used by Sánchez-Martínez and Forcada (2009) in order to be able to learn more general EATs which we shall refer to as generalised alignment templates (henceforth, GAT).As stated in the introduction, shallow-transfer RBMT systems use as IR sequences of lexical forms in both languages. Recall that the translation process in shallow-transfer RBMT is as follows: first the SL IR is obtained from the SL text, usually with the help of a monolingual dictionary and a part-of-speech tagger; then the SL IR is converted into a TL IR by applying shallow-transfer rules (in our case encoded as GATs) and using a bilingual dictionary; finally the TL text is generated from the TL IR with the help of a TL monolingual dictionary.A lexical formw, e.g. carN-gen:ϵ.num:sg, consists of:•a lemmaλ(w), e.g.λ(w)=car,a lexical category88Without loss of generality, ρ could also be used to represent lexical subcategories.ρ(w), e.g.ρ(w)=N (noun),a set of morphological inflection attributesα(w), e.g.α(w)={gen,num} (gender and number), andtheir valuesυ(w,a), e.g.υ(w,num)=sg (singular).Some morphological inflection attributes may be assigned an empty value (ϵ) because they do not apply to that language; in the example above,υ(w,gen)=ϵ because nouns do not have a gender in English. This is done for convenience so that lexical forms have the same morphological inflection attributes in the two languages involved in the translation. It is worth noting that the functions described above can equally be applied to lexical forms and words classes; what is more, α(·) and υ(·) can also be applied to restrictions.99We abuse the notation slightly in this case; note, however, that a restriction merely consists of a set of restricted morphological inflection attributes and their values.SL lexical forms are translated into TL lexical forms by looking them up in a bilingual dictionary. An SL lexical form may have more than one equivalent in the TL; in these cases, a lexical selection module (Tyers et al., 2012; Tyers, 2013) is responsible for selecting the most appropriate translation given the SL context prior to the execution of the structural transfer module. We shall refer to the result of translating an SL lexical formwinto a TL lexical form asτ(w)throughout this document.In order to be able to learn GATs we have introduced the following special values for the morphological inflection attributes:•The wildcard * value in the morphological inflection attribute of an SL word class signifies that it matches any value. Hence, a GAT z=(S, T, A, R),1010The elements (S, T, A, R) have the same meaning as in EATs: S is a sequence of SL word classes, T is a sequence of TL word classes, A is a set of pairs of word class indexes (i, j) with the alignment information between the word classes in S and T, and R is a set of restrictions over the TL morphological inflection information of the lexical forms matching the GAT.with S=(s1, s2, …, sn), and R=(r1, r2, …, rn), matches a sequence of SL lexical formsW=(w1,w2,…,wn)only if W and S have the same length and every SL lexical formwi∈Wmeets the following conditions:–either its lemma equals the lemma in the SL word class si, or sihas no lemma (because it has been generalised):λ(wi)=λ(si)∨λ(si)=ϵ;its lexical category equals the lexical category in the SL word class si:ρ(wi)=ρ(si);either the value of the morphological inflection attributes ofwiequal those in si, or the value of the corresponding morphological inflection attributes in sicontain wildcards:∀a∈α(si):υ(wi,a)=υ(si,a)∨υ(si,a)=*;the value of the morphological inflection attributes specified in the restrictions riare equal to those in the TL lexical form obtained by looking up the SL lexical formwiin the bilingual dictionary:1111The bilingual dictionary provides the translation of the lemma and also of the lexical category and morphological inflection attributes of the lexical form. For instance, the bilingual dictionary provides the gender that a noun must have when it is translated into Spanish.∀a∈α(ri),υ(τ(wi),a)=υ(ri,a).The SL reference $sjas the value of an attribute a of a TL word class timeans that the TL lexical formwi′produced as a translation takes the value of the corresponding attribute from the j-th SL lexical form matched by the GAT:υ(wi′,a)←υ(wj,a).The TL reference $tjas the value of an attribute a of a TL word class timeans that a takes the value from the corresponding morphological inflection attribute in the TL lexical form obtained after translating the j-th SL lexical form by looking it up in the bilingual dictionary:υ(wi′,a)←υ(τ(wj),a).It is worth noting that, even though the translation of most linguistic phenomena can be encoded using only TL references ($tj), there are situations, such as that described below, in which SL references ($sj) are needed. Consider the translation into English of the Spanish phrase es guapa,1212Translated into English as She is beautiful.with SL IRw1=serVERB-t:pres.p:3.num:sg,w2=guapaADJ-gen:f.num:sg. As the Spanish phrase contains no personal pronoun, the GAT that must be applied for its translation has to resort to the gender of the adjective in the SL to determine which pronoun, she or he, needs to be used, and an SL reference therefore needs to be used.Apart from the changes explained above, GATs are applied to translation in the same way in which the EATs of Sánchez-Martínez and Forcada (2009) are applied. The following example illustrates how the new attribute values $sjand $tjare used during translation. The GAT shown in Fig. 6encodes the translation of the English Saxon genitive construction—proper noun+possessive ending+noun—into Spanish. The wildcard attribute in the number makes it match both singular and plural nouns; the SL and TL reference values propagate the gender and number of the noun to the determiner. When translating the SL (English) phrase Mary's family, with the SL IRw1=MaryPN,w2='sPOS,w3=familyN-gen:ϵ.num:sg with the GAT in Fig. 6, the four TL lexical formsw1′⋯w4′produced as output are obtained as follows. The lemmas of the first and third lexical forms are taken from the GAT:λ(w1′)=λ(t1)=elandλ(w3′)=λ(t3)=de. The lemmas of the other two lexical forms are obtained by looking up the SL lexical forms aligned with them in the bilingual dictionary:λ(w2′)=λ(τ(w3))=familia,λ(w4′)=λ(τ(w1))=Mary. The lexical categories are taken from the TL word classes:ρ(w1′)=ρ(t1)=DT,ρ(w2′)=ρ(t2)=N,ρ(w3′)=ρ(t3)=PR, andρ(w4′)=ρ(t4)=PN. The morphological inflection attributes gen (gender) of the first and second TL lexical forms take their values from the corresponding attribute in the translation of the third SL lexical form (TL reference):υ(w1′,gen)=υ(w2′,gen)=υ(τ(w3),gen)=f. The morphological inflection attributes num (number) of these same TL lexical forms take their value from the corresponding attribute in the third SL lexical form (SL reference):υ(w1′,num)=υ(w2′,num)=υ(w3,num)=sg. The resulting sequence of TL (Spanish) lexical forms isw1′=elDT-gen:f.num:sg,w2′=familiaN-gen:f.num:sg,w3′=dePR,w4′=MaryPN, which after morphological generation leads to la familia de Mary.The complete process used to obtain shallow-transfer rules from a parallel corpus consists of the steps described in the remainder of this section and summarised in Fig. 7. First, word alignments and bilingual phrase pairs are obtained from the parallel corpus (Section 4.1). Multiple GATs, each one with a different level of generalisation, are then inferred from each of the bilingual phrase pairs obtained. This is done by using different sets of wildcard and reference attributes, and also with different lexicalised words (Section 4.2); these GATs, encoded with the formalism described in Section 3 do not suffer from the partial generalisation issue described in Section 2.1. After filtering certain GATs to deal with the noise present in the corpus and to prevent overgeneralisations (Section 4.3), the GATs with the most appropriate lexicalised words, generalisation level, and wildcard and reference attributes are automatically selected by finding the minimum set of GATs needed to correctly reproduce all the bilingual phrase pairs obtained from the corpus (Section 4.4). With this minimisation process, conflicts between GATs are removed and GATs with lexicalised word classes are selected only when they are strictly necessary in the context in which they appear; the second limitation described in Section 2.1 is therefore overcome. Any GATs that cause deficient chunking of the input are then discarded (Section 4.5) in order to get over the third limitation described in Section 2.1. Finally, the GATs selected are converted into the Apertium rule format, although they could be converted into the format used by any other shallow-transfer RBMT system.Word alignments and bilingual phrase pairs are obtained using the state-of-the-art method in order to obtain bilingual phrases pairs for their use in SMT (Koehn, 2010). This method, which was also followed by Sánchez-Martínez and Forcada (2009), consists of the following steps:1.Morphologically analyse both sides of the parallel corpus and solve the part-of-speech ambiguities in order to obtain sequences of lexical forms in both languages.Train IBM models 1, 3 and 4 (Brown et al., 1993), and the HMM alignment model (Vogel et al., 1996), for 5 iterations by means of GIZA++ for both translations directions (Och and Ney, 2003).1313http://code.google.com/p/giza-pp/.Compute the Viterbi alignment according to these models for both translation directions.Symmetrise the two sets of Viterbi alignments using the refined intersection method proposed by Och and Ney (2003) to obtain word-aligned sentence pairs.Extract bilingual phrase pairs that are consistent with the alignments (Koehn, 2010, Sec. 5.2.3).From each bilingual phrase p, many different GATs that correctly reproduce it—when applied to the SL phrase in p, the corresponding TL phrase is obtained—are generated, although not all of them will eventually be used for rule generation. The selection of GATs to be used for rule generation is described in Sections 4.4 and 4.5.Given a bilingual phrase pair p, the generation of GATs from it can be described as the initial generation of the most specific GAT, β(p) (Section 4.2.1), and the chained application of three different functions (σ1, σ2 and σ3), each one of which takes the set of GATs produced by the previous one as input and generates a new set of GATs from each GAT in the input set. Function σ1 removes lexicalised words (Section 4.2.2), function σ2 introduces wildcards, SL and TL references and removes restrictions (Section 4.2.3), and function σ3 ensures that each non-lexicalised TL word class is aligned with at most one SL word class (Section 4.2.4).The initial GAT z=β(p)=(S, T, A, R) created from a bilingual phrase pair p is the most specific GAT that can be obtained from it, and therefore only matches the SL phrase in p.Let p=(W, W′, A′) be a bilingual phrase pair with an SL sequence of lexical formsW=(w1,w2,…,wn), a TL sequence of lexical formsW′=(w1′,w2′,…,wm′)and alignment information A′={(i, j):i∈[1, n]∧j∈[1, m]}. Each SL word class si∈S in GAT z has the same lemma, lexical category and morphological inflection attribute values as the corresponding SL lexical formwiin p, i.e.∀i∈[1,n],si←wi. The same applies to the TL word classes:∀i∈[1,m],ti←wi′. The alignment information A in z is also copied from the bilingual phrase pair: A←A′. Finally, restrictions R are obtained by looking up each SL lexical form in the bilingual phrase pair in the bilingual dictionary as follows:∀wi∈W,α(ri)←α(wi)and∀ri,∀a∈α(ri),υ(ri,a)←υ(τ(wi),a).Fig. 8shows a bilingual phrase pair p and the initial GAT z obtained from it. Note that the restrictions limit the morphological attribute values to those in the bilingual dictionary.The next step as regards obtaining more general GATs is to remove from each initial GAT the lemma from some of the SL and TL lexical forms that are related according to the bilingual dictionary. Recall that, during translation, when a TL word class does not contain a lemma, the lemma of the TL lexical form produced is obtained by looking up the SL lexical form to which it is aligned in the bilingual dictionary.Function σ1 generates a new GAT for each of the possible subsets of the set E with the positions of the SL word classes from which the lemma can be removed. Given an input GAT z=(S, T, A, R), with S=(s1, s2, …, sn) and T=(t1, t2, …, tm), the set E is obtained by first computing for each SL word class sithe set Diof the TL word classes aligned to it whose lemmas are related according to the bilingual dictionary:Di={tj:(i,j)∈A∧λ(τ(si))=λ(tj)};and then including in E the positions of the SL word classes whose lemmas, according to the bilingual dictionary, are related to at least one TL word class:E={i:Di≠∅}.For each possible subsetF∈P(E),1414P(E)is the power set of E.σ1 generates a new GAT z′. Each new GAT z′=(S′, T′, A, R) is a copy of z in which the lemmas have been removed from the SL word classes whose positions are specified in F, and from the TL word classes aligned with them whose lemmas are related according to the bilingual dictionary:∀i∈F,λ(si′)←ϵand∀i∈F,∀j:(i,j)∈A∧λ(τ(si))=λ(tj),λ(tj′)←ϵ.As the empty set ∅ is always contained inP(E), the initial (non-generalised) GAT is always contained in the output of σ1 (identity transformation).Fig. 9shows the result of applying σ1 to the GAT shown in Fig. 8 (z0). In this example, the number of GATs to be generated is 4 and E={1, 3} because, according to the bilingual dictionary, the first SL lemma is translated as the first TL lemma, and the third SL lemma is translated as the second TL lemma.The use of wildcards and SL and TL references in the morphological inflection attributes allows the translation rules to be generalised to words with different values in their morphological attributes. This allows, for example, general reordering rules, like that presented in Fig. 3, to be learnt, which are usually independent of the gender and number of the words involved.Function σ2 generates a set of GATs for each input GAT z by introducing wildcards in some of the morphological inflection attributes of the SL word classes and references in the counterpart morphological attributes in the TL word classes. It also removes the restrictions associated with the attributes of the SL word classes whose values have been replaced with a wildcard.For each input GAT z=(S, T, A, R), it is first necessary to obtain the set of candidate attributes C which are allowed to contain wildcards in the SL and references in the TL, and then the sets Mj,aof possible SL references and TL references for each TL word class tjand morphological inflection attribute a∈C.A morphological attribute a is present in C only if for each TL word class tj∈T with a∈α(tj) it contains an empty value (υ(tj, a)=ϵ) or the non-empty value it contains can be obtained with an SL reference (∃i:υ(si, a)=υ(tj, a)) or with a TL reference (∃i:υ(ri, a)=υ(tj, a)):C={a:υ(tj,a)=ϵ∨(∃i:υ(si,a)=υ(tj,a))∨(∃i:υ(ri,a)=υ(tj,a))∀tj∈T:a∈α(tj)};Note that the restrictions are used to check whether an attribute value can be obtained with a TL reference, since their values have been obtained from the bilingual dictionary. The sets Mj,aof possible SL references and TL references for each TL word class tjand morphological inflection attribute a∈α(tj)∩C are computed using Algorithm 1.Algorithm 1Algorithm that computes the set of possible SL and TL reference values that a given morphological inflection attribute a of a TL word class tjcan have.Mj,a,1←{ $ti:(i,j)∈A∧υ(ri,a)=υ(tj,a)}Mj,a←Mj,a,1∪{ $si:(i,j)∈A∧υ(si,a)=υ(tj,a)∧$ti∉Mj,a,1}ifMj,a=∅ thenMj,a,2←{ $ti:υ(ri,a)=υ(tj,a)}Mj,a←Mj,a,2∪{ $si:υ(si,a)=υ(tj,a)∧$ti∉Mj,a,2}end ifreturnMj,aThis algorithm proceeds as follows. If attribute a can be obtained with a reference to an SL word class to which tjis aligned, the corresponding reference is added to Mj,a. If not, the algorithm adds references to other SL word classes from which attribute a can be obtained to Mj,a. In either case, the SL references are only included in Mj,aif TL references cannot be used. Mj,a,1 represents the TL reference attributes to SL word classes to which tjis aligned that give the value of the attribute a in tjas a result, while Mj,a,2 contains the TL reference attributes to SL word classes to which tjis not aligned and give the value of that attribute a as a result.Finally, a set of GATs GLis then obtained for each possible set of attributesL∈P(C), thus permitting GATs with different generalisation levels to be built: the more attributes in L, the more general the resulting GATs. As occurs with σ1, the empty set ∅ is always contained in L, and every input GAT is therefore also part of the result of applying σ2 to it.All the GATs in GLshare the same sequence of SL word classes S′, set of restrictions R′ and alignment information A′; they only differ in the sequence of TL word classes. S′ is a copy of the original sequence of SL word classes S in which the value of the morphological inflection attributes in L has been replaced with a wildcard:∀si∈S,∀a∈α(si):a∈L,υ(si′,a)←*;R′ is a copy of the original sets of restrictions R in which the attributes in S whose values have been replaced with a wildcard in S′ have been removed:∀si∈S,∀a∈α(si):a∈L,ri←ri−{a};and A′ is a copy of the original alignment information A.The different sequences of TL word classes to be generated, one for each GAT in GL, differ as regards the attribute values that need to be used. These values are obtained as the Cartesian productN=∏tj∈T∏a∈α(tj)ω(tj,a), where ω(tj, a) equals a set with the original attribute value if attribute a will not be assigned a reference, or otherwise a set with the references to be used:ω(tj,a)=Mj,aifa∈L∧|Mj,a|>0{υ(tj,a)}otherwise.Finally, a GAT is created for each element n∈N. The sequence of TL word classes T′ of each new GAT is a copy of the original sequence of TL word classes T in which the values of the attributes have been replaced with those in n.Fig. 10shows the four GATs (z1–z4) generated by σ2 for the input GAT z0 from the same figure. These GATs codify the reordering and gender and number agreement rule that must be applied for the English–Spanish translation of an adjective followed by a noun. The set of morphological inflection attributes that can be assigned a wildcard in the SL, and a reference in the TL is C={gen, num}; wildcards are permitted in the num (number) attribute because its value can be obtained by using an SL reference or a TL reference (in this case using both types of references) for both TL word classes; wildcards are permitted in the gen (gender) attributes because its value can be obtained using a TL reference. The sets of possible reference values to be used areM1,gen={$t2},M1,num={$t2},M2,gen={$t2}andM2,num={$t2},P(C)={{},{gen},{num},{gen,num}}.For a GAT to be useful in shallow-transfer RBMT, every non-lexicalised TL word class must be aligned with at most one SL word class: that from which, at translation time, the TL lemma will be obtained by looking up the SL lexical form matched in the bilingual dictionary.Function σ3 removes those alignments that would render z not applicable in shallow-transfer RBMT from each input GAT z=(S, T, A, R). This is done by first obtaining the set with the positions of the non-lexicalised TL word classes V:V={j:tj∈T∧λ(tj)=ϵ}.Then, for each TL word-class position j∈V, the set of possible alignments Xjis computed by considering the bilingual phrase pair (W, W′) from which the GAT z was obtained and ensuring that it can be reproduced using the selected alignment points:Xj={(i,j):(i,j)∈A∧λ(τ(wi))=λ(wj′)}.Finally, all the possible subsets of A that ensure that the original bilingual phrase pair can be reproduced from z are calculated as the Cartesian productY=∏j∈VXj, and σ3 generates an alternative GAT zy=(S, T, Ay, R) for each element y∈Y, where Aystands for the subset of A that contains exactly the elements from the tuple y.Fig. 11shows an input GAT z0 and the GAT z1 produced from it by σ3. The valid alignment points for each TL word class are X1={(1, 1)} and X2={(3, 2)}; the Cartesian product Y={((1, 1), (3, 2))} consists of a single element, which generates GAT z1.Once a set of GATs has been generated from each bilingual phrase pair, a filtering of the GATs obtained must be carried out in order to discard those that are very infrequent or are not able to reproduce a large proportion of the bilingual phrase pairs they match. This may occur as a result of either the noise present in the training parallel corpus or overgeneralisations.Given the set of bilingual phrase pairs P extracted from the parallel corpus (see Section 4.1) and a GAT z∈Z, the set of GATs obtained from P (see Section 4.2), we refer toM(z)⊆Pas the set of bilingual phrase pairs that are matched by z.1515A GAT matches a bilingual phrase pair if the SL word classes match the sequence of SL lexical forms and all restrictions are met (see Section 3 for more details).Some of these bilingual phrase pairs,G(z)⊆M(z), are correctly translated by z—when applied to their SL side, their TL side is obtained—while others,B(z)=M(z)−G(z), are not.The filtering consists of discarding, on the one hand, those GATs z whose number of correctly reproduced bilingual phrase pairsG(z)is below a threshold θ; and on the other, those GATs for which the ratio of bilingual phrase pairs correctly reproduced and matched to the total number of bilingual phrase pairs matched is below a threshold δ. Any GATs that encode very infrequent linguistic transformations, along with those that overgeneralise, are thus avoided. The number of correctly reproduced bilingual phrase pairs is calculated by considering the frequency in the training parallel corpus of each bilingual phrase pair. A GAT z is thus discarded ifQ(G(z))<θ∨Q(G(z))Q(M(z))<δ,whereQis the aggregated frequency of a set of bilingual phrase pairs:Q(P)=∑p∈Pfreq(p),and freq(p) is the absolute frequency in the parallel corpus of the bilingual phrase pair p.The objective of our approach is to obtain a set of GATs that is able to correctly translate at least the set of bilingual phrase pairs extracted from the training parallel corpus. What is more, the GATs in that set must be as general as possible in order to extend the linguistic knowledge obtained from the corpus to unseen input texts. This objective is achieved by selecting the minimum amount of GATs needed to correctly reproduce all the bilingual phrase pairs. Since the more general the GATs, the higher the amount of bilingual phrase pairs they match and (hopefully) reproduce, if the amount of GATs is minimised, the most general ones that are able to reproduce the bilingual phrase pairs in the training corpus are selected.Unlike the other approaches used to automatically learn shallow-transfer rules from parallel corpora (Sánchez-Martínez and Forcada, 2009; Caseli et al., 2006; Probst et al., 2002), here all the bilingual phrase pairs are considered together when checking their reproducibility by the set of GATs obtained. We thus treat conflicting rules at a global level, while previous approaches treat them locally.To define the minimisation problem, GATs need to be ordered according to their level of specificity. A GAT z=(S, T, A, R) is said to be more specific than another GAT z′=(S′, T′, A′, R′) if it has any component—either a lemma, a morphological inflection attribute or a restriction—that takes into account more fine-grained information than z′1616Another option would be to compare the sets of bilingual phrase pairs matched by each GAT and consider z as more specific than z′ ifM(z)⊂M(z′). However, when the training corpus is small (only a few hundred sentences), it may occur that z and z′ match the same set of bilingual phrase pairs in spite of z′ being more general than z because it has the potential to match more sequences of lexical forms when translating new texts.:more_specific(z,z′)⇔|S|=|S′|∧∀si∈S,(ρ(si)=ρ(si′)∧(λ(si)=λ(si′)∨λ(si′)=ϵ)∧∀a∈α(si),(υ(si,a)=υ(si′,a)∨υ(si′,a)=*)∧∀a∈α(ri),(υ(ri,a)=υ(ri′,a)∨a∉ri′))∧(∃si∈S:si≠si′∨∃ri∈R:ri≠ri′).On the basis of the set of bilingual phrase pairs P, the set of GATs Z and their relation of specificity defined by the function more_specific(·), the minimum set of GAT O⊆Z is chosen subject to the following constraints:C1:Each bilingual phrase pair is correctly reproduced by at least one GAT that is part of the solution:⋃zi∈OG(zi)=PIf a GAT zithat is part of the solution incorrectly reproduces the TL part of a bilingual phrase pair p, there is another GAT zjthat is part of the solution, is more specific than ziand correctly reproduces the TL part of p:∀zi∈O,∀p∈B(zi),∃zj∈O:more_specific(zj,zi)∧p∈G(zj)In practice, constraintC1needs to be relaxed because, as a result of the filtering method described above (see Section 4.3), there may not be a subset O⊂Z satisfying it, i.e., the minimisation problem may not have a solution because it is impossible to reproduce all the bilingual phrase pairs regardless of the set of GATs chosen. This occurs when the highly lexicalised GATs that would be needed to reproduce certain bilingual phrase pairs have been removed and there is a conflict between the less specific GATs that are able to reproduce them. When this happens, we find the set of bilingual phrases PO⊂P that maximises∑p∈POfreq(p)and makes the minimisation problem solvable, i.e., that permits finding a set of GATs that meets the constraintsC1andC2.There may also be multiple solutions to the minimisation problem, i.e., different sets of GATs with the same (minimum) size may satisfy the two constraints above. In this case, we choose the set of GATs containing the most general GATs. This is done by defining a function spec_level(z)1717Note the difference between more_specific(·) and spec_level(·). more_specific(·) defines a strict partial order in which two GATs are related if, and only if, the set of bilingual phrase pairs matched by one of them is a subset of the set of bilingual phrase pairs matched by the other. This makes the solution of the minimisation problem look like a hierarchy with general rules and more specific rules fixing the cases not correctly translated with the general ones. Contrarily, spec_level(·) simply allows our strategy to select from among different solutions with the same amount of GATs.that accounts for the level of specificity of a GAT z (see below), computing the aggregated level of specificity of the possible solutions to the minimisation problem, ∑z∈Ospec_level(z), and choosing the set with the smaller aggregated level of specificity as the solution. The level of specificity of a GAT z is simply obtained by counting the number of lexicalised words and the number of morphological inflection attributes in the SL word classes with non-wildcard values:spec_level(z)=γ1|{si:si∈S∧λ(si)≠ϵ}|+γ2∑si∈S|{a:υ(si,a)≠*}|+1.The first two terms in the equation above are assigned a weight so that lexicalised word classes have a higher impact on the final result than the morphological inflection attributes with non-wildcard values. This is achieved by making γ2=1 and γ1 higher than the highest possible value of the second term, that is,γ1=∑si∈S|α(si)|+1, since, in practice, a different minimisation subproblem is solved for each sequence of SL lexical categories (see below). The third term is added for convenience, to prevent spec_level(z) from returning a null value.The minimisation problem we have defined is similar to the well-known set covering problem (Garey and Johnson, 1979), which is NP-hard (Korte and Vygen, 2012, Sec. 15.7). Despite its complexity, it can be solved in a reasonable amount of time when the quantity of bilingual phrase pairs and GATs is relatively small—a common situation when the amount of training parallel corpora is scarce—by splitting the problem into independent sub-problems: one for each different sequence of the SL lexical categories. Each minimisation sub-problem is formulated as an integer linear programming problem (Garfinkel and Nemhauser, 1972). This kind of problems involves the optimisation of a linear objective function subject to linear inequality constraints. In our experiments we have used the state-of-the-art branch and cut approach (Xu et al., 2009). For a detailed description on how the minimisation problem is reformulated using linear inequations we refer the reader to Appendix A.The problem of selecting the minimum set of GATs that are needed to reproduce all the bilingual phrase pairs obtained from the training parallel corpus has been independently solved for each sequence of SL lexical categories. However, several GATs are used in the translation of a SL sentence, and each one translates a different sequence of SL lexical categories. The segmentation of the input SL sentences into chunks (sequences of SL lexical forms) is done by the GATs to be applied, which are chosen by the engine in a greedy, left-to-right, longest match fashion. It is therefore necessary to avoid the situation of having lexical forms that should be processed together—because they are involved in the same linguistic phenomenon—being assigned to different chunks.This section describes the process carried out in order to select the subset of the set of GATs obtained after solving the minimisation problem that ensures that the text to be translated will be chunked in the most convenient way. We select the sequences of SL lexical categories that GATs must contain in order to be part of the final solution; to do this, we follow a greedy approach that attempts to maximise the similarity between the TL side of the training parallel corpus and the result of translating its SL side using GATs in the same way as the RBMT engine would do. The method first identifies the minimum set of SL text segments (key segments) in the training corpus which need to be translated by a rule to obtain the highest similarity. Afterwards, the sequences of SL categories that ensure that the maximum number of key segments get translated properly are selected.LetKbe the set containing all the possible sets of text segments in the SL sentences of the training corpus,1818Note that the text segments inK∈Kdo not overlap and do not necessarily cover all the words in the corpus.andK★⊆Kbe the set of sets of text segments that maximise the similarity between the TL side of the training corpus and the translation obtained by translating each text segment inK∈K★with the most specific GAT available (as the RBMT engine would do) and the rest of the SL words in the training corpus word for word by looking them up in the bilingual dictionary. Here, similarity may be computed by using any standard MT evaluation measure.The set of key text segmentsIis one of the sets inK★. AsK★may contain more than a single set,Iis chosen so that it satisfies the following two conditions:1.Iis one of the sets with the fewest and shortest segments, i.e., with the minimum number of words covered by segments:I∈argminK∈K★∑k∈K|k|,where |x| denotes the number of words of text segment x.Iis one of the sets with the minimum average segment length:I∈argminK∈K★∑k∈K|k||K|These two conditions give priority to short text segments, and therefore to short GATs, over longer ones, in addition to the use of as few GATs as possible. If more than one set satisfies these two conditions,Iis chosen at random from among them.As exploring the whole setKis computationally unfeasible, in practiceIis obtained by processing one parallel sentence at a time and following a dynamic programming approach similar to the beam search approach used for decoding in SMT (Koehn, 2004).1919Note that, despite the fact that the key text segments are computed independently for each sentence, it is highly unlikely that the addition of a new sentence substantially affects the solution because all the key segments are considered together when selecting the sequence of lexical categories for which rules will be generated.Note that when computing the set of key text segmentsI, two text segments consisting of the same sequence of words are considered different if they appear in different positions in the corpus. This is also applicable to the description provided as follows.The sequences of lexical categories that GATs must contain in order to be part of the final solution are chosen from among the setLwith the candidate sequences of lexical categories, which are in turn obtained from the words in the set of key text segments:L=⋃g∈I{(ρ(wi))i=1|g|}.For each sequencel∈L, a score seq_qa(l) is computed. This score measures the impact on the translation quality of having rules matching the sequence of lexical categories l:seq_qa(l)=|key_seg_ok(l)||key_seg_ok(l)|+|key_seg_broken(l)|,where key_seg_ok(l) is the set of key text segments correctly translated by a rule matching the sequence of lexical categories l; and key_seg_broken(l) is the set of key text segments not correctly translated by a rule matching l plus the set of key text segments whose words are not translated together by the same rule as a consequence of having a rule matching l.On the one hand, key_seg_ok(l) is defined as:key_seg_ok(l)={g:g∈I∧(((ρ(wi))i=1|g|=l)∨(∃g′:g∈seg(g′)∧(ρ(wi))i=1|g′|=l∧∃K∈K★:g′∈K))},where seg(x) is the set of all possible (sub)segments of text segment x. The text segments returned by key_seg_ok(l) are the key text segments (g∈I) with a sequence of lexical categories l ((ρ(wi))i=1|g|=l), and the key text segments contained in longer segments (∃g′:g∈seg(g′)) with a sequence of lexical categories l ((ρ(wi))i=1|g′|=l) and correctly translated by a GAT (∃K∈K★:g′∈K).On the other hand, key_seg_broken(l) is defined as:key_seg_broken(l)={g:g∈I∧((∃g′:g∈seg(g′)∧(ρ(wi))i=1|g′|=l∧∄K∈K★:g′∈K∧∃z∈O:match(z,g′))∨(∃g″:(ρ(wi))i=1|g″|=l∧∃z∈O:match(z,g″)∧start(g″)<start(g)∧end(g″)<end(g)∧end(g″)≥start(g)))}where start(x) and end(y) refer to the position in the corpus of the first word of text segment x and the last word of text segment y, respectively; match(z, x) equals true if the GAT z matches the sequence of SL lexical forms of text segment x, otherwise zero. The text segments returned by key_seg_broken(l) are the key text segments (g∈I) contained in longer segments (∃g′:g∈seg(g′)) with a sequence of lexical categories l ((ρ(wi))i=1|g|=l), matched by at least one GAT (∃z∈O:match(z, g′)) and not correctly translated by any of the GATs matching it (∄K∈K★:g′∈K). It also returns the key text segments which are intersected on the left by another text segment g″ with a sequence of lexical categories l (∃g″:(ρ(wi))i=1|g″|=l) and matched by at least one GAT (∃z∈O:match(z, g″)). Note that any text segment intersecting on the left with a key text segment g and matched by a GAT prevents the words in g from being translated together by the same GAT. This happens, for instance, in the example presented at the top of Fig. 5 for the sentence The white house and the red cars: the GAT applied to the chunk The white house and the prevents the words in the chunk the red cars from being processed together by the GAT that would perform the gender and number agreement that is needed to produce a correct translation of that sentence into Spanish.A subset of the set of GATs O obtained as a result of the minimisation step described in Section 4.4 is then selected as follows:Osel={z=(S,T,A,R):z∈O∧(ρ(s))s∈S∈{l:l∈L∧seq_qa(l)≥μ}}where μ is a threshold whose value is automatically determined by trying all its possible values2020Actually, all the possible vales of μ do not need to be tested since those that generate the same set Osel will produce the same result.and choosing that which maximises the similarity of the TL side of the training corpus and the translation obtained when its SL sentences are translated with the set of GAT Osel. Note that not all GATs in Osel will eventually be used to generate shallow-transfer rules, since some of them may be discarded as a result of the next step.The number of GATs can be further reduced without decreasing the translation performance by removing those GATs which produce the same translations that a set of shorter GATs would produce. Let us suppose that GAT z produces the translation W′ when applied to the SL segment W. It often occurs that, when removing z from the set of GATs of the RBMT system, the engine still produces W′ when translating W. This may occur because the RBMT system splits W into two or more chunks and the translation of these chunks by the matching GATs yields W′, because the word for word translation of W produces W′ as a result, or because of a combination of these two reasons. If this occurs for all the SL segments that match z, then z can be safely removed from the set of GATs from which rules will be generated because it is redundant, i.e., the information in z is already contained in other GATs. Removing these longer GATs has actually improved the translation performance. Since long GATs are learnt from fewer examples and the useless ones are removed, shorter, more reliable GATs are applied.In order to detect and remove these redundant GATs, the following process is carried out. First, the GATs in Osel are sorted in order of decreasing length, while GATs of the same length are sorted by increasing level of specificity.2121The sorting is based on the function more_specific(·), defined in Section 4.4. GATs of the same length are arranged in a list compatible with the strict partial order defined by the function more_specific(·) by means of a topological sorting algorithm (Kahn, 1962).For each GAT z, the bilingual phrase pairs correctly reproduced by it,G(z), are then collected, and each bilingual phrase pairp∈G(z)is checked in order to ascertain whether or not, when translating the SL side of p with the set of GAT Osel−{z}, its TL side is obtained. If this requirement is met for allp∈G(z), z is definitively removed from Osel, i.e., Osel←Osel−{z}. It is therefore possible to guarantee that, after removing redundant GATs, the TL side of each bilingual phrase pair can be safely reproduced with the GATs that remain in Osel.For example, the Catalan–Spanish GAT z1 in Fig. 11 could be safely removed from the set of GATs Osel if the GAT in Fig. 12is also part of Osel because the presence of an adverb before the Catalan verb anar does not change the way in which the verb anar in the past tense followed by a verb in infinitive mood is translated. All the bilingual phrase pairs matching z1 in Fig. 12 can thus be reproduced by translating the adverb in isolation, i.e., by looking it up in the bilingual dictionary, and applying the GAT in Fig. 12 to the other two lexical forms.2222The proportion of GATs discarded because they can be replaced by shorter ones varies across language pairs and training corpus sizes. Generally, larger training corpora and more distant language pairs involve fewer GATs discarded. For instance, in the experiments described in Section 6, 75% of the Catalan–Spanish GATs with 5 SL lexical forms were discarded when the training corpus contained 250 sentences, while the proportion dropped to 41% for the training corpus with 5000 sentences. When Spanish–English rules were inferred from the training corpus with 5000 sentences, only 14% of the GATs with 5 SL lexical forms were discarded.Finally, the GATs resulting from the application of all of the above steps are converted into the rule format of the Apertium RBMT engine so that they can be used in real-world translation tasks. A list containing all the GATs and compatible with the strict partial order defined by the function more_specific(·) is built by means of a topological sorting algorithm (Kahn, 1962). This list contains the resulting GATs sorted in decreasing order of specificity and is used when generating the rules so that the most specific GAT is always applied when different GATs match the same input sequence of lexical forms. We refer the reader to Appendix B for the details of this conversion.

@&#CONCLUSIONS@&#
