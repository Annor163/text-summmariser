@&#MAIN-TITLE@&#
Feedforward kernel neural networks, generalized least learning machine, and its deep learning with application to image classification

@&#HIGHLIGHTS@&#
The feedforward kernel neural networks called FKNN are proposed.FKNN can work in both generalized-least-learning and deep-learning ways through implicit or explicit KPCAs.FKNN's deep learning framework DLP is justified by experiments about image classification.

@&#KEYPHRASES@&#
Feedforward kernel neural networks,Least learning machine,Kernel principal component analysis (KPCA),Hidden-layer-tuning-free learning,Deep architecture and learning,

@&#ABSTRACT@&#
In this paper, the architecture of feedforward kernel neural networks (FKNN) is proposed, which can include a considerably large family of existing feedforward neural networks and hence can meet most practical requirements. Different from the common understanding of learning, it is revealed that when the number of the hidden nodes of every hidden layer and the type of the adopted kernel based activation functions are pre-fixed, a special kernel principal component analysis (KPCA) is always implicitly executed, which can result in the fact that all the hidden layers of such networks need not be tuned and their parameters can be randomly assigned and even may be independent of the training data. Therefore, the least learning machine (LLM) is extended into its generalized version in the sense of adopting much more error functions rather than mean squared error (MSE) function only. As an additional merit, it is also revealed that rigorous Mercer kernel condition is not required in FKNN networks. When the proposed architecture of FKNN networks is constructed in a layer-by-layer way, i.e., the number of the hidden nodes of every hidden layer may be determined only in terms of the extracted principal components after the explicit execution of a KPCA, we can develop FKNN's deep architecture such that its deep learning framework (DLF) has strong theoretical guarantee. Our experimental results about image classification manifest that the proposed FKNN's deep architecture and its DLF based learning indeed enhance the classification performance.

@&#INTRODUCTION@&#
The wide popularity of feedforward neural networks in many fields is mainly due to two factors: (1) the strong approximation capability for complex multivariate nonlinear function directly from input samples; (2) The strong modeling capability for a large class of natural and artificial phenomena which are very difficult to handle with classical parameter techniques. However, when applied to many application scenarios, all parameters of feedforward neural networks [1,2] need to be adjusted in a backward way and thus there exists the dependence relationship between different layers of parameters in such networks, which results in a serious bottleneck issue: their traditional learning algorithms like BP algorithm are usually much slower than required, for example, taking several hours, several days and even falling into local minima. On the other hand, cross-validation and/or early stopping are sometimes adopted to circumvent the overfitting phenomena. In addition, before training a feedforward neural network by certain traditional learning algorithm like the BP algorithm in [3], we must fix the number of the hidden layers and the number of the hidden nodes of every hidden layer, and choose an appropriate error function in terms of the training task. If they are inappropriate, the performance of traditional learning algorithms will degrade a lot.In this work, we first propose an architecture, called FKNN, of feedforward neural networks with infinitely differential kernel based activation functions, which can include a large family of existing feedforward neural networks such as radial basis function (RBF) networks, sigmodial feedforward networks, self-organizing feature map (SOM) networks and wavelet neural networks [3], and hence can meet most practical requirements. The contributions of our work here can be highlighted in two main aspects: (1) When the number of the hidden nodes of every hidden layer and the type of the adopted kernel based activation functions are pre-fixed, different from the common understanding of learning, we prove that when all the hidden layers of such networks are tuning-free and their parameters are randomly assigned and even may be independent of the training data, such networks are universal approximators with probability one. Therefore, the least learning machine (LLM) [28], as a generalized version of the extreme learning machine (ELM) [4–13], can be further extended into its generalized version in the sense of adopting much more error functions instead of only the MSE function. (2) When the proposed architecture of FKNN networks is constructed in a layer-by-layer way, i.e., the number of the hidden nodes of every hidden layer may be determined after the explicit execution of a KPCA, we can develop FKNN's deep architecture such that its deep learning has strong theoretical guarantee with the enhanced performance for image classification. The more detailed contributions of this work can be summarized as follows:(1)Given the proposed architecture of FKNN networks with the pre-fixed number of the hidden nodes of every hidden layer and the pre-fixed type of the adopted kernel based activation functions, we reveal that for any hidden layer with its randomly assigned kernel parameter vectors, a special KPCA is implicitly performed and hence all the hidden layers may be tuning free. With this special KPCA, we can justify an appropriate number of the hidden nodes in the hidden layer in terms of the rank of the covariance matrix of the kernelized transformed dataset. This is a novel method of estimating an appropriate number of the hidden nodes in a hidden layer, which is also used to justify whether the pre-fixed number of the hidden node is appropriate or not.We reveal that the proposed feedforward neural networks behave like kernel methods, and hence their learning can be separated into two independent parts: implicitly executed KPCA plus a learning algorithm LA on the transformed data. However, rigorous Mercer kernel condition is not required. Theoretical results in kernel methods can help us answer why ELM, LLM and its generalized version here outperforms BP-like learning algorithms for feedforward neural networks. Unlike ELM and LLM which are based only on the MSE error function, the generalized LLM here can adopt various error functions.When FKNN networks are constructed in a layer-by-layer way, i.e., the number of the hidden nodes of every hidden layer is only determined after the explicit execution of a KPCA, we develop a deep architecture of FKNN networks with its deep learning framework DLF. Since this novel deep learning framework is built in terms of KLA=multi-layer KPCAs+LA, which makes the deep learning have rigorous theoretical guarantee the first time. We show that training this deep architecture can be finished within O(N) time complexity.When less abstract features are required, a lower level of the hierarchy in the proposed deep architecture can provide us a range of feature representations at varying levels of abstraction. Another advantage is that a lower level of the hierarchy can be used as the shared transformed data space for different tasks. Let us keep in mind that multi-layer feedforward neural networks with BP-like learning cannot have this advantage.The effectiveness of the proposed deep FKNN's architecture and its DLF based learning algorithm is confirmed by our experiments on image classification.The rest of this paper is organized as follows. In Section 2, we define the proposed FKNN network and its architecture. We prove that training such a FKNN network may be hidden-layer-tuning-free and then develop the proposed generalized LLM. In Section 3, we propose a deep FKNN architecture and its deep learning framework DLF. We also investigate the DLF-based image classification technique. In Section 4, we report our experimental results about DLF-based image classification and confirm its superiority over the classical technique in which a KPCA plus a learning algorithm is adopted. Section 5 concludes the paper, and future works are also given in this section.In this study, we consider the following feedforward kernel neural networks (FKNNs hereafter), as shown in Fig. 1. A FKNN network contains the input layer in which the input x=(x1, x2, …, xd)T∈Rd, L hidden layers in which each hidden node in each layer therein takes the same infinitely differential kernel function with different parameters as its activation functions, and the output layer in which the output y of FKNN network can be expressed as the linear combination of m activation functions in the last hidden layer by using the output weights α1, α2, …, αm. It is very easy for us to extend this architecture to its multiple outputs by using different linear combinations between the output layer and the last hidden layer, so we only consider a single output FKNN network for convenience here. Just likewise in current researches of feedforward neural networks, once the architecture of a FKNN network is fixed, i.e. the number of hidden nodes at every hidden layer and the type of the activation functions are pre-fixed, the remaining work will deal with the definition of its error function and the choice of its learning algorithm for the given training dataset.As we may know well, in order to assure the universal approximation of feedforward neural networks, their activation functions may take any infinite differential functions. Thus we should restrict FKNN's activation functions to be infinitely differential kernel functions. In terms of Table 1which summarizes the current representative feedforward neural networks, we can readily observe that such a restriction does not hamper us to justify its extensive suitability in covering current representative feedforward neural networks. In other words, FKNN can be taken to meet most practical requirements for feedforward neural networks.Now, let us first study a single hidden layer FKNN network, as shown in Fig. 2. Assume such a FKNN network has m hidden nodes in the single hidden layer whose activation functions are g(x, θ1), g(x, θ2), …, g(x, θm) where g(, ) denotes the adopted kernel function, θi(i=1, 2, …, m) denotes the parameter vector of the ith kernel function. For the given training dataset {x1, x2, …, xN} where xi∈Rd(i=1, 2, …, N), and N is the number of the training patterns in the training dataset, its training matrix X=[x1, x2, …, xN] and its centralized training matrixX¯={x1−x¯,x2−x¯,…,xN−x¯}in whichx¯=1N∑i=1Nxi.LetL=I−1N11T,I be an N×N identity matrix, 1 be a column vector with its every element being one, we haveX¯=XL.Thus, the covariance matrix of the training dataset can be written as(1)C=1NX¯X¯TKPCA extracts the principal components by calculating the eigenvectors of the kernelized covariance of a given dataset. For the above single hidden layer FKNN network, we have the following theorem.Theorem 1Assume the covariance matrixC=1NX¯X¯Tfor the given training matrix X=[x1, x2, …, xN] as above, the matrixD=1NGGTfor the single hidden layer of a FKNN network where the matrix G=[G1, G2, …, GN] andGi=(g(xi−x¯,θ1),g(xi−x¯,θ2),…,g(xi−x¯,θm))T(i=1, 2, …, N), there must exist a kernel feature mapping such that D can be generated from C after kernelizing it by this kernel mapping.Let us observe the covariance matrixC=1NX¯X¯T.Its every elementcij=∑k=1Nx¯kix¯kj(i, j=1, 2, …, N), wherex¯ki,x¯kjdenote the ith and jth components ofxk−x¯,respectively. For the matrixD=1NGGTfor the single hidden layer of a FKNN network, its every elementDij=∑k=1Ng(xk−x¯,θi)g(xk−x¯,θj)(i, j=1, 2, …, N). Therefore, this theorem holds true if we simply take the kernel feature mapping function φ(x)=(g(x, θ1), g(x, θ2), …, g(x, θm)).In terms of KPCA, Theorem 1 essentially implies that for a given training set, once the number m of the hidden nodes and the kernel type in the hidden layer are pre-fixed, for arbitrarily assigned parameter vectors θ1, θ2, …, θmwhich may ever be independent of the training data, a KPCA has been naturally and implicitly performed by taking the kernel feature mapping function φ(x)=(g(x, θ1), g(x, θ2), …, g(x, θm)) to realize a kernel data transformation from C to D. What is more, such a KPCA has the following distinct virtue: Unlike commonly used kernel methods like support vector machines (SVMs) and support vector regressions (SVRs) in [22], rigorous Mercer's condition [22] for kernel functions is not required, since kernel functions here have been presented explicitly in the kernel feature mapping functions, and the kernel trick, as a means of implicitly defining the feature mapping like in SVMs and SVRs, is not required due to the implicit execution of such a KPCA or after the inner products appear even if we consider its explicit execution of such a KPCA. For example, sigmodial kernel function (also see Table 1) k(x, y)=tanh(θ1xTy+θ2), θ1, θ2∈R is often used in feedforward neural networks. Obviously, it is not positive definite, thus according to Mercer's condition, it cannot be used in SVMs and SVRs. However, we can adopt it here for a FKNN network. Therefore, as two commonly used hidden layer activation functions in feedforward neural networks, sigmodial and Gaussian functions have been included in our FKNN architecture.Remark 1For the given training dataset, with arbitrarily assigned parameter vectors θ1, θ2, …, θm, in terms of KPCA with the corresponding kernel feature mapping function φ(x)=(g(x, θ1), g(x, θ2), …, g(x, θm)), we can obtain the corresponding eigenvalues and eigenvectors. In other words, the rank r of the kernelized covariance of the training dataset can be used to decide the number of the hidden nodes of a single hidden layer FKNN network. When m=r, it means a full-rank KPCA is performed. When m<r, a low-rank KPCA is performed. As pointed out in [23], a low-rank KPCA can help us overcome the overfitting phenomenon in feedforward neural networks and remove the noise affect in the kernel feature mapping space. When m>r, it means that redundant hidden nodes may remain in the hidden layer. Therefore, when we use the above KPCA in Theorem 1 to explain the data transformation from the input layer to the hidden layer of a single hidden layer FKNN network, we actually give a novel approach to determine an appropriate number of the hidden nodes, which can also be used to justify whether the pre-fixed number of the hidden nodes is appropriate or not.In [24], Yang et al. proved that a kernelized linear discriminant analysis (LDA) is equivalent to a KPCA plus LDA on the transformed dataset, i.e. KLDA=KPCA+LA. In [14,25,26], Wang et al. proved that kernelized SVM and its variants are equivalent to KPCA+SVM or SVM's variants. In the same year, Zhang in [23] gave more general results. They proved that the kernelized version of a learning algorithm can be implemented by performing the learning algorithm with the transformed data by the full-rank KPCA, if the learning algorithm satisfies the following two mild conditions simultaneously: (1) the output result of the learning algorithm can be calculated solely in terms of xTxi(i=1, 2, …, N), where xiis the training data point, and x is a new coming test data point. (2) Transforming the input data with an arbitrary constant does not change the output result of the learning algorithm. And then, they pointed out that some common kernel methods including kernelized canonical correlation analysis (KCCA), kernelized partial least squares method (KPLS), kernelized KNN (KKNN) indeed satisfy the above two mild conditions. In terms of the above theoretical results and Theorem 1, because a single hidden layer FKNN network here naturally and implicitly performs a KPCA, for any learning algorithm LA in the output layer satisfying the previous two mild conditions, this FKNN network naturally behaves like a kernelized LA (KLA in brevity) with the kernel feature mapping φ(x)=(g(x, θ1), g(x, θ2), …, g(x, θm)). Please note, without a special explanation, by a LA we mean that it satisfies the previous two mild conditions hereafter. What is more, the above theoretical results reveal that the training of a single hidden layer FKNN network can be separated into two independent parts: an implicitly performed KPCA between the input layer and the hidden layer plus a LA between the output layer and the hidden layer with the transformed dataset by KPCA.According to the principle of Vapnik's structural risk minimization [22,27], a learning algorithm LA(α1, α2, …, αm) in the last layer of a single hidden layer FKNN network can assure that with the probability 1−η[22,27]:(2)R(α1,α2,…,αm)≤Remp(α1,α2,…,αm)+hlog(2N/h)−log(η/4)N.where h denotes the VC dimension [22,27]. That is to say, for the given training dataset, once m and the chosen kernel type are pre-fixed in the hidden layer, the VC confidence term (i.e., the second term) in Eq. (2) will keep unchangeable whereas the empirical risk Remp(α1, α2, …, αm) and actual risk R(α1, α2, …, αm) depend on the one particular function chosen by the learning algorithm LA(α1, α2, …, αm). In other words, the fact that we only need to train the last layer of a FKNN does not give any limitation in the types of error functions. For example, except for the most frequently used MSE error function, we can also choose other error functions including these in Table 1. As we may know well, as two universal approximators with probability one [14], ELM (extreme learning machine) and its general version LLM (least learning machine) are actually based on the MSE error function to realize its fast training of a single hidden layer feedforward neural network through simply learning α1, α2, …, αmin the output layer with arbitrarily assigned weights in the hidden layer. Therefore, this remark actually says that for a single hidden layer FKNN network, we can adopt the same hidden-layer-tuning-free learning strategy as ELM and LLM, with much more types of error functions. Moreover, as illustrated in Fig. 3, existing learning algorithms including BP and genetic algorithms in Table 1 attempt to find out appropriate kernel parameter vectors in their respective learning ways, however, in essence, their endeavors cannot change the second term in Eq. (2), which means that in order to control the upper bound of the actual risk, we only need to control the upper bound of Remp(α1, α2, …, αm) in the last layer of feedforward neural network of this type, and hence we can choose other learning algorithms with other error functions to achieve this goal, as illustrated in Fig. 3.We can get the generalized LLM by replacing the loss function used by LLM with other loss functions. That is to say, by the generalized LLM we mean that it is a family of regressors which uses LLM's framework with other loss function instead of MSE loss function only. When a single hidden layer FKNN network is applied to function approximation problems, as stated in [14,28], if we take ridge regression as a LA in its last layer, we can obtain the least learning machine (LLM) for training on the given training dataset:(3)min12∑j=1mαj2+C∑i=1Nξj2s.t.∑j=1mαjg(xi,θj)=yi+ξii=1,2,…,NWith a sufficiently large regularizer C, LLM will become ELM. In other words, when LLM and ELM are applied to the proposed FKNN networks, they are two special cases of all potentially chosen learning algorithms. In particular, due to the existence of the matrix inversion in LLM and the matrix pseudo-inverse in ELM, their running time actually is O(N3) where N is the total number of the training patterns. When N becomes large, they will obviously become impracticable. Hence, by “extremely fast learning in ELM and least learning in LLM”, we only mean that their learning may be hidden-layer-tuning free but their training time still keeps very high (i.e., O(N3)). Even if LLM and ELM take the same strategy as in [11], i.e., Eq. (38) in [11], for large training datasets, their time complexity indeed has O(m3). According to the experiments in [11], m is generally taken as 1000 to meet the approximation/classification accuracy and hence their time complexity still keeps considerably high, especially for the application scenarios where N is huge but considerably less than the magnitude of O(10003)(=O(106)). However, in terms of the above remarks, this serious shortcoming can be readily battled down by choosing a LA with O(N) time complexity. For example, as a competitive model of LLM in Eq. (3), we can construct an alternative LLM in Eq. (4) with Vapnik's ɛ-insensitive loss function to achieve this goal with strong generalization capability.(4)min12∑j=1mαj2+C∑i=1N(ξi+ξi*)s.t.yi−∑j=1mαjg(xi,θj)≤ε+ξi,ξi>0i=1,2,…,N∑j=1mαjg(xi,θj)−yi≤ε+ξi*,ξi*>0Obviously, with arbitrarily assigned parameter vectors θ1, θ2, …, θm, this alternative LLM is a linear SVR in the training dataset {G1, G2, …, GN} in which Gi=(g(xi, θ1), g(xi, θ2), …, g(xi, θm))T(i=1, 2, …, N). Please note this training dataset {G1, G2, …, GN} can be calculated before solving Eq. (4). According to the solution strategy in [13], such a linear SVR can get its optimal solution with O(N) time complexity, which means its very applicability for a FKNN on large training datasets. To best of our knowledge, up to date, this is the best theoretical result about the convergence speed of current learning algorithms for feedforward neural networks with infinitely differential kernel functions for the application scenarios in which N is huge but N<m3. Let us keep in mind that because the kernel parameter vectors in the hidden layer is randomly assigned, the number m of the hidden nodes is generally a comparatively big value (e.g., 1000 in [11], and N being larger than m3 (e.g., 106) may seldom appear in application scenarios (imagine we have 106 training patterns!), therefore, the above alternative LLM in Eq. (4) has considerable applicability in practical scenarios. When N is larger than m3, we can take LLM or ELM with the same strategy in [11] (i.e., Eq. (38) in [11]).As seen in [6,11,28], numerous experimental results indicate that ELM and LLM outperforms the conventional SVM and SVR in approximation accuracy. Let us reveal the reason from a new perspective. As a competitive model of ELM and LLM, an alternative LLM in Eq. (4) can be readily derived as(5)min−12∑i=1N∑k=1N∑j=1m(αi−αi*)(αk−αk*)g(xi,θj)g(xk,θj)−∑i=1Nyi(αi−αi*)+ε∑i=1N(αi+αi*)s.t.∑i=1N(αi−αi*)=0,0<αi,αi*<CLet us recall the dual of the conventional SVR with the same Vapnik's ɛ-insensitive loss function [22,29]:(6)min−12∑i=1N∑k=1N(αi−αi*)(αk−αk*)k(xi,xk)−∑i=1Nyi(αi−αi*)+ε∑i=1N(αi+αi*)s.t.∑i=1N(αi−αi*)=0,0<αi,αi*<CBy comparing Eq. (5) with Eq. (6), we can easily find that the alternative LLM in Eq. (4) and hence ELM and LLM in essence employ the special kernel combination∑j=1mg(xi,θj)g(xk,θj)rather than only a single kernel k(xi, xk) in the conventional SVR. As we know very well, kernel combinations can generally enhance the performance of kernel methods. Therefore, the above observation actually helps us answer why ELM and LLM experimentally outperform the conventional SVM and SVR.Now, let us observe a multi-layer FKNN network. In terms of Theorem 1, for a multi-layer FKNN network, we can easily see that a KPCA implicitly behaves between the first hidden layer and the input layer, and another KPCA implicitly behaves between the first and second hidden layers. This process repeats until the last hidden layer in a multi-layer FKNN network. In other words, there implicitly exist multi-layer KPCAs, which may be viewed as a special KPCA with a successive nonlinear combination of the kernel feature mapping functions between the input layer and the last hidden layer. Thus, according to the above theoretical analysis, we may realize a LA between the output layer and the last hidden layer, with randomly assigned kernel parameter vectors in every hidden layer. When we understand the behavior of multi-layer FKNN networks in the above way, we can see the same benefits [14,28] as LLM, which is summarized as follows:(1)Since all the parameters in the kernel activation functions among all the hidden layers can be randomly assigned and all the patterns in the kernel activation functions among all the hidden layers can be randomly selected, and without any iterative calculation input data in every hidden layer are transformed into the next hidden layer only once in a forward layer-by-layer way, a multi-layer FKNN network has the advantages of both easy implementation and very fast learning capability, compared with BP-like learning algorithms where parameters in the network need to be iteratively adjusted in a backward gradient-descent way such that BP-like learning algorithms generally converge very slowly and even sometimes converge to local minima.Since all the parameters between all the hidden nodes can be randomly assigned, so no any dependence of parameters between different hidden layers exists!In fact, a multi-layer FKNN network here views the behavior of a feedforward neural network between the last hidden layer and the input layer as the successive encoding procedure for the input data in a difficult-to-understand way, due to the natural existence of KPCAs. When we understand the training behavior of FKNN networks from this new perspective as shown in Fig. 1, to large extent, this architecture can help us answer why feedforward neural networks behave like a black box.Please note, the generalized LLM's power can be easily justified by rich experimental results of LLM, ELM and linear SVM/SVR in [6,11,13,28,30–34], we do not report the experimental results on the generalized LLM in the experimental section, for the sake of the space of the paper.In this section, we will state that a deep FKNN's architecture can be built in a layer-by-layer way. Although a concrete deep FKNN's architecture is application-oriented and data dependent, its deep learning algorithm can always be characterized by a general deep learning framework DLF. Based on the proposed framework DLF, we explore deep FKNN's application in image classification.In [14,28], based on multi-layer KPCAs, we proposed the architecture of multi-layer feedforward neural networks and its LLM/ELM based fast learning methods. In the last section, we actually extend the conclusion in [11,14,15,28] and point out that more learning algorithms can be adopted in the hidden-layer-tuning-free mechanism of multi-layer feedforward neural networks. In this section, we will demonstrate that multi-layer feedforward neural networks with multi-layer KPCAs+LA can be used for deep learning.One may cheer that in terms of the seemingly surprising conclusions in the last section and LLM/ELM theory in [14], even if all the kernel parameter vectors in FKNN networks are tuning-free (i.e., randomly assigned) and learning is only required in the output layer of FKNN networks, FKNNs may become universal approximators with probability one. However, we should keep in mind that when learning in this way, FKNN networks actually behave like shallow architectures such as SVMs. When we consider how to assign appropriate kernel parameter vectors by explicitly executing KPCA in every hidden layer of FKNN networks, FKNN networks will behave like deep architectures which can help us represent complex data. Deep architectures learn complex mappings by transforming their input data through multiple layers of nonlinear processing. The advantages of deep architectures can be highlighted by the following motivations: the wide range of functions which can be parameterized by integrating weakly nonlinear transformations, the appeal of hierarchical distributed representations and the potential for combining unsupervised and supervised methods. However, at present, there exists the on-going debate over deep vs shallow architectures. Deep architectures are generally more difficult to train than shallow ones, since they involve difficulty nonlinear optimizations and many heuristics. The challenges of deep learning explain the appeal of SVMs which learn nonlinear classifiers via the kernel trick. Unlike deep architectures, SVMs are trained by solving simple QP problems. However SVMs cannot seemingly benefit from the advantages of deep learning.Here we will show that FKNN networks can help us enjoy both the success of deep learning and the elegance of kernel methods. Although we share a similar motivation as in previous works [35,36], our study here is very different. Our study here makes two main contributions. First, FKNN networks here can be built in a layer-by-layer way in terms of KLA=multi-layer KPCAs+LA rather than the strategies in [35–40], which makes their deep learning have rigorous theoretical guarantee the first time. Second, with the adopted fast KPCAs in the deep learning, the time complexity of training FKNN networks indeed becomes linear with the number of the training samples.In fact, multi-layer KPCAs map the input data into the kernelized data space in the last hidden layer through kernelized data spaces between multiple hidden layers. One may argue why we need multi-layer KPCAs. An example can help us explain the motivation of doing so and observe how to build an application-specific deep FKNN. Assume we want to accomplish two tasks, i.e., identify one male crime with a square face and the forehead of a mole, and the other male crime with a round face and high nose, from a face image database. Quite often, one first picks up all the male images from the face image database to obtain a shared data space for these two tasks. We can imagine this procedure as a nonlinear transformation between all male images and all the images. Based on the shared data space, one may then pick up square face images and round face images, respectively, from all the picked male images to construct two data sets for two tasks, which may be viewed as another nonlinear transformation. Finally, we can finish these two tasks, respectively, by identifying a face with the forehead of a mole in the square face images (a nonlinear transformation for the first task), a face with a high nose in the round face images (a nonlinear transformation for the second task). This example can be characterized by a deep architecture of FKNNs in Fig. 4where multi-layer KPCAs+LA is adopted.By observing the learning behavior of FKNN networks in Fig. 4, we can see the following virtues:(1)The number of the layers of a FKNN network can be increased in a layer-by-layer way and a KPCA behaves as a data transformation method in each hidden layer. This shares the same idea of the deep learning in [37–40]: Enough layers can ensure us to obtain enough complex nonlinear functions to express complex data. Except kernel parameter vectors in every layer, no any other parameter is required.If the number of the layers of a FKNN network is bigger than or equal to 2, even if a linear PCA is adopted in every hidden layer, a nonlinear data transformation in the last hidden layer will always be obtained. When a nonlinear kernel PCA is adopted in the first hidden layer, more complex nonlinear data transformation in the last hidden layer will be generated, which may be more beneficial for representing more complex data. Both linear PCAs and KPCAs in hidden layers actually reflect successive abstracts in a deep way for the input data or kernelized transformed data, respectively. In particular, Wang et al. proposed a KPCA based learning architecture for LLM or ELM based FKNN networks [28]. Mitchell et al. [35] realized a deep structure learning by using multi-layer linear PCAs and pointed out that their deep structure learning framework is a fairly broad one which can be used to describe the conventional Convolutional Networks and Deep Belief Networks whose successes in several applications have been largely sparking recent growing interest in deep learning in [35]. Chao and Saul implemented deep learning by using a special multi-layer kernel machines in [36]. Therefore, the proposed multi-layer FKNN framework is general in two senses: (1) it generalizes the work in [35] since except for linear PCAs more nonlinear KPCAs and more kernel functions can be involved; (2) it generalizes the work in [14] since except for the MSE error function adopted in LLM and ELM, more error functions can be considered within this framework.Although existing deep learning techniques have obtained impressive successes, the resulting systems are so complex such that they require many heuristics without theoretical guarantee (i.e., why can they be in theory separated into two independent learning components and what is the maximal number of the hidden nodes in every hidden layer) and they are very difficulty in understanding which theoretical properties are responsible for the improved performance. However, the proposed FKNN architecture has strong theoretical guarantee, i.e., KLA=multi-layer KPCAs+LA, and rich achievements about kernel methods can help us support the suitability of FKNN networks in deep learning. Besides, the proposed FKNN architecture naturally provides a lower level of the hierarchy which can be used as “the last output layer”, as shown in Fig. 4, thus resulting in a range of data representations at varying levels of abstraction.The entire FKNN network behaves like a kernelized learning algorithm KLA since multi-layer KPCAs can be viewed as a special KPCA and KLA=this special KPCA+ LA. The most distinctive advantage of such a multi-layer FKNN network exists in that we may attempt to choose a fast KPCA algorithm and a fast implementation of LA independently to achieve FKNN's fast deep learning. For example, in order to make the time complexity of FKNN's deep learning become linear with N, we may choose the fast KPCA method in [41], which is proved to have O(kN) time complexity, where k is the number of the extracted components, we may also choose ELM and LLM in [14,15,28], linear SVM in [13], KNN [31] and support vector clustering in [42] as the fast learning method in the last layer of the deep FKNN network in terms of the training tasks.Since we can carry out KPCAs in a layer-by-layer way, as seen in the dotted lines in Fig. 4, a potential advantage is that when less abstract features are required, a lower level of the hierarchy can be used as the “output” layer, readily providing a range of feature representations at varying levels of abstraction. The other advantage is that a lower level of the hierarchy can be used as the shared transformed data space for different tasks. Let us keep in mind that multi-layer feedforward neural networks with BP-like learning cannot have this advantage.Obviously, deep FKNN networks are application-oriented and data dependent, However, since they can be constructed in a layer-by-layer way, we can summarize their deep learning using a general deep learning framework DLF below. In other words, when oriented for a specific application, we may design a concrete FKNN and realize its deep learning implementation by instantiating and even simplifying the proposed framework DLF. For example, we may instantiate LA with the classical SVM in DLF. We may also omit the step 4 in DLF in terms of the trade-off between the deep learning performance and the running time. This learning framework consists of four main steps: (1) application-oriented data preparation; (2) realizing the data transformation by multi-layer KPCAs, in a layer-by-layer way from the input layer to the last hidden layer; (3) carrying out a learning algorithm LA on the transformed dataset after multi-layer KPCAs; (4) optimizing the kernel parameter vectors in multi-KPCAs, in terms of the obtained result by the learning algorithm in step (3), in a layer-by-layer way from the last hidden layer to the input layer. More concretely, we summarize the detailed DLF framework as follows.Learning framework DLF for a deep FKNN networkInput:The given dataset, e.g., an image database; Number of the hidden layers, with their respective kernel functions used in multi-layer KPCAs, of a deep FKNN network;Output:The parameters of the output layer determined by a LA, and/or the kernel parameter vectors in every hidden layer of a deep FKNN network;Step 1:Preprocess the given dataset form the input dataset for the deep FKNN network; fix the number of the layers in the FKNN, and the type of a kernel function in each hidden layer of the deep FKNN network;Step 2:Carry out a full-rank or low-rank KPCA with the chosen kernel function and its initial kernel parameter vector in every hidden layer, in a layer-by-layer way from the input layer to the last hidden layer. If a full-rank KPCA is taken, the number of the hidden nodes in a hidden layer is automatically determined by the full-rank KPCA. Otherwise, fix the chosen top-k principal components as the number of the hidden nodes, in terms of the extracted principal components by a low-rank KPCA.Step 3:Carry out the chosen learning algorithm LA in the output layer on the transformed dataset after multi-layer KPCAs such that the parameters in the output layer are determined and the kernel parameter vectors in the last hidden layer are optimized.Step 4:Determine an appropriate kernel parameter vector in every hidden layer by:(1) the grid search in every hidden layer in a layer-by-layer way from the last hidden layer to the input layer.;Or (2) choose certain optimization method with certain performance criterion (e.g. MSE) and the initial kernel parameter vector to optimize the kernel parameter vector in every hidden layer in a layer-by-layer way from the last hidden layer to the input layer.It should be pointed out that different from the strategies used in the existing typical deep learning techniques [35–40], the mature KPCA techniques can be exploited to play a crucial role in the proposed general deep learning framework DLF. Although both DLF and the existing deep learning techniques can indeed be used to build deep neural networks in a layer-by-layer way, DLF is very different from them in principle. In other words, although existing deep learning techniques have obtained impressive successes, the resulting systems are so complex such that they require many heuristics without theoretical guarantee (i.e., why can they be in theory separated into two independent learning components and what is the maximal number of the hidden nodes in every hidden layer) and that it would be very difficult to understand which theoretical properties are responsible for the improved performance. However, since KLA=multi-layer KPCAs+LA, DLF has strong theoretical guarantee.In order to exhibit the very applicability of the proposed deep FKNN network and the proposed deep learning framework DLF, here we consider a concrete application of image data classification, though there is no reason the DLF could not be applied to other type of data. We also hope to do so in near future. Now, let us state how we can apply DLF to image classification.For a given set of images as the training dataset, in order to do the data preparation well, we use a quad-tree decomposition to subdivide each image recursively, the level of the quad-tree was a set of non-overlapping l×l (In general, we use 4×4) pixel patches. We present the set of all l×l patches from all images in the training dataset to a deep FKNN network as the input data for a deep FKNN network. For simplicity, we adopt Gaussian kernel functions with the same kernel width in every hidden layer, and the top-k eigenvectors as a reduced-dimensionality basis. Each patch is projected into this new basis, and the reduced-dimensionality patches are then joined back together in their original order. The result of this is that each image had its dimensionality multiplied by k/16 (Fig. 5). These reduced-dimensionality images formed the next layer up in the deep FKNN network.This process is repeated, using the newly created layer as the data for the divide-KPCA-union process to create the next layer. At every layer after the first, the dimensionality is reduced by a fixed factor of 4. The process is terminated when the remaining data is too small to split, and the entire image is represented by a single k-dimensional feature vector at the last hidden layer of the deep FKNN network.As an illustration, if our original data is a set of m vectors, each length n (e.g., for 200 16×16 images, m=200, n=16×16=256), then we start with a raw data matrix D0, which is m×n. In case of images, this means each row of the matrix is an image. The level one divided data matrix, D1, in this network is generated by recursively dividing the vectors in D0 into l×l (4×4=16) dimensional patches, so it will be am⋅n16×16matrix. We apply a KPCA to D1, extract the top-k eigenvectors into F1, and use F1 as a basis to project the vectors of D1 onto. Applying the projection to the vectors in D1 results in P1, am⋅n16×kmatrix. Adjacent vectors in P1 are then united (using the inverse of the splitting operator), resulting in D2, which ism⋅n16⋅4×4k.And then, we can continue to apply a KPCA to build current hierarchy on D2. In general, Dtwill bem×n⋅k16⋅4t.When 16·4t=n, the hierarchy (i.e., the deep FKNN's architecture) is complete, giving the last hidden layer with dimensions of m×k. More concretely, based on the above, below we state the DLF-based image classification algorithm in which the instantiation of step 4 in DLF is not considered.DLF based image classification algorithmInput:Given the set D0 of m×n images, the number l of hidden layers in the FKNN, where l=log4(n/16), the number k of eigenvectors to keepOutput:Feature space hierarchy F, projected data hierarchy Dland classification accuracy for Dl.Step 1:Subdivide D0 into D1 as the input data of the deep FKNN network byD1=DivideQuads(D0);Step 2:Obtain the transformed images after multi-layer KPCA byfor t=1 to l doFt=KPCA(Dt, k) with Gaussian kernel function with its kernel width;Pt=FtDtDt+1=UnionQuads(Pt);end forStep 3:Carry out certain learning algorithm such as SVM, KNN and Naive Bayes on Dlto obtain the classification result for Dl.Please note, the DivideQuads function does a quad-tree style division of an image into four equal-sized sub-images, and the UnionQuads function inverts this operation. The function KPCA(Dt, k) computes an eigen-decomposition of the kernelized covariance of Dtand then returns the top-k eigenvectors.

@&#CONCLUSIONS@&#
