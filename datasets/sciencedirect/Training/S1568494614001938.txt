@&#MAIN-TITLE@&#
Growing topology representing network

@&#HIGHLIGHTS@&#
I propose a growing neural network developed from perspective of a generative model.This method has a mechanism generating nodes based on information criterion.This method finds topologies using connected graph-paths between kernels.

@&#KEYPHRASES@&#
Growing Neural Gas,Gaussian mixture model,Online learning,

@&#ABSTRACT@&#
This paper describes a method for finding the topology of a data distribution online using a new growing graph network architecture. Many growing neural networks for finding the topology of data online, such as the Growing Neural Gas, depend on the order and number of input data. For this reason, conventional methods have certain drawbacks: weakness to noise, generating redundant nodes, requiring a great deal of input data, and so on. The proposed method is robust with respect to these issues since it has been developed from the viewpoint of a generative model. This paper presents both the theory and an algorithm in this paper. Moreover, the effectiveness of the proposed method is shown by experiments comparing the proposed method with various growing graph networks.

@&#INTRODUCTION@&#
Finding the topology of input data vectors is important in various applications such as object recognition, character recognition, structure recognition, and so on. Finding the topology of data online is necessary particularly for intelligent agents, because the structure of the data distribution is dynamically transformed by changes in the environment around the intelligent agents.The Growing Neural Gas (GNG) proposed by Fritzke [1] is a typical method for finding the topology of data dynamically using a growing graph network. The network of a GNG has a graph structure where each node and each graph-path represent a reference vector and the similarity between nodes, respectively. The nodes and graph-paths are generated and updated in a self-organizing manner according to the input data vectors. Moreover, the required graph-paths survive automatically. This behavior of an GNG is performed in online learning. In other words, the GNG can dynamically adjust to new input data or transformation of the structure of the data distribution. As such, the GNG can be applied to hand recognition [2], map building by a robot [3], and so on.In past works, various methods other than the GNG have been proposed for finding the structure of a data distribution by growing a graph network. (In this paper, these methods are generically called “growing neural networks”.) The many conventional growing neural networks include building the graph network directly from the observed data vectors. Hence, growing the graph network using a conventional growing neural network is commonly affected by noise. Besides, the learning result of the graph has no consistency, since the growth of the graph depends on the input order of the data. Moreover, conventional growing neural networks tend to generate redundant nodes. In the GNG for example, the number of nodes increases permanently in proportion to the number of learning steps. The reason for this is because conventional growing neural networks represent the distribution structure of the data using a large number of nodes. These characteristics in conventional growing neural networks have a negative influence on their practical application to intelligent agents. For example, redundant nodes waste the limited memory resources of intelligent agents. Moreover, stability of the result is closely associated with the reliability of operating the system for practical purposes.The above problems can mostly be attributed to the fact that the input data vector is used directly in the learning process. To solve this problem, the growth of the graph should be controlled by estimating the generative model representing the distribution structure of the data.The aim of this work is to develop an algorithm for a growing neural network for application to intelligent agents from the perspective of a generative model. In particular, the generation of nodes is controlled by the information criterion. Likewise, the generation and update of graph-paths are controlled by generative models.In this paper, the algorithm for the proposed method is developed based on an online Gaussian mixture model (GMM); henceforth, this proposed method is referred to as the “GTR: growing a topology representing network”. The GMM represents a probability density function (pdf) combining multiple Gaussian kernels. Thus, the GMM builds the generative model represented by the pdf. The GTR is an extension of the GMM that includes the following mechanisms: online learning, generation of Gaussian kernels according to the information criterion, and finding topologies using connected graph-paths between kernels.This paper discusses the theory and an algorithm for the GTR. In addition, results of experiments comparing the GTR with three typical GNNs, namely, the GNG, Evolving Self-Organizing Map (ESOM), and Self-Organizing Incremental Neural Network (SOINN), are presented.If a growing neural network is used by an intelligent robot, the following characteristics are important: ability to operate with limited resources, robustness to noise, and fast creation of a useful graph network. In an autonomous robot that operates in a variety of environments such as on other planets and the sea floor, resources such as memory, are limited. Thus, it is preferable not to generate redundant nodes. Moreover, robustness to noise is required, since noise is often included in sensor information. In addition, a useful graph network should be created quickly from as little information as possible in order to adapt quickly to changes in the environment. The SOINN and ESOM are examples of growing neural networks that can adapt to such situations.Apart from the GNG, the SOINN, proposed by Furao et al. [4] has greater robustness to noise than any of the other growing neural networks. Moreover, in the SOINN, the graph networks are dynamically incremented even if the data are observed from different environments. The SOINN has been adapted for use in various applications including associative memory for noisy input [5], path planning for robots, and so on [6]. The ESOM [7], proposed by Deng et al., supports the feature of a fast growing graph network. Moreover, it is able to find the topology with very limited input data.The above two methods are extremely useful compared with other growing neural networks, and provide very effective techniques depending on the task. In applications for intelligent robots, however, various problems need to be solved. The SOINN requires a lot of learning data to find the topology. In addition, a great many nodes are generated depending on the task. Hence, it is difficult to use the SOINN in practice for the evolution of intelligent agents where resources such as memory, time, and input data are limited. Furao et al. proposed an enhanced self-organizing incremental neural network (ESOINN) which is an improvement of the SOINN. The ESOINN, which is more useful than the SOINN, can perform classifications accurately even if the distribution between classes is close [8]. However, the ESOINN still does not provide a solution to the problems mentioned above. On the other hand, in the ESOM, there are cases where the distribution of the input data cannot be represented appropriately by the generated graph network, since the node increment is controlled by only one threshold value (the radius of the territory at the node).In contrast, the algorithm for the proposed method has been developed from the viewpoint of a generative model to solve these problems. The graph network is generated based on a stochastic approach. Thus, the GNG, ESOM, and SOINN are considered in the comparative experiments. The ESOINN is not used for comparison because the aim of the comparative experiments is to verify the performance characteristic of the proposed method and to highlight the problems of conventional growing neural networks.Various methods for incremental learning based on the GMM have been proposed. Arandjelovic et al. proposed incremental learning for temporally-coherent GMMs [9]. In this method, the GMM is executed with online learning. Moreover, components of the Gaussian kernels are split or merged according to the input data. For this reason, an appropriate number of Gaussian kernels is generated. Bouchachia et al. proposed incremental learning based on growing GMMs [10]. This method also has mechanisms for merging and splitting the Gaussian kernel. Moreover, this method can label each Gaussian kernel using semi-supervised learning. In other words, online estimation of the parameter for the GMM and clustering are performed at the same time. However, these methods cannot perform unsupervised clustering. On the contrary, this is possible in the proposed method, using the generated graph network.The proposed method, the GTR, performs three processes concurrently: (1) online parameter estimation of the GMM, (2) controlling the generation of Gaussian kernels using an information criterion, and (3) generating a graph-path representing the topology between kernels and updating the graph-path's strength. First, a framework for the proposed method is presented, and then each of the above processes are explained in turn.The framework of tasks to which the proposed method will be applied, is given below.•Sequentially-observed data vectors cannot be stored in memory.The appropriate number of Gaussian kernels is unknown. (In the first stage of learning, the number of kernels is one.)The parameters (mixing parameter, and mean and covariance matrices) for each kernel are unknown.The class information for data vectors is unknown.Noise is added to the data vector.The GMM is widely known as a parametric approach for estimating the probability density function and represents the probability density function using a mixture of multiple Gaussian kernels.Now, at time t, let a d-dimensional data vectorxtbe observed. Then, in the GMM composed of K kernels, the probability density function p(xt) is described as(1)p(xt)=∑k=1KπkN(xt;μk,Σk),∑k=1kπk=1.Here, the Gaussian kernel is represented by a normal probability distributionN(xt;μk,Σk), πkis the mixing parameter of the kth kernel, andμkand Σkare the mean and covariance matrices, respectively, in the kth Gaussian kernel.In online parameter estimation for an online GMM, the extremum of the objective function(2)L(πk,μk,Σk)=log∑k=1KπkN(xt;μk,Σk)−λ∑k=1Kπk−1,defined using Lagrange's method for undetermined multipliers, is found using a hill climbing method. Updating expressions for each parameter are defined as follows:(3)μknew=μkold+ϵμ(Σkold)−1γk(xt)xt−μkold,(4)Σknew=Σkold+12ϵΣγk(xt)(Σkold)−1(xt−μknew)(xt−μknew)T(Σkold)−1−I,(5)πknew=πkold+ϵπγk(xt)πkold−1.Here, ϵμ, ϵΣ, and ϵπare the learning rates, such that 0<ϵμ, ϵΣ, ϵπ<1.0. The setting of parameters is done through trial-and-error. Usually ϵμis set to a larger value than ϵΣ and ϵπ. Moreover, γk(xt) is the posterior probability (responsibility) defined as:(6)γk(xt)=πkoldNxt;μkold,Σkold∑k′=1Kπk′oldNxt;μk′old,Σk′old.Generation of the Gaussian kernel is controlled by the Bayesian Information Criteria (BIC) [11]. The BIC are defined as:(7)BIC(K)=−2lnL(K)+2C(K)ln(n).Here, K, L(K), and C(K) are the number of kernels, the maximum likelihood in K kernels, and the degrees of freedom of the model, respectively. Moreover, n is the number of data points. In the GMM, the number of kernels is chosen to maximize the information criteria, BIC(K). In other words, the number of kernels is determined so that the maximum likelihood is given by as few kernels as possible. Generally, in the GMM, the number of kernels is controlled offline using all the input data. However, the GTR controls the generation of the kernel from a single observed data vector in online learning.Suppose that data vectorsx1,x2, …,xT−1 have been observed by time T−1. Now, at time T, let a new input data vectorxTbe observed. At this time, a new kernel is generated if BIC(K+1)>BIC(K), that is,(8)lnL(K+1)−lnL(K)>{C(K+1)−C(K)}ln(T)2,is implemented. L(K+1) is the likelihood of the GMM, in which a new kernel has been added. Here, L(K+1) and L(K) are expressed as:(9)L(K)=∏t=1TpK(xt),(10)L(K+1)=∏t=1TpK+1(xt).However, it is impossible to calculate Eq. (10), since previously processed input data cannot be stored in this framework (see Section “Framework”). Therefore, Eq. (10) is evaluated approximately using the current input data vectorxT. The approximate evaluation of Eq. (10) is computed as:(11)lnpK+1(xT)−lnpK(xT)>{C(K+1)−C(K)}ln(T)2+1.The derivation of Eq. (11) is described below.Let the probability density function pK+1(x) at the addition of a new kernel be defined as follows:(12)pK+1(x)=∑k=1Kπ′kNx;μk,Σk+πK+1Nx;μK+1,ΣK+1.Here, letπ′k=T−1Tπk. Moreover, the initial values of parameters πK+1, μK+1, and ΣK+1 in the new kernel are given by(13)πK+1=1T,μK+1=xT,ΣK+1=αI,where α is an arbitrary invariable.The following equation is derived from Eqs. (10) and (12):(14)L(K+1)=∏t=1T∑k=1Kπ′kNxt;μk,Σk+πK+1Nxt;μK+1,ΣK+1.Here, the second term on the right-hand side suggests that the contribution of prior input dataxt, t=1, 2, …, T−1 can be ignored. Then, Eq. (14) can be approximated as:(15)L(K+1)≃∏t=1T−1∑k=1Kπ′kNxt;μk,ΣkpK+1(xT)(16)=∏t=1T−1T−1TpK(xt)pK+1(xT)(17)=T−1TT−1L(K)pK(xT)pK+1(xT).Therefore,(18)L(K+1)L(K)≃T−1TT−1pK+1(xT)pK(xT).Finally, Eq. (11) is derived by taking the logarithm of both sides of Eq. (18). Here, when T≫1,(19)(T−1)lnT−1T≃−1.Therefore, it is possible to control the generation of the kernel using only the current input data.The GTR simultaneously generates graph-paths between the existing kernels and the new kernel. These graph-paths represent the topology between the data distributions on the Gaussian kernels. In addition, a strength, which can be decreased or increased with learning, is associated with each graph-path.The responsibility is used to generate and update the graph-path. As a result, the strength of the graph-path represents the probability of the existence of data in the space between connected kernels. The strength of the graph-path is an important clue for finding the topology of the data. Next, the underlying theory for “generating the graph-path” and “updating the graph-path” are explained.A graph-path is generated between the additional kernel and both the first and second winning kernels, where the first and second winning kernels, k* and k**, are defined as:(20)k*=arg minkγk(xT),∀k,(21)k**=arg minkγk(xT),∀k∉k*.Moreover, strengthsk*,k**of the generated graph-path between k* and k** is defined as:(22)snewk*,k**=βγk*γk**.Here, β is an arbitrary parameter, such that 0<β<1.0, and is usually set to 0.2.Necessary and unnecessary paths are highlighted by updating the strength of the path through learning. All paths connected to the first winning kernel are updated using the equation below.(23)snewk,k*=(1.0−β)soldk,k*+βγkγk*.The strength of the path is decided based on the greatness of the responsibilities on connected kernels corresponding to the input data. Thus, it can be assumed that the density of data is high in the space between kernels connected by the path. Moreover, the probability that connected kernels belong to the same class is high.The algorithm for the GTR consists of four processes: (1) evaluation process, (2) generation process, (3) update process, and (4) deletion process.The following processes are repeated during learning.The initial number of kernels is 0. When the first observed data vectorx1 is given, the first kernel composed of the initial parameter in Eq. (13) is generated.The probability density function pK(xt) in Eq. (1) and the assumed probability density function pK+1(xt) in Eq. (12) are calculated from the observed data vectorxt. Next, whether to generate a new kernel is determined by Eq. (11). If Eq. (11) is true, then perform the generation process given in “(2) Generation process”, else, perform the update process described in “(3) Update process”.In this process, a new kernel is generated. The initial parameters for the new kernel are set according to Eq. (13). Next, paths are generated between the additional kernel and both the first and second winning kernels using Eq. (22). Proceed to “(4) Deletion process” when the above process terminates.The parameters of the generated kernels are updated by Eqs. (3)–(5). Next, the paths connected to the first winning kernel are updated by Eq. (23). Furthermore, if the first and second winning kernels are not connected, a new path is generated between their kernels. Proceed to “(4) Deletion process” when the above process terminates.A kernel whose mixing parameter is close to 0 is deleted. In this work, a kernel that falls below the threshold parameter is also deleted. This kernel deletion is performed at each learning step.Furthermore, a path whose strength falls below the threshold parameter, is deleted. In this work, the threshold parameter is 0.25. It is recommended that the threshold is determined according to the tasks. This deletion is performed when all learning processes have terminated.Return to “(1) Evaluation process” if a data vector is observed. Otherwise, the learning is complete.In the parameter setting, author has found empirically that it is possible to obtain sufficient performance with the following parameters (if distribution range of the data is at about the same as experiments described below): α=0.15, ϵμ=0.01, ϵΣ=0.0001, ϵπ=0.00001, β=0.2, θ=0.15. Even if the range of data distribution is different, it is possible to obtain a good result generally to just change threshold parameter theta.A brief description of each parameter is described below. Alpha is a parameter related to the range of the initial node. Alpha is setting based on the range of data distribution. If the data are distributed widely, it is able to reduce the computation time by setting to a large initial alpha. ϵμ, ϵΣ and ϵπare related to update rate of the position, range and responsibility in Gaussian kernel respectively. In the guide of the parameter setting, it is better that the update rate of the position is set to larger than other parameters. The learning is stabilized by early moving to near appropriate position. The beta is related in the update speed of the path. If the beta is set to large, the update of the beta become unstable. The threshold theta decides the structure of graph at end of the learning. In the end of the learning, the path of the strength less than the threshold value are removed. If the range of data distribution is large, the final strength of the path are also reduced relatively. Therefore, it is necessary to appropriately set the threshold value corresponding with the range of the data distribution.

@&#CONCLUSIONS@&#
