@&#MAIN-TITLE@&#
Large-scale spectral clustering based on pairwise constraints

@&#HIGHLIGHTS@&#
We face the real-world problem of having a limited set of pairwise constraints.Using pairwise constraints connected components (CC) are generated.The points’ local neighborhoods of the same CC are dynamically adapted.Constraints propagation to CC neighborhoods to increase the clustering accuracy.Scalability is ensured by following a landmark strategy.

@&#KEYPHRASES@&#
Spectral clustering,Semi-supervised,Sparse coding,

@&#ABSTRACT@&#
In this paper, we present an efficient spectral clustering method for large-scale data sets, given a set of pairwise constraints. Our contribution is threefold: (a) clustering accuracy is increased by injecting prior knowledge of the data points’ constraints to a small affinity submatrix; (b) connected components are identified automatically based on the data points’ pairwise constraints, generating thus isolated “islands” of points; furthermore, local neighborhoods of points of the same connected component are adapted dynamically, and constraints propagation is performed so as to further increase the clustering accuracy; finally (c) the complexity is preserved low, by following a sparse coding strategy of a landmark spectral clustering. In our experiments with three benchmark shape, face and handwritten digit image data sets, we show that the proposed method outperforms competitive spectral clustering methods that either follow semi-supervised or scalable strategies.

@&#INTRODUCTION@&#
Spectral Clustering (SC) is a popular approach for solving clustering problems in a wide range of non-Euclidean spaces, linearly non-separable clusters and detecting non-convex patterns (Filippone, Camastra, Masulli, & Rovetta, 2008). SC methods are used in numerous real-world applications such as image segmentation (Tung, Wong, & Clausi, 2010), face recognition (Cevikalp & Triggs, 2010), feature fusion (Huang, Chuang, & Chen, 2012), speech recognition (Iso, 2010), 3D shape retrieval (Tatsuma & Aono, 2009) and protein sequences clustering (Paccanaro, Chennubhotla, Casbon, & Saqi, 2003). The key idea in SC is to achieve graph partitioning by performing eigendecomposition of a graph Laplacian matrix. The SC approach is formulated as follows: given a set of d-dimensional data points1Following standard notations, we use capital italic letters for matrices (e.g. A), lower-case bold letters for vectors (e.g.a) and calligraphic fonts for sets (e.g.A).1{x1,x2,…,xN}∈Rd, SC methods construct an undirected graphG=(V,E), represented by theW∈Rn×naffinity matrix (or the respective adjacency), whereVandEare the sets of vertices and edges, respectively. The goal is to find a k-way partitioning, i.e. k disjoint data subsets whose union is the whole data set, to minimize a particular objective. SC methods firstly calculate the degree matrixD=∑jWji∈Rn×n, a diagonal matrix whose entries are column (or row, since W is symmetric) sums of W. Then, a Laplacian matrix is constructed and its eigenvectors are used as the low k-th dimensional representation of the data. Finally, the k-means algorithm is applied to generate the clusters.Spectral clustering methods differ in how they define and construct the Laplacian matrix and thus which eigenvectors are selected to represent the partitioning, aiming to exploit special properties of different matrix formulations (Filippone et al., 2008; Luxburg, 2007). For the interested reader, Ulrike von Luxburg’s tutorial (Luxburg, 2007) includes examples of different Laplacians’ constructions. Moreover, different objective functions are used to derive the best cut. For example, Ratio Cut (Chan, Schlag, & Zien, 1993) tries to minimize the total cost of the edges crossing the cluster boundaries, normalized by the size of the k clusters, to encourage balanced cluster sizes. Normalized Cut (NCut) (Shi & Malik, 1997) uses the same objective criterion as Ratio Cut, normalized by the total degree of each cluster, making thus the clusters having similar degrees.However, irrespective of the selected approach, there are two important factors for applying a SC method to a real world application: (a) the scalability of the method to large datasets; and (b) the high clustering accuracy.Baseline SC methods (Chan et al., 1993; Shi & Malik, 1997) require (a)O(n2)time to calculate the W affinity matrix and consequently to construct the graphGand the Laplacian matrix L; and (b)O(n3)time to calculate the eigendecomposition of L. Both complexities prohibit the direct application of SC for generating clusters in large-scale data sets. Several accelerated methods (Cao, Chen, Dai, & Ling, 2014; Chen & Cai, 2011; Fowlkes, Belongie, Chung, & Malik, 2004; Liu, Wang, Danilevsky, & Han, 2013; Yan, Huang, & Jordan, 2009) have been proposed in the literature trying to reduce the initial problem size of n data points by selecting p (≪n) samples of the data set.2Additionally, several methods perform parallel SC in distributed systems (Chen, Song, Bai, Lin, & Chang, 2011; Kang, Meeder, Papalexakis, & Faloutsos, 2014) to reduce the computational time of SC.2Accelerated methods in their approximations calculaten×pdistances to constructGand perform the eigendecomposition to a highly reducedL∈Rp×pLaplacian matrix. Consequently, accelerated methods significantly decrease the high complexity of the baseline SC methods (Chung, 1997; Ng, Jordan, & Weiss, 2002).Nevertheless, with respect to the clustering accuracy, accelerated methods either fail in their approximations for a low number of p samples, or do not overcome the limited accuracy of the baseline SC methods. Baseline SC methods tend to unbalanced clusters, i.e. single nodes are separated from the rest of the graph. As a result they are noise-sensitive, i.e. few isolated points can easily draw the cuts away from the global partitions (Chang & Yeung, 2008). Additionally, baseline SC methods cannot exploit the information of a set of data points’ pairwise constraints, in order to increase the clustering accuracy, since they function in an unsupervised manner. Several works have extended SC in a semi-supervised way (Chen & Feng, 2012; Kulis, Basu, Dhillon, & Mooney, 2009; Wagstaff, Cardie, Rogers, & Schroedl, 2001), where the goal is to incorporate prior information into the algorithm, in order to improve the clustering results. This is achieved by adding a preprocessing step, where pairwise must-link (pairs of points that should belong to the same cluster) and cannot-link constraints (pairs of points that should belong to different clusters) are added to theW∈Rn×naffinity matrix. Semi-supervised SC methods achieve higher clustering accuracy compared to conventional SC methods. However, existing semi-supervised SC methods preserve the high complexity of the baseline SC methods and thus are not scalable.The contribution of the proposed method is threefold: (a) the clustering accuracy is increased by injecting prior knowledge of the data points’ constraints to a small affinity submatrix; (b) according to the Tarjan’s algorithm (Tarjan, 1972) connected components (CC) are automatically identified from the data points’ constraints, generating thus isolated “islands” of points. Then, for each CC the local neighborhood of points is adapted dynamically and constraints propagation is performed so as to increase the clustering accuracy; finally (c) the complexity is preserved low, by following a landmark spectral clustering strategy to ensure scalability. In our experiments with three benchmark face, shape and handwritten image data sets, we show that the proposed method outperforms state-of-the-art spectral clustering methods that either follow semi-supervised or scalable strategies in terms of clustering accuracy and computational cost.The rest of the paper is organized as follows: in Section 2 the proposed method is described in detail. In Section 3 the experimental results are presented and discussed, finally, in Section 4 the conclusions of this study are drawn.

@&#CONCLUSIONS@&#
