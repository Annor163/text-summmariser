@&#MAIN-TITLE@&#
A novel state space representation for the solution of 2D-HP protein folding problem using reinforcement learning methods

@&#HIGHLIGHTS@&#
A new state space representation of the protein folding problem in 2D-HP model is proposed for the use of reinforcement learning methods.The proposed state space representation reduces the dependency of the size of the state-action space to the amino acid sequence length.The proposed state space representation also provides an actual learning for an agent. Thus, at the end of a learning process an agent could find the optimum fold of any sequence of a certain length, which is not the case in the existing reinforcement learning methods.By using the Ant-Q algorithm (an ant based reinforcement learning method), optimum fold of a protein sequence is found rapidly when compared to the standard Q-learning algorithm.

@&#KEYPHRASES@&#
Reinforcement learning,Q-learning,Ant colony optimization,Protein folding,2D-HP model,

@&#ABSTRACT@&#
In this study, a new state space representation of the protein folding problem for the use of reinforcement learning methods is proposed. In the existing studies, the way of defining the state-action space prevents the agent to learn the state space for any amino-acid sequence, but rather, the defined state-action space is valid for only a particular amino-acid sequence. Moreover, in the existing methods, the size of the state space is strictly depends on the amino-acid sequence length. The newly proposed state-action space reduces this dependency and allows the agent to find the optimal fold of any sequence of a certain length. Additionally, by utilizing an ant based reinforcement learning algorithm, the Ant-Q algorithm, optimum fold of a protein is found rapidly when compared to the standard Q-learning algorithm. Experiments showed that, the new state-action space with the ant based reinforcement learning method is much more suited for the protein folding problem in two dimensional lattice model.

@&#INTRODUCTION@&#
The protein folding problem is a widely studied optimization problem which is known to be NP-complete. Once the proteins are synthesized, they fold a unique three-dimensional structure that makes them functional or biologically active. The mechanism behind the folding process is still unknown, but there are some mathematical models proposed to simulate the folding process and to find the correct fold of a protein from its amino-acid sequence. Perhaps the most widely studied model is the hydrophobic-polar (HP) lattice model, which is firstly proposed by Dill [1]. In this model, each amino-acid is treated either hydrophobic (H) or polar (P) and represented as a point on a two or three dimensional lattice structure.Lattices are grid like structures that guide the algorithms to form self-avoiding protein configurations, in which each amino-acid in the sequence is mapped to only a particular point on the grid. This mapping process is usually handled in two different ways. In the first one, the amino-acid sequence is considered as a constant chain and folding is performed by iteratively modifying the positions of each amino-acid on the grid without breaking this chain. While in the second one, each amino-acid in the sequence is consecutively added to form continuous and self-avoiding amino-acid chains on the grid which can be considered as a navigation problem or a robot path planning problem.It is shown that, reinforcement learning methods perform well on the solution of the robot path planning problems [2,3]. Thus, in this study the reinforcement learning methods are used for the solution of the protein-folding problem in two dimensional lattice model. There exist many studies [4–9] in literature that proposed different methods for the solution of this problem, but the use of reinforcement learning methods are quite new. In [10–13], authors used the Q-learning algorithm to solve the protein folding problem in two dimensional hydrophobic-polar (2D-HP) model.In order to use the reinforcement learning methods for the solution of the protein folding problem in 2D-HP model, first a state-action space should be defined properly. Thus, each move of the agent on the grid could be easily mapped into the defined state-action space.In the existing studies [10–13], a state-action space is defined for this purpose. However, in these studies the size of the defined state-action space is highly affected by the amino-acid sequence length. As the amino-acid sequence length increases, the size of the proposed state-action space also increases, dramatically. So, even for the small sized amino-acid sequences it is not computationally possible to create the state-action space at the beginning of the algorithm. The only way is to create the state-action space dynamically during the learning process, which is not desirable. Moreover, in these studies the state-action space is created for all amino-acid sequences individually. So, all amino-acid sequences have a unique state-action space and the algorithm must learn all of these state-action spaces separately. By this way, after a learning process, the proposed method could not be able to find the optimal fold of another amino-acid sequence, which conflicts with the philosophy of the term “learning”.In this study, to overcome above mentioned drawbacks a new state-action space is proposed. The proposed state-action space allows the agent to find the optimal fold of any amino-acid sequence (protein) with a particular length. This is achieved by incorporating the “learning” concept to the 2D HP protein folding problem by using the newly proposed state-action space. Moreover, by utilizing a swarm based reinforcement method (Ant-Q algorithm) the optimal fold is found rapidly when compared to the traditional Q-learning algorithm.The remaining part of this paper is organized as follows; in the following section the protein folding problem in 2D-HP model is introduced. In Section 3, existing state space representation of the protein folding problem in 2D-HP model is given. Then, the newly proposed state-action space is introduced. In Section 4, reinforcement learning algorithms, the Q-learning algorithm and the Ant-Q algorithm is given. Section 5 covers the experiments, results and discussions for the proposed method. Finally, Section 6 concludes the work.An amino acid sequence (or chain) is known to be the primary structure of a protein which is synthesized by using the information encoded in genes. This primary structure is then folded into a unique three dimensional structure which makes the protein functional. In literature, the challenge of inferring this three dimensional structure (tertiary structure) from the amino acid sequence is known as the “protein folding problem”. Since the discovery of the proteins three dimensional structures can provide important clues about the functionalities of the proteins, the protein folding problem is of crucial importance to the biological community.There are two main methods to experimentally determine the protein three dimensional structure; X-ray crystallography and NMR spectroscopy, both of which can provide information at atomic resolution. Unfortunately, there exist classes of proteins for which three dimensional structure reconstruction is not possible by using these experimental methods. Moreover, these experimental methods are very expensive and it is usually very time consuming to obtain the 3D structure of a protein by using these methods. For these reasons, computational methods are proposed to find out the three dimensional structure of the proteins from their amino acid sequences. However, it is also a problematic task to find the optimum fold a sequence computationally. Because, the number of possible protein conformations dramatically increases with the increased amino acid sequence length. Therefore, the proposed computational methods should explore the search space efficiently in a reasonable time. In order to achieve this, in computational methods the protein folding process is modeled with some mathematical energy functions. It is thought that, the three dimensional structure of a protein is its native state, which has the lowest possible energy conformation. The problem is thus evolved to find out the lowest energy conformation by minimizing these energy functions.Perhaps the most widely studied model is the hydrophobic–hydrophilic (HP) lattice model in both two and three dimensions, which is firstly proposed by Dill [1]. In this model, each amino acid side chain is classified either as ‘H’ hydrophobic (repelled by water) or ‘P’ hydrophilic or polar (liking water). Dill's survey of proteins identified the interactions between hydrophobic residues (amino acids) to be the dominant force in protein folding [14].Let us, define the primary structure of a protein consists of n amino acid asP. In 2D-HP lattice model this protein could be mathematically defined as below;(1)P=p1p2p3....pn,pi∈{H,P},∀1≤i≤nHere, pi∈{H, P} represents each amino acid in the chain which are either hydrophobic or hydrophilic (polar). A valid protein structure is defined with a functionC, such that each residue of the amino acid chain is mapped to the lattice points in Cartesian coordinates by this function. This could be mathematically defined as in Eq. (2).(2)B={P=p1p2p3....pn|pi∈{H,P},∀1≤i≤n,n∈N}G={G=(xi,yi)|xi,yi∈ℜ,1≤i≤n}C:B→GHere,C:B→Grepresents the mapping process of a residue pi∈{H, P} to a lattice point (xi, yi) in Cartesian coordinates. After this mapping process, for ∀1≤i, j≤n with |i−j|≥2 the energy of the resulting protein structure in 2D-HP lattice model is defined as in Eq. (3).E(C)=∑i,jI(i,j)(3)I(i,j)=−1,ifpi=pj=Hand|xi−xj|+|yi−yj|=10,otherwisewhere (xi, yi) represents the position of the amino acid pi∈{H, P} and (xj, yj) represents the position of the amino acid pj∈{H, P} in Cartesian coordinates. More clearly, the energy function is decreased by 1 for each two amino acids that are mapped byCon neighboring positions in the lattice, but that are not consecutive in the primary structureP. Such two amino acids are called as topological neighbors [12]. In Fig. 1, a sample configuration with energy −9 for the proteinP=HPHPPHHPHPPHPHHPPHPHis given.A valid protein configuration forms a self avoiding path which means, the mapped positions of two different amino acids must not be same in the 2D grid. By considering the resulting self avoiding path, a solution could be represented by an n−1 length sequence π=π1π2π3⋯πn−1, πi∈{L, R, U, D}, ∀1≤i≤n−1 of directions, which encodes the relative positions of the current amino acid to the previous one. Let us consider the configuration given in Fig. 1. The resulting sequence for this protein then could be given as π=RDDLULDLLURURULURRD.In order to study the protein folding problem in 2D-HP lattice model with reinforcement learning methods, first a state-action space that encodes the above mentioned sequence of directions should be proposed. In the following sections, the state-action space defined in the existing studies [10–13] and the state-action space defined in this study is given, respectively. (Note that, throughout the paper same notation is used with the Ref. [11].)The state spaceSproposed by Czibula et al. [10–13] consists of (4n−1)/3 states i.e.S={s1,s2,…,s(4n−1)/3}which is given in Fig. 2. At the beginning, the agent is at state s1. A statesik∈S(ik∈[1,(4n−1)/3])is reached by the agent at a given moment after it has visited statess1,si1,si2,…,sik−1is a terminal state if the number of states visited by the agent in the current sequence is n-1 i.e. k=n−2. A path from the initial to the final state forms a configuration for the proteinP. In Table 1resulting states at each step are given as an example for the protein sequenceP=HPHPPH.The action spaceAconsists of 4 actions L (Left), R (Right), U (Up), D (Down) which are the relative directions of the current position of the agent to the previous one. So, the action space could be given as A={a1, a2, a3, a4}, where a1=L, a2=R, a3=U and a4=D.At a given moment, from a states∈Sagent can move in 4 successor states, by executing one of the 4 possible actions. Thus, the transition probability between a state s and each neighbor state s′ of s is equal to 0.25.In the existing method, the state spaceS(the agent's environment) consists of (4n−1)/3 states i.e.S={s1,s2,…,s(4n−1)/3}. Let us consider the sequence given in Fig. 1 which has a total number of 20 amino acids. So, for this sequence the state spaceSconsists of (420−1)/3=3.665×1011 states. Thus, even to create the state-action space for a sequence length of 20, the required computational time is dramatically huge and it is not possible to create such a big space by an average PC. The only way is to create the state space dynamically, but there is not any information provided about this point in the above mentioned studies. To sum up, in the existing method the size of the state spaceSis strictly depends on the length of the amino acid sequence. As the length of the amino acid sequence n increases, the size of the state space also increases dramatically.Another point that should be comment on is the learning stage of the existing method. Let us again consider the amino acid sequence given in Fig. 1. Since the state spaceSencodes the relative positions of the current amino acid to the previous one, after a learning process all we have is a sequence of directions where the optimal one is given as RDDLULDLLURURULURRD for the sequence given in Fig. 1. Here, it should be noted that at the end of the learning process there is not any information provided about the characteristics of the amino acids whether they are hydrophobic or hydrophilic. This information is totally lost at the end of the learning process. Thus, for an another protein sequence the state-action space must be re-initialized and the agent must learn the environment for this new sequence. This situation conflicts with the philosophy of the term “learning”. Because in a learning process the previous information must be conserved.In the previous section the drawbacks of the existing reinforcement learning method are discussed. In this study, to overcome these drawbacks a new state space representation is proposed. Unlike the existing one, the proposed state-action space comprises the characteristics of the amino acids. By this way, the information stored in the state-action space is conserved. So, there is no need to re-initialize the state-action space for another amino acid sequence.This section mainly covers the definition of the proposed state action space. To allow a comparison to the existing method the proposed state-action space is studied in two different scenarios.Scenario 1In the first scenario, the agent tries to find the optimal policy for only a particular amino acid sequence which is also the case in the above mentioned existing method. In Fig. 3the proposed state-action space is given for this case. As it can be shown in Fig. 3, the new state-action space has a matrix like structure in which each column represents an element of the amino acid sequence that is hydrophobic or polar. Again there are four possible directions {L, R, U, D} that agent can move when it is at a state s.In this case, the total number of statesSconsists of only [4·(n−1)+1] states for a n length amino acid sequence. Let us again consider the amino acid sequenceP1=HPHPPHHPHPPHPHHPPHPHgiven in Fig. 1. The total number of states is [4·(20−1)+1]=77 states, which is very small when compared to the existing one (3.665×1011). For better understanding in Table 2all of the state action pairs are given for a short sequenceP2=HPHPPH.Note that, as in the existing method here also it is not possible to talk about an actual “learning”. Because, the agent learns the space for only the corresponding amino acid sequence and thus, the Q-table only consists of the state-action pairs for this individual amino acid sequence. However, when compared to the existing method, the new state-action space still has advantages. First, the size of the state-action space is reduced dramatically which allows creating the Q-table at the beginning of the algorithm. For the second advantage let us consider the sequenceP2=HPHPPH. SinceP2⊂P1, there is no need to learn the space for theP2again. The Q-table created for the sequenceP1already comprises the solution for theP2. But since the existing method only encodes the directions, it is not possible to deduce whetherP1comprisesP2or not.In the previous scenario, only a particular amino acid sequence is considered that is why the Q-table only consists of the state-action pairs for the corresponding sequence and hence the subsets of this sequence. However, to talk about an actual learning the state-action space should comprise all of the possible combinations of an n length amino acid sequence. In Fig. 4the proposed state-action space for this case is given. Since an amino acid could be either H or P, for a n length amino acid sequence there are 2ndifferent sequences. Thus, the state-action space should be designed to allow an agent to learn all of these sequences. So, after a learning process an agent could find the optimal fold of a sequence with the help of resulting Q-table which covers all of the state-action pairs. In Table 3the state-action space for n=3 is given for better understanding.Note that, in Table 3 the total number of states is 25. But this number must be doubled because the initial state S1 is considered as H but it could also be P. So, in total for n=3 there are 50 states. In this case, the total number of states is given asS=2+4⋅∑n=2N2n, where N represents the length of the amino acid sequence. When compared to the existing method (in which total number of states for n=3 is (43−1)/3=21) for n=3 the size of the proposed state-action space is bigger. However, as n increases the total number of states becomes much smaller than the existing one. For example for n=20 the size of the proposed state-action space isS=2+4⋅∑n=2202n≈8.4×106which is much smaller than the existing one (3.665×1011).As it can be shown, the newly proposed state-action space reduced the size of the state-action space dramatically for both scenarios. However, for Scenario 2 the size of the state-action space is still highly depends on the amino acid sequence length n. But it should be noted that, the proposed state action space allows the agent to predict the optimal fold for n+a length sequences, where a≥1. If the agent learns the space for all of the n length sequences, it will be possible to extend the space and predict the optimal fold for longer sequences. This is another important advantage of the proposed state-action space and could be studied separately.A basic reinforcement learning model consists of a set of environment states, a set of actions, rules of transitioning between states, and rules that determine the scalar reward of a transition. Having defined the state-action spaces, now the transition rules and rules that determine the reward of a transition is given. These parts could be associated to the learning stage of a reinforcement learning method.In [10–13], transition rules and the reward of a transition is defined as below.If the transition generates a configuration that is not valid (i.e. self-avoiding) the received reward is 0.01; the reward received after a transition to a non terminal state is small positive constant greater than 0.01 (e.g. 0.1); the reward received after a transition to a final state πn−1 is the minus energy of the proteinPcorresponding to the path π. These definitions are mathematically given in Eq. (4).(4)r(πk|s1,π1,π2,…,πk−1)=0.01ifaπis not valid−Eπifk=n−10.1otherwisewhere r(πk|s1, π1, π2, …, πk−1) represents the reward received by the agent in state πk, after it has visited states π1, π2, …, πk−1 and Eπrepresents the energy of the configuration formed by the path π.With the above defined transition rules and the reward of a transition, Czibula et al. [10–13] used the Q-learning algorithm as the reinforcement learning method to find the optimal policy.The Q-learning algorithm is known to be performed well on the small-sized state action spaces. However, as the size of the state-action space increases random choices of the actions prevents the agent to converge the optimal solution. To avoid this problem, recently swarm based reinforcement learning methods are proposed [15–20]. In these methods, a number of agents learn concurrently by exchanging the information that they gain during the individual learning.The performances of the swarm based reinforcement learning methods are highly affected by the way of exchanging the information among the agents. In [15], authors proposed an information exchange method based on ant colony optimization [21], which is inspired from behavior of real ants. Real ants deposit pheromone trails over the paths from their nest to the food source. Once the amount of the pheromone trail of a particular path is increased over time, this path becomes more attractive for the members of the ant colony. In [15], the same concept is used for the Q-learning algorithm, which is a widely used reinforcement learning algorithm. In this study, pheromone trails are deposited over the state-action space. Thus, the agents could avoid from random movements which helps to find the optimal policy rapidly, even for the complicated problems. In the above mentioned study, the authors also compared the performance of the proposed ant based reinforcement learning method, namely the PHE-Q method, with the Q-learning algorithm and some other swarm based reinforcement learning methods (BEST-Q, AVE-Q and PSO-Q) that they proposed earlier. The performance of the PHE-Q method is examined by applying it to two shortest path problems. After several experiments, it is observed that the PHE-Q algorithm could find better policies in a small number of episodes when compared to the other methods. In another study, an ant based reinforcement method Ant-Q is proposed for the solution of traveling salesman problem [20]. After several experiments, it is shown that the Ant-Q algorithm with delayed reinforcement is much more efficient than the other well known heuristic methods such as, Elastic Net (EN), Simulated Annealing (SA), Self Organizing Map (SOM), and Farthest Insertion (FI). Although, there are small differences between the Ant-Q and PHE-Q algorithms, in principle both algorithms use the same concept, the pheromone trails, to find out the optimal solution.In this study, the Ant-Q algorithm [20] is used to overcome above mentioned drawbacks of the standard Q-learning algorithm. Different from the existing methods given above, the Ant-Q algorithm uses different state transition rules and rewarding mechanism. Details of the transition rules and the rewarding mechanism can be found in Section 4.2.The Q-learning algorithm is an off-policy algorithm which is proposed to optimize the solutions in Markov decision process problems. It can be proven that, under sufficient training the algorithm converges to a close approximation of the action-value function for an arbitrary target policy. In Table 4a short description of the Q-learning algorithm is given.Ant-Q algorithm is proposed by Gamberdalla and Dorigo [20] for the solution of symmetric and asymmetric travelling salesman problem. The Ant-Q algorithm was inspired by work on the ant system (AS), a distributed algorithm for combinatorial optimization based on the metaphor of ant colonies proposed in [22,23].In this study the Ant-Q algorithm is adopted to the 2D-HP protein folding problem as follows:Let AQ(s, a) be a positive real value associated to the state-action pair (s, a). The AQ-values are the Ant-Q counterpart of Q-learning Q-values and is intended to indicate how useful it is to choose action a when the agent is in state S.Let HE(s,a) be a heuristic value associated to the state-action pair (s,a) which allows an heuristic evaluation of which transitions are better. In this study, the heuristic value is determined by the exponential of the number of new H–H contacts.Let k be an agent whose task is to find the optimum folding starting from the initial state. Associated to k there is the list Jk(s) of allowed states of to be visited, where s is the current state. This list implements a kind of memory, and is used to constrain agents to find feasible configurations, that is self-avoiding. When the agent is in the state s normally there are four possible actions. But since the resulting configuration must be self-avoiding all of these four actions are not always available. Here, Jk(s) stores the states which could be reached by the agent k by performing an available action when the agent in the state s.Based on the above considerations, when the agent is in the state s an action a is chosen with Eq. (5) which is known as the action choose rule (or the state transition) rule.(5)a=argmaxHE(s,u)[AQ(s,u)]δ⋅[HE(s,u)]βifq≤q0rotherwisewhere δ and β are parameters which weigh the relative importance of the learned AQ-values and heuristic values, q is a value chosen randomly with uniform probability in [0, 1], q0 (0≤q0≤1) is a parameter such that the higher q0 the smaller the probability to make a random choice, and r is a random variable selected according to a probability distribution given by a function of the AQ(s, u)’s and HE(s, u)’s, with u∈Jk(s).In Ant-Q algorithm m agents cooperates to find out the best solutions. Ant-Q values are updated by the following equation.(6)AQ(s,a)←(1−α)⋅AQ(s,a)+α⋅(ΔAQ(s,a)+γ⋅Maxz∈Jk(s)AQ(s,z))The update term given in Eq. (6) is composed of a reinforcement and of the discounted evolution of the next state. α and γ parameters are known as the learning step and the discount factor. In Ant-System (AS) and Ant-Q algorithm, the reinforcement term ΔAQ is always zero until the agent forms a complete fold. Different from the Q-learning algorithm in Ant-Q the reinforcement term ΔAQ(s, a) is computed at the end of the agent's tour. The computation method for this delayed reinforcement is given in the following parts of this section.In Table 5a short description of the Ant-Q algorithm is given. In [20] a generic description of the Ant-Q algorithm also can be found. The algorithm given in [20] is called generic because in their studies Gamberdella and Dorigo tried some other action choice rules (pseudo-random, pseudo-random-proportional, and random-proportional) and delayed reinforcement methods (global-best and iteration-best) to further improve the performance. In [20] it is shown that the pseudo-random-proportional action choice rule with the iteration-best delayed reinforcement method performs better when compared to the other options. In Eq. (7), the pseudo-random-proportional action choice rule for the agent k is given.(7)pk(s,a)=[AQ(s,a)]δ⋅[HE(s,a)]β∑u∈Jk(s)[AQ(s,a)]δ⋅[HE(s,a)]βifa∈Jk(s)0otherwisewhere pk(s, a) defines the probability of choosing an action a when the agent k is located in the state s.As mentioned before, different from the Q-algorithm the Ant-Q algorithm uses a delayed reinforcement method. In fact, due to the nature of the protein folding problem in 2D-HP model it is much more convenient to use the delay reinforcement. As in the travelling salesman problem here also the complete tour (or configuration) defines whether the solution is good or not. Thus, instead of the immediate reward, the final configuration should be considered and rewarded. In Eq. (8) the way of computing the iteration-best delayed reinforcement is given.(8)ΔAQ(s,a)=EkibLif(s,a)∈configuration found by the agentkib0otherwisewhereEkibrepresents the energy of the configuration found by the agent k and L represents the length of the amino acid sequence.

@&#CONCLUSIONS@&#
