@&#MAIN-TITLE@&#
Statistical adaptive metric learning in visual action feature set recognition

@&#HIGHLIGHTS@&#
A statistical adaptive metric learning (SAML) is proposed to classify action features.SAML explores multiple statistic combinations for feature sets in different scales.Discriminative statistic subspace is learned by a unified metric learning framework.High competitive performances are achieved by SAML on five benchmark databases.

@&#KEYPHRASES@&#
Feature set classification,Hybrid statistic modeling,Metric learning,Manifold selection,

@&#ABSTRACT@&#
Graphical abstract

@&#INTRODUCTION@&#
Set classification has been studied within computer vision communities for a long period of time. In gait recognition, for example, frame by frame static features of a certain object are considered as a feature set. Similarly in human action recognition, Spatial–Temporal features uniformly extracted from frames of an action atom are considered as a feature set [1]. In addition, image sets have been commonly used in face recognitions [2,3]. The task of feature set classification is to classify an input feature set to one of the sets in the training gallery [4]. Compared to image sets, feature sets are more diverse. They cannot be easily assumed to follow certain distribution or lie in some scale and affine invariant linear subspace. One of the effective techniques handling such problem is by using statistical representations to substitute the original feature samples. For action recognition in the wild scenarios, combinations of statistics from lower-order to higher-order have shown promising representation capabilities, while how to combine these multiple statistics in a near optimal way remains a technical challenge [4,5].In general, three types of statistics have been commonly applied on set modeling, i.e. sample-based statistics (SAS) [6-9], subspace-based statistics (SUS) [10-16] and distribution-based statistics (DIS) [2,3]. Utilizing affine transformation and centroid of samples, sample-based statistics represent d-dimension feature sets with first-order statistics in the Rdspace. A great advantage of SAS is that samples are considered as vectors so the nearest neighbor (NN) classification can be easily implemented with unified distance measures. But sample tests performed at every individual sample are often computationally expensive. Well-known sample-based methods include Minimum Mean Discrepancy (MMD) [17], Affine (Convex) Hull based Image Set Distance (AHISD, CHISD) [18], Set-to-Set Distance Metric Learning (SSDML) [9] and Information Theoretic Metric Learning (ITML) [19]. Differing from sample-based statistics, subspace-based statistics analyze sets lying on a specific Riemannian manifold. By learning the kernel functions or statistical metrics, the subspaces are projected back to the Euclidean spaces. The distance measures from the Riemannian manifold to the Euclidean space is often considered as the true geodesic distances, which lie in a Hilbert space [20]. Distance discriminant functions are then performed on the Hilbert space, and recognitions can finally be achieved by using Nearest Neighbors (NNs) method. Second-order statistic based methods have better representation of the data, but it is hard to design a discriminant function with a unified distance measure for the manifolds. Typical subspace-based methods include Mutual Subspace Method (MSM) [11], Discriminant Canonical Correlations (DCC) [10], Manifold Discriminant Analysis (MDA) [14], Grassmann Discriminant analysis (GDA) [13], Covariance Discriminative Learning (CDL) [15], Localized Multi-Kernel Metric Learning (LMKML) [16] etc. Distribution based statistic model each sample in the feature set with a distribution, which can be expressed as an expansion of the Riemannian manifold from the 2nd-order statistic spaceSymd+toSymd+1+. Such methods are often with 3rd-order statistics and may lead to complex parametric distribution comparison. Typical examples include Single Gaussian Models (SGM) [2], Gaussian Mixture Models (GMM) [3] and kernel version of ITML with DIS-based set model (DIS-ITML). Although 3rd-order statistics model sets with more consolidated representations, the hypothesis tests often require significant amount of computation in distribution comparisons.More adaptive forms of set modeling methods have been proposed by combining multiple statistical metrics in certain heuristic ways. Some of the recent hybrid statistical models include Projection Metric Learning (PML) on Grassmann manifold and hybrid Euclidean-and-Riemannian Metric Learning (HERML) [21]. The main idea of multiple statistic combination is to project measurements from multiple heterogeneous spaces into high-dimensional Hilbert spaces. The key issue then becomes the learning of unified discriminant functions from the training sets. Due to the simplicity of the subspace mapping, discriminant functions can frequently be found from single statistics. For instance, Set-to-Set Distance Metric Learning (SSDML) learns a proper metric between pairs of single vectors in Euclidean space to obtain more accurate set-to-set affine hull based distance for classification; Localized Multi-Kernel Metric Learning (LMKML) maps the 3rd-order statistics into Euclidean spaces by learning a unified metric discriminant function through reproducing kernel Hilbert spaces (RKHS). While hybrid multiple statistics perform well in realistic outdoor scenarios, their unified discriminant functions are often more difficult to design. Addressing the subspace discrimination problems, Shao et al. proposed a kernelized multiview projection (KMP) for action feature set recognition. KMP discriminatively assigns weights to multiple kernelized sets with a single feature to achieve a low-dimensional subspace. However, weighting kernels directly in the linear subspaces is not an optimal way for learning kernelized sets with multiple features in different scales [22].Aiming at classification for sets with multiple features, this paper proposes an adaptive subspace analysis method for learning hybrid statistical metrics. The analyzed single or multiple statistics can be used to classify sets through various feature combinations in different scales. Inspired by the discriminant function design in the second-order based methods, LogDet divergence is introduced as a unified discriminant function for our metric learning. With this discriminant function, our method effectively unifies different statistics into a common measurement. Thus nearest neighbor method can be easily performed for classification. The whole process of modeling and learning consists of several steps. Firstly, heterogeneous statistics including mean, covariance matrix and Gaussian distribution are introduced to project data into high-dimensional Hilbert spaces. Typically, d-dimensional mean vectors represent samples from RdtoSymd+expanded by a Point-to-Set projection; covariance matrices lie in Riemannian manifoldSymd+and multivariate Gaussian distributions expand the second order statistics into Riemannian manifoldSymd+1+expressed by relative entropy. Secondly, by embedding the heterogeneous spaces into high-dimensional Hilbert spaces, the Mahalanobis distance is introduced as our discriminant metric. Then, the Hilbert space selection is conducted based on the minimum Hilbert subspaces. The hybrid statistics are then reduced to single or multiple statistic combination. Finally, with LogDet divergence that maps all the Hilbert space points into Rd, a constrained kernel learning is performed. Recognitions are mainly conducted on video sequences in both static and dynamic scenarios using spatial image features such as edges, SIFT, HOG, and texture features.Let X=[X1,…,XN] denotes the training set formed by N feature sets, whereXi=[x1,x2,…,xM]∈Rni×dindicates the i-th feature set, 1 ≤ i ≤ N, and niis the number of samples in this set. It is known that the kernel function is always defined by firstly mapping the original features to a high dimensional Hilbert space, that is ϕ : Rd→ F or Sym+ → F, and then calculating the dot product of high dimensional statistics Φiand Φjin the new space. Considering ϕ as an explicit mapping with the statistical kernels,Φirdenotes the high dimensional feature of r-th statistics extracted from the feature set Xi. Here, 1 < r < R and R is the number of statistics being used.We uniformly map feature set Xi,1 ≤ i ≤ N with following three statistics: sample-based, subspace-based and distribution-based statistics.Sample-based statistics (SAS): Supported by Bregman divergence, mean vector is considered as one of the important properties describing the probability distributions. It is often used to measure the central tendency of set of samples. Given sample xk∈ Xi, 1 ≤ k ≤ M, the mean vector μiof Xiis computed as:ui=1M∑k=1Mxk.Subspace-based statistics (SUS): Within subspaces derived from eigen-decomposition, set variances are influenced by covariant matrix. Given sample xk∈ Xi, 1 ≤ k ≤ M, the covariant matrix Ciof Xiis computed as:Ci=1M−1∑k=1M(xk−μi)(xk−μi)T.Distribution-based statistics (DIS): Gaussian distribution is a very commonly occurring probability distribution, which is a continuous distribution with the maximum entropy for a given mean and variance. Therefore, the d-dimensional distribution of set Xiis modeled as a Single Gaussian Model (SGM) with an estimated d-dimensional mean vectorm^k,i,1≤k≤dand a covariance matrixĈi:x∼Nm^i,Ĉi.A point μ in the Euclidean space Rdcan be mapped into a symmetrical positive definite matrixμμT|Rd×d,|μμT|>0∈Symd+. For DIS, the space of Gaussian distribution is able to be embedded into a Riemannian manifoldSymd+1+[20].Theorem 1Let G={γ|dx|,x∈Rd} be a space of normal distribution, where |dx| is Lebesgue measure. Then its positive definite affine spaceAffd+has an explicit embeddingAffd+→Symd+1+lying on the Riemannian symmetric space Sld+1/SOd+1 .ProofDenote an affine group of G in Rd: Affd={(m,Q)|x → Qx+m,Q ∈ Gd,m ∈ Rd} acts transitively on G by γ|dx|→ (m,Q) ⊗ γ|dx|, where ⊗ denotes the transitive operator.Assumeγ0|dx|=(2π)−d/2e−12|x|2|dx|as an standard Gaussian distribution on Rd, where π−1(γ0|dx|)=Odis a positive measure. The transitive operation (m,Q) ⊗ γ|dx| can be explicitly written as(1)(m,Q)⊗γ|dx|=(2π)−d/2(detQ)−1e−12|Q−1(x−m)|2|dx|only when detQ > 0. Therefore, we add a restriction θ to keep affine group positive definiteθ:Affd+=(m,Q)|detQ>0. Since the restriction is also transitive on G, whereGd=Affd+/SOd. Considering Affdas a subgroup of a larger simple Lie group SLd+1={V|V∈Rd+1}, where Affd⊂ Sld+1, (m,Q) can be embedded as(2)Affd→Sld+1,(m,Q)→V,(m,Q)→(detQ)−1d+1Qm01Since the simple Lie group Sld+1 acts canonically on all symmetric definite positive matricesSymd+1+with determinant 1. In order to meet the restriction θ of the subgroup Affd, the simple Lie group Sld+1 is mapped to a symmetric definite positive spaceθ˜:Symd+1+=VVT|VVT∈R(d+1)×(d+1),|VVT|>0, where(3)Sld+1→Symd+1+,V→VVT,(m,Q)→(detQ)−2d+1QQT+mmTmmT1.Then the transitive action of the restriction for the simple Lie group Sld+1: Gd+1=Sld+1/SOd+1 lies on the Riemannian symmetric space with SOd+1 -principle bundleθ˜:Sld+1→Symd+1+,V→VVTHence, Theorem 1 is proved.□In DIS,QQT=C^Therefore, after embedding, we have 3 statistical representations:(4)ΦSAS=μμT|μμT∈Rd×d,|μμT|>0(5)ΦSUS=C|C∈Rd×d,|C|>0(6)ΦDIS=VVT|VVT=C^+m^m^Tm^m^T1,VVT∈R(d+1)×(d+1),|VVT|>0whereΦSAS∈Symd+,ΦSUS∈Symd+,ΦDIS∈Symd+1+. The SPD matrices lie in a specific Riemannian manifold, where tangent vector of Sym+ is often utilized as a projection to Rd. For the three different embedded statistics and all their possible combinations, we examine logarithm Hilbert subspace through an eigen-decomposition: log(Φ)=U(log(Λ))UTwhereΦ={ΦSAS,ΦSUS,ΦDIS,ΦSASΦSUS,ΦSASΦDIS,ΦSUSΦDIS,ΦSASΦSUS,ΦDIS} indicates the set of all possible statistic combinations;Λ={ΛSAS,ΛSUS,ΛDIS,ΛSASΛSUS,ΛSASΛDIS,ΛSUSΛDIS,ΛSASΛSUSΛDIS} indicates the set of all the possible eigen-value matrices ofΦ, respectively; U is the decomposed projection matrices. SetΦdescribes eigen-spacesΛof all the possible statistic combinations. The last element utilizes all the statistics, while the first three have only single statistic. The logarithmic eigen-values measures the tangent subspaces of the symmetric positive definite Riemannian manifold. In the mutual subspace method (MSM) and Discriminant Canonical Correlations (DCC) method the diagonal eigen-matrix is also known as canonical correlations [10,23].In our work, subspace with minimum deviation of the canonical correlations is selected from set Φ. The canonical correlation is computed as: cosΘ=diag(logΛ). Subspaces are selected by min {var(cosΘ)}, where var(⋅) calculates the deviation and diag(⋅) denotes the diagonal matrix.After the subspace embedding and selection, N labeled training set X={X1,…,XN} becomes R×N labeled statistical pointsΦ={Φ1,…,ΦR}=Φ11,…,ΦN1,…,Φ1R,…,ΦNRlying on Sym+ Riemannian manifolds, where R is the number of selected statistics and N is the number of sets for training.In the Lie group structure, the Log-Euclidean distance (LED) for Xi,Xj∈ Sym+ is defined as LED (Xi,Xj,t)=exp((1−t)log(Xi)+tlog(Xj)) where exp(⋅) and log(⋅) are matrix exponential and logarithm operator. Assume two Riemaninan points for r-th statisticΦir,Φjrhas two linear discriminative mappingYir,Yjron the domain of matrix logarithms wherelogΦirYir⊥logΦjrYjr, the geodesic distance betweenlogΦirandlogΦjrafter mapping is then computed by Euclidean distance in the domain of matrix logarithms:(7)dΦir,Φjr=∥logΦirYir−logΦjrYjr∥F=trlogΦirYir−logΦjrYjrlogΦirYir−logΦjrYjrTwhere ∥⋅∥Fis the matrix Frobenius norm and tr(⋅) is the matrix trace. Fig. 1shows the mapped subspaces graphically for a cannot-link (i,j) statistic pair. As Fig. 1 shows, θ=∠u′Ov′ is the principle angle. The Riemannian distance actually measures the principle angle ||θ||2. If the two points are similar, the distancedΦir,Φjr=1. If they are dissimilar, distancedΦir,Φjr=−1. The relationship holds only when the assumed linear mapping Yiand Yjare equally orthonormal whereY=Yir=YjrandYirYjr=Id, Idis a d×d identity matrix.Given a scale invariant positive definite matrixAr=YirYjr. It can be approximated with an identity matrix A0=Idwith the distance measurementdArΦir,Φjrconstrained by the must-link and cannot-link training pairsΦir,Φjrfor the r-th statistics.(8)dArΦir,Φjr=δijtrArlogΦir−logΦjrlogΦir−logΦjrTAs a distance discriminant function,dArΦir,Φjrshows outstanding scale invariant properties ranging from [−1,1]. δijis an indicator function for (i,j) pairs. If (i,j) is must-link, δij=1; if (i,j) cannot-link δij=−1. Aris a Mahalanobis-like distance metric approximated with an identity matrix. After the approximation of Ar, the classification can be simply completed by nearest neighbor (NN) method usingdAras its distance discriminant function.As discussed in Section 2.2, the learning problem is to approximate R Mahalanobis-like metrics A={A1,…,AR} with identity matrix A0 for R selected statistics. Two properties must be kept during the matrix approximation: First, Ar,r ∈ [1,R] must be scale invariant; second, Arcan be factorized as two linear mapping Ar=YiYj. Accordingly, Log-Determinant divergence is introduced as our measurement for the positive definite matrix approximation. Log-Determinant divergence is a typical measurement for positive definite matrix nearness problem proved in [24], which has also been proved to be convex and scale-invariant in [25]. The optimization problem is considered as seeking R discriminant metrics to describe the statistical characteristics of the N×R selected statistical feature sets. Finally, our metric learning problem is expressed as(9)minA1≤0,…,Ar≤0,ϵ1R∑r=1RDld(Ar,A0)+γDld(diag(ϵ),diag(ϵ0))s.t.δijR∑r=1RdArΦir,Φjr≤ϵij∀(i,j)δijdenotes the similarity between i-th and j-th kernel. If they are similar, δij=1, otherwise δij=−1. Dld(Ar,A0) is the LogDet divergence measuring the correlation between matrix Arand A0 whereDld(Ar,A0)=trAr,A0−1−logdetAr,A0−1−d. d is the dimension of A. ϵijdenotes the boundaries of similarity and dissimilarity, which is introduced as an upper/lower bound for the i-th and j-th sample covariance [26]. While the covariance between different classes may not always be the same, a trade-off is the self-deviation between ϵ and optimal slack ϵ0, which is supposed to be of unit at principle space. Thus, in order to control such a self-deviation, a second term is introduced as the trade-off correction term, where γ ≥ 0 is a parameter arbitrarily set before the computation. In Eq. (9), the first term of the minimization corresponds to the maximum of inter-class and the second term corresponds to the minimum of intra-class.Fig. 2gives a graphic illustration explaining the streamline of our methods. Firstly, various feature sets are mapped into their statistic subspaces; secondly, by constructing the multiple statistic combinations, some combination is selected based-on its canonical correlation. Then, Riemannian distance is used as a discriminant function measuring the different statistics in the selected combination separately in the heterogeneous Hilbert space. Finally, all the statistics are measured in the Euclidean space through Mahalanobis-like identity matrix approximation with Log-Determinant divergence.The form of Eq. (9) can be considered as Bregman optimizations for R selected statistics [25,27]. With Bregman projection, Eq. (9) has the following Lagrange form:(10)∇DldArt+1=∇DldArt+δijαlogΦir−logΦjrlogΦir−logΦjrT∇Dldϵijt+1=∇Dldϵijt−δijα/γwhere α is the Lagrange multiplier. The first and the second equation are the Bregman projection with respect to the minimizers A1,…,ARand ϵ, respectively. Minimizer Ar(r=1,…,R) and ϵ are then iteratively updated by solving Eq. (10)(11)Art+1=Art+δijα1−δijαdArtΦir,ΦjrArtlogΦir−logΦjrlogΦir−logΦjrTArtϵijt+1=γϵijt/γ+αϵijtSolution to Eq. (10) is provided in Appendix A. For solving the Lagrange multiplier, multiple inequality constrains of Eq. (9) have to be satisfiedδijR∑r=1Rtr(Ar(log(Φir)−log(Φjr))(log(Φir)−log(Φjr))T)≤ϵijfor all (i,j). Thus we introduce dual variablesλij≥0. By meeting the KKT condition, α can then be derived into a Euler–Lagrange equation with non-negative maintaining(12)δijR∑r=1RdArt+1Φir,Φjr=ϵijt+1α=minλij,α,λij=λij−αThe Lagrange multiplier α can be solved with Eq. (12). The proof of uniqueness and solution of α are provided in Appendix A. According to the optimization, statistical kernel Ar(1 ≤ r ≤ R) can be learned in the following algorithm:Algorithm 1Statistical adaptive metric learningIt is essential to point out that our method, statistical adaptive metric learning (SAML) is an adaptive version of hybrid statistical metric learning, e.g. Hybrid Euclidean-and-Riemannian Metric Learning (HERML). HERML individually fixes the number of learning Mahalanobis matrices as 3 for all kinds of feature sets. In HERML,(13)min{var(diag(logΛ))}=logΦSASΦSUSΦDIS=U(logΛSASΛSUSΛDIS)UT.Eq. (13) is arbitrarily set as an implicit condition. Only full statistic combination is used for their set modeling. Different from HERML, better statistic combinations are selected by analyzing the canonical correlations in SAML. Although the optimization looks similar, HERML is dealing with image sets while SAML is handling feature sets. Minor variations will usually not lead to large fluctuations in scale of images, but they may likely lead to large fluctuations in that of feature sets, e.g. HOG and SIFT. HERML individually combines all three metrics, while SAML explores the correlation among these metrics. Therefore, the subspace scaling and effective metric selection in SAML can achieve better performances for various feature sets.Since our methods is described as a Mahalanobis-like metric learning, it is also essential to note that our method clearly has three advantages over ITML. Firstly, the data scale is better kept by the subspace mapping in our method. Secondly, multiple statistic combination makes our method more adaptive to various data than that of ITML. Thirdly, the measurement of Log-Determinant divergence provides a more uniformed framework for identity matrix approximation than the distribution analysis in ITML.In ITML, a multivariate Gaussian distribution is defined directly on data asp(x;A)=1Zexp−12xTAx. Z is the normalizing constant. The probability density p(x;A0) is approximated by the initial density distribution, where A0 is the identity matrix. In ITML, the distance between the two distributions is formulated as the KL-divergence measuring the relative entropy:(14)KL(p(x;A0)∥p(x;A))=∫p(x;A0)logp(x;A0)p(x;A)dx.Then the problem identity approximation is derived as a KL-divergence minimization for ∀(i,j) pairs.(15)minA≤0KL(p(x;A0)∥p(x;A)),s.t.δijxiTAxj≤ϵij,∀(i,j).Finally, Eq. (15) is equivalent to the problem of Log-Determinant distance minimization:(16)minA≤0Dld(A,A0)+Dld(diag(ϵ),diag(ϵ0)),s.t.δijxiTAxj≤ϵij∀(i,j)where ϵijis the slack variable for the (i,j) pairs and ϵ0 is identity. δijis the indicator function for (i,j). If (i,j) are similar, δij=1, if (i,j) are not similar, δij=−1. From Eq. (16), the minimization problem of Eq. (16) in ITML can only be used for learning a single Mahalanobis metric directly on input data. The extension of Eq. (16) is basically constrained by its distribution assumption in Eq. (14). Relative entropy can be computed only when all the data samples are in the same scale. Contrary to ITML, the learning problem proposed in SAML is handling multiple statistics in a uniformed minimization framework. Taking advantage of statistical subspace embedding and Log-Determinant divergence, a scale invariant framework of Eq. (9) proposed in our method is capable of minimizing R Mahalanobis-like matrices and leading to better classification than that of ITML.

@&#CONCLUSIONS@&#
In this paper, we proposed a statistical adaptive metric learning method for feature set modeling and classification. The extensive experiments have shown that our proposed method outperforms several state-of-the-art methods in human action recognition over a variety of public data sets. To our best knowledge, the optimal selection of metric learning combination in the Hilbert spaces has not been investigated before this work. In the future, it would be interesting to expand our method for other possible hybrid metric selections with more robust statistic modeling feature sets in different structures and real-world classifications.