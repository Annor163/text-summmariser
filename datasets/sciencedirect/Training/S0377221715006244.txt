@&#MAIN-TITLE@&#
A method for the updating of stochastic kriging metamodels

@&#HIGHLIGHTS@&#
We propose a method for increasing the prediction accuracy of simulation metamodels.It can be used when simulations are run after the metamodel is initially estimated.We show how stochastic kriging metamodels can be updated with new observations.We recommend using smoothed variance to estimate stochastic kriging metamodels.

@&#KEYPHRASES@&#
Simulation,Forecasting,Simulation metamodeling,Stochastic kriging,

@&#ABSTRACT@&#
Two standard approaches to predicting the expected values of simulation outputs are either execution of the simulation itself or the use of a metamodel. In this work we propose a methodology that enables both approaches to be combined. When a prediction for a new input is required the procedure is to augment the metamodel forecast with additional simulation outputs for a given input. The key benefit of the method is that it is possible to reach the desired prediction accuracy at a new input faster than in the case when no initial metamodel is present. We show that such a procedure is computationally simple and can be applied to, for instance, web-based simulations, where response time to user actions is often crucial.In this analysis we focus on stochastic kriging metamodels. We show that if this type of metamodel is used and we assume that its metaparameters are fixed, then updating such a metamodel with new observations is equivalent to a Bayesian forecast combination under the known variance assumption. Additionally we observe that using metamodel predictions of variance instead of point estimates for estimation of stochastic kriging metamodes can lead to improved metamodel performance.

@&#INTRODUCTION@&#
Many real world systems are very costly or even impossible to experiment with. Consider, for example, the need to predict the performance of a new production plant under different demand scenarios before it is built. In such situations, scientists and practitioners often resort to the use of stochastic simulation models. However, stochastic simulations themselves can also be quite complex and researchers often approximate relationships between their input and output using simpler models called simulation metamodels (Kleijnen, 1987).The literature identifies various reasons why simulation metamodels can be useful for a researcher (Law, 2015). This text focuses on the prediction of stochastic simulation output. In this usage scenario, the key benefit is that we can obtain a forecast from a simulation metamodel much faster than from the original simulation model. Consider, for example, a scenario where one is interested in the mean of a real-valued output of the simulation for a given input. Assume that one simulation run takes 6 seconds and that it is calculated that in order to get an estimate of the mean having the accuracy required by the decision maker it is necessary to perform at least 50 simulation replications for this input. This means that one would have to wait for 5 minutes before obtaining the desired prediction. If a delay of 5 minutes is unacceptable, then the solution is to use a simulation metamodel. Under such a scenario, many simulation runs are precomputed and then a predictive model of the mean of the simulation output, conditional on its input, is built (Kleijnen & Sargent, 2000). When applied to a new input point, a typical metamodel produces its forecast very quickly.The reasoning above describing the motivation for using a metamodel as a surrogate for a simulation has one shortcoming. It has been implicitly assumed that the predictions given by the metamodel approximate the mean output of a simulation with the desired accuracy. It is clear that, in general, this assumption need not hold. The focus of this text is on how one can augment a prediction from an existing metamodel in order to improve its accuracy. The procedure is to perform some additional simulations at a new input for which the prediction is required and then combine them with the forecast obtained from the existing metamodel. The key observation is that the number of additional simulations required to reach a given desired accuracy will usually be less than in the case of not having any metamodel.In terms of the underlying motivation for studying metamodel prediction improvement, in recent years there has been a strong interest in offering simulation as a service over the WWW to end users (Byrne, Heavey, & Byrne, 2010). Take the case of a complex financial simulator which, given an investor’s portfolio allocation decision, produces some measure of its predicted performance. Such simulations can be quite time-consuming. On the other hand users are used to getting responses from the WWW with little delay. It would seem to be an ideal scenario for the application of simulation metamodels. Unfortunately, when faced with a high dimension of input space and high volatility and significant non-linearity of the simulation input–output function, the cost of producing a metamodel having a very good degree of accuracy uniformly over the whole input space may be prohibitive.It is worth noting that even though users want to receive a rapid response from a WWW service then they usually stay idle on the web page for a substantial amount of time due to them analyzing the results obtained. This leads to the following principle of user interaction flow:Step 1)the user requests a prediction;the server rapidly produces a response from a simulation metamodel and additionally provides an evaluation of prediction accuracy;the user studies the results and in the background additional simulations are performed;after some time the user can request (or automatically obtain) an augmented forecast with an improved accuracy.In fact, as is shown in the paper, in the second step the user can be informed how long he or she will have to wait in order to achieve a predefined (for instance twofold) improvement in the forecast accuracy.Note that the scenario described here can be executed independent of the exact delegation of computations between the server and the client (see Byrne et al., 2010 for a discussion of standard web-based simulation deployment architectures). It can be assumed that the metamodel prediction and/or additional simulations are executed in the user’s browser (implemented in JavaScript, for instance) or they can be obtained on request from the server (where there will be a much wider choice of possible computation technologies). In practical applications it is very attractive to delegate additional computations to the client side as this approach ensures much better scalability for websites that can expect heavy usage.The key novelty of the procedure proposed in this paper is that the standard simulation literature assumes that predictions are obtained either from a metamodel or directly from a simulation. In this work we propose a methodology that allows a combination of both approaches.A recent example of simulation benefiting from such an approach is provided by the multi-agent model developed by Scensei Inc. in 2014. It is a model of agricultural production and distribution whereby a policy maker wishing to encourage cultivation of some crops, and discourage cultivation and distribution of others, allocates a budget to a portfolio of programs designed to achieve his or her goals. The policy maker wishes to build different portfolios of programs by changing the input of the simulation and interactively (via a web browser interface) comparing those outputs which are of interest. A brief description of the goals of this model is given at http://goo.gl/YI6RzW.A single run of the agent-based model requires several minutes to compute, which seems a comparatively long time for a policy maker to sit idly in front of a web browser. Additionally, as the simulation is stochastic, it was assessed that at least 10 replications per individual input are required to get a reasonable estimate of the simulation output mean. The initial metamodel was built using 29,700 input combinations with 10 replications per point. The total number of simulation runs was chosen according to the available simulation budget. The web browser interface allows the policy maker to define and compare multiple input combinations at the same time. Using a metamodel, an initial estimate of simulation outputs for the chosen set of input combinations can be obtained in several seconds. Next, when the policy maker wishes to increase the precision of output estimates for some particular input (e.g. after initial screening of numerous options only several interesting alternatives remain) additional simulation runs at points of interest are computed.Of course the web-based simulation scenario described above is not the only one where a combination of metamodel forecasts and simulation output can be of interest. Any application where it is possible to pre-build a metamodel and additionally run some simulations when a prediction is required will benefit from the proposed methodology.In terms of the procedures considered in the paper, two main approaches to augmentation of metamodel predictions are considered.In the first approach we use a metamodel forecast as a Bayesian prior that is updated using the simulation output. This method weights the metamodel prediction with the average of new simulation outputs. Different types of weighting are common techniques in the forecast combination literature, see e.g. Wallis (2011), Mancuso and Werner (2013). The approach taken in this paper can be considered as an adaptation of the method in Winkler (1981) to the simulation metamodeling context. Similar update formulas were proposed recently by Frazier, Powell, and Dayanik (2009) for iterative updating of correlated beliefs in the knowledge-gradient policy for ranking and selection problems. The difference from this work is that Frazier et al. (2009) only considers a finite number of alternatives and concentrates on a very specific model structure (a Bayesian model with multivariate normal beliefs) and assumes one measurement per iteration of the procedure.In the other approach we consider an update of the metamodel with new observations and calculation of a new prediction. In particular in this text we focus on stochastic kriging (Ankenman, Nelson, & Staum, 2010) as a metamodeling technique. In this sense our work can be considered as an extension of Emery (2009), who developed update equations for classical kriging (assuming a deterministic data generating process). However, the results are significantly different since in the deterministic case a single measurement of a new alternative is sufficient to fully know its exact value.In the specific case of the stochastic kriging metamodel we show that updating it with a new observation, while keeping the metamodel metaparameters fixed, produces exactly the same results as Bayesian belief updating (the first approach). By showing this link we uncover properties of stochastic kriging metamodels that were not previously discussed in the relevant literature.In particular we show how to increase computation speed when stochastic kriging models are iteratively updated. Such a need arises for example in simulation optimization based on stochastic kriging (Roustant, Ginsbourger, & Deville, 2012). Secondly, we argue that the stochastic kriging estimation procedure can be improved by replacing the standard point variance estimates in an intrinsic noise matrix (Ankenman et al., 2010 denote this matrix as Σε) by smoothed variance evaluations.It is worth remarking that in the simulation literature the idea of aggregating forecasts from multiple simulation models using different approaches has been considered e.g. Fraley, Raftery, and Gneiting (2010), Merrick (2013), Raftery, Gneiting, Balabdaoui, and Polakowski (2005), Raftery, Keárný, and Ettler (2010), Schefzik, Thorarinsdottir, and Gneiting (2013). However, to the best of our knowledge, the case of combining a metamodel forecast with simulation sampling in a new design point has not been considered to date.The structure of the paper is as follows. In Section 2 we formulate the problem of combined forecasts and review alternative methods allowing them to be obtained for simulation models. Next, in Section 3 we present an example application of the proposed methodology and visualize its implications for the estimation of stochastic kriging metamodels.Following the standard requirements for reproducibility of computational studies (Peng, 2011), the source code for all the simulations presented in Section 3 are available for download at http://bogumilkaminski.pl/pub/metamix.zip. For all implementations we used R version 3.1.0 (R Core Team, 2014) with additional packages: DiceKriging (Roustant et al., 2012), randtoolbox (Dutang & Savicky, 2013) and dplyr (Wickham & Francois, 2014).A simulation model may be construed as a mappingY:D→Y,whereDis a set of inputs to the simulation model andYis a space of normally distributed random variables. This implies that our considerations are restricted to situations where Y(d) is normally distributed for every d. This is a requirement for obtaining analytical solutions. In the case of Y(d) not being normally distributed, the results presented in this paper may be treated as approximations. Furthermore, it is assumed that random variables from the setYhave support on real numbersR. It is possible to extend the presented analysis toRmfor m > 1, but real valued random variables are most common in applications and so we focus only on them.A single run of a simulation ford∈Dproduces a simulation output which is a realization of the random variable Y(d) and will be denoted by yiwhere i is the index of the simulation. Elementd∈Dwill be called the input of the simulation model. It is assumed that we have estimated a metamodelMμ:D→Rexplaining the expected value of Y (the so-called response surface of the simulation model) and a metamodelMσ2:D→Rexplaining the variance of Y.We fixd∈Dfor which a prediction is required by the user and defineμ=E(Y(d))andσ2=D2(Y(d)). In order to simplify the notation in the derivations presented in this section we will be concerned only with a single point d; therefore this is dropped from the notation (it should be stressed, however, that in practice and in Section 3 the method will usually be applied for multiple new points). The values of μ and σ2 are unknown to the researcher and will be estimated using metamodels and information from additional simulations at d.In pointd∈Drequested by the user we denote a metamodel prediction of mean μ as Mμwith an evaluation of the variance of this prediction asVMμ. Formally, we should write Mμ(d) etc., but as has been explained, in this section we are only concerned with metamodel forecasts in single point d so it can be dropped in order to simplify the notation. Furthermore, assume that a metamodel prediction of variance σ2 isMσ2with an evaluation of variance of this prediction denoted asVMσ2. It should be noted that in practice these values will be obtained from metamodels whose parameters are estimated. In such a case it would be more precise to writeV^Mμetc. However, we use the proposed notation in Section 2.3, where we actually perform analysis assuming that the metamodel is known, and in Sections 2.1 and 2.2 the reasoning does not depend on whether the metamodel is known or estimated. Therefore we use the notationVMμetc.As stated in Introduction, we wish to augment metamodel predictions by information from the simulation runs collected after the metamodels were estimated. Assume that we have a result of n IID simulation replications yiat point d which give us an estimator of mean μ:y¯=∑i=1nyi/n.Similarly, the estimator of variance σ2 of the simulation output may be calculated ass2=∑i=1n(yi−y¯)2/(n−1)and the variance of this estimator is approximatelyV^s2=2(s2)2/(n−1). In this formula we employ the assumption regarding the normality of Y(d) so(n−1)s2/σ2follows a chi-squared distribution withn−1degrees of freedom.Recall that we are interested in obtaining the estimate of μ combining the metamodel prediction and information from the new sample. This combined forecast will be denoted as μn(with its variance denoted asVμn) to highlight that it is obtained after collecting n new simulation outputs.Note that in the above formulas we concentrate on repeated runs of the terminating simulation case. However, the analysis can be adjusted to allow for analysis of non-terminating simulations, where an increase in simulation time reduces the variance of the obtained estimates.Proceeding to the discussion of possible methods allowing μnto be obtained, three approaches may be taken: (a) in the Bayesian approach use some estimate of σ2 as a surrogate for its known value, (b) in the Bayesian approach assume that σ2 is unknown and (c) update the metamodel with the new data. These approaches will be outlined in the following sections.We will apply a Bayesian approach. Assume that the output from the metamodel Mμdefines our prior beliefs about μ. Therefore, we take that before running any additional simulations our beliefs about μ haveN(Mμ,VMμ)distribution. Note that here, following an assumption that Y(d) is normally distributed, we conclude that the metamodel estimate of μ is also normally distributed. In fact, for large samples used to build a typical metamodel (such as stochastic kriging), it would be approximately normally distributed even if Y(d) did not meet this assumption.Assume that we have then made n measurements of Y(d) and have observed the sample meany¯. Recall that we have denoted the true (but unknown) variance of Y(d) as σ2. Following standard Bayesian inference with known variance assumption (Gelman, Carlin, Stern, & Rubin, 2004) our posterior beliefs about μ have the following distribution:(1)N(μn,Vμn)=N(1VMμMμ+1σ2/ny¯1VMμ+1σ2/n,(1VMμ+1σ2/n)−1).Note that from this formula we can obtain simple predictions about the reduction ofVMμbefore taking the n samples. For example in order to reduce the standard deviation of the estimator by a factor of 2, we need to samplen=3σ2/VMμtimes. This relationship can be used when interacting with the user to inform him or her about the benefits of performing additional simulations. In fact it is evident that a metamodel prediction can be perceived as if it was obtained from a sample having sizeσ2/VMμ– a value that can thus be called sample size equivalent of the metamodel prediction at point d.The above formula was based on the fact that we assumed that σ2 is known. However, in reality it is unobservable. Two approaches can be taken, either using its surrogate estimate (and ignoring the problem) or treating σ2 as unknown. Let us start with the surrogate estimate approach as it is simpler. We have three basic options which allow us to approximate σ2:Option 1)σ2 ≈ s2 (surrogate from the sample);σ2≈Mσ2(surrogate from the metamodel);σ2≈(Mσ2V^s2+s2VMσ2)(VMσ2+V^s2)−1(mixed surrogate).Option 1 is feasible when n is large, but it will probably not work well for small n (forn=1it is not even possible to apply it). Option 2 is good when n is small as it does not use sample information but there is a risk that the metamodelMσ2can be biased and systematically under- or over-estimate σ2. Option 3 could be called a second-order combining approach as it weights option 1 and option 2 following the formula analogous to the one given by Eq. (1). For small n it gives more weight to option 2 but as n increases option 1 becomes more important. It should be stressed here that unless n is very small (when option 1 is not applicable) our empirical tests (see Section 3) have shown that the choice of one of the options has secondary importance (a much more significant determiner is that the forecasts are combined), as it only influences weighting in Eq. (1).The above weighting scheme possesses one shortcoming: it neglects the fact that σ2 is unknown. This means that we expect every observation to decreaseVμn. However, it is possible that the initial metamodel estimate Mμis very biased or the sample meany¯is a result of a very unfortunate draw. In such cases there may be a gap between Mμandy¯much wider than that expected by the variance estimates. This fact was already apparent from the work of Winkler (1981) concerning the combination of probability distributions, which in fact derives the formula given by Eq. (1) starting from slightly different assumptions, but leading to the same conclusions. Therefore, in the next section a more general case, where knowledge of σ2 is not assumed, is presented.Under the assumption that σ2 is unknown, we apply a Bayesian approach again and use the predictions from metamodels for the mean (Mμ) and standard deviation (Mσ2) to produce our prior beliefs that are later updated by observations. Assume that the prior distribution of (μ, σ2) isN-Inv-χ2, which is a conjugate prior for normal distribution. It has four parameters μ0, κ0, ν0 andσ02,see Gelman et al. (2004):(2)μ|σ2∼N(μ0,σ2/κ0)σ2∼Inv−χ2(ν0,σ02)We set our beliefs about the mean with expectation ofμ0=Mμand an equivalent number of observations of meanκ0=Mσ2/VMμ(it is the sample size equivalent defined in Section 2.1 for option 2). Note that κ0 is set in such a way that if Mμwere estimated with κ0 observations with a measured varianceMσ2then the variance of mean estimator would beVMμ,and therefore this is a reasonable choice of κ0. In addition, μ0 is in line with our definition of a combined forecast as it can be viewed as the combined forecast before any measurements are made.For the beliefs about variance we assumeν0=κ0(values estimated from the same sample) andσ02=Mσ2. The reason for such a choice ofσ02is as follows. We know that the mean ofInv−χ2(ν0,σ02)isσ02ν0/(ν0−2)and its mode isσ02ν0/(ν0+2). Because our metamodel estimate of varianceMσ2will usually be predicted from a mis-specified model (probably such models would assume normality, which is not the case if we consider variance estimators), it is preferable to adopt a conservative approach and assume that the predictionMσ2is a harmonic mean of the mean and mode (as the mode is smaller than the mean for this distribution). Observe, however, that even for moderate values of ν0 the difference is very small. In summary, we have the following assumptions:(3)μ0=Mμκ0=ν0=Mσ2/VMμσ02=Mσ2.Using the prior specification and standard Bayesian inference (Gelman et al., 2004) we can evaluate the posterior parameter μnto get:(4)μn=κ0κ0+nμ0+nκ0+ny¯=1VMμMμ+1Mσ2/ny¯1VMμ+1Mσ2/n.Interestingly it is exactly the same as in the case of the surrogate estimate of metamodel variance using option 2. This result is again in-line with Winkler (1981), where a similar relationship is also present. However, it is evident, that in our case, if we used option 1 or 3 in the earlier analysis, we would get a slightly different estimate of μn.It is interesting to analyze the value ofσn2,which is directly related to the evaluation of the varianceVμnof μn. We have:(5)σn2=1ν0+n(ν0σ02+s2(n−1)+nκ0κ0+n(μ0−y¯)2).From this we infer that the distribution of the forecast is t-Student withν0+ndegrees of freedomtν0+n(μn,σn2/(κ0+n)). So we derive that its variance is equal to:(6)Vμn=1κ0+n(κ0Mσ2+s2(n−1)κ0+n−2+(Mμ−y¯)22+κ0−2n+n−2κ0).Recall that the estimate in the variance surrogate case given by Eq. (1) is:(7)1σ2/VMμ+nσ2.Thus one may observe that the fractions at the beginning of both expressions are roughly similar (identical if option 2 is taken) and the first factor in the parentheses plays the role of σ2 from option 3. The final part of Eq. (6) is not present in formula (7) and captures the cases of uncertainty introduced by large deviations between the predicted (Mμ) and observed (y¯) means – which was ignored in the surrogate models. However, it should be stressed that the fact that σ2 is unknown influences the uncertainty estimate. The predicted mean is identical to option 2.Now we will consider an approach where the original metamodelMμ:D→Ris updated with sample information. This means that we will add n sampled points (d, yi) to a training data set and reestimate the metamodel and make a prediction at point d.Such an approach is of course possible (and in fact applied in simulation optimization using the metamodeling approach), but is much more complex computationally than the simple approaches presented in earlier sections. A full estimation of a metamodel can be very expensive (for large training samples it can be even more expensive than running a single simulation) or even unfeasible (for instance complex metamodels estimation procedures might not be available for client side computations due to the need for domain-specific languages or significant memory requirements). Therefore, in this paper we consider the case where we only partially update the metamodel with new data – in particular we assume that its parameters are not reestimated. It will be shown how this concept can be implemented in the stochastic kriging case – a leading metamodeling technique, although it should be noted that the idea presented here is not limited to stochastic kriging. Many non-parametric predictive models have some metaparameters that would not have to be estimated in a similar fashion – for example: the smoothing parameter for smoothing splines or the kernel width in the Nadaraya–Watson estimator and local polynomial regression (Hastie, Tibshirani, & Friedman, 2011).Assume that a metamodel was built with the use of k observations (unique points fromD) using stochastic kriging (Ankenman et al., 2010). We want to add a new point d to a prediction matrix but not to reestimate kriging model parameters. This assumption is reasonable for large k as adding only one new point d does not add much new global information, thus the estimates would not change significantly in any case (even if the sample added a lot of local information). Moreover, because of this assumption we can postulate that the existing metamodel parameters are estimated with high precision, and therefore assuming that they are given (their true values are known) does not introduce a significant degree of error. In particular, in the case where the true values of the metamodel parameters were known, we would not reestimate them after obtaining new information and this is the technical assumption made in this section.Let us now formalize this idea. Assume that we have a stochastic kriging model explaining the simulation model response surface (Mμ:D→Rin our earlier notation) with a constant term and no trend. This is the original setup proposed by Ankenman et al. (2010) (it is an extension of ordinary kriging (Bivand, Pebesma, & Gómez-Rubio, 2013) to stochastic simulation). The notation of Ankenman et al. (2010) is used in this section:ktraining sample size (number of unique points fromD);ΣMk×kcovariance matrix for training sample;τ2constant diagonal element ofΣMinterpreted as overall varianceof Gaussian random field;Σɛk×kdiagonal matrix of variance of intrinsic noise at design points(we assume that the simulation does not use common random numbers);β0constant term of the metamodel;Y¯k–element column vector of observed output means at design points;S2k–element column vector of observed output variances at design points;1kk–element column vector of 1–s;Σdk–element column vector of covariance of training pointswith prediction pointd;Assuming that metamodel parameters are known, the prediction at point d before the measurement is, c.f. Ankenman et al. (2010); Chen and Kim (2014):(8)Mμ=β0+ΣdT(ΣM+Σɛ)−1(Y¯−β01k),and the variance of the prediction is:(9)VMμ=τ2−ΣdT(ΣM+Σɛ)−1Σd.It should be stressed again that the formulas given in Eqs. (8) and (9) are exact for the case where the stochastic kriging parameters are known. In practice β0 is estimated and ΣMand Σdare based on an estimated parameter τ2 and a spatial correlation function that is based on the distance between points inDspace which also has estimated parameters (but note that we do not have to assume thatY¯is known, c.f. Chen and Kim (2014)). Because formulas (8) and (9) are non-linear in parameters and the parameters are usually estimated using MLE, some bias may be expected, see Chen and Kim (2014) for formulas for the stochastic kriging case and Mehdad and Kleijnen (2015) for the discussion of the case of classic kriging. However, if k is high enough the approximation given by formulas (8) and (9) should be reasonable.Here, the given assumption is that after adding new output samples from a new point d we do not update these precomputed values. Again this assumption will be valid if k is large enough such that the new samples have an insignificant weight with respect to the previous evidence. In short, we extend ΣMand Σdwith data from a new point d but assume that β0, τ2 and the spatial parameters of ΣMand Σdare unchanged. In such a case, we obtain the following estimate of the mean at a new design point d after an extension of the stochastic kriging metamodel:(10)μn=β0+[ΣdTτ2][ΣM+ΣɛΣdΣdTσ2/n+τ2]−1[Y¯−β01ky¯−β0],and the variance of the prediction is:(11)Vμn=τ2−[ΣdTτ2][ΣM+ΣɛΣdΣdTσ2/n+τ2]−1[Σdτ2].In the above formulas, just as in Section 2.1, we have used σ2 to denote the true (and unknown) intrinsic variance of Y(d). We can simplify both formulas by noting that:[ΣM+ΣɛΣdΣdTσ2/n+τ2]−1=[(ΣM+Σɛ)−1000]+1σ2/n+VMμ[(ΣM+Σɛ)−1ΣdΣdT(ΣM+Σɛ)−1−(ΣM+Σɛ)−1Σd−ΣdT(ΣM+Σɛ)−11].The equivalence is an application of the Schur–Banachiewicz inverse formula (see for example Henderson & Searle, 1981) combined with Eq. (9).Using this observation, Eq. (10) may be rewritten as:(12)μn=1VMμMμ+1σ2/ny¯1VMμ+1σ2/n.It is interesting to observe that this formula is identical to the result given by Eq. (1), and the final result depends on which surrogate estimate of σ2 is used. Similarly, the variance formula reduces to:(13)(1VMμ+1σ2/n)−1.Once more, it is exactly the same as for the Bayesian treatment where we used a surrogate estimate of σ2.The results obtained mean that the use of stochastic kriging without the updating of the estimated parameters is equivalent to a Bayesian forecast combination under the known variance assumption. This observation has two consequences.Firstly, the calculation of an updated forecast with Eq. (1) is very simple and fast. This is significant because, operating under the assumption that the solution is deployed on the WWW, one can code it in almost any language, such as JavaScript, for example. We do not require a dedicated technology capable of estimating stochastic kriging models and forecasting from them.Secondly, the relationship suggests that it is worthwhile to consider using option 2 (or option 3) of surrogate variance estimates for the stochastic kriging model estimation. As a standard (Ankenman et al., 2010) recommend option 1 (use of variance observed in the sampleS2) to set the diagonal of Σε. In Theorem 1 they show that such an approach does not introduce prediction bias. Let us show that choosing option 2 (use of surrogate variance from a metamodel) also leads to an unbiased estimator.It is necessary to assume that in metamodelMσ2,estimation and prediction only use the vector of observed variancesS2. For example, if we used a stochastic kriging as a metamodel then the prediction formula would have the form:(14)Mσ2=β0σ+ΣdσT(ΣMσ+Σɛσ)−1(S2−β0σ1k),whereβ0σ,ΣMσandΣɛσare estimated parameters of the metamodel andΣdσis a covariance vector for a new prediction point for this metamodel. Note that the above formula is an affine transformation ofS2and its parameters are estimated using only information aboutS2.Now observe thatY¯is independent ofS2under a normality assumption (this fact is used by Ankenman et al. (2010) to prove Theorem 1). We assume that metamodelMσ2is a function ofS2only (in estimation and prediction). This means that predictions from metamodelMσ2are independent ofY¯. Therefore, following the same reasoning as in the proof of Theorem 1 in Ankenman et al. (2010) we see that if option 2 is used to estimate the Mμmetamodel, then its predictions are also unbiased. Observe that the elements ofS2are independent and predictions fromMσ2will not be independent in general, but the proof of Theorem 1 in Ankenman et al. (2010) does not require such independence.The only remaining question concerns the comparison of the mean square error of predictions from metamodels estimated using option 1 and option 2. Ankenman et al. (2010) show that the use of option 1 instead of true variances leads to MSE inflation – see Eqs. (18) and (19). Additionally, if we decrease the variance of the variance estimate then the inflation decreases. Here, it is evident why it might be rational to use option 2 – by using metamodel predictions of variance instead of point estimates of variance we can expect to have a less volatile variance. One precaution has to be made, however. The benefit from the reduced volatility of the variance estimates can be obtained if the estimates produced by the variance metamodel are unbiased. For instance, if we use a standard stochastic kriging metamodel, we have the assumption of normal errors and know that the variance actually has χ2 distribution, so for a small number of samples in each of k points we will have a misspecified model (thus it may be biased). However, as will be seen in Section 3, it is not a severe problem since in such cases the use of a metamodel surrogate instead of observed variance actually gives the largest benefits.In this section we provide two numerical experiments. In the first one we compare the effectiveness of options 1, 2 and 3 in combined prediction of the expected value of simulation outputs. In the second we analyze option 1 vs option 2 as estimation methods of stochastic kriging metamodels.In the example we will assume thatD=[−1,1]m(m denotes the number of dimensions of the simulation input) and consider a simulation specified by the following formulas. Form=1:Y(d)=N(sin(9d2),(2+cos(π(1+d)))2)and for m > 1:Y(d)=N(sin(9d12)+sin((∑i=2m3dim−1)2),(2+cos(∑i=1mπ(1+di)m))2).The idea behind the choice of such a specification was threefold. Firstly, response surface should have strongly varying first and second derivatives. Secondly, it should have a varying variance. Lastly, for m > 1 the influence of variable d1 should be different from the variables difor i > 1. In Fig. 1we plot E(Y(d)) and D2(Y(d)) for the casem=1to illustrate the first two features of the proposed specification of Y(d).Moving on to the description of the experiment design we set m to 1 or 5 to test the proposed method for different dimensions of input parameters set (see Table 1for parameter values and definitions). In each scenario we choosek=100design points and collectl=20samples of simulation output per point. The values of k and l were chosen so as to make the resulting estimates neither too exact or too volatile (in this way we obtain a good balance between the variance ofy¯and Mμin the analysis). Furthermore, as noted earlier, in formulas (8) and (9) we assume that the metamodel parameters are known, so k should not be too low (in particular it is larger than 10 times the dimension of input space, which is a practical rule of thumb, see Loeppky, Sacks, and Welch (2009) for discussion in the case of classic kriging).Design points are generated using Sobol numbers with the Owen–Faure–Tezuka scrambling (Dutang & Savicky, 2013) . Using this training data we estimateMμ:D→RandMσ2:D→Rmetamodels with the stochastic kriging methodology. The implementation used is the DiceKrig package (Roustant et al., 2012). We build the metamodels assuming that there is no trend and use the Matérn(5/2) covariance function. Following this, we chooset=100new design points (again from a scrambled Sobol sequence). In each test point we perform a combined prediction and test it using a varying sample size at new point n ∈ {1, 10, 25, 50, 100, 225, 500}. In each experiment we calculate the following predictions at new points:1.y¯sample mean (denoted ymean in the code);Mμprediction from the stochastic kriging metamodel (denoted ymeankm in the code);μnicombined forecast where i denotes option 1, 2 or 3 (denoted respectively ymeanmix_y, ymeanmix_km and ymeanmix_mix in the code).The implementation of the tests is given in the file experiment.r and is executed by running the experiment.bat file. Analysis of the results is performed by running analyse.r (precomputed results of the experiment are provided in http://bogumilkaminski.pl/pub/metamix.ziparchive).The code allows us also to generate predictions from updated stochastic kriging models denoted respectively ymeankm_y, ymeankm_km and ymeankm_mix. In order to obtain them, a switch xkm when calling mixpred has to be set. However, we leave it in the code only for testing purposes as the results are identical to the combined forecasts as shown in Section 2.3. Similarly, we do not compute predictions for the unknown variance case from Section 2.2 as it was shown to be identical to ymeanmix_km.In one run of the simulation we calculate the square of the deviation of each type of the predictions from the known μ (denoted ytrue in code) averaged over t new design pointsd1,d2,⋯,dt. We further denote it asISE^=∑i=1t(p(di)−μ(di))2/t,where p(di) is a considered prediction at the i-th new data point and μiis the true known expected value of simulation at this point. ISE stands for integrated squared error as the used formula approximates the average squared prediction error overD:∫D(p(x)−E(Y(x)))2dx/∫Ddx.We repeat the whole simulationr=1024times (number of macroreplication) and compute the means of the obtainedISE^s, denoted asIMSE^xwhere x is the type of prediction used.IMSE^stands for integrated mean squared error as the average ofISE^s approximates the average value of the mean squared error overD. This equivalence is a consequence of Fubini’s theorem, see for example Scott and Sain (2005).Table 2presents the comparison results. Before discussing them, a brief comment on the applied statistical testing procedure is in order. For each combination of m and n we have collectedr×t=1024×100observations. However, as stated above, we compute the means taken over t for one simulation run (calledISE^) as a single observation, since they approximate the integrated squared error of the prediction under the assumption of uniform distribution of design points over the simulation model domain. Additionally, in this way we handle the problem that errors within one run of the simulation are correlated – if we take two close new design points they will probably have a similar error. Thus we have 1024 observations ofISE^per combination of m and n. The average of those 1024 observations is denoted asIMSE^. Recall thatIMSE^is observed in five variants:y¯(sample mean), Mμ(metamodel prediction) andμni– combined prediction using option 1, 2 or 3.In Table 2, cells denoted with#indicate row-wise minimalIMSE^up to 4 digits of precision. Because in our caseμn2was always in the optimal set, we perform the t-test with H0 such that the mean difference between this value and the other predictions in each row is equal to 0. Cells indicated by * are the ones where H0 is rejected at a p-value level of 0.01. Because we are performing 52 tests in the table by ** we indicated the cells where H0 is rejected after applying a Bonferroni correction (see, for example, Shaffer, 1995 for the definition and motivation).Firstly, observe that increasing n (number of additional replications in new test point) decreasesIMSE^. This should obviously be expected as we add more and more information about the true value of μ at the test point. This relationship is not observed inIMSE^Mμas this is a value computed from predictions of an original metamodel (without updating). By analyzing columnsIMSE^μniwe notice that the biggest gains from the proposed combined forecasts are whenIMSE^y¯andIMSE^Mμhave the same order of magnitude. Secondly, it can be noticed that the combined predictionμn2was always statistically significantly better than the use of the sample meany¯and better than the original metamodel Mμexcept for the case ofn=1(even in this case the combined prediction has a lower mean, but the difference is small as here there is only a minimum of new information provided by the 1-element sample). We can conclude that, in general, the combined prediction can be recommended.As for comparison of options 1, 2 and 3, they are of comparable magnitude. In our setting we find option 2 to be optimal. Option 3 is comparable (only for large n andm=500it is statistically different). Option 1 is somewhat worse – especially form=5,though it should be noted that this result should be considered test-case specific. Our metamodel of varianceMσ2:D→Ris quite good and not very biased as the variance is a smooth function of the input (although varying in design space, see Fig. 1). In such a situation, using the variance metamodel for weighting gives good results asMσ2compares well toVMμsince they are both smoothed. The sample variance s2 is more volatile and does not relate as well withVMμwhen a combined prediction is made. It would be advisable to use s2 when we did not have any metamodel of variance or we knew that there is a high risk that it would be severely biased. Option 3 (mixed variance) lies in the mid-point between options 1 and 2 and could be recommended when we are unsure of the quality of the variance metamodel yet decide that it is still worth using.The above observations, combined with the results regarding the relationships between combined forecasts and stochastic kriging models updating, lead us to a further conclusion which was already indicated in Section 2.3. When building stochastic kriging metamodels it is worth considering whether to replace the standard surrogate estimates of variance from sample points (option 1), as recommended by Ankenman et al. (2010), by the forecasts taken from the variance metamodel (option 2) or a combination of them (option 3).We perform a simple experiment on our data set to verify this conjecture. We will use the same simulation model and test it in a one-dimensional (m=1) input space. At this point, we simply build a metamodel and later verify its predictions on 500 new points (generated as scrambled Sobol sequences). This time we fix a computing budget at 2000 so we can choose either to have more design points k or more samples per point l. These values must meet the conditionk×l=2000. The fixed number of samples per training point is used here to allow the performed analysis to focus on the properties of the mixing procedure and it is known that, in general, it is not an optimal approach; i.e., it is optimal to allocate more replications to points with higher intrinsic noise. For example, Ankenman et al. (2010) propose a two stage procedure where, after initial screening, an allocation of the remaining computing budget is performed so as to minimizeIMSE^.We compare the stochastic kriging metamodel of the mean estimated using the sample variance as a surrogate (option 1) with the metamodel estimated with the help of the two step procedure: (i) we build a standard stochastic kriging metamodel for the output variance and (ii) we build a stochastic kriging metamodel of the mean using predictions from the variance metamodel as surrogates (option 2). ByISE^iwe denote the average overt=100new design points of the square of the deviation of predictions for options 1 and 2 from the true value of μ. As before, we runr=1024replications of the sample and byIMSE^iands2(ISE^i)we denote the mean and variance ofISE^imeasured in such a sample.Table 3presents the comparison of option 1 and option 2 for building of the stochastic kriging model for different combinations of k and l (implementation of tests is given in sk_compare.r file). We can see that in this case option 2 always outperforms option 1. Additionally, we can observe a U-shaped relationship between l andIMSE^i. Of course l must not be too small, as we do not get enough data to estimate the variance accurately, but interestingly it does not have to be very high either. It is better to invest in having better coverage of the sample space. This observation is similar to the results obtained in two-level simulation modeling literature, see for example Sun, Apley, and Staum (2011). Also Quan, Yin, Ng, and Lee (2013) in a simulation optimization context use a comparable rule for initial iterations of their algorithm.As a final note it should once again be stressed that it will not always be the case that option 2 is superior to option 1 but, if the variance surface is well behaved and can be modeled with a good accuracy, we can expect such a relationship.In the paper we have proposed and analyzed new procedures that allow the combination of metamodel forecasts with additional samples of the simulation model. An important feature of the methodology, based on Eq. (1), is that it is very simple to implement and has a short execution time. Therefore, it can easily be applied in the WWW application scenarios described in Introduction, where client-side technologies (like JavaScript) might have limited support for performing complex numerical computations. In particular, we show that in all the considered methods (Bayesian with surrogate variance, Bayesian with unknown variance and stochastic kriging metamodel partial update) the formula for the combined forecast μnis identical and is an average of the initial metamodel prediction Mμand the new sample averagey¯weighted by inverses of approximations of their true variances.The results presented in Section 2.3 are focused on stochastic kriging metamodels. As noted earlier, however, the idea of a partial update of a predictive model can be applied to different types of non-parametric models. The partial updating approach can be used, for example, in simulation optimization procedures based on metamodels. Note that in this case the benefit is that we do not have to reestimate the metamodel after every iteration of optimization (which is expensive) but merely perform its partial updates at every step and perform full estimation more rarely (for instance every 10 iterations).We have shown that it can be beneficial to estimate stochastic kriging metamodels using smoothed variance estimates (for example taken as forecasts from a variance metamodel as was assumed in Section 3) instead of variances observed at design points. The same idea could be applied to other modeling techniques in which the variance at design points is used in the estimation process.

@&#CONCLUSIONS@&#
