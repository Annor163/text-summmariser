@&#MAIN-TITLE@&#
An evolutionary approach to preference disaggregation in a MURAME-based creditworthiness problem

@&#HIGHLIGHTS@&#
First application of the particle swarm optimization (PSO) to preference disaggregation problems in a MUlticriteria Ranking MEthod (MURAME) framework.Reformulation of the involved constrained optimization problem in terms of penalized unconstrained optimization problem for the PSO implementation.Application to large real credit scoring and credit ranking problems.

@&#KEYPHRASES@&#
Particle swarm optimization (PSO),Multicriteria decision analysis,Preference disaggregation,MUlticriteria RAnking MEthod (MURAME),Credit scoring/ranking problem,

@&#ABSTRACT@&#
In this paper, we propose to use an evolutionary methodology in order to determine the values of the parameters for implementing the MUlticriteria RAnking MEthod (MURAME). The proposed approach has been designed for dealing with a creditworthiness evaluation problem faced by an important north-eastern Italian bank needing to score and/or to rank firms (which act as alternatives) applying for a loan. The point of the matter, known as preference disaggregation, consists in finding the MURAME parameters which minimize the inconsistency between the MURAME evaluations of given alternatives and those properly revealed by the decision maker (DM). To find a numerical solution of the involved mathematical programming problem, we adopt an evolutionary algorithm based on the particle swarm optimization (PSO), which is an iterative metaheuristics grounded on swarm intelligence. The obtained results show a high consistency between the MURAME outputs produced by the PSO-based solution algorithm and the actual scoring/ranking of the applicants provided by the bank (which acts as the DM).

@&#INTRODUCTION@&#
The classical concept of preference disaggregation in multicriteria analysis regards the problem of specifying the preference model of the decision maker (DM) from a given reference set of her/his decisions, so that the evaluations of given alternatives obtained by the adopted multicriteria model is as consistent as possible with the actual evaluations of the DM.In [1] the general philosophy of preference disaggregation is presented, together with a description of the most important results obtained in the development of disaggregation methods over the last two decades. Moreover, recently the connections between the preference disaggregation methods and the machine learning tools have been investigated in [2].According to the various multicriteria methods, the preference disaggregation analysis can be implemented in different ways.For instance, the UTA method,11In short, its purpose consists in valuating given alternatives by assessing various additive utility functions, each of them consistent with the DM's a priori preferences, which aggregate given criteria in a final single evaluation for each considered alternative.one of the most representative example of the preference disaggregation approaches, aims at inferring additive value functions from a given ranking on a reference set by adopting linear programming techniques [3].With regard to the outranking methods,22In short, in these methods a given alternative outranks another alternative if with respect to a meaningful part of given criteria the former performs at least as good the latter, and with respect to the remaining criteria its performances are still acceptable. After the determination of the outranking assessments for each pair of alternatives, these assessments are aggregate in a final single evaluation for each considered alternative.such as those belonging to ELECTRE and PROMETHEE families [4,5], the considered preference model is characterized by several parameters, which consist of various thresholds (preference, indifference, veto) and weights associated to each criteria. The explicit direct determination of these preferential parameters by the DM cannot be considered realistic for several real-world applications, like for example the financial ones. Indeed, the involved institutions do not generally possess the knowledge to handle such quantitative approaches and thus to explicitly provide the values of the criteria thresholds and weights.On this subject, there is a general consensus in the literature to recognize the difficulty for the DM to determine precise values for the preferential parameters (see for instance [3,6–8]. Some possible reasons have been suggested: the DM's weak understanding of what these preferential parameters stand for; the possibility that the DM's preferences change; and the difficulties to achieve consensus in group decisions. Therefore, in all cases where the preferential parameters are not explicitly provided by the DM, the use of preference disaggregation methods may be appropriate to infer the values of the parameters themselves. In particular, much effort has been done in literature to deal with problems related to preference disaggregation in multicriteria outranking models [1]. Recently, some evolutionary algorithms have been used in special contexts. For example, [9] focuses on the multiple criteria classification method PROAFTN and uses an approach based on variable neighborhood search metaheuristic in order to disaggregate preferences. Also [10] handles classification problems, but the authors undertake their analysis in the ELECTRE TRI context. For determining the parameters, they propose to use a procedure based on an evolutionary methodology, namely the differential evolution (DE) algorithm, that allows to obtain a simultaneous estimation of all the parameters of the considered multicriteria model.In this paper, we also use an evolutionary methodology in order to determine the values of the parameters in an outranking method. This approach has been designed in order to deal with a credit scoring problem and a credit ranking one using a large real data set provided by an important north-eastern Italian bank, the Banca Popolare di Vicenza. The main elements of novelty of our paper are: first, we deal with a preference disaggregation problem in the context of MUlticriteria RAnking MEthod (MURAME), a multicriteria methodology developed in [11] and, to our knowledge, a topic not yet explored; then, in order to solve the preference disaggregation problem formulated in the MURAME framework, we employ an evolutionary algorithm for constrained optimization, recently introduced in [12,13], based on the swarm intelligence approach particle swarm optimization (PSO) [14].33As we illustrate in Section 3, the preference disaggregation optimization problem is constrained. But, as known, the PSO was conceived for solving unconstrained optimization problem. So, the PSO is not directly applicable to such a constrained optimization problem. For overcoming this difficulty, we decided to use the above mentioned solution algorithm based on PSO as, under mild assumptions, it is possible to prove that the solutions it provides coincide with those of the original constrained optimization problem.It is to point out that when considering multicriteria outranking models, the search for an optimum solution of the preference disaggregation problem is generally not an easy task because of the complexity of the involved optimization problem. Even more so, focusing on MURAME, the complexity of the optimization problem increases due to the fact that this method manages both the ranking (as done in literature up to now) and also the scoring of the considered alternatives.Before to continue, in order to avoid any possible misunderstanding, it is quite important to note what follows. In the standard financial terminology, by “credit scoring problem” one means the creditworthiness evaluation of applicants for loans.44As known, creditworthiness assessment of debtors and loan applicants is one of the main activities of financial institutions like banks and regulatory authorities. In short, it provides quantities for measuring credit features like the scoring or the rating of obligor quality, the probability that a debtor does not fulfill her/his obligations in accordance with agreed terms, and so on.This evaluation is articulated in two phases: first, scoring the applicants according to their credit risk characteristics; then, sorting them into a prefixed number of homogeneous creditworthiness groups. On the other hand, the expression “credit ranking problem” has not a so precise financial definition, although it concerns issues of the same kind, and it is less used than “credit scoring problem”.In this regard, note that we are not interested in problems of classifying debtors into different homogeneous risk groups (second phase of the credit scoring problem), as it would be usual in the application of classification techniques. Rather, recalling that we are in a multicriteria framework in which the ultimate purpose consists in producing the scoring and the ranking of a set of given alternatives, we focus our attention on the determination of cardinal scores for the applicants (what we call in this paper “credit scoring problem”) and on the determination of ordinal ranks for them (what we call in this paper “credit ranking problem”). These light terminological differences with respect to the standard ones are due to the fact that we act in a strongly multidisciplinary context. Of course, the scoring and the ranking of a set of generic alternatives are strictly connected problems. In fact, once the former is solved, the solution of the latter trivially follows. But, as we explain in Section 3, in evaluating the creditworthiness of loan applicants, the scoring and the ranking of the same set of firms can provide different financial information to the DM.Coming back to our experimental analysis, it is articulated as follows. First, we take into account the problem of scoring and ranking the firms applying for bank loans from the best to the worst according to a score computed through MURAME. As criteria we use a set of indicators supplied by the bank itself. Then, we consider a preference disaggregation problem to determine the bank's preference model. Such an approach consists in determining the MURAME parameters which minimize the inconsistency between the MURAME evaluations of the firms and the evaluations provided by the bank through a hidden internal model. In order to solve the preference disaggregation problem, we employ a recently proposed evolutionary algorithm based on PSO.The remainder of the paper is organized as follows. In Section 2 we briefly describe MURAME. In Section 3 we present the optimization problem that has to be solved to disaggregate the preference structure in a MURAME framework with respect to a credit scoring and ranking problem. In Section 4 we describe PSO and its implementation in a preference disaggregation context again with respect to a credit scoring and ranking problem. We present the application in Section 5. This application is articulated in two steps: first we investigate both the training and predictive performance of the considered approach in relation to the real world application; then, in the second part of the application, we deal with the problem of eliciting the bank's preferences. In Section 6 we conclude with some final remarks.MURAME is a multicriteria methodology that allows to obtain a scoring and consequently a complete ranking of a set of alternatives A={a1, …, ai, …, am}, on the basis of a set of given criteria {crit1, …, critj, …, critn}. In credit scoring and ranking problems, as the one considered in Section 5, the alternatives are the firms applicants for a loan and the criteria are the various indicators according to which the credit risk may be evaluated.MURAME has been proposed in [11] and combines two well known multicriteria methods, namely ELECTRE III [15] and PROMETHEE II [5] ones. Similarly to ELECTRE III, some key features of MURAME are the specification of the thresholds and the weights in the DM's preference model, the adoption of the concordance–discordance principle based on pairwise comparison of the alternatives for each criterion, and the notion of outranking. Moreover, like PROMETHEE II, MURAME aims to compute an overall score according to which a complete ranking of the alternatives is obtained.In this section we briefly introduce the concept of indifference, preference and veto thresholds explicitly considered by the method. Then we summarize the two phases in which MURAME is structured.How to model preferences is a crucial question in decision-making problems. We refer the reader to [16] for an overview of different types of preference structures and for a discussion of the main issues related to preference modeling. We remind that in classical preference systems there are no thresholds and weights, and that the DM, when comparing two alternatives ai, ak∈A, with i, k=1, …, m and i≠k, either states that one alternative is preferred to the other or shows its indifference between them.55For simplicity's sake, in the following of the paper we omit or ease notations of the type ≪with i, k=1, …, m and i≠k≫, unless it creates interpretative problems.There is no uncertainty in judgments.Unlike the approaches based on classical preference structure, ELECTRE III and MURAME make both use of the concepts of indifference, preference and veto thresholds, allowing therefore to consider also the case of hesitation in which the DM is not completely sure to prefer a given alternative to another one. This leads to the concept of “weak preference” which ≪shows the uncertainty on the decision-making between indifference and strict preference≫, as stated in [17].In the following we describe such a non-classical preference structure in which the case of hesitation is taken into account.Denoting by pjthe preference threshold and by qjthe indifference threshold associated to the criterion critj, with 0≤qj≤pj, the following preference relations with respect to critjare considered:aiPak(aiisstrictlypreferredtoak)iffgij>gkj+pjaiQak(aiisweaklypreferredtoak)iffgkj+qj≤gij≤gkj+pjaiIak(aiisindifferenttoak)iff|gij−gkj|≤qj,where ai, ak∈A, gijrepresents the mark of the alternative aiin relation to criterion critj(assumed to be maximized), and P, Q and I indicate the preference, the weak preference and the indifference relation with respect to critj, respectively. Preference models characterized by two preference thresholds can be appropriate to deal with many real-life situations where the human behavior is often imprecise and contradictory [17].MURAME implements such a non-classical preference structure in the two following phases.In the first phase, MURAME aims at defining an outranking relation by building for each ai, ak∈A, with i≠k, an outranking (or credibility) index.Let us start by defining for each criteria the local concordance Cj(ai, ak) and the local discordance Dj(ai, ak) indexes as follows:Cj(ai,ak)=1ifgkj≤gij+qj0ifgkj≥gij+pjgij−gkj+pjpj−qjotherwiseand(1)Dj(ai,ak)=0ifgkj≤gij+pj1ifgkj≥gij+vjgkj−gij−pjvj−pjotherwise,where pjand qjare respectively the preference and the indifference thresholds defined above, andvjin (1), withvj≥pj≥qj≥0, is the so-called veto threshold associated to the criterion critj. In particular,vjrepresents the power given to critjto put its veto when the difference between gkjand gijis greater than itself, forcing the local discordance index to reach its maximal value of 1 [18,11]).Let us continue by building the global concordance index C(ai, ak) by aggregating as follows the local concordance indexes:C(ai,ak)=∑j=1nwjCj(ai,ak),wherewjrepresents the normalized weight associated to criterion critj.Let us conclude by building for each ai, ak∈A an outranking (or credibility) index O(ai, ak) computed as follows:(2)O(ai,ak)=C(ai,ak)ifDj(ai,ak)≤C(ai,ak)∀jC(ai,ak)∏j∈T1−Dj(ai,ak)1−C(ai,ak)otherwise,where T⊆{1, …, n} denotes the subset of criteria for which Dj(ai, ak)>C(ai, ak). We can see that the outranking index is equal to the global concordance C(ai, ak), unless the performance of an alternative with respect to at least a criterion is so bad that it poses a veto to the global outranking relation so that the outranking index decreases. If there is maximum discordance even only for a single criterion, that is Dj(ai, ak)=1 for a given j, the outranking index (2) is equal to zero.In the second phase, MURAME computes for each alternative aithe following final score, the so-called net flow:(3)φ(ai)=∑k≠iO(ai,ak)−∑k≠iO(ak,ai),where O(ai, ak) is the outranking index computed as in (2).We notice that a complete ranking of the alternatives {a1, …, ai, …, am} is obtained by ordering them according to the decreasing values of the final score (3). We denote by rithe rank of the alternative ai.At this point it should be clear that, in order to apply MURAME, some parameters have to be determined, that is:•The vector of weightsw=(w1,…,wn), withwj≥0for all j and∑j=1nwj=1;The vector of indifference thresholds q=(q1, …, qn);The vector of preference thresholds p=(p1, …, pn);The vector of veto thresholdsv=(v1,…,vn), recalling the following relationships among the various thresholds:0≤qj≤pj≤vjfor all j.By doing so it is possible to use (3) to actually compute the values of the net flows φ(ai)≡φ(ai;w, q, p, v) that depend on all the parameters’ values.In the next section we deal with the problem of determining the optimal preference parameters through the preference disaggregation method we propose.The determination of the (w, q, p, v) parameters is a fundamental issue in multicriteria methods like MURAME. They can be obtained using direct or indirect procedures. The former requires that the DM explicitly determines the values of the parameters that express her/his preference structure, the latter pick out such values using preference disaggregation methods based on a reference set of decisions taken by the same DM [1,10]. In this case, a mathematical programming problem has to be solved to infer the parameters so that the obtained model is as much as possible consistent with the scoring and/or the ordering of the reference set provided by the DM.Of course, there exist several operational reasons for which the DM might need to face such a mathematical programming problem. Here we just provide a few examples. For instance, one considers a bank which has always evaluated the loan applicants on the basis of qualitative guidelines. At a certain time this bank, in order to make unequivocal its evaluation policy, decides to develop and of using a quantitative model coherent with the previously used qualitative one. Or, for instance, one considers a bank whose branches follow independent creditworthiness evaluation policies. At a certain time, the bank decides to verify how much these policies are coherent among them. Both these circumstances, and others equivalent, require to deal with preference disaggregation optimization problems.Now let us describe more in detail the logic which leads to a preference disaggregation optimization problem.As we have seen in Section 2, in evaluating a set of alternatives according to a given set of criteria, MURAME allows to obtain a score for each alternative and consequently a complete ranking of all the alternatives, from the best to the worst, thus providing information on the global preferences of the DM about them. Within this context, that is generally known in multicriteria decision-making as “aggregation paradigm” [1], the model is known a priori while the DM's global preferences are unknown.On the other hand, the “preference disaggregation paradigm” acts in an opposite way: the DM's global preferences are supposed to be given and the problem consists in specifying the model (namely the parameters (w, q, p, v) which are essential for implementing MURAME's two phases) from these global preferences. In this case a reference set is usually utilized for determining the DM's global preferences [1]. In order to make clear this aspect, let us suppose that we have a reference set A′ consisting of m′ alternatives on which the DM has expressed its verdicts. The reference alternatives could be either a set of past DM's decisions, or a subset A′⊆A of the whole set of the alternatives, or even some fictitious alternatives which can be easily evaluated by the DM. In particular, we assume that, for the alternatives in the reference set A′, it is available at least a rankr¯ifor each of them provided by the DM, and/or a scoreσ(ai)∈ℝassigned by the DM to each alternative.66As we detail in the final part of this section, it is important to highlight since now that the scoring and the ranking of the same set of firms can give different information to the DM. Because of this, in some cases the DM is interested only in the ranking or only in the scoring (although the ranking originates form the scoring), whereas in other cases the DM is interested in both.In the following, from a terminological point of view, in order to denote the input of the disaggregation process we will use the terms “actual decision”, “actual scoring” or “actual ranking” depending on the context, and “reference scoring” or “reference ranking” depending on the context, in a interchangeable way.The aim of the preference disaggregation approach in a MURAME framework is therefore to use the information gathered from the DM's actual decision (which reflects the DM's global preferences) in order to find the MURAME setting – that is the values of the parameters (w, q, p, v) – which allows to obtain a scoring and/or a ranking of the alternatives which is as consistent as possible with the actual scoring and/or ranking of them indirectly provided by the reference set. Hence, in order to determine the values of the MURAME parameters, the following mathematical programming problem has to be solved, in which a measure of inconsistency is minimized:(4)minw,p,q,vf(w,q,p,v)s.t.w≥0,∑j=1nwj=1q≥0p≥qv≥p,where 0 is a row vector of n zeros, and the so-called fitness function f(w, q, p, v) represents a measure of inconsistency between the scoring or the ranking of the alternatives produced by MURAME with parameters (w, q, p, v) and the reference scoring or ranking of the alternatives provided by the DM. At the end of this Section we propose two fitness functions for measuring such an inconsistency.Some observations need to be made with reference to the preference disaggregation optimization problem defined above:•The first one is related to the veto parameters. In this paper, we assume that all criteria have veto power. Nevertheless, it is possible to extend the formulation of the disaggregation problem to situations in which there is some criterion without thresholds on the veto parameter. To this aim, a vector e of binary variables ej∈{0, 1}, with j=1, …, n, could be introduced in the optimization problem (as done in [10]), distinguishing the criteria with veto power (ej=1) from those without veto (ej=0).77In this way one obtains a mixed-integer mathematical programming problem. Also in this case, as it is in our work, an evolutionary approach could be appropriate.Another observation is related to the input of the disaggregation process. The formulation of the optimization problem (4) implicitly assumes that the reference scoring and/or ranking provided by the DM remains constant during the process. But there exist other approaches, like those based on interactive methods, which alternate a computation phase with a phase of dialogue with the DM through which she/he can update her/his preferences [19,20]. However, the interactive methods may be successful in obtaining satisfactory results assuming that the DM has time and capabilities for co-operation [21], conditions that are not always verified in solving practical problems.From a mathematical point of view, we can note that problem (4) can be reformulated in a simpler way by introducing the auxiliary variables t=p−q and s=v−p, so that it becomes:(5)minw,q,t,sf(w,q,t,s)s.t.w,q,t,s≥0∑j=1nwj=1.This apparently simple mathematical programming problem hides its complexity in the objective function f(w, q, t, s). Indeed every choice of f(·, ·, ·, ·) requires that it produces the scores or an order – depending on the case – of the alternatives according to which a measure of the consistency of the model is calculated. Generally, it is then hard to write an exact analytical expression for f(·, ·, ·, ·) in terms of its variables w, q, t, s, so that the use of gradient methods for the optimization task is discouraged, and an evolutionary approach seems more appropriate.In this contribution, in order to exploit all the information contained in the input data provided by the bank, the scoring and consequently the ranking of a large data set of firms applying for a loan (for details see Section 5), we consider two kinds of fitness function: the first one for minimizing the inconsistency between the rankings, and the second one for minimizing the inconsistency between the scorings.The first fitness function allows to deal with the ordinal rank of the alternatives in the reference set and it is defined as follows:(6)S(w,q,t,s)=6∑i=1m′(r¯i−ri)2m′3−m′,wherer¯iis the rank of alternative aiin the reference set assigned by the DM, rithe one determined by the preference disaggregation process, and m′≤m is the total number of alternatives in the reference set. This fitness function is an application of the Spearman Rank Correlation Coefficient [22] to measure the strength of correlation between the two rankings. Its values are in the interval [0, 2], and S(w, q, t, s)=0 means that there is an exact correspondence between the ranking provided by the DM and that obtained by the model.The second fitness function, which allows to handle cardinal values, is represented by the following Δ function:(7)Δ(w,q,t,s)=∑i=1m′(φ(ai)−σ(ai))2m′,where σ(ai) is the (cardinal) score assigned by the DM to the alternative ai, and φ(ai) is the score determined by MURAME through (3).Similarly to the previous fitness function, Δ(w, q, t, s)=0 means that there is an exact correspondence between the scoring provided by the DM and that obtained by the model.Before continuing, it is important to put in evidence the differences existing between these two fitness functions, and the manner in which we use them.With reference to the differences, at a first glance the fitness function (6) might appear redundant. In fact, the minimization of the function (7) should imply the minimization of the function (6). In general, this is not true. As an exemplification, one can consider the case in which the DM provides a reference set of strongly biased scores. But beyond this formal aspect, in evaluating the creditworthiness of firms applying for bank loans, the scoring and the ranking of the same set of firms can provide different information to the DM, like we premised in Section 1. As an exemplification let us consider the behavior of the DM in two opposite scenarios: strong economic-financial growth, and strong economic-financial crisis. In the first scenario it is quite reasonable to assume that all the applicants, or their big majority, are solvent. So, recalling that generally the DM has no time for unnecessary activities, it should be advantageous for her/him to take into account only the ranking (and some ranking-based rule to allocate the credit). In this case, only the fitness function (6) should be of interest. In the second scenario, the ranking is generally not informative. In fact, even the applicants in the first positions could have so low scores to not be judged solvent. Therefore, in order to avoid worthless risk, it should be advantageous for the DM to use only the scoring (and some scoring-based rule to allocate the credit among the worthy applicants). In this case, only the fitness function (7) should be of interest. Likely, in intermediate scenarios both the fitness functions are of interest.With reference to the way in which to handle these fitness functions, several approaches are possible, included the single or multi-objective ones [23,24]. In the preference disaggregation process we propose in this paper, we consider the following convex linear combination as “global” fitness function:(8)f(w,q,t,s)=αS(w,q,t,s)+(1−α)Δ(w,q,t,s),0≤α≤1,in which the value of α depends on the financial-economic scenario and, of course, on the DM's tastes. Anyway, the determination of the proper value of α is exogenous with respect to the considered preference disaggregation process.In order to make that both metrics have the same range, and therefore to give to α the precise meaning of weight factor, we employed the procedure which follows. From (2) and (3) it is clear that the admissible range for the net flow of each alternative is φ(ai)∈[−(m−1), m−1], where m represents the number of considered alternatives. Therefore, we map the range of the scores σ(ai) assigned by the DM to [−(m−1), m−1] using a linear bijective map, so that transformed scores are in the same range as φ(ai). After this mapping, we note thatφ(ai)−σ(ai)∈[−2(m−1),2(m−1)]∀i,from which∑i=1m(φ(ai)−σ(ai))2∈0,4m(m−1)2.At this point, since it is known that1−6∑i=1m(r¯i−ri)2m3−m∈[−1,1],we also note that∑i=1m(r¯i−ri)2∈0,m(m2−1)3.This suggested us to reformulate the two metrics in the following way:Δ(w,q,t,s)=∑i=1m′(φ(ai)−σ(ai))24(m′−1)2andS(w,q,t,s)=3∑i=1m′(r¯i−ri)2m′2−1,so that their ranges are the same, that is Δ, S∈[0, m′].As far as concerns the choice of the values of α to use in the application we present in Section 5, since the bank providing the data set has shown no preference between the two considered fitness functions, we decided to act as follows. In the first part of the application we investigate the training and predictive performance of our approach for the three cases α=0, α=1, and α=0.5 (see Section 5.1). Then, on the basis of the results reported in that section, we choose one of these three values and we use it in the second part of the application, that is in the problem of eliciting the bank's preferences (see Section 5.2).In order to solve the optimization problem (5), we use a PSO-based solution algorithm (PSO-sa). PSO is a bio-inspired iterative metaheuristics for the solution of nonlinear global optimization problems [14,25]. The basic idea of PSO is to model the so called “swarm intelligence” [26] that drives groups of individuals belonging to the same species when they move all together looking for food. On this purpose, every member of the swarm explores the search area keeping memory of its best position reached so far, and it exchanges this information with the neighbors in the swarm. Thus, the whole swarm is supposed to converge eventually to the best global position reached by the swarm members.From a mathematical point of view, every member of the swarm (formally a particle) represents a possible solution of the investigated optimization problem, and it is initially positioned randomly in the feasible set of the problem. To every particle is also initially assigned a random velocity, which is used to determine its initial direction of movement.In the following of this Section, first we give a short description of the standard PSO metaheuristic in Section 4.1, then in Section 4.2 we present the implementation of our PSO-sa in the investigated preference disaggregation context.Let us consider the following global optimization problem:minx∈ℝdf(x),wheref:ℝd↦ℝis the objective function of the minimization problem. The dimensionality d of the solution space is given by the number of unknown parameters to determine. So, in our specific problem d=4n (we recall that n is the number of the considered criteria). In order to apply the PSO to find numerical solution of the minimization problem, we consider a population constituted by M particles. Note that in our specific problem a particle is a vector of dimension 4n. At the kth step of the PSO algorithm, three vectors are associated to each particle:•xlk∈ℝd, the position at the kth step of the lth particle, with l∈{1, …, M};vlk∈ℝd, the velocity at the kth step of the lth particle;pl∈ℝd, the best position visited so far by the lth particle.Further, let us denote by pbestl=f(pl) the value of the objective function in the position plof the lth particle, and by K the maximum number of iteration allowed. Finally, let us put in evidence that each particle interacts with other ones of the swarm in order to exchange information. The set of involved particles is called neighborhood. This information exchange is formalized in terms of graph in which the particles of the neighborhood constitute the nodes, and the information channels are represented by the edges. The structure of the connections of this graph (for instance, fully connection, ring connection, etc.) specifies the topology of the neighborhood.The PSO, in the version with inertia weight which is one of the most used, works as follows [27,14]:1.Set k=1 and evaluatef(xlk)for l=1, …, M. Set pbestl=+∞ for l=1, …, M.Iff(xlk)<pbestlthen setpl=xlkandpbestl=f(xlk).Update position and velocity of the lth particle, with l=1, …, M, according to the following equations:(9)vlk+1=wk+1vlk+Uϕ1⊗(pl−xlk)+Uϕ2⊗(pg(l)−xlk),(10)xlk+1=xlk+vlk+1,where the parameterwk, the so-called inertia weight, is generally linearly decreasing with the number of steps (i.e.wk=wmax+wmin−wmaxKk),Uϕ1,Uϕ2∈ℝdand their components are uniformly randomly distributed in [0, ϕ1] and [0, ϕ2] respectively (the parameters ϕ1 and ϕ2 are often called acceleration coefficients), the symbol ⊗ denotes component-wise product, and pg(l) is the best position in a neighborhood of the lth particle.88With reference to the application, the values of all the parameters involved in the PSO metaheuristic are suitably specified in Section 5.If a convergence test is not satisfied then set k=k+1 and go to 2.In this contribution, in order to achieve a good compromise between the speed of convergence of the PSO metaheuristic and the risk that the swarm gets trapped in a local optimum, we have considered the so called gbest topology, that is g(l)=g for every l=1, …, M, where g is the index of the better performing particle in the whole swarm, that isg=argminl=1,…,Mf(pl). This choice implies that the whole swarm is used as the neighborhood of each particle. Moreover, as stopping criterion, we decided that the algorithm terminated when the objective function have not a decrease of at least 10−4 in a prefixed number of steps.Since PSO was conceived for unconstrained problems, its direct application cannot prevent from generating infeasible particles’ positions when constraints are considered. To avoid this problem, different strategies have been proposed in the literature, and most of them involve the repositioning of the particles [28] or the introduction of some external criteria to rearrange the components of the particles [29,30]. In this paper, we follow the same approach adopted in [13,12], which consists in keeping PSO as in its original formulation and reformulating the optimization problem (5) into the following unconstrained problem:(11)minw,q,t,sP(w,q,t,s;ɛ)where the objective function P(w, q, t, s;ɛ) is defined as(12)P(w,q,t,s;ɛ)=f(w,q,t,s)+1ɛ[∑j=1nwj−1+∑j=1nmax{0,−wj}+∑j=1nmax{0,−qj}+∑j=1nmax{0,−tj}+∑j=1nmax{0,−sj}],in which ɛ is the so-called the penalty parameter.It is possible to prove, similarly to the results provided in [31,12], that there exists a penalty coefficient value ɛ* such that, for any ɛ<ɛ* the solutions of the unconstrained problem (11) and those of the related constrained problem (5) coincide. Notice that the existence of these theoretical results is the main reason for which we have chosen to use such a PSO-based solution algorithm instead of using other numerical solution methods not characterized by analogous results. Another reason for our choice to use the penalty function approach is due to the fact we do not consider appropriate to employ PSO-based strategies that rearrange the components of the unfeasible particles. In fact, the development of metaheuristics originally designed for a given purpose (namely the unconstrained optimization) for dealing with a further target (namely the management of constraints) generally make them and their settings so problem-dependent to lose the most of their solution capabilities when applied to new optimization problems, even slightly different from the original one. This is not the case of our PSO-sa, as proved in [12].However, the limitation of using a PSO-based solution algorithm, like any other metaheuristic or heuristic optimization methodology, is given by the fact that, although the solution is improved (or not worsened) at every step, there is not a theoretical guarantee that, after an adequate number of steps, a global minimum of the investigated problem is detected. Nevertheless, the PSO-based solution algorithm family often provides a suitable compromise between the performance of the approach (namely a satisfactory estimate of the global minimum solution for problem (11)) and its computational cost.With regard to the initialization procedure, we observe that, since we deal with an unconstrained problem, we could in principle generate initial values for the population in an arbitrary way. Anyway, like any metaheuristic, the performance of the considered PSO-sa could be improved if the choice of the initial population is such that the particles were more or less equi-spread in the feasible region without being concentrated in a restricted area. By doing so the particles can potentially explore larger areas of the solution space, and consequently can find better and better solutions. This happens using the following random selection procedure which is inspired to the one proposed and successfully used in [10].In order to obtain the initial weights, first we generate d1<⋯<dn−1 random numbers uniformly distributed in [0, 1], and then we set:w10=d1,w20=d2−d1,…,wn0=1−dn−1.To obtain the initial values of the variables qj, tj, sj, with j=1, …, n, in order to satisfy the constraints for the thresholds of problem (4) and to have such variables adequately distributed in the range betweeng¯j=max1≤i≤mgijandg_j=min1≤i≤mgij, first we generated three random numbersaj1<aj2<aj3uniformly distributed in [0, 1], and then we set:qj0=aj1(g¯j−g_j),tj0=(aj2−aj1)(g¯j−g_j),sj0=(aj3−aj2)(g¯j−g_j).Finally, for every particlexl0=(wl0,ql0,tl0,sl0), the components of the initial velocityvl0are generated as random numbers uniformly distributed in[−xh0,xh0], with h=1, …, 4n. Preliminary tests have shown that this procedure allows better results than the ones where initial velocities have been set on the basis of more standard procedures.In this section, we apply the above proposed methodology to a credit scoring/ranking problem. In order to evaluate the creditworthiness of a given set of firms, we adopt MURAME as proposed in [32], and use a large real world data set provided by a major bank of north-eastern Italy.The provided data set consists of information on about 12,000 firms applicants for a loan. These firms, which represent the alternatives of the considered creditworthiness evaluation problem, are classified into three groups – small, medium and large – with respect to their business turnover. The three groups are approximately of the same size. The evaluation criteria are represented by seven economic and/or financial indicators {I1, …, I7} computed starting from the balance sheets of the applicants. The values of these indicators are available for two consecutive years, 2008 and 2009. The scores of all the applicants for both the years are also provided (obviously, these scores implicitly specify the ranks of the same applicants in each of the two years).It is important to note that both the indicators and the values of the scores have been directly computed by the bank through proprietary models about which the bank has not given information to us. In other words, we know neither the economic and/or financial meaning of the various indicators nor the model by which the scores have been calculated. It is also important to observe that the number of the firms is so large for both the years that, at least with respect to the north-eastern Italy, this application can be generally considered a good test for our methodology.We have carried out an experimental analysis conducted in two parts which we describe in detail in the following sections. In short, in the first part we investigate both the training and the predictive performance of our methodology in relation to the considered specific real world data set, and in the second part we verify if the PSO-sa is able to find, using a realistic computational time, the values of the parameters of MURAME that make its results consistent with the scoring/ranking of the firms provided by the bank.Before describing in details the procedure implemented and commenting the obtained results, we would like to remark that the disaggregation preference problem in MURAME context is a quite difficult one if compared with other classification-based outranking models: on the one hand because of the intrinsic computational complexity of the calculation of outranking indexes, which requires that each alternative is compared with all the remaining ones; on the other hand, because of the nature of the two metrics considered in this work, since they measure the ranking accuracy of the model, which is clearly more difficult to achieve than the classification one.The aim of this first part of the experimental analysis is to evaluate both the training and the predictive performance of our methodology in relation to the considered specific real world application. In particular: by “training performance” we mean the capability of the methodology to find parameters’ values of MURAME such that the scoring/ranking obtained on a given reference set is the same as the scoring/ranking initially produced on the same reference set; by “predictive performance” we refer to the capability of the methodology, once the parameters have been determined, to produce the same scoring/ranking as the one obtained outside of the reference set.In order to ensure some statistical validation to this first part of the experimental analysis, we employ a bootstrap analysis starting from a sample of 4000 randomly selected firms. This sample is selected in such a way that it contains, in an equally distributed fashion, firms from the three groups following an uniform distribution of the scores assigned by the bank. The bootstrap analysis is structured in the following steps.(i)(Initialization step). The values initially considered for the MURAME parameters (w, q, p, v) are shown in Table 1, and the values initially considered for the parameters ϕ1, ϕ2, K, ɛ,wminandwmaxwhich have been chosen for implementing of the PSO-based solution algorithm are presented in Table 2. For clarity of exposition, we present the approaches by which these initial values have been determined after the description of the bootstrap analysis.A sample of H=100 firms is randomly selected without replacement from the 4000 ones. This sample acts as reference set.By using MURAME with the initial values of parameters showed in Table 1, we calculate for each firm selected as in step (ii) a score computed according to formula (3), thus obtaining a complete ranking of the H firms.(Training step). By using the PSO-sa described in Section 4.2, we determine new (generally good sub-optimal) values for the MURAME parameters99We recall that the values obtained for the MURAME parameters are “generally good sub-optimal” since the PSO-sa is a metaheuristic. In the following of this section, for simplicity's sake we utilize the only term “optimal” for meaning “generally good sub-optimal”.by solving the optimization problem (12). As objective function of this optimization problem we considered the fitness function (8), with α=0, 0.5, and 1. By doing so, we test the training and generalizing performance of our approach for both the two metrics separately, and for their average.(Out-the-bootstrap). Another sample of firms of the same size H is randomly selected from the 3900 ones which are remained after the selection of those belonging to the sample used in the training step. First we apply MURAME with the initial values of the parameters in order to obtain final scoring and ranking for these H firms. Then we apply again MURAME, but now with the values of the parameters as determined in the previous step, so obtaining other final scoring and ranking for the same H firms. The values of the fitness functions S and Δ, and of their average are finally computed in order to respectively evaluate the distance between the two scorings and the two rankings so obtained.We have repeated this bootstrap procedure from step (ii) to step (v) for N=1000 times in order to compute some basic statistics about the MURAME parameters and about the performance measures S, Δ and their average. In Fig. 1we illustrate the flow chart of the bootstrap analysis.Before to present the results, we have to conclude the description of the bootstrap procedure coming back to the initialization step, in order to briefly describe how the initial values of the various parameters have been determined.As regards the initial values of the MURAME parameters (Table 1), they have been calculated according to the following specific rules, as proposed in [32], by using data of the year 2008:wj=1n,qj=16(g¯j−g_j),pj=4qj,vj=5qj,whereg¯j=max1≤i≤mgij, andg_j=min1≤i≤mgij.As regards the initial values of the parameters of the PSO-sa (Table 2), an initial set of experiments has been conducted in order to determine the best values for the acceleration coefficients ϕ1 and ϕ2, the maximum number of steps K, and the value of the penalty parameter ɛ. For the initial and the final values of the inertia weight we adopted the ones most used in the literature. Note that for conducting each of this experiment we have iterated 10 times the above described bootstrap procedure, using as fitness function the average of S and Δ. A little bit more in detail, for any given parameter:•First we have tested different values while keeping fixed the values of the remaining ones;Then we have chosen the value which have allowed to obtain on average the best results for the objective function.For instance, in case of ɛ, first we have considered ɛ=1, ɛ=0.1, ɛ=0.01, …, and for each of these values we have performed 10 runs of the experiment. Then we have chosen the value ɛ=1 that has allowed to obtain on average the best results for the fitness function. The same approach has been adopted in determining the values of ϕ1, ϕ2 and K.Now, let us illustrate the results of the bootstrap analysis. The results are shown for a population size of M=200 and M=400 particles, and for the three fitness functions S, Δ and (S+Δ)/2. In Table 3we report the mean and the standard deviation of the fitness function values obtained at each iteration of the procedure for the training step. In Table 4we present the same quantities for the out-the-bootstrap phase.Note that generally M=200 particles are enough to obtain a good consistency between the outputs of the MURAME model produced by the PSO-sa and the actual scoring and ranking of the alternatives in the bootstrap data. This occurs both in the case of the training step and in the case of the out-of-the-bootstrap data, and for all the considered fitness functions (i.e. S, Δ, andS+Δ2). However, the fitness functions are characterized by different orders of magnitude. We have checked that it does not depend on their goodness, but it is due to the fact that the metric S is minimized over discrete variables (i.e. the ranks), whereas the metric Δ is minimized over continuous variables (i.e. the scores). So, both for M=200 and for M=400, the performances of the three fitness functions are more or less equivalent, although the values obtained by Δ appear better than those obtained byS+Δ2and, in their turn, the values obtained byS+Δ2appear better than the ones obtained by S.Nevertheless, we have also verified that the speed of convergence of S and ofS+Δ2are slower than that of Δ. Indeed, passing from M=200 particles to M=400 particles, Δ shows the highest rate of improvement. For this reason and for the fact that the scores implicitly specify the ranks of the applicants for a loan make us (slightly) lean towards the use of Δ as only fitness function in eliciting of bank's preferences.In Table 5we report the mean values of the MURAME parameters and their standard deviations determined by the PSO-sa in the bootstrap procedure, using respectively S and Δ as metrics, and a population of M=400 particles.There are some interesting things to remark. First, the mean values of the parameters determined using the two metrics respectively are very similar, and they are quite consistent with the actual ones reported in Table 1. Then, as for the values of the MURAME thresholds and weights, there are slightly greater differences, especially in the case of those related to the first indicator. Finally, it generally appears that values determined using Δ are closer to the actual ones: the sum of absolute deviations is 24.19 for Δ, and 25.90 for S.However, given that the sorting/ranking performances of our methodology is high, the (unavoidable) presence of non-zero standard deviations associated to the estimated parameters suggests that, so as to obtain a model consistent with the a priori order of the alternatives, there is a certain flexibility in the specification of the parameters themselves. Of course, in case that it is possible to establish a dialogue with the DM, this remark opens the way to a more precise determination of the considered parameters trough an interactive procedure between the model developers and the DM who could suggest modifications to the parameters values so that, while keeping the consistency with the sample decisions, they better express her/his preference structure.Given the presented results, and as we have already stated, it seems that in the considered application it could be sufficient to use only the metric Δ to obtain a good ranking accuracy of the model. In order to confirm this hypothesis, we have performed the following check. We have computed the average values of S using the best MURAME parameters found by the minimization procedure in the bootstrap analysis in which only Δ has been the metric driving such minimization procedure (considering a population of M=400 particles). We have obtained values of 0.0590 for the training step, and of 0.1076 for the out-of-the-bootstrap data. These results are quite similar to the ones reported for S in Tables 3 and 4. Based on these further findings, we have definitively decided to use only the metric Δ in the elicitation of bank's preferences phase that we illustrate in the next section.In this second part of our experimental analysis, we aim at determining the MURAME parameters of the bank's preference structure for both 2008 and 2009 years, assuming that it is not changed from the first year to the second one. In order to do this, we exploit the information on the score σ(ai) and on the related rankr¯ithat the bank assigned to each firm ai. We recall that σ(ai) represents the bank's score of firm ai.More in details, we want to establish if the PSO-sa is able, starting from a reference set and using a reasonable computational time, to determine optimal values of the MURAME parameters such that the scoring/ranking of the firms obtained through MURAME is as consistent as possible with the scoring/ranking provided by the bank. This analysis has been twice performed using two different reference sets respectively constituted by the data of m′=100 and m′=500 randomly selected firms. For both these sets the data are only from the year 2008. Then, in order to investigate the predictive capabilities of our methodology, we considered as validation set the data of all the available firms from the year 2009. We recall that these firms are classified in three sized-based groups (small, medium and large), and that each of these groups consists of about 4000 units. Finally, we set the number of particles to M=400. Note that in this experimental analysis we have decided to take into account both a large reference set (m′=500) and the largest population size used in the previous analysis (M=400) for investigating the impact of their high numerousness on the quality of the specified MURAME model.In Fig. 2we illustrate the flow chart of this preference disaggregation procedure for the considered real world problem.In Table 6we present the results obtained for the experimental analysis associated only to the small firms. Indeed, there are no important differences between the results obtained for the three groups of considered firms. This is quite reasonable since the firms are evaluated by the bank using the same internal model.In the columns “Ref. set” (Reference set) we illustrate the mean, over 20 runs, of the fitness function values computed by comparing the sorting and the ranking obtained by PSO-sa on the sample of firms used as reference set and the sorting and ranking provided by the bank for this reference set of the small firms. In the analysis we have considered m′=100 and m′=500 firms. The minimization trough PSO-sa has been performed using only Δ, but the results are presented both for S and for Δ. In particular, the values for S have been computed at the end of every run of our approach using its original formulations (6) which gives a better understanding of the goodness of the results obtained.The results in columns “Val. set” (Validation set) came from an analysis similar to that of step (v) presented in Section 5.1. In short, for each firm whose data belong to the considered validation set we have computed the score and the consequent rank through the application of MURAME. The used MURAME parameters have been the optimal ones determined by the PSO-sa on the reference set, again considering m′=100 and m′=500 firms. We recall that such results are average values based on the differences between the scoring and the ranking calculated by our approach and those provided by the bank.Finally, in the last column of Table 6 we show the rates of success. To calculate these quantities we have proceeded in the way which follows. First, we have classified the firms of the validation set in four fictitious rating classes, of approximately the same cardinality (1000 firms), based on the initial ordering provided by the bank. Then, at the end of every step of the PSO-sa we have classified the firms in the same way, based on the ordering of the firms produced by MURAME with the parameters found by PSO-sa itself, and we have computed the ratio of the number of firms correctly classified to the total number of firms. Finally, we have calculated the average values of these figures over the 20 runs.Note that there is substantially good consistency with respect to the ordering performance of the model, that is average values of S nearly zero, in the case of the analysis carried out on the reference sets. This is more evident when the reference set associated to m′=100 firms is used. The results for Δ in the reference set are slightly worse than the ones obtained in the bootstrap analysis. This is expected since in this case the scores of the firms are assigned in an external way, and not anymore by the MURAME model. Indeed, the task to find values of the MURAME parameters that allows to generate scores consistent with the external ones is clearly more complex, and there is even no guarantee at all that there exists a set of parameters able to produce full consistency.When the validation set is considered, we see that also with a greater reference set of m′=500 firms, the capability of our approach in finding ordering consistency decreases, as shown by the higher values of S, but still it appears a good classification performance.In particular, with specific attention to the greater reference set, in Table 7we show the average values of the optimal MURAME parameters.It is interesting to have a look at the weights of the criteria. It emerges clearly that indicators I2, I3 and I6 play a marginal role in the implicit preference structure expressed by the bank. For indicator I2, as it is shown in Table 8, this behavior can be a priori explained by its high correlation with indicator I4. In terms on means and standard deviations, such behavior is qualitatively analogous to the one presented in Table 5. In particular, the applied preference elicitation process puts in evidence that, although the two indicators give more or less the same information, the bank strongly prefers indicator I4 as, likely, its corporate meaning is more close to the DM's managerial philosophy. Then, recalling that the bank has given to us neither the formal description of the indicators nor their economic/financial meanings, we have not a so evident interpretation for indicators I3 and I6. Anyway, it is important to note that this additional information we have obtained could be used by the bank to improve or refine its internal credit scoring/ranking modeling policy. For instance, one considers the possibility to avoid to collect and/or to process uninformative data.We conclude this section with an observation on the computational time. As expected, we note that it depends on the numerousness both of the reference set and of the validation set. For example, given a population of M=400 particles in the bootstrap analysis, with a reference set associated to 100 firms, 18 minutes are needed in order to make 400 steps of the PSO-sa. This implies more than 300 hours in order to carry out the 1000 runs of the bootstrap analysis.1010The used computer has the following features: processor Intel® CoreTM(Quad Core 2.93GHz); memory 8GB.In the case of a reference set associated to 500 firms the computational time increases to almost 8h to implement 400 steps of the PSO-sa.The novelties of this contribution consist:•In dealing with the problem of preference disaggregation in a MURAME context;In tackling the associate preference disaggregation optimization problem by adopting a solution algorithm based on the PSO;In applying the proposed approach to a large real scoring/ranking problem.Since MURAME determines the outranking index with respect to each other alternative in the reference set, the computational effort required by our methodology is notably higher than the ones required by other multicriteria models, like for example the ELECTRE TRI-based models which consider only the reference profiles [33].The results obtained in this contribution show a high consistency between the scores/ranks produced by our methodology and the scoring/ranking of the firms provided by the bank. At the same time, the presence of non-zero standard deviations associated to the estimated parameters implies a certain degree of flexibility in the determination of such parameters that could be exploit to implement an interactive procedure by which to make explicit the unknown preference structure of the DM, provided that a dialogue with the DM herself/himself is feasible.Finally, as future research we intend to find the way of reducing the computational time by conveniently improving our methodology according to new ways by which the model compares each pair of alternatives. As example, one could implement a mere comparison of each alternative only with some reference profiles. Another research direction that we intend to follow consists in comparing the performance of the penalty function-based PSO-sa methods applied in this paper with other PSO-based methods of handling the constraints of the problem, and with other evolutionary methodologies like, for instance, the DE algorithm and its possible hybridization with PSO itself.

@&#CONCLUSIONS@&#
