@&#MAIN-TITLE@&#
An inexact proximal method for quasiconvex minimization

@&#HIGHLIGHTS@&#
We obtain global convergence of an inexact proximal method for quasiconvex problems.We give a sufficient condition to obtain convergence to an optimal point.The studied model is nondifferentiable but Lipschitz continuous.The results may be used, in particular, to solve decision economics problems.

@&#KEYPHRASES@&#
Computing science,Global optimization,Nonlinear programming,Proximal point methods,Quasiconvex minimization,

@&#ABSTRACT@&#
In this paper we propose an inexact proximal point method to solve constrained minimization problems with locally Lipschitz quasiconvex objective functions. Assuming that the function is also bounded from below, lower semicontinuous and using proximal distances, we show that the sequence generated for the method converges to a stationary point of the problem.

@&#INTRODUCTION@&#
It is well known that the class of proximal point algorithm (PPA) is one of the most studied methods for finding zeros of maximal monotone operators and in particular to solve convex optimization problems, see Auslender and Teboulle (2006), Burachik and Iusem (1998), Burachik and Scheimberg (2000), Chen and Teboulle (1993), Kiwiel (1997), Rockafellar (1976).In the last decades a great interest has emerged to extend the PPA for non monotone variational inequalities and non convex minimization problems not only for extending the convergence theory but by several applications in diverse science and engineering areas, see for example the works of Attouch and Bolte (2009); Attouch, Bolte, and Svaiter (2013); Kaplan and Tichatschke (1998); Pennanen (2002); Chen and Pan (2008).In particular the class of quasiconvex minimization problems has been receiving attention from many research works due to the broad range of applications in location theory, Gromicho (1998), Fractional programming and specially in economic theory, see for example Takayama (1995) and Mas-Colell, Whinston, and Green (1995). Some related papers are the following: Goudou and Munier (2009), Souza, Oliveira, da Cruz Neto, and Soubeyran (2010), Chen and Pan (2008), Cunha, da Cruz Neto, and Oliveira, (2010), Pan and Chen (2007), Papa Quiroz and Oliveira (2012), Langenberg and Tichatschke (2012), Brito, da Cruz Neto, Lopez, and Oliveira (2012).In this paper we are interested in extending the global convergence of an inexact proximal point method to minimize a quasiconvex function constrained on a nonempty closed convex set, that is,(1.1)min{f(x):x∈C¯},wheref:IRn→IR∪{+∞}is a quasiconvex function, C is open convex set on the euclidean spaceIRnandC¯is the closure of C. Obviously, ifC=IRnwe obtain the unconstrained minimization problem. Some convergence results have been recently obtained for some research works:Attouch and Teboulle (2004), with a regularized Lotka–Volterra dynamical system, have proved the convergence of the continuous method to a point which belongs to certain set which contains the set of optimal points; see also Alvarez, Bolte, and Brahic (2004), that treats a general class of dynamical systems that includes the one of Attouch and Teboulle.Souza et al. (2010), Cunha et al. (2010), Chen and Pan (2008) and Pan and Chen (2007) studied the iteration(1.2)xk∈argminx∈C¯{f(x)+λkd(x,xk−1)},whereC¯=IR+n,d is a certain distance to force the iterates xkto stay in C. Some examples of d based on the literature are the class of Bregman,φ−divergence and second order homogeneous distances. Under the assumption that f is bounded from below,dom(f)∩IR+n≠∅,the proximal parameter is bounded and assuming that f is differentiable, these research works obtained the global convergence of the method to a KKT point of (1.1). Furthermore, the sequence generated converges to a solution of the problem, should it exist, if the proximal parameters approach to zero.Brito et al. (2012), proposed an interior proximal algorithm inspired by the logarithmic-quadratic proximal method for linearly constrained quasiconvex minimization problems. For that method, they proved the global convergence when the proximal parameters go to zero. The latter assumption could be dropped when the function is assumed to be pseudoconvex.Langenberg and Tichatschke (2012), motivated from the work of Kaplan and Tichatschke (1998), studied the iteration (1.2) when C is an arbitrary open convex set and d is a Bregman distance. Assuming that f is locally Lipschitz and using the Clarke subdifferential, the authors proved the global convergence of the method to a critical point of (1.1).The above works although important have some disadvantages:•The main difficulty in extending the proximal method for nonconvex function, which was observed by Kaplan and Tichatschke (1998) and Langenberg and Tichatschke (2012), is that due to the nonconvexity of f the subproblems of (1.2) may not be convex and thus, from a practical point of view, we may obtain that minimization subproblems may be as hard to solve globally as the original one due to the existence of multiple isolated local minimizers. These authors have proved, under some appropriate conditions and chose a sufficiently large regularization parameters, the strong convexity of the proximal subproblems (and thus efficiently solvable subproblems) for a class of non convex functions, see Theorem 2 and Theorem 5 of Kaplan and Tichatschke (1998) and Langenberg and Tichatschke (2012) respectively. However, the above property is not true in general for arbitrary quasiconvex functions. So, we believe that a basic idea is to weaken the condition of minimizing a strongly convex regularization function by another one such that in each iteration we may use local information of the subproblems. If the regularized function is, for example, locally strongly convex we will have, in certain sense, efficiency in solving the subproblems. This motivates the following question: Is it possible to introduce a local stationary iteration that makes much more sense that the previously considered (1.2) for dealing with nonconvex problems?In Rockafellar (1976) it is shown that in some cases let the proximal parameter converges to zero, although the regularizing effect vanishes, provides superlinear convergence of the algorithm in the convex case. Motivated by this fact, Brito et al. (2012), Souza et al. (2010), Cunha et al. (2010), Chen and Pan (2008) and Pan and Chen (2007) have been proved the convergence of the proximal method to an optimal point when the parameters converges to zero. On the other hand, when the proximal parameters are sufficiently greater than zero but bounded from above, Langenberg and Tichatschke (2012), see Theorem 9 of that work, proved the convergence to a stationary point which may be in the worst case a saddle point (observe that stationary point or critical point point does not necessarily a global nor local minimum point). This motivates the following question: is it possible to obtain a convergence theory to an optimal point when the proximal parameters are bounded from above?Despite the fact that the proximal point method is not practical in its exact version, several works, for the convex case, have been shown that it is possible to obtain implementable algorithms with good convergence properties, see for example Alvarez, López, and Ramírez (2010), Liu, Sung, and Toh (2012), Santos and RC (2014). In our case, for a computational implementation of the proximal point algorithm for the quasiconvex case it is needed to solve the iteration (1.2) using a local optimization algorithm, which only provides an approximate solution. Thus it is important to consider inexact methods. Therefore, from the computational point of view, is it possible to introduce a inexact proximal method to solve (1.2) and prove the convergence of the iteration?In this paper, motivated by a recently work of Papa Quiroz and Oliveira (2012), we answer the questions of the first and third bullet points and partially we answer the question of the second bullet one, see Subsection 4.3, proposing the following proximal method: givenxk−1∈C,find xk∈ C and gk∈ ∂°f(xk) such that(1.3)∥xk−1−xk−ek∥≤max{∥ek∥,∥xk−xk−1∥}where(1.4)ek=gk+λk∇1d(xk,xk−1)with ∂° is the Clarke subdifferential and d is a proximal distance, see Sections 2 and 3 respectively.The condition (1.3) has been motivated from the work of Humes and Silva (2005) and Solodov and Svaiter (1999), where they considered the following criterion:∥gk+(xk−xk−1)∥≤σmax{∥gk∥,∥xk−xk−1∥}with σ ∈ (0, 1]. Observe that the above condition and (1.3) are different and so we may conclude that (1.3) is new in proximal point methods even for the convex case. Whenek=0in (1.4) andC=IR++nwe obtain the exact version studied by Papa Quiroz and Oliveira (2012) for the nonnegative orthant.Observe also that the conditions (1.3)-(1.4) are more practical than (1.2), where a global minimum point is required in each iteration, and thus more practical than the works of Souza et al. (2010), Cunha et al. (2010), Chen and Pan (2008), Pan and Chen (2007) and Langenberg and Tichatschke (2012). Therefore in our opinion the local stationary iterations (1.3)–(1.4) makes much more sense than the previously considered (1.2) for dealing with nonconvex problems.Under the assumption that f is proper, lower semicontinuous, locally Lipschitz and bounded from below onC¯and using a class of proximal distance we will prove that {xk} is well defined and if, in addition, f is quasiconvex it will be proved that {f(xk)} is decreasing and {xk} converges to some point ofU+:={x∈C¯:f(x)≤infj≥0f(xj)},assumed nonempty. Then, under the additional conditions that the proximal parameter {λk} is bounded from above and(1.5)∑k=1+∞∥ek∥λk<+∞(1.6)∑k=1+∞|〈ek,xk〉|λk<+∞,we obtain that the sequence {xk} converges to a stationary point of the problem. Observe that the above conditions (1.5)–(1.6) have been used in convex proximal methods, see for example Auslender, Teboulle, and Ben-Tiba (1999), Kaplan and Tichatschke (2004), Eckstein (1998), Xu, Bingsheng, and Xiaoming (2006), Solodov and Svaiter (2000). We also get to rid the assumption (1.6) for a class of induced proximal distances which includes Bregman distances given by the standard entropy kernel and all strongly convex Bregman functions.The paper is organized as follows: In Section 2 we give some basic results on quasiconvex theory and Clarke subdifferential of locally Lipschitz functions. In Section 3 we introduce the class of proximal distances that we will use along the paper. In Section 4 we present the inexact algorithm for solving minimization problems with quasiconvex functions and analyze its convergence properties. Finally, in Section 5 we give our conclusions.Throughout this paperIRnis the Euclidean space endowed with the canonical inner product ⟨, ⟩ and the norm of x given by ||x|| ≔ ⟨x, x⟩1/2. Given X⊂IRnwe denote bd(X) andX¯the boundary and closure of X respectively. Let B ∈IRn× be a symmetric and positive definite matrix, we define∥x∥B=〈Bx,x〉1/2.Given an extended real valued function f:IRn→ IR∪{ ± ∞} we denote its domain bydom(f):={x∈IRn:f(x)<+∞}.f is said to be proper ifdom(f)≠∅and∀x∈dom(f)we havef(x)>−∞.Finally, f is a lower semicontinuous function if for each x ∈IRnwe have that all {xl} such thatliml→+∞xl=ximplies thatf(x)≤lim infl→+∞f(xl).It is easy to prove that the lower semicontinuity of f is equivalent to the closedness of the level setLf(α)={x∈IRn:f(x)≤α},for each α ∈ IR.Lemma 2.1Let {vk}, {γk}, and {βk} be nonnegative sequences of real numbers satisfyingvk+1≤(1+γk)vk+βkand such that∑k=1∞βk<∞,∑k=1∞γk<∞.Then, the sequence {vk} converges.See Lemma 2, pp. 44, of Polyak (1997).□Observe that there is a sharper version of the above result, presented as Lemma 7 in Langenberg (2010).Definition 2.1Letf:IRn→IR∪{+∞}be a proper function.(a)For eachx∈dom(f),the set of regular subgradients (also called Fréchet subdifferential) of f at x, denoted by∂^f(x),is the set of vectors v ∈IRnsuch thatf(y)≥f(x)+〈v,y−x〉+o(∥y−x∥),wherelimy→xo(∥y−x∥)∥y−x∥=0.Or equivalently,∂^f(x):={v∈IRn:lim infy≠x,y→xf(y)−f(x)−〈v,y−x〉∥y−x∥≥0}. Ifx∉dom(f)then∂^f(x)=∅.The set of general subgradients (also called limiting subdifferential) of f at x ∈IRn, denoted by ∂f(x), is defined as follows:∂f(x):={v∈IRn:∃xl→x,f(xl)→f(x),vl∈∂^f(xl)andvl→v}.For a functionf:IRn→IR∪{+∞}and a pointx¯∈dom(f),the subgradient sets∂f(x¯)and∂^f(x¯)are closed, with∂^f(x¯)convex and∂^f(x¯)⊂∂f(x¯).See Rockafellar and Wets (1998), Theorem 8.6.□If a proper functionf:IRn→IR∪{+∞}has a local minimum atx¯∈dom(f),then0∈∂^f(x¯).See Rockafellar and Wets (1998), Theorem 10.1.□Definition 2.2Letf:IRn→IR∪{+∞}be proper function. We say that f is locally Lipschitz if for eachx∈dom(f)there existsϵx>0such that|f(z)−f(y)|≤Lx∥z−y∥,∀z,y∈B(x,ϵx),where Lxis some positive number (called the Lipschitz constant of f in a neighborhood of x) andB(x,ϵx):={y∈M:∥x−y∥<ϵx}.From the above definition we obtain that iff:IRn→IR∪{+∞}is a proper locally Lipschitz function, thendom(f)is open inIRn.Letf,g:IRn→IR∪{+∞}proper functions such that f is locally Lipschitz atx¯∈dom(f)∩dom(g)and g is lower semicontinuous function at this point. Then,∂(f+g)(x¯)⊂∂f(x¯)+∂g(x¯)See Mordukhovich (2006), Theorem 2.33.□Letf:IRn→IR∪{+∞}be proper locally Lipschitz function. Givenx∈dom(f)the generalized directional derivative of f at x in the direction v ∈IRndenoted by f°(x, v), is defined asf∘(x,v)=lim supt↓0y→xf(y+tv)−f(y)t,which is also called the Clarke’s generalized directional derivative, see Clarke (1975) or Clarke (1990).Definition 2.4Letf:IRn→IR∪{+∞}be a proper locally Lipschitz function. The generalized subdifferential of f atx∈dom(f),denoted by ∂°f(x), is defined by∂∘f(x):={w∈:IRn:f∘(x,v)≥〈w,v〉,∀v∈IRn}.If the proper locally Lipschitz functionf:IRn→IR∪{+∞}is convex, thenf∘(x,v)=f′(x,v):=limt↓0f(x+tv)−f(x)t(respectively,∂∘f(x)=∂Ff(x),where ∂Ff is the Fenchel subdifferential of f) for allx∈dom(f),see Clarke (1990), pp. 40, Proposition 2.3.6.From the above definitions it follows directly that for allx∈Rn,one has∂^f(x)⊂∂f(x)⊂∂∘f(x)(see Bolte, Daniilidis, Lewis, and Shiota, 2007, Inclusion (7)).Letf:IRn→IR∪{+∞}be a proper locally Lipschitz functions. A point x ∈ M is said to be a stationary point of f if 0 ∈ ∂°f(x).Letf:IRn→IR∪{+∞}be a proper locally Lipschitz function. The operator ∂°f is called locally weakly monotone iff for eachx∈dom(∂∘f)there existsϵxand ρx> 0 such that for allz,y∈B(x,ϵx)we have(2.7)〈u−v,z−y〉≥−ρx∥z−y∥2.for all u ∈ ∂°f(z), and for all v ∈ ∂°f(y)Letf:IRn→IR∪{+∞}be a proper function. f is called quasiconvex if for all x, y ∈IRn, and for all t ∈ [0, 1], it holds thatf(tx+(1−t)y)≤max{f(x),f(y)}.Observe that if f is a quasiconvex function thendom(f)is a convex set. On the other hand, while a convex function can be characterized by the convexity of its epigraph, a quasiconvex function can be characterized by the convexity of the level setsTheorem 2.1Letf:IRn→IR∪{+∞}be a proper function. Then, f is quasiconvex if and only if the set {x ∈IRn: f(x) ≤ c} is convex for each c ∈ IR.See Theorem 3.5.2 of Bazaara, Sherali, and Bazaara (1993).□Letf:IRn→IR∪{+∞}be a proper locally Lipschitz and quasiconvex function. If g ∈ ∂°f(x) and〈g,z−x〉>0,then f(x) ≤ f(z).See Theorem 2.1 of Aussel (1998).□Consider the problem(2.8)min{f(x):g(x)≤0,x≥0},where f:IRn→ IR and g:IRn→IRmsuch thatg(x)=(g1(x),…,gm(x))with gj:IRn→ IR, for eachj=1,…,mand x ≥ 0 mean that xi≥ 0, for eachi=1,…,n.Theorem 2.2Let f, gj:IRn→ IR,j=1,…,m,be differentiable quasiconvex functions. Ifx¯satisfies the KKT necessary condition and one of the following conditions is satisfied:(a)∂f∂xi(x¯)>0,for at least one variable xi,i∈{1,2,…,n},∂f∂xi(x¯)<0,for some i such thatxi¯>0,∂f∂xi(x¯)≠0,and f is twice differentiable in a neighborhood ofx¯,thenx¯is a global minimum of the problem (2.8).See Theorem 1 of Arrow and Enthoven (1961).□In this section we present the definitions of proximal distance and induced proximal distance, introduced by Auslender and Teboulle (2006).Definition 3.1A functiond:IRn×IRn→IR+∪{+∞}is called a proximal distance with respect to an open nonempty convex set C ⊂IRnif for each y ∈ C it satisfies the following properties:i.d(., y) is proper, lower semicontinuous, strictly convex and continuously differentiable on C;dom(d(.,y))⊂C¯anddom(∂1Fd(.,y))=C,where∂1Fd(.,y)denotes the classical subdifferential of the function d(., y) with respect to the first variable;d(., y) is coercive onIRn(i.e.,lim∥u∥→∞d(u,y)=+∞);d(y,y)=0.We denote by D(C) the family of functions satisfying the above definition.Property i. is needed to preserve convexity of d(., y), property ii will force the iteration of the proximal method to stay in C, andiiiwill be used to guarantee the existence of the proximal iterations. For each y ∈ C, let ∇1d(., y) denotes the gradient map of the function d(., y) with respect to the first variable. Note that by definition d(., .) ≥ 0 and fromivthe global minimum of d(., y) is obtained at y, which shows that∇1d(y,y)=0.Definition 3.2Given C ⊂IRn, open and convex, and d ∈ D(C), a functionH:IRn×IRn→IR+∪{+∞}is called the induced proximal distance to d if H is a finite-valued function on C × C and for each a, b ∈ C satisfies(Ii)H(a,a)=0.〈c−b,∇1d(b,a)〉≤H(c,a)−H(c,b),∀c∈C.Let us denoted by(d,H)∈F(C) to the proximal and induced proximal distance that satisfies the conditions of Definition 3.2. We also denote(d,H)∈F(C¯)if there exists H such that(Iiii)H is finite valued onC¯×Csatisfying(Ii)and(Iii),for eachc∈C¯.For eachc∈C¯,H(c, ·) has level bounded sets on C.Finally, to establish the global convergence of the proposed method we need to make further assumptions on the induced proximal distance H, mimicking the behavior of the euclidean norm, so we write(d,H)∈F+(C¯)if(Iv)(d,H)∈F(C¯).∀y∈C¯and ∀ {yk}⊂C bounded withlimk→+∞H(y,yk)=0,thenlimk→+∞yk=y.∀y∈C¯,and ∀ {yk}⊂C such thatlimk→+∞yk=y,thenlimk→+∞H(y,yk)=0.The main result on proximal point method will be when(d,H)∈F+(C¯).Several examples of proximal distances which satisfy the above definitions, for example Bregman distances, proximal distances based onφ−divergences, self-proximal distances, and distances based on second order homogeneous proximal distances, were given by Auslender and Teboulle (2006), Section 3.The conditions (Ivi) and (Ivii) will ensure the global convergence of the sequence generates by the algorithm introduced in Subsection 4.1. As we will see in Theorem 4.4, the condition (Ivii) may be substituted by the following:(Iviii)H(., .) is continuous in C × C and if {yk} ⊂ C such thatlimk→+∞yk=y∈bd(C)andy¯≠yis another point inC¯thenlimk→+∞H(y¯,yk)=+∞.According Langenberg and Tichatschke (2010), page 643, the above condition for induced Bregman distances holds when nonlinear constraints are active at y while the condition(Ivii)holds when only affine constraints are active at y.Given a symmetric and positive definite matrix B ∈IRn× andd∈D(C). We say that d is strongly convex with respect to the first variable and with respect to the norm ||.||B, if for each y ∈ C exists α > 0 such that〈∇1d(x1,y)−∇1d(x2,y),x1−x2〉≥α∥x1−x2∥B2,∀x1,x2∈C.Examples of proximal distances satisfying the above definition are the second order homogeneous distances, see Brito et al., Lemma 3.1. In particular, forC={x∈IRn:Ax<b}with A ∈IRm× , b ∈IRmandrank(A)=n,the following logarithmic-quadratic proximal distance satisfies the above propertyd(x,z)=∑i=1mμ(yi(z)yi(x)−yi(z)2ln(yi(x)yi(z))−yi(z)2)+ν2(yi(x)−yi(z))2whereyi(x)=bi−aiTx,yi(z)=bi−aiTz,aidenotes the row i of the matrix A, bithe components of b and ν > μ > 0 withB=ATAandα=ν.Another class that satisfies the above property is the class of Bregman distances with strongly convex Bregman functions.Definition 3.4Given a symmetric and positive definite matrix B ∈IRn× andd∈D(C). We say that d is locally strongly convex with respect to the first variable and with respect to the norm ||.||B, if for each y ∈ C and each x ∈ C there existsϵx>0and αx> 0 such that〈∇1d(x1,y)−∇1d(x2,y),x1−x2〉≥αx∥x1−x2∥B2,∀x1,x2∈B(x,ϵx).Letf:IRn→IR∪{+∞}be a proper locally Lipschitz function, d ∈ D(C),dom(∂∘f)∩C≠∅and B ∈IRn× a symmetric and positive definite matrix. Given an arbitrary point y ∈ C, if ∂°f is a locally weakly monotone operator with constant ρ > 0, d(., y) is locally strongly convex with respect to the norm ||.||B, with constant α and {βk} is a sequence of positive numbers satisfyingβk≥β>ραλmin(B),where λmin(B) denotes the smallest eigenvalue of B, thenF(.):=∂∘f(.)+βk∇1d(.,y)is locally strongly monotone with constantβαλmin(B)−ρ.The proof follows the same steps from Lemma 5.1 of Brito et al. (2012) but considering local information and substituting in that lemma T by ∂°f(.), ∇1Dϕby ∇1d and ATA by B respectively.□Letf:IRn→IR∪{+∞}be a proper locally Lipschitz function, if ∂°f is locally Lipschitz then it is locally weakly monotone.It is inmediate.□We are interested in solving the problem defined by(4.9)min{f(x):x∈C¯}wheref:IRn→IR∪{+∞}is a proper lower semicontinuous quasiconvex function, C⊂IRnis a nonempty open convex set satisfyingdom(f)∩C¯≠∅.Example 4.1Demand theoryIn the classical approach to consumer demand, the analysis of consumer behavior begins specifying the consumer’s preference ≽(an “at-least-as-good-as” binary relation) over the commodity bundles in the consumption setX⊂IR+n.This preference ≽ is assumed to be rational, that is, ≽ is complete and transitive, see Definition 3.B.1 of Mas-Colell et al. (1995).A function μ: X → IR is said to be an utility function representing a preference relation ≽ on X if the following condition is satisfiedx⪰y,ifandonlyifμ(x)≥μ(y),for all x, y ∈ X. We should observe that the utility function not always exist. In fact, define inX=IR2a lexicographic relationForx,y∈IR2,x⪰yifandonlyifx1>y1or(x1=y1andx2≥y2).Fortunately, a very general class of preference relations can be represented by utility functions, see for example Proposition 3.C.1 of Mas-Colell et al. (1995).If a preference relation ≽ is represented by an utility function μ, then the problem of maximizer the preference of the consumer on X is equivalent to solve the optimization problem(P)max{μ(x):x∈X}.On the other hand, a natural psychological assumption in economy is that the consumer tends to diversify his consumption among all goods, that is, the preference ≽ satisfies the following convexity property: X is convex and if x≽z and y≽z thenλx+(1−λ)y⪰z,∀λ∈[0,1].It can be proved that the if there exists an utility function representing the preference, then the convexity property of ≽ is equivalent to the quasiconcavity of the utility function μ. Thus, (P) becomes in a maximization problem with quasiconcave objective function. Takingf=−μwe obtain a minimization problem with a quasiconvex objective function on X. Some examples of quasiconcave functions in economic theory are:•The Cobb–Douglas functionμ:IR+n→IRsuch thatμ(x)=k∏i=1nxiαi,where αi> 0,∀i=1,…,nand k > 0.The C.E.S production functionμ:IR+n→IR,such thatμ(x)=k[∑i=1nδixi−ρ]−v/ρwhereδi∈(0,1),∀i=1,2…,n,∑i=1nδi=1,k>0,v∈(0,1)andρ>−1,ρ≠0.To solve (4.9), we propose the following interior proximal point method (IPPM): Initialization: Let a sequence of real positive numbers {λk} and an initial point(4.10)x0∈CMain Steps: Fork=1,2,…,and givenxk−1∈C,find xk∈ C and gk∈ ∂°f(xk) such that(4.11)∥xk−1−xk−ek∥≤max{∥ek∥,∥xk−xk−1∥}where(4.12)ek=gk+λk∇1d(xk,xk−1)and d is a proximal distance such that(d,H)∈F(C¯).Stop Criterium: Ifxk=xk−1,or 0 ∈ ∂°f(xk) then stop. Otherwise to dok−1←kand return to Main steps.Remark 4.1The inexact iteration (4.11) is a variant of the one studied by Humes and Silva (2005), from the geometric point of view this condition means that the side∥ek+(xk−1−xk)∥of the triangle is smaller (or equal) than the maximum of∥xk−xk−1∥and ||ek||. We will prove that this condition implies that the angle between the vectorsxk−xk−1and ekis acute and it will be used to assure that the sequence {f(xk)} is non increasing.Observe that ifek=0,then (4.11)–(4.12) becomes: find xk∈ C and gk∈ ∂°f(xk) such that0=gk+λk∇1d(xk,xk−1),which is more weak than0∈∂∘(f(.)+λkd(.,xk−1))(xk).If the condition (4.11) is satisfied, then〈ek,xk−1−xk〉≥0.From the cosines law and (4.11), we have〈ek,xk−1−xk〉=12(∥xk−1−xk∥2+∥ek∥2−∥xk−1−xk−ek∥2)≥0.□If f is a proper lower semicontinuous function and locally Lipschitz function and lower bounded ondom(f)∩C¯and d ∈ D(C), then the sequence {xk}, generated by the proximal method is well defined, that is, xk exists and xk∈ C.We proceed by induction. It holds fork=0due to (4.10). Assume thatxk−1exists andxk−1∈C.Defineg(x)=f(x)+λkd(x,xk−1)+δC¯(x)and consider the problemmin{g(x):x∈IRn}.Now, due to the lower boundedness and lower semicontinuity of f, as also, the lower semicontinuity and coercivity of d(., y), there existsx¯∈IRn(not necessarily unique by the nonconvexity of f) which is a global minimum of g. From Proposition 2.20∈∂^(f(.)+λkd(.,xk−1)+δC¯(.))(x¯)Then from Proposition 2.1 and Proposition 2.3 we have0∈∂f(x¯)+∂(λkd(.,xk−1)+δC¯(.))(x¯)Since thatd(.,xk−1)andδC¯(.)are proper, lower semicontinuous and convex functions and using Remark 2.3 we obtain that0∈∂∘f(x¯)+λk∂1Fd(.,xk−1)(x¯)+∂F(δC¯)(x¯)From Definition 3.1, ii, we conclude thatx¯∈C.Finally, takingxk=x¯there exists gk∈ ∂°f(xk) such that0=gk+λk∇1d(xk,xk−1),that is, (4.11)-(4.12) is satisfied.□Under the assumptions of the above theorem and if, furthermore, ∂0f is locally weakly monotone, d(., y) is strongly convex with respect to the first variable and with respect to the norm ||.||Bwith constant α, then, from Lemma 3.1, the functionf(.)+λkd(.,xk−1)is locally strongly convex forλk≥λ>ραλmin(B),where ρ is the locally weakly monotone constant.The above remark provides a class of non convex functions, in particular the class of functions f which ∂°f is locally Lipschitz, see Lemma 3.2, with regularized locally strongly convex functions. Thus, we expect local algorithms to perform efficiently enough to find good approximate solutions at reasonable time. For example, if this regularized function is locally self-concordant, see Nesterov and Nemirovskii (1994), we can use for computing the point xksatisfying (4.12) the damped Newton method which is commonly recognized as a standard approach particularly efficient in such cases. When the objective function f is nonsmooth we can work with the so called bundle methods.In this section, under some natural conditions, we prove the convergence of the proposed method. We begin imposing the following assumptions: AssumptionA. f is a proper lower semicontinuous quasiconvex function. AssumptionB. f is locally lipschitzian and bounded from below AssumptionC.dom(f)∩C¯≠∅. As we are interest in the asymptotic convergence of the method, we also assume that in each iterationxk≠xk−1,for all k. Ifxk=xk−1then∇1d(xk,xk−1)=0and from (4.12) we have that ek∈ ∂°f(xk), that is, xkis an approximation of the critical point of f.Proposition 4.1Under assumptions A, B and C and d ∈ D(C), we have that {f(xk)} is non increasing and converges.Asxk≠xk−1andd(.,xk−1)is strictly convex then(4.13)〈∇1d(xk,xk−1)−∇1d(xk−1,xk−1),xk−xk−1〉〉0As∇1d(xk−1,xk−1)=0,λk>0and from (4.12) we have(4.14)〈gk,xk−1−xk〉>〈ek,xk−1−xk〉≥0where the last inequality is due to Lemma 4.1. Finally from Lemma 2.2 we obtainf(xk)≤f(xk−1).The convergence of {f(xk)} is immediate from the lower boundedness of f.□Now, we define the following setU+:={x∈C¯:f(x)≤infj∈INf(xj)}.Observe that this set depends on the choice of the initial iterates x0 and the sequence {λk}. Furthermore, ifU+=∅then it is easy to prove thatlimk→+∞f(xk)=infx∈C¯f(x),and {xk} is unbounded.From now on we assume thatU+≠∅.So from the assumption A, we obtain thatU+is closed and convex set (see Theorem 2.1 for the convexity property).Proposition 4.2Under assumptions A, B and C and(d,H)∈F(C¯),we haveH(x,xk)≤H(x,xk−1)−1λk〈ek,x−xk〉,∀x∈U+Letx∈U+,then f(x) ≤ f(xk) from Lemma 2.2 we obtain〈gk,x−xk〉≤0,which from (4.12) impliesλk−1〈ek,x−xk〉≤〈∇1d(xk,xk−1),x−xk〉then, using property Iii of Definition 3.2 we have(4.15)λk−1〈ek,x−xk〉≤H(x,xk−1)−H(x,xk),that is,(4.16)H(x,xk)≤H(x,xk−1)−1λk〈ek,x−xk〉□Suppose the assumptions A, B, C and(d,H)∈F(C¯).If the following additional conditions hold:(4.17)∑k=1+∞∥ek∥λk<+∞(4.18)∑k=1+∞|〈ek,xk〉|λk<+∞thena.{xk} is Quasi-Féjer convergent toU+,that is,H(x,xk)≤H(x,xk−1)+ϵk,∀x∈U+,whereϵk=1λk(∥ek∥∥x∥+|〈ek,xk〉|)with∑k=1+∞ϵk<+∞.{H(x, xk)} converges∀x∈U+.Furthermore, if(d,H)∈F(C¯),then {xk} is bounded.a. Using the Cauchy–Schwartz inequality in the right-side of (4.16) we obtain(4.19)H(x,xk)≤H(x,xk−1)+1λk(∥ek∥∥x∥+|〈ek,xk〉|)Letϵk=1λk(∥ek∥∥x∥+|〈ek,xk〉|),thenH(x,xk)≤H(x,xk−1)+ϵk,and from the assumptions (4.17) and (4.18) we obtain that∑k=1+∞ϵk<∞.b. It is immediate from a. and Lemma 2.2. c. From a. we have(4.20)H(x,xk)≤H(x,x0)+∑k=1+∞ϵkthis implies thatxk∈LH(x,α)={y∈C¯:H(x,y)≤α},whereα=H(x,x0)+∑k=1+∞ϵk.From Definition 3.2, Iiv, LH(x, α) is bounded and thus {xk} is bounded.□It is possible to get rid the assumption (4.18) in Proposition 4.3 for a class of induced proximal distances which includes Bregman distances given by the standard entropy kernel and all strongly convex Bregman functions, see Kaplan and Tichatschke (2004). Of fact, suppose that the induced proximal distance H with(d,H)∈F(C¯)satisfies the following property:(Iix) For eachx∈C¯there exist α(x) > 0 and c(x) such thatH(x,v)+c(x)≥α(x)∥x−v∥,∀v∈C.Then, it is easy to imitate the proof of Proposition 1 from Kaplan and Tichatschke (2004), to obtain that H(x, xk) is convergent and {xk} is bounded, without the assumption (4.18).Under the assumptions of the previous proposition and assuming that(d,H)∈F+(C¯),the sequence {xk} converges to some point ofU+.From previous lemma {xk} is bounded, then there exists a subsequence{xkj}which converges tox¯,that islimj→+∞xkj=x¯.As f is lower semicontinuous, nonincreasing and {f(xk)} converges we havex¯∈U+.Suppose that there exists another sequence{xkl}such thatliml→+∞xkl=z∈U+.Using propertyIvii,from Definition 3.2, we obtainliml→+∞H(z,xkl)=0,and from the convergence of {H(z, xk)},limk→+∞H(z,xk)=0.Thuslimj→+∞H(z,xkj)=0.Using propertyIvi,from Definition 3.2, we obtain thatlimj→+∞xkj=z,that is,x¯=z.□The above result also is true if the condition(Ivii)on (d, H) is substitute by(Iviii).Of fact, letx¯and z inU+as in the proof of the above theorem. Consider two cases: ifx¯∈bd(C)then ifz≠x¯,from assumption(Iviii),H(z,xkj)→+∞,but this a contradiction becausez∈U+and the convergence of H(z, xk). On the other hand, ifx¯∈C,from the continuity of H(., .) on C × C we haveH(x¯,xkj)→0and from the convergence ofH(x¯,xk)we have thatH(x¯,xk)→0,now using the condition(Ivi)we obtain thatxk→x¯and thusz=x¯.Finally, we give the global convergence of {xk} to a stationary point of the problem. We should note that result is an easy adaptation to proximal distances of Theorem 9 of Langenberg and Tichatschke (2012).Theorem 4.3Under assumptions A, B, C,(d,H)∈F+(C¯),(4.17) and (4.18), and furthermore {λk} is bounded from above, the generated sequence {xk} converges to a stationary pointx¯∈Cof f, i.e. existsg¯∈∂∘f(x)such that∀x∈C¯we have〈g¯,x−x¯〉≥0.Since f is locally Lipschitz atx¯=limk→∞xkthere exists an open neighborhoodB(x¯,δ)and L > 0 such that f is Lipschitzian with modulus L onB(x¯,δ). Thus, there is k0 ∈ IN such thatxk∈B(x¯,δ)for any k ≥ k0. Hence, ∂°f(xk) is bounded for k ≥ k0 in the sense that ‖gk‖ ≤ L for all gk∈ ∂°f(xk) with k ≥ k0. Without loss of generality we can assume the convergencegk→g¯for someg¯∈∂∘f(x¯).On the other hand,〈gk,x−xk〉=〈ek,x−xk〉−λk〈∇1d(xk,xk−1),x−xk〉In view of (4.17) and that λkis bounded from below we obtain that〈ek,x−xk〉→0,so it is sufficient to analyze the convergence of−λk〈∇1d(xk,xk−1),x−xk〉.From Definition 3.2,(Iii), we obtain that(4.21)−λk〈x−xk,∇1d(xk,xk−1)〉≥λk[H(x,xk)−H(x,xk−1)].We analyze two cases: If {H(x, xk)} converges, thenλk[H(x,xk)−H(x,xk−1)]→0,since {λk} is bounded.If {H(x, xk)} does not converge, then this sequence cannot be monotonically decreasing, since it would otherwise be convergent as a nonnegative sequence. Thus, there are infinitely manyk∈Nsuch thatH(x,xk)≥H(x,xk−1). Let{kl}⊂Nbe a subsequence withH(x,xkl)≥H(x,xkl−1)for alll∈N,then there existsg¯∈∂∘f(x¯)such that〈g¯,x−x¯〉=liml→∞〈gkl,x−xkl〉=lim supl→∞〈gkl,x−xkl〉≥λk[H(x,xkl)−H(x,xkl−1)]≥0.□Under assumptions A, B, C,(d,H)∈F(C¯),satisfying the condition(Ivi)and(Iviii)instead of(Ivii),(4.17) and (4.18), and furthermore {λk} is bounded from above, the generated sequence {xk} converges to a stationary pointx¯∈Cof f.Its is immediate from Remark 4.5 and the proof of the above theorem.□In the last subsection we prove the convergence of the sequence {xk} to a stationary point of f onC¯which may be in the worst case a saddle point. In this subsection we give some conditions to guarantee the convergence of the above sequence to a global minimum point. Consider the following minimization problem:(4.22)min{f(x):gj(x)≤0;xi≥0;i=1,…,n;j=1,…,m}where f, gj:IRn→ IR are quasiconvex, f is continuously differentiable and gjare differentiable. LetC={x∈IRn:gj(x)<0;xi>0;i=1,…,n;j=1,…,m},and suppose that C is nonempty, f is bounded from below,(d,H)∈F+(C¯),and {λk} bounded from above, then from Theorem 4.3 we have that the sequence {xk} converges to a pointx¯∈IRnwhich is a KKT point of the problem (4.22), that is, there existλ¯∈IRmands¯∈IRnsuch that:∇f(x¯)+∑j=1mλ¯i∇gj(x¯)−s¯=0,gj(x¯)≤0,λ¯jgj(x¯)=0,si¯xi¯=0,si¯,xi¯≥0.Corollary 4.1Consider the problem (4.22) and suppose that f, gj:IRn→ IR are quasiconvex, f is continuously differentiable and gj are differentiable functions. Suppose also that f is bounded from below,(d,H)∈F+(C¯)or(d,H)∈F(C¯)satisfying the condition(Ivi)and(Iviii)instead of(Ivii), {λk} is bounded from above. If one of the following condition is satisfied:(a)limk→∞λk∂d∂xi(xk,xk−1)<0,for some ilimk→∞λk∂d∂xi(xk,xk−1)>0,for some i such thatxi¯>0limk→∞λk∂d∂xi(xk,xk−1)≠0,and f is twice differentiable in a neighborhood ofx¯then,x¯is a global minimum of (4.22).From Remark 4.12λk∂d∂xi(xk,xk−1)=eik−∂f∂xi(xk)as ek→ 0 and from Theorem 2.2 we have the aimed result.□•The main motivation of the paper was to extend the global convergence properties of an inexact proximal point method from the convex case to the quasiconvex one. However, when f is not convex and considering the errors criterion from the literature it was difficult, at least for us, to obtain the claim. So we introduce the following criterion, which is new even for the convex case,∥xk−1−xk−ek∥≤max{∥ek∥,∥xk−xk−1∥}.The above condition was used to prove that the sequence {f(xk)} is decreasing. Then, to ensure that the sequence {xk} satisfy the Quasi-Féjer convergent property we impose the following conditions:(5.23)∑k=1+∞∥ek∥λk<+∞and(5.24)∑k=1+∞|〈ek,xk〉|λk<+∞.It might be possible to get rid the condition (5.24) for a class of induced proximal distance following the paper of Kaplan and Tichatschke (2004) who presented an analysis how to get rid the assumption (5.24) for a class of Bregman functions which includes the standard entropy kernel and all strongly convex Bregman functions. They showed also the convergence of the proximal point method with the logarithmic-quadratic distance, obtained by Auslender, Teboulle, and Ben-Tiba (1999b), as also of a particular entropy-like distance without the condition (5.24) but using strongly the monotonicity (convexity for the minimization case) of the operator. For the quasiconvex case, we can see that the extension is not immediate and requires further analysis, so we may consider this research as a future work.Another future research is to substitute the criterion (4.11), (4.17) and (4.18) by another one. A possibility may be to introduce a relative error condition what whenC=IRnin (4.9) and consideringd(x,y)=12∥x−y∥2is1λk∥gk∥≤b∥xk−xk−1∥,with b > 0 and gk∈ ∂f(xk), for some subdifferential ∂. The above condition permits as well to consider the forward-backward algorithms and the Gauss–Seidel proximal alternating minimization algorithm, see Attouch et al. (2013).If the objective function of (4.9) is proper, lower semicontinuous and quasiconvex function but it is not locally Lipschitz, we can introduce the following method, which we call (IPPM)’: Initialization: Let a sequence of real positive numbers {βk} and an initial point(5.25)y0∈CMain Steps: Fork=1,2,…,and givenyk−1∈C,find ykandsk∈∂^f(yk)such that(5.26)∥yk−1−yk−ek∥≤max{∥ek∥,∥yk−yk−1∥}where(5.27)ek=sk+βk∇1d(yk,yk−1)with∂^is the set of general subgradients (also called Fréchet subdifferential), see Definition 2.1 of Papa Quiroz and Oliveira (2012) and d is a proximal distance such that(d,H)∈F(C¯).Stop Criterium: Ifyk=yk−1,or0∈∂^f(yk)then stop. Otherwise to dok−1←kand return to Main steps.. In this case, all the results from Remark 4.1 to Theorem 4.2 can be easily adapted and thus to obtain the convergence of the sequence {yk} to a pointy¯∈V+,whereV+:={x∈C¯:f(x)≤infj∈INf(yj)}.Unfortunately,∂^fis not locally bounded aty¯,see Theorem 9.13 of Rockafellar and Wets (1998), and so the extension of Theorem 4.3 is an open question.To compare our results with respect to the work of Attouch et al. (2013), consider the optimization problemmin{f(x):x∈IRn}wheref:IRn→IR∪{+∞}is a lower semicontinuous function bounded from below satisfying a Kurdyka-Lojasiewicz condition, see Definition 2.4 from Attouch et al. (2013). It can be observed that both (IPPM) and (IPPM)’ use another subdifferential and, furthermore, they does not satisfy the condition (36) from Algorithm 2 of Attouch et al. (2013). So, in general, our approach and the Attouch et al. approach are different. However, if we consider the limiting subdifferential in (IPPM)’and the subproblemsf(.)+βk2∥.−yk−1∥2are convex for some βkfor each k, then the sequence given by (5.27) of (IPPM)’ satisfies (36) and (37) of the algorithm 2 of Attouch et al. (2013), and if, furthermore, we substitute the condition (4.11) by the condition (39) of that paper, the (IPPM)’ algorithm might be seen as a particular case of the Algorithm 2 of Attouch et al. (2013).We give some conditions on f and d to obtain the (local) global strongly convexity offk(.)=f(.)+λkd(.,xk−1).A class of these functions are the functions f such that ∇f(.) are (locally) Lipschitz. For the local case, a critical point is that in each iteration of the (IPPM) algorithm is need to known a local Lipschitz constant defined on some ball which is a hard problem. Another critical point is when local just means a small region, in this case our method only make very small slow progress. To overcome these difficulties we propose the following methodology: for simplicity suppose that f ∈ C2(IRn) then ∇f is locally Lipschitz onIRnand thus for each x ∈IRnthere existϵ(x)>0and L(x) > 0 such that∥∇f(y)−∇f(z)∥≤L(x)∥y−z∥,for ally,z∈B(x,ϵ(x)).Taking in particular the pointx=xk−1of the (IPPM) algorithm, which is known, there existϵk−1=ϵ(xk−1)>0andLk−1=L(xk−1)>0such that∥∇f(y)−∇f(z)∥≤Lk−1∥y−z∥,for ally,z∈B(xk−1,ϵk−1).Assuming that d(., w) is strongly convex with respect to the first variable and with respect to the norm ||.||Bwith constant α, from Remark 4.3 ifλk>Lk−1αλmin(B)we have thatf(.)+λkd(.,xk−1)is strongly convex inB(xk−1,ϵk−1).In practice, we should find, in each iteration,ϵk−1and a positive parameter λksuch thatf(.)+λkd(.,xk−1)is strongly convex inB(xk−1,ϵk−1),so we propose the following (theoretical) method:Initialization: Fixϵ=1andTol=10−6Main Steps: Whileϵ>Toldofk=f(.)+λd(.,xk−1),where λ is a parameter to be determinated. If fkis strongly convex inB(xk−1,ϵ),for some λ, then takeϵk−1=ϵ,λk=λandϵ=Tol/2Otherwise, takeϵ=ϵ2.End of Main Steps. The proposed method finishes in two situations:•Ifϵk−1and λkare found, in this case we may use a (efficient) local algorithm to find the pointxk=argmin{f(x)+λd(x,xk−1):x∈B(xk−1,ϵk−1}Ifϵk−1<Tol,then the (IPPM) algorithm finishes atxk−1which is an approximate critical point of the problem.On the other hand, for the class of functions f which are not locally or globally strongly convex, which is the general case of our algorithm, we might consider the equation∇f(.)+λk∇1d(.,xk−1)=0and try to adapt the Kantorovich Theorem applied to the pointxk−1for each k. This idea has not been yet studied in the literature and it is a motivation to develop it in a future research.Other future works may be the extension of this method to solve multiobjective quasiconvex minimization problems as also to solve quasiconvex vector optimization problems.In modern optimization, proximal algorithms are important as a basic block for structured optimization. They can handle the non-smooth part of the objective function or constraints, see for example the forward-backward algorithm, alternating proximal minimization algorithms. In this perspective it would be interesting to study these algorithms in the quasiconvex case.

@&#CONCLUSIONS@&#
