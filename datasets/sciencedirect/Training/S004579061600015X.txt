@&#MAIN-TITLE@&#
A novel ensemble approach using individual features for multi-focus image fusion

@&#HIGHLIGHTS@&#
We developed Ensemble-Individual-Features (Ens-IF) approach for multi-focus image fusion by combining the decision information of individual features.The proposed Ens-IF approach is superior in comparison to the individual pixel-level and the feature-level fusion approaches.The proposed approach can be employed as a post-processor in medical imaging and confocal microscopy systems to reduce the blurring artifacts.

@&#KEYPHRASES@&#
Multi-focus,Image fusion,Ensemble,Confocal microscopy and CT images,

@&#ABSTRACT@&#
Graphical abstractImage, graphical abstract

@&#INTRODUCTION@&#
The process of image fusion integrates complementary information in multiple images to generate an image that contains more information [1]. However, in multi-focus image fusion, images with multiple focuses are combined to get an image that contains all-in-focus objects. Defocussed images, generally, result from limited field depth of optical lenses. While image is being captured, only objects at a particular distance are in-focus, and other objects are defocused. Thus, the captured image does not contain clear information that is essential for pattern recognition and computer vision applications. Image fusion approaches are also being used in medicine, surgery, flight aviation, robotics, sensor vision, and military applications.In medical diagnostics, medical images are acquired through different modalities. These images are also degraded with blur during their acquisition [2]. Positioning-aids are used in computed tomography (CT) and magnetic resonance (MR) imagery to prevent voluntary motion. In many cases, it is difficult to control patient movement. For example, pediatric patients, involuntary organ motion, respiration, and heart beating introduces blurring artifacts in images. As a result, the quality of CT and MR images is deteriorated. The effects of such blurring artifacts can be minimized through efficient image fusion techniques. By employing fusion technique, we can enhance the image quality of the imaging system. In this scenario, the proposed fusion approach might save the critical time of a radiologist and/or therapist consumed in manual adjustment of the imaging system.Blurring artifacts commonly occur in confocal microscopic images. The operator carefully adjusts the parameters of the microscope to capture high quality images of living tissues. These images are used for extraction of data at cellular resolution in the studies of biological systems. However, the use of such data acquisition technique becomes limited due to the introduction of diffractive blurring in images. As a result, image pixels corresponding to each object point is overlapped with neighboring pixels. To minimize this diffractive blurring problem, the frames of the confocal microscopy can be fused using an efficient fusion approach. This frames fusion step may lead to sharper image resulting in reduction of data errors. In this study, we have used various confocal microscopy and medical images.Previously, researchers have proposed both pixel-level and feature-level image fusion approaches in spatial and transform domains. In pixel-level image fusion approaches, image pixels are moved from the source image to the fused image under the fusion rule. For pixel-level fusion in transform domain, different multi-resolution transforms have been employed such as Laplacian pyramid [3], ratio of low pass pyramid [4], gradient pyramid [5], and morphological pyramid [6]. Another interesting work for pixel-level image fusion was done by Nadu et al. in principal component analysis (PCA) and discrete wavelet transform (DWT) domains [7]. Pixel-level image fusion is also carried out using Baye's entropy and spatial frequency [8]. The problem with pixel-level transform based approaches is that they are not translation invariant. A slight movement in object position may cause degradation in fusion performance.In feature-level image fusion, the source images are fused using features extracted from spatial or frequency domain. Feature-level image fusion in, wavelet transform domain, was proposed with simple max rule[9]. On the other hand, the decision-criteria based on fuzzy logic is proposed in [10]. Shutao et al. performed image fusion by fusing the source images using simple averaging to perform segmentation on the intermediate fused images. Corresponding segmented regions are then fused according to spatial frequency [11]. Long et al. performed image fusion based on clarity enhanced segmentation and regional sparse representation [12]. Further, Fuping et al. performed fusion using fractional differential and non-subsampled contourlet transform [13].Recently, feature-level image fusion approaches are also proposed using machine learning (ML) algorithms for the classification of focused/defocused image regions [14–17]. Artificial neural network (ANN) and support vector machine (SVM) based fusion approaches are proposed using visibility, spatial frequency, and edge features [14,15]. Another efficient variant of ANN, probabilistic neural network (PNN), is developed for feature-level image fusion in [16]. The main problem with ML based fusion approaches is that we have to train ML algorithms for the accurate construction of a decision map. We have to prepare training data with known target labels related to the focused/defocused image blocks. For this purpose the target labels are, mostly, assigned manually. This is tedious and sometimes intractable. If this labeling process is not done precisely, then the fusion performance of ML approaches is degraded for the diverse types of blurred images [18]. Under this scenario, we have proposed a novel Ensemble-Individual-Features (Ens-IF) approach to exploit information of individual features. Ensemble features has achieved considerable importance due to its effective exploitation of individual feature spaces. In ensemble approach, generally, the shortcomings/limitations of one feature are replaced with the advantages of others [18–20].In the proposed Ens-IF approach, diverse types of individual features are extracted from frequency and spatial domains. These informative features decide the more focused blocks of the source images. Next, the predicted labels of individual features are aggregated through majority voting to get the fused image. The quantitative and qualitative analysis reveals that our approach has produced more useful images than pixel-level approaches in PCA and wavelet domains. Further, our approach is more efficient than ML based feature-level image fusion approaches. The proposed Ens-IF approach does not need training time that is commonly required in ML based approaches. The main novelty of the proposed ensemble features approach is that it effectively utilizes the useful information of individual features to construct accurate decision maps for the fused images. The improved performance is evaluated for various confocal microscopy and CT blurred images.The remaining paper is organized as follows: In Section 2, we describe the proposed approach in detail. In Section 3, we explain the performance measures. Experimental details are given in Section 4. Results are discussed in Section 5. Finally, concluding remarks are given in Section 6.Fig. 1shows the block diagram of the proposed Ens-IF approach for multi-focus image fusion. In the proposed approach, first, we extract useful features such as visibility, spatial frequency, energy of gradient, variance, edges, entropy, correlation, discrete cosine transform (DCT), and DWT coefficients. These individual features determine correctly the focused blocks for the multi-focus image fusion. In the second stage, the predicted labels of individual features are combined using majority voting.For the fusion of multi-focus images, the input images I1 and I2 are decomposed into blocks of size M × N. Currently, for each blockBI1iandBI2iof input I1 and I2 images, 11-dimension feature vectors are formed, i.e.VI1i={v11,v12,v13,…,v111}andVI2i={v21,v22,v23,…,v211}. The detail of features computation is given in Section 2.1. For each ith block, the decision of focused/defocused image block is made using the values of eleven individual features as following:(1)dji={1ifv1j≥v2jforj=1,2,…,110otherwiseThe 11-dimension vector,dji, of binary values represent the focused/defocused categories of the image blocks. Then majority voting is used to obtain the combined decision. The block of the fused image is assigned to the class that receives the maximum votes. The ensemble decision valueu^iis computed as:(2)u^i={1forfmaxiatl=10forfmaxiatl=0The value of maximum votes (fmaxi) is computed as:(3)fmaxi=max∑j=111δ(di,j,l)forl∈{0,1}where,δ(di,j,l)={1ifdi,j=l0otherwise.The ith block of the fused imagefiis selected from the images I1/I2 corresponding to ensemble decision valueu^i, i.e.fi={BI1iifu^i=1BI2iifu^i=0This process is repeated for all blocks of the image I1/I2 to construct the decision map, which is then used to develop the fused image F.In the Ens-IF approach, diverse type of features information is used to determine the blurring effect in the source images. We have used frequency, gradient, and contrast based functions to measure the image focus. In the literature, the features based on these functions are commonly used to compute the blurring effects [7,21,22]. The higher values of these focuses measures represent the more focused image. On the other hand, for a defocused image, the lower values of high-frequency, gradient, and contrast functions would occur [23]. As a result of lower values, the sharp edges become faint and the blurring occur in small bright objects in the image [23]. Both the frequency and spatial domain features are extracted to discriminate the focused/defocused image blocks.Fig. 2shows a set of eleven features including four spatial domain features of visibility, edge, spatial-frequency (SF), and energy-of-gradient (EOG). Four frequency domain features include three DWT features and one DCT feature. The feature vector is made more discriminative by extracting texture features of entropy, standard deviation (STD), and variance (Var).Spatial features of visibility [22], spatial frequency (SF) [24], edge [25], and energy-of-gradient (EOG) [26] are employed to measure the focus in the image. Visibility (Vis) measure is derived from the human visual system. It calculates the intensity variation in the block pixels using the average block intensity value. On the other hand, SF measure computes the activity-level in the image by calculating the variation in frequency among the rows and columns of image blocks. As a result, the higher SF value indicates the more focused image block. Similarly, the higher value of EOG shows the more focused image. The image focus is also measured by computing the edge information. The focus image contains more edges information as compared to the defocus-image. We have used Canny Edge Detection algorithm for the detection of edges in the image blocks [25]. This algorithm works in five main steps of (a) image smoothing; (b) finding the image gradient; (c) calculating local maxima as edges; (d) finding double thresholding; and (e) detecting the weak connected edges. These four spatial features are computed as:(4)Vis=∑m=1M∑n=1N|B(m,n)−μBμBα+1|where μβis average intensity value of image block and αis a constant.(5)SF=(RF)2+(CF)2where, RF and CF are the row and column frequencies. The RF and CF are computed as:(6)RF=1MN∑m=1M∑n=2N(B(m,n)−B(m,n−1))2(7)CF=1MN∑m=1M∑n=2N(B(m,n)−B(m−1,n))2(8)EOG=∑i=1M∑j=1N(Bi2+Bj2)where,Bi=B(i+1,j)−B(i,j)andBj=B(i,j+1)−B(i,j)Three texture domain features of entropy, STD, and variance (Var) are also employed to measure the blurring effect. The entropy feature shows the average information in the image and its higher value indicates the more focused image. However, Var and STD measure highlight the contrast in the image. An image with high contrast would represent the more focused image. The values of these texture features are computed as:(9)Entropy=∑i=1LP(i)log2(P(i))where, L and P(i) represent the maximum possible gray levels and the probability of ith gray level.(10)STD=∑i=1L(i−i′)2h(i)where, L represents the number of gray levels,i′=∑i=0LihFand hFis the normalized histogram image block.(11)Var=1MN∑m=1M∑n=1N(B(m,n)−μB)2where, μβdenotes the mean value of the image block.Frequency features in DWT and DCT domain are used to measure the blur in the image blocks. DWT based focus measure is computed at coefficient level or window level. The coefficient based activity-level is measured by considering each coefficient separately [7]. However, window based activity-level is measured by averaging DWT coefficients of the image block [21]. By decomposition of image blocks to first level, we obtained three detail subbands and one approximation subband. The approximation subband does not provide useful edge information. Hence, the approximation subband is not included in the feature set. Only three detail subbands HH, HL, and LH are used for features measure. The wavelet activity-level (feature) is measured using “biorthogonal” function and then averaging the coefficients over the image block. On the other hand, DCT coefficients show the variation in image frequency and its high-frequency energy indicates the focus in the image block. The value of DCT energy (E) is computed by removing the value of the direct-components as follows:(12)E=[∑u=0M−1∑v=0N−1B2(u,v)]−B2(0,0)(13)where,B(u,v)=2Mc(u)c(v)∑i=0M−1∑j=0N−1B(i,j)cos(2i+1)πu2Mcos(2j+1)πv2Nc(0) =1/2, c(u) = 1, and u, v = 0,1, …,M-1.The performance of the proposed approach is evaluated in terms of SF, STD, Quality of edge (QE), and mutual information (MI). The higher value of SF and STD indicates the detail image with better focus. The computation of SF and STD measures are given in Sections 2.1.1 and 2.1.2, respectively. QE measure is computed using human perception information extracted from the source and fused images. The value of QE measure is computed as:(14)QE=∑i=1N∑j=1Mαx(i,j)γx(i,j)+αy(i,j)γy(i,j)∑i=1N∑j=1Mγx(i,j)+γy(i,j)The edge preservation values, αx(i, j)  and  αy(i, j), from the input images to the fused image are formed by multiplying the sigmoid mapping function of relative strength and orientation factors. In general γx(i, j)  and  γy(i, j) are functions of edge strength.MI measure indicates similarity between the fused image and the original image. Its numerical value is computed as:(15)MI=∑l1=1L∑l2=1LhO,F(l1,l2)log2hO,F(l1,l2)hO(l1)hF(l2)where, hO,hF, and hO, Frepresent the original, fused, and joint image histograms, respectively. L indicates the number of intensity-levels in the original and the fused images.Extensive experiments are performed to evaluate and compare the performance of the proposed approach. For this purpose, several real blurred images of Flowers, Calendar, Tree, and Lab are retrieved from www.imagefusion.org. The blurred confocal microscopic images and medical CT images of Abdomen, Brain, and Chest are acquired from the National institute of Laser and Optics Labs (NILOP), Islamabad, Pakistan. These blurred images are selected carefully to provide complementary information useful for the fusion.Since the multi-focus images contain non-uniform magnitude of focused pixels, thus in block based fusion approaches, the quality of fused images may be affected if large block size is used. Large block may contain both the focused and defocused pixels. Therefore, in order to compute the discriminant feature information, small block size is preferably used in multi-focus image fusion. Further, large block size may retain the blurring artifacts in the final fused image. Hence, in order to minimize the blurring artifacts within the block and at the adjacent-boundary of the focus and defocussed blocks, we have selected minimum block size of 3×3. This is the most suitable block size for better quality fused images. During experimentation, we observed with the increase of block size, the blurring artifacts appear at the boundary of the blocks, resulting in the degradation of fused image quality. The computational work of the proposed approach is carried out in Matlab-R11 environment. For 3×3 block size, to compute DWT and DCT coefficients, this software has employed “pad” function to make block size of the power of two.For the comparison, the implementation of pixel-level and feature-level fusion approaches are carried out. The detailed implementation of these approaches is available in [7,11,12,14,21]. We have obtained results using the same procedure and parameters setting as described in these approaches. The pixel-level Image fusion, in wavelet domain, is carried out using discrete stationary wavelet transform with “symlet” function of second order [7]. The source images are decomposed to first-level. The pixel-level image fusion, in PCA domain [7], is developed by arranging the blurred-images X and Y in two column vectors. Then eigenvector and eigenvalues of the covariance matrix are calculated. The values of normalized components(P1=0.55,P2=0.45)are calculated. The fused image F(i,j) is obtained using the blurred-images X(i,j) and Y(i,j) as:(16)F(i,j)=P1X(i,j)+P2Y(i,j)ML based feature-level fusion approaches are implemented using PNN [14] and SVM [21] algorithms. SVM fusion approach has obtained the optimal values of model parameter atc=70. The width of Gaussian function with radial-basis-kernel is found to beσ=0.50. However, PNN approach is developed using optimal spread parameter atσ=0.9.

@&#CONCLUSIONS@&#
