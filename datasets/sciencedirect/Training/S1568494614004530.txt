@&#MAIN-TITLE@&#
A fast and efficient two-phase sequential learning algorithm for spatial architecture neural network

@&#HIGHLIGHTS@&#
We review the structure and backpropagation-based learning algorithm of spatial architecture neural network (SANN), and analyse its weight connecting mode.We proposed a two-phase sequential learning algorithm (SFSL-SANN) to train the span-output-weight and feedforward-output-weight sequentially.The new learning algorithm is faster convergence and time-saving than BP-SANN, and produces better performance.This learning scheme can be extended for training other complex multilayer neural networks with modified learning algorithms.

@&#KEYPHRASES@&#
Spatial architecture neural network (SANN),Span-output-weight,Feedforward-output-weight,Sequential learning,

@&#ABSTRACT@&#
Spatial architecture neural network (SANN), which is inspired by the connecting mode of excitatory pyramidal neurons and inhibitory interneurons of neocortex, is a multilayer artificial neural network and has good learning accuracy and generalization ability when used in real applications. However, the backpropagation-based learning algorithm (named BP-SANN) may be time consumption and slow convergence. In this paper, a new fast and accurate two-phase sequential learning scheme for SANN is hereby introduced to guarantee the network performance. With this new learning approach (named SFSL-SANN), only the weights connecting to output neurons will be trained during the learning process. In the first phase, a least-squares method is applied to estimate the span-output-weight on the basis of the fixed randomly generated initialized weight values. The improved iterative learning algorithm is then used to learn the feedforward-output-weight in the second phase. Detailed effectiveness comparison of SFSL-SANN is done with BP-SANN and other popular neural network approaches on benchmark problems drawn from the classification, regression and time-series prediction applications. The results demonstrate that the SFSL-SANN is faster convergence and time-saving than BP-SANN, and produces better learning accuracy and generalization performance than other approaches.

@&#INTRODUCTION@&#
Artificial neural networks (ANNs) are computational (or mathematical) models that are inspired by the structure and/or functional aspects of biological neural networks (BNNs) to solve actual real life problems. Since ANNs can “learn” from samples which much like human beings and other advantages, e.g., the capability of computing any arithmetic or logical function in principle, good at providing fast and close approximations of the correct answer, best at identifying patterns and trends in data, etc., ANNs have been widely used in a variety of real applications with great success in recent years [1].Inspired by the connecting mode of neocortex neurons and the lateral inhibition mechanism, a novel ANN model, named spatial architecture neural network (SANN) [2] has been proposed in the previous work to solve real world regression and classification applications. Similar with the neuronal connect modes of excitatory pyramidal neurons and inhibitory interneurons within neocortex, the SANN has a multilayered structure. It consists of a number of neural layers of a similar structure cascaded one after another, which including feedforward and spatial span connections between different layers and also with recurrent lateral inhibitory connections among neurons in each hidden layer. One of the main structural features of SANN is the span connection between different neuronal types from any two non-adjacent horizontal layers, which corresponds to the canonical circuitry of pyramidal neurons in the upper layers II and III of neocortex. It means that these connections may form cortical columns and span all the thickness of the cortex [3]. And the second major structural principle of SANN is the introducing of lateral inhibitory connections between adjacent hidden neurons (within the same layer), which is used to organize inhibitory circuits and enhance the contrast of perception areas.Although the structure of ANN defines its function, the learning algorithm is the other key issue, which can guarantee and maybe even improve the performance of network. A supervised error backpropagation-based (BP) learning algorithm suitable for the special structure of SANN guaranteed that SANN can be applied in real applications such as nonlinear function approximation, pattern recognition and time-series prediction problems. The SANN training utilizing the BP algorithm (which can be henceforth abbreviate as BP-SANN) can obtain a higher training accuracy and generalization ability than multilayer feedforward neural networks (MLFNNs) and other neural network methods. However, there is still a major unresolved problem: there is more time need for training SANN and sometimes the learning efficiency is not very optimistic because of the existing of spatial span connections from different layers. This work is focused on how to reduce the training time and improve the learning efficiency without weakening the generalization properties.According to the connect characteristics of artificial neurons, the adjustable output weights (linking to the output layer) of SANN can be divided into two types: the span-output-weight and the feedforward-output-weight. Based on this concept, this work proposes a two-phase sequential learning strategy for SANN called span-output-weight and feedforward-output-weight sequential learning (SFSL) whose convergence speed of learning can be faster than BP-SANN while obtaining better generalization performance. Therefore, the proposed learning scheme tends to have less training time and have better generalization ability for BP-SANN.The paper is organized as follows. Section 2 first gives a brief review of SANN including the topology and gradient-based learning algorithm. In Section 3, we present the proposed two-phase sequential learning scheme SFSL, and performance evaluation of the SFSL-SANN is shown based on the benchmark problems in the areas of classification, regression, and time-series prediction is shown in Section 4. Finally, discussion and conclusions based on this work are highlighted in Section 5.The basic concepts, topology and back-propagation based learning algorithm of SANN are briefly reviewed in this section to provide the necessary background knowledge of the proposed two-phase sequential learning strategy of SANN. A brief summary and mathematical description of SANN are given first.SANN, an ANN model with spatial span connection and local lateral inhibition, has been developed based on the neuron connecting mode of neocortex and the information transformation mechanism of lateral inhibition. It originates from the ANN theory and cerebral cortex anatomical knowledge, especially the structure of neocortex. Different from the unified MLFNNs, SANN introduces the span connections between neurons from any two nonadjacent layers, and also adds lateral inhibitory connection between neurons from the same hidden layer. These two innovations are corresponding to the connection of pyramid neurons span all the thickness of cortex and the lateral inhibition of interneurons in the same layer, respectively. The structure of the (L+1)-layer SANN is shown in Fig. 1.In the following description, l∈[0, L] is called the layer index of the (L+1)-layer SANN. Where, l=0 denotes the input layer, l∈[1, L−1] indicates the lth hidden layer, and the output layer index is L. The neuron number of the lth layer in SANN is defined as nl. So, if a problem has n inputs and m outputs was learned by a SANN, then we have n0=n and nL=m.Without loss of generality, consider N arbitrary distinct samples (xq, tq)|q∈[1, N], where xq=[xq1, xq2, …, xqn]T∈Rnis a n×N input vector and tq=[tq1, tq2, …, tqm]T∈Rmis a m×N target vector. The output of a (L+1)-layer SANN can be represented by(1)oq=∑k=1mfL∑l=1L−1∑j=1nlωkjoqjl+bl+∑i=1nωkixqi+b0,i∈[1,n],q∈[1,N]where xqiis the ith input of the qth sample, oqis the output of the qth sample computed by SANN;oqjlis the output of the jth hidden neuron from lth layer with respect to the qth input vector xqi; ωkjis the weight connecting the jth neuron of lth layer and the kth output neuron; blis the threshold of the lth layer neuron; fl(·) is the activation function of the lth layer neuron.For the jth hidden neuron from lth layer with respect to the qth sample, the mathematical representation ofoqjlis given by(2)oqjl=fl[xqjl−x˜qjl]=fl∑l=0l−1(ωijoqil)−∑r=1,r≠jnlvrj(oqrl−θrj)wherexqilis the inner product of input and weight, including signals received from the (l−1)th layer neurons and the spatial span connections from the range of [0, l−2] layers, whilex˜qjlis the inhibitory input after lateral inhibited by surrounding neurons. θrjis the inhibiting threshold value of jth neuron due to rth neuron;vrjis the lateral inhibitory coefficient, which is a real number in [0, 1], with “0” meaning no inhibition and “1” meaning full inhibition.From the approximation point of view, a SANN with L+1 layers can approximate N distinct training samples with any arbitrary error. The goal of SANN learning process is to find the best combination of connection weights and biases for achieving the minimum error between desired output and actual network output of samples. Namely, to find specificωki*,ωji*,ωkj*andbl*such that(3)∑k=1mfL∑l=1L−1∑j=1nlωkj*oqjl+bl*+∑i=1nωki*xqi+b0*−tq=minωkj,bl∑k=1mfL∑l=1L−1∑j=1nlωkjoqjl+bl+∑i=1nωkixqi+b0−tqwhich is equivalent to minimizing the cost function(4)E=12∑q=1N(tq−oq)2.In the minimization procedure of BP-SANN, vector ω is iteratively adjusted as follows:(5)ω(c+1)=ω(c)−η∂E∂ω.Here η is the learning rate; vector ω is the set of weights and biases parameters, c is the iterative index.It is worthwhile to note that the lth layer neuron will not only receive the back-propagation error directly from the (l+1)th layer, but also receive the (l+2)th to Lth layer's back-propagation error through the spatial span connections. Take a 4-layer SANN for example, the gradient direction of the weight ωjilinking the input layer to the 1st hidden layer is the partial derivative of E with respect to ωji, namely:(6)δEδωji=δEδoqk·δoqkδoqg2·δoqg2δoqj1+δoqkδoqj1·δoqj1δωji.The BP-SANN presented above is a gradient-based learning algorithm. It has been proved that this method not only makes learning accuracy relatively high but also produces better generalization performance than MLFNNs. However, there are some drawbacks to back-propagation learning algorithm. For one, the introducing of spatial span connection in SANN brings more weights need to be trained in BP-SANN. A second shortcoming is that the lateral inhibitory connections in hidden layers increase the computational complexity of weight updating. Therefore, BP-SANN may be time-consuming and slow convergence in most real applications. The aim of this paper is to propose a fast and accurate two-phase sequential learning strategy to reduce the training time and improve the learning capability.Let us focus on the weights of SANN at first. It is apparent that there are three types of weights: the span weight, the feedforward weight and the inhibitory weight. For simplicity, the initial inhibitory coefficient generated follows Gaussian distribution and was fixed during the learning process. Thus, only the feedforward and span weights need to be trained. According to whether the weights is connect to the output neuron or hidden neuron, the tunable weights can be divided into output-weight and hidden-weight. Furthermore, combined the span and feedforward connecting modes, there are span-hidden-weight, span-output-weight, feedforward-hidden-weight and feedforward-output-weight. In order to improve the learning efficiency, only the output-weights are learned in the proposed SFSL approach, namely the span-output-weight ωkiand feedforward-output-weight ωkjare trained sequentially. Here, ωkiis the weight connecting from the ith hidden neuron of lth layer to kth output unit, where l∈[0, L−2]; and ωkjis the weight connecting the jth neuron from the (L−1)th hidden layer and the kth output unit.Before introducing the two-phase sequential learning algorithm, the problem formulation is presented.The proposed algorithm assumes that all the samples are available for training. Theoretically speaking, a (L+1)-layer SANN with nlneurons in the lth layer can approximate these N samples with zero error when there are N hidden neurons, namely∑l=0L−1nl=N. It implies that there existωki*,ωji*,ωkj*andbl*such that(7)∑k=1mfL∑l=1L−1∑j=1nlωkj*oqjl+bl*+∑i=1nωki*xqi+b0*=tq,q=1,2,…,N.In order to convenient representation,ωkjlis used to indicate the weight connecting from the jth neuron of lth layer to the kth output neuron. Then, Eq. (7) can be rewritten as:(8)∑i=1n0ωki0xqi+b0+∑l=1L−2∑j=1nlωkjlojl+bl+∑g=1nL−1ωkgL−1ogL−1+bL−1=tq,q=1,2,…,N.The above N equations can be written compactly as:(9)ΩΘ+ΨH=TwhereΩ=ωki0⋯ωkjl⋯ωkpL−2,Θ=xi⋯ojl⋯opL−2T,Ψ=[ωkgL−1]andH=[ogL−1].Before learning, the synapse weights and biases of SANN are all initialized to small random values, while the inhibitory coefficients during hidden layers are Gaussian distributed. In the first phase of learning, the steps of span-output-weight learning are as follows.Step 1: According to the mathematical description of SANN and the statement of problem in Section 3.1, we can obtain the span-output ΩΘ and the feedforward-output ΨH of SANN, which are generated from the training samples and the initialized parameters.Step 2: For calculating the span-output-weight, the Eq. (9) can be written as(10)ΩΘ=T−ΨH.Furthermore we have(11)ΞΛ=Δwhere,Ξ=ΘT=x11⋯xn1o11l⋯onl1lo11L−2⋯onL−21L−2⋮⋱⋮⋯⋮⋱⋮⋯⋮⋱⋮x1N⋯xnNo1Nl⋯onlNlo1NL−2⋯onL−2NL−2=xiqT···ojqlT···opqL−2TN×n0+n1+...+nL−2(i∈[1,n],j∈[1,nl],p∈[1,nL−2],q∈[1,N]),Λ=ΩT=ω110⋯ωm10⋱⋮ω1n0⋯ωmn0⋮ω11l⋯ωm1l⋮⋱⋮ω1nll⋯ωmnll⋮ω11L−2⋯ωm1L−2⋮⋱⋮ω1nL−2L−2⋯ωmnL−2L−2=(ωki0)T⋮(ωkjl)T⋮(ωkpL−2)T(n0+n1+...+nL−2)×m,i∈[1,n]j∈[1,nl]p∈[1,nL−2]k∈[1,m]andΔ=(T−ΨH)T.Ξ is called the span-output-matrix of SANN. The ith column of Ξ is the output vector of the ith neuron (which from the lth layer, l∈[0, L−2], and span connecting to output neurons) with respect to inputs x1, x2, …, xN, and the pth row of Ξ is the output vector of span-output layers with respect to input xq. Λ is called the span-output-weight matrix of SANN.Based on the initialized parameters, the training of span-output-weight is equivalent to finding a least-squares solution Λ* of the linear system ΞΛ=Δ, i.e.(12)ΞΛ*−Δ=minΛΞΛ−Δ.Without loss of generality, there exist a matrix Ω=Ξ†, the Moore–Penrose generalized inverse of matrix Ξ, such that ΩΔ is a minimum norm least-squares solution of the linear system ΞΛ=Δ. Then the span-output-weight Λ is estimated as(13)Λ*=Ξ†Δ.The trained span-output-weight Λ* given in Eq. (13) is a least-squares solution of Eq. (11). Here, we consider the case whererank(Ξ)=∑l=0L−2nlthe number of neurons which connecting to the output neurons through the span connections. Under this condition, Ξ† is given by(14)Ξ†=(ΞTΞ)−1ΞT.Substituting (14) into (13), Λ* becomes(15)Λ*=(ΞTΞ)−1ΞTΔ.It is clear from the above analysis that the key point of the span-output-weight learning is to effectively compute the least-squares solution of Eq. (11).The learning of span-output-weight is trying to solve the least-squares solution of system ΞΛ=Δ. Unfortunately, the process cannot guarantee SANN approximating the given problem with very (or arbitrary) small error due to the fact that the matrix Ξ may be nonsquare matrix. If there are enough hidden neurons, i.e.∑l=0L−2nl=N, matrix Ξ is square and invertible, then the SANN can approximate these samples with zero error. However, in most cases, in order to guarantee the generalization capability and reduce the computation order of complexity, the total number of hidden neurons is much less than the number of distinct training samples, i.e.∑l=0L−1nl≪N, so there may have system residual ɛ between SANN output ΩΘ+ΨH and desired output T, i.e.,(16)ΞΛ*+ɛ=Δwhereɛ=ɛ1⋯ɛNT|m×N.So, the second phase of SFSL is to train the feedforward-output-weight, namely finding a best combination of Λ* and Ψ* to minimize the cost functionJ(17)J=12∑q=1N(T−Ω(Λ*)T−ΨH)2where,(18)H=[ogL−1]=fL−1ωghohL−2+∑l=0L−3[ωgioil].For improving the maneuverability and applicability of SFSL, the feedforward-hidden-weight ωgh, which connecting the (L−2)th layer and the (L−1)th layer, was also considered as a part of feedforward-output-weight to be learned. In another words, the second phase of SFSL is to train ωghandωkgL−1.So, many popular useful learning algorithms for training a single hidden-layer feedforward neural network can be modified to train the feedforward-output-weight, such as iterative algorithms and intelligent optimization algorithms. Without loss of generality, a gradient-based iterative learning algorithm for training the feedforward-output-weight ωghandωkgL−1in the second phase of SFSL is given by,(19)∂J∂ωkgL−1=∑k=1m∂J∂o→qk·∂o→qk∂ωkgL−1∂J∂ωgh=∑k=1m∂J∂o→qk·∂o→qk∂o→qgL−1·∂o→qgL−1∂ωghwhereo→qkando→qgL−1are feedforward-outputs of Lth and (L−1)th hidden layer neurons respectively.Finally, the training may be terminated when the training error is below the stopping criterion of MSE (mean square error) tolerance τ. Hence, the most suitable network parameters is given by the value of Λ* and Ψ* whenJreaches its minimum. Overall, the two-phase SFSL algorithm for SANN is summarized as follows.it Phase 1: Span-output-weight learning phase:•Assign random weights which including the input, hidden, output and lateral inhibition weights.Calculate the initial feedforward-output and span-output based on the training sample(s) and the initialized SANN.Estimate the span-output-weight Λ*.Phase 2: Feedforward-output-weight learning phase:•Calculate the new span-output-matrix based on Λ*.Calculate the output error.Iterative update the feedforward-output-weight ωghandωkgL−1.A series of experimental studies is presented in this section to assess the effectiveness of the proposed SFSL-SANN in solving benchmark applications, which were described in Table 1. The performance and advantage of SFSL-SANN are first examined by using the examples of a parity-N problem and machine CPU performance prediction. Then, for performance comparison among the SFSL-SANN and other methods, two benchmark problems are investigated, i.e., Lorenz chaotic time-series prediction and Dst index prediction.All the simulations have been conducted in Matlab 7.11 environment running on an ordinary PC with 3.0GHz CPU. The sigmoidal additive activation function f(·)=1/(1+e−x) has been used in the simulations of BP-SANN and SFSL-SANN. For both BP-SANN and SFSL-SANN with additive hidden neurons, the initial weights and biases value randomly generated from the range [−1, 1], while the lateral inhibitory weights are Gaussian distributed. The output neurons are linear.In order to realize a fair comparison of the learning capability, the same network size in both SFSL-SANN and BP-SANN is used for the same application. For each trial of simulations, the dataset of the application (except for the parity-N problem) was divided into training and testing dataset with the number of samples indicated in Table 1. The process is random based, which means, take machine CPU for example we randomly select 100 samples out of 209 to form the training dataset, and the remaining samples is the testing dataset.SFSL-SANN and BP-SANN have first been compared in the benchmark problem: parity-N problem. The famous difficult problem has often been used for evaluating the ANN architecture and algorithm designs [4–6]. There are 2Ninput patterns in N-dimensional space and the input–output of parity problem is given by(20)y=0,if∑i=1Nxiisevennumber1,if∑i=1Nxiisoddnumberwhere xiis the network input vector, and y is the desired output.Here, N=3. Parity-3 problem can be visually illustrates in three dimensions as shown in Fig. 2. The truth table for the parity-3 function is shown in Table 2. From the approximation aspect of view, the parity-3 problem has been studied here with one hidden neuron, while the MLFNN with one hidden neuron cannot solve this problem [7,8]. The network architecture chosen for this problem is 3-1-1.The error measure adopted here is the MSE (mean square error) Eq. (4). The training time, iteration number, training MSE for all training samples were compared through the maximum, minimum and average values over 30 independent runs. The training process is terminated when the training MSE is less than 1e−15 or the max number of iterations is arrived to 30,000. The experimental results for this parity-3 problem are shown in Table 3.As observed from Table 3, both SFSL-SANN and BP-SANN can obtain much better approximation performances than BPNN (MLFNN with backpropagation learning), which cannot solve the parity-3 problem with one hidden neuron. In addition, the SFSL-SANN apparently performs better than the BP-SANN in terms of the training time and iteration number. Fig. 3illustrates the training time versus the number of iteration for BP-SANN and SFSL-SANN. Compared with SFSL-SANN, BP-SANN requires about 8–16 times iteration numbers and 5–12 times training time. Fig. 4gives the convergence curves of BP-SANN and SFSL-SANN with one sigmoidal additive hidden neuron on parity-3 application. Table 4details the parameters of training MSE relative to iteration number. Table 5lists the training mse and time of SANNs with other neural network approaches for all training samples. Fig. 4 and Tables 4 and 5 confirm that SFSL-SANN apparently has the better convergence rate than BP-SANN and MLFNNs (with PSOGSA and PSO-BP learning algorithms).From Table 5, we can find that SANN is apparently better than the MLFNNs. The results show that SANN is more accurate than MLFNNs, and SFSL-SANN with faster convergent rate as compared to BP-SANN. Although the performances of SANN training with the SFSL algorithm and the BP algorithm are very close, the SFSL-SANN spends less CPU time than the BP-SANN when achieving the same MSE. The training error curve for SFSL-SANN is not only steeper compared to BP-SANN, but also need less iteration. It implies that SFSL-SANN is of fast convergence, high efficiency and good accuracy.Machine CPU performance data, also named computer hardware data set, is a dataset obtained from the UCI Machine Learning Repository (http://archive.ics.uci.edu/ml/datasets/Computer+Hardware). The task of machine CPU problem is to predict the Estimated Relative Performance (ERP) based on a number of computer characteristics [11], namely (1) MYCT: machine cycle time, (2) MMIN: minimum main memory, (3) MMAX: maximum main memory, (4) CACH: cache memory, (5) CHMIN: minimum channels, and (6) CHMAX: maximum channels. There are 209 observations in this data set.In order to compare the proposed approach of SFSL-SANN with BP-SANN, in the first part of this experiment the 6 input attributes and one output attribute have been normalized to the range [−1, 1]. We choose a single-hidden layer SANN to solve this machine CPU problem. For this prediction problem, the termination criteria of the learning process are set as: target training MSE τ=1e−8, maximum iteration number T=5000. In addition, the parameters are set as: learn rate η=0.068, moment term α=0.7. Table 6summarizes the results of BP-SANN and SFSL-SANN with 2, 3 and 5 hidden neurons respectively. 20 trails have been conducted, and the average results are detailed in Table 6.All the evaluations are repeated for 10 times and the average, standard deviation of average, maximum and minimum training time, training root mean square error (RMSE) and testing RMSE are listed in Table 6. Furthermore, we also evaluated the “learning” capability of the network using a cross-validation by taking 50 samples and 159 samples randomly as training and test data. Fig. 5shows the learning error curves for SFSL-SANN and BP-SANN with 5 hidden neurons based on 100 and 50 training samples, respectively.Seen from this case, in the simulations SFSL-SANN can reach better training and testing accuracies with less training time when compared with BP-SANN.In order to compare our method with others available in the literature, in this second part of this experiment the 6 input attributes and one output attribute have been normalized to the range [−1, 1] and [0, 1], respectively. These samples were randomly divided into a training set consisting of 100 samples and a test set consisting of 109 samples. The comparison of SANN with other NN methods is summarized in Table 7.As can be seen from Table 7, SFSL-SANN can obtain best training and testing accuracy, and it spends less training time but with good learning and generalization performance as compared to the BP-SANN. Fig. 5 presents the training MSE of BP-SANN and SFSL-SANN with sample number varied. The learning curve of SFSL-SANN can become steeper even when there are smaller training samples. And the SFSL-SANN can achieve faster convergence and better learning accuracy than BP-SANN.Chaotic time-series prediction is one benchmark problem for test neural networks and algorithms [15–20]. The Lorenz attractor is a solution of the following three ordinary differential equations (ODE) [21]:(21)dxdt=σ(yt−xt)dydt=−xtzt+r(xt−yt)dzdt=xtyt−bztwhere x is the rate of convective overturning, y is the horizontal temperature variation, z is the vertical temperature variation.This experiment is on the prediction of Lorenz chaotic time-series system. The goal is to predict the next future value using d previous values. In order to compare fairly with other works, the parameters are setting as: σ=10, r=28, b=8/3. The chaotic time-series is generated from the numerical integration of x(t) component of Eq. (21) by Matlab ode45 solver with a step-size of h=0.01.It is noticeable that, in order to reduce the influence of transition, the 1500 data from 30th to 45th second are chosen as the training samples, while the remaining 1000 that from 45th to 55th second were used to test the network. The performance index is the normalized mean square error (NMSE) [20], which is defined as:(22)NMSE=∑(y(t)−yˆ(t))2∑(y(t)−y¯)2wherey(t),yˆ(t),y¯are the observed data, the predicted data, and the average of the observed data, respectively.In this case, a SFSL-SANN with 5 hidden neurons was used to learn this chaotic time series. The outputs and errors on the training and testing samples are shown in Fig. 6, which consists of four sub-graphs and compares the real data and the predicated values. The NMSE of predicting Lorenz chaotic time series by SFSL-SANN and other NN methods were reported in Table 8.The comparison results of Table 8 in terms of the prediction performance among the various models including the proposed method, which reveals that the proposed method produces much smaller prediction errors than those of the other methods. The results show the good statistical properties of the SFSL-SANN. It can be noticed that SFSL-SANN outperformed all the other models.Time-series online prediction is an important benchmark problem for examining the performance of ANNs. Prediction of the geomagnetic activity indices is very important for satellite alarm systems, which can be characterized by the following geomagnetic activity indices: Kp (Kennziffer planetarisch) index, AE (auroral electrojet) index and Dst (disturbance storm time) index. The SFSL-SANN is evaluated to predict the geomagnetic activity index Dst online, which has chaotic behavior with low dimensional chaos [22,23]. The model of predict system is given by(23)Dst(t)=f[Dst(t−1),Dst(t−2),Dst(t−3),Dst(t−4)]In order to realize the online prediction of Dst index, the incremental learning version of SFSL is as follows:Step 1: Span-output-weight learning•Assign random weights which including the input, hidden, output and lateral inhibition weights.Calculate the initial feedforward-output and span-output based on thefirsttraining sample: [x(:, 1), t(:, 1)] and the initialized SANN.Estimate the span-output-weight Λ*, and fixed.Step 2: Feedforward-output-weight learningfor i=1:Nfor j=1:Iter•Calculate the new span-output-matrix based on Λ* and training sample(s): [x(:, i), t(:, i)] or [x(:, 1:i), t(:, 1:i)].Calculate the output error.Iterative update the feedforward-output-weight ωghandωkgL−1.endHere, N is the sample number, Iter is iteration number for each new input sample. From the setting of Iter, one can control the learning progress. We can now conclude that the given online learning of SFSL is a partial update of the feedforward-output-weight. The data set (named OMNI2) used in this online prediction experimental is accessible from National Space Science Data Center (NSSDC). A 1004 hourly samples data set from Jan 1, 2000 has been used for online prediction. In this experimental, the input attributes of problems are normalized into the range [−1, 1] while the output samples of are normalized into the range [0, 1].Fig. 7presents the online prediction results of the proposed SFSL-SANN and Table 9illustrate the comparative results between proposed method and common methods, MLP, ANFIS and ABEDL. The reported RMSE of SFSL-SANN is given after the predicted and real values were anti-normalized. Fig. 7 shows the target and online predicted hourly Dst index and related error obtained from the proposed online SFSL-SANN during the first 1000h in year 2000. The curve illustrated in Fig. 7 shown that the Dst activity was learned quickly because of the least-squares solution of span-output-weigh. The learning error shown in Fig. 8also demonstrate that the predict error arrives very small even in the case of only a few samples were trained. While the ABEDL approach needs 25h to learn the activity of Dst index [22].According to Table 9, the best predict performance of Dst index is obtained by the proposed incremental SFSL algorithm. However, the proposed incremental learning method of SFSL only tune the feedforward-output-weight during the learning process, which will affects the performance of SANN. A more efficient online learning algorithm is also need be studied in the future.

@&#CONCLUSIONS@&#
