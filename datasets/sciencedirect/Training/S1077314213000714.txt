@&#MAIN-TITLE@&#
Statistical analysis of manual segmentations of structures in medical images

@&#HIGHLIGHTS@&#
We model segmentation uncertainty in CT images.We consider the tasks of computing summary statistics and identifying outliers.This framework is based on shape analysis of 2D closed, planar curves.The underlying statistical models are efficient in summarizing data variability.The models can be used in image registration and 3D surface reconstruction.

@&#KEYPHRASES@&#
Medical imaging,Segmentation uncertainty,Non-expert image segmentation,Statistical analysis of planar curves,

@&#ABSTRACT@&#
The problem of extracting anatomical structures from medical images is both very important and difficult. In this paper we are motivated by a new paradigm in medical image segmentation, termed Citizen Science, which involves a volunteer effort from multiple, possibly non-expert, human participants. These contributors observe 2D images and generate their estimates of anatomical boundaries in the form of planar closed curves. The challenge, of course, is to combine these different estimates in a coherent fashion and to develop an overall estimate of the underlying structure. Treating these curves as random samples, we use statistical shape theory to generate joint inferences and analyze this data generated by the citizen scientists. The specific goals in this analysis are: (1) to find a robust estimate of the representative curve that provides an overall segmentation, (2) to quantify the level of agreement between segmentations, both globally (full contours) and locally (parts of contours), and (3) to automatically detect outliers and help reduce their influence in the estimation. We demonstrate these ideas using a number of artificial examples and real applications in medical imaging, and summarize their potential use in future scenarios.

@&#INTRODUCTION@&#
Segmentation of medical images to extract anatomical structures is arguably the most important problem in medical image analysis. A variety of techniques ranging from variational methods to statistical methods to fuzzy logic have been applied to this problem but a general solution still remains elusive. The difficulty lies in capturing the vast observed variability, in the form of pixel values and anatomical shapes, into appropriate objective functions that enable high-quality segmentations. This is further compounded by low resolution, high noise and blurriness associated with common medical imaging modalities. The segmentation performance achieved by trained humans – physicians, clinicians, technicians, etc. – has been very difficult to match by automated systems. Since manual segmentations by skilled practitioners are generally expensive and the amount of imaging data being generated far out-paces the experts’ availability, it is not practical to rely on these trained personnel for all our segmentation needs.There are several directions for addressing this gap between the need and the available expertise. Firstly, motivated by the success of manual segmentations by trained experts, an important focus of current research in segmentation has been in training machine learning algorithms to improve their performance. A similar idea has been to develop structured prior models on shapes of interest and perform Bayesian segmentations – segmentations guided by prior knowledge of statistical shape variability associated with anatomical structures to be segmented. A second, completely different paradigm is to involve humans, albeit non-experts, who can volunteer their time and effort in segmenting medical images. This approach is part of a larger effort involving public participation in scientific data analysis. It is this second direction that motivates the methods presented in this paper. The last decade has seen a tremendous increase in what has been termed Citizen Science - non-experts helping scientists collect and/or analyze data. These projects range from Galaxy Zoo, where non-experts help classify galaxies [1], to Fold-It, where people help fold proteins [2], to measuring trees and counting birds in nests (Cornell Lab of Ornithology) [3]. These projects take advantage of human perceptual abilities and intuitions about shape (Galaxy Zoo/Fold-It) and the ability of volunteers to take measurements over a large spatial extent. We are currently trying to build the foundation for Citizen Science projects that further exploit the growth of digital imaging, both to support volunteers annotating 2D and 3D structures in imagery, and in building tools to allow volunteers to capture calibrated imagery to support more quantitative measurements. We believe these tools may support a diverse set of Citizen Science projects. This paper is about analyzing data arising from multiple manual segmentations of 3D medical image data, but could be applied to 2D photograph segmentations as well (e.g. finding birds, trees or cars in a scene). Shown in Fig. 1is an illustration of a manual segmentation of the liver. The segmentation is performed using multiple oblique and parallel image slices and the resulting two-dimensional curves can be used for full three-dimensional surface reconstruction.One challenge any data collection and analysis project faces is data verification and validation. This is particularly true in Citizen Science projects, where the participants can come from a wide variety of backgrounds and skill sets. To date, these projects have largely relied on carefully constructed training tutorials along with simple statistics to filter out incorrect data (be it malicious or unintentional). Simple statistics work well in the case of category labels (e.g. Galaxy Zoo or bird counting) because any given dataset is viewed by multiple people and each person marks multiple datasets. This provides fairly reliable statistics for both the datasets and reliability data on the individuals marking the datasets.We would like to extend the same statistical approach to support Citizen Science projects where the volunteers contribute contour data rather than category labels. An example of such data (brainstem) is provided in Fig. 2. We show a 2D CT image with an example segmentation, a zoom-in on part of the image where the boundary of the brainstem is partly visible, and five expert segmentations that form the raw data for techniques developed in this paper. In particular, we would like to compute sample statistics for multiple contours that let us answer the following specific questions about the data:(1)What is the mean or median contour? This helps filter out small noise and discrepancies in any individual person’s contour.What is the magnitude of the standard deviation of a set of contours? This provides a measure of confidence for the segmentations.Is a contour an outlier?What is the average distance – under a shape metric – of an individual’s contours from the average contour?Do the segmentations follow a Gaussian distribution or is the data better described by a mixture of Gaussian distributions? (This happens quite often when there is more than one interpretation of the instructions. For instance, one group might contour an inner wall, while the other group contours an outer wall.)In addition to answering the above questions, we are interested in providing the scientists (in this case physicians) with visualization tools that will help them analyze both inter and intra variation in their datasets, and to provide confidence analysis on their reconstructions. To this end, we provide visualizations of the major modes of variation, and their magnitudes, which provide some estimate of the reliability of different segments of the contour. It is important to note that these measures of confidence or reliability do not necessarily correlate with segmentation accuracy, but rather just consistency.Historically, the problem of shape analysis of curves has been studied with a variety of different mathematical representations of curves. These representations include sample points or landmarks [4–6], level sets [7], deformable templates [8], medial axes [9,10], algebraic invariants, and others. However, the most natural representation – a parameterized curve – is relatively infrequent. The difficulty in studying shapes of parameterized curves lies in the parameterization variability: a curve can be re-parameterized in infinitely many ways but still have the same shape. Davies et al. [11,12] use landmark models and compute optimal re-parameterizations using a minimum description length (MDL) type energy. Although such models have proven very useful in medical image analysis and have good specificity, generalizability and compactness (as compared to manually selected landmarks and arc-length parameterization), the method used to compute them has three main limitations. First, the optimization problem is defined in terms of ensembles and thus the distance between any two shapes depends on the other shapes in the ensemble. This contrasts the standard mathematical definition of a distance between objects. Second, the MDL energy does not preserve distances between shapes when they are re-parameterized in the same way. Intuitively, the shape metrics and statistical analysis should not change with a change in the parameterization, but rather only with a change in shape. Finally, the MDL driven optimization problem requires a pre-selection of a template, which can be rather arbitrary and choosing different templates may lead to different solutions.An emerging body of work has proposed using an algebraic approach to handle the problem of parameterization. In this approach, one unifies all possible re-parameterizations of a curve by forming equivalence relations, and each shape is denoted uniquely by an equivalence class. Shape metrics are defined and computed between equivalence classes, rather than individual curves. A very important advantage of this framework is that the process of comparing any two shapes, i.e. two equivalence classes, involves finding optimal registrations between the corresponding curves. (Note that the registration of points along curves corresponds to re-parameterizations of curves.) Thus, this framework, termed elastic shape analysis, leads to a simultaneous registration and comparison of shapes of curves, under a single metric, and to a principled approach to shape analysis. Notably, the methods involving landmarks, level sets, medial axes, and others, do not provide this ability to simultaneously register and analyze shapes. Several papers, including [13–15], have utilized an elastic framework, but we will use the approach presented in [16,17] for this paper. An important advantage of this approach is that amongst all methods for elastic shape analysis of contours, this is the only one so far that extends to contours in higher dimensions.Thus, the main contributions of this work are:(1)The application of elastic analysis to modeling contour segmentation uncertainty in medical images.A method for visualization of segmentation reliability based on principal component analysis.Extension of shape analysis methods presented in [16,17] that enables one to model rotation, scale and position in addition to shape.Definition and computation of the elastic median based on a general algorithm presented in [18].A formal statistical procedure to identify outliers in datasets based on elastic distances from the median.The rest of this paper is organized as follows. In Section 2, we summarize a mathematical/statistical framework presented in past papers, such as [17], and adapt these tools for use in the current problem. Specifically, we introduce an algorithm for estimating the sample median in the space of contours and use that median estimate for studying dominant modes of data variability. Experimental results involving brainstem and prostate images are presented in Section 3, and the paper concludes with a short summary in Section 4.The basic problem we face requires a technique for statistical analysis of planar, closed contours. There are several theories available for this task. In this section we summarize a recent approach for elastic analysis of contours that is particularly attractive for the current problem.Let a parameterized planar, closed curve be denoted asβ(t)∈R2, where t is the parameter. Since the curve is closed, it is more natural to parameterize it using the domainS1, instead of an interval, since there are no natural end points on a circle to match the two end points on an interval. There are several possibilities for mathematically representing β. One can simply use the x and y coordinate functions of β, as shown in Fig. 3b. Another possibility is to parameterize β using arc-length and compute the angleβ̇makes with the x axis, as shown in (c). Finally, one can take the derivative of this previous angle function to obtain the curvature function, shown in (d). Although arc-length parameterization (with fixed seed placement) removes the variability associated with the parameterization of curves, it suffers from the problems of suboptimal registration due to a linear registration of points across curves. It is more natural to include arbitrary parameterizations of curves in the analysis, and to seek optimal re-parameterizations during pairwise matching of curves. This allows for the possibility of nonlinear registrations of points, an aspect that is central to elastic shape analysis.Define the group of re-parameterization functions as:Γ={γ:S1→S1|γis an orientation-preserving diffeomorphism}.The re-parameterization of a curve β, termed the action of Γ on the space of curves, is given by composition, (β,γ)=β∘γ. One also needs a metric for comparing shapes of curves and the Euclidean metric is the most common choice in the literature. A major problem in methods that use Euclidean distances to compare shapes is that ∥β1−β2∥≠∥β1∘γ−β2∘γ∥, for a general γ∈Γ, where ∥·∥ is theL2metric for functions onS1. This means that a comparison of two curves depends on their parameterizations! A solution suggested by Srivastava et al. [16,19,17] is to use a new mathematical representation of curves, called the square-root velocity function (SRVF), given byq(t)≡β̇(t)‖β̇(t)‖. If a curve β is re-parameterized to β∘γ, then its SRVF changes from q to(q∘γ)γ̇; we will use the notation (q,γ) to denote the new SRVF. One of the reasons for using this representation is that ∥q1−q2∥=∥(q1,γ)−(q2,γ)∥, for all γ∈Γ.In the case where the curve is closed, the corresponding SRVF satisfies the condition∫S1q(t)‖q(t)‖dt=0. Thus, the space of all planar, closed curves, represented by their SRVFs, is given byC={q:S1→R2|∫S1q(t)‖q(t)‖dt=0}.Cis a nonlinear manifold because of the closure condition. In a general shape analysis framework one often removes the variability due to scale, rotation and position of the curves, but in this work these quantities are informative and are thus included in the analysis. The position variable is, unfortunately, lost in the SRVF representation, since it is based on the derivativeβ̇. We reinstate it in the analysis as a separate variable as described in Section 2.2.Cis a Riemannian manifold with the standardL2metric,〈v1,v2〉=∫S1〈v1(t),v2(t)〉dt, where the inner product in the integrand is the standard Euclidean inner product inR2. The task of computing geodesic paths between any two elementsq1,q2∈Cis accomplished numerically, using an algorithm called the path straightening algorithm, introduced first in [20] but adapted to the SRVF representation in [17]. This algorithm initializes a path inCconnecting q1 and q2, and iteratively “straightens” it until it becomes a geodesic. Letα:[0,1]∈Cdenote the resulting geodesic path. Then, the length of this geodesic path provides a geodesic distance between q1 and q2 inC:dc(q1,q2)=L[α]=∫01(〈α̇(τ),α̇(τ)〉)1/2dτ.Curves that are within a re-parameterization of each other result in different elements ofC. The unification of such curves is performed using an equivalence relation under the re-parameterization group action. That is, for all elements inCthat denote re-parameterizations of the same curve, we put them in the same equivalence class given by[q]=closure{(q∘γ)γ̇|γ∈Γ}. Each such equivalence class [q] is associated with a shape uniquely and vice versa. The set of all these equivalence classes is a quotient space and is denoted byS. The distance dccan be used to define a distance onSaccording to ds([q1],[q2])=infγ∈Γdc(q1,(q2,γ))=infγ∈Γdc((q1,γ),q2). This minimization is performed in practice by sampling each curve with some large number of points and then applying dynamic programming with an additional seed search.An important part in our analysis of contours is generating their statistical summaries – in the form of means and covariances – and using these sample statistics to analyze contours. With the possibilities of bad segmentations in mind, we also want the statistical summaries to be robust to outliers. Specifically, we are interested in a robust estimation of a representative contour using the concept of a median. Next, we provide the definition and algorithms to compute statistical summaries of closed, planar curves in the form of a mean, median and covariance. Furthermore, we provide a simple yet effective procedure for removal of outliers.Once we have defined a distance dsonS, the mean can be defined as follows. Let β1, β2,…,βndenote a set of given contours and q1, q2,…,qnbe the corresponding SRVFs. Then, the Karcher (or Frechet) mean curve (actually an equivalence class of curves) is[q¯]=argmin[q]∈S∑i=1nds([q],[qi])2. A gradient-based approach for finding this mean is given in several places [21,4], and is repeated here for convenience as Algorithm 1. The algorithms for computing the exponential map and its inverse – exp and exp−1 – are similar to the technique for finding geodesics and are presented in [17].Algorithm 1(Karcher Mean)Letq¯0be an initial estimate of the Karcher mean. Set j=0 and ∊1,∊2 to be small positive values.(1) For each i=1,…,n, computevi=expq¯j-1(qi).(2) Compute the average directionv¯=(1/n)∑i=1nvi.(3) If‖v¯‖<∊1, then stop. Else, update usingq¯j+1=expq¯j(∊2v¯).(4) Set j=j+1 and return to Step 1.The main problem with the mean estimator is that it is susceptible to noise, especially outliers. To handle that problem, one often uses the median value instead. Next, we introduce the concept of a median estimator for a collection of contours under the framework of elastic analysis.Again, let β1, β2,…,βndenote a set of given contours and q1, q2,…,qnbe the corresponding SRVFs. Then, the median curve (again an equivalence class of curves) is[q̃]=argmin[q]∈S∑i=1nds([q],[qi]). A gradient-based approach for finding the geometric median on general Riemannian manifolds is given in [18]. A version of this procedure particularized to our framework is given as Algorithm 2. The only difference here from the mean algorithm is in the weights applied to the shooting vectors vito obtain the weighted averageṽ. It is important to note that in some cases the mean and median algorithms may converge to a local solution. Also, in both algorithms we set ∊1 to 1% of the initial gradient and ∊2=0.3. Once we have a median contour we can address the problem of outlier detection.Algorithm 2(Median)Letq̃0be an initial estimate of the median. Set j=0 and ∊1,∊2 to be small positive values.(1) For each i=1,…,n, computevi=expq̃j-1(qi).(2) For each i=1,…,n, computedi=ds(q̃j,qi).(3) Compute the update directionṽ=∑i=1n(vi/di)(∑i=1n1/di)-1.(4) If‖ṽ‖<∊1, then stop. Else, update usingq̃j+1=expq̃j(∊2ṽ).(5) Set j=j+1 and return to Step 1.We will take a distance-based approach to outlier detection. For this purpose we will utilize the median rather than the mean because it is a more robust estimate in the presence of outliers. Given the sample median of the data, we proceed by computing the geodesic distances between the estimated median and each of the curves in the data (ds([q̃],[qi]),i=1,…,n). Using these distances we can compute the quartiles (Q1,Q3) and the interquartile range (IQR=Q3−Q1) and label observations corresponding to distances that are greater than Q3+kIQR as outliers. The choice of this cutoff is based on a pre-determined expected probability of type one error, and can be adjusted for different desired error probabilities by varying k. For the purpose of this paper, we use the standard k=1.5.In order to estimate the covariance structure of our data we will again utilize the estimated median rather than the mean. The covariance can be estimated with respect to any point on a Riemannian manifold but since we argue that the median provides a robust estimate of the center of mass of the data, that is the point we choose [22]. The evaluation of the covariance around the mean using elastic analysis has been previously discussed in the context of shapes of closed curves [23,17], 3D open curves [24] and surfaces [25]. The general computation of the covariance around the median is as follows. Letvi=expq̃-1(qi),i=1,2,…,n,vi∈Tq̃(S). Then, the covariance kernel can be defined as a functionKq:[0,1]×[0,1]→Rgiven byKq(ω,τ)=(1/(n-1))∑i=1n〈vi(ω),vi(τ)〉. In practice, since the curves have to be sampled with a finite number of points, say m, the resulting covariance matrices are finite-dimensional. Often the observation size n is much less than m and, consequently, n controls the degree of variability in the stochastic model. In the case of learning statistical models from the observations, one can reach an efficient basis forTq̃(S)using the traditional principal component analysis (PCA) as follows. LetV∈R2m×nbe the observed tangent data matrix with n observations and m sample points inR2on each tangent. LetK∈R2m×2mbe the Karcher covariance matrix and let K=UΣUTbe its SVD. The submatrix formed by the first r columns of U, call itU∼, span the principal subspace of the observed data and provide the observations of the principal coefficients asC=U∼TV∈Rr×2m. We can validate the computed models through random sampling. This can be done using a wrapped Gaussian distribution as follows. A multivariate Gaussian model for a tangent vector re-arranged as a long vectorv∈R2mis given byv=∑i=1nziΣiiUi, wherezi∼iidN(0,1)and Σiiis the variance of the ith principal component. One can re-structure the elements of v to form a matrix of sizeR2×mand obtain a random SRVF usingq=q̃+v. Finally, this SRVF can be mapped to a parameterized curve using integration.One issue that we encounter in our framework is that the SRVF representation automatically removes the translation variability of the given contours (see the definition of SRVF given earlier). In order to include it in the analysis, we treat it separately by first computing the mean position of the given contours and then performing a joint computation of the covariance (including the contour information and an additional translation vector). This gives us a natural way of including translation in summarizing the variability of our data using PCA.In this section we present multiple artificial examples to showcase the methods described in this paper. Each example presents a different type of result that emphasizes part of the described methodology. In most of these examples we compare the generated models to those computed using arc-length parameterization while removing the seed variability (using SRVFs).We begin with an example that showcases the robustness of the median estimation compared to the Karcher mean in the presence of clear outliers. In Fig. 4, we display ten contours that come from the MPEG 7 database; the first five come from the same class, while the next five are outliers. We performed the mean and median estimation six times, first using the first five curves with no outliers, second using the first five curves with one outlier (curve six), then with two outliers (curves six and seven), etc. We expect the estimated median to be more robust to outliers than the mean. We see a clear deterioration of the shape of the estimated Karcher mean (blue) as we increase the number of outliers in the data. The median contour (red) is a much better estimate, although it also begins to deteriorate when the number of outliers becomes equal to the number of curves coming from the same class.In this example we would like to address the compactness, generalizablity and specificity of our models. For this purpose, we use a dataset of 20 hammer contours (from MPEG 7). For the sake of brevity we do not display the original dataset. It is important to note that in this example we remove the rotation variability of the data (in a pair-wise manner using Procrustes alignment). First, in Fig. 5, we display the first three modes of variation (within 2 standard deviations around the median drawn in red) in the computed models for both elastic and arc-length analysis as well as random samples from the Gaussian distribution. The shown elastic analysis principal directions of variation represent the given data well and thus the computed models appear to have good specificity. This can also be observed in the generated random samples, which are all valid. These properties are not true in the case of arc-length analysis. In fact, one of the random curves has self-crossings representing an invalid instance. Next, we want to quantify the generalizability of our models using leave-one-out reconstructions of the given data. For each of the curves in the data we compute its reconstruction with a certain number of PCs and compare it to the true curve using dsdefined in Section 2.1. In Fig. 6a we plot the distances (averaged over all curves in the data) as a function of the number of PCs used for reconstruction. The blue curve represents elastic analysis and the red one represents arc-length analysis. We note that the elastic models have significantly better generalizability than the arc-length ones. For each number of PCs we also performed a paired t-test at the 0.05 significance level, which rejected the null hypothesis that the difference between mean distances generated by the two methods is zero. In addition, for arc-length analysis, we computed the same result using dc(also defined in Section 2.1). We plot this result in black in the same figure. As expected, these average distances are significantly larger than those shown in red. Additionally, we provide five qualitative reconstruction results where we plot the true curve in black, the elastic reconstruction in blue and the arc-length reconstruction in red. Finally, in Fig. 6b, we plot the variance of each PC (elastic=blue,arc-length=red). We note that the elastic models are much more compact than the arc-length ones.In Figs. 7 and 8, we display results of using the above described statistical modeling techniques to analyze artificial heart contours (from MPEG 7). In this dataset, we have included three clear outliers to test whether our method can detect them. We began by computing the elastic mean (blue) and median (red), and the arc-length mean (black) and median (green) of the data. Again, we see that the medians provide a more robust statistical summary of the data, although the differences here are smaller due to an overwhelming number of inliers in the data. The difference here is mainly seen in the size of the estimate. The means are always smaller than the medians, which reflects the difference in scale of the inliers and the outliers. Furthermore, the arc-length mean and median are smaller than the elastic ones due to additional averaging out of features. The distance based outlier cutoff in this example is 0.5863 and we identified three clear outlying observations, corresponding to curves 21, 22 and 23. We removed these outliers from the data and computed the covariance structure using the re-estimated medians. It is important to note that after re-estimating the median and checking for outliers again, none of the contours were flagged. We display the resulting principal components as vector fields on the median curve (these are scaled according to the standard deviation of each principal component). The corresponding magnitude of each vector is also displayed using colors, where red areas correspond to higher variability in the given data. We see that the main variation, computed using elastic analysis, matches the variability observed in the original data (overall shape, indent at top of the heart, indents on the sides of the heart). This implies that our models are good at capturing the underlying data structure. On the other hand, when arc-length analysis is used this trend is not as clear. Also, the main variation accounted for by PC1 corresponds to a nearly tangential vector field along the boundary of the median heart. This mainly represents a misalignment of points, and can cloud the overall interpretation of the model. Also, as shown in Fig. 6c, the variance of PCs computed using elastic analysis (blue,cumulative variance=9.80) is much lower than that computed using arc-length analysis (red,cumulative variance=22.72) suggesting a more compact model.In this section we display examples of elastic statistical models computed for real segmentation data. The data used here comes from oblique slices of images of the brainstem and prostate, and a few parallel image slices of the same anatomical structures. In some examples we also compare expert segmentations to those performed by non-experts. In each example, we estimate both the mean and median of the manual segmentations. We also compute the covariance structure at the median. In computing these models, we first consider the outlier detection problem. Fig. 9shows the boxplots of elastic distances to the median for each of the 12 presented examples. As can be seen, we identified an outlier in the non-expert segmentations in Ex. 9. This outlier was removed from the data before computing the elastic models.In many of the presented examples we note that the median is, as stated earlier, a more robust estimate of the center of mass of the data than the mean. We also see that the computed models efficiently summarize the given data and that in most cases only two principal components can account for most of the variation. The principal components, which represent the covariance structure we estimated are displayed as vector fields on the median curve. One important fact to note here is that we are only displaying the positive direction (median+PC) and the vectors were scaled according to one standard deviation from the median. We also provide a corresponding magnitude plot for each of the principal components. These visualizations can be used to identify areas on the segmented curves that the segmenters were unsure about (red colors correspond to more uncertainty/variability). The computed uncertainty models can later be used as additional information in surface reconstruction from 3D images and image registration.In the first four examples provided in Figs. 10 and 11we consider four oblique views of the same brainstem structure. We are interested in modeling five manual expert segmentations for each view. We see that our estimates of the mean and median are valid representatives of the given data. In addition, the computed principal components efficiently reflect the variability in the given segmentations. In Figs. 13 and 14, we display some additional statistical models for expert segmentations of the brainstem and prostate structures using different oblique and parallel views. Again, we note that the models constructed using our method describe the given data well. In Fig. 12 we zoom-in on the segmentation variability presented in Fig. 11a and Fig. 14a for improved display. We also highlight areas of the segmentations where most variability is observed. It is clearly evident that the magnitude of the principal components in those areas is high (marked in red), which implies that our models are good at capturing such variability.Finally, inFigs. 16, 17, we provide statistical models for segmentations of the brainstem and prostate performed by non-experts. In two of the examples we are modeling ten contours. These segmentations were performed on the same images as those shown in Fig. 10, Fig. 11a–c and Fig. 13, Fig. 14a for expert segmenters. This is a view angle that the experts had never seen before when contouring (they always contour on transverse slices) and they were only indirectly aware of where the slice was [26]. In contrast, the non-experts were given three example segmentations on a different image (same view) as references, which are displayed in Fig. 15. They were also presented with simple instructions to look for specific anatomical cues in the image in order to accurately delineate the structure of interest. The contours in the example images were made by intersecting the nonparallel planes with a surface model reconstructed from many physician-reviewed contours drawn on the parallel cross-sections of the dataset. We notice a very interesting result in these two models. The non-experts had lower variability in their segmentations than the experts. We hypothesize that this trend is due to the given example segmentations. Perhaps in some cases the non-experts were trying to emulate the given example images by finding similar shapes in the image data.

@&#CONCLUSIONS@&#
