@&#MAIN-TITLE@&#
Sparse representation with multi-manifold analysis for texture classification from few training images

@&#HIGHLIGHTS@&#
The prime pyramid expands the training dataset with less redundancy.Sparse representation provides a compact model for textural images in each class.Multi-manifold analysis improves discriminative power while mitigates overfitting.Reasonably high classification accuracy is achieved with very few training images.

@&#KEYPHRASES@&#
Texture classification,Sparse representation,Manifold learning,Multi-manifold analysis,Few training image,

@&#ABSTRACT@&#
Texture classification is one of the most important tasks in computer vision field and it has been extensively investigated in the last several decades. Previous texture classification methods mainly used the template matching based methods such as Support Vector Machine and k-Nearest-Neighbour for classification. Given enough training images the state-of-the-art texture classification methods could achieve very high classification accuracies on some benchmark databases. However, when the number of training images is limited, which usually happens in real-world applications because of the high cost of obtaining labelled data, the classification accuracies of those state-of-the-art methods would deteriorate due to the overfitting effect. In this paper we aim to develop a novel framework that could correctly classify textural images with only a small number of training images. By taking into account the repetition and sparsity property of textures we propose a sparse representation based multi-manifold analysis framework for texture classification from few training images. A set of new training samples are generated from each training image by a scale and spatial pyramid, and then the training samples belonging to each class are modelled by a manifold based on sparse representation. We learn a dictionary of sparse representation and a projection matrix for each class and classify the test images based on the projected reconstruction errors. The framework provides a more compact model than the template matching based texture classification methods, and mitigates the overfitting effect. Experimental results show that the proposed method could achieve reasonably high generalization capability even with as few as 3 training images, and significantly outperforms the state-of-the-art texture classification approaches on three benchmark datasets.

@&#INTRODUCTION@&#
Among all properties of an object, such as colour, shape, and motion, texture is one of the most significant characteristics which human vision and machine vision systems utilize in interpreting scenes and performing object identification. As an active field in computer vision, texture classification has been widely applied in many areas including medical image analysis [1–3], remote sensing [4], material characterization [5], and content-based image retrieval (CBIR) [6]. Due to the existence of the large intra-class variation (Fig. 1) and inter-class similarity in textural images robust texture classification is very challenging.A general texture classification framework comprises two steps: the textural feature extraction, and classification. Previous works mainly focused on designing the robust feature extraction methods and using classifiers such as Support Vector Machine (SVM) and k-Nearest-Neighbour (kNN) for supervised classification [8,9]. Some representative feature extraction methods include the Local Binary Patterns (LBP) [10], texton-based approaches [11,12,7,13], filter bank based methods [14–17] and bag-of-keypoints [18,8]. The texton-based approaches represent a textural image as a histogram of textons, where the textons are computed by utilizing a clustering algorithm such as k-means to cluster the local feature vectors extracted from the training images. There are numerous ways to extract the local feature vectors from images in the texton-based approaches. In [11,12,7], a bank of filters is utilized and all the filter responses on each pixel are concatenated as a local feature vector. By carefully selecting the filters, the filter bank based feature could be invariant to image translation and rotation. Another popular kind of local features is image patches [13], which utilize the raw pixel values in a fixed-size image patch around each pixel as a local feature vector, and it was proven to outperform the filter bank based feature on some datasets. Recently, Liu and Fieguth [19] applied random projection on the image patch to get a random feature based on the compressive sensing theorem, which not only reduced the dimension of the feature, but also achieved comparative results with the patch-based method [13]. The filter bank based methods filter an image using a bank of well designed filters, and extract features from each filtered image. All the features are then concatenated together as the image feature. Different from the texton-based approaches using filter bank responses on each pixel as a local feature vector, all the features extracted by the filter bank based methods are in image-level but not pixel-level, thus they could not give a statistical characterization of textures. Therefore the filter bank based features are usually not as discriminative as those descriptors that could capture the local primitives of textures for texture classification, e.g., the texton-based approaches, though they could be designed to be invariant to many image variations such as translation, scale, and rotation. The bag-of-keypoint approaches utilize the region detectors such as Harris–Laplace (HL) detector to detect the interest areas of images, and then apply the image descriptors like Scale-Invariant Feature Transform (SIFT) and Histogram of Oriented Gradients (HOG) to extract local features from them [8]. The bag-of-keypoint features are invariant to image translation, rotation and affine transform. However, since the local features extracted from the detected keypoints of textural images are sparsely distributed, the bag-of-keypoint features are usually not discriminative enough. To tackle this problem, another method was proposed by extracting the local SIFT feature from densely sampled locations [20,21], and it was shown in [20] that the dense feature worked better than bag-of-keypoints for scene classification. However, the dense SIFT sampling loses scale and affine invariance.Texture classification could be regarded as a statistical learning problem, where one template is learnt from each training image (through feature extraction) and a classifier is learnt from all the templates of the training images. Ideally, if the feature extraction method is robust enough (not only invariant to different imaging conditions but also discriminative), the templates learnt from images of the same class will be close to each other and those learnt from images of different classes will be far away from each other, reaching small intra-class variation and large inter-class variation. Thus, a simple classifier with a few training images could easily distinguish test images from different classes. However, it is not always feasible to design a very discriminative feature extraction method, and also since the classifier is local in the input space (both SVM and kNN are local estimators), it requires a large number of training images to achieve a high generalization capability [22]. As shown in Section 3, the performance of the state-of-the-art texture classifiers will significantly decline when the number of training images per class decreases. Since collecting labelled image data is costly, it is common in practice that only a small number of images are available for training. Thus, it is critical to develop robust classification methods that only need a small amount of training images to achieve high generalization capability in the classification of test images (in fact not only for texture classification, most computer vision tasks have such a desire).A few attempts have been made to solve the problem of classifying textures from a small number of training images. For instance, Drbohlav and Chantler [23] brought out a method to classify textural images captured under different illuminations from a single training image per class. They filtered an image with a directional derivative operator to model the textural appearance under a specific illumination direction, and then utilized a filter bank to compute the image features. To compare two images under unknown illumination directions, a set of feature vectors were calculated for a complete set of illumination directions for each image. The distance between the closest pair of feature vectors of the two images is adopted as the distance between them. Targhi et al. [24] developed an approach to classify textures under unknown lighting conditions from a small number of training images by generating additional training data using a photometric stereo. However, these works only considered single variation of textures, i.e., illumination change, which were not applicable to the real world texture classification where textures are usually subject to multiple imaging condition variations, as illustrated in Fig. 1.In this paper, we aim to develop a novel framework that only needs a few training images to classify textures with various image variations such as translation, rotation, scale, illumination and view-point change. The following three major aspects are considered in the proposed texture classification framework:1.Since most textures are uniform and repetitive on pattern distribution, we could divide a textural image into many subimages, where each subimage represents one aspect of the texture and is regarded as a new sample. Subsequently, by using these subimages for training, more variations of the texture are incorporated which is beneficial for achieving higher generalization capability of the model.It is presumed that a more compact model requires less training samples to learn a generalized representation of signals. Because textures are sparse and the sparse representation suggests a more compact model than the local estimators [22], the sparse representation is favourable to model the textural images.Considering that supervised learning from a small number of training images is prone to overfitting, which results in low generalization capability in the classification of new images, it is important to consider both the discrimination and generalization of a model in the learning process. Regarding each texture as lying in a low dimensional manifold, it is expected that through a multi-manifold analysis, on the one hand the distance between different texture classes could be enlarged, thus increasing the discriminative power of the model; on the other hand the intra-class variation can be decreased, therefore mitigating the overfitting effect.Based on the above considerations, we develop a sparse representation based multi-manifold analysis (SR-MMA) framework for texture classification from few training images. After extracting a set of image patches from each training image as the new training samples, we utilize sparse representation to model these new samples by assuming that each sample of a texture is generated from a sparse representation of a set of basis. Subsequently, we propose a supervised multi-manifold analysis algorithm to learn a projection matrix for each texture class that considers both the discrimination and generalization of the model. The test images are classified using a modified sparse representation based classification method by plurality voting.The rest of the paper is organized as follows. Section 2 presents details of the proposed method of texture classification from few training images via SR-MMA. Experiments are shown in Section 3. Section 4 concludes the paper.In this section we address the problem of texture classification from few training images. The small number of training images available are denoted as {Tl,l=1,…N}, where N is the total number of training images for C classes.As widely acknowledged, texture could be regarded as a periodical repeat of patterns in space. In most textural images, the patterns are uniformly distributed, thus an arbitrary region (larger than a certain size which is determined by the number and size of patterns) in such a textural image has similar appearance to the whole image, and could be used to describe the whole image. One example is shown in Fig. 2. By equally dividing a textural image into 4 or 16 subimages, those subimages still look similar to the original image. On the other hand, because of the existence of randomness on pattern distribution and noise in images, different regions of an image might have small variations. Therefore, by dividing a textural image into several regions (either overlapped or non-overlapped), each region as a subimage, these subimages not only characterize the original image, but also incorporate variations which are beneficial to learning a robust model with high generalization capability.For textures whose patterns are not uniformly distributed, accurate classification becomes harder since the training and test images might be captured from different parts of a texture. In this situation, dividing the image into subimages for training is sometimes more important because they could cover different aspects of a texture. For example, as shown in Fig. 3, two images from the KTHTIPS2 database [26] are captured from the same texture but cover different areas. From a first glance, it is easy to find out that the image on the right is generated from the labelled region of the one on the left. Since the texture is not uniformly distributed, using the whole image of the left one for training might not classify the image on the right correctly. However, if we divide the left image into subimages which include the labelled region for training, a better result could be achieved.In this paper, we apply the spatial pyramid technique to divide a textural image into several non-overlapped subimages. To create a spatial pyramid image representation, the traditional method [21] divides an image into increasingly coarser grids (subimages) when the pyramid level increases, e.g., each subimage in pyramid level l (in level 0 is the original image) is divided into four equal-sized subimages in level l+1. Thus, the subimages generated by this method are highly correlated with each other, e.g., each subimage could be expressed as a linear combination of four other subimages (simply by plus or minus). In order to decrease the correlation between subimages at different levels, which otherwise will make the generated subimages redundant, we design a different spatial pyramid called prime pyramid in which the number of division along each dimension of an image changes as a sequence of prime numbers, i.e., {2,3,5,7,11,13,…}, with the spatial level. For example, under the prime pyramid an image will be divided into 2×2 subimages in level 1, 3×3 in level 2, 5×5 in level 3, and so on. In addition, because of the existence of scale change in textural images, we also adopt the scale pyramid to expand the textural images in scale direction, as shown in Fig. 4. The subimages generated from both the scale and spatial pyramid are utilized as the new training samples.The new training samples generated from the original training images are denoted as {Tlj,l=1,…,N,j=1,…,Ns}, where Nsis the number of training samples generated from each image.In fact, generating new training set has also been adopted in previous literature. Chen et al. [27] utilized the genetic algorithm (GA) to generate new training samples from the original training set, and employed an evolutionary classifier called Sparse Network of Winnows (SNoW) to evaluate the generated new samples. Then a manifold-based method was applied to re-sample the resulted generations. By repeating the process for several generations, an optimized training set with much more samples than the original training set was obtained and used for face detection. However, this method is not suitable for our work in which the number of training samples of each class is too small, e.g., 1 or 3, as in their method the original training set should cover a fair amount of the core set of each class distribution to obtain a good optimized training set.In a general term, sparse representation (SR) aims to represent a sparse signal as a linear combination of a small number of atoms from a dictionary, by solving a ℓ0-norm regularized linear regression problem —(1)minx∥x∥0,s.t.Ax=ywhere A=[a1,a2,…,aN]∈Rm×Nis the dictionary, y∈Rm×1 is the signal, and x∈RN×1 is the coefficient vector (sparse code). It is proved that if the input y is sparse enough the ℓ1-norm regularization (Eq. (2)) could get the same solution as the ℓ0-norm [28], which is unique since the ℓ1-norm is convex.(2)minx∥Ax−y∥22+α∥x∥1where α is a regularization parameter.Most natural images are demonstrated to be very sparse through the DCT transform (JPEG) and wavelet transform (JPEG 2000), especially for faces and textures [19,29], thus it is rationale to use the sparse representation to model the textural images. Since sparse representation also allows for a distributed representation of signals, it could suggest a more compact model than the local estimators [22]. Furthermore, it is worth noting that a sparse representation of a signal on a supervisedly learnt dictionary is naturally discriminative [30]. Therefore, sparse representation could provide many beneficial properties to image classification. It has been applied in many image classification tasks, including face recognition [30], object categorization [31], and texture classification [32,33].In a sparse representation based classification (SRC) scheme for face recognition reported in [30], Wright et al. concatenated all the training images together as the dictionary A, and then represented each test image as a sparse linear combination of the atoms in A by solving a ℓ1-minimization equation (Eq. (2)). The residuals of the sparse code (the optimized coefficient vectorx^) on each image class are computed by(3)riy=∥y−Aδix^∥22,fori=1,…,Cwhereδix^is a function that keeps the elements inx^associated with the ith class unchanged, and makes others as zero. The test image y is classified to the class with the least residue. It was demonstrated that SRC was very robust to noise, occlusion and corruption of test faces in face recognition.Yang et al. [31] introduced a method to incorporate SR into the bag-of-words based image classification. They learnt a dictionary for the sparse representation from the local words of all the training images, and calculated a sparse code for each word in an image by SR. Each image was represented as a feature vector by max-pooling the sparse codes of all the local words in it. A linear supported vector machine (SVM) was finally adopted for classification. Following Yang's work, Xie et al. [32] applied sparse representation for texture classification by utilizing the image patches as the local words and generating image features from the SR coefficients of the image patches. A Nearest Neighbour classifier was adopted for classification. The method was demonstrated to achieve better results than the k-means bag-of-words framework on the CUReT database.Since Yang's and Xie's works are based on the template matching for classification, they still suffer from the shortage of the local estimators that a fair amount of training images are needed to reach a good generalization. In contrast, since the SRC proposed by Wright et al. classifies images through the reconstruction error of a joint representation, it usually needs less number of training images to generalize.In this work, we extend SRC for robust texture classification from few training images. By regarding each texture as lying in a separate low dimensional subspace, a textural image could only be represented as a sparse linear combination of the atoms in the dictionary of the class it belongs to. If the image is represented on dictionaries of other classes, the reconstruction errors would be large. Thus, a textural image could be classified by comparing all the class-specific sparse representation reconstruction errors. Specifically, we first learn a dictionary of sparse representation {Ai,i=1,…,C} for each class from the pyramid generated training samples {Tlj,l=1,…N,j=1,…,Ns} using an online sparse representation method [34], and then calculate a sparse representation of any test sample y on the dictionary of each class by solving a ℓ1-norm optimization problem (Eq. (4)), which is denoted asx^i,i=1,…,C. The sample y could be classified by comparing the reconstruction errors on each class according to Eq. (5).(4)minxi∥Aixi−y∥22+α∥xi∥1(5)labely=argmini∥y−Aix^i∥22,i=1,…,C.The proposed class-SRC method could be regarded as a simple supervised dictionary learning (SDL) approach for sparse representation. By utilizing the label information of training images in dictionary learning, SDL can learn a more discriminative dictionary for classification in many different ways [35–41]. The most simple SDL method is to learn a dictionary for each class, and then either concatenate the dictionaries as one [35] or use them separately [36] which we adopt in this work, to calculate the sparse codes. More sophisticated SDL approaches explored the discriminative power of using image labels by either maximizing the joint probability of training images and their labels [37], or incorporating a classifier (linear, bilinear, or softmax) into the model to learn the dictionary and classifier together [38–40], or simultaneously minimizing the intra-class covariation and maximizing the inter-class covariation of the sparse codes based on the Fisher discrimination criterion [41]. Though more discriminative power can be acquired using the sophisticated approaches, most of them are either hard and time consuming to tune or prone to getting stuck in local minima. A detailed introduction and comparison of the SDL approaches can be seen in [37]. In this work, we do not use the sophisticated SDL approach, but instead we apply a multi-manifold analysis on top of the simple class-SRC method to acquire more discriminative power, which we present in the next section.It is worthwhile to point out that the class-SRC method is very suitable to model the pyramid generated new training samples since the scale and spatial pyramid generated subimages of a textural image could cover different aspects of a texture, and through their sparse linear combination different texture realizations can be generated to simulate those captured under various imaging conditions to facilitate a robust texture classifier.Since the training samples are generated from just a few images, merely using the class-SRC to model them could be in lack of discriminative power and potentially subject to overfitting. To deal with these problems, we propose a novel multi-manifold analysis method to learn a projection matrix for each texture class by considering both the discrimination and generalization of the model.Denoting the class-specific projection matrices as {Di,i=1,…,C}, they are learnt by optimizing the following function:(6)minD1D2…DCJD1D2…DC=J1D1D2…DC+λJ2D1D2…DCwhere J1(D1,D2,…,DC) is the discriminative term, J2(D1,D2,…,DC) is the generalized term which is a manifold regularization in this work, and λ is a slack variable that compromises the two terms.Based on the projection matrix, we define a new formula to calculate the reconstruction error of image y on class i as:(7)E˜y=∥Diy−DiAix^i∥22=y−Aix^iTDiTDiy−Aix^iwhich could be seen as the original sparse representation reconstruction error∥y−Aix^i∥22calculated under a Mahalanobis metricM=DiTDi.Following [42], we introduce an intra-class error and inter-class error for each sample image respectively. The intra-class error of image y is represented asE˜cy=∥Dcy−DcAcx^c∥22, where c is the ground-truth class y belongs to, and the inter-class error of y is the least reconstruction error of those on classes other than c, which is defined asE˜dy=∥Ddy−DdAdx^d∥22, whered=argmini∥Diy−DiAix^i∥22,i=1,…,Candi≠c. To correctly classify an image, its intra-class error should be smaller than its inter-class error, and the smaller the ratio between the two, the more confidence will be gained in classification. Thus, we propose to learn the projection matrices in a discriminative way by minimizing the ratio of the intra-class error to inter-class error of all the training samples as the work in Zhang et al. [42]. We define the discriminative term in Eq. (6) as follows:(8)J1=1N∗Ns∑l=1N∑j=1NsSβ1RTlj−1whereRTlj=E˜cTlj/E˜dTljis the intra-class error to inter-class error ratio of Tlj, and Sβ(x)=1/1+exp(−β∗x) is a sigmoid function that has an “S” shape and could be seen as a smoothed version of the step function centred at x=0. Through minimizing J1, R(Tlj) will become smaller and smaller, which thus gain the method more confidence for classification, and will make the model more discriminative.In order to make the learnt projection matrices also general, we incorporate a smooth prior in the model. The smooth prior is a manifold assumption (also known as a graph regularization in [43,44]) that if two data yiand yjare close in the original space, they should also be close in the projected space by a projection matrix D, in which these two data will become Dyiand Dyj. We present the manifold regularization in the following form:(9)J2=1N∗Ns∑l=1N∑j=1Ns∑k=1Kwljk∗Sβ2MTljTljkwhere M(Tlj,Tljk)=∥Dc∗Tlj−Dc∗Tljk∥22, {Tljk,k=1,…,K} are the K Nearest Neighbours of Tijinside the same class, and {wljk,k=1,…,K} are the weights between them which reflect the degree of closeness. We define the weight between Tljkand Tljas:(10)wljk=exp−∥Tlj−Tljk∥22/σ2where σ is an empirically determined parameter. Interestingly, a similar multi-manifold analysis has also been utilized for the face recognition from a single training image [45], where each training image was partitioned into several non-overlapping patches and the multiple manifolds were learnt from these patches. They achieved quite promising results in several face recognition tasks which is also inspiring to us.To minimize Eq. (6), we use a gradient descent (GD) method to optimize the class-specific projection matrices. Since the discriminative term J1 and generalized term J2 are clearly defined, we could easily calculate the gradient of J(D1,D2,…,DC) to each projection matrix Diusing the chain rule as:(11)gDi=δJδDi=δJ1δDi+λδJ2δDi=1N∗Ns∑l=1N∑j=1NsδSβ1RTlj−1δRTlj∗δRTljδDi+λN∗Ns∑l=1N∑j=1Ns∑k=1Kwljk∗δSβ2MTljTljkδMTljTljk∗δMTljTljkδDi=1N∗Ns∑cTl=i∑j=1NsδSβ1RTlj−1δRTlj∗RTlj∗δE˜cTlj/δDiE˜cTlj−1N∗Ns∑dTl=i∑j=1NsδSβ1RTlj−1δRTlj∗RTlj∗δE˜dTlj/δDiE˜dTlj+λN∗Ns∑cTl=i∑j=1Ns∑k=1Kwljk∗δSβ2MTljTljkδMTljTljk∗δMTljTljkδDiwhere(12)δSβ1RTlj−1δRTlj=β1expβ11−RTlj1+expβ11−RTlj2(13)δE˜cTljδDi=2DiTlj−Acx^cljTlj−Acx^cljTs.t.c=i(14)δE˜dTljδDi=2DiTlj−Adx^dljTlj−Adx^dljTs.t.d=i(15)δSβ2MTljTljkδMTljTljk=β2exp−β2MTljTljk1+exp−β2MTljTljk2(16)δMTljTljkδDi=2DcTlj−TljkTlj−TljkTs.t.cTl=i.Then the projection matrix for each class is updated in an iterative way byDi=Di−γgDiuntil convergence or maximum iteration number is met, where γ is the learning rate.Instead of using random projection to initialize the projection matrices, the projection matrices are initialized by directly optimizing the manifold regularization term, in order to prevent bad local minimum which might be caused by random initialization in the projection matrix learning. To make the calculation easier, we write the manifold regularization in a slightly different form of J2 as follows:(17)minDii=1,…,Cfi=∑cTlj=i∑k=1Kwljk∥Di∗Tlj−Di∗Tljk∥22=tr(Di[∑cTlj=i∑k=1KwljkTlj−Tljk(Tlj−Tljk)T]DiT)=trDiHDiTwhereH=∑cTlj=i∑k=1KwljkTlj−TljkTlj−TljkT. SupposingDi∈RNd×Nf, the optimized solution could be achieved by concatenating the eigenvectors corresponding to the least Ndeigenvalues of H, which then becomes the projection matrix initialization of class i.We present the whole sparse representation based multi-manifold analysis method for projection matrix learning in Algorithm 1.Algorithm 1Sparse representation based multi-manifold analysis.Given a test image y, we first divide it into several subimages following the same way as to the training images, and denote all the subimages as {yj,j=1,…,Ns}, where Nsis the number of subimages generated. For each subimage yj, we calculate its sparse code on the dictionary of each class by solving Eq. (2) asx^ji, and label it using the projection matrix based class-SRC method as follows:(18)Labelyj=argmini∥Diyj−DiAix^ji∥22,i=1,…,C.The image y is classified to the class which has been selected as a label to its subimages for the most times via a plurality voting strategy:(19)Labely=argmaxi∑j=1NsLabelyj==i,i=1,…,C.The proposed method for texture classification from few training images is concluded in Algorithm 2.Algorithm 2Texture classification from few training images.Input: training images {Tl,l=1,…,N}, a test image y.1:New training samples generation:Divide each training image into a set of subimages using the scale and spatial pyramid technique described in Section 2.1 as the new training samples, denoted as {Tlj,l=1,…,N,j=1,…Ns};Dictionary learning of sparse representation:Utilize the online dictionary learning approach to learn a dictionary from the training samples of each class respectively, which construct a set of class-specific dictionaries {Ai,i=1,…,C};Projection matrix learning:Apply Algorithm 1 to learn a set of class-specific projection matrices from the training samples, denoted as {Di,i=1,…,C};Classification:Divide y into several subimages {yi,i=1,…,Ns} following the same way as to the training images, and label each subimage using the projection matrix based class-SRC method via Eq. (18). The test image y is finally classified by a plurality voting approach according to Eq. (19).Output: the final classification result: Label(y).

@&#CONCLUSIONS@&#
