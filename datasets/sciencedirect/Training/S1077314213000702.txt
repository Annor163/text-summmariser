@&#MAIN-TITLE@&#
Simultaneous segmentation of prostatic zones using Active Appearance Models with multiple coupled levelsets

@&#HIGHLIGHTS@&#
Landmark free Active Appearance Model can segment multiple objects simultaneously.Prostate, central gland, peripheral zone segmented with dice=.81, .79, .68.Hierarchical system uses existing segmentations to drive other segmentations.

@&#KEYPHRASES@&#
Active Appearance Models,Prostate segmentation,Levelsets,

@&#ABSTRACT@&#
In this work we present an improvement to the popular Active Appearance Model (AAM) algorithm, that we call the Multiple-Levelset AAM (MLA). The MLA can simultaneously segment multiple objects, and makes use of multiple levelsets, rather than anatomical landmarks, to define the shapes. AAMs traditionally define the shape of each object using a set of anatomical landmarks. However, landmarks can be difficult to identify, and AAMs traditionally only allow for segmentation of a single object of interest. The MLA, which is a landmark independent AAM, allows for levelsets of multiple objects to be determined and allows for them to be coupled with image intensities. This gives the MLA the flexibility to simulataneously segmentation multiple objects of interest in a new image.In this work we apply the MLA to segment the prostate capsule, the prostate peripheral zone (PZ), and the prostate central gland (CG), from a set of 40 endorectal, T2-weighted MRI images. The MLA system we employ in this work leverages a hierarchical segmentation framework, so constructed as to exploit domain specific attributes, by utilizing a given prostate segmentation to help drive the segmentations of the CG and PZ, which are embedded within the prostate. Our coupled MLA scheme yielded mean Dice accuracy values of .81, .79 and .68 for the prostate, CG, and PZ, respectively using a leave-one-out cross validation scheme over 40 patient studies. When only considering the midgland of the prostate, the mean DSC values were .89, .84, and .76 for the prostate, CG, and PZ respectively.

@&#INTRODUCTION@&#
Statistical Shape Models (SSMs) use shape information to yield an accurate, shape constrained, segmentation of an object of interest, and are extremely popular in medical image segmentation [1–6]. A common SSM methodology is the Active Appearance Model (AAM) segmentation algorithm [7]. AAMs attempt to learn both the appearance, and the shape, of an object of interest. In addition, AAMs aim to learn the relationship between the shape and appearance. When using an AAM to segment a new image, the appearance of the new image is matched to the AAM, and the associated shape yields a segmentation of the object of interest.AAMs achieve this by performing principal component analysis (PCA) on a set of intensities defining the object of interest to yield a low dimensional intensity projection. The intensity projections are then concatenated with the shape information, and PCA is performed a second time. The eigenvectors resulting from the second “coupled” PCA define the linear relationship between shape and appearance. Thus a given coupled projection defines both shape and appearance, and can be used to reconstruct the high dimensional intensity and shape information. However, traditional AAMs define the shape by a set of Cartesian coordinates defined from the landmarks of a single object. Yet there are several issues with using landmarks to construct statistical shape models,1.A large number of anatomical landmarks are required to accurately capture shape variations.Anatomical landmarks must be accurately aligned and landmark correspondences must be established on all training images [8].Automated method for landmark detection and alignment can be prone to errors [9].Landmarks require triangulation, and the triangulation algorithm may be prone to errors [10].Leventon et al. [11] first proposed performing PCA on a series of signed distance maps (levelsets) to capture shape variations, to overcome the issues with landmark based SSMs. A levelset is defined as a set of positive values at every pixel outside the object of interest, and a set of negative values at every pixel inside the object of interest. Therefore, a value of 0 would represent the surface of the object. The simplest way to compute a levelset is at each pixel in the image, calculate the Euclidean distance to the closest border pixel, and negate that value if the pixel values within the object of interest. To define multiple levelsets, the signed distance to the border of each object is computed.Tsai et al. [12] concatenated the levelsets of multiple objects prior to performing PCA, essentially “coupling” the levelsets. Hence a single set of low dimensional values (a “projection”) is used to represent the shape of multiple objects. This allows for simultaneous segmentation of multiple objects. In addition, coupling the individual SSMs allows one to take advantage of the inherent dependency between the spatial location of multiple adjoining organs. The technique used by Tsai et al. [12] was in 2D, and the SSMs included not just shape, but also pose information. Akhondi-Asl and Soltanian-Zadeh [13] developed a coupled SSM in 3D, which only accounted for shape variations (but not pose) by first aligning the training shapes. Akhondi-Asl and Soltanian-Zadeh [13] then explored whether coupling the SSMs actually improved segmentation accuracy over simply constructing individual SSMs. It was found that in most cases, shape coupling improves results only when the levelsets were first aligned prior to training.In this work, we present a new 3D AAM framework which is landmark-free, and which can segment multiple objects simultaneously in a new image via coupling multiple levelsets. Our model, the multiple levelset AAM (hereafter denoted as MLA) uses multiple coupled level sets to model the 3D shapes, thereby helping to alleviate many of the issues facing traditional landmark-based AAMs. The MLA offers the advantage of (1) not having to deal with the landmark identification problem and (2) not having to triangulate a series of landmarks to generate a 3D model. In addition, we take a similar approach to that proposed by Leventon et al. [11] and Akhondi-Asl and Soltanian-Zadeh [13], in that multiple levelsets are coupled to allow for simultaneous segmentation of multiple objects.Previous work in coupling multiple levelsets involved concatenating the high dimensional levelsets of multiple objects [12,13]. Yet this can be computationally infeasible when dealing with (a) multiple levelsets, (b) a large number of training images, or (c) very large images. To overcome these issues, the MLA performs PCA on each shape, prior to coupling the levelsets, similar to how AAMs perform PCA on the intensities prior to coupling.In addition, the MLA can also use existing segmentations of one or more organs to generate more accurate segmentations of the adjacent organs (for example using the prostate segmentation to simultaneously segment the bladder and rectum [14]). This is accomplished by generating the coupled projections using both intensities and levesets, whereas prior AAM models are only able to consider intensity information [7].This approach also allows the MLA to be used hierarchically, in which one object can first be segmented, and then used to drive the segmentations of other embedded objects. For example, in the case of prostate MRI, the central gland (CG) and peripheral zone (PZ) are substructures of the prostate itself (see Fig. 1). Using an existing prostate segmentation to segment the CG and PZ reduces the search space, which can help hone in on the embedded substructures within the gland. In addition, the coupled model allows for structural linking of the adjoining sub-structures, thereby permitting incorporation of anatomic constraints.Our MLA is applied to the task of prostate segmentation from endorectal, 3 Tesla, T2-weighted (T2-w) MR images. The prostate gland consists of internal structures including the peripheral zone (PZ), central zone (CZ), and transition zone (TZ), where the latter two structures are jointly referred to as the central gland (CG) [15] (see Fig. 1). While most tumors are found in the PZ, tumors can also be found in the CG, and CG tumors can have drastically different appearance than PZ tumors [16–18]. In the PZ (where most cancers are found), tumors are typically chacterized by hypointense regions on MRI images, in stark contrast to the usually hyperintense PZ regions. However, in the CG, tumors are typically noticeable due to their homogeneous texture, as compared to the traditionally hetergeneous texture in the CG. In recent years, several computer aided detection (CAD) systems have been developed for detecting tumors from prostate MRI imagery [19–25]. Since the tumors in the PZ can appear dramatically different from tumors in the CG, CAD systems would invariably benefit from knowing where each internal prostate structure was located. In addition, treatment options can even be tailored to an individual patient, as CG tumors have been found to be significantly less aggressive compared to PZ tumors [26].However, most extant prostate segmentation systems only consider the prostate capsule boundary [6,27–32]. Makni et al. [33] developed a system for distinguishing the internal prostate structures on multiparametric MRI, but assumed the prostate is already segmented. Additionally Makni et al. [33] required the use of both T2-w and DCE-MRI to drive the segmentation, as opposed to just a single protocol; imaging with additional parameters (over and above standard T2-w) leads to an increased imaging time and hence cost of exam. Liu and Yetik [25] circumvented the issue of zonal segmentation within the prostate by proposing a spatially aware CAD system for cancer detection, to automatically identify in which part of the prostate the tumor was located.In this work, we aim to use our MLA to automatically and simultaneously segment the prostate, PZ, and CG from T2-weighted MRI alone. As with Liu and Yetik [25], our ultimate goal is to develop a spatially aware CAD system for prostate cancer detection, but by leveraging the explicit, automated segmentations of the different prostate zones. A second application is to create patient-specific treatment models based on the zonal location of the tumor.The rest of the paper is organized as follows. Section 2 describes the methodology for training the MLA and using the MLA to segment a new image. Section 3 describes the experimental design and dataset. Section 4 presents the results. A brief discussion is presented in Section 5, while Section 6 presents concluding remarks and future directions.

@&#CONCLUSIONS@&#
