@&#MAIN-TITLE@&#
Multi-level learning based memetic algorithm for community detection

@&#HIGHLIGHTS@&#
We propose a fast memetic algorithm to uncover community structure in networks.The proposed algorithm is based on novel multi-level learning strategies at nodes, communities and network partitions levels.Our algorithm need not know the number of clusters in advance.Our algorithm has superior performances in speed, accuracy and stability.

@&#KEYPHRASES@&#
Memetic algorithm,Genetic algorithm,Community structure,Complex networks,Modularity,

@&#ABSTRACT@&#
Complex network has become an important way to analyze the massive disordered information of complex systems, and its community structure property is indispensable to discover the potential functionality of these systems. The research on uncovering the community structure of networks has attracted great attentions from various fields in recent years. Many community detection approaches have been proposed based on the modularity optimization. Among them, the algorithms which optimize one initial solution to a better one are easy to get into local optima. Moreover, the algorithms which are susceptible to the optimized order are easy to obtain unstable solutions. In addition, the algorithms which simultaneously optimize a population of solutions have high computational complexity, and thus they are difficult to apply to practical problems. To solve the above problems, in this study, we propose a fast memetic algorithm with multi-level learning strategies for community detection by optimizing modularity. The proposed algorithm adopts genetic algorithm to optimize a population of solutions and uses the proposed multi-level learning strategies to accelerate the optimization process. The multi-level learning strategies are devised based on the potential knowledge of the node, community and partition structures of networks, and they work on the network at nodes, communities and network partitions levels, respectively. Extensive experiments on both benchmarks and real-world networks demonstrate that compared with the state-of-the-art community detection algorithms, the proposed algorithm has effective performance on discovering the community structure of networks.

@&#INTRODUCTION@&#
With the advancement of Internet and Web 2.0 techniques, many real-world complex systems, including online communities, power grids, collaboration systems, disease control systems, resource distribution systems and information recommendation systems, are closely related to our daily activities by sharing information [1–4]. In a world of the huge and disordered information, how to use an effective computing model to mine and analyze the potentially useful information has become a commonly concerning issue [4]. In recent years, the research on complex networks has attracted more and more attentions in the fields of biology, physics, sociology and mathematics [1–3,5]. The topological structure of complex systems can easily be modeled as a complex network with linked nodes. More specifically, the entities of complex systems can be represented as the nodes (or vertices) of networks, and the relations between entities can be modeled as links (or edges) of networks. Community structure is a common and important property in both complex networks and complex systems [6]. In complex networks, communities are made up of a set of nodes which have more connections with each other than those with the remaining nodes in the network [7–10]. The community structure property is indispensable to reflect the potential structural behavior of networks [6]. In complex systems, communities are composed of a few entities which have similar properties. Generally, the potential functionality of complex systems is related to its community structure property [6].Various methods have been proposed to detect communities in complex networks. Among them, one of the most popular techniques is based on the optimization of an objective, modularity, which is the most widely used criterion to evaluate the quality of the community structure of networks [6,11]. The modularity optimization methods to detect communities are based on searching a particular network partition which has the maximal modularity [6,11]. Recent studies in [12,13] demonstrate that the number of local maxima in the optimization of modularity is exponentially growing with the increase of the size of complex networks.Many heuristic algorithms based on optimizing modularity for community detection have been proposed in recent years [14–26]. Some of them optimize from one solution to a better one and thus easily get into local optimal solutions [27]. Moreover, some of them are sensitive to the optimized order and thus the detected network partitions are not stable [6,27]. For instance, the fast greedy method (FM) [14], which iteratively joins a pair of communities with the largest gain in modularity, tends to obtain quite large communities and neglect small ones [6], and thus it is easy to get into a local optimal network partition. The modularity-specific label propagation algorithm (LPAm), proposed by Barber and Clark [19], is sensitive to the optimized order of the nodes, and thus the revealed network partitions are different at different independent trials. Moreover, it is also easy to get into a local optimal network partition where the detected communities are similar in total degree [20]. The multistep techniques, proposed by Blondel et al. [17] and Schuetz and Cafisch [18], which iteratively merges a set of nodes and communities, are also easy to get into a local optimal solution. This is because the merged nodes and communities can hardly be separated again [28]. Several techniques have been proposed to enhance both the accuracy and the stability of modularity-based optimization methods. Lai et al. [29] and Yan and Gregory [30] use network preprocessing techniques to improve the accuracy. Both of them are based on the assumption that the vertices in the same community possess more similar behaviors than those in different communities. Lancichinetti and Fortunato [31] adopt the consensus clustering technique to enhance both the accuracy and the stability of the resulting network partitions. Pizzuti, Shi and we adopt multiobjective optimization algorithms to discover a proper and stable network partition which has a high value of modularity [25,32,33]. In this study, we try to use an intelligent hybrid technique, memetic algorithm (MA), to improve the performance of modularity-based community detection algorithms by simultaneously evolving a population of solutions to better ones.With the rapid development of computer science, mathematics and biology, the research on intelligent algorithms for solving practical engineering problems has attracted increasing attentions in recent years. Many intelligent algorithms, including genetic algorithm (GA) [34], memetic algorithm [35–37], artificial neural network [38], simulated annealing [15], swarm intelligence algorithms [39], fuzzy system [40–44], artificial immune system [39], and so on, have been designed as problem-specific techniques for tackling and solving real-world applications [32,45–51]. Compared with the traditional algorithms, the problem-specific intelligent algorithms can effectively find a proper solution with high quality in a reasonable period of time [39].Memetic algorithms (MAs) are hybrid global-local heuristic search methodologies [35]. The global heuristic search is usually a form of population-based method, while the local search is generally considered as an individual learning procedure for accelerating the convergence to an optimum [35]. In general, the population-based global search has the advantage of exploring the promising search space and providing a reliable estimate of the global optimum [36]. However, the population-based global search is difficult to discover an optimal solution around the explored search space in a short time. The local search is usually designed for accelerating the search and finding the best solutions on the explored search space. Therefore, this hybridization, which synthesizes the complementary advantages of the population-based global search and the individual-based local search, can effectively produce better solutions [37]. Recent studies on MAs have demonstrated that they are effective and efficient for tackling the optimization problems in many real world applications [35–37,48–51]. MAs have also been used for uncovering communities in networks in recent years [45,52–54]. For instance, in [45], we proposed a memetic algorithm, named as Meme-Net, to uncover communities at different hierarchical levels. Meme-Net shows its effectiveness. However, its high computational complexity makes it impossible to search communities on slightly large networks. Shang et al. [52] and we [53] try to adopt simulated annealing as an individual learning procedure to decrease the computational complexity of Meme-Net. However, their computational complexity are still very high relative to classical modularity-based community detection algorithms. The algorithm in [54] adopts the technique in [17] as the local search. Meanwhile, it takes a large amount of time and energy on generating initial population as it directly use the algorithm in [17] to initialize a population of solutions. Therefore, the algorithms in [45,52–54] are difficult to apply to real-world problems.Motivated by the above descriptions, in this study, we present a fast memetic algorithm with multi-level learning strategies to detect communities by optimizing modularity. We term the proposed algorithm as MLCD for short. MLCD adopts a genetic algorithm as the global search and uses the proposed multi-level learning algorithms to accelerate the convergence. The proposed multi-level learning strategies work on the network at node, cluster, and network partition levels, respectively. By iteratively executing GA and multi-level learning strategies, a network partition with high modularity can be accurately and stably obtained. We also employ a modularity-specific label propagation rule to update the cluster identifier of each node at each operation. The simple update rule guarantees the rapidity of the proposed algorithm. Experiments on GN and LFR benchmarks and 12 real-world networks demonstrate that compared with the classical community detection algorithms, MLCD has the superior performance in stably finding a proper community structure of networks. It is also shown that compared with Meme-Net, MLCD takes much less time to find a more proper and stable community division of networks.The remainder of this paper is organized as follows. In the next section, the problem definition is given. Section 3 gives a detail description for the proposed algorithm. In Section 4, experiments on GN and LFR benchmarks and 12 real-world networks are given to demonstrate the effectiveness of the proposed algorithm. Finally, the conclusion is given.Let us consider an unweighted and undirected graph G=(V;E) which has |V|=n vertices (or nodes) and |E|=m edges (or links), the connection of the graph G can be represented as an adjacency matrix A. Its element Aijis 1 when a link between nodesviandvjexists and 0 otherwise [6]. In order to detect the underlying structure of complex networks, we need to know the definition of community. However, there is no agreement of the definition of community in networks. Its definition is closely relate to the specific systems and practical applications [6]. In most cases, an algorithmic definition of community, which is based on the degree of vertex in graph, is adopted [7]. An algorithmic community (or a sub-graph S∈G) is defined as a cluster whose internal degree, the total weight of the links within the sub-graph (kSin=∑i∈s,j∈sAij), is larger than its external degree, the total weight of links towards the rest of the network (kSout=∑i∈s,j∉sAij) [7].Note that, many network divisions satisfy the above algorithmic definition. Most of these divisions cannot reflect the potential structural organizations and functional behaviors of complex systems. In real-world applications, good network divisions can effectively illustrate their potential structural organizations. Therefore, it is necessary to adopt a criterion to evaluate the quality of network partitions.Many criteria have been proposed to evaluate network partitions, and they are devised according to the knowledge in the fields of physics, biology and sociology. Modularity, proposed by Grivan and Newman [8] based on the graph theory, is up to now the most representative quality criterion for measuring the quality of network partitions. Modularity measures the difference between the actual fraction of edges within communities and its expected value in a null model [8]. Considering a network with m edges and k communities, the modularity can be expressed as:(1)Q=∑s=1klsm−(ks2m)2,where lsand ksare the total number of links and the total degree of nodes in the cluster s, respectively. The value of Q ranges from −1 to 1. When Q<0, it means that the network partition is useless for reflecting the community structure of the network. If there is no difference in the fraction of edges within between communities and a null model, then we will get Q=0. For a given network, a network partition with a higher modularity value corresponds to a better community division of the network [8,11]. If the criterion Q is adopted to evaluate the quality of a community division of networks, the problem of uncovering communities in complex networks becomes a modularity-based optimization problem.The proposed algorithm MLCD is easy to implement, and its process can be described as follows. Firstly, initialize a population of solutionsXB={x1,x2,…,xNp}through a problem-specific strategy, where Npis predefined as the size of the population. Then, the genetic operators, including crossover and mutation procedures, are performed according to the predefined ratio on the randomly chosen solutions XC. Next, use the proposed multi-level learning strategies to accelerate the above evolutionary process. Finally, update the population and perform the next iteration. The above process ends when the termination criterion is met. The main framework of MLCD is shown in Fig. 1.In what follows, more detailed descriptions about the initialization procedure, genetic operators and the multi-level learning strategies will be given.In this study, each solution (chromosome or individual) xa, 1≤a≤Np, in the population can be represented as [34]xa={xa1,xa2,…,xan},wherexajrepresents the cluster identifier of vertexvjin chromosome xaand it can take an integer value in the range of 1 to n. In the decoding step, ifxai=xaj, the nodesviandvjare in the same cluster. Otherwise, the nodesviandvjare in different clusters. When each node is assigned to a cluster, a network partition with k clusters is obtained. The most striking feature of this representation is that it is unnecessary to set the value of k in advance which is automatically determined in the decoding step. An illustration of this representation for a toy network with 7 nodes is given in Fig. 2.In order to avoid unnecessary divisions containing unconnected nodes in initial solutions, we adopt a simple heuristic initialization process [34,45]. The initialization process can be described as follows: firstly, each gene in chromosome is put in a different cluster (i.e.xai←i, 1≤i≤n, 1≤a≤Np). Then, generate a random sequence. Next, for each vertexvichosen in that sequence, update its cluster identifier with one of its neighbors’ (i.e.xai←xaj, ∃j∈{j|Aij=1}). The above processes independently run Nptimes, and a population of solutions are generated. The solution, which has maximal modularity in the population, is chosen as xg.The heuristic initialization process has the following advantages. (1) Its operation is easy to implement, and the generated solutions are unsupervised. This makes the proposed algorithm less sensitive to the optimized order. (2) It just takes into account of the effective connections of the network and avoids generating unnecessary network divisions. (3) The initialization algorithm has a low computational complexity and this makes the initialization process run fast.The genetic operators, including crossover and mutation, are important to change the genetic composing of the chosen solutions XC. The crossover operator is to generate offspring chromosomes which can inherit their parent chromosomes’ communities, and the mutation operator is to generate spontaneous random changes in chromosomes. The crossover and mutation operators can effectively prevent the proposed algorithm from getting into a local optimal network partition.The traditional crossover operations, including the uniform crossover [32] and the two-point crossover [47], have great randomness and large uncertainty, and thus the resulting descendants can hardly inherit useful communities from their parents. In this study, we employ a two-way crossover operation [45]. The crossover procedure is given as Algorithm 1. Firstly, randomly choose two chromosomes xaand xbfrom XC. Then, randomly choose a vertexviand identify the cluster identifiers of nodeviin xaand xb(xaiandxbi, respectively). Next, generate a random value in the range of 0–1. If the generated value is smaller than the crossover probability pc, then assign the cluster identifier of the corresponding vertices which have the same cluster identifier asxaiin xawithxaiin xb(i.e.xbj←xai,∀j∈{j|xaj=xai}). At the same time, assign the cluster identifier of the corresponding vertices which have the same cluster identifier asxbiin xbasxbiin xa(i.e.xaj←xbi,∀j∈{j|xbj=xbi}). The above processes repeatedly operate ⌊Nc/2⌋ times.Algorithm 1Two-way crossover1:Input: XCand the crossover probability pc.2:r←13:whiler≤⌊Nc/2⌋ do4:Randomly select two chromosomes xaand xb(a≠b) from XCand randomly select a vertexvi.5:Randomly generate a value f within [0 1].6:iff≤pcthen7:xbj←xai,∀j∈{j|xaj=xai}.8:xaj←xbi,∀j∈{j|xbj=xbi}.9:end if10:r←r+111:end whileThis two-way crossover operation can generate descendants which inherit most communities from their parents [45]. As is shown in Fig. 3, a toy network G has two communitiess1={v1,v2,v3,v4}ands2={v5,v6,v7}. The parent chromosomes xa={1, 1, 1, 1, 2, 3, 4} and xb={1, 2, 3, 4, 5, 5, 5} have the community structure s1 and s2, respectively. Assuming that the vertexv5is chosen, in the following operations, the nodesv5,v6andv7in xaare assigned with the cluster identifier of the corresponding nodes in xb, and thus produce a descendant xc={1, 1, 1, 1, 5, 5, 5}. However, as for the uniform and two-point crossover operations, they can hardly inherit the communities from their parents.In order to decrease useless exploration, we use a neighbor-based mutation operator [32,47] which considers the effective connections among nodes. The neighbor-based mutation procedure works on the generated population XC, and it works as follows. For each nodeviin a chromosome xc(xc∈XC), firstly, generate a random value in the range of 0–1. If the generated value is smaller than the mutation probability pm, then the cluster identifier of the nodeviis randomly mutated to one of its neighbors’ (i.e.xci←xcj,∃j∈{j|Aij=1}). Among the generated offspring chromosomes, the chromosome, which has the maximal modularity value, is chosen as xl.In this study, we propose a multi-level learning based local search to refine the individual xl. The multilevel learning based local search is proposed based on the potential knowledge of the node, community and partition structures of networks, and it is composed of three learning techniques from low level to high level. Each level learning technique can converge to a local optimal solution rapidly and the higher level learning techniques have the ability to escape from the local optima obtained by the lower level learning techniques. In the realization of the proposed multi-level learning techniques, in order to avoid generating unnecessary divisions containing unconnected nodes and communities and to find the best solution as soon as possible, we update the cluster identifier of nodes and communities with their neighbors’ when the increment of modularity ΔQ is maximal. Moreover, we employ a modularity-specific label propagation rule to update the cluster identifier of each node and community at each operation. This simple update rule guarantees the rapidity of the proposed multi-level learning techniques. In addition, in order to reduce the sensitivity of the proposed learning techniques to the optimized order, we update the cluster identifier of nodes and communities in a random order.A node-level learning strategy, which is similar to the algorithm LPAm [19], is chosen as our first-level learning technique, and it takes xlas the initial solution. The node-level learning strategy works on each node of the network and its processes is shown in Algorithm 2. Given a network G with n nodes, the first-level learning strategy works as follows. Firstly, generate a random sequence (i.e. {r1, r2, …, rn}). Then, for each vertexvrichosen in that generated sequence, update its cluster identifierxlriwith one of its neighbors’ using the modularity-specific label propagation rule described in Section 3.4.4. The above processes end when the cluster identifier of every node has no change. The first-level learning strategy can help GA quickly converge to an optimal solution around one of its search space.Algorithm 2Node-level learning strategy1:Input: xland G.2:repeat3:Generate a random sequence (i.e. {r1, r2, …, rn}).4:for each nodevriof Gdo5:Updatexlriusing the modularity-specific label propagation rule.6:end for7:until the cluster identifier of each node is not changed.8:Output: xl.Note that, the first-level learning strategy tends to uncover a network partition with similar communities in total degree, and thus it is easy to fall into a local optimum [20]. For example, as is shown in Fig. 4, for a toy network G1 with 16 nodes, the intuitive community division divides the network into two communities (drawn in circle and square shapes respectively) with Q=0.4127. The first-level learning strategy tends to divide this network into four communities with Q=0.3988, as is shown in Fig. 4(b). The modularity value of this network partition is not increased by removing any node from its community and placing it in that of its neighbors’. Evidently, the community division obtained by the first-level learning strategy falls into a local optimal solution. Therefore, it is necessary to take measures to help the first-level learning strategy escape from its local optimum.Actually, for small size networks, the hybrid technique, which iteratively employs GA and the first-level learning strategy, can escape from the local optimum obtained by the first-level learning strategy. A schematic illustration of how GA helps the first-level learning strategy escape from its local optimum is shown in Fig. 5. Assuming that the local optimal solution obtained by the first-level learning strategy is x1 and there is one solution like x2 in the population, the offspring chromosome x1′ which corresponds to the true community division of the toy network G1 is generated after employing the two-way crossover operation on the two chromosomes x1 and x2. Therefore, the hybrid algorithm, termed as M-LPAm, which iteratively performs GA and the first-level learning strategy, has the ability to help the first-level learning strategy escape from its local optimum. However, for the larger scale networks, M-LPAm is difficulty to solve this problem. This is because the probability of the existence of the chromosome like x2 in the population becomes more and more small. In this study, we devise a community-level learning strategy to help the first-level learning strategy further escape from its local optimum.A community-level learning technique which is similar to the second phase of the BGLL algorithm [17] is devised as our second-level learning strategy, and it is shown in Algorithm 3. The second-level learning strategy works on each community of the solution xlgenerated by the first-level learning strategy. Assuming that the chromosome xlhas k communities (e.g. s1, s2, …, sk), the second-level learning strategy works as follows. Firstly, a new network, whose nodes are now the communities of xl, is generated. In this case, the links between the new nodesvi′andvj′are the total links between the nodes in the corresponding communities siand sjof xl. A schematic illustration of the above operation is given in Fig. 6(a) with a toy network G1. The original partition of the toy network G1 is composed of 4 communities, which have 4, 2, 4, and 6 nodes respectively. In the generated new network, there are four nodes and each node corresponds to one community of the original network. The weight of links between the new nodes is the total weight of links between nodes in the corresponding communities. Secondly, assign each new node with a unique cluster identifier, i.e. Xi=i, 1≤i≤k, where Xirepresents the cluster identifier of the new nodevi′. Thirdly, employ the first-level learning strategy on X to find a better partition of the new network G′. In Algorithm 3, we use the function of NodeLearning() to represent the node-level learning technique. Finally, decode the partition of new network G′ to the community division of the original network G. The second-level learning procedure returns a new chromosome xe. Compared with the second phase of the BGLL algorithm, the community-level learning technique has different strategies in terms of the optimization order and the cluster identifier update rule.Algorithm 3Community-level learning strategy1:Input: xland G.2:Generate a new network G′=(V′, E′), whereV′={v1⁡′,v2⁡′,…,vk⁡′}.vi⁡′is the corresponding community siin xl, 1≤i≤k. The connections between the new nodes can be represented as an adjacency matrix A′, whose elements Aij′ is computed asAij′←∑i∈si∑j∈sjAij.3:Set Xi=i, 1≤i≤k.4:X← NodeLearning(X,G′).5:whilei≤kdo6:xej←Xi,∀j∈{j|vj∈si}.7:end while8:Output: xe.The second-level learning strategy can help the first-level learning strategy jump out of its local maximum. As is shown in Fig. 6(b), after merging the upper left three communities, the generated partition is the same as the true one with Q=0.4238.Note that, the first two level learning strategies are also easy to fall into local optimal solutions. This is because the merged communities can hardly be separated again. Therefore, the new partition may not always be good enough (although it is better than the previous local maximum). The above view is confirmed by Rosvall and Axelsoon [28]. For example, as is shown in Fig. 7(a), the new toy network G2 with 16 nodes is intuitively divided into three communities (drawn in circle, triangle and square shapes respectively), with Q=0.4298. After performing the first-level learning and the second-level learning strategies, the obtained partitions tend to divide the toy network into four communities with Q=0.4094 (Fig. 7(b)) and two communities with Q=0.4127 (Fig. 7(c)), respectively. Evidently, these divisions correspond to local optima in the modularity space. As is shown in Fig. 7(c), the left large community can hardly be separated again by performing the first two level learning strategies.In this study, a structural learning strategy, which works on the network partition layer, is proposed to help the first two level learning strategies escape from their local optima.The third-level learning strategy is the proposed partition-level learning procedure, and it is shown in Algorithm 4. It works on two chromosomes,xg={g1,g2,…,gk1}andxe={s1,s2,…,sk2}, where k1 and k2 are the number of clusters of xgand xe, respectively. The partition-level learning strategy consists of two phases, and its first phase works as follows. For each cluster gi, 1≤i≤k1, of xg, the corresponding nodes in giare divided into a set of clusters according to the following two principles: (1) If the nodes are in the same cluster in both xgand xe, they are divided into the same cluster; (2) If the nodes are in the same cluster in xg, while they are in different clusters in xe, they are divided into different clusters. The first phase returns a basic consensual network partitionxf={g1′,g2′,…,gk3′}, where k3 is the number of clusters of xfand its value depends on the difference between xgand xe. The second phase is employing the first two level learning strategies on xfto find a better community division of networks. In Algorithm 4, we use the functions of NodeLearning() and CommunityLearning() to represent the node-level learning and the community-level learning techniques, respectively. The partition-level learning procedure returns a new chromosome xl′.A schematic illustration of the partition-level learning strategy on two individuals xgand xeis given in Fig. 8(c). As is shown in Fig. 8(c), individual xghas two communitiesg1={v1,v2,v3,v4}andg2={v5,v6,v7,v8,v9,v10,v11,v12,v13,v14,v15,v16}, and individual xehas two communitiess1={v1,v2,v3,v4,v5,v6,v7,v8,v9,v10}ands2={v11,v12,v13,v14,v15,v16}. For the nodes of the first cluster g1 of xg, they are in the same cluster in xe. Therefore, in the generated individual xf, the nodesv1v2,v3andv4form a cluster. For the nodes of the second community g2 of xg, the nodesv5,v6,v7,v8,v9andv10have the same cluster identifier 1 in xe, and the nodesv11,v12,v13,v14,v15andv16have the same cluster identifier 2 in xe. Therefore, in the generated individual xf, the nodesv5,v6,v7,v8,v9andv10form a cluster and the nodesv11,v12,v13,v14,v15andv16form a new cluster.Algorithm 4Partition-level learning strategy1:Input: G,xg={g1,g2,…,gk1}andxe={s1,s2,…,sk2}.2:Setxfi←0, i=1, 2, …, n, and C←0;3:for each cluster giof xgdo4:Identify the nodes in giand set them asgi={vi1,vi2,…,vi|gi|}, where |gi| is the size of gi.5:for each nodevijof gido6:ifxfij=0then7:C←C+1;8:xfq←C, where∀q∈{q|vq∈gi&xeij=xeq};9:end if10:end for11:end for12:xf← NodeLearning(xf,G).13:xl′← CommunityLearning(xf,G).14:Output: xl′.The partition-level learning strategy can help the first two level learning strategies to escape from their local maxima. As is shown in Fig. 8, after performing the partition-level learning strategy, the large community s1 of xehas been broken into two communities and the true partition has been uncovered.The reasons why the partition-level learning strategy can improve the accuracy of the first two level learning strategies are as follows. Firstly, the first two level learning strategies tend to get into local optimal network partitions, mainly because it can hardly be separated again after merging two or more nodes and communities together and forming a single one [28]. The partition-level learning strategy makes it possible for the merged nodes and communities to be separated again. After reconstructing the separated nodes and communities, a good network partition with high modularity value can be obtained, as is shown in Fig. 8. Moreover, the partition-level learning strategy is a consensus clustering technique working on the solutions xgand xe. The same divisions between xgand xeare preserved, and the differences between them are divided. Therefore, the partition-level learning strategy collects the structure property from two or more solutions, rather than one solution. It makes it possible to generate better solutions than that uncovered by the first two level learning strategies which just collect the structure property from one possible solution. Finally, the structural learning strategy can generate new solutions which are different from xgand xe. Therefore, it can enhance both the population diversity and the global searching capability of GA.In this study, in order to decrease the computation complexity, we adopt a simple way to calculate the ΔQ obtained by moving an isolated nodeviinto its neighbor cluster sj. The ΔQ can easily be computed by:(2)ΔQ=lsj+li,sjm−ksj+ki2m2−linm−ksj2m2−ki2m2=li,sjm−kiksj2m2,wherelsjis the number of links inside cluster sj,li,sjis the number of links from nodevito nodes in sj, kirepresents the degree of the nodeviandksjis the total degree of nodes in sj[17]. Let xiand Cidenote the cluster identifier of the nodeviand the cluster si, respectively, the above equation also can be written in terms of the adjacency matrix A as follow(3)ΔQ=1m∑q≠iAiqδ(xq,Cj)−12mki∑q≠ikqδ(xq,Cj),where δ is the Kronecker delta. In this study, the maximum ΔQ is chosen as the update rule for updating the cluster identifier of each isolated nodevi. Therefore, the new cluster identifier Xiof the nodevican be written as(4)Xi=argmaxC∑q≠iAiqδ(xq,C)−12mki∑q≠ikqδ(xq,C).The rule of Eq. (4) is the modularity-specific label propagation update rule [19]. As is known from Eq. (4), when Xi=xi, it indicates that updating the cluster identifier ofviwith any one of its neighbors’ cannot increase the value of modularity. When Xi≠xi, it indicates that updating the cluster identifier ofviwith Xiproduces the maximal increment of modularity.In this part, the time complexity of the proposed algorithm MLCD is analyzed. Given a network with n nodes and m edges, at each generation, firstly, we need to perform the crossover operator ⌊Nc/2⌋ times and the mutation operator Nctimes at most, where Ncis the size of the chosen population. The time complexity of the calculation of modularity is O(m). Therefore, the time complexity of the genetic operator is O(Nc(n+m)). Then, we need to perform the multi-level learning strategies. The time complexity of the first-level learning strategy is O(rm), where r is the number of steps required to reach a local maximum in modularity space [20]. The second-level learning algorithm needs to merge pairs of communities which requires O(mlogn) basic operations [18]. Therefore, the time complexity of the second-level learning algorithm is O(hmlogn), where h is the number of steps required to reach a local maximum in modularity space. The time complexity of the first step and the second step of the partition-level learning strategy are O(n) and O(rm+hmlogn), respectively. Generally, r=logn and h≈logn[20]. Therefore, the overall time cost for the multi-level learning strategies at each generation is O(m(logn)2). Finally, we need to update the population, which needs O(Np+Nc) basic operations. In practical applications, we have the following relations: Np+Nc≤Nc(n+m)≤m(logn)2. Therefore, the time complexity of the proposed MLCD algorithm at each generation is O(m(logn)2).By comparing MLCD with Meme-Net, it can be seen that both of them model the community detection in networks into an optimization problem, and use the framework of memetic algorithm to solve the modeled optimization problem. Moreover, both of them have excellent performance in discovering the potential community structure of networks. However, they are different in the following aspects.Firstly, the motivation of Meme-Net is to find a set of hierarchical community structures on small-scale networks by optimizing an objective with a resolution parameter λ. If we want to reveal a community structure at a certain hierarchical level, it is necessary to set the value of λ in advance. The motivations of MLCD are to find the best community division of networks quickly and stability without knowing the number of clusters in advance, and to extend our previous work to handle larger scale networks.Secondly, the modeled optimization problems are different. Meme-Net and MLCD model the community detection of networks into an optimization problem based on modularity density and modularity, respectively. The optimization of modularity density tends to reveal a community structure of a network which has greater internal link density than external link density of communities, while the optimization of modularity tends to discover a community structure which has more intra-community links than inter-community links.Moreover, Meme-Net adopts a hill climbing strategy based local search which works on a network at nodes level, while MLCD uses the proposed multilevel learning techniques based local search which works on a network at nodes, communities, and network partitions levels, respectively. Compared with the proposed multilevel learning techniques, the hill climbing strategy has higher computational complexity. Given a network with n nodes and m edges, at each step of the hill climbing technique, it needs to consider all the possible network partitions. Moreover, it requires logn steps to reach a local maximum in the modularity density space. In addition, the computation of the modularity density of each network partition requires O(m) basic operations. In the worst case, the hill climbing strategy is necessary to consider n2 network partitions. Therefore, the overall time cost of the hill climbing technique at each generation is O(mn2logn). For a sparse network nlogn≈m, the time complexity of the hill climbing strategy is O(m2n), which is far greater than that of GA, O(Npm), where Npis the size of the population. Due to the high computational complexity (O(m2n)), Meme-Net can only handle small-scale networks. For the proposed algorithm MLCD, its low computational complexity (O(m(logn)2) makes it possible to tackle larger scale networks.In addition, Meme-Net and MLCD have different initialization process. In the initialization of Meme-Net, the cluster identifiers of αn nodes are assigned to all of their neighbors. The diversity and the quality of the generated initial population are controlled by the parameter α. For the MLCD, it no longer needs a controlling parameter to adjust the diversity and the quality of the generated initial population. The initialized solutions are generated by assigning the cluster identifier of each node with one of its neighbors’.Finally, the experimental comparisons between MLCD and Meme-Net demonstrate that MLCD has superior performance than Meme-Net in terms of the quality of the detected community structure, the stability and the time consumption.

@&#CONCLUSIONS@&#
