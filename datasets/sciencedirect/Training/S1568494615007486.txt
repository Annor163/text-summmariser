@&#MAIN-TITLE@&#
Supervised fuzzy reinforcement learning for robot navigation

@&#HIGHLIGHTS@&#
A novel method for combination of supervised learning and fuzzy reinforcement learning (FRL) is proposed.Supervised learning is used for initialization of value (worth) of each candidate action of fuzzy rules in critic-only based FRL algorithms.The subsumption architecture is used for robot navigation.The proposed algorithm, called SFSL, is used to drive a real robot (E-puck) in an environment with obstacles.SFSL outperforms FSL in terms of speed learning, and the number of failures.

@&#KEYPHRASES@&#
Robot navigation,Supervised learning,Reinforcement learning,Fuzzy systems,

@&#ABSTRACT@&#
This paper addresses a new method for combination of supervised learning and reinforcement learning (RL). Applying supervised learning in robot navigation encounters serious challenges such as inconsistent and noisy data, difficulty for gathering training data, and high error in training data. RL capabilities such as training only by one evaluation scalar signal, and high degree of exploration have encouraged researchers to use RL in robot navigation problem. However, RL algorithms are time consuming as well as suffer from high failure rate in the training phase. Here, we propose Supervised Fuzzy Sarsa Learning (SFSL) as a novel idea for utilizing advantages of both supervised and reinforcement learning algorithms. A zero order Takagi–Sugeno fuzzy controller with some candidate actions for each rule is considered as the main module of robot's controller. The aim of training is to find the best action for each fuzzy rule. In the first step, a human supervisor drives an E-puck robot within the environment and the training data are gathered. In the second step as a hard tuning, the training data are used for initializing the value (worth) of each candidate action in the fuzzy rules. Afterwards, the fuzzy Sarsa learning module, as a critic-only based fuzzy reinforcement learner, fine tunes the parameters of conclusion parts of the fuzzy controller online. The proposed algorithm is used for driving E-puck robot in the environment with obstacles. The experiment results show that the proposed approach decreases the learning time and the number of failures; also it improves the quality of the robot's motion in the testing environments.

@&#INTRODUCTION@&#
The main goal in robot navigation is directing the robot to move from a starting point to a target point without hitting obstacles [1,2]. In dynamic and uncertain environments, applying global search algorithms do not suffice to address robot navigation due to the lack of a complete model or the map of environment. Researchers have used the local search and the local path planning algorithms with help of data obtained from robot's sensors such as sonar and infrared devices [3].Learning methods are widely used for tuning local controllers’ parameters. Supervised learning is one of the first methods that have been used for training robot controllers [4,5]. In a supervised learning method, a supervisor drives robot through environment and in each time step, the supervisor command and corresponding sensors’ data are gathered. Then, the collected data are used for adjusting the controller's parameters based on gradient descent methods [6–8].However, supervised learning in robot navigation has serious drawbacks as follows:1In complex situations, a supervisor cannot make right decisions, so the wrong decisions can cause significant errors in the collected (training) data.Since the supervisor tries to drive the robot in the best way, many of the states may not be explored (visited).The inconsistency in data causes significant errors: For example, assume that a robot is in front of an obstacle and can avoid the obstacle either from the left or the right. In such a condition, a human supervisor may decide to turn robot to the right (rotation angle +45°) or turn to the left (rotation angle −45°). Different decisions in similar situations cause big disturbance in the training procedure. Since the objective function for a training procedure is to minimize the sum of squared errors, a value close to zero (i.e., forward movement) will be obtained by gradient descent methods for this condition. This causes (or may cause) a collision with an obstacle (see Fig. 1).Considering mentioned shortcomings in supervised learning, the interactive learning algorithms are widely used in robot navigation [2,9–11]. Reinforcement learning (RL) is a modern powerful interactive algorithm that can learn only by trial and error and delayed reward. RL capabilities, such as no need for desired outputs, training based on a scalar reinforcement signal, the possibility of online interactive training and high degree of exploration, encourage researchers to use RL in robot navigation problem. Due to large dimension of the discrete state-action pairs, continuous RL algorithms such as fuzzy RL (FRL) are usually employed to overcome the curse of dimensionality [2,12,13].Here, we focus on continuous RL algorithms based on critic-only architecture. Critic-only is a known architecture in RL employed in Fuzzy Sarsa Learning (FSL) [14] and Fuzzy Q-Learning (FQL) algorithms [15]. These algorithms present a solution for tuning the conclusion parts of rules in fuzzy inference systems.RL algorithms are often time consuming and slow in training procedure. Since the state space in robot navigation is large, this leads to long learning time. Moreover, in the beginning of training phase, the robot does not have any knowledge, so the number of failures (punishments) would be high. These failures may cause to damage the robot in practical experiments. Our motivation in this paper is to combine “supervised learning” with “fuzzy reinforcement learning”, so that we decrease learning time and the number of failures in RL as well as prevent weaknesses of supervised learning.Some researchers have addressed the related works in discrete space. In [16] a linear combination of supervised learning and Q-learning (one of the well known discrete RL methods) was proposed. In this approach, the selected actions by supervisor have a high chance for selection in each state during applying Q-learning. In [17], considering discrete action state space for robot navigation, a human guides the robot within the environment, when all states and actions are recorded in the defined Supervisor Table. Then, ɛ-greedy method is used for action selection in Q-learning, where the suggested actions from Supervisor Table have high chance to select.In [18] supervised reinforcement learning is used for autonomous humanoid robot docking. It uses Gaussian distributed states activation so inputs can be continuous, however the action space is discrete and there are only 4 actions.Above mentioned approaches have been proposed in the discrete (state or action) space, whereas we here focus on continuous state and action space. In [19] supervised adaptive dynamic programming was introduced for cruise control system. In there, actor- critic architecture was used for continuous RL and supervisor only guides RL process in three ways: (1) gives additional reward, (2) gives additional direct control signal to the agent; and (3) gives hints for exploration. In [2], the training data generated by the supervisor was used to determine the initial amounts of consequence parts of fuzzy rules in an actor-critic based FRL algorithm. This approach has two major weaknesses:1As mentioned earlier, destructive effect of inconsistency in data makes significant errors in adjustable amounts.Lack of suitable exploration in actor-critic architecture [18], since the final amounts are determined around the obtained amount by training data and it causes trapping in local extrema.In this paper, we propose a new method for combination of supervised learning and critic-only FRL algorithm. The critic-only architecture is selected since it has high potential for management of the balance between exploration and exploitation [20], which is a desired feature in robot navigation problem. The proposed idea is developed on FSL as a critic-only FRL algorithm. In our approach, instead of determining an action for each state, the value or worth of each candidate action is determined by training data. Then, for improving the performance, the final online tuning is done by FSL. The proposed combination makes the learning process faster, improves the learning quality, and reduces the number of failures (obstacle collisions). To the best of our knowledge, the proposed approach is the first work for combination of supervised learning and FRL based on critic-only architecture.The main contributions of the paper are as follows:1Proposing an approach for using training (supervised) data for initialization of the value (worth) of each candidate action in critic-only based FRL architecture.Designing controller using subsumption architecture [21] for robot navigation.Applying the proposed method in practice for navigation of a real robot (E-puck).The paper is structured as follows. In Section 2, FSL algorithm is described. Design of fuzzy controller structure is presented in Section 3. In Section 4, our idea for combination of FRL and supervised learning is proposed. Section 5 presents the experimental results of applying the proposed method in a real-world application. Finally, Section 6 contains conclusion of the paper.FSL and FQL are two critic-only FRL algorithms. For FQL method not only there is not any theorem or lemma for convergence but also there are some divergent examples. In contrast, the existence of stationary points was established for FSL in [12]. Moreover, the experimental results in [12] signify higher learning speed and action quality for FSL compared to FQL. Therefore, we develop our idea based on FSL.FSL is an extension of Sarsa learning (a well-known RL algorithm) [12] for continuous state and action spaces using a zero order Takagi–Sugeno (T–S) fuzzy system [6] as function approximator. In this section, we describe FSL briefly; readers can find the comprehensive information about FSL in [12].Sarsa method estimates the value of action an in state s denoted by Q(s, a) for the current policy according to the following update formula [22]:(1)Q(st,at)←Q(st,at)+αt[rt+1+γQ(st+1,at+1)−Q(st,at)]where α is the learning rate, γ is the discount factor, and rt+1 is the immediate reward received from the environment after applying action atin state st.Consider an n-input one-output zero order T–S fuzzy system with R rules of the following form [22]:Ri: If x1 is Li1 and…and xnis Lin, then(oi1withvaluewi1)or…or(oimwithvaluewim)where s=x1×⋯×xnis the vector of n-dimensional input state, Li=Li1×⋯×Linis the n-dimensional strictly convex and normal fuzzy set of the ith rule with a unique center, m is the number of possible discrete actions for each rule, oijandwijare the jth candidate action and the approximate value of the jth action in the ith rule, respectively. The goal of FSL is to adaptwijon-line to be used to obtain the best policy. The number of candidate actions and their amounts are fixed for the entire state space and must be determined by designer according to problem characteristics. Although the candidate actions oijin the consequence of the fuzzy rules are discrete, the final inferred action (i.e., the fuzzy system output) is a continuous action in the range ofa∈[mini,j(oij),maxi,j(oij)].The action selection probability of the ith candidate action in the ith rule in state stis computed based on the following modified Softmax policy [22]:(2)p(aij)=exp(μi(st)wij/T)∑k=1mexp(μi(st)wik/T)where μi(st) is the normalized firing strength of ith rule for state st, and T>0 is the temperature factor.Notice that to calculate the overall action, first an action is selected for each rule from among the candidate actions of that rule. Denoting the selected action in ith rule and its corresponding value byoii+andwii+, respectively, the system output (i.e., the overall continuous action) and its corresponding approximate Action Value Function (AVF) are computed as follows [22]:(3)at(st)=∑i=1Rμi(st)oii+(4)Qˆt(st,at)=∑i=1Rμi(st)wtii+Thus, the final continuous action is the weighted sum of the selected discrete actions of the rules.Applying action at, the environment goes to the next state st+1, and the agent receives reinforcement signal rt+1. The next final action at+1 is chosen based on the present weightwt. Then, the weight parameters of the ith rule are updated by [22]:(5)Δwt+1ij=αt+1×ΔQˆt(st,at)×μi(st)ifj=i+0otherwisewhereΔQ˜is the approximate action value temporal difference error determined by:(6)ΔQˆt(st,at)=rt+1+γQˆt(st+1,at+1)−Qˆt(st,at)γ and α are the discount factor and the learning rate, respectively. As a practical matter, learning rate is gradually decreased as a function of time [7]. The algorithm procedure of FSL is summarized below:1.Observe state st+1 and receive reinforcement signal rt+1.Select a suitable action of each rule using modified Softmax action selection Eq. (2).Compute final action at+1and the approximate AVFQˆt(st+1,at+1)using Eqs. (3) and (4), respectively.ComputeΔQˆand update w by Eqs. (6) and (5), respectively.Compute new approximate AVFΔQˆt+1(st+1,at+1)using Eq. (4).Apply the final action.t←t+1 and return to step 1.In this paper, we use an E-puck robot, a mini wheeled mobile robot, developed at the EPFL for education purposes [23–25]. The E-puck robot has already been used in a wide range of applications [26,27] .E-puck is cylindrical in shape with a diameter of 70mm and a weight of 150g (see Fig. 2). Eight infrared sensors placed around the robot measure closeness of obstacles in a 4cm range. Readers can find the mathematical model of a wheeled mobile robot in [28,29].The robot has two lateral wheels which can rotate in both directions. It is able to count the number of pulses generated by encoders which are installed on each wheel for estimating the distance traversed by each wheel. Also, depending on type of applications, we can install on auxiliary equipment such as camera, and hook [26,27].Our objective is to design a controller to drive an E-puck from a start point to a target point without hitting obstacles. The amounts of infrared sensors indicate the approximate distance between the robot and obstacles. The outputs of infrared sensors mounted in the right, the front, and the left side are used to generate the inputs of the controller as follows:(7)Sside(t)=max(snside,1,snside,2),side∈{Left,Front,Right}snside,1 and snside,2 denote the outputs of the first and second sensor in the mentioned side. The output range of infrared sensors in E-puck is between 0 and 3500 (0: when the sensor does not sense any obstacle in the area, and 3500: when the sensor is almost contacted to obstacles). The fourth controller's input is the robot head angle with respect to the goal (see Fig. 3), denoted by θ(t), and θ∈[−180°,180°]. These 4 inputs are normalized and used as inputs of the controller. The controller's output is the rotation angle of the robot in the range of [−45°,+45°]. The robot's velocity is considered almost fixed in each time step.For decreasing complexity of controller, we utilize the subsumption architecture as a behavior-based control architecture [21,30]. The subsumption is best-known reactive robotic architecture developed by Brooks [21]. His idea was decomposing a complex task to some parallel simple tasks or behaviors. This focus on simplicity leads to a design where each individual layer operating asynchronously without any central control. In general, the different layers are not completely independent. Levels are constructed from components referred to as ‘modules’; they consist of small asynchronous processors. Inputs and outputs of these modules can be inhibited or suppressed. The result is a robust and flexible robot control system.Here two modules can be considered for the robot in environment. The first when the robot is near the obstacles, in this situation, the robot should avoid obstacles meanwhile tries to move toward the goal's position. In another module, there is not any obstacle around the robot, so it is able to rotate, whereby its head is located in the front of the goal and then the robot moves straightforward to the goal. Using subsumption architecture, we define two behaviors for the above mentioned situations: “obstacle avoidance” and “goal seeking”.Fig. 4shows the designed structure of the behavior-based control system. Both modules (behaviors) receive sensor information; if the sensors do not recognize any obstacle around the robot, the module of “goal seeking” is active and its output suppresses the output of module of “obstacle avoidance”; otherwise “obstacle avoidance” module determines the angle of robot motion at each time-step for moving toward goal whereas the robot avoids obstacles. It has to be noticed that with respect to the defined tasks for modules and subsumption architecture, unlike some related works in [31–34]; in our designed structure no module is required to combine the output of modules (behaviors). Therefore, it decreases the computation cost and the system complexity.Generating the appropriate output for “obstacle avoidance” module is a complex task; so we use a fuzzy controller for this module. For fuzzy controller, a structure compatible with FSL (introduced in Section 2) is considered. A zero order T–S fuzzy controller with four inputs and one output is suggested. The inputs include robot's distances from the obstacles in left, right and front sides (obtained by Eq. (7)) and the robot head angle with respect to the target (θ).In this section, our proposed method that combines supervised learning and FSL is explained. In the first phase, a supervisor drives the robot in the environment and training data is collected. In contrast to the available methods which use training data for determining an action for each state; here, a new approach to determine the worth of each candidate action in each state is presented. In fact, training data is used to initialize wij(the value of each candidate action defined in Section 2) in each rule of fuzzy controller. For example, if the supervisor selects different actions in a state in separated visits; the worth of each selected action is proportional to the number of times that action has been selected.The output of fuzzy controller (see Eq. (3)) is the weighted sum of the selected actions of rules. So the worth of the controller's output is determined by the weighted sum of the worth of the selected actions of rules. Therefore, for determining the worth of discrete actions in rules, it is found out a set of possible candidate actions so that the computed final output would be almost close to the suggested output (by supervisor) and then, the worth of found candidate actions would be increased.Regarding the above description, we propose the following algorithm for determining the worth (wij) of candidate actions of rules. Consider pth sample of collected data as a pair of input–output (xp, yp) where xp is the input of the controller and yp is the suggested output by supervisor.For the weight (wij's) initialization, the following steps are done for each pair of data (xp, yp).1For xpas the input of fuzzy controller, firing strengths of fuzzy rules are computed and then four dominant rules (the rules with the highest firing strength (μ)) are selected. These rules are indexed by dl, l=1,2,3,4, where μd1>μd2>μd3>μd4.l=1.The yp is divided byμdl.The result of division is compared with each candidate actions of the rule (Odlj's). The closest action to this quotient is selected and indexed by kl.cdlkl(the amount of klth action's counter in dlth rule) is increased one unit where Cijrecords the number of selection of jth candidate action in ith rule.The product of the klth action of the dlth rule and firing strength of the rule (μdl) is computed and the result is subtracted from the actionyp:(y′p=yp−μdl×adlkl).If l≠4 then l=l+1 and ypis substituted byy′pand goes to step 3.Finally, when the above steps were performed for all data, the value (worth) of the jth action in ith rule is initialized as follows:The pseudo-code of the proposed method is given in Fig. 5.In the second phase, FSL algorithm is used for online fine-tuning of conclusion parts of the fuzzy controller where its wijs have been initialized by the proposed method. We call our proposed combination method: Supervised Fuzzy Sarsa Learning (SFSL). Fig. 6shows the block diagram of SFSL.The total procedure of SFSL can be explained in the following steps:1.Moving robot in the environment by supervisor, and gathering training data.Initialization of the value (worth) of the candidate actions in each rule by proposed method (see Fig. 5).Final tuning of the conclusion parts of the rules using FSL.

@&#CONCLUSIONS@&#
