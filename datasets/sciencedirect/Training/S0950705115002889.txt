@&#MAIN-TITLE@&#
Case-based maintenance: Structuring and incrementing the case base

@&#HIGHLIGHTS@&#
Case base maintenance.Prototyping.Instance reduction.Competence and performance optimization.Structuring and updating a case base.

@&#KEYPHRASES@&#
Case-based mining,Case-based maintenance,Prototyping,Instance reduction,Auto-increment,Competence,Performance,

@&#ABSTRACT@&#
To avoid performance degradation and maintain the quality of results obtained by the case-based reasoning (CBR) systems, maintenance becomes necessary, especially for those systems designed to operate over long periods and which must handle large numbers of cases. CBR systems cannot be preserved without scanning the case base. For this reason, the latter must undergo maintenance operations.The techniques of case base’s dimension optimization is the analog of instance reduction size methodology (in the machine learning community). This study links these techniques by presenting case-based maintenance in the framework of instance based reduction, and provides: first an overview of CBM studies, second, a novel method of structuring and updating the case base and finally an application of industrial case is presented.The structuring combines a categorization algorithm with a measure of competence CM based on competence and performance criteria. Since the case base must progress over time through the addition of new cases, an auto-increment algorithm is installed in order to dynamically ensure the structuring and the quality of a case base. The proposed method was evaluated through a case base from an industrial plant. In addition, an experimental study of the competence and the performance was undertaken on reference benchmarks. This study showed that the proposed method gives better results than the best methods currently found in the literature.

@&#INTRODUCTION@&#
Case-Based Reasoning (CBR) is an approach to problem solving and learning through the storing of solutions to similar problems such as cases in a memory called a case base [1]. The case is a body of knowledge representing an experience, defined to a description of the problem resolution event (new case). It is composed of two options: a description of the situation representing a problem and a solution used to remedy this situation (case=(Problem P, Solution S)). A case is placed in a case base and is called a source case which will be used to solve a new case called the target case. A general CBR cycle may be described as having five phases: elaborating, retrieving, reusing, revising and retaining. From a case to be solved, the elaboration phase builds a new problem in a target case by completing or altering the problem description from a possibly incomplete description. According to a similarity metric, cases similar to the target case are first found (the retrieving step) and then adapted by solution construction (reusing). Finally the solution is validated if necessary. The retaining phase consists of storing the new case once validated, provided storage is considered relevant.A CBR System contains four knowledge containers [34]: (1) the vocabulary knowledge describing the case and the problem domain, (2) the retrieval knowledge including the similarity measure and the indexing method, (3) the adaptation knowledge and (4) the case base itself.However, most CBR systems, designed to operate over long periods and that must handle large volumes of data and cases encounter problems in the retrieval and adaptation phases. The latter can be costly in terms of time [9]. To maintain the quality of system (i.e.) the speed of the retrieval process, maintenance of the case based reasoning system and particularly of the knowledge containers becomes necessary.There are two types of maintenance studies, the maintenance policies and the integration of maintenance with case-based reasoning processes [46]. Reinartz et al. [33] define two phase’s steps and tasks (Reviews and Restore) necessary to integrate maintenance into a CBR process. Heister and Wilke [21] propose an architecture integrating maintenance component in a CBR system that is composed of knowledge container and modeling tools.During maintenance, the contents of each of the four knowledge containers may be revised [45] in order to improve the performance objectives, e.g., the quality of the proposed solution [3].In past decades, a lot of studies have been done in retrieval knowledge maintenance promoting the performance of similarity measurement. Craw et al. [11] developed a method to optimize CBR retrieval. Bonzano et al. [8] combines introspective learning for feature weighting in CBR. Feature weights for a set of cases are adjusted dynamically during case retrieval by Zhang and Yang [57]. Zhang and Yang [56] maintain the important measures of different features of the case base, by integrating a learning network method for feature weighting within the CBR system. To strengthen the retrieval performance, many works are interested in the case base container and combine problems of feature selection and case organization [54,2,58].Salamó and López-Sánchez [49,50] and Salamó and Golobardes [47] focused on feature weighting and instance selection methods based on Rough Set Theory (RST).Those works mainly pay attention to the feature weighting and similarity measurement techniques partially or simultaneouslyRegarding the maintenance of adaptation knowledge and the case-base, Shiu et al. [51] transform the case-base to a set of small case bases each associated with adaptation rules generated by fuzzy decision tree [45].Iglezakis and Roth-Berghofer [22] suggest that a CBR system cannot be maintained without scanning the case-base. Maintenance activities are activated only via the case-base which plays a major role in the maintenance of CBR systems. This explains why the bulk of the work done in this field is primarily based on Case-Base Maintenance (CBM). In fact, the case-base is the knowledge container that is the most sensitive to changes in the CBR system and its consultation is essential in order to set maintenance operations in motion [23].CBR systems are large scale case bases. It is thus necessary to keep a compact and competent case base [49], to maintain the quality of the case base and the speed of the retrieval process [52]. New content cases must be added, existing cases may need to be revised, and out-of-date cases must be deleted; this is a classic example of the case-base maintenance problem [16].The k-Nearest Neighbors classifier frequently used in Case-based reasoning remains as (i) one of the most well-known algorithms for supervised non-parametric classification [14] in Pattern Recognition, data mining and Case based maintenance [10,16] (ii) as a benchmark for experimental studies in machine learning [42]. To improve the quality of the case-base, many researchers develop reduction of cases method in the CBR community [55,52,22], or instance reduction in machine learning community and particularly in instance based learning [5]. Reinartz [32] proposes a unifying view on Instance Selection, we base on this proposition in order to first draw the link between the work done in instance reduction and in CBM, review it, and then propose a new approach to reconstructing the case-base.According to Smiti and Elouedi [52] there are 2 types of CBM policies: partitioning of the case base which builds a case-base structure [54,9], and CBM optimization which uses an algorithm to delete or update the whole CB [19], (Smyth and McKenna, 1995), [37].In this study we present, an instance based learning case in the context of classification, where all descriptors are indicated and the solution represents a class.Different investigations in CBM are reviewed taking into account the framework of Reinartz [32]. Indeed, CBM policies following Reinartz’s framework can be presented as clustering (also known as case-base partitioning) and prototyping (also known as case-base optimization) steps. We propose a classification of the case-base optimization taking into account selection criteria and search procedure to complete the taxonomies of CBM algorithms presented in machine learning. Indeed, the most fundamental criteria that allow evaluating the case base quality in case-base optimization are the performance and competence.We are especially interested in the latter in order to achieve the objectives related to search-time problems as well as to reduce case-base size while preserving its quality. Case-base quality depends on a number of criteria that are discussed in the same section. Section 2 ends with a summary of the state-of-the-art study. Several observations and conclusions are given allowing us to propose our CBM method based on the principle that to maintain a case-base, it is necessary to evaluate and optimize its quality by structuring it and auto-incrementing it with new cases while maintaining its structure.Section 3 describes a novel method based on the structuring of a case base and its auto-increment. Indeed, after having structured the case base, it will be incremented by new cases. However, this integration must be made under specific conditions in order to ensure system quality. Consequently, several questions arise. Which case is to be retained among those solved? How is the case to be indexed? How can it be introduced with respect for the structure of the expert base, to allow a delicate updating of expertise? What is its contribution to the improvement of expert-base quality? An auto-increment algorithm will therefore be proposed in the same section. Section 4 addresses a comparative study of existing methods conducted on particularly significant benchmarks according to competence and performance criteria. The suggested method is applied in Section 5 via a supervised industrial system of pallet transfer (SISTRE).Maintenance in CBR involves different operations: outdated, redundant or inconsistent cases may be deleted; groups of cases may be merged to eliminate redundancy and improve reasoning power; cases may be re-described to repair incoherencies [37]. Furthermore, case-base maintenance implements policies for revising the organization or contents (representation, domain content, accounting information, or implementation) of the case base in order to facilitate future reasoning [24]. Note that this definition considers the information defining an indexing scheme to be an intrinsic organizational component of the case base itself. Case-base maintenance involves revision of indexing information, links between cases, and/or other organizational structures and their implementation [37,44].In this context, maintenance is based on applying update policies for case-base representation and is implicated in their reorganization in order to facilitate future reasoning in response to sets of performance objectives. The state-of-the-art of case-base maintenance relies on the various methods used. Case-base maintenance aims to reduce case search time while improving the performance of the system. Time is reduced by minimizing case-base size or by the partitioning it into several smaller parts.Hereafter, we present the different points of comparison between these methods and develop the crucial points. There are several different ways to categorize existing case-base maintenance methods. In this study we take an interest in case size reduction, hence we can draw an analogy with instance-reduction techniques.Dai and Hsu [13] split up instance reduction algorithms into two types: clustering based learning algorithms and instance based learning algorithms. However, Reinartz [32] has gathered the methods specific to the tasks of instance selection in three steps: Sampling, Clustering and Prototyping. These steps are viewed by Liu and Motoda [26] as three basic generic components of instance reduction and can be instantiated to individual algorithms.Samplingis a procedure in which a sample Si is drawn through a random process by which each Si receives its appropriate probability πi of being selected [26]. A general advantage of sampling is the efficiency of most of its techniques in terms of execution time. However, the existing techniques are not limited only to this method as they are usually combined with others.Clusteringis one approach to finding regularities from unlabeled data [26]. It is the classification of objects into different groups or, more precisely, the partitioning of a data set into subsets (clusters) so that the data in each subset (ideally) share some common trait, often proximity, according to some defined distance measure. Our method takes this aspect into account.Prototypingis the process of quickly putting together a working model (a prototype) in order to test various aspects of a design, illustrate ideas or features and gather early user feedback. Prototyping is often treated as an integral part of the system design process, where it is believed to reduce project risk and cost. Indeed, the prototypes are more condensed descriptions of sets of characteristics.Prototyping supposes that a simple characteristic can represent the information of an entire subset of characteristics [29]. The basic idea is, once clusters have been obtained within a larger space, it is then necessary to find among them the most representative prototype of the set. Prototyping thus allows selecting of the relevant cases or the subset of cases which represents the whole set according to a given criterion. This allows removing some cases which results in reducing the case base to a smaller size having the same characteristics as the original.The case-base maintenance approach can be divided into two policies, oneconcerning case-base partitioningand the other one concerningoptimization. Policies found in clustering and prototyping steps integrated each one sampling steps. The partitioning policy consists of dividing the case base into several search spaces. This enables selection in an increasing manner those attributes which are rich in information and which can cover the structure of the case base [54]. In contrast, the optimization policy consists of deleting less relevant cases.In the partitioning of case base, Clustering and feature selection techniques have been successfully applied to CB maintenance [2,54,30].Zhu et al. [58] propose a hybrid CBR system composed of neighborhood rough set method as in feature selection technique and by cluster analysis approach, a growing hierarchical self-organizing map (GHSOM), to organize a large case base in a hybrid CBR system.Smiti and Elouedi [52] combine DBSCAN (Density Based Spatial Clustering of Application with Noise) and Gaussian-Means algorithms to cluster the case base into small CB’s, easier to maintain each one individually. To reduce the size of each partition and preserve maximum competence, outliers and internal cases detection methods are applied. These proposed maintenance policies are named Weighting, Clustering, Outliers and Internal cases Detection based on clustering technique.Salamó and López-Sánchez [49] develop three strategies for feature selection based on rough sets for dimensionality reduction in Case-Based Reasoning classifiers, and propose [50] an adaptive case-based reasoning model that develops the case base during the reasoning cycle by adding and removing cases. Salamó and Golobardes [48] introduce a dynamic case base maintenance (DCBM) model that updates the case base based on Reinforcement learning (solving process). This approach integrates the solving algorithm in a case base maintenance process.To improve performance of CBR systems, Zhu et al. [58] presents an integrated reduction technique and cluster analysis approach to manipulate the problems of feature selection and case organization in large CBR system.Hamidzadeh et al. [18] propose an instance reduction method based on hyper-rectangle clustering. A hyper-rectangle is defined by its min and max points. The clustering algorithm selects subset of instances near or within the boundaries of classes. The size of training set is reduced, improving generalization accuracy.Ferrandiz and Boullé [16] propose a new instance selection method, introducing new classification scheme for instance selection named the Voronoi-Based Relabeling scheme, (VBR scheme) for short, which is a relabeling method relying on Voronoi partitions. The new optimization algorithm finds the best set of instance, by exploiting additive criterion. The kept instances are retained as training data for different classifiers and not only for the kNN algorithm.Searching for efficient approaches in instance reduction is still an active field of research [18,17,43,42]. A most common technique to reduce the size of the training set, to decrease computational cost and sensitiveness to noise is the use of Prototype Selection method PS [10]. The method proposed by the authors combines the classification accuracy of retaining all the training set and Prototyping Selection methods provided in kNN classification. This PS method first reduces the training set by using an algorithm and performs the classification of the new element. Prototype Selection PS allow a faster Nearest Neighbor classification by keeping only the most profitable prototypes of the training set.Prototypes can be defined as generalized numerical examples (Salzberg, 1991) or tuples composed by both numerical and categorical values like hyperrectangle.Calvo-Zaragoza et al. [10] list representative sets of PS algorithms, all based on Nearest Neighbor NN published in the literature, such as Condensing Nearest Neighbor (CNN), and Editing Nearest Neighbor (ED). Those algorithms dedicated to the optimization policies of CBM, are classified by different authors [15,17,53].Some authors have proposed different taxonomies of CBM algorithms [43,17]. Garcia et al. [17] describe the different properties issued from variable selection and adapted to the PS methods, the properties specific to the prototyping selection methods and at the end the properties that can influence the results of an instance selection algorithm in combination with a given classifier.Fazzolari et al. [15] according to Garcia et al. [17] classified the Prototype Selection PS method and the Training Set Selection TSS methods using common proprieties. We retain the most relevant properties for the methods of PS: evaluation of search, direction of search propriety, type of selection.–The Evaluation of search: according to the strategy used to add or remove instances in the subset S. Garcia et al. [17] define (i) a filter strategy “when the KNN rule is used for partial data to determine the criteria of adding or removing and no leave – one-out validation scheme is used to obtain a good estimation of generalization accuracy”. (ii) And the wrapper strategy “when the kNN rule is used for the complete training set with the leave-one-out validation scheme.The Direction of search propriety is defined as well in the framework of feature selection algorithm in data mining community by [26,51,5] and refined by adding Three items (Batch search, mixed search and fixed search a subfamily of mixed search (i.e. item that we do not retain).Search Direction (Starting point of the best subset case search): there are a variety of directions in which search can proceed.•Forward Selection (FS) or incremental search consists of building a reduced case base by successive addition, starting from an initially empty case base, according to a criterion to be maximized. There are two categories of methods: one maximizing the competence criterion and the other the performance criterionBackward Elimination (BE) or decremental search: the case-base will be screened entirely when its size reaches a certain threshold, usually followed by the process of case deletionBatch search involves deciding if each instance meets the removal criteria before removing any of them.Mixed search begins with preselected subset and can iteratively add or remove any instance which meets the specific criterion.The type of search carried out by the PS algorithms that seek to retain set of point with respect to the decision boundaries (border instances, central instances) can be of 3 types: condensation, edition and hybrid methods.•Condensation methodstry to obtain a consistent subset by removing unnecessary instances that will not affect the classification accuracy on the training set.Edition methodsaim to remove noisy instances, allowing the classifier to increase its accuracy.Hybrid methodssearch for a subset in which both noisy and unnecessary instances are concurrently eliminated.The taxonomy of Garcia et al. [17] takes into account the type of selection, the search direction and the evaluation search. The Fig. 1illustrates this classification.Leyva et al. [42] propose a classification of the PS algorithm taking into account the type of selection and defines (i) the condensation method like the methods of the family of Condensed Nearest Neighbor (CNN) Selective Nearest Neighbor (SNN) Minimal Consistent Set (MCS) and Fast Nearest Neighbor Condensation (FCNN). These methods are very noise sensitive. (ii) The edition method follows the family of Edited Nearest Neighbor (ENN); Repeated ENN (RENN).Theses methods reject the instances that affect in classification with their neighbor-hoods like. They are characterized by being good noise filters, and achieve little reductions in the number of instances [42] (iii) The hybrid methods are a combination of both edition and condensation strategies like Instance Based Learning3 (IB3), decremental Reduction Optimisation Procedure DROP3, and Iterative case filtering ICF. The IB3’s algorithm use previous selected instances to build incrementally selection to classify. It retains instances misclassified, and selection the instances having a best accuracy of the member of the class and eliminate the instance with the worst accuracy.Verbiest et al. [53] classified the IS method based on the type of selection and the evaluation of search. The authors propose a FRPS Fuzzy Rough Prototype selection which is a wrapper-oriented edition method. The instances are ordered according to a measure based on fuzzy rough set theory. A wrapper approach is used to prune instances and a kNN algorithm to classify the instances.The main principal of the case-based optimization policies is to build a maintenance model that represents the implicit information of the case. We propose to classify the CBM algorithms based on two additional proprieties that are defined in the feature selection framework: the search procedure and the selection criteria:–Search procedure (subset evaluation function): there are three search procedures: complete, heuristic and random generations. Each search procedure corresponds to the nature of the case base and to the chosen method for case selection.Selection criteria: represent the evaluation criteria of the case base. The various existing methods can simultaneously use one or more criteria.Stop criteria: generally, case-base maintenance methods have a stopping criterion of selection or removal of case-base cases. This criterion plays a threshold role and it is important to determine it well.First we describe these criteria adapted to the case-base.An effective case base can answer as many queries as possible, efficiently and correctly. Several criteria allowing the evaluation of case-base quality have been proposed in the literature. We can quote inconsistency, redundancy and abstractness, introduced by Racine and Yang [31], and relevancy, introduced in [5].However, the important criteria contributing to the evaluation of a case base are: competence and performance. These criteria can be based on coverage and reachability notions.Performance:Case-base performance is measured by the answer time necessary to compute a solution for case targets [35]. This measure is bound directly to adaptation and result costs.Competence:Case-base competence is measured by the range of problems that can be satisfactorily solved [35]. Case-base competence thus represents the case coverage which it contains. Several strategies which make it possible to measure the case coverage have been proposed in the literature [39,40,41].Indeed, competence is based on the two notions of “coverage” and “reachability”.Given a case base CB be {c1,…,cn} and a set of target cases “T” {t1,…,tr}. A case “c” is composed of two parts: problem and solution.Case c={Ps, Ss}Target t={Pt, ?}Coverage of a case c: is the set of target problems that it can be used solved by its retrieval and adaptation.Reachability of a case: is the set of cases that can be used to solve the target problem.(1)Coverage(c∈CB)={t∈CB:Solves(c,t)}Reachability(c∈CB)={t∈CB:Solves(t,c)}Thus, coverage and reachability is formalized by Smyth and Keane [35] for c∈CBCoverage (c)= {Adaptable (c, t)}(1bis)Reachable (c)= {Adaptable (t, c)}In the case of instance based learning where there is not the adaptation phase, this definition becomeCoverage (c)= {Similar threshold (c, t)}(1ters)Reachable (c)= {Similar threshold (t, c)}Good competence of the case base CB means that its coverage is high and that its reachability is low.Moreover, we note that the definition of coverage and reachability, introduced by Smyth and Keane [35], takes into account both the problem and solution parts of case.Smyth and Keane (1995) assume that the case-base itself is a sample of the underlying distribution of target problems.On line, the CBR objective is to solve a target case and find the solution St to the problem Pt. We do not have the solution target={Pt, ?} unlike at the offline step (structuration of the case-base) where the set of target case is a distribution of case-base.Unlike the offline step (the case-base structuring) where the set of target cases is a distribution of the case-base, the target solution at this step is not known.During the problem solving step, a potential solution is assigned to each target. It is named target∗={Pt, St}. Once the target∗ is analyzed, it is integrated in the case-base where it either becomes a case or it is deleted.As Dai and Hsu [13], we address the strategies of sample selection developed within the case-based optimization policy, namely: forward selection (incremental search) and backward elimination (decremental search).In the first category of FS, Smyth and McKenna present a method that uses an explicit case competence model based on notions of coverage and reachability. Their relative coverage (RC) metric provides a precise measurement of competence contributions for individual cases.(2)RC(c)=∑c′∈CoverageSet(c)1ReachabilitySet(c′)With definition of coverage and reachability (1)CoverageSet (c∈CB)= {c′∈CB: Solves (c, c′)}ReachabilitySet (c∈CB)= {c′∈CB: Solves (c′, c)}The RC metric, associated with the condensed nearest-neighbor (CNN) algorithm, allows successive retention of only those cases which have not yet been solved by a case that has already been retained, in order to obtain a new reduced case base [39]. Consequently, the selected cases make an important contribution concerning case-base recovery.Yang and Zhu [55] describe a case-addition algorithm for case-base compaction that uses a problem-neighborhood model of case coverage. The cases based on benefit/usefulness are successively added to the case set so far retained.The interesting part of these methods is the use of models and metrics that make it possible to guide the case-base size reduction by preserving good competence. The cases are ranked by the metrics in order to add the most interesting cases to the reduced case base. To obtain a reduced case base, computational time becomes greater. Indeed, for each added case it is necessary to re-examine the entire original case base.In the second category of methods maximizing the performance criterion, Leake and Wilson developed a Relative Performance (RP) metric aimed at assessing the contribution of a case to the adaptation performance of the system [25]. The RP value for a case rejects how its contribution to adaptation performance compares to other cases. Likewise, another metric was developed concerning a Performance Benefit (PB) metric estimating the actual numerical savings that the addition of each case provides. Yet, in comparison to the RP-CNN and PB-CNN methods, RC-CNN produces a better rate of case-base size reduction, whereas the former methods give a better result in adaptation cost.The FS strategy includes only one criterion. However, the majority of these strategies use measures of performance or competence in combination with the CNN algorithm. Indeed, these strategies classify the cases in the initial case-base according to the used measure value used (RC, RP or PB) and they then begin the process of a case-addition within a case base that is initially empty according to the CNN algorithm. Both RP-CNN and PB-CNN strategies are very close in terms of results concerning the performance of the case-base. These last two strategies only concern performance whereas the RC-CNN strategy and the Yang and Zhu’s algorithm [55] are only concerned with case-base competence. In fact, they aim to reduce case-base size while maintaining its competence.In summary, the FS strategies treat case-base quality by exploiting only one criterion that is optimized according to reduction of case-base size. The majority of these strategies (RC, RP and PB) combine the CNN algorithm with either a competence or performance measure.The FS strategy results depend on the first case selected, which is randomly selected in CNN, or is case-dependent in RC, RP or PB measure. If the first case is badly chosen (due to a skew of measure), the reduced case base is affected and is of low quality. Consequently, in this study we focus on the backwards elimination (BE) methods, which, though costly in computing time, provide an overview of the case base.From a given case base, this strategy emphasizes cases according to its criteria so as to reduce their number and thus shrink the case base to a specific number of cases. Evaluation criteria such as competence, redundancy and inconsistency, have been used in different methods, which will be explained below.In the BE method, the case-base will be screened entirely when its size reaches a certain threshold, usually followed by the process of case deletion. There are two kinds of BE methods: (1) suppression methods for using case-base screening and, (2) methods from the case categorization.The first BE method, Random Deletion (RD) is a very simple baseline method, when a case is randomly selected and deleted once the case-base size exceeds a predefined limit [27]. Another method, Ironically (Ir), is slightly more complex. It calculates the frequency with which each case is retrieved and deletes those which are not frequently accessed in the case base [28].BE methods have thus evolved and take into account a quality criterion in their study. Among these methods we find Utility Deletion (UD) which is based on Minton’s utility metric and chooses a case item for deletion based on an estimate of its performance benefits [36].A utility metric is defined by Minton as:(3)metric=(ApplicFreq×AverageSavings)-MatchCostwhich takes into account the cost of maintaining case and the average savings multiplied by application frequency.The utility problem manifests itself as a trade-off between the solution quality associated with large case base and the efficiency problem of working with a large case base. Furthermore, solution quality increases with case-base size [38].Lastly, there exist other methods such as the Deletion Based on Redundancy and Inconsistency method which are endowed with two modules of detection, one with redundancy and the other with inconsistency. After a series of tests using these two modules and concerning each case found in the base, the specific cases may, after user approval, be either removed or kept [31]. Other methods take into account other notions, such as density in Deletion Based on Case-Base Size and Density.The first of these, proposed by Smyth and Keane, studies case-base size and the density distribution of the cases contained in the base. It attempts to respect the homogeneity of case-density size [38].RNN (Gates, 1972) the reduced nearest neighbor algorithm keeps removing instances until no more misclassification is generated by the remaining instances. It is hence better than CNN in terms of instance reduction percentage and classification speed. An extension of this algorithm is the Generalized Condensed Nearest Neighbors (GCNN), it is the same as CNN, but assigns instances which satisfy an absorption criterion (calculated in terms of the nearest neighbors and enemies (the nearest instances of other classes)). An instance that is absorbed using the absorption criterion could better reduce the size compared to CNN algorithm.Wilson and Martinez (1997) introduced a notion of association where an instance is an associate of another instance if it is a one of its k nearest neighbors. This notion is applied in a family of five decremental reduction optimization procedure algorithms called DROP1-DROP5. Drop3 uses ENN to remove the noisy instance before executing drop1 and presents a best performance (Wilson and Martinez, 2000), [18].Dai and Hsu [13] propose three Reverse Nearest Neighbor Reduction RNNR algorithm versions based on Reverse Nearest Neighbor (RNN) and selecting the better RNR-L1 that achieve higher accuracy than ICF and DROP3.The ICF method, whose type selection is hybrid was introduced by Brighton and Mellish [6], uses an algorithm which iteratively removes a case whose absence produces better results than if it were retained. This algorithm also uses coverage and reachability as selection criteria. It repeatedly uses a deletion rule that removes cases whose reachability size is greater than that of the coverage until the conditions of the rule are no longer satisfied.The majority of these methods do not give satisfactory results concerning the optimization of case-base size according to the studied criterion. Moreover, there are some methods which are difficult to implement, and those which are easy to implement do not produce convincing results. We are interested particularly in the ICF method because it produces better results than the others [6].ICF outperforms alternative local methods such as ENN, CNN, IB3 and DROP3. It is thus the best candidate for representing local methods[16].Leyva et al. [42] argue that the ICF method presents some drawbacks but “it is an interesting method that opened the promising field of using LSs in Instance Selection, whose study may be a source of inspiration for new proposals in this field.” The authors propose three instance selection IS methods based on local sets, which follow different and complementary strategies. Indeed the authors resolve the problem of instance reduction (maximizing accuracy and reduction size of case base), as a bi-objective problem and use a local set concept to develop three IS methods. These methods differ in the selection strategy and presents complementary results (the first maximizes the accuracy, the second one targets the reduction of size and the third makes a compromise between these objectives.The second kind of BE is a categorization method used for modeling the case-base competence as proposed by Smyth and Keane (1998, 1999, 2001a, 2001b). The cases included in a case-base are categorized according to their competence. The key concepts in categorizing cases are the coverage and reachability previously discussed.The methods developed in this area generate a set of target cases so that case base is categorized. Two basic hypotheses underlie these models: in the first the case base corresponds to a sample target or to potential cases, and in the second the space problem is regular, which means that similar problems have similar solutions.Footprint deletionis the strategy used to remove irrelevant cases, thereby guiding the case base toward an optimal configuration (in the sense that competence is maximized while size is minimized).The case categories described above provide a means of ordering cases for deletion in terms of their competence contributions. Auxiliary cases are selected for deletion before support cases, which are chosen before spanning and pivotal cases. The optimal case-base can be constructed from all the pivotal cases plus one case from each support group. This strategy is not designed to eliminate the need for performance-based methods such as utility deletion [35].Footprint Utility deletionis the hybrid strategy between footprint deletion and utility deletion. First, the footprint method is used to select candidates for deletion. If there is only one such candidate then it is deleted. However, if there are numerous candidates, rather than selecting the one with the least coverage or largest reachability set, the candidate with the lowest utility is chosen [35].These two previous methods use a specific case categorization. Four categories of cases are considered:Pivotal Cases:a case is pivotal if it is reachable by no other case but itself. Its deletion directly reduces the competence of a system.Pivot(c)iff Reachable(c)-{c}=∅Spanning Cases: spanningcases do not directly affect competence. They are so named because their spaces of coverage link (or span) regions of the problem space that are independently covered by other cases.Spanning(c)iff¬Pivot(c)Λcoverage(c)∩Ut∈Reachable(c-{c})Coverage(c)≠∅The spanning is intra-class if t and c have as solution the same class. Otherwise it is inter-class.Support Cases:support cases are a special class of spanning cases and, again, do not affect competence directly. They exist in groups, each support providing coverage similar to the others in a group. While the deletion of any one case from a support group does not reduce competence, the removal of the group as a whole is analogous to deleting a pivot, and does reduce competence.Support(c)iff∃t∈Reachable(c)-{c}:Coverage(t)⊂Coverage(c)Auxiliary Cases:a case is auxiliary if the coverage it provides is subsumed by the coverage of one of its reachable cases. Auxiliary cases do not affect competence at all and their deletion only reduces the efficiency of the system.Support(c)iff∃t∈Reachable(c)-{c}:Coverage(c)⊄Coverage(t)The methods of BE strategy make it possible to optimize only one quality criterion of the case-base at the expense of other criteria, the competence for RD and the performance for UD. Although FUD takes competence and performance criteria into account in its method, no evaluation of the case-base has been made concerning its performance. In contrast, the development of redundancy and inconsistency criteria (DRI) provides good competence but bad performance. Consequently, we will jointly study a criterion for case selection that takes into accounts both performance and competence to ensure a higher quality case base.It is noteworthy that the working sample relating to target cases is exploited to evaluate case-base quality according to the criterion under study. This sample of cases represents the entire case base for the methods RD, Ir, UD, DRI and DSD, whereas in the other two methods, FD and FUD, the sample is only represented by a subset randomly selected from the case base.In the first case, the study is of greater interest since the entire case base is scanned and therefore all possibilities are represented. However, computation time is greater than in the second case. We can also specify that all methods make use of a heuristic search procedure except RD which consists of a random procedure as its name implies.The advantage of these methods is the case categorization used for their deletions. Clear and effective, they also produce good results in execution speed once implemented. Their disadvantage is that their setup is costly in computing time. Moreover, Yang and Zhu [55] have shown that these categorization methods do not guarantee the preservation of competence. Cases having a strong coverage can be removed.The contribution of this paper is in the area of backward elimination strategy. Indeed, we propose a methodology for case-base optimization by deleting the lowest quality cases in the case base. The case quality is being assessed on the basis of competence and performance criteria.The proposed case-base maintenance method is comprised of two stages (see Fig. 2):This stage is composed of three steps: (i) evaluation of the case base, (ii) categorization of different cases (clustering) and (iii) prototyping.The step of assessing case-base quality is undertaken by the joint use of the two criteria of competence and performance. Competence is characterized by the calculation of case-base coverage and reachability. Performance is characterized by the accuracy of the case base and the number of cases. This step is for the preparation of case categorization, an evolution of the competence model of Smyth and McKenna [38], with five case categories defined: pivotal, support, auxiliary, inter-class spanning and intra-class spanning. The next step, “prototyping”, is for the selection of the most representative cases, which, in turn, depends on case categories.This stage makes it possible to incrementally add to the case base new cases that have just been solved by the CBR cycle.From the average recovery cases of the restructured case base, the coverage and reachability of the new case is calculated, taking into account the problem and solution parts of the case in comparison to the case base. This requires defining of measures that take into consideration the coverage and reachability of the two case parts of problem and solution. We will rely on these measures in order to decide to increment or not the new cases in the case base. It would thus be possible to respect the structure of the previously set up case base. If the solution is reachable, we focus on the problem part of the new case in order to account for the structure of cases previously defined.The adopted approach relies on the following points.The proposed approach consists of gathering into different categories cases considered to be similar according to their coverage and reachability. A prototype is then selected for some categories by choosing the case which represents its group as well as possible.From a complete case base, cases are removed to reduce the case base according to a set of criteria. This suppression is carried out using a categorization algorithm as well as a competence measure.For each case in the case base, we estimate its coverage and reachability from a subset of cases randomly drawn, that will represent the problem space, i.e. the cases which will be potentially reachable by the source cases.The objective of the proposed method is to preserve the competence of the case-base while optimizing its size and maintaining good performance. Competence is quantified by a measure (CM) that keeps with the two basic concepts: coverage and reachability.To achieve high competence in the case base, it is necessary to maximize the coverage of cases and minimize their reachability. As for performance, it depends on the accuracy of the case-base cases (good classification) according to their number (storage).(4)CM(c)=Vc(c)Vr(c)Vc(c)=Cardinality of the c case covering set.Vr(c)=Cardinality of the c case reachability set.A value is determined from the CM measure making it possible to determine when to stop deletion. This measure reflects the global competence value of the complete set of the reduced case base which is equal to the sum of the CMs of each case, divided by the number of cases in the case base. Thus, when the global competence value is equal to “1” and the coverage of each case in the case base is alone in its own class, then deletion stops.(5)GlobalCompetence(CB)=∑i=1nCM(ci)nwhere n: is the number of cases in the case base.The proposed method is composed of two steps consisting, on the one hand, of a way to structure the case base, and, on the other hand, of the means to dynamically ensure its auto-increment.As mentioned above, structuring is achieved by backward elimination and is comprised of: evaluation, categorization and prototyping.

@&#CONCLUSIONS@&#
