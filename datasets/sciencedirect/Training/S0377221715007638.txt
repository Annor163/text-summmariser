@&#MAIN-TITLE@&#
Default probability estimation via pair copula constructions

@&#HIGHLIGHTS@&#
We provide a methodology to accurately evaluate firm default probability.Multivariate contingent claim model on balance sheet data for firm asset value.Use of Bayesian parametric mixture models for vine-marginal modelling.Asset and liability dependence structure is modelled via pair copula constructions.Our approach allows us to correctly estimate the PD of defaulted as well as of sound firms.

@&#KEYPHRASES@&#
Default probability,Markov chain Monte Carlo,Multivariate contingent claim,Pair copula,Vines,

@&#ABSTRACT@&#
In this paper we present a novel approach for firm default probability estimation. The methodology is based on multivariate contingent claim analysis and pair copula constructions. For each considered firm, balance sheet data are used to assess the asset value, and to compute its default probability. The asset pricing function is expressed via a pair copula construction, and it is approximated via Monte Carlo simulations. The methodology is illustrated through an application to the analysis of both operative and defaulted firms.

@&#INTRODUCTION@&#
Default risk is defined as the risk of a loss when a debtor (in our case a firm) does not fulfil its commitments in a financial contract, and a default event takes place. The probability of default (PD) is the probability that a default happens.Following the growing financial uncertainty, there has been intensive research by institutions, regulators and academics to develop models for firm evaluation and PD estimation. The existing methodologies differ on the available information and data used for assessing the firm value. They can be broadly classified in models based on market data and on accounting data.Within the market data based models, the most popular are structural models; see Merton (1970, 1974, 1977) and their extensions; for recent and complete reviews, see e.g. Ji (2010); Laajimi (2012); or Sundaresan (2013). The asset value is considered to be exogenous and it is treated as the underlying asset in a contingent claim framework. A common assumption is that the asset value follows a geometric Brownian motion, and its drift and volatility coefficients do not depend on the capital structure of the firm. Black and Scholes’ formula is applied to compute the asset price, and consequently the PD can be easily estimated, see Black and Scholes (1973).The second class of models use accounting data and financial ratios to evaluate the firm value, and its PD. They origin from the works of Beaver (1968) and Altman (1968) who developed univariate and multivariate models, based on linear discriminant analysis, to predict the default of specific firms by using a set of financial ratios. Another commonly used default prediction model is based on logistic regression, as proposed by Ohlson (1980).The previous models have been analysed both in a classical and in a Bayesian framework. For some recent works in the classical framework, see e.g. Bharath and Shumway (2008); De Giuli, Fantazzini, and Maggi (2008); Kreinin and Nagi (2008); Su and Huang (2010); Altman, Fargher, and Kalotay (2011); Bo, Li, Wang, and Yang (2013); Bo, Tang, Wang, and Yang (2011); Bhimani, Gulamhussen, and da Rocha Lopes (2014); Leow and Crook (2015); Tobback, Martens, Van Gestel, and Baesens (2014); and references therein. For the Bayesian analysis see e.g. Kiefer (2009, 2010, 2011); Park, Sirakaya, and Kim (2010); Tasche (2011); Kazemi and Mosleh (2012); Orth (2013); Liu, Hua, and Lim (2015); and references therein.A popular and efficient tool in risk management is the copula function, introduced by Sklar (1959). The advantage of copulas is the ability to obtain the joint multivariate distribution embedding the variable’s dependence structure. Unfortunately, while there is a wide range of possible alternative copula functions for the bivariate case, in the multivariate setting the use of families different from Normal and Student’s t is rather scarce, due to computational and theoretical limitations. For this reason Joe (1996) introduced Pair Copula Constructions (PCCs) to represent complex structures of dependence among multivariate data. PCCs constitute a flexible and very appealing tool for financial analysis, see e.g. Vaz de Melo Mendes, Mendes Semeraro, and Camara Leal (2010); Min and Czado (2010); Allen, Ashraf, McAleer, Powell, and Singh (2013); Dißmann, Brechmann, Czado, and Kurowicka (2013); Bernard and Czado (2013); and reference therein. A collection of potentially different bivariate copulas is used to construct the joint distribution of interest via PCCs, allowing to represent different types and strengths of dependence in an easy way.In this paper we propose a novel approach for PD estimation, that combines features of both structural and accounting based models. We consider a contingent claim model based on balance sheet data, where the dynamic of the equity is described via a PCC and calculated using Monte Carlo simulations. We apply Bayesian parametric mixture models in the new context of vine marginal modelling, for balance sheet data of defaulted and non-defaulted firms. The PD is obtained in a fairly straightforward way from the equity distribution.The outline of the paper is the following. In Section 2 we briefly present copula models and PCCs. In Section 3 we introduce a novel balance sheet multivariate contingent claim model for PD estimation based on PCCs. In Section 4 the model estimation methodology is presented. Section 5 describes the application of the proposed methodology to the PD estimation of defaulted and operative companies. Finally, concluding remarks are given in Section 6.Copulas are very popular and appealing statistical tools, that allow us to describe complex multivariate patterns of dependence binding together the marginal distributions. They are applicable to a wide variety of fields, such as economics, finance and marketing; for a review see e.g. Jaworski (2010).A copula is a multivariate distribution function with uniform marginals on the interval [0, 1]. Once applied to the univariate marginals, it returns the multivariate joint distribution, enclosing all the information about the dependence structure of the variables. Thus, the use of copulas allows us to split the distribution of a random vector into its individual marginal components, and the dependence structure is modelled through the copula function without losing information; for more details see e.g. Joe (1997) and Nelsen (1999).Sklar’s theorem is the most important result in copula theory. It states that, given a vector of random variablesX=(X1,…,Xd),with d-dimensional joint cumulative distribution functionF(x1,…,xd)and marginal cumulative distributions Fm(xm) withm=1,…,d,there exist a d-dimensional copula C such that(1)F(x1,…,xd)=C(F1(x1),…,Fd(xd);θ),whereθdenotes the set of parameters of the copula. To simplify the notation, in the remainder of the paper, we setC(F1(x1),…,Fd(xd))=C(F1(x1),…,Fd(xd);θ).For an absolutely continuous joint distribution F with strictly increasing continuous marginal distribution functions, the d-dimensional copula is uniquely defined. Conversely, according to Nelsen’s corollary, the inversion method allows us to express the copula in the following wayC(u1,…,ud)=F(F1−1(u1),…,Fd−1(ud)),whereF1−1,…,Fd−1are the generalised inverse functions of the marginals.The joint density function isf(x1,…,xd)=c(F1(x1),…,Fd(xd))·f1(x1)⋯fd(xd),wherec(F1(x1),…,Fd(xd))is the d-variate copula density, provided its existence.In this paper, we fit the data into a given model following a parametric approach. Nonparametric methods for copula density estimation also exist, see e.g. Sancetta and Satchell (2004); Shen, Zhu, and Song (2008) and Kauermann, Schellhase, and Ruppert (2013).The existing literature on copulas mainly focuses on the bivariate case. In the multivariate case, Normal and Student’s t copula are the most popular, while the use of other multidimensional copulas is rather limited, due to the complexity of their construction, see e.g. Aas and Berg (2009). However, Normal and Student’s t copula are often not flexible enough to represent the dependence structure of the data. Hence, multivariate extensions of Archimedean copulas were proposed in the form of partially nested Archimedean copulas by Joe (1997) and Whelan (2004) hierarchical Archimedean copulas by Savu and Trede (2006) and multiplicative Archimedean copulas by Morillas (2005) and Liebscher (2006). Nevertheless, these multivariate extensions imply additional restrictions on the parameters that limit their flexibility. A possible solution to this problem is provided by PCCs, that will be described in the following section.We now briefly introduce PCCs, the related notation and terminology; for more details see e.g. Czado (2010). PCCs were originally proposed by Joe (1996), and later discussed in detail by Bedford and Cooke (2001, 2002); Kurowicka and Cooke (2006) and Aas, Czado, Frigessi, and Bakken (2009). For some recent works in a parametric and nonparametric framework see e.g. Min and Czado (2010); Bauer, Czado, and Klein (2012); Nikoloulopoulos, Joe, and Li (2012); Weiss and Scheffer (2012); Haff and Segers (2015).A PCC represents the complex pattern of dependence of multivariate data via a cascade of bivariate copulas, and permits to construct flexible high-dimensional copulas by using only bivariate copulas as building blocks, see Aas et al. (2009). Therefore, the joint distribution is obtained on the basis of bivariate pair copulas, that may be conditional on a specific set of variables, allowing to model the dependence among the marginals.In order to obtain a PCC we proceed as follows. First of all we factorise the joint distributionf(x1,…,xd)of the random vectorX=(X1,…,Xd)as a product of conditional densities(2)f(x1,…,xd)=fd(xd)×fd−1|d(xd−1|xd)×…×f1|2⋯d(x1|x2,…,xd).The factorisation in (2) is unique up to re-labelling of the variables, and it can be reexpressed in terms of a product of bivariate copulas. By Sklar’s theorem the joint distribution of the subvector(Xd,Xd−1)can be expressed in terms of a copula densityf(xd−1,xd)=cd−1,d(Fd−1(xd−1),Fd(xd))×fd−1(xd−1)×fd(xd),wherecd−1,d(·,·)is an arbitrary bivariate copula (pair copula) density. Hence, the conditional density ofXd−1|Xdcan be easily rewritten as(3)fd−1|d(xd−1|xd)=cd−1,d(Fd−1(xd−1),Fd(xd))×fd−1(xd−1).Through a straightforward generalisation of Eq. (3), each term in (2) can be decomposed into the appropriate pair copula times a conditional marginal density. More precisely, for a generic element Xȷ of the vector X we obtain(4)fxj|v(xj|v)=cxj,vℓ|v−ℓ(Fxj|v−ℓ(xj|v−ℓ),Fvℓ|v−ℓ(vℓ|v−ℓ))×fxj|v−ℓ(xj|v−ℓ),where v is the conditioning vector, vℓ is a generic component of v,v−ℓis the vector v without the component vℓ,Fxj|v−ℓ(·|·)is the conditional distribution of xȷ givenv−ℓ,andcxj,vℓ|v−ℓ(·,·)is the conditional pair copula density. The d-dimensional joint multivariate distribution function can hence be expressed as a product of bivariate copulas and marginal distributions by recursively plugging Eq. (4) in Eq. (2). Such decomposition is named PCC, as introduced by Joe (1996).Note that the PCC decomposition in (4) is based on the simplifying assumption that the conditional copulas depend on the conditioning variables only indirectly through the conditional distribution functions that constitute their arguments. However, as demonstrated by Haff, Aas, and Frigessi (2010), the simplified PCC is a good approximation, even when the simplifying assumption is far from being fulfilled by the actual model.The PCC is order dependent. A different choice of the variable order leads to a different PCC and to a different factorisation of the joint multivariate distribution.Furthermore, given a specific factorisation there are still many different parameterisations. For high-dimensional distributions, the number of possible PCCs is very high, see Czado (2010) and Morales-Napoles (2011). Hence a suitable representation of all of them is necessary. For this reason, Bedford and Cooke (2001, 2002) introduced Regular vines (R-vines) as a pictorial representation of PCCs.R-vines are a particular type of graphical models, that use a nested set of trees to represent the decomposition of the joint distribution into its bivariate components, incorporating the dependence structure of the variables of interest. Two special cases of R-vines are Canonical vines (C-vines) and Drawable vines (D-vines), see Kurowicka and Cooke (2006). Here we consider a four dimensional problem, for which R-vines are either C-vines or D-vines. We concentrate on D-vines because, differently from C-vine, they do not assume the existence of a particular node dominating the dependencies.A vineV(d)on d variables is a nested set of treesT1,…,Td−1. The edges of tree Tτare the nodes of treeTτ+1,τ=1,…,d−1. In a R-vine, if two edges of tree Tτshare a common node, they are represented in treeTτ+1by nodes joined by an edge. A D-vine is an R-vine where all nodes do not have degree higher than 2, that is each node is connected to no more than two other nodes.In a D-vine, each node corresponds to a variable or a set of variables. A pair-copula density is associated to any edge, with the edge label indicating the subscript of the pair-copula density. An example of a four-dimensional D-vine is provided by Fig. 1. The first tree is constructed ordering the variables according to their pairwise dependence, where the first two nodes correspond to the variables with the strongest association, and so on; the dependencies between nodes {1} and {2}, between {2} and {3}, and between {3} and {4} are modelled using bivariate copula distributions. In the second tree, the conditional dependencies between nodes {1, 2} and {2, 3}, and between {2, 3} and {3, 4} are modelled via pair copula densities. In the third tree, the conditional dependence between nodes {1, 3|2} and {2, 4|3} is modelled via a pair copula density.Using the D-vine representation, the joint density can be decomposed in terms of conditional copula densities (identified by the labels of the edges in the considered trees) times the marginal densities of the examined variables. The joint density for the D-vine represented in Fig. 1 is given byf(x1,…,x4)=∏τ=14fτ(xτ)×c12×c23×c34×c13|2×c24|3×c14|23,wherecab=cab(F(xa),F(xb)).More generally, the density of a D-vine of dimension d takes the formf(x1,…,xd)=∏τ=1dfτ(xτ)×∏j=1d−1∏i=1d−jci,i+j|i+1,…,i+j−1×(F(xi|xi+1,…,xi+j−1),F(xi+j|xi+1,…,xi+j−1))which is the product of d marginal densities fτandd(d−1)/2bivariate copulasci,i+j|i+1,…,i+j−1(·,·)evaluated at the conditional distribution functions F(· | ·).If marginal or conditional independence between pairs of variables holds, the corresponding pair copulas are equal to one and hence the PCC and joint density simplify accordingly. The case of independence is depicted in the corresponding vine by missing edges between nodes, obtaining a forest vine, as shown in Fig. 2. Here, conditional independence between variables 2 and 4 given 3 is represented by the missing edge {2, 4|3}, which reduces the number of levels of the PCC.In this paper we propose a novel contingent claim model for PD estimation via PCCs on balance sheet data, that refines and improves Merton’s analysis. Our approach allows us to evaluate, at any time t, the company ability to service its debts, and consequently to efficiently predict its PD in a flexible way. In the following section we describe the main characteristics of Merton’s model, and introduce our approach.According to Merton (1970, 1974, 1977), the evaluation of the firm total assets Atis based on the structural variables equity Etand bond Bt,At=Et+Bt.A very common assumption is that the value Atof the firm follows a geometric Brownian motiondAt=μAAtdt+σAAtdWt,where μAis the instantaneous expected return of the asset, σAis the volatility, and Wtis a Wiener process. Under the assumptions of market efficiency, no arbitrage opportunity and continuous hedging, the market value of equity satisfies(5)Et=AtN(d1)−e−rTDN(d2),where r is the risk free interest rate, N(·) is the cumulative standard Normal distribution function, D is the face value of bond at maturity T, d1 is given byd1=log(At/D)+(μA+0.5σA2)TσATandd2=d1−σAT. Furthermore, the volatility of the equity is(6)σE=AtEtN(d1)σA.The asset value and its volatility, Atand σA, respectively, cannot be directly observed; however they may be obtained solving Eqs. (5) and (6), see e.g. Ronn and Verma (1986). Consequently we can easily obtain d2, andPD=Pr[AT≤BT]=N(−d2).This model has some drawbacks. Its structure implies that equity and asset values are non negative in trading markets, whereas negative asset and equity are possible in accounting, see e.g. Peterkort and Nielsen (2005). Furthermore, only part of the total debt is traded and observable at specific accounting periods. Finally, Merton’s model might underestimate failure probability, see e.g. De Giuli et al. (2008) and Su and Huang (2010). One possible solution to these issues is proposed in the following section.In order to solve the asset observability issue, we model the firm value via a contingent claim on the underlying securities (equity and debt). We use balance sheet data as a proxy of market data, and we apply PCCs to model the equity dynamic. For a recent work on PCCs in contingent claim analysis see Bernard and Czado (2013).The value of a contingent claim at maturity T can be written in a general form as G(S1(T), S2(T)), where G(·) is the pay-off function, and S1(T) and S2(T) are the underlying securities at maturity T. In this framework the final value of the firm is given byAT=G(ET,BT;T)whereETandBTdenote, respectively, equity and debt at maturity T. In a similar way we can express the equity asET=G1(AT,BT;T)=(AT−BT)where G1(·) is the pay-off function with density g1(·). The equity value at time t is computed as(7)Et=G1(At,Bt;t)=P(t,T)∫0∞∫0∞G1(AT,BT;T)g1(AT,BT)dATdBT,where P(t, T) is the risk free discount factor.The firm value and its return volatility are not directly observable, hence we use balance sheet data, denoted by AT(asset) and BT(liability), as reliable proxy of the market data, see e.g. Eberhart (2005). Assets represent what a firm owns, whereas liabilities are debts arising from business operations. We decompose ATand BTin current (CT) and long term components (LT) on the basis of the considered time period; that isAT=ACT+ALTandBT=BCT+BLT. Current assets will be converted into cash within one year, whereas long term assets will be converted after more than one year. In a similar way, the firm expects to pay off current liabilities within one year; whereas, the firm expects to settle long term liabilities after one year. Comparing current/long term assets with current/long term liabilities we can obtain a quick gauge of the financial status of the firm. In fact, standard accounting ratios commonly used to investigate the financial strength and efficiency of a firm are based on these quantities.Eq. (7) can be rewritten in terms of balance sheet data as followsEt=G2(ACt,ALt,BCt,BLt;t)=P(t,T)∫0∞∫0∞∫0∞∫0∞G2(ACT,ALT,BCT,BLT;T)×g2(ACT,ALT,BCT,BLT)dACTdALTdBCTdBLT,where G2(·) and g2(·) are respectively the pay-off function and its density for the decomposed data.By using Sklar’s theorem the four-dimensional density functiong2(ACT,ALT,BCT,BLT)can be expressed via a copula, and the equity becomes(8)Et=P(t,T)∫0∞∫0∞∫0∞∫0∞G2(ACT,ALT,BCT,BLT;T)×c(FAC,FAL,FBC,FBL)fACfALfBCfBLdACTdALTdBCTdBLT,where c(·) denotes the four-dimensional copula density function,FAC,FAL,FBC,FBLare the marginal cumulative distribution functions, andfAC,fAL,fBC,fBLare the marginal probability density functions.In order to improve the flexibility of the model, allowing the dependence pattern of each pair of variables to be represented by a different copula, we reexpress the previous equation via PPCs. The four-dimensional copula density functionc(FAC,FAL,FBC,FBL)is decomposed in terms of a sequence of bivariate copulas, not necessary belonging to the same family of distributions, via a D-vine decomposition. The specific decomposition depends on the particular data structure under examination; see Section 5 for the details.Simulating from the D-vine decomposition we can approximate the equity function in Eq. (8) via Monte Carlo method as followsE˜t=P(t,T)1N∑k=1NG2(A˜CTk,A˜LTk,B˜CTk,B˜LTk;T),where N is the number of simulations,E˜t,A˜CTk,A˜LTk,B˜CTkandB˜LTkare the simulated values of equity, current and long term assets and liabilities. We then estimate the PD at time t as(PD)t=Pr(E˜t≤0). More details about simulating from a D-vine can be found in Aas et al. (2009).The dynamic of the equity value in Eq. (8) depends on the parameters of the copula and those of the marginal distributions. We denote withθthe parameter vector of the copula functionc(FAC,FAL,FBC,FBL),and withδmthe parameter vector of the marginal distribution m, m ∈ {AC, AL, BC, BL}. The vectorΔ=(δAC,δAL,δBC,δBL)contains the parameters of the marginals, andΨ=(Δ,θ)represents the full set of parameters associated to (8).In order to estimate Ψ we follow the Inference Functions for Margins (IFM) procedure proposed by Joe and Xu (1996). The IFM method estimates the marginal parameters Δ in a first step, and then estimates the copula parametersθ, givenΔ^IFM,in a second step.In order to model the marginals we adopted a parametric approach based on a two-component Normal mixture. This approach was motivated by extensive tests and simulation studies, that are described in Section 5.1.The use of mixture distributions to model multimodal phenomena is a popular technique, which has attracted the interest of several authors in the literature, see e.g. McLachlan and Peel (2000). Peel and McLachlan (2000) use the ECM algorithm to fit mixtures of Student’s t distributions to data containing groups of observations with heavy tails or atypical observations. Komarek and Lesaffre (2008) propose to model the random effects of generalised linear mixed models by a mixture of Gaussian distributions, estimating the parameters in a Bayesian context using MCMC techniques. Escobar and West (1995) use mixture of Dirichlet processes for density estimation. For a review of Bayesian nonparametric methods for density estimation see e.g. Müller, Quintana, Jara, and Hanson (2015). Benaglia, Chauveau, Hunter, and Young (2009) provide a set of R functions, based on EM algorithms, for analysing a variety of finite mixture models, such as mixtures of regressions, multinomial mixtures, nonparametric and semiparametric mixture models. Schellhase and Kauermann (2012) represent unknown densities, allowed to depend on covariates, by a mixture of basis densities, using penalised splines.The current and long term assets and liabilities present bimodal distributions. This behaviour can find an explanation in the effect of the managerial actions and decisions performed to improve the status of the firm. These actions and decisions directly impact the dynamic of current and long term assets and liabilities, and this can intuitively explain the presence of two separated clusters of data.LetF(xmt)be the cumulative distribution function of the marginal m at time t. We estimate each marginal distributionF(xmt)via a two-component Normal mixture model, assuming different means but equal variances (location-shift model)(9)F(xmt)=∑p=12ηpΦ(xmt|μp,σ2).In (9)ηpis the classification probability for component p (with ηp≥ 0 and∑p=12ηp=1), andΦ(xmt|μp,σ2)is the Normal cumulative distribution function with mean μpand variance σ2. The likelihood is given byL(xm)=∏t=1n∑p=12ηpϕ(xmt|μp,σ2),wherenis the number of balance sheet observations, and ϕ is the probability density function of the Normal distribution.Although based on standard distributions, mixture models pose highly complex computational challenges. In particular, one major difficulty is parameters estimation. The literature about mixture models offers various solutions both in the classical and in the Bayesian framework. Considering the classical approach, the most popular method is the EM algorithm, which is a numerical optimisation procedure allowing to calculate the maximum likelihood estimator. However this algorithm may fail to converge to the mode of the likelihood, see e.g. Marin, Mengersen, and Robert (2005). The Bayesian approach constitutes a more flexible and computationally convenient solution to the estimation of mixture models, allowing complex structures to be decomposed into a set of simpler structures through the use of latent variables. Moreover, the Bayesian approach permits, via the use of prior distributions, to incorporate into the model available additional information coming from different data sources. Furthermore, differently from the classical approach, the Bayesian one provides reliable parameter estimates even for sample sizes of limited dimension.For the previous reasons, we use the Bayesian approach to model the dynamic of current and long term asset and liability data. The posterior distribution of the mth marginal is given byπ(δm,η|x)∝(∏t=1n∑p=12ηpϕ(xt|δm))×π(δm,η),wherexis the balance sheet data vector, π(δm, η) is the joint prior distribution ofδmand the vector of classification probabilitiesη. The posterior π(δm, η|x) is computationally intractable to work with; hence, the data augmentation MCMC algorithm is used to estimate the parameters of the mixture distributions, see Tanner and Wong (1987). The data augmentation algorithm introduces a vector of latent variablesz=(z1,…,zn),that represents the allocations associated to each observation xt. Hence, the posterior density can be expressed asπ(δm,η|x)=∫Zπ(δm,η|z,x)π(z|x)dz,where π(z|x) denotes the predictive density of the latent datazgivenx, withz=(z1,…,zn),and π(δm, η|z, x) is the conditional density of the parameters given the augmented data. Moreover,π(δm,η|z,x)=π(δm|η,z,x)π(η|z,x),andπ(η|z,x)=π(η|z),since the distribution is independent ofx. The data augmentation algorithm uses an iterative procedure simulatingzfirst, then generatingηfrom π(η|z) and finally generatingδmfrom π(δm|η, z, x). The densities π(η|z) and π(δm|η, z, x) are easier to sample than the original posterior.Assuming independency between parameters a priori, we specify the following prior distributionszt∼Bernoulli(η1)(η1,η2)∼Dirichlet(α1,α2)μp∼Normal(bp,Bp)σ2∼Γ−1(ν/2,νS/2),where a convenient choice of hyperparameters α1, α2, bp, Bp, ν, S leads us to vague prior distributions. A sensitivity analysis was carried out proposing different hyperparameter values; however, the high similarity of all results suggested that the model is insensitive to prior parameter choice.We need to point out that the simulations were implemented using the software JAGS (Just Another Gibbs Sampler; Plummer (2003)), where the risk of unidentifiability of the model due to label switching was avoided specifying the constraint of unique ordering of the segments, with ascending means of the segment distributions.To estimate the copula parametersθwe apply the following five phases procedure.In the first phase a suitable D-vine decomposition is selected to model the copulac(FAC,FAL,FBC,FBL;θ). We select as a first tree the one maximizing the pairwise dependencies between the considered variables. As a measure of pairwise dependence we use the Kendall’s τ, calculated for each edge connecting two nodes. The problem of finding the maximum weighted sequence of the variables can be transformed into a travelling salesman problem instance and solved accordingly, see Brechmann (2010). The structure of remaining trees is completely determined by the structure of the first one. Therefore, the strongest dependencies are captured in the first tree, allowing to obtain a more parsimonious model, with more stable parameter estimates.In the second phase suitable pair copulas are chosen. For each pair of variables we select the best fitting pair copula using the Akaike Information Criterion (AIC), which is chosen among other criteria (i.e. the Vuong and Clarke goodness-of-fit test developed by Vuong (1989) and Clarke (2007), and Bayesian Information Criterion (BIC)) for its good performance in simulation studies. However, before calculating the AIC, the Genest and Favre bivariate asymptotic independence test (Genest & Favre (2007)) is performed to check for independence on each pair of variables of the D-vine. If conditional independence between variables is observed, the number of levels of the pair copula decomposition is reduced, and hence the construction is simplified, as discussed in Section 2.2.In the third phase, the parameters of the copulas in the first tree are estimated. For each copula there is at least one parameter to be determined. The number of parameters depends on which copula type is selected in the previous phase. To estimate the copula parameters we employ the maximum likelihood estimation method, using the sequential updating parameter estimates as starting values, see Aas et al. (2009) for more details.In the fourth phase, given the results of the first tree, we compute pseudo-observations via the conditional distributions F(x|v). These values are then used as input for the next trees of the D-vine.In the fifth phase, the procedure illustrated from phase 2 to phase 4 is repeated for all trees of the D-vine.We consider four fraudulent bankruptcy cases, related to well known financial scandals: Cirio (1993–2002), Enron (1997–2000), Parmalat (1990–2003), and Swissair (1988–2000). To test the behaviour of our methodology, we also examine the Sysco company, a firm operating in the same period of time of the previous ones, with a strong financial reputation, and presenting some characteristics in common with some of the examined defaulted firms. For comparative purposes, for this last firm we consider balance sheet data of the years 1990–2003. With the exception of Enron, the other defaulted firms are now operating under the direction of a different leadership group.We use semestral balance sheets data downloaded by the “Thomson Reuters” and the “Bloomberg” databases. The data have been converted into monthly observations assuming uniform distribution in the semesters. For Swissair and Enron the complete balance sheets for the year of failure are not available.We now briefly describe the profile of each examined firm, outlying the events that lead to the bankruptcy of the defaulted firms.Cirio is an Italian food company founded in 1856. Its bankruptcy in 2002 was the consequence of the fraudulent financial policy of its managerial group.Enron was an American energy, commodities, and services company created in 1985 through the merger of two natural gas companies. Before its collapse in 2001 it was one of America’s leading companies with a solid reputation, and it was one of the highest-rated companies of Wall Street. At the end of 2001 it was made public that its apparently solid financial conditions were substantially sustained by an institutionalised, systematic, accounting fraud. The company declared bankruptcy in December 2001.Parmalat was created in 1961 as a small pasteurisation plant in Parma (Italy). It subsequently became a multinational corporation in the 80’s with different food product lines, and expanded further in the 90s. It was listed for the first time in the Milan stock exchange in 1990. Parmalat collapse in 2003 was the biggest case of financial fraud and money laundering perpetrated by a private company in Europe. It was the first Italian corporate crash with international implications.Swissair presents a different story from the previous defaulted firms. It was formed in 1931 from the merging between Balair and Ad Astra Aero and it was one of the major international airlines with a strong financial stability. It rapidly declined from one of the major international airlines with the strongest balance into bankruptcy in 2001. This rapid decline was the consequence of inefficient alliance policies, management inability and economic turndown following the terroristic attacks of “September 11”.Sysco is an American marketer and distributor of foodservice products. It was founded in 1969 and became public in 1970. Nowadays, it is a solid company with a very good reputation.In the following section we report a detailed analysis of the four defaulted companies, and we present the main important results of the Sysco company.We modeled the current and long term assets and liabilities of the considered companies employing the two-component Normal mixture model described in Section 4.1. The choice of this model was determined by extensive tests and simulation studies.First, in order to identify the best model for the marginals, we assessed the fit to the data of classical parametric models, such as the Normal, the left-truncated Normal in zero, the log-Normal, the Gamma, the Exponential and the Weibull distribution. We implemented a bootstrap version of the univariate Kolmogorov-Smirnov test, with 1,000 Monte Carlo simulations. The bootstrap Kolmogorov–Smirnov tests for the hypothesis that the actual data were generated by the corresponding theoretical distribution. Table 1 shows the results of the bootstrap Kolmogorov–Smirnov tests for the marginals of the Enron dataset. We obtained very similar outputs for the other datasets considered in this paper. For each theoretical distribution being tested the average p-value over the 1,000 simulations and the percentage of times the null hypothesis is not rejected are displayed. Since the null hypothesis was always rejected at the 0.05 level, we concluded that none of the classical parametric model tested was suitable for our data.Due to the poor fit of classical models to our marginals, and since the current and long term assets and liabilities present bimodal distributions, we opted for two-component parametric mixture models. We tested several families of parametric mixture distributions for the marginals of all the considered datasets, and the Normal mixture always outperformed other models. In particular, we selected the Normal, the log-Normal and the Gamma mixtures, since many other models are related to them: the truncated Normal is related to the Normal and the Exponential and Weibull are related to the Gamma distribution. Fig. 3depicts the histogram of Enron current assets (in EUR) fitted with three different mixture models and the Normal mixture (solid line) clearly shows the best fit. Similar results were obtained for the remaining marginals and datasets.Before choosing to model the marginals with a two-component Normal mixture model, we estimated the number of components p using Bayes factors, as suggested by Kass and Raftery (1995); Richardson and Green (1997); and Marin et al. (2005). We calculated Bayes factors for all the marginals of the considered companies, comparing the model with two components with all models with a number of componentsp=1,3,4,…,10. Placing the model with two components in the numerator of the Bayes factors, the results we obtained were greater than one for all the marginals, showing that the two-component Normal mixture is the most strongly supported model by the data. For illustration, Table 2lists the Bayes factor results for the Enron current assets data. BFr, sdenotes the Bayes factor of model r against model s. The results are not surprising, since inspection of the histograms of the marginals clearly reveals bi-modal distributions.In addition, we tested the fit of the two-component Normal mixture with the symmetric location-shifted semiparametric model of Bordes, Mottelet, and Vandekerkhove (2006) and Hunter, Wang, and Hettmansperger (2007), which is based on a mixture of unspecified densities, assumed symmetric about zero, see Benaglia et al. (2009). Fig. 4shows the histogram of Enron long term liabilities (in EUR) fitted with the two-components Normal mixture (solid line) and the semiparametric model (dashed line). In the first row plot the semiparametric model was obtained with no specification of the initial mean values of the mixture components, while in the second row plot the semiparametric model was obtained specifying the initial mean values of the mixture components. In the former case, the semiparametric model shows a worse performance than the Normal mixture, adding a new unnecessary component to the mixture. In the latter case the performance of the semiparametric model is very similar to the Normal mixture and does not improve the fit to the original data. The application of the semiparametric model to the remaining marginals of the other considered datasets yields very similar results.Therefore, we modelled the current and long term assets and liabilities using a two-component Normal mixture, since this was the best model to fit the marginals. For each single firm we report the estimates of the parameters (posterior means) of the corresponding mixture models in Table 3, together with the 95% credible intervals (in brackets).The classification probabilities ηpare quite close to 0.5 for Cirio data, for the asset marginals of Parmalat data, for the current assets and long term liabilities of Swissair data, and for Sysco data, denoting a balanced number of observations in the two mixture components. On the contrary, Enron data, the liability marginals of Parmalat data, long term assets and current liabilities of Swissair data show very different classification probabilities η1 and η2. This means that different proportions of observations are allocated to the components of the mixture and that one of the two components captures the greatest number of data. The location parameters of the two Normal components of the mixture μpare well separated, especially for Enron, Parmalat and Sysco, denoting that the mixture model is able to express the mean difference between the two components. The dispersion parameter σ2 is particularly high for Enron, Parmalat and Sysco, while it is lower for Cirio and Swissair.Enron and Parmalat have the most unbalanced mixture components, especially with reference to the liability marginal data. The data of these two companies are characterised by very different values of classification probabilities ηp, very different means μp, and very high normal variance values σ2. The resemblance of the structure of assets and liabilities in Enron and Parmalat may be explained by the similar behaviour of these two companies during the years before their default. Parmalat indeed has been referred to as the “Europe’s Enron”.We now present a graphical analysis of the results of Enron company. We have performed a similar analysis for the remaining four companies, but we do not report it here for lack of space.Fig. 5shows the histograms of each marginal measured in EUR (grey bars), fitted with the location-shift model of two Normal components (black and grey lines) described in Section 4.1.FACis displayed in the top left panel,FALin the top right,FBCin the bottom left andFBLin the bottom right. Let us consider the picture related to the current assets marginal of Enron data (top left panel of Fig. 5). The histogram shows a highly bimodal distribution which justifies the use of a finite mixture model. Similar comments arise from the analysis of the fitted histograms of the remaining marginals.Fig. 6shows the sampled values of the μ1 parameter on the horizontal axis and of the μ2 parameter on the vertical axis.FACis displayed in top left panel,FALin the top right,FBCin the bottom left andFBLin the bottom right. It is interesting to note that our data are not affected by label switching, since the segments are rather well separated for μ, as there are no points on the diagonal on the μ1 versus μ2 plots.Focusing on the MCMC results, here we illustrate the outcomes of the Enron long term assets data, since the results of the remaining marginals are very similar to those presented. Figs.7, 8and 9 depict MCMC trace plots and posterior densities, obtained using kernel density estimation from the R package “bayesmix” of Grüen (2014), for the parameters η, μ and σ2, respectively. We run the algorithm for 4,000 iterations, discarding the first 1,000 iterations as burn-in period. The trace plots show that the chains are well mixing, exploring freely the sample space and clearly reaching convergence to the target distribution. Moreover, the unidentifiability problem due to label switching, that may lead to biased estimates, in our case does not occur. Finally, the posterior density plots have regular forms and do not show multimodalities.Following the procedure described in Section 4.2 we select an appropriate pair copula decomposition for the D-vine. For each one of the defaulted firms the order of the marginals that maximizes the pairwise Kendall’s τ indexes in the first tree isACT−BCT−BLT−ALT.In Tables 4, 5, 6 and 7 we display, for the defaulted firms, the list of pair copulas for each D-vine, the selected copula families, the copula parameters (one or two according to the type of copula) and the corresponding Kendall’s τ. The results are obtained using the R package “CDVine” by Brechmann and Schepsmeier (2013). From the selected copula families, we see evidence of different types of asymmetric dependence. This demonstrates that the choice of PCCs is appropriate, since it guarantees enough flexibility to model the complex and asymmetric dependence structure of the data at hand. Note that only the Cirio D-vine (Table 4) has none conditional independent variable pairs. For these data the Genest and Favre (Genest & Favre (2007)) independence test rejected independency for all the copulas involved. An independence copula has been selected instead forcACT,BLT|BCTin the second tree for Parmalat and Swissair (Tables 6 and 7), whilecALT,BCT|BLThas been identified as an independence copula for Enron (Table 5). In these cases the D-vine structure is simplified and we do not need to estimate the parameters of the copulacACT,ALT|BCT,BLTin the third tree. The presence of conditional independence in this last case suggests a weak relationship between the current and long term assets, given the values of liabilities. From the unconditional pair copulas, we note an existing dependence between current and long term assets or liabilities, and also a dependence between the two different types of liabilities. A strong dependence in conditional copulas instead may suggest imbalance, when current assets are financed by long term liabilities, or a serious liquidity problem, when long term assets are financed by current liabilities. These situations need particular attention, because they may prelude to the default of the firm.For the Sysco company the order in the first tree isACT−ALT−BLT−BCT.In Table 8 we display the list of pair copulas for the D-vine, the selected copula families, the copula parameters (one or two according to the type of copula) and the corresponding Kendall’s τ. In this case, an independence copula has been selected forcACT,BLT|ALT. The D-vine structure is simplified and we do not need to estimate the parameters of the copulacACT,BCT|ALT,BLTin the third tree. The presence of conditional independence in this last case suggests a weak relationship between the current asset and liabilities, given the values of the long term ones.To estimate the PD we follow the methodology described in Section 3.2. For each firm we generate 10,000 simulations from the selected D-vine to obtain the equity distribution and the PD. Fig. 10 depicts the equity densities of Cirio, Enron, Parmalat, Swissair and Sysco, respectively on the top left, top right, middle left, middle right and bottom panel. The Figure was obtained using kernel density estimation. The value of the PD is written in the relevant plot and corresponds to the area under the curve where the equity is zero or negative. The PD values are very high for all defaulted firms, lying in the range0.4468−0.6892; in contrast, the Sysco PD is only 0.0026, as we expected for a healthy company, where the probability of going bankrupt is very low. We notice that the PD values of Enron and Swissair are slightly lower than the other defaulted firms. However, the available balance sheet data did not include the last year of activity of Enron and Swissair. This might have affected the final results, since the inclusion of the last year’s data would certainly have increased the corresponding PD values.For comparative purposes we contrasted our results with those obtained applying the original Z-score proposed by Altman (1968), e.g. to the Enron company. Fig. 11 shows the line plot of Altman’s Z-score for the time horizon between 1997 and 2001. According to Altman (1968), a company is considered to be in the “safe” zone (healthy) when z > 2.99, it is in the “grey” zone (moderate risk of default) when 1.81 < z < 2.99, and it is in the “distress” zone (high danger of default) when z < 1.81. Altman’s Z-score is clearly not able to predict the failure of the Enron company, since it locates the firm in the distress zone only until 1998; subsequently, from 1999 to 2000, it moves the firm to the grey zone (erroneously suggesting an improved performance); and finally (when the actual default actually occurred) places the firm in the safe area, with a Z-score of 3.22. Besides, in the considered period Altman’s Z-score is not decreasing, but even rising, leading to completely misleadingconclusions.The Z-score’s inability of predicting the default is due to the fact that, unlike the PCC model, it does not consider the dependence pattern among the different components of the balance sheet data. On the contrary, the proposed PCC model allows us to measure the dependencies and to detect in advance alarming situations, which are not identifiable using other traditional models. Moreover, our PCC approach permits to adopt different and more suitable marginal distributions, better reflecting the structure of the data at hand.Moreover, the calculation of Altman’s Z-score, unlike the PCC model, involves balance sheet data as well as economic and income data, without analysing the relationship among these quantities. For this reason, the Z-score might mask dangerous default risks, classifying a company as “safe”, when it is truly in distress.The aim of this paper was to propose a novel methodology for PD evaluation. Our final goal was to calculate the PD of large firms using their balance sheet data. We measured the firm value via a contingent claim, whose pricing function may be expressed using copulas. The marginals are given by the current and long term assets and liabilities. Hence, the equity function is expressed by a four-dimensional D-vine copula. To test the performance of our methodology we applied it to four fraudulent defaulted stocks and to the data of a healthy firm. In order to estimate the marginals we employed a Bayesian mixture model, able to model the presence of two clusters in the asset as well as liability data. The structure of the marginals in defaulted firms reflects the choices of the management, trying to balance high and low accounting items during the period before the default. Considering the copula, we chose to employ PCCs, because they allow for a great flexibility in modelling the dependence structure of the marginals. As demonstrated by the results, the pair copulas selected in the D-vines belong to different families and describe various types of dependence. The analysis of these dependencies in defaulted firms data already reveals substandard loans and situations of serious imbalance due to liquidity issues, especially when the firm tries to balance long term assets with current liabilities. Finally, we calculated the PD of the five considered firms, simulating from the D-vines and obtaining the equity distributions. The final results show a high PD for the defaulted firms, suggesting their forthcoming bankruptcy. The PD of the Sysco company is instead much lower than those of the defaulted firms, denoting a good performance. A traditional indicator like Altman’s Z-score may be incapable of predicting the risk of default, since it is not flexible enough and it does not incorporate the dependence structure of the involved quantities. On the contrary, the proposed methodology has proven to be successful in the evaluation of PD and would certainly benefit analysts and managers, advising them to take actions against a potential bankruptcy.Possible extensions of our work include the estimation of the whole model in a full Bayesian framework, the application of nonparametric approaches for PCCs, the use of balance indicators instead of accounting items and the use of a our methodology to analyze the contagion in sectors of activity. Finally, it would be interesting to apply our approach to Altman’s Z-score, to model the dependence between the different quantities involved.

@&#CONCLUSIONS@&#
