@&#MAIN-TITLE@&#
Multiobjective PSO based adaption of neural network topology for pixel classification in satellite imagery

@&#HIGHLIGHTS@&#
We present the work on efficient classification of multispectral images using soft computing approach.Selection of most discriminative spectral bands and determination of the number of hidden layer neurons are the two most critical issues.We proposed a new multiobjective particle swarm optimization based methodology for adaption of neural network structure for pixel classification of Satellite Imagery.It simultaneously estimates the most discriminative spectral features and the optimal number of nodes in hidden layer.Xie-Beni and β indexes of proposed algorithm are better than MLC and Euclidean Classifier.

@&#KEYPHRASES@&#
Land cover classification,Multiobjective optimization (MOO),Neural network,Particle swarm optimization,Remote sensing imagery,

@&#ABSTRACT@&#
The proposed work involves the multiobjective PSO based adaption of optimal neural network topology for the classification of multispectral satellite images. It is per pixel supervised classification using spectral bands (original feature space). This paper also presents a thorough experimental analysis to investigate the behavior of neural network classifier for given problem. Based on 1050 number of experiments, we conclude that following two critical issues needs to be addressed: (1) selection of most discriminative spectral bands and (2) determination of optimal number of nodes in hidden layer. We propose new methodology based on multiobjective particle swarm optimization (MOPSO) technique to determine discriminative spectral bands and the number of hidden layer node simultaneously. The accuracy with neural network structure thus obtained is compared with that of traditional classifiers like MLC and Euclidean classifier. The performance of proposed classifier is evaluated quantitatively using Xie-Beni and β indexes. The result shows the superiority of the proposed method to the conventional one.

@&#INTRODUCTION@&#
Multispectral satellite images of the Earth's surface are important source of spatial data for derivation of land cover maps. We need to identify land cover class like vegetations, waterways, man-made structures and road network from satellite images. The aim of image classification is to categories all pixels into one of the land cover classes, which may be used to produce thematic map of land cover presenting the image. This approach is called ‘per pixel’ classification based on spectral data [1].Traditional parametric statistical approaches to supervised classification include the Euclidean, maximum likelihood (MLC) and Mahalanobis distance classifiers. Such approaches depend on the assumption of a multivariate Gaussian distribution for the data to be classified A problem with the statistical approach is that the data in feature space may not follow the assumed model. Another problem area of statistical pattern recognition in remote sensing is the “Hughes phenomenon” [2]. In recent years the artificial neural network has been applied to general pattern recognition problems. A fundamental difference between statistical and neural approaches to classification is that statistical approach depends on an assumed model whereas neural approach depends on data [3].Use of neural network for the classification of satellite images is not new. In fact the first use of ANN for such task was reported by Benediktsson et al. [4]. In remote sensing literature, different neural network architectures are employed in supervised and unsupervised manner and for variety of purposes [5–8]. The neural networks have reported to yield comparable or superior accuracy compared to statistical classifiers [9]. The neural networks are particularly suitable for remote sensing problems as they are more suitable with less reliable training samples and are less subject to “Hughes phenomenon” with properly chosen network architecture [10]. In general for supervised classification of multispectral satellite imagery, feedforward neural network with single hidden layer is found suitable. Also the pixel gray scale value in available spectral bands is used as input feature for classification.In our literature survey, it is found that determination of number of hidden layer neurons is critical issue and most of researchers have obtained the number of hidden layer neurons either experimentally or by same heuristics. Atkinson et al. [3] proposed that the number of hidden neuron is equal to (2N+1) where N is number of features. Kasapoglu and Ersoy [11] have empirically chosen one hidden layer with 15 neurons. Bernard et al. [12] have averaged the result over tests on different neural networks with 8 to 21 nodes in the intermediate hidden layer and found that in this range overall performance did not vary widely.The number of hidden layer neurons is set to the square root of the product of the number of input features and output classes in [13,14]. Halder et al. [15] have used two hidden layers network for supervised classification and experimentally determined the number of hidden neurons in each layer to get the optimum result for comparison. A Gaussian synapse artificial neural network is used by Crespo and Duro [16] to identify different crops and ground elements from remote sensing data sets. The networks are structurally adapted to the problem complexity as superfluous synapses and/or nodes are implicitly eliminated by the training procedure, thus pruning the network to the required size straight from the training set.In [17], for multispectral images, the evolved network has two hidden layer with six neurons in each layer. The networks consisting of more than one hidden layer do not show significant increases in accuracy compared with those containing just one.Thus, we have not found any general criteria for defining suitable network architecture. Bigger networks tend to have poor generalization capability than small networks. We believe that the number of hidden layer neuron depends on the classification problem in hand and must be determined methodologically. From our experimental analysis, it is observed that both the input features and the number of hidden layer nodes together affect the classification accuracy and therefore must be considered simultaneously. We found that no one has work on these two issues simultaneously.There are recent studies that investigate application and performance of population-based evolutionary algorithms applied to the architectures of NNs. Fieldsend and Singh [18] suggest an evolutionary MOO algorithm for the structure optimization of four-hidden layer NNs for stock data prediction. They use task-specific objectives, the risk and the profit, for the structure optimization of the neural regression model and compare their results to the results of a single-objective optimization approach. For face and car classification problem [19], hybrid evolutionary multiobjective algorithm is used to optimize the weights and the structure of the feedforward NN. The optimization algorithm generates next parental population of NNs from previous parental population and offspring population obtained in three steps. The offspring is mutated by variation operators like add and delete connection and node, with a goal to reduce the number of weights. The two objective functions, number of weights and classification error are subject to minimization. A new method that allows the automatic development of ANNs by using Genetic Programming (GP) is proposed by Rivero et al. [20]. The evolved topologies can have any kind of connectivity, with connections, for example, between input and output neurons. But GA has some shortcomings such as more predefined parameters, more intensive programming burden and complicated evolutionary operators [21].PSO is most successful SI algorithm used by the researchers for many applications. The high speed of convergence of the PSO algorithm attracted researchers to develop multiobjective optimization algorithms using PSO [22]. Many researchers have employed PSO and its modified versions to train the NNs [23–26]. We found few studies on PSO based network architecture design.Kiranyaz et al. [21] proposed a multidimensional PSO technique to evolve optimal feedfoward ANN configuration. It seeks the positional optimum in the error space and the dimensional optimum in the architecture space. However each particle has been subjected to two processes. The first one is a regular positional PSO in N dimensional search space. The second one is a dimensional PSO, which allows the particle to navigate through dimensions. It is based on single objective function that is MSE. For video-based face classification, a supervised learning strategy based on a Multiobjective Particle Swarm Optimization (MOPSO) is introduced for ARTMAP neural networks. It is based on the concept of neural network evolution in that particles of a MOPSO swarm seek to determine network weights and architecture such that generalization error and network resources are minimized [27]. A memetic pareto PSO based method that evolves architecture and connections of RBF network simultaneously is proposed by Qasem et al. [28]. The binary multiobjective PSO and real multiobjective PSO have been used to optimize the architecture of RBFN and its parameters simultaneously, respectively. The proposed method modifies the MOPSO algorithm and augments it with local search (BP) to form the memetic approach. But the algorithm is found to be quite slow. Almeida and Ludermir [29] presents a method for optimizing the parameters and performance of ANNs through a search for weights, architectures (nodes and layers), transfer functions and training algorithm rates. The proposed method is based on fully connected supervised MLP composed of a combination of ES, PSO and GA as well as the backpropagation and Levenberg–Marquardt training algorithms. However the proposed method requires the considerable time required for the execution of the search.In all above studies, proposed algorithm determines only hidden layer while input and output layers remain fixed. Recently, multiobjective optimization (MOO) and swam intelligence techniques have attracted the attention of researchers in the field of satellite image processing also. Bazi and Melgani [30] proposed multiobjective PSO based method for model selection of SVMs used for satellite images. A multiobjective optimization algorithm to simultaneously optimize a number of fuzzy cluster validity indexes for classification of remotely sensed images is proposed by Bandyopadhya et al. [31]. In [32], a multiobjective particle swarm optimization (MOPSO) framework is applied to simultaneously estimate the class clustering statistical parameters, discriminative bands and class numbers, for unsupervised clustering the hyperspectral images. However the method has high computational cost.In this paper, we present MOPSO based integrated approach to find most discriminative spectral band (input layer nodes) and optimal number of hidden neuron for feedforward single hidden layer ANN. We also present the large number of experiments conducted to study the behavior of neural network for classification of remotely sensed imagery.The rest of paper is organized as follows. In Section 2, we present detailed analysis of neural network to study its behavior for multispectral image classification. Based on the finding during experiments, we formulate the problems associated with use of network and propose a solution on it, in Section 3. We briefly discuss the concept of particle swarm organization and multiobjective optimization techniques in Sections 4 and 5, respectively. The proposed MOPSO based approach is explained in Section 6. Finally, result is discussed in Section 7 and conclusion is presented in Section 8.In following section, we describe the detail of experiments carried out to study the behavior of neural network for satellite image classification. The analysis of experiments to study the impact of various parameters on classification accuracy is presented.In our experiments we have used three layered (single hidden layer) neural network. It is trained by backpropagation algorithm [33]. After learning, the network is used as a classifier to classify the whole image. One input node is used to represent the each spectral band or features. Thus the number of input nodes is determined by the number of spectral bands i.e. by dimension of the input pattern. The input pattern consists of normalize gray scale value of a pixel in selected spectral bands. Also we have used one output node per ground cover class i.e. the number of output nodes is equal to the number of classes in the image. The number of hidden layer nodes is varied from 1 to 8 for experimental analysis.We have used Landsat satellite images of Washington, DC, city area [34]. The six images are of size 512×512pixels each and corresponding to six spectral bands: b1: visible blue (450–520nm), b2: visible green (520–600nm), b3: visible red (630–670nm), b4: near infrared (760–900nm), b5: middle infrared (1550–1750nm) and b6: thermal infrared (10,400–12,500nm). The four major classes identified in the images are: water, urban area, vegetation and roads. Fig. 1shows two images corresponding to bands 3 and 4.In our work, we have randomly selected the samples of each class by visual inspection of the image with the help of Matlab software. Total 50 samples of each class were selected and equally divided into 25 samples each to form training and test set. For training and testing input patterns, the desired output vector was obtained by setting the low value of 0.1 for the output node that do not corresponds to the pixels assigned class and high value of 0.9 for the node that does corresponds to the pixels assigned class i.e. for the input pattern of class 1, the desired output vector will be [0.9, 0.1, 0.1, 0.1], for class 2, it is [0.1, 0.9, 0.1, 0.1] and so on.We have performed the experiments to study the behavior of neural network for given classification problem. The numbers of spectral bands are increased from 2 to 6. We started with spectral band combination of visible blue and red i.e. bands b1 and b2. Then we added remaining bands one by one. The number of hidden layer nodes is changed from two to eight for each of the above input feature combination. We have trained each network with training data set sizes of 5, 10 and 25 pixels. Also for each of the network, ten different initial weights were selected for training. Thus we have conducted total 1050 experiments with all combinations of above variables. Based on the result of above experiments, we now discuss the effect of these variables one by one.From the confusion matrices, it is observed that for input features combination b1, b2 and b1, b2 and b3 (any number of hidden node and sample size), test sample classification accuracy shows great dependency upon initial weights. This may be because of inadequate input features. With two hidden nodes (any input feature and sample size), accuracy was significantly affected by the initial weights. It seems that network does not have enough flexibility to adjust the weights. With sufficient input features (b1, b2, b3 and b4 and more) along with at least three hidden nodes, dependency upon initial weights was greatly reduced. With input features more than three and hidden node greater than two, variation in accuracy due to initial weights reduces with increase in sample size as shown in Fig. 2. Thus we conclude that with lower number of neurons in the hidden layer, network does not have enough flexibility to adjust the weights. Therefore there is large variation in accuracy due to the change in initial weights. Also due to insufficient input feature, there is considerable change in accuracy due to the initial weights. The increase in number of hidden nodes beyond minimum does not have effect on the variation in accuracy due to initial weights.The investigation of confusion matrices for the training sample size revealed following facts. Up to three spectral band (any number of hidden nodes), increase in sample size does not improves classification accuracy and overall test sample accuracy is very low (40–60%). This may be because of inadequate input features. With four spectral bands and hidden layer nodes more than two, increase in training sample set size from 5 to 15 and then to 25 do improve the accuracy. But the improvement is insignificant (2%) for increase in samples from 15 to 25. See Fig. 3. That is, the rate of improvement in accuracy decreases with increase in the sample size. It is also observed that for the given neural network architecture, the training time increases and variation in accuracy due to the change in the initial weights reduce with increase in sample size.Thus with inadequate number of input and hidden layer nodes, increase in sample size does not improves classification accuracy and overall accuracy remains low. With proper network architecture, accuracy increases with sample size but the rate of improvement decreases with increase in the sample size. Thus we must prepare training sample set not keeping in mind the size but the quality representation of the classes to be identified.Regarding the hidden layer nodes following observations are made. With two nodes in the hidden layer (any input feature combination), accuracy is very low and varies greatly with initial weights. This is because network does not have enough flexibility to adjust the weights. The classification accuracy increases significantly as the hidden nodes are increased from two to three as shown in Fig. 4. After that, increase in hidden node size does not have significant effect on the accuracy. It seems that the network with two hidden nodes is not suitable for this problem since it does not have enough flexibility to adjust the weights.It is observed that with input feature combination b1, b2 and b1, b2 and b3, accuracy is very less (60–70%). This is because that network does not train properly for classes 3 and 4, due to inadequate number of input features. The addition of spectral band b3 does not improve the accuracy. As seen in Fig. 5, for input feature combination b1, b2, b3 and b4 accuracy jumps to 90% as the network is well trained for all classes. Thus it seems that spectral band b4 brings additional information to improve the accuracy. After this, increase in input features does not improve the accuracy. Thus inadequate input features results in poor classification. Addition of some input feature brings the more information and helps to improve the accuracy but some may not. We should not use the spectral band that does not bring any discriminating information. This also indicates that good classification can be achieved by using the subset of the available features or spectral bands. The effect of increasing the number of input features is not necessarily increase the accuracy of the classification but to increase the computing time requirements.Form above experimental analysis, we made following observations:•The dependency upon initial weights can be reduced to great extent with proper input features, sufficient number of hidden nodes and adequate sample size.Increase in sample size does not improve the accuracy but consumes time.For proper classification, minimum number of hidden node is must. Beyond that, increase in hidden nodes does not improve the accuracy. On the contrarily, network may lose its capacity to generalize and increases the training time.The classification accuracy is not function of the number of input features but depends upon the ‘information’ provided by the features. Therefore input features should be selected so that they contain distinct information for each output class. So we must have some method to select the useful features.Therefore to improve the classification accuracy and reduce computations or to increase the speed of classification, we require most discriminative spectral features and optimal number of hidden layer nodes. Thus objective is to detect most discriminative spectral band and to design an optimal ANN classifier to efficiently classify satellite images into various land cover classes.In this work, we proposed to solve this complex problem within multiobjective particle swarm optimization framework to simultaneously estimate the most discriminative spectral band and to determine the number of nodes in hidden layer. Due to conflicting nature of both task use of multiobjective optimization is justified. The PSO based approach is employed due to its high speed of convergence.PSO is population (called as swarm) based search method invented by Kennedy and Eberhart [22]. It is stochastic optimization technique inspired by the social behavior of animals. Each particle (a candidate solution) of a given population can benefit from the past experiences of all other individuals in the same population. During the iterative search process, each particle will adjust its velocity and position according to its own experience as well as those of the other particles in the swarm. Let us consider a swarm of particle of size S i.e. Pi(i=1, 2, … S). Let Pt(t) be the current position, Vi(t) be its velocity at iteration t and Pbi(t) the best position identified. Let Pgbe the best global position found over all trajectories traveled by the particles of the swarm. Position optimality is measured by the fitness functions defined based on the given optimization problem.During the search process, the particles move according to the following rule:(1)Vi(t+1)=wVi(t)+c1r1(Pbi(t)−Pi(t))+c2r2(Pg(t)−Pi(t))(2)Pi(t+1)=Pi(t)+Vi(t+1)where r1 and r2 are random variables drawn from a uniform distribution in the range [0,1], c1 and c2 are two acceleration constants with respect to the best global and local positions respectively. These parameters determine the relative bias of the best position of the particle (self-experience) and the global best position (experience of group members). The inertia weight w is used as a tradeoff between the global and local exploration capabilities of the swarm. Eq. (1) allows the computation of the velocity at iteration (t+1) for each particle in the swarm and the particle position is updated with Eq. (2). These equations are iterated until maximum number of iterations is completed or the best value of the adopted fitness function is reached.In multiobjective optimization (MOO), search is performed over a number of conflicting objective functions [35]. It yields number of nondominated Pareto-optimal solutions. The aim of search is to find the optimal vectorx¯∗=[x¯1∗,x¯2∗,...,x¯v∗]Tof v decision variables which optimizes the objective functionf¯(x¯)=[f1(x¯),f2(x¯),...,fk(x¯)]Tvector of k objective functions. All admissible solutions lie in the feasible region defined by the number of equality and non equality constraints. A decision vectorx¯∗is called Pareto-optimal if and only if there is nox¯that dominatesx¯∗. Thusx¯∗is Pareto-optimal if there exists no feasible vectorx¯which cause a reduction on some criterion without a simultaneous increase in at least another. Among the available MOO techniques, we have used, the methodology proposed by Coello Coello and Lechuga [36].We shall now describe the proposed MOPSO based scheme to get subset of spectral feature and optimal number of hidden layer nodes of neural network classifier for the per pixel classification of satellite image.Each particle in the swarm is considered as a vector that encodes the following two variables to be optimised in terms of binary value.The first part of a particle encodes the candidate subset of input features among the available B spectral bands as follows:(3)f(i)=1ifithfeature is selected=0ifithfeature is not selectedwhere i=1 to B.The second part encodes the number of nodes in hidden layer as follows.(4)h(i)=11≤i≤H=0H<i≤Hmaxwhere H is selected number of nodes in the hidden layer and Hmax is maximum allowable nodes in the hidden layer. The structure of each particle is as shown in Fig. 6.The fitness function is used to evaluate the performance of each candidate solution. In present context we need to jointly optimize the two different criteria to estimate the spectral feature and number of hidden layer nodes. The first fitness function we have used is the mean squared error (MSE) on training data set. The MSE must be minimized to get good classification accuracy. It aims to determine the most discriminative spectral features that improve the accuracy. Thus MSE deals with our first objective to determine most discriminative features.The second objective function has to deal with the number of nodes in hidden layer and it should be in conflicting with the first fitness function, MSE. To achieve this, we proposed to use the number of nodes in hidden layer itself as a fitness parameter and it should be minimized in the optimization process. During our experimentations, we have seen that the lower the number of hidden layer nodes, more was the MSE. Thus the use of this parameter as a fitness function is justified.The steps involved in the proposed algorithm for multiobjective PSO based adaption of neural network topology for pixel classification is as follows.(a)Generate randomly an initial swarm of size S. Initialize each particle position Pi(I=1, 2, … S) as follows.(i)Randomly select the subset of the spectral bands to be used among available B bands and set the coordinates of selected features to 1. Keep all other coordinates to 0, as explained in Section 6.1.1.Randomly select the number of hidden nodes to be used i.e. H and set that number of coordinates to 1 while remaining to zero, as explained in Section 6.1.2.Set the velocity vectors Vi(t) associated with the particles to zero. Set the best position of each particle with its initial position, i.e. Pbi=Pg.For each candidate particle Pi, train an ANN classifier with the encoded feature set and the number of hidden nodes. Also compute the corresponding fitness functions: MSE and H.Identify the nondominated solutions by applying the algorithm described in [36] and store them in external repository.(e)Select the best global position from the repository. Update the speed of each particle using following equation:Vi(t+1)=wVi(t)+c1r1(Pbi(t)−Pi(t)+c2r2(Pg(t)−Pi(t).Update the position of each particle using discrete binary PSO algorithm.Pi(t+1)=1sig(Vi(t+1))>0.5=0elseFor each candidate particle Pitrain an ANN classifier and compute the corresponding fitness functions.Identify the nondominated solutions and update the contents of repository. Also update the best position of each particle if its current position Pbihas a smaller fitness functions.If the termination criterion is not yet reached, return to step (e).(i)Select the best global particle Pgfrom the nondominated Pareto optimal set and extract the number of hidden layer nodes and the subset of detected features as encoded in that particle structure. Thus we get optimal neural network topology.Train such optimal network using training data set.Then use trained network in feedforwad direction to classify each pixel in image.We have implemented the proposed MOPSO algorithm on the multispectral images data set used in our experimental study described in Section 2. Table 1lists the algorithm-specific parameters of proposed MOPSO algorithm. Some of these parameters were determined through numerical experiments after multiple simulation runs. The particle velocity range is kept as mentioned in [37].We run the algorithm for number of times with different values of parameters. Each run of algorithm gives a set of nondominated solution. Fig. 7shows such Pareto-optimal front. The result of different runs of algorithm is listed in Table 2. We have selected the solution having lowest MSE and minimum number of hidden neurons.It shows that the most discriminative feature set obtained is b2, b4, b5 and b6 or b3, b4, b5 and b6. Also the optimal number of nodes in hidden layer should be three. This validates our finding in Section 2 that for this image set the number of hidden neurons must be at least three. Thus for given classification problem, the optimal neural network structure consists of four input neurons, three hidden and four output neurons.In remote sensing applications, availability and collection of ground truth or training sample is a critical issue. Many studies are being done to design classifier to provide robust performance with less number of training samples. As discussed in Section 2, to take the advantage of generalization capacity of neural network, we must prepare training sample set not keeping in mind the size but the quality representation of the classes to be identified. Therefore we trained the above network structure with small training data size of 5, 25 and 50. This neural network is trained with selected input feature pattern i.e. b3, b4, b5 and b6 and then used as classifier.For comparative study, the classification was also done by traditional supervised classifiers: the Euclidean classifier and maximum likelihood classifier (MLC). As shown in Table 3, the overall test sample classification accuracy obtained with proposed algorithm is 95%. For training set size 25 and 50, we do not found improvement in accuracy for respective classifiers. Though quantitatively the difference between the accuracy results of the proposed MOPSO algorithm and the MLC classifiers is not significant, but qualitatively classification provided by our algorithm is much better. The MOPSO algorithm is able to classify the finer details in the image while other classifier fails to do so. The result of classification is shown in Fig. 8. All four classes are well classified and fine structures like road, bridges are also detected. Fig. 9shows grayscale classified image.The XB index are 3.5, 9 and 0.75 for Euclidean, MLC, MOPSO–ANN classifier respectively. The values of β index are 2.2, 2 and 2.3 respectively as shown in Table 3. Thus quantitatively as well as qualitatively our algorithm provides significant improvement in classification compared to both traditional classifiers. Thus even with lower training sample the performance of neural classifier remains robust compared to both traditional classifiers.

@&#CONCLUSIONS@&#
