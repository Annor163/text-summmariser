@&#MAIN-TITLE@&#
BIORV-NSA: Bidirectional inhibition optimization r-variable negative selection algorithm and its application

@&#HIGHLIGHTS@&#
A bidirectional inhibition optimization r-variable negative selection algorithm (BIORV-NSA) is proposed to generate less mature detectors and cover more “black holes”.BIORV-NSA algorithm is composed of two sub algorithms that are self set edge inhibition strategy and detector self-inhibition strategy.When doing comparative experiments, BIORV-NSA is divided into two kinds of algorithms, which are a bidirectional weak inhibition optimization r-variable negative selection algorithm and a bidirectional strong inhibition optimization r-variable negative selection algorithm, respectively.

@&#KEYPHRASES@&#
Bidirectional inhibition optimization r-variable negative selection algorithm,Self set edge inhibition strategy,Detector self-inhibition strategy,Detection rate,Detector self-tolerance,

@&#ABSTRACT@&#
The original negative selection algorithm (NSA) has the disadvantages that many “black holes” cannot be detected and excessive invalid detectors are generated. To overcome its defects, this paper improves the detection performance of NSA and presents a kind of bidirectional inhibition optimization r-variable negative selection algorithm (BIORV-NSA). The proposed algorithm includes self set edge inhibition strategy and detector self-inhibition strategy. Self set edge inhibition strategy defines a generalized radius for self individual area, making self individual radius dynamically be variable. To a certain extent, the critical antigens close to self individual area are recognized and more non-self space is covered. Detector self-inhibition strategy, aiming at mutual cross-coverage among mature detectors, eliminates those detectors that are recognized by other mature detectors and avoids the production of excessive invalid detectors. Experiments on artificially generating data set and two standard real-world data sets from UCI are made to verify the performance of BIORV-NSA, by comparison with NSA and R-NSA, the experimental results demonstrate that the proposed BIORV-NSA algorithm can cover more non-self space, greatly improve the detection rates and obtain better detection performance by using fewer mature detectors.

@&#INTRODUCTION@&#
The idea of applying biological science to solve the problems of computer science and engineering has existed for many years, among which, the artificial immune system (AIS) is one of the most important researches [1]. AIS simulates the function of biological immune system and provides a feasible solution to the complicated problems, which performs very well in the fields of machine learning, pattern recognition and anomaly detection and so on.Research on AIS began in the late 1990s. In December of 1996, the first international symposium on immune system was held in Japan and firstly proposed the concept of AIS [2]. In 1997, Ishida made a comprehensive description on AIS [3]; meanwhile, Dasgupta also published the early work about AIS model and its theory [4]. In 1994, Forrest pioneered the use of AIS theory into the field of computer anomaly detection and proposed a well-known negative selection algorithm (NSA), which is based on the self/non-self discrimination mechanism in the biological immune system and analyzes the relationship between algorithm reliability and the size of detector set from a mathematical viewpoint [5]. In 1998, Chu et al. introduced the mathematical model into the immune algorithm and pointed out the advantages of AIS different from other optimization algorithms [6]. In 2000, de Castro proposed a clonal selection algorithm according to the clonal selection principle [7]. In 2001, Timmis et al. proposed a resource limited artificial immune algorithm, which simulated the race control mechanism in natural biological immune system to control population growth and termination conditions of the algorithm, and it has successfully applied to the knowledge discovery in database and other fields [8]. In 2005, Zhang et al. proposed the radius variable negative selection algorithm (R-NSA) which made radius be variable and decreased the amount of “black holes” existing in the original NSA [9]. In the same year, Tao presented dynamic intrusion detection based on the immune model [10]. In 2008, Timmis deeply analyzed the clonal selection algorithm, immune network and negative selection algorithm, and furthermore proved the usefulness of AIS [11]. In 2009, Zhang et al. regarded the differential evolution method as the mutation operator of immune algorithm and proposed anti idiotypic clonal selection algorithm [12]. After 2010, researches on anomaly detection in AIS are more popular and more optimized algorithms have been proposed out [13–15].In these algorithms mentioned above, NSA proposed by Forrest [5] is one of the important algorithms of AIS that is applied to generate detectors in anomaly detection. Since NSA was firstly conceived, it has attracted many AIS researchers and practitioners and has gone through some phenomenal evolution. For example, NSA algorithms could not achieve a high detection rate as its fixed radius. Notably, R-NSA proposed by Zhang [9] makes the detector radius be variable and improved the detection rate of NSA to some extent. However, R-NSA is still not the most optimal algorithm, and R-NSA has also some inevitable disadvantages. Aiming at the defects existing in NSA and R-NSA, bidirectional inhibition optimization r-variable negative selection algorithm (BIORV-NSA) is proposed in this paper.The remainder of the paper is organized as follows. In the next section, some definitions about AIS and NSA are introduced in detail; then the definite procedure of NSA is elaborated and its merits and disadvantages are also pointed out. In Section 3, the core idea of the proposed BIOR-NSA is illustrated and its details are also given out. Section 4 describes our experiments on artificially generating data and two standard real-world data sets from UCI, and compares BIOR-NSA with NSA and R-NSA. Finally, Section 5 summarizes the whole paper briefly and points out directions for future works.Immunization is the state maintaining process of physical body which relies on antibodies to discriminate self and non-self antigens. In the artificial immune theory, antibodies are defined as detectors which are used to recognize non-self elements, defining Ab as antibody, thus the detection performance depends on the quality of detectors. Antigen refers to the element in training data set, defining Ag as antigen and the collection of all antigen elements is called the antigen set denoted as AG, in which, the normal elements are called self set denoted as SS, the abnormal elements are called non-self set denoted as NS. Regulating SS∪NS=AG, SS∩NS=ϕ[16]. All space that are detected by immune system are defined as SCOPE, the range detected by antibody Ab is denoted as ScopeAb, the effective space of self set is called SCOPESS, self set individual detection space is named as ScopeSs[16]. Not considering the space limiting condition, there exist ScopeAb⊄SCOPE, AG⊂SCOPE, SCOPESS⊂∪ScopeSs[17]. The basic definitions on NSA and AIS required by this paper are defined as follows:Definition 1Affinity: It refers to the matching degree between antibody and antigen, which is usually used to represent recognition threshold of a detector in the detection space. The most commonly used affinity is distance affinity, also called the detector radius. The affinity based on distance or the detector radius is calculated as follows [18]:(1)affinity(Abi,Agj)=∑k∈valid−prop(Abi.propk−Agj.propk)2where Abiand Agjdenote antibody and antigen respectively, propkrepresents the k th attribute of antibody or antigen.[19] Pattern: It is a symbol string consisting of l symbols denoted as X=X1X2X3…Xl, among which symbol Xi(i=1, 2, 3…l) takes 0 or 1 in this paper.Matching rules [20]: At present there are many matching rules such as Hamming distance matching, r-contiguous matching and r-chunk matching etc. The most commonly used is r-contiguous matching and its definition is as follows: for the strings of length L, a=a1a2a3…aLand b=b1b2b3…bLmeet the r-continuous bits matching length, if and only if, ∃i≤L−r+1 makes aj=bj, j=i, i+1, …, i+r−1.Negative selection algorithm is presented to recognize self and non-self according to the recognition principle of the biological immune system, which simulates the immune tolerance of T lymphocytes, randomly produces the detectors and tolerates to eliminate the detectors that can recognize self, thus the rest are kept as the mature detectors that are used to detect non-self individual. The algorithm comprises of the data representation phase, the training phase and the testing phase. In the data representation phase, data are represented in a binary or in a real valued representation. The training phase and the testing phase are introduced as follows:(i)The training phase of the algorithm or the detector generation phase is shown in Fig. 1. If the randomly generating detector R does not match any self individual in self set, then R becomes a mature detector and put it into the mature detector set D, until mature detector set D is eventually formed.The testing phase is shown in Fig. 2. Using mature detector set D to test the inspecting data a, if matching is successful, the inspecting data a is regarded as non-self data, Otherwise, start the next round of judgment.Through analyzing NSA algorithm, it can be observed that NSA has very strong robustness which does not much rely on known data, can identify the unknown abnormal data and it has also inborn ability of parallel execution. But NSA has many shortcomings that the detector radius is generally constant so that non-self area uncovered by some detectors has to rely on other more mature detectors, which leads to require more mature detectors to cover non-self space. The smaller the detection radius, the more detectors are required, and the higher the dimension, the bigger the detector radius is also.In addition, from NSA algorithm, we can know about whether the detector is mature or not depends on the self set, and the operation scale keeps exponential relationship with the scale of self set. The larger the detection space is, the higher the detector generation cost would be. Because mature detectors are randomly generated based on probability theory, there would exist cross-coverage of mature detectors. That is to say, detection range of mature detectors would cover each other. In a very short period of time, the detectors could reach saturation, and lastly it would cause that all non-self area could not be covered completely by mature detectors, the undetectable non-self space are called “black holes”.Aiming at the drawbacks existing in NSA, in order to improve the performance of NSA greatly, this paper presents a bidirectional inhibition optimization r-variable negative selection algorithm (BIORV-NSA). The proposed BIORV-NSA algorithm mainly includes two sub sections that are self set inhibition strategy and detector self-inhibition strategy, among which, self set inhibition strategy makes the detector radius be variable and achieves better coverage of non-self space with the mature detectors; detector self-inhibition strategy avoids mutual cross-coverage among the mature detectors through the candidate detectors tolerating the already existing mature detectors, which reduces the detector generating cost and the number of mature detectors, making the randomly generating process of detectors be controllable. The detailed introductions about self set inhibition strategy and detector self-inhibition strategy are as follows respectively:In AIS, the detection radius of every antibody is variable. Theoretical analysis demonstrates that the smaller the detection radius, the more detectors are required. In the same way, when the number of detectors is fixed, the smaller the detector radius, the not covered “black holes” would be very larger; contrarily, we can get better results. Define the radius of antibody Ab.r also called the detector radius, which is represented as the formula (2):(2)Ab.r=min(affinity(Ab,SS)−SS.r)where SS.r refers to a generalized radius of self individual and it is generalization estimation of the effective self region around self individual. The region taking SS.r as radius is equivalent to “self area” and there only exists self individual in this area. As when the antibody self tolerates, the detecting set is not complete self set, but the training set. There would exist one or more individual detector radius overflow area. That is, for each sub region ScopeSS. If SS.r=0, then SCOPESSwould be equivalent to ∪ScopeSS. The coincidence degree between SCOPESSand ∪ScopeSSdepends on the discrete degree of antigens, the more discrete the antigen, the more different the size of the two region. Therefore, in the process of the antibody self tolerating, the detection radius of each antibody is inhibited by the detection radius of boundary self individual, and the inhibition degree is affected by the discrete degree of antigen. When the inhibition arrives at the generalized radius of self individual SS.r, this value is the inhibition special value. In fact, this inhibition value would be less than the value of self set detection radius; the dynamic radius function is as follows:(3)Ab.r=min(affinityAb,SS−k⋅SS.r)k∈(0,1)Self set edge inhibition strategy is shown in Fig. 3:As Fig. 3 illustrates, it is visible that self set edge inhibition strategy can reduce the mature detectors generating cost to a certain degree. Under the same amount of detectors with NSA, it can cover a wider area, so the size of “black holes” is naturally lowered relatively. Under the strategy of dynamic changing radius, the coverage of detection range can be improved significantly, and improve detection rate indirectly. To some extent, the recognition of the critical antigen near to self individual threshold is solved, and simultaneously reduces the parameters dependence of the whole detection process on the predetermined detection radius.Through executing self set inhibition strategy, the area of “black holes” is obviously reduced, however, the number of needed detectors is still not what we expected and there exist excessive invalid detectors in NSA. As the typical detector generation mechanism in NSA is a randomized algorithm, uncontrollable randomly generated detectors bring great cost to the algorithm performance. According to the random properties of the detectors, there is the following corollary:Corollary 1When the size of antibody set AB reaches a certain degree, there is:On this condition, there exists:(5)affinity(Abi,Abj)+Abi.r<=Abj.rAbi∈ScopeAbjThe relationship between Abiand Abjis shown in Fig. 4:Corollary 1 is also represented as:ScopeAbi⊆ScopeAbj(Abi,Abj∈AB)⇔affinity(Abi,Abj)+Abi.r<=Abj.r∧Abi∈ScopeAbj, which can be proved by contradiction and the detail proof is the following two steps:(1)Necessity of the proposition is proved below:(i)Suppose thatAbi∉ScopeAbj, it is obvious thatScopeAbi⊄ScopeAbj(Abi,Abj∈AB), which is inconsistent with the corollary self.Assume that affinity(Abi, Abj)+Abi.r>Abj.r andAbi∈ScopeAbj, affinity(Abi, Abj) is the distance from the central point of Abito the central point of Abj, asAbi∈ScopeAbj, then the distance between the two central points should be less than or equal to the radius of Abj, that is affinity(Abi, Abj)≤Abj.r. Suppose β=Abj.r−affinity(Abi, Abj), then β is the distance from Abito the boundary of Abj, asScopeAbi⊆ScopeAbj, then β≥Abi.r, that is β−Abi.r≥0. According to the above hypothesis affinity(Abi, Abj)+Abi.r>Abj.r, then there are affinity(Abi, Abj)+Abi.r>affinity(Abi, Abj)+β, the simplification is Abi.r>β, i.e. β−Abi.r<0. It is visible that there exists contradiction with the above β−Abi.r≥0. The proposition's necessity is proved out.Sufficiency of the proposition is proved below:SupposeScopeAbi⊄ScopeAbj(Abi,Abj∈AB), then there are two cases: whenAbi∉ScopeAbj, obviously it is inconsistent with the proposition; whenAbi∈ScopeAbj, note that β=Abj.r−affinity(Abi, Abj), among which, β is the maximum distance from the central point of Abito the edge of Abj. As already supposingScopeAbi⊄ScopeAbj, it illustrates β should be less than the detection radius of Abi, i.e. β−Abi.r<0. AsAbi∈ScopeAbj, there exists affinity(Abi, Abj)≤Abj.r. And because affinity(Abi, Abj)+Abi.r≤Abj.r, then Abi.r≤Abj.r−affinity(Abi, Abj), i.e. Abi.r≤β, that is β-Abi.r≥0, It is visible that there also exists contradiction with the above β−Abi.r<0. Sufficiency of the proposition is proved out.Combining the proposition's necessity with the proposition's sufficiency, the Corollary 1 is proved to be correct lastly.From Fig. 4, it can be observed that not all the detectors can be effectively utilized, the more the number of detectors, for the r-variable detectors, there are more detectors in the state where corollary 1 describes. In order to avoid this problem, referring to the function of immune tolerance, the following strategies are adopted: if the candidate detectors are recognized by the existing mature detectors, the candidate detectors would be eliminated. The inhibition degree depends on the recognition degree. This inhibition can greatly reduce the cost of generating mature detectors. The judgment of the tolerance success is shown in the formula (6):(6)affinity(Abi,Abj)>Abj.r−α⋅AB.r,Abj∈ABFrom the above description about self set edge inhibition strategy and detector self-tolerance inhibition strategy, it can be observed that the proposed optimal algorithm for NSA facilitates two kinds of performance optimization, so the proposed algorithm is called bidirectional inhibition optimization r-variable negative selection algorithm (BIORV-NSA).Although BIORV-NSA algorithm effectively improves the detector coverage rate and decreases the cost of generating the mature detectors. Under this algorithm, if strictly limiting the detector self-inhibition strategy, it would lead to the fact that the generating randomized detectors could no longer mature in a sustained period of time. If the number of mature detectors is strictly limited, we need to ease antibody self inhibition strategy, which would cause the boundary areas to be recovered again.In order to solve the problems existing in BIORV-NSA, the following measures are needed to taken: assuming that the candidate detectors could not mature in continuous time t, and then suppose mature detectors have met the basic requirements. Time t depends on the largest detection radius among the mature detectors and the number of residual requiring generating. Remember the maximum detection span in the SCOPE spatial dimension is SCOPE.max_r, the maximum detection radius of the mature detectors is AB.max_r, and the remaining amount is AB.residual. The required detection quantity N is expressed as follows:(7)N=AB.residual⋅SCOPE.max_r⋅γAB.max_rIn the formula (7), the detector value N is regarded as a measure value of continuous immature individual. If outnumbering this threshold value, then the generation of the mature detectors is terminated. γ represents the mandatory of the coverage extent, as γ increases, the detection amount and the invalid detectors judgment times are both increased.From the above definite descriptions about self set edge inhibition strategy, detector self-tolerance strategy and the regulation of detector self-inhibition, it can be observed that many parameters are used to denote the relative concepts existing in BIORV-NSA. With the difference of inhibition parameters, different inhibition scheme can be formulated for different problems. The core idea of BIORV-NSA is described as follows:Step 1 Self set SS and the self set detection radius SS.r are firstly determined, detector inhibition counter is denoted as CounterABand CounterABis set to zero initially.Step 2 Randomly generating antibody Ab.Step 3 According to self set edge inhibition strategy mentioned above, antibody Ab performs inhibition tolerance on self set SS, if tolerance fails and Ab is recognized by self set, then this Ab is eliminated. Continuously repeat step 2, if the self recognition fails, according to the functions defined in self set edge inhibition strategy, assign dynamic radius SS.r to Ab and then execute self set edge inhibition strategy.Step 4 According to the detector self-inhibition strategy and the regulation of detector self-inhibition, Ab is judged whether there exists cross-coverage with the other mature detectors. If exists overlapping, and when the identification arrives at the threshold value N mentioned in the formula (6), Ab is abandoned, then CounterABincreased. Contrarily, if the inhibition is failed, then Ab is added to AB mature detector set and CounterABreset to 0.Step 5 Repeating steps 2 to step 4 continuously until the mature detector set AB is full or CounterABarrives at threshold value N represented by the formula (6).Fig. 5illustrates the core idea of the BIORV-NSA.From the core idea of the proposed BIORV-NSA, it can be further presented that the boundary antigens can be identified as the introduction of the generalization radius SS.r, and the required mature detectors are greatly reduced because many invalid detectors are eliminated. That is to say, BIORV-NSA performs self set individual dynamic optimization and the detector inhibition optimization for NSA. In order to verify the performance of BIORV-NSA, the following simulation experiments are executed.

@&#CONCLUSIONS@&#
