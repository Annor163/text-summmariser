@&#MAIN-TITLE@&#
A kernel entropy manifold learning approach for financial data analysis

@&#HIGHLIGHTS@&#
A kernel entropy manifold learning algorithm for financial data (MLFD)MLFD employs the information metric to measure the relationships between two financial data points.MLFD yields reasonable and accurate low-dimensional embedding of the original financial data set.The accuracy of the financial early warning is improved by MLFD.

@&#KEYPHRASES@&#
Manifold learning,Financial analysis,Low-dimensional embedding,Information metric,

@&#ABSTRACT@&#
Identification of intrinsic characteristics and structure of high-dimensional data is an important task for financial analysis. This paper presents a kernel entropy manifold learning algorithm, which employs the information metric to measure the relationships between two financial data points and yields a reasonable low-dimensional representation of high-dimensional financial data. The proposed algorithm can also be used to describe the characteristics of a financial system by deriving the dynamical properties of the original data space. The experiment shows that the proposed algorithm cannot only improve the accuracy of financial early warning, but also provide objective criteria for explaining and predicting the stock market volatility.

@&#INTRODUCTION@&#
Traditional financial analysis methodologies include quantitative model and textual analysis. The quantitative model is the analysis about financial data by the use of statistical analysis tools or artificial intelligence technologies, which relies on the selection about basic important factors, such as financial ratios, technical indexes, and macroeconomic indexes [1]. The textual analysis utilizes text mining techniques to analyze the context of financial reports, which are dependent on the identification of a predefined set of keywords [2]. Since different factors or keywords are selected for different studies, the results are often subjective.The real financial indicators are numerous while the complex high-dimensional data tends to obscure the essential feature of data [4]. Identifying intrinsic characteristics and structure of high-dimensional data is important for financial analysis. Inspired by the Quantitative Structure–Property Relationship (QSPR) method [3], whose core idea is that the microscopic structure of a material determines its macroscopic properties, this paper tries to find the inherent relationships between data points of financial dataset, and further derive the overall characteristics of the financial system.Manifold learning, which explores the inherent low-dimensional manifold structure of high-dimensional data, is a valid choice for this task. In the field of financial analysis, data information characteristics, i.e. probability distributions, are important. However, many existing manifold learning algorithms concern about space geometric characteristics [5–8]. When Probability Density Functions (PDFs) are constrained to form a sub-manifold of interest, the straight-shot distance is no longer an accurate description of the manifold distance [50]. For financial data sets, each data point represents a listed company, while the distance between the data points indicates the degree of difference between the financial positions of listed companies. If the difference was characterized only by the geometric space distance between data points, it may not only unfit the practical significance of financial analysis, but also cause problems in the subsequent analysis. Therefore, this study employs the information metric to measure the relationships between listed companies and obtains the relationship metric model.Real-world financial data is often nonlinear [10] and linear mapping manifold learning cannot fully capture the data information. Though Qiao et al. proposed a nonlinear mapping [11], the method is too complicated for the current problem. Kernel is often used to discover nonlinear structure in data [12,13]. The objective of this paper is to propose a kernel entropy manifold learning (KEML) algorithm to obtain the low-dimensional representation of high-dimensional financial data from the perspective of manifold learning. The KEML algorithm is extended to a kernel feature space so that the low-dimensional embedding can reflect the characteristics of the original financial data set. Experiments using small and medium-sized companies from China A-share Stock Market are designed to validate the proposed algorithm.The rest of the paper is organized as follows: Section 2 reviews related works. Section 3 describes the modeling of financial data manifold and the proposed algorithm. Section 4 reports the experimental study and the last section concludes the paper.Over the past few decades, machine learning algorithms have been widely used in the financial field and have been reported to be quite effective in some cases [14]. Machine learning quantitative models include single algorithms, such as ANN [15–17], SVM [18–20] and SOM [21,22], and hybrid techniques, which combine two or more algorithms. Many studies have been conducted to develop hybrid techniques for financial analysis. Serrano-Cinca and Gutiérrez-Nieto [23] combined partial least square (PLS) regression model and principal component analysis (PCA) and multiple linear regression (MLR) for bankruptcy prediction. Yolcu et al. [24] used a hybrid artificial neural network containing linear and nonlinear components for time series forecasting. Kao et al. [25] combined multivariate adaptive regression splines (MARS) and support vector regression (SVR) for stock index forecasting. Lu et al. [26] used independent component analysis (ICA) and support vector regression (SVR) in financial time series forecasting.Context-based text analysis had been used to analyze unstructured data in financial reporting. Groth and Muntermann [27] and Chan and Franklin [2] and Humpherys et al. [28] adopted text mining technology to analyze the unstructured data of financial reports to improve prediction accuracy of financial risk. Schumaker and Chen [29] used textual representations of financial news articles to estimate the discrete stock price. Olson et al. [30] compared data mining methods for bankruptcy prediction.The financial dataset can be considered as a system, in which each data point is an element. The intrinsic relationships between elements constitute the system structure, which determines the characteristics of the system. Inspired by the idea of QSPR, this study tried to explore the intrinsic structure of the system, and then discover the overall status of the system.A manifold is a topological space which is locally Euclidean. High-dimensional data observed in real world are often the consequences of a small number of factors [31]. Manifold learning algorithms assume that the input data resides on or close to a low-dimensional manifold embedded in the ambient space [32]. Thus it is possible to construct a mapping that obeys certain properties of the manifold and obtain low-dimensional representation of high-dimensional data with good preservation of the intrinsic structure in the data [32].Currently dimension reduction techniques are mainly divided into two categories: linear and nonlinear methods. The most well known linear method is principal component analysis (PCA), which is based on correlation matrices [38]. PCA is a classical feature extraction and data representation technique widely used in pattern recognition and computer vision. Sirovich and Kirby utilized PCA to represent pictures of human faces [54]. Turk and Pentland presented the well-known Eigenfaces method for face recognition in 1991 [54]. Kernel PCA (KPCA), a kernel extension of PCA, is also a very influential method. KPCA performs traditional PCA in a kernel feature space, which is nonlinearly related to the input space [38].Compared with traditional dimension reduction approaches, manifold learning has advantages such as nonlinear nature, geometric intuition, and computational feasibility. Many manifold learning methods have been developed over the years. Isometric Feature Mapping (ISOMAP) [6] and Locally Linear Embedding (LLE) [7] are the earliest ones. The key idea of ISOMAP algorithm is to preserve the geodesic distance among points on the manifold and embed data into low-dimensional space by multidimensional scaling. LLE computes the reconstruction weights of each point and then minimizes the embedding cost by solving an eigenvalue problem to preserve the proximity relationship among data.Local tangent space alignment (LTSA) constructs local linear approximations of the manifold in the form of a collection of overlapping approximate tangent spaces at each sample point, and then aligns those tangent spaces to obtain a global parameterization of the manifold [5]. LTSA maps the high dimensional data points on a manifold to points in a lower dimension Euclidean space. This mapping is isometric if the manifold is isometric to its parameter space [5]. Local Multidimensional Scaling (LMDS) is a data embedding method based on the alignment of overlapping locally scaled patches [8] and inputs are local distances. A subset of overlapping patches is chosen by a greedy approximation algorithm of minimum set cover. The patches are aligned to derive global coordinates and minimize a residual measure. LMDS is locally isometric and scales with the number of patches rather than the number of data points. LMDS produces less deformed embedding results than LLE [8].These manifold learning algorithms use geodesic distance metric or weight measurement to calculate similarities between data points. In many problems of practical interest, however, the manifold geometry is unavailable and the calculation of geodesics must be done in a model-free, nonparametric fashion [34]. In applications like financial analysis, for example, only considering the geometry structure of data space may miss some essential characteristics of data and destroy the proximity relations (topology) of the original data space [9].This study adopted an information theory-based metric to measure the difference between data points. Shannon suggested that “information entropy plays a central role in information theory as measures of information, choice, and uncertainty” [35]. Kolmogorov complexity [36] measures information content of an object. Bennett et al. [37] proposed the information distance theory and proved the fundamental universal theorem. Information distance measures the essential relationship between things. Due to its parameter-free, feature-free, and alignment-free characteristics, it can be used to deal with unstructured and incomprehensible data. A distance is a function D with nonnegative real values, defined on the Cartesian product X×X of a set X. It is called a metric on X if for every x, y, z∈X:⋅Dxy=0iffx=ytheidentityaxiom;⋅Dxy+Dyz≥Dxzthetriangleinequality;⋅Dxy=Dyxthesymmetryaxiom.A set X provided with a metric is called a metric space. For example, every set X has the trivial discrete metric D(x, y)=0 if x=y and D(x, y)=1 otherwise [37]. The information metric between stochastic sources X and Y is defined as D(x, y)=H(x|y)+H(y|x) [37]. Here H(x|y) is used to measure the difference between probability distributions.In recent years, entropy-based distance metric has been investigated by the manifold learning field. Costa and Hero [33] proposed geodesic-minimal-spanning-tree (GMST) method that jointly estimates both the intrinsic dimension and intrinsic entropy on the manifold. Jenssen [38] developed kernel entropy component analysis (KECA) for data transformation and dimensionality reduction. KECA reveals structure relating to the Renyi entropy of the input space data set. Carter et al. [34] proposed Fisher Information Nonparametric Embedding (FINE) which utilizes the properties of information geometry and statistical manifolds to define similarities between data sets using Fisher information distance. FINE showed that this metric can be approximated using nonparametric methods. Carter et al. [50] presented methods for low-dimensional representation of information-geometric data and illustrated the methods in flow cytometry and demography analysis.The proposed KEML algorithm is different from the above mentioned methods in the following ways: 1) each data point in a financial data set is regarded as a subset of the probability distribution and all data points constitute a space of probability distributions. It seeks to discover a statistical manifold on a probability density space, while previous algorithms are based on the Euclidean vector space. 2) In the KEML algorithm, information divergence is used for measuring pairwise distance rather than Euclidean distance. 3) Though there are previous manifold learning algorithms using the Rényi entropy, the proposed algorithm utilizes it differently. For instance, Costa and Hero [33] employed the Rényi entropy of the sample points to measure the data compression on the manifold, while the proposed algorithm adopts the Rényi entropy to estimate the distance metric, which was the criteria for the topological relations in high-dimensional data. 4) The construction of the probability density space is different from previous statistical manifold learning algorithms. For example, the Information-Geometric Dimensionality Reduction (IGDR) algorithm proposed by Carter et al. [50] also used the Rényi entropy as the distance metric. However, each index is considered as a class label of a subset with N sample points in the IGDR. Thus the original dataset is divided into d subsets and d is the number of indicators. While in the proposed algorithm, each point xi is considered as a set, in which each feature index is a sample. The original dataset was divided into N subsets.In 1930s, Whitney proposed the embedding theorem which an m-dimensional manifold can be realized in a Euclidean space of dimension 2m+1. A potentially low-dimensional copy of the manifold can be recovered and a bijection exists between the original and its copy [52]. Based on Whitney's theorem, Takens provided a theoretical foundation for the reconstruction of an m-dimensional manifold when only a scalar time series is observable [52]. Li et al. [53] proved that high-dimensional time series can be parsimoniously represented by a dynamical process defined on a low-dimensional manifold.The dynamical model parameters can be learned in the dimensionality-reduced state space [53]. The statistical quantities that characterize the properties of a dynamical system can be obtained through the parameters. Kolmogorov entropy (K entropy), a quantitative measure of uncertainty, is used to describe the degree of system movement disorder or random [47]. The larger the K value, the greater the information loss and the greater the degree of chaos. For random behaviors, K entropy is unbounded when the information is completely lost. For regular motions, K entropy should be zero when no information is generated. For a low-dimensional chaotic dynamics system, K entropy is a finite value greater than zero [47].In this paper, each data set is treated as a dynamical system and the low-dimensional embedding of a dynamical system can be learned through the proposed algorithm. Then the statistical quantity, such as K entropy, can be derived from the low-dimensional embedding to characterize the properties of the dynamical system.The objects in this study are n listed companies (X1, X2, …, Xn), each Xihas D financial indicators (Xi1, Xi2, …, XiD). Each company Xiis a data set consisting of financial indicators, which is Xi=(Xi1, Xi2, …, XiD). χ is a family of data sets χ={X1, …, Xi, …, Xn}(i=1, …, n), where Xi=(Xi1, Xi2, …, XiD). Assume that each data set Xihas an underlying probability distribution function pidetermined by D financial indicators and the parameters are unknown. Then, we can get a collection of Probability Density Functions (PDFs) P={p1, …, pn} which lie on a statistical manifold π. In the statistical manifold π, each element is a probability distribution pi. We try to reconstruct π in the space of probability densities using available information in P. That is to find an embedding A:p(x)→y, where y∈ℝm, m<D. Different from the traditional manifold learning algorithm in Euclidean space, the proposed algorithm is to discover a low-dimensional embedding in the density space, i.e. a statistical manifold of probability distributions.To obtain the low-dimensional embedding from the high-dimensional data sets, pairwise sample distance which measures the amount of information change between data points should be preserved. There is the corresponding Kullback–Leibler divergence KL(P,Q) [50] between any two probability distributions P and Q, whereKLPQ=Elogfxgx=∫fxlogfxgxdx, P and Q are described by the density function f(x) and g(x), respectively. Divergence is an approximate distance function, which meets the non-negative distance definition, but does not satisfy the symmetry and triangle inequality. The Renyi quadratic entropy is h(p)=−log∫p2(x)dx, where p(x) is the probability density function generating the data set, or sample X=x1, x2, …, xN[38]. Since the logarithm is a monotonic function, we may concentrate on the quantity V(p)=∫p2(x)dx. Alternatively, the formula may be formulated as V(p)=εp(p), where εp(g) denotes expectation with regard to the density p(x).To estimate V(p) and h(p), the kernel estimation, given bypx∧∞1N∑i=1Nkx−xi,σ[39], where k(x−xi, σ)=exp(−‖x−xi‖2/σ2), is introduced. The non-parametric estimation of Renyi entropy is:(1)h^p=−log(∫p2x^dx)=−log1N2∑i=1N∑j=1N∫kx−xi,σkx−xj,σdx=−log1N2∑i=1N∑j=1Nkxi−xj,2σ.Formula (1) indicates that the Renyi second order entropy can be decided by the Mahalanobis distance between any two samples in a collection.For n listed companies, each company has D financial indicators. The following information metric model can be formulated:Suppose Pi=(pi1, pi2, …, piD), (i=1, 2,.., n) is the probability distribution vector of the i-th listed company financial indicators, i.e.,(2)pij≥0,∑j=1Dpij=1,i=1,2,…,n.The information metric between any two companies named Renyi divergence:(3)hPiPj=∑m=1Dpimlogpimpjm.Since h(Pi, Pj)≠h(Pj, Pi), the cross entropy does not satisfy the symmetry and can be transformed using the following formula:(4)LethPiPj=h(Pi,Pj)+h(Pj,Pi)=∑m=1Dpimlogpimpjm+∑m=1Dpjmlogpjmpim=∑m=1Dpimlogpim+∑m=1Dpjmlogpjm−∑m=1Dpimlogpjm−∑m=1Dpjmlogpim.piand pjcan be obtained from the above kernel density estimation.Real world financial data sets are nonlinear. This study proposes a kernel extension of the metric to tackle the nonlinear problem. For a given data set X1, X2, …, Xn∈RDwith a positive definite mercer kernel Θ:RD×RD→R, there exists a unique Reproducing Kernel Hilbert Space (RKHS) Ω of real valued functions on RD. Let ϕ:RD→Ω be a feature map from the input space RDto Ω, and Θij=Θ(Xi, Xj)=〈ϕ(Xi), ϕ(Xj)〉=ϕ(Xi)Tϕ(Xj). Let ϕ(X) denotes the data matrix in RKHS, such that ϕ(X)=(ϕ(x1), …, ϕ(xn)). The Euclidean distance between ϕ(Xi) and ϕ(Xj) in the feature space isϕXi−ϕXj=Θii+Θjj−2Θij.Then, in the feature space, the estimated probability density function is(5)px∧∞1N∑i=1Nkϕx−ϕxi,σ,where(6)kϕx−ϕxi,σ=exp−ϕx−ϕxi2/σ2=exp−Θ..+Θii−2Θ.i/σ2.Substitute Eq. (6) with Eq. (5), Eq. (5) can be reformulated as;(7)px∧∞1N∑i=1Nexp−Θ..+Θii−2Θ.i/σ2,and formula (7) can be substituted with Eq. (4). Suppose Pi=(pi1, pi2, …, piD), (i=1, 2,.., n) is the probability distribution vector of the ith-listed company financial indicators, i.e.,(8)pij≥0,∑j=1Dpij=1,i=1,2,…,nh(Pi,Pj) becomes(9)hPiPj=1N∑m=1D∑n=1Nexp(−Θiimm+Θiinn−2Θiimn/σ2log(1N∑n=1Nexp(−Θiimm+Θiinn−2Θiimn/σ2)]+1N∑m=1D∑n=1Nexp(−Θjjmm+Θjjnn−2Θjjmn/σ2log1N∑n=1Nexp−Θjjmm+Θjjnn−2Θjjmn/σ2]−1N∑m=1D∑n=1Nexp(−Θiimm+Θiinn−2Θiimn/σ2log1N∑n=1Nexp−Θjjmm+Θjjnn−2Θjjmn/σ2]−1N∑m=1D∑n=1Nexp(−Θjjmm+Θjjnn−2Θjjmn/σ2log1N∑n=1Nexp−Θiimm+Θiinn−2Θiimn/σ2].In LLE, the local linear structure between the neighbors remains unchanged after dimensionality reduction. For financial data points, the concept and scope of neighbors can be extended. Different from image data sets, adjacency relationships of financial data do not wholly depend on geometric relationships of data points. As stated in Section 3.3, the original data set can be mapped into the linear RKHS by the kernel function. Assume that every data point and its neighborhood data points are located on the same linear manifold. When reproducing low-dimensional manifold, the corresponding data points in the intrinsic low-dimensional space maintain the same global neighbor relationship. To obtain the low-dimensional representation of data sets, KEML algorithm constructs extended local linear structure and preserves the global topological characteristics in the inherent low-dimensional manifold. As mentioned above, the relationship metric hij=h(Pi,Pj) reflects the essential relationships between the financial data points. h11 represented the information distance of data point 1 with itself, h12 represented the information distance between data point 1 and data point 2 and so on. Therefore, we can further obtain the global relationship metric matrixH=h11h12…h1n…………hi1hi2…hinhn1hn2…hnn, which is the reconstruction weight matrix and may be mapped to low-dimensional embedding manifold. The low-dimensional embedding Y reflects the corresponding reconstruction weight relationship of the sample points in the high-dimensional input space. Similar to the LLE method, we obtain the low-dimensional embedding by solving the following optimization problem:(10)MinΦY=∑i=1nYi−∑j=1nhijYj2.To eliminate the coordinates translation, rotation, and scaling factor of the low-dimensional embedding, two constraints are added: (1)∑i=1NYi=0, (2)1N−1∑i=1NYiYiT=I. Furthermore, Eq. (10) can be written asΦY=∑i=1nYi−∑j=1nhijYj2=I−HTYT2=trYMYT, where M=(I−H)T(I−H) is a n×n matrix. In order to minimize the cost function, the low dimensional embedding Y should be taken as the corresponding eigenvectors v1, …, vd+1 to the smallest d+1 eigenvalues of the matrix M, that is Y=[ν2, ⋅⋅⋅, νd+1], and d is determined by Renyi information dimension [51]. Information dimension was defined as follows:d=−limε→0∑i=1NPilogPilogεwhere Pirepresented the probability of a point falling into the i-th unit, and here, it denoted the probability distribution of the i-th company which has been obtained above, ε was the standard body, and N was the number of points. Thus, we can get the intrinsic dimension by information dimension d.In KEML, the reconstruction weight matrix adopts information matrix H, which reflects the essential relationships between the financial data points. As a bridge between high-dimensional observation space and low dimensional embedding space, the matrix H makes the low-dimensional embedding space maintains the original topological relations. KEML algorithm has the global optimal solution, without iteration. The computational complexity of KEML is dominated by three parts: N data points were projected into kernel space by use of kernel function, which the computational complexity was O(N2); calculated the information distance between the data points in kernel space, which the computational complexity is O(N(N−1)); calculated the low-dimensional embedding through the relationship distance matrix H, which the computational complexity is O(N2).The proposed KEML algorithm is summarized as follows (Table 1):Step 1Construct the relationship measure matrix HLet Θ be the N×N kernel matrix with its (i, j) th entry Θij=ϕ(xi)Tϕ(xj)=ϕ(xi, xj). ϕ is a data-independent kernel associated with the kernel matrix Θ. In the KEML algorithm, the Gaussian kernel was adopted for all the kernel-based methods. Compute the relationship model hij, which builds the relationships between financial data points, and then get the relationship matrix H.Calculate the low-dimensional embeddingCompute the eigenvalues of matrix (I−H)(I−H)Tand the corresponding eigenvector Y.Select dataSelect the smallest d non-zero eigenvalues and the corresponding eigenvector Y.The KEML algorithm produces d-dimensional manifold M from the original data set X1, X2, …, Xn∈RD. (R, ρ) is the original space and (M, ρ1) is d-dimensional space, where ρ and ρ1 represent respectively the corresponding space mapping. As known from the preceding analysis, the mapping Φ:R→M meets: (1) Φ is subjective; (2) ∀x, y∈R, ρ(x, y)=ρ1(Φ(x), Φ(y)) is true. Assume (R, ρ) and (M, ρ1) are isometrically isomorphic. According to Whitney's theorem [48], (M, ρ1) can be embedded in (R, ρ). (M, ρ1) is regarded as the reconstructed space of the system S. Since (R, ρ) and (M, ρ1) are isometrically isomorphic, the evolving trajectory of S in the reconstructed space M is diffeomorphism in the original space R. That is, we can restore the original dynamics of the system in the sense of topologically equivalent to maintain its chaotic characteristics, such as the Kolmogorov entropy [47].Assuming the d-dimensional reconstructed space attractor track X(t), P(i1, i2, …, in) is the joint probability when X(t=τ) in the box i1, X(t=2τ) in the box i2,…, and X(t=nτ) in the box in, then K entropy is defined asK=−limτ→0limε→0limn→1nτ∑i1,i2,…,inPi1i2…inlogP(i1,i2,…,in)=limτ→0liml→0limn→∞1nτ∑Kn+1−Kn.Kn+1−Knmeasures the information loss of the system from time n to time n=1. K entropy defines the average loss rate of the system. In the KEML algorithm, we obtain the statistical manifold π={p(x; θ)|θ∈ℝm} of n companies, the joint probability Pi=(pi1, pi2, …, piD) of the i-th company, and the information distance hij=h(Pi, Pj) between any two companies. If each company in the data set is taken as an independent subsystem, we can measure Kn+1−Knby hij, and then K entropy of any company can be obtained as Ki. Furthermore, we can get the K entropy of the system S.In this section, three experiments are conducted to validate the proposed KEML algorithm using data from China A-share Stock Market. All algorithms are implemented in MATLAB 2010Rb.The 2006–2010 annual financial data of small and medium-sized companies (a total of 205) from China A-share Stock Market were chosen from Wind Information Database for the experiment. Twenty-seven financial indicators were selected to reflect six aspects of the companies' financial positions [40] (Table 2).According to [40], the following relationships between financial indicators can be expected. First, there is a positive correlation between solvency and cash flow. It indicates that when cash flows of a listed company are less than adequate, its debt levels are high. Second, there are positive correlations between operating capacity, profitability and ability to grow. Especially, there exists a significant positive correlation between profitability and ability to grow, indicating that the growth rate of a listed company is consistent with its ability to generate earnings. Third, there exist significant negative correlations between cash flow and operating capacity, profitability, and ability to grow. This suggests that when a listed company pursues an increase in net cash flow, its profitability, growth rate and operational turnaround situation will be negatively affected. Fourth, solvency is negatively correlated to operational index, profitability, and ability to grow.The experiments are conducted as follows:First, the KEML are compared with six dimensionality reduction methods: KPCA, LTSA, LMDS, ISOMAP, LLE, and PCA. The quantitative indicator Procrustes Measure (PM) is used to measure the resulting low-dimensional embeddings. As a quantitative indicator, PM is a nonlinear measurement of goodness [42–44] and a smaller PM value indicates a more accurate embedding [42]. MATLAB provides a PM function to compute the corresponding PM values.Second, the resulting low-dimensional embeddings in the first experiment are applied to provide early financial warnings. K-means clustering method is used to divide the low-dimensional embedding into two clusters to identify abnormal companies. The clustering results were examined respectively using F1−scores and risk expectation.Third, the KEML algorithm is used to analyze the overall running characteristics of the stock market. As financial markets can be viewed as a highly complex evolving system [41], the experimental data constitutes a subsystem of the complex system. KEML explores the subsystem geometric space, which makes it possible to further use the relevant analytical tools to study the running characteristics of the system.

@&#CONCLUSIONS@&#
