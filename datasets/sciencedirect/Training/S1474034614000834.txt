@&#MAIN-TITLE@&#
Model-Tree Ensembles for noise-tolerant system identification

@&#HIGHLIGHTS@&#
We introduce a novel method for modeling dynamic systems.Comparable predictive performance to identification methods from control engineering.Output error evaluation on realistic process engineering case studies.Output error results show noise resilience even with up to 20% of noise added.

@&#KEYPHRASES@&#
Decision tree ensemble,Fuzzified model tree,Nonlinear dynamic system identification,

@&#ABSTRACT@&#
This paper addresses the task of identification of nonlinear dynamic systems from measured data. The discrete-time variant of this task is commonly reformulated as a regression problem. As tree ensembles have proven to be a successful predictive modeling approach, we investigate the use of tree ensembles for solving the regression problem. While different variants of tree ensembles have been proposed and used, they are mostly limited to using regression trees as base models. We introduce ensembles of fuzzified model trees with split attribute randomization and evaluate them for nonlinear dynamic system identification.Models of dynamic systems which are built for control purposes are usually evaluated by a more stringent evaluation procedure using the output, i.e., simulation error. Taking this into account, we perform ensemble pruning to optimize the output error of the tree ensemble models. The proposed Model-Tree Ensemble method is empirically evaluated by using input–output data disturbed by noise. It is compared to representative state-of-the-art approaches, on one synthetic dataset with artificially introduced noise and one real-world noisy data set. The evaluation shows that the method is suitable for modeling dynamic systems and produces models with comparable output error performance to the other approaches. Also, the method is resilient to noise, as its performance does not deteriorate even when up to 20% of noise is added.

@&#INTRODUCTION@&#
In this paper, we address the task of identification of nonlinear dynamic systems from measured input–output data. In particular, we address the discrete-time variant of this task, which can be transformed into a regression problem of predicting the next state/output of the system from states and inputs in the recent past. Different regression approaches have been used for solving this task, including neural networks, support vector machines and Gaussian process regression. While most approaches to solving this task try to minimize the one-step prediction error, the learned models are typically evaluated in terms of their simulation (output) error.We explore the use of tree ensemble methods for regression for modeling dynamic systems from measured data. We propose a novel approach for learning ensembles of model trees with randomized attribute selection and fuzzified splits. The approach includes an optimization step of ensemble pruning, which is based on the simulation (output) error criterion. We evaluate the performance of our Model-Tree Ensembles, comparing them to existing state-of-the-art methods used for system identification, focusing on their performance on noisy identification data.The remainder of this section first introduces the task of discrete-time modeling of dynamic systems. It then discusses existing approaches to solving this task and some of their shortcomings. It next discusses tree ensemble approaches for regression that we plan to use for overcoming these deficiencies. The section concludes by laying out the aims of this paper and giving an outline of the remainder of the paper.The task of discrete-time modeling of nonlinear dynamic systems using measured input–output data is to find difference (recurrence) equations using the input variable (u) and output and system variable (y). These equations describe the system at a time instant k using past values of the input and output variables. Through the external dynamics approach [1] the modeling problem is reformulated as a regression task. The value of the system variable(s) at time instantk,y(k), needs to be predicted from the lagged values of the input and system variable(s),u(k-1),u(k-2),…,u(k-n),y(k-1),y(k-2),…,y(k-n)using a static function approximator.The evaluation of the performance of a dynamic system’s model is carried out according to the purpose of the model and often requires a stringent and purpose-specific evaluation. When evaluating a model using one-step-ahead prediction, as shown in Fig. 1(a), the predicted values for the system variable are compared to the measured values. On the other hand, the procedure of simulation, illustrated in Fig. 1(b), introduces one substantial difference: the one-step-ahead model predictions are fed back to the model to produce predictions for the more distant future.While the first step of one-step-ahead prediction and simulation is the same, in simulation, the predicted value of the system variable y at time k (yˆ(k)is fed back as input to the model, instead of a measured value (y(k)) at timek+1. Due to the realistic possibility of error accumulation in the case of an inaccurate model, divergence of the simulation predictions from the measured values may occur as we move further into the future. The cumulative error of simulation is referred to as the output error, while in the case of one-step-prediction the error is referred to as prediction error.The task of discrete-time modeling of nonlinear dynamic systems from measured data can be approached using different modeling techniques. Over the last few decades, numerous different methods have emerged. The earlier approaches included for example the block-oriented Hammerstein and Wiener systems [2] and the semi-parametric Volterra method [3]. More recent approaches include the widely used basis-function approaches of Artificial Neural Networks [1] and fuzzy modeling, as well as the nonparametric approaches of kernel methods [4] and Gaussian Process models [5], to list just a few.Existing approaches for dynamic systems identification can be classified according to the type of model they produce. Some approaches learn one global model describing the whole system, while other learn multiple-models. The following paragraphs describe the features that distinguish methods belonging to the two categories.Methods that build one global model include, for example the well-known Artificial Neural Networks [1], Gaussian Process models [5], and support vector regression [4]. They learn one global model by using a nonlinear optimization procedure on all identification (training) data in a single optimization task. The learned model is valid in the whole operating region.Artificial neural networks, which can be seen as universal approximators, are very powerful and flexible methods also used for modeling (nonlinear) dynamic systems [6]. Different architectures of neural networks exist, the most common ones being multilayer perceptron (MLP) networks and radial basis function (RBF) networks. However, in spite of their advantages, as for the other approaches in this class, their main disadvantages are the lack of transparency and curse of dimensionality [7].Gaussian Process models are nonparametric, probabilistic black-box models that have been used for modeling dynamic systems [5]. One of their advantages is the measure of confidence for the predictions they provide, which helps in assessing the quality of the model prediction at each point. This approach is related to Support Vector Machines and especially to Relevance Vector Machines [5].The multiple model approaches build several local models, each of them valid in a subregion of the whole operating region. They are also referred to as local model networks [8]. They include neuro-fuzzy approaches like the Adaptive Neuro Fuzzy Inference System – ANFIS [9], Local Linear Model Trees – Lolimot [1], and other methods based on the operating regime approach [10].The determination of the subregion boundaries and the subset of features used for each subregion is frequently referred to as structure determination and is usually the first subproblem these methods address. The second subproblem is the identification of local model parameters. As expected, different solutions exist for these two subproblems, while some iterative methods even solve both subproblems at once [1]. Out of the plethora of possible solutions for the structure identification part, the most frequently used techniques are: grid partitioning, tree-based partitioning, fuzzy clustering, product space clustering, genetic algorithms, and partitioning based on prior knowledge [8]. The local model identification subproblem is solved by using either local or global least squares optimization methods, the choice of which has an effect on the accuracy and interpretability of the local models.In practice, the measured input–output data used for identification of the dynamic systems is disturbed by noise. The quality of the identified model is dependent on the resilience of the learning method to noisy data. Existing approaches for modeling from data handle the overfitting-to-noise issue using different strategies: probabilistic strategy, strategies including averaging of predictions, or pruning over-complex models. These meet with a variable degree of success and handling noise remains a challenging issue.Modeling dynamic systems in some areas (such as aerospace engineering) requires high-dimensional models. Further more, discrete-time modeling of such systems, and in particular the introduction of lagged variables using the external dynamics approach, increases the dimensionality of the data used in the identification process. This can present a challenge for many modeling methods.Existing approaches scale differently as data dimensionality (number of variables and number of data points) increases [7]. Artificial Neural Networks with their two most common architectures multilayer perceptron and radial basis functions have as disadvantages the curse of dimensionality and lack of transparency. One of the widely used fuzzy modeling approaches, ANFIS [9], also suffers from the dimensionality problem: the number of parameters that it needs to estimate is proportional topnwhere n is the number of input variables and p is the number of membership functions assigned to each variable. The learning of Gaussian Process models [5] is limited to at most a few thousand data points.Most approaches to modeling dynamic systems minimize prediction error during learning (parameter optimization or structure determination). However, the validation of the learned model is typically performed by simulation. This presents another challenge and raises the question whether it is possible to directly optimize the output error while learning, instead of optimizing the prediction error.In this context, the works of Nelles [1] (cf. also [11]) conclude that a decrease of the prediction error does not necessarily lead to a decrease in the output error of the model, when using neural-networks as models. Similarly, Kocijan and Petelin [12] deal with the same question in the context of Gaussian Process models. They conclude that the direct optimization of output error is a much harder task as compared to the optimization of prediction error, since the optimization surface in the former case contains many local optima and its contour plots depict quite irregular shapes.However, at least some nonlinear identification methods make use of the output error for model structure selection. An example is the Local Linear Model Trees – Lolimot method [1], which iteratively adds complexity to a tree structure. In each iteration, the method solves the parameter optimization problem using least squares estimation, evaluates the intermediate model using simulation, and tries to improve the structure by adding one more node to the tree. The author concludes that the structure search, a nonlinear optimization problem solved by a greedy hill-climbing approach, could benefit from directly estimating the simulation error. This approach is possible because: (a) the iterative nature of the approach means that after each iteration an intermediate solution, i.e., a fuzzy model tree, is ready to be used; and (b) the number of iterations, or total number of nodes in the tree is typically not large, so the time-consuming evaluation of the output error on the whole identification set does not increase the overall time complexity substantially.As outlined above, early research in discrete-time system identification focused on parametric and semi-parametric regression approaches. More recently, non-parametric approaches dominate, including Artificial Neural Networks, kernel methods, and Gaussian Process models. In this context, we propose the use of another non-parametric approach to regression, coming from the area of machine learning, namely tree ensembles.As tree ensembles are a very successful predictive modeling approach, we propose the use of tree ensembles for regression for the task at hand. Ensembles of regression trees have been extensively used and different variants thereof explored (such as bagging and random forests). Bagging of model trees has also been considered.Trees and tree ensembles have a number of properties that directly address some of the challenges mentioned in the previous section. On one hand, this includes the handling of noisy data by tree pruning. On the other hand, this includes the efficiency of tree induction. Finally, this includes the predictive power and efficiency of tree ensembles, such as random forests. The following subsections outline the advantages of using tree based methods and tree ensembles, and present the obstacles that need to be overcome for their successful application to system identification tasks.The issue of overfitting to noise in decision trees and tree ensembles is controlled by the depth of the individual tree or the trees in the ensemble [13]. Larger trees are more sensitive to noise and prone to overfitting, while smaller trees are less sensitive and less prone to overfitting. Tree pruning procedures can be used to reduce the depth of an overly large tree, and control the overfitting to noise.Tree learning algorithms are robust and have the potential to scale well to large predictive modeling problems. The algorithms apply the divide-and-conquer principle, by splitting the available learning data into smaller subsets as the tree is constructed. Thus, the potentially complex optimization problem is broken down to several simpler optimization subproblems. Each optimization subproblem uses a proper subset of the whole set of identification data, so the identification procedure is simplified. This gives the tree learning algorithms the ability to efficiently handle a large number of training instances (identification points).Tree based methods are not sensitive to irrelevant attributes/features included in the data. Recently they have been also extended to deal with multiple dependent/target variables (cf. e.g., [14]), making them suitable to handle (with appropriate modifications) the multi-output case of system identification.Ensembles for regression, also called committees of predictors, are known to improve predictive accuracy. This is known in the field of neural-network ensembles [15] as well as tree-based ensembles [16]. Among the reasons for their success are the smoothing effect on individual model estimates and the reduction of variance of the ensemble [17], as compared to the one of the individual trees.Motivated by the success of the bagging strategy, Breiman [18] concluded that adding randomness within the base learners can be beneficial for the tree ensembles. The randomness increases the diversity of the ensemble and allows for even more accurate predictions. The introduction of randomness into the tree building algorithm is achieved by randomly choosing a different subset of features in each internal node to be used as candidate split variables. The methodology is known as Random Forests [18] and has been shown to perform well as compared to many other methods: discriminant analysis, support vector machines, and neural networks. Such an ensemble method can also handle very large numbers of attributes/features, thus effectively dealing with the curse of dimensionality.However, both bagging and Random Forests have been originally designed to use regression trees. Since bagging only manipulates the training data, and does not randomize the base learners, it can be used with any base learner, including algorithms that learn model trees. The only other ensemble method for regression that utilizes model trees, to our knowledge, is the semi-random model tree ensemble approach [19]. This approach modifies the base-level tree-learning algorithm to produce balanced trees: the number of points falling in each terminal node of the tree is approximately the same. This approach is thus not well-suited for dynamic systems, whose identification is performed on data that are not evenly distributed in the instance space. The partitioning of the instance space using semi-random model tree ensembles would be denser around the equilibrium points, as these regions contain more points than the out-of-equilibria regions. As a consequence, the critical out-of-equilibria regions would be covered by a small number of partitions, resulting in poor approximations.Several researchers using ensembles for regression have concluded that the prediction performance of the ensemble increases when the ensemble structure is optimized. In the context of neural-network ensembles for regression, the work of Perrone and Cooper [20] uses weighted averaging of the base model predictions. The weights are optimized, such that the squared prediction error is minimized. The work of Aho et al. [21] (learning rule ensembles for regression) also includes an ensemble optimization step, which selects the best subset of rules for the ensemble and determines their respective weights.A special case of weighting uses only zero/one weights. In this case ensemble optimization means reducing the number of models in the ensemble. This is called ensemble selection [22] or ensemble pruning [23,24] and has been shown to improve the overall performance of the ensemble.Besides their potential advantages, there are several possible obstacles to the use of trees and tree ensembles for regression for the task of system identification. They include the crisp decision boundaries and the extrapolation behavior. Also, these methods are aimed at optimizing prediction (and not simulation/output) error during learning.The task of modeling dynamic systems requires a modeling procedure which produces a smooth function/mapping between the inputs and the outputs, i.e. a smooth fit. Tree-based models have crisp decision boundaries in the internal nodes, causing non-smooth responses. These discontinuities at the boundaries could increase the prediction error and potentially degrade the simulation performance. One possible approach to solving this problem is to use smoothing with the help of fuzzy set theory.Several different tree learning algorithms have been introduced, which build upon the ideas of fuzzy set theory and produce smooth responses. Most of the related work considers smoothing regression tree predictions (constant predictions of the local models), and only some of it deals with smoothing the predictions of linear model trees. For example, some methods [25] start by learning a crisp tree, convert the splits from crisp to fuzzy, and optimize the fuzzy parameters. Other fuzzy tree methods learn directly both the tree structure and fuzzy parameters [26].Regression trees (and ensembles thereof) have poor extrapolation behavior. For inputs falling outside the range of the identification data, predictions of the model are limited to constant values. One possible solution is the use of linear model trees instead of regression trees. Also, as outlined above, the existing tree and tree ensemble methods optimize the prediction error. We make an effort to optimize the simulation error during the learning of the ensemble model.In this paper, we set out to explore the use of tree ensemble methods for regression for the task of modeling dynamic systems from measured data. Aiming to exploit the above-mentioned strengths of tree ensemble methods and overcome their deficiencies in this context, we propose a novel approach for learning ensembles of model trees. The individual trees are built with randomized attribute selection and have fuzzified splits. The approach also includes an optimization step of ensemble pruning, which is based on the simulation (output) error criterion. We evaluate the performance of our Model-Tree Ensembles, comparing them to existing state-of-the-art methods used for system identification, focusing on their performance on noisy identification data.The remainder of the paper is organized as follows. Section 2 presents the Model-Tree Ensembles (MTE) method for regression. This section describes the proposed MTE algorithm and illustrates the methodology on a simple static function approximation problem. Section 3 presents the experimental evaluation on noisy data, using two data sets, and discusses the empirical results. Section 4 presents further discussion of the suitability of the proposed method for dynamic system identification, states our conclusions and outlines some directions for further work.In this section, we present our approach to learning fuzzified model tree ensembles, which includes randomized attribute selection and ensemble pruning. The Model-Tree Ensembles are illustrated on a simple example. A preliminary version of this approach was considered earlier by Aleksovski et al. [27], however it was evaluated using only prediction error (and not output error). Also, it did not optimize the ensemble structure by performing ensemble pruning, a procedure designed toward optimizing the output error.Three key features distinguish our approach from other existing machine learning approaches. First, the split attribute randomization is unique, as described below. Second, after each tree is learned, the conditions in its internal nodes are fuzzified. Finally, a subset is selected from all the trees learned and only the selected trees form the final ensemble.The remainder of this Section describes the ensemble construction, the model tree learning algorithm and the ensemble selection employed. Finally, it presents the computational complexity of the method and illustrates its properties on a simple modeling problem.The ensemble construction procedure we use is based on the popular bagging approach. It creates M bootstrap replicates, i.e., random samples with replacement, of the identification set, which have an equal number of data points as the identification set. Using each of the M samples, a base learner algorithm is used to build a collection of M model trees:f1,f2,…,fM. Finally, the ensemble structure is optimized, using an ensemble selection procedure, as described in Section 2.3.The final ensemble model is used for prediction by averaging the predictions of each of the base models. The pseudocode describing the ensemble construction and ensemble selection procedures is shown in the upper and lower parts of Table 1, respectively. The base learner algorithm used is a modification of the M5′ model tree algorithm [28] and is described in the next subsection.Model trees are hierarchical structures of nodes, where the inner nodes consist of feature/attribute tests, while the terminal nodes consist of local models: functions of the features. The splits in the inner nodes of the tree perform a partitioning of the instance space into a set of partitions, for which local models are learned. Regression trees have constants in the terminal nodes, while model trees typically use linear models with a constant term. A simple tree is shown in Fig. 2(a), while Fig. 2(b) depicts the partitioning of the two-dimensional instance space represented by this tree.The algorithm that learns a fuzzified model tree with split attribute randomization, to be used as a component model of the ensemble, is based on the M5′ model tree learning method. Its operation can be divided into three phases: tree growing phase, tree pruning phase and tree fuzzification. The pseudocode of the algorithm, including all three phases, is given in Tables 2 and 3. Note that the novel parts we introduce concern the randomized selection of attributes, and the fuzzification of model trees.The tree growing phase is a recursive procedure which creates the initial tree structure. It determines whether to split the set of data, and if splitting is to be performed, also determines the split parameters. The optimal split is chosen from a set of candidate splits created in a randomized fashion as described below.First, a random subset of all features is created, as in random forests [18]. Then, for each candidate attribute in this subset, an optimal cut-point is selected using the standard deviation reduction (SDR) score. Note that the size of the feature subset (K) is a function of the number of available features and this function is a parameter of the algorithm. When the size of the random subset of features is equal to the total number of available features, we eliminate the randomness in the split attribute selection and obtain bagging as a special case.The candidate splits are evaluated using the SDR, i.e., standard deviation reduction [28] score(1)SDR=σD2-DlDσDl2-DrDσDr2where D is the set of identification points falling in the current node,DlandDrare the left and right subsets of the identification data set created by the split, andσD2is the standard deviation of the target attribute in the set D. The split that maximizes this score is chosen for the current tree node. The data points are divided into two subsets, based on the chosen split, and the procedure is recursively applied on each subset.The growing phase stops when either of the two stopping (pre-pruning) criteria are met. The first criterion is low variance of the target variable in a partition, as compared to the overall variance of that variable. The second is small number of identification points in a partition, i.e., less than the value of the algorithm parameter N, which could prevent reliable estimation of linear model coefficients.The tree pruning phase employs a bottom-up post-pruning procedure that reduces the size of the tree. Overly large trees, built in the first phase, are prone to overfitting. Tree pruning reduces overfitting by reducing the tree size. The procedure starts by estimating linear models in all nodes of the tree, using linear regression. The linear model in a node is estimated using only features found in the tests in the subtree rooted at that node. The linear model estimation procedure also includes an attribute removal part: attributes are dropped from a linear model when their effect is small, i.e., expected to increase the estimated error.After estimating linear models in all tree nodes, the pruning procedure evaluates the prediction error of the linear model at each inner node of the tree with the prediction error of the subtree rooted at that node. The prediction error of the linear model learned for a tree node and the prediction error of the subtree below the node are calculated using only data points corresponding to the sub-partition that the node defines. The decision to replace the subtree with a terminal node, i.e., to prune the subtree, is made if the estimated (squared) error of the subtree is larger than or equal to the (squared) error of the linear model.In the pruning phase, the decision to prune is based on evaluation of the prediction error instead of the simulation error (for dynamic system identification). The reason is that the missing data in the bootstrap sample prohibit the calculation of the simulation error. Also, note that as compared to the original implementation of the M5′ algorithm in WEKA [29], we modify two aspects of the algorithm. The first modification concerns the smoothing1Note that the smoothing procedure in M5′ is not related to the smoothing by fuzzyfication that we propose and describe below.1procedure of M5′, which in our experience produces low performing models, and is turned off. The second modification concerns the building of linear models. We stop building a linear model, when a matrix singularity (within the computer’s numerical precision) is detected in the LU matrix decomposition procedure of linear regression. The tree node is converted into a terminal node in this case.To smooth out the discontinuities that each split introduces in the model tree, we implement a split fuzzification procedure. A crisp split of the forms[xj<c], wherexjis the j-th feature, and c is a cut-point, is transformed to a fuzzy split by using a sigmoidal membership functionμ(xj,c,α)with a fuzzy parameterα(the inverse split width):(2)μ(xj,c,α)=11+exp(-α(xj-c))The choice between sigmoidal, Gaussian or triangular membership functions [1] does not have an effect on the identification performance in the case when the function parameters are not optimized globally. So, for the sigmoidal membership function of Eq. (2), the value ofαis calculated such that the overlap between the two subpartitions is equal to a predetermined percentagepoverlapof the size of the partition in the dimension of the split attribute. Larger values forpoverlapmean smoother transitions between the local models. The optimal value ofpoverlapis determined using (internal) cross-validation.The prediction of a fuzzy tree with one fuzzy splitμ(xj,c,α)and two local modelsfLM1andfLM2is calculated by using the following formula:(3)fˆ(x)=μ(xj,c,α)fˆLM1(x)+(1-μ(xj,c,α))fˆLM2(x)The smoothing of the model tree response using fuzzification is performed after the crisp tree is built. An alternative to the efficient divide-and-conquer approach would be to take the fuzzification into account when estimating the linear models. However, this would decrease the efficiency of the M5′ model tree learning algorithm, as the number of data points that have to be considered when estimating a linear model is equal to the total number of identification points.The utilization of linear models in the tree, instead of constants, solves the extrapolation problem, outlined in Section 1.4.3. The predictions of the model, when faced with inputs falling outside the range of the identification data, are no longer limited to constant values: they are linear functions of the input values. Also, by using the tree fuzzification phase, the problem of discontinuities in the model predictions is solved. The smooth switching between local models helps to create smooth simulation output of the model.After the ensemble is built, it is optimized by using a greedy ensemble selection procedure. Trees that do not contribute to the accuracy of the ensemble are removed from the ensemble. A tree’s contribution is evaluated by considering the output error of the reduced ensemble without the tree and comparing its performance to the current ensemble. By evaluating the output error of the ensemble on the identification data (instead of the prediction error), we aim to produce a more successful model of the dynamic system. In the next paragraph we describe the ensemble selection procedure in more detail.The selection procedure operates in a greedy fashion, reducing the ensemble size by one tree in each step, as shown in the lower part of Table 1. It stops when no improvement can be made to the performance of the ensemble. For dynamic systems, simulation on the identification data is performed and the evaluation of the performance of the ensemble is carried out by using the output error function.After the ensemble selection procedure, assume that the resulting ensemble has M trees:E={f1,f2,…fM}. The prediction of the ensemble is a uniformly weighted average of the model tree predictions:(4)f¯(x)=1M∑i=1Mfiˆ(x)The computational complexity of the top-down growing procedure for a single tree isO(P·D·logD), where D is the number od data points (examples) and P is the number of descriptive variables. The complexity of the bottom-up pruning procedure isO(D·P2). This term accounts for the learning of the local linear models in all nodes of the tree. Note that the least-squares estimation of a linear model in a tree node is performed by using only variables found in the tests in the subtree rooted at that node. Finally, the complexity of the ensemble selection procedure depends on the square of the number of trees in the ensemble M (which is set to a constant value).To illustrate how ensembles of model trees are learnt, consider the regression problem of fitting the static nonlinear function(5)f(x)=sin(2πx)+2xusing 100 points(x,y), with x uniformly distributed in the interval [0,1]. Four fitting scenarios are presented. In the first scenario, one model tree is used to approximate the nonlinear function. The second fits an ensemble model composed of 50 model trees to the nonlinear function. The third and the fourth scenario fit one model tree and an ensemble of 50 model trees, respectively, to a noisy version of the data, where white noise with a standard deviation equal to 20% of the target variable deviation was added.The results of the fits are shown in Fig. 3. The top parts of the figure present the approximator fit and the true nonlinear function, while the bottom parts of the figure present the error of the fit:f(x)-fˆ(x). Some properties of the model tree and the ensemble of model tree approximations can be clearly seen from Fig. 3. The single model tree with fuzzy splits, whose approximation is presented in Fig. 3(a), does not produce a good fit around the pointsx=0.3andx=0.7, where the derivative of the function changes sign. The parts of the function where the derivative has the same sign (the intervals (0,0.3), (0.3,0.7), and (0.7,1)) are approximated better, but not perfectly, as shown by the sloped error lines that can be seen in the bottom part of Fig. 3(a).The ensemble of 50 model trees, using randomized splits and averaging of the individual tree predictions (Fig. 3(b)), performs better in the problematic regions. The error on the regions where the derivative changes sign is lowered, and the error lines corresponding to the three intervals are much closer to zero and straight, as depicted in bottom part of Fig. 3(b).On the other hand, the fit to noisy data (Fig. 3(c) and (d)), is not as accurate as in (a) and (b), according to the mean-squared error of the fit: it increases by a factor of approximately 4. However, the ensemble fit improves over the fit of a single tree both for the parts where the derivative changes sign, as well as for the three mentioned intervals. The ensemble fit to the noisy data is mostly smooth and the model is not severely affected by the noisy identification data.This section sets out to evaluate the performance of the Model-Tree Ensembles method proposed above and to compare it with some state-of-the-art methods for dynamic system identification. We are especially interested in its performance in the presence of noise. More specifically, the aim of this section is to address the three following experimental questions:1.Is the MTE method resilient to noise?How does the method compare to selected state-of-the-art methods when identifying dynamic systems from noisy data in controlled conditions?How does the method perform on real-world noisy data?Two data sets (case studies) are used in the experimental evaluation. The first one is a synthetic data set generated from a dynamic system model, to which noise has been artificially added. The second data set presents measured data for a semi-industrial pilot gas–liquid-separator plant.In order to answer the first experimental question, the model tree ensembles method is evaluated on the synthetic data set with artificially added white noise. Besides the data without noise, we also consider data with three different noise levels: 5%, 10%, 20%. When introducing noise at leveln%, we add to each measured valuexiof the variable x noise, which is sampled from the distributionN(0,nσx/100)whereσxis the standard deviation of the variable x.The second question is addressed by a comparison to representative methods, i.e., selected state-of-the-art approaches for modeling nonlinear dynamic systems. In particular, the method is compared to a multilayer perceptron neural network approach, a neuro-fuzzy approach, and a tree-based fuzzy approach. The third question is tackled by using a measured data set originating from a process engineering system.We compare the MTE method with three methods that are well-established in the area of system identification: Neural Networks [4], ANFIS [9] and Lolimot [1]. From the parameter identification perspective, two of the methods utilize global optimization of the parameters: Artificial Neural Networks and ANFIS. On the other hand, Lolimot and the proposed Model-Tree Ensembles use local optimization of the local model parameters.The models that the compared methods learn are of two types: a feed-forward neural-network model and a Takagi–Sugeno fuzzy model. Two different methods that build a Takagi–Sugeno model are compared, since the learning strategies are different. ANFIS uses a separate structure identification step and global optimization of the model parameters, while Lolimot uses an integrated structure identification and local parameter estimation approach. A brief overview of the properties of the methods is given in the following paragraphs.We use feedforward Artificial Neural Networks (NN) [4], more specifically a multilayer perceptron with one hidden layer of neurons, trained by using a backpropagation procedure. The number of neurons in the hidden layer is the only parameter whose value needs selection. We use the Neural Network Toolbox implementation in Matlab. The network training is performed using Levenberg–Marquardt optimization.The Adaptive network based fuzzy inference system (ANFIS) [9] is a hybrid neural-network approach, which builds a Takagi–Sugeno fuzzy model. ANFIS solves the parameter estimation problem by using a hybrid learning rule that combines the backpropagation gradient descent and the least-squares method. The structure identification task – determining the number of fuzzy rules and initial positioning of the fuzzy rule centers is handled by using different methods: grid partitioning of the instance space, fuzzy clustering, or a tree-based approach [30].ANFIS suffers from the curse of dimensionality as the number of input dimension gets larger. In this work, we use the Matlab implementation of the ANFIS method (available in the Fuzzy Logic Toolbox). For the structure identification problem, we utilize the fuzzy c-means clustering method. We do not use the clustering method’s automatic procedure of determining the number of clusters, since (in our experience), it produces sub-optimal models. Instead, we utilize a version of the clustering algorithm with a single tunable parameter: the number of fuzzy clusters.The Local Linear Model Trees (Lolimot) [1] method is a multi-model approach which builds a fuzzy model tree. It solves the structure identification and parameter estimation problems in an integrated, iterative procedure. In each iteration, the method adds one local model to the tree structure and calculates the parameters of the model using local parameter estimation. It has been successfully used for identification of dynamic systems [1].More specifically, Lolimot builds linear model trees with smooth transitions between the affine local models in the tree leafs. It fits a Gaussian basis function in the center of each leaf node, where the standard deviation vector is calculated based on the size of the partition in each dimension. The Lolimot algorithm does not perform pruning, but instead, the building of the tree is halted when the tree size increases beyond a predefined number of leaf nodes.Using Lolimot for dynamic system identification requires determining the optimal lag for the premise part (variables included in split tests) and the optimal lag for the consequent part (variables used in linear models in the tree leafs). The best performing Lolimot tree is obtained by a trial-and-error procedure: running the method with different lag value combinations for the premise and consequent part and evaluating the obtained fit. Additional details about the method are given by Nelles [1].The parameters of the MTE methodology are set and used as follows. The number of trees in the ensemble (M) is set to 50. Our preliminary experiments with different numbers of trees showed no improvement in accuracy by using larger values. Also, the value of the minimal number of identification points in a partition (N), is not crucial for the overall accuracy of the model being learned. From our experience, setting this parameter to its minimal value of 4, results in optimal model performance. On the other hand, the size of the random subset of feature attributes (K), used in the tree growing phase, has an effect on the overall performance of the method. The value of this parameter determines whether the ensemble approach is bagging or Random Forests.After the optimal lag and optimal value for the parameter K have been found, the overlap width parameterpoverlapis optimized. The values considered for this parameter range from 5% to 50%. After the parameter optimization step, the training of the ensembles is carried out, by learning 50 trees. Finally, the ensemble selection procedure is employed to reduce the number of trees in the ensemble. Its usefulness to the model-tree ensemble methodology is empirically demonstrated in Appendix A of the paper.Our choice to use randomness in the split attribute selection, was due to the experience that it can produce models with more diverse predictions. A comparison of the MTE method with or without random split attribute selection (Forests of Model Trees (FMT) vs. bagging of Model Trees (BMT) respectively) is also shown in each of the tables in the Experimental results section (Section 3.6).

@&#CONCLUSIONS@&#
