@&#MAIN-TITLE@&#
A new distance-based total uncertainty measure in the theory of belief functions

@&#HIGHLIGHTS@&#
A new total uncertainty measure in evidence theory is proposed.The new measure is directly defined in the evidential framework.The new measure is not a generalization of those in the probabilistic framework.The belief intervals and distance metric are used for the new measure’s design.The new measure has no drawbacks in traditional ones and has desired properties.

@&#KEYPHRASES@&#
Belief functions,Evidence theory,Uncertainty measure,Belief interval,Distance of interval,

@&#ABSTRACT@&#
The theory of belief functions is a very important and effective tool for uncertainty modeling and reasoning, where measures of uncertainty are very crucial for evaluating the degree of uncertainty in a body of evidence. Several uncertainty measures in the theory of belief functions have been proposed. However, existing measures are generalizations of measures in the probabilistic framework. The inconsistency between different frameworks causes limitations to existing measures. To avoid these limitations, in this paper, a new total uncertainty measure is proposed directly in the framework of belief functions theory without changing the theoretical frameworks. The average distance between the belief interval of each singleton and the most uncertain case is used to represent the total uncertainty degree of the given body of evidence. Numerical examples, simulations, applications and related analyses are provided to verify the rationality of our new measure.

@&#INTRODUCTION@&#
Dempster–Shafer evidence theory (DST) [1], also called the theory of belief functions, has been widely used in many applications related to uncertainty modeling and reasoning, e.g., information fusion [2], pattern classification [3,4], clustering analysis [5], fault diagnosis [6], and multiple attribute decision-making (MADM) [7,8].In the theory of belief functions, there are two types of uncertainty including the discord (or conflict or randomness) [9] and the non-specificity [10], hence the ambiguity [11]. Various kinds of measures for these two types of uncertainty and the total uncertainty including both two types were proposed. The measures of discord include the discord measure [9], the strife [9], the confusion [12], etc; the measures of non-specificity include Dubois and Prade’s definition [10] generalized from the Hartley entropy [13] in the classical set theory, Yager’s definition [14], and Korner’s definition [15], etc. The most representative total uncertainty measures are the aggregated measure (AU) [16] and the ambiguity measure (AM) [11].In essential, no matter AU or AM, they are the generalization of Shannon entropy [17] in probability theory, but not the direct definition in the framework of the theory of belief functions. That is, they pick up a probability according to some criteria or constraints established based on the given body of evidence (BOE), and then calculate the corresponding Shannon entropy of the probability to indirectly depict the degree of uncertainty for the given body of evidence. As mentioned in the related references [11,18], AU and AM have their own limitations. For example, they are insensitive to the change of BBA. These limitations to some extent are related to the inconsistency [19,20] between the framework of the theory of belief functions and that of the probability theory. Therefore, to avoid the limitations of the traditional definitions for the uncertainty measure, in our work, a new total uncertainty measure is proposed directly in the framework of the theory of belief functions without the switching between different frameworks. We analyze the belief interval and conclude that belief intervals carry both the randomness part and the imprecision part (non-specificity) in the uncertainty incorporated in a BOE. Thus, it is feasible to define a total uncertainty measure for a BOE. Given a BOE, the distance between the belief interval of each singleton and the most uncertain interval [0, 1] is used for constructing the degree of total uncertainty. The larger the average distance, the smaller the degree of uncertainty. Since there is no switch of theoretical frameworks, our new definition has desired properties including the ideal value range and the monotonicity. Furthermore, the uncertainty measure can provide more rational results when compared with the traditional ones, which can be supported by experimental results and related analyses.The rest of this paper is organized as follows. Section 2 provides the brief introduction of the theory of belief functions. Commonly used uncertainty measures in the theory of belief functions are introduced in Section 3. Some drawbacks of the available total uncertainty measures including AM and AU are also pointed out in Section 3. In Section 4, a new total uncertainty measure is proposed. Some desired properties and related analyses on the new proposed definition are also provided. Experiments and simulations are provided in Section 5 to show the rationality of our proposed total uncertainty measure. An application of the new total uncertainty measure on feature evaluation is provided in Section 6. Section 7 concludes this work.The basic conception in the theory of belief functions [1] is the frame of discernment (FOD), whose elements are mutually exclusive and exhaustive, representing what we concern.m:2Θ→[0,1]is called a basic belief assignment (BBA) defined over an FOD Θ if it satisfies(1)∑A⊆Θm(A)=1,m(∅)=0When m(A) > 0, A is called a focal element. A BBA is also called a mass function. The set of all the focal elements denoted byFand their corresponding mass assignments constitute a body of evidence (BOE)(F,m).The belief function (Bel) and plausibility function ((Pl)) are defined as(2)Bel(A)=∑B⊆Am(B)(3)Pl(A)=∑A∩B≠∅m(B)The belief function Bel(A) represents the justified specific support for the focal element (or proposition) A, while the plausibility function Pl(A) represents the potential specific support for A. The length of the belief interval [Bel(A), Pl(A)] is used to represent the degree of imprecision for A.Two independent BBAs m1( · ) and m2( · ) can be combined using Dempster’s rule of combination as [1](4)m(A)={0,£A=∅∑Ai∩Bj=Am1(Ai)m2(Bj)1−∑Ai∩Bj=∅m1(Ai)m2(Bj),£A≠∅There are still some other alternative combination rules. See [21] for details. The theory of belief functions has been criticized for its validity [19,20,22–25]. It is not a successful generalization of the probability theory, i.e., there exists inconsistency [20] between the framework of the theory of belief functions and that of the probability theory.In the theory of belief functions, there are two types of uncertainty including the discord (or the conflict or the randomness) and the non-specificity, hence ambiguity [11].Measures of discord are for describing the randomness (or discord or conflict) in a BOE [11]. Available definitions are listed below. Although with various names, they are all for the discord part of the uncertainty in a BOE.(1) Confusion measure [12](5)Conf(m)=−∑A⊆Θm(A)log2(Bel(A))(2) Dissonance measure [14](6)Diss(m)=−∑A⊆Θm(A)log2(Pl(A))(3) Discord measure [26](7)Disc(m)=−∑A⊆Θm(A)log2[1−∑B⊆Θm(B)|B−A||B|](4) Strife measure [9](8)Stri(m)=−∑A⊆Θm(A)log2[1−∑B⊆Θm(B)|A−B||A|]Note that all these definitions can be considered as Shannon entropy-alike measures. More detailed information on these measures can be found in [9].Non-specificity [15,27] means two or more alternatives are left unspecified and represents an imprecision degree. It only focuses on those focal elements with cardinality larger than one. Non-specificity is a special uncertainty type in the framework of belief functions theory when compared with the probabilistic framework. Some non-specificity measures [10,14,15] were proposed. The most commonly used non-specificity definition is [10](9)NS(m)=∑A⊆Θm(A)log2|A|It can be regarded as a generalized Hartley measure [13] from the classical set theory. When the BBA m( · ) is a Bayesian BBA, i.e., it only has singleton focal elements, it reaches the minimum value 0. When BBA m( · ) is a vacuous BBA, i.e.,m(Θ)=1,it reaches the maximum value log2(|Θ|). This definition was proved to have the uniqueness by Ramer [28] and it satisfies all the requirements of the non-specificity measure.(1) Aggregated Uncertainty (AU) [16](10)AU(m)=max[−∑θ∈Θpθlog2pθ]s.t.{pθ∈[0,1],∀θ∈Θ∑θ∈Θpθ=1Bel(A)≤∑θ∈Apθ≤1−Bel(A¯),∀A⊆ΘIn fact for AU, given a BBA, the probability with the maximum Shannon entropy under the constraints established using the given BBA is selected and its corresponding Shannon entropy value is defined as the value for AU. Therefore, it is also called as “upper entropy” [29]. It is an aggregated total uncertainty (ATU) measure, which captures both non-specificity and discord. AU satisfies all the requirements [29] for uncertainty measure including probability consistency, set consistency, value range, monotonicity, sub-additivity and additivity for the joint BBA in Cartesian space.(2) Ambiguity Measure (AM) [11](11)AM(m)=−∑θ∈ΘBetPm(θ)log2(BetPm(θ))whereBetPm(θ)=∑θ∈B⊆Θm(B)/|B|is the pignistic probability [30] of a BBA. In fact AM uses the Shannon entropy of the pignistic probability of a given BBA to represent the uncertainty.The traditional total uncertainty measures have their own drawbacks. AM cannot satisfy the sub-additivity (for joint BBA in Cartesian space) which has been pointed out by Klir and Lewis [18]. Moreover, AM is criticized due to its logical non-monotonicity [29] as shown in Example 1.Suppose that the FOD is {θ1, θ2, θ3}. There are two BBAs as follows:m1({θ1,θ2})=1/3,m1({θ1,θ3})=1/2,m1({θ2,θ3})=1/6;m2({θ1,θ2,θ3})=1/3,m2({θ1,θ3})=1/2,m2({θ2,θ3})=1/6.Obviously, from the two BBAs, there existsBel2(A)⩽Bel1(A),Pl1(A)⩽Pl2(A),∀A⊆Θ.That is to say, all the belief intervals of m1 can be contained by those corresponding belief intervals of m2, which means that m2 has a higher level of uncertainty. We can calculate their corresponding AM and AU as follows:1.5546=AM(m1)⩾AM(m2)=1.5100;AU(m1)=AU(m2)=1.5850=log23.We can see that the results of AM are counter-intuitive, because AM violates the monotonicity, i.e., AM decreases the total quantity of uncertainty in this example where a clear increment of uncertainty of m2 compared with that of m1. Although AU does not bring out a decrease of the quantity of uncertainty, it generates two equal values, which is also counter-intuitive, because m2 should have higher level of uncertainty. We provide further analyses on this example. The belief function and plausibility function of m1 and m2 areBel1({θ1})=0.0000,Pl1({θ1})=0.8333;Bel1({θ2})=0.0000,Pl1({θ2})=0.5000;Bel1({θ1,θ2})=0.3333,Pl1({θ1,θ2})=1.0000;Bel1({θ3})=0.0000,Pl1({θ2})=0.6667;Bel1({θ1,θ3})=0.5000,Pl1({θ1,θ3})=1.0000;Bel1({θ2,θ3})=0.1667,Pl1({θ1,θ3})=1.0000;Bel1(Θ)=1.0000,Pl1(Θ)=1.0000;Bel2({θ1})=0.0000,Pl2({θ1})=0.8333;Bel2({θ2})=0.0000,Pl2({θ2})=0.5000;Bel2({θ1,θ2})=0.0000,Pl2({θ1,θ2})=1.0000;Bel2({θ3})=0.0000,Pl2({θ2})=0.6667;Bel2({θ1,θ3})=0.5000,Pl2({θ1,θ3})=1.0000;Bel2({θ2,θ3})=0.1667,Pl2({θ1,θ3})=1.0000;Bel2(Θ)=1.0000,Pl2(Θ)=1.0000.Since AU tries to find a probability maximizing the Shannon entropy, and the uniform p.m.f. (probability mass function)P(θ1)=P(θ2)=P(θ3)=1/3,which has the maximum Shannon entropy, satisfies all the constraints (shown in Eq. (10)) established using Bel1, Pl1 and those using Bel2 and Pl2. Therefore, both AU1 and AU2 reach the maximum valuelog23=1.5850.Suppose that the FODΘ={θ1,θ2}. A BBA over Θ ism({θ1})=a,m({θ2})=b,m({Θ})=1−a−b,where a, b ∈ [0, 0.5]. Here, we calculate AU, AM values corresponding to different values of a and b. The change of AU and AM values with the change of a and b are illustrated in Fig. 1.As shown in Fig. 1, AM reaches its maximum whena=b,because whena=b(no matter a or b’s value is large or small), the pignistic probability is uniformly distributed. This is counter-intuitive, because AM cannot discern the effect of total set’s mass assignment values to the uncertainty degree. For example,m1({θ1})=m1({θ2})=0.5andm2({θ1})=m2({θ2})=0.25,m2(Θ)=0.5,there existsAM(m1)=AM(m2). Obviously, it is irrational to say that m1 and m2 have the same degree of total uncertainty. AU never changes with the change of a and b. The value of AU is alwayslog22=1. The reason is analyzed as follows. The constraints for calculating AU in this example areBel({θ1})=a⩽P(θ1)⩽1−b=Pl({θ1});Bel({θ2})=b⩽P(θ2)⩽1−a=Pl({θ2});Bel(Θ)=1−a−b⩽P(θ1)⩽1=Pl(Θ);Since AU tries to find a p.m.f. with the maximum Shannon entropy, and the uniformly distributedP(θ1)=P(θ2)=0.5always satisfies the constraints above (because a, b ∈ [0, 0.5]), no matter how a and b change,P(θ1)=P(θ2)=0.5is always picked up when calculating AU and thus, AU always equals tolog22=1. However, intuitively, the degree of uncertainty should change with the change of a and b. So, AU cannot well describe the degree of total uncertainty here.As we can see above, both AM and AU borrow the uncertainty measure (Shannon entropy) in the probability theory framework and they both have drawbacks. However, as aforementioned, the belief functions theory is not a successful generalization of the probability theory. There exists inconsistency between the two frameworks. We do not prefer such a switch between different frameworks, which might cause problems in representing the uncertainty in belief functions. Therefore, in our work, we design the total uncertainty measure directly in the framework of belief functions theory as introduced in the next section.The belief interval [1] [Bel(A, Pl(A)] of a given focal elements A in a BBA m can represent its uncertainty degree as analyzed below.Case I: The most uncertain case isBel(A)=0andPl(A)=1,i.e., the belief interval is [0,1].Case II: The clearest case isBel(A)=1andPl(A)=1(A is assure to occur) orBel(A)=0andPl(A)=0(A is assure to never occur).Case III: WhenBel(A)=a,Pl(A)=b,∀a, b ∈ (0, 1), the belief interval is [a, b]. For A, the degree of imprecision can be represented byb−a,the probability of whether A occurs or not (randomness) is large than a and less than b.Case IV: WhenBel(A)=Pl(A)=a,∀a ∈ (0, 1), the belief interval is [a, a]. It means that A has no imprecision and whether A occurs or not cannot be clearly determined (it is with the randomness, i.e., the probability a).Therefore, the information related to the uncertainty carried by a belief interval both include the randomness part and the imprecision part (non-specificity). Thus, given a BBA, we can fully utilize the information of the belief intervals to measure the total uncertainty of the BBA. Then, how to use the information of belief intervals?Given the belief interval of A, i.e., [Bel(A), Pl(A)], if the belief interval is farther from the most uncertain case [0,1], then A has smaller uncertainty; if the belief interval is nearer to the most uncertain case [0,1], then A has larger uncertainty. Then, how to describe the distance between belief intervals? Here we use the distance between interval numbers [31,32] as follows. Given two interval numbers [a1, b1] and [a2, b2], a strict distance is defined as(12)dI([a1,b1],[a2,b2])=[a1+b12−a2+b22]2+13[b1−a12−b2−a22]2The belief interval [Bel(A), Pl(A)] can be considered as an interval number. If we replace [a1, b1] by [Bel(A), Pl(A)] and [a2, b2] by [0, 1], then, the distance obtained can be used to represent how far it is from A to the most uncertain case. As shown in Eq. (12), in dI, the proportion of[(Bel({θi})+Pl({θi}))/2−1]2and[(Pl({θi})−Bel({θi}))/2−1]2is 1/3. Generally speaking, an interval number is a kind of fuzzy data [31]. To define the distance between two fuzzy data, the value of such a proportion depends on the shape [31] (could be the symmetric triangular, normal, parabolic, square root fuzzy data, and interval number) of the fuzzy data model. Here, we use the belief intervals (interval numbers), therefore, there is a rational and natural assumption that the interval is with a uniform distribution. Under such an assumption, the proportion should be 1/3 and dIcorresponds to Mallows’ distance between two distributions, hence a strict distance metric for interval numbers. By using such a strict distance metric for interval numbers, we can further define a total uncertainty measure for a BOE as follows.Suppose that m is a BBA over the FODΘ={θ1,…,θn}. First, calculate the belief interval [Bel({θi}), Pl({θi)}] ,i=1,…,nfor each singleton θi. Then, calculate the distance between each [Bel({θi}), Pl({θi)}] and [0, 1], which represents the degree of departure from the most uncertain case for each singleton. Since larger distance (larger departure from the most uncertain case) means smaller total uncertainty, one minus the normalized and averaged belief interval distances (for all singletons) is used as the total uncertainty measure for the BBA m as shown in Eq. (13)(13)TUI(m)=1−1n·3·∑i=1ndI([Bel({θi}),Pl({θi})],[0,1])where3is the normalization factor11The distance dI([Bel({θi}), Pl({θi})], [0, 1]) reaches its maximum value when the belief interval is [0,0] or [1,1]. Therefore, the normalization factor is1/dI([0,0],[0,1])=1/dI([1,1],[0,1])=3..The newly proposed TUIhas no drawbacks as those of AU and AM pointed out in Examples 1 and 2. See the Examples 3 and 4 in Section 5 for details.Here, we provide an illustrative example to show how TUIis calculated.Suppose that FOD isΘ={θ1,θ2,θ3}. A BBA over Θ ism({θ1})=0.3,m({θ2,θ3})=0.5,m(Θ)=0.2.First calculate the belief function and plausibility function for singletonsBel({θ1})=m({θ1})=0.3,Bel({θ2})=m({θ2})=0.0,Bel({θ3})=m({θ3})=0.0,Pl({θ1})=m({θ1})+m(Θ)=0.5,Pl({θ2})=m({θ2,θ3})+m(Θ)=0.7,Pl({θ3})=m({θ2,θ3})+m(Θ)=0.7.Then, calculate the distance between each belief interval of singleton and the interval [0, 1]dI([Bel({θ1}),Pl({θ1})],[0,1])=dI([0.3,0.5],[0,1])=[0.3+0.52−0+12]2+13[0.5−0.32−1−02]2=0.2517;dI([Bel({θ2}),Pl({θ2})],[0,1])=dI([0,0.7],[0,1])=[0+0.72−0+12]2+13[0.7−02−1−02]2=0.1732;dI([Bel({θ3}),Pl({θ3})],[0,1])=dI([0,0.7],[0,1])=[0+0.72−0+12]2+13[0.7−02−1−02]2=0.1732;thenTUI=1−3/3·(0.2517+0.1732+0.1732)=0.6547.The range of TUIis [0, 1], which is desired for practical use. For a vacuous BBAm(Θ)=1,all the belief intervals for singletons are [0, 1]. Therefore, by using Eq. (13), its degree of total uncertainty is 1, which is the maximum. For a categorical BBAm(θi)=1,the belief interval for θiis [1, 1] and those of other singletons are [0, 0]. Therefore, by using Eq. (13), its degree of total uncertainty is 0, which is the minimum.Proof of the range: For a vacuous BBA, when using Eq. (13),TUI=1. On the other hand, ifTUI=1,i.e., all the belief intervals for singletons are [0, 1], the corresponding BBA must be a vacuous one. So the maximum value is unique. If a BBA is not a vacuous one, then some singletons’ belief intervals can be strictly contained by [0, 1]. Therefore, according to Eq. (13), the corresponding TUIwill be smaller than 1. Therefore, the unique maximum value of TUIis 1.For a categorical BBAm(θi)=1,when using Eq. (13),TUI=0. On the other hand, ifTUI=0,the only possible belief interval for singletons are either [0, 0] or [1, 1]. For a BBA, one and only one belief interval for singleton could be [1, 1]. So, the corresponding BBA should bem({θi})=1,m({θj})=0,∀j≠i,which represents the clearest case. If a BBA is notm({θi})=1,m({θj})=0,∀j≠i,some singletons’ belief intervals are neither [0, 0] nor [1, 1]. Therefore, according to Eq. (13), the corresponding TUIwill be larger than 0. So, the minimum value of TUIis 0.End of ProofGiven an uncertainty measure UN and two BBAs m1 and m2, if∀A∈P(Θ):Pl1(A)⩽Pl2(A),Bel1(A)⩾Bel2(A)or∀A∈P(Θ):[Bel1(A),Pl1(A)]⊆[Bel2(A),Pl2(A)]there exists UN(m1) ≤ UN(m2), then it is said that UN satisfies the monotonicity [29]. The physical meaning of the monotonicity is that an uncertainty measure in the theory of belief functions must not decrease the total quantity of uncertainty in situations where there is a clear decrease in information (increment of uncertainty).Proof of the monotonicity: For two BBAs defined over the same FOD, if it holds that∀A∈P(Θ):[Bel1(A),Pl1(A)]⊆[Bel2(A),Pl2(A)],then, there exists ∀θi∈ Θ: [Bel1({θi}), Pl1({θi})]⊆[Bel2({θi}), Pl2({θi})]. According to the property of the distance of interval numbers in Eq. (12), there exists dI([Bel1({θi}), Pl1({θi})], [0, 1]) ≥ dI([Bel2({θi}), Pl2({θi})], [0, 1]).Then, according to Eq. (13), TUI(m1) ≤ TUI(m2), i.e., the monotonicity holds.End of ProofIt is declared in [11,29] that AU and AM satisfy the probability consistency and set consistency.Probability consistency: When m is a Bayesian BBA (all focal elements are singletons), AU reduces to Shannon entropy in probability theoryAU(m)=AM(m)=−∑θ∈Θm({θ})log2(m({θ}))Set consistency: When m focuses on a single set (i.e.,m(A)=1,∀A⊆Θ), AU reduces to a Hartley measure in classical theoryAU(m)=AM(m)=log2(|A|)It should be noted that it is meaningless to apply these two requirements or properties to our new measure. First, our new TUIis designed directly in the framework of belief functions theory, but not a generalization of the measures in probability framework or those in classical set theory. Second, the theory of belief functions is not a successful generalization of the probability theory. It cannot degenerate back to the probability framework. For example, a Bayesian BBA is not a probability but still a special BBA, which cannot satisfy the additivity for mutually exclusive events and cannot satisfy the unity for the total set in Kolmogorov probability axioms [33]. Therefore, the validity and strictness of such a consistency is doubtful.Furthermore, AU also satisfies the sub-additivity and additivity, which defined for the joint BBA in Cartesian space. In our work, we do not concern the joint BBA in Cartesian space, therefore, we will not analyze the sub-additivity and additivity in this paper.We will further verify our new total uncertainty measure by examples and simulations in the next section.This Example 3 is a revisiting of Example 1. Using Eq. (13), we can obtain thatTUI(m1)=0.6667andTUI(m2)=0.7778,i.e., m2 has a higher level of uncertainty, which is intuitive. It can be proved that TUIsatisfy the monotonicity. The proof can be found at Section 4.3.2.This Example 4 is a revisiting of Example 2. Our new total uncertainty measure TUI, non-specificity measure NS, dissonance measure Diss are also calculated. The results are shown in Fig. 2.AU never changes as analyzed in Example 2, which is counter-intuitive. AM values never changes, so far asa=bholds, which is also counter-intuitive as aforementioned in Example 2. As we can see in Fig. 2, TUIbrings out rational results. It reaches the maximum whena=b=0,i.e.,m(Θ)=1. Whena+bis fixed, the maximum values are reached ifa=bholds. For example, suppose thata+b=0.3. The TUIvalues are shown in Fig. 3. This makes sense, becausem(Θ)=1−(a+b)is fixed, the non-specificity part is fixed. Whena=b,the conflict part reaches its maximum value.Note that NS reaches 0 (minimum), whena=b=0.5(m(Θ)=0); it reaches 1 (maximum), whena=b=0(m(Θ)=1). Diss reaches 1 (maximum), whena=b=0.5; it reaches 0 ( minimum), whena=0orb=0(no conflict).Suppose that the FOD isΘ={θ1,θ2,θ3}and a BBA over Θ ism(A)=1,∀A=Θ. We make changes to the BBA step by step. In each step, m(Θ) has a decrease ofΔ=0.05and each singleton massm({θi}),i=1,2,3has an increase of Δ/3. Finally, m(Θ) reaches 0 andm({θi})=1/3. We calculate total uncertainty measures including TUI, AM, AU, non-specificity NS, and dissonance (Diss) at each step. The changes of uncertainty values are shown in Fig. 4.As we can see in Fig. 4, with the change of the BBA in each step, the non-specificity changes from the maximum to zero. This is intuitive, because the mass assignments are transferred from the focal elements with larger cardinality to those with smaller cardinality. The measure for conflict, i.e., Diss here, becomes larger and reaches the maximum in the final. This is also intuitive, because the BBA changes from a vacuous BBA (no conflict) to a Bayesian BBA (with uniform distribution on all singletons) in the final. The traditional total uncertainty measures AU and AM never change and they are always at the maximum value. This is counter-intuitive because the total uncertainty degree of a vacuous BBA should be the largest, and thus, it should be larger than that of a Bayesian BBA (even if, it is a Bayesian BBA with uniform distribution on all singletons). The reasons for the irrational results of AU and AM are analyzed as follows. They are both defined based on some probabilistic transformation from BBAs. In this example, for the probabilistic transformation used in AM and AU, the results are always a uniformly distributed probability mass function (p.m.f.):P(θi)=1/3,i=1,2,3,therefore, AM and AU will never change here. Our new measure TUIprovides rational results. It becomes smaller and smaller when the BBA changes from a vacuous one to a Bayesian one as shown in Fig. 2. Therefore, according to our opinion, it is inappropriate to define total uncertainty measure in the theory of belief functions by indirectly using the uncertainty measure in probability framework, i.e., Shannon entropy. It should be better not to switch the framework but to directly design in the framework of belief functions theory. This is also the motivation of our work in this paper.TheFOD isΘ={θ1,θ2,θ3}. A BBA over Θ ism(A)=1,∀A=Θ. We make changes to the BBA step by step. In each step, m(Θ) has a decrease ofΔ=0.05and mass assignment of one singleton m({θ1}) has an increase ofΔ=0.05. In the final step,m(Θ)=0andm({θ1})=1. We calculate total uncertainty measures including TUI, AM, AU, non-specificity measure NS, and dissonance (Diss) of the BBA at each step. The changes of these uncertainty values are shown in Fig. 5.The BBA changes from a vacuous one to a categorical one. As shown in Fig. 5, non-specificity NS becomes smaller and smaller, which is intuitive. The conflict part Diss is always 0, which is also intuitive, because at most two focal elements are available for the BBA in each step (including {θ1} and Θ), therefore, there is no conflict. AU, AM, TUIall decrease intuitively. However, AU is not sensitive to the BBA’s change at the beginning stage (nearly unchanged). This has already been criticized by Jousselme et al. [11].Examples 7–9 are used for reference from [11]. Suppose that the FOD isΘ={θ1,…,θ8}. A BBA over Θ ism(A)=1,∀A=Θ. We make changes to the BBA step by step. In each step, m(Θ) has a decrease ofΔ=0.05,and the mass assignment for one focal element B with cardinality s < 8 has an increase ofΔ=0.05. In the final step,m(B)=1. Here, we can sets=2,4,6,respectively. Given an s value, we repeat the whole procedure of BBA change. Under the different s, the changes of values for uncertainty measures including TUI, AM, AU, NS, and Diss are shown in Fig. 6.As shown in Fig. 6, when the mass assignments are transferred to a smaller size focal element, the non-specificity NS intuitively decreases. The total uncertainty AU, AM, TUIall decrease. If s has a smaller value, the total uncertainty and the non-specificity will decrease faster, because the mass assignments are transferred to a smaller size focal element. Comparatively, AU is not sensitive to the change of the BBA. The conflict part (Diss) is always 0. The reason is analyzed as follows. At the beginning, the BBA is a vacuous one; in the middle steps, the BBA has two focal elements including the total sets; in the final step, it has only one focal element. Therefore, there is no conflict.Suppose that the FOD isΘ={θ1,…,θ10}. A BBA over Θ ism(Θ)=1−a,andm(A)=a,|A|=s⩽10. Given an a, and the initial|A|=1,we make changes to the BBA step by step. |A| increases by 1 in each step. In the final, the BBA becomes a vacuous one. We seta=0.3,0.5,0.8,respectively. Given an a value, we repeat the whole procedure of BBA change. Under the different a, the changes of values for uncertainty measures including TUI, AM, AU, NS, and Diss are shown in Fig. 7.As shown in Fig. 7, non-specificity NS, total uncertainty measures including AU, AM, and our TUIall intuitively increase with the change of BBA at each step. If a has a larger value, they will increase faster, because relatively more mass assignments are transferred to a larger size focal element. Comparatively, AU is not sensitive to the BBA’s change. The conflict part (Diss) is zero and never changes. The reason is analyzed as follows. Before the final step, the BBA has two focal elements including the total sets; in the final step, it has only one focal element. Therefore, there is no conflict.Suppose that the size of an FOD is|Θ|=5. Randomly generate 10 BBAs withk,1⩽k⩽25−1focal elements using the algorithm [11] in Table 1.In this simulation, the number of focal elements is set to 15. Those generated BBAs are then combined one by one using Dempster’s rule of combination in Eq. (4). At each instant t of the evidence combination, the values of NS, AU, AM, Diss, and TUIare calculated. The whole procedure is repeated for 100 times and the averaging values of these different uncertainty measures at different t are shown in Fig. 8. It is shown that all the uncertainty measures, except for Diss, decrease with the increase of the combination steps. This makes sense, because it is intuitive that the total uncertainty decreases in the information fusion procedure such as the evidence combination. The non-specificity measure drops faster and more significantly than the total uncertainty measures. This is because that in the evidence combination based on Dempster’s rule, the focal elements are split into focal elements with smaller cardinality. Also, since the focal elements are split into smaller size focal elements, the conflict in the BBA will significantly increase at the beginning steps. At the final steps, since there is less further split of focal elements and the consensus effect in combination becomes more dominant, the conflict in the BBA decreases slightly as shown in Fig. 8.Here we use our new total uncertainty measure in feature evaluation for pattern classification to further show its rationality.Three classes of samples are artificially generated. There are 200 samples in each class. Each sample has four dimensions. In each class, each dimension of samples is Gaussian distributed with different mean and standard deviation (Std for short) as shown in Fig. 9and Table 2.As shown in Fig. 9 and Table 2, since the three Gaussian probability density functions (PDFs) in feature 2 are quite well separated, the class discriminability of feature 2 is the best; the class discriminability of feature 4 is the worst (totally overlapped), and that of the feature 3 is also very bad. The class discriminability of feature 1 is in the middle. This can also be verified by using the discrimination criterion [34] as follows:(14)J=tr(Sw)tr(Sb)where tr denotes the trace of a matrix. Suppose that there are C classes and each class Cihas Nisamples. The degree of inner-class cohesion Swand the degree of inter-class separability Sbare as follows [34]:(15){Sw=∑i=1CP(Ci)E[(X−1Ni∑X∈CiX)(X−1Ni∑X∈CiX)T]Sb=∑i=1CP(Ci)(1Ni∑X∈CiX−M)(1Ni∑X∈CiX−M)Twhere X is a feature (vector) of a sample and(16)M=1C∑i=1C(1Ni∑X∈CiX)is the mean of all the classes’ centroids. If J of some feature (or set of features) in Eq. (14) is smaller, then such a feature (or set of features) is more crisp and discriminable.For the four dimensions of the artificially generated samples, there existsJ(1)=0.2666,J(2)=0.1336,J(3)=488.9156,J(4)=981.3525It means that in terms of discriminability, feature 2 is the best; feature 4 is the worst; feature 3 is also very bad, and feature 1 is in the middle, which is accordant to the judgment based on the Gaussian parameters.Now, we use different total uncertainty measures (including AM, AU, and our new TUI) for the feature evaluation. Intuitively, the feature with smaller total uncertainty measure should be better (higher discriminability).First, we generate BBA from each sample xqin the three-class artificial samples on different feature i ∈ {1, 2, 3} according to [35](17)mxqi(Aj)=|Aj|−α/(β−1)dqj−2/(β−1)∑Ak≠∅|Ak|−α/(β−1)dqk−2/(β−1)+δ−2/(β−1)where parametersα=1,β=2as suggested in [34]. Here we give an illustrative example as in Fig. 10.In Fig. 10, three different colors represent three different classes. c1 denotes the centroid of samples in class 1; c1, 2 denotes the centroid of samples in class 1 and class 2; c1, 2, 3 denotes the centroid of samples in classes 1, 2, and 3. Calculate the distance d between xqand those centroids of single classes and compound classes. Then according to Eq. (17), the BBA can be generated.Second, we calculate AU, AM, and our proposed TUIfor all the generated BBAs (corresponding to each sample on different features). Then, calculate the average values of AU, AM and TUIfor different features. The results are shown in Table 3.We consider the average total uncertainty measures as scores for different features. The feature with smaller score is better (with high discriminability)As shown in Table 3 the feature evaluation based on the average TUIvalues is accordant to the results obtained using discrimination criterion. That is, feature 2 is the best; feature 4 is the worst; feature 3 is also very bad, and feature 1 is in the middle.AMcan also provide the correct feature evaluation result. Note that feature 3 is more ambiguous than feature 4. The difference of average TUIfor features 3 and 4 is 0.0003; The difference of average AM for features 3 and 4 is also 0.0003. However, the value range of TUIis [0, 1], while the value range of AM is [0, log23]. Therefore, our proposed TUIis more sensitive when measuring the ambiguity in different features, which agrees with the analysis in above examples.The average AU values for both features 3 and 4 are equal. They are both the maximum valuelog23=1.5850. The reason is analyzed below.According to the BBA generation approach used, because in features 3 and 4, all the centroids (7 centroids, 3 singleton classes, and 4 compound classes) are very close, all the distances between xqand those centroids are almost equal when generating BBAs for xqon features 3 and 4. Therefore, each focal element of the BBA generated for xqon features 3 and 4 has the mass value approximating to 1/7. According to all the BBAs generated, for feature 3,m(A)=(1/7)±0.010; for feature 4,m(A)=(1/7)±0.007,A⊆{c1,c2,c3}. According to Eq. (10), since AU is calculated based on the maximization of Shannon entropy given the constraints established using belief functions as17≈m({c1})︸Bel({c1})≤P(c1)≤m({c1})+m({c1,c2})+m({c1,c3})+m({c1,c2,c3})≈47︸Pl({c1})17≈m({c2})︸Bel({c2})≤P(c2)≤m({c2})+m({c1,c2})+m({c2,c3})+m({c1,c2,c3})≈47︸Pl({c2})17≈m({c3})︸Bel({c3})≤P(c3)≤m({c3})+m({c1,c3})+m({c2,c3})+m({c1,c2,c3})≈47︸Pl({c3})P(c1)=P(c2)=P(c3)=1/3,which corresponds to the maximum Shannon entropy, always satisfies the constraints, so, the AUvalue is fixed tolog23=1.5850. Based on AU, we cannot judge which one is better for features 3 and 4.In summary, our proposed TUIcan be well used in feature evaluation for pattern classification, and it performs better than AM and AU.

@&#CONCLUSIONS@&#
In this paper, a new total uncertainty measure based on the distance of belief intervals is proposed in the framework of belief functions theory. There is no switch between frameworks in the new measure. It is based on a new perspective that for a body of evidence, the farther it is from the most uncertain case, the less uncertainty it carries. It has been experimentally shown that our new measure can rationally represent the total uncertainty in BOEs. The new measure also has some desired properties and behaves more rational than traditional measures like AU and AM in some cases as shown in our experiments and simulations. Furthermore, our new measure can provide more rational results in practical applications such as the feature evaluation.Till now, the theory of belief functions is still not so solid and needs further refining. The available works mainly focus on the uncertainty modeling and reasoning in belief functions theory and there have emerged many valuable results. However, the performance evaluation in belief functions theory is far from mature, which has become its bottle neck for the further development. In future work, we will focus on performance evaluation in the theory of belief functions and try to use our new total uncertainty measure to implement the theoretical evaluation and the application-based evaluation in the theory of belief functions.