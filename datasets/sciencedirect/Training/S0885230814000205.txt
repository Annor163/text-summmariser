@&#MAIN-TITLE@&#
Improving recognition of proper nouns in ASR through generating and filtering phonetic transcriptions

@&#HIGHLIGHTS@&#
Accurate phonetic transcription is more difficult to obtain for proper nouns than for regular words.The method consists in a process that extracts phonetic transcriptions from actual utterances, and then filters them.We were able to obtain a decrease of the Word Error Rate (WER) on segments of speech with proper nouns.Results on segment without proper nouns were not affected.

@&#KEYPHRASES@&#
Speech recognition,Phonetic transcription,Proper nouns,SMT,Moses,G2P,

@&#ABSTRACT@&#
Accurate phonetic transcription of proper nouns can be an important resource for commercial applications that embed speech technologies, such as audio indexing and vocal phone directory lookup. However, an accurate phonetic transcription is more difficult to obtain for proper nouns than for regular words. Indeed, phonetic transcription of a proper noun depends on both the origin of the speaker pronouncing it and the origin of the proper noun itself.This work proposes a method that allows the extraction of phonetic transcriptions of proper nouns using actual utterances of those proper nouns, thus yielding transcriptions based on practical use instead of mere pronunciation rules.The proposed method consists in a process that first extracts phonetic transcriptions, and then iteratively filters them. In order to initialize the process, an alignment dictionary is used to detect word boundaries. A rule-based grapheme-to-phoneme generator (LIA_PHON), a knowledge-based approach (JSM), and a Statistical Machine Translation based system were evaluated for this alignment. As a result, compared to our reference dictionary (BDLEX supplemented by LIA_PHON for missing words) on the ESTER 1 French broadcast news corpus, we were able to significantly decrease the Word Error Rate (WER) on segments of speech with proper nouns, without negatively affecting the WER on the rest of the corpus.

@&#INTRODUCTION@&#
Proper nouns constitute a special case when it comes to phonetic transcription, at least in French, which is the language used for this study. Indeed, there is much less predictability in how proper nouns may be pronounced than for regular words. This is partly due to the fact that, in French, pronunciation rules are much less normalized for proper nouns than for other categories of words: a given sequence of letters is not guaranteed to be pronounced the same way in two different proper nouns.The lack of predictability also finds its roots in the wide array of origins proper nouns can come from: the more foreign the origin, the less predictable the pronunciation, with variations covering the whole range from correct pronunciation in the original language to a Frenchified interpretation of the spelling.The high variability induced by this low predictability is a source of difficulty for Automatic Speech Recognition (ASR) systems when dealing with proper nouns. For an ASR system, being confronted with a proper noun pronounced using a phonetic variant very remote from any variant present in its dictionary is a situation similar to encountering an unknown word, if the language model cannot compensate for the acoustic gap. Such errors can have a strong impact on word error rate (WER): according to a comparative study of out-of-vocabulary impact of words in spontaneous and prepared speech (Dufour, 2008) the recognition error on an out-of-vocabulary word propagates through the language model to the surrounding words, causing a WER of about 50% within a window of 5 words to the left and to the right (again, in French). This highlights that the influence of the quality of the phonetic dictionary of proper nouns extends further than just the recognition of proper nouns themselves. It is particularly true in the case of applications where proper nouns are frequently encountered, such as transcription of broadcast news. However, aside from its potential impact on WER, accurate recognition of proper nouns can also be very important—independently of the frequency of their occurrence—in other contexts such as in the case of automatic indexing of multimedia documents, or transcription of meetings.Setting up a phonetic dictionary of proper nouns (or any other class of words) requires grapheme to phoneme (G2P) conversion, be it manual or automatic. Automatic G2P conversion techniques are widely studied in the literature. Strik and Cucchiarni (1999) present an overview of techniques in 1999 and propose to classify the G2P systems into two categories: the knowledge-based approaches, which use existing linguistic knowledge to derive pronunciations, and the data-driven approaches, which derive pronunciation models from acoustic data. Knowledge-based approaches are further divided between formalized (e.g. rule based) and non-formalized (e.g. dictionary lookup). de Calmès and Pérennou (1998) propose a dictionary look-up strategy (non-formalized knowledge-based). Béchet (2001), Tihoni and Pérennou (1991), and Réveil et al. (2012) present rule-based knowledge-based techniques. Réveil et al. (2012) propose a rule-based strategy that integrates different type of features (orthographic, syllabic, morphological, …) to describe the rule context. A large variety of knowledge-based techniques are proposed in the literature: Torkkola (1993), Ma and Randolph (2001), Jensen and Riis (2000), and Seng et al. (2011) propose local classification strategies and Galescu and Allen (2001), Bellegarda (2005), and Bisani and Ney (2008) propose some pronunciation-by-analogy approaches. Many data-driven (acoustic-based) strategies can also be found in the literature (Holter and Svendsen, 1999; Byrne et al., 1998; Deligne and Mangu, 2003; Svendsen et al., 1995).We propose an acoustic-based method to build a dictionary of phonetic transcriptions of proper nouns by using an iterative filter to retain the most relevant parts of a large set of phonetic variants, the latter being obtained by combining three G2P methods with extraction from actual audio signals (Laurent et al., 2009).In this work for French, we compare three different G2P systems to initialize the process, and we use a two-level iteration to converge on the best filtered dictionary. In order to reduce noise, an iterative filter is applied to invalidate the variants that are deemed irrelevant because never used, and the ones that are found to be too prone to generating confusion with other words.First, related works will be presented. After proposing an overview of the method, we will focus on the grapheme-to-phoneme systems used to initialize the process. In the next part, the proposed method will be described, and before concluding, experiments and results will be introduced and commented on. The intermediate (before filtering) and final sets of phonetic transcriptions are evaluated in terms of Word Error Rate (WER) and Proper Noun Error Rate (PNER), computed over the corpus of French broadcast news from the ESTER evaluation campaign (Galliano et al., 2005).

@&#CONCLUSIONS@&#
