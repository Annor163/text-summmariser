@&#MAIN-TITLE@&#
Combining data and meta-analysis to build Bayesian networks for clinical decision support

@&#HIGHLIGHTS@&#
We focus on complex clinical problems where data is available in small amounts.Our methodology compensates for the lack of data by using published evidence.It combines multivariate data with univariate meta-analysis to build BN models.The method is illustrated by a medical case study on trauma care.Our method outperformed data-driven techniques and MESS model in the case study.

@&#KEYPHRASES@&#
Clinical decision support,Bayesian networks,Evidence-based medicine,Evidence synthesis,Meta-analysis,

@&#ABSTRACT@&#
Complex clinical decisions require the decision maker to evaluate multiple factors that may interact with each other. Many clinical studies, however, report ‘univariate’ relations between a single factor and outcome. Such univariate statistics are often insufficient to provide useful support for complex clinical decisions even when they are pooled using meta-analysis. More useful decision support could be provided by evidence-based models that take the interaction between factors into account. In this paper, we propose a method of integrating the univariate results of a meta-analysis with a clinical dataset and expert knowledge to construct multivariate Bayesian network (BN) models. The technique reduces the size of the dataset needed to learn the parameters of a model of a given complexity. Supplementing the data with the meta-analysis results avoids the need to either simplify the model – ignoring some complexities of the problem – or to gather more data. The method is illustrated by a clinical case study into the prediction of the viability of severely injured lower extremities. The case study illustrates the advantages of integrating combined evidence into BN development: the BN developed using our method outperformed four different data-driven structure learning methods, and a well-known scoring model (MESS) in this domain.

@&#INTRODUCTION@&#
It is a challenge to build effective decision-support models for complex clinical problems; such problems involve multiple interacting factors [1,2] and to account for both the factors and their interaction a ‘multivariate’ model is needed [3]; these can have many forms: our focus is on Bayesian networks. In general, a multivariate model can be built in a number of ways: (1) purely from data using statistical and machine learning techniques [4], (2) from a combination of clinical knowledge and data [5–7] or (3) from published literature using multivariate meta-analysis techniques [8]. Each of these techniques has been shown to be successful in certain conditions but in this paper, we focus on clinical problems where none of these techniques is sufficient, on its own, to build a useful decision support model. That is, our focus is on problems that are complex, important but also rare: their rarity makes it hard to collect very large datasets (so called ‘big data’); their complexity demands a sophisticated multivariate model but their importance ensures that a large number of relevant research studies is available.In these domains, the first method of building models – purely from data – results in simple models that cannot deal with the complexity of the problem [1] because there is not enough data to support a complex model. The third approach fails because clinical studies rarely publish information detailed enough for multivariate meta-analysis [9]. Instead, many medical studies report ‘univariate’ relations between a single factor and an outcome. Randomised controlled trials, for example, analyse the effect of a single treatment by using randomisation to decrease the confounding effect of other variables [10]. Similarly, many observational studies report the relation between individual risk factors and outcomes even when their dataset contains information about multiple factors. The second approach – combining knowledge and data – could work but it ignores the large body of published evidence; our challenge is therefore to exploit the results of a meta-analysis of studies reporting univariate relations to supplement a dataset that is otherwise inadequate to support a complex multivariate model.Decision support directly from univariate relations is limited, as the effects of interactions between variables are not taken into account. For example, evidence about individual effects of a treatment and a comorbidity factor can be analysed in separate meta-analyses. However, if the treatment and comorbidity factor interact with each other, their joint effect may be completely different from their individual effects. As a result, decision support provided by the meta-analysis of individual effects may be invalid for a patient who is exposed to both the treatment and the comorbidity factor (see [2,10,11] for a more detailed discussion of generalising clinical evidence).To improve this situation, we propose a method of combining the results of meta-analyses, clinical knowledge and data to provide decision support for complex decision problems where the data is scarce. Our method combines ‘univariate’ meta-analysis following a systematic review, with a small ‘multivariate’ dataset and expert knowledge. Bayesian networks (BN) offer a powerful framework to combine evidence from different sources [1,5,12,13]. Our methodology integrates the evidence from a meta-analysis into BN development by using it first to identify the BN structure and then to help determine the BN parameters; this second step uses auxiliary parameter learning models similar in some ways to techniques that can be used for meta-analysis. We illustrate the application and results of this method with a clinical case study into the prediction of the outcomes of severely injured lower extremities.In the remainder of this paper, Section 2 recaps of a Bayesian meta-analysis technique to combine probabilities. Section 3 describes our methodology for developing a BN based on the results of a meta-analysis. Sections 4–6 present the case-study, results and conclusions respectively.The method we propose in Section 3 assumes a possibly small multivariate patient dataset is available together with univariate results of a meta-analysis of probabilities. In this section, we give a recap of the meta-analysis of probabilities by briefly presenting an existing Bayesian technique [14,15]. The results obtained from this meta-analysis technique can be used in the method of Section 3, though other techniques could also be used. The recap also serves to introduce hierarchical Bayesian models, which are also used in Section 3.Meta-analysis is an important form of clinical evidence as it combines and summarises the relevant published evidence that is identified by a systematic literature review. Meta-analysis can be used to combine different types of statistics including odds ratios, risk ratios and probabilities [14]. We focus on the meta-analysis of probabilities as the parameters of a BN are composed of probabilities. Fig. 1shows a random-effects Bayesian meta-analysis model that takes the variation between studies into account, and does not assume normality for the distribution of the individual studies.The binomial distribution is the probability distribution of the number of positive outcomes in n independent experiments where the probability of a positive outcome is p for every experiment. In the meta-analysis model, the result of each individual study i is modelled with the binomial distribution shown below, where riis the number of positive outcomes observed in the study i, piis the true study probability of the study i, and niis the sample size of the study i.ri∼Binomial(pi,ni)The normal distribution is a convenient way of modelling the pooled estimate and the variation between studies. We use an inverse logit transformation to model the true study probability piwith the normal distribution. The mean μ of this distribution represents the transformed pooled estimate, and the variance τ2 represents the variation between studies.logit(pi)=θiθi∼Normal(μ,τ2)The predictive probability distribution can also be calculated by using an inverse logit transformation of this normal distribution. The predictive distribution is a recommended way of presenting the results of a meta-analysis as it represents the uncertainty from both the pooled estimate and the variation between studies (see [14] and chapter 8 of [16] for more detailed information on predictive distributions in meta-analysis).θnew∼Normal(μ,τ2)logit(pnew)=θnewFinally, priors must be selected for the pooled estimate and between-study standard deviation. The non-informative priors shown below can be used if informative priors are not available.μ∼Normal(0,1000)τ∼Uniform(0,2)In order to calculate the posteriors of μ, τ2 and pnew, we enter the observed number of positive outcomes riand sample sizes nifrom each reviewed study. The posteriors can be calculated by using the dynamic discretisation algorithm [17] in AgenaRisk [18] or the Markov Chain Monte Carlo (MCMC) sampling technique in OpenBUGS [19].The previous section described a Bayesian meta-analysis technique for pooling probabilities. In this section, we present a methodology that uses data, expert knowledge and the pooled probabilities from a meta-analysis to define the structure (Section 3.1) and parameters (Section 3.2) of a BN decision support model. Our methodology assumes that expert knowledge, a meta-analysis of univariate relations from a relevant systematic review and some data about multivariate relations are available. However, the amount of data may be insufficient to learn the parameters of some relations in the BN.A BN structure can be developed in two stages: selecting variables, and identifying the relations between those variables. Domain experts use the results of a meta-analysis to select the important variables for the BN. The experts review every variable that is considered to be clinically important in the meta-analysis. During the review, they define mechanistic relations between each of these variables and the outcome. These definitions enable us to (1) build a causal BN structure that is consistent with clinical knowledge and (2) identify the variables that are clinically important considering the aims and scope of the model. Variables that are outside the scope of the model are excluded even when they have a clinically significant effect in the meta-analysis.The mechanistic relations between the observed factors and outcome may depend on clinical factors that are not available in the data or not examined in the meta-analysis. For example, the data may not distinguish between the measurements and true state of a variable, or may exclude a part of the important causal factors in the domain (see [6] and chapters 1 and 2 of [20] for a more detailed discussion of this issue). In this case, latent variables are used to model clinical knowledge in the BN.Meta-analysis of a univariate relation provides a probability conditioned on a single variable, such as P(Y|X1). Such probability distribution cannot be directly used for a BN variable that is conditioned on multiple parents such as P(Y|X1,...,Xn). In this section, we present a parameter learning method for combining the results of a univariate meta-analysis and data to learn the parameters of a BN variable that has multiple parents. Our method uses auxiliary Bayesian models to learn the parameters of the BN used for decision support. These auxiliary models are hierarchical models with a structure that is conceptually similar to the Bayesian meta-analysis model described in Section 2. We introduce the proposed method by a simple example in Section 3.2.1, and examine the application of this method to more complex BN models in Section 3.2.2. The conditional probabilities provided by a meta-analysis may be relevant to variables that are not directly linked in the BN structure. We examine this issue in Section 3.2.3.In this section, we introduce our parameter learning method with the simple BN shown in Fig. 2.This BN has 3 variables and each of its variables has 2 states:X1=x11,x12X2=x21,x22Y={y1,y2}The conditional probability distribution (CPD) of a discrete variable is encoded in a node probability table (NPT) in a BN. Table 1shows the NPT of the variable Y. We require 4 parameters for this NPT:P(y1|x11,x21),P(y1|x11,x22),P(y1|x12,x21)andP(y1|x12,x22).The parameters of the variable Y can be learnt from data using the maximum likelihood estimate (MLE) approach. For example,P(y1|x11,x21)) can be estimated by dividingM[y,x11,x21]toM[x11,x21], whereM[y,x11,x21]represents the count of data instances where Y=y1,X1=x11andX2=x21, andM[x11,x21]represents the count of data instances whereX1=x11andX2=x21.Py1|x11,x21=M[y1,x11,x21]M[x11,x21]Suppose we have a dataset, with a sample size of M=250, to learn the parameters of the BN in Fig. 2. Fig. 3shows a part of the relevant counts from this imaginary dataset. There are only 3 data instances where Y=y1,X1=x11andX2=x21as shown byM[y1,x11,x21]=3in this figure.Our aim is to estimate the parameters of the BN. Although the overall sample size of the data is not small, there is not an adequate amount of data for learning some of the parameters. For example, there are only a few data instances to learn the probability ofP(y1|x11,x21)sinceM[y1,x11,x21]=3andM[x11,x21]=10.As well as the data, suppose we have the results of a meta-analysis that analyses the relation between Y and X1. This meta-analysis pools the conditional probabilities ofP(y1|x11)reported in different studies. The result of the meta-analysis is reported by the mean,μpnew(y1|x11), and variance,σpnew2(y1|x11), of the predictive distribution of the pooled conditional probability (see Table 2). A way of calculating these statistics is described in Section 2.The results of the meta-analysis cannot be directly used for the BN parameters since the variable Y is conditioned on both X1 and X2 in the BN model whereas it is conditioned only on X1 in the meta-analysis. In other words, there is no parameter to useP(y1|x11)directly in the NPT of the variable Y (see Table 1).In the remainder of this section, we present a novel technique that combines the data shown in Fig. 3 and the meta-analysis results shown in Table 2 to learn the parametersP(y1|x11,x21)andP(y1|x11,x22)for the NPT of the variable Y. The generalisation of this method for a larger number of parents and states is described in Section 3.2.2.Fig. 4shows a BN representation of the implemented technique. The BN representation is divided into five components that are described in the remainder of this section:1.Data: This part uses the binomial distribution to model the relation between the conditional probability distributions (CPD) that we aim to estimate and the observed counts in the data. For example, the number of data instances whereX1=x11,X2=x21and Y=y1, shown byM[y1,x11,x21], has a binomial distribution where the probability parameter isP(y1|x11,x21)and the number of trials parameter isM[x11,x21]. The binomial distributions used in this part are shown below:My1,x11,x21∼BinomialMx11,x21,Py1|x11,x21My1,x11,x22∼BinomialMx11,x22,Py1|x11,x22M[x21]∼BinomialM,P(x21)M[x22]∼BinomialM,P(x22)Probability distributions for NPT: This part contains the CPDs that we aim to estimate for the NPT of Y. We assign uniform priors for these distributions, informative expert priors can also be used when available:P(y1|x11,x21)∼Uniform(0,1)P(y1|x11,x22)∼Uniform(0,1)Marginalisation of NPT distributions: Since the variable Y is conditioned only on 1 variable in the meta-analysis and 2 variables in the BN, we model the probability distribution from the meta-analysis,P(y1|x11), as the marginalisation of the probability distribution from the BN parametersP(y1|x11,x21)andP(y1|x11,x22):P(y1|x11)=∑X2(P(y1|x11,X2)*P(X2))=P(y1|x11,x21)P(x21)+P(y1|x11,x22)P(x22)Probabilities required for marginalisation: In order to calculate the marginalisation in part 3, we need the probability distributions ofP(x21)andP(x22). We assign uniform priors for these variables. We also assign a constraint to ensure that the sum ofP(x21)andP(x22)is equal to 1.P(x21)∼Uniform(0,1)P(x22)∼Uniform(0,1)∑X2P(X2)=P(x21)+P(x22)=1Values from meta-analysis: The pooled estimateμpnew(y1|x11)from the meta-analysis is modelled with the normal distribution truncated to a unit interval as it represents a probability value, denoted by TNormal[0,1] (μ,σ2). We useP(y1|x11)from the marginalisation in part 3 andσpnew2(y1|x11)from the predictive distribution as the mean and variance of this normal distribution respectively. The values from the meta-analysis are modelled as:μpnew(y1|x11)∼TNormal[0,1](P(y1|x11),σpnew2(y1|x11))In the following section, we describe the generalisation of this technique for estimating the parameters of variables with more parents or states.Let Y be a BN variable that has n parents, andX={X1,X2,…,Xn} be the set of parents of Y (see Fig. 5). Both Y and its parents have multiple states:Y={y1,…,yk}Xi={xi1,…,xik}Our dataset contains a total of M data instances aboutXand Y (see Table 3). We also have pooled conditional probability and variance estimates of the predictive distribution of P(Y|Xi) from a meta-analysis (see Table 4). A way of calculating these predictive distributions is described in Section 2.Fig. 6shows an abstract graphical illustration of the generalised auxiliary parameter learning model. This model is a generalisation of the model shown in Fig. 4. This illustration is not a BN; it is a schema for building an auxiliary parameter learning model for any number of states and parent variables. The size of the auxiliary parameter learning model grows rapidly with increasing number of parents and states.In Fig. 6, the variables shown by ellipses are unknown variables that will be estimated by the model. The variables shown by rounded rectangles are observed with the values from the meta-analysis, and the variables shown by rectangles are observed from the dataset. The constraints that sum probabilities to 1 are not included in this figure to simplify the illustration. By running this auxiliary model, we estimate probability distributions for the parameters P(Y|X) required by the NPT of Y. Since the BN requires only a point estimate of the parameter, not the entire distribution; we use the mean of this distribution as the BN parameter.According to our model, the data related to Y, i.e. M[Y,X], is generated by the binomial distribution with the probability of success P(Y|X) and the number of trials M[X].M[Y,X]∼Binomial(M[X],P(Y|X))M[Y,X] represents the count of data instances for specific values of X1,…,Xnand Y. For example,M[y2,x11,x23,…,xn4]represents the number of data instances whereY=y2,X1=x11,X2=x23,…,Xn=xn4. Similarly M[X] represent the number of data instances where X1,…,Xnhave certain values.Our aim is to estimate the CPD of P(Y|X). We assign a uniform prior for this distribution; informative expert priors can also be used when available.P(Y|X)∼Uniform(0,1)The meta-analysis results are conditioned on a fewer variables than the CPD in the BN. Therefore, the expected values of the meta-analysis results are modelled as a marginalisation of the CPD. The meta-analysis provided the pooled conditional probability estimates about P(Y|Xi) that are modelled as the marginalisation of P(Y|X)P(Y|Xi)=∑X⧹{Xi}P(Y|X)P(X⧹{Xi})P(X⧹{Xi})is also estimated by the binomial distribution below where M denotes the total number of data instances, andM[X⧹{Xi}]denotes the counts of data instances withX⧹{Xi}.P(X⧹{Xi})has a uniform prior.M[X⧹{Xi}]∼Binomial(M,P(X⧹{Xi}))P(X⧹{Xi})∼Uniform(0,1)The pooled estimates from the meta-analysisμPnew(Y|Xi)are modelled with a normal distribution truncated to a unit interval [0–1] as it represents a probability. The mean of this distribution is the marginalisation of the CPD, i.e. P(Y|Xi), and the varianceσP(Y|Xi)2represents the degree of uncertainty we assign to the meta-analysis results. We enter the mean and variance of the predictive distribution in meta-analysis as observations forμPnew(Y|Xi)andσPnew(Y|Xi)2. We use the truncated normal distribution as it is convenient to define the expected value and variance parameters for it butμPnew(Y|Xi)may not be normally distributed as it represents a probability value between 0 and 1.μPnew(Y|Xi)∼TNormal[0,1](P(Y|Xi),σPnew(Y|Xi)2)Finally, we introduce constraints to ensure that the sum of every probability distribution is equal to 1.∑YP(Y|X)=1∑X⧹{Xi}P(X⧹{Xi})=1∑YP(Y|Xi)=1The method described in Sections 3.2.1 and 3.2.2 assumes that the variables analysed in the meta-analysis are neighbours in the BN. In this section we look at how this assumption can be relaxed to handle the more general case where the BN contains other – intermediate – variables between the variables analysed in the meta-analysis. This situation is illustrated in Fig. 7; the meta-analysis combines the published probabilities for P(Y|X) but the BN contains another variable I between X and Y so that the values in P(Y|X) are no longer parameters of the BN. We examine how we can use information from a meta-analysis about non-neighbouring variables to estimate the parameters of a variable in the BN. In particular, we use P(Y|X), calculated from a meta-analysis, to estimate P(I|X), giving the parameters of the X→I relation, when we know or have data for P(Y|I) describing the other intermediate relation I→Y.Since every variable in a BN is conditioned on its parents, P(Y|X) provided from the meta-analysis is equal to:P(Y|X)=∑IP(Y|I)P(I|X)Based on this, we can estimate every parameter of P(I|X) as:P(ik|X)=P(Y|X)-∑m∈SP(Y|im)P(im|X)P(Y|ik)In this equation, S is the set of states of the variable I except the state ik:S=Val(I)⧹{ik}Consequently, the parameters of P(I|X) can be estimated given that meta-analysis provides us with P(Y|X), and we know or have data to learn P(Y|I). In order to get a point estimate for parameters of P(I|X), the number of the states of I must not exceed the number of the states of Y. Otherwise, we can get an interval for the values in P(I|X) but cannot estimate the exact values.For example, let X, I and Y in Fig. 7 have two states each with the values {x1,x2}, {i1,i2}, {y1,y2} respectively. Suppose a meta-analysis provides us with P(y1|x1)=0.8, and we learn the probabilities P(y1|i1)=0.9, P(y1|i2)=0.3 from the data. Since I has two states: P(i2|x1)=1−P(i1|x1). From these values, we can calculate P(i1|x1) as:P(i1|x1)=P(y1|x1)-P(y1|i2)P(i2|x1)P(y1|i1)=P(y1|x1)-P(y1|i2)(1-P(i1|x1))P(y1|i1)=0.833When I has more states than Y, we cannot get the exact values of the BN parameters but we can find an interval of possible values. Let I has the states {i1,i2,i3} instead of {i1,i2}. Since I has three states, P(i3|x1)=1−P(i1|x1)−P(i2|x1). Let P(y1|i3)=0.5, and the other values be the same as in the example above. The parameters of P(i1|x1) can be calculated by solving the equation below, for each independent value of Y:P(i1|x1)=P(Y|x1)-(P(Y|i2)P(i2|x1)+P(Y|i3)P(i3|x1))P(Y|i1)In this case, we cannot get an exact value for P(I|x1) as the intermediate variable I has more states than Y, resulting in more unknowns than equations. Instead, P(I|x1) can get any value as long as it satisfies the following conditions:P(i2|x1)=-1.5+2P(i1|x1)P(i3|x1)=2.5-3P(i1|x1)0⩽P(I|x1)⩽1We could use expert knowledge, by eliciting additional constraints, to narrow down the set of acceptable values for the parameters of I. In our example, P(i1|x1) can get any value between 0.75 and 0.83 to satisfy the conditions above. However, some of these values may not make sense to the domain experts, and we can eliminate these values by adding additional constraints. For example, the experts could say that P(i1|x1) should only get values above 0.8 and we could reflect it by adding the P(i1|x1)>0.8 constraint to the conditions above.The technique described above can also be applied when more intermediate variables are present. Fig. 8shows a BN that has n intermediate variables between Y and X. A similar case, where 2 intermediate variables are present, is encountered in the case study described in the following section (see Section 4.4.1). An exact estimate can be found given that we know or have data for P(Y|X), P(Y|Ik) for k=2,…,n, and that Y does not have fewer states than I1.P(i1k|X)=P(Y|X)-∑m∈SP(Y|In)P(In|In-1)…P(I2|i1m)P(i1m|X)P(Y|i1k)whereS=Val(I1)⧹{i1k}.Using the method described in Section 3, we developed a BN to predict viability of a Lower Extremity with Vascular Trauma (LEVT). This section presents the development of the LEVT BN, and Section 5 presents its results.

@&#CONCLUSIONS@&#
