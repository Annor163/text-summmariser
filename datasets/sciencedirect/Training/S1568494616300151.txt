@&#MAIN-TITLE@&#
A self-organizing cascade neural network with random weights for nonlinear system modeling

@&#HIGHLIGHTS@&#
A self-organizing cascade neural network is proposed for nonlinear system modeling.A method is derived to select the input and hidden units for the network.A simple way is derived to train the weights of the network.The proof of the convergence has been given.Predict the key parameters in a wastewater treatment plant using the network.

@&#KEYPHRASES@&#
Feedforward neural network,Self-organizing cascade neural network,Nonlinear system modeling,Random weights,Wastewater treatment plant,

@&#ABSTRACT@&#
In this paper, a self-organizing cascade neural network (SCNN) with random weights is proposed for nonlinear system modeling. This SCNN is constructed via simultaneous structure and parameter learning processes. In structure learning, the units, which lead to the maximal error reduction of the network, are selected from the candidates and added to the existing network one by one. A stopping criterion based on the training and validation errors is introduced to select the optimal network size to match with a given application. In parameter learning, the weights connected with the output units are incrementally updated without gradients or generalized inverses, while the other weights are randomly assigned and no need to be tuned. Then, the convergence of SCNN is analyzed. Finally, the proposed SCNN is tested on two benchmark nonlinear systems and an actual municipal sewage treatment system. The experiment results show that the proposed SCNN has better performance on nonlinear system modeling than other similar methods.

@&#INTRODUCTION@&#
System models are crucial for effective analysis and control in practice [1]. A good model should adequately describe the behavior of a system. When no or partial knowledge of a system is available, results on system modeling are used to obtain the process variable trajectory information from a set of measured input–output data [2]. Most industrial systems are nonlinear in nature, hence nonlinear system modeling has attracted a lot of attention in many areas, such as control [3], process monitoring [4] and soft sensor [5,6].In fact, it is a difficult task to model industrial systems because of the need to estimate the structure and parameters of such nonlinear systems [7]. Although the methods based on conventional mathematics have been widely used for nonlinear system modeling in recent decades [8,9], they often become not suitable when the industrial systems exhibit complicated characteristics in terms of strong nonlinearity, multivariable coupling, complex biochemical reaction, variations of operation conditions together with unknown model structure and parameters [10].Theoretically, feedforward neural networks (FNNs) can approximate any continuous function defined on a compact set to any desired degree of accuracy [11,12]. This universal approximation property makes FNNs suitable to model nonlinear systems, especially those which are hard to be mathematically described [13]. Hence, FNNs have attracted a lot of attention for nonlinear system modeling [7,14–19]. For example, a novel self-organizing radial basis function neural network is proposed for nonlinear system identification and modeling [14]. An automatic axon–neural network (AANN) is proposed to establish an artificial neural network with self-organizing architecture and suitable learning algorithm for nonlinear system modeling [18]. A multiple neural network, which integrates the structure selection, parameter identification and hysteresis network switching with guaranteed neural identification performance, is introduced to model a kind of nonlinear systems [19]. In these scenarios noted above, the architecture of multilayer perceptron (MLP) has been employed, and the gradient-based algorithms as well. It is noted that the performance of FNNs depends on the network architecture and learning algorithms greatly [18]. Selecting suitable network architecture and fast learning algorithm for FNNs is still challenging for nonlinear systems modeling.Recent research results show that the architecture of cascade neural networks (CNNs) is more powerful than that of MLP in most cases [20,21]. Although some algorithms have been proposed to train the CNNs with fixed topology, it is difficult for them to determine the number of hidden units to match with a given application [22,23]. Cascade correlation (CC) algorithm, one of the most popular constructive algorithms, can be used to automatically construct CNNs [24,25]. Instead of just adjusting the weights of a network with fixed topology, the CC algorithm begins with a minimal network with no hidden unit, then trains and adds new hidden units to the existing network one by one. Once a new hidden unit has been added, its input weights are frozen in the later process. Every time, only the new hidden unit and output units are trained. Each hidden unit added to the network has the maximal magnitude of the correlation between its output and the last error. However, this objective function (correlation) used to train and select new hidden units may result in a large network with poor generalization performance [26,27]. Hence, some improved objective functions have been evaluated in [26]. An orthogonal least squares based cascade network (OLSCN) has been proposed to construct CNNs by using a novel objective function, with which the sum of squared errors (SSE) of the network can be maximally reduced after a new hidden unit is added [27]. In these scenarios noted above, gradient-based algorithms have been used to optimize their objective functions. However, gradient-based algorithms suffer from local minima problem, slow convergence and very poor sensitivity to learning rate setting [28].To overcome these difficulties faced by gradient-based algorithms, random vector functional link (RVFL) networks have been studied [28–31], where the weights between the input layer and the hidden layer can be randomly assigned and no need to be tuned. Then, the weights connected with the output units can be calculated by solving a linear regression problem. Such a randomized learning scheme dramatically reduces the training time and many experimental results indicate that the learner's generalization performance is favorably good [28–31]. It has been proved that RVFL networks can universally approximate any continuous function [29,32]. For its simplicity and effectiveness, this flat-net architecture with random weights has been successfully applied to many areas [33–36]. However, to the best of our knowledge, there are seldom similar algorithms for CNNs.As a result of the research noted above, a self-organizing cascade neural network (SCNN) with random weights is proposed to model nonlinear systems. This proposed SCNN has a powerful growing cascade architecture, the connections of which can be across all the layers. Firstly, the mathematical expression of the growing architecture is presented, and the contribution value (CV) of units is derived by the orthogonal least squares (OLS) method to select new input and hidden units from candidates for SCNN. With the derived CV, each of the selected units affords the maximal reduction of the sum of squared errors (SSE). Secondly, the weights feeding into the output units are updated in an incremental way without gradients or generalized inverses during the training process, while the input weights of hidden units are randomly assigned and no need to be tuned. Thirdly, a stopping criterion based on training and validation errors is introduced to select the optimal network size. SCNN is a self-organizing network which automatically determines its weights and architecture to match with a given application. Finally, SCNN is test on two benchmark problems (Nonlinear dynamic system modeling and Mackey–Glass time-series prediction) and an actual municipal sewage treatment system in a wastewater treatment plant (WWTP). The experiment results show that SCNN can model nonlinear systems effectively.The rest of this paper is organized as follows. The details of the proposed SCNN algorithm are described in Section 2, including the growing architecture, the derived contribution value of units, the method used to update the weights, the stopping criterion, the detailed steps and the proof of convergence. Section 3 presents the experiment results which demonstrate the performance of the proposed method compared with some existing algorithms. Finally, we conclude this work in the last section.As shown in Fig. 1, the architecture of SCNN is growing and similar to that of the cascade neural network [24,27], in which all hidden units connect with each other and the connections are across all the layers. Each hidden unit receives the connections from the input units as well as the pre-existing hidden units. All original inputs and the outputs of hidden units are fed to every output unit. Furthermore, there are some connections which connect the bias input (permanently set to +1) to all hidden and output units. It can be seen from Fig. 1 that the architecture of SCNN is growing step by step. Firstly, the bias unit with +1 is added to an empty network without input and hidden units, then all the input and hidden units are added one by one. To mathematically formulate the growing architecture, in the following, the bias unit, input units and hidden units are numbered in sequence as done in [27],(1)thejth unit=bias unitj=1the(j−1)th input unit2≤j≤n+1the(j−n−1)th hidden unitj>n+1Letwqj|q=1,2,…,j−1be the input weights of the jth (j>n+1) unit, wherew1jis the bias weight of the jth unit, then the output of the ith unit is formulated as follows,(2)φij=1j=1xij2≤j≤n+1,i=1,2,…,Nfj∑t=1j−1φitwtjj>n+1.Suppose that the network has L units (including input units, bias unit and hidden units) and m linear output units, then the output of the network is formulated as follows,(3)yil=∑j=1Lφijθjl,i=1,2,…,N;l=1,2,…,m,where θjlis the weight which connects the jth unit to the lth output unit, and θ1lis the bias weight of lth output unit. Then the prediction error of the network is given by(4)eil=til−yil,i=1,2,…,N;l=1,2,…,m.LetϕL,θL,ELrepresent the input matrix of output layer, the weight matrix connecting to the output units and the error matrix, respectively, then the architecture of SCNN can be formulated as,(5)T=ϕLθL+EL,whereT=(t1t2…tm)N×mis the target output matrix. Obviously, Eq. (5) can be solved by using the OLS method.DefineϕL=(φ1φ2…φL), the column vectorsφj(1≤j≤L) can be made mutually orthogonal by the classical Gram–Schmidt algorithm as follows,(6)q1=φ1δij=qiTφjqiTqi1≤i<jqj=φj−∑i=1j−1δijqi,2≤j≤L.According to Eq. (6), the regression matrixϕLcan be decomposed into(7)ϕL=QLRL,(8)QL=(q1q2⋯qL),RL=1δ12⋯δ1L01⋯δ2L⋮⋯⋱⋮00⋯1,whereqiqj=0(i≠j).According to [27], the OLS solution of Eq. (5) and the error matrix can be formulated by Eqs. (9) and (10), respectively.(9)θL=RL−1(QLTQL)−1QLTT,(10)EL=(IN−QL(QLTQL)−1QLT)T.Furthermore, the SSE of SCNN can be calculated by(11)SSEL=∑l=1melTel=∑l=1mtlTtl−∑j=1L(qjTtl)2qjTqj.The reduction of the SCNN's SSE, caused by the Lth units, can be calculated as follows,(12)ΔSSEL=SSEL−1−SSEL=∑l=1m(qLTtl)2qLTqL.However, the random weights sometimes make the column vectors of hidden layer output matrix of the network not full rank [37], i.e. ||qL||=0. Once such case happens, Eq. (12) will be out of action. So, we define the contribution value (CV) of the Lth unit as follows,(13)CVL=∑l=1m(qLTtl)2qLTqL+δ2,where δ is a very small positive number. In this paper, the defined CV is used to select the input and hidden units for SCNN.In SCNN algorithm, the weights feeding into hidden units are randomly assigned under some probability distribution and no need to be tuned, while the weights connected with output units are updated in an incremental way without gradients or generalized inverses.In the following, the method to optimize the weights connected with output units is derived. According to Eqs. (8) and (9), defineβL=(QLTQL)−1QLTT, then we haveβL=((QL−1,qL)T(QL−1,qL))−1(QL−1,qL)TT=(QL−1TQL−1)−100(qLTq)−1QL−1TTqLTT=(QL−1TQL−1)−1QL−1TT(qLTq)−1qLTT.That is(14)βL=βL−1λL,λL=qLTTqLTqL.According to Eq. (8), letηL−1=(δ1Lδ2L…δ(L−1)L), thenRLcan be rewrote as(15)RL=RL−1ηL−1T01.According to block matrix theory, we have(16)RL−1ηL−1T01RL−1−1−RL−1−1ηL−1T01=IL×L,whereIL×Lis a unit matrix with L columns. Hence, we have(17)RL−1=RL−1−1−RL−1−1ηL−1T01,R1=1,and(18)θL=RL−1βL.According to Eqs. (14), (17) and (18), the weights connected with output units can be calculated in an incremental way without gradients or generalized inverses during the growth of the network.In this paper, the PQ stopping criterion which balances the generalization loss and training progress to maximize the average quality of solutions is used [38]. Let Etr(t) andEva(t)be the training and validation errors at training time t, respectively. The value Eopt(t) is defined as the lowest validation error obtained in times up to t,(19)Eopt(t)=minλ≤tEva(λ).According to [38], the generalization loss at training time t is defined as follows,(20)GL(t)=Eva(t)Eopt(t)−1.Define a training strip of length k as a sequence of k times numbered n+1, …, n+k, where n is divisible by k. k is generally set to 5 [38]. Then the training progress Pk(t) measured at time t after a training strip k can be calculated as follows,(21)Pk(t)=∑t′=t−k+1tEtr(t′)k×mint−k+1≤t′≤tEtr(t′)−1.Then, the PQ stopping criterion is defined as(22)PQ=GL(t)−Pk(t).Let the growth of SCNN stop after the first end-of-strip time t with PQ>0. However, the validation error does not monotonously decrease. It may be not the optimal network size when the PQ stopping criterion is satisfied. Hence, after the growth stops, the model which leads to the minimal validation error is chosen as the final one to be test on unseen data.Given N arbitrary distinct training samples {(xi,ti)|xi∈Rn,ti∈Rm, 1≤i≤N}, validation samples, bounded nonlinear activation function f(x), the maximum number of hidden units permitted to be added to the network (denoted by Lmax) and the number of input units K.TN×mdenotes the target out matrix.Step 1)Create a network with no unit connected with the output units. Initialize the number of units L=0 and the training errorSSE0=∑i=1N∑j=1mtij2;Add the bias unitφ1 to the empty network as shown in Fig. 1, and increase by one the number of units L=L+1. Initialize the regression matrixϕL=φ1,qL=φ1,QL=φ1,RL−1=1,βL=qLTTqLTqL. Calculate SSELaccording to Eq. (12);Select and add K input units to the network one by one;3.1)Calculate the CV of each candidate input variable in the given training samples. Firstly, make each candidate input variable orthogonal with the existing units of the network using Eq. (6). Then calculate the CV of each candidate input variable using Eq. (13);Add the input variableφwith the maximal CV to the existing network as shown in Fig. 1, and letqdenote its orthogonal vector;Let L=L+1,φL=φ,qL=q, and updateQL=QL−1qL,ϕL=ϕL−1φL;Calculate the SSELaccording to Eq. (12);update the output weightsθLasθL=RL−1βL, whereRL−1=RL−1−1−RL−1−1ηL−1T01,ηL−1=q1TφLq1Tq1q2TφLq2Tq2⋯qL−1TφLqL−1TqL−1,βL=βL−1λL,λL=qLTTqLTqL;Remove the added variable from the given training samples and let K=K−1;Repeat (3.1)–(3.6) until K=0.Select and add hidden units to the network one by one;4.1)Randomly generate a number of candidate units and calculate their contributions to the SSE reduction, respectively. Firstly, make each candidate unit orthogonal with the existing units of the network using Eq. (6). Then calculate the CV of each candidate unit using Eq. (13);Install the candidate unitφ′ with the largest CV into the existing network as shown in Fig. 1, and denoteq′ as its orthogonal vector;Let L=L+1,φL=φ′,qL=q′, and updateQL=QL−1qL,ϕL=ϕL−1φL;Calculate the SSELaccording to Eq. (12);update the output weightsθLas done in Step 3.5;Calculate and record the validation error using the validation data set.Calculate GL(t), Pk(t) and PQ according to Eqs. (19)–(22);Repeat (4.1)–(4.7) until one of the following conditions holds, the number of hidden unit is larger than Lmax or PQ>0;According to the recorded validation errors, the model with the minimal validation error will be tested on unseen data.Similar to RVFL networks [28–32], SCNN usually needs more hidden units to obtain the super performance than the FNNs with tunable input weights.According to Step 3, SCNN can automatically select the important input variables from the given data set collected from the actual nonlinear system.To discuss the convergence of SCNN, a function approximation task with one output is considered for simplicity and without loss of generality [38]. The output unit of SCNN uses the identity activation function for the approximation problem. Therefore, the function that is approximated by SCNN can be expressed as follows,(23)fn(x)=∑j=1nβj*φj(x),where φj(x) is the jth input of the output unit, andβj*calculated by OLS represents the optimal weight between the jth unit (defined as Eq. (1)) and the output unit. Let Γ be the set of all functions that can be implemented by the hidden units of SCNN, then φj(x)∈Γ.For any continuous target function f(x), define en(x)=f(x)−fn(x) as the approximation error. Given function sequence as follows,(24)f˜n(x)=∑j=1n〈e˜j−1(x),φj(x)〉||φj(x)||2φj(x)wheree˜j(x)=f(x)−f˜j(x)(j=1,2,…,n)ande˜0(x)=f(x), then we have||en(x)||≤||e˜n(x)||. Next, we need to provelimn→∞||e˜n(x)||=0. In [38], such a proof has been given. Here, we only give some results in [38] for subsequent explanations.Proposition 1[38]For a fixed φ(x)∈Γ(||φ(x)||≠0), the expression||f(x)−(f˜n−1(x)+βnφ(x)||achieves its minimum if and only ifβn=〈e˜n−1(x),φ(x)〉/||φ(x)||2, wheree˜n−1(x)=f(x)−f˜n−1(x). AndΔemax(φ(x))=〈e˜n−1(x),φ(x)〉2/||φ(x)||2whereΔemax(φ(x))=||e˜n−1(x)||−||e˜n−1(x)−βnφ(x)||is the maximal error reduction caused by φ(x).Given that span (Γ) is dense in L2 and ∀φ(x)∈Γ, 0<||φ(x)||<b for some b∈R. If φ(x) is selected to maximize〈e˜n−1(x),φ(x)〉2/||φ(x)||2, thenlimn→∞||f(x)−f˜n−1(x)−βnφ(x)||=0.It can be seen from Eq. (24) thatf˜n(x)=f˜n−1(x)+βnφn(x)whereβn=〈e˜n−1(x),φn(x)〉/||φn(x)||2, which complies with Proposition 1. Hence||e˜n(x)||achieves its minimum andΔemax(φn(x))=〈e˜n−1(x),φn(x)〉2/||φn(x)||2. In addition, each hidden unit added to SCNN affords the maximal error reduction and φn(x) is bounded, which complies with Theorem 1. Hence, we have(25)limn→∞||en(x)||≤limn→∞||e˜n(x)||=limn→∞||f(x)−f˜n(x)||=0.It means that the convergence of SCNN can be guaranteed.

@&#CONCLUSIONS@&#
In this paper, a self-organizing cascade neural network (SCNN) with random weights is proposed to model nonlinear systems. This proposed SCNN incorporates the weight optimization into the architecture design. A metric called contribution value (CV) is derived by using the OLS method to measure the contribution of a unit to the SSE reduction of the network. The units with the maximal CV are selected from the candidates and added to the network one by one until the PQ stopping criterion is satisfied. Meanwhile, the weights connected with the output units are calculated in an incremental way without gradients or generalized inverses during the growth of SCNN, while the other weights are randomly assigned and no need to be tuned. Such a weight optimization scheme makes SCNN learn fast. Experimental results have shown that the generalization performance of SCNN is favorably good on nonlinear system modeling.