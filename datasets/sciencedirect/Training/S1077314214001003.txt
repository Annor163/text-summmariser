@&#MAIN-TITLE@&#
Multigrid convergent principal curvature estimators in digital geometry

@&#HIGHLIGHTS@&#
We propose discrete principal curvature estimators based on integral invariants.We prove the multigrid convergence of these estimators.We provide an experimental evaluation on synthetic and real data.

@&#KEYPHRASES@&#
Digital geometry,Curvature estimation,Multigrid convergence,Integral invariants,

@&#ABSTRACT@&#
In many geometry processing applications, the estimation of differential geometric quantities such as curvature or normal vector field is an essential step. In this paper, we investigate a new class of estimators on digital shape boundaries based on integral invariants (Pottmann et al., 2007) [39]. More precisely, we provide both proofs of multigrid convergence of principal curvature estimators and a complete experimental evaluation of their performances.

@&#INTRODUCTION@&#
In many shape processing applications, the estimation of differential quantities on the shape boundary is usually an important step. Their correct estimation makes easier further processing, like quantitative evaluation, feature detection, shape matching or visualization. This paper focuses on estimating the curvature tensor on the boundary of digital shapes. Such digital structures are subsets of the 3-dimensional digital spaceZ3and come generally from the digitization of some Euclidean shape. Of course, the curvature tensor estimation should be as close as possible to the curvature tensor of the underlying Euclidean shape before digitization. Digital data form a special case of discrete data with specific properties: (1) digital data cannot sample the boundary of the Euclidean shape (i.e. they do not lie on the shape boundary), (2) digital data is distributed around the true sample according to arithmetic noise, which looks rather uniform over a range[-h,h]from a statistical point of view, where h is the digitization grid step. Another way of stating these characteristics is to say that the Hausdorff distance between the Euclidean shape and its digitization is someO(h). Of course, the quality of the estimation should be improved as the digitization step gets finer and finer. This property is called the multigrid convergence[25,11]. It is similar in spirit with the stability property in geometry processing: given a continuous shape and a specific sampling of its boundary, the estimated measure should converge to the Euclidean one when the sampling become denser (e.g.[2,35]).Our objective is to design a curvature tensor estimator for digital data such that: (1) it is provably multigrid convergent, (2) it is accurate in practice, (3) it is computable in an exact manner, (4) it can be efficiently computed either locally or globally (evaluation at a single surface point or extraction of the curvature tensor field), (5) it is robust to further perturbations (like bad digitization around the boundary, outliers).Digital data being discrete in nature, it is interesting to look at the curvature estimation techniques on triangulated meshes. In computer graphics and geometry processing, there exists a vast family of techniques to estimate either the mean or Gaussian curvatures, or sometimes the full curvature tensor. Most of them are local (i.e. limited to a 1-ring or 2-ring of neighbors) but exhibit correct results for nice meshes. They generally fall into three categories: fitting, discrete methods, curvature tensor estimation. We may refer to [44,20] for comprehensive evaluations, and Desbrun et al. [17] or Bobenko and Suris [5] for an entirely discrete theory. Most of them have not theoretical convergence guarantees even without noise on the mesh. We may quote [37,42] as approaches trying to tackle perturbation through averaging.For Gaussian curvature estimated with Gauss-Bonnet approach (angle defect), Xu [47] provides a stability theorem for triangulated mesh whose vertices lie on the underlying smooth manifold, with valence 6 and parallelogram condition (each 1-ring of neighbors is projected as a parallelogram onto a plane). Assuming a sampling with densityδ, he provides an additional convergence property whenever the sampling is perturbated by someO(δα), butα>2(inadequate for discrete data). Note that if the triangulated mesh did not satisfy these requirements, such estimation did not converge.The integral measures of curvatures, based on normal cycle theory [13,14] is another notable approach for estimating curvature information on a triangulated mesh. The authors exhibit some convergence results for triangulated meshes with vertices lying on the underlying smooth Euclidean shape boundary. In this case, if the mesh has Hausdorff distance to shape boundary below∊, convergence is obtained with speed/errorO(∊)under some hypotheses.Finally, in geometry processing, interesting mathematical tools have been developed to design differential estimators on smooth surfaces based on integral invariants [39,38]. They consist in moving a kernel along the shape surface and in computing integrals on the intersection between the shape and the kernel. The authors have demonstrated that some integral quantities provide interesting curvature information when the kernel size tends to zero. They also achieve stability depending on the kernel radius and on∊, for instance in the case of a mesh sampling. Our new estimators rely on the same ideas.When having only discrete data (i.e. a cloud of points), the most natural way to approach curvature(s) is to fit a polynomial surface of degree two at least. Perhaps the best representative of these techniques is the osculating jets of Cazals and Pouget [8]. The authors provideO(δ2)convergence results when the data is a surface sampling, assumingδis the density of points. There is no theoretical result in presence of noise, although the least-square fitting of osculating jets is very robust to noise in practice.Another family of techniques exploits the Voronoi diagram [1,34,35]. The idea behind these approaches is, instead of fitting the tangent space, to estimate at best the orthogonal space. The convolved covariance measure introduced by Mérigot et al. [35] is particularly appealing since this measure achieves robustness even for arbitrary compact sets, essentially inO(∊). It is in some sense an integral measure of the covariance matrix of the normal cone around the point of interest. However, convergence of curvature(s) is subject to several parameters r and R which contribute contradictorily to the Hausdorff error. In practice, this approach gives results comparable to osculating jets for curvatures.Recently, several authors have developed new interesting approaches for estimating the normal vector field on noisy point clouds, even in the presence of sharp features [32,6,48]. Furthermore, Boulch and Marlet [6] gives probabilistic convergence results. Although they cannot be used “as is” for curvature computation, they could be used in parallel with curvature estimation techniques to locate sharp features in a first pass, and to limit curvature estimations to smooth zones.In digital geometry, we usually consider multigrid convergence as an essential criterion [11]. Hence, in dimension 2, parameter free convergence results have been obtained for length [9] and normal vector estimation [16]. Based either on binomial convolution principles [33,18], or polynomial fitting [40], convergence results can also be obtained for higher order derivatives of digital curves. Algorithms are parametrized by the size of the convolution or fitting kernel support and convergence theorems hold when such support size is an increasing function of the grid resolution and some shape characteristics.For curvature estimation along 2D curves, multigrid convergence of parameter-free estimators is still challenging, although accurate experimental results have been obtained with maximal digital circular arcs [41] and with global optimization [23]. In 3D digital space, several empirical methods exist for estimating curvatures, but none achieves multigrid convergence (e.g. see [31,19]). In [10], we recently presented a digital estimator for mean curvature for 2D and 3D digital objects, which achieves multigrid convergence inO(h13).This paper completes [10] to propose a new curvature tensor estimator for digital data, which casts carefully the Integral Invariant (II) method of [39,38] into the digital world. This estimator is a non-trivial extension of our mean digital curvature estimator [10], since it involves the computation of digital moments and covariance matrices, and requires results from matrix perturbation theory.The contributions of the paper can be sketched as follows. First, we define digital versions of integral invariant estimators with multigrid convergence results (Theorems 3 and 4). We provide an explicit formula for the kernel size, which guarantees uniform convergence inO(h13)for smooth enough curves and surfaces (Theorem 6). Furthermore, we demonstrate that these estimators have simple, exact and efficient implementations (available in DGtal library [46]). We provide an extensive comparative evaluation of these estimators (mean curvature, principal curvatures), which shows that they compete with classical ones in terms of accuracy (Section 4). Computation speed is also considered, and our method is for instance ten times faster than the osculating jets. Finally, we show empirical results illustrating the robustness to noise and outliers of our estimators.Since we are interested in evaluating both theoretically and experimentally the behavior of a given differential estimator on digital object boundaries, we first have to formalize links between Euclidean objects and digital ones with the help of a digitization process. Let us consider a familyXof smooth and compact subsets ofRd. In Section 2.3 we will be more precise on the notion of smoothness for shapesX∈X. We denoteDh(X)the digitization ofXin ad-dimensional grid of grid step h. More precisely, we consider classical Gauss digitization defined as(1)Dh(X)=def1h·X∩Zd,where1h·Xis the uniform scaling ofXby factor1h. Furthermore, the set∂Xdenotes the frontier ofX(i.e. its topological boundary). Ifz∈Zd, thenQzdenotes the unit d-dimensional cube ofRdcentered on z. The h-frontierΔhZof a digital setZ⊂Zdis defined asΔhZ=def∂(h·∪z∈ZQz). Therefore, the h-frontier ofDh(X)is a(d-1)-dimensional subset ofRd, which is close to∂X. We will precise the term “close” later in this section. Since this paper deals with multigrid convergence, digital shapes will always come from the digitization of continuous ones. To simplify notations, the h-frontier of the Gauss digitization at step h of a shape X will simply be denoted by∂hX=defΔhDh(X), and called later on h-boundary of X.As discussed in various previous works (see for instance [11] for a survey), the idea of multigrid convergence is that when we define a quantity estimator onDh(X), we check if the estimated quantity converges (theoretically and/or experimentally) to the associated one onXwhen h tends to zero. More formally,Definition 1Multigrid convergence for local geometric quantitiesA local discrete geometric estimatorE^of some geometric quantity E is multigrid convergent for the familyXif and only if, for anyX∈X, there exists a grid stephX>0such that the estimateE^(Dh(X),xˆ,h)is defined for allxˆ∈∂hXwith0<h<hX, and for anyx∈∂X,(2)∀xˆ∈∂hXwith∥xˆ-x∥∞⩽h,E^Dh(X),xˆ,h-E(X,x)⩽τX,x(h),whereτX,x:R+⧹{0}→R+has null limit at 0. This function defines the speed of convergence ofE^toward E at point x ofX. The convergence is uniform for X when everyτX,xis bounded from above by a functionτXindependent ofx∈∂Xwith null limit at 0.When a geometrical quantity is global (e.g. area or volume), we do not need an explicit mapping between∂Xand∂hX, and Definition 1 can be rephrased to define multigrid convergence of global geometric quantities[11]. A local discrete estimator however estimates a geometric quantity at points on the h-frontier of a digital set, otherwise said at any point on the interpixel representation of the digital set boundary. This definition encompasses usual definitions where input points are pointels, linels or surfels.In some proofs, a more precise mapping between pointsx∈∂Xandxˆ∈∂hXis required. For any shapeX∈Rd, the medial axisMA(∂X)of∂Xis the subset ofRdwhose points have more than one closest point to∂X. The reach reach(X) of X is the infimum of the distance between∂Xand its medial axis. Shapes with positive reach have principal curvatures bounded by±1/reach(X). The (orthogonal) projectionπXis the mapping fromX⧹MA(∂X)onto∂Xthat associates to each point its closest point in∂X(cf. Fig. 1(b)).This projection can be restricted to domain∂hXin order to define a mappingπhXfrom the h-frontier∂hXto the boundary∂X. This mapping was called back-projection in [29]. For any 2D shape X with positive reach, for0<h⩽reach(X), Lemma B.9 [29] indicates that the mapπhXis well-defined and onto. It shows that the Hausdorff distance of boundaries∂hXand∂Xis no greater than22h, hence they get closer and closer as the grid step is refined.In d dimensions, it is possible to show1The proof follows the same lines as Lemma B.9 [29].1that their Hausdorff distance is no greater thand2h. Furthermore, it is a known fact thatπXis continuous overRd⧹MA(∂X), hence over∂hXwith an adequate h.In geometry processing, integral invariants have been widely investigated to define estimators of differential quantities (see [39,38] for a complete overview). For short, the main idea is to move a kernel on pointsx∈∂Xand to compute integrals on the intersection betweenXand the kernel. Even though different kernels (e.g. Euclidean ball, Euclidean sphere) and different integration functions can be considered, we focus here on volumetric integral invariants defined as follows:Definition 2GivenX∈Xand a radiusR∈R+∗, the volumetric integralVR(x)atx∈∂Xis given by (see Fig. 1(a))(3)VR(x)=def∫BR(x)χ(p)dp,whereBR(x)is the Euclidean ball with radius R and center x andχ(p)the characteristic function ofX. In dimension 2, we simply denoteAR(x)such quantity.Several authors have detailed connections betweenVR(x)and curvature (resp. mean curvature) at x for shapes inR2(resp.R3) [7,39,38].Ifκ(X,x)is the curvature of∂Xat x andH(X,x)is the mean curvature of∂Xat x, we have:Lemma 1[38]For a sufficiently smooth shapeXinR2,x∈∂X, we have(4)AR(x)=π2R2-κ(X,x)3R3+O(R4).For a sufficiently smooth shapeXinR3andx∈∂X, we have(5)VR(x)=2π3R3-πH(X,x)4R4+O(R5).Such results are obtained by Taylor expansion at x of the surface∂Xapproximated by a parametric functiony=f(x)in 2D andz=f(x,y)in 3D. From Eqs. (4) and (5) and with a fixed radius R, one can derive local estimatorsκ̃RandH∼Rrespectively:(6)κ̃R(X,x)=def3π2R-3AR(x)R3,H∼R(X,x)=def83R-4VR(x)πR4.In this way, when R tends to zero, both estimated values will converge to expected ones (respectivelyκand H). More formally:(7)κ̃R(X,x)=κ(X,x)+O(R),H∼R(X,x)=H(X,x)+O(R).Similarly, directional information such as principal curvatures and thus Gaussian curvature can be retrieved from integral computations. Indeed, instead of computing the measure ofBR(x)∩Xas in Definition 2, we consider its covariance matrix. Given a non-empty subsetY⊂Rd, the covariance matrix of Y is given by(8)J(Y)=def∫Y(p-Y‾)(p-Y‾)Tdp=∫YppTdp-Vol(Y)Y‾Y‾T,whereY‾is the centroid of Y andVol(Y)its volume. For non-negative integersp,qand s, we recall the definition of(p,q,s)-momentsmp,q,s(Y)of Y:(9)mp,q,s(Y)=def∭Yxpyqzsdxdydz.Note that the volumeVol(Y)is the 0-momentm0,0,0(Y), and that the centroidY‾is the vector of 1-moments normalized by the 0-moment, i.e.(m1,0,0(Y),m0,1,0(Y),m0,0,1(Y))T/m0,0,0(Y). For simplicity, let us denote by A the Euclidean setBR(x)∩X. The covariance matrix of A is then rewritten as2⊗denotes the usual tensor product in vector spaces.2:(10)J(A)=m2,0,0(A)m1,1,0(A)m1,0,1(A)m1,1,0(A)m0,2,0(A)m0,1,1(A)m1,0,1(A)m0,1,1(A)m0,0,2(A)-1m0,0,0(A)m1,0,0(A)m0,1,0(A)m0,0,1(A)⊗m1,0,0(A)m0,1,0(A)m0,0,1(A)T.In [39], the authors have demonstrated that eigenvalues and eigenvectors ofJ(A)provide principal curvature and principal direction information:Lemma 2[39, Theorem 2]Given a shapeX∈X, the eigenvaluesλ1,λ2,λ3ofJ(A), whereA=BR(x)∩Xandx∈∂X, have the following Taylor expansion:(11)λ1=2π15R5-π483κ1(X,x)+κ2(X,x)R6+O(R7),(12)λ2=2π15R5-π48κ1(X,x)+3κ2(X,x)R6+O(R7),(13)λ3=19π480R5-9π512κ1(X,x)+κ2(X,x)R6+O(R7),whereκ1(X,x)andκ2(X,x)denotes the principal curvatures of∂Xat x.3There is a typographic error inλ1in the paper [39].3Hence, similarly to Eq. (6), one can define local estimatorsκ̃R1,κ̃R2and finally the Gaussian curvatureK∼R=defκ̃R1·κ̃R2as functions of{λi}1,2,3and R. From Lemma 2, all these estimators converge in the continuous setting when R tends to 0.When dealing with digital shapesDh(X), implementation of these estimators becomes straightforward: choose a radius R, center a Euclidean (or digital) ball at chosen points of∂hX(e.g. centroids of linels or surfels), compute the quantities (area, volume, covariance matrix) and finally estimate curvature informationκ̃,H∼,κ̃1,κ̃2orK∼. However, several issues are hidden in this approach: What are meaningful values for R according to the shape size and geometry? Do points of∂hXconverge to pointsx∈∂Xfor which Lemmas 1 and 2 are valid? Does counting the number of pixels (resp. voxels) converge toAR(x)(resp.VR(x))? Does the digital covariance matrix converge to the expected one? The rest of the paper addresses all these questions.In [10], we have demonstrated that digital versions of estimators defined in Eq. (6) lead to efficient and multigrid convergent estimators for digitizations of smooth 2D shapes. In this section, we briefly describe the overall structure of this proof since similar arguments will be used in Section 3 to demonstrate that our digital principal curvature estimators do converge uniformly.First, we used existing results on digital area or volume estimation by counting grid points. Hence, for 2D shapesX∈Xand 3D shapesX′∈X, we have(14)Area^(Dh(X),h)=defh2Card(Dh(X))=Area(X)+O(hβ),Vol^(Dh(X′),h)=defh3Card(Dh(X′))=Vol(X′)+O(hγ),forβ=γ=1in the general case andβ=γ>1with further constraints onX(e.g.C3with non-zero curvature) [27,21,26].Then, we focused on the convergence of the area estimation on Euclidean shapes defined byBR(x)∩Xatx∈∂Xin dimension 2. We defined a digital curvature estimatorκˆR(Dh(X),x,h)by applying the area estimation by counting onBR(x)∩Xand Eq. (6), see [10, Eq. (11)]. We first demonstrated thatκˆR(Dh(X),x,h)converges toκ(X,x)(note that curvatures are evaluated at the same pointx∈∂X):Theorem 1Convergence ofκˆRalong∂X, [10]Let X be some convex shape ofR2, with at leastC2-boundary and bounded curvature. Then there exists positive constantsh0,K1andK2such that(15)∀h<h0,R=kmhαm,∀x∈∂X,κˆR(Dh(X),x,h)-κ(X,x)⩽Khαm,whereαm=β2+β,km=((1+β)K1/K2)12+β,K=K2km+3K1/km1+β, withβas above. In the general case,αm=13.Then, we showed that moving the digital estimation fromx∈∂Xtoxˆ∈∂hXdoes not change the convergence results:Theorem 2Uniform convergenceκˆRalong∂hX, [10]Let X be some convex shape ofR2, with at leastC3-boundary and bounded curvature. Then, there exists positive constantsh0and k, for anyh⩽h0, settingR=kh13, we have∀x∈∂X,∀xˆ∈∂hX,∥xˆ-x∥∞⩽h⇒κˆR(Dh(X),xˆ,h)-κ(X,x)⩽Kh13.In [10], we also presented similar results and convergence speed for mean curvature estimation in 3D from digital volume estimation.To demonstrate that principal curvature estimators can be defined from digital version of integral invariants, we use exactly the same process:1.We first demonstrate that digital estimations of covariance matrix are multigrid convergent (Sections 3.1 and 3.2).Then, we give explicit error bounds on both the geometrical moments and the covariance matrix when we change the reference point fromx∈∂Xtoxˆ∈∂hX(Sections 3.3 and 3.4).Finally, we gather all these results to demonstrate that principal curvature estimators are uniformly multigrid convergent for allxˆ∈∂hX(Section 3.5).In this section, we derive digital principal curvature and principal direction estimators by digital approximation of local covariance matrices. Convergence results rely on the fact that digital moments converge in the same manner as volumes [26]. In the whole section, the considered family of shapesXis composed of compact subsets ofR3with positive reach, the boundary of which isC3and can be decomposed into a finite number of monotonous (convex/concave) pieces. Compactness is required so that the boundary belongs to the shape.C3-smoothness is required in the truncated Taylor expansion of Pottmann et al. [39,38] relating covariance matrix and curvatures. Positive reach guarantees that two pieces of boundaries are not too close to each other, and this fact is also required in the previous truncated Taylor expansion (although this is not stated in their paper). The finite decomposition into monotonous pieces induces that integrals as limits of sums converge at speed at leastO(h).Following the same principles as the area and volume estimators by counting, we define the digital(p,q,s)-momentsmˆp,q,s(Z,h)of a subset Z ofZ3at step h as(16)mˆp,q,s(Z,h)=defh3+p+q+sMp,q,s(Z),whereMp,q,s(Z)=def∑(i,j,k)∈Zipjqks. To shorten expressions, we denote byσthe sump+q+s, which will always be an integer in{0,1,2}.There exist multigrid convergent results for digital moments that are similar to the multigrid convergence of the area and the volume estimator (see Eq. (14)). Since their speed of convergence depends on the orderσof the moment, we may thus write for some constantμσ⩾1[26]:(17)mˆp,q,s(Dh(Y),h)=mp,q,s(Y)+O(hμσ).The involved constantsμiare at least 1 in the general case, and some authors have established better bounds in places where the Gaussian curvature does not vanish (e.g. see [28] whereμ0=3825-∊, or [36], Theorem 1, whereμ0=6643-∊).We wish to apply this formula to the setA=BR(x)∩X, whose size decreases with h. Big “O” notation in Eq. (17) hides the fact that the involved constant depends on the shape size, scale and maximal curvature. Hence, we need to normalize our moment estimation so that the error is no more influenced by the scale:(18)mˆp,q,sDh(A),h=h3+σMp,q,s1h·BR(x)∩X∩Z3=h3+σMp,q,sRh·B11R·x∩1R·X∩Z3=R3+σhR3+σMp,q,sDh/RB11R·x∩1R·X=R3+σmˆp,q,sDh/RB11R·x∩1R·X,hR.The shapeB11R·x∩1R·Xtends toward a half-ball of radius 1 as R decreases. Therefore, we may apply Eq. (17) on Eq. (18) and consider that the involved constant does not depend on R or h. Note that we use below the obvious relationmp,q,s(R·Y)=R3+σmp,q,s(Y).(19)mˆp,q,sDh(A),h=R3+σmp,q,sB11R·x∩1R·X+R3+σOhRμσ=mp,q,s(BR(x)∩X)+O(R3+σ-μσhμσ)=mp,q,s(A)+OR3+σ-μσhμσ.Eq. (19) is a multigrid convergence result for digital moments of subsetsBR(x)∩Xvalid for R decreasing as h decreases.For any digital subsetZ⊂Z3, we define its digital covariance matrixJ^(Z,h)at step h as:(20)J^(Z,h)=defmˆ2,0,0(Z,h)mˆ1,1,0(Z,h)mˆ1,0,1(Z,h)mˆ1,1,0(Z,h)mˆ0,2,0(Z,h)mˆ0,1,1(Z,h)mˆ1,0,1(Z,h)mˆ0,1,1(Z,h)mˆ0,0,2(Z,h)-1mˆ0,0,0(Z,h)mˆ1,0,0(Z,h)mˆ0,1,0(Z,h)mˆ0,0,1(Z,h)⊗mˆ1,0,0(Z,h)mˆ0,1,0(Z,h)mˆ0,0,1(Z,h)T.We now establish the multigrid convergence of the digital covariance matrix toward the covariance matrix. In this case, we know the exact position of the point x at which both digital and continuous covariance matrices are computed. The following theorem only takes into account the integral approximation error.Theorem 3Multigrid convergence of digital covariance matrixLetX∈X. Then, there exists some constanthX, such that for any grid step0<h<hX, for arbitraryx∈R3, for arbitraryR⩾h, with non-emptyA(R,x)=defBR(x)∩X, we have:∥J^(Dh(A(R,x)),h)-J(A(R,x))∥⩽∑i=02O(R5-μihμi).The constants hidden in the big O do not depend on the shape size or geometry.∥·∥denotes the spectral norm on matrices.To simplify expressions, we setA=defA(R,x),Ah=defDh(A(R,x)). We begin by translating the sets A andAhtowards the origin w.r.t. x. We must use a vector that takes into account the digitization, hence we shiftAhby the vectorxh, the integer vector closest toxh, and we shift A with the vectorhxh. We further setA∼h=defDh(A)-xhandA∼=defA-hxh. Following these definitions,(21)J^(Dh(A(R,x)),h)=J^Ah,h=J^A∼h+xh,h.Using the translation invariance for covariance matrix4For any finite subsetY⊂R3, for any vectorv∈R3,J(Y+v)=J(Y).4which implies that for any finite subsetZ⊂Z3, for any integral vectorv∈Z3, for anyh>0,J^h(Z+v)=J^h(Z), we have(22)J^A∼h+xh,h=J^A∼h,h.Writing down the definition of digital covariance matrix (see Eq. (20)), we have:(23)J^A∼h,h=mˆ2,0,0A∼h,h⋱-1mˆ0,0,0A∼h,hmˆ1,0,0(A∼h,h)⋮⊗mˆ1,0,0(A∼h,h)⋮T.We remark thatA∼h=Dh(A)-xh=DhA-hxh=Dh(A∼). Consequently, we apply convergence result of Eq. (19) onto setA∼and insert them into Eq. (23) to get(24)J^(A∼h,h)=m2,0,0(A∼)+O(R5-μ2hμ2)⋱-1m0,0,0(A∼)+O(R3-μ0hμ0)(m1,0,0(A∼)+O(R4-μ1hμ1))2⋱.Note that constants in big O are independent of X thanks to the normalization. In Eq. (24), we recognize easilyJ(A∼)plus other terms. We bound the other terms from above with two facts: (i) the radius R is greater than h, (ii) sinceA∼is non-empty and close to the origin, we apply Eqs. (A.2) and (A.4) of Lemma 4 for setA∼⊂BR(t)witht=x-hxh, noticing that∥t∥∞⩽h2. We obtainJ^(A∼h,h)=J(A∼)+O(R5-μ2hμ2)+O(R5-μ0hμ0)+O(R5-μ1hμ1).We conclude sinceJA∼=JA-hxh=J(A)(Translation invariance for covariance matrix).□In general, we do not know the exact position of x but only some approximationxˆtaken on the digital boundary∂hX. We therefore examine the perturbation of the moments when they are evaluated at a shifted positionx+t.Lemma 3For any measurable subsetX⊂R3and any vectortwith normt=def∥t∥2⩽R, we have for0⩽p+q+s=defσ⩽2:(25)mp,q,sBR(x+t)∩X=mp,q,sBR(x)∩X+∑i=0σO∥x∥itR2+σ-i.The proof is detailed in Appendix A.2.We now establish the multigrid convergence of the digital covariance matrix toward the covariance matrix even when the exact point x is unknown.Theorem 4Multigrid convergence of digital covariance matrix with position errorLetX∈X. Then, there exists some constanthX, such that for any grid step0<h<hX, for arbitraryR⩾h, for anyx∈∂Xand anyxˆ∈∂hX,∥x-xˆ∥∞⩽h, we have:∥J^Dh(A(R,xˆ)),h-J(A(R,x))∥⩽O∥x-xˆ∥R4+∑i=02OR5-μihμi,withA(R,y)=defBR(y)∩X. The constants hidden in the big O do not depend on the shape size or geometry.The fact that∥x-xˆ∥∞⩽h⩽Rinduces thatA(R,x)andA(R,xˆ)are both non-empty. We cut the difference of two matrices into two parts:∥J^(Dh(A(R,xˆ)),h)-J(A(R,x))∥⩽∥J^(Dh(A(R,xˆ)),h)-J(A(R,xˆ))∥+∥J(A(R,xˆ))-J(A(R,x))∥.For the first error term, we apply directly Theorem 3 at pointxˆ. For the second term, we sett=defxˆ-x,t=def∥t∥. Then we use the invariance of the covariance matrix with respect to translation to shift the problem toward the origin:∥J(A(R,xˆ))-J(A(R,x))∥=∥J(A(R,x+t))-J(A(R,x))∥=∥J(A(R,x+t)-x)-J(A(R,x)-x)∥=∥J((BR(x+t)-x)∩(X-x))-J((BR(x)-x)∩(X-x))∥=∥J(BR(t)∩(X-x))-J(BR(0)∩(X-x))∥=∥J(BR(t)∩X′)-J(BR(0)∩X′)∥,withX′=defX-x. We will apply Lemma 3 for the different moments in the covariance matrix J. We denote byYtthe setBR(t)∩X′and byY0the setBR(0)∩X′.∥J(Yt)-J(Y0)∥=m2,0,0(Yt)-m2,0,0(Y0)⋱-1m0,0,0(Yt)m1,0,0(Yt)⋮⊗m1,0,0(Yt)⋮T+1m0,0,0(Y0)m1,0,0(Y0)⋮⊗m1,0,0(Y0)⋮T.MatrixJ(Yt)-J(Y0)contains differences of geometrical moments of order two (e.g.m2,0,0(Yt)-m2,0,0(Y0)) and quantities in the form ofΔ=defm1,0,0(Yt)2m0,0,0(Yt)-m1,0,0(Y0)2m0,0,0(Y0)(component(1,1)inJ(Yt)-J(Y0)matrix). From Lemma 3, every error on second-order moments is inO(tR4). To boundΔquantities, we first observe that∣m0,0,0(Yt)-m0,0,0(Y0)∣=πR2(t+O(t2)+O(tR2))using Theorem 7 in [38]. Hence,Δ=m1,0,0(Yt)2m0,0,0(Y0)+O(tR2)-m1,0,0(Y0)2m0,0,0(Y0)=O(tR2)m1,0,0(Yt)2m0,0,0(Y0)2+m1,0,0(Yt)2-m1,0,0(Y0)2m0,0,0(Y0).Sinceab+O(x)=ab+ab2O(x), using Lemma 3 anda2-b2=(a-b)(a+b),Δ=O(tR4)+(m1,0,0(Yt)+m1,0,0(Y0))m1,0,0(Yt)-m1,0,0(Y0)m0,0,0(Y0)=O(tR4)+(O(tR3)+O(R4))m1,0,0(Yt)-m1,0,0(Y0)m0,0,0(Y0)(Lemma 4,Eq.(A.2))We use Lemma 3 and the fact that sincet<R,m0,0,0(Y0)=O(R3), we have:Δ=O(tR4)+(O(tR3)+O(R4))O(tR3)m0,0,0(Y0)=O(tR4).The same bound is found for all terms of the matrix. Putting everything together gives the result.□Following the truncated Taylor expansion of Lemma 2, we define estimators of curvatures from the diagonalization of the digital covariance matrix.Definition 3Let Z be a digital shape, x some point ofR3andh>0a grid step. ForR⩾h, we define the integral principal curvature estimatorsκˆR1andκˆR2of Z at pointy∈R3and step h as(26)κˆR1(Z,y,h)=6πR6(λˆ2-3λˆ1)+85R,(27)κˆR2(Z,y,h)=6πR6(λˆ1-3λˆ2)+85R,whereλˆ1andλˆ2are the two greatest eigenvalues ofJ^(BR/h1h·y∩Z,h)).We recall the following result of matrix perturbation theory [3,43,4]:Theorem 5Lidskii-Weyl inequalityIfλi(B)denotes the ordered eigenvalues of some symmetric matrix B andλi(B+E)the ordered eigenvalues of some symmetric matrixB+E, thenmaxi∣λi(B)-λi(B+E)∣⩽∥E∥.We prove below that our integral principal curvature estimators are multigrid convergent toward the principal curvatures along the shape.Theorem 6Uniform convergence of principal curvature estimatorsκˆR1andκˆR2along∂hXLetX∈X. Fori∈{1,2}, recall thatκi(X,x)is the i-th principal curvature of∂Xat boundary point x. Then, there exist positive constantshX,k,Ksuch that, for anyh⩽hX, settingR=kh13, we have∀x∈∂X,∀xˆ∈∂hX,∥xˆ-x∥∞⩽h⇒∣κˆRi(Dh(X),xˆ,h)-κi(X,x)∣⩽Kh13.We prove the result for the first principal curvature, the proof for the second one is similar. According to Definition 3,λˆ1andλˆ2are the two greatest eigenvalues ofJ^BR/h1h·xˆ∩Z,hwithZ=Dh(X)). We derive easily:J^BR/h1h·xˆ∩Dh(X),h=J^BR/h1h·xˆ∩1h·X∩Z3,h=J^1h·(BR(xˆ)∩X)∩Z3,h=J^Dh(BR(xˆ)∩X),h=J^Dh(A(R,xˆ)),h.Theorem 4 indicates thatJ^(Dh(A(R,xˆ)),h)andJ(A(R,x))are close to each other with a norm difference bounded byO(∥x-xˆ∥R4)+∑i=02O(R5-μihμi). Since both matrices are symmetric by definition, Theorem 5 implies thatλˆ1andλˆ2are close to the eigenvaluesλ1(J(A(R,x)))andλ2(J(A(R,x)))with the same bound.5Note that since error bounds tend to zero as h tends to zero, the ordering of the eigenvalues in both matrices is the same for a sufficiently small h.5We thus write:κˆR1(Dh(X),xˆ,h)=6πR6(λˆ2-3λˆ1)+85R=6πR6λ2-3λ1+O(∥x-xˆ∥R4)+∑i=02O(R5-μihμi)+85R.We then substitute the truncated Taylor expansion of Lemma 2 into the latter equation and we bound∥x-xˆ∥by h. After some calculations, we get:(28)κˆR1(Dh(X),xˆ,h)=κ1(X,x)+O(R)+O(h/R2)+∑i=02O(hμi/R1+μi).SettingR=khα, we optimize the valueαto minimize all errors. Sinceμi⩾1for the shape X, the optimal value isα=13. The bound follows. □It is worth to note that the preceding error bound could be improved at neighborhoods where the Gaussian curvature does not vanish: the constantsμiare then closer to1.5. However, there is the issue of estimating more precisely the position ofxˆwith respect to x. In the best known case [30], uniform convergence with bound≈O(h0.434)can be expected for radius R with a size proportional toh0.434.In Theorem 6, we focused on multigrid convergence of principal curvature quantities. However, similar results can be obtained for principal curvature directions as well [15].We present an experimental evaluation of curvature estimators in 2D and 3D (mean and principal curvatures). We have implemented our Integral Invariant estimators (II) in the open-source library DGtal[46]. DGtal allows us to construct parametric or implicit shapes in dimension 2 and 3 for any grid step h. Furthermore, DGtal allows comparison with former approaches available in dimension 2: curvature from Most-centered Maximal Segment with length information (MDSS) [12,16], curvature from Most-centered Digital Circular Arc (MDCA) [41] and Binomial based convolution (BC) [18]; and in dimension 3: curvature from polynomial surface approximation (Jet Fitting) [8] using a binding between DGtal and CGal[45]. Jet Fitting approach requires a point set on which the polynomial fitting is performed. In our multigrid setting, the radius of the spherical kernel used to construct the local point-set around a given surface element is the same as the radius of the integral invariant kernel (R=khα).As described in Section 2, brute-force implementation is trivial. We first need to construct a kernel from a Euclidean ball with radius given byR=khαas described in theorem statements. Then, the digital object boundary is tracked and the kernel is centered on each surface elements. For 2D and 3D mean curvature estimators, the volumetric integral of the intersection between the kernel and the object is computed; for 3D principal curvature estimators, the covariance matrix of this intersection is computed and then eigenvalues and eigenvectors are deduced from it by diagonalization.With this approach, we achieve a computational cost ofO((R/h)d)per surface element (i.e. the size of the kernel at grid step h). However, we can take advantage of the digital surface structure to considerably speed up this algorithm: if we consider a surface tracker for which surface elements are processed by proximity (the current surface element is a neighbor of the previous one through a translation vectorδ→), the area/volume estimation can be done incrementally. Indeed, they are countable additive:Area^(Dh(X)∩BR(x+δ→),h)=Area^(Dh(X)∩BR(x),h)+Area^(Dh(X)∩(BR(x+δ→)⧹BR(x)),h)-Area^(Dh(X)∩(BR(x)⧹BR(x+δ→)),h).Similarly we have for moments:mˆp,q,s(Dh(X)∩BR(x+δ→),h)=mˆp,q,s(Dh(X)∩BR(x),h)+mˆp,q,s(Dh(X)∩(BR(x+δ→)⧹BR(x)),h)-mˆp,q,s(Dh(X)∩(BR(x)⧹BR(x+δ→)),h).Then, if we precompute all kernelsDh(BR(0±δ→)⧹BR(0))for someδ→displacements (based on surface element umbrella configurations, 8 in 2D and 26 in 3D for∥δ→∥∞=h), the computational cost per surface element can be reduced toO((R/h)d-1). Finally, in the ideal case of a Hamiltonian traversal of the surface, only the first surfel has to be computed using kernelBR(xˆ)and every subsequent neighboring surfel is processed using sub-kernelsDh(BR(0±δ→)⧹BR(0)).In our experimental evaluation, we need to compare the estimated curvature values with expected Euclidean ones on parametric curves and surfaces on which such curvature information is known. We have chosen three 2D shapes to perform our evaluation: an ellipse (Fig. 2(a)) which matches the theorem’s hypotheses (convexC3shape), a flower (Fig. 2(b)) and an accelerated flower (Fig. 2(c)) which do not satisfy the hypotheses exactly (C3but non-convex shapes). In 3D, we chose a sphere (Fig. 2(d)), a rounded cube (Fig. 2(e)) and Goursat’s surface (Fig. 2(f)). As in 2D, the sphere and the rounded cube match the theorem’s hypotheses, and we have a non-convex shape with Goursat’s surface. Equations, parameters and shape domains of these Euclidean objects are given in Table 1. In order to quantitatively interpret error measurements given in the graphs, we detail expected minimum and maximal curvature values in this table. To compensate subpixel/subvoxel digitization effects when digitizing a continuous object, we evaluate the estimators on digitization of 10 random translations of the continuous objects (continuous objects are translated by a vector randomly selected in[-1,1]2or[-1,1]3). Estimated quantities are compared to expected Euclidean ones for each pair of digital/continuous contour points (xˆ∈∂hXandx∈∂X). In our experiments, we consider two different metrics on point-wise errors to get a global error measurement. Firstl∞metric is used to quantify worst-case error since it reflects the uniform convergence of Theorems 2 and 6. In other experiments, we also considerl2error to get an average error analysis.From Theorems 2 and 6, theory indicates that the best candidate forαisαm=def13to ensure a multigrid convergence inO(h13). We first need to confirm this setting consideringαvalues in12,25,13,27,14. Fig. 3(a, b and c) presents results for the worst-case (l∞) distance between the true expected curvature values and the estimated ones. For multigrid ellipses, we observe experimental convergence for severalαvalues (except forα=12). As suggested by Theorem 2, forα=αm=def13the behavior of thel∞error is experimentally inO(h13). The theorem is defined in the general case, but the big O error can be improved with some further hypothesis on the shape. This could explain why better convergence speeds seem to be obtained whenα=27and14. For non-convex shapes (flower and accelerated flower), we still observe the convergence. Interestingly, valuesαgreater than13(and thus larger digital kernel size) seems to lead to slightly better estimations. The theoretical rationale behind this observation should be explored in future works.We performed the same analysis in 3D for the mean and principal curvature estimators. Section 3 in [10] and Theorem 6 theoretically prove thatα=13leads to anl∞error at least inO(h13). Fig. 4gives results ofl∞error for variousαvalues. We observe that for both convex and non-convex shapes,α=13provides expected convergence speed in mean and principal curvatures estimation. As in 2D, better convergence speed can be obtained withα=27and14.In Fig. 3(d, e and f) we compare the proposed 2D curvature estimator (II withα=13) with BC, MDSS, MDCA estimators for thel2(mean error) andl∞error metrics. In these noise-free shapes,l∞convergence speeds of MDCA is close to II. We observe a convergence for BC, but with lower convergence speeds. Note that MDSS exhibits no experimental convergence here. We observe for ellipses that BC provides betterl2results than II for high h values, but the behavior of both curves show that II will be better when h is refined. For flower and accelerated flower, we have the same behavior forl2than forl∞error.In all graphs, we had to stop the computations for BC and MDCA for the following reasons: in our implementation of BC, the mask size becomes too large for small h values which induces memory usage issues. For MDCA, circular arc recognition in DGtal is driven by a geometrical predicate based on a determinant computation of squared point coordinates. Hence, small h values lead to numerical capacity issues and thus instability (which could be solved considering arbitrary precision integer numbers but would lead to efficiency issues). The proposed integral invariant estimator does not suffer from these kinds of issues. Note that for the finest experimenth=0.00017, digital shapes are defined in a digital domain 235,2952. At this scale, the digital ellipse has 648,910 contour elements.Fig. 5illustrates the comparison with Jet Fitting on mean and principal curvatures withl∞error metric on a sphere (a, d and g), a rounded cube (b, e and h) and Goursat’s surface (c, f and i). We notice Jet Fitting performs better on a sphere for mean and principal curvatures than our estimators. On a rounded cube or Goursat’s surface and for the mean curvature, Jet Fitting has similar convergence speed than II. However, II has slightly lowerl∞errors. For principal curvatures, we observe that our estimator significantly outperforms principal curvatures from Jet Fitting. If we look at mean errors (l2metrics) in Fig. 6, similar behaviors can be observed. Note that for the finest experimenth=0.04, digital shapes are defined in a digital domain5003. At this scale, the digital rounded cube object has 1,125,222 surface elements.We have also evaluated the behavior of our estimators on noisy data. Given a digital binary object, our noise model consists in swapping the grid point value at p with probability defined by a power lawβ1+dt(p)for some user-specifiedβ∈[0,1](dt(p)corresponds to the distance of p to the boundary of the original digital shape). Such noise model, so-calledKanungonoise [22], is particularly well-adapted to evaluate the stability of digital geometry algorithms [24]. In Fig. 7(a and d), examples of noisified objects are given. Our experimental setting can be described as follows: for both the flower in 2D and the rounded cube in 3D (h=0.1for both), we slightly increase the noise parameter from 0 to 1 and we plot thel∞error. In dimension two (Fig. 7(e)), we observe that estimators based on geometrical object recognition (MDSS and MDCA) are highly sensitive to contour perturbations. Both BC and II are extremely robust to small noise but forβgreater than 0.5, II significantly outperforms BC. In dimension 3, we observe that both II and Jet Fitting approaches lead to quite stable results. Indeed, some computations in the Jet Fitting approach rely on a point-set PCA (Principal Component Analysis) which also provides robust statistics.In Fig. 8, we detail timings in logscale of various estimators on the flower object in 2D and the rounded cube in 3D. As expected, approaches based on object recognition in dimension 2 (MDSS and MDCA) provide faster computations. We also observe that II is a bit slower but has an asymptotic behavior much more favorable that BC. In dimension 3 (Fig. 8(b)), we observe that Jet fitting and II behaviors are similar and that II is 10 times faster than our implementation of Jet fitting.Finally, Fig. 9shows mean curvature and principal directions mapped on various shapes (rounded cube, Goursat’s surface, Leopold surface) and on an object (Stanford bunny). From these experiments, we can see that principal directions are nicely captured by covariance matrix eigenvectors.

@&#CONCLUSIONS@&#
