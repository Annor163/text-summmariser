@&#MAIN-TITLE@&#
Training of support vector machine with the use of multivariate normalization

@&#HIGHLIGHTS@&#
We analyze SVM (support vector machines) techniques.We propose a multivariable normalization of the inputs both during the training and classification processes.The multivariable normalization applied to a real SVM is equivalent to the use of a SVM that uses the Mahalanobis distance measure.The study confirms the improvement achieved in the classification processes.

@&#KEYPHRASES@&#
Support vector machines,Normalization,Mahalanobis distance,Classification algorithms,

@&#ABSTRACT@&#
SVM (support vector machines) techniques have recently arrived to complete the wide range of classification methods for complex systems. These classification systems offer similar performances to other classifiers (such as the neuronal networks or classic statistical classifiers) and they are becoming a valuable tool in industry for the resolution of real problems. One of the fundamental elements of this type of classifier is the metric used for determining the distance between samples of the population to be classified. Although the Euclidean distance measure is the most natural metric for solving problems, it presents certain disadvantages when trying to develop classification systems that can be adapted as the characteristics of the sample space change. Our study proposes a means of avoiding this problem using the multivariate normalization of the inputs (both during the training and classification processes). Using experimental results produced from a significant number of populations, the study confirms the improvement achieved in the classification processes. Lastly, the study demonstrates that the multivariate normalization applied to a real SVM is equivalent to the use of a SVM that uses the Mahalanobis distance measure, for non-normalized data.

@&#INTRODUCTION@&#
SVMs have rapidly become tools for general use in the field of pattern recognition. The simplest application of this technique is the problem of binary classification (where only two classes are defined). The underlying idea [1] is to find a hypothesis H that minimizes the probability of empirical error (the probability that H contains an error in a test set selected at random). In Ref. [2], it is demonstrated that minimizing the empirical error is equivalent to finding the hyperplane (Figs. 1 and 2) that lies at the maximum distance from the closest training samples for the two classes.Let there be n samples, independent and identically distributes, taken from an unknown probability distribution P(x, y), consisting of pairs conformed by a vectorxi∈Rn, i.exiT=(xi1,xi2,…,xin)1×nand a class label (x1, y1), (x2, y2), …, (xl, yl).We wish to construct a SVM that knows how to relate the values of the vectors xiwith the corresponding values of the labels yi, by means of a decisiony:Rn⟶{−1,1}, which lies between two hyperplanes that define a margin of maximum size.The simplest case [3], is where the data can be classified by a separating hyperplane, with the equation(1)ωT·x+b=0where ω is a vector normal to the hyperplane, called the weight vector, given byωT=(ω1,ω2,…,ωn)1×nand b is known as the bias.This hyperplane must be optimum, and so the values of ω and b must fulfil(2)min(ω,b)12∥ω∥2=min(ω,b)12ωT·ωsubject to the restrictions(3)−yi(ωT·xi+b)+1≤0fori=1,2,…,lThe solution to this optimization problem [4] is found at the saddle point of the Lagrangian function. In order to find this, we must minimize the Lagrangian with respect to ω and b, at the same time maximizing it with respect to αi.In this way, the optimum weight vector and the optimum bias will be defined by(4)ω*=∑i=1lαi*·xi·yi(5)b*=1k∑s=1k(ys−(ω*)T·xs)where k is the number of support vectors (withαi*>0with i=1, 2, …, l are the Lagrange multipliers).As expressed in Ref. [5], in the majority of cases it is not possible to separate using a linear frontier in the space dimension of the data. In this case, the SVM can project the vector of input data into non-linear regions, by means of mapping these points xi. Using a suitable non-linear projection, it will be possible to separate the support vectors in that hyperspace.To do this, we need to take the points and, using the functionφ:Rn⟶Rm, project them into the feature spaceRm. In this way, if we obtain an optimum separation hyperplane in the spaceRm, we can state that a region of non-linear separation also exists inRn.According to Ref. [5], the principal problem of this projection, known as the curse of dimensionality, is the potential increase in computation time. For this reason, a symmetrical function is used, known as a kernel[6], calculated from the points in the input space asK(u,v)=φ(u)·φ(v), which enables the operations to be executed in the input space. Thus, the scalar product does not necessarily need to be evaluated in the feature space (provided the kernels comply with the Mercer Conditions[7]).The region of non-linear separation can be found as the solution to the linear problem, but with φ(xi) instead of xi(6)min(φ(ω),b)12∥φ(ω)∥2=min(φ(ω),b)12φ(ω)T·φ(ω)and subject to the restrictions(7)−yi(φ(ω)Tφ(xi)+b)+1≤0forαi≥0,i=1,2,…,lThe optimum solution, as in the linear case, is found at the Lagrangian saddle point, and so the Lagrangian must be minimized with respect to φ(ω) and b, at the same time as maximizing it with respect to αi.So, the optimum bias is obtained as follows(8)b*=1k∑s=1kys−∑i=1lαi·yi·k(xsT·xi)where k is the number of support vectors.The classification is made using the same function as in the linearly separable case, but in this case, it is not possible to explicitly calculate the weight vector (for which we would need to know φ). Therefore, we employ the following expression [8]:(9)u(x)=∑i=1nαi·yi·k(xiT·x)+b*The normalization of the input data of the SVM is an option that is increasingly used in the classification process of SVMs. There are many articles dealing with this, suggesting a wide combination of proposals. Among the numerous proporsals for normalization we have reviewed in the literature, we can cite [9–12] to improve the accuracy or [13] to improve the speed up of the learning phase.We have studied some of these normalizations [14,9] of which we can highlight those proposed by Ref. [11]. These are the Min–Max Normalization and the Zero-Mean Normalization.The formulation of the Min–Max normalization is:(10)D′(i)=D(i)−min(D)max(D)−min(D)·(U−L)+Lwhere D′ is the normalized data matrix, D is the natural data matrix and U and L are the upper and lower normalization bounds.This type of normalization method is used to normalize a data matrix into a desired bound. The most popular bound is between 0 and 1. We also change bound values to between 0 and −1 or 1 and −1.The formulation of the Zero-Mean normalization is as follows:(11)D′=D−D¯σwhereD¯is the mean of the data matrix D and σ is the standard deviation of the same data matrix. In this normalization method, the mean of the normalized data points is reduced to zero. As a result, the mean and standard deviation of the natural data matrix are required.The established SVM at the current time uses Euclidean distance in its training and classification processes, a measure that carries the disadvantage of dependence on the unit of measurement of the variables. Thus, where there is no natural fixed unit, it is unjustified to use this metric.As proposed in Ref. [15], one way of avoiding this problem is to divide each variable by a term that eliminates the effect of the scale. This leads to the family of weighted Euclidean metrics, defined by:(12)dij=[(xi−xj)TM−1(xi−xj)]12where M is a diagonal matrix used to standardize the variables and make the measure invariable in the face of changes of scale. For example, if we put the standard deviations of the variables on the diagonal of M, the expression (12) becomes(13)dij=∑s=1nxis−xjsSs212=∑s=1nSs−2(xis−xjs)212which can be seen as Euclidean distance where the weight of each coordinate is inversely proportional to the variance (Zero-Mean Normalization [11]).Our proposal is a multivariate normalization, which means (as we will demonstrate below) that the SVM employs the Mahalanobis distance. The use of this metric has been considered by several authors. In Ref. [16] the use of Mahalanobis kernel is employed to deal with hyperellipsoidal regions. Ref. [6] defines the distance between a vector xiand its Mean VectorX¯by(14)di=[(xi−X¯)TS−1(xi−X¯)]12The ability of this distance metric to take the form of an element into account based on its correlation structure, means that it is a very interesting distance metric for SVM.Therefore, as demonstrate in following sections, the Multivariate Normalization that we might use is equivalent to training and classifying the SVM using the Mahalanobis distance.Multivariate normalization used in this section is a known method and is not a novel argument. Many classification techniques consider the transformation of the training set to improve performance of the classification (see [17]). In this work, the use of multivariate normalization is the means to attain our goal, that is, the use of the Mahalanobis distance in the training process of SVM, as discussed in the next section.Let S be the matrix of variances-covariances. The spectral decomposition of S is defined in the following form:(15)S=A·D·ATwhere D is a diagonal matrix containing the values of matrix S along its principal diagonal and matrix A is an orthogonal matrix whose columns are the normalised eigenvectors of matrix S.Multivariante standarization is defined in the following form:(16)yi=S−12·(xi−X¯)fori=1,2,…,lwhereS−12is the inverse ofS12. After this transformation, the new vectors yihave the Identity Matrix In×nas the matrix of variances and covariances.In this section, we demonstrate that the Euclidean distance of the normalized data coincide with the Mahalanobis distance of the original data.Given the individuals xiand xjand their standardized data yiand yj, Euclidean distance between individuals yiand yjis determined by:(17)dE=[(yi−yj)T·(yi−yj)]12=[(xi−xj)T·(S−1/2)·(S−1/2)T·(xi−xj)]12Thus, the Euclidean distance of Eq. (17) can be rewritten as:(18)dE=[(xi−xj)T·(S−1)·(xi−xj)]12which coincides with the Mahalanobis distance between the original vectors i and j; dM(xi, xj).Therefore, the Euclidean distance of the normalized data is the same as the Mahalanobis distance of the original.In this section, we demonstrate that a SVM that uses the Mahalanobis distance coincides with the multivariate normalization of the data, through the application of a SVM that uses Euclidean distance. Thus, a transformation of the training set is equivalent to the replacement the distance employed by an SVM, that is, is equivalent to the replacement the Euclidean distance by the Mahalanobis distance.Let there be a SVM, which uses Euclidean distance but whose input data11Both the training series and the various elements to be classified.are normalized using our multivariate normalization(19)yi=S−1/2·(xi−X¯)We have, therefore, (yi, zi) where i=1, 2, …, l and and zi=1 or zi=−1.We look for the optimal separating hyperplane(20)ωT·y+b=0Therefore, we have to minimize(21)12·∥ω∥2=12ωT·ωsubject to the restrictions(22)−zi·(ωT·yi+b)+1≤0for1,2,…,lLastly, working through this problem, we observe that it is equivalent to minimizing(23)LD(α)=12·∑i=1l∑j=1lαi·αj·zi·zj·yi·yj−∑i=1lαi=12·αT·H·α+dT·αwith dT=(−1, −1, …, −1)1×n, subject to the restrictions(24)∑i=1lαi·zi=0yαi≥0fori=1,2,…,lwhere H is a n×n matrix, and such that(25)Hij=zi·zj·yi·yj=zi·zj·[S−1/2·(xi−X¯)]T·[S−1/2·(xi−X¯)]Let the original data be (xi, zi) where i=1, 2, …, l and zi=1 or zi=−1. We search for the optimal separating hyperplane(26)ωT·(x−X¯)+b=0Firstly, we calculate the Mahalanobis distance of the means vector,X¯to the hyperplane (26). To do this, we transform vectorX¯and the hyperplane (26) by means of the Multivariate Normalization.VectorX¯, by means of the normalization(27)Y¯=S−1/2(X¯−X¯)is converted into the null vector 0.If we consider the normalization (19), we have(28)S1/2·y=x−X¯If we substitute (28) in the equation of the hyperplane (26) we obtain the following expression for the hyperplane(29)ωT·S1/2·y+b=0Therefore, we have to minimize(30)12·∥ωT·S1/2∥2=12·(S1/2·ω)T·(S1/2·ω)subject to the restrictions(31)−zi·(ωT·(xi−X¯)+b)+1≤0for1,2,…,lThis is equivalent to minimizing the following function(32)Lp(ω,b,α)=12·(S1/2·ω)T·(S1/2·ω)+∑i=1lαi[−zi·(ωT·(xi−X¯)+b)+1]=12·ωT·S·ω−∑i=1lαi·zi·ωT·(xi−X¯)−b·∑i=1lαi·zi+∑i=1lαiFor the minimization, we first apply the partial derivatives and make them equal to 0 obtaining the following results:(33)ω=∑i=1lαi·zi·S−1·(xi−X¯)(34)dLpdb=0→∑i=1lαi·zi=0Then, if we substitute expressions (33) and (34) in (32) we obtain(35)Lp(ω,b,α)=12·∑i=1lαi·zi·(xi−X¯)T·S−1/2·∑j=1lαj·zj·S−1/2·(xj−X¯)−∑i=1lαi·zi·∑j=1lαj·zj·(xj−X¯)T·S−1·(xi−X¯)−b·0+∑i=1lαi=−12·∑i=1lαi·zi·S−1/2·(xi−X¯)T·∑j=1lαj·zj·S−1/2·(xj−X¯)+∑i=1lαi=−12·∑i=1l∑j=1lαi·zi·αj·zj·[S−1/2·(xi−X¯)]T·[S−1/2·(xj−X¯)]+∑i=1lαiIn this way, we must maximize the expression (35), subject to the restrictions(36)∑i=1lαi·zi=0andαi≥0fori=1,2,…,lIn conclusion, this problem is equivalent to expression (23) and, therefore, we can conclude that the SVM using normalized data and Euclidean distance coincides with the SVM using Mahalanobis distance and the original data.The modification of the distance measure in the SVM algorithm must be analyzed from the point of view of algorithmic complexity and computational cost. The use of techniques to improve computation time, based on an efficient use of hardware resources of the system, as described in Ref. [18], may be warranted when the variation in the algorithm involves a substantial increase in calculation time.In the multivariate normalization proposed, the operations with a higher computational cost are:•The calculation of the variances-covariances matrix S, with order of complexity of O(l·n2).The spectral decomposition of S, with order of complexity of O(n3).In the training sets used in this work, n is a small value not greater than 10. For more extreme cases, as developed in Ref. [19], n is 26 and l was approximately 16,000. In this case, for a computer with 2, 4GHz, Intel Q6600 CPU, using Matlab2012b, the calculation of the variances–covariances matrix takes a time of 0.012s. This transformation, therefore, does not imply a high computational cost for practical purposes.Table 1shows the execution times (in seconds) for different values of n and l with the same computer.

@&#CONCLUSIONS@&#
In this article we have reviewed the theoretical foundations of SVMs and how they have become basic tools in the field of pattern recognition.It is widely known in the literature that SVMs have drawbacks in classification accuracy. So many scientists propose a large amount of normalizations of the inputs data of the SVMs in training and classification processes.Our approach proposes the use of multivariate normalization to achieve this improvement in the efficiency of classification. About this proposal for normalization, we can make three statements which are demonstrated in this article:•In Section 3, we have shown mathematically that the multivariate normalization of the inputs applied to a SVM with Euclidean distance is equivalent to the use of SVM with the Mahalanobis distance.The SVM using the new Mahalanobis metric has been tested on several populations and the results show that the SVM with the Mahalanobis distance improve the efficiency of the SVM with the traditional Euclidean distance and, also, improve the efficiency of the SVM with Euclidean distance and the best normalization studied in the literature.Finally, at the end of Section 3, we can see that the computational cost of the multivariate normalization of data input is trivial, compared with the average time of these processes.To sum up, we can say that the use of multivariate normalization on SVM with Euclidean distance is nearly identical to the use of the SVM with Mahalanobis distance in terms of training time and classification, and exactly equal in efficiency. A reformulation of the SVM using the Mahalanobis distance is not interesting, when we can continue working with the multivariate normalization and classical SVM, that is, the SVM and Euclidean distance.