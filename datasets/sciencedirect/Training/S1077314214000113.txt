@&#MAIN-TITLE@&#
A self-adaptive Gaussian mixture model

@&#HIGHLIGHTS@&#
Dynamic learning-rate adaptation to cope with fast illumination changes.Embedded global illumination change factor into GMM and shadow removal formulation.MDGKT as pre-processing to reduce noise in the temporal, spatial and spectral domains.

@&#KEYPHRASES@&#
Background subtraction,Gaussian mixture model,Shadow removal,Illumination sadden change,Object detection,

@&#ABSTRACT@&#
Most background modeling techniques use a single leaning rate of adaptation that is inadequate for real scenes because the background model is unable to effectively deal with both slow and sudden illumination changes. This paper presents an algorithm based on a self-adaptive Gaussian mixture to model the background of a scene imaged by a static video camera. Such background modeling is used in conjunction with foreground detection to find objects of interest that do not belong to the background. The model uses a dynamic learning rate with adaptation to global illumination to cope with sudden variations of scene illumination. The algorithm performance is benchmarked using the video sequences created for the Background Models Challenge (BMC) [1]. Experimental results are compared with the performance of other algorithms benchmarked with the BMC dataset, and demonstrate comparable detection rates.

@&#INTRODUCTION@&#
Detecting object motion is a fundamental task in video sequence analysis that is relevant to a wide range of applications, including video monitoring and surveillance of traffic and pedestrians, as well as other visual tracking tasks, such as gesture recognition for the human–machine interface. Visual surveillance and monitoring systems employ static CCTV (Closed-Circuit Television) cameras and object detection is performed using background modeling to separate moving objects from a nominally static background. A background model can be built by recording the value of image pixels that are un-changed over a reasonable period of time. Object detection then proceeds by detecting foreground pixels by comparison with the appropriate background values using background subtraction. However, this simple change detection approach is based on the assumption that the changing value of a pixel is only associated with object(s) moving through the scene. This is a false assumption, and it is necessary to be able to account for the effect of a host of other ‘disturbances’ to the background – pixel noise, illumination changes (both local or global), changing weather conditions (for outdoor scenes), and visual artifacts such as shadows and reflections. A robust system should be independent of the scene, and capable of dealing with movement through cluttered areas, objects overlapping in the visual field, gradual illumination changes (e.g. time of day, evening and night), sudden illumination changes (e.g. switching a light on or off, clouds moving in front of the sun), camera automatic gain control (e.g. camera white balance and auto-iris compensation, which are often applied to optimally map the amount of received light to the digitizer’s dynamic range), moving background (e.g. camera vibration, swaying trees, snow and rain), slow-moving objects, cast shadows and the reflection of objects from strongly-reflecting surfaces (e.g. a window).A widely-used method for estimating a reliable background image that can cope with these variable environmental conditions is based on Gaussian Mixture Models (GMM). The standard GMM [7] uses a single adaptation or learning rate that is a compromise between the different rates of illumination change that may occur in the scene. As a result, it needs to be tuned to the rate of variation in global illumination common in outdoor scenes. However, such scenes are subject to varying rates, associated with the diurnal cycle (long term) and local weather conditions (e.g. density of cloud cover (medium term) and cloud motion (short term), when the sun suddenly appears (or disappears) from behind an occluding cloud). If the pixel intensities of the scene background remain static for many image frames, the model variance becomes small and a sudden (short term) change in global illumination can then turn the entire frame into foreground. Alternatively, a low learning rate produces a model with a wide variance that will have a low sensitivity of detection. So, a single learning rate results in a trade-off in detection accuracy: for a high learning rate, the model updates too quickly, and slow-moving objects are absorbed into the background model, resulting in a high false negative detection rate. A low learning rate will fail to adapt to sudden illumination changes, and the detector is swamped, resulting in a high false positive rate.This paper presents an improved GMM algorithm that is less sensitive to sudden changes in the global illumination compared to the GMM presented in the paper [2,8–10,16,40]. It employs a spatio-temporal Gaussian smoothing algorithm and a self-adaptive GMM for background modeling. Foreground detection is improved by the integration of shadow and highlight suppression. The main contributions of this paper are: (1) dynamic learning-rate adaptation to cope with fast illumination changes; (2) embedding a global illumination change factor into the GMM and shadow removal formulation; (3) a multi-dimensional Gaussian kernel density transform as a pre-processor to reduce noise in the temporal, spatial and spectral domains. Extensive experimental results using the dataset of the Background Modeling Challenge [1] are used to evaluate the performance of the algorithm and present a comparative performance with other algorithms from the BMC [1].The paper is organized as follows: a literature review is given in Section 2; a description of the new algorithm is given in Section 3; extensive experimental results and quantitative evaluation using the BMC dataset are presented in Section 4; Section 5 concludes the paper.

@&#CONCLUSIONS@&#
We have proposed a new online, self-adaptive Gaussian mixture model and improved shadow removal algorithm to deal with sudden illumination changes. The algorithm has a dynamically adaptive learning rate and models global illumination changes of the background frame-by-frame. At the cost of only one additional parameter per Gaussian, this modification dramatically improves the convergence and accuracy of background subtraction whilst maintaining the same temporal adaptability. This is achieved by incorporating a modified adaptive schedule (a counter keeping track of the number of pixels that have contributed to a Gaussian component in temporal space) into a recursive filter. Comprehensive analysis of experimental results through statistical evaluation performance metrics obtained using annotated ground truth from BMC shows that our method leads to a significantly better performance than others.We believe that no perfect system exists. For instance, the F-measure values of synthetic sequences 412 and 422, and real videos 005 and 008 for SAM are still low. Background modeling and subtraction in itself is application-oriented. Some of these problems cannot be solved simultaneously because of the differing needs associated with the semantic interpretation of the moving foreground and background. In this paper we have only discussed the basic pixel-level processes and frame-level global illumination changes, including additional MDGKT pre-processing operations, but excluding other checks, such as ghost removal. Furthermore, we intend to combine low level and high level processes in order to identify vehicles that are stationary for long periods, e.g. at traffic lights.The segment of pseudocode for the SAG algorithm:// pseudo-code of SAG algorithmthreshold initialization: cT, c0, α, cf, dc;initial background;for (f=0; f<TotalFrameNumber; f++){input frame(f);// calculate global illumination factor g between modeled background and current framecalculate g;for (i=0; i<TotalPixelNumber; i++){BelongModel=false;BelongBackground=false;TotalWeight=0;input pixel(i)for (m=0; m<ModelNumber; m++)if not BelongModel{calculate MDif (TotalWeight<1−cf) and (MD<dc){ BelongBackground=true; }If (MD<dc){BelongModel=true;Update wm, βm, μm, σm, cm;TotalWeight=TotalWeight+wm}}if not BelongModel{wm+!=α;μm+1=x(t); σm+1=σ0; cm+1=1;}// foreground segmentationif not BelongBackgroundpixel(i)∊foreground;}update background;output foreground;}