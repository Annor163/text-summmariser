@&#MAIN-TITLE@&#
Clustering clinical trials with similar eligibility criteria features

@&#HIGHLIGHTS@&#
A feature-based approach identifies clinical trials with similar eligibility text.Identified clusters are effective according to crowd-sourced evaluation.Trial clusters can facilitate similarity-based trial search and systematic review.

@&#KEYPHRASES@&#
Medical informatics,Clinical trial,Cluster analysis,

@&#ABSTRACT@&#
ObjectivesTo automatically identify and cluster clinical trials with similar eligibility features.MethodsUsing the public repository ClinicalTrials.gov as the data source, we extracted semantic features from the eligibility criteria text of all clinical trials and constructed a trial-feature matrix. We calculated the pairwise similarities for all clinical trials based on their eligibility features. For all trials, by selecting one trial as the center each time, we identified trials whose similarities to the central trial were greater than or equal to a predefined threshold and constructed center-based clusters. Then we identified unique trial sets with distinctive trial membership compositions from center-based clusters by disregarding their structural information.ResultsFrom the 145,745 clinical trials on ClinicalTrials.gov, we extracted 5,508,491 semantic features. Of these, 459,936 were unique and 160,951 were shared by at least one pair of trials. Crowdsourcing the cluster evaluation using Amazon Mechanical Turk (MTurk), we identified the optimal similarity threshold, 0.9. Using this threshold, we generated 8806 center-based clusters. Evaluation of a sample of the clusters by MTurk resulted in a mean score 4.331±0.796 on a scale of 1–5 (5 indicating “strongly agree that the trials in the cluster are similar”).ConclusionsWe contribute an automated approach to clustering clinical trials with similar eligibility features. This approach can be potentially useful for investigating knowledge reuse patterns in clinical trial eligibility criteria designs and for improving clinical trial recruitment. We also contribute an effective crowdsourcing method for evaluating informatics interventions.

@&#INTRODUCTION@&#
The past few decades have witnessed heightened expectations for transparency in scientific research. Vast troves of clinical and research data have been digitized and made publicly available by governmental agencies, corporations, and private organizations. The availability of these data has generated a great need for innovative methods that leverage such Big Data to improve healthcare delivery and to accelerate clinical research [1]. However, gaining meaningful insights from this Big Data is fraught with challenges.For example, in one of the largest clinical trial repositories, ClinicalTrials.gov1http://clinicaltrials.gov/.1, there are more than 145,745 clinical trials as of May 2013. Information overload is an unsolved problem when searching for relevant clinical trials in this repository. Methods have been developed to address this problem [2–8], such as web-based EmergingMed2http://www.emergingmed.com.2, SearchClinicalTrials.org3http://searchclinicaltrials.org/.3, and the UK Clinical Trials Gateway4http://www.ukctg.nihr.ac.uk.4, and mobile device-based NCITrials@NIH5http://bethesdatrials.cancer.gov/app/.5, ClinicalTrials Mobile6http://www.clinicaltrials.com/industry/clinicaltrials_mobile.htm.6, and ClinicalTrials.app7http://www.iphoneclinicaltrials.com/.7. Although these methods are helpful in narrowing the search for trials, they require users to come up with effective queries, which can be a difficult task given the complexity of eligibility criteria [9] and of medical terminologies.One alternative to clinical trial search based on a user query is case-based search, which identifies trials similar to an example trial. Such an approach can remove the burden for query formulation from the user and is deemed to be useful in multiple usage scenarios. For clinical trial volunteers, a trial for which they qualify but cannot join due to closed enrollment, geographic distance from the recruitment site, or other practical reasons, can serve as a starting point in the search for trials recruiting similar patients. For clinical trial investigators, case-based search might help identify colleagues recruiting similar patients for related diseases and inform the eligibility criteria design of a new trial. For meta-analysis researchers, this method can identify studies with similar eligibility features and help uncover knowledge reuse patterns among related studies or improve the efficiency of systematic reviews.To support the aforementioned use cases, in this paper we present an automated approach to identifying clinical trials with similar eligibility criteria, across and within diseases, based on the similarity in semantic eligibility features. In the context here, a semantic feature is a clinically meaningful patient characteristic, such as a demographic characteristic, a symptom, a medication, or a diagnostic procedure, used to determine a volunteer’s eligibility for a trial. It contains either one word, (e.g., “cardiomyopathy”) or multiple words (e.g., “biopsy-proven invasive breast carcinoma”) [8]. We focused on similarity measures at the concept level because as noted by Korkontzelos et al. [10], decreasing the length of lexical units, from sentences to phrases or tokens, can solve the sparsity problem in identifying eligibility criteria that are important for a particular study, though a potential tradeoff of this method is that unimportant functional words and phrases are more frequent than meaningful ones in the biomedical domain.An important premise of our proposed approach is that numerical values in eligibility criteria, such as constants in expressions for age and laboratory results, are not necessary considerations for determining eligibility criteria similarity at the concept level. For example, our method does not differentiate “Age: 50–65” from “Ages: 10–17”, or differentiate “HbA1C>6.5” from “HbA1C<6.5”. For clinical trials with a small number of eligibility criteria features, this limitation might result in incorrect clustering of trials with semantically different eligibility criteria. However, eligibility criteria are rich in features, with an average of 38.5 features per trial on ClinicalTrials.gov. When two trials are deemed similar using our method, a majority of eligibility features must match; therefore, the differences in the attributes associated with any feature have minimal influence on overall trial similarity. In other words, it is unlikely that a trial recruiting patients aged 50–65 would match a trial recruiting patients aged 10–17 in all other eligibility features. The presence of many features helps our method distinguish trials recruiting different target populations despite the disregard for numerical values in any given feature.The rest of this paper is organized as follows. We first describe our processes for semantic feature extraction and trial clustering based on feature similarities. Then we introduce a crowdsourcing method for evaluating the similarities of the resulting clusters using Amazon’s Mechanical Turk. On this basis, we present the performance metrics for this method.Fig. 1illustrates the methodology framework. We obtained the free-text eligibility criteria for all registered trials (N=145,745 as of September 2013) listed on ClinicalTrials.gov. We then used the Unified Medical Language System (UMLS) Metathesaurus to recognize all biomedical concepts, which serve as the semantic features, and assigned a suitable UMLS semantic type for each of them. On this basis, we constructed a trial-feature matrix to cluster trials using pairwise similarity. Our design rationale and implementation details are further provided below.Although UMLS’s parser, MetaMap, is the mostly widely used parser for biomedical concept recognition, we chose to develop our own concept recognition algorithm to avoid the limitations in MetaMap output as identified by Luo et al. [11]. For example, the criterion “Patients with complications such as serious cardiac, renal and hepatic disorders” was parsed by MetaMap Transfer (MMTx) as {Patients |Patient or Disabled Group} {with complications |Pathologic Function} {such as serious cardiac, renal |Idea or Concept} {and|} {hepatic disorders |Disease or Syndrome}. These results were not granular enough. Additionally, MMTx returned the phrase “such as serious cardiac, renal” as a single constituent, which was problematic.Excluding trials with no or non-informative text, such as “please contact site for information” (e.g., NCT00000221), for each remaining trial listed on ClinicalTrials.gov, we extracted its eligibility criteria text and preprocessed it by removing white spaces. We then performed sentence boundary detection for feature extraction. We first tried commonly used sentence boundary detectors such as the NLTK sent_tokenize function [12] but they alone were ineffective due to the variability in the formatting of the criteria text, e.g., some sentences lacked boundary identifiers or used different bullet symbols as separators. Therefore, we first applied bullet symbols or numbers as splitting identifiers and then applied NLTK on the remaining text chunks. For example, the eligibility criteria text of trial NCT00401219 contained both bullet symbols and a sentence boundary identifier. Therefore, the text was first split using the bullet symbols and then chunked using the identifiers. We improved the NLTK function to handle words like “e.g.” and “etc.”, which were incorrectly separated by the period symbol.We identified terms using a syntactic-tree analysis after part-of-speech (POS) tagging. This method was better than an n-gram-based method for pair-wise similarity calculation because the latter generated overlapping terms, which could lead to overestimation of similarity, or omitted candidate features that were not sufficiently frequent, which could cause underestimation of similarity. After testing several parsers, we utilized an open library8https://gist.github.com/alexbowe/879414.8to generate syntactic trees based on POS tags labeled by NLTK. Using predefined parsing rules, we traversed the syntactic trees and extracted phrases using NLTK WordNet lemmatizer and stemming modules. For example, from the sentence “a multi-center study of the validity” the algorithm would generate the following syntactic tree: {(S a/DT (NP (NBAR multi-/NN center/NN study/NN)) of/IN the/DT (NP (NBAR validity/NN)))}. From the tree, two noun phrases were extracted using NBAR tag (one predefined rule): “multi-center study” and “validity”.Being candidate semantic features, all terms were looked up in the UMLS using normalized substring matching rather than exact string matching. The advantage of this fuzzy term mapping strategy is that partial or complete term could be mapped to a UMLS concept. For example, we can extract a semantic feature “serious hypertensive disease”, where “hypertensive disease” is a UMLS concept, from term “serious systemic arterial hypertension” even if the latter as a whole does not exist in UMLS. For a term p, each word w was assigned as a start point for substring generation after checking with a list of English stop words, a list of non-preferred POS tags, and a list of non-preferred semantic types. For a start point wi, substring from wito an end point word wj(i<j<length(p), wj∊p) was generated as sijwith j from reverse direction (largest substring first). sijwas then processed through UTF decoding, word normalization (by NLTK WordNet Lemmatizer and word case modifier), word checking (on punctuations, numeric, English stop words, and medical related stop words), and acronym checking to match with UMLS concepts. If there was no match, it moved to substring si(j−1) for next matching until j=i+1. Once there was a match, the start point wiwas set to point wj(skip the start points between wiand wj); otherwise, i was set to i+1 for next round of matching until i equals length(p)−1.A term can be associated with multiple UMLS concepts with different semantic types. We performed concept disambiguation using a set of predefined semantic preference rules [13]. For example, a term “pregnancy test negative” was associated with two UMLS concepts, one being “pregnancy test negative” of the semantic type “Laboratory or Test Result” and the other being “reported negative home pregnancy test” of the semantic type “Finding”. In the UMLS Semantic Network, “Laboratory or Test Result” is a subtype of “Finding”. Hence, the more specific concept “pregnancy test negative” was assigned to this term.We did not distinguish between inclusion and exclusion criteria for semantic feature extraction because not all trials, such as trial NCT00000114, had separate inclusion and exclusion criteria sections. The extracted unique semantic features were used for generating a trial-feature matrix and for calculating trial similarity. In the trial-feature matrix, each row corresponds to a set of semantic features from a certain trial and each column shows a certain feature existing in different trials. If a trial ticontains semantic feature sfm, row i and column m was recorded as 1, otherwise as 0.There are plenty of measures of semantic similarity between concepts used in Natural Language Processing [14–18]. Pedersen et al. [19] presented the adaptation of six domain-independent measures and showed that an ontology-independent measure was most effective. Particularly for text clustering, Huang [20] compared 5 widely used similarity measures on 7 datasets and showed that the Jaccard similarity coefficient achieved best score on a well-studied dataset containing scientific papers from four sources. We adopted the Jaccard similarity coefficient for calculating pairwise similarity as it can assess both similarity and diversity [21]. For a collection of trials T = {t1, ti,…tj,…tk} containing k trials, the pairwise similarity Simi of any two trials tiand tjwas calculated as follows:Simi(ti,tj)=0,|SF(ti)|or|SF(tj)|=0|SF(ti)∩SF(tj)||SF(ti)∪SF(tj)|,otherwiseSF(ti) and SF(tj) are two sets of semantic features corresponding to tiand ti, respectively. If either SF(ti) or SF(tj) contains no semantic features, then the similarity is recorded as 0. Otherwise, it is calculated as the number of shared features (SF(ti)∩SF(tj)) divided by the number of features in the union (SF(ti)∪SF(tj)).Due to the large number of trials and the large volume of semantic features, calculating the similarity between every possible pair of trials would be computationally intensive. To improve efficiency, we first ranked all trials by their counts of semantic features. Trial pairs with a large difference in their feature counts were discarded, since the large count gap would lead to a low similarity as the shared features were too few compared to the union features. We defined two rules to select similar trial pairs: |SF(ti)|>2*|SF(tj)| and |SF(ti)|<|SF(tj)|/2, indicating that trial pairs with similarity below 0.5 were considered to have unsatisfactory similarity and discarded.There are many clustering models based on connectivity, centroid, distribution, and so on [22–26]. Methods such as K-means and hierarchical clustering were also assessed. Inspired by the known algorithm Nearest Neighbor Search (NNS) [27], for each unique trial, we constructed a cluster by using this trial as the center and by identifying all its near neighbors. To measure nearness, we calculated the distance between each neighbor and the central trial using the following formula: distance=1-similarity, where the similarity was the previously calculated pairwise similarity between the trial pair. For any central trial x, only trials whose similarities to the trial were greater than or equal to a predefined similarity threshold δ were included in the cluster centered on x. Therefore, we refer to these clusters as center-based clusters. Connected center-based clusters were merged to form similarity-based clinical trial network using the DBScan algorithm [28]. In order to facilitate visualization and statistical analyses of clusters, we removed structural information (i.e., center vs. neighbor) and identified trial sets with distinctive membership compositions from all center-based clusters and named these sets as unique clusters. Fig. 2illustrates the center-based and unique clusters for example trials.The Amazon Mechanical Turk (MTurk9https://www.mturk.com/.9) is an online crowdsourcing platform that enables human workers to perform human intelligence task (HIT) [29]. It has been shown to be effective for similarity evaluation. For example, Snow et al. [30] reported high agreement between MTurk non-expert annotations and existing gold standards provided by experts on five natural language tasks including a word-similarity evaluation task. MTurk has also been used for evaluating biomedical informatics research. For instance, Maclean and Heer [31] presented crowdsourcing patient-authored text medical word identification tasks to MTurk non-experts and achieved results that were comparable in quality to those achieved by medical experts. Therefore, we used MTurk for evaluating the similarity within our clusters.Regarding the limitations of using MTurk, Mason and Suri [32] pointed out some of these in their systematic review of MTurk with respect to efficiency improvement, quality assurance, security, ethics and privacy. They also suggested possible techniques to control submission quality, which were partially applied in our evaluation. Lee [33] summarized the 9 benefits and 4 limitations of using MTurk, including “instructions needed clarification over several tests”. Following these suggestions, we designed a two-phase evaluation and defined answer rejection rules to ensure data quality from MTurk. In the first phase, we determined the optimal similarity threshold. In the second phase, we evaluated the clusters generated using the optimal threshold.Based on empirical results we selected three candidate thresholds for optimization: i.e., 0.7, 0.8 and 0.9. We generated trial pairs having a similarity equal to each threshold. To obtain a representative sample, we plotted the distribution of average word counts per pair of texts as a box-plot. We then selected a total of 20 pairs of texts, 5 from each quartile, from this distribution curve for each of the three thresholds. In total we generated 60 pairs of texts for evaluation.The evaluation set was published as a HIT on the MTurk website with a reward of $1.20 offered for completion of the entire HIT. For each pair of texts, workers were asked whether they agreed with the statement “The texts in the pair are similar.” Workers were instructed to avoid differences in actual numbers (e.g., age, cutoffs for laboratory values, etc.) and to focus only on broad criteria concepts when calculating similarity. Available answer choices were “Strongly agree”, “Agree”, “Neither agree nor disagree”, “Disagree”, and “Strongly disagree”. To quantify the mean and standard deviation (SD) of the answers for each candidate threshold, we mapped each of the above choices to numbers 5, 4, 3, 2, and 1, respectively. The optimal similarity threshold was selected by comparing the means and SDs of the three candidate thresholds.To assure that workers were paying attention and not just randomly selecting answers, and to filter out “spammers” or “bots” [32], we inserted the hidden phrase “Waldo is hiding here” into one of the comparison text pairs. In the instructions, workers were informed that this phrase would appear in some of their eligibility criteria texts. They were instructed to select the answer choice “Found Waldo” upon seeing the phrase in the text rather than any of the choices pertaining to similarity. The instructions also explained that workers who failed to discriminate between pairs with or without the hidden phrase would result in rejection, without payment, of their work and a negative review of their performance posted to their profile. The entire HIT for any worker who failed the hidden phrase identification was deemed invalid and excluded. We continued recruiting workers until we got a total of 10 valid, completed HITs. This resulted in 10 evaluations by unique workers of the entire 60-pair set.Using the optimal threshold from Phase I, we generated unique clusters. To ensure a fair sampling of cluster sizes, the distribution of cluster sizes, measured in the number of nodes, was presented as a box plot. An evaluation set of 40 clusters, 10 selected from each quartile, was then generated. The evaluation set was published as a HIT on the MTurk website with a reward of $1.20 offered for completion of the entire HIT. For each cluster, workers were asked to rate if every cluster contained similar texts using a 5-Likert scale. As in Phase I, workers were instructed to ignore the differences in attribute values, e.g., age, cutoffs for laboratory values, and to focus only on inclusion eligibility concepts when determining similarity for each trial pair.The user interface design for the HIT is shown in Fig. 3. Clicking “click to view cluster” opens a page (Fig. 4) containing eligibility criteria texts from ClinicalTrials.gov for comparison.We employed the same hidden phrase identification method for quality control as described in Phase I. Additionally, by performing the HIT ourselves, we estimated that it would be difficult to perform an accurate assessment of 40 clusters in less than 20min. Accordingly we excluded the entire HIT if it was completed in less than 20min. We continued recruiting workers until we got a total of 20 (double the number in Phase I) valid and complete HITs. This resulted in 20 evaluations by unique workers for the entire 40 cluster set.

@&#CONCLUSIONS@&#
We developed an automated approach for clustering trials of similar eligibilitycriteria. Our evaluation confirmed the similarities within clinical trial clusters, which can be valuable for researchers and patients alike. Our experience with the Amazon Mechanical Turk confirmed that with careful data quality control, crowdsourcing was an effective approach to engage the public to participate in evaluations of biomedical informatics interventions. We hope our clinical trial search method can be integrated into clinical trial search engines to make clinical trial search easier for end users.T.H. designed and implemented the method, performed evaluations, data analysis, and results interpretation, and led the writing of the paper. A.R. participated in the evaluation design, performed data analysis and interpretation, wrote the paper with other authors. M.R.B. participated in the evaluation design, results analysis, and paper drafting. C.W. conceptualized the idea, supervised its design, implementation, and evaluation, and wrote the paper.