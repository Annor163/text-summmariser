@&#MAIN-TITLE@&#
A gradient approach for value weighted classification learning in naive Bayes

@&#HIGHLIGHTS@&#
Propose a new weighting method in the context of naive Bayes classification learning.Assign different weights for each feature value.A gradient approach for automatically calculating the weights of each feature value.Its performance is compared with that of other state-of-the-art methods.Experiments show the method could improve the performance of naive Bayes.

@&#KEYPHRASES@&#
Classification,Bayesian learning,Feature weighting,Gradient descent,

@&#ABSTRACT@&#
Feature weighting has been an important topic in classification learning algorithms. In this paper, we propose a new paradigm of assigning weights in classification learning, called value weighting method. While the current weighting methods assign a weight to each feature, we assign a different weight to the values of each feature. The proposed method is implemented in the context of naive Bayesian learning, and optimal weights of feature values are calculated using a gradient approach. The performance of naive Bayes learning with value weighting method is compared with that of other state-of-the-art methods for a number of datasets. The experimental results show that the value weighting method could improve the performance of naive Bayes significantly.

@&#INTRODUCTION@&#
In some classifiers, the algorithms operate under the implicit assumption that all features are of equal value as far as the classification problem is concerned. However, when irrelevant and noisy features influence the learning task to the same degree as highly relevant features, the accuracy of the model is likely to deteriorate. Since the assumption that all features are equally important hardly holds true in real world application, there have been some attempts to relax this assumption in classification. Zheng and Webb [22] provide a comprehensive overview of work in this area.The first approach for relaxing this assumption is to combine feature subset selection with classification learning. It is to combine a learning method with a preprocessing step that eliminates redundant features from the data.Feature selection methods usually adopt a heuristic search in the space of feature subsets. Since the number of distinct feature subsets grows exponentially, it is not reasonable to do an exhaustive search to find optimal feature subsets. In the literature, it is known that the predictive accuracy of naive Bayes can be improved by removing redundant or highly correlated features [13]. This makes sense as these features violate the assumption that each feature is independent on each other.Another major way to help mitigate this weakness, feature independence assumption, is to assign different weights to different features in classification learning. Since features do not play the same role in many real world applications, some of them are more important than others. Therefore, a natural way to extend classification learning is to assign each feature different weight to relax the conditional independence assumption. Feature weighting is a technique used to approximate the optimal degree of influence of individual features using a training set. While feature selection methods assign 0/1 values as the weights of features, feature weighting is more flexible than feature subset selection by assigning continuous weights. Therefore, feature selection can be regarded as a special case of feature weighting where the weight value is restricted to have only 0 or 1.Even though there have been many feature weighting methods proposed in the literature, many of them have been applied in the domain of nearest neighbor algorithms [20], and have significantly improved the performance of nearest neighbor methods.In this paper, we propose a new paradigm of weighting method, called value weighting method. While the current weighting methods assign a weight to each feature, we assign a weight to each feature value. Therefore, the value weighting method is a more fine-grained weighting method than the feature weighting method. While there have been a few work focused on feature weighting in the literature, to our best knowledge, there has been no work which assigns different weight to each feature value in classification leaning. We extended the current hypothesis space of classification learning into next level by introducing a new set of weight space to the problem. Therefore, this paper proposes a new dimension of weighting method by assigning weights to feature values.The new value weighting method provides a potential of expanding the expressive power of classification learning, and possibly improves its performance. Since features weighting methods improve the performance of the classification learning, we will investigate whether assigning weights to feature values can improve the performance even further.The main contribution of this paper is to provide a new paradigm of weighting method in classification learning. In this paper, we study the value weighting method in the context of naive Bayesian algorithm. We have chosen naive Bayesian algorithm as the template algorithm since it is one of the most common classification algorithms, and many researchers have studied the theoretical and empirical results of this approach. It has been widely used in many data mining applications, and performs surprisingly well on many applications [5].There have been only a few methods for combining feature weighting with naive Bayesian learning [10,11,21]. The feature weighting methods in naive Bayes are known to improve the performance of classification learning. We will investigate, in this paper, whether the value weighting method provides enhanced performance in the context of naive Bayes.The basic assumption behind the value weighting method in this paper is that each feature value has different significance with respect to class value. When we say a certain feature is important/significant, we believe that the importance of the feature can be decomposed. For instance, suppose we are to learn about a rare form of pregnancy-related disease, and try to calculate the importance of feature gender. Obviously, if the value of gender is male, that observation has no effect with respect to the target feature(pregnancy-related disease). On the other hand, the value of female in gender feature surely has significant impact to the target feature. If we assign the same weight to each feature value, we will lack the capability to discriminate the predicting power residing across feature values.The rest of this paper is structured as follows. In Section 2, we describe the related work on weighting methods in naive Bayesian learning. Section 3 shows the basic concepts of value weighted naive Bayesian learning, and Section 4 discusses the mechanisms of the new value weighting method. Section 5 shows the experimental results of the proposed method, and Section 6 summarizes the contributions made in this paper.

@&#CONCLUSIONS@&#
