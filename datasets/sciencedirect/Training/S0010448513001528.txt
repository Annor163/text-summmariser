@&#MAIN-TITLE@&#
Progressive and iterative approximation for least squares B-spline curve and surface fitting

@&#HIGHLIGHTS@&#
A new progressive and iterative approximation method for least square fitting (LSPIA) is presented.LSPIA can handle a point set of large size.LSPIA is so flexible that it allows the adjustment of the number of control points, and a knot vector in the iterations.LSPIA is easy to make the fitting curve hold the shape preserving property.LSPIA can be performed in parallel efficiently.

@&#KEYPHRASES@&#
Progressive and iterative approximation,Least square fitting,Iteration,Geometric design,

@&#ABSTRACT@&#
The progressive and iterative approximation (PIA) method is an efficient and intuitive method for data fitting. However, in the classical PIA method, the number of the control points is equal to that of the data points. It is not feasible when the number of data points is very large. In this paper, we develop a new progressive and iterative approximation for least square fitting (LSPIA). LSPIA constructs a series of fitting curves (surfaces) by adjusting the control points iteratively, and the limit curve (surface) is the least square fitting result to the given data points. In each iteration, the difference vector for each control point is a weighted sum of some difference vectors between the data points and their corresponding points on the fitting curve (surface). Moreover, we present a simple method to compute the practical weight whose corresponding convergence rate is comparable to that of the theoretical best weight. The advantages of LSPIA are two-fold. First, with LSPIA, a very large data set can be fitted efficiently and robustly. Second, in the incremental data fitting procedure with LSPIA, a new round of iterations can be started from the fitting result of the last round of iterations, thus saving great amount of computation. Lots of empirical examples illustrated in this paper show the efficiency and effectiveness of LSPIA.

@&#INTRODUCTION@&#
Progressive and iterative approximation (PIA) method  [1,2] is an efficient and intuitive method for data fitting. It cannot only avoid the computational cost of solving a large system of linear equations, but generate a series of approximation curves or surfaces as well. However, in the classical PIA method, the number of the control points is equal to that of the data points. It is not feasible when the number of data points is very large. Although the classical PIA method is extended in Ref.  [3] to approximate a given data set, the limit of the generated curve (surface) sequence is not the least square fitting (LSF) result to the data set. Certainly, the least square fitting is one of the most commonly used mathematical tools in practice. Therefore, in this paper, we devise a progressive and iterative approximation method, namely, progressive and iterative approximation for least square fitting (abbr. LSPIA), whose limit is the least square fitting result to a given data set.Similar as the classical PIA method, LSPIA starts with an initial blending curve (surface), and constructs a series of fitting curves (surfaces) by adjusting the control points iteratively. In each iteration, the adjusting vector of each control point is a weighted sum of some difference vectors between the data points and their corresponding points on the fitting curve (surface). Compared with the traditional least square method, LSPIA has the following advantages:•LSPIA can handle point set of large size;LSPIA is so flexible that it allows the adjustment of the number of control points, and knot vector in the iterations;LSPIA is easy to make the fitting curve hold the shape preserving property;LSPIA can be performed in parallel efficiently.This paper is organized as follows. In Section  1.1, we briefly review the related work. Then we introduce the iterative method of LSPIA and show its convergence in Section  2. In Section  3, we study how to assign appropriate values for the weight of LSPIA. Afterwards, Section  4 presents some numerical examples. Section  5 discusses its advantages and shortcomings, and Section  6 concludes this paper.

@&#CONCLUSIONS@&#
