@&#MAIN-TITLE@&#
Biologically-inspired robust motion segmentation using mutual information

@&#HIGHLIGHTS@&#
Presents novel neuroscience inspired information theoretic approach to motion segmentation based on mutual information.New model of current findings in biological vision is presented and link established to existing motion segmentation algorithms.Comparative performance evaluation across four challenging datasets.Comparative performance evaluation against competing segmentation methods.

@&#KEYPHRASES@&#
Biologically-inspired vision,Background modelling,Segmentation,Surveillance,Performance evaluation,

@&#ABSTRACT@&#
This paper presents a neuroscience inspired information theoretic approach to motion segmentation. Robust motion segmentation represents a fundamental first stage in many surveillance tasks. As an alternative to widely adopted individual segmentation approaches, which are challenged in different ways by imagery exhibiting a wide range of environmental variation and irrelevant motion, this paper presents a new biologically-inspired approach which computes the multivariate mutual information between multiple complementary motion segmentation outputs. Performance evaluation across a range of datasets and against competing segmentation methods demonstrates robust performance.

@&#INTRODUCTION@&#
The ability to extract objects of interest from video sequences, using detected motion, remains an active area of research within the computer vision community. The capacity to provide real-time segmentations – silhouettes and bounding boxes – of objects (especially pedestrian) assists in the tracking and reasoning of the behaviour. Surveillance scenes often contain change that may be inaccurately detected as object motion such as changes in lighting, periodic motion, moving shadows and reflections. In addition the quality of surveillance footage is often poor, and at a low resolution resulting in noisy motion and ghosts. An example of these challenges is shown in Fig. 1. The extraction of objects of interest is frequently tackled by removing all irrelevant pixels in each frame. This is referred to as motion segmentation. To date no segmentation algorithm is robust under all these conditions.In this paper, we propose a new formulation of pixel-based foreground segmentation which is motivated by recent results in biological vision which exploit the mutual information between multiple segmentation channels. The paper is divided as follows. Firstly, Section 2 details the biological motivation and mapping to a combination of parametric background modelling approaches. This is followed in Section 3 by approaches to fusing the outputs of multiple segmentation algorithms and introduces the multivariate mutual information forumulation adopted in this work. In Section 4 the datasets, evaluation methodology and the results of experiments are presented before concluding in Section 5 with conclusions and recommendations for future research.The ability of primates to recognise objects of interest, regardless of illumination and background, drives much of the biologically inspired computational vision systems. A new biologically inspired vision system is introduced in this section that models current vision research which has not previously been examined by the computational vision community.In Section 2.1 the model of primate vision conventionally accepted by the computer vision community is presented. Section 2.2 provides descriptions of state of the art biologically inspired computational vision systems that refer to this model. Section 2.3 progresses onto accounts of current published neuro-biological, physiological and psychological vision research and highlights descriptions of retinal functions, inputs to the ventral and dorsal streams, and ventral and dorsal stream behaviour that have not been considered in modelling primate visual systems in the computer vision community. Based on this, a new model of understanding is presented and the behaviours of these retinal functions are summarised.It is widely acknowledged that the rods and cones (photoreceptors) of the primate retina detect light and cells of the inner retina providing the initial stages of the visual processing. The retinal ganglion cells convey this information, via pathways in the lateral geniculate nucleus, to the ventral and dorsal streams in visual cortex. Fig. 2represents a model of these traditionally accepted components, frequently referred to in biologically inspired computational vision systems.Within the retina, shown in Fig. 2 as the blue area, the photoreceptor rod cells respond to achromatic brightness and the photoreceptor cone cells respond to short (blue), medium (green) and long (red) chromatic wavelengths. These nerve impulses are passed onto the network of horizontal, amacrine and bipolar cells, which provide cumulative information to retinal ganglion cells, shown in Fig. 2 as the midget and parasol ganglion cells. The midget ganglion cells have been associated with providing chromatic information and parasol ganglion cells with luminance and contrast.The lateral geniculate nucleus (LGN), illustrated as the green area in Fig. 2, receives the assembled information from the ganglion cells, in the form of pathways. The parvocellular pathway is conventionally understood to receive information from the midget ganglion cells, and as such provides a means to direct colour information to the visual cortex. It is customary to describe the magnocellular pathway as a swiftly responsive structure, presenting the visual cortex with luminance and contrast information.Finally, the visual cortex (VC), emphasised as the purple area in Fig. 2, includes two different streams: the ventral stream, associated with form, and the dorsal stream associated with motion.Ref. [43] state that because bio-inspired vision models based on a vertebrates visual system are limited and require high computational cost, real-time applications are seldom addressed. As flies are capable of exploiting optical flow, which modelled by calculating the local image motion with Reichardt motion detectors (and referred to as Elementary Motion Detectors), they use this as inspiration and employ EMD as the first extraction primitive to characterise motion in a scene. Sequences are initially pre-processed by extracting edges within each frame using a Sobel edge extraction procedure. The Reichardt motion detector is then used to extract sideways moving features. Noise is removed from the resulting saliency map with a neural structure that allows the emergence of rigid bodies (independent moving objects in the scene) using “velocity channels”. The technique is limited to greyscale images and suffers from being unable to identify to objects moving in parallel at the same speed. The system proposed by [48] follows on from their own theory of a feed forward path of object recognition that accounts for the first 100–200ms of processing in the ventral stream of primate visual cortex. It is based on Hubel and Wiesels findings in 1962 of a cats visual cortex [31]. Unlike the conventionally accepted chromatic input to the primate ventral stream, the approach takes a grey scale input and uses a set of scale and position-tolerant feature detectors, to simulate the properties of V1 and V4 (Fig. 2 shows V1 and V4 within the ventral stream). A major limitation of the system for real-time application is the processing speed which is limited by some of its modules that typically take tens of seconds, depending on the size of the input image. The authors have yet to address whether the recognition results obtained can be extended to the analysis of video. Ref. [27] offer an improvement on the system proposed by [48] focusing on improving the biological Standard Model Feature (SMF) for scene classification in a video surveillance environment. They develop a new energy computation component to improve SMF in occlusion and disorder cases as basic SMF models can only handle shift and invariance. An energy function is used in order that patches for saliency are not chosen randomly. An earlier analysis of energy density is used to conduct a local energy measurement after the initial basic feature extraction stage. Again the technique is limited to greyscale images. Using accounts of the primate visual cortex [7] have developed a neurodynamical computational vision model of motion segregation in the dorsal stream, as described in [41]. The model includes two modules, corresponding to the primate visual cortex (highlighted as the purple area in Fig. 2): V1 represents a motion hypothesis on the same scale of resolution on which it was detected, and V5 uses a coarser spatial resolution, where the accuracy of both location and velocity is reduced by a factor of five in accordance with physiological findings of Albright and Destmone in [2]. The authors conclude that it is a step towards producing a biologically inspired model which may be capable of real-time computation. Ref. [54] use a principle referred to as Slow Features Analysis (SFA) which bears foundations in neuroscience. SFA extract slowly varying features from a quickly varying input signal. These features have been shown by [54] to reveal sensible motion components correlated with specific semantic classes such as complex flame motion, waterfalls and fountains. As perceptions vary on a slower timescale compared to input signals from the environment, the SFA model learns to generate a slower, more invariant output signal. Temporal variations created by motion are minimised to in order to learn the stable representations of objects in motion. Motion features are defined by threading together short temporal sequences of SFA outputs. The motion features can be interpreted as spatio-temporal atoms describing the stable motion components inside a small space time window. Again this model relies on grey scale video as an input. The authors state that employing it for motion segmentation is a direction for future work. In [61] features of objects are extracted “in a way similar to that of the ventral stream processing”, referring to Diddays two visual stream model [18] published in 1975 and Mishkins slightly earlier publication than previously mentioned, with Ungerleider, in [56]. They use an RGB image input and proceed with a cortex-like centre surround operation in the spatiotemporal domain, by sub-sampling the image data into various spatial scales resulting in a set of images with horizontal and vertical scale reductions. Sets of features are extracted from the spatiotemporal stream and manipulated across various scales to detect those which locally stand out from their surround, similar to that of an edge detector. The authors state that due to the lack of a full understanding about the object recognition process in the visual cortex, the recognition mechanism that was implemented was a statistical classifier (SVM). In contrast Benoit et al. [8] recognise that consideration must be taken of the processing of the retinal signals that occur in primate vision, in order to assist further processing of that input, in a primate biologically inspired manner, in the visual cortex. They base their retinal architecture on Meads silicon model [40] albeit improved in terms of spatial and temporal properties. Their system contains two processing modules, one based on the retina for motion information extraction and the second representing a model of the V1 cortex area providing motion event detection. Their focus on the retinal processing includes passing information to their parvocellular channel model and magnocellular channel model from the midget ganglion cells model and parasol ganglion cells model respectively. These are shown in Fig. 2 in green. This transformed information then is presented to their V1 model of the visual cortex. The system concentrates on using grey level image processing as the authors state the cell actions at the retinal level are unknown and further investigation is required to produce a better model.Current neurobiology, visual neuroscience, physiology and psychology research provide descriptions of the input to the ventral and dorsal streams that have not been considered in computational vision systems modelling primate visual systems. Ganglion cell types other than midget and parasol cells also project to the LGN [44,17,13]. Ref. [17] provides a detailed description of these cell types, referred to as bistratified ganglion cells. They project their information to a further pathway in the lateral geniculate nucleus which is referred to as the koniocellular pathway [44,17,13,28,42,12]. A new illustration representing these recognised processes, including the bistratified ganglion cells and the koniocellular pathway is shown in Fig. 3.The retinal ganglion cells function in a distinct manner. The received wavelength signals can be used in the course of perceiving form or motion, independent of their role in the subjective experience of colour. Contra-distinctively to the traditional accepted processes, the networked routing provides the midget cells with some contrast information [34], alongside the bistratified and parasol cells and therefore contrast information is present within both the ventral and dorsal streams. In addition prominent computation has been found to occur in the retina: the detection of object motion while rejecting background motion (resulting from subtle eye movements) [6] through specific interactions of amacrine and bipolar cells and presented to the ganglion cells. The koniocellular layer has been found to project to both the ventral and dorsal streams [28]. Finally recent primate vision research suggests there is communication between the dorsal and ventral streams, contrary to the traditionally accepted definitions used by the computer vision community of independent luminance motion information and colour object information occurring in the dorsal and ventral streams respectively. Ref. [39] ascertain that both luminance and chromatically defined motion is analysed in the dorsal stream and [21] provide evidence that the dorsal stream participates in object recognition and some dorsal–ventral integration may be considered. Furthermore the study by [63] states that the continuous interchange of information between the two streams is necessary and provides evidence that interaction is present in order to produce adaptive behaviour, for example, in order to elaborate the position in space and the shape of a 3D object. In effect the individual streams of information are weaved back together.The current understanding of the individual behaviours of the three types of ganglion cells is described in detail in a vast array of vision research literature. These components in turn project this information to their respective lateral geniculate nucleus (LGN) streams, and these three streams have been ascertained by the neuroscience vision research community to have distinct behaviours and output. In this section brief descriptions of these components and their respective LGN streams and behaviours are presented.Parasol retinal ganglion cells receive many inputs and are responsively fast. They react to achromatic information and low contrast stimuli from the rods, and medium and long wavelength cones. They are unable to transmit information about wavelength independent of intensity and as such are not very sensitive to changes in colour. These cells are more sensitive to light since they are three times larger in diameter to the midget retinal ganglion cells. This information is relayed to the magnocellular pathway which is a fast system which contributes to the perception of luminance and motion derived from both achromatic and chromatic wavelengths, though it is unable to transmit any chromatic wavelength signals [44,34,17,13,12].Midget retinal ganglion cells are involved in colour encoding. They react to chromatic information from the rods, and medium and long wavelength cones (green and red cones respectively) in the retina. They have low sensitivity because of their small receptive fields, but because of that they are densely packed and their resolution ability is higher. They respond weakly to changes in contrast unless that change is great. However, though these cells are found predominantly in the fovea of the retina, those located in the periphery show a non-opponent luminance response, indistinguishable from the parasol cells. The red/green colour opponent information and achromatic contrast detection information, provided by the synergy of the medium and long wavelength cones in the fovea, and those of the periphery able to distinguish brightness only, are relayed through the slow parvocellular pathway. This pathway transmits information about long and medium wavelengths and fine detail. Motion perception information is presented but is far weaker than that of the magnocellular pathway and is dependent on the available chromatic contrast [44,34,17,13,12].Bistratified retinal ganglion cells are involved in colour perception. They receive inputs from all rods and cone types but respond to rods and small wavelength cones (blue cones) 23 only. They have the lowest resolution ability, their density is extremely low and they have very large receptive fields. They have moderate to low spatial resolution and react to moderate changes in contrast. This information is projected to the koniocellular pathway which contributes to colour perception dependant on the small wavelength cone output and contributes to motion perception [44,34,17,13,42,12]. Table 1summarises the functions of the magnocellular, parvocellular and koniocellular streams in the Lateral Geniculate Nucleus.Recent research in [63,12] have shown that the output of the magnocellular, koniocellular and parvocellular pathways provide mutual information to both ventral and dorsal streams, in order to supply the visual cortex with robust data about objects of interest and their location. Modelling this behaviour a form of multivariate mutual information is employed to enable the quantification of the amount of mutual information provided by the foreground segmentations of the modelling approaches described in this section. Background models may be seen to be analogous with the retinal suppression of global image motion as described by [6]. Using RGB colour space video sequences as input, the function of each of the parvocellular, magnocellular and koniocellular streams may each be modelled in a similar statistical manner. This section provides details of how these streams may be mapped to computational vision pixel-based background models.A background statistical model, which approximates behaviour of the parvocellular stream function [34], is able to distinguish between the brightness and its chromaticity of any one pixel, over time. This relates most closely to the method of [29]. It is able to separate its wavelength (colour) information to include pixels with changes in luminance and contrast within its background model. The remaining pixels, with changes in colour and a limited amount of motion information. Fig. 4represents a graphical representation of the brightness distortion and chromaticity distortion in three dimensional RGB colour space.Eiis the initial (background) colour value for pixel i, andIiis the current colour value of the image. The line OE from the origin toEirepresents the chromaticity line. Brightness distortion is a scalar valueαand scales the point along OE where the orthogonal line fromIiintersects OE. Chromaticity distortionCDiis the orthogonal distance between the observed colour and the line OE. The values forαand CD are calculated for each of N background framesαi=IR(i)μR(i)σR2(i)+IG(i)μG(i)σG2(i)+IB(i)μB(i)σB2(i)μR(i)σR(i)2+μG(i)σG(i)2+μB(i)σB(i)2whereσR(i),σG(i)andσB(i)are the standard deviation andμR(i),μG(i)andμB(i)are the means of theith pixel’s red green and blue values computed over N background framesCDi=IR(i)-αiμ(i)σR(i)2+IG(i)-αiμ(i)σG(i)2+IB(i)-αiμ(i)σB(i)2and then normalised to find a single threshold for all pixelsai=∑i=0Nαi-12Nαi^=αi-1aibi=∑i=0NCDi2NCDi^=CDibiThe method constructs histograms of the normalisedα^andCD^values and takes a detection rate as input to automatically select thresholds. For segmentation, incoming pixels are used to calculateαi^andCDi^values which are compared to those of the background model. The pixel classification for the ith pixel as defined by [29] is:1.Original background if bothαi^andCDi^are within a threshold of those in the background model.Shadows or shaded background if the chromaticityCDi^is within the threshold, but the brightnessαi^is below.Highlighted background if the chromaticityCDi^is within the threshold, but the brightnessαi^is above.Moving foreground object if the chromaticityCDi^is outside of the threshold.The resulting motion segmentation (Fig. 5) from the original frame (Fig. 1) show the model is able distinguish subtle differences in colour due to its motion sensitivity, but because of its motion sensitivity (due to both the temporal resolution and contrast sensitivity) parts of fluttering tape in the wind appear as foreground. Both the illumination and motion sensitivity provide the foreground segmentation with shadows.A statistical model that presents foreground segmentation approximating behaviour of the magnocellular stream function is one that is able to provide high contrast information but does not distinguish between colour and its intensity. It must be sensitive to changes in luminance and motion [34]. This most closely relates to the mixture model approach of Stauffer and Grimson [51]. Gaussian mixture models (GMM) s are able to model each component distribution as a soft classification; that is they are able to produce a distribution without specifying exactly what each cluster must represent. Yet as a whole, the mixture model covers the entire set of features (colour, brightness, intensity and luminance) that the data represents. The clusters formed represent more than one feature of information, and in this way the model becomes sensitive to contrast and motion. The resulting motion segmentations show that the model is able distinguish subtle differences in colour due to its motion sensitivity. Both the illumination and motion sensitivity provide the foreground segmentation with shadows. The recent history of a pixel is modelled by a mixture of K Gaussians (K usually varies from 3 to 5). The mixture is weighted by the frequency with which each of the Gaussians explains the background. The probability of observing a foreground pixel x is:(1)P(x)=∑j=1KwjN(x,μj,Σj)where w is the weight of the Kth Gaussian distribution,μis the mean,Σis the covariance matrix and N is a multivariate Gaussian density function.The resulting motion segmentation (Fig. 6) from the original frame (Fig. 1) show the model is able distinguish subtle differences in colour due to its motion sensitivity, but because of its motion sensitivity (due to both the temporal resolution and contrast sensitivity) parts of fluttering tape in the wind appear as foreground. Both the illumination and motion sensitivity provide the foreground segmentation with shadows.Similar to that of the Gaussian Mixture Model, the Colour Mean and Variance (CMV) algorithm, described in [58] captures the brightness, motion and colour information but only for a single colour channel. In this way the algorithm is able to provide foreground segmentation, similar to the behaviour of the koniocellular pathway [34]. Encapsulating features in distinct distributions, using one independent channel value, removes the ability to capture some of the colour contrast information in the model, enabling any subtle changes to appear as foreground. The changes in the objective luminance of a pixel provide additional necessary motion information, but it is not as precise a measure as perceived brightness change and as such the motion sensitivity is coarser. The resulting motion segmentations show the model is able distinguish between some subtle differences in colour, however is of lower resolution and provides low resolution shadow information from its motion sensitivity. CMV builds a statistical background model to represent an independent Gaussian distribution for each normalised colour channel (R,G,B) and a Gaussian distribution of the luminance (A) of each normalised pixel colour:(2)n(x,μ,σ)=12πσ2exp-(x-μ)2/2σ2where x is the value of a single channel R, G, or B, or luminance (A),μis the mean andσis the standard deviation of that channel. A pixel is classified as foreground if it is found to be more than 3 standard deviations of the R, G, B or A distributions.The resulting motion segmentation (Fig. 7) from the original frame (Fig. 1) show the model is able to distinguish between some subtle differences in colour, but is of lower resolution (shown by the merging of moving objects in close proximity in Fig. 7) and provides low resolution shadow information from its motion sensitivity.A number of approaches have been adopted in the literature for combining or fusing the outputs of multiple motion segmentation algorithms. Ref. [38] exploit optimal algorithm selection and key parameters tuning. A library of segmentation algorithms are fine tuned against predetermined ground truth images. The features extracted, alongside the optimal algorithm parameters, are saved as a case. They are ranked by a number of criteria. For each image a new case is created composed of a vector of image features, the chosen algorithm, and its optimised parameters. A multilayer perceptron (MLP) neural network is trained with this stored knowledge for algorithm selection. As the technique relies on predetermined ground truth this rules out generality. A Support Vector Machine (SVM), used by [5] views the feature information as two sets of vectors in an n-dimensional space. It constructs a separate hyper-plane in that space which maximises the margin between the two data sets. Ref. [22] employ Expectation Maximisation (EM) as a fusion engine. Principal Component Analysis (PCA) is first applied to perform dimensionality reduction to improve the performance of EM and reduce the computational load. It is claimed that the approach applied to fusion of three popular optical flow algorithms (where the U and V component images are treated as image planes and EM applied to them) reduces the percentage of missing target pixels by 33%, although only one outdoor driving sequence has been used for evaluation. Boosting is an alternative. In [62] each base classifier must be trained, sequentially, using feature points that are weighted. The weight of a feature point is increased if a previous classifier misclassifies it. Once all of the classifiers are trained, their decisions can be combined through a weighted majority vote method or others. Popular boosting methods Adaboost and LogitBoost both have structural space, a cost function, and a selection algorithm. The AdaBoost algorithm minimises an upper bound of the target misclassification error, and LogitBoost minimises a negative binomial log-likelihood, as cost functions. Serre, Wolf, Bileschi, Riensenhuber and Poggio model a neurobiological design of a primate cortex [48]. It is designed using hierarchical alternating layers of simple units and complex units. Simple units (16 Gabor filters for each layer) combine their inputs with a (bell shaped) tuning function to increase selectivity. Complex units pool their inputs (from the output of the previous Simple unit layer) through a MAX function. The image (grey scale only) is propagated through the hierarchical architecture. Standard Model Features (SMFs) are extracted from the complex units and classified using SVM or boosting (Gentle boosting providing the best performance). It was discovered that because there are variations in the amount of clutter and in the 2D transformations, it is beneficial to allow the classifier to choose the optimal features extracted from either the high or low level SMFs at a point in time, to improve the performance. A major limitation of the system in the use of real world applications remains its processing speed which is typically tens of seconds per image. Ref. [32] fusion of motion segmentation approach is based on a K-nearest-neighbour-based fusion procedure that mixes spatial and temporal data taken from two input label fields. The first one is a spatial segmentation of a frame at time t which contains regions of uniform brightness while the second label field is an estimated version of the motion partition. The two segmentation maps are estimated separately with an unsupervised Markovian segmentation routine. The fusion occurs with an iterative optimisation algorithm called Iterative Conditional Mode whose maximum local energy for each site, at each iteration, is obtained with a K-nearest neighbour algorithm.Mazeed, Nixon and Gunn [3], whose work is closest to the work described in ths paper, employ Bayes. Two background models are produced using a Mixture of Gaussians algorithm and a brightness and chromaticity algorithm referred to as Statistical Background Disturbance Technique (SBD). When the classifiers agree (pixel is foreground or background) a decision is set accordingly. When classifiers disagree, conditional probability for the chosen class by each class is calculated. The product of each class of conditional probabilities provide the parameters for the final decision(3)argmaxi∈1,2p(x|wCLSFi)P(wCLSFi)where w is a class of either a background (BG) or a foreground (FG) for the classifierCLSFi. The maximum conditional probability for each classifier is used with the classifer’s confidence measureP(wCLSFi)to find the decision for the algorithm. The main limitation of the approach is that it limited to combination of two classifiers and that the priors are calculated using an exhaustive search method based on the training data to obtain the optical values giving minimum classification errors.While Bayesian inference, as well as other methods details above, have been exploited for classification in motion segmentation, application of mutual information to fuse multiple motion segmentation outputs has not been studied. The approach taken here in selecting mutual information as a method to combine multiple classifiers (the output from the LGN pathways) is threefold: Firstly, in the same way the recognised behaviours of the LGN pathways influenced the modelling of such, the identified interactions between these channels of visual information that occur in the visual cortex influenced the choice of mathematical approach we use to model such findings. Recent neurophysiological and vision research highlight that the output of all three LGN pathways is shared within the visual cortex [39,21,63,12]. Indeed [15] state that when considering the encoding of visual information in the brain, the statistical independence between luminance and chromatic edges in natural scenes vary depending on the dataset of natural images used and “mutual information” may be found. These findings rule out choosing methods of combining classifiers where the classifiers are competing and a single classifier is found to be the “expert” at each instance, for example, Behaviour-Knowledge Space [47] and those such as the majority vote and K-nearest neighbour algorithm. As the information theory principle of mutual information measures the amount of information one random variable contains about another it is seemingly a sensible mapping to choose to model the neurophysiological and vision findings. Secondly, consideration is taken regarding the data used from a statistical view point. Multiple classifiers that produce probabilities as an output may be combined using the product or average of the probabilities or the “Naïve Bayes” rule however these combiners require that the individual classifiers use mutually independent subsets of features [35]. This is not the case with the output from the LGN pathways as each pathway produces an interpretation of identical data that each is presented with. Mutual information may also be described as a technique that measures the mutual dependency of one random variable with another and it is certainly the case with the LGN outputs that there will be some commonality. In addition mutual information classifiers have been found to provide an objective solution [30]. Finally, as the LGN pathways are modelled using real-time computational vision techniques, it is pertinent to choose a combining method such as mutual information which, unlike techniques such as boosting, requires no additional training on the data presented and may provide a fused result “on-the-fly”.In information theory the entropy of a discrete random variable X is the measure of the amount of uncertainty associated with the value of X. Shannon entropy, denoted by H, of a discrete random variable X, includes a probability measure. If p represents a probability mass function of X then Shannon entropy can be described in terms of a discrete set of probabilities(4)H(X)=-∑i=1p(xi)logp(xi)Mutual information I measures the amount of information that can be obtained about one random variable by observing another. Mutual information can be expressed as(5)I(X;Y)=H(X)-H(X|Y)=H(Y)-H(Y|X)=H(X,Y)-H(X|Y)-H(Y|X)=H(X)+H(Y)-H(X,Y)whereH(X)andH(Y)are the marginal entropies,H(X|Y)andH(Y|X)are the conditional entropies, andH(Y|X)is a measure of what Y does not say about X.I(X;Y)is non-negative. Mutual information is a well established technique for medical image registration of several modalities [46,14] due to its insensitivity to changes in lighting condiitons ability to address a wide range of non-linear image transformations. It has also been shown to be well suited to registration of images of the same modality [46].Trivariate mutual information is described in various ways in the literature with reference to both the definition and in the use of notation. Fig. 8provides examples of the assorted ways that [46] discovered it had been defined and used in his survey of multivariate mutual information in terms of entropies. The darker shaded areas represent the mutual information in each case. Ref. [46] asserts that a property of the definition of Fig. 8a. is that it is not necessarily non-negative. In Fig. 8b. the deeper shaded middle section denotes that this area is counted twice.Fig. 9provides examples of how the notation varies between authors. The diagrams labelled Fig. 9a–c depict a bivariate and two trivariate examples respectively and the notation to describe them given by Studholme [52]. He uses a ‘;’ to separate the arguments for mutual information, while a ‘,’ denotes a union of two variables. The notation used by Pluim [46] differs in that to describe the same examples in the diagrams labelled Fig. 9d–f ‘,’ is used as the separator between the arguments and is not a union. Further to the differences found in notation in the literature, Ref. [36] states that the termI(X;Y;Z)is illegal. For clarity in this work the notation used throughout is that of [36] which is consistent with [52] and later authors [20,45]. It is noted that [14] uses the ‘,’ notation as a separator.In this work the variablesX,Yand Z are the probability in each LGN stream (parvocellular, magnocellular, and koniocellular) that a pixel is foreground. Here mutual information is used as a measure of the information or interaction between any two or all three LGN streams. To this end, CMI (Combined Mutual Information) is defined as a linear combination of trivariate mutual information for all three LGN streams and bivariate mutual information for each pair of LGN streams such that none of the constituent entropies are counted twice. To avoid the use of any terms which could be considered illegal, the only trivariate mutual information used here will be of the formI(X;Y|Z)which is the mutual information between X and (Y given Z) and is considered a legal term [36].Bivariate mutual informations areI(X;Y),I(X;Z)andI(Y;Z)(Fig. 10a–c respectively) and are expressed in terms of Shannon entropies as (see Figs. 11 and 12)(6)I(X;Y)=H(X)+H(Y)-H(X,Y)I(X;Z)=H(X)+H(Z)-H(X,Z)I(Y;Z)=H(Y)+H(Z)-H(Y,Z)Trivariate mutual informations areI(X;Y|Z),I(X;Z|Y)andI(Y;Z|X). In terms of Shannon entropiesI(X;Y|Z)is defined as(7)I(X;Y|Z)=-H(Z)+H(X,Z)+H(Y,Z)-H(X,Y,Z)The quantityI(X;Y)-I(X;Y/Z)is shown in Fig. 10d. and may also be defined as(8)I(X;Y)-I(X;Y/Z)=I(X;Z)-I(X;Z|Y)=I(Y;Z)-I(Y;Z|X)Therefore a consistent quantity CMI, with no overlapping entropies may be defined as(9)CMI=I(X;Y)+I(X;Z)+I(Y;Z)-2[I(X;Y)-I(X;Y|Z)]CMI can thus be expanded to give(10)CMI=I(X;Y)+I(X;Z)+I(Y;Z)-2[I(X;Y)]+2[I(X;Y|Z)]=-I(X;Y)+I(X;Z)+I(Y;Z)+2[I(X;Y|Z)]which can be expressed in terms of Shannon entropies as(11)CMI=-H(X)-H(Y)+H(X,Y)+H(X)+H(Z)-H(X,Z)+H(Y)+H(Z)-H(Y,Z)+2[H(X,Z)+H(Y,Z)-H(X,Y,Z)-H(Z)]and can be simplified as(12)CMI=H(X,Y)+H(X,Z)+H(Y,Z)-2H(X,Y,Z)Since(13)H(X)=-∑i=1p(xi)logp(xi)CMI may be rewritten as(14)CMI=-∑x,yp(x,y)logp(x,y)-∑z,yp(y,z)logp(y,z)-∑x,zp(x,z)logp(x,z)+2∑x,y,zp(x,y,z)logp(x,y,z)and yields an expected value over all possible instances ofX,Yand Z.The quantities given below, that are summed to find CMI, exist at all pointsx,y,z.(15)p(x,y)logp(x,y)p(x,z)logp(x,z)p(y,z)logp(y,z)p(x,y,z)logp(x,y,z)The two variable quantities are each defined on a 2D grid and the three variable quantity is defined on the 3D space (x,y,z). Hencep(x,y,z)logp(x,y,z)may have a different value at all points (x,y,z) where asp(x,y)logp(x,y)is only defined on thex,ygrid and values at any point (x,y) are the same for all z. It is therefore possible to define a quantity pVC at each point based on the point wise constituents of CMI.(16)pVC=-p(x,y)logp(x,y)-p(y,z)logp(y,z)-p(x,z)logp(x,z)+2p(x,y,z)logp(x,y,z)This provides a non-negative result and is referred to as the Visual Cortex (VC) model in the following text.The approximated probability mass functions produced by respectively the GMM, Brightness and Chromaticity, and Colour, Mean and Variance algorithms provide the mutual information required to produce silhouettes of objects of interest. For Brightness and Chromaticity, the probability that a pixel is foreground (FP) may be computed as (see Section 2.4.1 for notation)(17)FP=p(1-p(CD^i))p(α^i)p(α^i)For Colour, Mean and Variance, the probability that a pixel is foreground (FP) may be computed as follows:(18)FP=p(Ri∪Gi∪Bi∪Ai)The probability for the Gaussian Mixture Model may be computed as given in Eq. (1).Fig. 7 represents the classification by the VC model of foreground pixels (white) from the original frame in Fig. 1.

@&#CONCLUSIONS@&#
This paper has presented a novel neuroscience inspired information theoretic approach to motion segmentation. In applying current neurological and physiological research in primate vision, a system has been created to improve the robustness of a multidimensional motion segmentation system. The major result found in this investigation is in using the current understanding of the primate visual system as inspiration and guidance for choosing both feature sets (the LGN pathways), and the means of fusing them (the Visual Cortex model), considerably improves the appearance of the obtained silhouettes, without the need for subjective parameter adjustments, or the use of arbitrary thresholds. This presents an advantage over established multidimensional models which frequently rely on decisions, based on some weighting, whether a feature set provides the correct segmentation. These techniques are burdened with adjusting parameters, which do not necessarily provide the correct decision for all cases. This work has presented the performance evaluation of the biologically inspired motion segmentation system in challenging and diverse scenarios using a variety of evaluation metrics. In addition the evaluation results of state of the art automated visual surveillance systems have been presented to enable comparisons to be drawn. It shows that biologically inspired automated visual surveillance detection systems may be considered comparable to the current state of the art surveillance systems in detection and tracking. Existing real-time computational vision techniques have been exploited in the production of feature sets similar to that which the primate retina produces with a view towards real-time biologically inspired visual surveillance systems. The “reasoning” made within the visual cortex model employs a technique already well-established in the registration of medical images. It is envisaged that refining the LGN pathway approximations to closer representations of the biological system may result in robust performance beyond that of the current model. Further research into biologically guided object detection may provide a further processing model with a view to presenting robust object detection in addition to motion segmentation.