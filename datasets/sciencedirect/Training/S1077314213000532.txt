@&#MAIN-TITLE@&#
Color texture analysis using CFA chromatic co-occurrence matrices

@&#HIGHLIGHTS@&#
New descriptors are proposed for color textures, computed from their CFA images.These are multi-channel descriptors, like chromatic co-occurrence matrices (CCMs).They use specific CFA neighborhoods, allowing to compare textures without demosaicing.They outperform CCMs computed from the demosaiced images in classification accuracy.Avoiding the demosaicing step, they also save much processing time.

@&#KEYPHRASES@&#
Color texture analysis,Chromatic co-occurrence matrix,Bayer color filter array,CFA demosaicing,Texture classification,VisTex and Outex databases,

@&#ABSTRACT@&#
Most color cameras are fitted with a single sensor that provides color filter array (CFA) images, in which each pixel is characterized by one of the three color components (either red, green, or blue). To produce a color image, the two missing color components have to be estimated at each pixel of the corresponding CFA image. This process is commonly referred to as demosaicing, and its result as the demosaiced color image.Since demosaicing methods intend to produce “perceptually satisfying” demosaiced color images, they attempt to avoid color artifacts. Because this is often achieved by filtering, demosaicing schemes tend to alter the local texture information that is, however, useful to discriminate texture images. To avoid this issue while exploiting color information for texture classification, it may be relevant to compute texture descriptors directly from CFA images.From chromatic co-occurrence matrices (CCMs) that capture the spatial interaction between color components, we derive new descriptors (CFA CCMs) for CFA texture images. Color textures are then compared by means of the similarity between their CFA CCMs. Experimental results achieved on benchmark color texture databases show the efficiency of this approach for texture classification.

@&#INTRODUCTION@&#
Color texture classification is a particular problem of color image retrieval, closely related to appearance-based object recognition. It is a major field of development for several industrial vision applications [1]. To respect real-time constraints, these applications generally require as short a processing time as possible for the texture classification scheme.The supervised classification problem is to retrieve, among a set of color texture images grouped into classes (the target images), those that represent the same texture as a given image (the test image). Each texture being characterized by a set of descriptors, the matching scheme used for texture classification compares the descriptors of the test image with those of the target images by means of a similarity measure. The target images are then ranked with respect to their similarity to the test image, which provides the class of texture to which the test image belongs.Palm [2] has shown that the texture discrimination quality provided by the sole analysis of luminance texture descriptors is improved by taking into account both the spatial arrangement of the colors in the image plane and their distribution in a color space. Thus, color information is worth being used along with texture descriptors to serve as cues in a texture classifier.To classify color textures, three kinds of descriptors are encountered in the literature:•“luminance-based texture descriptors” mixed with color statistical features [3,4];“single-component texture descriptors” that take into account the spatial relationships within a single color component (i.e., the red1For interpretation of color in Figs. 1–3 and 5–12, the reader is referred to the web version of this article.1(R), green (G), or blue (B) color component) [5–7];“multi-component texture descriptors” that consider spatial relationships both between different color components and within a single one [8,2,9].Even if the first two approaches seem to be computationally faster, they do not take into account the spatial relationships between color components. As a consequence, the information on the color texture as a whole is ignored, subsequently reducing the classification quality [8]. That is why we opt for descriptors that capture spatial relationships within and between different color components to classify color textures.The underlying assumption of color texture classification is that digital color images are discrete functions well describing the observed textures. So, the user has to select the color camera with great care to ensure that the color texture is well represented by the acquired images. Two main criteria have to be retained when selecting the camera. First, since a color analysis improves the quality of texture discrimination, the color characterizing each pixel should be as close as possible to the color stimulus reflected by the observed surface element. Second, the color textures are represented by areas with high spatial frequencies. Texture images must then be acquired so that the spatial sampling frequency of the camera sensor is twice higher than the highest spatial frequency of the observed texture patterns, to avoid aliasing. Therefore, camera resolution is an important parameter that influences the choice of sensor technology for texture image acquisition.The digital color cameras available on the market may be distinguished according to the type of sensor(s)—either charge-coupled device (CCD) or complementary metal-oxide semiconductor (CMOS)—, and to whether they incorporate three sensors or only one single sensor. Three-sensor technology is in accordance with the trichromatic theory: each of the three sensors incorporated in such cameras is sensitive to one of the primary colors (red, green, or blue). In most of such devices, the color stimulus reflected by each surface element of the observed scene is split onto the three sensors by means of a trichroic prism assembly, made of two dichroic prisms [10]. Three color component images (IR,IG, andIB), in which each pixel is characterized by one single color component, are simultaneously acquired by the three sensors, and their combination yields the final full-color imageI, in which each pixel is characterized by the three color componentsR,G, and B (see Fig. 1a).Although three-sensor technology provides high-quality full-color images, the manufacturing costs of the sensors and of the optical device are high. As a consequence, cameras fitted with such sensors have not yet been affordable to everyone, nor widely distributed. Due to these cost constraints, most color cameras are equipped with a single sensor, which also allows for an increased spatial resolution at given cost in comparison with three-sensor cameras. The surface of this single sensor is covered by a color filter array (CFA), which consists of an array of spectrally selective filters (see Fig. 1b). Each photosite is then made mainly sensitive to a given wavelength band corresponding to a single color component. The Bayer filter array is the most widely used one; it provides a CFA imageICFAin which each pixel is characterized by only one among the three color components (see Fig. 2). The levels of the missing two color components must be determined to estimate the color of each pixel. This process is commonly referred to as CFA demosaicing; its result is the demosaiced color imageIˆ, in which each pixel is characterized by an estimated color (see Fig. 1b). Demosaicing is either achieved by an electronic device inside the camera itself, or by an external software that processes the CFA image delivered by the camera as a raw image file.The recently published demosaicing schemes can be grouped into two main categories: spatial methods, that analyze the image plane, and methods that examine the frequency domain [11]. Spatial methods exploit assumptions about spatial and/or spectral correlation between colors of neighboring pixels. Frequency-selection methods convolve the CFA image with dedicated masks to retrieve the luminance, then the demosaiced color image.Losson et al. [12] compare the performances reached by ten recent demosaicing schemes, applied to twelve images coming from the Kodak benchmark database [13]. Two kinds of quality measurements are used: classical fidelity criteria (e.g., the peak signal-to-noise ratio—PSNR), and measurements sensitive to demosaicing artifacts (e.g., to false colors, or to a jagged pattern called “zipper effect”). All these criteria yield consistent quality rankings for the tested demosaicing schemes. This detailed evaluation highlights that the methods mainly using the frequency domain outperform the methods only relying on spatial scans. More precisely, the methods proposed by Dubois [14] and by Lian et al. [15] provide the best demosaicing results, regardless of the quality measurement used. Unfortunately, these demosaicing schemes are rather time-consuming because they filter the CFA image with large convolution masks, and because they scan the CFA image several times.Since demosaicing methods intend to produce “perceptually satisfying” demosaiced color images, they are usually designed to avoid generating color artifacts [12]. Thus, many demosaicing schemes tend to alter the local texture information that is useful to discriminate the textures [16]. As shown in Fig. 3, even the best demosaicing schemes are prone to generate color artifacts in areas with high spatial frequencies. As a result, the demosaiced color image may not well represent the texture observed by the camera, and a texture classification scheme is expected to reach lower performances when applied to a set of demosaiced color images than when applied to the corresponding full-color images.It is therefore relevant to consider how to avoid the demosaicing step in any classification scheme of color texture images. One of the advantages of this approach is to decrease the computation time of the classification process. This paper precisely intends to investigate how such classification can be achieved directly from CFA images, and to evaluate the performance reached thereby, in terms of both classification accuracy and computation efficiency.In this paper, we present a texture classification scheme based only on the analysis of CFA images acquired by single-sensor color cameras, which has never been proposed—to our knowledge.This scheme relies on the assumption that the CFA images of the observed textures are relevant functions to describe these textures. Grounds for such a claim can be found in the works of Gunturk et al. [18] and Lian et al. [15], showing that the color components are strongly correlated with each other in areas of high spatial frequencies. As a consequence, sampling one single color component per pixel (as in the CFA image) slightly alters the description of the observed texture.To achieve color texture classification without demosaicing, we propose a new color texture descriptor that is directly computed from the CFA image. Derived from the well-known chromatic co-occurrence matrix (CCM), that captures the spatial relationships both within and between the color component images [8,2,9], this descriptor is hereafter referred to as the CFA chromatic co-occurrence matrix (CFA CCM).The rest of the paper is organized as follows. The second section describes how CCMs are computed from a color image, and introduces two measures of similarity between these descriptors. The third section sets a formalism for the CFA image, and explains how to compute CFA CCMs. To assess the efficiency of color texture classification using such CFA descriptors, the fourth section presents experimental results obtained with two benchmark color texture databases.Chromatic co-occurrence matrices (CCMs) are well-known and efficient texture descriptors [19,8,2,9]. In this section, we first give details about how CCMs are generally extracted from a color image. Then we describe how these descriptors can be compared thanks to dedicated similarity measures.A CCM is a statistical texture descriptor that both takes into account the color distribution of an image and the spatial interactions between the colors of pixels. This multi-channel descriptor can be viewed as a generalization of the gray-level co-occurrence matrix proposed by Haralick [20]. It was introduced by Rosenfeld [21] and taken up by Palm [2] as a texture descriptor extracted from a color image.A full-color imageIis composed of three color component imagesIk,k∈{R,G,B}. In each imageIk, a given pixel P is characterized by the levelIk(P)of the color component k. A three-component vector, defined asI(P)=(IR(P),IG(P),IB(P)), is therefore associated with each pixel P. When the spatial coordinates(x,y)of P are needed, the same vector may also be denoted asI(x,y)=IR(x,y),IG(x,y),IB(x,y).Let k andk′,(k,k′)∈{R,G,B}2, be two of the three color components, and q be the number of levels used to quantize the color components. LetMk,k′[I]be the CCM that captures the spatial interactions between the color components k andk′of the pixels in the full-color image I, according to a given neighborhoodN. The cellMk,k′[I](i,j)of this matrix,0⩽i⩽q-1,0⩽j⩽q-1, contains the number of times that a pixel Q whose level is j for the color componentk′, occurs in the neighborhoodN(P)of a pixel P whose level is i for the color component k:(1)Mk,k′I(i,j)=∑P∈I∑Q∈N(P)1ifIk(P)=iandIk′(Q)=j,0otherwise.The neighborhoodNused to compute the CCMs is made of the 8 pixels at uniform distance (also called infinity-norm distance) d along the four main directions of the image plane (horizontal and vertical directions, and the two diagonals) (see Fig. 4):(2)N(P)={Q∈I‖PQ‖∞=d,θmodπ/4=0},whereθdenotes the angle between the horizontal unit vector and the vector PQ defined by the pixels P and Q. The parameter d is adjusted by the user to fit the texture granularity at best. Note that, even if different neighborhoods can be considered [2], this one is consistent with that proposed by Haralick [20].As a measure of the local interaction between pixels computed from the whole image, this CCM is sensitive to variations of the image size and must be normalized by the total number of co-occurrences to decrease such sensitivity. Letmk,k′[I](i,j)be a cell of the normalized CCM:(3)mk,k′[I](i,j)=Mk,k′[I](i,j)∑a=0q-1∑b=0q-1Mk,k′[I](a,b),so that:(4)∑i=0q-1∑j=0q-1mk,k′[I](i,j)=1.The relationships between levels of neighboring pixels within any of the three color component imagesIkare represented by a single-component (normalized) CCMmk,k[I],k∈{R,G,B}. In order to provide information about the spatial relationships between the color components, multi-component matrices (namelymk,k′[I],k≠k′) must be considered [2]. These matrices contain the number of pairwise occurrences of levels between different color components. Becausemk,k′[I]andmk′,k[I]are transpose of each other, these matrices contain the same information. Therefore, only one of them is useful to describe the co-occurrence information between the color components k andk′.For a given imageIand a given neighborhoodN, six normalized CCMs are thus computed:•three single-component matricesmR,R[I],mG,G[I], andmB,B[I],three multi-component matricesmR,G[I],mR,B[I]andmB,G[I],These six chromatic co-occurrence matrices require6·q2memory cells for storage. They capture both the distribution and spatial arrangement of the color component levels [2].The matching scheme between the test imageItestand the target imageItaris based on a pairwise comparison of their respective chromatic co-occurrence matricesmk,k′[Itest]andmk,k′[Itar],(k,k′)∈S. Several measures have been proposed to compare probability density functions, and normalized co-occurrence matrices can be regarded as such. Therefore, any of the measures considered by Rubner et al. to evaluate the dissimilarity between histograms [22] can be used to compare CCMs. Since the objective of this paper is to show the interest of texture descriptors extracted from CFA images, we retain two similarity measures among the existing ones: the intersection between CCMs [23] (one of the simplest similarity measures), and the Jeffrey divergence [22] (one of the most efficient measures to compare textures).The intersection between two CCMs, for any pair of color components(k,k′)∈S, is expressed as:(5)Intermk,k′[Itest],mk,k′[Itar]=∑i=0q-1∑j=0q-1minmk,k′[Itest](i,j),mk,k′[Itar](i,j).Note that this expression is an extension of the intersection between two image histograms, proposed by Swain et al. [24]. The similarity measure between the test and target images is the mean of their pairwise CCM intersections [25]:(6)SIMItest,Itar=16∑(k,k′)∈SIntermk,k′[Itest],mk,k′[Itar].When the two imagesItestandItarshare a similar spatial arrangement of colors, their similarity measure value SIM is close to 1 (see Eqs. (4) and (5)). Although it does not necessarily mean that the two images contain the same texture, we assume so. On the other hand, a similarity measure value close to 0 means that the two textures are significantly different.The Jeffrey divergence (JD) is derived from the Kullback–Leibler divergence (KLD). Both can be regarded as agreement measures between two probability density functions. In contrast toKLD,JDis symmetric and presents a better numerical behavior [22]: it is stable when comparing two empirical distributions. The Jeffrey divergence is expressed as:(7)JDmk,k′[Itest],mk,k′[Itar]=∑i=0q-1∑j=0q-1mk,k′[Itest](i,j)·logmk,k′[Itest](i,j)m¯k,k′(i,j)+mk,k′[Itar](i,j)·logmk,k′[Itar](i,j)m¯k,k′(i,j),wherem¯k,k′(i,j)=12·mk,k′[Itest](i,j)+mk,k′[Itar](i,j).When the two images contain the same texture, the Jeffrey divergence between their CCMs is close to 0, whereas it tends to infinity when they are quite different. The dissimilarity between the two texture images is the mean of the six pairwise divergence measures between their respective CCMs:(8)DISSIMItest,Itar=16∑(k,k′)∈SJDmk,k′[Itest],mk,k′[Itar].Classification schemes based on the similarity between CCMs provide good results [26], but at high memory and computation costs (see Section 4.4). It is therefore relevant to classify color textures using a similarity measure between CCMs directly computed from their CFA images.First, we describe the CFA image acquired by any single-sensor color camera, and how the demosaicing process estimates a color image from it. Then, we introduce how to compute CFA chromatic co-occurrence matrices (CFA CCMs) directly from the CFA image, in order to avoid the demosaicing step for color texture classification.To set a formalism for the CFA image, let us compare the acquisition process of a color image in three-sensor and single-sensor cameras. Fig. 1a outlines a three-sensor camera architecture, in which the full-color image of a scene is formed by combining data from the three sensors. A single-sensor color camera generates a color image in a quite different way, as shown in Fig. 1b. In the first step, the single sensor forms a raw image, called CFA image and denoted byICFA, in which a single color componentR,Gor B is associated to each pixel. The second step is the demosaicing process, that consists in estimating a demosaiced color imageIˆfromICFA.Considering the Bayer CFA of Fig. 2, to each pixelP(x,y)inICFAis associated a single available color component:•the R color component if x is odd and y is even,the G color component if x and y are of same parity,the B color component if x is even and y is odd.LetICFAk,k∈{R,G,B}, be the subset of pixels inICFAwhose available color component is k (see Fig. 5):(9a)ICFAR={P(x,y)∈ICFAxisoddandyiseven}(Figure6a),(9b)ICFAG={P(x,y)∈ICFAxandyareofsameparity}(Figure6b),(9c)ICFAB={P(x,y)∈ICFAxisevenandyisodd}(Figure6a),so thatICFA≡ICFAR∪ICFAG∪ICFAB. To determine the color of each pixel P of the demosaiced color image, the demosaicing process (denoted byD) generally retains the color component available at the same location inICFA, and estimates the other two components:(10)ICFA(P)→DIˆ(P)=ICFA(P),IˆG(P),IˆB(P)ifP∈ICFAR,IˆR(P),ICFA(P),IˆB(P)ifP∈ICFAG,IˆR(P),IˆG(P),ICFA(P)ifP∈ICFAB.Each color component triplet in Eq. (10) represents an estimated color. Out of the three components inIˆ(P), the available one at pixel P inICFAis denoted byICFA(P), and the missing other two components amongIˆR(P),IˆG(P), andIˆB(P), are estimated by demosaicing.In the CFA image of Fig. 2, four different neighborhood configurationsNCFA(P)are encountered, according to the location of the considered pixel P. Fig. 6shows the four5×5spatial neighborhoods centered at a given pixel P, according to the subsetICFAkto which it belongs (i.e., to the parity of its coordinates x and y). Notice that the neighborhoodsNCFA(P)are structurally similar forP∈ICFARandP∈ICFAB(see Fig. 6a and d), apart from the slight difference that available components R and B are exchanged; therefore, they can be analyzed in the same way. Also notice that two neighborhood configurations are encountered at pixelsP(x,y)∈ICFAG, according to the parity of x and y (see Fig. 6b and c).Section 2 explained the standard way to compute CCMs from color images, and detailed two measures used to compare them in the texture matching scheme. When CFA images are used as input, a similar procedure may be applied, providing that CCMs are replaced by a specific descriptor that we now introduce.To characterize a texture represented by a given CFA imageICFA, we propose to compute six CFA chromatic co-occurrence matrices (CFA CCMs)MCFAk,k′[ICFA],(k,k′)∈S. To achieve this, we examine the neighborhoodNCFAk,k′(P)of each pixel P whose available color component is k inICFA(i.e.,P∈ICFAk). This neighborhood is made of neighboring pixels Q whose available color component isk′. The matrix cellMCFAk,k′[ICFA](i,j)contains the number of times that, inICFA, a pixel Q whose available color component isk′and whose level is j, occurs in the neighborhood of a pixel P whose available color component is k and whose level is i. This CFA CCM cell is expressed as:(11)MCFAk,k′ICFA(i,j)=∑P∈ICFAk∑Q∈NCFAk,k′(P)1ifICFA(P)=iandICFA(Q)=j,0otherwise.In a CFA image, to count the co-occurrences between two color components k andk′, we therefore consider all pixels P whose available color component is k (P∈ICFAk) and, in their neighborhoodNCFAk,k′(P), the pixels Q whose available color component isk′(Q∈ICFAk′). Unlike the neighborhoodN(P)used to compute CCMs (see Eq. (2) and Fig. 4),NCFAk,k′(P)is specific to the color component pair(k,k′)because of the CFA color sampling. However, we try to make these two neighborhoods consistent with each other at the utmost as follows.•For a given color componentk∈{R,G,B},NCFAk,k(P)can be defined exactly asN(P), provided that the distance d from P to Q (used to defineN) is even:(12)NCFAk,k(P)={Q∈ICFAk‖PQ‖∞=2·n,θmodπ/4=0},n∈N∗being a distance parameter. Indeed, in the four different neighborhood configurationsNCFA(P),P∈ICFAk, the color component k is available at the 8 pixels located along the four main directions at uniform distancedk,k=d=2·nfrom P. This is shown forn=1on Figs. 6 and 7a fork=R, on Figs. 6b and Fig. 7b fork=G, and on Fig. 6d and c fork=B. This can also be easily verified by examining larger neighborhoods of size(4·n+1)×(4·n+1),n>1.For two different color components(k,k′)∈{R,G,B}2,k≠k′,NCFAk,k′(P)can no more be made of the 8 pixels inN(P), sincek′is unavailable at these pixels in the CFA image. Instead, we use the 4 neighboring pixels Q at whichk′is available and located along the four main directions at uniform distancedk,k′=2·n-1fromP∈ICFAk:(13)NCFAk,k′(P)={Q∈ICFAk′‖PQ‖∞=2·n-1,θmodπ/4=0}.Note that the pixels inNCFAR,G(P)andNCFAB,G(P)are then only located along the horizontal and vertical directions (see Fig. 7d and f forn=1), whereas pixels inNCFAR,B(P)are only located along the two main diagonal directions (see Fig. 7e). This remains true whenn>1.In short, for any(k,k′)∈S,NCFAk,k′(P)is defined as the pixels located along the four main directions at uniform distancedk,k′fromP∈ICFAk:(14)NCFAk,k′(P)={Q∈ICFAk′|‖PQ‖∞=dk,k′,θmodπ/4=0},with:(15)dk,k′=2·nifk=k′(whichyields8pixelsinNCFAk,k′(P)),2·n-1ifk≠k′(whichyields4pixelsinNCFAk,k′(P)).Fig. 7 shows the set of neighborhoodsNCFAk,k′(P),(k,k′)∈S, used to compute the CFA CCMsMCFAk,k′ICFA, for anyP∈ICFAkas the central pixel andn=1. Uncolored pixels on this figure are not considered as neighbors. Like for CCMs, these neighborhoods can be defined with a large distance value to describe coarse textures. To show the considered co-occurrences between a pixel in the CFA image and its neighbors located at different distances, Fig. 8illustrates the three neighborhoodsNCFAR,k′(P), for allk′∈{R,G,B}, anyP∈ICFARas the central pixel, andn=2. Note that whatever n, the number of pixels inNCFAR,k′remains either 8 (fork′=R) or 4 (fork′≠R) (see Figs. 7 and 8).Like for full-color images, the similarity between the test and target CFA images is then either measured by the mean of the pairwise intersections between their respective CFA CCMs (see Eq. (6)), or by the mean of their pairwise Jeffrey divergences (see Eq. (8)). But, unlike for a full-color image, the number of neighbors used to count the co-occurrences in a CFA image depends on the considered CFA CCM. Indeed, in a CFA image of size N pixels (assumed divisible by 4), there areCard{ICFAR}=N4red samples,Card{ICFAG}=N2green samples, andCard{ICFAB}=N4blue samples, so that the total numbers of co-occurrences are8·N4forMCFAR,R[ICFA],8·N2forMCFAG,G[ICFA],8·N4forMCFAB,B[ICFA],4·N4forMCFAR,G[ICFA],4·N4forMCFAR,B[ICFA], and4·N4forMCFAB,G[ICFA](including neighbors outside the image bounds). In order to ensure that the co-occurrences of the six color component pairs are equally weighted when measuring the similarity between two images, each cell of the CFA CCMMCFAk,k′[ICFA]must be normalized by its own total number of co-occurrences as:(16)mCFAk,k′[ICFA](i,j)=MCFAk,k′[ICFA](i,j)∑a=0q-1∑b=0q-1MCFAk,k′[ICFA](a,b).Without such normalization, the intersection between the matricesMCFAG,Gof two CFA images would weight, in the similarity measure, four times the intersection between their matricesMCFAk,k′whenk≠k′, and two times the intersection between their matricesMCFAk,kwhenk∈{R,B}. This normalization also decreases the CFA CCM sensitivity to the image size.Notice at last that, like for CCMs,mCFAk,k′andmCFAk′,kcontain the same information for a given CFA image (see demonstration in appendix). The texture represented by the CFA image is then characterized by a set of six matrices:•three single-component matricesmCFAR,R[ICFA],mCFAG,G[ICFA], andmCFAB,B[ICFA];three multi-component matricesmCFAR,G[ICFA],mCFAR,B[ICFA]andmCFAB,G[ICFA].

@&#CONCLUSIONS@&#
