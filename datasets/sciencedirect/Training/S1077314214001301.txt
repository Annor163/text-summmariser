@&#MAIN-TITLE@&#
Joint multi-person detection and tracking from overlapping cameras

@&#HIGHLIGHTS@&#
Joint person detection and tracking in overlapping camera views.Novel two-step likelihood optimization using preprocessing and evaluation step.Occlusions taken into account when measuring features, using 3D scene reconstruction.Presenting new challenging indoor and outdoor multi-person surveillance datasets.Outperformance of state-of-the-art detection and tracking methods.

@&#KEYPHRASES@&#
Person tracking,Person detection,Overlapping cameras,

@&#ABSTRACT@&#
We present a system to track the positions of multiple persons in a scene from overlapping cameras. The distinguishing aspect of our method is a novel, two-step approach that jointly estimates person position and track assignment. The proposed approach keeps solving the assignment problem tractable, while taking into account how different assignments influence feature measurement. In a hypothesis generation stage, the similarity between a person at a particular position and an active track is based on a subset of cues (appearance, motion) that are guaranteed observable in the camera views. This allows for efficient computation of the K-best joint estimates for person position and track assignment under an approximation of the likelihood function. In a subsequent hypothesis verification stage, the known person positions associated with these K-best solutions are used to define a larger set of actually visible cues, which enables a re-ranking of the found assignments using the full likelihood function.We demonstrate that our system outperforms the state-of-the-art on four challenging multi-person datasets (indoor and outdoor), involving 3–5 overlapping cameras and up to 23 persons simultaneously. Two of these datasets are novel: we make the associated images and annotations public to facilitate benchmarking.

@&#INTRODUCTION@&#
The ability to visually track persons across a scene is important for many application domains, such as surveillance or sports analysis. In this paper, we are interested in the more challenging scenarios involving multiple persons in complex environments (i.e. dynamic and cluttered backgrounds, varying lighting conditions). Multiple view analysis allows to compensate for the effects of occlusions and noisy observations. Cost and logistics considerations will, however, often limit the number of overlapping cameras that can be employed. We aim for methods that work with as few as 3–4 cameras from very different, diagonal downward viewing directions with overlap (as opposed to ceiling-mounted cameras with a bird-eyes view). These conditions could arise in a sports stadium (e.g. football, basketball), the main lobby of a building (e.g. bank, government) or at a critical infrastructure (e.g. train station or airport hall). Our cameras are synchronized and calibrated offline, but several previous work on self-calibration [1,2] exists that could relax this assumption.The considered wide-baseline camera set-up makes it difficult to establish individual feature correspondences across views, especially in the presence of sizable inter-person occlusion. We aim for robustness by performing the analysis based on a 3D scene reconstruction, namely, using the visual hulls of objects obtained by volume carving. The main challenge is thus to establish correct correspondence across views at the object level. Matching different objects together across multiple views leads to erroneous 3D objects, so-called ghosts, see Fig. 1. We will address this challenge with a novel, two-step likelihood optimization approach; this is part of a recursive tracker that is meant for online analysis.The remainder of this paper is organized as follows. Section 2 covers the most closely related work. Section 3 provides an overview of the proposed approach, while Section 4 contains the technical details. In Section 5, we present the experimental results. We discuss the chosen camera set-up and computational issues in Section 6. The conclusions and suggestions for future work are listed in Section 7.

@&#CONCLUSIONS@&#
