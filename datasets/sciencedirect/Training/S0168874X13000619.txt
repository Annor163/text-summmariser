@&#MAIN-TITLE@&#
Eigensensitivity analysis of damped systems with distinct and repeated eigenvalues

@&#HIGHLIGHTS@&#
A combined normalization which combines two traditional normalizations is presented.Design sensitivity analysis of viscously damped eigenproblems is studied.A compact and well-conditioned algebraic method in case of distinct eigenvalues is proposed.An N-space algorithm for computing eigensensitivity with repeated eigenvalues is proposed.

@&#KEYPHRASES@&#
Design sensitivity analysis,Eigensensitivity,Damped systems,Repeated eigenvalues,Floating raft,

@&#ABSTRACT@&#
This paper considers the computation of eigensolution sensitivity of viscously damped eigensystems with distinct and repeated eigenvalues. To simplify the computation, a combined normalization, which combines two traditional normalizations, is presented. Based on the combined normalization, a method for sensitivity analysis of eigenvalues and eigenvectors is studied. In the case of distinct eigenvalues, the proposed method can determine the eigenvector derivatives directly and is robust since the components of coefficient matrices are all of the same order of magnitude. The computational cost of the second-order sensitivities of eigenvectors can be reduced remarkably since the matrix decomposition of the coefficient matrix is available from the computation process of the first-order eigensensitivities. In the case of repeated eigenvalues, an algorithm is presented for computing the eigensolution sensitivities. The algorithm maintains N-space without using state-space equations such that the computational cost is reduced. The method is accurate, compact, numerically stable and easy to be implemented. Finally, three numerical examples have demonstrated the validity of the proposed method. The capacity of predicting the changes of eigensolutions with respect to the changes of design parameters in terms of the first- and second-order eigensensitivities is studied with application to the analysis of a two-stage floating raft isolation system.

@&#INTRODUCTION@&#
Design sensitivity analysis of engineering structures deals with the computations of the rate of performance measure from changes in the design parameters describing the structure. A significant body of research has been devoted to the computation and application of design sensitivity analysis (see, e.g., van Keulen et al. [1], Adelman and Haftka [2], Haftka et al. [3], Haug et al. [4] or Choi and Kim [5] or Chen [6]). Computational methods for design sensitivity analysis have received much attention over the past four decades, particularly those related to the eigenvalue problems. The eigensolution sensitivities of structural and mechanical systems with respect to structural design parameters have become an integral part of many engineering design methodologies including structural design optimization, structural health monitoring, structural reliability, dynamic model updating, structural dynamic modification, approximate reanalysis techniques and many other applications. Although computing eigenvalue sensitivity is straightforward, finding eigenvector derivatives raises several challenges, due in part to the singularity problem and repeated eigenvalues.The modal method [7,8] evaluated the derivative of each eigenvector as a linear combination of the whole eigenvectors. Adhikari [9,10] and Adhikari and Friswell [11] derived N-space modal methods of viscously damped systems without using 2N-space (state-space) formulation where N is the system dimension. Although these 2N-space methods are exact in nature, the 2N-space approach usually needs heavy computational cost for real-life multiple degree-of-freedom (DOF) systems due to the size of system matrices of state-space equations is double. Adhikari [12] first extended the modal method to the more general symmetric and asymmetric systems with nonviscous (viscoelastic) damping. To guarantee the accuracy of the derivative of each eigenvector, the modal method needs a linear combination of all the eigenvectors, which is a significant computational task. Often only the lower order frequencies and associated mode shapes are calculated. That is, approximated sensitivity may be evaluated depending on the number of modal terms. Corrections to the model truncation problem have been investigated by several authors [6,13–17].Fox and Kapoor [7] also suggested a direct algebraic method, which calculates the eigensolution derivatives by assembling the derivatives of eigenproblems and the additional constraints obtained from the derivative of normalization condition into a linear system of algebraic equations. Garg [18], Rudisill [19], Rudisill and Chu [20] investigated the algebraic method to address the problem of asymmetric eigensystems. Lee and Jung [21,22] derived an algebraic method with symmetric coefficient matrices to solve the eigensolution derivatives. But these methods [18–22] are only restricted to the first-order representation of the equation of motion. Later, Lee et al. [23,24] further extended their algebraic method to symmetric systems with viscous damping. Choi et al. [25] developed their method for the second- and higher-order derivatives of eigensolutions for symmetric systems with viscous damping. Unfortunately, Wu et al. [26] pointed out that these methods [22,24,25] were not correct since a mistake was made in the computation of derivatives of the normalization condition for the eigensystems with repeated eigenvalues. Choi et al. [27], Guedria et al. [28], Chouchane et al. [29] and Xu et al. [30] further extended the algebraic method to compute the eigensolution sensitivities of asymmetric viscously damped systems. Recently, Li et al. [31] extended the algebraic method to the more general nonviscous damped systems. Li et al. [32] developed an algebraic method for asymmetric nonviscously damped systems, which can evaluate the sensitivities of eigenvalues and eigenvectors without using the left eigenvalues.Another efficient method proposed by Nelson [33] finds the eigenvector derivatives with distinct eigenvalues for undamped eigensystems by expressing the derivative of each eigenvector as a linear combination of a particular solution and a homogeneous solution of the singularity problem. The particular solution can be found by identifying the element of the corresponding eigenvector with the largest absolute value and constraining its derivative to zero. It is worth mentioning that each eigenvector can always satisfy the singularity problem and is therefore considered as a homogeneous solution of its derivative. The arbitrary constraining the derivative of the largest absolute value can be compensated by the computation of the coefficient of the homogeneous solution. Sutter et al. [34] pointed out Nelson's method is more efficient than the modal method for the reason that the modal method needs all or most of the eigenvectors to find the eigenvector derivatives. Friswell [35] developed Nelson's method to compute the second- and higher-order eigensolution derivatives for undamped systems. Later, Friswell and Adhikari [36] extended Nelson's method to viscously damped systems. Guedria et al. [37] extended Nelson's method to second-order derivatives of eigensolutions of viscously damped systems. Recently, Adhikari and Friswell [38] extended Nelson's method to nonviscously damped systems. More recently, Omenzetter [39] extended Nelson's method to general nonlinear eigensystems. Although Nelson's method mentioned above gives exact results and only needs the eigenpairs of interest, these methods are clumsy for programming.Jankovic [40] gave the exact analytical solutions for the first- and higher-order derivatives of eigensolutions of linear and nonlinear eigenproblems. Murthy and Haftka [41] surveyed the methods for sensitivity analysis of the eigenproblem with general non-Hermitian matrices. Mottershead et al. [42] pointed out that the eigensensitivity-based method is probably the most successful approach to model updating. Other methods have been developed for the calculation of the sensitivity of mode shape including the iterative method [43,44], QR-based method [45,46], Davidson-based method [47], the combination method [48], the finite difference method [49,50] and the substructuring method [51–53].The methods [7–53] to compute eigensensitivity are only restricted to the case of distinct eigenvalues. Choi and Kim [5] pointed out that repeated eigenvalues are far more likely to happen in optimized structures. Thompson and Hunt [54] paid attention to optimal designs that are constructed with repeated eigenvalues. Olhoff and Rasmussen [55] showed that a repeated buckling load may occur in an optimized clamped-clamped column. Ojalvo [56], Mills-Curran [57], Dailey [58] developed Nelson's method for solving the derivatives of eigensolutions of undamped eigensystems with repeated eigenvalues. Methods [56–58] are based on deleting rows and columns of the singular system to form a reduced coefficient matrix. Unfortunately, the methods [56,58] may fail in some circumstances (see Mills-Curran [59] for details). Shaw and Jayasuriya [60] generalized the methods [56–58] to compute the eigensolution derivatives in the case of repeated eigenvalues with repeated first-order derivatives. Tang et al. [61,62] investigated the eigensolution derivatives of general asymmetric systems with repeated eigenvalues. Later, Xu and Wu [63] presented a new normalization and developed a method for the computation of eigensolution derivatives of asymmetric viscously damped systems with distinct and repeated eigenvalues. More recently, Li et al. [64] suggested a new normalization for the left eigenvectors, from which the left and right eigenvector derivatives can be computed separately and independently for asymmetric viscously damped systems with distinct and repeated eigenvalues. Li et al. [65] showed the undamped, viscously or nonviscously damped eigenproblems can be considered as a degenerated case of general nonlinear eigenproblems and developed a unified eigensensitivity method for both distinct and repeated eigenvalues. However, most of the existing methods to obtain the derivatives of the complex eigensolutions with repeated eigenvalues employ first-order (2N-space) formulation.In this study, the symmetric eigenproblems with viscous damping is considered. To simplify the computation of eigensolution derivatives, a new combined normalization, which combines two traditional normalizations, is presented. The aim of this paper is to propose a compact method to calculate eigenvector derivatives directly by constructing the coefficient of the homogeneous solution to be zero in the case of distinct eigenvalues. In the case of repeated eigenvalues, an N-space algorithm is presented for computing the eigensolution sensitivity such that it is easy to be implemented and need lower computational cost.Consider the formulation for structural modal analysis described by a linear viscously damped symmetric system with N DOF(1)(λi2(p)M(p)+λi(p)C(p)+K(p))φi(p)=0fori=1,2,…,NwhereM(p),C(p)andK(p)∈ℝN×Nare the mass, damping and stiffness matrices, respectively, whose components are assumed to be continuously differentiable with respect to the design parameter p. λiis the ith eigenvalue andφiis the ith eigenvector. For underdamped systems, the eigenvalues associated with the above eigenproblem appear in complex conjugate pairs. Often the following normalization is adapted(2)φiT(p)(2λi(p)M(p)+C(p))φi(p)=1The arbitrariness of eigenvectors can be removed by this normalization condition which has been widely applied in the design sensitivity analysis of symmetric damped eigenproblems.The paper is concerned with the derivatives of eigenvalues and associated eigenvectors at p=p0, and hereafter “p0” is omitted for variables evaluated at p=p0. The eigensolutions of viscously damped symmetric systems and the derivatives of system matrices (the mass, damping and stiffness matrices) are assumed to be known. For convenience, the following notation is adopted(•),p≡∂(•)∂p,(•),pp≡∂2(•)∂p2Differentiating Eq. (1) with respect to the design parameter p and evaluating them at p=p0 yields(3)Fiφi,p=hiwhereFi≡λi2M+λiC+Khi≡−(2λiM+C)φiλi,p−(λi2M,p+λiC,p+K,p)φiPremultiplying each side of Eq. (3) byφiT, the eigenvalue derivatives can be obtained [11](4)λi,p=−φiT(λi2M,p+λiC,p+K,p)φiφiT(2λiM+C)φiBy utilizing normalization (2), this formula can be simplified as [9,10](5)λi,p=−φiT(λi2M,p+λiC,p+K,p)φiThe derivatives of the eigenvectorsφi,pcannot be determined directly by utilizing Eq. (3), because the matrix Fiis singular due to the fact that the rank of the matrix Fiof order N is (N−1) for systems with distinct eigenvalues (often it is called the singularity problem).Eigensensitivity analysis for viscously damped systems has received much attention over the past two decades. However, previous works have focused primarily on eigenvectors normalized by Eq. (2) alone. Alternatively, one can choose other kinds of normalizations, e.g.(6){φi(p)}ni=1Here{•i}edenotes the eth component of vector•iandniis selected by finding{φi(p)}niwhich is the largest element in absolute value on the column ofφi(p). Following the procedures of Nelson's method [36] by using normalization (6), it is easy to verify that the eigenvector derivatives can be determined directly due to the fact that the coefficient of the homogeneous solution is equal to zero. Caution must be exercised here, the eigenvalue sensitivities must be evaluated from Eq. (4), and the compact formula (5) does not hold.This section proposes a new normalization which attempts to determine the eigenvector sensitivities directly and holds the compact formula (5) of the eigenvalue sensitivities. For p=p0, we assume the new normalization is same with the traditional normalization Eq. (2). To express it clearly, we give the form(7)φiT(2λiM+C)φi=1The normalization is usually presumed to be a differentiable function, at least in the neighborhood of the current design point, i.e., the point at p near p0. Here we suggest that the new normalization satisfies the following form for p near p0(8){φi(p)}ni={φi}niIt means that the largest element in absolute value on each eigenvector is a constant for p near p0. When p=p0, normalization Eq. (8) is always satisfied, and normalization Eq. (7) can be imposed to guarantee the unique solutions of eigenvectors at p=p0. So the combined normalization is consistent. Compared to the traditional normalization Eq. (2), the combined normalization not only preserves the compact formula of eigenvalue sensitivities expressed by Eq. (5), but also determines the eigenvector derivatives directly (the details can be seen in the following subsections).For structural optimization, optimal solution usually need be obtained by calculating the derivatives many times. If the combined normalization is used, the eigenvectors should be renormalized at the beginning of each iteration, which seems to be clumsy for programming. However, it is clear that the complexity of implementation and the computational effort of renormalization are less than those of the computation of the homogeneous solution of Nelson's method [36,37]. Consequently, design sensitivity analysis based on the combined normalization, will be also efficient and easy to be implemented in optimal design.This section assumes that all eigenvalues and eigenvectors are distinct, that is, they are not repeated. A separate section will be devoted to repeated eigenvalues.For the foregoing discussions, the derivatives of eigenvectorsφi,pcannot be found directly by utilizing Eq. (3). Therefore, an additional constraint should be imposed to uniquely determine the derivatives of eigenvectors. It is customary to obtain the constraint by differentiating normalization Eq. (2). Here, the constraint can be derived by differentiating normalization Eq. (8)(9){φi,p}ni=0To apply the above constraint, constructF˜iby zeroing out row niand column niof Fi, but keep the nith diagonal element of Fi, and constructh˜iby zeroing out the nith elements of hi. The linear system of algebraic equations for solving the eigenvector derivatives can be given by(10)F˜iφi,p=h˜iNote that the coefficient matrix of the above system is a symmetric matrix of dimension (N×N), and has exactly the same band structure as the original system. The formula of Eq. (10) is similar with the computation of the particular solution of Nelson's method [33,36], but the proposed method need not calculate the homogeneous solution. That is, the proposed method can find the eigenvector sensitivities directly. Hence the presented method is compact and easy to be implemented. In addition, Nelson's method [33,36] suggests the nith diagonal element ofF˜iis set to unity, under such operations, sometimes the components in the coefficient matrixF˜iwill be not all of the same order of magnitude, which often causes a big condition number. Under the circumstances, there is small perturbation of the input which leads to a big change in the output (similar study can be seen in Li et al. [66]). To reduce the condition number, we keep the nith diagonal element ofF˜i. In this sense, the presented method can be more robust than Nelson's method. The similar formula is also presented in Li et al. [64] for asymmetric systems with viscous damping. One of the remarkable characteristics of the proposed method is that its numerical stability is also proved, as demonstrated in Section 4.Differentiating Eq. (3) with respect to the design parameter p, the second-order sensitivities of eigensolutions satisfy(11)Fiφi,pp=hi,p−Fi,pφi,pwhereFi,p≡λi2M,p+λiC,p+K,p+(2λiM+C)λi,phi,p≡−(2λiM+C)φiλi,pp−(λi2M,p+λiC,p+K,p)φi,p−(λi2M,pp+λiC,pp+K,pp)φi−2(2λiM,p+C,p)φiλi,p−(2λiM+C)φi,pλi,p−2Mφiλi,p2Eq. (11) can be reformed as(12)Fiφi,pp=−(2λiM+C)φiλi,pp−biwhere(13)bi=(λi2M,pp+λiC,pp+K,pp)φi+2(λi2M,p+λiC,p+K,p)φi,p+2(2λiM,p+C,p)φiλi,p+2(2λiM+C)φi,pλi,p−2Mφiλi,p2Premultiplying Eq. (11) byφiT, in a similar way, the second-order derivatives of eigenvalues can be obtained(14)λi,pp=−φiT(λi2M,pp+λiC,pp+K,pp)φi−2φiT(λi2M,p+λiC,p+K,p)φi,p−2φiT(2λiM,p+C,p)φiλi,p−2φiT(2λiM+C)φi,pλi,p−2φiTMφiλi,p2or(15)λi,pp=−φiTbiThe right-hand side of Eqs. (14) or (15) is a function of the ith eigenvalue and corresponding eigenvector and their first-order sensitivity as well as the first and second sensitivities of system matrices.Again, the constraint of the second-order derivatives of eigenvectors can be derived by differentiating normalization Eq. (8) twice(16){φi,pp}ni=0To apply the above constraint, in a similar way, the second-order derivatives of eigenvectors can be obtained using Eq. (12) and take the form(17)F˜iφi,pp=h˜i(2)wherehi(2)=−(2λiM+C)φiλi,pp−biHere vectorh˜i(2)can be constructed by zeroing out thenith elements ofhi(2).As can be seen, the second-order sensitivities of eigenvectors only require the calculation of vectorh˜i(2)because the symmetric coefficient matrixF˜iand its matrix decomposition (e.g. LDLTor LU decomposition) are available from the computation process of the first-order sensitivities of eigenvectors. For this reason, the computational cost of the second-order sensitivities of eigenvectors will be reduced remarkably once the first-order sensitivities are obtained (the first-order derivatives obtained by solving Eq. (10) need the number of operations O(N3), however, only O(N2) is required for finding the second-order derivatives). It is worth mentioning that bihas been calculated in the computation process of the second-order sensitivities of eigenvalues. Therefore, the second sensitivities of eigenvectors are also compact, simple and easy to be implemented.The complete algorithm for computing the first- and second-order sensitivities of eigensolutions with distinct eigenvalues is summarized below:(1)Solve eigenproblem(λi2M+λiC+K)φi=0and obtain the eigenvalues and eigenvectors.Normalize the eigenvectorsφiby the normalizationφiT(2λiM+C)φi=1. Note this normalization is only for the design point at p=p0. In the neighborhood of the current design point, i.e., the point at p near p0, the eigenvectors should be normalized by{φi(p)}ni={φi}ni.Calculate the eigenvalue sensitivities by the compact formula,λi,p=−φiT(λi2M,p+λiC,p+K,p)φi.ComputeFi≐λi2M+λiC+K.Computehi≐−(2λiM+C)φiλi,p−(λi2M,p+λiC,p+K,p)φi.Find niby selecting{φi}niwhich is the largest element in absolute value on the column ofφi.ConstructF˜iby zeroing out row niand column niof Fi, but keep the nith diagonal element of Fi.Constructh˜iby zeroing out the nith elements of hi.SolveF˜iφi,p=h˜iand obtain the first-order sensitivities of eigenvectors,φi,p.Computebi=(λi2M,pp+λiC,pp+K,pp)φi+2(λi2M,p+λiC,p+K,p)φi,p+2(2λiM,p+C,p)φiλi,p+2(2λiM+C)φi,pλi,p−2Mφiλi,p2Calculate the second-order sensitivities of eigenvalues using the formulaλi,pp=−φiTbi.Computehi(2)=−(2λiM+C)φiλi,pp−bi.Constructh˜i(2)by zeroing out the nith elements ofhi(2).SolveF˜iφi,pp=h˜i(2)and obtain the second-order sensitivities of eigenvectors,φi,pp.Note that the algorithm of this method contains only a few steps, so it is very compact, simple and easy to be implemented.Assume eigensystem Eq. (1) has(1<m≤N)m repeated eigenvalues. Without loss of generality, we denote(18)Λm=diag(λ˜1(p),λ˜2(p),⋯,λ˜m(p))(19)X(p)=[φm1(p)φm2(p)⋯φmm(p)]whereλ˜k(p)for k=1,2,…,m are equal.λ˜(p)=λ˜k(p)is the eigenvalue of multiplicity m(1<m≤N)for the eigenspace spanned by the columns ofX(p). The associating eigenproblem can be given by(20)M(p)X(p)Λm2(p)+C(p)X(p)Λm(p)+K(p)X(p)=0(21)XT(2λ˜M+C)X=ImSimilarly, normalization Eq. (21) is only for p=p0. HereImis the identity matrix of order m.Actually, an infinite number of linear combinations of the eigenvectorsφmk(p)will also satisfy Eqs. (20) and (21). A unique eigenvectorsX˜(p)called adjacent eigenvectors for which derivatives can be found need to be determined from the given eigenvectorsX(p). The adjacent eigenvectorsX˜(p)can be expressed in terms ofX(p)as(22)X˜(p)=X(p)αwhere α is an orthogonal transformation matrix of dimension (m×m). Thus, the adjacent eigenvectors also satisfy the orthogonal normalization for p=p0:(23)X˜T(2λ˜M+C)X˜=αTXT(2λ˜M+C)Xα=ImCombiningM(p)X(p)Λm(p)−M(p)X(p)Λm(p)=0and Eq. (20) can easily obtain the state-space eigenproblem:(24)A(p)Z(p)=B(p)Z(p)Λm(p)where(25)A=[−K00M],B=[CMM0]andZ={XXΛm}And eigenvectorsZ˜of the 2N eigensystem can be given by(26)Z˜=Zα={XαXαΛm}={X˜X˜Λm}Post-multiplying each side of Eq. (24) by α, and using the above equation yields another equivalent eigenproblem:(27)A(p)Z˜(p)=B(p)Z˜(p)Λm(p)The differentiation of the previous eigenproblem with respect to the design parameter p for p=p0 yields(28)(A−λ˜B)Z˜,p−BZ˜Λm,p=−(A,p−λ˜B,p)Z˜The eigenvector sensitivitiesZ˜,pcan be obtained in a Nelson form:(29)Z˜,p=V+Z˜cwhere V is the particular solution and c is a coefficient matrix. Based on the idea of the modal method, the eigenvector sensitivity can be expressed as a linear combination of all the eigenvectors. The particular solution V may be expressed by a linear combination of the eigenvectors of the 2N damped symmetric system except those in eigenvectorsZ˜. The eigenvector sensitivityX˜,pcan also be expressed by(30)X˜,p=P+X˜cwhere P is the particular solution of the N-space damped system. One hasV=[PQ]whereQ∈ℂN×mis an undetermined matrix. Note the particular solution P is a linear combination of the eigenvectors of the N-space system except those in eigenvectorsX˜.Based on the orthogonality condition, one can obtain(31)Z˜TBV=αZTB[PQ]=0which can be rewritten in the form of the original space(32)XT(C+λ˜M)P+XTMQ=0SubstituteZ˜,pfrom Eq. (29) into Eq. (28) yields(33)[−K−λ˜C−λ˜M−λ˜MM][PQ]−[CMM0][Xλ˜X]αΛm,p=[K,p+λ˜C,pλ˜M,pλ˜M,p−M,p][Xλ˜X]αThe lower equation from Eq. (33) can be given by(34)MQ=λ˜MP+MXαΛm,pSubstitute this equation into Eq. (32), one can then obtain(35)XT(C+2λ˜M)P+XTMXαΛm,p=0This expression gives the relation between the particular solution P and the eigenvalue sensitivity and can be considered as a constraint condition. An important property of Eq. (35) is that the relation does not depend on the undetermined matrix Q. One idea frequently come up is whether or not to have a method can deal with the derivatives of eigensolutions of damped systems in the form of the original space.Let us consider the eigenproblem of the original space to find the eigenvector sensitivitiesX˜,p(36)M(p)X˜(p)Λm2(p)+C(p)X˜(p)Λm(p)+K(p)X˜(p)=0The differentiation of this eigenproblem with respect to p for p=p0 yields(37)(λ˜2M+λ˜C+K)X˜,p+(2λ˜M+C)X˜Λm,p=−(λ˜2M,p+λ˜C,p+K,p)X˜SubstitutingX˜,pfrom Eq. (30) into the previous equation and post-multiplying each side byαTyields(38)(λ˜2M+λ˜C+K)PαT+(2λ˜M+C)XαΛm,pαT=−(λ˜2M,p+λ˜C,p+K,p)XThe above expression also gives the relation between the particular solution P and the eigenvalue sensitivity.Combine Eqs. (35) and (38), one can obtain a linear system of algebraic equations(39)[λ˜2M+λ˜C+Kξ(2λ˜M+C)XXT(C+2λ˜M)ξξ2XTMX][P˜ξ−1G]=−[(λ˜2M,p+λ˜C,p+K,p)X0]where ξ is a non-zero constant,P˜∈ℂN×m,G∈ℂm×mare unknown matrices which can be determined by solving the above system. Obviously,P=P˜αandG=αΛm,pαT. One can obtain(40)Gα=αΛm,pThe derivatives of repeated eigenvalues can be determined by solving this subeigenproblem. The transform matrix α is normalized asαTα=Imsuch that it satisfies the assumption that α is an orthogonal matrix. Once matrixP˜and the transform matrix α are determined, the particular solution P and adjacent eigenvectorsX˜can be obtained. For simplification, the linear system (39) can be written in the following form(41)Amηm=bmwithAma (N+m)×(N+m) coefficient matrix,ηma (N+m)×m matrix andbma (N+m)×m matrix. It should be pointed out thatAmis a symmetric matrix and the linear system can be solved by the LDLT decomposition method [67]. Sometimes the elements in the coefficient matrix are not all of the same order of magnitude which may cause a big condition number (e.g., see Li et al. [66] for similar study). To reduce the condition number, we determine the non-zero constantξby finding the largest element in absolute value of matrixλ˜2M+λ˜C+Kand dividing the largest element in absolute value of matrixXT(C+2λ˜M). Note the coefficient matrixAmis a full rank matrix, as demonstrated in Section 4. Substituting the non-zero constant ξ into system (39) or (41), leading to a nonsingular, numerically well-conditioned linear system of algebraic equations.This section will show that the homogeneous solution can also be carried out in the form of the original space. Due to the fact that the particular solution has been found, it only remains to compute the coefficient matrix, c. Following the operations in these methods [58,63,64], differentiating Eq. (27) twice yields(42)cΛm,p−Λm,pc+0.5Λm,pp=RwhereR=Z˜T(A,p−λ˜B,p)V−Z˜T(B,pZ˜+BV)Λm,p+0.5Z˜T(A,pp−λ˜B,pp)Z˜Because the particular solution V satisfies the orthogonality condition (31), the right hand of Eq. (42), namely, R, can be simplified asR=Z˜T(A,p−λ˜B,p)V−Z˜TB,pZ˜Λm,p+0.5Z˜T(A,pp−λ˜B,pp)Z˜By using Eq. (25) , R can be expressed in the form of the original spaceR=−X˜T[(λ˜2M,p+λ˜C,p+K,p)P+(2λ˜M,p+C,p)X˜Λm,p+0.5(λ˜2M,pp+λ˜C,pp+K,pp)X˜]LetR=[rjk],c=[cjk]. If we assume that the derivatives of repeated eigenvalues are distinct, the off-diagonal elements of matrix c can be determined without requiring the second-order eigenvalue derivatives,Λm,ppsinceΛm,ppis a diagonal matrix. Thus, the off-diagonal elements ofccan be obtained:(43)cjk=rjkλ˜k,p−λ˜j,pifj≠kNote the diagonal elements of matrixcΛm,p−Λm,pcare equal to zeros, the second-order eigenvalue derivativesΛm,ppcan also be found(44)Λm,pp=2rjkifj=kIt is interested to note that the second-order eigenvalue sensitivities of damped systems can also maintain N-space formula.To determine the diagonal elements of matrix c, just like the case of distinct eigenvalues, an additional constraint should be imposed. Often the additional constraint can be derived by differentiating the normalization. In this study, a new normalization is proposed. For p=p0, the adjacent eigenvectors satisfy the orthogonal normalization (23). LetX˜=[x˜jk]for j,k=1,2,…,m. In the neighborhood of design point p0, namely, p near p0, we assume that the new normalization satisfies(45)x˜nkk(p)=x˜nkkFindnksuch thatx˜nkkis the largest element in absolute value on the kth column ofX˜. It means that the largest element in absolute value on each eigenvector is a constant. The additional constraint can be obtained by differentiating the above normalization with respect to parameter p that yields(46)x˜nkk,p=0which means the element of rownkand column k ofX˜,pis set to zero. LetP=[pjk]. The diagonal elements of matrix c can be calculated by(47)cjj=−(∑k=1,k≠jmx˜njkckj+pnjj)x˜njjforj=1,2,⋯,mSo far the elements of the coefficient matrix c are obtained. The eigenvector derivativesX˜,pcan be then determined by substituting the particular solution P and the coefficient matrix c of the homogeneous solution into Eq. (30).The complete algorithm for computing the eigenvalue and eigenvector sensitivities with repeated eigenvalues is summarized below:(1)Solve eigenproblemMXΛm2+CXΛm+KX=0and obtain the eigenvalues and eigenvectors.Normalize the eigenvectors X usingXT(2λ˜M+C)X=Im.Determine the non-zero constant ξ by finding the largest element in absolute value of matrixλ˜2M+λ˜C+Kand dividing the largest element in absolute value of matrixXT(C+2λ˜M).ComputeΑm=[λ˜2M+λ˜C+Kξ(2λ˜M+C)XXT(C+2λ˜M)ξξ2XTMX].Computebm=−[(λ˜2M,p+λ˜C,p+K,p)X0].SolveAmηm=bmgiven by Eqs. (39) and (41) and obtainP˜and G.Solve the subeigenproblemGα=αΛm,pwhereΛm,pis the diagonal eigenvalue matrix and α is normalized usingαTα=Im. Note the diagonal element ofΛm,pis eigenvalue sensitivities.The adjacent eigenvectorsX˜can be obtained byX˜=Xα. NoteX˜=Xαis only for p=p0. For p near p0, the adjacent eigenvectors should be normalized byx˜nkk(p)=x˜nkk.Determine the particular solutionP=P˜α.ComputeR=−X˜T[(λ˜2M,p+λ˜C,p+K,p)P+(2λ˜M,p+C,p)X˜Λm,p+0.5(λ˜2M,pp+λ˜C,pp+K,pp)X˜]Construct the(m×m)matrix c by the rulecjk={rjk/(λ˜k,p−λ˜j,p)ifj≠k−(∑k=1,k≠jmx˜njkckj+pnjj)x˜njjifj=kLetX˜,p=P+X˜c. The columns ofX˜,pare the eigenvector derivatives.Note that the algorithm of the proposed method maintains N-space formulation without using the state-space approach. For this reason, the computational cost of the proposed method can be remarkably reduced in comparison with those of 2N-space approaches.Consider the formulation for structural modal analysis described by undamped eigenvalue problems(48)(λi(p)M(p)+K(p))φi(p)=0The ith eigenvalue is expressed asλi=(jωi)2wherej=−1,ωi∈ℝis the ith undamped natural frequency.The combined normalization of undamped eigenproblems can be obtained in a Similar way, i.e., for p=p0, we assume the traditional orthogonal normalization is imposed and has the following form:(49)φiTMφi=1and for p near p0, the normalization is the same as it of the symmetric damped system.The eigenvalue sensitivities can be obtained [7]:(50)λi,p=−φiT(λiM,p+K,p)φiAs can be seen, the derivatives of eigenvalues of undamped systems can be considered as a degenerated case of those of damped systems. By following the same procedure as in Eq. (10), the linear system of undamped eigenproblems to determine the eigenvector derivative can be given by(51)F˜iφi,p=h˜iHereFi,hishould be modified intoλiM+Kand−λi,pMφi−(λiM,p+K,p)φi. This formula is similar to the computation of the particular solution of Nelson's method [33], but the presented method can find the eigenvector sensitivities directly.In the case of repeated eigenvalues, the undamped eigenproblem with repeated eigenvalues can be given by(52)M(p)X(p)Λm(p)+K(p)X(p)=0(53)XTMX=ImNote the orthogonal normalization (53) is only for p=p0 and the normalization is the same as it of the symmetric damped system for p near p0. Similarly, the adjacent eigenvectorsX˜(p)can be expressed byX˜(p)=X(p)αand there is(54)M(p)X˜(p)Λm(p)+K(p)X˜(p)=0The differentiation of this eigenproblem with respect to the design parameter p for p=p0 yields(55)(λ˜M+K)X˜,p+MX˜Λm,p=−(λ˜M,p+K,p)X˜The eigenvector sensitivityX˜,palso can be expressed as a particular solution P and a homogeneous solution(56)X˜,p=P+X˜cThe particular solution P can be expressed by a linear combination of the eigenvectors of the N undamped symmetric system except those in eigenvectorsX˜. Thus, the orthogonal constraint condition of the particular solution can be obtained as(57)XTMP=0The particular solution can be determined by using equation,P=P˜αwhereP˜can be obtained by solving the following linear system constructed by combining Eqs. (55) and (57).(58)[λ˜M+KξMXXTMξ0][P˜ξ−1G]=−[(λ˜M,p+K,p)X0]It is well known that in the context of viscously damped systems, their normalization is inconsistent with undamped or classically damped modal theories. For that reason, Eq. (41) cannot be degenerated to Eq. (58). Alternatively, one can choose the undamped eigenvectors normalized by2λ˜XTMX=Im, under such circumstances, it is easy to verify that Eq. (41) can be degenerated to Eq. (58) by following the same procedures. Here, the non-zero constant ξ can be determined by finding the largest element in absolute value of matrixλ˜M+Kand dividing the largest element in absolute value of matrix MX.The derivatives of repeated eigenvalues can be then obtained:(59)Gα=αΛm,pMatrix R can be modified into(60)R=X˜T(K,p+λ˜M,p)P+X˜TM,pX˜Λm,p+0.5X˜T(K,pp+λ˜M,pp)X˜Construct the (m×m) matrix c by the rulecjk={rjk/(λ˜k,p−λ˜j,p)ifj≠k−(Σk=1,k≠jmx˜njkckj+pnjj)x˜njjifj=k.Finally, the eigenvector derivativesX˜,pcan be obtained by the expression,X˜,p=P+X˜c. The columns ofX˜,pare the eigenvector derivatives.As can be seen, the derivatives of eigensolutions for symmetric undamped systems with distinct and repeated eigenvalues can be established by utilizing the similar procedures as in the previous subsections (i.e., Subsections 3.2 and 3.3).The objective in this section is to show that the two systems of algebraic equations expressed in Eqs. (10) and (40) are always solvable, i.e., how to prove that these coefficient matrices will not be singular or badly scaled.This section will show that the coefficient matrixF˜iof Eq. (10) has a rank N.The constraint Eq. (9) can be rewritten as an equivalent form(61)Wiφi,p=0whereWi={nithcolumn0⋯010⋯0︸N}HereWiis a (1×N) vector and the nith component of it associated with the ith eigenvector is set to unity. Let(62)Gm=[FiWiTWi0]By subtracting the last column of Gmfrom every other column and subtracting the last row of Gmfrom every other row, one has(63)Gm(1)=[00Fi11⋮Fi13⋮000⋯000⋯0100Fi31⋮Fi33⋮000⋯010⋯00]Exchange the nirow with the last row, one can obtain(64)Gm(2)=[F^i001]whereF^i=[0Fi11⋮Fi1300⋯010⋯00Fi31⋮Fi330]Multiply the nirow of Eq. (64) by the nielement of Fiyields(65)Gm(3)=[F˜i001]In order to show that the coefficient matrixF˜iof Eq. (10) is non-singular, only need to prove that Gmis a full rank matrix. To prove the coefficient matrix Gmis nonsingular, consider a (N+1)×1 vectordand use the fact that the equation Gmd=0 has the unique solution d=0.Assume Gmd=0 ford=[ϒτ]T∈CN+1. Substitute Gmfrom Eq. (62) into Gmd=0. One can obtain:(66)Fiϒ−WiTτ=0(67)Wiϒ=0Pre-multiplying Eq. (66) byφiTand utilizing Eq. (1) andWiφi={φi}ni, one can show τ is equal to zero. Substituting τ=0 into Eq. (66) yields(68)Fiϒ=0It is clear thatφiis a particular solution of the above equation. Henceϒcan be expressed by(69)ϒ=aiφiwhereaiare constant coefficients. Substituting this expression into Eq. (67), and utilizingWiφi={φi}ni, one can obtainai=0, i.e., d=0. Therefore, it can be concluded that Gmis always a full rank matrix and the coefficient matrixF˜iof Eq. (10) is non-singular.In order to prove the coefficient matrixAmof Eq. (41) is nonsingular, consider a (N+m)×m matrix s and use the fact that equationAms=0has the unique solutions=0. The coefficient matrixAmtakes the form(70)Am=[λ˜2M+λ˜C+Kξ(2λ˜M+C)XXT(C+2λ˜M)ξξ2XTMX]AssumeAms=0fors=[ΘΞ]T∈C(N+m)×m. That is(71)(λ˜2M+λ˜C+K)Θ+ξ(2λ˜M+C)XΞ=0(72)ξXT(C+2λ˜M)Θ+ξ2XTMXΞ=0Pre-multiplying Eq. (71) byXTand utilizing Eq. (20) and normalization (21), one can obtainΞis equal to a zero matrix. SubstitutingΞ=0into Eq. (71) yields(73)(λ˜2M+λ˜C+K)Θ=0Obviously, X is a particular solution of Eq. (73). Thus,Θcan be expressed by(74)Θ=X{c11c12⋯c1mc21c22⋯c2m⋮⋮⋱⋮cm1cm2⋯cmm}wherecikfor i, k=1,2,…,m are constant coefficients. Substituting Eq. (74) andΞ=0into Eq. (72), and utilizing normalization Eq. (21), one can obtaincik=0. That is to say,s=0. Therefore, it can be concluded thatAmis always a full rank matrix.Consider the following four DOF mass-spring system with viscous damping shown inFig. 1. The system matrices M, K and C are, respectively,M=[m0000m0000m0000m],K=[4k+k1−k100−k15k100004k00006k]andC=[4c00004c00004c00006c]where k=1000N/m, m=1kg, c=10Ns/m and k1=1000N/m. The stiffness k was chosen as the design parameter p, and the eigensolution derivatives were considered at p0=1000N/m. The derivatives of system matrices areM,p=0,K,p=[4000000000400006],C,p=0The derivatives of eigensolutions are uniquely determined by the proposed method and shown inTable 1.To illustrate the accuracy of the proposed method and show how to use the new combined normalization, the finite difference method is considered. The finite difference method is the easiest method to implement for calculating the eigensolution derivatives [3,32]. The eigensolution derivatives approximated by the first-order forward differences can be given asΔλiΔp=λi(p0+Δp)−λi(p0)Δp,ΔφiΔp=φi(p0+Δp)−φi(p0)ΔpAs can be seen, for each design parameter p, the finite difference method must perform a complete reanalysis of eigensolutions. In addition, the finite difference method evaluates the true derivative with an error that is on the order of the step size Δp. Although the finite difference method is easy to program, it suffers from computational inefficiency and possible errors [32,65,66].Table 2 lists the approximated eigensolution derivatives computed by the finite difference method with respect to Δp=1N/m. It should be noted that normalization Eq. (7) is used to normalize the eigenvectors at design point p0 and the normalization (8) is used to normalize the eigenvectors at p0+Δp in the case of distinct eigenvalues, and similar procedures for eigenvectors associating to repeated eigenvalues.As it can be seen from Tables 1 and 2, the eigensolution derivatives computed by the proposed method show a good agreement with the results approximated by the finite difference method.To illustrate the robustness of the proposed method, a simple but representative three bar truss structure, shown inFig. 2, is considered. The truss structure has Young's modulus E=2.1×1011N/m2, material density ρ=7860kg/m3. The structure is modeled into three truss elements whose element stiffness and mass matrices areKe=AEle[1−1−11],Me=Aρle6[2112]where A is the area of the cross-section of the truss element and leis the length of the truss element. After performing assembly and applying the boundary condition, the global system matrices can be given byK=AEle[2−10−12−10−11],M=Aρle6[410141012]where leis the length of the truss element. Assume the damping matrix is a linear combination of the stiffness and mass matrices,C=Mθ+Kϑwhereθandϑare the Rayleigh coefficients and equal to 1.0×10−6. For numerical study, we suppose A=0.0001m2 and le=0.01m. The length lewas chosen as the design variable p and the eigensolution derivatives were considered at p0=0.01m. The derivatives of system matrices areK,p=−AEle2[2−10−12−10−11],M,p=Aρ6[410141012]andC,p=0.0005(K,p+M,p)The system has three pairs of conjugate complex eigenvalues: −3.7468×104±2.7117×105i, −4.0076×105±8.0057×105i, −1.3190×106±9.4777×105i. For convenience, the mode shape associated to eigenvalue −1.3190×106+9.4777×105i was only considered in this example. In this case, the condition number of coefficient matrixF˜iof Eq. (10) is 11.412 and the coefficient matrix isF˜i=[3.0693×109−9.1201×109i0003.0693×109−9.1201×109i1.7720×109−5.2655×109i01.7720×109−5.2655×109i1.5346×109−4.5601×109i]One would expect numerical stability of computational approach; however, we get quite a big condition number of the similar coefficient matrixF^iin Nelson's method [36] and the value is 1.32713×1010. The coefficient matrix is given byF^i=[10003.0693×109−9.1201×109i1.7720×109−5.2655×109i01.7720×109−5.2655×109i1.5346×109−4.5601×109i]As it can be noted, the condition number is remarkably reduced, hence the proposed method is numerically well-conditioned.To illustrate the application and efficiency of the proposed method, a two-stage floating raft isolation system, shown inFig. 3 is considered. It should be mentioned that an advanced isolation system used in engineering application is more complicated than the general two-stage isolation system and it can provide a much better vibration reduction than the latter.As shown in Fig. 3, the machine vibration is attempted to be isolated by mounting two machines on a single intermediate raft structure. The two machines have the values: m1=200kg, m2=250kg. The length–width–thickness of raft plate and foundation plate are 1200–800–20mm and 2000–800–40mm, respectively. The four sides of raft plate are all free. The two short sides of foundation plate are clamped and two long sides of it are free. The raft and foundation plates haveYoung's modulus: E=2.0×1011N/m2Density: ρ=7800kg/m3Poisson's ratio: μ=0.3.Two springs and dashpots are used to mount two machines on a single intermediate raft structure, and eight springs and dashpots are fixed between raft plate and foundation plate. The spring stiffness coefficients and the damping coefficients have the following values:k1=1.0×105N/m,k2=5.0×105N/m,c1=1.0×102Ns/m,c2=5.0×102Ns/m.As shown inFig. 4, the finite element model, which is meshed using HyperMesh and modeled using the finite element software-NASTRAN [68], is discretized into 192 elements and has 1258 DOF. System matrices of the finite element model are assembled by NASTRAN and exported by means of DAMP language [69]. By using the system matrices, the proposed method is programmed by MATLAB. The distribution of the nonzero terms of system matrices is shown inFig. 5 obtained by using the MATLAB function spy(·). This floating raft isolation system is a non-proportionally damped systems since it does not satisfy the mathematical condition presented by Caughey and O'Kelly [70] and Adhikari [71]. The lower 10 complex eigenvalues are listed inTable 3. It should be noted that the imaginary part of each complex eigenvalue is the natural frequency in rad/s. The real part of each complex eigenvalue is a measure of the decay rate of the raft system. The system is stable since all the real parts of eigenvalues or decay rate coefficients are negative.Case 1: Material density is considered as the design parameterTo illustrate the computation of the first- and second-order derivatives of eigenvalues and eigenvectors, the material density is chosen as the design parameter p.Table 4 lists the first- and second-order derivatives of some eigenvalues with respect to material density at 7800kg/m3. It has been checked that the first- and second-order derivatives of eigenvalues calculated by the presented method have the same results with those calculated by Nelson's method [37]. As can be seen from Eq. (14), the second-order sensitivity of eigenvalue is a function of the first-order sensitivity of the corresponding eigenvector. It implies that the first-order sensitivities of eigenvectors obtained by using the proposed method can be also exactly obtained.In general, the eigensolution sensitivity can be used to predict the changes of eigenvalues and eigenvectors with respect to the changes of design parameters, select a search direction and form an approximate model for optimization process, and assess the effects of uncertainties of structure or geometry property to system eigensolutions. In this example, the capacity of predicting the changes of eigensolutions with respect to the changes of design parameters is considered. As we know, to avoid performing a complete reanalysis of eigensolutions, Taylor expansion is usually used to approximate the change in eigensolution with respect to an arbitrary change in design parameters based on their first- and second-derivatives of eigensolutions. By using only the first term in Taylor expansion, the change in eigenvalue can be given byλichanged=λiinitial+∂λiinitial∂pΔpwhile considering the first two terms in Taylor expansion, the change in eigenvalue takes the formλichanged=λiinitial+∂λiinitial∂pΔp+12∂2λiinitial∂p2(Δp)2In the same way, the changed eigenvectorsφichangedcan be also approximated.Next, the first- and second-order approximations are used to estimate the change in eigensolution from the change of material density.Figs. 6 and 7 show the influence of changing the material density from 6500 to 9500kg/m3 on the imaginary and real parts of the third eigenvalue, respectively. The first- and second-order approximations of the third eigenvalue are compared with the exact eigenvalue. As can be seen from Fig. 6, the third natural frequency decrease to the variation of material density, the first-order approximation predicts the change in the frequency with good accuracy in the small perturbation region and the second-order approximation improves the approximate results and can predict the change in the frequency with excellent accuracy, even in the eigenvalues with a big change. As can be seen from Fig. 7, the third decay rate coefficient increase to the variation of material density. It is obvious that the first-order approximation predicts the change in the third decay rate coefficient with good accuracy in the small perturbation region and the second order approximation improves the approximate results. As a measure of the error of the first- and second-order approximations of eigenvectors, the relative error is defined asError=(φiexact−φiapprox)H(φiexact−φiapprox)(φiexact)Hφiexact×100%whereφiexactis the exact values,φiapproxis the approximate values and the superscript H denote the conjugate transpose.Fig. 8 shows the error of changing the material density on the third eigenvector. As can be observed, the first-order approximation predicts the change in the third eigenvectors with good accuracy and the second-order approximation makes the error reduce.Case 2: Young's modulus is considered as the design parameterIn this case, the Young's modulus is chosen as the design parameter p.Table 5 lists the first- and second-order derivatives of some eigenvalues with respect to Young's modulus at 2.0×1011N/m2. It is worth mentioning that the first- and second-order derivatives of eigenvalues calculated by the presented method are identical with those calculated by Nelson's method.Next, the first- and second-order approximations are used to estimate the change in eigensolution from the change of Young's modulus.Figs. 9 and 10 show the influence of changing the Young's modulus from 1.6×1011 to 2.4×1011N/m2 on the imaginary and real parts of the third eigenvalue, respectively. As can be seen from Figs. 9 and 10, the third natural frequency increase to the variation of Young's modulus, the third decay rate coefficient decrease to the variation of Young's modulus. In addition, the first-order approximation predicts the change in the frequency and decay rate coefficient with good accuracy in the small perturbation region and the second-order approximation can predict the change in the frequency with excellent accuracy, even in the eigenvalues with a big change.Fig. 11 shows the error of changing the Young's modulus on the third eigenvector. As we can see, the first-order approximation predicts the change in the third eigenvectors with good accuracy and the second-order approximation makes the error reduce.To illustrate the computational efficiency of the proposed method, the eigensolution sensitivities have been also computed by Nelson's method [37]. In this case, the Young's modulus is chosen as the design parameter p. The computational time of Nelson's method and the proposed method is considered with respect to the number of the computed eigensolution derivatives. The CPU time of the proposed method to obtain the first-order derivatives of the first 50 eigensolutions is 113.640s, which slightly reduced in comparison with those elapsed by Nelson's method (115.543s). The reason is that the first-order derivatives of eigenvectors need the matrix decomposition (e.g. LDLTor LU decomposition) ofF˜iin Eq. (10), which need the number of operations O(N3), however, only O(N2) is required for finding the homogeneous solution of Nelson's method [37] that is not required for the proposed method (on the number of operations has been studied in Li et al. [66]).Fig. 12 shows the computational time of Nelson's method and the proposed method with respect to the number of the second-order derivatives of eigensolutions. Since matrix decomposition ofF˜iand vector biof Eq. (17) are available from the computation process of the first-order sensitivities of eigenvectors and the second-order sensitivities of eigenvalues, the solution of Eq. (17) only takes O(N2). In addition, the homogeneous solution of Nelson's method also requires O(N2). For this reason, the computational cost of the second-order sensitivities of eigenvectors is reduced remarkably compared with Nelson's method.

@&#CONCLUSIONS@&#
Design sensitivity analysis of engineering structures deals with the computations of the rate of performance measure from changes in the design parameters describing the structure. The eigensolution sensitivity can be used to predict the changes of eigenvalues and eigenvectors with respect to the changes of design parameters, select a search direction and form an approximate model for optimization process, and assess the effects of uncertainties of structure or geometry property to system eigensolutions. This paper studied the eigensolution derivatives for symmetric viscously damped eigenproblems with distinct and repeated eigenvalues. To simplify the computation, a combined normalization, which combines two traditional normalizations, is presented. Based on the combined normalization, a method for sensitivity analysis of eigenvalues and eigenvectors is studied. In the case of distinct eigenvalues, the proposed method can determine the eigenvector derivatives directly and is robust since the components of coefficient matrices are all of the same order of magnitude. In the case of repeated eigenvalues, an algorithm is presented for computing the eigensolution sensitivities. The algorithm maintains N-space without using state-space equations such that the computational cost is reduced. The method is accurate, compact, numerically stable and easy to be implemented. In addition, the proposed method can be extended to compute the eigensolution derivatives of undamped systems with the similar produces. Finally, three numerical examples have demonstrated the validity of the proposed method. The capacity of predicting the changes of eigensolutions with respect to the changes of design parameters in terms of the first- and second-order eigensensitivities is studied with application to the analysis of a two-stage floating raft isolation system. The computational cost of the second-order sensitivities of eigenvectors can be reduced remarkably since the matrix decomposition of the coefficient matrix is available from the computation process of the first-order eigensensitivities and it has been verified by a two-stage floating raft isolation system with 1258 DOF.