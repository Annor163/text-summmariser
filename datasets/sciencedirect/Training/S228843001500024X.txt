@&#MAIN-TITLE@&#
Hybrid Type II fuzzy system & data mining approach for surface finish

@&#HIGHLIGHTS@&#
A new methodology in predicting a CNC machining output has been investigated.A data mining technique and a hybrid type II fuzzy system are applied.Two different types of membership functions were created to generate a hybrid system.Fuzzy rules are automatically modified in the process of genetic algorithm training.The results showed that the hybrid system generated a far better accuracy.

@&#KEYPHRASES@&#
Data mining,Fuzzy set theory,Surface roughness,Metal cutting,Machining quality characteristics,

@&#ABSTRACT@&#
In this study, a new methodology in predicting a system output has been investigated by applying a data mining technique and a hybrid type II fuzzy system in CNC turning operations. The purpose was to generate a supplemental control function under the dynamic machining environment, where unforeseeable changes may occur frequently. Two different types of membership functions were developed for the fuzzy logic systems and also by combining the two types, a hybrid system was generated. Genetic algorithm was used for fuzzy adaptation in the control system. Fuzzy rules are automatically modified in the process of genetic algorithm training. The computational results showed that the hybrid system with a genetic adaptation generated a far better accuracy. The hybrid fuzzy system with genetic algorithm training demonstrated more effective prediction capability and a strong potential for the implementation into existing control functions.

@&#INTRODUCTION@&#
In this study, a data mining technique (i.e., a new heuristic algorithm) for reduct selection in the Rough Set Theory (RST) is applied to select significant factors (features). Literature review suggests that the RST has not been widely applied to metal cutting problems thus making this research novel [1,2]. In the RST, features characterize each object, and it discovers the dependencies between them. Compared to the usual statistical tools that use population-based approach, the RST uses an individual, object-model based approach that makes a very good tool for analyzing quality control problems [3]. The RST is also able to identify “defective” and “significant factors” simultaneously, which is unique and useful in solving quality control problems. After significant factors are identified, a Fuzzy Logic Theory (FLT) is used to construct the approach that adapts and predicts the surface finish because of the following reasons: (1) using a FLT enables fast and easy synthesis and modification of the control rule base; (2) if a rapid adaptation, using only a few data points, with good accuracy is obtained, the process can respond to the changes; and (3) adaptation is more suitable for today’s machining environment because the adaptive approach can be integrated into the CNC controller to compensate for process variations. The applied Fuzzy Logic System (FLS) is a combined system of Type I and Type II. Different types of system express different strengths to handle heterogeneous factors as well as variables in the process. For example, Type I is effective to deal with “crisp” type of membership function, while Type II is adequate to handle “uncertain” type of membership. The Type II FLS has not been widely used to solve machining process problems thus making this study unique. Finally, the Genetic Algorithm (GA) is incorporated to the FLS for fuzzy adaptation. The combination of the unique strength in each domain is expected to provide a better solution space.In current practice, setting machining parameters are usually conducted by the experience of skilled engineers. Once set, the parameters are usually unchanged during machining, unless prominent anomalies are present. The proposed scheme can be incorporated into the intelligent CNC controllers, and used as constant monitoring device as the machining operations are carried out. Such practice can significantly improve the machining efficiency as well as the perceived quality of the machined parts. It is expected that the proposed approach will help compensate for the unforeseeable variations in the machining process, hence ultimately affects the overall quality of the CNC machining operations.Data mining is the process of extracting and refining knowledge from large database [4–6]. The extracted information is used to predict, classify, model, and summarize the data being analyzed. The RST is a fundamental theory of data mining. This theory was originated by Pawlak [7] and was developed to classify imprecise, uncertain, or incomplete information or knowledge expressed in terms of data acquired from experience. Therefore, it complements the FST [8]. The rough set approach is suitable for processing qualitative information that is difficult to analyze by standard statistical techniques [9]. It integrates learning-from-example techniques, extracts rules from a data set of interest, and discovers data regularities [10]. The RST has been applied to address a variety of problems [11], including (1) representation of uncertain or imprecise knowledge; (2) empirical learning and knowledge acquisition from experience; (3) knowledge analysis; (4) analysis of conflicting; (5) evaluation of the quality of the available information with respect to its consistency and the presence or absence of repetitive data patterns; (6) identification and evaluation of data dependencies; and (7) approximate pattern classification. The RST is introduced as an extension of set theory for the study of intelligent systems characterized by using incomplete information to classify imprecise, uncertain, or incomplete information or knowledge expressed in terms of data. Indeed, the RST is an effective tool for multi-attribute classification problems. In RST, data is expressed in a decision table in which each row represents an object and each column represents an attribute. Formally, the decision table is represented by an information function [12]:(1)S=〈U,Q,V,f〉where U is a finite set of objects, Q is a finite set of attributes,V=∪q∈QVqQUOTE and Vqis a domain of the attribute q, and f:U×Q→V is the total decision function such that f(x,q)∈Vqfor every q∈Q, x∈U QUOTE . The main theme of RST is concerned with measuring what may be described as the “ambiguity” inherent in the data. The essential distinction is made between objects that may definitely be classified into a certain category, and those that may possibly be classified. Considering all decision classifications yields to what is referred to as the “quality of approximation” that measures the proportion of all objects from which definite classification may be achieved. A rough set can be described as a collection of objects that in general cannot be precisely characterized in terms of their values or sets of attributes, but can be characterized in the form of lower or upper approximations [13,14]. The upper approximation includes all objects that possibly belong to the concept, while the lower approximation contains all objects that definitely belong to the concept. As each object is characterized with attributes, discovering the dependencies between attributes and detecting the main attributes is of primary importance. Attribute reduction is one unique aspect of the rough set approach. A reduct is a minimal sufficient subset of attributes, which provides the same quality of discriminating concepts as the original set of attributes.Let us consider the five objects inTable 1, each with four input features and an output feature (outcome). To derive the reduct, consider the first feature F1. The set of objects corresponding to the feature value F1=0 is {1, 2, 3, 5}. This set {1, 2, 3, 5} cannot be further classified solely using the relation F1=0. It is discernible over the constraint F1=0, which is expressed as [x][F1=0]={1, 2, 3, 5}. For the objects in set {1, 5}, the output feature is O=2. For object 3, the output feature is O=1 and for object 2, the output feature is O=0. Therefore, additional features are needed to differentiate between O=0, 1, or 2. Applying this concept, the classification power of each feature can be evaluated. For instance, the feature value F1=1 is specific to O=1. This discernible relation can be extended to multiple features, e.g., [x][F1=0]∧[F2=1]={1, 3} and [x][F1=0]∨[F2=1]={1, 2, 3, 5}, where ∧ and ∨ refers to “or” and “and”, respectively.Most of the rough set based approaches may generate more than one reduct for an object. This paper adapts the reduct generation procedure proposed by Pawlak [12] and presents it in the form of the reduct generation procedure as illustrated inFig. 1. The reduct generation procedure enumerates all possible reduct with one, two and three features that are presented inTable 2.Feature sets are used for predicting an object’s outcome with algorithms. We adapt the reduct generation procedure proposed by Pawlak [12]. The data set is randomly divided into the training set and the testing set. The rule-extraction algorithm is developed to derive the rules from the training set. Then, we also propose the procedure to validate the derived rules on the basis of the testing set. The basic idea behind the heuristic algorithm developed by Kusiak and Tseng [15] is to consider a sequence of the rule sets R1, R2, …, Rq. If one wants to construct the set N={Ni}, follow this sequence: first N1∈R1 is chosen, then N1∈R1, …, and finally Nq∈Rq. Let N(p)={N1, N2, …, Np} denote the selected elements at iteration p of the rule extraction algorithm.This algorithm is initialized in Step 1. Rule sets R1, R2, …, Rq , which correspond to each object generated. In Step 2, the final distance measure matrix is also generated. Selecting the best possible element Np∈Rp is performed in Step 4. In Step 5, the counter for iterations is incremented. Note that one can implement the same algorithm for solving a set of rules with two, three, or up to (n–1) features. After the desired rules have been derived through the REA (seeFig. 2), the next step is to compose those rules to elicit the significant features in the system.The validation of generated rules can be illustrated as follows:Fig. 3.A FLS is a control system, able to handle numerical data and linguistic knowledge simultaneously. In general, it is a nonlinear mapping of input data to a scalar output data. It includes fuzzifier, rules, membership functions, inference engine, and defuzzifier [16]. The key difference between the Type I and II FLS is the membership function. Basically, the membership function of Type-I FLS is completely “crisp,” while the membership function of Type-II FLS remains “fuzzy (uncertain).” Therefore, Type-II FLS is more capable to handle the factor, which includes uncertain values by nature. Typical examples for the use of Type I and II FLS in this study include “cutting speed” that uses Type I because the content of the cutting speed is more certain in nature. “Tool wear” uses Type II since the tool wear changes over time and it is difficult to measure or define precisely.Literature review reveals that there are four sources of uncertainties associated with the FLS [16]: (1) the linguistic meaning of the words that are used in the antecedents and consequents of rules can be uncertain. For different people, the same word may mean different things; (2) consequents may have a range of values and not just a single value; (3) measurements that activate the FLS may be uncertain; and (4) the data that are used to tune the parameters of the FLS may be noisy. Furthermore, Type II FLS is very suitable for solving problems under the following conditions [17]: (1) measurement noise is non-stationary; (2) a data-generating mechanism is time-varying; and (3) linguistic terms being used have a non-measurable domain. Consequently, Type II FLS is more capable of operating the uncertain cases because it provides greater flexibility. Furthermore, the output of Type II FLS is possibly a random value with a range.The framework of Type II FLS is illustrated inFig. 4. Generally, Type II FLS is able to incorporate uncertainties about measurements, fuzzy rules, consequent choices, and unreliable training data into its outputs without sacrificing relevance.The membership function of Type-II FLS can be easily visualized beginning with a Type I FLS membership function. For example, assuming the membership function of Type I FLS is a triangle (seeFig. 5), one could blur the original “crisp” triangle to make a “fuzzy” triangle, which represents the membership function of Type II FLS. Note the contents of the input and output parameters in the membership function of Type II FLS can be set with a range.Fuzzy membership functions of input and output variables and fuzzy rule derivation are illustrated in more detail in this section. Assuming “cutting speed” (CS) and “tool wear” (W) are the two input variables, which have been identified as the significant factors in CNC machining (in this case, turning opertions). The output variable is designated as “surface roughness” (SF). The cutting speed has seven membership functions. The row vector for cutting speed can be stated in the form:(2)CST={S,MS,M,MF,F}where S=slow, 600ft/min; MS=medium slow, 650; M=medium, 700; MF=medium fast, 750; and F=fast, 800. An isosceles triangle is used to represent the membership function of cutting speed. Tool wear (measured along the tool flank) has three membership functions, such that(3)WT={S,M,L}where S=small, 0.000in.; M=medium, 0.025; L=large, 0.05. The maximum allowable wear is set 0.030in. The membership functions for tool wear are defined as follows:(4)ζs(W)=[0.015−W]0.015−1,where0≤W≤0.015(5.1)ζM(W)=[W−0.000]0.025−1,where0≤W≤0.025(5.2)ζM(W)=[0.040−W]0.015−1,where0.025≤W≤0.040(6)ζL(W)=[W−0.025]0.025−1,where0.025≤W≤0.050Surface roughness has five membership functions, such that(7)SFT={F,MF,M,MRR}where F=fine, 50μin.; MF=medium fine, 70; M=medium, 90; MR=medium rough, 110; R=rough, 130. A singleton fuzzy output represents the membership functions. Surface roughness values are selected from the experiment data, which represent the average values of surface roughness under three different levels of tool wear and cutting speed. Since there are five and three partitions in input variables, there would be 15 rules. Basically, fuzzy rules dictate the relationship between the input variables and the output variables, which allow the proper selection of control actions according to the characteristics of the fuzzy inputs. Fuzzy rule statements are summarized inTable 3. Each rule includes IF, THEN statement. For example, the first rule started from upper left corner states that IF CS=S and W=S THEN SF=F.Here, a hybrid FLS is the combination of Type I and II FLS. Every FLS consists of at least four components such as fuzzifier, inference, defuzzifer and rules. In Type II FLS, one additional component called “type-reducer” is incorporated to deal with the interval type of output. Next, detailed description of the membership function of Type I and II FLS is introduced. Basically, the membership function of Type II FLS includes inner and outer triangles. The outer triangle is determined by the minimum, maximum, and most likely values called x1, m, and x2, while the inner triangle is determined by x3, m, and x4 (seeFig. 6). The membership function of Type I FLS included only one triangle as shown inFig. 7.The membership functions of Type I and II FLS are convertible. When x1=x3 and x2=x4, the output of the membership function of Type II FLS w+ and w– is overlapped asw+=w–=w. Therefore, the membership function of Type I FLS can be perceived as a special case of Type II FLS. As a result, the T-Norm and S-Norm of the membership function of Type I and Type II FLS can be verified in accordance with Mendel [16].(8)T-Norm(w+,w−)=(min(w1+,w2+,…,wn+),min(w1−,w2−,…,wn−))(9)S-Norm(w+,w−)=(max(w1+,w2+,…,wn+),max(w1−,w2−,…,wn−))where n is the number of fuzzy variables. In the type-reducer component, the type reduced set is(10)f(w+,w−)=α(1−S)+(1−α)(w++w−)/2where S is the segment area between w+ and w–, α is a coefficient between 0 and 1. To simplify the computation of fuzzy output, one can take the average of w+ and w– as the output. However, the average value might lead to overlooking some important information. For example, the average of the following two sets of data is identical: w+=0.8, w–=0.2 and w+=0.6, w–=0.4. Note that if the value of S increases (i.e., the segment area between w+ and w– increases), then the range of the fuzzy output augments. In order to overcome pitfall of the “average” approach, a new segment area (w+–w–) to compensate the fuzzy output is introduced. To simplify the computation of area S, we assume the value of (w+–w–) is equivalent to S:(11)f(w+,w−)=α(1−w++w−)+(1−α)(w++w−)/2where α=0, w+=w–, f (w+, w–)=f(w)=w. Therefore, this formula is also suitable for the membership function of Type I FLS.The Genetic Algorithm (GA) operates on a population P(k) of solutions rather than a single solution [18,19]. The operation of the standard GA is shown inFig. 8.In Fig. 8, the initialization and evaluation are the first steps of performing the standard GA and followed by the selection function, which intends to select a part of the initial population. The crossover function aims to achieve genetic diversity in the population by exchanging some genetic material in the relevant population members, while the mutation function aims to introduce an element of randomness. Despite its promise, a quite serious limitation of the GA is its primarily intention for unconstrained search. A number of techniques have been proposed for handling constraints in the GA. One such technique is to penalize infeasible solutions when evaluating each member of the population. An example of a well-known design system, which uses penalty functions, is Engineous [20]. Constraints in Engineous have a goal, actions, a weight, and conditions [21]. While penalty functions are useful when all variables are measured in the same unit, extreme care must be taken to avoid problems in scaling when applying penalty functions [22]. Perhaps, the primary difficulty with penalty functions is that slightly infeasible solutions, which otherwise would be of high quality, can contain a great deal of useful “genetic” materials. With high penalties, these solutions and their genetic materials are lost. With low penalties, the population may become filled with infeasible solutions.This problem has led to a number of penalty functions, which increase with time [18,22]. Another basic technique for handling constraints is to use backtracking whenever constraints are violated. During initialization, crossover, or mutation, any attribute values that cause constraint violations are retracted and new values are assigned. The cycle repeats itself until a feasible solution is obtained.The prediction accuracy and learning speed of the GA are proved much better than back propagation algorithm in the artificial neural networks (ANNs) [18–20]. GA adaptation starts with approximate control rules derived from the empirical models and refines the control rules through a learning process when process variations occur. The fuzzy input remains the same, while the fuzzy output membership functions are adapted to minimize errors. In GA, the weights associated with the degree of input membership values are adapted because the inputs (CS and W) are assumed to affect the process output differently under process variations. Unlike the conventional ANNs, where a large number of training data is necessary, a single-step learning method can be selected. When the first part is made, the outcome of the surface roughness is measured and the weights go through a series of adaptation processes to minimize errors between the process output and the predicted values for successive parts. An error evaluation function is given as follows:(12)MinE(i)=1/2∑i=1n(yi−di)where E(i)=error between the actual surface roughness and the fuzzy output, yi=fuzzy output, di=process output, and n=a number of parts.

@&#CONCLUSIONS@&#
