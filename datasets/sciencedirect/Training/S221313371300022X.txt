@&#MAIN-TITLE@&#
CosmoHammer: Cosmological parameter estimation with the MCMC Hammer

@&#HIGHLIGHTS@&#
We analyse MCMC methods for cosmology regarding parallelisability and efficiency.We present the Python framework CosmoHammer for parallelised MCMC sampling.It enables us to estimate cosmological parameters on high performance clusters.To test the efficiency of CosmoHammer we use an elastic cloud computing environment.

@&#KEYPHRASES@&#
Markov chain Monte Carlo methods,Cloud computing,Cosmological parameter estimation,

@&#ABSTRACT@&#
We study the benefits and limits of parallelised Markov chain Monte Carlo (MCMC) sampling in cosmology. MCMC methods are widely used for the estimation of cosmological parameters from a given set of observations and are typically based on the Metropolis–Hastings algorithm. Some of the required calculations can however be computationally intensive, meaning that a single long chain can take several hours or days to calculate. In practice, this can be limiting, since the MCMC process needs to be performed many times to test the impact of possible systematics and to understand the robustness of the measurements being made. To achieve greater speed through parallelisation, MCMC algorithms need to have short autocorrelation times and minimal overheads caused by tuning and burn-in. The resulting scalability is hence influenced by two factors, the MCMC overheads and the parallelisation costs. In order to efficiently distribute the MCMC sampling over thousands of cores on modern cloud computing infrastructure, we developed a Python framework called CosmoHammer  which embeds emcee, an implementation by Foreman-Mackey et al. (2012) of the affine invariant ensemble sampler by Goodman and Weare (2010). We test the performance of CosmoHammer for cosmological parameter estimation from cosmic microwave background data. While Metropolis–Hastings is dominated by overheads, CosmoHammer is able to accelerate the sampling process from a wall time of 30 h on a dual core notebook to 16 min by scaling out to 2048 cores. Such short wall times for complex datasets open possibilities for extensive model testing and control of systematics.

@&#INTRODUCTION@&#
Bayesian inference is a standard procedure in cosmology when the measurement results are compared to predictions of a parameter-dependent model. The likelihood functionL(θ)is defined as the conditional probability of a measurement outcomexgiven the model parameters are fixed toθ:L(θ)≔p(x|θ). Bayesian inference tells us how to update our knowledge from a prior distributionq(θ)to a new posterior distributionpnew(θ)which accounts for the recent measurement:pnew(θ)∝q(θ)p(x|θ).For exploring the likelihood functionLor the posterior distributionpnewof the parameters, Markov chain Monte Carlo (MCMC) algorithms are today’s method of choice when no functional expressions are available. Starting with the analysis of cosmic microwave background (CMB) data in 2001 (Christensen et al., 2001; Knox et al., 2001), MCMC methods turned into a vital tool in the analysis of astronomical data from all sorts of cosmological probes.Most of the analyses in the literature rely on a particular instance of an MCMC method, the so-called Metropolis–Hastings algorithm from 1970 (Metropolis et al., 1953; Hastings, 1970). It is based on a random walk in the parameter space of the likelihood, serially proposing new positions that are accepted or rejected according to its likelihood weights. The Fortran program CosmoMCby Lewis and Bridle (2002), for example, is a widely used and very successful tool for parameter estimation in the cosmology community, based on the Metropolis–Hastings algorithm and the Boltzmann integrator CAMB(Code for Anisotropies in the Microwave Background by Lewis et al., 2000). A large number of scientific projects have used CosmoMC, among them prominent observations as the Wilkinson Microwave Anisotropy Probe (WMAP) (Dunkley et al., 2009) or the Sloan Digital Sky Survey (Tegmark et al., 2004).Recently, Foreman-Mackey et al. (2012) presented emcee, a Python implementation of a novel MCMC algorithm by Goodman and Weare (2010) which has several potential advantages over Metropolis–Hastings: the sampling depends on less tuning-parameters and is independent of linear transformations of the parameters. Furthermore, emcee is not based on a single iterative random walk but uses an ensemble of walkers which can be moved in parallel.Creating samples for the estimation of cosmological parameters from CMB measurements, for example, typically takes a few hours or even days on a desktop computer when using the Metropolis–Hastings algorithm. Whenever the sampling process has to be repeated a number of times in order to study the fit of various distinct models to the data or to find out about the systematics of a measurement, such a long run time gets increasingly problematic. We therefore focus on the parallelisability of MCMC methods in order to minimise the wall time of the calculation.One of the main questions we try to answer is how fast one can, in principle, generate a useful sample of a given distribution when using emcee on an extended computing environment such as a cloud service or grid computer. For this purpose, we combined the Boltzmann integrator CAMBand the WMAP likelihood code and data (Larson et al., 2011; Komatsu et al., 2011) (both written in Fortran90) with the emcee sampler by Foreman-Mackey et al. (2012) in a Python framework–called CosmoHammer in the following–allowing us to study the example of parameter inference from CMB data in detail. As parameter estimation with CMB data is well documented in the literature and standard MCMC tools are publicly available, it is also a good reference point for the performance of the algorithm by Goodman and Weare as compared to a Metropolis–Hastings sampler.CosmoHammer has been designed for optimal computational performance on large scale computing environments such as the Amazon elastic compute cloud (EC2).11http://aws.amazon.com/ec2/.We carry out a careful analysis of CosmoHammer’s sampling efficiency, focusing in particular on the implications arising from the simultaneous sampling on multiple computers.The architecture of our code makes CosmoHammer easily extendable to other applications, as it is straight forward to plug in the Python modules containing further likelihood functions or codes for theory predictions.We proceed as follows: in Section  2, the analysis of cosmological data using MCMC methods is briefly introduced and limitations of the current state-of-the-art are discussed. Section  3 explains how we address these limitations using the algorithm by Goodman and Weare (2010) and its implementation by Foreman-Mackey et al. (2012). Introducing our code in Section  4, we review its components, explain its architecture and outline the parallelisation scheme. We test the code in Section  5 by sampling the WMAP 7 likelihood and comparing the results to MCMC chains from the Metropolis–Hastings sampler CosmoMCby Lewis and Bridle (2002). In Section  6 we assess the performance of CosmoHammer on different cloud computing configurations. We discuss the results and conclude in Section  7. Finally, we explain the installation of the package and give detailed examples for running the algorithm in the Appendix.In the following, we give a brief introduction to MCMC methods by discussing the Metropolis–Hastings (MH) algorithm as an example. Afterwards, we focus on the difficulties one faces when applying those methods to cosmological data.Assume that we are interested in a probability density functionp(θ)(called target distribution in the following) which is not given explicitly but can be calculated numerically up to a constant factor. If we want to learn aboutp(θ)we need to estimate it from a finite number of numerical evaluations. MCMC algorithms generate a sample distributed according to the target distribution in a probabilistic fashion. We illustrate it using the MH algorithm as an example. A classic textbook for further information is MacKay (2003), which is also on the web.22http://www.inference.phy.cam.ac.uk/itprnn/book.html.Imagine the sampling process as a random walk in the parameter space of the target distribution. The walk has to be initialised at a pointθ0and a common method is to start at a random position close to the region where the likelihood is expected to be centred. In order to determine the next position, we need to specify a proposal densityPwhich has to be easy to sample and is usually chosen to be a Gaussian distribution. The probability of randomly proposing the new positionθ′when being atθt–thetth position in the sample–is given byP(θ′;θt). After samplingθ′fromP, the new position is accepted with probabilitymin(1,p(θ′)p(θt)P(θt;θ′)P(θ′;θt)).The updatedθt+1is finally given byθ′if the step is accepted or byθtif it is rejected. The set{θt}t∈{0,…,T}converges to a sample from the target distributionp(θ)for largeT(Metropolis et al., 1953; Hastings, 1970).It is worth noting that the efficiency of the sampling crucially depends on the chosen proposal distribution. IfPmainly proposes positions in the relevant parts of parameter space where the target distribution is large, the chain very quickly converges to a sample ofp(θ). Yet, if the proposal distribution tends towards positions wherep(θ)has low probability, most of the steps are rejected and samples will be heavily correlated. Using a proposal which is close top(θ)is a convenient choice, as it guarantees that the proposed positions and the target are distributed similarly.In cosmology the target distribution often arises from Bayesian inference. We consider the example of parameter estimation from CMB data for illustration. An observation measuring the CMB yields the angular power spectrum of the radiation as a result. At the same time, this power spectrum can be predicted from theoretical models for the evolution of the universe. The minimal concordance model,ΛCDM, depends on six parameters and it takes a few seconds to calculate it using a numerical Boltzmann integrator like CAMB.The likelihood functionLof the parameters in the model arises from the conditional probability of the measurement resultXgiven the parametersθ=(θ1,…,θd):L(θ)≔p(X|θ).If we already have information on the parameters in the form of a prior distributionq(θ)from the previous observations,Lis used to update it according to Bayes’ rule:pnew(θ)∝L(θ)q(θ).BothLand the posteriorpnewdepend on the results of the numerical Boltzmann integrator and are hence not available as analytical functions ofθ. As the number of parameters in this problem is usually greater than or equal to six, MCMC methods have to be used for estimatingLorpnew. Although MCMC sampling makes the estimation feasible for large dimensions, it still suffers from the fact that calling the likelihood function–i.e. running the Boltzmann integrator for some specified parameters and comparing the results to the data in the CMB example–is costly in terms of time and resources. Consequently, we want our sampler to be as efficient as possible in terms of likelihood function calls per converged sample.Additionally, MH sampling is by definition a serial process, iterating the calls to the likelihood function several thousand times. Using a single MH chain for inferring the parameters ofΛCDMfrom CMB data with an existing MCMC software package like CosmoMC, for example, takes at least a few hours on a notebook. Such runtimes are practical in cases where a single parameter estimation run suffices, but quickly become limiting in general applications.Testing how well various other models different fromΛCDMfit the data implies that the likelihood has to be explored separately for every distinct model. Furthermore, it is often necessary to introduce so-called nuisance parameters which model the process of data generation. Altering the number and the effect of the nuisance parameters is a way to test for the systematics in a measurement and requires the MCMC sampling process to be repeated multiple times. Whenever the MCMC analyses have to be iterated, it is hence desirable to decrease their run time—either by improving on the underlying MCMC process or by running the calculations in parallel on a cluster or cloud computing environment. In the next section, we explain why the Python sampler emcee is a good choice for achieving such a speed-up.We learned in Section  2 that MCMC sampling can be very time consuming in cosmological applications when the likelihood function is hard to evaluate numerically. Here we study how this can be overcome when using more recent MCMC algorithms such as the one by Goodman and Weare (2010) (called GW in the following) instead of MH. We therefore introduce the algorithm and its implementation by Foreman-Mackey et al. (2012) before discussing its advantages in terms of efficiency and parallelisation.Instead of a single position which is updated during the course of the sampling, GW uses an ensemble of walkers which are spread on the parameter space of the target distribution. At every iteration, the walkers are randomly assigned to a partner walker chosen from the ensemble and a random point on a line connecting their positions is proposed as the next step.More formally, letθtidenote the position of walkeriaftertiterations. When updating this position toθt+1i, we pick another walkerθtjat random withj≠i, sample a valuezfrom the fixed distributionq(z)={1zifz∈[1a,a]0otherwise ,with tuning parameteraand propose the positionθ′=θtj+z(θtk−θtj). After evaluating the target distributionpat the proposed position, we accept the step ifzd−1p(θ′)p(θti)≥r,withrbeing a random number from[0,1]anddthe dimension ofp.GW is affine invariant, i.e. invariant under linear transformations of the target distribution. This implies in particular that the sampler is not sensitive to the scales of the parameters and does not depend on the covariances of the target distribution.The algorithm by Goodman and Weare (2010) was slightly altered and implemented as the Python module emcee by Foreman-Mackey et al. (2012). In this implementation, the algorithm does not update the walkers serially but instead divides them into two subgroups and updates all of the walkers from one subgroup at a time, using the other half of the walkers as their references.In Section  2 we mentioned that MH needs a proposal distributionPwhich is governing the efficiency of the algorithm. Thinking ofPas ad-dimensional Gaussian distribution with unknown covariance matrix, one finds that the MH hasd(d+1)/2tuning parameters. When the scales of variances and covariances of the target distribution are well known before the sampling, the tuning parameters are very helpful: by supplying the MH with an appropriate covariance matrix, the parameter space is explored efficiently and the sample quickly converges. Yet, if the target distribution is unknown and the covariance matrix is only a guess, the efficiency of the algorithm generally decreases. In the case of non-Gaussian target distributions even a tuned Gaussian proposal might not match the target and the efficiency can be low.On the other hand, GW is affine invariant and thus not sensitive to the scales of the target distribution. Hence, the efficiency of the sampling process does not depend on having a good estimate of the target distribution before the sampling and a tuning of the algorithm is not necessary.Equally important is the use of an ensemble of walkers instead of a single chain. As the emcee implementation updates a large number of positions simultaneously at every iteration of the process, it can be parallelised in order to take advantage of a compute cluster (see Section  4).The emcee sampler based on GW thus has the potential to improve upon the limitations outlined in Section  2. First, we expect the sampling process to be fairly robust to changes in data and model. Second, it is possible to distribute the sampling on the multiple nodes of modern cluster or cloud computing environments. Both advantages can be achieved without tuning of the sampler or parallelisation of the underlying likelihood and theory codes.We developed a Python framework called CosmoHammer for the estimation of cosmological parameters. The software embeds the Python package emcee by Foreman-Mackey et al. (2012) and gives the user the possibility to plug in modules for the computation of any desired likelihood. The major goal of the software is to reduce the complexity when one wants to extend or replace the existing computation by modules which fit the user’s needs as well as to provide the possibility to easily use large scale computing environments.We applied the principle of chaining modules for the computation of the likelihood as depicted in Fig. 1. The class diagram shows the underlying architecture and the most important classes. The architecture allows for the development of self-contained and tested modules which can be assembled in different sampling configurations. The internal design separates parameter space exploration, theory prediction and likelihood computation. This makes it easy to extend or replace these modules by new algorithms.The concept of CosmoHammer divides the modules in two logical groups: modules for the computation of the likelihood and core modules. The core modules produce information like the CMB power spectrum or other theory predictions for the likelihood modules. The individual core modules can be combined in an instance of the LikelihoodComputationChain module (see Fig. 1). The chain stores the modules and initialises them in the right order. Furthermore, it ensures that the sampled parameters stay within physically motivated bounds during the sampling process.The modules in the LikelihoodComputationChain communicate via the ChainContext in which arbitrary data can be stored and retrieved. This minimises the dependencies between the individual modules and ensures that they can be replaced without the need to change or extend CosmoHammer.CosmoHammer comes with a set of modules which compute the CMB power spectrum and the WMAP likelihood by wrapping the theory prediction code CAMBby Lewis et al. (2000) and the likelihood code from the WMAP team (Larson et al., 2011; Komatsu et al., 2011). It integrates the two Fortran modules by wrapping them using numpys F2PY.33http://www.scipy.org/F2py.The Fortran to Python interface generator provides the connection between Python and Fortran. In this way we can take full advantage of the well tested and widely used Boltzmann integrator CAMBas well as of the WMAP likelihood computation module while combining them with the emcee sampling algorithm. Furthermore, by using the wrapped code we benefit from the performance of Fortran in the convenient Python environment.To use CAMBand the WMAP code we create an instance of the LikelihoodComputationChain and add an instance of the CambCoreModule and CmbWmapLikelihoodComputationModule. The CambCoreModule delegates the computation of the power spectrum to the CambWrapperManager and the CmbWmapLikelihoodComputationModule in turn delegates the likelihood computation to the WmapWrapperManager.Alternatively an instance of the preconfigured CmbLikelihoodComputationChain can be created. This chain extends the regular LikelihoodComputationChain and ensures the correct setup of the two modules. A detailed example is given in Appendix B.CosmoHammer also provides different samplers. The samplers embed the emcee package and are responsible for logging and storing the obtained results. The CosmoHammerSampler is used when running on a single physical computer using one or multiple threads. The functionality, however, is limited to a single computer. The extended MpiCosmoHammerSampler provides the required functionality when CosmoHammer should take advantage of a computation cluster with multiple physical nodes like a cloud or grid computer. This sampler uses Message Passing Interface (MPI)44http://mpi4py.scipy.org/.for the communication between the nodes in the cluster.The implementation uses the paradigm of workload partitioning in which the work is split into blocks of nearly equal length. Every node then processes its block and returns the results to the master node. The master node gathers all results and merges them into a single list which then is returned to emcee.When using a compute cluster the nodes often come with a large number of computational cores. Writing code that fully benefits from such a large number of cores is usually difficult. Therefore, it makes sense to split the workload also on the node since using a smaller number of cores per computation while performing multiple computations in parallel is typically more efficient. In Section  6.2 it can be seen how the execution time decreases when CosmoHammer uses multiple processes on one physical node.If it is desired to distribute the workload to several nodes in a cluster as well as to spawn multiple processes on a node, the provided ConcurrentMpiCosmoHammerSampler can be used. This sampler introduces another level of parallelisation by using Python’s built in multiprocessing package.By default, all samplers initialise their walkers in a ball around a given centre point as suggested in Foreman-Mackey et al. (2012) The starting positionθ0iof every walkeriis then computed as follows:θ0i=Pc+N(0,1)∗Pw,wherePcis the centre point andPwis the start width. BothPcandPwhave to be supplied by the user for every parameter. Furthermore, CosmoHammer comes with a built in generator producing a top-hat distribution as follows:θ0i=Pc+ϵ∗Pw,whereϵis a pseudorandom number ranging from −1 to 1.If one prefers to launch the sampling process with a different starting strategy, it is possible to pass a custom implementation of a PositionGenerator to the CosmoHammerSampler instance.For testing the efficiency of CosmoHammer we compare it to CosmoMC, a widely used MCMC engine for cosmological parameter estimation by Lewis and Bridle (2002). CosmoMCis a Fortran90 code which also uses CAMBand the WMAP likelihood code for estimating cosmological parameters from CMB data, but it employs the MH algorithm for creating its MCMC chains. It furthermore contains an extensive choice of additional datasets and analysis modules.To optimise the sampling process, Lewis and Bridle (2002) use the following seven default parameters for describing theΛCDMmodel: the physical baryon densityΩbh2, the physical dark matter densityΩDMh2, the ratio of the approximate sound horizon to the angular diameter distanceθ, the reionisation optical depthτre, the scalar spectral indexns, the primordial superhorizon power in the curvature perturbation on0.05Mpc−1scalesΔR2, and finallyASZ, a Sunyaev–Zel’dovich template normalisation. Furthermore, some of the parameters are rescaled to end up with similar orders of magnitude.As emcee is affine invariant, the particular choice of parametrisation is not expected to influence its performance. For simplicity, we decided to use the same ones as CosmoMCup to rescaling, yet replacingθby Hubble’s constantH0.Following the lines of Foreman-Mackey et al. (2012) we adopt the autocorrelation time as our primary quality criterion for an MCMC sampler. Consider a probability density functionp(θ), an MCMC sample{θt}of this distribution, and a functionf(θ)whose mean〈f〉=∫f(θ)p(θ)dθwe wish to estimate. The autocorrelationCffoff(θ)evaluated at the sample points{θt}with lagTis defined as:Cff(T)≔〈(f(θt)−〈f〉)(f(θt+T)−〈f〉)〉.Typically, the autocorrelation of an MCMC sample is non-zero and decaying with increasing lag:Cff(T)∝exp(−Tτff). However, if the points{θt}were independent of each other, the autocorrelation function would vanish for allT≥1.Let us also define the normalised autocorrelation functionρff(T)=Cff(T)Cff(0),whereCff(0)is the variance of the sample{ft}≔{f(θt)}. There are two relevant timescales connected toρff(T). On the one hand, there is the exponential autocorrelation timeτexpwhich is defined by(1)τexp=limsupT→∞T−log|ρff(T)|.On the other hand, the integrated autocorrelation timeτintis given by(2)τint=12+∑T=1∞ρff(T).In generalτexpandτintare not equivalent, although they are both equal toτfffor exponentially decaying autocorrelation functionsCff.The reason whyτexpandτintare relevant for the analysis of MCMC samples is connected to the issues of thermalisation at the beginning of an MCMC chain (often called burn-in) and the statistical errors one has to account for when evaluating the expectation value off(θ)using the sample{θt}(see the lecture notes by Sokal, 1989 for more information).The exponential autocorrelation time tells us how many iterations should be discarded at the beginning of the Markov chain in order to avoid an initialisation bias, since it measures the time we have to wait for two positions in the sample{ft}to be close to uncorrelated. Discarding a few exponential autocorrelation times at the beginning of the sampling typically suffices to suppress the bias.At the same time, the integrated autocorrelation time determines the standard error of the mean through:(3)V ar(f̄)=2τintNV ar(ft),whereNis the size,f̄is the mean, andV ar(ft)is the variance of the sample{ft}.An estimate forCff(T)is given by(4)Cff(T)≈Cˆff(T)=1N−T∑t=1N−T(ft−f̄)(ft+T−f̄).Estimating the integrated autocorrelation time from (4) is not easy because of issues regarding the statistical noise in the largeTlimit ofCˆff(T). Yet, the autocorrelation function for our problem is exponentially decaying, meaning that we can also evaluateτexpinstead ofτint. We find that estimatingτintfrom a fit toCˆff(T)is the best choice for evaluating the autocorrelation time of this particular problem (see Appendix C for more information). Asτexpandτintare equivalent for our purposes, we denote both as the autocorrelation timeτfor simplicity.We typically demand that MCMC samples satisfy the following accuracy criterion: the statistical error of the meanf̄we wish to estimate–defined in Eq. (3)–has to be smaller than a given fractionϵof the standard deviation of its marginal distribution. In other words(5)V ar(f̄)V ar(f)=2τN≤ϵand consequently(6)N≥2τϵ2,whereNis the total size of the sample. For multiple walkers or independent chains,Nis given by the number of steps per walker or chainn(called sequential steps in the following) times the number of walkersLand hence(7)n≥2τϵ2L.Eq. (5) only holds when the sample is unbiased by the initialisation and this is true in the asymptotic limit of largen. When sampling on a cloud computing infrastructure, however, we need a large number of walkersLfor maximum parallelisability and hence expect rather smallnaccording to (7).Yet, the sample gets close to unbiased when discarding the initial steps of every walker or chain as burn-in. We already mentioned in Section  5.1 that such a burn-in phase is expected to last for a few autocorrelation times. When using only a few walkers, i.e.Lclose to one,nis large and the burn-in phase is a subdominant part of the overall sampling. AsLgrows, though, the number of sequential stepsnbecomes comparable to the burn-in length and the discarded samples turn into a dominant fraction of the overall sample size. We analyse the consequences of this result in Section  5.4.In the following we want to compare CosmoHammer to two different configurations of CosmoMC. In each case we sample the likelihood given by the Fortran90 code and the data of the WMAP 7 team.The first CosmoMCinstance is an out-of-the-box approach, using the standard configuration shipped with the CosmoMCpackage. It initialises the chain in a small ball around an estimated mean of the likelihood using the numbers from Table 1and supplies a covariance matrix to specify the Gaussian distribution which is used as the proposal. We will refer to this configuration as fine-tuned CosmoMC.The second approach employs an option of CosmoMCwhich splits the sampling into two phases. Starting with an initial guess for the covariance matrix in the tuning phase–a diagonal matrix with estimated variances in our case–the sampler continuously updates the proposal’s covariance matrix from the last half of the generated samples. Afterwards, CosmoMCuses the generated proposal to create the samples for estimating the likelihood. This approach will be called self-tuning CosmoMCfrom now on. When using multiple independent chains, CosmoMCallows one to automatically stop the tuning phase once a convergence criterion is fulfilled. We use this option to ensure that the sampling after the tuning phase is comparable to the fine-tuned CosmoMCprocess. For our analyses, we used 10 independent chains for both CosmoMCconfigurations.For CosmoHammer we call CAMBin exactly the same fashion as the standard CosmoMCconfiguration does (i.e. we use the same theoretical model for our cosmology) and also initialise the walkers according to Table 1. We furthermore used emcee with 350 walkers, a rather arbitrary pick which was convenient to work with.The bounds listed in Table 1 are needed to ensure that the parameters which are passed to the Boltzmann integrator CAMBmake sense physically. Whenever the sampler proposes a position which is out of bounds, CosmoHammer returns zero probability immediately.A good estimate for the autocorrelation function is important for the analysis of the sample. For both CosmoMCand CosmoHammer the autocorrelationsCˆX(T)=1n−T∑t=1n−T(Xt−X̄)(Xt+T−X̄),withXbeing one of the seven dimensions of the parameter-space, behave equivalently for all parameters. As the estimation of the autocorrelation times for the different sampler configurations is not straightforward, we give a detailed description of our procedure in Appendix C. The results can be found in Table 2. It is not very surprising that the CosmoMCprocesses are more efficient in terms of calls per independent sample, as they are using a well tuned proposal to sample a target distribution which is itself close to normally distributed. On the other hand, emcee needed no tuning while still performing reasonably well in terms of autocorrelation time.Additionally we need to consider the optimal length of the burn-in period before the actual sampling starts. As was discussed in Section  5.2, this is particularly important if we want to iterate a large number of walkers. Let{Xti}denote the position of walkeri∈{1,…,L}at iterationt∈{1,…,n}for parameterX. For finding the burn-in length, we observed the mean sequential stepX̄tX̄t=∑i=1LXtifor increasing timet. Once the positions of the walkers are drawn from the target distribution in an unbiased way, we expectX̄tto vary around the true mean as predicted by the standard error in the meanσ(X̄t)=σL,whereσis the standard deviation of the marginal target distribution for parameterX. As long as this is not the case, the walkers are still biased by the initialisation.Using this method we find that a safe choice for the burn-in period is 250 for both the fine-tuned CosmoMCand CosmoHammer, as can be seen in Fig. 2for parameterΔR2. This is surprising as both processes have very different autocorrelation times, but the MCMC algorithm of emcee turns out to be very efficient in getting rid of its initialisation bias. The self-tuning algorithm needs about 2000 iterations to estimate its proposal distribution, but already starts at an unbiased position afterwards. The reason for such a long tuning phase is that the sampling efficiency is very poor when the sampler is untuned in the beginning.We can now consider the statistical errorϵfrom Eq. (7) as a function of burn-in or tuning lengthb, sequential stepsn, number of walkersL, and autocorrelation timeτ:(8)ϵ=2τ(n−b)Lforn>b.This is visualised in Fig. 3for all sampler configurations, usingτfrom Table 2 and the corresponding burn-in and tuning values.We find that the most efficient way to create a sample which satisfies the accuracy criterion (5) is to use a fine-tuned CosmoMC. This is not surprising, as it has a good estimate of the target distribution before it even starts the sampling process, resulting in a short autocorrelation time and burn-in phase. Yet, a well tuned MH is typically not available when analysing new data. In this case, the self-tuning CosmoMCconfiguration or similar concepts with lengthy tuning phases have to be used for configuring the sampler.It is this tuning phase which turns out to be the bottle-neck for the parallelisation of a MH sampler. We can see in Fig. 3 that for as few as 10 walkers, CosmoHammer reaches the 10% error regimeϵ≤0.1before the self-tuning CosmoMCrun even finalises the tuning atn=2000. When estimating the parameterXfrom the target distribution, the result usually readsXˆ=X̄±σ(X),with estimateXˆ, sample meanX̄, and standard deviationσ(X). Consequently, the statistical error does not affectXˆup to the second digit ofσ(X)whenϵis smaller than 0.1 and is hence sufficiently small for most applications in cosmological parameter estimation.We know from the previous discussion that the errors we expect for our estimates of the mean are of the orderσ2τ/N, whereσis the standard deviation of the target distribution for the respective parameter. The total sample size after the burn-in was chosen to beN=250×350=87,500for CosmoHammer, predicting an accuracy in the mean estimate of about 3.4% relative to the standard deviation. The same precision will be reached when using aboutN=3500×10=35,000samples from CosmoMC.As we choseNsuch that the error is smaller than 3.4% of the standard deviation, the parameter estimates of the different sampler configurations are supposed to vary on this scale, too. We can see from Table 3that this is indeed the case. Finally, Fig. 4shows the projections of the 7-dimensional likelihood into one and two dimensional marginal distributions.We conclude that the samples of CosmoMCand CosmoHammer behave just as expected from our analysis in Section  5.1. In particular, this means that the quality of the sampling is well understood once the autocorrelation of the MCMC process is known. Tests based on multiple independent instances of the employed sampler configurations support these conclusions.The performed computations required a large amount of computational power. We therefore decided to explore the possible benefits of cloud computing by means of CosmoHammer. One of the major advantages of this computing strategy is that the configuration of the cloud can be easily tailored to the problem at hand. In the cloud more computational power can be added within minutes by renting extra compute instances on demand, resulting in an optimised execution time. The following section describes the environment and the configuration used to perform the benchmarks.As cloud service provider we decided to use Amazon EC2 in combination with the Starcluster Toolkit (Software Tools for Academics and Researchers).55http://star.mit.edu/cluster/.Table 4shows the configuration of the instance types we used for the benchmarks. The high performance computing cluster consisted of one master node and several worker nodes. At the moment of the benchmarks one cc2.8xlarge Instance ships with2×Intel Xeon E5-2670, eight-core architecture with Hyper-Threading, resulting in 32 cores per node. We used a m1.large instance as master node mainly to benefit from the high I/O performance in order to reduce the loading time of the WMAP data.To compile the native Fortran modules, we used Intel’s ifort compiler and mkl libraries, Python 2.7, and numpy 1.6.2.The results depicted in Fig. 5have been realised with one to 64 worker nodes (32–2048 cores) and different combinations of processes and threads per node. The processes define the number of computations executed in parallel and the threads represent the number of cores used for one computation. In the case of 4 processes and 8 threads, for instance, there were four Python processes working in parallel on every node, each of them spawning eight threads.For all test runs, CosmoHammer was configured to use 350 walkers and to run 500 sampling iterations, resulting in 175,000 samples per run. This sample size was also used for the parameter estimation in Section  5.5. We used a LikelihoodComputationChain with the CambCoreModule and the CmbWmapExtLikelihoodComputationModule for the computation of the likelihood.Fig. 5 shows that the wall timeTas a function of number of coresNbehaves as a power law:T∝N−α, i.e. it is linear on a logarithmic scale with a meanαof 0.89. The best result was achieved using 64 nodes with 32 cores, four processes and eight threads. Using this configuration, the computation took about 16 min.

@&#CONCLUSIONS@&#
