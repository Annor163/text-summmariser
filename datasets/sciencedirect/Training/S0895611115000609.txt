@&#MAIN-TITLE@&#
Automatic differentiation of melanoma from dysplastic nevi

@&#HIGHLIGHTS@&#
Melanoma vs. dysplastic classification.The features extracted globally outperformed the local approach.Individual texture features and their combination achieved better results than others.Random forest achieved better results in comparison with SVM and GB.The framework achieved the highest SE of 98% and SP of 70%.

@&#KEYPHRASES@&#
Melanoma,Dysplastic,Dermoscopy imaging,Classification,Machine learning,Texture,Colour,Shape features,

@&#ABSTRACT@&#
Malignant melanoma causes the majority of deaths related to skin cancer. Nevertheless, it is the most treatable one, depending on its early diagnosis. The early prognosis is a challenging task for both clinicians and dermatologist, due to the characteristic similarities of melanoma with other skin lesions such as dysplastic nevi. In the past decades, several computerized lesion analysis algorithms have been proposed by the research community for detection of melanoma. These algorithms mostly focus on differentiating melanoma from benign lesions and few have considered the case of melanoma against dysplastic nevi. In this paper, we consider the most challenging task and propose an automatic framework for differentiation of melanoma from dysplastic nevi. The proposed framework also considers combination and comparison of several texture features beside the well used colour and shape features based on “ABCD” clinical rule in the literature. Focusing on dermoscopy images, we evaluate the performance of the framework using two feature extraction approaches, global and local (bag of words) and three classifiers such as support vector machine, gradient boosting and random forest. Our evaluation revealed the potential of texture features and random forest as an almost independent classifier. Using texture features and random forest for differentiation of melanoma and dysplastic nevi, the framework achieved the highest sensitivity of 98% and specificity of 70%.

@&#INTRODUCTION@&#
Malignant melanoma is a type of skin cancer and although it accounts for less than 2% of all skin cancer cases, it is the deadliest type and causes the vast majority of deaths [1]. According to the latest report, melanoma caused over 20,000 deaths annually in Europe [2]. American Cancer Society also reported the estimated deaths of melanoma in 2013 as 9480 individuals and new cases as 76,690 individuals. Nevertheless melanoma is the most treatable kind of cancer emphasizing the importance of early diagnosis. Currently in the clinical field, the “ABCDE” rule is the clinical routine which is used to detect malignant melanoma at its earliest stage [3]. This routine seeks for characteristics synonymous with malignant melanoma at its early stage and can be listed as: asymmetry, irregular borders, variegated colours, diameters greater than 6mm and evolving stages. The detection is performed by visual inspection and further analysis, using clinical imaging techniques such as dermoscopic imaging. Visual inspection, similarity between different lesions and the necessity to perform patient follow-up over years make the diagnosis task difficult for the dermatologists and more prone to errors notably the repeatability of the detection. Due to this fact and to the importance of early diagnosis of melanoma, the research communities have dedicated their efforts to developing computerized lesion analysis algorithms. These computer-aided systems implement automatic processing to facilitate the task of specialists and focus on different aspects such as segmentation, detection and classification of the lesions. In this work we focus on classification task, particularly discriminating melanoma and dysplastic nevi. It has been reported that from 2 to 8% of the Caucasian population have dysplastic nevi (also known as atypical moles, i.e. unusual benign moles that may resemble melanoma) [4]. Individuals who have dysplastic nevi syndrome or dysplastic nevi with family history of melanoma face a higher risk of developing melanoma. However, only a small number of these dysplastic nevi might develop into melanoma and most dysplastic nevi will never become cancer [5]. Notably discrimination of these two categories is more challenging due to their similarities [6].In the past decade, different approaches have been proposed for detection of melanoma. The developed methods used the common classification framework in the computer vision field: the pipeline of this framework usually consists of the four following steps: (i) segmentation, (ii) feature detection, (iii) feature selection/extraction and (iv) classification. Furthermore, those methods were based on two different screening imaging techniques, either clinical or dermoscopic images. Clinical imaging was the first screening technique and the initial research focused on this modality. This technique acquires images (digital or not) of skin lesions and represents what a clinician sees with naked eyes [29].Later, clinical imaging was replaced by a more suitable technique such as dermoscopy imaging (also known as epiluminescence microscopy). This techniques has been used extensively by dermatologists and researchers, and several frameworks based on this modality has been proposed by the research community to detect malignant melanoma [30–32].Fig. 1, depicts a summary of most of these methods by reporting their results in terms of sensitivity, specificity and dataset size. Sensitivity or recall (SE) refers to the number of correctly identified cancer cases to the total number of cancer cases in the dataset and specificity or true negative rate (SP) refers to the proportion of negatives or correctly identified non-cancer cases to the total number of non-cancer cases in the dataset. The methods are also categorized based on their differentiation scope, melanoma from benign (M vs B), melanoma from benign and dysplastic (M vs B+D) and melanoma versus dysplastic only (M vs D). The summary of the state of the art methods showed that fewer attention has been paid to the discrimination of melanoma and dysplastic in the past, which we believe is more challenging for both specialists and automated algorithms.It is difficult to offer a fair comparison among the methods since their performances are reported in the literature using different datasets. These methods also use multiple machine learning approaches to distinguish melanoma lesions, such as AdaBoost (AdB), artificial neural network (ANN), support vector machines (SVM), k-nearest neighbour (kNN) or random forest (RF). However, AdB, SVM and ANN appear to be the most popular methods. The detected features to feed these classification methods can also be categorized. Table 1summarizes these features with their corresponding references, by using a similar methodology as in [29].Features can be extracted either from a global or local manner. A global feature is extracted by taking the lesion as a whole, while local features are extracted more densely. A grid can be defined over the lesion and each descriptor is extracted for each small portion of the grid. The use of local features increases the size of the feature vector. It also increases its computation cost and the complexity of the feature space. Hence, the bag of features (BoF) approach was commonly used to tackle these drawbacks [14,17,37,13,22,12,38].In this work we focus our research on dermoscopic images and propose an automatic framework for detection of melanoma from dysplastic lesions since their distinction, based on their similarities, is more challenging for both the classifier algorithm, the general practitioner and even the expert dermatologists and fewer studies have addressed the issues (see Fig. 1). We evaluate the performance of well-known texture descriptors besides common colour and shape features to discriminate the two classes. To the best of our knowledge although a combination of colour, shape and texture features were used previously [25,23,21], these features were mostly designed to mimic the characteristics of “ABCD” rule. In this research we consider those features besides well-known texture features such as local binary pattern, grey-level co-occurrence matrix, Gabor filter and histogram of gradients and present a comparison between their individual and combined performances, while they are extracted by both global and local methods. We also consider three classifiers such as Support Vector Machine, Gradient Boosting and Random Forest.The rest of the paper is organized as follows: Section 2 describes the proposed framework. Section 3 illustrates the experiments and proposed validation. The results and discussion are illustrated in Section 4 and finally, the conclusions are presented in Section 5.Different sections of our pipelines as illustrated in Fig. 2are explained in the following.Previous to classification, we perform lesion segmentation on the dermoscopic images to focus our processing only in the lesion area. It was observed through empirical validation, that the CIE XYZ was the most suitable colour space in which segmentation can be performed. More precisely, all lesions are segmented by analysing the probability distribution of Z component (see Fig. 3b). In the probability distribution, a Gaussian mixture with two components can be observed. The first Gaussian corresponds to the pixel intensities of the lesion whereas the second Gaussian corresponds to the pixel intensities belonging to the skin. Thus, finding the valley between these two Gaussian bells allows us to separate the lesion from the skin as depicted in Fig. 3a. The valley corresponds to the global minimum between the two Gaussian distributions. This global minimum is detected by finding, first the peaks (red crosses in Fig. 3b) of the smoothed probability density distribution and then finding the minimum between them (green cross in Fig. 3). The original distribution is smoothed using cubic interpolation.Reviewing the literature, one could observe that some features have been used more widely than others such as colour and shape characteristics. This is due to the fact that computerized systems are developed to mimic dermatologists’ assessment, by using for instance the discriminative characteristics defined in the clinical “ABCD” rule. Thus, we also incorporate some common shape and colour descriptors, along with texture features. The shape (S) and first colour (C1) features and grey level co-occurrence matrix (T2) are chosen since they have been used widely in the past. However the rest of the features, such as opponent colour space (C2), Local binary pattern (T1), Gabor filter (T3), histogram of oriented gradients (T4) and SIFT(T5) are chosen since they barely have been used for detection of melanoma and they are well-known colour and texture descriptors in pattern recognition. These features are summarized in Table 2and are explained in the following.−Shape features were created with reference to [39]. This descriptor measures thinness ratio, border asymmetry, distance variance of border points to the centre and statistics (minimum, maximum, average and variance) of gradient operator along the lesion border.Colour variance and colour histogram is a well-known feature descriptor which has been used widely in the past for detection of melanoma features. This descriptor contains, the mean and variance of nine colour channels (R, G, BH, S, VL, A, B) and histogram of R, G and B channels. Each histogram for each channel is constructed with 42-bins, leading to a final descriptor size of (9×2)+(42×3)=144.Opponent colour space angle and hue histogram were first proposed by [40] as local colour features. These descriptors were chosen due to their robustness to photometric variations (shadow, shading, specularities and changes of the light source) and geometrical variations (viewpoint, zoom and object orientation). These rotation invariant and robust descriptors are derived from RGB channels using the following equations:(1)hue=arctan3(R−G)R+G−2BangdimO=arctan3(Rdim′−Gdim′)Rdim′+Gdim′−2Bdim′where dim denotes the spatial coordinates of (x, y) andRdim′,Gdim′,Bdim′denote the first order derivatives of R, G, B with respect to the coordinates.The colour feature descriptor is created by taking histogram of opponent angleangdimOand hue channels. These feature descriptors are created with 42 bins as well.Completed local binary pattern (CLBP) is a discriminative rotation invariant feature descriptor proposed by Guo et al. [41]. CLBP is a completed version of local binary pattern (LBP) descriptor, proposed by [42]. In both descriptors, a central pixel (gc) in a defined neighbourhood by R radius is compared to its neighbourhood pixels (gp, with distance of R from the central pixel) and their differences are encoded in terms of binary patterns. The binary patterns are calculated for each pixel in the given image and their histogram, defines the final descriptor. The CLBP is a complete version of LBP since, beside considering the sign of the differences as LBP, it also considers the magnitude differences, from the central point to its neighbourhood, and the grey level of the central points. Fig. 4represents this process. In this figure, the sign, magnitude and central grey level binary pattern are represented by CLBPS, CLBPMand CLBPCwhich are created by encoding the local distance components and the central grey levels to binary patterns. In the proposed framework, we consider the radius of 24 pixels and we calculate the rotation invariant, uniform and normalized CLBP features. This radius was chosen after testing the algorithm with different neighbourhoods such as {8, 16, 24}.Grey-level co-occurrence matrix (GLCM) is one of earliest texture descriptor methods, proposed by Haralick et al. [43]. This approach has been used widely for texture analysis applications including melanoma detection. In this approach, texture features are extracted based on statistics measurement of co-occurrence probabilities. The co-occurrence distribution represents the occurrence probabilities of all pairwise combinations of the gray levels in the defined window [44]. In other words, it counts how often a pixel with gray intensity of i occurs adjacent to a pixel with gray intensity of j. The spatial distance and orientation of interests, between the pixels are defined by distance D and angle of θ. The co-occurrence probability between grey level i and j is defined using expression (2), where Pijrepresents the conditional probability of occurrences of grey value i adjacent to grey value of j, given the distance and orientation of D and θ, respectively and G is the quantized number of grey levels.(2)Cij=Pij∑i,j=1GPijIn this framework, we quantize the images to 32 grey levels and calculate co-occurrence probabilities given the distance (D) of 9 pixels and four different orientations of {θ=0°, 45°, 90°, 135°}. For each orientation 22 texture features as a combination of features proposed by [43–45] are calculated. The final texture descriptor is an average of stated four measurements. This approach allows us to have rotation invariant descriptor. The proposed distance and quantized value were chosen after testing variety of distances {1, 3, 7, 9} and quantized grey-levels {16, 32, 64}.Gabor filter is a linear filter which extracts edges and texture information from the image and was found to perform similar to human visual perception. Gabor filter is defined as a modulation of a Gaussian kernel with a sinusoidal wave. The Gabor expression is shown in Eq. (3). This function is basically a Gaussian with standard deviations of σxand σythat vary along x and y axes and it is modulated by a complex sinusoidal with a wavelength of λ. In this equation, θ represents the orientation of the Gabor filter and ψ is the phase offset and s is the scale factor.(3)g(x,y;θ,ψ,σx,σy)=exp−12x′2σx2+y′2σy2cos2πx′λ+ψx′=s(xcosθ+ysinθ)y′=s(−xsinθ+ycosθ)In this work as described by [46], the images are convolved with a set of Gabor filters characterized by different orientations and scales. The features are created by considering the mean and variance of the resulting filtering. We used 6 different orientations ({π/6, π/3, π/2, 2π/3, 5π/6, π}) along 4 scales, downsizing by the factor of two between each scale.Histogram of oriented gradient was proposed by [47]. In simple terms, HoG counts occurrence of gradient orientations in a localized patches of an image. This descriptor is invariant to geometric and photometric transformations. HoG calculates the gradient values of the image by applying derivative filters ([−1, 0, 1], [−1, 0, 1]T) or Sobel mask to the image and measuring the magnitude and orientations of the gradients. It then creates the weighted histograms of gradient orientations in patches of the image. The orientations are either weighted by magnitude of gradients or a function of gradient magnitudes. In order to account for changes in illumination and contrast, the cells are grouped into larger spatially connected blocks and the final descriptor is a vector of the normalized cell histograms from all of the blocks.SIFT or Scale invariant feature transform, proposed by Lowe et al.[48], is a dense rotation, translation and scale invariant feature descriptor, which is used successfully by computer vision society. This descriptor, first identifies the key locations by convolving the original image (I(x, y)) with Gaussian kernels (G(x, y, kσ)) at different scales (kσ) and defining the maxima and minima of difference of successive Gaussian blurred images (L(x, y, kσ)).(4)D(x,y,σ)=L(x,x,kiσ)−L(x,x,kjσ)L(x,y,kiσ)=G(x,y,kiσ)*I(x,y)In the next step, the identified key-points are filtered, to reject the ones with low contrast or the ones localized along the edges. The remaining key-points are then assigned with one or more orientations, based on local image gradient directions. In this step the magnitude and orientation gradient for each pixel in a neighbouring region of the key-point in the Gaussian blurred image L is calculated, and a 36-bin histogram is defined by adding weighted measures of the orientations. Each orientation is weighted by its magnitude and a Gaussian-weighted window. The key-point is then assigned with the highest orientation peak of the histogram and the local peaks within its 80%. The final key-point descriptor is then created by taking a 16×16 neighbourhood around the key-point. This neighbourhood is divided to 16 sub blocks of 4×4 and for each sub block an 8-bin orientation histogram is created. Concatenation of these histograms (i.e. 16 regions) leads to a 128-dimensional feature vector as the final descriptor.As already stated in the introduction, we will use both global and local features. The global features are extracted from a bounding box defined around the previously segmented lesions. In the case of the local features, we are using the bag of features (BoF) approach which is illustrated in Fig. 5.Following the segmentation, a patch detection step is carried out by identifying informative regions of the lesions. These patches can either be sampled densely or sparsely [17]. The dense sampling extracts more information regarding the lesion appearance. However, it might retain redundant features. In the contrary sparse sampling is based on detecting salient key points from the most informative regions of the lesions [17]. Our proposed BoF approach is based on the first strategy. Thus first a grid is centred on each segmented lesion and only the patches in which the proportion of lesion is greater than a third of the patch size are selected. Defining N as the number of selected patches for each lesion and d as the number of feature dimensions, then each lesion will be characterized by a N×d feature matrix (see Fig. 5, “feature extraction”). The next step in bag of features consists of building the dictionary of “visual-words”. All the feature matrices of the training set are concatenated together and k-means clustering method (see Fig. 5, “clustering”) is used to define the “visual-words”. K-means is an iterative algorithm which finds k centroids by alternating assignment and update steps. The assignment steps is based on L2 norm (Euclidean) distance. Different initialization methods can be used in order to assign the initial k clusters [49], here the initial k clusters are selected based on greedy k-means++ method [50]. Depending on the framework and application, different choices of the number of “visual-words” (number of k clusters) can be made. In our framework, this number is found through exhaustive search, varying from 15 to 700 clusters. The k which leads to the highest performance of classifier is chosen as the final number of clusters. Finally, the probability distribution (e.g., histogram) of the “visual-words” of each feature matrix is computed (feature quantization) and is used to feed the classifier to be trained. In the prediction stage, the histogram of the feature matrix corresponding to the new lesion is computed using the previously learned dictionary and, finally, it is classified by the previously trained classifier.Prior to classification, all the feature sets are linearly rescaled (normalized) between −1 and 1. Feature normalization is essential in order to improve the discrimination capabilities of the similarity measures [51]. Posterior to feature normalization, solely for global features, in order to remove correlated features and to find the optimal dimension, we apply a linear dimension reduction approach, such as principal component analysis (PCA) [52]. The optimum dimension of PCA normally is found by considering the minimum dimension (eigenvectors), accountable for 95% of sum of eigenvalues. However, there is no definitive rule on how to choose the number of eigenvectors. Thus for our framework, the optimum number of dimensions is found through exhaustive search over the feature dimensions. All the possible number of dimensions (from 1 to original size) are tried and the optimal number of dimensions is found when the best classification accuracy is achieved.In this paper, we provide a comparison between the following well-known classifiers: SVM, GB and RF. A brief description of each classification method is given below.−Support vector machines: SVM [53] is a well known machine learning approach which aims to separate two classes by finding the best hyperplane which maximizes the margin between these two classes. Maximizing the margin is equivalent to minimizing the norm of the normal vector of the hyperplane with the constraint that no points should lie in the margin and can be solved as an optimization problem.Gradient boosting: GB is a generalization form of AdB, which is able to use real-value weak learners and minimizes different loss functions [54,55]. GB builds the ensemble in a greedy manner and iteratively selects weak learners and adjust their weights so that they minimize the loss function. Common choice for the weak learners are decision stumps or regression trees while the loss function is generally an exponential loss or a logarithmic loss [56].Random forest: RF is an ensemble of decision trees and was introduced in [57]. In the classification stage, multiple decision trees are trained with a different bootstrap sample of the original data. Each bootstrap sample of dimension D is used for training one decision tree and at each node of the decision tree, the best split among the randomly (d<<D) selected subset of descriptors is chosen. Each tree is grown to its maximum length without any pruning. In the prediction stage, a sample is propagated and voted by each tree and is usually labelled by majority voting.Except for the shape, which was only used as global feature and for SIFT which was only used as local-feature, for the rest, global-features (from the whole lesion) and local-features (bag of feature) were extracted and we evaluated their individual and combination attributes using the three proposed classifiers. Comparison and the applied experiments are explained in following sections.The experiments have been carried out using a fraction of the dermoscopy dataset of the Vienna General Hospital. This is a large-scale dataset with over 5000 lesions including 4277 benign lesions, 1002 dysplastic lesions and 101 malignant melanoma images. In this dataset the dysplastic and melanoma lesions were surgically removed and their ground truth were provided by histological diagnosis [20]. This dataset was used as well in [58,39]. Using the proposed segmentation approach we were able to correctly segment and use around 95.43% of the images consisting of 90 melanoma, 950 dysplastic and 4090 benign lesions. The excluded 4.57% of the images were not reliably segmented due to existence of artefacts such as hair and nonuniform illumination.

@&#CONCLUSIONS@&#
