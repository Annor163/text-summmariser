@&#MAIN-TITLE@&#
Identifying visual attributes for object recognition from text and taxonomy

@&#HIGHLIGHTS@&#
A system to automatically mine attributes for object recognition is proposed.A taxonomy defined over classes is used to mine attributes.We integrate distributional similarity to improve the quality of mined attributes.The mined attributes are used in supervised and zero-shot learning settings.We report improved recognition accuracies compared to state of the art methods.

@&#KEYPHRASES@&#
Object recognition,Zero-shot learning,Attribute mining,Attribute-based classification,

@&#ABSTRACT@&#
Attributes of objects such as “square”, “metallic”, and “red” allow a way for humans to explain or discriminate object categories. These attributes also provide a useful intermediate representation for object recognition, including support for zero-shot learning from textual descriptions of object appearance. However, manual selection of relevant attributes among thousands of potential candidates is labor intensive. Hence, there is increasing interest in mining attributes for object recognition. In this paper, we introduce two novel techniques for nominating attributes and a method for assessing the suitability of candidate attributes for object recognition. The first technique for attribute nomination estimates attribute qualities based on their ability to discriminate objects at multiple levels of the taxonomy. The second technique leverages the linguistic concept of distributional similarity to further refine the estimated qualities. Attribute nomination is followed by our attribute assessment procedure, which assesses the quality of the candidate attributes based on their performance in object recognition. Our evaluations demonstrate that both taxonomy and distributional similarity serve as useful sources of information for attribute nomination, and our methods can effectively exploit them. We use the mined attributes in supervised and zero-shot learning settings to show the utility of the selected attributes in object recognition. Our experimental results show that in the supervised case we can improve on a state of the art classifier while in the zero-shot scenario we make accurate predictions outperforming previous automated techniques.

@&#INTRODUCTION@&#
While much research in object recognition has focused on distinguishing categories, recent work has begun to focus on attributes that generalize across many categories [1–15]. Attributes such as “pointy” and “legged” are semantically meaningful, interpretable by humans, and serve as an intermediate layer between the top-level object categories and the low-level image features. Moreover, attributes are generalizable and allow a way to create compact representations for object categories. This enables a number of useful new capabilities: zero-shot learning where unseen categories are recognized [3,4,7], generation of textual descriptions and part localization [2,5,10], prediction of color or texture types [1], and improving performance of fine-grained recognition tasks (i.e. butterfly and bird species or face recognition) [3,6,12,13,15] where categories are closely related.However, using attributes for object recognition requires answering a number of challenging technical questions – most crucially, specifying the set of attributes and the category–attribute associations. Most prior work uses a predefined list of attributes specified either by domain experts [3,4,12] or researchers [2,6,11,13], but such lists may be time-consuming to generate for a new task, and the attributes in the generated list may not correspond to the optimal set of attributes for the task at hand. A natural alternative is to identify attributes automatically, for example, from textual descriptions of categories. However, this is challenging because the number of potential attributes is large, and evaluating the quality of each potential attribute is expensive.In this paper, we present a system that automatically discovers a list of attributes for object recognition. As we approach the problem from a computer vision perspective, we are mainly concerned with “visual” attributes that directly relate to the appearance of objects, such as “red” or “metallic”. However, an attribute that relates with visual qualities in general may not be selected by our system if it does not help the recognition task, e.g. “metallic” is not a useful attribute if the recognition task is to classify car brands. In contrast, the word “fragrant” does not refer to a visual quality, however due to its indirect correlation to visual features (e.g. its link to flowers), it may be selected as a useful attribute for object recognition by the proposed method. In the remainder of this paper we use the term “visual attribute” to refer to any word that may help object recognition from images.Our main contributions are as follows. Firstly, we introduce two methods to select words in a text corpus that are likely to refer to visual attributes. One of the methods we propose uses a taxonomy defined over categories and promotes words whose occurrence in textual descriptions of categories is coherent with the given taxonomy. The other method builds upon the previous one and integrates distributional similarity of words into the attribute selection process. Secondly, we propose a way to assess the quality of a candidate word as an attribute for object recognition from images. In the experiments, we provide evaluations of the proposed attribute selection strategies for effectively identifying attributes, in the plant and animal domains, and present the efficacy of the proposed techniques at selecting visual attributes. Furthermore, we analyze the mined attributes semantically and then use them for plant and animal identification tasks.We use three input sources in the proposed methods: textual descriptions and image samples of categories and a taxonomic organization of the object domain. In the plant identification task, the goal is to identify a plant species from the visual appearance of its foliage. We use the plant foliage image dataset provided in ImageCLEF’201211http://www.imageclef.org/2012plant identification task [16]. This dataset contains 9356 foliage images of 122 species of which 6689 are for training and 2667 are for testing. For this dataset, we mine a set of text documents containing encyclopedic information on categories from the web, using Wikipedia,22http://en.wikipedia.org/wiki/Main_Page.Encyclopedia of Life33http://eol.org/.and the Uconn Plant Database.44http://www.hort.uconn.edu/plants/.For the animal identification task, we use the animals-with-attributes (AwA) database provided by [4] to evaluate our approach. This is a popular database to test attribute-based recognition and zero-shot learning approaches. This dataset contains 30,475 images of 50 animals where 24,295 images of the selected 40 animals are used for training and 6180 images of the remaining 10 classes are reserved for testing in the zero-shot learning setting. Similar to the plant identification, we mine textual descriptions for each of the 50 animals in that set using Wikipedia and A-Z Animals.55http://a-z-animals.com/.In both of the recognition tasks, the challenge is to find the words referring to visual attributes in the mined documents. We test the effectiveness of the automatically selected attributes for recognition in both zero-shot learning and in traditional supervised learning settings.66All the collected textual descriptions are available online at: https://drive.google.com/file/d/0Bx-64dmWqUHIT09JRGZDOGxPNkk/view?usp=sharing.Our approach consists of two main components. After reviewing related work in 2, we describe a method for assessing the visual quality of a proposed attribute for object recognition in Section 3. The assessment procedure involves training a binary attribute classifier, where the quality of a candidate word depends on the success of the attribute classifier. Classification-based attribute selection is effective but computationally expensive; consequently, in Section 4, we propose a set of techniques for nominating candidate attributes that are likely to be of high visual quality: we leverage multi-level discriminability across a category taxonomy, and distributional similarity of the words in the text corpus. Our nomination process takes feedback from the visual quality assessment of candidate attributes, making increasingly accurate predictions as it learns more about the types of words that are found to be of high quality. Once the set of attributes is determined, in Section 5, we illustrate how the selected attributes can be used for object recognition in two different settings. Finally, in the experiments section (Section 6) we present experiments to compare attribute selection strategies. Then, the selected attributes are used for classification of categories in two challenging recognition tasks.

@&#CONCLUSIONS@&#
