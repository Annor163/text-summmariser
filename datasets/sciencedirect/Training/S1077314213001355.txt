@&#MAIN-TITLE@&#
Invariant representation of facial expressions for blended expression recognition on unknown subjects

@&#HIGHLIGHTS@&#
The organization of facial expressions is person-independent.One expression can be defined by its relative position to 8 other expressions.We synthesized the 8 expressions from plausible distortions.The representation is robust to the type of data (shape and/or texture information).As expression is accurately described in a 4 dimensional expression space.

@&#KEYPHRASES@&#
Facial expression,Invariant representation,Delaunay tessellation,Manifold of expressions,Piecewise affine warping,Transfer learning,

@&#ABSTRACT@&#
Facial expressions analysis plays an important part in emotion detection. However, having an automatic and non-intrusive system to detect blended facial expression is still a challenging problem, especially when the subject is unknown to the system. Here, we propose a method that adapts to the morphology of the subject and that is based on a new invariant representation of facial expressions. In our system, one expression is defined by its relative position to 8 other expressions. As the mode of representation is relative, we show that the resulting expression space is person-independent. The 8 expressions are synthesized for each unknown subject from plausible distortions. Recognition tasks are performed in this space with a basic algorithm. The experiments have been performed on 22 different blended expressions and on either known or unknown subjects. The recognition results on known subjects demonstrate that the representation is robust to the type of data (shape and/or texture information) and to the dimensionality of the expression space. The recognition results on 22 expressions of unknown subjects show that a dimensionality of the expression space of 4 is enough to outperform traditional methods based on active appearance models and accurately describe an expression.

@&#INTRODUCTION@&#
A major goal of many new government programs is to find better ways that will allow the elderly to stay in their own homes longer, rather than moving to a care facility. One main domain for aging in place is to increase security and raise an alarm when some specific changes in behavior occur. Such systems have to be non-intrusive, easy to use and agreeable. Among the ways of finding a change in behavior and feelings, facial expressions analysis plays an important part, for they display human emotions and moods [1].This paper explores a system based on a camera which can be included into a Set-top box. Such a system requires:•accuracy to detect neighboring blended expressions (as displayed in Fig. 1);completeness to detect new unknown expressions;robustness to cope with the diversity of facial morphologies;flexibility to adapt itself to a subject without a previous learning phase.Many emotion recognition systems based on facial expression analysis have been proposed in the last decade [2–4]. Efforts have been made to compare these methods. First, popular facial expression databases are available (e.g. the Cohn–Kanade database [5] and the HUMAINE database [6]). More recently, challenges are organized to compare these systems in the same conditions. FERA 2011 [7] aimed at detecting FACS Action Units and discrete emotions. AVEC 2011 [8] and AVEC 2012 [9] aimed at detecting the variations of four affect dimensions (valence, power, expectancy and arousal). The drawback of such challenges is that the results of the challenges are computed on the emotion prediction, so that the performances of the global processes are compared rather than each step of the systems. This is especially true with multimodal systems.Most emotion recognition systems based on facial expression analysis concatenate two steps: facial feature extraction and classification [2]. They are often influenced by the work of Ekman, so that they either concentrate on the classification of a small amount of facial expressions (most of the time the 6 basic emotions universally associated with distinct facial expressions [1]) or on Action Units (AUs) [10]. Some attempts have been made to cope with other expressions such as pain [11,12]. The systems focus then on the classification of whether or not the expression is shown, let alone the other expressions. These systems do not deal with expressions that are not included in any training database. Yet, real life expressions are rarely prototypic ones [1]. That is why, more recently, systems focus on a continuous representation of emotions. Ishii et al. [13] proposed a method to generate a subject-specific emotion map compliant with Russell’s circumplex model [14]. The mapping is then previously learnt for each subject by a supervised learning algorithm. The AVEC2012 challenge [9] proposed a comparison of methods for automatic emotion analysis, where the emotions were labeled by FEELTRACE [15] in four dimensions [16]. The best scores were obtained by [17] with a system that predicts the emotion by directly applying a radial basis function process on the facial and audio features. The main limit of these systems is that they directly transform facial features into emotion prediction. Unfortunately, the facial features carry both information of expression and identity of the subjects, so that such systems need a huge amount of expressive faces for their learning phase and do not perform good results in generalization to unknown subjects. In FERA 2011 challenge [7], the person-independent discrete emotion recognition did not exceed 75.2%, although the person-specific performance was 100%. That is why we propose an intermediate step between facial feature representation and classification. This step is a continuous invariant expression representation of the facial expressions.The choice of representation is known to influence the recognition performance. In this article, we focus on a facial expression representation that deals with expressions not included in any training database and with morphological differences between unknown subjects and subjects enrolled in the training database. In our method, we assume that the neutral face of the subject is known for both the known and the unknown subjects. As we need to deal with subtle differences between expressions, we use appearance features based on Active-Appearance Models (AAMs) [18]. AAMs are known to provide important spatial information of key facial landmarks [19]. Yet, good AAM tracking on unknown subjects is still a challenging problem. In this paper, the faces have been annotated by hand to avoid problems due to AAM tracking.The main contribution of this article is the specification of a new invariant representation of emotional facial expressions. The originality of the approach is that we did not focus on the characteristics of one expression but on the organization of this expression with respect to the others. This organization has been extracted from data and we have attested that this organization is person-independent. We used this invariant organization to transform the appearance space which is person-specific into the expression space which is person-independent and to give a direction–intensity signature to each expression. The recognition tasks are then processed in the expression space with a basic classifier (see overall process on Fig. 2). Another important contribution of this work is that we create a person-specific appearance space adapted to the morphology of the subject without a previous learning phase of the subject. The particularity of the approach is that we synthesized the expressions of the unknown subjects in order to create their own complete appearance space. The synthesized expressions are compliant with the organization of the expressions previously found, which signifies that the appearance space contains all the expressions defined in the expression space even if the new subject has not displayed them.The remainder of this paper is organized as follows. In the next section, the relevant previous studies on facial expression representation are examined. Section 3 describes the limits of traditional AAM based expression analysis. In Section 4, we describe the invariant representation used to perform the transformation of the appearance space into the expression space and the individual steps of the transformation. Section 5 generalizes the method to unknown subjects. Section 6 demonstrates the accuracy and the robustness of the proposed system in expression recognition. Section 7 concludes the paper.

@&#CONCLUSIONS@&#
