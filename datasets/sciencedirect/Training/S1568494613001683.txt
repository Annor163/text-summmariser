@&#MAIN-TITLE@&#
A team-oriented approach to particle swarms

@&#HIGHLIGHTS@&#
Decoupling of exploration and exploitation of the search process is proposed to solve the stagnation problem of the PSO.A “team oriented approach” is introduced; each team has dedicated purpose of either “exploration” or “exploitation”.A novel method is proposed for the exploitation purpose; modified PSO model is used for the exploration purpose.The proposed algorithm provides higher quality solution with faster convergence and increased robustness.Such results are obtained with no significant increase in complexity.

@&#KEYPHRASES@&#
Large Scale Global Optimization,Memetic algorithms,Particle Swarm Optimization,Swarm intelligence,

@&#ABSTRACT@&#
The Particle Swarm Optimization (PSO) is a simple, yet very effective, population-based search algorithm. However, degradation of the population diversity in the late stages of the search, or stagnation, is the PSO's major drawback. Most of the related recent research efforts are concentrated on alleviating this drawback. The direct solution to this problem is to introduce modifications which increase exploration; however it is difficult to maintain the balance of exploration and exploitation of the search process with this approach. In this paper we propose the decoupling of exploration and exploitation using a team-oriented search. In the proposed algorithm, the swarm is divided into two independent teams or sub swarms; each dedicated to a particular aspect of search. A simple but effective local search method is proposed for exploitation and an improvised PSO structure is used for exploration. The validation is conducted using a wide variety of benchmark functions which include shifted and rotated versions of popular test functions along with recently proposed composite functions and up to 1000 dimensions. The results show that the proposed algorithm provides higher quality solution with faster convergence and increased robustness compared to most of the recently modified or hybrid algorithms based on PSO. In terms of algorithm complexity, TOSO is slightly more complex than PSO but much less complex than CLPSO. For very high dimensions (D>400), however, TOSO is the least complex compared to both PSO and CLPSO.

@&#INTRODUCTION@&#
Particle Swarm Optimization (PSO) is a simple and easy to implement algorithm. Despite its simplicity, in many cases it performs better than genetic and other evolutionary algorithms [1,2]. In part, that is why PSO is arguably one of the most widely researched areas of computational intelligence. The elegant and central idea of the PSO is based on a massless particle, trying to make its way to optima, driven by its own past experience and that of its social peers, which loosely mimics the foraging behavior of the bird flock or fish school [3]. The PSO algorithm has been successfully applied to many real-world problems [4]. However, the swarm in PSO quickly converges to a single attractor point [5].Also, in the later stages of the search process, the swarm lacks diversity and, hence, tends to loose global search ability. The problem of stagnation in PSO can be ascribed to difficulty in exploring new areas when all particles are clustered very close to each other. Most of the recent research in PSO has been concentrated on attempting to overcome the stagnation problem. In this regard, and within the framework of PSO, we can identify at least two approaches addressing this problem. The first is the control of the algorithm parameters and the second is the improvement of the learning strategy.At a given iteration, the basic PSO update rules of the dth dimension for the ith particle in a D-dimensional search space can be given by [3,6]:(1)vi,d=(w×vi,d)+c1×r1×(pbesti,d−xi,d)+c2×r2×(gbestd−xid)(2)xi,d=xi,d+vi,dwhere x and v are the position and the velocity, respectively; w, c1 and c2 are the inertia weight and the acceleration constants, respectively. The variable pbestdis the personal best position attained by the dth dimension of the particle and gbestdis the dth dimension of the global best position attained by the swarm.As revealed by (1) and (2), the PSO search process is influenced by the acceleration constants and the inertia weight. One possibility to maintain a balance between exploration and exploitation in the PSO approach is to properly select the acceleration coefficients. Kennedy and Eberhart [3] suggested fixing the constants at a value of 2, so that both multiplication factors could be averaged to unity to ensure a balance in the search process. Another approach is to use the inertia weight, w in the velocity update equation [6]. A large value of w would enable more exploration (and vice versa). It was soon identified that by controlling these parameters effectively during the search process, more robustness can be achieved. The literature includes various alternatives such as: time varying inertia weight [6–8], time varying acceleration coefficients [9], adaptive inertia weight using fuzzy logic [10], and more recently adaptive control of acceleration coefficients using fuzzy logic [11,12]. While these modifications provided improvements over the conventional PSO algorithm, still they are not much effective in solving the PSO's main problem i.e., lack of exploration capabilities at later search stages. Recent analysis by Gao and Xu [13] confirmed that the inertia term in the velocity update is effective, for exploration, only in the initial stage of the search process; after which the effect gradually fades. In addition, exploration is based on the difference between a particle's position and historic best experiences, which too decreases in later stages; causing the whole swarm to converge to local minima.The earlier version of PSO used the global star (or gbest model) approach for social influencing; i.e., the best performing particle can influence the whole swarm immediately [3]. However, it was realized that limiting the range of the social influence to only nearby particles, similar to what is now known as the local version of PSO or lbest model, can be more advantageous [14–16]; as it offers more diversity and hence more exploration. Therefore, different neighborhood topologies have been suggested [14,15], each with its merits and demerits. Janson and Middendorf [17] suggested a tree like branching hierarchy for the population. The particle can move up or down in the hierarchy based on its pbest performance. Generally, for complex problems, the local version of the algorithm performs better than the gbest model. Nevertheless, it too suffers from stagnation issues in the later stages of the search process; once the particles come close to each other. A Comprehensive Learning PSO (CLPSO) was proposed in [18]. In this comprehensive learning technique, each dimension of the particle learns from the historic best positions of different particles, which results in an increased diversity in the search space and helps in improving the performance for multimodal functions. However, a larger search space at the end of the search process reduces exploitation, which translates into slower convergence for unimodal functions. In [19] a single learning exemplar was used. It is derived from pbest and gbest (or lbest) by applying orthogonal learning (OL). This approach gives a reasonable performance enhancement at the expense of increased complexity.A further improvement attempt is hybridization; including algorithmic components from other evolutionary algorithms, for more enhanced search. Angeline [20] used selection in favor of better performing half population, which can be viewed as exploitation enhancement. Given that the swarm in PSO always converges to a single attractor point [5], the mutation operator is a good choice to escape from local minima. In terms of the search process the mutation operation can be considered as an exploration enhancement. Over the years, different hybrid algorithms using the mutation operator have been suggested [21–28]. These approaches differ mainly on three aspects: the way the particle is selected, the way mutation is applied, and the mutation rate. The mutation operator was applied to the best particle (or any other particle), using random numbers based on various probability density functions (pdf's) including but not limited to Uniform, Gaussian, Cauchy or custom-defined distributions [21–26]. In many cases, during mutation, the position is allowed to change only within a fixed sub-space of the search area. The size of this subspace has a direct impact on the exploration capabilities of the algorithm. Consequently, Ling et al. [28] proposed a mutation operator having variable mutation space using wavelet theory, which performs better than fixed space mutation as a smaller mutation space, in later stages, gives a better exploitation capability.Petalas et al. [29] used a directed random walk to improve exploitation for the standard PSO. Various schemas are suggested for the use of both gbest and pbest to improve the local search. Zhu et al. [30] proposed an algorithm based on CLPSO [18], for exploration and a modified Intelligent Single Particle Optimizer (ISPO) [31] for exploitation. In each iteration, particles are selected depending on their weighted fitness to perform the local search. Another memetic algorithm based on PSO was proposed in [32]. This algorithm uses a multi-parent crossover operation from GA on selected best particles, as a local search. The PSO is allowed to run for predefined epochs and then a local search is initiated. Juang [33] used a hybrid version of GA and PSO. In the proposed algorithm, the two halves of the next generation's swarm are derived using the Elites (better performing half of the current population). The first half of the next generation is enhanced Elites by the application of PSO and the other half is obtained by standard GA operation on the Elites.Since most of the proposed modifications had something to offer toward the improvement of the original PSO, de Oca, et al. [34] investigated a composite PSO algorithm which combines various improvements suggested over time into a single algorithm. The proposed algorithm includes: decreasing inertia weight [6], time varying population topology [17] and velocity update based on Fully Informed PSO (FIPSO) [35], in some cases the resulting algorithm performed better than PSO variants from which its component were taken. However, it ended up being much more complex than the original PSO.The large number of alternatives and modifications to PSO stems from the fact that it is difficult to maintain the balance between exploration and exploitation in the search process. In addition, while stagnation, the major problem of PSO can be solved by introducing algorithmic components which favor exploration over exploitation, it is still too difficult to achieve a tradeoff between solution quality and convergence speed. In their seminal paper, Kennady and Eberhart [3], introduced several ideas to update the positions of particles. One of such ideas is to have particles with different roles, namely “Explorers” and “Settlers”. The explorers would surf a larger area within the problem domain, while the settlers would search around the best position found so far. This ensures diversity in the population as well as robustness in the algorithm. However, the approach did not show improvement over the traditional PSO update equations and was not used further. Nevertheless, if proper algorithmic components are used for each search aspect, it presents an interesting possibility to deal with the dilemma of exploration and exploitation.In this paper, we propose a team-oriented search approach which decouples the exploitation and exploration by dividing the swarm into two teams and equipping them with proper algorithmic components necessary for each search task; a further step from Kennady and Eberhart's initial idea of particles with different characteristics. A new local search method is proposed for the exploitation. In this exploitation task, the search radius around the best position is controlled by the quality of the particle's performance. For exploration, a modified PSO structure is used. To avoid “Two Step Forward, One Step Backward” phenomenon [36,37] a single learning exemplar is used. The learning exemplar is derived from the local version of PSO, or lbest model. To further increase diversity, a mutation operator is used on the exploration particles.The paper is organized as follows: In Section 2, the proposed algorithm is explained in detail. The evaluation criteria and the testing set up are covered in Section 3. In Section 4 extensive evaluation results for various benchmark functions are presented along with comparative studies with recently published results. Section 5 presents the conclusions.The objective of this work is to solve the stagnation problem of the PSO, by maintaining a balance between exploration and exploitation in the search process while preserving simplicity. For this purpose, the modified PSO is integrated with a simple local search method. In the proposed algorithm, the population is divided in two groups (or teams); each dedicated to perform the specified task of either exploration or exploitation. The only link between the two teams is the information about the best position found so far. Due to this distinct nature of the algorithm we named it Team Oriented Swarm Optimization, acronymed as ‘TOSO’, which so happened to be a “medicinal sake traditionally drunk during New Year celebrations in Japan, to flush away the previous year's maladies and to aspire to lead a long life” [38].The particles performing exploration are referred to as explorers, and their group as the exploration team. These particles are governed by a modified version of the Social Only PSO [39], in which the lbest model is used instead of gbest model. There are two reasons for this selection. The first is that using a single learning exemplar solves the two steps forward and one step backwards dilemma [36,37]. The second reason is that, to overcome the stagnation problem, it is necessary to maintain exploration efforts even in the late stages of the search process. Diversity in the team is the key to better exploration of the search area, which makes the lbest model an appropriate choice for the task. The use of lbest model ensures that information propagation is slow among the explorers and, hence, provides more time for exploration. The selection of neighborhood is also an important factor. Within the neighborhood, the smaller the connectivity between the particles, the slower will be the information progression [15]. Delaying the information propagation about the best position gives more time for the explorers to pursue their individual search. Therefore, the neighborhood for the explorers is defined using the ring topology. This topology for the neighborhood is particularly suitable because each explorer is connected to only two neighbors; hence, information progression is much slower compared to other highly connected topologies.In the proposed approach, the velocity update is omitted and the new position of the explorer is calculated directly from the previous position. For any given iteration, the new position of the dth dimension of the ith explorer is evaluated using:(3)xi,d=α×rand1×(lbesti,d−xi,d)where α is the exploration factor (to be discussed later in this section) and rand1 is a uniform random number between [0,1]. Each dimension is multiplied by a different random number to increase diversity. From (3), it can be seen that the explorer jumps from its current to next position using a jump quantum controlled by the exploration factor, α, along with the difference between the particle's current position and lbest.As inertia and cognitive learning terms of the original PSO are omitted and once the particles are clustered around each other, in the worst case scenario, explorers may be trapped in local minima and the whole exploration team may stagnate. To overcome this, a GA-like Random mutation is used. The objective of the mutation operation when applied to the exploration team is to introduce as much diversity in the team as possible throughout the search process. Empirical studies [26] indicate that the Random mutation (in which particle is reinitialized within the search space) with constant mutation rate is expected to perform better for complex multimodal problems compared to other mutation approaches. This makes it a suitable choice for the requirements of the exploration team.According to a uniform mutation probability pm, an explorer can be reinitialized in the search space, or reborn, and starts its search again from that point. However, its historical experience is preserved unless the new position is better than pbest. Re-initialization (or rebirth) is achieved, with a probability pmusing:(4)xi,d=Rmin,d+rand2×(Rmax,d−Rmin,d)where rand2 is a uniform random number between [0,1] (each dimension is multiplied by a different random number) and Rmax,dand Rmin,dare the maximum and the minimum of the search space of the dth dimension, respectively. The pseudo code for the proposed exploration method is shown in Fig. 1.In the context of the search process, the exploitation can be considered as equivalent to performing fine search in the area surrounding the best position. The set of particles performing this task are referred to as “the Exploitation Team”, or equivalently, “the exploiters”. For this task, PSO can be used with some modification, e.g., the modification proposed by Angeline [20], which uses the selection operator of GA in favor of better performing population or the social-only PSO model suggested by Kennedy [39]. However, these modifications do not provide direct control over the exploiters’ search radius around best position.As an alternative, we propose a simple yet effective method. Instead of flying toward the best position, the exploiters start their journey from the best position and move in small steps in the surrounding hyperspace. In other words, before each iteration, the exploiters are repositioned at the best position, around which they perform a more refined search with a controlled small neighborhood. The new position of the jth exploiter is obtained by adding a motion Mj,dgiven by:(5)Mj,d=βj×rand3×(Rmax,d−Rmin,d)(6)xj=xj+Mj,dwhere rand3 is a random number generated from the standard normal distribution, Rmax,dand Rmin,dare the maximum and the minimum of the search space of the dth dimension and βjis the exploitation factor (covered in more detail later in this section). The use of the standard normal distribution for rand3 favors exploiting closer neighborhoods over farther ones. In addition, it should be noted that at any given time only one dimension of the exploiter is updated. Hence, in order to update all the dimensions, it is necessary that the number of exploiters is at least equal to (or greater than) the search dimensions. However, for higher dimensional problems this will result into very large swarm size and inefficient use of function evaluations (FEs). To solve this problem and enable the search with relatively small swarm size, the dimension of the exploiter is selected randomly. Also, the position update process is repeated D times to ensure that in any given iteration, on the average, all dimensions are updated. Unlike PSO, the position of the particle is not updated on the basis of the difference vector between the current and best positions. It rather moves away from best position in hyperspace in small steps, controlled by the exploitation factor, β.TOSO has two parameters: the exploration factor, α and the exploitation factor, β. In addition, a proper choice of mutation probability is also required. In this work, a mutation probability of 20% is chosen with no rigorous justification and it was found to be a reasonable choice for overall performance improvement. The exploration factor α and the exploitation factor β can be maintained constant. However, using time-varying factors, somewhat similar to time varying inertia weight in PSO [6], is more likely to give superior results. To maintain simplicity in this work the following empirical formulae are used:(7)α=min(fitness)1+max(fitness)(8)β(k)=γ1+γ2×e(γ3(k−1)/0.5×ps−1)−1e(γ3)−1where fitness is the vector containing the fitness of all explorers; γ1, γ2, γ3 are constants (to be determined later) and ps is the swarm size. The constant k represents the rank of the particle based on its fitness. After every function evaluation, exploiters are ranked based on their fitness, e.g. the exploiter with minimum fitness will be ranked 1st and one with maximum fitness will be ranked last, 15th in this paper (as exploitation team consists of fifteen exploiters). (8) is the same empirical formula used in [18] for assigning learning probability in CLPSO. However, here it is used for assignment of the exploitation factor.For a given iteration unlike the exploitation factor β, the exploration factor α, is the same for every particle of the exploration team. It is controlled by the best and the worst performers of the team. As revealed by (8), the value of β assigned to particles can be controlled by γ1, γ2 and γ3. The values of γ1 and γ3 determine the search radius of the best and worst performing particle of the exploitation team, whereas γ2 will determine how β is assigned to the other exploiters. In this paper, it was chosen that the best exploiter will search in 0.01% of the range and the worst performing exploiter will search in 50% of the range from their current position. This determines the values of the constants as: γ1=0.001, γ2=0.499 and γ3=2. Fig. 2depicts the distribution of β among the exploiters based on their fitness ranking using the aforementioned constant values in (8). These values ensure that the best performing exploiter will perform very fine search in the next update, whereas the exploiter with the worst performance will search with a much larger radius, limited by γ2. Following this procedure, each exploiter is assigned a unique exploitation factor before each position update.To investigate the effect of the exploitation factor, β, on the performance of the algorithm, a test was carried out using a set of widely-used 30 dimensional benchmark functions: Schwefel's Problem 2.26 and shifted versions of Ackley, Rastrigin and Griewank functions (discussed in more detail in the next section). For the test we use (8) to evaluate β along with various fixed values. Each run is carried out for 200,000 FEs. For statistical evaluation, 25 runs were performed. Results are depicted in Fig. 3. The test was repeated with different values of β. From the convergence plots it can be observed that the algorithm with assignment-based exploitation factor (using (8)) gives the overall best results.The next step is to integrate both search tasks. The information link between the two teams is the best position. At the beginning of the search the population is randomly initialized over the whole search space and equally divided into two teams. The fitness and best position of the swarm is evaluated. The iterative procedure involves position updates for both the teams. The pseudo code of the process is shown in Fig. 4.

@&#CONCLUSIONS@&#
In this paper, a team-oriented (PSO-based) new hybrid algorithm is proposed. The essence of this approach is the decoupling of the two aspects of the search process (exploration and exploitation). This decoupling is effective in enhancing the algorithm's capability to solve unimodal and complex multimodal problems. For exploitation, the new local search technique is proposed, in which exploiter particles start from the best position and its search area around the best position is defined based on its fitness ranking. For exploration, a modified local PSO framework (based on ring topology) is used. To provide more diversity to explorers, random mutation is introduced. The information link between the two teams is the best position found during the search process.The proposed algorithm was compared to eleven modified and hybrid PSO algorithms recently suggested on wide variety of test problems. The comparison shows that the proposed algorithm is quite effective on almost all class of test problems included in the tests. The results obtained show that with the team oriented approach it is possible to extract maximum benefits out of algorithm components used in hybridization. In all the tests, TOSO outperformed the conventional PSO. In addition, over all the tests TOSO provided significantly better/comparable results to most recent PSO modifications. Moreover, it scaled well on very high dimensional complex functions up to 1000 dimension with relatively small population size of thirty particles.In the present work the aim was to keep hybrid algorithm simpler and free of tunable parameters. The proposed algorithm has only one parameter (mutation probability, pm) which requires adjustment, but the tests carried out on wide variety of test functions indicated that it can be safely fixed to 20%.However, in the quantitative analysis, it was observed that performance improvement over PSO comes at the cost of increased complexity for low dimensions. Although the results indicate that for high dimensions, increase in complexity of the proposed algorithm is much less than that of PSO. In addition to this, results obtained for the test carried out on the composite functions indicate that even though TOSO performed competitively, it still lacks on the exploration front. To summarize, from the obtained results, it is apparent that the notion of isolating search aspects of exploration and exploitation is quite beneficial to the search process, yet the exploration model used in this work can be improved further and needs more attention.It is noteworthy that the algorithm presented in this work is not unique. The team oriented approach can be applied to any hybridization effort. It is possible to use a different model for the exploitation and the exploration teams than reported in the present work. The whole idea is to equip the teams with specific learning methods to perform their tasks effectively. For example, for the exploration team, it is possible to use the comprehensive learning used in CLPSO or the local version of the PSO. Any neighborhood topology which makes information flow slower among particles like hierarchal topology with small branching degree suggested in [17] is also suitable for exploration. Same applies to the exploitation team. For the exploitation any local search method or updated algorithmic component from PSO or other evolutionary algorithm can be used to perform more refined search around best position.