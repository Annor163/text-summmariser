@&#MAIN-TITLE@&#
CSTORE: A desktop-oriented distributed public cloud storage system

@&#HIGHLIGHTS@&#
A desktop-oriented distributed public cloud storage system is proposed.Three-level mapping hash method is used to distribute and locate data.Using migration and rank extension to implement load balancing and fault recovery.Sequence numbers are used to guarantee consistency.Implement data deduplication and use Bloom filter to recycle rubbish.

@&#KEYPHRASES@&#
Desktop-oriented,Metadata,Hash-rule,Bucket,Storage,

@&#ABSTRACT@&#
Previous distributed file systems aim at storing very large data sets. Their architectures are often designed to support large-scale data-intensive applications, which cannot cope with massive daily users who want to store their data on the Internet. In this paper, CSTORE is proposed to support mass data storage for a large number of users. The user-independent metadata management can ensure data security through assigning an independent namespace to every user. Operating logs are applied to synchronize simultaneous sessions of the same user and resolve conflicts. We also implement a block-level deduplication strategy based on our three-level mapping hash method for the large quantity of repeated data. The migration and rank extension on the hash rules are defined to achieve load balancing and capacity expansion. Performance measurements under a variety of workloads show that CSTORE offers the better scalability and performance than other public cloud storage systems.

@&#INTRODUCTION@&#
In the booming Internet, the requirement of dealing with huge amounts of data is very common. For traditional storage solutions cannot accommodate the multi-petabyte of data, large-scale distributed storage systems are introduced [1–6]. However, the way of using data is changing today. Especially, an Internet user can read or write files on PC, mobile device and even web with many online sessions at the same time. Moreover, in the area of cloud computing [7,8], a large number of end users begin to move their desktop data [9,10] to the backend cloud service system, and storage system metadata operations account for as many as half of the workloads of typical file systems [11]. Desktop users will continually access the metadata rather than actual data, such as renaming, moving, and looking up filenames. Therefore, the metadata of user files should be separated efficiently and operated safely. Previous distributed storage systems fail to consider these application scenes. In Ceph with the object-based storage architecture, the metadata are not completely separated from actual data [1]. PVFS [2] with compatible POSIX API, cannot access the data with different sessions at the same time and the only metadata server limits the throughput when there are a large number of access requests. Though some new distributed storage systems with special client library (non-POSIX API), such as Google file systems [3], Apache HDFS [4], Amazon Dynamo [5], and Amazon S3 [6], propose some design architectures to resolve the problems above, the design goal of those systems do not focus on the considerable desktop data problem of a large number of Internet users. Additionally, the storage systems with the locating mechanism such as DHT [12] take much overhead either in stabling the structured overlay network, or in storing and searching the mapping table. Therefore, it is necessary to develop a new storage system for the considerable desktop data.In the paper, we design a desktop-oriented distributed public cloud storage system, namely CSTORE, to resolve the above problems. First, a storage system is required to deal with a large number of users and their tree-based directories. For this purpose, we adopt a metadata cluster to provide the separated namespace service. We also introduce the update on version and operation log methods to face the multiple-sign-on challenges. Furthermore, on the basis of logical sequence number, the replication and data synchronization guarantee the availability and reliability of data. Inspired by object-based storage [13], we store actual data as blocks in SU (storage unit). In order to initialize hash rule, and load balance, SU is controllable by RS (rule server).We present the architecture and implementation of CSTORE, a cloud storage system with high performance and scalability. CSTORE can provide users with independent namespaces that can be accessed from multiple devices. The replication on file system level makes the file data reliable and available. Moreover, CSTORE is compatible to most of the UNIX/Linux file systems for its support of POSIX semantics. We compare performance of CSTORE with a public cloud storage system in accessing metadata and different size of data.The rest of this paper is organized as follows: In Section 2, previous work and motivation are summarized; In Section 3, the system architecture overview of CSTORE is presented; In Section 4, the solution to our goals is provided. In Section 5, we evaluate the performance of CSTORE and compare it with other public cloud storage systems in different data scales.Distributed file systems, such as Ceph, HDFS, and PVFS, have been studied for years. The focus of these file systems is to meet various requirements, such as scalability and throughput under heavy concurrency. But all of these systems fail to resolve the massive small independent namespaces and multiple-sign-on problems. Consequently, some commercial data storage services for desktop users have emerged, e.g., Riak CS [14].In Ceph, the CRUSH algorithm [15] is introduced to distribute object replicas to the structured storage cluster, which eliminates the workload of metadata cluster and improves the locating efficiency for clients. In Ceph, sub-tree partitioning [16] is also introduced to improve the scalability and decrease the load balance difficulty of the system. In addition, the replicas are adopted to guarantee data safety during failure detection. However, in Ceph, the metadata are stored in memory cache in MDS (metadata server). As for safety, MDS must commit journal to the OSD [15]. The synchronous I/O fails to achieve the high performance when multiple clients upload or download thousands of files simultaneously. In CSTORE, MU (see Section 3.3) and SU (see Section 3.2) are adopted to store metadata and actual data respectively and we separate the metadata from actual data completely. MU is used to manage the metadata which contains a large number of users’ namespace and their tree-based directories.The architecture of HDFS has one namenode to handle metadata and multiple datanodes to store file chunks. The namenode exposes the mappings from files to chunks. And its 64-MB chunk size is designed for write-once read-many semantics, which is highly suitable for Hadoop/Map-Reduce applications [17] and rarely applied in common circumstances. Thus the consistency model is simplified because of the lack of concurrent writes. By locating the computation and storage on the same node, HDFS makes full use of the bandwidth and reduces the cost. The block in HDFS also has replicas. HDFS maintains the number of replicas. In addition, HDFS block placement strategy does not take into account datanode disk space utilization. Therefore, it adopts a balancer to balance disk space utilization. In order to meet the requirement of multiple desktop users, it is necessary to solve the issue that the major operations include modifying the metadata or uploading and downloading small files. In CSTORE, MU (metadata unit) is used to cope with the challenge of a large number of these operations. In addition, much little blocks are used to store data and deduplication was introduced to save the space of storage.A complete PVFS file system is composed of a metadata server and several storage servers. It adopts a united namespace and RAID [18] to guarantee the reliability of file data. Since PVFS clients usually involve scientific applications, concurrent file access is supported and the POSIX sequential consistency is guaranteed for non-conflicting writes. The clients can easily cache the file’s layout because the location of the stripe units can be algorithmically derived from its offset in the file. Besides, PVFS supports UNIX/Linux file system API with most POSIX semantics. As mentioned above, PVFS has only one metadata server which is prone to become the bottleneck. Meanwhile, PVFS has only one global namespace. Therefore, it hardly copes with the multiple users to access data at the same time. Moreover, CSTORE can deploy multiple MUs (see Section 3.3) to solve the metadata server bottleneck. Besides, CSTORE supports multiple namespace to satisfy access requests of different users.Riak CS is a cloud storage solution built on the open source distributed database Riak [19]. Riak CS handles the uploaded large files and exposes an S3-compatible API, user and administrative functions, and usage reporting. In Riak CS, there is no master node and each node has the same responsibility. It divides objects into different chunks associated with metadata storage. Under Riak CS, Riak node data is automatically distributed evenly across nodes with consistent hashing. Meanwhile, Riak stores data with a simple key/value model. Key/value pairs are logically grouped together in a namespace called bucket. As writing new keys to Riak, the data’s bucket/key pair is hashed. The resulting value is mapped into a 160-bit integer space. This integer space can be conceptualized as a ring to determine where data are placed.

@&#CONCLUSIONS@&#
We have presented CSTORE, a distributed public cloud storage system that allows users to efficiently store desktop files. CSTORE is built around several contributions: an independent namespace based on the three-level mapping hash method, the namespace and file data consistency mechanism and block-level deduplication strategy. These contributions let CSOTRE achieve high storage utilization, fast locating for metadata and file data. We have shown that CSTORE achieves better performance than Riak CS in terms of processing metadata, small files and large files. Besides, CSTORE can failover successfully when a failure happens.In the future work, we will study the physical storage model of metadata to reduce disk expenses and improve the performance. The I/O performance of CSTORE will be continuously optimized.