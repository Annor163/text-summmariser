@&#MAIN-TITLE@&#
Rapidly finding CAD features using database optimization

@&#HIGHLIGHTS@&#
This paper describes a declarative feature recognizer which utilizes concepts from database query optimization.It gives a general way to translate feature definitions to efficient SQL query.It uses lazy evaluation to reduce the work performed by the CAD modeler.It also uses estimated cost of reorder various geometric computations to further improve performance.Our approach provides linear time performance with respect to model size for common features.

@&#KEYPHRASES@&#
Feature recognition,Database query planning,Declarative features,

@&#ABSTRACT@&#
Automatic feature recognition aids downstream processes such as engineering analysis and manufacturing planning. Not all features can be defined in advance; a declarative approach allows engineers to specify new features without having to design algorithms to find them. Naive translation of declarations leads to executable algorithms with high time complexity. Database queries are also expressed declaratively; there is a large literature on optimizing query plans for efficient execution of database queries. Our earlier work investigated applying such technology to feature recognition, using a testbed interfacing a database system (SQLite) to a CAD modeler (CADfix). Feature declarations were translated into SQL queries which are then executed.The current paper extends this approach, using the PostgreSQL database, and provides several new insights: (i) query optimization works quite differently in these two databases, (ii) with care, an approach to query translation can be devised that works well for both databases, and (iii) when finding various simple common features, linear time performance can be achieved with respect to model size, with acceptable times for real industrial models. Further results also show how (i) lazy evaluation can be used to reduce the work performed by the CAD modeler, and (ii) estimating the time taken to compute various geometric operations can further improve the query plan. Experimental results are presented to validate our main conclusions.

@&#INTRODUCTION@&#
Feature recognition aims to extract certain substructures from a solid model; it has been the subject of extensive research during the past thirty years  [1–3]. One major application of feature recognition is to computer-aided process planning (CAPP), the generation of sequences of instructions for manufacturing  [4]. More recently, the trend of integrating computer-aided design (CAD) with computer-aided engineering (CAE) has led to a requirement for model simplification. As much as 80% of overall analysis time can be spent on mesh generation in the automotive, aerospace, and ship building industries  [5,6]. By removing (typically small) features which have little effect on the analysis results, simplified models can be meshed more quickly and robustly for finite element analysis, and in turn analyzed more quickly, as the meshes are simpler  [7–10].Fig. 1, which extends a figure in  [11], shows some typical (simple) industrial features. Manually finding them is tedious, and in extreme cases, infeasible to carry out reliably, as complex models may have upwards of tens of thousands of small features, of many types and forms. Traditional automatic feature recognition (AFR) algorithms face two challenges. Firstly, features are diverse, and different applications need to find different features. Most existing work concerns fixed algorithms for finding predetermined features  [12]. However, it is infeasible to hard-code all possible useful features for all possible domains in advance. The second issue is that many approaches to feature finding have high computational complexity: times taken to find features can rise rapidly when dealing with complex features and large, detailed models.The first issue above is challenging, as it is difficult for engineering end-users to define effective algorithms for finding features. One solution is to use a declarative approach: this allows users of a feature finder to simply state what properties a feature has, and how a feature is composed, rather than having to give an algorithm to find instances of the feature.The performance issue is a bottleneck in industrial CAD–CAE integration, especially as engineering designs are becoming rapidly more complicated. A nuclear submarine may contain 300 times more parts than an automobile; for the latter, it can take about four months to prepare a mesh from the CAD model  [5]. It is well known that existing approaches to finding complex features based on such techniques as subgraph pattern matching, forward chaining using frame-based reasoning, and pattern-matching techniques are computationally impractical  [13,14]. Even when using a declarative approach, naively turning such a definition into an algorithm results in a series of nested loops, which takes far too long to execute for any non-trivial feature. Gibson, who pioneered the declarative approach, considered six specific optimizations for transforming the naive code into a faster algorithm  [15,16]. He used this approach to solve various 2D feature recognition problems. However 3D problems involving complex features and large models require other kinds of optimization. In previous work  [17], we noted that relational database management systems (DBMS) also use a declarative language, SQL, to formulate database queries, and that much research has gone into optimizing the execution plans into which the queries are translated  [18]. We demonstrated that these optimizations as built into a DBMS can be used to advantage when turning declarative feature definitions into executable algorithms for finding features. Our high-level declarative feature language allows end-user engineers to define new features relevant to their problem domain. Finding features–instances of these declarations–is translated into an SQL query, which is then input to a relational DBMS (SQLite) coupled to a CAD modeler (CADfix) as a back end. Geometric and topological information is processed instead of data from tables. Our main conclusions are as follows: naive translation of a feature declaration based onedistinct entities (i.e. faces, edges, vertices, or subfeatures) leads to an execution plan withenested loops, causing feature finding to take exponential timeO(ne)for a model withnentities. However, SQLite’s optimizer is often capable of optimizing such plans into ones taking quasi-quadratic time for simple features, giving a significant improvement, and times which are potentially viable for a real system. We analyzed which optimizations in SQLite’s query optimizer led to this performance, and also compared them to the specifically crafted optimizations devised by Gibson  [16].This paper builds upon that previous work. We have replaced the SQLite database engine with PostgreSQL, as its query optimizer is more powerful (and also allows more complex SQL queries which we expect to be useful in future research). Doing so has provided us with several further insights: (i) query optimization works quite differently in these two databases, (ii) with care, an approach to query translation can be devised that works well for both databases, despite these differences, (iii) for various simple common features, more or less linear performanceO(n)can be achieved with respect to model size, and (iv) acceptable performance can be achieved for real industrial models. PostgreSQL is clearly a more suitable database engine for a CAD feature recognizer, as SQLite typically gives quasi-quadratic performance. We analyze how linear time performance is achieved, and compare the PostgreSQL optimization approach with SQLite query optimization and Gibson’s work. We have also investigated (i) how lazy evaluation can be used to reduce the work performed by the CAD modeler, and (ii) how estimates of the time taken to compute various geometric operations can be used to further improve the query plan. Experimental results are presented to validate our main conclusions.The rest of this paper is organized as follows. Section  2 discusses previous work. Section  3 overviews our architecture, while Section  4 details our contributions to feature recognizer speed: effective translation, lazy evaluation, and selectivity. Section  5 presents our experimental results and discusses them, while Section  6 considers limitations and future work and Section  7 concludes the paper.We start by briefly summarizing prior work on feature recognition, much of which is historical—yet the need for feature recognition is perhaps greater now than ever before.Since the seminal work on geometric model analysis and classification by Kyprianou  [19], much work has considered feature recognition. Classic feature recognition systems can be categorized into graph-based, volumetric decomposition, and hint-based approaches  [3,4]. Graph-based methods first translate a B-rep model and a target feature into attributed face adjacency graphs (AAG), and then perform graph matching. Several successful feature recognition systems are based on this approach  [9,11,20–23]. There are three main drawbacks to graph-based approaches. Firstly, they are less successful at coping with interacting features, and features with variable topology, such asn-sided bosses for arbitraryn. Secondly, they are slow. In general, subgraph isomorphism is an NP-hard problem and has worst case exponential complexity  [24]. Thus, some partitioning strategy or hints must be used  [12], but even then times can be too long for large models or complex features. Thirdly, it is difficult to extend the approach to real industrial tasks involving complex geometry and topology  [14].Volume decomposition and recomposition approaches are also quite general, and better at dealing with interacting features  [25]. Such methods usually decompose a CAD model into a set of intermediate volumes which are then combined to produce features  [3]. Classical works include  [26–28]. However, they again exhibit exponential complexity  [3] and are limited to low degree analytical surfaces  [12].Hint-based approaches are computationally efficient for small features but depend on the generation and definition of hints  [29], and refer to hard-coded features—it is not easy for end users to modify them or define new features  [12]. Key papers here include  [30–32]. Various other approaches have also been suggested for recognizing features, e.g. using octrees to identify assembly features based on spatial and contact face adjacency relationships  [33], artificial neural networks to assist in the recognition of complex features  [34–36], etc.The outstanding key challenges, as noted, are performance, and the need for end users to be able to define their own features. To overcome the problem that engineers who understand what a feature is may not be expert in devising geometric algorithms to find such features, Martino  [37] developed a teaching-by-example technique for form feature recognition, which first recognizes the protrusions and depressions of the component using syntactic pattern matching then performs graph matching. Suh  [38] defined features textually in terms of a set of fundamental features and their arrangement represented by fundamental spatial relationships, turning feature recognition into a constraint satisfaction problem. This replaces the usual face adjacency graph search by a hint-based constraint-graph traversal. They also investigated several optimizations to reduce the search space. This approach has worst case time complexity ofO(mn2)wheremis the number of nodes in the relationship graph andnis the number of fundamental features in the part. The idea to use a declarative approach has been considered previously. N-REP is a declarative system based on EXPRESS  [39,40]; it uses a graphical interface to allow users to define features by selecting necessary entities. A tailored system for locating turning features for mill-turn parts was developed based on N-REP, with overallO(n2)complexity; herenis the number of machined faces  [41]. However, its feature recognition performance more generally is unclear. Other declarative work includes that by Gibson  [16] and our previous paper  [17].Performance is another tough requirement, especially as real engineering designs become increasingly complex. Graph matching based methods have long been criticized for their high computational complexity, and various methods have been proposed to overcome this problem. Field  [22] defined five classes of machining feature and used oriented face adjacency graph search to achieve linear performance. However, the system only supports prismatic machined features. Regli exploited distributed computing to provide a system with complexity betweenO(n2)andO(n5), depending on the particular configuration of geometric entities and implementation details  [14]. Feature vectors can also be used to optimize graph-based matching, achievingO(n3)performance  [24]. The approach first turns subgraphs into adjacency matrices, then groups and orders different elements, finally encoding ordered adjacency matrices as vectors. This reduces the subgraph isomorphism problem to a computation over three nested loops. Simple declarative approaches also suffer from performance problems, as a naive execution plan involves multiple nested loops, as previously noted.In summary, an ideal feature recognition system should be general, allowing end users to define new kinds of features relevant to their application, but it should leave the system to devise an efficient algorithm. This suggests a declarative rather than procedural approach to feature definition. However, the algorithm generator will need optimization techniques to ensure sufficient performance.Relational databases have long been used as the main way of storing large amounts of related business information. Information is retrieved using declarative queries; if these were naively translated into execution plans, the time taken would be far too long. There is thus a large body of work on optimizing query processing. We rely on this to efficiently retrieve features from CAD models using declarative feature definitions.In the discipline of relational algebra, relational database systems model data and perform queries using set operators. SQL is a high level declarative language used to implement relational algebra in relational database management systems  [42]. A typical SQL query is a SELECT statement which retrieves data from one or more tables. When selecting data from multiple tables, a JOIN operator is typically used to specify how the data from one table is related to that in another table. Joins may be classified as INNER, OUTER (LEFT, RIGHT, FULL), and CROSS types; for details see  [43]. The INNER join is most commonly used, as it is amenable to optimization. In SQL, INNER JOIN is often expressed implicitly as in the example query below. This form is the one used in our proposed approach.Here, items after SELECT name the information the user wishes to retrieve from the database. The keyword FROM is followed by several range tables, which are the source of the target information. Only certain information is used from each table: WHERE specifies various predicates the selected elements should satisfy. WHERE clauses can include subqueries such as the SELECT clause in brackets; they can also be (implicit) join predicates like the one equating a.commit_id to c.id, which connects (i.e. joins) two range tables via a common value. The predicates in WHERE statements are evaluated on all tuples, generating a temporary target list, while the HAVING clause further aggregates the temporary target list to produce the final results. We will use this idea later.When the query is executed, a query optimizer is used to determine a suitable plan, or algorithm, from the declarative form of the query. Considerable effort may be put into query planning, as the savings over straightforward plans may be significant, and indeed turn an infeasible query into a feasible one. Query optimization is a mature field  [44]. Normally, a declarative query is first turned into a relational calculus expression, and the query optimizer then generates various execution paths with equivalent results, using two stages: rewriting, and planning  [44]. The former rewrites the declarative query in the expectation that the new form may be more efficient. An example of this approach is sargable rewriting (i.e. a transformation to take advantage of an index). Planning transforms the query at a procedural level, via relational algebra transformations. Then a cost based planner is used to choose the plan predicted to be fastest based on statistical information about the database. System-R, one of the earliest databases to support SQL, pioneered such optimization  [45]. Its use of dynamic programming to select the best query plan has been adopted by most commercial databases  [44].Space precludes a full discussion of query optimization technology; for more information see  [18]. However, we note that the planner may generate the search space by transforming the query in the following ways:Generalizing join sequencingMany queries involve multiple joins. This step finds an efficient execution order in which to process them. While the operations are commutative and associative, join tuples are not necessarily symmetric, so a translated execution tree with Cartesian products may result in poor performance for some orders of evaluation  [45]. Approaches include turning asymmetric one-sided outer joins into equivalent but re-ordered expressions  [46] by shuffling GROUP BY and JOIN   [47] clauses, an important optimization supported by most current database systems  [48–51].A multi-block query includes several SELECT-FROM-WHERE structures in a single query. Such a query can be converted into a single block query via view merging, nested subquery merging (also called subquery flattening  [48]), and semijoin-like techniques, as explained in  [45].Database systems use various methods to scan tables, including sequential scans, index scans, and bitmap index scans. Index and bitmap index scanning are much more efficient than sequential scanning, because only parts of the table have to be considered  [49]. The planner chooses an appropriate scan method based on selectivity, a quantity which determines the effectiveness of an index in terms of the proportion of the data filtered out  [52].JOIN operations can be translated into procedural algorithms in various ways. The main alternatives use either nested loops, hash joins, or merge joins. Nested loops are normally used for small tables but the other approaches work much better for large tables  [52], and are widely used in mainstream database systems  [49–51].As our work builds on Gibson’s, we briefly describe his contribution in more detail. He suggested that a declarative approach to feature definition could be an effective solution to the problem of allowing user-defined features  [15,16,53]. He also noted that naive translation of the declarative form leads to inefficient algorithms, and that optimization is necessary.He defined features in a language with similarities to EXPRESS  [39]. Features are based on entities (which may be a face, edge, vertex or subfeature), and predicates linking them. Such a declaration can be easily rewritten as an algorithm using a set of nested FOR loops, one per entity in the definition, and IF statements, one per predicate. Executing this takes exponential time in the number of entities in the feature definition, so is infeasible for anything but trivial features. Gibson investigated six strategies for optimizing this basic plan; they are clearly related to those used in database optimization, although Gibson did not consider this point of view. His strategies belong to four categories with respect to their effect on time complexity:Strength reduction and loop re-sequencingBoth methods aim to reduce time spent inside each nested loop. They reduce recognition time by some constant factor but do not change the time complexity, which remainsO(nk)wherekis number of loops andnis the total number of entities in the model. In SQL, join reordering is analogous to loop re-sequencing  [54].These are both ways of splitting a declarative definition into parts—featuretting refers to splitting a feature into subfeatures. This reduces the time complexity fromO(nk)toO(max(n1k1,…,nmkm))wheremis the number of parts andniis the number of entities in parti. Database systems do not typically automatically split queries into several simpler queries—queries are usually performed on all tuples of the search space, so we cannot rely on the database engine optimizer to do this for us. However, if the user defines features in terms of subfeatures (a natural divide-and-conquer approach to problem solving), such a split is achieved manually, reducing time complexity.Precomputing an index allows relevant entities to be directly retrieved, rather than having to scan all entities during query processing. This effective technique is used both in Gibson’s approach and database engines. Time improvements depend on the selectivity of the index.This approach narrows the search space by finding WHERE statements containing equalities and associated conditions. The key idea is to replace an inner loop by results satisfying the conditions, reducing the time complexity. Database subqueries share a common goal with Gibson’s assignment approach, but adopt flattening which works differently in detail.Our previous work  [17] extended Gibson’s work from 2D to 3D models, more typical of real engineering, and considers a greater number of basic entities. We followed his declarative approach, but rather than devising an ad-hoc set of query optimizations, we took advantage of database optimization techniques. We translated declarative feature definitions into SQL queries which could then be automatically optimized by a database engine (SQLite) before evaluation using a CAD modeler (CADfix). SQLite has a compact but effective query optimizer  [48]: it provides sargable rewriting including BETWEEN and OR optimizations, and provides algebraic space and method-structure space transformations such as reordering joins, subquery flattening, automatic indexing and group-by optimizations. Its nearest neighbor heuristic planner provides an efficient polynomial-time algorithm to select a good plan. Our experiments showed that this approach could effectively find various basic features (in particular through-holes, notches, and slots) in models, and experimentally determined that the time complexity is reduced from exponential to approximately quadratic for these simple features. The main optimization processes used by SQLite to achieve this are reordering joins, using a covering index, and subquery flattening.In the current work, we have replaced the database engine by PostgreSQL. One goal was to see whether the optimizations provided by SQLite could be replicated, and to determine whether different database engines would arrive at similar query execution plans when used for feature recognition. As our results later show, SQLite and PostgreSQL take very different approaches to query optimization. Our previous approach for translating feature declarations into SQL queries which worked well for SQLite was much less successful when used with PostgreSQL. This led us to reconsider how to perform translation, culminating in a new approach which works well with both databases. We also show that PostgreSQL query optimization is more powerful for reasons explained later; the significant outcome is that simple features can now be found in linear time.We also extend our earlier work by considering further improvements that can be brought about by lazy evaluation, and by using estimates of time required to compute various geometric operations.Our approach is based on letting a database engine optimize the query plans used to perform featuring recognition, allowing us to leverage the large body of research on database query optimization. The first important contribution of this paper is an approach to translating declarative feature definitions into carefully SQL designed queries, which work well for multiple database systems. Different database systems take different approaches to query optimization, and if the query is presented to the database in a form which is not well handled by its optimizer, poor performance will result.The second idea we consider is lazy evaluation. Some geometric predicates, e.g. determining whether the area of a curved face exceeds a threshold, require significant calculation. For efficiency, rather than evaluating such a predicate for all relevant entities, it is better to only evaluate it for those entities for which it is definitely needed. For example, if a face fails to meet some other constraint such as being connected to a certain edge, we may never need its area.Thirdly, when we do have to perform geometric computations, some are much cheaper than others. It may be quicker to perform a simple computation on many entities rather than a very slow computation on just a few entities. In cases when multiple predicates filter a list of entities, determining how many entities there are of various kinds, and how long different predicates are expected to take to compute, can be used to choose the best order in which to apply the sequence of filters.The second and third kinds of optimization above are typically not used in database systems, as most queries are based on reading data from tables, which is quick, and takes a more or less constant amount of time.We now briefly summarize our system architecture, which remains essentially unchanged from our earlier work  [17], apart from the additional selectivity module and training models. The feature recognizer includes a translator, importer, query planner, executor, and selectivity trainer, interfaced to a CAD modeler (see Fig. 2). Commands to open a model, or draw feature instances on the CAD model, are handled by the command analyzer, and appropriate requests are passed to the modeler. Another command is used to declaratively define a feature. A further command can then be used to seek instances of the feature in a loaded model. At this point, the translator uses the definition to generate an SQL query which is in turn optimized by the query planner internal to the chosen database engine. The importer analyzes the query and caches necessary simple relations retrieved from the CAD modeler for speed; only topological relations and simple information such as edge convexity are treated in this way. The query planner analyzes the query as well as the numbers of entities in the basic topological relations to determine the expected cheapest plan. This takes into account the cost of computing each predicate. The executor executes the chosen query plan, using data from the local cache and other information requested directly from the CAD modeler. The resulting feature instances can be output in text format or drawn on the original CAD model.The current implementation uses PostgreSQL as the database engine—it is free, has open source which aids understanding of its query optimizer, and has clearly structured code which facilitates linking it to the CAD modeler. PostgreSQL supports a range of query optimization approaches. The most important ones include (i) alternative ways to access data using sequential scans, bitmap index scans, or index scans according to filter selectivity (using statistics obtained by ANALYZE), (ii) alternative ways of processing joins to shrink the search space and reduce time complexity, using nested loops, hash joins, merge joins or procedural code, and (iii) reordering join sequences. PostgreSQL’s optimizer uses System R’s dynamic programming approach when the number of tables is small, but switches to a genetic algorithm to solve the join ordering problem when there is a large number of FROM tables  [55,56].CADfix  [57,58] is used as the CAD modeler. It is a commercial geometry translation and repair package primarily intended for 3D model data exchange between different engineering systems and applications. It already provides some defeaturing tools, although we do not make use of these. We use CADfix (via its API) to load CAD models (and repair them to ensure consistent, connected topology), and to interrogate their topology and geometry. It is also used to draw the features found.We now discuss the three main contributions of this paper which improve feature finding performance: effective translation, lazy evaluation and predicate ordering.Effective translation should translate declarative feature definitions into SQL queries which can be efficiently processed by the database engine, independently of how it subsequently performs query optimization. We consider four issues, the predicates used to define features, the translation rules, uniqueness of entities, and the performance achieved.Features are defined in terms of necessary component entities: faces, edges, vertices, and subfeatures. Predicates–truth functions returning a Boolean answer–are used either to define relationships between entities (relational predicates), or characteristics they should exhibit (attribute predicates). The available predicates include:A key point is that the predicates are carefully chosen to be simple. This both aids the user who is writing feature definitions, and in translating the definitions into queries. For example, by using Bounds(edge_id:e,face_id:f), the user does not need to think in terms of following all edges around the boundary of a face, but simply in terms of which edges belong to that boundary.The translator transforms each predicate into a query fragment; multiple predicates are connected using AND. Different ways of translating declarations into SQL queries differ in efficiency.Attribute predicates typically involve only a single entity and some condition that the entity must satisfy, encoding a binary relation. Such predicates can be written as SQL fragments in a straightforward way. For example a predicate Convexity_is(edge, convex) in the definition can be translated into the SQL fragment edge.convexity = convex. These predicates act as filters which reject data not meeting some requirement. A query optimizer can efficiently deal with them by indexing the data.Relational predicates are more complex as they involve multiple entities. Bounds(edge, face) is of this type; it indicates adjacency of some face and some edge. It is one of the most important predicates, used in almost every feature definition. Since the edge and face are arbitrary, when executing a feature query, we must in principle iterate over all faces and all edges to determine which ones satisfy this relationship. As such a predicate involves two variables, it cannot be effectively written as a filter.Our previous work  [17], based on the SQLite database, straightforwardly translated feature definitions into SQL queries using a series of EXISTS clauses. Entities satisfying bounds predicates linking edges and faces (and other relational predicates) were found using a preloaded, cached range table:As SQLite effectively performs self-join optimization, bounds are handled efficiently, allowing simple features to be found in time approximatelyO(n2)for models withnentities. However, on replacing SQLite with PostgreSQL, we found that this did not happen. PostgreSQL has no self-join optimization, and instead uses a strategy based on cross-joins via a Cartesian product. Such optimization fails to reduce the complexity of nested loops corresponding to multiple predicates, and even for simple models, could take days to return results. This led us to rethink the way translation was performed.Assuming that we are dealing with manifold models, each edge bounds only two faces, so the number of bounds relationships is twice the number of edges. We can take advantage of this observation as follows. Instead of thinking in terms of edge–face  pairs joined by a bounds relationship, we think in terms of edge–face1–face2  triples. We extract and cache these in what we call a full-edge-form table. For a given edge, the triples edge–face1–face2  and edge–face2–face1  are both cached, as a feature definition might insist that face1 has a lower id (i.e. unique identifier) than face2, or vice versa (to prevent a symmetric feature from being reported multiple times with different labeling, as we discuss later). This doubles the table size, but provides more flexibility, and has little impact on performance.Feature declarations can be automatically rewritten to use the full-edge-form relationship, rather than the bounds relationship. Feature definitions often specify e.g. that two edges border the same face, or that they belong to different faces. This can be expressed using an equality or inequality predicate. For example, the SQL fragment full_edge_e1.f2=full_edge_e2.f1 corresponds to the need to find a pair of tuples with patterns ei, fa, fb and ej, fb, fc in the full-edge-form table; note that fb occurs in both, as face 2 in the first tuple and as face 1 in the second. Typically, many relations of this kind will occur in the WHERE clause of the generated query. This approach replaces the need to iterate over all face–edge pairs to find ones satisfying a bounding relationship, to simply retrieving those few tuples which match a pattern indicating equality. Most database systems can recognize such relations as corresponding to inner joins, and can readily optimize them  [49–51].At runtime, when the parser encounters any Bounds predicate in the feature definition, the importer module executes a query to create and load the full-edge-form relation. Other predicates are evaluated at runtime and cached into temporary tables as explained later.An example of a definition of a notch feature from our previous work is given in Fig. 3. The inequality clauses linking faces prevent finding the same feature multiple times, as discussed under symmetry in Section  4.1.3. Our previous translation approach for the SQLite database results in the SQL query below11Here and elsewhere we simply omit repeated clauses of a similar nature to shorten the paper. In each case, omitted clauses are replaced by an ellipsis while keeping the first and last clauses.:All edges have convexity specified in the SQL via a numerical code in which CONCAVE=1, CONVEX=2, TANGENTIAL=3, MIXED=4.However, using the full-edge-form approach, this is now translated into the following SQL:A further change we have made is that additional inequality clauses are automatically added during translation to meet the implicit user expectation that all named entities should be different unless explicitly stated otherwise—this is discussed further in Section  4.1.3.In practice, only bounds and convexity predicates are translated into SQL fragments represented by WHERE clauses. Other predicates involving geometry, area, etc., are translated into SQL fragments via a HAVING clause. This design permits lazy evaluation, as we describe later. Bounds and convexity predicates are almost always needed, and can be determined at little cost, so there is little point in using lazy evaluation in these cases. This approach is consistent with previous methods based on adjacency graphs, which use topological information to find potential parts of features and then use other conditions to refine the results.Feature declarations are difficult to write correctly, and as in other areas of geometric computing, special cases can often cause difficulties. Consider, for example, through holes. A through hole in a cube, or most other models, has end loops which lie on distinct faces. However, a through hole in a cylinder can have both end loops lying on the same face, which is a special case. Whether such a special case should be permitted or excluded is a matter for the user. However, it is clear that in most cases, if a feature definition mentions e.g. two faces f1 and f2, it is the intent that they should typically be distinct. In our current system, we make this assumption for all entities in feature declarations, so the Different_id clauses we originally used in the notch example are no longer needed in our current system. This makes it easier for users to write feature declarations. If necessary, the user may override this assumption by adding clauses of the form ALLOWING f1=f2 to state that some particular entities may be the same.This assumption differs from the way an SQL query finds features: each entity is filtered out from a range table, and there is no guarantee that values are distinct. To ensure that entities with different names are distinct, a straightforward approach is to automatically insert an SQL fragment like f1<>f2 into the final query for each pair of entities of the same kind.A further issue is that many features are topologically symmetric in some way, and this can lead to repeatedly finding the same solution in which the names of the entities are permuted. For example, see the notch in Fig. 3: interchanging the roles of faces F1 and F2, and F3 and F4 (as well as various edges) gives another interpretation of the same notch. Such symmetries are in general difficult to detect and handle automatically, and we currently leave this to the user to resolve. One way to do this is to add further conditions on the identities of entities. For example, for notch features, if the user adds Lower_id(F1,F2), Lower_id(F3,F4), it will prevent features from being reported twice.We now consider the performance achieved by the above translation. Using the EXPLAIN ANALYZE database command when executing a feature query provides information about the plan the database uses. Our experiments show that, after optimizing, PostgreSQL uses hash joins, while SQLite relies on indexing. We consider in detail how the query is optimized by PostgreSQL with our new approach, and how our old translation is optimized by SQLite. We consider a basic feature with only Bounds and Convexity predicates. While in principle, features with other geometric predicates (e.g. concerning face type) will theoretically take longer, using lazy evaluation as proposed in next section helps to overcome this problem.In practical SQL queries, it is common for two tables to be connected by equi-join predicates. In feature queries, the Bounds predicates are such equi-join predicates, and the query is an implicit inner join query. When at least of the inputs to a join has few items, it can be effectively computed using nested loops; merge joins are an improvement when there are two large inputs. However, if (as is typically true) main memory is plentiful, hash joins provide substantially better performance than nested loops and merge joins  [59]. Hash joins are the most frequently used join algorithm in current commercial database systems  [60], and are responsible for PostgreSQL’s better feature recognition performance than SQLite’s.In our previous feature finder, predicates were translated into EXISTS subqueries (we refer to this as the old approach). Such subqueries are multi-block queries. They are usually turned into single block queries by merging any subqueries into the main body. While SQLite can perform subquery flattening optimization, it is not used for EXISTS subqueries  [48], as confirmed by examining execution plans. Using the old translation approach, a typical query fragment, from our notch feature finding experiment (see later), might bewith a corresponding execution plan reported by SQLite to beThe numbers indicated are generated automatically by the SQLite query planner, and change case by case. Valency here means the number edges surrounding a face.The execution plan shows that EXISTS introduces correlated subqueries: inner queries depend on outer queries. Here, the inner tables valency, convexity, and bounds have references to the outer table faces AS f1.Consider the valency query first. The executor executes the outer table scan on faces, taking timeO(f)wherefis the number of faces, and then executes the inner scan on the valency table using an automatically created covering index. This a temporary index just used in this query to find tuples satisfying subquery predicates. It incurs a cost ofO(flog(f)), as the valency table has the same number of entities as the face table, and sorting is needed to make the index. Then, similarly, the outer query goes through all edge rows, and for each row, searches in an index. This takes timeO(elog(e)+felog(b))wherebis the size of the bounds table; the convexity table is the same size as the edge table. As, the bounds table contains2eentries, so this is overall timeO(eloge). Now, as models get more complex, generally, the individual faces do not get more complex, there are just more of them. Typically, faces have a small fixed maximum number of edges. This observation, taken together with Euler’s formula, means that in complex models, as the number of faces grows, the number of edges approximately grows in proportion, i.e.O(e)=O(f)=O(n)wherenis the number of entities in the model. Overall, then, processing EXISTS takes timeO(n2log(n)): subqueries correspond to outer tables each running an inner scan over a unique index. Aslog(n)varies slowly, this explains the quasi-quadratic performance empirically observed in our previous paper.The new scheme proposed in this paper is more efficient; EXISTS clauses of the type used above are not required. The simplest kind of hash join includes two steps: first, the smaller relation is used to construct a hash table, then the larger relation’s tuples are used to probe the hash table to find matches (we refer to this as the new approach later). To understand the performance, consider the simplest situation: two (unindexed) relational tables, both withO(n)tuples. The cost is composed of four linear components: reading the inner table, hashing the inner table, reading the outer table, and probing the hash table, giving a total cost ofO(n). This expectation is verified in our experiments later.We now consider a further method to gain additional speed. The idea of lazy evaluation is to avoid computing things until the very moment that they are definitely needed—there is no point in computing things which may later turn out to be unnecessary. For example, suppose p(x) and q(y) are predicates (without side-effects), which may be expensive to evaluate. Consider the expression p(x) AND q(y). We could evaluate both and then compute the result using logical AND. However, if p(x) is false, the overall expression must be false, and we do not need to compute q(y) at all, saving unnecessary work.Lazy evaluation can help to ensure that we only evaluate predicates on a small candidate set. In our feature finder, predicates are evaluated at runtime either by local lookup in cache tables, or remotely by the CAD modeler; some of the latter may take a long time. For example, when finding features such as small pockets, it will be an improvement to only compute the areas of faces definitely belonging to pockets, rather than the areas of all faces.Lazy evaluation is realized in our system by steps in the translation stage, the importer, and the executor. The translated query is first analyzed by an importer, which then retrieves basic relations and entity properties of the model from the CAD modeler. Returned topological information such as bounds relations, and geometric properties which can be rapidly determined such as convexity, are cached locally in database tables. Bounds relations are cached as full-edge-form tables and geometric information is cached in (id,property) tables. All are used as range tables in the final query. Expensive predicates are expressed as foreign SQL functions and evaluated during execution, by calling the CAD modeler directly.The time at which predicates are evaluated is determined by how a feature definition is translated into SQL. Predicates placed in WHERE clauses are evaluated on all tuples of the range tables. Predicates placed in HAVING clauses are only evaluated on temporary results which fulfill the conditions in the WHERE clauses. Thus, our translator puts potentially expensive predicates into HAVING clauses for efficiency. The only predicates placed into WHERE clauses are basic topological predicates (which can be optimized by hash joins) and fast geometric predicates (which can be optimized by use of an index).For example, if the user wants to find large step ribs (see Fig. 4), whose middle face has an area greater than 50 units, a feature definition might be translated as:The result in this case is that the area function is called many fewer times—only for mid-faces of step ribs, and not for all model faces.For additional efficiency, caching is used: each time we evaluate an expensive predicate such as one involving the area of a face, we first see if it is already available in a local table. If not, a remote call is made to CADfix to calculate the result, which is then also cached in the local table.Our final approach to provide speed gains concerns predicate ordering. Query optimization in database systems includes reordering subtasks in a query for efficiency—if a series of filters is applied, we would like the first filter to reject as much as possible so that subsequent filters have less data to process. Standard database query optimization chooses an approach based on statistical information, including the fraction of column entries that are null, the average size of column entries, whether the number of distinct values is likely to increase as the table grows or not, and so on  [61]. It is usually assumed that retrieving each data item takes a constant amount of time, whereas in our system, some information must be computed by the CAD modeler, and so the time taken may vary considerably according to the predicate involved. We therefore modify the standard database query optimizer to take this into account.Our approach is based on the idea of selectivity, the probability that a given predicate will return TRUE. In a HAVING clause with multiple predicates, the order in which they are evaluated does not affect the result. If all predicates took the same time to evaluate, for efficiency, we should thus evaluate them in decreasing order of selectivity, to reject as much as possible early on. However, some take longer to evaluate, which should also be taken into account: if all predicates were equally likely to be false, we should evaluate the fastest ones first, to reduce the number of slower evaluations. These two requirements can be combined to give an overall optimal order of evaluating the predicates by considering the merit of a predicate,m=sc, wheresis its selectivity, andcis its expected cost (time taken to evaluate it). The fastest way to evaluate a clause is to evaluate the predicates in order of decreasing merit.However, for a given model, we know neither the selectivity, nor the cost of executing a given predicate. Nevertheless, we can obtain estimates for these quantities by a prior offline analysis of a collection of CAD models. Ideally these would be models of a similar kind to the one being considered—a collection of similar water pumps, for example, if we are finding features in a water pump.LetP(a1,…,an)be a predicate withnarguments, which for simplicity we take to be discrete values. Suppose the training set hasMmodels. The selectivity for thekth model taken individually is(1)sk=Ok/IkwhereOkis the number of entities in modelkfor which the predicatePis true, andIkis the number of entities in modelkthatPcan be applied to. The average selectivity of this predicate over the whole training set is(2)E(s)=∑1MOk/∑1MIkWhen predicates involve continuous values, the definition of selectivity needs to be modified somewhat. For example, face area is a continuous variable, with a corresponding predicate which checks if it is within a given range: face_area_in_range(face_id:int, rmin:real, rmax:real). Selectivity is now(3)s=∫rminrmaxP(A)dA/∫0∞P(A)dAwhereP(A)is the probability density that an arbitrary face has a certain area. In practice, this may be estimated by constructing a histogram of face areas for all models.We can also estimate the average time needed to execute each predicate by processing the same collection of models offline.Suppose a query has two predicatesp1andp2, with average costsc1andc2and average selectivitiess1ands2. We can estimate the times needed to execute these in different orders to be:(4)p1thenp2:t12=s1c1+s1s2c2p2thenp1:t21=s2c2+s1s2c1,and choose the order of execution according to which of these numbers is smaller. This analysis may be readily generalized to larger numbers of predicates.

@&#CONCLUSIONS@&#
This paper has presented an extensible feature recognition system based on declarative feature definitions. It can find simple features in large CAD models in linear time in theory, and in a fraction of a second in practice. The key to this performance lies in making use of mature database optimization techniques. Our main contribution is a novel approach to translating declarative feature definitions into SQL queries, with improved performance over earlier work. Further improvements are achieved by use of lazy evaluation to avoid computing complex predicates whose results are not needed, and use of selectivity to reorder query processing. Our method brings with it various limitations, as just discussed, and the opportunities for further work on this approach.