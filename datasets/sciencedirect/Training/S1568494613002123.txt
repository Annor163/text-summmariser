@&#MAIN-TITLE@&#
ModEnPBT: A Modified Backtracking Ensemble Pruning algorithm

@&#HIGHLIGHTS@&#
A ModEnPBT algorithm is proposed based upon our EnPBT algorithm, aiming at overcoming its drawbacks in solution space definition.ModEnPBT has concise definition of solution space, with no redundant solution vectors. It possesses relatively higher computing efficiency.Backtracking algorithm can systematically search for the solutions of a large-scale combinatorial optimization problem.Experimental results on three benchmark classification tasks verified the effectiveness and superiority of ModEnPBT.

@&#KEYPHRASES@&#
Ensemble pruning,Selective ensemble,Backtracking algorithm,Ensemble Pruning via Backtracking algorithm (EnPBT),Modified Ensemble Pruning via Backtracking algorithm (ModEnPBT),

@&#ABSTRACT@&#
This paper proposes a new Modified Backtracking Ensemble Pruning algorithm (ModEnPBT), which is based upon the design idea of our previously proposed Ensemble Pruning via Backtracking algorithm (EnPBT), and however, aiming at overcoming its drawback of redundant solution space definition. Solution space of ModEnPBT is compact with no repeated solution vectors, therefore it possesses relatively higher searching efficiency compared with EnPBT algorithm. ModEnPBT still belongs to the category of Backtracking algorithm, which can systematically search for the solutions of a problem in a manner of depth-first, which is suitable for solving all those large-scale combinatorial optimization problems. Experimental results on three benchmark classification tasks demonstrate the validity and effectiveness of the proposed ModEnPBT.

@&#INTRODUCTION@&#
Integrating the predictive results of multiple classifiers, that are complementary among each other, has the possibility to improve the classification accuracy of an ensemble with respect to individual network members. Two classifiers are regarded as complementary if their errors are uncorrelated. When complementary classifiers are combined in an ensemble, correct predictions are enhanced by the integration process [1].However, despite their remarkable performance, ensemble systems have a severe drawback that, it is usually necessary to combine numerous classifiers to ensure that the error converges to its asymptotic value. This leads to requirements of very long time for training and rather large memory for storage of the ensemble, which could be serious problems for online applications [2,3]. A possible solution to these problems is ensemble pruning, i.e. pruning those network members with low predictive accuracy from the original ensemble, and in contrast, choosing those components with high predictive accuracy into the pruned subensemble [2]. The predictive accuracy of network members is determined based upon the so-called pruning set.Actually, Zhou et al. have analyzed the relationship between the generalization ability of an ensemble and the correlation of its individual components in their paper, and revealed that a proper subset of the original ensemble sometimes performs better than the original ensemble itself [4,5]. Therefore, besides the reduction in time and space complexity, ensemble pruning has another potential benefit, i.e. a probable improvement to the performance of the original ensemble [6–13].However, it has been proven to be an NP-complete problem to realize ensemble pruning effectively and efficiently [14,15]. Enumerative algorithm for searching the best subset of member networks is not easily worked for those large scale ensembles. Although greedy algorithms are fast in speed, but since they only consider a rather small subspace within the whole solution space, they often yield suboptimal solutions of the ensemble pruning problem [2,6–8,10,14,16].A novel Ensemble Pruning method based on Backtracking algorithm (EnPBT) is proposed in our previous work. In comparison with other pruning methods including greedy algorithm, the pruned ensemble achieved with EnPBT algorithm generally possesses significantly stronger classification and generalization performance. However, we find an obvious defect in EnPBT that, the definition of its solution space contains many redundant solution vectors. And naturally, the solution space tree of EnPBT also contains many redundant solution vectors. This causes a number of redundant explorations of EnPBT algorithm, and finally affects its entire searching efficiency.Aiming at the above mentioned defect of EnPBT algorithm, we propose a Modified Backtracking Ensemble Pruning algorithm (ModEnPBT) in this work. In contrast, its solution space is compact with no repeated solution vectors. And naturally, its solution space tree is also concise, having no redundant solution vectors. Therefore, it possesses relatively higher searching efficiency in comparison with EnPBT algorithm.This paper is organized as follows. Section 2 presents Backtracking algorithm at length, including a brief introduction about Backtracking algorithm, a description about the general Backtracking algorithm and a formal iterative procedure of Backtracking algorithm. Section 3 describes the proposed Modified Ensemble Pruning Method Based on Backtracking algorithm (ModEnPBT) in detail. Section 4 presents results of the experimental study. Finally, conclusions are given in Section 5.In lots of real world problems, as well as in most of the NP-hard problems, a solution can be acquired by exhaustively searching through numerous but a limited number of possible solutions. And virtually, there does not exist an algorithm that uses a method other than exhaustive search for all these problems. Therefore, it is necessary to develop a systematic searching technique, which could cut down the search space to possibly a much smaller space. A general technique for organizing the exploration known as Backtracking is presented in [17,18]. It is an organized exhaustive searching method, and it could avoid searching all the possible solutions in most cases. In general, Backtracking is suitable to solve problems whose potential solution space is relatively large but finite [17,18].Backtracking is a type of searching method provided with both systematical and jumping searching characteristics. It searches the solution space tree from its root according to the depth-first searching strategy, while the solution space tree comprises all the solutions to a given problem. When any node of the solution space tree is reached, the algorithm always estimates, in advance, whether the node does not comprise any solutions to the problem for sure. If it does do not comprise any solutions to the problem for certain, then the whole sub-tree, with the node being its root, will be jumped over from the systematical searching procedure. Otherwise, the algorithm enters into that sub-tree, continuing its search according to the depth-first searching strategy [17,18].If the Backtracking algorithm is utilized to find all the solutions of one problem, or one of its optimal solutions, then it will not terminate until it backtracks to the root of solution space tree, and, simultaneously, all the sub-trees of the root node have been searched all over by the algorithm. And when the Backtracking algorithm is used to find any feasible solution of the problem, it will finish so long as one feasible solution to the problem is found. This type of algorithm, systematically searching for the solutions of a given problem in a manner of depth-first, is known as the Backtracking algorithm, which is suitable for solving those large-scale combinatorial optimization problems [17,18].In this section, the general Backtracking algorithm is described as a systematic search method that can be applied to a class of searching problems whose solutions consist of a vector (x1, x2, …, xi) satisfying some predefined constraints. Here i is some integer between 0 and n, where n is a constant that is dependent on the problem formulation [17,18].In Backtracking, each xiin the solution vector belongs to a finite linearly ordered set Xi. Thus, the Backtracking algorithm considers the elements of the Cartesian product X1×X2×⋯×Xnin lexicographic order. Initially, the algorithm starts with the empty vector. It then chooses the least element of X1 as x1. If (x1) is a partial solution, the algorithm proceeds by choosing the least element of X2 as x2. If (x1, x2) is a partial solution, then the least element of X3 is included; otherwise x2 is set to the next element in X2. In general, suppose that the algorithm has explored the partial solution (x1, x2, …, xj). It then investigates the vectorv=(x1,x2,…,xj,xj+1). There exist the following three cases [17,18]:(1)If v represents a final solution to the problem, the algorithm records it as a solution and either finishes in case only one solution is required or continues to explore other solutions.(The advance step.) If v represents a partial solution, the algorithm advances by choosing the least element in the set Xj+2.If v is neither a final nor a partial solution, there exist the following two subcases:If there are still other alternative elements to choose from in the set Xj+1, the algorithm sets xj+1 to the next member of Xj+1.(The backtrack step.) If there are no other alternative elements to choose from in the set Xj+1, the algorithm backtracks by setting xjto the next member of Xj. If again there are no more elements to choose from in the set Xj, the algorithm backtracks by setting xj−1 to the next member of Xj−1, and so forth.The general Backtracking algorithm using the iterative procedure is presented as follows:The Formal Iterative Backtracking algorithmInput: Explicit or implicit description of the sets X1, X2, …, Xn.Output: A solution vectorv=(x1,x2,…,xi),0≤i≤n.1.v←()2.flag←false3.k←14.whilek≥15.whileXkis not exhausted6.xk←next element in Xk; append xkto v7.ifv is a final solution then set flag←true and exit from the two while loops8.else ifv is partial thenk←k+1 {the advance step}9.end while10.Reset Xkso that the next element is the first.11.k←k−1 {the backtrack step}12.end while13.ifflagthen output v14.else output “no solution”Ensemble methods generate a variety of hypothesis in the training phase, which are integrated to reach a final decision by either weighted or unweighted voting, stacking, or some other combination methodology in the classification phase [1]. The result of combining the decisions of the classifiers in an ensembleET={ht(x)}t=1Tusing equally weighted voting is [1]:HET(x)=argmaxy∑t=1TI(ht(x)=y),y∈ϒwhere I(·) is an indicator function (I(true)=1 and I(false)=0), ht(x) is the class label decided by the tth member of the ensemble, and Y={1, 2, …, l} is the set of possible class labels [1].The ensemble pruning problem can be defined as follows. LetET={ht(x)}t=1Tbe an ensemble of T component nets to be pruned. We want to find a subset S⊆ETsuch that the total error counts made by the subensemble S based upon the testing set Te={(xi, yi), i=1, 2, …, NTe}∑i=1NTeI(S(xi)≠yi)is minimized, where xirepresents a feature vector in the test set, and yirepresents the value of corresponding target variable.When employing Backtracking algorithm to solve a specific problem, the first task required to do is to definitely define the problem solution space, which should comprise one optimal solution of the problem at least [17,18]. For instance, to prune an initial ensemble with three network members, its solution space was defined as:{(net1),(net1,net2),(net1,net2,net3),(net1,net3),(net1,net3,net2),(net2),(net2,net1),(net2,net1,net3),(net2,net3),(net2,net3,net1),(net3),(net3,net1),(net3,net1,net2),(net3,net2),(net3,net2,net1)}in our previous paper. Based on the above defined solution space, we successfully proposed a novel method for Ensemble Pruning via Backtracking algorithm (EnPBT), which is a brand-new selective ensemble technique that has not appeared in the literature. Its validity has been verified through several simulation experiments in our previous work.However, it is quite apparent that, in the above solution space, solution vector (net1, net3) and (net3, net1) both indicate that component net1 and net3 are selected into the pruned subensemble. That is to say, the above defined solution space has a serious problem, i.e. it includes considerably redundant solution vectors. Take the solution vector (net1, net2, net3) as an example. It repeatedly appears 6 times, while the only difference among them is their different orders of each network member to being selected into the subensemble.Therefore, aiming at the above mentioned problem, in this work, we modify the definition of solution space for this specific problem as:{(1,1,1),(1,1,0),(1,0,1),(1,0,0),(0,1,1),(0,1,0),(0,0,1),(0,0,0)}where solution vector (0,1,1) denotes component net2 and net3 are selected into the pruned subensemble, and net1 is excluded.After the problem solution space has been defined, the next work required to do is to realize its reasonable organization, in order to search the entire solution space easily with Backtracking algorithm [17,18]. In our previous paper, the ensemble pruning solution space was organized as one solution space tree. A solution space tree for Backtracking algorithm to prune an ensemble of 4 models is illustrated in Fig. 1. For the initial ensemble comprises NumComponentNets network members, the problem solution space tree devised in the previous work possesses NumComponentNets!leaf nodes, and NumComponentNets!paths from root node to each leaf node in total.Just as the repeated and redundant definition of the problem solution space in our previous paper, its problem solution space tree is also very redundant, which can be observed clearly from Fig. 1.Based on the modified definition of problem solution space in this work, its problem solution space is naturally organized as a modified solution space tree. Fig. 2shows, in contrast, the modified solution space tree for Backtracking algorithm to prune an ensemble of 4 models. For the initial ensemble comprises NumComponentNets component networks, the problem solution space tree devised in this work possesses 2NumComponentNetsleaf nodes, and accordingly, the modified problem solution space is of size 2NumComponentNetswith no redundant solution vectors. And as is known to all that, 2NumComponentNets≺NumComponentNets!. Therefore, it can be concluded that the problem solution space and its tree devised in this paper possesses significantly smaller size in comparison with those devised in the previous one.When the solution space of ensemble pruning problem is organized as a solution space tree, ModEnPBT algorithm sets out from its root node, and explores the whole solution space tree in both depth-first and jumping manners. First of all, the starting node becomes one active node, and at one time, the current expanding node. From the current expanding node, the exploration moves to a new node in depth. Then, this node becomes a new active node and the current expanding node. If the exploration cannot move in depth from the current expanding node, then the node becomes a dead node. In other words, the node is no longer an active one. Then, the exploration should backtrack to the nearest active node, and make this active node become the current expanding node.The searching of ModEnPBT algorithm will enter into the subtree with one node being its root only when a partial solution is resulted upon reaching that node. Otherwise, the whole subtree with the node as its root will be leapt over from the searching procedure. We call the currently constituted selective subensemble a partial solution if a smaller pruned error is acquired upon its constitution. ModEnPBT algorithm explores the solution space tree in this kind of depth-first and jumping working manner, until it finds the required solution or until there is no active node existed in the whole solution space tree. Then the best solution obtained till that time is output by the ModEnPBT algorithm as the near-optimal pruned subensemble.For example, for the solution space tree illustrated in Fig. 2, when ModEnPBT algorithm is employed to find the near-optimal pruned subensemble with initially 4 network members based on the pruning dataset, the exploration sets out from root node A of the solution space tree, searches to node B, D, H, L1, etc. During the exploration procedure of the ModEnPBT algorithm, each time when a new node is arrived, one solution of selective subensemble is constructed, and simultaneously, the pruned error of the currently constructed subensemble based on the pruning dataset is calculated.A variable MinEnsembleErrorCount is utilized to record the minimum pruned error count found till then. The variable MinEnsembleErrorCount will be updated whenever necessary, that is to say, when a smaller pruned error is acquired upon reaching a new node, i.e. when a smaller pruned error count is achieved upon constituting a new selective subensemble during the ModEnPBT searching procedure. And accordingly, the best solution of selective subensemble is updated with this newly obtained one. The currently resulted selective subensemble is termed a partial solution if a smaller pruned error is acquired upon its construction.The searching of ModEnPBT will enters into a subtree with one node being its root only when a partial solution of selective subensemble is constituted upon reaching that node. Namely, the searching of ModEnPBT will not enter into and instead jump over the subtree with one node as its root unless a partial solution is resulted upon reaching that node. In this manner, ModEnPBT algorithm continues to search the entire solution space jumpily, and the best solution explored till the end of searching procedure is output by ModEnPBT algorithm as the near-optimal solution of pruned ensemble.Concretely, as illustrated in Fig. 2, when node J is reached, the currently resulted selective subensemble consists of component Net1 and Net3. The pruned error count of this newly constructed subensemble based on the pruning dataset is calculated accordingly. If the newly obtained pruned error count is smaller than the MinEnsembleErrorCount achieved till that time of ModEnPBT searching process, the variable MinEnsembleErrorCount is updated to be the newly obtained value. And correspondingly, the best solution of pruned subensemble is refreshed with this newly obtained subensemble.The searching of ModEnPBT will enter into the subtree with node J being its root and move to node L5 only when the partial vector of (1,0,1) constituting a partial solution, i.e. the resulted selective ensemble of (Net1, Net3) achieves a smaller pruned error in comparison with other solutions obtained until then. Otherwise, the whole subtree with node J being its root will be overleapt, and the exploration of ModEnPBT will backtrack to node E. And then, the searching procedure moves to node K. Node K becomes a new active node and the current expanding node.In this manner, ModEnPBT algorithm continues to search the whole solution space systematically and leapingly, and the best solution explored till the end of ModEnPBT algorithm is output as the near-optimal solution of pruned subensemble.The modified Ensemble Pruning by Backtracking (ModEnPBT) algorithm using the iterative procedure is presented as follows:ModEnPBT: The Modified Ensemble Pruning by Backtracking AlgorithmInput: The original ensemble system, the number of component nets in the initial ensemble, i.e. NumComponentNets and an ensemble pruning dataset.Output: The near-optimal pruned ensemble based on the pruning set, i.e. a selective subensemble: SelectedNets.1.SelectedNets=ones(1, NumComponentNets)*22.FinalSolutionFlag←false3.k←1, initialize the minimum ensemble error count, i.e. MinEnsembleErrorCount, to be an enough large positive value.4.whilek≥15.while (k<=NumComponentNets) & (SelectedNets(1, k)>=1)6.SelectedNets(1, k)=SelectedNets(1, k)−17.PartialSolutionFlag←false8.Calculate the current selected subensemble error count, i.e.CurrentEnsembleErrorCount, on the pruning dataset.9.ifCurrentEnsembleErrorCount< MinEnsembleErrorCountthenMinEnsembleErrorCount←CurrentEnsembleErrorCountPartialSolutionFlag←true10.ifMinEnsembleErrorCount=0 then set FinalSolutionFlag←true and exit from the two while loops{ SelectedNets is the best solution on the pruning dataset}11.else ifPartialSolutionFlag is true thenk←k+1 {advance}12.end while13.SelectedNets(1, k)←214.k←k−1 {backtrack}15.end while16.Output SelectedNets as the near-optimal solution of selective subensemble based on the pruning dataset.To compare the classification performances of ModEnPBT algorithm with our previously proposed EnPBT algorithm, and some other state-of-the-art ensemble pruning algorithms and baseline methods, so as to verify the effectiveness of ModEnPBT, three groups of experiments on benchmark classification tasks are performed, including the Date Calculation task [19,20], the Semeion Handwritten Digit Recognition task and the task of Pen-Based Recognition of Handwritten Digits, with the latter two being drawn from the UCI Machine Learning repository [21]. Table 1presents the detailed information about these data sets (i.e. number of attributes, classes, size of the whole dataset, training set, pruning set and test set). In order that an adequate amount of data is available for training, pruning validation and testing, respectively, those datasets with proper size are chosen in our simulated experiments.For the experiments of this work, 5BBC-ICBP-ES, 6BBC-ICBP-ES and 7BBC-ICBP-ES are employed as the three initial ensembles prior to pruning operations. The n-Bits Binary Coding ICBP Ensemble System (nBBC-ICBP-ES) mentioned here is a kind of neural network ensemble system proposed in one of our previously published papers [22]. It is constructed by two layers of subsystems, with the first layer consisting of many small ICBP components, and the second layer being an aggregation subsystem. Those many small ICBP components of the first layer of subsystem are constructed based on the particular characteristic of ICBP network model.Here, the ICBP network refers to the Improved Circular Back-Propagation (ICBP) neural network developed by us in one of our earlier works [23]. ICBP differentiates from the standard BP network in that an extra anisotropic input node is added to the input layer. On account of the extra added anisotropic input node, ICBP acquires an advantageous property which SBP [24,25,26,27,28] network does not have, i.e. by assigning different arrangement of values “1” and “−1” to the connecting weights between the extra input node and all the hidden nodes, we can directly develop a set of heterogeneous ICBP networks with diversified hidden layer activation functions.Specifically, nBBC-ICBP-ES is built by four steps [22]:(1)The number of hidden nodes of the single hidden layer, denoted as Nh, is set to be an appropriate one (e.g., 5, 6 or 7) for an ICBP root model.The connection weights between the extra input node of a root ICBP and all its hidden nodes are assigned with every possible arrangement of binary values “1” and “−1” in a binary string of length Nh, each one forming a new ICBP component, thus totally a whole set of 2Nhsmall ICBP models is obtained. All those possible arrangements of binary values “1” and “−1” for the connecting weights between ICBPs extra input node and all the hidden nodes are just like one binary coding system of “1” and “−1”, which is just the reason why the name of n-Bits Binary Coding ICBP Ensemble System (nBBC-ICBP-ES) comes from.The whole set of small ICBP models of size 2Nhare employed to build the first layer of sub-system entirely, which are trained based on the training data set, respectively.Finally, in the second layer of aggregation sub-system, the final classification decision is made based on the outputs of those component ICBPs in the first layer of subsystem, following the majority voting rule.The reason why nBBC-ICBP-ES is adopted as the initial ensemble system for this work is very natural and intuitive. Since nBBC-ICBP-ES is employed very successfully in our previous work. It is simple but efficient and effective, and its effectiveness has been verified through experiments on several Benchmark classification tasks. And it is expected that the research work about Modified Backtracking Ensemble Pruning (ModEnPBT) algorithm could improve the classification performance and generalization capability of the initial nBBC-ICBP-ES furthermore, so that a well-behaved pruned ensemble could be achieved, which is an original neural network system completely resulted from our own research works.In our experiments, besides EnPBT algorithm, ModEnPBT algorithm is also compared with three kinds of directed hill climbing ensemble pruning (DHCEP) methods, which use Uncertainty Weighted Accuracy (UWA) [29], Complementarity (COM) [1] and Reduce-Error pruning (RE) [1] as their measures, respectively. Additionally, two baseline methods are implemented in comparison, corresponding to two extreme pruning scenarios. The first one selects the best single model (BSM) in the original ensemble, according to the performance of the models on the pruning set, while the second one retains all network components of the original ensemble (ALL) [29].The experiment described in this section is performed 10 times repetitively for each classification task utilizing a different randomized ordering of its instances. All reported results are averages over these 10 repetitions. For each running, each dataset is divided into three disjunctive parts: training dataset, pruning dataset and testing dataset, initially, with the detailed information about the size of each part for each task listed out in Table 1.

@&#CONCLUSIONS@&#
Successful ensemble pruning has the benefits of reduction in time and space complexity of the original ensemble, and probable improvement to its predictive performance. However, it has been proven to be an NP-complete problem to effectively and efficiently realize ensemble pruning.We proposed a novel Ensemble Pruning method based on Backtracking algorithm (EnPBT) in our previous work. It possesses significantly stronger classification and generalization performance in comparison with other state-of-the-art pruning methods including greedy algorithm. However, its definition of solution space contains a mass of redundant solution vectors, which might heavily affect its computing efficiency.A Modified Backtracking Ensemble Pruning algorithm (ModEnPBT) is proposed in this paper aiming at the above mentioned flaw of EnPBT. In contrast, ModEnPBT has concise and compact definition of solution space, with no redundant solution vectors. It possesses relatively smaller size of solution space and higher computing efficiency. Experimental results on three benchmark classification tasks verified the effectiveness and superiority of ModEnPBT.