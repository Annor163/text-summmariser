@&#MAIN-TITLE@&#
A combination selection algorithm on forecasting

@&#HIGHLIGHTS@&#
Propose an optimal selection algorithm from all individual models using information theory.The selected optimal subset significantly outperforms the combination of all individual models.Provide a theoretical approach instead of experimental assessment.The proposed algorithm shows considerably improved performance on forecasting.

@&#KEYPHRASES@&#
Neural networks,Seasonal autoregressive integrated moving average,Combination forecast,Information theory,

@&#ABSTRACT@&#
It is widely accepted in forecasting that a combination model can improve forecasting accuracy. One important challenge is how to select the optimal subset of individual models from all available models without having to try all possible combinations of these models. This paper proposes an optimal subset selection algorithm from all individual models using information theory. The experimental results in tourism demand forecasting demonstrate that the combination of the individual models from the selected optimal subset significantly outperforms the combination of all available individual models. The proposed optimal subset selection algorithm provides a theoretical approach rather than experimental assessments which dominate literature.

@&#INTRODUCTION@&#
Forecasting has received the considerable research during the past three decades. Three main types of forecasting models (Li, Song, & Witt, 2005; Song & Li, 2008) are Time series model (Cao, Ewing, & Thompson, 2012; Cho, 2001; Coshal & Charlesworth, 2011), Causal econometric model (Li, Song, & Witt, 2006; Naude & Saayman, 2005; Page, Song, & Wu, 2012; Roget & Gonzalez, 2006) and new emerging Artificial Intelligence based model, such as neural network, fuzzy time-series theory, genetic algorithms, and expert systems (Bodyanskiy & Popov, 2006; Cao et al., 2012; Carbonneau, Laframboise, & Vahidov, 2008; Chen & Wang, 2007; Cho, 2003; Hadavandi, Ghanbari, Shahanaghi, & Abbasian-Naghneh, 2011; Law & Au, 1999; Pai & Hong, 2005; Wong, Xia, & Chu, 2010; Wu & Akbarov, 2011). From these studies, researchers often seek to identify the best individual model to generate a forecast. However, combination forecasting has proven to be a highly successful forecasting strategy in many fields, which has been demonstrated by empirical studies.Forecast combination was pioneered in the sixties by Bates and Granger (1969). Since then it has been demonstrated that forecast combinations are often superior to their constituent forecasts in many fields (Greer, 2005; Hall & Mitchell, 2007; Holden & Peel, 1986; Lessmann, Sung, Johnson, & Ma, 2012; Li, Shi, & Zhou, 2011; Newbold & Granger, 1974; Sánchez, 2008; Timmermann, Elliott, & Granger, 2006; Winkler & Makridakis, 1983; Zheng, Lee, & Shi, 2006). The most widely used and studied combination forecast methods are ensemble methods, such as bagging (Breiman, 1996) and boosting methods. The typical boosting methods are AdaBoost (Freund & Schapire, 1997), LogitBoost (Tibshirani, Friedman, & Hastie, 2000) and MultiBoost (Webb, 2000). These methods which have the learning capability have two steps: step 1: construct a set of predication models; step 2: predicate a new pattern by taking a weighted vote of their predications. The average or median is used for the continuous outputs, and the majority voting is used for the categorical outputs of the set of predication models from step 1. The most applications are the categorical outputs from the set of predication models. For examples, Wezel van and Pothars (2007) applied ensembles methods (bagging and boosting) to the customer choice modelling problem to improve customer choice predictions. Abellán and Masegosa (2010) proposed the ensemble method using credal decision trees, and showed the good percentage of correct classifications and an improvement in time of processing, especially for large data sets. Finlay (2011) applied bagging and boosting methods to the credit risk assessment to classify consumers as good or bad credit risks, and proposed a new boosting algorithm, ‘error trimmed boosting’. Experiments showed that the bagging and boosting methods outperform other multi-classifier systems, and ‘error trimmed boosting’ outperforms bagging and AdaBoost by a significant margin.For the continuous outputs from the set of predication models, Li, Wong, and Troutt (2001) proposed an approximate Bayesian algorithm for combining forecasts using several examples. Zou and Yang (2004) developed an algorithm called ‘AFTER’ to calculate the weights in the combination forecasting with one-step-ahead forecasting, where the weights are updated for each additional observation. The results demonstrated the advantage of the ‘AFTER’ algorithm. He and Xu (2005) applied the self-organizing algorithm to combine the forecasting models, and demonstrated the superiority by an example of the total retail sales of consumer goods in Chengdu. All individual candidate models are used in the combination for these researches (Li et al., 2001; Zou & Yang, 2004; He & Xu, 2005).For tourism demand forecasting, the outputs of the individual models are continuous variables. The most common combination forecasting models are linear combination of all available individual forecast models in tourism literature. The researchers (Andrawisa, Atiyaa, & El-Shishiny, 2011; Chan, Witt, Lee, & Song, 2010; Coshal & Charlesworth, 2011; Freitas & Rodrigues, 2006; Lessmann et al., 2012; Menezes de, Bunn, & Taylor, 2000; Shen, Li, & Song, 2011) have demonstrated the efficiency of combination forecasts and the superiority of combination forecasts in contrast to individual forecasts. However all available individual models are used as inputs for the combinations. The question is whether we can optimally select a subset of all individual models instead of all individual models in constructing the combination model. If a subset of individual models as inputs for a combination model can improve performance over using all available individual models as inputs in terms of accuracy and robustness, then this subset of individual models is called as an ‘optimal subset’.One of the important issues is how to select the optimal subset of individual models from all those available individual models without having to try all possible combinations of the individual models. This poses an important challenge as examining all possible combinations of individual models only provides an experimental assessment which does not have a rigorous proof from a theoretical perspective. Furthermore, trying all possible combinations would involve intensive computation and is extremely time-consuming if the total number of individual forecasting models is large. The total number of all possible combinations is∑m=2MCMm/Γ(m+1)excluding the individual models for one combination method if there are M individual candidate models available, whereCMm=M×(M-1)×(M−2)×⋯×(M−m+1) and Γ(m+1)=m×(m−1)×⋯×2×1. For example, there are 502 possible combinations for one combination method if M equals nine (nine individual models in total).Combination selection forecasting is rarely studied in the literature. Costantini and Pappalardo (2010) and Kisinbay (2010) employed the encompassing test for combination forecasts algorithms. Costantini and Pappalardo (2010) proposed a hierarchical procedure for the combination, where the procedure was investigated using short-term forecasting models for monthly industrial production in Italy. Kisinbay (2010) demonstrated that the combination forecasts algorithm outperform the benchmark model forecasts using the US macroeconomic dataset, the algorithm developed by Kisinbay (2010) was adopted to analyse US data in the IMF working paper by Baba and Kisinbay (2011).An optimal subset selection from all individual forecasting models is studied in this paper. The optimal subset may contain one individual model, up to a maximum of all individual models. If the selected subset contains only one single model, this means that the individual model gives the best performance out of all possible combinations of individual models.An optimal subset selection algorithm using information theory (Mackay David, 2003) is proposed in this paper. The linear combination models proposed by Shen, Li, and Song (2008), Shen et al. (2011) and Wong, Song, Witt, and Wu (2007) are used to examine the optimal subset selection algorithm for this study. The information concepts have never been applied to the selection of individual models as combination models, and all available individual models are used as inputs for the linear combination methods in tourism demand forecasting literature. For this reason, it is useful to explain the developments in information theory that contribute to forecasting.Traditionally, the best single forecasting model is selected from several individual models in terms of accuracy. In most cases, the best single model may not have extracted all the information that is relevant for the actual output values. The combination models may be able to offer more information to provide a better prediction compared with an individual model. Shannon’s information theory (Mackay David, 2003) argues that we can select an optimal subset of all individual models, and this subset contains enough information to forecast the actual outputs. Optimal subset selection using information theory is widely used in other fields such as the pattern recognition and neural networks fields.Sridhar, Bartlett, and Seagrave (1999) proposed an algorithm using information theory for combining neural network models. This algorithm identifies and combines useful models regardless of the nature of their relationship to the actual output. The algorithm was demonstrated through three examples including the application to a dynamic process modelling problem. The obtained results demonstrated that the algorithm could achieve highly improved performance as compared with a single optimal network or the stacked neural networks based on a linear combination of neural networks.Many algorithms on feature selection based on mutual information (MI) were developed. The algorithm ‘mutual information based feature selection’ (MIFS) based on MI between the individual and the class variables was developed by Battiti (1994) for selecting the features in the supervised neural net learning. However this algorithm can only calculate the MI between one single variable with another single variable. Kwak and Choi (2002) analyzed the limitations of the MIFS algorithm (Battiti, 1994) and proposed an ‘MI feature selection uniform information distribution’ (MIFS-U) algorithm to overcome its limitations. Both MIFS and MIFS-U algorithms can provide better performance compared with the feature selection algorithms such as principal component analysis and neural networks, and have been successfully applied in many experimental design problems. However, both algorithms involve a parameter and it is difficult to determine the range of its value. The fixed parameter is used in the MI based feature selection ‘minimal redundancy maximal relevance’ (mRMR) algorithm (Peng, Long, & Ding, 2005). The ‘normalized mutual information feature selection’ (NMIFS) algorithm was proposed in the paper (Estévez, Tesmer, Perez, & Zurada, 2009) based on the normalized MI by the minimum entropy of both features. The average normalized MI is used as a measure of redundancy of the individual feature and the subset of selected features. The experiments demonstrated that the NMIFS algorithm enhances the MIFS, MIFS-U and mRMR algorithms. The parameter is also fixed in the NMIFS algorithm, which is an advantage comparing with the algorithms MIFS and MIFS-U.In term of speeding, ‘fast correlation based filter’ (FCBF) is fast due to that a few evaluations of bivariate mutual information are computed. The FCBF is a ranking method combined with the redundancy analysis (Yu & Liu, 2004). Fleuret (2004) proposed the forward selection and ‘conditional mutual information maximization criterion’ (CMIM) in term of binary feature selection and showed that CMIM is competitive with the FCBF in selecting binary features. Meyer, Schretter, and Bontempi (2008) proposed a ‘matrix of average sub-subset information for variable elimination’ (MASSIVE) using variable complementarity for microarray data sets. Their experimental results demonstrated that MASSIVE is competitive with the FCBF and CMIM, and outperforms mRMR for some data sets. All these MI feature selection algorithms are based on nominal or binary feature selection. The continuous feature can be transformed to the nominal feature by dividing the variable domain into the finite number of regions with an equal size, where the variable is assumed to be a constant within the region. It is noted that a reasonable size of data should be used in order to transform the continuous feature to the nominal feature. The Kernel-based method (Christopher, 1995) which is based on the Parzen’s window (Parzen, 1962) is employed in this study. The reasons are that (1) the data set used in this study has a small sample size; (2) features (outputs of individual models) are continuous variables; and (3) the data set has a low dimension of input features comparing with the microarray data.The mutual information (MI) (Mackay David, 2003) which is symmetric is a measure of the dependence between random variables. The MI is a positive value and if and only if the variables are independent with the zero MI value. The MI between two discrete random vector variables U and V is defined as follows(1)MI(U,V)=∑u∈U∑v∈Vp(u,v)logp(u,v)p(u)p(v)where p(u,v) is a joint density function and p(u) and p(u) are the marginal density functions. The MI between two continuous random vector variables X and Y is defined as(2)MI(X,Y)=∫-∞∞∫-∞∞p(X,Y)logp(X,Y)p(X)p(Y)dXdYwhere p(X,Y) is a joint density function, and p(X) and p(Y) are the marginal density functions. Using the entropy concept, (1) and (2) can be written as (3) and (4) below(3)MI(U,V)=H(U)+H(V)-H(U,V)(4)MI(X,Y)=H(X)+H(Y)-H(X,Y)where H(U,V) and H(U) are defined in (5) for discrete random vector variables and (6) for continuous random vector variables, respectively.(5)H(U,V)=-∑u,vpu,vlogpu,v,H(U)=-∑upulogpuwhere pu,vis the probability when U=u and V=v, puis the probability when U=u.(6)H(X,Y)=-∫-∞∞∫-∞∞p(X,Y)logp(X,Y)dXdY,H(X)=-∫-∞∞p(X)logp(X)dXwhere p(X,Y) is a joint density function and p(X) is the marginal density function. H(X) is an entropy and a measure of the amount of uncertainty associated with the value of X.H(X,Y) is a joint entropy which measures how much entropy is contained in a joint system of two random vector variables (X and Y). We need to work out the terms∫-∞∞p(X)logp(X)dXand∫-∞∞∫-∞∞p(X,Y)logp(X,Y)in (6) in order to calculate MI(X,Y) in (4). There are no analytical solutions for these terms. Thus we use approximations of these terms presented in (7) according to the definition of ‘Expectation’ for continuous variable.(7)∫-∞∞p(X)logp(X)dX≈1N∑i=1Nlogp(X(i))∫-∞∞∫-∞∞p(X,Y)logp(X,Y)dXdY≈1N∑i=1Nlogp(X(i),y(i))where N is the size of the data X={X(1),X(2),…,X(i),…,X(N)}, X(i)(i=1,…,N) is a d dimensional vector and Y={y(1),…,y(i),,…,y(N)}.The Parzen’s window (Parzen, 1962) with the multivariate Gaussian Kernel-based function (Bishop, 1995) is the most popular construction method for computing the density function p(X) and p(X,Y). p(X(i),y(i)) in (7) is one more dimension of density function p(X(i)). p(X(i)) and p(X(i),y(i)) can be written as (8)(8)p(X(i))=1N∑k=1N1(2πσ2)d/2exp-‖X(i)-X(k)‖22σ2p(X(i),y(i))=1N∑k=1N1(2πσ2)(d+1)/2exp-‖X(i)-X(k)‖2+y(i)-y(k)22σ2where || || is the Euclidean norm or Euclidean distance, σ is the kernel width smoothing parameter and can be determined by the training data. For this study, the range of the kernel width is from 0.1 to 1, step by 0.05, and the best σ is selected using the training data which is used to construct the model. X and Y in (8) are the inputs and actual output, respectively. It is noticed that for large dimension d, the density functions p(X(i)) and p(X(i),y(i)) in (8) tend to zero forσ>1/2π,infinity forσ<1/2πand constant forσ=1/2π.However, in this study d is not large comparing with the other data sets such as microarray data sets.It is essential to introduce the ‘forecasting error measurement’ (FEM) when measuring the performance of a forecasting model. The Mean Absolute Percentage Error (MAPE) is recommended as the most appropriate error measurement (Hanke & Reitsch, 1995; Makridakis et al., 1982) and the MAPE formula is1N∑t=1Nyt-yˆtytwhere ytis the true value andyˆtis the predicted value at time t, N is the size of time series. The Mean Absolute Scaled Error (MASE) is suggested as the best available measure of forecast accuracy (Hyndman & Koehler, 2006) and the MASE formula is1N-1∑t=1Nyt-yˆt∑i=2N|yi-yi-1|These two popular forecast accuracy measures are used in this paper. The MAPE as an example is used as FEM in the MI algorithm of optimal subset selection in Section 2.3.To apply the MI algorithm, we first divide the whole data set D into two data sets: the training data DTrainand the test data DTest. Each column of the whole data set D is the outputs of each individual forecasting model, the dimension of D is the total number of individual forecasting models. The training data DTrainis used to identify an optimal subset from all individual models using the MI theory, the test data DTestis used to validate the optimal subset selection results. There are two common selection methods: forward selection and backward selection (Theodoridis & Koutroumbas, 1999). Both methods accept or reject features, one at a time, in order to construct an optimal subset. Here, the forward selection is applied and the MI algorithm is described as below.MI Algorithm for optimal subset selection:Step 1: Set the initial selected individual model set M={ } which is an empty set, the initial selected data set S={ } which is an empty data set, the training data DTrain=[F1,F2,…,Fj,…,FM]N×M, which is an N rows and M columns matrix, where Fj=(fj1,fj2,fj3,…,fjN)′,(1⩽j⩽M)is the outputs (forecasting values) of the individual model fj(1⩽j⩽M)on the training data set DTrain, N is the size of the training data DTrainand M is the total number of the individual models.Step 2: For each column (Fj) of the training data set DTrainusing equations (4), (6), (7) and (8) calculate the MI between S∪Fjand the actual outputs y, which is MI(S∪Fj,y), and find the maximum MI value among all j which isMaxjMI(S∪Fj,y), where ∪ indicates the combine set.Step 3: Put the individual model fjcorresponding to the maximum MI valueMaxjMI(S∪Fj,y)into M, M={fj}, put Fjinto S, S={Fj} and delete Fjwhich is column j from the training data DTrain.Step 4: Calculate the forecasting error measurement (FEM) described in Section 2.2 for the data set S.Step 5: Repeat Step 2, Step 3 and Step 4 until there is non-significant improvement of the FEM value on S (it implies that the current FEM value is bigger or very close to the previous FEM value). Thus, M is the optimal subset which contains some individual models excluding the current individual model.The order of the individual models for the optimal subset is determined by the MI algorithm and the size of optimal subsets is determined by the FEM. Slightly different results may be obtained if a different FEM is used as the criteria in Step 4.The individual models are the foundation of applying the MI Algorithm for optimal subset selection. Several different time series approaches as the individual forecasting models are adopted in this paper. The world ‘GDP’ and ‘CPI’ and other economic factors as proxies of the influencing factors can be used as inputs if we apply causal econometric models or new emerging artificial intelligence models. However this paper concentrates on forecasting UK inbounds tourism arrivals, not on the impact study of the factors on the UK inbounds tourism arrivals. Thus, the time series are employed and the adopted individual models are described in Section 2.4.In this study, nine individual or single time series forecasting models are used, some of these individual time series are most popular forecasting models, and some of these individual time series are newly emerging techniques. The most frequently used time series in the tourism demand forecast literature (Song & Li, 2008) are adopted in this study. The nine individual models are described in the following sections.The foundations of SVR neural networks were first developed by Vapnik (1995) and Vapnik, Golowich and Smola (1996). SVR are gaining popularity due to many attractive features and their promising empirical performance in the fields such as image processing and finance. The research has produced promising results that have been reported by Tay and Cao (2001) and Ni and Nguyen (2007). There are also applications using SVR for tourism demand (Chen & Wang, 2007; Pai, Hong, Chang, & Chen, 2006). The experimental results revealed that the proposed models outperform the Autoregressive Integrated Moving Average (ARIMA) approaches.In SVR, the training data (used to construct a forecasting model) is a subset of the whole available data and is considered as a set of pairs (X(1),y(1)),…,(X(i),y(i)),…,(X(N),y(N)) where X(i)⊂Rmdenote the input space (m is the width or dimension of the inputs) and y(i)⊂R denote the corresponding actual target value for i=1,2,…,N, where N is the size of the training data set. For this study, X(i)={yt−1yt−2,…,yt-m}m⊂Rmare the vectors of the historical tourism demand observations at time t where t=m+1, m+2,…,N and i=t−m, and y(i)=yt⊂R are the actual target values at time t. For example, X(1)={y4y3y2y1}, X(2)={y5y4y3y2} and corresponding target values y(1)=y5, y(2)=y6 for m=4. The purpose of the regression problem is to determine a function that can predict future values accurately. The generic SVR forecasting model with forecasting valueyˆthas the following general form(9)yˆt=f(X)=(W·Φ(X))+bwhere X has the form X(i), W⊂Rm, b⊂R are the best weights and base to be determined using the training data set, Φ denotes a nonlinear transformation from Rmto a high dimensional space andyˆtis the forecasting value of yt. The goal of SVR is to find the best values of W and b in (9) such that the nonlinear model (9) can be best fitted with the input data X and the output data yt. The best values of W and b in (9) can be determined by the training data.The data used in this paper is the quarterly data, thus the previous one year (inputs width m=4), a year and a quarter (m=5) up to the previous two years (m=8) are used as inputs to construct the five different time series, respectively. The SVR model is generated using MATLAB (Version 2011b) software.The Box–Jenkins forecasting time series model – ARIMA proposed by Box and Jenkins (1970) has become widely used in many fields for time series analysis including tourism demand forecasting. The quarterly inbound UK tourism arrivals data which has a seasonal time series feature is used in this study, thus the Seasonal ARIMA ARIMA(p,d,q)(P,D,Q)swith period s (s=4) is applied here due to the quarterly data. The ARIMA(p,d,q)(P,D,Q)smodel is as follows(10)φp(B)ΦP(Bs)[(1-B)d(1-Bs)Dyˆt-μ]=θq(B)ΘQ(Bs)atwhere B is a backward shift operator with Byt=yt−1 and Bat=at−1.yˆtis the value to be forecasted and atis the residual at time period t, μ is the overall mean of series which is a constant. ϕp(B)=1−φ1B−φ2B2−,…,−φpBpis a non-seasonal auto-regression of order p, θq(B)=1−ϑ1B−ϑ2B2−,…,−ϑqBqis a non-seasonal moving average of order q, ΦP(Bs)=1−Φ1Bs−Φ2Bs2−,…,−ΦPBsPis a seasonal auto-regression of order P, ΘQ(Bs)=1−Θ1Bs−Θ2Bs2−,…,−ΘQBsQis a seasonal moving average polynomial of order Q. The valueyˆtin Eq. (10) is a forecasting value. The best fitted seasonal ARIMA model can be automatically generated using SPSS (Version 19) software.The Winters’ multiplicative exponential smoothing (Douglas, Lynwood, & John, 1990) model is a popular time series forecasting method. Multiplicative decomposition considers the effects of seasonality to be multiplicative, which is, growing (or decreasing) over time. The model is presented in (11) below(11)yˆt=Tt×St×Lt+εtwhereyˆtis the forecasting at time t, Ttrepresents the trend component, Strepresents the seasonality and Ltis the long term cycles and ɛtis the error. This method requires at least two years of back data for forecasting. The Winter’s additive exponential smoothing model is not considered as an individual model in this study, because it is a special case of the Seasonal ARIMA. The best fitted Winters’ multiplicative exponential smoothing can be automatically generated using SPSS (Version 19) software.The Naïve 1 and Naïve 2 models (Chu, 2004; Oh & Morzuch, 2005) which are very popular models in tourism demand forecasting are adopted in this paper. A Naïve method simply states that future forecasts are equal to the most recently available value. The Naïve 1 model operates on the assumption that the number of tourists at time t,yˆtis the same as the value at time t−4 denoted by yt−4 and is described asyˆt=yt-4. The Naïve 2 model operates on the assumption that the number of tourists at time t,yˆtis equal to the value at time t−4 multiplied by a modification factor which includes the influence of the yt−8 (long range value) and can be written asyˆt=yt-4{1+(yt-4-yt-8)/yt-8}.In general, there are three linear combination methods available in the literature for tourism demand. These three linear combination methods studied by Shen et al. (2008) and Wong et al. (2007) are evaluated in this study. They are Simple Average (SA), Variance Covariance (VACO), and Discounted Mean Square Forecast Error (DMSFE) methods. Four individual models as inputs with one-step-ahead forecasting on these three linear combinations were evaluated by Wong et al. (2007), and seven individual models as inputs with multiple-step-ahead forecasting horizons were examined for these three linear combinations by Shen et al. (2008).The SA combination method can be expressed asY^tC=∑j=1Mwjyˆt(j)where wj=1/M,yˆt(j)is the forecast value (output) from the jth single forecasting model andY^tCis the combined forecast model at time t, M is the total number of individual forecasting models. In this simple average combination, each individual forecasting model makes an equal contribution (same weight) to the combined valueY^tCwith∑j=1Mwj=1. The VACO combination form is the same as the SA form, but the weight wjis defined aswj=∑i=1Nyi-yˆi(j)2-1/∑j=1M∑i=1Nyi-yˆi(j)2-1, where yiis the ith true target value of the training data set, andyˆi(j)is the ith forecasting value of the training data set from the jth individual forecasting model. N is the total number of the training data set. The DMSFE combination form is the same as the form of SA, but the weight wjis defined aswj=(∑i=1NβN-i+1(yi-yˆi(j))2)-1/∑j=1M(∑i=1NβN-i+1(yi-yˆi(j))2)-1. The VACO method is a special case of the DMSFE when β=1. β is chosen as 0.95, 0.9, 0.85 and 0.8, respectively which is the same as in the papers (Li et al., 2005; Song & Li, 2008) in this study. It is noted that wjcomputed in VACO and DMSFE also satisfies the constraint∑j=1Mwj=1.All the above combination methods, SA, VACO and DMSFE, are linear combinations as discussed by Shen et al. (2008) and Wong et al. (2007). The advantage of these combinations is that they are simple and easy to apply. The parameters (weights) are fixed and are easy to calculate from the data set.The International Passenger Survey is available from the Office for National Statistics, UK and provides information on UK tourism arrivals and expenditure according to country of origin and purpose of visit. Tourists passing through passport control are randomly selected for interview. The results are based on face-to-face interviews with samples of passengers as they enter or leave the UK. Quarterly (Q) UK inbound visit numbers for Q1 1993 to Q4 2007 extracted from the IPS are used for this study, since the financial and economic crises began in 2008. The tourism industry is affected predominantly by the factors which are weather effect, festival effect, calendar effect in both the origin and destination countries (Lim, 2001). Fig. 1shows that arrivals for holiday and study purposes have a high degree of seasonality, arrivals for business purpose has least degree of seasonality, and the degree of seasonality for arrivals of visit friend/relatively (VFR) purpose is in the middle of holiday/study and business purposes.It is imperative to test for the presence of unit roots and seasonal unit roots in univariate series. The commonly used unit-root tests are the Augmented Dickey–Fuller (ADF) test (Dickey & Fuller, 1979), the Phillips–Perron (PP) test (Phillips & Perron, 1988), and the Hylleberg–Engle–Granger–Yoo (HEGY) test (Hylleberg, Engle, Granger, & Yoo, 1990) for a hypothesis of a seasonal unit-root which determines the nature of seasonal variation in the series. For examples, the ADF test is applied by Goh and Law (2002) and the PP test is applied by Gounopoulos, Petmezas, and Santamaria (2012). The hypothesis ADF, PP and HEGY tests are presented in formula (12), and the results of the ADF, PP (using Eview) and HEGY (using R) are illustrated in Table 1.(12)ADF test:Xt=μ+γt+βXt-1+α1ΔXt-1+α2ΔXt-2+⋯+αpΔXt-p+etPP test:Xt=μ+γt+βXt-1+etHEGY test:(1-L4)yt=π1z1,t-1+π2z2,t-1+π3z3,t-2+π4z3,t-1+etwhere et∼N(0,σ2), z1,t=(1+L+L2+L3)yt, z2,t=−(1−L+L2−L3)yt, z3,t=−(1−L2)yt.ADF and PP tests:H0:β=1(series has a unit root)H1:β<1(series has no unit root)HEGY test (Ghysels, Lee, & Noh, 1994):H0:π2=π3=π4=0(series has a seasonal unit root)H1:π2<0,π3<0,π4≠0(series has no seasonal unit root)The time series is nonstationary if H0 is accepted, which has a unit root or seasonal unit root. Otherwise, it is stationary. The ADF and PP test results in Table 1 show that some series have unit roots at level. However there is no unit root (H1 is accepted at 1% significant level) with the first difference in all cases as expected. The rejection of H0 for the HEGY test means that the series does not have a seasonal unit root. The test results support the application of Box–Jenkin model—Seasonal ARIMA in this study.One to four quarters ahead forecasting from individual models that are described in the previous section are used for the optimal subset selection using the MI algorithm in this paper. The same process with the paper (Shen et al., 2008) is used for individual models generated here.The individual forecasting models are constructed based on the data from Q1 1993 to Q4 1997 inclusive (training data). The out-of-sample forecasts (test data) are generated for Q1 1998 to Q4 2007 inclusive with one to four quarters ahead forecasting using the following recursive forecasting techniques.Recursive forecasting:(1)Forecast one to four quarter ahead (Q1 1998 to Q4 1999) using the initial training data (Q1 1993 to Q4 1997).Forecast one to four quarter ahead (Q2 1998 to Q1 2000) using the enhanced training data (Q1 1993 to Q1 1998) by adding one data point (Q1 1998) to the training data set.Continue step 2) until forecast the last point of test data set (Q4 2007) using the enhanced training data (Q1 1993 to Q3 2007).The results from this process are 40 one quarter ahead forecasting values for each individual forecasting model, 39 two quarters ahead forecasting values, 38 three quarters ahead forecasting values and 37 four quarters ahead forecasting values that are generated from each individual model.There are nine individual models in total for this study, five SVR with different dimensions of inputs from 4 to 8, Naïve 1, Naïve 2, Seasonal ARIMA and Winters’ Multiplicative Exponential Smoothing models. There are 40 (Q1 1998–Q4 2007), 39 (Q2 1998–Q4 2007), 38 (Q3 1998–Q4 2007) and 37 (Q4 1998–Q4 2007) one to four quarters ahead forecasting values that are generated from each individual forecasting model. The first 24 (Q1 1998–Q4 2003), 23 (Q2 1998–Q4 2003), 22 (Q3 1998–Q4 2003) and 21(Q4 1998–Q4 2003) forecasting values (training data) from all individual forecasting models are used to select an optimal subset using the MI algorithm. The period from Q1 2004 to Q4 2007 (test data) is used to test this selected optimal subset. The framework of this case study is illustrated in the following (Fig. 2).

@&#CONCLUSIONS@&#
This paper has proposed a novel optimal subset selection approach from all available individual models using information theory. This optimal subset from individual models shows good performance and robustness in general. The optimal subsets significantly outperform the non-optimal combination of all individual models as inputs and also give similar performance to the best subsets of individual model in most cases.The assessment of finding an optimal subset using the MI theory reveals that we can avoid both using a combination of all individual models, and finding the optimal set by trying all possible combinations which involves huge calculations and is time consuming. The most important thing is that it is only an experiment by finding the optimal set using trying all possible combinations method. However, the proposed MI algorithm provides a theoretical approach for finding the optimal set. This paper reveals that the combination from the small size of individual models can achieve higher performance than the best individual model or the combination of all individual models. This significantly enhances the forecasts literature.The optimal subset selection using the MI algorithm is by nature a ‘heuristic’ approach. It provides us with a good solution, i.e. it may not give a unique solution and it may not guarantee that the optimal subset is the same as the best subset. However, the optimal subset of individual models using the MI algorithm shows robust and good performance.Two main results are observed: (1) The optimal subset forecast model performs statistically better than the combination model using all available individual models as inputs, and (2) the dimension of the optimal subset forecast model is in the range of two and five individual models. This research can help both government organizations and the tourism related industries, since accurate forecasting on tourism demand is critical for their policy and decision making. This can benefit the transportation, accommodation, catering, entertainment and retailing sectors. For examples, this research can help (1) make the appropriate government policies which can promote development of hospitality and tourism industries such as hotels, restaurants and attraction sites; (2) provide the guidance for both central governments and tourism related industries on capacity management, such as for the department of transportation on reducing congestion during the tourism seasons in order to achieve government ‘public service agreement targets’ (PSA targets). The tourism related industries can benefit to healthy run business by employing right number of staff and control business scale; (3) provide the guidance for both central governments and tourism related industries on investment such as airport, transport networks and tourism attraction sites.Time series individual forecasting models are used in this research. The causal econometric model using ‘GDP’ and ‘CPI’ influencing factors will be considered in the future research to see if the forecasting accuracy can be improved, and the combination selection algorithm can be enhanced.This paper only used the data up to 2007 inclusive. The up to date data will be used in the future to test the robustness of the combination selection algorithm proposed in this study, in particular the impact of the financial crisis on the forecasting performance.The linear combination methods are adopted in this research. The nonlinear combination methods can be applied to evaluate the combination selection algorithm.The future work can also consider to exam the combination selection algorithm for the other tourism data sets. Another issue is to see whether the dimension of the optimal subset is still in the range of two and five for using a large number of available individual models.