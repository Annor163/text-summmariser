@&#MAIN-TITLE@&#
Principal differential analysis for detection of bilabial closure gestures from articulatory data

@&#HIGHLIGHTS@&#
Electromagnetic articulographic data represented in task dynamics.Dynamics of lip movement represented by linear differential equations.Linear differential equations learned by principal differential analysis (PDA).Bilabial/non-bilabial classification using parameters of PDA with high accuracy.

@&#KEYPHRASES@&#
Bilabial closure gesture,Principal differential analysis (PDA),Articulatory data,Tract variables,Classification,

@&#ABSTRACT@&#
In this paper, a new statistical method for detecting bilabial closure gestures is proposed based on articulatory data. This can be surprisingly challenging, since mere proximity of the lips does not imply their involvement in a directed phonological goal. This segment-based bilabial closure detection scheme uses principal differential analysis (PDA) to extract articulatory gestures. The dynamic patterns of the tract variables (TVs) lip aperture, lip protrusion, and their derivatives, are captured with PDA and used to detect and quantify bilabial closure gestures. The proposed feature sets, which are optimized using sequential forward floating selection (SFFS), are combined and used in binary classification. Experimental results using the articulatory database MOCHA-TIMIT show the effectiveness of the proposed method demonstrating promising performance in terms of high classification accuracy (95%), sensitivity (95%), and specificity (95%).

@&#INTRODUCTION@&#
Bilabial closure often precedes sudden airflow out of the mouth, as in the plosive onset in pie, buy, or is concurrent with airflow out of the nose, as in the nasal onset in my (Lofqvist and Gracco, 2010). In either case, the resulting acoustics can be fairly similar to other phones in the respective classes, which can affect speech recognition (Rudzicz, 2011). However, articulatory features are robust across speakers, share commonalities (in many cases) across languages, and are generally insensitive to situational changes (Zhao et al., 2013). We propose a new scheme for detecting articulatory gestures based on principal differential analysis (PDA) and articulatory data. Here, gestures are directed reconfigurations of the vocal tract to achieve a discrete phonologically relevant goal, such as lowering the velum. For this purpose, we have focused on directed bilabial gestures in the MOCHA-TIMIT database (Wrench, 2000), which consists of electromagnetic articulography (EMA) data tracking the positions and velocities of point-sensors affixed to the articulators.Task dynamics is a combined model of skilled articulator motion and abstract vocal tract configuration (Saltzman and Munhall, 1989) that provides a coherent and biologically plausible model of speech production with consequences for phonology (Browman and Goldstein, 1986), neurolinguistics, and the evolution of speech and language (Goldstein et al., 2006). In this theory, tract variables (TVs) generally refer to the locations and degrees of vocal tract constrictions, as functions of time. Each gesture is a directed motion to complete some phonologically or acoustically relevant task within one of the following TVs: lip aperture (LA), lip protrusion (LP), tongue tip constriction location (TTCL) and degree (TTCD), tongue dorsum constriction location (TDCL) and degree (TDCD), velum (VEL), glottis (GLO), and lower tooth height (LTH). For instance, a gesture to close the lips would occur within the LA variable and would set that variable to zero. The dynamic influence of each gesture in time on the relevant tract variable is modeled by a non-homogeneous second-order linear differential equation mimicking a highly coupled spring-mass system. Typically, the coefficients of these systems have been set empirically by experts (Saltzman and Munhall, 1989), but with several exceptions. For example, McGowan (1994) used a genetic algorithm to recover task dynamic parameters from acoustic speech signals, and Lammert et al. (2013) use both artificial neural networks and locally-weighted regression to estimate kinematic relationships of speech production in task dynamics. Similarly, Howard and Huckvale (2005) train an inverse mapping between an articulatory synthesizer's control parameters and their auditory consequences, in a manner similar to work done with the DIVA system (Guenther and Perkell, 2004), and Nam et al. (2012) use an iterative analysis-by-synthesis procedure using time-warping in task dynamics to learn relevant parameters. Although clear differences exist between these efforts, in general their aims were to realistically estimate and simulate gestural dynamics given data. By contrast, our approach of using principal differential analysis (which is unique among this work) is primarily a means towards an end, namely the classification of articulatory features.

@&#CONCLUSIONS@&#
