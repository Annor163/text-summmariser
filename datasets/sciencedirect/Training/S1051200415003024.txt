@&#MAIN-TITLE@&#
A diffusion subband adaptive filtering algorithm for distributed estimation using variable step size and new combination method based on the MSD

@&#HIGHLIGHTS@&#
A subband adaptive filtering algorithm is adopted for distributed estimation.The upper bound of the intermediate MSD and its recursion with local MSD are investigated.The MSD-optimal step size achieves the fastest convergence rate on every iteration.The combination coefficients are assigned using the intermediate MSD.The algorithm achieves fast convergence rate and small steady-state errors.

@&#KEYPHRASES@&#
Distributed estimation,Adaptive filter,Diffusion subband adaptive filtering algorithm,Variable step size,Combination method,

@&#ABSTRACT@&#
This paper proposes a novel diffusion subband adaptive filtering algorithm for distributed networks. To achieve a fast convergence rate and small steady-state errors, a variable step size and a new combination method is developed. For the adaptation step, the upper bound of the mean-square deviation (MSD) of the algorithm is derived and the step size is adaptive by minimizing it in order to attain the fastest convergence rate on every iteration. Furthermore, for a combination step realized by a convex combination of the neighbor-node estimates, the proposed algorithm uses the MSD, which contains information on the reliability of the estimates, to determine combination coefficients. Simulation results show that the proposed algorithm outperforms the existing algorithms in terms of the convergence rate and the steady-state errors.

@&#INTRODUCTION@&#
The distributed estimation strategy has been widely used for many applications such as wireless and sensor networks since it is more robust but less computational comparing to a centralized estimation strategy [1–8]. It aims to adaptively estimate unknown parameters based on input and output data at every node in the distributed network, where each node is considered to operate as an individual adaptive filter. Recently, several real-time processing strategies for the distributed estimation were suggested in [9–15]. Among them, the diffusion LMS algorithm [10] suggested a scheme that makes all nodes share and fuse the estimates of their neighbor nodes by employing a linear combiner. The general version of this algorithm has been provided in [13], where Adapt-then-Combine (ATC) and Combine-then-Adapt (CTA) diffusion LMS algorithms are formulated that allow the higher level of information exchange. Several diffusion-type algorithms have been developed based on the ATC and CTA schemes to improve the performance in terms of the convergence rate and the steady-state estimation errors by adjusting step-size parameter in the adaptation step or assigning suitable coefficients for linear combiner in the combination step. The step size is an important parameter that determines a compromise between a fast convergence rate and a small steady-state estimation errors, and thus some researches solved the conflict between them by varying the step size of the DLMS algorithm [16–19]. These algorithms achieved improved performance comparing to the fixed step-size DLMS algorithm, but they suffer from a deteriorated convergence rate when the input signal is highly correlated. Thus, there still remain issues for developing algorithms with better performance under highly correlated input signals. Besides the step size parameter, the combination coefficients for linear combiner can affect the performance of the algorithm. Recently, various approaches to determine the combination coefficients have been proposed in [10–14,20–23]. Most papers that deal with diffusion networks adopt the static combination method suggested in [10–14], where the combination coefficients are determined based on the network topology solely. Since the network topology cannot absolutely represent the reliability of each node, some adaptive combination methods have been proposed in [22,23]. These adaptive combination methods are advantageous under various signal-to-noise ratio (SNR) conditions comparing to the static combination methods. However, they require rather high computational complexity and still carry out suboptimal performance when different step sizes or highly correlated input signals are used across the nodes.In this paper, we propose a variable step-size scheme and a combination method for a diffusion subband adaptive filtering algorithm to improve the performance in the aspect of the convergence rate and the steady-state estimation errors regardless of the characteristics of input signals. The proposed algorithm consists of two steps; the adaptation step for intermediate updates and the combination step for local updates. In the adaptation step, the subband adaptive filtering algorithm [24] is adopted for the intermediate weight estimation because it is beneficial in a sense that it improves the convergence rate by whitening the input and output signals. Also, the upper bound of the intermediate mean-square deviation (MSD) is derived and minimized to decide the step-size parameter, an important factor that determines the compromise between the fast convergence rate and the small steady-state estimation errors, so as to achieve the largest decrease of the MSD on every iteration. In the combination step, a novel combination method is proposed that interprets the MSD values as the reliability indicators of the estimates at each node. The combination coefficients are determined inversely proportional to the intermediate MSDs of neighboring nodes so as to assign the more weight to the more reliable node. Based on the proposed combination coefficients, the local MSD is derived as the harmonic sum of the intermediate MSDs of neighboring nodes. Simulation results show that the proposed algorithm achieves improved performance in terms of the convergence rate and the steady-state estimation errors in various environments.Fig. 1shows a distributed network consisting of N nodes. Each node k hasNkas its neighbor that is a set of nodes connected to the node k including itself and observes the input vectoruk,n=[uk,n,uk,n−1,⋯,uk,n−M+1]Tand the output signaldk,n=uk,nTwo+vk,nfor every time instant n, wherewois an unknownM×1weight vector andvk,nis a measurement noise. Every node can exchange their information with neighbor nodes. If we assume that only the intermediate estimates at each node are exchanged without measurement data, an ATC type of DLMS [13] algorithm intermediately estimates thewoas(1)ψk,n=wk,n−1+μk,nuk,n(dk,n−uk,nTwk,n−1),whereμk,nis the step-size parameter andψk,nandwk,n−1are the intermediate and the local estimates of the unknown weight for node k, respectively.Instead of the least-mean-square update, the subband adaptive filtering structure [24] can be considered in the adaptation step. The desired signaldk,n, the input signaluk,n, and the outputyk,n=uk,nTwk,n−1are partitioned intoNssubband signalsdl,k,n,ul,k,nandyl,k,n, respectively, through the analysis filters forl=0,1,⋯,Ns. Then, they are critically decimated to a lower sampling rate corresponding to their reduced bandwidth. If we set time index for the decimated signal as i, the decimated desired signal and the decimated output signal can be represented asdl,k,i=ul,k,iTwo+vl,k,iandyl,k,i=ul,k,iTwk,i, respectively, whereul,k,i=[ul,k,iNs,ul,k,iNs−1,⋯,ul,k,iNs−M+1]Tandvl,k,iis a decimated measurement noise. Then, the intermediate weight update equation at node k and time i becomes(2)ψk,i=wk,i−1+μk,i∑l=0Ns−1ul,k,iul,k,iTul,k,i(dl,k,i−ul,k,iTwk,i−1),whereNsdenotes the number of analysis filters that partition the incoming signal intoNssubband signals to make each of them occupy a portion of the original frequency band.In a combination step, the local weight vector at node k is updated through the estimation exchange with neighbor nodes as(3)wk,i=∑j∈Nkaj,kψk,i,whereaj,kis the combination coefficient that satisfiesaj,k≥0and∑j=1Naj,k=1. The coefficients are determined by reflecting the network characteristics and Table 1shows the examples of the existing combination methods. The parameternkdenotes the number of neighbor nodes inNkandσvk2represents the variance of the measurement noise at node k.In this section, we adopt the variable step-size scheme to the adaptation step in (2) to achieve fast convergence rate and small steady-state estimation errors. The upper bound of the intermediate MSD is analyzed in advance, where the step size is derived by minimizing the MSD by extending the work of [25] and [26] to the distributed situation. Furthermore, we propose a new combination method to assign the combination coefficients in (3) by utilizing the MSD as a reliability indicator.In this section, the upper bound of the intermediate MSD is analyzed, where the MSDs for the intermediate and local estimates are defined as(4)MSDkw=E(w˜k,iTw˜k,i|Dl,k,i∪Ul,k,i)=Tr(Pk,iw)≜pk,iw,(5)MSDkψ=E(ψ˜k,iTψ˜k,i|Dl,k,i∪Ul,k,i)=Tr(Pk,iψ)≜pk,iψ,wherePk,iw=E(w˜k,iw˜k,iT|Dl,k,i∪Ul,k,i),Pk,iψ=E(ψ˜k,iψ˜k,iT|Dl,k,i∪Ul,k,i),Dl,k,i={dl,k,j|0≤j<i},Ul,k,i={ul,k,j|0≤j<i}, andψ˜k,i=wo−ψk,iandw˜k,i=wo−wk,iare the intermediate and local weight-error vectors, respectively. The input vectors are considered to have deterministic quantities when dealing withPk,iwandPk,iψbecause they are conditional expectation concerning toul,k,i.Developing (2) in terms ofψ˜k,iandw˜k,i, it is described as(6)ψ˜k,i=w˜k,i−1−μk,i∑l=0Ns−1ul,k,iul,k,iTul,k,i(dl,k,i−ul,k,iTwk,i−1).To extractw˜k,i−1terms in (6), we can rewrite it as(7)ψ˜k,i=(I−μk,i∑l=0Ns−1ul,k,iul,k,iTul,k,iTul,k,i)w˜k,i−1−μk,i∑l=0Ns−1ul,k,ivl,k,iul,k,iTul,k,i,where it is assumed thatvl,k,iandvm,k,iforl≠mare independent according to [24]. Multiplying both sides of (7) with its transpose and neglecting the dependencies betweenw˜k,i−1andvl,k,i, the following relation is obtained:(8)Pk,iψ=Fk,iPk,i−1wFk,iT+μk,i2∑l=0Ns−1σvl,k2ul,k,iul,k,iT(ul,k,iTul,k,i)2,whereFk,i=I−μk,i∑l=0Ns−1ul,k,iul,k,iTul,k,iTul,k,iandσvl,k2is variance ofvl,k,i. Taking trace of both sides of (8) leads to(9)Tr(Pk,iψ)=Tr(Fk,iTFk,iPk,i−1w)+μk,i2∑l=0Ns−1σvl,k2ul,k,iTul,k,i.According to [24],ul,k,iTum,k,iforl≠mis negligible because it is much smaller than theul,k,iTul,k,i. Therefore, the value ofTr(Fk,iTFk,iPk,i−1w)can be calculated according to [24] and [27](10)Tr(Fk,iTFk,iPk,i−1w)=Tr((I−μk,i∑l=0Ns−1ul,k,iul,k,iTul,k,iTul,k,i)T×(I−μk,i∑l=0Ns−1ul,k,iul,k,iTul,k,iTul,k,i)Pk,i−1w),=Tr(Pk,i−1w−2μk,i∑l=0Ns−1ul,k,iul,k,iTul,k,iTul,k,iPk,i−1w+μk,i2∑l=0Ns−1ul,k,iul,k,iTul,k,iTul,k,iPk,i−1w),=Tr(Pk,i−1w)+(μk,i2−2μk,i)∑l=0Ns−1ul,k,iTPk,i−1wul,k,iul,k,iTul,k,i,≤Tr(Pk,i−1w)+(μk,i2−2μk,i)Nsλmin(Pk,i−1w),whereλmin(Pk,i−1w)is the minimum eigenvalue ofPk,i−1w. Sinceλmin(Pk,i−1w)is equal to or less thanTr(Pk,i−1w)/M, it can be replaced withTr(Pk,i−1w)/βM, whereβ≥1is a positive constant. Thus, (9) can be written as(11)pk,iψ≤pk,i−1w+(μk,i2−2μk,i)Nspk,i−1wβM+μk,i2∑l=0Ns−1σvl,k2ul,k,iTul,k,i.Remark 1The decimated noise varianceσvl,k2may not be available in practice. This problem can be solved by adopting the noise power estimation method stated in [28]. The estimate of the decimated noise varianceσˆvl,k2can be calculated withσˆvl,k2=σˆel,k2−rˆl,k,iTrˆl,k,iσˆul,k2,whereσˆel,k2,rˆl,k,i, andσˆul,k2can be estimated iteratively byσˆel,k2←ασˆel,k2+(1−α)el,k,i2,rˆl,k,i←αrˆl,k,i+(1−α)ul,k,iel,k,i,σˆul,k2←ασˆul,k2+(1−α)ul,k,iNs2.The optimal step size for each node at instant i can be obtained by differentiating both sides of (11) with respect toμk,i. Setting this derivative equal to zero, the optimal step size becomes(12)μk,i=Nspk,i−1wβMNspk,i−1wβM+∑l=0Ns−1σvl,k2ul,k,iTul,k,i.If we consider the upper bound of bothpk,iψandpk,i−1w, the equality of (11) holds. Substituting the step size (12) to (11) leads to(13)pk,iψ=(1−μk,iNsβM)pk,i−1w,whereμk,iandNssatisfy0<μk,i≤1andNs<<M.In the combination step, the local weightwk,iis determined with convex combination ofψk,iaccording to (3). Since determining the appropriate combination coefficients is another way of improving the performance of the diffusion algorithms, we suggest the combination method that operates with the MSD recursion in (11).The intermediate MSD,pj,iψ, offers how the intermediate estimate is close to thewo, and therefore, the combination coefficient forψj,ican be determined inversely proportional topj,iψ. To satisfy∑j=1Naj,k,i=1, the coefficients are defined as(14)aj,k,i={(∑m∈Nk(pm,iψ)−1)−1(pj,iψ)−1forj∈Nk,0forj∉Nk.To extract the relation betweenpk,iwandpj,iψ, (3) is developed in terms ofw˜k,iandψ˜j,iby subtractingwofrom both sides, then it becomes(15)w˜k,i=∑j∈Nkaj,k,iψ˜j,i.MultiplyingwoTto both sides of (15) and taking expectation lead to(16)E(w˜k,iwoT)=E(∑j∈Nkaj,k,iψ˜j,iwoT).Substitutingw˜k,i+wk,iandψ˜k,i+ψk,ito thewoin both sides of (16) leads to(17)E(w˜k,i(w˜k,i+wk,i)T)=E(∑j∈Nkaj,k,iψ˜j,i(ψ˜j,i+ψj,i)T).The intermediate and the local weight errorsψ˜k,iandw˜k,ican be assumed to be orthogonal to the intermediate and the local estimatesψk,iandwk,i, respectively, according to the orthogonal principle generally used in estimation theory, and then the diagonal terms ofE(w˜k,iwk,iT)andE(ψ˜k,iψk,iT)are negligible. Therefore, we can attain the relation betweenpk,iwandpj,iψforj∈Nkby taking trace to both sides of (17) and applying the combination coefficients in (14) as(18)pk,iw=∑j∈Nkaj,k,ipj,iψ,(19)=nk∑m∈Nk(pm,iψ)−1.Remark 2The proposed algorithm mainly operates with the recursion ofpk,iψin (11), which interacts with the optimal step sizeμk,iin (12) and the local MSDpk,iwin (19). These updates are interactive, and thus they operate jointly. Table 2summarizes the proposed algorithm.In this section, the mean convergence analysis of the proposed algorithm is presented.To interpret the mean convergence performance of the proposed algorithm, the following quantities are defined.Ψi≜col{ψ1,i,ψ2,i,⋯,ψN,i},Wi≜col{w1,i,w2,i,⋯,wN,i},W˜i≜col{w˜1,i,w˜2,i,⋯,w˜N,i},Mi≜diag{μ1,i,μ2,i,⋯,μN,i}⊗IM,Ai≜Ai⊗IM,Ai≜[a1,1,i⋯a1,N,ia2,1,i⋯a2,N,i⋮⋮aN,1,i⋯aN,N,i],Di≜col{d1,i,d2,i,⋯,dN,i},Ui≜diag{U1,i,U2,i,⋯,UN,i},Vi≜col{v1,i,v2,i,⋯,vN,i},dk,i≜[d1,k,i,d2,k,i,⋯,dNs,k,i],Uk,i≜[u1,k,i,u2,k,i,⋯,uNs,k,i],vk,i≜[v1,k,i,v2,k,i,⋯,vNs,k,i],where ⊗ denotes the Kronecker product. According to the diagonal assumption generally used in the subband adaptive filtering algorithm [24], the approximationUk,iTUk,i≈diag{‖u1,k,i‖2,‖u2,k,i‖2,⋯,‖uNs,k,i‖2}is established. Then, the desired signal can be written asDi=UiTW˜i+Viand the proposed algorithm can be represented with the followings:(20)Ψi=Wi−1+MiUi(UiTUi)−1(UiTW˜i−1+Vi),(21)Wi=AiTΨi.Rewriting the above equations with the weight error, we can obtain the following relation:(22)W˜i=AiT(I−MiUi(UiTUi)−1UiT)W˜i−1−AiTMiUi(UiTUi)−1Vi.To analyze the mean convergence behavior, the expectations to both sides of (22) should be evaluated. For the tractable analyses, several assumptions on the signal are adopted.Assumptions1. Both the step size and the combination coefficients are independent of the input signal, the measurement noise, and the weight error.2. The noisevl,k,iis i.i.d. zero-mean in time with varianceσvl,k2and spatially independent. Also, it is assumed to be independent of the input signals.These assumptions cannot really hold for the time-varying step size and combination coefficients, but they are generally used in adaptive filtering because the results are proved to match well to the empirical results [19,22].Under these assumptions, taking the expectation of both sides of (22) leads to(23)E(W˜i)=E(AiT)(IMN−Gi)E(W˜i−1),whereGi=E(MiUi(UiTUi)−1UiT).Using the block maximum norm introduced in [22], (23) is proceed to the following inequality.(24)‖E(W˜i)‖b∞≤‖E(AiT)‖b∞‖(IMN−Gi)‖b∞‖E(W˜i−1)‖b∞,where‖⋅‖b∞denotes the block maximum norm that is defined by‖x‖b∞=max1≤k≤N⁡‖xk‖for the vectorx∈RMNthat consists ofx=col{x1,x2,⋯,xN}withxm∈RMand the matrix norm induced from the block maximum norm is defined as(25)‖A‖b∞≜maxx≠0⁡‖Ax‖b∞‖x‖b∞.If the inequality(26)supi≥0⁡{‖E(AiT)‖b∞‖(IMN−Gi)‖b∞}<1holds, then we can conclude that‖E(W˜i)‖b∞→0and the proposed algorithm converges. One sufficient condition for (26) is‖E(AiT)‖b∞≤1and‖(IMN−Gi)‖b∞<1for alli≥0.It is proved that the condition‖E(AiT)‖b∞≤1is ensured when{aj,k,i}j∈Nkis a convex combination for allk∈{1,2,⋯,N}andi≥0[22]. As we assign the combination coefficientsaj,k,iwith convexity,‖E(AiT)‖b∞≤1always holds for the proposed combination method.Now then, let us consider the requirement for the‖(IMN−Gi)‖b∞<1for alli≥0. Considering the assumption that the step sizes are independent of input signals, it follows that(27)‖(IMN−Gi)‖b∞=max1≤k≤N⁡‖IM−μ¯k,iRuk‖,(28)=max1≤k≤N⁡max1≤m≤M⁡|1−μ¯k,iλm(Ruk)|<1,whereμ¯k,idenotes the expectation of the step size parameter,λm(Ruk)is eigenvalue ofRukfor1≤m≤M, andRuk=E(Uk,i(Uk,iTUk,i)−1Uk,i)=E(∑l=0Ns−1ul,k,iul,k,iTul,k,iTul,k,i). (28) is established if the inequality(29)0<μ¯k<2/λmax(Ruk)is satisfied for allk=1,2,⋯,N, whereλmax(Ruk)is the largest eigenvalue ofRuk. For large M, the fluctuations of subband energy‖ul,k,i‖2from one iteration to the next will be small enough, then the following approximation is justified [24,28]:(30)E(∑l=0Ns−1ul,k,iul,k,iTul,k,iTul,k,i)≈∑l=0Ns−1E(ul,k,iul,k,iT)E(ul,k,iTul,k,i),(31)=∑l=0Ns−1E(ul,k,iul,k,iT)Mσul,k2,whereσul,k2denotes the variance of theul,k,iNs. Because the spectrum of each input signal in the subbands is closer to that of the white noise [24], it can be approximated thatE(ul,k,iul,k,iT)=σul,k2IM. Then,λmax(Ruk)becomesNs/Mthat is less than 1.Since the proposed step size always satisfies0<μk,i<1for allk=1,2,⋯,Nandi≥0, its expectation is also in the same range,0<μ¯k<1. Therefore, the condition0<μ¯k<2/λmax(Ruk)is satisfied and the mean convergence of the proposed algorithm is established.

@&#CONCLUSIONS@&#
In this paper, the diffusion subband adaptive filtering algorithm with the variable step size and a new combination method was proposed. In the adaptation step, we derived the optimal step size by minimizing the upper bound of the intermediate MSD of the filter to achieve the largest decrease of the MSD on every iteration. Thus, the fast convergence rate and the small steady-state estimation errors could be established. Furthermore, in the combination step, the novel combination method was suggested that interprets the MSD as information on the reliability of the estimates at each node. It enables to reflect information on the noise variance as well as input characteristics when assigning combination coefficients. Simulation results showed that the proposed algorithm had achieved better performance than the competing algorithms regardless of the input characteristics.