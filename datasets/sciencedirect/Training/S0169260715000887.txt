@&#MAIN-TITLE@&#
Polyp-Alert: Near real-time feedback during colonoscopy

@&#HIGHLIGHTS@&#
We surveyed recent research in polyp detection in screening colonoscopy.We introduced a fast polyp detection algorithm cable of running at 10 frames per second on an off-the-shelf workstation. It correctly detects 97.7% of polyp shots in our data set.We evaluated our technique on 53 video files of procedures performed using Olympus and Fujinon scopes.Real-time polyp detection and feedback potentially help to reduce polyp miss rates to improve quality of care and quality of documentation.The proposed polyp detection algorithm is fast, effective, and potentially useful for improving quality of colonoscopy.

@&#KEYPHRASES@&#
Near Real-time,Polyp detection,Colonoscopy,Medical imaging/video,

@&#ABSTRACT@&#
We present a software system called “Polyp-Alert” to assist the endoscopist find polyps by providing visual feedback during colonoscopy. Polyp-Alert employs our previous edge-cross-section visual features and a rule-based classifier to detect a polyp edge—an edge along the contour of a polyp. The technique employs tracking of detected polyp edge(s) to group a sequence of images covering the same polyp(s) as one polyp shot. In our experiments, the software correctly detected 97.7% (42 of 43) of polyp shots on 53 randomly selected video files of entire colonoscopy procedures. However, Polyp-Alert incorrectly marked only 4.3% of a full-length colonoscopy procedure as showing a polyp when they do not. The test data set consists of about 18h worth of video data from Olympus and Fujinon endoscopes. The technique is extensible to other brands of colonoscopes. Furthermore, Polyp-Alert can provide as high as ten feedbacks per second for a smooth display of feedback. The performance of our system is by far the most promising to potentially assist the endoscopist find more polyps in clinical practice during a routine screening colonoscopy.

@&#INTRODUCTION@&#
Colorectal cancer is the second leading cause of cancer-related deaths, claiming close to 50,000 lives annually in the United States [1]. Colonoscopy is an important screening tool that has contributed to a significant decline in colorectal cancer-related deaths. During colonoscopy, a tiny video camera at the tip of the endoscope generates a video signal of the internal mucosa of the human colon. The video data is displayed on a monitor for real-time diagnosis by the endoscopist. Abnormalities like polyps can be found and removed during the colonoscopy. Despite the success of colonoscopy in lowering cancer-related deaths, a significant average miss rate for detection of both large polyps and cancers exists and is estimated to be around 4–12% based on several reports [2–4]. In recent years, many computer-aided polyp detection techniques have been developed with the ultimate goal to assist the endoscopist in lowering the polyp miss rate. However, automatic polyp detection in recorded video data during colonoscopy (or colonoscopy video) is very challenging due to the noisy nature of endoscopic images caused by camera motion, strong light reflections, the wide-angle lens that cannot be automatically focused, and the location and appearance variations of polyps within the colon.Colon polyps are generally classified based on their appearance as pedunculated, sessile, or flat. Sessile polyps grow on the surface of the colon without a stalk. Pedunculated polyps have short or long stalks. Flat polyps grow along the surface of the colon. In general, sessile polyps, the head of pedunculated polyps, and flat polyps have ellipse shape when small. Some polyps may grow into cancer. Complete removal of polyps during colonoscopy can prevent them from possibly growing into colon cancer. Polyps vary in their appearance, shape, size, amount of protrusion, and location in the colon. The same polyp may appear very differently in different images due to amount of colon insufflation, degree of colon muscular contraction, viewing angle, and distance from the capturing camera (Fig. 1, rows 1 and 2). Stool or therapeutic instruments may occlude polyps (Fig. 1, row 3).Due to the complexity of appearance of polyps and characteristics of colonoscopy video, existing techniques to detect an image containing a polyp or to mark a polyp region in an image require long processing time. The state-of-the-art technique took about 3 to 4s per image in a recent survey [5–20]. Furthermore, a detection rate (recall) of close to 90% generally comes with a high false alarm rate of at least 1 false region per image [19]. High false alarm rates and long analysis times are inadequate for real-time feedback of polyp regions during colonoscopy as part of an automated method to reduce polyp miss rates.Our contributions in this paper are threefold. First, we present a comprehensive survey of existing literature on polyp detection techniques in colonoscopy. Several polyp detection techniques in colonoscopy were developed in recent years and were nicely summarized in [21]. We added to the previous survey result [21] with information about techniques developed after the survey report along with additional qualitative performance criteria. Most of these techniques have not been quantitatively compared against each other due to lack of common ground truth, common performance metrics, and detailed implementation of the algorithms for reimplementation to conduct a fair comparison. Second, we designed algorithms and developed software modules for fast polyp edge detection and polyp shot detection. Third, we developed the Polyp-Alert software system that analyzes colonoscopy video for polyps and shows visual feedback of detected polyp edges on screen. We tested Polyp-Alert on a 64-bit Intel (R) Xeon 2.0GHz duo-core CPU with 6.0GB RAM running 64-bit Windows 7 Professional and similar systems. Polyp-Alert is able to continuously analyze image streams and identify a detected polyp edge at the rate of 10 frames per second (fps) for a typical NTSC frame rate of 29.97fps: i.e., the technique analyzes one of every three frames. Existing polyp detection algorithms are too complex to achieve this analysis rate. The fast analysis time means less latency for feedback. Ideally, we want to achieve real-time analysis within one frame time interval to have instant visual feedback. Our technique is by far the most promising given the combination of speed and accuracy. It may help in recognition of polyps for both novice as well as experienced endoscopists, especially at the edges of the image. It also may help when the endoscopist is distracted or fatigues. In addition, real-time or near real-time polyp shot detection will also enable automated real-time documentation for the endoscopist who will simply verify the correctness at the end of the procedure.The remainder of this paper is organized as follows. We discuss the related work of both polyp image detection and shot segmentation/extraction algorithms in Section 2. We present our polyp image detection technique in Section 3. We present polyp tracking/warning in Section 4. We experimentally evaluate our system in Section 5. We conclude the paper and describe the future work in Section 6.Our aim is to detect polyps in colonoscopy images from standard endoscopes in routine colonoscopy practice. There are polyp detection techniques proposed for images from magnifying endoscopes or Narrow-Band Imaging (NBI) [22,23]. NBI or magnifying endoscopes are not typically used for the entire procedure. Note that pit patterns [23] of polyps are not necessarily seen using standard scopes; indeed, detection of pit patterns requires close up inspection which happens after the endoscopist has identified a lesion as a potential polyp. A number of research works have investigated polyp detection for Colon Capsule Endoscopy (CCE) images as summarized in [24–30]. In CCE, A small capsule consists of an imaging device and wireless transmitter which is ingested by the patient. The images are transmitted to the wireless receiver carried by the patient. Images are read by the domain expert after the end of the procedure to look for abnormalities. Treatment cannot be done during CCE. Therefore, anytime a polyp is found the patient needs to undergo a colonoscopy. In addition, the capsule moves according to the organ movement. It cannot be controlled to look at the entire colon wall.We define some terminologies to be used hereafter. A polyp image is an image showing a polyp. A polyp shot is a group of consecutive images in the video showing the same polyp(s). Multiple polyps in the same frame are considered in the same shot. Some frames in a polyp shot may not have any polyps in them due to temporary fast camera movement that causes blurry frames or the polyp is briefly out of the field of view of the camera before it reappears in the field of view. We define a polyp edge as a binary edge on the contour of a polyp.In recent years, many algorithms for computer-aided automatic polyp detection have been proposed for images from standard endoscopes [5,6,8,11,21,31]. Many find a polyp region and polyp images while a few detect polyp shots. The commonality among these techniques is pre-processing that discards noise, feature extraction that extracts discriminative features (e.g., texture, shape, color), and machine-learning classification on the features to identify polyp regions, images, or shots. These techniques take long processing time as a tradeoff to effectiveness, making them unsuitable for an application that assists the endoscopist during an ongoing procedure. Table 1shows the qualitative performance comparison.The average analysis times in the AT column are the times reported in the original publications. Since the experimental settings (e.g., image resolution, operating systems, implementation programming language, machine capability) are different, the reported times of these techniques cannot be directly compared with each other. However, the information is useful as a rough guide to indicate the room for improvement in terms of analysis speed for each technique in order to provide feedback during a live procedure. Ideally, the best case is to finish the analysis of the current image frame before the next frame arrives. That is about 1/30s. for NTSC and 1/25s. for PAL signals. However, Table 1 shows that the current solutions are not able to achieve this stringent time requirement. For some techniques, the times were not reported in the original papers. Of all the existing techniques in Table 1, only the work of Hwang et al. [5] mentions polyp shot detection. The analysis time was not reported. The technique is inherently slow due to the polyp region detection step that uses a watershed image segmentation technique.Our previous work published in 2013 [34] compared our part-based multi-derivative edge cross-section profile (ECSP) features with Local Binary Pattern (LBP) and Opponent Color LBP (OCLBP) [6]. OCLBP was shown to perform best on over 15,000 images [6]. LBP is a popular texture feature for polyp detection [6][20]. An LBP pattern of a pixel is represented by a binary sequence of length L. The lth position in the binary sequence is assigned a 0 if the pixel value is smaller than the value of its lth neighboring pixel, or assigned a 1 otherwise. An LBP pattern is mapped to a unique LBP value. A histogram of LBP values is used as an LBP feature of an image.An OCLBP feature [6,20] is a concatenation of three intra-channel LBP histograms and six inter-channel LBP histograms. Each of the three intra-channel LBP histograms is obtained from pixels in each of the RGB color channels, respectively. Let Cidenote channel i where i=1, 2, and 3. The six inter-channel LBP histograms (C1–C2, C1–C3, C2–C1, C2–C3, C3–C1, and C3–C2) are obtained from every combination of two different color channels. For example, the LBP pattern of C1–C2 is obtained from comparing the pixel value in the red color channel (C1) with values of its neighboring pixels in the green color channel (C2).Our ECSP features outperform LBP and OCLBP for polyp image detection in both analysis speed and accuracy when tested on 42 distinct polyps. This is the largest number of distinct polyps we have seen in the literature thus far. The free-response receiver operating characteristic curves (FROC) show the comparison of our technique with LBP and OCLBP. Under the same true positive rate, the average false alarm rate of our technique is under 0.5, which is significantly lower than those of LBP and OCLBP. Furthermore, false regions generated by our technique occur within only 20.5% of non-polyp images. However, the false regions generated using LBP and OCLBP are almost equally distributed on different types of non-polyp images. As a result, nearly every non-polyp image (99.8%) has at least one false region. Compared with these sliding-window based methods, our method detects polyp edges, which reflects more accurate locations of the polyps. Thus, the results of our technique are more meaningful in clinical practice. Our technique took 7.1s to process one image implemented using MATLAB. From Table 1, our technique [34] is by far the most promising one to provide feedback in real time. Tajbakhsh et al. [35] presented an edge like descriptor based on the similar principle as our work in Ref. [34]. The technique detected polyp edges by sliding a square patch on the image, extracting features based on the pixel information surrounding the edge normal directions and classifying features using Random Forest classifier. The technique in [35] implemented a sampler edge descriptor based on our edge normal and cross-section property in Ref. [34]. However, the technique does not directly compare with our technique. The technique was tested on only 150 test images and the analysis time was not reported. In this paper, we focus on improving our technique in [34] in terms of analysis speed while maintaining high recall and low false alarms in order to provide near real-time visual feedback.A video shot is a sequence of consecutive frames captured under a single recording operation. Frames within the same shot usually have similar visual properties. Shot segmentation divides a long video file into a number of shots and outputs the start and end frames of these shots. The criterion for grouping frames into a shot is typically based on low-level image features (e.g., color or intensity histograms, significant changes in locations of edges), and temporal information [36–38]. Sharp changes between shots are easier to detect than gradual changes such as wipes, fade-in/out, and dissolves. Shot segmentation techniques operating on compressed video directly have been proposed [39] to reduce processing time.As we intend to use the software to process uncompressed video from a capturing device during colonoscopy, we focus on the related work of shot segmentation algorithms on uncompressed video and divide them into three major categories: (1) thresholding-based algorithms, (2) clustering-based algorithms, and (3) model-based algorithms.(1)Thresholding-based algorithms: A similarity score is calculated between two consecutive frames to measure the similarity of the frames. The similarity score is generally calculated from low-level image features such as pixel intensity values, average intensity values of block-based regions, and color histograms. Pixel comparison methods compute the difference of intensity values between two pixels at the same location in two different images [40]. The average difference of intensity values over all pixels is used to indicate the similarity of the two frames. A cut is detected if the difference is larger than a threshold. Many alternative approaches can be used to compute the similarity score. Block-based comparison methods [41,42] divide an image into overlapping or non-overlapping blocks and calculate similarity scores among corresponding blocks between two images. The average score of these similarity scores between two images is used to compare with a threshold to find a shot cut. Comparing to the pixel comparison, the block-based comparison considers the overall content between images and it is more robust to small camera or object motion. However, the block-based comparison methods are still sensitive to large motion of camera or objects. Histogram comparison computes the distribution of intensity values represented by histogram bins. The differences of histogram bin values between two frames are used as a similarity score. Histogram comparison is less sensitive to motion of camera or objects. Threshold selection is an important factor that affects effectiveness of these techniques. Thresholds are typically determined empirically based on training data sets. Hanjalic et al. [37] proposed a statistical method that determines threshold values automatically for different types of videos.Clustering-based algorithms: In Ref. [43] an unsupervised learning algorithm is used to cluster frames into shots. Many features can be used for clustering such as intensity or color histograms. Compared to threshold-based algorithms, clustering-based algorithms do not have shortcomings in selecting an appropriate threshold. However, they do not perform well for detecting gradual transitions [39].Model-based algorithms: These algorithms either define or train underlying statistical models for classifying consecutive video frames into shot boundary and non-shot boundary frames. Example statistical models are the probabilistic distribution of pixel intensity values [44], and the hidden Markov model for modeling states of frames in shots [45]. Thresholds are not required in the model-based algorithms. However, these models need to be pre-trained. A suitable model may be difficult to find for some types of videos.Traditional shot segmentation techniques are not suitable for colonoscopy video because of the following reasons. (1) Color information is not as useful since colonoscopy images have limited color ranges of mostly red; with stool, the red color is mixed with green or yellow. (2) There is no significant movement of a specific object of interest in the image, but small movement of the camera can cause rotation, translation, and scaling of the whole image. (3) Strong light reflection, low illumination and blurriness cause significant changes in low-level features, which makes it difficult to group them into one shot. Robust shot segmentation methods that account for the monotone image pixel color property and the effect of camera motion such as strong light reflection and partially blurred images are desirable for segmenting colonoscopy video.We previously introduced shot segmentation for colonoscopy video in [46,47]. In Ref. [46], we segment a colonoscopy video into shots using the global histogram comparison in the threshold-based approach. In Ref. [47], we extract operation shots, each showing the cable part of a therapeutic or biopsy instrument in the field of view of the camera. In this paper, as we focus on detecting the appearance of a specific object, a polyp, in a video, we utilize an edge tracking algorithm to track the detected polyp edges for polyp shot extraction.Our polyp image detection centers on detecting whether an edge is of a polyp. If the image has at least one detected polyp edge, it is considered as having a polyp in it. The polyp edge detection algorithm mainly relies on edge features obtained from our part-based multi-derivative edge cross-section profile (ECSP) [34]. In this section, we briefly review our part-based multi-derivative ECSP and propose ECSP-based features for near real-time polyp image detection.The edge cross-section profile is calculated as a function obtained from the surrounding regions of a detected Canny edge in the image [34,51]. Fig. 2illustrates an example ECSP of a polyp and associated features. We do not detect the stalk of polyps as not all polyps have stalks. For most polyps, we found that the average brightness intensity gradually drops near the edge of the polyp and increases quickly and significantly in the protruding part and levels out. See Fig. 2(d). Other objects do not exhibit this property consistently. Besides polyp detection, we have also applied ECSP for appendiceal orifice detection [50,51] and retroflexion detection [48,49] in colonoscopy video. The ECSP function of an edge represents the average intensity values of its surrounding regions along the directions perpendicular to the edge pixels. To detect a target object in an image, we first obtain its Canny edge, calculate the edge cross-section profile, and finally extract features from its multi-derivative functions: the ECSP function, its 1st order derivative function, and its 2nd order derivative function for classification. To obtain a representative set of features for classification, each multi-derivative function is segmented into parts on some key positions, for example, zero-crossing positions on the function. Features are calculated on the segmented parts by modeling these parts differently using the methods suitable for individual parts. Because this approach allows different parts to be modeled by different methods or have different features, ECSP-based features can effectively describe the complex properties of polyps. The example features for polyp image detection in [34] include the shape of the segmented parts on edge profiles, texture of the polyp region, estimated polyp protrusion, smoothness of polyp surface, etc. The performance using ECSP-based features on over 800 images of 42 distinct polyps outperforms LBP and OCLBP, the best features for this problem in terms of both accuracy and analysis time. Interested readers are referred to the detailed descriptions in [34,51].Table 2shows all the features we use for polyp edge detection. Most of these features are obtained from our part-based multi-derivative ECSP.Edges along the polyp are somewhat curved. We use the feature Curvature representing the curvature of an edge and the feature EdgeLinearity representing the mean squared error of the edge pixel 2D locations fitted by the least square linear fitting method to remove straight edges. The feature Eellipse is calculated as a percentage of edge pixels that are not covered by the dilated ideal ellipse. The dilation is to account for some nearby edge pixels that are not located perfectly on the ideal ellipse. A threshold for this feature is used to remove edges that do not fit well on an ideal ellipse obtained using the least squares ellipse fitting method [52] on pixel locations of all extracted binary edges. The features h2, h2/h1, h3, h4, h4–h3, h4/h3 are derived from the edge cross section profile, capturing various properties of protrusion. They are labeled in Fig. 2. The features are pre-selected using J48 Decision Tree. The value range of these features are pre-determined using the value ranges in rules generated by J48 Decision Tree and manually tuned based on the leave-one-out-cross-validation method on the training data as described in Section 5.1.To calculate the texture feature Vtexture, the region of interest ROIconcave is a matrix of pixel values in the region area on the concave side of an edge. See Fig. 2(b). In [34], we apply a smoothing filter called Locally Weighted Scatterplot Smoothing (LOWESS)[53] on each row of ROIconcave (the protruding part of the polyp) to remove the effect of small and impulsive speckle noise before calculating texture features on ROIconcave. A texture feature is computed to capture the tendency of the intensity increase along the protruding direction of a polyp. In most cases, this tendency is not obvious without any smoothing due to the noise. The LOWESS filter is the most time-consuming component of our feature extraction. Hence, we replace this calculation in [34] by a simpler texture calculation method as shown in Fig. 3. Clearly, the protruding direction of a polyp is along the row direction of a ROIconcave. To capture this protruding characteristic, we only need to consider the width of a ROIconcave in the calculation. For a ROIconcave with a large width, we use a sufficiently large sliding window to handle the noise as shown in Line 1 of Fig. 3 and only estimate the intensity increasing trend based on the two end points of the sliding window. For a ROIconcave with a narrow width, we use a small window since it generally has smooth surface. This is because the corresponding edge is most likely a short edge usually caused by light reflection. Lines 4–9 in Fig. 3 shows the calculation for each pixel. If the pixel value on the right end of the sliding window is greater than the pixel value on the left end of the sliding window, we consider that the intensity increases from left to right. The average value in the matrix B representing the increasing trend of intensity values is returned as the texture feature Vtexture. The boundary values in ROIconcave are not taken into account in the calculation since its majority number of pixels is already sufficient to represent the trend of intensity increase along the horizontal directions. Obviously, as the trend of intensity increase mostly occurs from the edge of a polyp to its most protruding center, only the width that represents pixel values along the horizontal directions of a ROI instead of the height needs to be considered to calculate this texture feature.An edge is detected as a polyp edge if all the corresponding features in Table 2 are within their value ranges. Otherwise, it is detected as a non-polyp edge. To minimize the execution time, we use a coarse-to-fine feature rejection strategy. That is, we calculate one feature after another, starting from the calculation of the feature with the fastest computation time. We list the computation orders of these features in Table 2.If the value of any feature is not in its value range as specified in Table 2, the edge is detected as a non-polyp edge without the calculation of the subsequent features. This coarse-to-fine strategy avoids calculating all features for non-polyp edges, reducing the overall analysis time especially for images with many binary edges. Fig. 4shows the pseudo code of our polyp edge detection. Method names are shown in all capital letters. The SimpleextureFeature method is shown in Fig. 3 whereas the other methods are omitted since the ideas come from our previous paper [34]. Lines 2–6 identify a linear edge or an edge that does not quite fit an ideal ellipse as a non-polyp edge. Lines 8–16 filter out edge profiles that do not demonstrate important polyp properties. Line 17 filters out edges without the texture feature of the intensity increase trend calculated using our algorithm in Fig. 3. We consider an image as a polyp image if any of its edges is detected as a polyp edge.This step is the key contribution of this paper. It combines edge tracking with our fast polyp image detection described in Section 3. Edge tracking is used to improve the recall of polyp detection and reduce the number of falsely detected edges. We track Canny edges obtained from the Canny edge detector across multiple frames and detect whether the edges are polyp edges. A real polyp edge should appear in subsequent frames in a short time window. Otherwise, the detected polyp edge is considered a falsely detected edge. We use Pyramid Lucas–Kanade (Pyramid LK) to track the downsampled edge pixels on each Canny edge. Each edge pixel is tracked by searching the corresponding edge pixel in a predefined window in the next frame. If at least a predefined number of pixels on an edge are successfully tracked crossing two frames, an edge is successfully tracked. Only the tracked edges are used as the input for the polyp edge detection. A detected polyp edge is highlighted and displayed as visual feedback after it has been detected in a sufficient number of consecutive clear frames. Multiple polyps per image can also be detected and given feedback on if the edges of these polyps satisfy the aforementioned criteria. The details are as follows.Pyramid LK tracking is a sparse optical flow tracking method which has a reliable performance when given points with good features to track [54,55], such as corners. The method calculates the tracked feature points from the top layer of the first Gaussian Pyramid. The calculated feature points are used at the next Gaussian Pyramid layer for motion estimation. Generally, Pyramid LK with a larger Gaussian Pyramid level (PL) has a better tracking accuracy but a higher computational time. A larger PL is better for tracking an edge with a large motion because it searches the corresponding tracked point in a finer Pyramid image.Selection of feature points to track: Many types of local features can be used for tracking. Most popular trackable local features are the pixels with large derivative values in two orthogonal directions [54]. These pixels are considered as interest points, such as corners [56]. Generally, we do not consider pixels on edges good candidates for tracking because many edges of objects have a local linear shape. The pixels on edges with local linear shape have only large derivative values in one direction, but small derivative values in another orthogonal direction. However, if pixels on edges have large derivative values in both orthogonal directions, these edge pixels can be considered as good points to track, such as pixels on curve edges. Polyp edges are curve edges in most cases. We consider a subset of pixels on polyp edges as good feature points to track.Track feature points between two edges: We track feature points–pixels on two edges i and i+1 in two consecutive frames t and t+1 as follows. First, we downsample the pixels on edge i using the sampling factor of Le, i.e., selecting one pixel out of every Lepixels on that edge. We use the downsampled edge pixels as feature points. To track feature points, we apply the OpenCV function cvCalcOpticalFlowPyrLK [54] which implements the Pyramid LK tracking algorithm. The function calculates the error ɛtfor each feature point as the difference of pixel values between the feature point and its tracked point. The tracked point is selected by searching within a small square window Wstof the radius size Rwsurrounding the location of the feature point. Two feature points are successfully tracked if both of the following conditions are satisfied: (1) the tracking error ɛt<Thldst; (2) the distance distt<Thlddistt, where disttis the Euclidean distance between the feature point on edge i in frame t and its tracked point on edge i+1 in frame t+1.Track edges between two frames: We consider that the edge i in frame t track the edge i+1 in its consecutive frame t+1 if at least one feature point on edge i has its tracked point on edge i+1. If there is more than one such tracked edge in the previous frame t, we select the edge with the largest number of tracked points as the tracked edge i+1 of the edge i.Besides polyp image detection and edge tracking, we have two additional challenges to overcome. Challenge 1: The polyp may temporarily disappear from the FoV but quickly reappears. This could be due to many reasons. For instance, the endoscopist may briefly rotate the camera away to check the surrounding area, move the camera too fast or too close, or inject water into the colon for cleaning, causing blurry frames. We should group these frames together into the same polyp shot to minimize false boundaries. Challenge 2: The feedback on the detected polyp should be shown as soon as possible but in the manner that minimizes false feedback. We handle these complexities in our algorithm in Fig. 5.We define triggering duration (Np) as the number of times (clear frames) a tracked edge must be detected as a polyp edge (by the polyp edge detection algorithm in Fig. 4) before we give feedback on it. This is to address Challenge 2. For each clear frame, we set Npto either Nshort or Nlong, where Nshort<Nlong Initially, we set Npto the longer duration Nlong to reduce the chance of recognizing a false start shot boundary (Line 15 in Fig. 5). However, once the start boundary of the shot has been identified and all tracked polyp edges are lost in this frame and either (1) the current frame is still within the wait duration (Line 18 in Fig. 5) or (2) there are remaining non-polyp edges tracked in this shot (Line 19 in Fig. 5), we set Npto the shorter duration, Nshort. This is to show feedback quicker on the polyp that was previously alert, but disappeared and reappeared again in the same shot.To address Challenge 1, we do not terminate the current shot right way at the first frame without any tracked polyp edge in the frame. We continue to check for a new polyp edge for the wait duration (Nwait) defined as the number of clear frames since the last frame with at least one tracked polyp edge. We terminate the current shot only if no polyp edges are found during the wait duration and no tracking of any edges in the shot remains. We check the latter condition because if some non-polyp edges are still tracked, the polyp that was previously in FoV is more likely to be close by and may soon reappear within the FoV. NPshould be larger than Nshort to reduce the chance of missing an alert on a detected polyp edge when the camera moves fast.In the algorithm in Fig. 5, we use five list variables. The polypShot is the return variable listing shot information for all the detected shots, including the shot id as well as the starting frame and the ending frame of each shot. The trackedEdge is a list of tracked edge information between two clear frames, prevFrm and currFrm. The polypEdge records all the tracked polyp edges within a polyp shot. The lostPolypEdge records the tracked polyp edges in the shot which have lost their tracking. Any detected polyp edge finally loses the tracking of it when the camera moves away from the polyp. Therefore, we keep copying pointers to edges from polypEdge to lostPolypEdge when these edges can no longer be tracked (Lines 51–53) When all detected polyp edges lose their tracking, the two lists contain the same edges. The edgeSet records all tracked edges (including polyp edges and non-polyp edges) in frames with polyp edges in the same shot. Obviously, if two tracked frames are in the same shot, the two lists, trackedEdge and edgeSet, have at least one common edge (either a polyp edge or a non-polyp edge). The lists edgeSet, polypEdge, and lostPolypEdge are reset to empty before searching for the next shot. Lines 18–23 apply some of these variables to implement the aforementioned method for setting Npto either Nshort or Nlong. Lines 25–30 handle the situation when the current shot ends. In Lines 25–27, if no end frame of this shot has been set previously, it is set to the last polyp frame found. In many cases, the end frame of the current shot has already been set earlier in Lines 21-23 to be the most recent frame with at least one tracked edge (either a polyp edge or non-polyp edge) in the current polyp shot. Line 29 increments the shot ID and clears the content of the variables for the current shot. At this point, the start frame of the new shot has not been set.The starting and ending frames of a polyp shot: For each tracked edge, the system keeps counting the number of times the edge is detected as a polyp edge. If the current frame currFrm.id has at least one tracked edge with its count value reaching the trigger duration Np, the visual feedback on this edge is triggered and the system records the frame currFrm.id—Np+1 as the starting frame of a new polyp shot (Lines 44–46). The system also records the current frame number containing at least one tracked polyp edge in the variable lastTrackedPolypFrmID. The lastTrackedPolypFrmID is repeatedly updated until all tracked polyp edges are lost. If after this frame, some non-polyp edges are still tracked for some period of time without any tracked polyp edge, we still consider the frames in the same polyp shot. The system sets the last frame before tracking of all non-polyp edges and polyp edges is lost as the end of the current shot (Lines 21–23 in Fig. 5). This condition is checked whether the same edges appear in both trackedEdge and edgeSet.The visual feedback: The DisplayAlert method in Line 40 in Fig. 5 can be implemented to provide different types of feedback, ranging from a simple text display on the side, an audio cue indicating that this image shows a polyp, to the precise locations of the detected polyp edges. The latter is more informative but it is more intrusive than the first two types. Once a tracked edge is identified as a polyp edge for Nptimes, feedback is given about this edge until tracking on this edge is lost. To determine which type of feedback is most effective in practice requires an extensive study in future clinical trials. We present our implementation that labels the detected polyp edges on screen in Section 4.3.We implemented all the algorithms described in this paper in C/C++ using Microsoft Visual Studio 2008 with Open Source Computer Vision library (OpenCV) Version 2.1 [54]. We implemented two new software modules—edge tracking module and polyp shot extraction module written as Windows Dynamic Linked libraries running on our middleware software SAPPHIRE [57].SAPPHIRE is a novel middleware and software development kit for stream programing on a heterogeneous system of multi-core multi-CPUs with optional hardware accelerators such as Graphics Processing Unit (GPU). A stream program consists of a set of tasks where the same tasks are repeated over multiple iterations of data (e.g., video frames). Examples of such programs are video analysis applications for computer-aided diagnosis and computer-assisted surgeries. Our design goal for SAPPHIRE is to reduce the implementation efforts and ease collaborative software development of stream programs while supporting efficient execution of the programs on the target hardware. SAPPHIRE also comes with several basic modules such as modules for reading frames from MPEG files, grabbing frames from video capturing devices, writing frames to MPEG 2 format, etc. Image frame data can also be processed directly by OpenCV, eliminating the need to create an additional image copy.A SAPPHIRE program is the middleware with a collection of software modules together implementing desirable tasks. Given a text configuration file listing module names and optional module parameters and values as an input, the SAPPHIRE middleware constructs a parallel task graph of these modules based on the user-defined types of data, prunes unneeded modules from the graph, loads the remaining modules in their own thread to take advantage of multi-core multi-CPUs, and routes the data among the modules according to the task graph. SAPPHIRE provides access to each frame of data in chronological order. The middleware provides an API for modules to communicate with the middleware (as opposed to modules communicating directly with other modules).Table 3shows the functional output, the frame analysis rate in frames per second (fps) of each module, and the dependencies among the modules used in this paper. The module for real-time version of the blurry frame detection algorithm presented in [46] was previously implemented. This module outputs a data packet for each image frame it analyzes whether the frame is clear or blurry. It considers a frame as blurry if it does not have a sufficient number of connected edge pixels detected using Canny edge detector. The two new modules are edge tracking and polyp shot extraction that take image input from the data source module (from a file or from a capturing device) and data packets output from blurry frame detection. The edge tracking module implemented our edge tracking algorithm which takes Canny edges from the clear image detection module as input, and outputs the tracked edge information.Our implementation of feedback marks five red dots spaced at an equal distance along the detected polyp edges in the original image for display. The green circle is displayed as the mean position of the five red dots. It is used to draw the endoscopist attention to the detected polyps. See the two example feedback shown in Fig. 6. The visualized feedback with five red dots marks the precise location of edge position of polyp. This design makes it easy for the endoscopist to see. The green circle position indicates the concave side of ROI, which is corresponding to the approximated center location of a detected polyp. The mean position of five red dots is used due to the simplicity of their calculation and the stability of their mean position in terms of camera movement. A formal user study is needed to validate the usability of this design once real-time feedback is achieved.

@&#CONCLUSIONS@&#
