@&#MAIN-TITLE@&#
Solving average cost Markov decision processes by means of a two-phase time aggregation algorithm

@&#HIGHLIGHTS@&#
We introduce a two-phase time aggregation algorithm for MDPs.The algorithm enables policy improvement outside of the time aggregated MDP domain.The two phases enable optimization over the entire state space.Improved approximate solutions can be obtained by employing the proposed approach.

@&#KEYPHRASES@&#
Dynamic programming,Markov decision processes,Embedding,Time aggregation,Stochastic optimal control,

@&#ABSTRACT@&#
This paper introduces a two-phase approach to solve average cost Markov decision processes, which is based on state space embedding or time aggregation. In the first phase, time aggregation is applied for policy optimization in a prescribed subset of the state space, and a novel result is applied to expand the evaluation to the whole state space. This evaluation is then used in the second phase in a policy improvement step, and the two phases are then alternated until convergence is attained. Some numerical experiments illustrate the results.

@&#INTRODUCTION@&#
Recent developments in the theory and simulation techniques for Markov decision processes (MDP) (e.g., Busoniu, Ernst, Schutter, & Babuska, 2010; Cao, Ren, Bhatnagar, Fu, & Marcus, 2002; Chang, Fu, Hu, & Marcus, 2007; Leizarowitz & Shwartz, 2008; Powell, 2007) have led to a growing body of literature on MDP modeling for real world problems (e.g., Anderson, Boulanger, Powell, & Scott, 2011; Arruda & do Val, 2008; Pennesi & Paschalidis, 2010; Zhang & Archibald, 2011). Much of this increased interest is due, in part, to the development of powerful techniques to deal with MDPs of very large dimensions, encompassed in a framework known as approximate dynamic programming (ADP) (Bertsekas & Tsitsiklis, 1996; Powell, 2007; Sutton & Barto, 1998).In the context of the ADP framework, there is a vast literature covering a variety of techniques, such as heuristic search (Hansen & Zilberstein, 2001) and real-time dynamic programming (Barto, Bradtke, & Singh, 1995; Bonet & Geffner, 2003), which make use of asynchronous updates and heuristic search to accelerate convergence, as well as topological value iteration (Dai & Goldsmith, 2007; Dai, Mausam, & Weld, 2009), that processes information related to the graphical features of MDPs to decide the optimal ordering of the value function updates. Asynchronous updates are also exploited in (Akramizadeh, Afshar, Menhaj, & Jafari, 2011; Moore & Atkeson, 1993), while a sequence of increasingly accurate approximate models is used in (Arruda, Ourique, LaCombe, & Almudevar, 2013).Among the most popular ADP techniques one finds value function approximation (e.g., Arruda, Fragoso, & do Val, 2011; Boyan & Moore, 1995; Li & Littman, 2010), and simulation coupled with state space reduction (e.g., Arruda & do Val, 2008; Cao et al., 2002; Chang et al., 2007). There is a wide range of theory and applications within the ADP framework covering value function approximation techniques, especially for discounted cost MDP problems (see, e.g., Powell, 2012). In particular, the abstract representation of the value function in terms of algebraic decision diagrams (e.g., Hoey, St-aubin, Hu, & Boutilier, 1999; Joshi & Khardon, 2011; St-aubin, Hoey, & Boutilier, 2000) can be efficiently used to solve some large scale discounted MDPs. While much progress has been made and a few promising directions have been devised (e.g., Arruda et al., 2011; Ormoneit & Sen, 2002; Powell, 2007; Tsitsiklis & Van Roy, 1997), convergence results for general approximation architectures remain to be proved. Moreover, performance bounds for such techniques tend to be very specialized (e.g., Gordon, 1995; Tsitsiklis & Van Roy, 1997; Lin, Hui, Hua-Yong, & Lin-Cheng, 2009).State space reduction techniques, known as embedding or time aggregation, can be traced back, in the context of control theory, at least to (Zhang & Yu-Chi, 1991), but we shall be particularly interested in the works of (Cao et al., 2002; Chang et al., 2007; Leizarowitz & Shwartz, 2008). It is well known that a great advantage of time aggregation is that, unlike state aggregation (e.g. Bertsekas, 2012), it preserves the Markov property. As a result, it can be used to produce an equivalent formulation with reduced state space. In that context, Fainberg (1986) studied the construction of embedded MDP models for the total cost criterion, whereas Leizarowitz and Shwartz (2008) investigated embedding techniques for average cost MDPs. An earlier work, (Cao et al., 2002), investigated embedding in a scenario where the control policy within a certain region of the state space is fixed and focused on reducing the computational burden of the solution procedure. This approach was later extended to deal with a continuous time stochastic control problem (Xu & Cao, 2011), and also inspired further work on algorithms for embedded (time aggregated) MDPs e.g., Ren and Krogh (2005), Sun, Zhao, and Luh (2007), Arruda and Fragoso (2011). Similar concepts were applied in the context of discount MDPs (Hauskrecht, Meuleau, Kaelbling, Dean, & Boutilier, 1998), where hierarchical models were employed to decompose the process. A thorough discussion of compact representations for MDPs can be found in (Boutilier, Dean, & Hanks, 1999).The time aggregation approach, which transforms an MDP into another equivalent MDP with reduced state space, can be of great assistance when one wishes to find approximate solutions in reduced computational time. To accomplish such reduction, one can trade speed for accuracy and specify a priori an outer policy that prescribes a pure control action to each state outside of a prescribed region of interest F of the state space S. An appropriately optimized inner policy is then obtained and both outer and inner policies are composed to result in a sub-optimal policy over S. This policy minimizes the long term average cost over all control policies that adopt the prescribed outer policy. Note that optimality cannot be guaranteed unless the outer policy is optimal, i.e., it is comprised of optimal control actions for every state inFc=S⧹F. In particular, optimality can be attained for large scale MDPs with a large number of uncontrollable states, i.e. states for which only a single control action is available (see Cao et al., 2002).A distinguishing feature of this paper is that, unlike the current literature in time aggregation, it addresses also the problem of iteratively refining the outer policy. The rationale is simple: to apply time aggregation iteratively, but refining the outer policy at each iteration, until the outer policy converges to an optimal outer policy. At this point, the time aggregation approach is able to retrieve an optimal policy for the original MDP, over the entire state space S. The proposed outer policy refinement routine can be seen as a policy improvement step of the classical policy iteration algorithm e.g., Bertsekas (2012), which makes use of the value function of the latest policy obtained by time aggregation. A novel contribution of this paper is the way we derive this value function, making use of some new results that are introduced in this paper. Firstly, we prove that the value function obtained by the time aggregation algorithm for each state in the subset F is numerically equal to the value function obtained by a classical policy evaluation algorithm for this same state. We then make use of this result to derive the value function for each state inFcas the value of a classical stochastic shortest path problem starting from this state to reach any state in the target region F.To sum up, we propose a two-phase time aggregation algorithm to solve MDPs to optimality. The two phases of the algorithm, which are applied successively up to convergence, work as follows: in the first phase, time aggregation is applied for some prescribed outer policy; then in the second phase, a policy improvement step is applied that refines the outer policy. We prove that the proposed algorithm converges monotonically to the optimal policy under general conditions on the structure of the MDP. It is worth pointing out that the proposed approach can be seen as a variation of the classical policy iteration algorithm with a policy search in the subset F at each iteration. The policy search is performed by the time aggregation algorithm, which finds the best possible policy in F given that the policy inFcis fixed.This paper is organized as follows. Section 2 presents the studied problem. Section 3 features the time aggregation approach and derives a novel result on the correspondence between the value functions of the embedded MDP and the original MDP, for a fixed control policy. This result is then applied in Section 4 to derive a two phase algorithm for the studied problem. The convergence of the proposed algorithm to the optimal solution is then proved in Section 4.1. Numerical experiments are presented in Section 5 to illustrate the approach, and Section 6 concludes the paper.Consider a time homogeneous discrete time Markov decision process (MDP) with a finite, possibly very large, state space S. LetA(i)∈Ndenote the set of feasible control actions at state i and defineA≔{A(i),i∈S}, and suppose that a functionf:S×A→R+represents the one-period cost of the process, whereR+denotes the set of nonnegative real numbers.LetL:S→Arepresent a stationary control policy over the state space S, and letLbe the set of all feasible stationary control policies. Under policyL, one selects control actiona=L(i)at each time the controlled process visits statei∈S. Following a visit to statei∈S, and the application of a control actiona∈A(i), the process moves to statej∈Swith probabilitypija. Hence, the evolution of the controlled processes under a control policyL∈Lis governed by a Markov chain{Xt,t⩾0}, and the one-period transitions are determined by the transition matrixPL={pijL(i)},i,j∈S. Following Cao et al. (2002), we assume that the controlled process is ergodic under all policies. Assume that the one-period cost function f is a measurable positive real-valued function and let(1)ηL=limN→∞1N∑k=0N-1f(Xk,L(Xk))be the long term average cost of the controlled chain. Because the controlled chain is ergodic, this cost is independent of the initial state. The objective of the decision maker is to find the optimal policyL∗∈L, which satisfies(2)ηL∗⩽ηL,∀L∈L.Now let us select a subsetF⊂S, and define an outer policyLout:Fc→Aas the set of control actions prescribed by policyLfor all states outside of F, i.e.Lout={L(i),i∈Fc}, whereFc≜S⧹Fis the complement of F. Similarly, an inner policyLin:F→A,Lin={L(i),i∈F}denotes the control strategy prescribed by policyLfor the subset F. Clearly, we haveL=Lin∪Lout. We letLinandLoutdenote the sets of all feasible inner and outer policies, respectively. Fig. 1illustrates the concepts of inner and outer policies.Typically the set F is a relatively small subset of S. It may be comprised, for example, of the states which are more important from some control standpoint, or the states that are expected to be visited more frequently under some particular class of control policies. For example, in a storage control problem, one would expect the set F to be comprised of the states that are within some desirable vicinity of the zero-stock state.Now let us select an outer policyd:Fc→A,d∈Lout, a priori and define the following problem(3)MinimizeηL,L∈LsubjecttoLout=d.The time aggregation approach solves Problem (3), for some outer policy d prescribed a priori at the decision maker discretion. Naturally, unless d is the only feasible outer policy, solving Problem (3) involves some loss of performance, and optimality can no longer be guaranteed to the original problem. Indeed, selecting the outer policy d in such a way as to mitigate the loss of performance depends on the sensibility and prior knowledge of the decision maker.With respect to any optimal policyL∗which solves (2), we define:(4)Lin∗=L∗(i),i∈F,and(5)Lout∗=L∗(i),i∈Fc,for some prescribed subsetF∈S. The following Lemma states the somewhat intuitive result that, once the outer policy is chosen to be comprised of optimal actions inFc, then an optimization in F yields the optimal policy in S.Theorem 1Supposed=Lout∗, thenProblem (3)is equivalent toProblem (2).Letηd∗solve Problem (3). It is not difficult to see that the solution space of (3) is a subset ofL, the solution space of (2). Hence, we conclude thatηd∗⩾η∗. Moreover, we also know thatL∗is a feasible solution to (3). This impliesηd∗⩽η∗. From the two inequalities, we conclude thatηd∗=η∗.□Now, let us concentrate on solving Problem (3). Letτ0=0and defineτi=min{t>τi-1:Xt∈F},i=1,2,…to represent the times at which the process visits a prescribed subset F of the state space S. The embedded chain{Yi≔Xτi,i⩾0}maps the sequence of visits of the original MDP to the subset F. As a result, it has state space F and its properties can be determined by investigating the trajectories between consecutive visits to F. We recall that, in Problem (3), all feasible policiesLshare the same structureLout∈Loutwithin the setFc.The time aggregation approach involves dividing the original Markov chain into segments, each segment corresponding to a trajectory between two successive visits to the set F. Hence, the segment corresponding to a given stateYi=Xτiis{Xτi,Xτi+1,…,Xτi+1-1}, and the expected cost of a segment starting at statei∈F, under control actionL(i)=a, is(6)hf(i,a)≔E∑t=τiτi+1-1f(Xt,L(Xt))|Xτi=i,L(i)=a=E∑t=0τ1-1f(Xt,L(Xt))|X0=i,L(i)=a,i∈F,L∈L,hf(i,a)=Ef(i,a)+∑t=1τ1-1f(Xt,Lout(Xt))|X0=i,L(i)=a,where the second equality in the first expression follows from the strong Markov property. Note, from Eq. (6), that the outer policyLout∈Loutand the control action a applied at state i completely determine the quantityhf(i,a). Note that the time aggregated process can be seen as a semi-Markov process, where the duration of each period starting at stateYi∈Fcorresponds to the length of the trajectory{Xτi,Xτi+1,…,Xτi+1-1}. Hence, in order to solve for the optimal policy, we need to keep track of the expected elapsed time of the trajectories starting at each statei∈F, which can be defined as:(7)h1(i,a)=hf(i,a),forf(j,a)≡1,∀j∈S,a∈A(j).For the embedded process{Yt,t⩾0}, the transition probability from statei∈Fto statej∈F, under controla∈A(i)and outer policyLout∈Loutis given byp̃ija. Letτ={mint>0:Xt∈F}be the first return time to F and definepijLout(τ)=P(Xτ=j|X0=i),i∈Fc,j∈F. Then, for anyi∈Fc, this quantity can be calculated iteratively as:pijLout(τ)=pijLout(i)+∑k∈FcpikLout(i)·pkjLout(τ).That results in|F|systems of linear equations, each of them with|Fc|equations and the same number of unknowns. The time complexity to solve for all the transition probabilities isO(|F||Fc|3). Alternatively, these systems lead to:PLout(τ)=I-PFc,FcLout-1PFc,FLout,wherePFc,FcLout=pijLout(i),i,j∈Fc,PFc,FLout=pijLout(i),i∈Fc,j∈F.As a result, the transition probabilities for the embedded process are given by:p̃ija=pija+∑k∈Fcpika·pkjLout(τ).Thus, the complexity of evaluating exactly the transition probabilities for all state-action pairs isO(|F||A|). Taking into account the evaluation of the last term in the equality above, we get a complexity ofO(|F|·(|A|+|Fc|3). Similar arguments yields a complexity ofO(|F|·|A|+|F|3)for the evaluation of both functionalshfandh1for every state-action pair in F. However, as in (Cao et al., 2002, Section 6), simulation based techniques can be used to estimate these quantities which are more economical in terms of computational cost for problems with very large state spaces.Let nowπ̃={π̃(i),i∈F}be the set of steady state probabilities of the embedded process{Yt,t⩾0}under a control policyL, and define:(8)n¯L=∑i∈Fπ̃(i)h1(i,L(i)),L∈L,It is proved in Cao et al. (2002) that, for each control policyL, the long term average cost of the original MDP is numerically equal to(9)ηL=limM→∞1M∑m=0M-1hf(Ym,L(Ym))1M∑m=0M-1h1(Ym,L(Ym))=∑i∈Fπ̃(i)hf(i,L(i))∑i∈Fπ̃(i)h1(i,L(i))=π̃HfLn¯L,L∈L,whereπ̃is the row vector of steady state probabilities of the embedded process{Yt,t⩾0}andHfLis a column vector of the individual valueshf(i,L(i)),i∈F.Now, let us fixLout=d,d∈Loutand solve Problem (3). To solve this problem, one can apply the Incremental Value Iteration (IVI) algorithm in (Sun et al., 2007, Algorithm 2, page 2180). Loosely speaking, the IVI algorithm solves a series of parametric MDPs with a modified cost functionrδ=(hf-δnh1), refining the parameterδnuntil the average cost of the modified MDP converges to zero. They demonstrate that when that happens, the parameterδnis numerically equal to the average cost of the original problem (Sun et al., 2007, Corollary 1, Page 2179). Each parametric MDP is solved by means of value iteration, hence the complexity of each iteration of the IVI algorithm isO(|F|3). This process results in a pair(V‾d∗,ηd∗), which satisfies the Poisson Equation in the time aggregated domain (Sun et al., 2007):(10)hf(i,a)-ηd∗h1(i,a)+∑j∈Fp̃ijaV‾d∗(j)=V‾d∗(i),i∈F,a=Ld∗(i),whereV‾d∗:F→Ris a real valued function in F andηd∗=ηLd∗is a scalar that also solves Eq. (1) for policyLd∗. The optimal policy for Problem (3), denoted byLind∗∈Lincan be recovered by using the expression below:(11)Lind∗(i)=argmina∈A(i)hf(i,a)-ηd∗h1(i,a)+∑j∈Fp̃ijaV‾d∗(j),i∈F.Note that Eq. (10) in this paper corresponds to Eq. (24) in Step 2 of the IVI algorithm in (Sun et al., 2007, page 2180), whereas our Eq. (11) corresponds to the expression in Step 5 of the referred algorithm. In both expressions,ηd∗is equivalent toδnandV‾d∗(j)corresponds togˆm(j).In the next subsection we study the properties of the value function for a fixed control policyL, in particular with respect to a prescribed stopping time τ. By prescribing τ as the minimum time to reach a subsetF∈Sof the state space, Lemma 3 derives the value function in the complementary subsetFc, making use of both the long term average costηLof policyL, and the value function in the prescribed subset F.Let us now focus on the classical solution of average cost MDPs. LetVbe the space of real valued functions in S and letV:S→Rbe an element of this space. It is well known that the long term average cost of any given policyL∈Lcan be found by solving the Poisson Equation e.g., Puterman (1994, Corollary 8.2.7, page 344), which reads:(12)f(i,a)+∑j∈SpijaVL(j)=VL(i)+ηL,i∈S,a=L(i).Furthermore, it is also well known that, for any policyL∗∈Lwhich solves (2), it holds that e.g., Puterman (1994, Eq. (8.4.2), page 354):(13)mina∈A(i)f(i,a)+∑j∈SpijaVL∗(j)=VL∗(i)+ηL∗,∀i∈S.Lemma 1 derives a multi-step evaluation of the value function, which is applied to a specific stopping time in Corollary 2, which will be applied to derive the value function at a given state as a function of a stopping time. Lemma 3 applies this result to define a specific shortest path evaluation of the value function in a specific region of the state space.Lemma 1SupposeηLsolves(12)for someL∈L. Then, for all positive integers n:(14)E∑k=0n-1f(Xk,L(Xk))+VL(Xn)X0=i=VL(i)+nηL,i∈S,L∈L.It is not difficult to see that, for policyL∈L,Ef(X0,L(X0))+VL(X1)X0=i=f(i,a)+∑j∈SpijaVL(j),i∈S;∴Ef(X0,L(X0))+VL(X1)X0=i=VL(i)+ηL,i∈S,wherea=L(i), and the last equality follows from (12), which holds by hypothesis. Hence, it is proved that (14) holds forn=1.Now, suppose it also holds for some positive integer(n-1), i.e.(15)E∑k=0n-2f(Xk,L(Xk))+VL(Xn-1)X0=i=VL(i)+(n-1)ηL,i∈S.Applying (12) and the Markov property, one can easily see that:(16)VL(Xn-1)=∑j∈Sf(j,L(j))+∑k∈SpjkaVL(k)-ηL1{Xn-1=j},where1{B}is the indicator function of statement B, which assumes value 1 (one) whenever B holds true and is zero otherwise, anda=L(j). Substituting (16) in (15), we obtainE∑k=0n-1f(Xk,L(Xk))+VL(Xn)X0=i=VL(i)+nηL,i∈S,and that completes the proof. □The result below follows directly from Lemma 1.Corollary 2For any stopping timeτ⩾0and any policyL∈L, we have(17)E∑k=0τ-1f(Xk,L(Xk))+VL(Xτ)X0=i=VL(i)+ηLE[τ],i∈S;(18)VL(i)=E∑k=0τ-1f(Xk,L(Xk))-ηL+VL(Xτ)X0=i,i∈S.In the remainder of this Section, and inspired by the results in Bertsekas (1998), we consider an associated stochastic shortest path (SSP) problem whose domain is the complementary regionFcof the state space, where F is a subset of interest. In Lemma 3 we show how the solution of this problem can be used to derive the value function inFcas a function of the value function in F, obtained by time aggregation in Section 3.3. Hence, the proposed SSP will be used in conjunction with time aggregation to derive the value function inFc, which will be employed in a policy improvement step in the two-phase algorithm, introduced in Section 4. The policy improvement step will lead to an improved control policy inFc. This improved policy will be then fed back to the time aggregation step, thus resulting in an improved overall solution to the original problem. This process will be repeated in the two-phase algorithm, detailed in Section 4, until convergence is attained.We now consider an associated stochastic shortest path problem to visit F for the first time from any statei∈Fc. This is obtained by leaving unchanged all the transition probabilitiespija,i∈Fc,j∈S,a∈A(i), and by making each statem∈Fa termination state, for someF⊂S, whose complement is defined asFc=S⧹F. Once the system reaches a terminal states, it incurs a termination cost and goes to an artificial target state, labeled ts, which is never left afterwards. Hence, for a given policyL∈L, we can define the stochastic shortest path probability matrixPSSPLas:PSSPL=PFc,FcLPFc,FL0001001,wherePFc,FcL={pijL(i),i,j∈Fc},PFc,FL={pijL(i),i∈Fc,j∈F}.For each statej∈Fc, we define an instantaneous costg(i,a)=f(i,a)-λ, where λ is an scalar parameter. We also define a terminal costT(m)∈Rfor each terminal statem∈F. At the artificial target state, no cost is incurred.Hence, the cost function for a policyL∈Lis:g(i,L(i))=f(i,L(i))-λ,fori∈FcT(i),fori∈F,0,fori=ts.Following Bertsekas (1998) we call this problem theλ-SSP. We note that the value of λ and the termination costT(m)will be appropriately defined in such a way as to make the value function of the prescribedλ-SSP coincide with that of the original problem for all statesi∈Fc, as detailed in Lemma 3.Note that our definition of aλ-SSP problem is slightly different from that of Bertsekas (1998) in that we define a target regionF∈S, instead of a single target state. Note also that, from a control standpoint, it suffices to define an outer policyLout:Fc→A, since thepSSPi,tsa=1,∀i∈F,a∈A. For more details on stochastic shortest paths, we refer to Bertsekas (2012).LethLout,λ(i)be the value function of a feasible stationary policyLout:Fc→Afor this problem, starting from statei∈Fc. That is(19)hLout,λ(i)=E∑k=0τ-1f(Xk,Lout(Xk))-λ+T(Xτ)X0=i,i∈Fc,withτ=min{t>0:Xt∈F}. The finiteness of τ is guaranteed under the initial hypothesis that all policies are ergodic (Cao et al., 2002; Leizarowitz & Shwartz, 2008). Making use of one-step transition probabilities, this value can also be expressed as:hLout,λ(i)=f(i,Lout(i))-λ+∑j∈FpijLout·T(j)+∑k∈FcpikLout·hLout,λ(k),i∈Fc.Hence, the exact calculation of the functionhLout,λ:Fc→Rcan be made by solving a system of|Fc|equations and the same number of unknowns, directly or iteratively. The complexity of such a task isO(|Fc|3). However, as in (Cao et al., 2002, Section 6), simulation based techniques can be used to estimate these quantities which are more economical in terms of computational cost for problems with very large state spaces.Lemma 3Consider a λ-SSP problem withλ=ηLandT(m)=VL(m),∀m∈F, andLout(i)=L(i),∀i∈Fc, for someL∈L. ThenhLout,λ(i)=VL(i),∀i∈Fc.To prove the result, it suffices to substituteλ=ηLandT(m)=VL(m),m∈Fin (19) and compare it to (18). □From the discussion in Section 3.1, it follows that solving Problem (3) results in a control policyLd∗=Lind∗∪din the original MDP described in Section 2. Hence, the pair(VLd∗,ηd∗)satisfies Eq. (12), sinceVLd∗:S→Ris a value function of a fixed control policy in the original problem andηd∗is its long term average cost.The following theorem establishes the correspondence between the time aggregated value functionV‾d∗:F→Rin (10) andVLd∗, which is the value function of following policyLd∗in the original MDP.Theorem 2Let(V‾d∗,ηd∗)solve Eq.(10)for someF∈Sand assumeLind∗:F→Asatisfies Eq.(11). Then, lettingLd∗=Lind∗∪ddenote the corresponding control policy in the original MDP, it follows that:(20)VLd∗(i)=V‾d∗(i),∀i∈F,andηLd∗=ηd∗, where the pair(VLd∗,ηd∗)solves(12).ThatηLd∗=ηd∗is already established in Eq. (10). Now letτ=min{t>τi-1:Xt∈F}. Then, considering thatp̃ija=P(Xτ=j|X0=i,L(i)=a),i,j∈F, Eq. (18) applied toLd∗becomes:VLd∗(i)=E∑k=0τ-1f(Xk,Ld∗(Xk))-ηLd∗+∑j∈Fp̃ijaVLd∗(j)X0=i,a=Ld∗(i),VLd∗(i)=hf(i,a)-ηd∗h1(i,a)+∑j∈Fp̃ijaVLd∗(j),i∈F,a=Ld∗(i),where the last equality is obtained by applying the definitions in (6) and (7), and by substitutingηd∗forηLd∗. And the proof is concluded by noting that the last equality corresponds to Eq. (10).□In this section, we make use of Theorem 2 and Lemma 3 to determine the complete solution of the Poisson Equation for the policyLd∗, obtained via time aggregation. Theorem 2 implies that, by solving the time aggregated problem, we obtain the partial solution of the Poisson Equation for the subsetF⊂S. Then, by solving aηd∗-SSP as defined in (19), and making use of Lemma 3, we are able to find the partial solution of the Poisson Equation for the subsetFc.Then, in possession of the whole solution of the Poisson equation for the current policy, a policy improvement step (Puterman, 1994, Chapter 8) can be executed within the original MDP to find an improved outer policyd̃. The whole procedure can then be repeated for the newly found outer policy and so on, until an optimal solution is found. The proposed algorithm is presented below:Algotithm 1Two Phase Time Aggregation(1)Choose F, selectLout=d0,d0∈Loutand makek=0.(2)Solve Problem (3) for(V‾d∗,ηd∗), withd=dk. Makeηdk=ηd∗.(3)FindLindkby solving Eq. (11) forLind∗, and makingLindk=Lind∗.(4)MakeLdk=Lind∗∪dkandVLdk(i)=V‾d∗(i),∀i∈F, in the original MDP.(5)Solve aλ-SSP-Eq. (19)- forλ=ηdkandT(m)=VLdk(m),∀m∈F. Use Lemma 3 to obtainVLdk(i),∀i∈Fc.(6)Apply a policy improvement step for alli∈Fcdk+1(i)=argmina∈A(i)f(i,a)+∑j∈Fp̃ijaVLdk(j),i∈Fc,selectingdk+1(i)=dk(i)whenever possible.(7)Ifdk+1=dk, STOP. Otherwise, go to Step 8.(8)k←k+1; return to Step 2.Step 1 of Algorithm 1 selects a partial control policy inFc. This partial policy is used in Step 2, where the algorithm solves a time aggregation problem. The optimal policy for this problem, a partial policy in F, is recovered in Step 3. Both partial policies are composed to result in a control policy for the original MDP in Step 4. Step 5 solves a λ-SSP and applies Lemma 3 to recover the value function for the current policy, within the subsetFc. The complete value function is then used in a policy improvement step in Step 5, where a new partial control policy inFcis obtained, and the process restarts in Step 2 until convergence is attained.The computational issues of Step 2 and Step 5 are discussed in Sections 3.1 and 3.2, respectively. It is worth mentioning, however, that one can directly solve (3) (Step 2 of Algorithm 1) and obtain the associated value function (Step 5 of Algorithm 1) at the same time by solving a modified MDP in the original state space, in which the feasible actions for all states inFcare limited to those prescribed by the outer policydk, with no restriction to the control actions in F. In that sense, the time to solve this modified MDP – in the original state space – can be seen as an upper bound to the total time of executing Steps 2 and 5 of Algorithm 1. Thus, solving Problem (3) by time aggregation is recommended only when the sampling strategy makes it faster than solving an ordinary MDP with optimization in F, or whenever the size of the state space renders the latter algorithm intractable. For the sake of simplicity, we chose to solve the modified MDP described above in our numerical runs in Section 5.We note that the proposed algorithm is a form of policy iteration with optimization in F, in which the outer states inFcare updated simply based on a policy improvement step, while at the inner states in F an optimization is realized by means of time aggregation to find the best possible inner policy with respect to the outer policy. As the numerical experiments in Section 5 illustrate, this optimization routine can significantly accelerate the convergence of the algorithm with respect to the traditional policy iteration algorithm. The convergence of Algorithm 1 to the optimal policy is proved in the next subsection.Lemma 4Letdk+1∈Loutbe the outer policy selected in Step 6 ofAlgorithm 1, and letLdk+1‾=Lindk∪dk+1. Then,ηLdk+1‾⩽ηLdk.The arguments in this proof follow closely those in (Puterman, 1994, Proposition 8.6.1, page 379). Note thatVLdk:L→Ris known, as it was previously obtained in Steps 4 and 5 of Algorithm 1. The same applies toηdk, which was found in Step 2 of the same algorithm. We recall from Section 3.2, that these quantities satisfy Eq. (12), the Poisson Equation in the original domain. Rearranging this expression and converting it to matrix notation, one obtains:(21)ηLdk·e=fLdk+PLdkVLdk-VLdk,ηLdk·e=fLdk+(PLdk-I)VLdk,where e is a vector of unitary components with cardinality|S|. LetπLdk+1‾be a row vector that represents the invariant distribution associated with applying policyLdk+1‾in the original MDP, which satisfies(22)πLdk+1‾(PLdk+1‾-I)=0,πLdk+1‾e=1.Then, standard MDP results yield that:ηLdk+1‾=πLdk+1‾fLdk+1‾.Adding and subtractingηLdkand making use of (22), we can write:ηLdk+1‾=ηLdk+πLdk+1‾fLdk+1‾-ηLdke+πLdk+1‾(PLdk+1‾-I)VLdk=ηLdk+πLdk+1‾fLdk+1‾-ηLdke+(PLdk+1‾-I)VLdk.Note that Eq. (21) implies that the term in brackets is zero ifLdk+1‾=Ldk. Hence, sinceLdk+1is chosen to minimize the quantity in brackets in Step 6 of Algorithm 1, this quantity is upper bounded by zero (it is less than or equal to zero). Thus, we haveηLdk+1‾⩽ηLdk.□LetLdk∈Lbe the policy found in the Step 4 of the k-th iteration ofAlgorithm 1. Then, for any successive iterations ofAlgorithm 1it holds that,ηLdk+1⩽ηLdk,k⩾0.Letdk+1∈Loutbe the outer policy selected in Step 6 of Algorithm 1, and letLdk+1‾=Lindk∪dk+1. Then, from Lemma 4, it follows that(23)ηLdk+1‾⩽ηLdk.Furthermore, note thatηLdk+1=ηdk+1, whereηdk+1is determined in Step 2 of Algorithm 1. Hence, we have(24)ηLdk+1⩽ηLdk+1‾.The expression above holds true becauseLdk+1‾is a feasible solution for the optimization problem solved in Step 2 of Algorithm 1. Therefore, from Eq. (23) and (24), it follows thatηLdk+1⩽ηLdk,and that ends the proof.□From Theorem 3, one can see that each new policy determined by Algorithm 1 is better than its previous counterpart in terms of average cost. That means that the algorithm converges in a finite number of iterations because, as the state and action spaces are finite, there are only a finite number of policies to choose from, and the average cost cannot be improved forever. It remains to be shown that the policy to which Algorithm 1 converges is indeed an optimal policy, and this result is proved in the next theorem.Theorem 4Letdk+1=dk, for some integerk⩾1inAlgorithm 1. Then,Ldk=L∗, whereLdkis the policy found in Step 4 of this algorithm andL∗is a solution ofProblem 2.Suppose, for the sake of contradiction, thatdk≠Lout∗, whereLout∗was defined in (5). Then, Eq. (13) is not satisfied for at least one statei∈Fc, which implies thatdk+1(i)≠dk(i)for somei∈Fcin Step 6 of Algorithm 1. This, in turn, implies thatdk+1≠dk, which contradicts the initial hypothesis. Hence, it follows thatdk=Lout∗.Hence, by Theorem 1 it follows that the output of Step 4 of Algorithm 1 must be a solution to Problem 2, and that concludes the proof. □We conclude this section with some remarks on the numerical efficiency of the two-phase method.Remark 1As mentioned in Section 3.1, the complexity of Step 2 of Algorithm 1 is of orderO(|F|·|A|+|Fc|3)+O(|F|3), while the complexity of Step 5 is of orderO(|Fc|3), as outlined in Section 3.2. Hence, the complexity of one iteration of the two-phase algorithm is of orderO(|F|·|A|+|Fc|3)+O(|F|3)+O(|Fc|3), while the complexity of one iteration of a standard policy improvement is of orderO(|S|3). For the pure control policy, which the numerical example of the paper belongs to,|A|≈|S|. For applications such that|F|≪|Fc|and thus|Fc|≈|S|,O(|F|·|A|+|Fc|3)+O(|F|3)+O(|Fc|3)≈O(|Fc|3)≈O(|S|3); for applications such that|F|=|Fc|=cand|S|=2c,O(|F|·|A|+|Fc|3)+O(|F|3)+O(|Fc|3)≈3c3≈O(|S|3). In all these applications, the complexity of an iteration of two-phase algorithm is similar to that of a standard policy improvement.The superiority of the proposed method over policy iteration comes from the fact that it obtains better bounds on the value function and on the long term average cost at each iteration, thus making a better usage of the computational resources. The improvement comes from the optimization in Step 2 of Algorithm 1, which allows a search for the optimal policy within the subset F, given a prescribed policy inFc. The region F can be selected to be comprised of the most attractive states in S with respect to the cost functionc:S→R, which tend to contribute more to the long term average cost. Hence, an optimization in F, which can have little impact in the overall computation if|F|≪|S|, can lead to substantial gains over a single policy improvement step in this region, thus accelerating convergence and leading to good solutions in a reduced time. This effect is illustrated in Section 5.To illustrate the proposed approach, we solve a production and inventory problem with 3 classes of customers and a single machine, which can produce one product at a time and can alternate between products without any significant setup time. The demands for each class of customers is Poisson and arrives at rates3,2, and 1, respectively, while the production time follows an exponential distribution with rate 8. We also assume that each demand order is comprised of a single item. The stock/deficit cost is given by:c(x)=|x1|+2|x2|+3|x3|.Thus, the stock/deficit costs are ordered in the inverse order of the demand rates, with the most demanded items having the smallest stock/deficit cost. Whenever a new demand arrives or the production of a new product is finished, the decision maker decides whether to continue the production for the same class of customers or to produce for another class of customers, or alternatively, to halt production. If the production is halted at the time a decision is made, the decision maker decides whether to keep the production halted or to start the production for one of the classes of customers. We also note that the production facility can continue production even if there is no customer waiting.The maximal allowed backlog for each product is of 100 units, with no demand being accepted when this backlog level is reached and the maximal stock level allowed for each product is 25 units. Hence, the stock level of each product is an integer variable in the range{-100,…,25}. Hence, the state space S is comprised of1263≈2·106elements, each corresponding to a possible stock/deficit combination for the three products. The objective is to find the policyL∗which minimizes the average cost and satisfies expression (2).The proposed problem was solved up to a tolerance of103in a Intel Core i5, 2.8GHz, 4GB RAM Computer, running Windows 7. We compared the results obtained by Algorithm 1 with those obtained by a standard policy iteration algorithm (see, e.g., Puterman (1994)). Both algorithms yielded a long term average stock/deficit cost of 3.4095 monetary units, and their convergence is depicted below. For the proposed algorithm, we pick the region F as the set of states whose stock/deficit levels are in the interval[-10,9], i.e.F≔{x∈S:-10⩽x1⩽9,-10⩽x2⩽9,-10⩽x3⩽9}. Both algorithms were initialized with the same ad-hoc policy, which prescribes the production of the product with the lowest stock level, whenever this level is up to a maximum of 10 units. Otherwise, the production is halted. Whenever the lowest stock level is common to two or more products, we choose to produce that with the highest stock/deficit holding cost.Fig. 2shows the average cost evolution versus cumulative computation time for both the standard policy iteration algorithm and the proposed algorithm, with each bar corresponding to a single iteration. Note that the decrease in the average cost for the proposed 2-phase algorithm is much steeper, with the algorithm converging in 5 iterations. The policy iteration algorithm, on the other hand, takes 13 iterations to converge, with a less incremental decrease in the value function at each iteration. It is worth pointing out that the immediate decrease in the value function at the first iteration is due to the optimization in the set F that is performed at Step 2 of Algorithm 1. It may be argued that the region F is more important from a control standpoint, once one would expect a properly optimized policy to spend most of the time visiting low stock/deficit states. Hence, it may be expected that the actions taken at those states impact more in the long term average cost, and that is corroborated by the experimental results.Comparing the results in Fig. 2 in terms of the cumulative computation time, one notes that the proposed approach is also significantly superior in terms of total elapsed time. In fact, the computation time is reduced from around 1422min to around 260min, which means that the proposed algorithm converges in about18,3%of the total policy iteration convergence time. The experimental results are summarized in Table 1below. One can see that, in this case, optimizing within an attractive region of the state space in terms of instantaneous cost function leads to the vicinity of the optimal policy in just two iterations, while it takes 12 iterations for the standard policy iteration to achieve similar performance. As pointed out earlier, the states in F tend to be visited more often in the long term, thus contributing more to the long term average cost. As a result, an optimization of the control policy in F tends to lead to substantial gains in the long term average cost, as the experiments demonstrate.We saw in the last section that the use of the proposed algorithm can significantly enhance the performance with respect to standard policy iteration for a given selected subset F. In this subsection we strive to evaluate the effect of the set F on the performance of Algorithm 1. To accomplish that we set the region F as the set of states whose stock/deficit levels are in the intervalI=[I1,I2], i.e.F≔{x∈S:I1⩽x1⩽I2,I1⩽x2⩽I2,I1⩽x3⩽I2}, and varied the interval I from[-8,10]to[6,25], adding up a single item in bothI1andI2at each experiment. We note that the cardinality of the set|F|is kept constant throughout the experiments, as the intention is to evaluate the effect of the choice of subsets F of the same cardinality, but with different locations within the state space S.The results of the experiments showed that as we increased the values ofI1andI2, the average cost at the first iteration consistently deteriorated, but such a deterioration did not affect the performance, for the algorithm continued to converge in 5 iterations, with no perceivable change in the computation time. However, as we reached the intervalI=[4,23], the algorithm took six iterations to converge and required about 20% more computation time than the original intervalI=[-10,9]that was used in the experiments of Section 5. From this intervalI=[4,23]to the last intervalI=[6,25], we note increases both in the total iteration count and also in the overall computation time. The experimental results for selected intervals are displayed in Fig. 3.To avoid showing many similar results, we selected six intervals to display in Fig. 3. To facilitate the comparison, the cumulative computation time is normalized with respect to that obtained in the experiment of Section 5. Note that, although the average cost at the first iteration varies significantly for intervals[-9,10]and[3,22], the algorithm manages to maintain its performance both in terms of iteration count and computation time. For the remaining intervals, however, the changes are more significant: intervalI=[4,23]requires six iterations and about 120% of the computation time for the original F, while intervalI=[5,24]requires 7 iterations and 140% of the same reference computation time. The worst performance comes for interval[6,25], that requires 9 iterations and around 165% of the reference computation time.The results suggest that, in this case, the algorithm seems to be relatively robust to the choice of F, its performance varying slowly around the set F suggested in the last section. That is a very interesting result, in that it shows that small variations in the set F tend not to compromise the performance of the algorithm. In the experiments we saw that the performance deteriorates once we get too far from the zero-stock region. That seems to be, in this case, because the low stock/deficit states tend to be more attractive from a control standpoint, once their costs are the most attractive. Hence, they tend to be the most visited if the system is properly managed, thus contributing more to the overall average cost. Selecting states close to this region ensures, in the present example, that the optimization in Step 2 of Algorithm 1 has the potential to result in substantial gain in terms of the long term performance of the system, as demonstrated by the experimental results. Finally, it is worth mentioning here that it should be necessary to go more deeply on the algorithm robustness issue regarding the choice of F, since the conclusion here was drawn from just one example. This also applies to the claim here regarding the low stock/deficit region as the most attractive region from a control standing point. Further foray on these issues is certainly necessary.Considering the discussion in the preceding paragraph, one can infer that the performance will tend to that of the standard policy iteration algorithm for a choice of F too far the zero stock level. To demonstrate this effect we set up a new experiment where Algorithm 2 is run withF≔{x∈S:-100⩽x1⩽-81,-100⩽x2⩽-81,-100⩽x3⩽-81}. The computation time is normalized with respect to that of the standard policy iteration algorithm, and the results are displayed in Fig. 4.As expected, the performance of the proposed algorithm is similar to that of standard policy iteration. That happens because we chose to compose the region F a set of states which are very bad from a control standpoint: large cost states which tend to be seldom visited for any reasonably good production/inventory policy. As a result, an optimization in this region does not result in an effective improvement in performance, as these states tend to contribute very little to the long term average cost. The results in Fig. 4 are consistent with this evaluation.This paper proposes a two phase time aggregation based algorithm for solving average cost Markov decision processes. The algorithm combines a time aggregation step within a region of interest F and a stochastic shortest path solution procedure outside of this region. The first step determines a sub-optimal control in F and the value function corresponding to the best solution, given a prescribed control policy inFc. Such a value function, used as termination cost of the stochastic shortest path problem, allows one to find the value function inFc, which in turns permits the application of a classical policy improvement step that leads to the improvement of the control policy inFc. Both steps are alternated until the algorithm converges. In addition, the convergence of the proposed algorithm to the optimal solution is proved and a numerical example illustrates the proposed approach.The numerical experiment suggests that the optimization step in the subset F of the state space, prescribed by the proposed algorithm, can significantly reduce the convergence time both in terms of iteration count and total computation time with respect to standard policy iteration.Future research directions include the application of the proposed approach to very large scale problems, possibly with a partial solution of the stochastic shortest path problem, constrained to a specific target region of the setFc. Another possible line of research involves applying reinforcement learning techniques to approximate the value function in the complementary subsetFcfor a fixed control policy, which would enable a posterior policy update step in this region.

@&#CONCLUSIONS@&#
