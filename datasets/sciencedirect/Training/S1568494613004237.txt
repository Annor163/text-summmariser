@&#MAIN-TITLE@&#
Perceptual relativity-based semi-supervised dimensionality reduction algorithm

@&#HIGHLIGHTS@&#
The perceptual relativity has been applied to improve the performance of classification on the sparse, noisy or imbalanced data, indicating the possibility of other perceptual laws in cognitive psychology being considered for classification.A novel dimensionality reduction method has been designed for semi-supervised dimensionality reduction combined with relative transformation. It is more adaptive to parameters selection problem and obtain better performance.

@&#KEYPHRASES@&#
Relative transformation,Side-information,Local reconstruction,Semi-supervised learning,Graph construction,

@&#ABSTRACT@&#
As we all know, a well-designed graph tends to result in good performance for graph-based semi-supervised learning. Although most graph-based semi-supervised dimensionality reduction approaches perform very well on clean data sets, they usually cannot construct a faithful graph which plays an important role in getting a good performance, when performing on the high dimensional, sparse or noisy data. So this will generally lead to a dramatic performance degradation. To deal with these issues, this paper proposes a feasible strategy called relative semi-supervised dimensionality reduction (RSSDR) by utilizing the perceptual relativity to semi-supervised dimensionality reduction. In RSSDR, firstly, relative transformation will be performed over the training samples to build the relative space. It should be indicated that relative transformation improves the distinguishing ability among data points and diminishes the impact of noise on semi-supervised dimensionality reduction. Secondly, the edge weights of neighborhood graph will be determined through minimizing the local reconstruction error in the relative space such that it can preserve the global geometric structure as well as the local one of the data. Extensive experiments on face, UCI, gene expression, artificial and noisy data sets have been provided to validate the feasibility and effectiveness of the proposed algorithm with the promising results both in classification accuracy and robustness.

@&#INTRODUCTION@&#
Real world data, such as digital photographs, gene expression profile, face data sets and web text, usually have the character of high dimensionality. To avoid the problem of “curse of dimensionality” possibly existing in high-dimensional tasks, dimensionality reduction is often conducted [1]. Dimensionality reduction is widely recognized as one of the key steps in areas such as computer vision, machine learning and pattern recognition and its main goal is to map high dimensional data into a meaningful representation of lower dimensional space in which important features are preserved. Various useful feature extraction methods have been proposed for decades, among which Principal Component Analysis (PCA) [2] and Linear Discriminate Analysis (LDA) [3] are two most classical and well-known linear subspace learning methods. As an unsupervised method, PCA seeks projections with the covariance of samples maximally preserved and meanwhile ensures the extracted features have least reconstruction error. In contrast to PCA, LDA is a supervised method, which aims to search for a set of projection vectors such that the ratio of between-class scatter to within-class scatter is maximized. For the use of class label information, LDA is often more powerful in discriminant applications than PCA. However, if instances sampled from a space confined to nonlinear subspace, the embedding results of these two methods are both distorted. So with the presentation and development of manifold learning, lots of nonlinear feature extraction methods such as LLE [4], ISOMAP [5], LTSA [6], and so on, have been proposed, to address this problem.It is notable that in many practical classification tasks, labeled samples are fairly expensive to obtain as labeling often requires expensive human labor and much time, and meanwhile unlabeled samples are very easy to collect. However, in supervised learning, the unlabeled samples are completely excluded from the training process, which leads to a potential waste of valuable classification information buried in unlabeled samples. Therefore, “Semi-supervised Learning” with both labeled and unlabeled data has recently attracted more and more attention [7]. The goal of semi-supervised classification is to use unlabeled data to improve the generalization of the classifier. By using unlabeled samples, semi-supervised approaches usually have better generalization ability than the corresponding supervised ones. And, due to the extra focus on labeled samples, semi-supervised approaches often possess higher performance than the unsupervised ones. As we all know, pairwise constraints information also called side-information is more general than label information, since we can obtain side information from label information but not vice verse [8]. So learning with side information is becoming an important area in the machine learning. With this in mind, Zhang et al. [9] proposed semi-supervised dimensionality reduction (SSDR), which utilizes both the must-link and cannot-link constraints effectively. However, it fails to preserve the local structure of the data except preserving the global covariance structure. Even worse, SSDR is sensitive to noise and outliers, because it considers the similarity between samples coarsely. Cevikalp et al. [10] proposed constrained locality preserving projections (CLPP) recently, which can use the must-link and cannot-link information and also use the unlabelled data by preserving local structure. However, there's no reliable approach to determine an optimal parameter t to construct the adjacency graph when there exist noise and outliers of samples. Wei and Peng [11] proposed a method named the neighborhood preserving-based semi-supervised dimensionality reduction algorithm (NPSSDR), which makes full use of side information, not only preserves the must-link and cannot-link constraints but also can preserve the local structure of the input data in the low dimensional embedding subspace. However, it is still susceptible to noise and outliers as it equally treats the noisy samples and normal samples. All of these methods mentioned above are variants of graph embedding [12]. Graph construction is crucial to graph embedding, so constructing a faithful graph of samples for graph embedding is promising. Unfortunately, as the dimensionality of sample increases, the distance metric of samples becomes meaningless. Consequently, the graph structure of samples is deteriorated.Meanwhile even worse, in real-world life, there will be abundant of the sparse, noisy and imbalance data, which will influence the construction of graph and hence lead to the degraded performance of semi-supervised dimensionality reduction. Considering that past semi-supervised dimensionality reduction methods are sensitive to the selection of neighborhood parameter and rely more on the construction of graph, a novel algorithm of relative semi-supervised dimensionality reduction (RSSDR) is proposed from a different respective in this paper, to overcome this problem by utilizing the perceptual relativity in terms of cognitive psychology to semi-supervised dimensionality reduction. It performs the relative transformation [13] over the training samples to build the relative space. Subsequently the algorithm set the edge weights of neighborhood graph through minimizing the local reconstruction error in the relative space and can preserve the global geometric structure of the sampled data set as well as preserving its local one. The feasibility and effectiveness of RSSDR are verified on the high-dimensional, sparse and noisy data sets with the promising results in classification accuracy and robustness.There are two main contributions of RSSDR in this paper: (1) the perceptual relativity has been applied to improve the performance of classification on the sparse, noisy or imbalance data, indicating the possibility of other perceptual laws in cognitive psychology being considered for classification. (2) A novel dimensionality reduction method has been designed for semi-supervised dimensionality reduction combined with relative transformation. It is more adaptive to parameters selection problem and obtains better performance. Therefore, RSSDR is robust to noise and less sensitive to choice of parameters. So it is more applicable to real world tasks.The rest of this paper is organized as follows: Section 2 presents some basic concepts. A novel method is designed in Section 3. The proposed method is evaluated through experiments in Section 4. The paper is concluded with a summary and discussion of possible future work in Section 5.In this section, we present two concepts that our approach is based on and which will serve as building blocks.Locally linear embedding [4], proposed by Roweis and Saul, tries to find a nonlinear manifold by stitching together small linear neighborhoods. The main principle of LLE is to preserve local neighborhood relation in both the intrinsic space and embedding one by finding a set of weights that perform local linear interpolations that closely approximate the data. Each sample in the observation space is a linearly weighted average of its neighbors. In general, the procedures of LLE can be stated in three steps:Step 1. Defining neighbors for each data point. In this paper, k-nearest neighbor is used to model the relationship between two nearby points, and Euclidean distance is used to define the neighborhood, that is,dij=xi−xj.Step 2. Finding reconstruction weights that allow neighbors to interpolate original data accurately. It can be obtained by minimizing the following reconstruction error:(1)minε(W)=∑i=1Nxi−∑j=1NWijxj2where N(x)irepresents the neighborhood of xiand ∑Wij=1.Step 3. Finding new data points that minimize interpolation error in the space with lower dimension. Computing the low-dimension embedding Y of X that best preserves the local geometry represented by the reconstruction weights. That is to minimize the cost function as follows:(2)minε(W)=∑i=1Nyi−∑j=1NWijyj2where M=(I−W)T(I−W), I is the identity matrix. For more details of LLE, please refer to [4].Compared with machine classification, human being has natural ability in classification on the sparse, noisy and imbalance data, from which we can get some inspiration. To nicely perform the classification on this kind of data, it's natural for machine to learn from human being. The existing classification approaches, such as those to recognize faces, gene expression data, require hundreds if not thousands of samples for training, while human visual recognition just need train through very few samples [14]. This is because humans routinely classify objects according to both their individual attributes and membership in higher order groups, where individual attributes may be influenced and regulated by their group [15]. This can be illustrated from Fig. 1.It can be observed from Fig. 1 that the circle x looks bigger than its original size as it is surrounded by smaller circles while the circle y appears smaller than its original size as it is surrounded by bigger circles. Consequently, when we observe x and y simultaneously, x is perceived to be bigger than y, although they are of the same sizes in fact [16]. This cognitive characteristic is very important for us to distinguish an object from its surrounding objects and can be then formalized using geometry model to process the data more efficiently. One way is to define a transformation on the original space to build a new space whose dimensions are composed of all points in the original space. The newly created space is called the relative space and can be generated through relative transformation. Relative transformation can make the data more distinguishable [17]. Some data can be distinguishable in the relative space while they cannot be identified in the original space. The relative transformation is also simple and efficient in dealing with noisy data or outliers, which can be illustrated in Fig. 2.The Fig. 2 demonstrates that the point x4 may be regarded as a noisy point or an outlier in the original space since it is far away from the other three points, which is consistent with human perception. However, d(x3,x1) is equal to d(x3,x4) in the original space, which means that the point x4 has the same possibility with the point x1 to be taken as a nearest neighbor of the point x3. This is inconsistent with human perception. In contrast, in the relative space, the outlier or noisy point becomes further away from the normal points, so it can be recognized easily. Furthermore, it may also make points, which originally lie on the same surface of the manifold closer to each other and points from the different surfaces further away from each other, which is especially useful to the sparse data. Finally, this approach has a simple mathematical basis and it allows a compact mathematical description of arbitrarily shaped neighborhood in the original space. The relative transformation can be formulated as following:fr:X→Y⊂Rn,yi=fr(xi)=(di1,…,dij,…,din)∈Y,dij=xi−xj, where n is the number of elements in dataset X, the point xiin the original space is mapped to the point yi∈Rnin the relative space.AlgorithmY=fr(X)input:X be the original space;output:Y be the relative space as output.step 1:select the sample x∈X, then calculate the distances between it and any other element xi∈X where distance is defined to be Euclidean distance as LLE.step 2:map the sample x∈X to the point y∈Y in the relative space by the following way:y=(y1,…,yi,…,yX)∈RX,yi=d(x,xi),i=1…Xstep 3:repeat the previous steps until all samples in X are mapped to the relative space Y.Here, the side-information based semi-supervised linear dimensionality reduction problem is defined as follows. First, supposing there is a set of D-dimension points X={x1, x2, …, xn}T, xi∈RD, together with some pairwise must-link (M) and cannot-link (C) constraints as side-information: M:(xi, xj)∈M, if xiand xjboth belong to the same class; and, C:(xi, xj)∈C, if xiand xjboth belong to different classes. Then, a transformation matrix W (W=w1,w2,...,wd∈RD×d(d≪D)) is found such that the transformed low-dimension projection Y(yi=WTxi) can preserve the structure of the dataset as well as the side information. That is, points in M should be close to each other while points in C should be as far as possible from each other. Since it is easy to extend to high dimensions, for the convenience of discussion, the one-dimension case is considered here.As for must-link constraint M and cannot-link constraint C, Qmdenotes the intra-class compactness and Qcdenotes the inter-class separability, respectively. Defined to describe the compactness of labeled samples belonging to the same class, Qmis measured as follows:(3)Qm=∑ij(xi,xj)∈Mor(xj,xi)∈M(wTxi−wTxj)2=2∑i(wTxiDiimxiTw)−2∑ij(wTxiSijmxjTw)=2wTX(Dm−Sm)XTw=2wTXLmXTwSijm=1,if(xi,xj)∈Mor(xj,xi)∈M0,elsewhere Dmis a diagonal matrix whose entries are column (or row, because Smis symmetric) sums of Sm,Diim=ΣjSijm,Lm=Dm−Smis a graph Laplacian matrix that is positive semi-definite.Similarly, the inter-class separability Qcon cannot-link constraints can be termed as follows:(4)Qc=∑ij(xi,xj)∈Cor(xj,xi)∈C(wTxi−wTxj)2=2wTX(Dc−Sc)XTw=2wTXLcXTwSijc=1,if(xi,xj)∈Cor(xj,xi)∈C0,elsewhere Dcis a diagonal matrix,Diis=ΣjSijs, Lc=Dc−Sc.If the goal is just placing samples in the same class to be close to each other while those in different classes as far as possible from one another, one can define a raw objective function similar to LDA criterion in terms of Qcand Qm:(5)w∗=argmaxwQcQm=argmaxwwTXLcXTwwTXLmXTwAlthough Eq. (5) achieves the goal, it only considers the side information and does not make full use of information buried in the unlabeled samples. Therefore, in order to take advantage of unlabled samples, the following assumption (non-neighborhood hypothesis) is made: the non-neighboring samples in high-dimension space are as far as possible from one another in the projected subspace. For example, if xiis far from xjin high-dimension space, their low-dimension projection should be as far as possible from each other.Firstly, a term Qfis introduced to describe the global topological structure and measured as follows:(6)Qf=∑ij(wTxi−wTxj)2Sijf=2wTX(Df−Sf)XTw=2wTXLfXTwSijf=1,ifxi∉Nk(xj)andxj∉Nk(xi)0,elsewhere Dfis a diagonal matrix,Diif=ΣjSijf,Lf=Df−Sf.Secondly, as we all know, graph is used to characterize data geometry (e.g., manifold) and thus plays an important role in data analysis including graph-based semi-supervised dimensionality reduction. Despite its importance, graph construction has not been studied extensively [8], and the way to establish high-quality graphs is still an open problem [18]. In recent years, fortunately, graph construction problem has attracted substantial attention [19–21,18]. Due to the impact of the high-dimensional, noisy, sparse and imbalance data, the topological structure of constructed neighborhood with most current approaches is unstable. To address these issues, based on the relative cognitive law, by which the relative space is further constructed. The relative transformation can improve the distinguishing ability among data points and reduce the impact of noise and sparsity such that it may be more line with people's intuition. So, to capture its local topological structure besides preserving the global topological structure of the data set, we want to set the edge weights of neighborhood graph through minimizing the local reconstruction error in the relative space. To measure this character, a term Qr1 is introduced to describe the relative local topological structure. The local reconstruction error in the relative space is detailed as follows:local reconstruction error in the relative spaceinput:X be the original space; k be the neighborhood size.output:local reconstruction error M.step 1:turn X into the relative space according to the function Y=fr(X).step 2:in the relative space, calculate xi∈X find its neighborhoods Nr(xi).step 3:we assume that all the neighborhoods of input space are linear as Roweis and Saul [4] did, so each data point can be optimally reconstructed using a linear combination of its neighbors. Hence, local reconstruction error can be written according to the Eq. (1):ε1r(A)=∑ixir−∑j,xjr∈N(xir)Aijxjr2step 4:in the low dimensional space, we want to minimize the cost function as follows according to the Eq. (2):ε2r(Yr)=∑iyir−∑j:xjr∈N(xir)Aijyjr2=trace(YMrYT)whereMr=((I−W)r)T(I−W), I is the identity matrix.Finally, let Qrl=Mrand by integrating both Eqs. (5) and (6), we can form an objective function of RSSDR as follows:(7)w∗=argmaxwQc+αQfQm+βε2r=argmaxwQc+αQfQm+βQrl=argmaxwwTX(Lc+αLf)XTwwTX(Lm+βLrl)XTwTo solve the above optimization problem, we use the Lagrangian multiplier and finally we have:X(Lc+αLf)XTw=λX(Lm+βLrl)XTwwhere α, β are control parameters to balance the importance of Qfand Qrl. It is easy to know that the d eigenvectors corresponding to the d largest eigenvalues can form the transformation matrix W. Here X(Lm+βLrl)XTmight be singular, we often use PCA as a pre-processing tool with the 98% principal components preserved (also reduce noise), then we apply dimensionality reduction techniques (RSSDR, NPSSDR, SSDR, CLPP) on the projected data.The performance of semi-supervised learning can be enhanced by combination with relative transformation. Here, we resort to perceptual relativity techniques to address the graph construction of semi-supervised methods and propose relative semi-supervised dimensionality reduction. Generally RSSDR can be stated in five steps as follows:Algorithm RSSDR (X,M,C,α,β,k)input:X be the original space; side information: M and C constraints, α, β be balancing parameters, k be the neighborhood size.output:transformation matrix W∈RD×d(d≪D).step 1:construct intrinsic graph based on positive constraints and define its weight matrixSijmas:Sijm=1,if(xi,xj)∈Mor(xj,xi)∈M0,elseand the corresponding Laplacian matrix Lmbased on it is Lm=Dm−Smwhere Dmis a diagonal matrix withDiim=ΣjSijm.step 2:based on the negative constraints, construct penalty graph describing the inter-class separability as follows:Qc=∑ij(xi,xj)∈Cor(xj,xi)∈C(wTxi−wTxj)2=2wTX(Dc−Sc)XTw=2wTXLcXTwSijc=1,if(xi,xj)∈Cor(xj,xi)∈C0,elsestep 3:inspired by the good performance of relative transformation on the high-dimensional, noisy and sparse data. Firstly, we turn the original space into the relative space according to the function Y=fr(X) and set the edge weights of neighborhood graph through minimizing the local reconstruction error through LLE method in the relative space. Define its relative weight matrixSijras follows:Sijr=1,ifxi∈Nkr(xj)andxj∈Nkr(xi)0,elsewe obtain M which describes the local topological structure according to:ε2r(Yr)=∑iyir−∑j:xjr∈N(xir)Aijyjr2=trace(YMrYT)step 4:the global topological structure of data sample Qfis defined as:Qf=∑ij(wTxi−wTxj)2Sijf=2wTX(Df−Sf)XTw=2wTXLfXTwstep 5:set α, β, k and let the predictive functionw∗be as follows:w∗=argmaxwQc+αQfQm+βε2r=argmaxwQc+αQfQm+βQrl=argmaxwwTX(Lc+αLf)XTwwTX(Lm+βLrl)XTwstep 6:obtain transformation matrixW=[w1,w2,...,wd]

@&#CONCLUSIONS@&#
Graph's construction plays an important role in graph-based semi-supervised learning methods. Fortunately, graph's construction has received substantial attention especially in recent years. Inspired by the good performance of relative transformation method on classification, this paper defines a novel method RSSDR integrating both relative transformation and semi-supervised dimensionality reduction, which considers the graph construction among data samples in the relative space instead of original space. Because of its simplicity, generality and distinguished ability in dealing with sparse and noisy data, the advantage of RSSDR not only achieves amazing result but also is robust to noise. The most importance of RSSDR seems that it opens a new direction as a fundamental methodology to develop new semi-supervised methods, which simulate all kinds of cognitive laws in terms of cognitive psychology. In the future, we plan to take into more cognitive laws into account, such as CGM [26] based on which a new classifier is designed and obtains good performance by utilizing CGM to find k nearest neighbors. We will also develop more variants of techniques in terms of more cognitive laws and apply them to semi-supervised methods for cancer classification and affective computing.