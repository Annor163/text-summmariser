@&#MAIN-TITLE@&#
FleCube: A flexibly-connected architecture of data center networks on multi-port servers

@&#HIGHLIGHTS@&#
Propose novel directly-connected architectures.Study the properties behind the novel architecture.Design the single-path routing and multi-path routing algorithm.

@&#KEYPHRASES@&#
Multi-port server,Data center networks,Multi-path routing,

@&#ABSTRACT@&#
Underlying network provides infrastructures for cloud computing in data centers. The server-centric architectures integrate network and compute, which place routing intelligence on servers. However, the existing multi-port server based architectures suffer from determined scale and large path length. In this paper, we propose FleCube, a flexibly-connected architecture on multi-port servers without using any switches. FleCube is recursively constructed on division of multiple ports in a server by means of complete graph. FleCube benefits data center networks by flexible scale and low diameter, as well as large bisection width and small bottleneck degree. Furthermore, we develop multi-path routing (MPR) to take advantage of parallel paths between any two servers. MPR adopts random forwarding to distribute traffic load and relieve network congestion. Analysis and comparisons with existing architectures show the advantages of FleCube. Evaluations under different degrees of network traffic demonstrate the merits of FleCube and the proposed routings.

@&#INTRODUCTION@&#
Underlying architecture of data center networks (DCNs) provides infrastructures for cloud computing applications, such as web search, email and on-line gaming, as well as infrastructural services, such as GFS [10], HDFS [4], and BigTable [5]. The topologies of existing DCNs architectures fall into switch-centric and server-centric architectures [33]. Fat-tree [2], VL2 [11], Portland [26], Jellyfish [28], S2 [32], Scafida [17], Poincare´[8], and SWDC[27] belong to the former category, in which servers are attachments of switches fabric. DCell [14], BCube [13], CamCube [1][7], FiConn [21], HCN&BCN [15], DPillar [24], SWCube&SWKautz [22], and FSquare [23] fall into server-centric category, in which servers undertake the task of processing and forwarding data. According to the usage of switch, server-centric architectures fall into two categories: with and without using switches. Without switches, the directly-connected architectures thoroughly place routing intelligence on servers. Recent research shows that, based on hardware forwarding, the performance of multiple ports in servers is close to that in commodity switches [25]. With the enhancement of forwarding function of servers, multi-port servers will be universally deployed in future DCNs [14][13][1]. This paper studies the architecture of DCNs without using any switches.The existing directly-connected architecture, CamCube [1][7] is constructed from a 3D torus topology by replacing nodes with 6-port servers. CamCube integrates the overlay and underlying network, and can provide coordinate-based API to send/receive packets with fault-tolerance. However, based on 6-port servers, CamCube suffers from coarse design space. CamCube can only be built at sizes of n3, which is corresponding to n-ary 3-cube. Furthermore, CamCube suffers from a large path length in large-scale data centers, e.g., in a 20-ary 3-cube with 8,000 servers, the largest path length is 30 and the average path length is 15. To overcome the above deficiencies, we propose a novel directly-connected architecture on multi-port servers.In this paper, we propose FleCube, a flexibly-connected architecture for interconnecting multi-port servers, without using any switches or routers. FleCube is recursively constructed on division of the multiple ports of servers, in which a high-level FleCube is built from low-level FleCubes by means of complete graph. In spite of the flexibly-connected structure, FleCube demonstrates various advantages in DCNs design. FleCube enjoys the flexible structure on the given number of ports in a server, and the number of servers in FleCube grows double-exponentially with the length of division. For example, given the port number of 12 in a server, FleCube on division {4, 4, 4} and {3, 3, 3, 3} accommodates 44205 and 0.2 billion servers, respectively. FleCube provides a large number of parallel paths and large bisection width, which can distribute traffic load and relieve congestion on network. Network diameter and bottleneck degree are small in FleCube, which can improve network efficiency and fault tolerance. For example, level-3 FleCube with tens of thousands servers has a diameter of 7, and level-4 FleCube with hundreds of millions of servers has a diameter of 15. To take advantage of parallel paths, we propose multi-path routing (MPR), which adopts a random forwarding mechanism to distribute traffic load and relieve network congestion. In MPR, flows are confined in a small range of networks and have no effect on higher level. We conduct simulations on FleCubes under different degrees of traffic to evaluate the performance of FleCube and MPR. Simulations reveal the performance of FleCube in average length of routing path and the capability of MPR in spreading out network congestion.The rest of paper is organized as follows. Section 2 describes physical structure and properties of FleCube. Section 3 presents multi-path routing. Section 4 gives comparisons. Section 5 evaluates FleCube and routing, and Section 6 concludes our work.In this section, we present the physical structure and the properties of FleCube.FleCube uses servers equipped with multiple ports to construct its architecture. In FleCube, a multi-port server is directly connected to other servers via bidirectional communication links, without using any switches or routers.FleCube is a recursively defined architecture on division of the multiple ports of servers. If each lower-level FleCube is treated as a virtual node, a higher-level FleCube is constructed from the lower-level FleCubes by means of complete graph.For clarity, let n denote the number of ports in a multi-port server. Let permutation {k1, k2,…,kr} denote a division of n, with length of r and ki(ki≥ 1) ports in group i (i ∈ [1, r]). Let FleCubeidenote a level-i FleCube, and fidenote the number ofFleCubei−1s in a FleCubei. LetFleCubei−1[j](j ∈ [0, fi)) denote the jthFleCubei−1in a FleCubei. A FleCube constructed on {k1, k2,…,kr} is also referred as FleCube{k1, k2,…,kr}. In Fig. 1, we taken=4with division {3, 1} as an example to illustrate the construction of FleCube.A level-1 FleCube is constructed usingk1+1n-port servers by means of complete graph via ports in group 1. As shown in Fig. 1, there are 4 servers connected to each other into a complete graph in each FleCube1. Assuming severs in FleCube1 are arranged in a logical cycle. Each server is assigned an identifier a1 in a clockwise direction, taking a value from [0,ki+1). The link between two servers in FleCube1 is referred as the level-1 link. In FleCube1, we definef1=k1+1. A level-2 FleCube is constructed usingf2=k2·s1+1FleCube1s via ports in group 2. In a FleCube2, all FleCube1s are connected by means of complete graph, if each of them is treated as a virtual node. In Fig. 1, there are 5 FleCube1s interconnected into a complete graph in the FleCube2. Assuming FleCube1s are arranged in a logical cycle. Each FleCube1 is assigned an identifier a2 in a clockwise direction, taking a value from [0, f2). For a higher-level FleCubei, it is constructed in the same way as above. The procedure of building a FleCubeifromFleCubei−1s is shown in Algorithm 1. The link betweenFleCubei−1s in a FleCubeiis referred as a level-i link.Let sidenote the number of servers in a FleCubei. To construct a FleCubeionFleCubei−1s with kiports of each server, the relationship between fiandsi−1satisfiesfi=ki·si−1+1.Notice that each server in FleCube1 is assigned an identifier a1 and eachFleCubei−1in FleCubeiis assigned an identifier ai, i ∈ [2, r]. We assign each server in FleCubera r-tuple in form of [ar,ar−1,…,a2, a1], ai∈ [0, fi) for i ∈ [1, r]. In this tuple, a1 indicates the index of a server in FleCube1 where it is located, and aifor i ∈ [2, r] indicates the index ofFleCubei−1in FleCubeiwhere the server is located.Note that there is a level-i link between any twoFleCubei−1s in a FleCubei. This link is adopted as routing path between any two servers among this twoFleCubei−1s. As any two servers are connected with a level-1 link in a FleCube1, this link services as routing path between this two servers.Let src and dst denote the source and destination servers, respectively. Assuming that they are in the same FleCubeiand differentFleCubei−1s. LetFleCubei−1srcandFleCubei−1dstdenote the twoFleCubei−1s where src and dst locate, respectively. Let (s1, s2) denote the level-i link betweenFleCubei−1srcandFleCubei−1dst,where s1 and s2 locate inFleCubei−1srcandFleCubei−1dstrespectively. Algorithm 2shows the procedure of path generation from src to dst. It first checks whether src and dst are adjacent. If so, it returns the link between them (lines 1–3). If not, it gets the level-i link (s1, s2) betweenFleCubei−1srcandFleCubei−1dst(line 4). Then the whole routing path is divided into three parts: src to s1, (s1, s2), and s2 to dst (lines 5–6). The complete path from src to dst is (src, s1)+(s1, s2)+(s2, dst) (line 7). Take Fig. 1 as an example, let [1, 0] and [3, 1] denote the source and destination servers, respectively. They are in the same FleCube2 and different FleCube1s. The algorithm first gets ([1, 3], [3, 2]) between FleCube1[1] and FleCube1[3]. Then it gets ([1, 0], [1, 3]) and ([3, 2], [3, 1]) in FleCube1[1] and FleCube1[3] independently. The complete path between [1, 0] and [3, 1] is ([1, 0], [1, 3], [3, 2], [3, 1]).The routing path generated by Algorithm 2 follows a divide-and-conquer principle, which takes advantage of the recursive characteristic of FleCube. We refer this routing as divide-and-conquer routing (DCR).FleCube is constructed on the flexible division of multiple ports in terms of complete graph, which provides it several nice properties for data center networks. In this section, we study the topological and routing properties, which serve as the foundation of performance for FleCube. We first present the scalability, flexibility, and parallel paths in FleCube. Then we study the diameter, bottleneck degree, and bisection width of FleCube. Finally, we study how many servers can be accommodated in a FleCube given the network diameter and the number of ports in a server.Theorem 1The number of servers, sr, scales double-exponentially with levels r in FleCube.Note thatsi=fi·si−1andfi=ki·si−1+1,thus, we havesi=ki·si−12+si−1for i > 1. For the division {k1, k2,…,kr} with ki≥ 1 (i ∈ [1, r]), we havesi≥si−12+si−1=(si−1+12)2−14>(si−1+12)2−12. Thus, we havesi+12>(si−1+12)2>(s1+12)2i−1for i > 1. Note thats1=k1+1,therefore, we havesi>(k1+32)2i−1−12for i > 1. So we havesr>(k1+32)2r−1−12. Therefore, srscales double-exponentially with levels r in FleCube.□Theorem 1 shows that FleCube has an excellent scalability for large-scale data center networks. For example, withn=12,FleCube3 defined on division {4, 4, 4} accommodates 44205 servers, and FleCube4 on {3, 3, 3, 3} accommodates as many as 0.2 billion servers.A concerned question is how many FleCubes can be created, given the number of ports n in a server. The quantity of FleCubes can be measured by the number of permutations of {k1, k2,…,kr}, with∑i=1rki=nand ki≥ 1. Let gndenote the number of permutations, then we have the following theorem on gn,Theorem 2gn=2n−1,where n is the number of ports in a server.For an n-element set in a line, there aren−1interspaces among these elements. We insertr−1clapboards into thesen−1interspaces, with no more than one clapboard in each interspace. Then, n elements are divided into r segments. The number of elements in segment i (i ∈ [1, r]) is treated as kiin corresponding FleCuber. There are(n−1r−1)kinds of inserting clapboards for a r-division. As r taking a value from 1 to n, the total number of permutations is∑r=1n(n−1r−1). It is the sum of binomial coefficients in the polynomial expansion of(1+x)n−1. Letx=1,we getgn=2n−1.□Theorem 2 shows there are2n−1FleCubes that can be created, if n is given. For example, withn=12,there are 2048 FleCubes can be constructed, and withn=24,this number is about 8 million. The flexibility of FleCube allows it to meet variable scale of DCNs.Edge-disjoint parallel paths provide redundant paths for routing, which can be used to reduce load on a single link. Let Prdenote the number of edge-disjoint parallel paths between any two servers in FleCuber. We have Theorem 3 on Pr:Theorem 3Pr=n,where n is the number of ports in a server.Consider servers a and b. Each of them has a set of adjacent servers, denoted by Setaand Setb, respectively. For the order of both sets, we have|Seta|=|Setb|=n. The relationships of elements between Setaand Setbfall into 3 categories. (1) If a and b are adjacent, we have a ∈ Setband b ∈ Seta. In this case, there is a link between a and b. (2) If a and b share neighbor(s), the number of neighbors is|Seta⋂Setb|. In this case, there is a path between a and b via each of the common neighbors. (3) Apart from (1) and (2), each of the left servers in Setabelongs to or connects to aFleCuber−1,which is different with others. It is similar in Setb. No matter theseFleCuber−1(s) are shared or not, there are paths between Setaand Setbvia theseFleCuber−1(s). This implies there are paths between a and b via the left servers in Setaand Setb. There is no intersection among (1), (2), and (3), and the union of them covers Setaand Setb. Thus, the number of edge-disjoint parallel paths between any two servers is the order of the adjacent set, i.e.,Pr=n.□Theorem 3 shows that, each port of a source server provides an independent path with others to the destination server. For n-port server, FleCube provides n parallel paths between any two servers.Diameter is the maximum path length between any two servers. Let Drdenote the diameter of a FleCuber. We have Theorem 4 on Dr,Theorem 4Dr<=2r−1,where r is the number of levels in FleCuber.Consider a routing path of DCR. Note that, level-i link (s1, s2) recursively divides a FleCubeiinto two independent parts. Let Ridenote the times of recursions. R1 is 1 in a FleCube1. In the worst case,Ri=2Ri−1+1for i ∈ [2,r], where the recursion occurs level by level. This leads the maximum path length between any two servers in a FleCubei, withRi=2i−1. Thus, we haveDr<=2r−1in FleCuber.□Theorem 4 shows that the diameter of FleCube grows exponentially with the number of levels. Considering the double-exponential scalability of the total number of server, diameter is small in FleCube. For example, the diameter is 15 in FleCube4 with hundreds of millions of servers. For DCNs with tens of thousands servers, level-3 FleCube with diameter 7 can meet requirements.Bottleneck degree is the maximum number of flows over a single link under an all-to-all communication. In this model, there is a flow between any two possible servers simultaneously. A small bottleneck degree implies that the communication traffic is spread out over all links. Let pidenote the number of flows over a level-i link under an all-to-all communication. We have Theorem 5 on pi(i ∈ [1, r]),Theorem 5pi=2·fi−1fi·sr−siki+si−12for i ∈ [1, r].For a level-1 link, flows can be divided into two parts: intra- and inter-FleCube1. For the intra part, flow derives from the adjacent servers of this link. The number of flows in this part is 1. For the inter part, we obtain it from the following analysis: A FleCube1 hass1(sr−s1)flows with servers outside this FleCube1. Among them, flows communicating with current server will not travel on level-1 link in this FleCube1, the ratio of which is1f1; The leftf1−1f1ratio of flows will evenly travel level-1 links once in this FleCube1. The total number of level-1 links in a FleCube1 isk1·s12. Thus, each level-1 link carriesf1−1f1·2·s1(sr−s1)k1·s1flows in the inter part. Together, we havep1=f1−1f1·2·(sr−s1)k1+1.For a level-2 link, we follow the same analysis as above. The number of flows in intra part is s12. The number of flows in inter part isf2−1f2·2·s2(sr−s2)k2·s2. We havep2=f2−1f2·2(sr−s2)k2+s12. It is similar for a higher level link. Thus,pi=2·fi−1fi·sr−siki+si−12for i ∈ [1, r].□Let BoDrdenote the bottleneck degree of a FleCuber. As defined, we haveBoDr=max{p1,· · ·, pr}. Characteristic of formula piimplies that the bottleneck degree is a variable on parameters of permutation, which is conducive to different network requirements.Bisection width denotes the minimal bandwidth to be removed to partition a network into two equal parts. A large bisection width implies that the structure provides a high routing performance and fault tolerance for data center networks. Let Brdenote the bisection width of a FleCuber. We have Theorem 6 on the lower boundary of Br,Theorem 6Br>sr·k8,wherek=max{k1, k2,…,kr}.Notice that,si=ki·si−12+si−1for i > 1. Considering piin Theorem 5, we havepi=2·fi−1fi·sr−siki+si−12<2·sr−siki+si−12=2·sr−2·si+kisi−12ki=2sr−si−si−1ki<2srki,for i ∈ [1, r]. Letk=max{k1,k2,…,kr}, thuspi<2srk. Based on [20], Brisk2srtimes of bisection width in its embedding complete graph. Thus,Br>k2sr·sr24=sr·k8,wherek=max{k1,k2,…,kr}.□Let N denote the total number of servers in a FleCube. Theorem 6 shows the bisection width of FleCube is O(N). This implies that FleCube intrinsically provides fault tolerance and multi-path routing on top of it.Another essential issue should be considered is how many servers can be accommodated in a FleCube, given network diameter d and the number of ports n in a server. We have Theorem 7 on this issue,Theorem 7Given network diameter d and the number of port n in a server, the total number of servers N in FleCube satisfiesN>(k1+32)d+12−12,where k1 < n.ConsiderDr<=2r−1in Theorem 4. In terms of d, we have2r−1=d,i.e.,r=log2(d+1). Note that in proof of Theorem 1,sr>(k1+32)2r−1−12. Thus, we haveN>(k1+32)d+12−12,where k1 < n.□Actually, Theorem 7 shows a rough lower boundary of N in terms of n and d. The capacity of servers in FleCube is far greater than it. For example, givenn=10andd=15,the maximum capacity is 1.7 hundred million servers withr=4. Consider the double-exponential scalability, FleCube3 is enough for most multi-port servers based architectures. For example, givenn=10andd=7,FleCube3 constructed on division {5, 3, 2} composes of 26,106 servers.To take advantage of parallel paths, we propose multi-path routing (MPR), which benefits DCNs by relieving network congestion and improving bandwidth utilization.Network bandwidth is scarce resource in cloud computing, which results in fierce competitions among applications. Competitions on bandwidth give rise to optimized solutions, such as throughput-delay trade-off [30][3][29], bandwidth allocation [19][31][16]. However, optimized solutions cannot solve competitions fundamentally. Despite DCR takes advantage of the single path routing in FleCube, it does not consider the large number of parallel paths between any two servers. Multiple paths routing [18][6][9][28] can effectively alleviate the competitions and congestions on a single path, thereby improving network efficiency and reducing latency. To overcome the shortage of DCR, we propose multi-path routing (MPR) in this section.FleCube benefits DCNs from a large number of parallel paths between any two servers. Specifically, each port of the source server provides an edge-disjoint path with others to the destination server. However, can each path be adopted in the multi-path routing? For example, let [1, 1] and [1, 3] denote the source and destination server in Fig. 1. As we can see, paths ([1, 1], [1, 3]), ([1, 1], [1, 0], [1, 3]), and ([1, 1], [1, 2], [1, 3]) are alternative for multi-path routing. Path ([1, 1], [0, 0], [0, 2], [3, 3], [3, 2], [1, 3]) is not suitable to appear in the multi-path routing from [1,1] to [1,3].The large number of parallel paths gives rise to another two challenges: loop routing and across path. In loop routing, a packet will be spread on a circular path until the end of time-to-live (TTL). This will lead to waste of resources in the whole life period of a packet. Across path is a result of duplicate selection of path in a distributed multi-path routing, which partly eliminates the benefits of multi-path routing. Flows will accumulate and cause congestion on the cross path, which increases latency and reduces network utilization.Due to the large server population in data centers, we seek to compute multi-path routing in a distributed manner, relying on local information of the current server. One straightforward solution is that the source server sends flows to its neighbors and these neighbors forward flows to the destination, separately. We refer it as primary multi-path routing (PMPR).We take examples of a FleCube2 in Fig. 2to display the multiple paths in PMPR. In example 1, let [1, 1] and [1, 3] denote the source and destination, respectively. There are 3 paths in PMPR from [1, 1] to [1, 3], composing of paths ([1, 1] ,[1, 3]), ([1, 1], [1, 0], [1, 3]), and ([1, 1], [1, 2], [1, 3]). In example 2, let [1, 1] and [4, 0] denote the source and destination, respectively. There are 4 paths in PMPR from [1, 1] to [4, 0], composing of paths ([1, 1], [1, 2], [4, 3], [4, 0]), ([1, 1], [0, 0], [0, 1], [4, 0]), ([1, 1], [1, 3], [3, 2], [3, 0], [4, 1], [4, 0]), and ([1, 1], [1, 0], [2, 1], [2, 3], [4, 2], [4, 0]).Algorithm 3shows the procedure of primary multi-path routing (PMRP). Let src and dst denote the source and destination, respectively. Assuming they are in the same FleCubeiand differentFleCubei−1s:FleCubei−1srcandFleCubei−1dst. In part I, src randomly sends flows via ports in group 1 to i (lines 1–2). Part II shows the process of a current server cur forwarding flow. Upon receiving a flow, cur checks whether it is the destination. If so, cur delivers the flow to the upper layer and returns (lines 3–6). Otherwise, cur checks whether it is on path(src, dst) of DCR. If so, cur delivers the flow along path(src, dst) (lines 7–9). Else, cur checks whether cur and src are in the sameFleCubei−1. If so, cur randomly forwards the flow from a level-i link (lines 11–13). Otherwise, cur forwards the flows along path(cur, dst) (lines 15–16).PMPR relies on the neighbors of src to forward flows to dst. By passing the third FleCubei−1and path of DCR, PMPR can confine flows in a FleCubeiand avoid loop routing. PMPR achieves its distributed multi-path routing relying on the information of current server.By passing the third FleCubei−1,PMPR can avoid cross path in the upstream. However, as shown in Fig. 3, PMRP cannot avoid cross paths. It is a result of the random selection of path on the current server. Notice that the proof of Theorem 3 provides a solution to edge-disjoint parallel paths between any two servers. We take these paths as multiple paths in multi-path routing (MPR).Algorithm 4shows the generation of parallel paths in MPR. GetAdjacent( ·, i) returns the set of neighbors of a server from level 1 to level i (lines 1–2). The procedure first adds path(src, dst) into the multiple paths of MPR, and sets the corresponding neighbors, u0 and v0, occupied in Setsand Setd(lines 3–5). Then the procedure adds the path of PMPR via available neighbors u and v, until all servers in Setsand Setdare occupied (lines 6–14).MPR benefits multi-path routing by confining flows in FleCubei. Furthermore, MPR has the following properties:Theorem 8The number of paths used in MPR is∑j=1ikj,where i is the index of the lowest level FleCube shared by src and dst.The proof can be obtained from Theorem 3. Theorem 8 implies that the relative position of src and dst determines the number of parallel paths used in MPR. The higher level of FleCubei, the larger number of parallel paths.The following theorem shows the maximum length path used in MPR.Theorem 9The maximum length path in MPR is less than2i−1+3,where i is the index of the lowest level FleCube shared by src and dst.Algorithms 3 and 4 show that some neighbors of src forward flow to the third FleCubei−1s. Then these FleCubei−1s forward flows to dst independently. This will lead to the maximum routing path from src to dst. It is the longest path in a FleCubeiplus two links, from src to the FleCubei−1via a neighbor. Thus, we get2i−1+3.□Contrast to path of DCR, the maximum path length in MPR increases only by 2 links. It is a preference for high-level FleCubeishared by src and dst.CamCube [1][7] and FleCube belong to directly-connected architecture on multi-port servers. Fig. 4illustrates a CamCube network with 27 6-port servers. For a CamCube with the number of servers N, the diameter is about3N32,and the bisection is2N23. Notice that CamCube is constructed on 6-port servers and FleCube can be built on any multi-port servers. In this section, we use FleCube built on 5, 6, 7, and 8-port servers to compare with CamCube built on 6-port servers in various aspects.Given the number of ports in a server and network diameter, we compare the number of networks in CamCube and FleCube. We choose four typical diameter values for comparison,d=3, 7, 15, 31. Fig. 5plots the cumulative curves of the number of networks versus the given diameter of networks. As we can see, with the increment of the given diameter, FleCubes with 7-port and 8-port servers accommodate more networks than CamCube. For 6-port servers, FleCube is not worse than CamCube within the scope of the given diameter. For 5-port servers, FleCube can accommodate more networks than CamCube with a given diameter less than 15.Due to flexibility of ports division, FleCube has a large span of scales. Notice that CamCube is constructed on 6-port servers and FleCube has a double-exponential scalability, we adopt FleCube built on 5,6,and 7-port servers in the comparison of scalability. We use the lower and upper boundary of the number of servers to denote the scalability of FleCube. Fig. 6plots the boundary versus the given diameter of networks. As we can see, each upper boundary is far greater than that of CamCube. Each lower boundary is also greater than that of CamCube when diameter larger than 7. When the diameter is no more than 7, each lower boundary of FleCube is less than that of CamCube. This implies that FleCube has a large span of capacity to accommodate various demands of scale.Given the total number of servers in DCNs, the diameter is an important measure of network performance. In fairness, we use 6-port servers only in FleCube. Fig. 7plots the diameter of networks versus the number of servers accommodated by FleCube and CamCube. As we can see that diameter of FleCube grows in small increment with the exponential growth of server number, while the diameter in CamCube grows exponentially under the same condition.FleCube is a flexible structure defined on the division of the multiple ports, therefore, different divisions will result in different bisection width with a great span. Due to the complexity of the bisection width and the diversity of divisions, we observe the lower boundary of the bisection width within certain network scales. We choose the lower boundary of bisection width of FleCube to compare with that in CamCube. Fig. 8plots bisection width versus the number of servers in FleCube and CamCube. As we can see, when the number of servers is larger than 4096 (16-ary CamCube), each bisection width of FleCube is larger than that of CamCube. For the scale less than 4096, bisection width of FleCube is less than that of CamCube, with several opposite cases. Due to the diverse division of multiple ports and the discrete samples, bisection width in FleCube shows fluctuation characteristics. However, the linear lower boundary of bisection width is a common lower boundary in FleCube, which is larger than that in CamCube.The directly-connected architectures integrate networking in the multi-port servers, while architectures connected through switches rely on switches to forward data. According to [23][12][25], forwarding capacity on multi-port server in hardware is close to that of COTS switches per port, as mentioned in [22] with c=1 in normalized switch delay. Servers in architecture connected through switches are equipped with small number of ports, of which the typical value is 1, 2, 3, and 4; while servers in directly-connected architectures are equipped with more ports. Since servers can send data from each port, the more number of ports in a server benefits architecture by more multi-path routing paths from the source node.In the multi-port server based architectures, DCell, FiConn, and FleCube have a double-exponential scalability of server population on the number of levels, which is more aggressive than BCube. The capacity of DCell, BCube, and FiConn is determined by the number of levels and switch ports, while it is the division of multiple ports of servers in FleCube, which provides a large flexibility. Given the number of ports n in a server and the total number of servers N, Table 1shows comparisons of FleCube with other state-of-the-art server-centric architectures. “BiW” and “BoD” denote bisection width and bottleneck degree, respectively. Diameter is measured in terms of “server-to-server-direct” in FleCube, and it is “server-to-server-via-a-switch” in other architectures. l denotes the number of levels in DCell, FiConn and FleCube. As we can see, BCube and FleCube have the same bisection width and bottleneck degree, which are better than that of DCell and FiConn. Compared with DCell and FiConn, FleCube is a low-diameter network. For modular data centers, BCube has a short diameter of typically 4 in terms of “server-to-server-via-switches”, while it is 3 or 7 in FleCube in terms of “server-to-server”. With the same forwarding capability per port in server and switch, FleCube can provide a good network performance. For large scale data centers, the number of ports in a server in FleCube is typically double or triple of that in DCell or BCube, while there are large number of switches in DCell, BCube, and FiConn. Let m denote the number of ports in a switch. Both DCell and FiConn need N/m m-port switches, BCube needs nN/m m-port switches, and there is no deployment of switches in FleCube. Notice that for the different number of ports in a server, FleCube needs the same or a larger number of wires than other architectures. Take the multiple ports in a server, switches, and wires into consideration, FleCube does not introduce excessive cost in the construction of networks.

@&#CONCLUSIONS@&#
