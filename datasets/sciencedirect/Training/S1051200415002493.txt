@&#MAIN-TITLE@&#
A maximum entropy method to assess the predictability of financial and commodity prices

@&#HIGHLIGHTS@&#
We propose a novel signal processing method for financial time series analysis.We predict the entropy by a least square minimization approach.We evaluate (by theory and simulation) the mean and variance of the predictions.We apply our technique to several sets of historical financial data.The efficiency of our technique is shown versus conventional econometrics approach.

@&#KEYPHRASES@&#
Signal processing algorithms for financial engineering,Maximum entropy theory,Optimum linear prediction,Autocovariance function,Time series analysis,

@&#ABSTRACT@&#
A novel signal processing method for the analysis of financial and commodity price time series is here introduced to assess the predictability of financial markets. Our technique, exploiting the maximum entropy method (MEM), predicts the entropy of the next future time interval of the time series under investigation by a least square minimization approach. Like in conventional ex-post analysis based on estimated entropy, high entropy values characterize unpredictable series, while more stable series exhibit lower entropy values. We first evaluate (by theory and simulation) the performance of our method in terms of mean and variance of the predictions. Then, we apply our technique to several sets of historical financial data, correlating the entropy trend to contemporary socio-political events. The efficiency of our technique for application to financial engineering analysis is shown in comparison with the conventional approximate entropy method (usually applied in econometrics).

@&#INTRODUCTION@&#
Notwithstanding their fundamental importance, signal processing and finance have been usually considered as two separate fields both in the general knowledge and in academic (higher) education [1]. Signal processing applications, which hold promising potential, are yet relatively unexplored within finance [2,3]. Conversely, in the last years there has been an explosive growth in the research area relating economics and mathematical modeling [4,5], especially in the fields of financial markets and trading researches and applications [6–11]. This work aims at exploiting signal processing methodologies applied to finance. In particular, we propose to apply prediction methods, usually employed in signal processing or communications problems, for assessing the predictability of market time series.A typical example of signal processing application in finance is represented by financial stock trading, where most of the operators (i.e. the stock traders) ignore but fully exploit the potentialities of signal processing. They use analysis and prediction methodologies that are classical problems in research related with signal processing. In fact, stock traders usually try to profit from short-term price volatility with trades lasting anywhere, from several seconds to several weeks. Hence, the knowledge about the dynamic characteristics of the series under investigation becomes of fundamental importance in order to effectively perform effective forecasting procedures. The observation of historical data as well as the analysis of their standard deviation (e.g. volatility and price fluctuations) can be useful indicators of the dynamic characteristics of the series. Volatility is strictly related with the amplitude of the series fluctuations: high volatility results in large deviations from the mean, hence stating high unpredictability for that series. The observation of historical data as well as the analysis of their standard deviation (e.g. volatility and price fluctuations) can be useful indicators of the dynamic characteristics of the series. For example, the Chicago Board Options Exchange (CBOE) computes since 1993 the volatility index VIX® to measure market expectations of the near-term volatility implied by stock index option prices. As stated by [12], the VIX® index essentially offers a market-determined, forward-looking estimate of one-month stock market volatility. Most studies in the literature that tackle the information content of implied market volatility employ the VIX® index, see for example [13,14]. Options-implied volatility is typically more informative than time-series volatility models based on stock market index returns for forecasting purposes, though the latter may sometimes carry incremental information [15,16]. Since the VIX® index can be considered as a barometer of the overall market sentiment as to what concerns investors' risk appetite [17], many trading strategies rely on the VIX® index for hedging and speculative purposes. Then, volatility is strictly related with the amplitude of the series fluctuations: high volatility results in large deviations from the mean, hence stating high unpredictability for that series.According to [18], financial market time series (FMTS) may deviate from constancy exhibiting two different behaviors: (i) the series is characterized by high standard deviation, and (ii) the series show a lot of irregularities. It is really important to discriminate between these two cases because they lead to different conclusions about the series predictability. In fact, the degree of variation from the mean is not usually related with unpredictability, while the amount of irregularities drastically affects the further forecasting process, resulting in unpredictable series. For example, if it would be possible to ensure to an investor that the series of future prices would be characterized by a precise sinusoidal pattern (although characterized by high deviation from the mean), then future prices can be planned according to a precise forecasting strategy. An example of a practical forecasting strategy exploited in the presence of sinusoidal patterns is shown in [19,20].We can address, as usual, to standard deviation as a measure of deviation from the mean, while we will use entropy as the metric for evaluating the irregularities and, hence, the predictability of a series. In particular, it is well known since the publication of the pivotal work of the mathematician C. Shannon “A Mathematical Theory of Communications” in 1948, [21], that the concept of entropy is related to that of uncertainty. Hence, high values of the Shannon entropy results in an unpredictable series, while lower values mean less uncertainty and hence a more predictable behavior of that series. This concept has been also applied to non-stationary signals as well. In particular, in [22] it is found that, observing performance of entropy by time, the value of entropy is corresponding to “the predictivity of signals”. Namely, the entropy value becomes smaller, the predictivity becomes higher on the entropy curves by time. This concept has led to the publication of many works in the field of entropy-based analysis of financial markets. For example, the validity of the entropy approach in analyzing financial time series is demonstrated in [23]. Then, in [18], an empirical method for evaluating the entropy of a series is proposed, namely the approximate entropy (ApEn). ApEn is able to obtain the entropy estimation by modifying an exact regularity statistic, namely the maximum entropy method (or Kolmogorov–Sinai entropy). In particular, the authors use the approximate entropy technique as a marker of market stability, with rapid increases possibly foreshadowing significant changes in a financial variable. Entropy has been also used to quantify efficiency in foreign exchange markets, [24], in stock markets [25–28], and also to gain insight into the evolution of the aggregate market expectations [29,30]. Recently, studies focusing on energy commodity markets have been carried out under the entropy-based approach. As an instance, an entropy analysis of crude oil price dynamics is revealed in [31], while evidences from informational entropy analysis in evaluating the efficiency of crude oil markets were discussed in [32]. Then, in [33] a market efficiency index based on the ApEn metric is discussed for application to several energy commodities. However, all the aforementioned works evaluate the entropy of historical data and, applying ex-post considerations, try to declare the predictability of the series, i.e. they implicitly assume that the series under investigation are characterized by a stationary behavior. This means that they suppose that the past statistical features of the analyzed series remain unaltered also in the future.In this paper, we move further by proposing an algorithm to assess the predictability of FMTS (and in particular of energy commodity market time series), by predicting the entropy regarding the future behavior of the series under investigation. We do not estimate the entropy of the analyzed series; rather, we predict the entropy of the next (i.e. future) time interval of the series. We remove the assumption of stationary series (i.e. we work in the presence of non-stationary signals), and then we exploit the maximum entropy method (MEM) to obtain the predicted entropy. In addition, our prediction is performed according to optimum prediction methods, such as the least squares minimization scheme. Finally, and according to the conventional entropy analysis (see for example [18,31,32]), if we predict high entropy values we are facing with unpredictable series, while more stable market time series exhibit lower predicted entropy values.The remainder of this paper is organized as follows. Section 2 depicts the system model highlighting first the maximum entropy method, and then the conventional entropy estimation approach used in finance and economics. In Section 3 our entropy prediction method is described in details, while its performances, in terms of mean and variance of the estimates, are shown in Section 4. Applications of our technique to financial market time series are illustrated in Section 5, versus the conventional ApEn approach, and finally our conclusions are briefly summarized in Section 6.Conventional entropy theories are usually related to infinite data series, corresponding to an infinitely accurate precision and resolution for entropy evaluation. However, practical data are finite time series data, sampled with a sampling rateTsand characterized by limited resolution. The problem is that accurate estimation of the series entropy requires a big amount of data to be processed, and the results will be greatly influenced by the system noise. In 1967, Burg proposed a new approach to spectral estimation by attempting to derive a procedure for high resolution when only a small number of data of the estimates of an autocorrelation sequence are available [34,35]. This is called the maximum entropy method (MEM). MEM gives a highest frequency resolution compared to auto-correlation and covariance methods [36]. In addition, [37] and [38] showed that MEM is equivalent to the least-squares method for fitting an autoregressive (AR) model (or all-pole model) to the given data. MEM relates the entropy rate of a time series with its power spectral density (PSD). Hence, knowing the PSD of a series, allows us to know its entropy rate. Previous characterizations of the maximum entropy spectral density assume that the process is stationary and Gaussian [36,39]. Nonetheless, in economics and financial time series analysis literature there is not a theoretical consensus among researchers that price sequences should exhibit stationary Gaussian process. For example, in the financial literature prices of high volume-traded markets, (such as US markets), are considered to follow a random walk process in order to avoid any trade-off and to eliminate predictable patterns [40]. In the following, we will consider a stationary Gaussian time-series only to show that in this particular case we expect to obtain the maximum entropy (i.e. the entropy upper bound). Then, in our approach (depicted in Section 3) we will completely remove the stationary Gaussian hypothesis.More in details, letx(1),x(2),…,x(N)be a stationary Gaussian time series (of length N samples and with a sampling rateTs) with autocovariance functionCov(k), wherek=−N,…,+N. Then, if we denote withS(ω)the PSD of the Gaussian time series, the entropy rate, h (in the following referred as entropy), is given by the following [37]:(1)h=12ln⁡(2⋅π⋅e)+14⋅π⋅∫−ππln⁡(2⋅π⋅S(ω))⋅dωwhereln⁡(⋅)is the natural logarithm. It is now interesting to underline that the entropy of a finite segment of a stochastic process is upper-bounded by the entropy of a segment of a Gaussian random process, according to (1). This means that a white time series is characterized by the maximum entropy, i.e. it is obviously unpredictable as also noted in [36]. Then, lower entropy values result in more predictable time series, while the value of entropy is not found to decrease for noise [22]. Hence, the entropy can be used as an indicator of the time series predictability.In [41], the ApEn method is introduced to numerically quantify the entropy content of a finite time series, as a measure of the regularity of the series itself (see Fig. 1). The regularity of the series clearly reflects in the predictability of the series. The ApEn computations are conceptually simple and are based on the likelihood that templates in the time series which are similar remain similar on the next incremental comparisons. In other words, the presence of repetitive patterns of fluctuation in a time series renders it more predictable than a time series in which such patterns are absent. ApEn needs two input parameters to be specified in order to evaluate the approximate entropy of a given time series: a block or run length m, and a tolerance window r. Then, the ApEn procedure first measures the logarithmic frequency that runs of patterns that are close (within the tolerance window r) for m contiguous observations remain close (within the same tolerance r) on the next incremental comparison.More in details, the ApEn algorithm can be formalized as follows (see [18] and [41] for a detailed analysis). A time seriesx(n)of length N (i.e. withn=1,2,…,N) sampled at time intervalsTsis considered. The length N can be also related to a time scaleτ=N⋅Ts. Let us now select two m-dimensional sequence vectors,u(i)andv(j), defined as follows:(2)u(i)={x(i),x(i+1),…,x(i+m−1)}(3)v(j)={x(j),x(j+1),…,x(j+m−1)}withi≠j, i≥ 1, andj≤N−m+1. The distance between these two sequences is defined as:(4)du,v(i,j)=max⁡{u(i+q)−v(j+q)}where0≤q≤m−1. If the distance expressed by (4) is smaller than a specified tolerance r, the two vectors are called similar. Then, for each of theN−m+1vectorsu(i), the number of similar vectorsv(j)is given by measuring their respective distances. Now, ifNViis the number of vectorsv(j)similar tou(i), then the relative frequencyfi(m,r,τ)to find a vectorv(j)which is similar tou(i)within a tolerance level r and in the time scale τ, is given by:(5)fi(m,r,τ)=NVi(N−m)where (N−m) is the number of vectorsv(j)≠u(i)that are potentially similar tou(j). Now, we look at the relative frequency of the logarithm of (5), defined as:(6)∅(m,r,τ)=1N−m+1⋅∑i=1N−m+1ln⁡[fi(m,r,τ)]Finally, the approximate entropy is estimated by the following statistics [31]:(7)AE(m,r,τ)=1Ts⋅[∅(m,r,τ)−∅(m+1,r,τ)]Widely used values for the first two input parameters arem=1,m=2, andr=20%of the standard deviation of the analyzed time series. Recently, in [31] a modified version of the ApEn procedure, namely the Multiscale approximate entropy (MApEn), is introduced in order to overcome the aforementioned limitations of the original ApEn, In particular, the authors in [31] apply the MApEn algorithm to energy commodity markets, to characterize and monitor the dynamics of crude oil prices. They consider the entropy of a price time series as an index of the market complexity: high entropy values are related to less predictable market evolution (high complexity market). They evaluate the approximate entropy for different time-scales, performing low-pass filtering of the price difference dynamics. One main drawback of this method is that the low-pass filtering introduces correlation in the analyzed time series (and also changes the original data) so that the estimation of the approximate entropy is biased by this filtering operation and depends on the considered time-scale. For instance, a simple uncorrelated series should be always characterized by high entropy values. However, applying the MApEN method for high time scales results in decreasing the complexity of the random signal, since the low-pass filtering removes the most complex dynamics of the input time series (that now paradoxically exhibits a lower entropy value).The novelty of our approach, namely the maximum entropy estimator (MEE), is that we now predict (ex-ante) the entropy of the next (future) time interval, instead of estimating (ex-post) the entropy of the observed series. In other words, the ApEn as well as the MApEn methods estimate the entropy of the observed series, hence making ex-post considerations about the predictability of the series itself. Conversely, our approach allows us to make some ex-ante considerations (based on the historical observed data) by predicting the entropy of the series in the next time interval. Moreover, the conventional methods implicitly assume the stationarity of the series, i.e. they assume that future values of the series are characterized by the same behavior observed in the past. Here, we completely remove this statement (i.e. we work with non-stationary signals), assuming that future values of the series are different from the observed ones, but can be predicted as a linear combination of these past values. Then, in full accordance with the ApEn-based methods, if the predicted entropy is high, this means that the series under investigation would be characterized by high unpredictability. On the contrary, if we estimate lower entropy values, the series would be characterized by low irregularities, hence resulting in a more predictive behavior for the series itself.The starting point of our proposed signal processing method is to first obtain the predicted autocovariance sequence of the series in the next (unknown) time interval. Then, we can easily obtain its power spectral density, thus finally obtaining the searched entropy according to the MEM theory outlined in (1). In the case of our interest, future values of the predicted autocovariance sequence are obtained as an optimum linear combination of past (observed) autocovariance values. These values are weighted and linearly combined by means of a number p of prediction coefficients. The Levinson–Durbin recursion is used to solve the equations of the AR prediction coefficients that arise from the least-squares formulation [42]. The inputs of the optimum linear predictor are hence p autocovariance sequences.According to the block scheme in Fig. 2, we first divide the input series into a number (K+1) of consecutive blocks. Then, we remove from each block the mean estimated from the previous block. This step is required in order to remove the deterministic components and highlight the innovation process of the series itself. In fact, we are working with financial series that are positive series (i.e. they are series of prices), and characterized by a (positive or negative) trend. If we consider a financial time series as a sequence of random observations, this random sequence, or stochastic process, may exhibit some degree of correlation from one observation to the next [43]. Exploiting the correlation structure allows us to decompose the time series into a deterministic component (expressed as a function of any information known at the previous time, including past innovations) and a random component (i.e., the uncertainty or the innovation). In the context of financial series, the random components are interpreted as the market innovations, and usually assumed to be Gaussian processes [44]. Therefore, we assume original observation of financial sequence (given data) model free, but contamined by a stationary Gaussian process as an error term (i.e. innovation term) into it. Hence, we estimate the deterministic components as the mean of the previous block, and then we subtract this mean from the next block. The first block is used only to evaluate its mean and then is discarded. Then, for the remaining K blocks (withK≥p) the autocovariance sequences are computed. Only p autocovariance sequences over K (the most recent ones) are used as the inputs of the optimum linear predictor.Finally, these p autocovariance sequences are weighted and linearly combined by the p optimum prediction coefficients, in order to obtain the predicted autocovariance sequence, and then the searched entropy. In the following subsection, the MEE signal processing algorithm is described in details.Given a time seriesx(n)of length N samples, i.e.n=1,2,…N, the proposed MEE algorithm works accordingly to the following steps (see Fig. 2):1.The N samples ofx(n)are divided in (K+1) blocks, each of lengthM=N/(K+1)samples.The mean of each i-th block (withi=0,…,K) is then estimated according to the following:(8)μˆi=1M∑j=1Mxi(j)wherexi(j)stands for the j-th sample of the i-th block, withj=1,…,M.Now, starting fromi=1, the mean of the previous ((i−1)-th) block is subtracted from the current (i-th) block, according to the following:(9)yl(j)=xl(j)−μˆl−1wherel=1,…,Kand againj=1,…,M. Note that we have now only K blocks (notK+1), since the first block (that is the one that contains the oldest samples) is used to evaluate the mean to be used in the next block and then is discarded (see again Fig. 2). As explained before, this step is realized so that each block now contains only the market innovations with respect to the past.Now, the autocovariance sequence of each l-th block is estimated according to the following:(10)Covˆl(k)=1M∑j=1Myl(j)⋅yl⁎(j−k)−|αˆl|2wherek=0,±1,…,±M,y⁎()means complex conjugate, andαˆlis the mean of the l-th block, estimated according to (8), withyl(j)instead ofxi(j).Finally, p autocovariance sequences (over K) become the input of the optimum linear predictor of parametric order p. The outputs of the optimum linear predictor are p prediction coefficients that are used to predict the autocovarianceCov˜K+1(k)of the next (future) block. This sequence is evaluated according to the following:(11)Cov˜K+1(k)=∑b=0p−1ab⋅CovˆK−b(k)whereCovˆK−b(k)are the (K−b) previously estimated autocovariance sequences, now linearly combined by a number p of AR coefficientsab. The predicted autocovariance sequence expressed by (11) is transformed in the frequency domain obtaining the PSD of the analyzed block defined as:(12)S(ω)=∑kCov˜K+1(k)⋅e−j⋅ω⋅kThen, in full accordance with the MEM theory described in Section 2.1, the entropy (of the innovation process) of the (future) (K+1)-th block is estimated according to (1), where the PSD is now expressed by (12). In conclusion, if the entropy tends to lower values, the series is characterized by high predictivity (small innovations), otherwise the series is unpredictable (high innovations). Finally, it has to be noted that the Levinson–Durbin recursion is used to solve the equations of the auto-regressive (AR) prediction coefficients used in eq. (11). In practice, the p AR coefficients are chosen as the best coefficients that minimize the error terme(t). More in details,e(t)is the innovation or error term concerning omitted variables or socio-political events which can or cannot be determined before the time t regarding its structure, i.e.,e(t)can be anAR(k)process or a white noise, with constant mean and variance. The error term should be white since the optimum linear predictor acts as a whitening filter. But, since a small number of prediction coefficients are usually employed, the innovation term could not be white. In such a situation, we can consider the Said–Dickey method or nonlinearity such as chaos or conditional heteroskedasticity [45] to whitening the error term, or we can also increase the prediction order (i.e. use more prediction coefficients) as well. The order of the predictor (i.e. how much of the past story should be taken into account to evaluate the future) drastically affects the performance of the method, in terms of both computational complexity and accuracy of the prediction, as shown in Section 4 for some case studies of interest.In order to evaluate the performance of the proposed predictor, we will analyze in the following some case studies, first theoretically evaluating the entropy of the input series, and then comparing this value with the one obtained by our MEE approach. In particular, we evaluate the mean and standard deviation of the entropy estimated through our method, varying some parameters of interest such as: the prediction order p, the number of blocks K, and the number of samples M per block. In addition, since the innovation process for financial series is usually assumed as a Gaussian process [see [37], and references therein], we have considered three cases of interest for the input series: (i) white Gaussian series; (ii) Gaussian AR filtered series, and (iii) Gaussian moving average (MA) filtered series, respectively. In all the following analysis, a series of lengthN=5000has been considered, and a high number of Monte-Carlo simulation trials (103 independent runs) have been implemented to numerically evaluate the performance of our method so to assure that the set of samples is not arbitrary. Note that these series are auto-regressive moving average (ARMA) type, but can be auto-regressive integrated moving average (ARIMA) structure as well [46,47]. However, these two models are characterized by the same predicted entropy since the drift term (that characterizes the ARIMA structure) is completely predictable, and hence it adds no contribution in the evaluation of the entropy. Thus, in the following we focus only on ARMA series, without loss of generality.(i) Entropy of white Gaussian seriesLet us now consider that the input seriesx(n)of length N samples is a white Gaussian series of meanMxand varianceσx2. For the sake of the simplicity, we can considerMx=0, since the mean does not add contribution in the evaluation of the entropy (i.e. translations of a random variable have the same entropy as the untranslated random variable). From the direct application of the MEM theory, see (1), we can theoretically compute the entropy h of this white Gaussian series as a function of its variance. In particular, it is well known that the entropy of the white Gaussian series (of varianceσx2) can be written as [48]:(13)h=12ln⁡(2⋅π⋅e)+12⋅ln⁡(2⋅π⋅σx2)The mean and standard deviation of the entropy estimated according to our MEE approach are presented versus the prediction order, for a white Gaussian series of lengthN=5000samples, and for several numbers of blocks in Fig. 3and in Fig. 4, respectively. In particular, Fig. 3 reports here the curves referring to the estimations obtained exploiting a number of blocks K equal to 5, 10, 20, and 50. For the sake of completeness, the value of the theoretical (i.e. true) entropy of the series is reported. It can be seen from this graph that the best estimates can be obtained with lower values of the prediction order (for all the considered curves). This is obviously true, since the series is a white Gaussian series, and hence, increasing the correlation between consecutive blocks (i.e. increasing the prediction order p) means decreasing the efficacy of the estimation. Finally, in Fig. 4 the standard deviation is reported, again versus the prediction order and for several values of the number of blocks. It is interesting to note that the standard deviation decreases, while the number of blocks used to estimate the entropy decreases. In fact, decreasing the number of blocks implies increasing the number of samples per block (with a fixed length of the series ofN=5000samples), and hence is equivalent to increase the accuracy of the estimation.(ii) Entropy of Gaussian AR seriesLet us now consider that the input series is a Gaussian AR series of parametric order. For the sake of simplicity, and without loss of generality, we can assume in the following that the series under investigation is a first-order Gaussian AR series. In particular, the first order Gaussian AR series is obtained, in the case of our interest, as the output of a one-pole filter when the input is the white Gaussian series of the previous case. The only vinculum is that we consider that the Gaussian AR series has the same varianceσx2of the input white Gaussian series. This means that the one-pole filter is defined as follows:(14)HAR(ω)=1−a21−a⋅e−j⋅ωwhere a is the pole of the filter, with|a|<1. Again, as depicted in [48], it is well known that the entropy of a first-order Gaussian AR series (of varianceσx2) is the same as expressed by (13). It is also evident that the entropy does not depend on the order of the Gaussian AR series, neither on the values of the poles. Therefore, even if it is well-known that an AR series whose characteristic polynome has 1 or more roots is classified as a nonstationary process, our algorithm is still efficient in predicting its entropy. In fact, the entropy rate of the Gaussian AR series is equal to the one of the Gaussian white series. Hence, linear filtering generates no change in the per unit time entropy of the process (if the variance of the output series is equal to that of the input series).In Fig. 5, the entropy estimated according to our MEE approach is presented versus the prediction order, for a first order Gaussian AR series of lengthN=5000samples, and for several numbers of blocks. Again, the value of the theoretical (i.e. true) entropy of the series is also reported on the graph. It can be seen that the best entropy estimation can be obtained exploitingK=50blocks, and a prediction order ofp=10. Then, Fig. 6reports the standard deviation of these estimates versus the prediction order and for several values of the number of blocks. As before, the standard deviation decreases, decreasing the number of blocks used to estimate the entropy. Hence, the curve withK=50is characterized by the highest standard deviation. Finally, it has to be noted that the standard deviation of each curve goes to zero, increasing the order of the prediction.(iii) Entropy of Gaussian MA seriesFinally, let us now consider that the input series is again the white Gaussian series of the first case, but now filtered with a moving average (MA) filter. With the constraint that the variance of the output Gaussian MA series is the same of the input series, the MA filter (composed by2S+1samples) is hence defined as follows:(15)HMA(ω)=12S+1sin⁡[ω2(2S+1)]sin⁡(ω2)Again, since linear filtering provides no change in the entropy of the output process, the entropy of the Gaussian MA series is the same as in (13), and the result does not depend on the number of samples of the MA filter.As before, Fig. 7and Fig. 8report the entropy estimated by our approach and the standard deviation of these estimates, respectively, versus the prediction order, for a white Gaussian series of lengthN=5000samples, and for several numbers of blocks. Referring to Fig. 7 (where, as before, the curve of the true theoretical entropy is shown), the best estimates can be obtained exploiting the highest number of blocks (since the filtered series is characterized by high correlation between samples and consecutive blocks). This is witnessed by the fact that, increasing the prediction order, the accuracy of the estimation for each curve increases. Then, Fig. 8 depicts the standard deviation of the previous estimates, showing an analogous behavior as in the previous cases. In fact, the curve withK=50is characterized by the highest standard deviation. Moreover, the standard deviation of each curve goes to zero, increasing the order of the prediction.

@&#CONCLUSIONS@&#
