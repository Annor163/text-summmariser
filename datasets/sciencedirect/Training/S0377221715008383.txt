@&#MAIN-TITLE@&#
An empirical comparison of classification algorithms for mortgage default prediction: evidence from a distressed mortgage market

@&#HIGHLIGHTS@&#
We evaluate default prediction performance of machine learning/regression models.Including boosted trees, random forests, penalised linear/semi-parametric logistic regression.Using data on over 300,000 residential mortgage loans.The results indicate varying degrees of predictive power.Statistical tests suggest boosted regression trees outperform penalised logistic regression.

@&#KEYPHRASES@&#
Boosting,Random forests,Semi-parametric models,Mortgages,Credit scoring,

@&#ABSTRACT@&#
This paper evaluates the performance of a number of modelling approaches for future mortgage default status. Boosted regression trees, random forests, penalised linear and semi-parametric logistic regression models are applied to four portfolios of over 300,000 Irish owner-occupier mortgages. The main findings are that the selected approaches have varying degrees of predictive power and that boosted regression trees significantly outperform logistic regression. This suggests that boosted regression trees can be a useful addition to the current toolkit for mortgage credit risk assessment by banks and regulators.

@&#INTRODUCTION@&#
Credit default (i.e., failure to keep up with loan repayments) has cost implications for creditors in terms of losses or profits forgone and to other debtors in terms of higher prices (i.e., interest rates) and possible rationing of credit. Residential mortgages are one of the main types of lending and therefore a major potential source of credit risk for banks. Credit risk and credit scoring models to predict mortgage default are used by financial institutions and regulators to measure, assess, and inform decisions to mitigate various aspects of mortgage credit risk. A widely established technique for this type of modelling is Logistic Regression (LR).In recent years, there has been an increased research interest in a number of alternatives to LR and whether those could produce more accurate credit risk models. Particularly, with the development of new predictive modelling techniques in machine learning and the statistical literature, various studies have assessed how these newer approaches perform compared to more established methods with regards to scoring unsecured consumer loans such as personal loans and credit cards (Baesens, Gestel, Viaene, M.Stepanova, Suykens, & Vanthienen, 2003; Kennedy, Nameea, & Delaney, 2013b; Lessmann, Seow, Baesens, & Thomas, 2015). However, when it comes to secured lending, research findings regarding credit risk assessment of mortgage loans are much more scarce, despite the fact that they are among the largest class of assets on European banks’ balance sheets. This paper attempts to assess, using real-world mortgage loan-level data, whether a selection of these newer methods can provide improved predictive performance over more established methods such as Logistic Regression (LR).Evaluating and comparing how various techniques perform with regards to mortgage default prediction serves a number of goals. First, for profitability and credit risk management purposes, financial institutions are interested in determining borrower creditworthiness through separation into good and bad categories. This is the central objective of credit scoring (Thomas, 2009). The outputs of these credit scoring methods can also contribute to the implementation of risk-adjusted loan pricing systems. Even a small improvement in the predictive power of such models could thus have a substantial impact on the quality of a bank’s loan book and pricing strategy.Second, adequate regulatory capital buffers are required so that banks would be able to cope with unforeseen losses in excess of expected loss. Accurate assessment of the risk or probability of mortgage loan default is critical for determining regulatory capital requirements. For retail credit risk classes such as mortgages, the Probability of Default (PD) models developed for this purpose are usually fixed in horizon (one year) and have so far been typically modelled using logistic regression; being able to build more accurate models would enable more appropriate capital levels being set.Third, the systemic banking crisis in Ireland and elsewhere in Europe has, in several of these countries, intensified the use of predictive models for operational management of credit arrears (Matthews, 2011). In this context, predictive models estimating the probability of a loan experiencing arrears in the near future are used to drive various decision-making strategies. This probability may depend on borrower attributes at application, borrower repayment behaviour such as past arrears or loan modifications, the presence of negative equity (i.e., the value of the property dropping below that of the loan), as well as regional economic conditions. Given that financial and operational resources are limited for financial institutions and regulators, improvements to these models and their estimates could assist in better segmenting borrowers and targeting scarce resources to where they are needed most in early-prevention initiatives and active arrears management.Developments in statistical and machine learning approaches to classification (i.e., prediction problems where the target variable of interest is discrete, e.g. default or no default) have led to a variety of applications in credit risk. Previous reviews of various modelling approaches and empirical evaluations have been carried out by Baesens et al. (2003), Crook, Edelman, and Thomas (2007), Crook and Bellotti (2009), Brown and Mues (2012), Kennedy et al. (2013b), and Lessmann et al. (2015). Some of their results suggest that newer approaches such as ensemble classifiers offer some improvement in predictive ability over logistic regression which could prove valuable for managing credit risk. However, the suggested performance boost is not guaranteed; on some datasets, newer techniques may not substantially improve predictive performance (Hand, 2006). This implies that empirical work is needed to determine if and where this is the case.The main research question in this paper therefore is whether these alternative modelling approaches from the statistical/machine learning literature indeed offer improved predictive performance for mortgage credit risk compared to Logistic Regression (LR). LR is chosen as the baseline as it performs relatively well as a classifier in other credit scoring settings, and because of its relative ease of interpretation and widespread use in the financial services sector. To answer this question, a number of alternative approaches were selected. The modelling approaches included in the empirical comparison are: semi-parametric Generalised Additive Models (GAMs), Boosted Regression Trees (BRT), and Random Forests (RF). These approaches each enable a flexible approach to modelling data with a complex structure (Hastie, Tibshirani, & Friedman, 2009).There are several reasons to choose these types of models among alternatives. First, there may be non-linear effects of predictors on the response variable. For example, using option pricing theory, Deng, Quigley, and Van Order (2000) and Das and Meadows (2013) argue that mortgage borrowers may hold an option to default if their home is in negative equity, i.e., the current loan to value is greater than 100 per cent. Empirical work for various mortgage markets confirms that negative equity is an important predictor for default and that loan to value does not have a simple linear relationship with the log odds of defaulting (Foote, Gerardi, & Willen, 2008; Haughwout, Peach, & Tracy, 2008; Kelly, 2011).22Negative equity is of course not the sole reason for default. As noted by Foote et al. (2008) and Van Order (2008), borrowers may default for a multitude of reasons which also include trigger events such as illness, unemployment, divorce, or a lack of financial resources to overcome the trigger event.Similarly, other variables such as loan vintage or borrower age are sometimes found to be non-linearly related to default risk. In contrast, one of the assumptions underpinning LR is that predictors are assumed to have a linear and monotonic effect. This may thus not hold in practice. Moreover, categorising or binning continuous variables, in an attempt to approximate this non-linearity, may result in mis-specification and loss of information. GAMs, BRT and RF on the other hand can all, to some extent, approximate non-linear functions of continuous predictors. This may allow identification of these effects and, if needed, the introduction of additional terms in a logistic regression model to approximate them.Second, although arguably harder to interpret than LR, all three alternative approaches are not simply black-box models as they provide some degree of model explanation and insight into risk drivers. For example, GAMs can be assessed through statistical significance tests and spline plots. Variable importance measures and important interactions can be identified in BRT and RF (Caruana, Lou, & Gehrke, 2012; Elith, Leathwick, & Hastie, 2008; Hastie et al., 2009; Liu, Vu, & Cela, 2009). This may reduce the risk of model mis-specification and help make these models acceptable to practitioners. In addition, their use can potentially lead to improved predictive performance – i.e., the default predictions produced by these more recent techniques may be more accurate.In the present application, a third justification for choosing LR, GAMs, BRT and RF is that their training algorithms tend to scale relatively well with the size of the data. All four techniques can cope with the large datasets analysed in the study within a reasonable amount of computation time. Although we experimented with Support Vector Machines (Vapnik, 1998), which have previously been found to be competitive for credit scoring (Bellotti & Crook, 2009) and bankruptcy prediction (Van Gestel, Baesens, & Martens, 2010), we did not include them in the final study due to the weaker scalability of available implementations.33Sometimes, it is challenging to directly interpret the resulting model, which is considered a drawback in a highly regulated practical setting. However, in the case of SVMs, Martens, Baesens, Gestel, and Vanthienen (2007) demonstrate that it is possible to extract understandable rules that approximate an SVM classifier.The algorithmic complexity involved in solving the general SVM quadratic programming problem is between O(N2) and O(N3), where N is the number of training observations (Bordes, Ertekin, Weston, & Bottou, 2005). The complexity of Radial Basis Function SVMs may even be higher, i.e. between O(dN2) or O(dN3) (where d is the data dimensionality) (Sreekanth, Vedaldi, Jawahar, & Zisserman, 2010), which proved prohibitive for several of the training samples used in this study.This paper extends the existing credit scoring literature in four main ways. First, it specifically focuses on mortgages. Detailed accounts of the various modelling approaches to credit scoring are included in Crook et al. (2007), Crook and Bellotti (2009), Thomas (2009), Hand (2009b), and Martin (2013). However, with the exceptions of Galindo and Tamayo (2000), or Feldman and Gross (2005) and Kennedy, Namee, Delaney, O’Sullivan, and Watson (2013a), most of the literature concentrates on credit card or personal lending only. This is somewhat surprising given the importance of mortgage lending as a business line to banks in advanced economies, but may be due to a lack of publicly available information from credit registers or third-party data providers in Europe, as well as commercial considerations by financial institutions.Second, this paper adds to the findings on classifier comparison by making a focused comparison of four techniques on four portfolios of recently collected real-world data. Specifically, BRT, with the exceptions of Lo, Khandani, and Kim (2010), Brown and Mues (2012), and Lessmann et al. (2015), have received relatively little attention to date in the credit scoring literature. Although Lo et al. (2010) used BRT to score credit card borrowers, they did not compare their performance to other classifiers. A comparison by Bastos (2008) found that BRT performed well compared to Neural Networks (multilayer perceptrons) and Support Vector Machines on two credit scoring tasks. GAMs were used by Berg (2007) to assess corporate credit risk, but they do not appear to be applied widely in mortgage credit risk modelling. In addition, several of the comparative studies of classifiers use datasets that may no longer be representative of the much larger scale of data available for predictive modelling within today’s retail banks.Third, the imbalanced nature of the portfolios considered in this paper, i.e., the large difference in the relative proportion of non-defaulters and defaulters, forms another topic of interest within the credit scoring literature. The impact that such imbalanced datasets have on the quality of the resulting models was studied by Burez and Van den Poel (2009), Brown and Mues (2012), and Kennedy et al. (2013b). Both Kennedy et al. (2013b) and Brown and Mues (2012) found that LR nonetheless holds up relatively well, along with other classifiers. However, the experiments set up in Brown and Mues (2012) indicated that BRT and RF started to outperform other classifiers when the level of class imbalance was further increased in their datasets – none of which were mortgage data. This paper thus contributes to these findings by applying the selected classifiers to four imbalanced real-world mortgage datasets so as to test whether BRT and RF offer a similar performance advantage in this setting.Fourth and finally, the context for our study is a distressed European mortgage market within a recessionary economic environment, which sets it apart from other studies, as most of the published research is not informed by the current crisis or is based on the US mortgage market (Haughwout et al., 2008). Also, our findings may be relevant to financial institutions in other parts of the world that have not recently experienced severe downturns or housing market crises and thus have limited data available to fit robust models under such scenarios.The remainder of this paper is structured in the following manner. The next section describes the specific modelling techniques or classification algorithms used in the paper. This is followed with a description of the parameter tuning and data. After that, the main results are presented and discussed; the final section concludes.The aim of each model is to produce a loan-level prediction for a binary variable;Y=1signifies default andY=0indicates no default. This prediction is made using n observations of training data with p predictor variables. Each observation(xi,yi),i=1,⋯,n,consists of a predictor vector (xi) and an associated response (yi=0or1). The predictor variables are a mix of continuous and categorical variables. We define default as greater than 90 days arrears.Logistic Regression (LR) is known as a classifier that performs reasonably well across many application settings and data types, including credit scoring (Brown & Mues, 2012; Kennedy et al., 2013b; Lessmann et al., 2015). To avoid the problems associated with stepwise regression, and to make the model comparison as fair as possible, Regularised Logistic Regression (RLR) is used in this paper, with the final model chosen on the basis of the H-measure (see Section 4.1).44We are grateful to one of the reviewers for the suggested use of alternatives to stepwise regression. Note that stepwise regression was also tried, which produced similar performance ranks for LR.This type of logistic regression uses penalisation to improve the model fit. These penalties can include ℓ1 (the lasso), ℓ2 (ridge regression) or mixtures of the two (elastic-net) (Friedman, Hastie, & Tibshirani, 2010). The best-fitting penalisation method is chosen by cross-validation.The penalised negative binomial log-likelihood is given by Eq. (1). The β coefficients are chosen to minimise this objective function. The term on the left of the equation is the negative binomial log-likelihood. The additional term on the right (λ onwards) penalises the coefficients using two types of penalty terms, with ||β||1 and∥β∥22denoting the ℓ1 and the squared ℓ2 norms of the β coefficients.55The coefficient β0 is a scalar and is not typically penalised; β is a vector. This formulation is based on the implementation in the R package glmnet.(1)min(β0,β)−[1n∑i=1nyi·(β0+xiTβ)−log(1+e(β0+xiTβ))]+λ[(1−α)12∥β∥22+α∥β∥1]The effect of the ||β||1 term (also known as a lasso penalty) is to perform variable selection when λ is sufficiently large by setting their coefficients exactly equal to zero. The role of the∥β∥22term (also known as a ridge penalty) is to shrink coefficients towards zero as λ becomes larger. There are some drawbacks with the individual penalties. First, a model trained with a ridge penalty only will include all predictors, even if they are irrelevant, with the degree of coefficient shrinkage increasing with λ. Second, a lasso-based model may only select one predictor from a group of correlated variables and ignore the others. As it is usually difficult to determine before a model is estimated which predictors are truly important, a mixture of both penalties can be useful. The α parameter in Eq. (1) controls the degree of mixing between the lasso penalty (α=1) and ridge regression (α=0). Both λand α are determined by cross-validation based on the training data. The advantages of this approach are that coefficient shrinkage and variable selection can be carried out simultaneously in a numerically stable manner through this penalty structure. This may improve predictive performance and avoid some of the problems with stepwise regression (Derksen & Keselmanl, 1992).Generalised Additive Models (GAMs) retain many of the features of LR and are statistically interpretable. They are a useful alternative when the log-odds of default may be a non-linear function of some of the predictors, as their output can be based on a sum of smoothed functions of predictor variables (Hastie et al., 2009). As the response data in this paper are binary, the logistic link function is used in the GAM. When linear terms and/or categorical variables are included alongside variables that are smooth terms, like in this application, the resulting model is termed as a semi-parametric GAM. Eq. (2) shows the model that is estimated. The terms,xj,j=1,⋯,q,represent variables from the training dataset that are smoothed, whilexj,j=q+1,⋯,p,are variables assumed to have a linear effect on the log-odds of defaulting and are fit parametrically.(2)logit(P(y=1|x))=β0+∑j=1qsj(xj)+∑j=q+1pβjxjThe smooth functions in the GAM, sj(xj), are estimated using penalised regression splines. An individual smooth term can use cubic splines as a building block.66A cubic spline is a piecewise cubic function with continuous first and second derivatives.This involves individual cubic polynomial regressions being run for different intervals of a given input variable, the results of which are combined at certain points (knots) to create a continuous curve or smooth function for that predictor. A penalty term for each smooth function of the covariates is included in the model. This is to ensure the smooth functions do not overfit the data. A parameter for each smoothed variable (λ) controls the trade-off between goodness of fit and smoothness.Tuning of this smoothing parameter is critical: if the λ values are too high, the data will be over-smoothed; if they are too low, then the data will be under-smoothed (Wood, 2006). In both cases, the spline estimate will not closely approximate the true function, which will affect predictive performance. A technique called Generalised Cross Validation (GCV) is used to select the optimal smoothing parameter value given the data (Wood, 2006). This technique is similar to estimating prediction error based on a leave-one-out cross-validation estimation but using a more computationally efficient procedure (Wood, 2006).77An alternative approach is to use a backfitting algorithm based on a scatterplot smoother or by other variants of penalised splines. The back-fitting algorithm is described in detail in Hastie et al. (2009).The tree-based models in this paper draw on Classification and Regression Trees (CART) developed by Brieman, Friedman, Stone, and Olshen (1984). This is a classification technique based on two central ideas: recursive partitioning and pruning. Recursive partitioning involves repeatedly splitting or dividing and then sub-dividing the predictor space into a series of smaller segments that are more homogeneous; i.e., each segment is ideally composed of observations belonging to a single class. The resulting model assumes the structure of a tree. In CART, pruning is used to reduce the size of trees based on various measures of predictive error such as misclassification rate, Gini index, or deviance. This is necessary to avoid fitting every minor variation in the input data. The overall goal is to have a tree that explains relevant patterns and generalises well to unseen data. However, because CART is recursive, current splits depend on previous splits, making the resulting model outputs sensitive to small changes in the input data, such as when unseen data is applied to the model. Two subsequent algorithms – boosted regression trees and random forests – sought to improve upon CART.Boosted regression trees combine tree-based recursive partitioning with the concept of boosting developed by Freund and Schaipre (1997) and extended with a statistical interpretation by Friedman, Hastie, and Tibshirani (2000), Friedman (2001), and Friedman (2002).88These papers interpreted the algorithm in a likelihood framework and developed boosted logistic and other regression-based approaches. The papers also led to additions of shrinkage and bagging to the algorithm. Shrinkage refers to limiting the contribution of each sub-component of the model, through taking small increments in each forward stage-wise iteration. Bagging refers to only a random subset of data being used in each iteration. This random sampling is thought to reduce the variance, and thus improve predictive performance of the final model. A comprehensive overview of boosting is given in Hastie et al. (2009) and Bühlmann and Hothorn (2007).Because the present application (mortgage default prediction) is a binary classification problem, the loss function used is binomial deviance. The algorithm used is called stochastic gradient boosting and is based on Friedman (2001, 2002).99This section draws on the descriptions given in Elith et al. (2008), Berk (2008), Hastie et al. (2009), and Ridgeway (2013).After initialisation, the algorithm minimises this loss function in each step by the stage-wise addition of a new tree that leads to the best reduction in the loss function, given the chosen tree size.The procedure starts by choosing initial values such as the log odds of default based on the training data. A random sample of observations is drawn without replacement, and the difference between the response and the starting value is calculated. These are known as the vector of negative gradients.1010The components of the negative gradient vector are sometimes referred to as pseudo-residuals, see Hastie et al. (2009, page 360–61), or Berk (2008, page 270). The use of a random subset of the data, known as the bag fraction, to construct a tree at each iteration in the algorithm has been found to improve predictive ability (Friedman, 2002).Based on this data, a tree is constructed by choosing the variables and split points giving the maximum reduction in the loss function at this step. The algorithm updates by first calculating the predicted probability of defaulting based on the current tree and the random subset of data. These are then added to the existing fitted values up to that step and subtracted from the response to obtain a new set of negative gradients. A new random sample of observations is drawn from these and a new tree fit. This proceeds until the material improvement in the overall model fit is less than some small tolerance. Each time a tree is added to the model, its contribution is multiplied by a parameter termed the learning rate. The effect is to limit or shrink the contribution of any one tree to the overall model prediction. A final BRT model is the sum of several hundreds or thousands of trees multiplied by the learning rate.Boosting has not been without its critics. In particular, Mease and Wyner (2008) have been critical regarding the reasons for the algorithm’s resistance to overfitting and the way it has been interpreted in the statistical literature.Random Forests (RF) are another tree-ensemble classifier developed by Brieman (2001). There are three important differences between RF and the tree-based approaches outlined earlier. The first difference between RF and CART is that in a RF many trees are grown based on bootstrapped sub-samples of the training data. The second difference is that each time a split variable is chosen within an individual tree in a RF, the algorithm only chooses from a small random subset of predictors of size mtry. This is in contrast to CART or BRT where all of the predictors are evaluated to produce the best split. This process is repeated over many trees to create a ‘forest’ or ensemble of trees the predictions of which are averaged to produce an output. Randomly selecting a subset of predictors rather than trying all has the effect of reducing correlation among the trees in the random forest. Averaging predictions over all trees in the forest reduces variance, resulting in improved predictive ability compared to CART. A third difference is that random forests can be grown in parallel, as each tree can be grown independently, whereas the BRT algorithm proceeds sequentially depending on the output from the previous iteration. Random forests have been applied to a variety of domains such as bioinformatics, image recognition, as well as in financial applications such as customer attrition and credit scoring (Kruppa, Schwarz, Arminger, & Ziegler, 2012; Lessmann et al., 2015; Malley, Kruppa, Dasgupta, Malley, & Ziegler, 2012).This section specifies how the various models were estimated and tuned, as well as describing the datasets.The penalised LR models include the main effects and pairwise interactions between predictors. The models are estimated using the R packages glmnet and caret (Friedman et al., 2010; Kuhn, 2008). The performance criterion for selecting the final model is the H-measure (to be further discussed in Section 4.1). The grid search considered a value range for the parameter α from 0 to 1, in 0.1 increments, and for λ, a sequence of 20 values from 0.005 to 1. The best combination was chosen using 10-fold cross-validation.The semi-parametric GAM models are estimated using the R package mgcv (Wood, 2013). The degree of smoothing of the spline functions is chosen by Generalised Cross Validation (GCV).Two parameters are key for BRT tuning. The learning rate (lr) or shrinkage parameter determines the contribution of each tree. A lower learning rate means that each tree has a lower weight in the final model. Tree complexity (tc) determines the degree of interaction between predictor variables. For example, a tc of 1 fits an additive model (each tree having a root and two leaves); a tc of 2 fits a model with up to two-way interactions. This paper uses the R package gbm and a modified version of the code from Elith et al. (2008). A grid search over these two parameters, i.e. learning rate [0.01, 0.005, 0.0025, 0.001], tree complexity [2–6], and a third parameter, bag fraction [0.5, 0.625, 0.75], was conducted to find the combination with the highest H-measure on the validation data. The number of trees (nt) is determined automatically by the function gbm.step using 10-fold cross-validation, for a given learning rate and tree complexity.Finally, when tuning the RF, the number of predictors from which to select at each split (the mtry parameter) was varied over the range [1–4, 6, 8]. The number of trees in the forest was fixed at 1000. The version of the algorithm used here is based on Brieman (2001) and implemented in the R package randomforest (Liaw & Wiener, 2002). Initial results suggested that the class imbalance was affecting RF performance for some of the portfolios. Therefore, undersampling of non-arrears cases was carried out by taking balanced bootstrap samples from the original data. For example, if there were 1000 default cases in the training data, each time a tree is induced, this would be done on a different bootstrap sample containing all 1000 default cases and a random selection of 1000 non-default cases. This methodology is outlined in Breiman, Chen, and Liaw (2004) and Kuhn and Johnson (2013). Compared to conventional undersampling, it has the advantage of making better use of all available training data, by not eliminating some majority class observations altogether but drawing a different sample at each step of the algorithm. The best parameter values are determined through 10-fold cross-validation using the R package caret; the optimal model is again selected based on the H-measure.This section describes the data collected by the Central Bank of Ireland on which our analysis was conducted. The data are composed of four separate portfolios of owner-occupier mortgage loans of Irish lenders. The sample represented 55 per cent of the Republic of Ireland’s mortgage market as of December 2010. For predictive modelling purposes, only those loans that were not yet in default at the observation point of December 2010 were retained; the target variable of interest is whether those loans moved to default status by December 2011. The predictor variables (i.e. the potential inputs to each model) are all measured either at December 2010 or prior to that.When added together, the training, validation and test samples used in this paper amount to 322,915 cases across the four portfolios.1111Because of confidentiality restrictions, details for individual portfolios cannot be given.The minimum training set size is over 31,000 and the maximum is just under 50,000 observations. The minimum test set size is approximately 18,000, the maximum just over 28,000. The proportion of default outcomes in the training data ranged from 3 to 9 per cent.1212The training and test set sizes and class distribution is given for all portfolios and not for individual portfolios to preserve data confidentiality.The data for each portfolio was divided randomly into training, validation, and test set, with a 50/20/30 split. The class distribution in the training, validation and test data was preserved to match the imbalance observed in each portfolio. The models are estimated or trained on the training data, where necessary tuned on the validation data, and performance is assessed using the test data. LR and GAMs are trained on a combined training plus validation sample as they do not require a separate validation sample for tuning. In the case of BRT and RF, only the training data are used for model fitting whilst the validation set is used to tune further the parameters and select the best performing model.The dataset variables are described in Table 1.1313For a more detailed description of a larger dataset from which these data were drawn, we refer the reader to Kennedy and McIndoe-Calder (2012).The selected observations each relate to the main loan associated with a given property serving as collateral. The dependent variable is a binary variable defined as the equivalent of a borrower being more than 90 days past due (e.g. by missing three consecutive monthly payments) on their mortgage at some point over the outcome window. This is a standard measure of default used in capital requirement regulations in Ireland.The predictor variables are a mix of continuous and categorical data and include a range of application and behavioural information. The updated loan-to-value ratio for December 2010 (variable Current LTV) is calculated by dividing the loan balance at that time by the indexed market value of the property (i.e., applying the December 2010 index to the original property value).1414The house price index used to estimate market values in December 2010 is composed of Dublin and non-Dublin property prices as well as house or apartment property types.Early arrears is a binary variable indicating whether the borrower had a non-zero arrears balance that was greater than 10 per cent of but less than one month’s full mortgage instalment in December 2010.1515The rationale for a floor of 10 per cent of a one-month payment is to exclude borrowers that have a very small arrears amount, as this may be due to the loan nearly curing or technical reasons such as an incorrect standing order.Due to data limitations, this variable is not available for Portfolio 3. Past arrears status (variable Recent Default) may indicate that some borrowers could be at higher risk of defaulting in future. Finally, borrowers may have previously received a loan modification from their bank. This can occur while remaining current or after entering arrears and may be part of short-term forbearance.There are some limitations to the data used in this study. First, some borrower-specific features are observed at origination (marital status, income) but not subsequently updated. Individual borrowers’ personal and economic circumstances in December 2010 are likely to be important for prediction but remain unobserved after origination. Economic conditions such as the unemployment rate of the geographical region in which the borrower is located can only approximate the individual borrower’s economic circumstances.Second, additional unobserved features of borrower behaviour may also be relevant for default prediction. For example, borrowers could use the information advantage concerning their own economic and life circumstances that they have compared to their bank. They may be able to conceal their true ability to repay and default strategically (Das, 2012). These features are never observed and cannot be approximated using the data available for this study. Therefore, while the literature suggests several types of potential predictors of default, the predictors in this empirical study cannot be expected to explain all the idiosyncratic causes of default.Third, after being checked for outliers and other errors, the data included missing values. Four categorical variables had missing values: property type, borrower’s marital status and gender, and loan interest rate type. The percentage of cases with missing values for these variables ranged from 0 per cent to 24 per cent across the four portfolios. These were recoded as unknown rather than excluding the observation. The reason for this is that the alternative of imputation is a difficult problem which imposes a structure on the data, and if mis-specified, may itself lead to bias (Horton & Kleinman, 2007). Apart from these categorical variables, income at origination also contained some missing values with the percentage of cases with missing values for these variables ranged from 0 per cent to 27 per cent across the four portfolios. This is because of two reasons. A first cause were general data quality problems relating to banks inconsistently recording application information including income. Second, in some cases where a mortgage was topped up, extended, or refinanced, the institutions reported only the latest value for these income-related variables, as collected at the point of origination of those subsequent loans; the relevant values at the point of origination of the main mortgage were thus lost. Rather than proceeding by case-wise deletion or mean/median imputation, and thus potentially biasing the sample by excluding these cases, we imputed missing values using the k-Nearest Neighbour (kNN) algorithm.1616Replicating the same analysis on a smaller dataset following case-wise deletion gave results similar to those discussed in the remainder of this paper. The statistical performance tests showed BRT outperforming LR at a 5-per cent significance level. The results for this robustness check are shown in Appendix B.A value of 50 for the number of nearest neighbours (k) was chosen for the imputation.1717This was derived through empirical testing on two of the portfolios that either had no missing income or a very low number of missing income observations. After random deletion of a proportion of non-missing values in those datasets, using 50 nearest neighbours (k=50) in the imputation procedure led to the lowest estimation error for the income variable. Inclusion of a binary missing value indicator for income did not turn out to be a significant predictor of future default status.A commonly used measure for assessing the performance of a score-based classifier is the Area Under the Curve (AUC). This refers to the area under the Receiver Operating Characteristic (ROC) curve, which is a pairwise plot of the true positive rate against the false positive rate, as the classification threshold is varied over its entire range.1818In this application, the true positive rate, also known as the sensitivity, is the fraction of defaulters that are correctly classified using a given threshold value (i.e. having a score greater than the threshold). The false positive rate (1-specificity) is the fraction of non-defaulters classified incorrectly as defaulters, using the same threshold value.An AUC value closer to 1 suggests better discrimination ability between defaults and non-defaults; a value of 0.5 implies that the classifier performs no better than chance. Using the AUC as a performance measure is standard practice in credit scoring but not without its problems. Hand (2009a) argued that, when interpreted in terms of costs, the AUC measure treats the relative severities of misclassifications differently when multiple classifiers with different respective score distributions are compared, implying that the AUC is intrinsically incoherent.1919This point is debated by Flach, Hernandez-Orallo, and Ferri (2011).As a coherent alternative to the AUC, Hand (2009a) therefore proposed the H-measure. The advantage of using the H-measure as a classification performance measure is that it allows one to specify a distribution of likely misclassification costs (c) that is independent of the classifier; this choice is discussed in detail by Adams, Anagnostopoulos, and Hand (2012). Because of the class imbalance between defaulters and non-defaulters, this paper uses the default setting suggested there (corresponding to a Beta distribution with its mode set atc=π1,i.e. the proportion of defaults in the dataset). This means that the reported H-measures put relatively greater weight on correctly classifying default cases than on incorrectly classifying non-default cases. As with the AUC, a higher H-measure is associated with better performance.In this paper, unless otherwise stated, model comparisons are carried out using the H-measure. The AUC is nonetheless included as it is still widely used in practice. Where classifiers are compared based on the AUC, model selection/tuning for LR, BRT and RF has been done on the AUC instead.Statistical tests can indicate whether there is a significant difference between how well different classifiers perform over a set of available datasets. Friedman’s (1940) test can be used to compare the various models based on their performance rankings for a chosen performance metric such as the H-measure (Demsar, 2006). The test statistic is χ2 distributed withk−1degrees of freedom, where k is the number of classifiers. Its null hypothesis is that there is no difference between the classifiers’ performance ranks. A less conservative variant of the Friedman statistic, also reported in this paper, is the Iman and Davenport (1980) test.In the event that there are significant differences according to either of these tests, various post-hoc tests can be used to compare pairs of individual classifiers. These tests adjust p-Values to control for error propagation in multiple pairwise comparisons. Comparing the best-performing classifier with every other classifier requires the use of a particular approach which accounts for this family-wise error using what is known as Holm’s procedure (Garcia & Herrera, 2008; Holm, 1979).Holm’s procedure starts by evaluating the performance rank differences between the best performer and each other model and, for each such pair, calculates the test statistic outlined in Garcia and Herrera (2008) each of these values is then compared against a normal distribution table to produce a significance value (p-Value). Next, the procedure sorts these p-Values in ascending order, comparing each piin the resulting sequence,p1,...,pk−1,with an adjusted p-Value,αk−i,where α is the required significance level. If piis less than the adjusted p-Value, the relevant null hypothesis is rejected, in which case the corresponding model is considered significantly worse than the best performer. This proceeds until a null hypothesis cannot be rejected; any remaining performance differences can thus be ignored. The Java code by Garcia and Herrera (2008) is used to calculate the Friedman, Iman–Davenport statistics, and Holm’s post-hoc tests.

@&#CONCLUSIONS@&#
