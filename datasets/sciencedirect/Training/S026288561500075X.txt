@&#MAIN-TITLE@&#
Complex event recognition using constrained low-rank representation

@&#HIGHLIGHTS@&#
A novel low-rank model for complex event representationSemantic cues are induced in our model by constraining it to follow human annotation.We demonstrate extensive experiments on TRECVID MED 11 and MED 12.We compare our method to seven recent methods and achieve state of the art.

@&#KEYPHRASES@&#
Complex event recognition,Low-rank optimization,Activity recognition,Action concepts,

@&#ABSTRACT@&#
Complex event recognition is the problem of recognizing events in long and unconstrained videos. In this extremely challenging task, concepts have recently shown a promising direction where core low-level events (referred to as concepts) are annotated and modeled using a portion of the training data, then each complex event is described using concept scores, which are features representing the occurrence confidence for the concepts in the event. However, because of the complex nature of the videos, both the concept models and the corresponding concept scores are significantly noisy. In order to address this problem, we propose a novel low-rank formulation, which combines the precisely annotated videos used to train the concepts, with the rich concept scores. Our approach finds a new representation for each event, which is not only low-rank, but also constrained to adhere to the concept annotation, thus suppressing the noise, and maintaining a consistent occurrence of the concepts in each event. Extensive experiments on large scale real world dataset TRECVID Multimedia Event Detection 2011 and 2012 demonstrate that our approach consistently improves the discriminativity of the concept scores by a significant margin.

@&#INTRODUCTION@&#
The increasing popularity of digital cameras has been creating a tremendous growth in social media websites like YouTube. Along with the increased number of user-uploaded videos, the need to automatically detect and recognize the type of activities occurring in these videos has become crucial. However, in such unconstrained videos, automatic content understanding is a very challenging task due to the large intra-class variation, dynamic and heterogeneous background, and different capturing conditions. Therefore, this problem has recently gained significant attention.Most activity recognition methods are developed for constrained and short videos (5–10s) as in [3–6]. These videos contain simple and well-defined human actions such as waving, running, and jumping. In contrast, in this paper, we consider more practical videos with realistic events, complicated contents, and significantly variable lengths. Refer to Fig. 2. The standard activity recognition methods do not incorporate evidences for detecting a particular action/event when deal with such unconstrained videos. To this end, the most recent approaches resorted to using low-level events called “concepts” as an intermediate representation [2,1]. In that, a complex event is described using concept scores, which is the occurrence confidence for the concepts in the video. For example, the event Birthday Party can be described as the occurrence of singing, laughing, blowing candles, jumping … etc.In the context of concept-based event representation, substantial consequences arise as a result of the complex nature of these unconstrained videos. First: The examples used to train each concept have significant variations, and thus the resulting concept models are noisy. Second: The concept-content of each event may still vary among the samples of each event, mainly because of the variable temporal lengths and capturing conditions. Therefore, the obtained concept scores used to describe each event are also significantly noisy. Third: The automatically obtained concept representation strictly relies on local visual features, and lacks context and semantic cues, which humans naturally infer (refer to Fig. 1). In this paper, we address these consequences using a novel low-rank formulation, which combines the precisely annotated videos used to train the concepts, with the rich concept scores. Our approach is based on two principles: First, the videos of the same event should share similar concepts, and thus should have consistent responses to the concept detectors. Therefore, the matrix containing the concept scores of each event must be of low-rank. Second, since the videos used to train the concept models were manually and carefully annotated, the resulting low-rank matrix should also follow the annotation. For example, concepts like person falling or person flipping may falsely fire in the Birthday Party event, and concepts like person opening present or person dancing may not fire in some videos of events like Birthday Party where these concepts actually occurred. Therefore, by enforcing our constraints, such hurdles can be avoided.Fig. 3summarizes the steps involved in our method. We split the training data into two sets: (1) the event-level annotated data, which has only event labels and (2) the concept-level annotated data, which has both event-level and concept-level labels. We use the concept-level annotated data to train concept detectors, which we run on the event-level annotated data and obtain their concept scores. Consequently, we stack the concept scores for each event in a matrix and find their low-rank representation such that it also follows the basis of the concept annotation. The resulting training data combines the two sets in one rich and consistent training set.The low-rank constraint has been vigorously employed in different computer vision problems such as tracking [7], feature fusion [8], face recognition [9], and saliency detection [10]. However, to the best of our knowledge, low-rank estimation of concept scores has never been used before. More importantly, our formulation is more general than the standard RPCA [11] in that we allow the estimated low-rank matrix to follow a prior pattern (the annotation in our scenario). On the other hand, since we exploit the low-rank constraint, our method is more robust against noisy concepts and cluttered background than [2,1,12], and significantly outperforms the state-of-the-art, as we demonstrate in the experiments.The main contribution of this paper is a novel low-rank formulation, through which we find a new representation for each event, which is not only low-rank, but also constrained by the concept annotation, thus suppressing the noise, and maintaining a consistent occurrence of the concepts in each event. Our constrained low-rank representation is not restricted to a certain type of features; which allows us to employ a combination of state-of-the-art features, including STIP [6], DTF-MBH [3], and DTF-HOG [3].The rest of the paper is organized as follows: Section 2 reviews the related work. Section 3 describes the process of computing the constrained low-rank event representation. In Section 4, we describe how to find the optimal solution for the low-rank optimization. The experimental results are presented in Section 5. Finally, Section 6 concludes the paper.

@&#CONCLUSIONS@&#
