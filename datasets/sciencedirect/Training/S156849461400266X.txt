@&#MAIN-TITLE@&#
A radial basis function network classifier to maximise leave-one-out mutual information

@&#HIGHLIGHTS@&#
An orthogonal forward selection algorithm is proposed for constructing radial basis function classifiers based on maximises the leave-one-out mutual information between the classifier's predicted class labels and the true class labels.Integrated within each OFS step, a Bayesian procedure of hyperparameter fitting is introduced to infer the l2-norm local regularisation parameter from the data.The results obtained demonstrate that the proposed algorithm automatically constructs very parsimonious RBF classifiers with excellent classification generalisation performance.

@&#KEYPHRASES@&#
Cross validation,Mutual information,Orthogonal forward selection,Radial basis function classifier,

@&#ABSTRACT@&#
We develop an orthogonal forward selection (OFS) approach to construct radial basis function (RBF) network classifiers for two-class problems. Our approach integrates several concepts in probabilistic modelling, including cross validation, mutual information and Bayesian hyperparameter fitting. At each stage of the OFS procedure, one model term is selected by maximising the leave-one-out mutual information (LOOMI) between the classifier's predicted class labels and the true class labels. We derive the formula of LOOMI within the OFS framework so that the LOOMI can be evaluated efficiently for model term selection. Furthermore, a Bayesian procedure of hyperparameter fitting is also integrated into the each stage of the OFS to infer the l2-norm based local regularisation parameter from the data. Since each forward stage is effectively fitting of a one-variable model, this task is very fast. The classifier construction procedure is automatically terminated without the need of using additional stopping criterion to yield very sparse RBF classifiers with excellent classification generalisation performance, which is particular useful for the noisy data sets with highly overlapping class distribution. A number of benchmark examples are employed to demonstrate the effectiveness of our proposed approach.

@&#INTRODUCTION@&#
Model evaluation in terms of good generalisation performance is essential in the development and analysis of data-based learning algorithms for the construction of object classifiers. A fundamental concept in the evaluation of model generalisation capability is that of cross validation [1]. For example, in regression application, leave-one-out (LOO) cross validation is often used to estimate generalisation error by choosing amongst different model architectures [1]. In general, cross validation is required in most algorithms for model generalisation evaluation, and this often contributes significantly to computational cost for many model paradigms. Luckily for the linear-in-the-parameters models, the LOO cross validation can be exercised without actually splitting the training data set and estimating the associated models, by making use of the Sherman–Morrison–Woodbury theorem [2].Moreover, for the linear-in-the-parameters models, the orthogonal least squares (OLS) based forward selection algorithm can efficiently construct parsimonious models [3,4], and has been a popular learning tool for associative neural networks, such as radial basis function (RBF) networks [5], fuzzy and neuro-fuzzy systems [6,7] as well as wavelets neural networks [8,9]. The OLS algorithm for RBF network learning [5] has also been utilised in a wide range of engineering applications, including aircraft gas turbine modelling [10], fuzzy control of multi-input multi-output nonlinear systems [11], power system control [12], fault detection [13], electric arc furnace load modelling [14], macromodelling of nonlinear digital I/O drivers [15], real-time power dispatch [16], fine tracking of NASA's 70-m-deep space network antennas [17], identification of urinary tract infection [18], stent reendothelialization [19], taxonomy and remote sensing of leaf mass per area [20], and many more.For regression applications, regularisation methods based on a penalty function on l2-norms of the model parameters are developed to carry out parameter estimation and model structure selection simultaneously [21–27]. From the powerful Bayesian learning viewpoint, it can be shown that for linear-in-the-parameters models this parameter regularisation is equivalent to a maximised a posterior probability (MAP) estimate of the parameters by adopting a Gaussian prior for the model parameters [22,24–28]. Furthermore, a regularisation parameter is equivalent to the ratio of the related hyperparameter to the noise parameter, lending to an iterative evidence procedure for solving the optimal regularisation parameters [24–28]. Note that, with the OLS algorithm, the evidence procedure for updating regularisation parameters becomes particularly efficient [22,25–27].In information theory, the mutual information (MI) between two random variables is a quantity that measures the mutual dependence of the two variables [29,30]. The MI measure, as a fundamental measure in communications, has also been extensively used in regression applications, such as nonlinear system modelling [31,32], and pattern recognition applications, such as the feature selection [33], the registration of medical images [34] and gene classifications [35]. Note that in the existing literature MI criteria are normally used for training regression models or classifiers. Naturally if the MI is used as model structure selection metrics for classifier design, there is still the need to address model generalisation issue.Against this background, in this work we propose to construct two-class RBF classifiers using the orthogonal forward selection (OFS) scheme, which selects one model term at each stage of the construction procedure by maximising the leave-one-out mutual information (LOOMI) between the classifier's predicted class labels and the true class labels, as well as incorporates a Bayesian procedure of hyperparameter fitting to efficiently derive the regularisation parameters. The paper contains two elements of novel contribution. Firstly, an original derivation of analytically evaluating the LOOMI efficiently is introduced, which facilitates the automatic model structure selection process with no need of using a predetermined error tolerance to terminate the forward selection process. Secondly, a novel Bayesian framework of calculating local regularisation parameters is designed specifically for the forward selection process, which leads to a very sparse classifier. Classification results for a number of benchmark examples demonstrate that our proposed approach efficiently construct very sparse RBF classifiers with excellent generalisation performance.It is worthy emphasising that our contributions are significant. In the existing literature, the MI is used for training regression models and classifiers, but not used for model structure selection by optimising model generalisation capability. Instead of focusing on the usual training performance, to the best of our knowledge, our work is the first one that applies the MI for the effective model structure determination by introducing the novel LOOMI to incrementally maximise the classifier's model generalisation capability directly. Bayesian regularisation is also a well-known and widely used technique, e.g. in the support vector machine (SVM) and the relevance vector machine (RVM) [24] as well as in our previous orthogonal forward selection (OFS) based learning algorithms [22,25–27]. All these existing Bayesian regularisation approaches however involve an iterative procedure for updating the set of regularisation parameters. Specifically, given the values of all the regularisation parameters, model selection is carried out, and the resulting model is then used to update the set of regularisation parameters. This procedure iterates until both the selected model and the set of regularisation parameters converge. In this study, we introduce a novel Bayesian analysis for local regularisation parameter selection effectively nested within the OFS step. More particularly, each OFS stage also effectively fits one regularisation parameter from the data and this task is computationally very fast. Thus there is no need for iteratively performing the model selection and fitting the regularisation parameters several times. This paper is organised as follows. Section 2 introduces the two-class classifier construction using the OFS procedure and the concept of mutual information. In Section 3, we introduce model selection based on fast computing of the LOOMI. In Section 4, we carry out a Bayesian analysis for local regularisation parameter selection nested within the forward selection step. Section 5 presents the complete OFS algorithm that integrates joint parameter estimation with Bayesian regularisation and LOOMI model term selection. In Section 6, experimental results are employed to demonstrate the effectiveness of our proposed approach. Our conclusions are given in Section 7.Consider the N labelled training data samples that belong to an approximately balanced two-class data set, denoted asDN={x(k),y(k)}k=1N, wherex(k)=[x1(k)x2(k)⋯xm(k)]T∈ℝmare m-dimensional feature vectors, and y(k)∈{±1} is the class type ofx(k). We use the data set DNto construct a RBF classifier of the form(1)y˜(M)(k)=sgn(yˆ(M)(k)),yˆ(M)(k)=f(M)(x(k))=∑i=1Mθiϕi(x(k)),where(2)sgn(y)=−1,y≤0,1,y>0,y˜(M)(k)is the estimated class label forx(k) based on the M-term RBF model outputyˆ(M)(k), and M is total number of regressors or model terms, while θiare the model weights, and the regressor ϕi(x) takes the form of Gaussian basis function given by(3)ϕi(x)=exp−∥x−ci∥2τin whichci=[c1,ic2,i⋯cm,i]T is the centre vector of the ith RBF unit and τ>0 is a RBF width parameter. We assume that each RBF unit is placed on a training data, namely, all the RBF centre vectorsciare selected from the training data{x(k)}k=1N, and the RBF width τ has been predetermined, for example, using cross validation.Denotee(M)(k)=y(k)−yˆ(M)(k)as the M-term modelling error for the data pointx(k). Over the training data set DN, further denotey=[y(1)y(2)⋯y(N)]T,e(M)=[e(M)(1)e(M)(2)⋯e(M)(N)]T, andΦM=[ϕ1ϕ2⋯ϕM] withϕl=[ϕl(x(1))ϕl(x(2))⋯ϕl(x(N))]T, 1≤l≤M. We have the M-term model in the matrix form of(4)y=ΦMθM+e(M).HereθM=[θ1θ2⋯θM]T. Let an orthogonal decomposition of the regression matrixΦMbe(5)ΦM=WMAM,where(6)AM=1a1,2⋯a1,M01⋱⋮⋮⋱⋱aM−1,M0⋯01and(7)WM=[w1w2⋯wM]with columns satisfyingwiTwj=0, if i≠j. The regression model (4) can alternatively be expressed as(8)y=WMgM+e(M),where the “orthogonal” model's weight vectorgM=[g1g2⋯gM]T satisfies the triangular systemAMθM=gM, which can be used to determine the original model parameter vectorθM, givenAMandgM.Further consider the following l2-norm regularised orthogonal least squares criterion for the model (8)(9)Le(ΛM,gM)=∥y−WMgM∥2+gMTΛMgM,whereΛM=diag{λ1, λ2, ⋯, λM}, which contains the local regularisation parameters λi≥0, for 1≤i≤M. The solution forgMis obtained by solving ∂Le/∂gM=0, yielding(10)gi(R)=wiTwiwiTwi+λigi(LS),with the usual least squares solution given bygi(LS)=wiTy/wiTwi.The approach taken in this study is to construct a classifier in a forward selection manner, i.e. ϕi(x) is selected from a pool of candidate set and added one at a time to the classifier with some objective that is directly related to the classification performance, such as the misclassification rate [36] or the area under curve (AUC) of receiver operating characteristics (ROC) [37]. For example, by defining the M-term signed decision variable as(11)sk(M)=sgn(y(k))yˆ(M)(k)=y(k)yˆ(M)(k),the misclassification rate over the training data set DNcan be evaluated according to(12)MR(y,yˆ(M))=1N∑k=1NId(sk(M)),where the indication functionIdis defined as(13)Id(s)=1,s≤0,0,s>0.Alternatively, in this study, the MI between the two binary variables y(k)∈{±1} andy˜(M)(k)∈{±1}is used, and this is defined by(14)MI(y,y˜(M))=∑y(k)∑y˜(M)(k)p(y(k),y˜(M)(k))×log2p(y(k),y˜(M)(k))p(y(k))p(y˜(M)(k)),where p(•) denotes the associated probabilities and p(•, •) denotes the associated joint probabilities, respectively. Over the training data set DN, these probabilities can be specifically calculated as(15)p(y(k)=−1)=1N∑k=1NId(y(k)),p(y(k)=1)=1−p(y(k)=−1),(16)p(y(k)=1,y˜(M)(k)=1)=1N∑k=1N(1−Id(sk(M)))y(k)+12,p(y(k)=1,y˜(M)(k)=−1)=1N∑k=1NId(sk(M))y(k)+12,p(y(k)=−1,y˜(M)(k)=1)=1N∑k=1NId(sk(M))1−y(k)2,p(y(k)=−1,y˜(M)(k)=−1)=1N∑k=1N(1−Id(sk(M)))1−y(k)2,and(17)p(y˜(M)(k)=−1)=p(y(k)=1,y˜(M)(k)=−1)+p(y(k)=−1,y˜(M)(k)=−1),p(y˜(M)(k)=1)=1−p(y˜(M)(k)=−1).However, note that both the criteria (12) and (14) measure the classifiers’ performance on the training data set only. In order to measure the model's generalisation capability, the expected classification performance over a fresh data set that has not been used in training should be employed. For the classifier construction based on the misclassification rate, this can be achieved based on the LOO misclassification rate [36]. Similarly, it is possible to develop the leave-one-out MI (LOOMI) by combining the concept of LOO cross validation with the MI. This is derived in the following section.When building a classifier, the ultimate goal is the best classification performance over unseen data. In our case, at each forward selection stage, we are faced with the task of model term selection aimed incrementally at this goal. The concept of leave-one-out (LOO) cross validation is often used to estimate generalisation error by choosing amongst different model architectures [1]. In the following, we develop the concept of LOOMI measure specifically for a forward selection stage.At the lth forward selection step, where l>1, the proposed algorithm selects the lth RBF unit based on a fast calculation of the LOOMI as detailed in this section. Consider the forward selection process at the stage where this l-unit model is produced. Let us denote the l-unit classifier, identified using the entire training data set DN, as f(l)(x). The modelling error of this l-term classifier for the kth data point is given by(18)e(l)(k)=y(k)−f(l)(x(k))=y(k)−yˆ(l)(k).If we “remove” the kth data point from the training data set and use the remaining (N−1) data points to identify the l-unit classifier instead, then the “test” error of the resulting model, which is denoted as f(l,−k)(x) for notational convenience, can be calculated on the data point removed from training. Specifically, the test output of this l-unit classifier at the kth data point not used in training is computed by(19)yˆ(l,−k)(k)=f(l,−k)(x(k)),and the associated predicted label is given by(20)y˜(l,−k)(k)=sgn(yˆ(l,−k)(k)).The test error at the kth data point, referred to as the LOO modelling error, is denoted as(21)e(l,−k)(k)=y(k)−yˆ(l,−k)(k),and the associated LOO signed decision variable is then defined by(22)sk(l,−k)=y(k)yˆ(l,−k)(k).Denote the setS(l)={sk(l,−k)}k=1N. The LOOMI is defined as the MI between the two binary variables y(k)∈{±1} andy˜(l,−k)(k)∈{±1}, and is a functional ofS(l), given by(23)Jl=MI(S(l))=∑y(k)∑y˜(l,−k)(k)p(y(k),y˜(l,−k)(k))×log2p(y(k),y˜(l,−k)(k))p(y(k))p(y˜(l,−k)(k))in which the associated probabilities are calculated based on (15) as well as (24) and (25) given below(24)p(y(k)=1,y˜(l,−k)(k)=1)=1N∑k=1N(1−Id(sk(l,−k)))y(k)+12,p(y(k)=1,y˜(l,−k)(k)=−1)=1N∑k=1NId(sk(l,−k))y(k)+12,p(y(k)=−1,y˜(l,−k)(k)=1)=1N∑k=1NId(sk(l,−k))1−y(k)2,p(y(k)=−1,y˜(l,−k)(k)=−1)=1N∑k=1N(1−Id(sk(l,−k)))1−y(k)2,(25)p(y˜(l,−k)(k)=−1)=p(y(k)=1,y˜(l,−k)(k)=−1)+p(y(k)=−1,y˜(l,−k)(k)=−1),p(y˜(l,−k)(k)=1)=1−p(y˜(l,−k)(k)=−1).For linear-in-the-parameters models, the LOO metrics, such as the LOO mean square error (LOOMSE) [26] and the LOO misclassification rate [36], can be generated without actually splitting the training data set and estimating the associated models, by making use of the Sherman–Morrison–Woodbury theorem [2]. Similarly, the LOOMI can also be obtained analytically without actually splitting the training data set and estimating the associated models. Specifically, we point out that the evaluation of the LOOMI given by (23) makes use of (15), (24) and (25), in which only the signed variablesk(l,−k)and the class label y(k) are needed. Sincesk(l,−k)can be analytically generated, the LOOMI of (23) can also be analytically computed. Clearly this helps computational efficiency significantly. Moreover, we show how the recursive computation as a consequence of orthogonal decomposition contribute further to computational efficiency in the model construction procedure based on maximising the LOOMI.Specifically, let us represent the l-unit model identified using the entire training data set as(26)yˆ(l)(k)=∑i=1lgi(R)wi(k),wherewi(k)is the kth element ofwiandgi(R)is given in (10). Following the concept of LOO cross validation discussed above, it can be shown that the LOO modelling error at the kth data point is given by [2](27)e(l,−k)(k)=e(l)(k)ηk(l)where e(l)(k) is the l-term modelling error defined in (18) andηk(l)is referred to as the LOO error weighting, which can be calculated by [26,36](28)ηk(l)=1−∑i=1lwi2(k)κi+λi,whereκi=wiTwi. Eq. (27) is equivalent to(29)y(k)−yˆ(l,−k)(k)=y(k)−yˆ(l)(k)ηk(l).Multiplying the both sides of (29) with y(k) and applying y2(k)=1 yield(30)1−sk(l,−k)=1−y(k)yˆ(l)(k)ηk(l).that is,(31)sk(l,−k)=∑i=1ly(k)gi(R)wi(k)−∑i=1l(wi2(k)/(κi+λi))ηk(l)=ψk(l)ηk(l).It follows that the signed variablesk(l,−k)of (31) for 1≤k≤N can be obtained very efficiently via the recursive formula(32)ψk(l)=ψk(l−1)+y(k)gl(R)wl(k)−wl2(k)κl+λl,(33)ηk(l)=ηk(l−1)−wl2(k)κl+λl.Hence the LOOMI Jldefined in (23) can be calculated efficiently.The initial condition of the forward selection is referred to as the forward selection step one when the classifier (1) has only one term. It is noted that at this stage the predicted class labels for all the data samples are identical to be either 1 or −1, dependent only on the sign ofg1(R)=θ1, regardless of which candidate regressor is selected. Hence for the first step of forward selection, we select the first regressorϕ1 based on minimising the LOOMSE [26], defined as1N∑k=1N(e(l,−k)(k))2=1N∑k=1N(e(l)(k))2(ηk(l))2.Since at the first stagew1=ϕ1, we havee(1)(k)=y(k)−g1(R)ϕ1(k),ηk(1)=1−ϕ12(k)κ1+λ1,where ϕ1(k) is the kth element ofϕ1,κ1=ϕ1Tϕ1andg1(R)=(ϕ1Ty/(κ1+λ1)).The regularised OFS algorithm has two essential elements, model term selection and parameter estimation. Based on the l2 regularisation, the closed-form solution of (10) can be interpreted as a maximum a posterior probability (MAP) estimate of the parameters with a Gaussian prior. It is known that the regularisation parameter is equivalent to the ratio of the related hyperparameter to the noise parameter within Bayesian framework, and can be optimised by maximising the marginal probability (evidence) as detailed below. In this section, we link the lth forward regression step to Bayesian learning framework and then derive the local regularisation parameter by maximising the evidence. Specifically consider the OFS modelling process that has produced the (l−1)-node RBF model. Let us denote the constructed (l−1)-column regression matrix asWl−1=[w1w2⋯wl−1]. The model output vector of this (l−1)-node RBF is given by(34)yˆ(l−1)=∑i=1l−1gi(R)wi,and the corresponding modeling error vector can be obtained ase(l−1)=y−yˆ(l−1). The lth stage forward regression step is aimed at forming a l-node RBF model by adding the lth model column wl. Clearly this step can be represented by(35)e(l−1)=glwl+e(l).In a standard Bayesian two-level inference framework, the first level of inference infers the model parameters according to the principle of MAP estimation [28]. Specifically, consider (35) wherewlis assumed to be known, and the prior over glis assumed to be Gaussian(36)p(gl|hl)=hl2πexp−hl2gl2,with hl>0 denoting the hyperparameter. The optimalgl(R)is obtained by maximising the posterior probability of gl. The posterior probability of glis given by(37)p(gl|e(l−1),hl,ɛl)=p(e(l−1),gl|hl,ɛl)p(e(l−1)|hl,ɛl)=p(e(l−1)|gl,ɛl)p(gl|hl)p(e(l−1)|hl,ɛl),where the likelihood is assumed to be Gaussian(38)p(e(l−1)|gl,ɛl)=ɛl2πN/2exp−ɛl2∥e(l−1)−glwl∥2,and ɛl>0 denotes the inverse of the noise variance in the target.Maximising logp(gl|e(l−1), hl, ɛl) with respect to glis equivalent to minimising the following Bayesian cost function(39)LB(hl,ɛl,gl)=ɛl∥e(l−1)−wlgl∥2+hlgl2.It can easily verified that the criterion (39) is equivalent to (9) with the relationship λl=hl/ɛl.In order to infer from the data which value of λlis more plausible given the data, the second level inference with Bayesian framework is, for a given model basis vectorwl, to evaluate the evidence p(e(l−1)|hl, ɛl) given by(40)El(hl,ɛl)=p(e(l−1)|hl,ɛl)=∫p(e(l−1),gl|hl,ɛl)dgl,where(41)p(e(l−1),gl|hl,ɛl)=p(e(l−1)|gl,ɛl)p(gl|hl)=ɛl2πN/2hl2πexp−ɛl2∥e(l−1)−wlgl∥2−hlgl22.Notingκl=wlTwlandgl(LS)=wlTe(l−1)/κlas well asgl(R)=(ɛlκl/(ɛlκl+hlgl(LS))), the evidence is derived in Eq. (42):(42)El(hl,ɛl)=ɛl2πN/2hl2πexp−ɛl∥e(l−1)∥22∫exp−ɛl2−2gl(LS)κlgl+ɛlκl+hlɛlgl2dgl=ɛl2πN/2hl2πexp−ɛl∥e(l−1)∥22expɛl2(gl(LS))2κl22(ɛlκl+hl)∫exp−ɛlκl+hl2gl−ɛlgl(LS)κl2((ɛlκl+hl)/2)2dgl=ɛl2πN/2hlɛlκl+hlexp−ɛl∥e(l−1)∥22expɛl2(gl(LS))2κl22(ɛlκl+hl)=ɛl2πN/2hlɛlκl+hlexp−ɛl∥e(l−1)∥22exp(gl(R))2(ɛlκl+hl)2,The log evidence is given by(43)logEl(hl,ɛl)=N2logɛl2π+12loghl−12log(ɛlκl+hl)−ɛl∥e(l−1)∥22+(gl(R))2(ɛlκl+hl)2.Setting ∂logEl(hl, ɛl)/∂ɛl=0 and recalling λl=hl/ɛlyield(44)∂logEl(hl,ɛl)∂ɛl=N2ɛl−κl2(ɛlκl+hl)−∥e(l−1)∥22+(gl(R))2κl2+(ɛlκl+hl)gl(R)∂gl(R)∂ɛl=0so that(45)ɛl=N−(κl/(κl+λl))∥e(l−1)∥2−(gl(R))2(κl+2λl).By setting (∂logEl(hl, ɛl))/∂hl=0, we have(46)∂El(hl,ɛl)∂hl=12hl−12(ɛlκl+hl)+(gl(R))22+(ɛlκl+hl)gl(R)∂gl(R)∂hl=0,yielding(47)hl=κl(gl(R))2(κl+λl).(45) and (47) constitute the recalculation formula for maximising the log evidence for a given wl. The above algorithm is simply fitting one regularisation parameter to one stage of the OFS, which selects a single term for the regression model in an orthogonal space. Therefore, it is computationally very efficient. Note that this regularisation parameter fitting is very different to all the existing regularisation based OFS algorithms [22,25–27] which involve an iterative procedure between the OFS model selection and the updating of all the regularisation parameters. Our proposed novel approach is computationally much more attractive. Similar to any Bayesian approach, the question as to whether the Gaussian prior is suitable can be argued. Indeed the convergence of the solution, (45) and (47), is data dependent. Whene(l−1) appears as random noise, the regularisation parameter will be driven to a high value and this yields a zero-value associated model parameter, leading to sparse models. On the other hand, this may be undesirable for some cases. For example, some low noise data sets with a complicated decision boundary requires little regularisation. Otherwise, the data sets may become ill-conditioned causing numerically instability for (45) and (47). Hence in our algorithm, at any stage if λldiverges or is above a very high value, it is reset as a small number, e.g. 10−6, to allow the OFS to continue. This strategy proves to be useful for the overall numerically stability of the algorithm. Note that the termination of model selection is determined entirely by the LOOMI, unrelated to this strategy.The complete algorithm is presented below integrating (i) the model term selection criterion based on maximising the LOOMI, (ii) Bayesian local parameter regularisation, and (iii) the modified Gram–Schmidt orthogonalisation procedure [3]. Since every training data point is considered as a candidate centre, the candidate regression matrixΦN∈ℝN×N. Define(48)ΦN<l−1=[w1w2⋯wl−1ϕl<l−1⋯ϕN<l−1],withΦN<0=ΦN. If some of the columns inΦN<l−1have been interchanged, this will still be referred to asΦN<l−1for notational simplicity.Initialisation. As explained at the end of Section 3, denote the first selected model term based on the LOOMSE minimisation asϕ1. Set w1=ϕ1. Given a very small positive value λ1 (e.g. 10−6), perform the following Bayesian iteration procedure for a predetermined number of times (e.g. 10):g1(R)=w1Tyκ1+λ1,ɛ1=N−(κ1/(κ1+λ1))∥y∥2−(g1(R))2(κ1+2λ1),h1=κ1(g1(R))2(κ1+λ1),λ1=h1ɛ1.Then recalculateg1(R)=w1Ty/(κ1+λ1), sete(1)=y−g1(R)w1, and for 1≤k≤N calculateψk(1)=y(k)g1(R)ϕ1(k)−ϕ12(k)κ1+λ1,ηk(1)=1−ϕ12(k)κ1+λ1,where ϕ1(k) denotes the kth element ofϕ1.At the beginning of the lth selection stage, we have the regression matrix given in (48). Perform the following steps:Step 1): Set λlto a very small positive value (e.g. 10−6). For l≤j≤N, denote the kth element ofϕj<l−1asϕj<l−1(k), and computeκl[j=(ϕj<l−1)Tϕj<l−1,gl(R),[j=(ϕj<l−1)Te(l−1)/(κl[j+λl),ψk(l),[j=ψk(l−1)+y(k)gl(R),[jϕj<l−1(k)−(ϕj<l−1(k))2κl[j+λl,ηk(l),[j=ηk(l−1)−(ϕj<l−1(k))2κl[j+λl,sk(l,−k),[j=ψk(l),[jηk(l),[j,for 1≤k≤N. Then calculate(49)Jl[j=∑y(k)∑y˜(l,−k),[j(k)p(y(k),y˜(l,−k),[j(k))×log2p(y(k),y˜(l,−k),[j(k))p(y(k))p(y˜(l,−k),[j(k))in which the associated probabilities p(•), p(•, •) are calculated based on (15), (24) and (25), respectively, withsk(l,−k)being replaced bysk(l,−k),[jas appropriate. Herey˜(l,−k),[j(k)is conceptually used to denote the LOO predicted class label. Note that onlysk(l,−k),[jare required in the calculation of the LOOMI (49).Step 2): Find(50)Jl=Jl[jl=maxJl[j,l≤j≤N.Then the jlth column and the lth column ofΦN<l−1are interchanged. The jlth column and the lth column ofANare interchanged up to the (l−1)th row. This effectively selects the resulting lth regressorϕl<l−1in the subset model.Step 3): Setwl=ϕl<l−1, and perform the following Bayesian iteration procedure for a predetermined number of times (e.g. 10)gl(R)=wlTyκl+λl,ɛl=N−(κl/(κl+λl))∥e(l−1)∥2−(gl(R))2(κl+2λl),hl=κl(gl(R))2(κl+λl),λl=hlɛl.λlwill be reset as 10−6 if it diverges or larger than a threshold (e.g. 106). Then sete(l)=e(l−1)−gl(R)wl. Calculateψk(l)andηk(l)for 1≤k≤N using (32) and (33), respectively, and computesk(l,−k)=ψk(l)/ηk(l)for 1≤k≤N. This is followed by updating the value of the LOOMI Jlaccordingly.Step 4): Use the modified Gram–Schmidt orthogonalisation procedure [3] to derive the lth row ofANand to transformΦN<l−1intoΦN<lwl=ϕl<l−1,al,j=wlTϕj<l−1wlTwl,l+1≤j≤N,ϕj<l=ϕj<l−1−al,jwl,l+1≤j≤N.Termination. The selection procedure is terminated with the subset model of the M significant regressors when the following condition is detected(51)JM+i≤JM,1≤i≤p,subject to a minimum model size, where p is a preset number of steps.Similar to any model selection, the model construction using forward selection deals with a tradeoff between overfitting and underfitting. The underfitting occurs if the model is too simple to handle a complex problem. The overfitting scenario is related to fitting a model to noisy training data by overly increasing model complexity. This overfitted model however is unlikely to have good classification performance for the unseen data. Since the LOOMI is based on cross validation, provided there is a sufficient number of model terms, JMwill be monotonically increasing until it reaches a global maximum, indicating that a suitable model size M has been achieved.Since generally M≪N, we can approximately estimate the computational cost of the proposed algorithm and conclude that it has a similar computational complexity to our previously proposed LOO cross validation based OFS algorithms, e.g. [36,37,38]. These algorithms have a computational complexity at O(N2), scaled by a small number of variables used in the algorithms. This is because at each OFS stage, there are also O(N) candidates for model term selection in our algorithm, and the operations at each stage are based on vector operation with the size N. Thus, each OFS stage has a complexity of O(N2). The total cost of the algorithm is therefore O(N2) scaled by M which is far smaller than N.

@&#CONCLUSIONS@&#
An OFS algorithm for automatically constructing RBF classifiers has been proposed based on a new model-term selection criterion that maximises the leave-one-out mutual information between the classifier's predicted class labels and the true class labels. Integrated within each OFS step, a Bayesian procedure of hyperparameter fitting has been introduced to infer the l2-norm local regularisation parameter from the data. In our algorithm, model terms are selected by directly optimising the classifier's generalisation performance, and Bayesian evidence procedure for fitting the local regularisation parameter significantly enhances the sparsity of the constructed RBF classifier. Consequently, our RBF classifier construction procedure automatically terminates without any additional stopping criterion to yield very parsimonious RBF classifiers with excellent classification generalisation performance. Several benchmark examples have been employed to demonstrate the effectiveness of our proposed approach, in particular the ability of constructing very sparse models automatically with similar good generalisation performance as some well-known existing state-of-the-arts methods reported in the literature.Our future work will further investigate this sparse classifier construction algorithm using other real-life benchmark data sets. For the challenging class of high-dimensional classification problems, where the feature space dimension is extremely large, in thousands or even tens of thousands, but the sample size is extremely small by comparison, in hundreds or even in tens, efficient feature selection becomes essential. We are currently investigating suitable feature selection techniques for integrating with the proposed algorithm in order to tackle this type of challenging high-dimensional classification problems effectively.