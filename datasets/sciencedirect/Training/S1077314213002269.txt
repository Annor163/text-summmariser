@&#MAIN-TITLE@&#
The 3dSOBS+ algorithm for moving object detection

@&#HIGHLIGHTS@&#
A novel neural-based moving object detection algorithm is proposed.Main novelties: initial background estimation, shadow handling, spatial coherence.It accurately handles moving backgrounds, gradual light variations, and shadows.It provides robustness against false detections for different videos.A deep analysis of experimental results on the BMC dataset is reported.

@&#KEYPHRASES@&#
Background subtraction,Motion detection,Neural network,Self organization,

@&#ABSTRACT@&#
We propose the 3dSOBS+ algorithm, a newly designed approach for moving object detection based on a neural background model automatically generated by a self-organizing method. The algorithm is able to accurately handle scenes containing moving backgrounds, gradual illumination variations, and shadows cast by moving objects, and is robust against false detections for different types of videos taken with stationary cameras. Experimental results and comparisons conducted on the Background Models Challenge benchmark dataset demonstrate the improvements achieved by the proposed algorithm, that compares well with the state-of-the-art methods.

@&#INTRODUCTION@&#
Several methods have been proposed for moving object detection based on background subtraction [1–4]. Nonetheless, the best approach has not yet been devised, as also witnessed by recent activities aimed at comparing the state-of-the-art methods on publicly available video sequence datasets [5,6].Moving object detection, as a specific case of object segmentation, can be viewed as a clustering problem in a feature space derived from the color and motion information, and therefore is well suited for unsupervised modeling approaches. Unsupervised learning is in general preferred over supervised learning because the latter requires a set of training samples, which may not be available, especially when the image features are unknown or when a certain degree of automation is desired [7].Many approaches have been explored, including statistical background modeling (e.g., single Gaussian [8] or Mixture of Gaussians [9]), clustering-based background modeling (e.g., Codebook [10]), neural-based background modeling (e.g., [11]), eventually corroborated by the introduction of fuzzy concepts to handle imprecision and uncertainties [12,13]. Neural network-based solutions have received considerable attention due to the fact that these methods are usually more effective and efficient than traditional ones, relying on the well-known advantages of neural networks, such as adaptivity, parallelism, and learning [14]. Among the most recent results, the 3D Self-Organizing Background Subtraction algorithm [15] implements an approach for moving object detection based on a neural background model automatically generated by a self-organizing method, that accurately handles most of the well known background maintenance issues.The main contribution of our paper relies on an enhanced version of the algorithm concerning the introduction of: (a) initial background estimation; (b) shadow detection and removal; and (c) spatial coherence. These enhancements lead to the 3dSOBS+ algorithm that can accurately handle scenes containing moving backgrounds, gradual illumination variations, and shadows cast by moving objects, and provides further robustness against false detections for different types of videos taken with stationary cameras. Experimental results and comparisons conducted on the Background Models Challenge (BMC) benchmark dataset [6] demonstrate the improvements achieved by the algorithm and that it compares well with the state-of-the-art methods.The paper is organized as follows. In Section 2 we describe in a unified framework the neural background model adopted from [15] for moving object detection and its enhanced version here proposed, while in Section 3 we provide a thorough analysis of experimental results on the BMC dataset and comparisons with several state-of-the-art methods. Conclusions are drawn in Section 4.The proposed algorithm relies on our main idea to adopt a biologically inspired problem-solving method based on visual attention mechanisms. The aim is to obtain objects that keep the user attention in accordance with a set of predefined features, including gray level, motion, and shape features. Our approach defines a method for the generation of an active attention focus to monitor dynamic scenes for surveillance purposes. The idea is to build the background model by learning in a self-organizing manner many background variations. Based on the learned background model through a map of motion and stationary patterns, our algorithm detects moving objects and selectively updates the background model. The obtained self-organizing neural network can be organized as a 2D grid of neurons [11] or a 3D grid of neurons [15], in both cases producing a representation of training samples with lower dimensionality, at the same time preserving topological neighborhood relations of the input patterns.In the following we will describe the algorithm and how it is able to tackle the most frequent and difficult background maintenance problems [16], including bootstrapping, illumination changes, waving trees, cast shadows, and robustness against false detections.Given an image sequenceIt, for each pixelxin the image domain, a neural map is built, consisting of n weight vectorsmti(x),i=1,…,n, which will be called a model for pixelx. If each sequence frame has N rows and P columns, the complete set of modelsMt(x)=(mt1(x),…,mtn(x))for all pixelsxof the t-th sequence frameItis organized as a 3D neural mapBtwith N rows, P columns, and n layers. Therefore, the neural background modelBtconsists of n imagesLti,i=1,…,n, of the same size of imageIt, which we call layers, and each layerLticontains, for each pixelx, the i-th weight vectormti(x). A pictorial representation of this 3D neural model can be found in [15].In [15], the neural model is initialized by setting all the model layers equal to the first frame of the sequence. However, in order to better handle cases where also in the initial frames the background is occluded by foreground objects (the so-called bootstrapping problem [16]), in this paper we propose to adopt one of the methods for background estimation, such as those proposed in [17] and references therein, to achieve an initial estimateE0of the scene background. As an example, in our experiments we adopt the temporal median method [18], that consists in estimating the initial backgroundE0as the temporal median over a subset of F initial sequence framesI0,…,IF-1. Then, the neural background modelB0is initialized by setting all weight vectors related to a pixelxequal to the pixel brightness value of the estimated background, that is,(1)m0i(x)=E0(x),i=1,…,n.Therefore, the resulting initial neural mapB0consists of n layers, each of which is a copy of the estimated backgroundE0.After initialization, at each time step t, background subtraction is achieved by comparing each pixelxof the tth sequence frameItwith the pixel current modelMt-1(x), in order to determine if there exists a best matching weight vectormt-1b(x)that is close enough to it. If no acceptable matching weight vector exists,xis detected as foreground; otherwise,xis detected as a background pixel. In case of background pixels, further learning of the neural map enables the adaptation of the background model to slight scene modifications, such as gradual illumination changes. This learning is achieved by updating the neural weights according to a visual attention mechanism of reinforcement, where the best matching weight vectors, together with their neighborhood, are reinforced into the neural map.The above described steps will be detailed in the following subsections, also extending the background model update in order to enhance robustness against false detections and to better handle cast shadows.At time t, the valueIt(x)of each incoming pixelxof the t-th sequence frameItis compared to the pixel current modelMt-1(x)=(mt-11(x), …,mt-1n(x)), to determine the weight vectormt-1b(x)that best matches it:(2)d(mt-1b(x),It(x))=mini=1,…,nd(mt-1i(x),It(x)),where the metricd(·,·)is suitably chosen according to the specific color space being considered. For the experiments reported in Section 3, the Euclidean distance of vectors in the HSV color hexcone, as suggested in [19], has been adopted, that gives the distance between a weight vectormt-1i(x)and a pixel valueIt(x)as:(3)d(mt-1i(x),It(y))=‖(mV·mS·cos(mH),mV·mS·sin(mH),mV)-(IV·IS·cos(IH),IV·IS·sin(IH),IV)‖22,where(mH,mS,mV)and(IH,IS,IV)indicate the hue, saturation, and value components ofmt-1i(x)andIt(x), respectively.In order to adapt the neural background model to scene modifications, the modelMt-1(x)for pixelxat time t should be updated. The best matching weight vectormt-1b(x)(computed according to Eq. (2)) and its neighboring weight vectors of the b-th layer of modelBtare updated according to weighted running average:(4)mtb(y)=(1-α(x,y))mt-1b(y)+α(x,y)It(y),∀y∈Nx.HereNx=y:x-y⩽w2Dis a 2D square spatial neighborhood ofxwith half-widthw2Dincludingx. Moreover,(5)α(x,y)=γ·G2D(y-x)·1-Dt(x)·1-St(x),whereγrepresents the learning rate,G2D(·)=N(·;0,σ2D2I)is a 2D Gaussian low-pass filter [20] with zero mean andσ2D2Ivariance,Dt(x)indicates the background subtraction mask value for pixelx, that will be described in Section 2.3.3, andSt(x)is the shadow mask value indicating if pixelxbelongs to the shadow cast by an object in the scene, that will be described in Section 2.3.4. Theα(x,y)values in Eq. (5) are weights that allow the neural model to smoothly take into account the spatial relationship between current pixelxand its neighboring pixelsy∈Nx, thus preserving topological properties of the input (close inputs correspond to close outputs). The choice of the learning factorγdepends on the scene variability: largeγvalues enable the network to faster learn changes corresponding to the background, but also leading to false negatives, that is, inclusion into the background model of pixels belonging to foreground moving objects. On the contrary, lower learning rates make the network slower to adapt to rapid background changes, making the model more tolerant to errors due to false negatives through self-organization. Indeed, weight vectors of false negative pixels are readily smoothed out by the learning process itself.The 2D update of Eq. (4) involves only model weight vectors lying in the same layer b as the best matching weight vectormt-1b(x). In order to further enable the reinforcement ofmt-1b(x)in the model for pixelx, also weight vectors ofxbelonging to layers close to layer b are updated. This further update is achieved by weighted running average:(6)mti(x)=(1-δ(x))mt-1i(x)+δ(x)It(x),that involves the weight vectorsmti(x)ofxsuch thati-b⩽w1D; that is, it involves the weight vectors that belong to a 1D inter-layer neighborhood ofmt-1b(x)having half-widthw1D. In Eq. (6),δ(x)is set as(7)δ(x)=ν·G1D(x)·1-Dt(x)·1-St(x),whereνis the learning rate (whose choice is based on the same mathematical ground as for the learning rateγin Eq. (5)),G1D(·)=N(·;0,σ1D2)is a 1D Gaussian low-pass filter with zero mean andσ1D2variance in the 1D inter-layer neighborhood, andDt(x)andSt(x)are the mask values already adopted in Eq. (5).It is useful to think of spatial coherence in terms of the intensity difference between spatially contiguous pixels. This means that neighboring pixels showing small intensity differences are coherent, while neighboring pixels with high intensity differences are incoherent. In this paper, we exploit the spatial coherence of scene objects as compared with the scene background. It is introduced in the background model updating formula by computing the background subtraction mask valueDt(x)in Eqs. (4) and (6) as(8)Dt(x)=1ifNCFt(x)⩽0.50otherwise.The Neighborhood Coherence FactorNCFt(x)is defined as [21]:(9)NCFt(x)=|Ωx||Hx|,where|·|refers to the set cardinality,Hx={y:|x-y|⩽h}is a 2D spatial neighborhood ofxof size (2h+1)×(2h+1) includingx, andΩxindicates the set of pixelsybelonging toHxthat have in their background model a best match that is close enough to their valueIt(y):(10)Ωx={y∈Hx:d(mt-1b(y),It(y))⩽∊}.The threshold∊enables the distinction between foreground and background pixels. It should be chosen taking into account that high values allow the model to include several observed pixel intensity variations, thus enriching it with the past history of observed background variations; on the other hand, lower values allow the model to avoid including foreground objects, thus leading to a more accurate background model.The thresholding functionDtdefined in Eq. (8) ensures selectivity in the update of the background model, allowing it to adapt to scene modifications, but without introducing into it the contribution of pixels that do not belong to the background scene. Indeed, the Neighborhood Coherence FactorNCFt(x)gives a relative measure of the number of pixels belonging to the spatial neighborhoodHxof a given pixelx, whose value is well represented by the background model. IfNCFt(x)>0.5, most of the pixels in this neighborhood have values well represented by the background model, and this should imply that also the value of pixelxis well represented by the background model, and can thus be considered as a background pixel (mask value 0 in Eq. (8)). If, on the other side,NCFt(x)⩽0.5, most of the pixels in this neighborhood are not well represented by the background model, and also the pixelxshould be considered as a foreground pixel (mask value 1 in Eq. (8)).The introduction of spatial coherence in the updating formula allows the background subtraction algorithm to achieve higher robustness against false detections, as it will be verified through experimental results (see Section 3).Shadows cast on the background by moving objects apparently behave themselves as moving. In order to accurately detect the moving object shape, if a pixelxis in the shadow cast by some object, then it should be considered as background; however, it should not be used to update the corresponding weight vectors, in order to avoid the reinforcement of shadow information into the background model. This selectivity in the background model update can be achieved by setting to one the binary mask valueSt(x)in Eqs. (5) and (7) in casexis a shadow pixel, and setting it to zero otherwise.In order to discriminate shadow pixels, in this paper we extend the approach adopted in [11], based on the idea that a shadow cast on the background lowers its value and saturation, without significantly changing its hue. Therefore, given the current modelMt-1(x)for a pixelxof the tth sequence frameIt, the shadow mask valueSt(x)appearing in Eqs. (5) and (7) is set as(11)St(x)=1if∃mt-1i(x)∈Mt-1(x):|IH-mH|⩽τH∧IS-mS⩽τS∧τVl⩽IVmV⩽τVu0otherwisewhere(mH,mS,mV)and(IH,IS,IV)indicate the hue, saturation, and value components ofmt-1i(x)andIt(x), respectively. Values for thresholdsτVl,τVu,τS, andτHadopted in our experiments will be detailed in Section 3.1.The background subtraction and update procedure described in the previous subsections can be sketched for each pixelxas the 3dSOBS+ (Enhanced 3D Self-Organizing Background Subtraction) Algorithm reported in Fig. 1.

@&#CONCLUSIONS@&#
The 3dSOBS+ algorithm enhances the self-organizing background subtraction approach for moving object detection, introducing initial background estimation, spatial coherence, and shadow detection and removal. It robustly handles gradual light changes, waving trees, and cast shadows, both for tiny and large moving objects. Indeed, experimental results and comparisons conducted on the extensive Background Models Challenge benchmark dataset demonstrate that the proposed approach is accurate and compares well with the state-of-the-art methods.Anyway, there is still room for improvements. Indeed, through experiments we highlighted some cases, such as sudden illumination variations and reflections, where the accuracy could be improved, basically by introducing higher level analysis, e.g., as in [28,29].