@&#MAIN-TITLE@&#
Enhanced needle localization in ultrasound using beam steering and learning-based segmentation

@&#HIGHLIGHTS@&#
Beam steering is used to enhance the appearance of needles in ultrasound images.Needle segmentation is challenging, even when using beam steering-based needle enhancement.Needle segmentation is posed as a machine learning problem. A boosted classifier using a bank of log-Gabor wavelets is proposed for needle pixel classification. The classifier is trained on ex vivo and clinical nerve block datasets.As a result of improved segmentation performance compared to previous methods, needle localization accuracy and robustness are increased using the proposed method.

@&#KEYPHRASES@&#
Ultrasound,Regional anesthesia,Segmentation,Classification,Needle guidance,

@&#ABSTRACT@&#
Segmentation of needles in ultrasound images remains a challenging problem. In this paper, we introduce a machine learning-based method for needle segmentation in 2D beam-steered ultrasound images. We used a statistical boosting approach to train a pixel-wise classifier for needle segmentation. The Radon transform was then used to find the needle position and orientation from the segmented image. We validated our method with data from ex vivo specimens and clinical nerve block procedures, and compared the results to those obtained using previously reported needle segmentation methods. Results show improved localization success and accuracy using the proposed method. For the ex vivo datasets, assuming that the needle orientation was known a priori, the needle was successfully localized in 86.2% of the images, with a mean targeting error of 0.48mm. The robustness of the proposed method to a lack of a priori knowledge of needle orientation was also demonstrated. For the clinical datasets, assuming that the needle orientation was closely aligned with the beam steering angle selected by the physician, the needle was successfully localized in 99.8% of the images, with a mean targeting error 0.19mm. These results indicate that the learning-based segmentation method may allow for increased targeting accuracy and enhanced visualization during ultrasound-guided needle procedures.

@&#INTRODUCTION@&#
Needle-based clinical procedures such as nerve block, fluid aspiration, therapeutic injections, and biopsy can be performed under ultrasound (US) image guidance [2–7]. The use of US imaging allows for real-time visualization of needles within the desired anatomical context. Electronically steering the US beam in a direction perpendicular to the needle orientation produces strong specular reflections that enhance needle visualization [8,9]. However, needle visualization is still challenging in the following scenarios: (1) the needle is not completely contained in the 2D US imaging plane, (2) the background tissue contains long, linear objects such as bone, fascia, or tissue boundaries that resemble the needle, and (3) the needle orientation does not match the US beam steering angle. For example, Fig. 1shows a B-mode US image frame acquired during a nerve block procedure using beam steering hardware and software for improved needle visualization. The needle is only slightly in-plane, and automatic segmentation of the needle is made more challenging by the nearby bright, linear anatomical structures that resemble the needle. In such cases, a robust and reliable segmentation of the needle would allow the ultrasound system to automatically optimize the beam steering angle and image processing settings to increase the contrast of the needle relative to the background structures. In addition, proper identification of the needle position and orientation can help improve targeting accuracy.Needle localization methods can be divided into two groups: hardware and software methods. Hardware methods seek to detect the needle by augmenting it with tracking sensors or other physical modifications. Hardware approaches to needle tracking include: localization using needle vibration and Doppler US [10], attaching small US receivers to the needle and using time-of-flight principles for spatial localization [11], and using magnetic tracking sensors and camera attachments for localization [12–14].Software-based methods seek to segment the needle using image processing algorithms. The focus of recent work in this area has largely been applied to 3D US. For example, the pose of long, straight instruments augmented with passive markers was found in 3D US images using a GPU accelerated generalized Radon transform [15]. The algorithm presented in [16] demonstrated localization of long straight tools in near real-time without the use of hardware acceleration by fitting voxels to a maximum likelihood-based tool model using the RANSAC algorithm. A volume rendering-based approach to tool localization was demonstrated in [17]. In [1], line filtering methods were employed to increase the contrast of the needle relative to background structures in 3D US.Compared to 3D US, 2D US is currently more widely used and has an easier learning curve for needle guided procedures than 3D US due to ease of image interpretation. Nevertheless, automatic needle localization in 2D US still remains challenging. In [18], curved needles in 2D US images were detected using a polynomial Hough transform and a modified coordinate transform. In [19], a real-time Hough transform was developed to increase needle tracking speed, where needle segmentation was accomplished using intensity thresholding. The threshold was found by computing the percentage of pixels that should belong to the needle based on its presumed length, and then finding the corresponding threshold from the intensity histogram. In [20], needle segmentation was performed using a combination of a variance filter, intensity thresholding, and morphological operations. Principle component analysis was used for final pose estimation. Segmentation of prostate biopsy needle path was investigated in [21,22]. In [21], metrics based on pixel temporal difference, temporal variance, and spatial variance, were combined to detect a biopsy core. In [22], a graph-cut-based segmentation was performed on images filtered with a second-order Gaussian derivative.In real clinical cases, a key challenge is accurate segmentation when the needle is not the most prominent linear structure in the image and the true orientation of needle entry deviates from that expected by the operator. We hypothesized that framing needle segmentation as a classification problem would make the segmentation more robust to inaccurate prior assumptions about needle appearance and orientation. A segmentation method based on statistical boosting of wavelet features was developed and tested on challenging ex vivo and clinical datasets with sub-optimal needle positioning and the presence of tissue artifacts. Results showed enhanced localization accuracy using the proposed method.

@&#CONCLUSIONS@&#
