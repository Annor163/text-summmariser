@&#MAIN-TITLE@&#
Tensor completion via a multi-linear low-n-rank factorization model

@&#HIGHLIGHTS@&#
We extend the low-rank matrix completion problem to a low-n-rank tensor completion problem.An efficient algorithm based on the multi-linear low-n-rank factorization model is proposed in this paper.The nonlinear Gauss–Seidal method that only requires solving a linear least squares problem per iteration is applied to solve this model.The proposed algorithm is much less computational cost than the trace norm minimization algorithm especially facing the large data.

@&#KEYPHRASES@&#
Tensor completion,Multi-linear low-n-rank factorization,Nonlinear Gauss–Seidal method,Singular value decomposition,

@&#ABSTRACT@&#
The tensor completion problem is to recover a low-n-rank tensor from a subset of its entries. The main solution strategy has been based on the extensions of trace norm for the minimization of tensor rank via convex optimization. This strategy bears the computational cost required by the singular value decomposition (SVD) which becomes increasingly expensive as the size of the underlying tensor increase. In order to reduce the computational cost, we propose a multi-linear low-n-rank factorization model and apply the nonlinear Gauss–Seidal method that only requires solving a linear least squares problem per iteration to solve this model. Numerical results show that the proposed algorithm can reliably solve a wide range of problems at least several times faster than the trace norm minimization algorithm.

@&#INTRODUCTION@&#
A tensor is a multidimensional array which is the higher-order generalization of vector and matrix. It has many applications in information science, computer vision and graph analysis [1]. In the real world, the size and the amount of redundancy of the data increase fast, and nearly all of the existing high-dimensional real world data either have the natural form of tensor (e.g. multi-channel images and videos) or can be grouped into the form of tensor (e.g. tensor face [2]). Therefore, challenges come up in many areas when one confronts with the high-dimensional real world data. Tensor decomposition is a popular tool for high-dimensional data processing, analysis and visualization. Two particular tensor decomposition methods can be considered as higher-order extensions of the matrix singular value decomposition: CANDECOMP/PARAFAC (CP) [3,4] and the Tucker [5]. Tensor decomposition gives a concise representation of the underlying structure of tensor, revealing when the tensor data can be modeled as lying close to a low-dimensional subspace. Although useful, they are not as powerful. For general tensors, tensor decomposition does not deliver best low rank approximation, which will limit its applications.In this paper, we will try to recover a low-n-rank tensor from a subset of its entries. This problem is called the tensor completion problem. It is also called missing value estimation problem of tensors. The problem in computer vision and graphics is known as image and video in-painting problem [6,7]. The key factor to solve this problem is how to build up the relationship between the known elements and the unknown ones. Owing to this reason, the algorithms for completing tensors can be coarsely divided into local algorithms and global algorithms. Local algorithms [8,9] assume that the further apart two points are, the smaller their dependence is and the missing entries mainly depend on their neighbors. Thus, the local algorithms can only exploit the information of the adjacent entries. However, sometimes the values of the missing entries depend on the entries which are far away and the local algorithms cannot take advantage of a global property of tensors. Therefore, in order to utilize the information of tensors as much as possible, it is necessary to develop global algorithms that can directly capture the complete information of tensors to solve the tensor completion problem.In the two-dimensional case, i.e. the matrix, the rank is a powerful tool to capture the global information and can be directly determined. But for the high-dimensional case, i.e. the tensor, there is no polynomial algorithm to determine the rank of a specific given tensor. Recently, based on the extensions of trace norm for the minimization of tensor rank, some global algorithms [6,7,10–12] solving the tensor completion problem via convex optimization have been proposed. Liu et al. [6] first proposed the definition of the trace norm of an n-mode tensor as||X||⁎=(1/n)∑i=1n||X(i)||⁎. And similar to matrix completion, the tensor completion was formulated as a convex optimization problem. For tackling this problem, they developed a relaxation technique to separate the dependant relationships and used the block coordinate descent (BCD) method to achieve a globally optimal solution. The contribution of this paper is realized at the methodological level by considering a more general kind of the tensor completion problem. By the extension of the concept of Shatten-q norm for matrix, Signoretto et al. [10] defined tensor Shatten-{p, q} norm, which is formulated as||X||p,q=((1/n)∑i=1n||X(i)||Σ,qp)1/pand consistent with that for matrix. Compared to the trace norm defined in [6], Shatten-{p, q} norm is a more general tensor norm and the trace norm of tensor can be seen as a special case of Shatten-{p, q} norm(||X||1,1=(1/n)∑i=1n||X(i)||Σ,11=(1/n)∑i=1n||X(i)||⁎=||X⁎||). Though the general tensor Shatten-{p, q} norm was defined in this paper, they mainly focused on the special case trace norm of tensor in their algorithm. Similar to the above two works, Gandy et al. [7] used the n-rank of a tensor as sparsity measurement and tried to find the tensor of lowest n-rank that satisfies some linear constraints. In their algorithm, the tensor completion was converted into a multi-linear convex optimization problem. Based on the Douglas–Rachford splitting technique [13,14] and the alternating direction method of multipliers [15], trace norm was introduced as the convex envelope of the n-rank and an efficient algorithm to solve the multi-linear convex optimization problem was proposed. In these trace norm based algorithms, they consider the tensor completion problem as recovering a low-n-rank tensor from a subset of its entries, that is,(1)minX∈ℝI1×I2×⋯×IN12||X−Y||Fs.t.1N∑i=1N||X(i)||⁎≤cYΩ=ℳΩwhereX,Y,ℳare n-mode tensors with identical size in each mode. The elements ofℳin the setΩare given, while the remaining elements are missing.X(i)is the mode-i unfolding ofX.||⋅||⁎denotes the trace norm defined by the sum of all singular values of the matrix. On the other hand, Zhang et al. [16] exploited the recently proposed tensor-singular value decomposition (t-SVD) [17] that is a group theoretic framework to solve the tensor compression and recovery problem. They first constructed novel tensor-rank like measures to characterize informational and structural complexity of tensor.The core strategy of all these algorithms to achieve the optimal solution is the same as that they estimate the variables sequentially, followed by certain refinement in each iteration. Although the details of the solution procedure in each algorithm are different, unfortunately, the refinement in each iteration of all these algorithms requires computing singular value decompositions(SVD) that a task is increasingly costly as the tensor size and n-ranks increase. It is therefore desirable to exploit an alternative algorithm more efficient in solving tensor completion problem.In this paper, a new global algorithm for tensor completion called tensor completion via a multi-linear low-n-rank factorization model (TC-MLFM) is proposed. As the size and structure of each mode of the given tensor are not always the same (e.g. RGB images), the new algorithm combines n-ranks of each tensor mode by weighted parameters. However, the problem is that the function is generally NP-hard and hard to approximate due to the non-convex optimization ofrank(X(i)). To solve this problem, we use n-rank factorization optimization problem to substituterank(X(i)). The new function is solvable and considered as our model. With the new weighted objective model, the proposed algorithm can utilize the mode information of the tensor with choice. To solve this model, a minimization method based on the nonlinear Gauss–Seidal method [18] that only requires solving a linear least squares problem per iteration is applied. By adopting this method along each mode of the tensor other than minimizing the trace norm in Eq. (1), the new algorithm can avoid the SVD computational strategy and reliably solve a wide range of tensor completion problems much faster than the trace norm minimization algorithm.The rest of the paper is organized as follows. Section 2 presents some notations and basic properties of tensors. In section 3, we review the definition of Tucker decomposition and tensor n-rank, which suggests that a low-n-rank tensor is a low rank matrix when appropriately unfolded. Section 4 discusses the detailed process of the proposed algorithm. Section 5 reports experimental results of our algorithm on simulated data and image completion. Finally, section 6 provides some concluding remarks.In this paper, the nomenclatures and the notations in [1,19] on tensor are partially adopted. Scalars are denoted by lowercase letters (a, b, c, …), vectors by bold lowercase letters (a,b,c, …) and matrices by uppercase letters (A, B, C, …). Tensors are written as calligraphic letters(A,ℬ,C,…).n-Mode tensors are denoted asA∈ℝI1×I2×⋯×IN.Its elements are denoted asai1…ik…iN, where1≤ik≤IK,1≤K≤N. The mode-n unfolding (also called matricization or flattening) of a tensorA∈ℝI1×I2×⋯×INis defined asunfold(A,n)=A(n). The tensor element(i1,i2,…,iN)is mapped to the matrix element(in,j), where(2)j=1+∑k=1k≠nN(ik−1)JkwithJk=∏m=1m≠nk−1Im.Therefore,A(n)∈ℝIn×J, whereJ=∏k≠nk=1NIk. Accordingly, its inverse operator fold can be defined asfold(A(n),n)=A.The n-rank of a N-dimensional tensorA∈ℝI1×I2×⋯×IN, denoted byrn, is the rank of the mode-n unfolding matrixA(n).(3)rn=rankn(A)=rank(A(n)).The inner product of two same-size tensorsA,ℬ∈ℝI1×I2×⋯×INis defined as the sum of the products of their entries, i.e.(4)〈A,ℬ〉=∑i1∑i2⋯∑iNai1…ik…iNbi1…ik…iN.The corresponding Frobenius norm is||AF||=A,A. Besides, theℓ0norm of a tensorA, denoted by||A0||, is the number of non-zero elements inAand theℓ1norm is defined as||A1||=∑i1…ik…iN|ai1…ik…iN|. It is clear that||A||F=||A(n)||F,||A||0=||A(n)||0and||A||1=||A(n)||1for any1≤n≤N. The n-mode (matrix) product of a tensorA∈ℝI1×I2×⋯×INwith a matrixM∈ℝJ×Inis denoted byA×nMand is sizeI1×⋯×In−1×J×In+1×⋯×IN. In terms of flattened matrix, the n-mode product can be expressed as follows:(5)Y=A×nM⇔Y(n)=MA(n).The Tucker decomposition [5] is a form of higher-order principal component analysis. It decomposes a tensor into a core tensor multiplied (or transformed) by a matrix along each mode. Thus, in the three-way case whereA∈RI1×I2×I3, we have(6)A≈S×1X×2Y×3Z.hereS∈ℝR1×R2×R3is called the core tensor and its entries show the level of interaction between the different components.X∈ℝI1×R1,Y∈ℝI2×R2,Z∈ℝI3×R3are the factor matrices (which are usually orthogonal) and can be thought of as the principal components in each mode. IfR1,R2,R3are significantly smaller thanI1,I2,I3, respectively, the core tensorScan be thought of as a compressed version ofAand then we considerAas a low-n-rank tensor. Formally,Ais called low-n-rank tensor if its unfoldings are low-rank matrices. Thus we can use the ranks of unfoldings of a tensorAto learn low-n-rank tensorA. This rank should not be confused with the idea of tensor CP-rank [3,4]. An illustration of Tucker model for third-order tensors is given inFig. 1. For notational simplicity, we illustrate our results in this paper using third order tensors, while generalizations to high order cases are straightforward.This section is separated into two parts. Part 1 extends the matrix completion problem to tensor case and converts the tensor completion problem into a weighted multi-linear low-n-rank factorization model. Part 2 applies the nonlinear Gauss–Seidal method to solve the objective model and presents the details of solution procedure.The derivation starts with the well-known optimization problem for the low rank matrix completion [20,21]:(7)min:L∈ℝm×nrank(L)s.t.LΩ=MΩ,whererank(L)denotes the rank of L, and the elements of M in the setΩare given while the remaining elements are missing. Eq. (7) aims to use a low rank matrix L to approximate the given matrix with missing elements. The optimization problem in Eq. (7) is a non-convex optimization problem since the functionrank(L)is non-convex.The higher-order tensor completion problem can be generated from the matrix (i.e. 2nd-order tensor) case by utilizing the form of Eq. (7), leading to the following formulation:(8)minℒ∈ℝI1×I2×⋯×IN.rankCP(ℒ)s.t.ℒΩ=ℳΩ,where the rank ofℒdenotes the CP-rank of tensor,ℒ,ℳare n-mode tensors with identical size in each mode. The elements ofℳin the setΩare given while the remaining elements are missing.The definition of CP-rank, in the form ofrankCP(X), is the minimum number of rank-1 tensors that generateXas their sum [3,4]. In other words, CP-rank is the minimum number of components in an exact CP decomposition. The CP-rank of a tensor is defined as an exact analogue to the definition of matrix rank, but the properties of matrix are quite different from that of tensors. For instance, the CP-rank of a real-valued tensor may actually be different from mode to mode. One major difference between matrix rank and tensor CP-rank is that there is no straightforward algorithm to determine the CP-rank of a specific given tensor. Therefore, Eq. (8) is difficult to solve. In fact, the problem is NP-hard [1,22].On the other hand, the n-rank is defined as the dimension of the vector space spanned by the mode-n unfolding matrix. As discussed in Section 3, when the given tensor is a low-n-rank tensor, the n-ranks instead of the CP-rank of a tensor can be used to capture its global information. Therefore, we can minimize the n-ranks of the given tensor instead of minimizing the CP-rank to solve the tensor completion problem. As a result, a functionFwhich minimizes the n-ranks of the given tensor to replace Eq. (8) is obtained as the following shows:(9)F(minℒ∈ℝI1×I2×⋯×IN.(rank(L(1)),rank(L(2)),…rank(L(N)))s.t.(L(i))Ω=(M(i))Ω,whereL(i),M(i)are the mode-i unfoldings ofℒandℳ. As the size and structure of each mode of the given tensor are not always the same, the contribution of each mode to the final result may be different. Then the n-rank minimization problem of each mode can be combined by weighted parameters:F(minℒ∈ℝI1×I2×⋯×IN.(rank(L(1)),rank(L(2)),…rank(L(N)))=∑iNλi(minL(i)rank(L(i)).).thus, the tensor completion problem becomes:(10)∑i=1Nλi(minL(i)rank(L(i)).)s.t.(L(i))Ω=(M(i))Ω.Eq. (10) aims to find a low-n-rank tensorℒto approximate the given tensor with missing elements. A tensor is a multidimensional array or an element of the tensor product of N vector spaces. That is to say matrix is special instance of tensor.By comparing Eq. (10) to Eq. (7), we can observe that Eq. (10) is derived from the tensor completion problem and can be viewed as a weighted multi-linear matrix completion problem. In other words, matrix completion problem is also a special instance of tensor completion problem.Although the elements involved in are all matrices, it is a highly non-convex optimization problem since the optimism function includes n-ranks. Without converts there is no efficient solution to this optimization problem [23]. In this paper, our goal is finding a low-n-rank tensorℒso that||(L(i))Ω−(M(i))Ω||F2(i=1 to N ) is minimized. In fact, any matrixS∈ℝm×nhaving a rank up to D can be expressed as a matrix multiplicationS=XYwhereX∈ℝm×DandS∈ℝD×n. In order to solve the function Eq. (10), additional auxiliary elenmentsZ(i),X(i)andY(i)will be introduced, whileZ(i)=X(i)Y(i). To simplify the problem, we will minimize a F-norm instead of directly minimize the rank of the mode-i unfoldings. Thus, Eq. (10) can be converted into the following form:(11)∑i=1Nλi(minX(i),Y(i),L(i)12||Z(i)−L(i)||F2.),s.t.Z(i)=X(i)Y(i)(L(i))Ω=(M(i))Ω.Instead of directly solving Eq. (11), we can solve the following problem:(12)ℋ(X(i),Y(i),L(i))=minX(i),Y(i),L(i)12||X(i)Y(i)−L(i)||F2,s.t.(L(i))Ω=(M(i))Ωfori=1,2…N,respectively.The functionℋ(X(i),Y(i),L(i))is differentiable and the gradient of the functionℋ(X(i),Y(i),L(i))is shown as follows:(13)gradℋ(X(i),Y(i),L(i))=(∂ℱX(i),∂ℱY(i),∂ℱL(i))=((X(i)Y(i)−L(i))Y(i)T,X(i)T(X(i)Y(i)−L(i)),X(i)Y(i)−L(i)),s.t.(L(i))Ω=(M(i))Ωfori=1,2…N,respectively.Let(∂ℱ/X(i))=0,(∂ℱ/Y(i))=0,(∂ℱ/L(i))=0, obtaining the optimalX(i),Y(i),L(i):(14)X(i)←L(i)Y(i)T(Y(i)Y(i)T)−1,Y(i)←(X(i)X(i)T)−1X(i)TL(i),L(i)←L(i)Y(i)T(Y(i)Y(i)T)−1(X(i)X(i)T)−1X(i)TL(i)+PΩ(M(i)−X(i)Y(i))fori=1,2…N,respectively.whereATis the transposed matrix ofA.B−1denotes the Moore–Penrose pseudo-inverse matrix ofBthat is a generalization of the inverse matrix. In linear algebra,BB−1B=B,B−1BB−1=B−1,(BB−1)T=BB−1and(B−1B)T=B−1B. In Eq. (14),Y(i)T(Y(i)Y(i)T)−1=Y(i)−1and(X(i)X(i)T)−1X(i)T=X(i)−1. Thus, Eq. (14) can be formulated as follows:(15)X(i)←L(i)Y(i)−1←L(i)Y(i)T(Y(i)Y(i)T)−1,Y(i)←X(i)−1L(i)←(X(i)X(i)T)−1X(i)TL(i),L(i)←L(i)Y(i)−1X(i)−1L(i)+PΩ(M(i)−X(i)Y(i))fori=1,2…N,respectively.In [13], a sequence of lemmas have been derived. According to these lemma,orth(L(i)Y(i)T)←L(i)Y(i)T←L(i)Y(i)−1, whereorth(W)is an orthonormal basis for the range spaceR(W)ofW. Therefore, Eq. (15) can be converted into the following form:(16)X(i)←orth(L(i)Y(i)T)←L(i)Y(i)T←L(i)Y(i)−1←L(i)Y(i)T(Y(i)Y(i)T)−1,Y(i)←(orth(L(i)Y(i)T))TL(i)←X(i)−1L(i)←(X(i)X(i)T)−1X(i)TL(i),L(i)←orth(L(i)Y(i)T)(orth(L(i)Y(i)T))T+PΩ(M(i)−X(i)Y(i))fori=1,2…N,respectively.In order to optimize the algorithm, the nonlinear Gauss–Seidal method [18] can be competent. The optimized process starts with initializations. The core idea of this strategy is to optimize a group of variables while fixing the other groups. The variables in the optimization areX(i),Y(i)andL(i), which can be divided into three groups. To achieve the optimal solution, the method estimatesX(i),Y(i)andL(i)sequentially, followed by certain refinement in each iteration. The underlying optimization can be implemented using the initialization. The final solution is deduced by utilizing the result of each mode with the weighted parametersλiin Eq. (11) given by the following:(17)ℒ=∑i=1Nλiℒ(i)/∑i=1NλiThe pseudo-code of the TC-MLFM algorithm is given inTable 1 below.

@&#CONCLUSIONS@&#
