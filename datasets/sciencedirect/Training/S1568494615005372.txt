@&#MAIN-TITLE@&#
Recursive subspace system identification for parametric fault detection in nonlinear systems

@&#HIGHLIGHTS@&#
Eigenstructure based fault detection for nonlinear systems.Recursive subspace based system identification techniques.Two linear models updated in parallel.Local eigenvalues residuals as symptoms.

@&#KEYPHRASES@&#
Model-based fault detection,Parametric fault,Subspace system identification,Recursive estimation,Feedforward neural network,

@&#ABSTRACT@&#
This work addresses the problem of detecting parametric faults in nonlinear dynamic systems by extending an eigenstructure based technique to a nonlinear context. Two local state-space models are updated online based on a recursive subspace system identification technique. One of the models relies on input–output real-time data collected from the plant, while the other is updated using data generated by a neural network predictor, describing the nonlinear plant behaviour in fault-free conditions. Parametric faults symptoms are generated based on eigenvalues residuals associated with two linear state-space model approximators. The feasibility and effectiveness of the proposed framework are demonstrated through two case studies.

@&#INTRODUCTION@&#
The increasing complexity and integration of industrial processes, some of them highly critical, has made it imperative to provide supervision systems with dedicated tools that could isolate and accommodate malfunctions or, generically faults, whenever needed. Furthermore, fault detection and isolation is a fundamental prerequisite for implementing conditioning-based maintenance procedures, in which the regular and systematic inspection of systems parts are replaced by analysing particular signals, along with decision-making actions that are performed on the basis of extracted features from data, either in real-time or offline.Fault detection and isolation (FDI), as a whole, consists in making binary decisions concerning a given malfunctioning hypothesis and to determine its nature and location (see e.g. [14,3]). In general, FDI techniques rely on hardware-based schemes or on analytical redundancy approaches, or even on a combination of both. The former methodology is essentially based on comparing identical readings, collected with additional hardware, while the analytical or software based approaches make use of mathematical models and dedicated estimation methods (see e.g. [13,45]), in order to detect fault events. As this approach, commonly, does not require any additional hardware, its implementation is more attractive and cost-efficient. Nevertheless, analytical FDI methods are undoubtedly more challenging, as they need to cope with model uncertainties, unknown/unmeasurable disturbances and outliers, which may globally bias fault symptoms, and thus compromising the underlying sensitivity and specificity.Among analytical approaches (see e.g. [21]), model-based fault detection and isolation techniques commonly resort to a set of residuals between a plant's readings and the outputs of a given predictor. By taking into account the residuals’ magnitude and, possibly, other features, a classifier triggers symptoms regarding the presence or absence of faults. The residuals generation can be implemented based on state and output observers (see e.g. [44,31], parity relations (see e.g. [42,10,29]), or on parameters estimation using system identification techniques (see e.g. [43,7]). Concerning system identification based techniques, a model of the plant under normal operating conditions, assuming no faults, is derived either online or offline, and by detecting relevant changes in the model parameters (see e.g. [35]), the presence of faults are then isolated.In a number of FDI problems the presence of parametric faults can be detected from changes in the eigenstructure of a linear state-space model describing the system dynamics. This model can be obtained from input–output data collected from the system and using, for instance, subspace-based linear system identification techniques (see e.g. [41]). In this case, as the derived model is described in state-space form, the eigenvalues associated with a given parameterization are immediately retrieved from the system matrix. Although this framework has been successfully applied in a number of case studies (see e.g. [2,1,8]), when the linearity assumption does not hold, the approach is doomed to fail, owing to the unreliability of residuals generation, which is partly related to model-plant mismatch. Hence, resorting to eigenvalues-based algorithms for FDI in a nonlinear context requires a completely different problem conceptualization and formulation.The motivation and main contribution of this work is to extend a linear fault detection methodology relying on the system eigenstructure to nonlinear systems, for which faults are modelled as changes in the internal system dynamics. The approach makes use of a recursive subspace based system identification technique, along with the approximation capabilities of Nonlinear Autoregressive with Exogenous Inputs (NARX) neural networks, while assuming the input–output certainty equivalence principle. In this framework, two linear models are recursively updated in parallel. One of the models resorts to input–output data collected from the plant, while the other relies on input–output data provided by a NARX predictor. The corresponding eigenvalues are used to generate a set of residuals, from which symptoms of possible faults are triggered by a decision system.The remainder of this paper is organized as follows. In Section 2 subspace-based methods for state-space system identification are discussed and two formulations presented, namely one where the parameters are estimated offline and the other based on a recursive implementation. Further, the model-based approach to fault detection is also discussed in terms of residuals and symptoms generation. Section 3 is devoted to presenting and describing the proposed approach for parametric fault detection in nonlinear systems, while Section 4 discusses some results obtained from two case studies. Finally, concluding remarks are drawn in Section 5.System identification deals with the problem of deriving an empirical model for a dynamical system based on input–output data. In the context of FDI, a model of the plant in normal or nominal operating conditions is first obtained. When a fault occurs, the underlying system behaviour in terms of outputs, inputs or internal dynamics, will differ from that predicted by the nominal model. As such, any fault event will lead to a change in the parameterization values.The system identification problem aims at finding a relationship g(·) between past instantiations (uk−1, yk−1) and current outputs y(k) (1), by appealing to a given regression technique, and taking into account an ordered data set sampled from the plant.(1)y(k)=g(uk−1,yk−1)+ϑ(k)where ϑ is an additive noise term and(2){uk−1}≜u(k−1)⋯u(k−α)T{yk−1}≜y(k−1)⋯y(k−β)Twithα,β∈ℕ+.Among possible model structures g(·) to approximate the input–output behaviour of a plant, the present work considers the linear state-space models and a NARX neural network, with the choice of these structures dictated by the nature of the proposed FDI framework. The linear state-space model-based online identification relies on a recursive subspace technique, whereas the NARX neural network predictor training is carried out offline, using an iterative optimization algorithm.A general feature of all Subspace System Identification (SID) methods is that they do not require a priori model parameterization, namely the model order, as its estimation is internally performed by the algorithm. Furthermore, the estimate of the underlying matrices relies on algebraic techniques, which makes them a very robust approach and less time consuming, compared to other methodologies, such as Prediction Error methods. Nevertheless, these methods can only provide suboptimal solutions (see e.g. [9,19]), which may ultimately impact on the approximation order and prediction performance.Assume the linear time-invariant system described in state-space form as follows:(3)x(k+1)=Ax(k)+Bu(k)+ω(k)y(k)=Cx(k)+Du(k)+υ(k)wherex∈ℝn,y∈ℝl,u∈ℝm,A∈ℝn×n,B∈ℝn×m,C∈ℝl×nandD∈ℝl×m, whileυ∈ℝlandω∈ℝnare unobserved Gaussian distributed, zero mean, white noise sequences, accounting for the measurement noise and process noise, with covariances defined according to:(4)Eω(p)υ(p)ωT(q)υT(q)=QSSTRδpqwith E denoting the expected value operator and δpqthe Kronecker index. Moreover, suppose the available data collected from the plant are ergodic, the number of samples is sufficiently large (N→∞), and Eq. (3) satisfies the following orthogonality property:(5)Ex(k)u(k)ωT(k)υT(k)=0Consider a data-set comprising an ordered sequence of input–output data collected from a plant, namely,(6)UN={u(0),u(1),…,u(N−1)}YN={y(1),y(2),…,y(N)}To come up with estimates for the state-space matrices (A, B, C, D) (up to within a similarity transformation) and error covariance matrices (Q, R, S), the data set ZN={UN, YN} is organized under the form of past and the future block Hankel matrices. For the input sequence UN, the underlying block Hankel matrices take the following form:(7)Up=u(0)u(1)⋯u(j−1)u(1)u(2)⋯u(j)⋮⋮⋱⋮u(i−1)u(i)⋯u(i+j−2)(8)Uf=u(i)u(i+1)⋯u(i+j−1)u(i+1)u(i+2)⋯u(i+j)⋮⋮⋱⋮u(2i−1)u(2i)⋯u(2i+j−2)The past and future output block Hankel matrices, Yp≡Y0|i−1 and Yf≡Yi|2i−1 are given according to,(9)Yp=y(0)y(1)⋯y(j−1)y(1)y(2)⋯y(j)⋮⋮⋱⋮y(i−1)y(i)⋯y(i+j−2)(10)Yf=y(i)y(i+1)⋯y(i+j−1)y(i+1)y(i+2)⋯y(i+j)⋮⋮⋱⋮y(2i−1)y(2i)⋯y(2i+j−2)Concerning the block Hankel matrices associated with the stochastic subsystem, they are built in with the outputs ys(k), the process noise ω(k) and the measurement noise υ(k), which are generically defined as above, namely (Yps,Yfs), (Mps,Mfs) and (Nps,Nfs), respectively.The past and future state vector sequences Xpand Xf, are as follows:(11)Xp=(x(0),x(1),…,x(j−1))Xf=(x(i),x(i+1),…,x(i+j−1))while the Toeplitz matrices associated with the deterministic and the stochastic subsystems are given by:(12)Hid=D00⋯0CBD0⋯0CABCBD⋯0⋮⋮⋮⋱⋮CAi−2BCAi−3BCAi−4B⋯D(13)His=000⋯0C00⋯0CACB0⋯0⋮⋮⋮⋱⋮CAi−2CAi−3CAi−4⋯0and the extended observability matrix associated with the deterministic system is given as:(14)Γi=CCACA2⋮CAi−1The following input–output matrix equations play a fundamental role in subspace identification (see e.g. [32]):(15)Yp=ΓiXpd+HidUp+YpsYf=ΓiXfd+HidUf+YfsYps=ΓiXps+HisMp+NpYfs=ΓiXfs+HisMf+NfSubspace system identification methods consist of two main steps. The first stage deals with the computation of a projection matrix Πi≜HA/HBof the row space of a block Hankel matrix HA, into the row space of a block Hankel matrix HB, from which the extended observability matrix Γiand/or an estimate of the state vector sequenceXˆfcan be retrieved. The second step concerns the computation of the state-space matrices (A, B, C, D), as well as the error covariance matrices (Q, R, S). For further details the reader is referred to [33,23,17,32].Recursive identification deals with the problem of updating the model parameters over time. The first known recursive subspace based identification formulation relied on the adaptation of the singular value decomposition (SVD) [39]. One drawback of this approach concerns the restriction to spatially and temporally white noise disturbances, otherwise the estimates will be biased. In order to deal with this intrinsic constraint, two Multiple Input–Multiple Output Error State-space Subspace Method (MOESPs) based on instrumental variables have been proposed, namely the Past Input MOESP (PI-MOESP) and the Past Output MOESP (PO-MOESP). The latter is particularly attractive, owing to the fact that the underlying instrumental variables include both past inputs and past outputs. Moreover, this formulation allows taking other dynamics into account, such as those associated with the output noise. Taking into account the above considerations, relies in this work on the PO-MOESP method.As for updating the extended observability matrix, the so called propagator method [27] is used in the present work. The main advantages of this method concern the use of a linear operator and a quadratic cost function, which allows its implementation in terms of a recursive least squares (see [24]).The recursive algorithm can be summarized as follows: (i) online updating of the observation vector, by using a QR factorization, along with Givens rotations (see e.g. [30,22]); (ii) recursive estimation of the extended observability matrix, considering the online updating of the propagator; (iii) estimation of the state-space matrices (A, B, C, D).(i)QR factorization updatingConsider the following decomposition:(16)UpΨYp=R1100R21R220R31R32R33Q1Q2Q3where Rijand Qiare block matrices associated with the QR factorization; Ψ is an instrumental variable comprising past inputs and outputs, such thatlimj→∞1jΘiΨT=0,Θi=HisMf+Nfand rank(XΨT)=n. In such conditions, the following expression holds [40]:(17)limj→∞1jR32Q2=limj→∞1jΓiXThe procedure for updating the QR factorization with a new data pair {u(τ), y(τ)} is as follows [20]:(18)ζR1100R21R220R31R32R33ui(τ+1)ψ(τ+1)yi(τ+1)Q1(τ)0Q2(τ)001withζ∈ℝ+a forgetting factor to weigh past information.By applying two sequences of Givens rotations in Eq. (18), the factor R associated with the QR factorization is converted into the following block lower triangular matrix:(19)ζR1100R21R220R31R32R33ui(τ+1)ψ(τ+1)yi(τ+1)rotG1(τ+1)·rotG2(τ+1)=R11(τ+1)000R21(τ+1)ζR22(τ)0ψˇ(τ+1)R31(τ+1)ζR32(τ)ζR33(τ)zˇ(τ+1)·rotG2(τ+1)=R11(τ+1)000R21(τ+1)R22(τ+1)00R31(τ+1)R32(τ+1)ζR33(τ)ziˇˇ(τ+1)withψˇandzˇvectors obtained after applying the first Givens rotation, and taking into account the information included in ui, whilezˇˇis the vector obtained after the second Givens rotation, in order to include the information embedded inψˇ.By considering Eq. (17), it follows,(20)E[ziˇzˇiT−ziˇˇzˇˇiT]=ΓiRxΓiTEq. (20) shows that (19) leads asymptotically to a given covariance matrixRzi, from which the subspace spanned by the columns of the extended observability matrix can be consistently retrieved. The procedure for updating the covariance matrix can be summarized through the following expression:(21)R˜zi(k)=ζRˆzi(k−1)+zˇi(k)zˇiT(k)−zˇˇi(k)zˇˇiT(k)(ii)Extended observability subspace basis updatingThe updating of a given basis for the extended observability matrix is based on the so-called propagator method [27]. This method has the advantage of coping with unknown coloured disturbances.Assume the pair (A, C) is observable and the order of system n to be known. Then, there is a permutation matrixS∈ℝli×li, such that the extended observability matrix Γican be decomposed into two blocks, namelyΓi1∈ℝn×nandΓi2∈ℝ(li−n)×n.(22)SΓi=Γi1Γi2Taking into account the propagator operator, Pi, it follows thatΓi2=PiTΓi1, and Eq. (22) can be rewritten as,(23)SΓi=InPiTΓf1Now, by replacing (23) in (20), it follows that,(24)Rzi=InPiTRx¯InPiwhich can be rewritten as,(25)Rzi=Rzˇi1−Rzˇˇi1Rzˇi1zˇi2−Rzˇˇi1zˇˇi2Rzˇi2zˇi1−Rzˇˇi2zˇˇi1Rzˇi2−Rzˇˇi2=Rx¯Rx¯PiPiTRx¯PiTRx¯PiEq. (25) shows that the propagator can be found by minimizing the following Frobenius norm (see [24] and references therein):(26)J(Pi)=∥Rˆzˇi2zˇi1−Rˆzˇˇi2zˇˇi1−PiT(Rˆzˇi1−Rˆzˇˇi1)∥F2If all the matrices in (26) are non-singular, the argument of minimizing Eq. (26) is given by [25]:(27)PˆiT=(Rˆzˇi2zˇi1−Rˆzˇˇi2zˇˇi1)(Rˆzˇi1−Rˆzˇˇi1)−1This optimal solution can then be recursively updated by making use of the recursive least squares (RLS) algorithm.(iii)Estimation of state-space matricesConsidering that the dimension of the underlying vector basis is known, an estimate for the state-space matrices is readily obtained from the extended observability matrix, and taking into account data-driven block Hankle matrices. These estimates for matrices A and C can be obtained according to:(28)Cˆ=Γˆi(1:l,:)Aˆ=[Γˆi(1:l(i−1),:)]†Γˆi(l+1:li,:)withΓˆi=InPˆiT.With respect to the estimates for matrices B and D, they are found by means of a least squares estimator applied toF(Bˆ,Dˆ), defined as [34]:(29)F(Bˆ,Dˆ)=BˆΓˆi−1†Hi−1dDˆ0−AˆCˆΓˆi†Hidnamely,(30)(Bˆ|Dˆ)=argmin(Bˆ|Dˆ)J(F(Bˆ,Dˆ))It should be mentioned that the assumption of a fixed model order can be regarded as a drawback, not only from a numerical point of view [24], but also in situations in which the system experiences a major structural change.Algorithm 1 summarizes, under the form of a pseudo-code, the methodology used in the implementation of the recursive subspace system identification.Algorithm 1Recursive subspace identificationRequire:u(τ+1), y(τ+1)Ensure:Aˆ,Bˆ,Cˆ,Dˆwhile true doUpdate the QR factorization (Eq. (18))Apply two sequences of Givens rotations (Eq. (19))Extract the vectorszi1andzi2Reorganize the observation vector(zi1Tzi2T)T, { rank = n }Obtain the propagator PiEstimate the extended observability matrixΓˆiEstimate(Aˆ|Cˆ)Estimate(Bˆ|Dˆ)end whileMultilayer neural networks comprising one hidden layer and including sigmoidal activation functions are known to be universal approximators (see e.g. [11,15]). An important class of multilayer perceptrons is represented by an input vector comprising past inputs and past outputs, the so-called regressor, and commonly dubbed as NARX neural networks. This neural network structure (Fig. 1) can be generically described by:(31)ynet(k)=g(y(k−1),…,y(k−ℓy),u(k−1),…,u(k−ℓu))with u(k) and y(k) denoting the system's input and output, ℓuand ℓythe lag windows associated with past inputs and outputs, and g(·) a nonlinear mapping. This input–output relationship can be rewritten as,(32)ynet(k)=g(φ(k),θ)with θ the neural network parameters vector, consisting of weights and biases, and the regressor φ(k) given as:(33)φT(k)=(y(k−1),…,y(k−ℓy),u(k−1),…,u(k−ℓu))The activation functions associated with the output layer neurons were chosen to be linear, while for the neurons included in the hidden layer, the corresponding activation functions σ(·) are nonlinear continuous and differentiable sigmoidal functions, upper and lower bounded (see e.g. [4,12]), namely:•limt→±∞σ(t)=±1;σ(t)=0⇐t=0;σ′(t)>0;limt→±∞σ′(t)=0;max(σ′(t))≤1⇐t=0.The number of neuron to be included in the output layer corresponds exactly to the number of outputs of the underlying system. For the hidden layer and input layer, the number of neurons should be selected taking into account the required approximation order, constrained to a minimal structural complexity, in order to guarantee an adequate generalization capability. Several approaches can be found in literature to deal with this somehow irreconcilable dilemma, including pruning techniques (see e.g. [6,5,16]), or the computation of the model order so as to maximize the generalization capability of a neural network topology. Teoh et al. [36] proposed a method based on the degree of linear independence of patterns in the hidden space, along with pruning/growing techniques, while in [38] the authors propose explicit analytical formulas, as a function of the required approximation order.Finally, concerning the selection of the number of past samples to be included in the regressor, to the best of the authors’ knowledge, there is no effective and generic framework to deal with the choice of ℓyand ℓu. Still, specific methodologies borrowed from linear system identification can be used to come up with estimates, but they tend to overestimate the dimension of the input vector, which results in a higher structural complexity than required for a given neural network. A different approach takes advantage of evolutionary computation to come up with an optimal dimension and composition of the input vector.This section presents the proposed model-based fault detection architecture, and the rationale behind its conceptualization is discussed in terms of structure and operation.The rationale behind the proposed framework for detecting parametric faults in nonlinear systems is propped up on the input–output certainty equivalence principle. Such an assumption implies that the nonlinear function g(·) emulating the nonlinear plant dynamics has approximation order Nα→∞.Definition 1Approximation orderA functionf∈CNα(K→ℝ)approximates a functiong∈CNα(K→ℝ)at x=0, with orderNα∈ℕif and only if f(0)=g(0), f′(0)=g′(0), f′(0)=g′(0),…, f(Nα)(0)=g(Nα)(0), whereCNα(K→ℝ)denotes the Nα-times continuously differentiable from the set K to the setℝ.The concept of approximation order is intrinsically linked to the Taylor polynomial expansion (34).(34)TNα{ξ}(x)=∑τ=0Nαξ(τ)(0)τ!x(τ)withTNα{ξ}the Taylor polynomial expansion of function ξ(x).Definition 2Taylor approximationA sufficiently smooth function f approximates another smooth function g, with orderNα∈ℕif and only if,(35)TNα{f}=TNα{g}The Taylor polynomialTNα{f}(x)is essentially a local approximation of f(x), provided the function is sufficiently smooth.Definition 3AnalyticityA functionf∈CNα(K→ℝ)is analytical (in zero) if and only if there exists δ>0 such that,(36)T∞{f}(x)=f(x),∀x∈Kwith∥x∥<δA functionf∈CNα(K→ℝ)is nicely analytical if and only if(37)T∞{f}(x)=f(x),∀x∈Kand(38)limτ→∞sup∥f(τ)(0)∥τ!1τ<1Proposition 1Global approximation (see [38])If f and g are nicely analytical functions, then there will be for every ς>0 a Nςsuch:(39)TNς{f}=TNς{g}⇒supx∈K|f(x)−g(x)|<ςProposition 1 states that every improved local approximation leads to a better global approximation.Consider a nonlinear nicely analytical functiong∈CNα(K→ℝ)to be approximated by a Nςth order Taylor polynomial expansion, with Nα>Nς, in the sequel. The remainder of the corresponding Taylor expansion,RNς(x), after the Nςth term, can be computed through the Lagrange formula,(40)RNς(x)=g(Nς+1)(c)(Nς+1)!xNς+1with C between 0 and x.Lemma 1Taylor polynomial remainderLetg∈CN∞(K→ℝ)be a nicely analytical function and consider two arbitrary Taylor polynomial approximations with degrees Nαand Nς, with Nα<Nς<∞, then(41)|RNα(x)|>|RNς(x)|>|RN∞(x)|According to Lemma 1, the higher the Taylor polynomial expansion degree the lower the approximation error. Therefore, if a function g approximates a nonlinear system dynamics with order Nα, then the corresponding sum of squared error depends on the approximation order, and will drop by increasing Nα.Theorem 1Consider a nonlinear dynamic system described by a nicely analytical functionf∈CN∞(K→ℝ)to be approximated by a nicely analytical functiong∈CN∞(K→ℝ). Then,(42)limNα→∞RNα(x)=0⇒f(x)=g(x)If the input–output certainty equivalence principle holds, then strictly indistinguishable linear state-space approximations, Σ=Σ(A, B, C, D), to the plant can be obtained through a subspace system identification technique, by irrespectively taking into account a data set either collected from the plant or generated with a supposedly “perfect” nonlinear model, provided the initial conditions and the input sequence are identically the same.Theorem 2Consider a nonlinear dynamic system approximated by a nicely nonlinear analytical functiong∈CN∞(K→ℝ)with approximation order N∞, from which two data sets Zpand Zmare obtained by applying identical input sequencesu(k)∈ℝq, and considering equal initial conditions. Assuming Σp=(Ap, Bp, Cp, Dp) and Σm=(Am, Bm, Cm, Dm) are two linear state-space realizations whose parameters are recursively updated over time, then for every k>0 the following holds true:(43)Ap(k)=Am(k)Bp(k)=Bm(k)Cp(k)=Cm(k)Dp(k)=Dm(k)The proof of Theorem 2 can be carried out by appealing to induction and reductio ad absurdum.Corollary 1If for a given k>τ>0 the eigenvalues of Apdiffer from the eigenvalues of Amthen this implies that the nonlinear plant internal dynamics have changed.Corollary 1 expresses the rationale behind the proposed approach to detect parametric faults in nonlinear dynamic systems, based on the eigenvalues residuals.The framework for detecting parametric faults in nonlinear dynamic systems relies on comparing the eigenstructure of two linear state-space realizations, one as an approximation to the nonlinear plant, and the other to the NARX neural network predictor. The neural network is trained off-line using a sufficient informative data set collected from the nonlinear plant, in a fault-free context.The architecture (see Fig. 2) consists of two linear state-space models, one aiming at locally approximating the plant dynamics and the other nonlinear model of the plant, represented by a neural network predictor. Both local approximations are recursively updated by means of subspace-based technique (see Algorithm 1). Taking into account the underlying system matrices, namely Apand Am, two sets of eigenvalues in descending order of magnitude are respectively obtained, λ(Ap) and λ(Am), and compared in the residual generator, according to (44).(44)Δλ(k)=|λp(k)−λm(k)|with |·| the element-wise absolute value operator, λp=λ(Ap) and λm=λ(Am).Taking into account the residual vector, Δλ(k), the Decision Making module categorizes non-zero residuals as resulting from parametric faults. Yet, this discriminant Δλ(k)>0, ∀k>0 is only applicable for the conditions of Theorem 2, specifically for zero model-plant mismatch conditions.In practice, however, no matter how accurate the model of the plant is (Nα<N∞), the above discriminant is never null, even in the absence of faults. This is related to a cumulative effect of noise and outliers, and also to the sub-optimality associated with subspace based algorithms.To improve the specificity of the fault detection methodology, this discriminant should be relaxed, by considering a threshold for the magnitude of Δλ, namelyΔλ(k)>δ,∀k>0,δ∈ℝ+. In case of a residual exceeds the threshold δ, the classifier will accordingly tag the symptom as a fault.One way for estimating the threshold δ associated with a given monitoring variable is by considering the univariate statistical approach to limit sensing, which is typically implemented through a Shewhart chart (see e.g. [37]). In this methodology, the upper and lower control bounds on the Shewhart chart have a major impact on the sensitivity and specificity of the framework, and they depend on the multiple of the standard deviation, which is user defined. Finally, it should be emphasized that the fault accommodation problem is not addressed in this work.The proposed framework for detecting parametric faults in nonlinear dynamic systems is evaluated through simulations using a nonlinear benchmark system model, specifically a Continuous Stirred Tank Reactor (CSTR), and through experiments carried out on a test-bed consisting of an air-stream heating system.The CSTR benchmark system comprises a constant volume reactor cooled by a co-current single coolant stream, as shown in Fig. 3, where an irreversible exothermic reaction in a liquid medium takes place within the reservoir (see e.g. [28]). The reactor's main purpose is to deliver the concentration of the outlet effluent CAat a prescribed value, by manipulating the coolant flow rate qccirculating in the reactor's jacket. The choice for this system was motivated by its modelling simplicity, and by the fact that it exhibits a nonlinear behaviour characterized by multiple steady states.The process can be described by the following differential equations:(45)dCAdt=qAV(CA,i(t)−CA(t))−γ0CA(t)exp−ER·TA(t)dTAdt=qAV(TA,i(t)−TA(t))+γ1CA(t)exp−ER·TA(t)+γ2qc(t)1−exp−γ3qc(t)(Tc,i(t)−TA(t))where CAand TAdenote the concentration and temperature in the tank, assuming that the reactor is perfectly mixed, and qcthe coolant flow rate. The remaining parameters of the system borrowed from [18] are presented in Table 1.Taking into account the nominal values for the CSTR shown in Table 1, the operating region is constrained to:(46)0<CA<1.00mol/lTA>350.00K0≤qc≤qc,maxl/minAs for the steady state operating regime (R|0), this work considers:(47)CA★=0.10mol/lTA★=438.54Kqc★=103.41l/minTaking into account (45), the linear approximation aroundR|0in the sate-space form is described by:(48)C˜˙A(t)T˜˙A(t)=−9.998−4.679×10−21.799×1037.325C˜A(t)T˜A(t)+0−8.775×10−1q˜c(t)y(t)=10C˜A(t)T˜A(t)withC˜A=CA−CA★,T˜A=TA−TA★,q˜c=qc−qc★and y the measured variable.In order to assess the performance of the parametric fault detection approach, in a context where the system is assumed to be linear, the eigenvalues (λ|0) of (48) are provided to the residual generator module, while changing the system's dynamics by injecting parametric faults. In this scenario the residual generator is implemented element-wise, according to the following expression:(49)Δλ(k)=|λ|0(k)−λ|L(k)|where λ|L(k) is obtained from a local linear state-space realization, at each discrete time k.In what follows, the CSTR is assumed at nominal regime,R|0, and at some point of the simulation a parametric fault is injected on the system. Several different faults, represented by a perturbation on an internal dynamics parameter η, namelyη˜=α·η,α∈ℝ+, were considered. The perturbed parameters, as shown in Table 2, include the activation energy (E/R), the pre-exponential factor (γ0), the process flow rate (qA) and the inlet coolant temperature (Tc,i).Figs. 4–7show the CSTR time response, along with the underlying eigenvalues residuals, for each fault. All the tested faults were injected on the system at time 30min, while considering a sampling time of 0.25s. Analysing these figures, it becomes clear that a change in the system dynamics induced by parametric faults is reflected on the eigenvalues residuals (Δλ). Since the eigenvalues are complex numbers, the corresponding residuals are coincident. Based on these residuals, the Decision Making module, consistently, signals the presence of a parametric fault, by generating a symptom for each Δλ>0. Moreover, it should be mentioned that the detection delay is particularly short and the sensitivity of the method is strong.To evaluate the impact of the linear dynamics assumption on the framework specificity, the current operating point is changed at instant 30min, by means of a step change on the coolant flow rate (qc) from 103.41l/min to 108.0l/min, and considering no fault acting on the system. The results are shown in Fig. 8. Although the simulations consider fault-free conditions, the computed local eigenvalues changed owing to the system's nonlinearity, which results in non-zero residuals. As such, the Decision Making module ends up miss-classifying the underlying event, tagging it as resulting from a parametric fault. This simulation illustrates the impact of the linear system assumption on the specificity of the underlying framework.The test-bed consists of a heating system, namely the Feedback® Process Trainer PCT 37-100 (Fig. 9). It comprises a variable-speed axial fan adjusted manually via a potentiometer, which circulates an air stream along a polypropylene tube. The airflow rate is heated while passing across a heating element, under the form of a grid, with a maximum power of 80W, for an input voltage of +10V, and controlled by means of a thyristor circuit. The system includes a thermistor detector for sensing the temperature at one of the available measurement points along the tube.The process represents a nonlinear time-varying system, where the main source of nonlinearities is associated with an input dependent static gain (see Fig. 10), whereas the time varying behaviour is mostly related to thermal energy storage in the physical parts of the system, particularly in the air conduit.The communication infrastructure between the computer, where the fault detection platform is located and the process trainer, concerns a peer-to-peer communication link, based on a Measurement Computing® PMD-1208LS series data acquisition board.11See http://www.mccdaq.com.The parametric faults under detection are represented by changes on the internal system dynamics, which are induced by changing the air-stream flow rate. This kind of faults leads to an alteration in the plant pure delay and static gain.

@&#CONCLUSIONS@&#
This paper addressed the problem of parametric faults detection in nonlinear dynamic systems. The proposed framework relies on system identification techniques to generate symptoms from residuals computed using the eigenvalues of two local linear state-space models. These models are updated online based on a recursive subspace state-space algorithm. One of the state-space models is updated using real-time readings from the plant, while the other is adjusted taking into account input–output data generated by a neural network predictor, emulating the nonlinear plant dynamics under fault-free conditions.Two case studies were considered in assessing the feasibility and performance of the proposed approach. One consisted on a Continuous Stirred Tank Reactor benchmark system, described by a nonlinear model, which was considered for testing the linearity hypothesis in detecting parametric faults modelled by a change in the nonlinear system dynamics. The second case study included a real air-stream heating system, in which faults, under the form of fan speed disturbances, resulted in a shift in the air-stream flow rate, which induces changes on the system's delay and static gain. Results from simulations and experiments not only demonstrated the feasibility of the proposed approach for detecting parametric faults in nonlinear dynamic systems, but also made its out-performance clear in terms of sensitivity and specificity with regard to the linear based assumption counterpart.