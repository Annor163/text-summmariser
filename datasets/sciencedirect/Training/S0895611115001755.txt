@&#MAIN-TITLE@&#
NSCLC tumor shrinkage prediction using quantitative image features

@&#HIGHLIGHTS@&#
Lung tumors shrink during radiotherapy, with much variation between patients.Pre-treatment CT images can be used to predict tumor shrinkage after treatment.Potential uses include identifying patients who will benefit from adaptive radiation therapy.

@&#KEYPHRASES@&#
Quantitative image feature,texture,tumor shrinkage,prediction,NSCLC,

@&#ABSTRACT@&#
The objective of this study was to develop a quantitative image feature model to predict non-small cell lung cancer (NSCLC) volume shrinkage from pre-treatment CT images. 64 stage II-IIIB NSCLC patients with similar treatments were all imaged using the same CT scanner and protocol. For each patient, the planning gross tumor volume (GTV) was deformed onto the week 6 treatment image, and tumor shrinkage was quantified as the deformed GTV volume divided by the planning GTV volume. Geometric, intensity histogram, absolute gradient image, co-occurrence matrix, and run-length matrix image features were extracted from each planning GTV. Prediction models were generated using principal component regression with simulated annealing subset selection. Performance was quantified using the mean squared error (MSE) between the predicted and observed tumor shrinkages. Permutation tests were used to validate the results. The optimal prediction model gave a strong correlation between the observed and predicted tumor shrinkages with r=0.81 and MSE=8.60×10−3. Compared to predictions based on the mean population shrinkage this resulted in a 2.92 fold reduction in MSE. In conclusion, this study indicated that quantitative image features extracted from existing pre-treatment CT images can successfully predict tumor shrinkage and provide additional information for clinical decisions regarding patient risk stratification, treatment, and prognosis.

@&#INTRODUCTION@&#
A goal of oncology therapies is to utilize information gained from treating previous patients to deliver treatment specific to the patient and disease. The current tumor node metastasis (TNM) staging system for non-small cell lung cancer (NSCLC) is used for this purpose and utilizes anatomical information such as the tumor size, location, spread, and lymph node involvement [1]. Although this system is based on the study of over 67,000 NSCLC cases, NSCLC patients with the same TNM staging often have very different clinical outcomes. To improve this, many models have been proposed and added additional non-anatomical features to the TNM staging system. For example, Poullis et al. were able to improve NSCLC staging by adding the readily available physiological features of age and body mass index [2]. Others that used single nucleotide polymorphism genotyping to predict survival were able to predict NSCLC recurrence using serum biomarkers [3–5].Quantitative CT imaging has also been used to assess patient outcomes. In the Response Evaluation Criteria in Solid Tumors (RECIST) guidelines, tumor response to therapy is gauged by one-dimensional measurements of tumor size [6,7]. However, using more complex quantitative image features such as the tumor heterogeneity and radiodensity can result in a significant prognostic improvement over RECIST [8]. In addition, quantitative image features derived from image textures have been prognostic in several applications [9,10]. In light of these findings, the new field of radiomics aims to achieve automated high-throughput extraction of these quantitative image features and others under the hypothesis that they are related to gene expression and phenotype. Recently, radiomics has been applied to NSCLC survival prediction. Kumar et al. [11] extracted 215 3D features for 32 patients who underwent two CT imaging sessions spaced 15min apart on the same machine. To assess intra-patient (test-retest) reproducibility, they calculated feature concordance correlation coefficients and dynamic ranges and determined that 33 of these 215 features were informative, reproducible, and non-redundant. Using this information, Basu [12] extracted these 33 features from a separate set of 95 NSCLC patients, binarized patient outcomes around 2 year survival, and tested various classifiers to obtain an area under the curve (AUC) of 0.68. Aerts et al. [13] used similar techniques on a larger data set (412 patients) to obtain an AUC of 0.70.These pioneering studies by Basu and Aerts clearly indicate that quantitative image features have prognostic potential for NSCLC survival. These initial successes have since been further developed by many different groups for both CT and PET images [14–16]. However, variations in their datasets, such as imaging parameters [17], and details of the cancer treatment used may be limiting their prognostic power. Similarly, outcome metrics like overall survival are affected by numerous factors. In the present study, we focused on reducing extraneous noise sources to increase prognostic power. First, all NSCLC patients in our study were of similar stage, received relative biological effectiveness (RBE) equivalent treatments, and were imaged with the same protocols on a single CT machine. Second, instead of making binary predictions of survival, we will make continuous predictions about tumor volume change (i.e. tumor shrinkage). There are two potential applications of this information. First, predictions of tumor shrinkage could help identify patients who could benefit from adaptive radiation therapy, where their radiation therapy is changed part-way through the course of treatment. A second potential application is that a mid-course comparison of the expected and actual tumor shrinkage may guide further adaptation of the treatment plan. For example, if the tumor is not responding as expected, the radiation dose may be increased. Finally, an advantage of this approach is that it removes uncertainty regarding patient follow-up after treatment and will better isolate the effect of the treatment itself.We obtained simulation and weekly free-breathing, non-contrast 4DCT images from 66 patients with locally advanced, pathologically proven stage II-IIIB NSCLC. As part of a separate prospective trial comparing treatment modalities, these patients were randomly assigned treatment either by IMRT (36 patients) or protons (30 patients). The protons were delivered with passively scattered proton therapy with an assumed RBE of 1.1. Both groups were treated to a 74Gy (RBE) dose level and had concurrent chemotherapy. All simulation images were acquired with the same GE Medical Systems LightSpeed 16 machine (GE Healthcare, Milwaukee, WI) with helical scans using kVp=120, mAs=450, and a standard reconstruction convolution kernel. Axial images were 512×512pixels with voxel dimensions of 0.98×0.98×2.5mm3.To minimize the effects of respiratory motion, physicians contoured the gross tumor volumes (GTVs) on the end-of-exhale (T50) phase of planning 4DCT images. These contours were propagated onto weekly images using an in-house demons-based deformable image registration algorithm [18]. Deformed contours were visually checked to ensure that they matched the new tumor shape. For each patient, the weekly GTV volume divided by the planning GTV volume was defined as the tumor response for a particular week. Although both treatment regimens lasted longer than six weeks, for logistical reasons some patients did not have 4DCT scans after week six. Therefore, week six was chosen as the “final” time point for prediction of the tumor response from the initial planning images. We believe this is a reasonable choice because most of the tumor shrinkage happens in the first few weeks of treatment, and the volume change is relatively small toward the end of the treatment. This is based on published head and neck data [19] and our own unpublished analysis of NSCLC patients.For modeling, both the IMRT and proton data sets were pooled in order to obtain a larger set of observations, and one patient from each group was thrown out for being a tumor response outlier (deviating by more than two standard deviations). Results from Chen et al. [20] support pooling the two data sets by showing that each group had remarkably similar tumor responses (see Table 1). This is expected since the GTVs of each group received RBE equivalent treatments. Although normal tissue dose distributions may vary between the two groups and may affect normal tissue complication probability, for the purpose of our study only the RBE dose to the GTV is relevant.We extracted quantitative image features from the pre-treatment planning GTVs using in-house software [21] that used the following 3D feature sources: geometry, intensity histogram, absolute gradient image, co-occurrence matrix (COM), and run-length matrix (RLM). See Table 2for a complete listing of features extracted from each feature source. For the absolute gradient image, each voxel is defined as the difference between paired adjacent voxels in all three directions added in quadrature. For a given image and direction, the run-length matrixρt,fis defined as the number of voxel runs in the image with intensity t and run length f ([22]). RLM features were extracted for 13 different 3D directions. For a given image and displacement vectorΔ→, a co-occurrence matrixci,jis defined as the probability that two voxels separated byΔ→will have the intensity values of i and j, respectively [23]. COM features were extracted for six different one-voxel displacements: left, right, superior, inferior, anterior, and posterior.Prior to feature extraction, we pre-processed the GTV regions of interest (ROIs) by pruning, i.e. removing voxels below a certain Hounsfield unit [HU] cut-off. This was done to reduce contouring variability, remove air cavities, and better define the solid tumor. However, the optimal HU cut-off was not known a priori, so each unprocessed ROI gave rise to several variants, each of which had different HU cut-offs. The final ROI may have holes in it (e.g. if the tumor is a donut shape. The geometry features (Table 2) of volume and area are calculated prior to pruning. The other geometry features with the prefix ‘prune’ refer to the volumes etc. after pruning using the HU cut off. All other image features are calculated on the pixels labeled as being contained in the ROI—i.e. not the holes.In addition, different bit depths can be assigned to the GTV voxels; however, the optimal bit depths were unknown. Thus, each of these variants then gave rise to additional variants for which the gradient, RLM, and COM image bit depths were varied. Therefore, for each patient's ROI, many additional ROIs with different pruning and bit depths were created. Fig. 1illustrates this process. For each unique HU cut-off and bit depth combination, the resulting ROIs were pooled from the 64 patients and features were extracted to create a feature matrix with each row corresponding to a patient and each column corresponding to a feature.We define a feature set as a collection of feature matrices where each feature matrix within the feature set was generated using a unique set of extraction parameters (e.g. HU cutoff and bit depths). To investigate the predictive power of various feature sources, we created four feature sets. Each of of the four feature sets used different feature sources and feature extraction parameters as indicated by Table 3.The fitness of prediction models was quantified using the mean squared error (MSE) between predictions and observations. The method for obtaining the MSE of a feature matrix f with column selection vectors→using leave-one-out cross-validation is described here and in Fig. 2. First, a row is omitted from a feature matrix and the remaining training observations are projected into z-score space. These are then projected onto principal component (PC) space. Dimensionality reduction is achieved by removing PC dimensions that have trivial components for all training observations. Next, multiple linear regression is performed using the PC space columns indicated by the Boolean vectors→. Tumor responses are used as the regression target. The resulting model is known as the principal component regression (PCR) model [24].To test the PCR model, the omitted row is converted into the same z-score PC space as the training observations and fed into the model in order to obtain a tumor response prediction for the hidden observation. The squared difference between the predicted and observed tumor response is recorded. This process is repeated for each row of the feature matrix f to obtain the leave-one-out cross-validation MSE. This MSE is a measure of the prediction success of the column selection vectors→and the feature extraction parameters associated with F. Since the volume change is a ratio of volumes (final/initial), it is unit-less, as is the MSE.The previous section described how to obtain MSE as a function of the feature matrix and the column selection vector (i.e.MSEF,S→). This section will describe how the simulated annealing principal component regression (SA-PCR) technique was used to find the f ands→which give the bestMSEF,S→. First, a feature set from Table 3 was selected. For each feature matrix F in this feature set, a simulated annealing process [25] is performed using a geometric cooling function with Tt=1×10−2 and Tf=1×10−4. The Boolean vectors→is randomly initialized and then allowed to evolve according to the simulated annealing process through N=1000 steps usingMSEF,S→as the objective function.Out of all of the feature matrices in the feature set, the F andS→that give the lowest MSE are taken to be the best F andS→Since each feature matrix is generated by a unique set of feature extraction parameters, the best F then implies the best feature extraction parameters (i.e. HU cut-off and bit depths). The bestS→indicates which dimensions of the feature z-score PC space are best for multiple linear regression modeling. Together the best F andS→define the best model.Because simulated annealing is stochastic, for the same input feature set the SA-PCR algorithm may return different best MSEs for each execution. Therefore, in order to test the stability of the results, we repeated the entire SA-PCR process 100 times for each feature set in Table 3.Even though the SA-PCR process implements leave-one-out cross-validation (above), because it considers many feature matrices and many feature z-score PC space selection vectors, there is a possibility that the best MSE and associated F andS→that it finds are due to coincidence or spurious correlations. To test for this possibility, we performed a negative control study. We again performed the entire SA-PCR process 100 times for each feature set in Table 3 as was done to study the stability of the SA-PCR results. However, in this case, for each run the tumor response vector was randomly permuted with respect to the associated feature matrix (independently for each run). This obliterated any prognostic relationship between the feature matrix and the tumor response vector.

@&#CONCLUSIONS@&#
