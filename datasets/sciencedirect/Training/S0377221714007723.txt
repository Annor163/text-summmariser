@&#MAIN-TITLE@&#
Multiobjective optimization: When objectives exhibit non-uniform latencies

@&#HIGHLIGHTS@&#
We combine two well-known methods into a coherent algorithm.We place a block instead of a single rectangle in each step.We divide the input sheet into successively smaller sheets at each step.We outperform all existing approaches on standard benchmark instances.

@&#KEYPHRASES@&#
Multiobjective optimization,Evolutionary computation,Delayed objective functions,Closed-loop optimization,Budgeted optimization,

@&#ABSTRACT@&#
Building on recent work by the authors, we consider the problem of performing multiobjective optimization when the objective functions of a problem have differing evaluation times (or latencies). This has general relevance to applications since objective functions do vary greatly in their latency, and there is no reason to expect equal latencies for the objectives in a single problem. To deal with this issue, we provide a general problem definition and suitable notation for describing algorithm schemes that can use different evaluation budgets for each objective. We propose three schemes for the bi-objective version of the problem, including methods that interleave the evaluations of different objectives. All of these can be instantiated with existing multiobjective evolutionary algorithms (MOEAs). In an empirical study we use an indicator-based evolutionary algorithm (IBEA) as the MOEA platform to study performance on several benchmark test functions. Our findings generally show that the default approach of going at the rate of the slow objective is not competitive with our more advanced ones (interleaving evaluations) for most scenarios.

@&#INTRODUCTION@&#
Multiobjective optimization problems require the simultaneous optimization of multiple (often conflicting) objectives over a given space of candidate solutions. These problems occur in many practical applications, rather often as bi-objective problems, with typical pairs of objectives as quality vs cost, strength vs weight, or accuracy vs complexity (e.g., of a model). A concrete example of a multiobjective problem is the design of a bridge11Which may indeed be made of concrete—no pun intended., such that it passes one or more strength tests, and such that its cost of construction is not too high.Although in tackling such problems, it is possible to treat one objective as a constraint, or to weight or prioritize objectives to form a scalar optimization problem, a more general approach to multiobjective optimization (and the one we follow here) is to postpone or avoid the assignment of weights or priorities, and instead to seek a representation of all the optimal trade-offs of the objectives (the Pareto optimal front) to allow posterior exploration of the optimal choices, and a final solution to be selected (see Fig. 1).The problem that we identify and tackle in this paper stems from the fact that, to date, almost all such methods for multiobjective optimization, including the many methods based on evolutionary algorithms, assume that each candidate solution is evaluated on all the objectives simultaneously. Thus, every candidate solution explored is associated with its vector of objective values, it can be plotted in the objective space (as in Fig. 1), and more importantly it can take part in relative assessments of its multiobjective fitness (or utility) so that fitness-biased selection (particularly) can be carried out. However, given that the objective functions to be evaluated could be of quite different character, this means these algorithms are somewhat overly-restricted, and could be inappropriate or inefficient for cases, under a time-budgeted optimization (see Jansen and Zarges, 2013), where the different objective functions vary in evaluation times (or latencies).Consider an extreme example. We wish to optimize the formulation of a washing powder, and our two objectives are washing excellence and cost. In this case, it is easy to imagine that assessing washing excellence may be a laborious process involving testing the powder, perhaps on different materials and at different temperatures. By contrast, the cost of the particular formulation can be computed very quickly by simply looking up the amounts and costs of constituent ingredients and performing the appropriate summation. (This scenario is rather typical of the whole area of closed-loop optimization, where optimization algorithms direct and obtain results from real physical experiments, as described elsewhere (Box, 1957; Caschera et al., 2010; Rechenberg, 2000; Small et al., 2011), but, equally, objective functions may all be computational but still have widely differing latency (or delay).)It is not obvious, we think, what a good strategy for the above scenario would be, given current multiobjective evolutionary algorithms (MOEAs) and the state of the field. Nor is it clear how much potential loss of performance comes about by using a basic MOEA going at the speed of the slowest objective (a default approach to the problem). These are the two main questions we seek to answer in the remainder of this paper. For simplicity, we focus mostly on the bi-objective case, although some of our definitions are general, and we include a discussion of extensions to m > 2 objectives in a later section.We define the problem more formally in the following. The notation we introduce here allows us to describe MOEAs that are suitable for evaluations that are not necessarily performed on all objectives at the same time. In the next section on algorithms we will present some such general schemes and indicate how they are instantiated on some basic, well-known MOEAs.Definition 2.1Multiobjective optimization problemThe general formulation of the problem is: ``maximize'' f(x) subject to x ∈ X, where x is an n-dimensional candidate solution vector, X is the search domain and f = (f1, …, fm) is a vector objective functionf:X→Rmmapping solutions to an m dimensional real-valued objective space. The term ‘maximize’ is written in quotes in order to indicate that there are not unique maxima to such a problem in general, and a further definition is needed to define an ordering on candidate solutions (see below).Consider two solutions x1 and x2. We say that x1 dominates x2, also written as x1≻x2, if and only if ∃i such that fi(x1) > fi(x2) and ∀j, fj(x1) ≥ fj(x2).The Pareto optimal front, also denoted PFtrue, is the set of points{f(x)|x∈X,¬∃y∈X,y≻x}.Any set of points in the objective space with elements that are all non-dominated within the set is called an approximation set. Such sets can be partially ordered according to the better relation (Zitzler et al., 2003), analogously to the dominance order on points. The aim of multiobjective optimization can be defined as finding the best possible approximation set, where best is determined by this order. As a proxy method for assessing approximation sets, we use the hypervolume and attainment surfaces, as recommended by Zitzler et al. (2008).Definition 2.5Total budgetThe total budget for solving an optimization problem is the total number of time steps B available for solving it, under the assumption that only solution evaluations consume any time.We assume parallelization of the evaluation of solutions is available, in two senses. First, a solution may (but need not) be evaluated on one objective in parallel to its being evaluated on another objective. Secondly, a number of (at most λ) solutions may be evaluated at the same time (i.e., as a batch or population) on any objective, provided their evaluation is started at the same time step, and finishes at the same timestep (i.e. batches cannot be interrupted, added to, etc., during evaluation). For sake of simplicity, we assume λ is the same for all objectives.Assume that each objective i can be evaluated inki∈Z+timesteps (for a whole batch). Here, we consider a bi-objective case, and for simplicity, we define k1 = 1 and k2 = kslow > 1, so that the slower objective is kslow times slower than the faster one.22In reality, the objectives of a problem may not have latencies that are exact multiples of each other, of course, and we will discuss this scenario in more detail in Section 6.From Definitions 2.5–2.7, it follows that the total budget of evaluations per objective is different. The budget for f1 is λB, whereas the budget for f2 is λ⌊B/kslow⌋.Note, these per-objective budgets are derived and are the best possible, assuming that solutions are always evaluated in parallel batches of size λ, and new batches are evaluated immediately after the one just finished with no timestep unused (both objectives). Assuming a standard MOEA applied to the problem, by contrast, one would obtain only λ⌊B/kslow⌋ evaluations on both objectives.In this section, we will describe a number of algorithmic schemes that are able to operate in the model of budgeted multiobjective optimization defined above. One approach to the problem, as posed, would be to run a standard MOEA at the speed of the fast objective but to use fitness approximation (or “inheritance”) whenever the slow objective function is “busy” with evaluating an earlier batch. Such an approach relies rather heavily on the fitness approximation scheme, and its performance will certainly depend on it closely. We briefly consider methods such as this one, which use approximation, in Section 5 (and these types of scheme were also the focus of our previous work; Allmendinger and Knowles, 2013a). Our main original contribution in this paper, in contrast, is to propose and analyze a number of schemes that do not use approximation for the slow objective. We define the schemes in general terms, then provide four concrete strategies (there are two variants of Scheme 3) based on them. Finally, we indicate how these can be instantiated on existing generational MOEAs (and also explain, later in Section 6, how the schemes can be applied within steady-state MOEAs).The basis of the proposed schemes is the observation that the slower objective is only evaluated every kslow time steps. Then three distinct approaches can be identified for using the faster objective(s)33Note that the schemes are defined for two objectives only, but could also work for the case where m > 2 with just one slower objective, and the set of faster ones all evaluating at the same speed. For m > 2 and all objectives having different latencies (or delays), we have not designed a scheme yet.:•Scheme I: To go at the rate of the slower objective (and go at the same rate on the faster objective, skipping time steps) using a standard MOEA. This approach uses the full budget of evaluations on the slow objective, but it does not fully utilize the evaluation budgets available for the faster objective.Scheme II: To go at the rate of the faster objective using a standard single-objective evolutionary algorithm (EA), for part of the optimization, and then switch to a final, evaluation of some selected solutions on the slower objective at some timestep tswap. Until the time point tswap, this approach fully exploits the budget of evaluations for the fast evaluation, but it does not utilize the budget of evaluations available for the slower objective.Scheme III: Interleaving the evaluation of the objectives so that both per-objective budgets are used to their full extent. Fig. 2illustrates this for the two-objective case.Note that all three of these schemes result in a final population that has been evaluated on both objectives. It may seem intuitive that Scheme III is likely to be the most desirable, as more evaluations are done per unit time (and time is budgeted), but there remains the question of how to co-ordinate the additional evaluations done on the fast objective only, with a population of points that is evaluated on both objectives.Four strategies based on the above schemes are investigated in this work: Waiting, Fast-First, Brood Interleaving, and Speculative Interleaving. In the following each strategy is explained in more detail.In order to define concrete instantiations of the schemes, the following notation is introduced: The set Gidenotes the EA population at time step i. In contrast to this, the search trace Sicomprises the entire set of solutions generated by the EA up to time step i.The setsGifastandGislowdenote the sets of solutions to be evaluated on the fast and slow objective at time step i, respectively. Note that these can be distinct from each other and from the current offspring population—the need for this will become clear later in this section. The setsGi′,Gi′′,andGi′′′represent auxiliary sets of solutions at time step i, such as the offspring population or parents selected for reproduction, and KiandKj′the current and offspring interleaving population devoted for optimizing the fast objective function by Speculative Interleaving, respectively.The most straightforward strategy to deal with delayed objectives is an implementation of Scheme I, as shown in Algorithm 1. This strategy deals with delayed objectives by maintaining a single population (for both objectives) that goes at the rate of the slow objective on both objectives. In other words, evolution waits for all evaluations to be completed before continuing, thus losing evaluation time on the fast objective (Lines 4 and 11).We call this strategy Waiting, and it can be easily embedded into a standard MOEA. This approach can be seen as a representative of the standard approach used in the MOEA literature, where possible differences between the delay of objectives have not been considered.A simple strategy for the implementation of Scheme II is as follows (see Algorithm 2): The strategy Fast-First aims to maximize the number of fast evaluations, while ensuring that a final set of λ solutions has been evaluated on all objectives.This can be achieved by going, for most of the optimization duration, at the rate of the faster objective so as to optimize the faster objective only, and then by switching to the slower objective at the latest possible point (i.e. tswap = B − kslow, which allows for a final round of evaluations on the slow objective) (Line 5). The faster objective can be optimized using a standard single-objective EA. In the simplest case (considered here), the subsequent switch to the slower objective would involve the evaluation of a diverse set of solutions (here the best, distinct λ solutions found with respect to the fast objective) on the slower objective.Again, it is clear that this strategy can be easily embedded into a standard single-objective EA. The main modification required is the maintenance of the search trace Sirequired for the final filtering step (Line 12). The approach bears some resemblance to the previous use of diversity-preserving MOEAs in applications where additional objectives become available at the end of the optimization only (Chmielewski, 2013).The final two strategies, Brood and Speculative Interleaving, are implementations of Scheme III, and are less straightforward than the Waiting and the Fast-First strategies discussed above. Unlike Waiting and Fast-First, strategies in Scheme III attempt to utilize the full budget of evaluations available under both the fast and the slow objectives. The two strategies differ between each other in the way the results from fast and slow evaluations are integrated. Let us first present the general idea of strategies in Scheme III before explaining the strategies Brood and Speculative Interleaving in more detail.The pseudocode given in Algorithm 3presents a general Scheme III strategy, which we divided into several modules: After initializing the population G0 and evaluating it in parallel on the slow and fast objective, (a) and (b), an inner (single-objective) EA is initialized with the current generation on the fast objective,G0fast,and applied to optimize the fast objective for the remainder of the interleaving period (i.e. kslow − 1 generations). The main loop first creates a parent population based on multiobjective selection applied to the current populationGi−kslow,and then enters the Interleaving module to create the new populationsGislowandGifast. These populations are then evaluated in parallel, and an inner (single-objective) EA is used to optimize the fast objective for the remainder of the interleaving period. Finally, environmental selection is applied to the current populationGi−kslowand the current generation on the slow objectiveGislowto give the new population Gi. The main loop is repeated until the budget B is used before non-dominated solutions on both objectives are identified and returned to the user.The idea of Brood Interleaving is to use the fast objective function to look ahead at possible offspring of the current generation being evaluated on the slow objective. This looking ahead can then be used to bias the next generation on the slow objective (hopefully in a positive way) so that the slow objective is used as efficiently as possible.The way in which the look ahead is achieved is via something reminiscent of (soft) brood selection (Altenberg, 1994; Tackett and Carmi, 1994; Walters, 1998). In brood selection, more offspring are created by variation than will pass into the next generation, and this surplus (or brood) is then reduced by performing some filtering on the brood. In the case of Brood Interleaving, the fast objective is used to filter out prospective offspring of the generation currently under evaluation for the slow objective that perform worse than their parent(s) on the fast objective. This filtering should have the effect of avoiding the evaluation of offspring dominated by its parents. However, it also prevents the evaluation of those solutions that outperform their parents on the slow objective only, and it is unclear to what extent this biases the search.Pseudocode for Brood Interleaving is given in Algorithm 4. A population of individuals, G0, is initialized and this becomes the generation zero for both the slow and fast objectives. Evaluations are done in parallel (interleaved) on the slow and fast objectives (Lines 3–6). While the slow one is evaluating the zeroth generation, the fast one finishes, and subsequent generations for the fast objective are constructed and evaluated. These subsequent generations are offspring (i.e., genetic variants) of generation zero (Line 6).When the slow objective function finishes, it is time to construct the next generation for the slow objective,Gislow,upon performing multiobjective parental selection on the current EA populationGi−kslowto give (the parent population)Gi′(Line 9). The next generation on the slow objective,Gislow,is then made up (partially) of those offspring ofGi′(already evaluated on the fast objective) whose parents (one or both of them) improved over at least one of their parents’ fast objective function evaluations (Lines 10 and 11).At this point the next generation can be smaller or larger than the batch size λ (e.g. if only few or many offspring improved upon their parents, respectively). In the latter case, solutions are removed at random from the generation (Line 14). In the former case, the generation is filled with solutions resulting from uniform selection fromGi′and subsequently applying variation (Line 16); note that these solutions need to be evaluated on both the fast and slow objective, occupying some of the spots available in the next generation for the fast objective,Gifast. The remaining spots for evaluation underGifastare filled with solutions resulting from uniform selection fromGislowand subsequently applying variation (Line 19). After evaluating the generations for the slow and fast objective, and the interleaving populations (Lines 21–24), environmental selection is applied to the current EA populationGi−kslowand the set of solutions evaluated on the slow objective,Gislow,to give the EA population of the next iteration, Gi(Line 25).The strategy of Speculative Interleaving (see Algorithm 5) is similar to Brood Interleaving except that it aims to maintain selection pressure at all time steps. Prior to the return of slow evaluations, only partial information about solutions is available. This means that a single-objective selection scheme needs to be used during the wait for slow evaluations, which will only account for solution evaluations under the fast objective. This single-objective scheme is embedded into a single-objective EA that has the task to drive the evolution of the generations on the fast objective. It is clear that this may introduce a bias toward the optimization of the fast objective, yet it is unclear to what extent this will outweigh the advantages gained from increased selection pressure.Increased selection pressure is achieved by creating the interleaving populations,Gi+jfast,j = 1, …, kslow − 1, by applying a single-objective EA (i.e. single-objective selection, variation, and potentially elitism) to Kj, j = 1, …, kslow − 1 (Lines 6–8, and Lines 26–28). Since the interleaving populations evolve while the slow objective is evaluated, we need to record all the solutions or ancestors (inGislow) used to create a solution inGi+jfast,j = 1, …, kslow − 1; note, assuming that a solution is created from two parents, then the number of distinct ancestors associated with a solution may vary between 2 and2kslow−1. Subsequently, an offspring is included intoGislowif its ancestors (oneor multiple) are selected by multiobjective parental selection (Lines 11 and 12) and it outperforms at least one of them with respect to the fast objective (Line 13).This strategy resembles principles of speculative parallelization used in some parallel simulated annealing methods (Marchesi et al., 1994).When augmenting the algorithm schemes introduced in the previous section on a generational-based MOEA, then the environmental and multiobjective parental selection scheme, and the variation mechanism (crossover and/or mutation), need to be replaced with the ones used by the MOEA selected (e.g. in Lines 7 and 13 in Algorithm 1, and Lines 6, 9, 16 and 19 in Algorithm 4). For Fast-First (Line 6 in Algorithm 2) and the process of creating interleaving populations within Speculative Interleaving (Lines 6–8, and Lines 26–28, in Algorithm 5), we would replace these operators with the ones used by the single-objective EA selected.To understand the relative performance of the above strategies, we conduct an empirical study on a variety of problem instances with delayed objectives. In particular, we focus on the analysis of the following aspects:1.The optimization performance of Waiting, Fast-First and Interleaving strategies compared to the optimization performance obtained in an undelayed environment, in general terms.The relative optimization performance of the more advanced methods of Fast-First and the two Interleaving strategies compared to the optimization performance of a standard (Waiting) strategy, in general terms.The effect of problem type on the relative optimization performance of the strategies. Specifically, we expect this to be influenced by the correlation between objectives, the length of the delay, and the ruggedness of the fitness landscape being optimized.Time budget effects: the relative optimization performance of the different strategies as a function of total optimization time.Given the design of the algorithms, we can set out the following rough expectations regarding their individual and relative performance:1.In most scenarios, the performance of an MOEA in the absence of delays would be expected to provide us with an estimated “upper bound” on the optimization performance of any of the alternative strategies (embedded within the same MOEA). This is because a delay has the effect of reducing the per-objective budgets available within these strategies. Any further increase in the length of the delay has the effect of reducing these budgets further. Therefore, any performance gap between the three strategies and this “upper bound” may be expected to increase as a function of the length of the delay.Similarly, the Waiting strategy can be seen to provide an estimated “lower bound” on the optimization performance of any sensible strategy that we could design to account for the presence of delays. This is because Waiting is equivalent to a scenario with the same delay length on both objectives. As a consequence, the per-objective budget for the fast objective is not fully used, and the gap between the available and the used evaluations increases as a function of the length of the delay. Clearly, an improved strategy should attempt to fully utilize unused portions of the per-objective budgets. Assuming that the size of the evaluation budget has a tangible effect on optimization performance (increasing monotonically in the number of evaluations), any performance advantage of such a strategy (compared to Waiting) should then be expected to increase with the length of the delay.It is interesting to note that the overall number of evaluations (summing across all objectives) performed by the Fast-First strategy may indeed be smaller than the overall number of evaluations used by Waiting (e.g. if 1 < kslow ≤ 2), as Fast-First effectively abandons large parts of the per-objective budget for the slow objective. In terms of maintaining a ‘minimum number of overall evaluations’ (as defined by the Waiting strategy), a Fast-First strategy is, arguably, not particularly sensible for small delays (roughly kslow ≤ 2, unless objectives are highly correlated).The Fast-First strategy performs a “focused” (single-objective) optimization on the fast objective, followed by a final round of evaluations on the slow objective. This makes optimal use of the per-objective budget for the fast objective. On the other hand, the per-objective budget for the delayed objective is not used until the very end of the optimization. Hence, such a strategy could be considered to be optimal in the presence of a perfect correlation between objectives (in which case the evaluation of the slower objective would result in the duplication of information at all times). The extent to which Fast-First can cope with objectives that are less highly correlated is doubtful, but may be affected somewhat by the mechanisms of diversity maintenance within the underlying EA.In contrast to Fast-First, both Speculative Interleaving and Brood Interleaving make full use of the per-objective budgets for each objective. In order to do so, the time lag until the return of the delayed objectives needs to be used to partially evaluate an additional λ(kslow − 1) solutions. The two schemes differ in the way these additional solutions are obtained. Brood Interleaving simply generates λ(kslow − 1) offspring, which are partially evaluated on the fast objective. Once results from the delayed objectives are returned, the results obtained on the fast objective are employed to inform selection of the next population. In contrast to this, Speculative Interleaving uses a greedy strategy that corresponds to a temporary switch to a single-objective EA (optimizing several generations on the fast objective only). The difference to Fast-First is that these episodes of single-objective optimization are then interleaved within generations of the multiobjective EA (once the delayed evaluations return).Out of the two Interleaving strategies, Brood Interleaving may be expected to be less affected by the presence/absence of correlation between objectives, and the ruggedness of the fitness landscape, as its bias toward the fast objective is less pronounced (a preference for the fast objective is only introduced during the filtering step every kslow time steps). On the other hand, the reduction of selection pressure (no selection pressure is applied during waits for the delayed objective) will likely hinder convergence, especially for larger delays.In contrast, Speculative Interleaving maintains selection pressure during all generations, which may help in driving the search—promoting quick convergence to local optima on rugged fitness landscapes. On the other hand, selection pressure may be too biased toward the fast objective, particularly so for large delays (which will increase the ratio of fast compared to slow evaluations). This effect will be undesired for problems with little (or anti-)correlation between objectives.The following subsection describes in detail the binary test functions (all functions are to be maximized), the parameter settings, the MOEAs and the evaluation measures used to provide empirical investigation of the performance differences.Test functions: The first test function, a mapped bi-objective OneMax problem, which is inspired by the generalized OneMax problem (Droste et al., 2006), is a family of functions that allows us to control the correlation between objectives. Assuming n1(x) to be the number of 1s in a candidate solution vector x, n1(y) the number of 1s in y, where y is a mapped version of solution vector x, then the mapped bi-objective OneMax problem can be defined asf=(f1,f2)=(n1(x),n1(y)),whereyi=(xi+mapi)mod2,i=1,…,n.The mapped value of a decision variable is mapi∈ {0, 1} and is set independently for each i = 1, …, n by flipping a coin biased by the degree of correlation corr ∈ [ − 1, 1] desired. For instance, for a problem with no correlation between the objectives (corr = 0), the probability of mapi= 1 or 0 is 0.5. For a maximal positive correlation (corr = 1), we set mapi= 0, i = 1, …, n, while for a maximal negative correlation (corr = −1) we set mapi= 1, i = 1, …, n. For an intermediate correlation of c, we set a map bit to zero with a probability of (1.0 + c)/2.The second test function used in this study is the leading ones trailing zeros (LOTZ) function (Laumanns et al., 2004), which can be defined asf=(f1,f2)=(∑i=1n∏j=1ixj,∑i=1n∏j=in(1−xj)).The Pareto front of this problem consists of solutions of the form 1a0bwith a + b = n. The LOTZ problem has often been used to investigate theoretical properties of MOEA algorithms, such as running times (Laumanns et al., 2004). Since the problem is well-understood it should also aid the process of understanding the impact of delayed objectives on performance.The third test function we use is the family of multiobjective NK landscapes, or MNK landscapes (Aguirre and Tanaka, 2007). MNK landscapes extend Kauffman’s NK model (Kauffman, 1993) to multiple objectives by associating a different NK landscape instance to each objective. An NK landscape instance can be used to model epistatic interactions between bits so as to control the ruggedness (number and density of local optima) of the fitness landscape being optimized. More formally, an MNK landscape with m objectives (we fix m = 2 in this work) can be defined asf=(f1,…,fm),wherefi=1N∑j=1Ngj(xj,z1(j),z2(j),…,zK(j)),i=1,…,m.In this equation, N defines the number of bits (in our notation this variable is denoted as n), and the function gjthe fitness contribution of xjand the K bits (also called neighbors),z1(j),z2(j),…,zK(j),interacting with bit xj. Typically, the K neighbors are selected at random, and the fitness contributions gjinitialized uniformly in the range [0; 1) (i.e. fi∈ [0; 1)). These settings are set independently for each objective function fi. The value of K can vary between the objective functions to tune the ruggedness of each objective function separately (with larger values of K resulting more rugged landscapes), while, of course, the number of bits N remains constant. For the sake of gaining a better understanding of the effect of delayed objectives on performance, in this work we focus on the simple case where the value of K is identical for each objective function (but the K neighbors and the fitness contributions gjare set anew and at random for each objective function). Unlike the mapped OneMax and LOTZ problem, MNK landscapes will allow us to investigate the performance of the different strategies as a function of the ruggedness of the landscape optimized.44Recently, MNK landscapes have been extended to control the correlation between objectives (Verel et al., 2011). We do not consider this problem here but investigate the impact of correlations between objectives using the mapped OneMax problem, which is more straightforward to analyze.Search algorithms:To test the delay-handling strategies described in Section 3 we augment them onto the indicator-based EA (IBEA) (Zitzler and Künzli, 2004), a generational MOEA maximizing (in this case) the hypervolume indicator (Zitzler, 1999).The algorithm uses binary tournament selection (with replacement) for parental selection, uniform crossover (Syswerda, 1989), bit flip mutation, and does not check whether a solution has been evaluated previously, i.e. identical solutions may be evaluated multiple times. The parameter settings of the MOEA are given in Table 1. For Fast-First, and the process of generating interleaving populations within Speculative Interleaving, a single-objective EA with the same parental selection and variation operators is employed (note, in this case tournament selection is based on a single objective only); environmental selection is done using a (μ + λ)-ES (evolution strategy) reproduction scheme.Note, for IBEA we are using the adaptive version, involving scaling the hypervolume indicator values in combination with a fitness scale factor of κ = 0.05 (as recommended by Zitzler and Künzli, 2004).55All strategies have been coded up in Java within the jMetal framework (Durillo and Nebro, 2011). The code to run the strategies within IBEA and other generational MOEAs, such as NSGA-II (Deb et al., 2002), as well as steady-state MOEAs, such as SEMO (Laumanns et al., 2004) and SMS-EMOA (Beume et al., 2007), is available at http://www.ucl.ac.uk/~ucberal/.If not otherwise stated we use a budget of B = 40, a search space of size n = 20, and assume that objective f2 is the slow objective. For the mapped OneMax problem and MNK landscapes, we create a new problem instance at random for each algorithmic run assuming a fixed level of correlation (the problem instances are the same for each strategy investigated). Any results shown are average results across 30 independent algorithm runs. We use paired comparison by employing a different seed for the random number generator for each EA run but the same seeds for all strategies described above.Evaluation measures:We use two approaches to assess the performance of a multiobjective optimizer subject to delayed objectives. The first is based on the hypervolume indicator (Zitzler, 1999; Zitzler et al., 2003). This indicator assesses the size of the bounded region (in the objective space) dominated by a set of points (vectors of objective function values of non-dominated solutions). The region dominated is bounded on one side by a set of non-dominated solutions, and on the other side by a bounding point. If the bounding point is set appropriately and kept the same when comparing multiple sets of non-dominated solutions, then larger indicator values indicate that a solution set is better than another one in terms of various desirable aspects including diversity, extent and proximity to the Pareto optimal front. As, for the problems considered here, the value ranges of the objectives are known, the bounding point is set to the worst possible point in the objective space shifted by 1 in each objective. To compare the hypervolume obtained by different algorithms, we run each algorithm multiple times on a problem and report the median and interquartile ranges of the hypervolume across the algorithmic runs. To investigate significant differences between algorithms with respect to the hypervolume, we use a repeated-measures statistical test, the Friedman test (Friedman, 1937). This test does not assume an underlying distribution of the data (i.e. is non-parametric).The second approach is based on attainment surfaces (Fonseca and Fleming, 1996). Attainment surfaces express graphically the performance of an MOEA in terms of the surface in the objective space that can be attained in a fraction of algorithmic runs. For each strategy, we show the surfaces that can be attained in 50% of the runs, also known as median attainment surface.

@&#CONCLUSIONS@&#
