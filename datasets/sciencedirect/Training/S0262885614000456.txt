@&#MAIN-TITLE@&#
Classification and weakly supervised pain localization using multiple segment representation

@&#HIGHLIGHTS@&#
Target problem of pain classification and localization using weakly-labeled pain videosAlgorithm combines multiple instance learning with multiple segment representation.Rigorous experiments show that our algorithm achieves state-of-the-art performance.Empirically evaluate the contributions of different components of our algorithmVisualize discriminative facial patches, as learned by our algorithm

@&#KEYPHRASES@&#
Emotion classification,Action classification,Pain,Temporal segmentation,Bag of Words,Weakly supervised learning,Boosting,Bagging,

@&#ABSTRACT@&#
Automatic pain recognition from videos is a vital clinical application and, owing to its spontaneous nature, poses interesting challenges to automatic facial expression recognition (AFER) research. Previous pain vs no-pain systems have highlighted two major challenges: (1) ground truth is provided for the sequence, but the presence or absence of the target expression for a given frame is unknown, and (2) the time point and the duration of the pain expression event(s) in each video are unknown. To address these issues we propose a novel framework (referred to as MS-MIL) where each sequence is represented as a bag containing multiple segments, and multiple instance learning (MIL) is employed to handle this weakly labeled data in the form of sequence level ground-truth. These segments are generated via multiple clustering of a sequence or running a multi-scale temporal scanning window, and are represented using a state-of-the-art Bag of Words (BoW) representation. This work extends the idea of detecting facial expressions through ‘concept frames’ to ‘concept segments’ and argues through extensive experiments that algorithms such as MIL are needed to reap the benefits of such representation.The key advantages of our approach are: (1) joint detection and localization of painful frames using only sequence-level ground-truth, (2) incorporation of temporal dynamics by representing the data not as individual frames but as segments, and (3) extraction of multiple segments, which is well suited to signals with uncertain temporal location and duration in the video. Extensive experiments on UNBC-McMaster Shoulder Pain dataset highlight the effectiveness of the approach by achieving competitive results on both tasks of pain classification and localization in videos. We also empirically evaluate the contributions of different components of MS-MIL. The paper also includes the visualization of discriminative facial patches, important for pain detection, as discovered by our algorithm and relates them to Action Units that have been associated with pain expression. We conclude the paper by demonstrating that MS-MIL yields a significant improvement on another spontaneous facial expression dataset, the FEEDTUM dataset.

@&#INTRODUCTION@&#
Pain is one of the most challenging problems in medicine and biology and has substantial eco-social costs associated with it [9]. It has been estimated that there might be more than 30million people in USA with chronic or recurrent pain [34]. Also nearly half of Americans seeking treatment from a physician report pain as their primary symptom. The United States Bureau of the Census estimated the total cost for chronic pain to exceed $150billion annually in year 1995–96 [9,34]. Thus there has been a significant research effort in improving pain management over the years.Identifying pain among patients is considered critical in clinical settings since it is used for regulating medications, long-term monitoring, and gauging the effectiveness of a treatment. Pain assessment in most cases involves patient self-report, obtained through either clinical interview or visual analog scale (VAS) [9]. For the latter case the nurse asks the patient to mark his pain on a linear scale with ratings from 0 to 10, denoting no-pain to unbearable-pain. The fact that VAS is easy to use and returns a numerical rating of pain has made VAS the most prevalent pain assessment tool. However VAS suffers from a number of drawbacks such as subjective differences, and patient idiosyncrasies. Therefore it cannot be used for unconscious or verbally-impaired patients [6] and may suffer from high individual bias. These drawbacks have led to a considerable research effort to identify and quantify objective pain indicators using human facial expression [33]. However most of these methods entail manual labeling of facial Action Units or evaluations by highly trained observers, which in most cases is time consuming and unfit for real-time applications.Over the years there has been a significant progress in analyzing facial expressions related to emotions using machine learning (ML) and computer vision [19]. Most of this work has focused on posed facial expressions that are obtained under controlled laboratory settings and differ from spontaneous facial expression in a number of ways [4,7]. We refer our readers to a survey on automatic facial expression recognition (AFER) by Bartlettet al. [4] that has identified the difficulties faced by AFER on spontaneous expressions. A major challenge of spontaneous expressions is temporal segmentation of the target expressions. Videos may exist in which the target emotion or state was elicited, but the onset, duration, and frequency of facial expressions within the video are unknown.A significant contribution to in research on spontaneous expressions was the introduction of UNBC-McMaster Shoulder Pain dataset [21] that involves subjects experiencing shoulder pain in a clinical setting. This dataset was provided with two levels of annotations for measuring pain — (1) per-frame pain ratings based on a formula applied to Action Unit (AU) annotations, and (2) per-video pain ratings as measured by experts (see Section 5.1). This work utilizes the per-video pain ratings for training a binary pain classification system. Pain localization is then evaluated using the per-frame pain ratings based on AU labels, which are more costly to obtain. Thus our setting is such that each video is labeled for the presence or absence of pain, but there is no information about the location or duration of facial expressions within each video. This setting is referred to as weakly labeled data and poses a challenge for training sliding window classifiers and further limits the performance of the standard approach of obtaining fixed length features through averaging and training a classifier. Previous approaches [2,22] follow a common paradigm of assigning each frame the label of the corresponding video and using them to train a support vector machine (SVM). Pain is detected in a video if the average output score (distance from separating hyperplane) of member frames is above a pre-computed threshold. Such approaches suffer from two major limitations: (1) not all frames in a video have the same label and (2) averaging output scores across all the frames may dampen the signal of interest. This paper proposes to address these challenges by employing multiple instance learning (MIL) framework [35].MIL is an approach for handling ‘weakly labeled’ training data. In such cases the training data only specifies the presence (or absence) of a signal of interest in the data without indicating where it might be present. For instance in the case of pain vs no-pain detection, a sequence label only specifies if a subject is not in pain without any details regarding the time point or duration of pain. Other techniques for tackling weakly labeled data include part-based models [11] and latent models such as pLSA and LDA [37]. Most of these approaches try to identify the signal of interest by inferring the values of some latent variables while minimizing a loss function. MIL was introduced to address the problem of weakly supervised object detection [13,35]. Compared to other approaches, MIL offers a tractable way to train a discriminative classifier that avoids complex inference procedures. MIL has been successfully employed for face recognition from video [35] and more recently has been proposed for handling labeling noise in video classification [18].This work focuses at detecting spontaneous pain expression in video when given only sequence level ground-truths. The phrase detection is used throughout the paper to denote the joint tasks of pain classification and localization in time. Explicitly, classification refers to predicting the absence/presence of pain in a video, while localization refers to predicting pain/no-pain at the frame level. The novelty of this work lies in combining MIL with a dynamic extension of concept frames, into a novel framework called multiple segment-multiple instance learning (MS-MIL). Our major contributions are as follows:1.Inherent drawbacks in previous approaches for pain detection in videos are identified and a pipeline has been proposed to address these concerns. The most salient feature of our approach is that it can jointly classify and localize pain by using only sequence level labels (Section 2).For addressing the demands of the pain detection task, we propose to represent each video as a bag containing multiple segments which are modeled using MIL. The multiple segment based representation and MIL are able to address spontaneous expressions, such as pain, that can have uncertain locations, durations and occurrences (Section 4).The performance of MS-MIL is compared on the detection task with other competitive algorithms. We also perform systematic evaluation to highlight the contribution of multiple segment representation and MIL, in MS-MIL, separately. These results indicate the advantage of using the MS-MIL approach along with some interesting insights (Section 6).The problem of detecting pain through facial expressions in general includes many challenges and this work is trying to focus on a particular aspect of the problem. Other challenges in objective pain measurement include differences between acute and chronic pain, as well as differences in personality including pain catastrophizing, which may affect the intensity of pain expression. We are undertaking a separate study to begin to address some of these factors [14].The first computer vision work on automatic pain detection in videos on the UNBC-McMaster Pain dataset was by Ashraf et al. [2]. Their approach started by first extracting AAM based features from each frames and using these to cluster the frames in order to create a training data with size that is manageable by a SVM. Following this, each of these clustered frames was assigned with the label of their corresponding sequence and used to train a linear SVM. Finally during prediction each test-frame was assigned a score based on its distance from separating hyperplane. Then a test-video was predicted to be in pain if the average score of its member frames exceeded a threshold. Lucey et al. [22] extended this work by borrowing ideas from the related field of visual speech recognition and proposed to compress the signal in the spatial rather than temporal domain using the Discrete Cosine Transform (DCT). Lucey et al. [22] used the system in [2] as their baseline system and showed significant improvement in performance using their idea.Previous works didn't address the ambiguity introduced by weakly labeled data, and each member frame was assigned the label of the sequence. Such approaches lead to a lower performance compared to the case when ground-truth for each frame is known [1,2]. We address this particular concern by proposing to use MIL (in-place of SVM) which has been designed specifically to handle weakly labeled data.Secondly, [22] highlighted that incorporating the dynamics of the pain signal is difficult since there is no information about the number of times pain expressions can occur or their location and duration in a sequence. Following this, [22] suggested to add temporal information by appending adjacent frames onto the frame of interest, as input to the SVM [25]. [22] tested this idea of appending adjacent frames in their paper, however they found that their performance degraded. One possible explanation is that SVM classifiers are not well suited to weakly labeled training data and may suffer from mislabels when the data is in this form.Motivated by the last idea we propose to incorporate temporal dynamics by representing each sequence not as individual frames (as done earlier) but as sets of frames, referred to as ‘multiple segments’. The benefits of such a representation are reaped by using MIL, which can efficiently handle data in such form. Since MIL handles data as bags, we can visualize every sequence as a bag containing multiple segments. Multiple segments (MS) have twofold advantages: (1) it allows pain expression to have random duration and occurrence, and (2) it incorporates temporal information by pooling across multiple frames in a segment. Thirdly, the earlier work performed prediction for each sequence using the average decision score of its frames. Such an approach may not be optimal in all situations since the averaging operation tends to dampen the signal of interest. The MIL framework employed in this work avoids this limitation by using the max operation to predict the label of a bag based on the posterior probability of its instances (see Section 3).Another potential approach to the problem of pain detection comes from the classical approach to action recognition from computer vision literature [17,39]. This approach is based on BoW architecture and composed of three steps: feature extraction, encoding features using a dictionary of visual words and pooling with l1 normalization. Since each video is represented as a fixed length vector, we shall refer to these techniques as global-feature based approaches. [39] have provided a systematic evaluation of different components of this pipeline on two human action datasets. These techniques are known to work well for problems with uniform actions that span the entire video such as CK+ facial expression dataset [20] or KTH human action dataset [16]. However their performance falls down when actions have high intra-class variations and are localized in the video, which is true for the pain detection problem as well. We also found this hypothesis to be true during our experiments and attribute it to the argument that pooling features across the entire video tend to reduce discriminative ability of the features.In a recent paper [31] Tax et al. explored the question of whether it is always necessary to fully model the entire sequence, or whether the presence of specific frames, called ‘concept frames’, might be sufficient for reliable detection of facial expressions. In their study two different approaches for AFER were investigated: (1) modeling full sequences using approaches such as Hidden Markov Models and Conditional Random Fields, and (2) modeling only certain frames, for AU detection in sequences. The author in [31] also suggested that for modeling only particular key frames, algorithms such as MIL are required and investigated one such approach. Through extensive experiments the authors showed that for reliable classification, modeling certain key frames is sufficient compared to modeling the entire sequence. A limitation of ‘concept frames’, however, is that they do not incorporate temporal information, which could potentially be exploited by learning algorithms such as MIL (and to some extent SVM [30]).The present paper takes a leap forward by proposing a dynamic variant of ‘concept frames’. Here we extend the idea of ‘concept frames’ to ‘concept segments’ consisting of multiple frames. These ‘concept segments’ can be thought of as localized sub-expressions that contain the expression of interest in a sequence. We propose that reliable detection of facial expression can be achieved by detection of key localized segments using tailored algorithms such as MIL. [30] explored a segment based approach, called k-Seg SVM, and employed a structured-SVM to detect temporal events (AU segments in their case). Our work differs from this work in several respects, most notably that [30] is a completely supervised algorithm requiring location information in the training data, whereas the approach presented here operates on weakly labeled data. Authors in [8] represented a video by concatenating features from 6 key-frames (segments) that were identified by clustering based on the output of an emotion classification task. We overcome the possible limitations of this work by allowing the videos to be represented by a variable number of segments of varying lengths and performing classification by explicitly spotting the segment containing target expression.The general machine learning paradigm involves finding a classification function that minimizes a loss functionL(D, h(x)) over training data provided as N samples and their corresponding labels, D={xi, yi}i=1N, where xi∈X and yi∈Y. Rather than handing training data in the form of individual samples, the MIL paradigm is designed to handle problems involving training data in the form of bags, B={Xi, yi}i=1N, whereXi=xijj=1Ni, yi∈Y and Niare the number of instances in Xi. Since this work deals with only binary classification problems, the output space Y∈{−1, 1}. Such problems occur frequently in computer vision since it is easier to obtain a group label for the data compared to individual labels and such labels can also suffer from annotator bias and noise [18]. Recently several works have adopted MIL to address these concerns in domains such as handling label noise in video classification [18], face recognition in videos with subtitles [40], and object localization [13].As shown in Fig. 1the MIL framework defines two kinds of bags, positive and negative, in a similar fashion to positive and negative instances in traditional machine learning. A bag is a positive bag if it contains at least one positive instance, while a negative bag contains no positive instance.We have employed multiple instance learning based on boosting (MilBoost) algorithm proposed by Viola et al. [35] for this work. In the next two sections we shall give an overview of Friedman's gradient boosting framework [12], which is the backbone of MilBoost. This will be followed by the description of MilBoost.We shall define the gradient boosting in the realm of traditional learning framework and then discuss its extension to the MIL framework.Boosting involves constructing a strong classifier HT(x) by iteratively combining many weak classifiers ht(x), where the subscript t(t=1…T) represents the index of the classifier added at the tthiteration. All weak classifiers are constrained to belong to a certain family of functionsH, such as stumps or trees.(1)HTx=∑t=1Tαthtx(2)HTx=HT−1x+αThTxEq. (2) can be seen as a numerical optimization strategy that iteratively minimizes a loss functionL(D, HT−1(x)) over training data D by moving in certain optimal direction given by hT. Under this strategy, the loss function at step T can either be seen as a function of the current classifier HT−1 or the parameters that define the family of functionsH.Friedman suggested following the latter approach since it offers an intuitive way to solve the above optimization problem. HT−1(x) can be considered as n dimensional vector whose ithcomponent is HT−1(x). Following this idea, the gradient descent strategy is employed to minimize the loss function by moving some steps in the direction of the negative-gradient of the loss function wrt HT−1(x). This negative gradient is denoted by wiin Eq. (3). In the remaining sections of this paper we shall refer to w as weights and the rationale behind this will be evident in Section 3.2.(3)wi=−∂L∂HT−1xx=xiThus the gradient boosting framework prescribes to minimize the loss function by moving in the direction w computed at each iteration. Since HTis a linear combination of HT−1 and w, it would be smooth only when w∈H. However it will be too idealistic to assume this in all cases. Friedman proposed to tackle this problem by projecting w over the function spacemaxby finding the best approximation ht∈Hto w.(4)ht=argmaxh∑i=1NwihxiWe shall refer to Eq. (4) as the ‘projection step’ and note that hthas the maximum correlation with w. Once htis computed, step size αtis found via a line search to minimize L(D, HT(x)). In the next section we shall discuss how gradient boosting is extended to the MIL framework.MilBoost combines the gradient boosting framework with the concept of MIL, where training data occurs as bags. As defined in Section 3, the ithbag is denoted by Xiand the jthinstance inside it is represented as xij. The posterior probabilities over bags and instances are defined as:(5)pi=Pryi=1|Xi(6)pij=Pryij=1|xij.We shall be using the original formulation defined in [35] for the loss function given by the negative log-likelihood:(7)L=−∑iNtilogpi+1−tilog1−piwhere ti=1 if yi=1 and ti=0 if yi=−1.This formulation for the loss function seems intuitive since the only information available about a MIL dataset is label information for each bag (yi). We lack any information about the probabilities (or labels) of individual instances (pij). These instance probabilities can also be seen as latent variables, whose values are inferred during the boosting process [3].MIL assumes that a positive bag contains at least one positive instance. Hence the probability of a bag being positive (pi) is defined in terms of individual instances as:(8)pi=maxjpij.Since the max function is not differentiable, a number of differentiable approximations to the max function have been proposed for MilBoost [3,35,40]. In this work we shall refer to these approximations as softmax functions g(pij). The most common choice of soft-max function in earlier works is noisy-or (NOR). A major disadvantage with NOR is that it deviates from the max function as the size of the bag increases, which we shall refer to as ‘bagsize-bias’. To illustrate this shortcoming we consider a toy example which consists of two bags B1 and B2 of sizes of 3 and 5. The instance probabilities for these bags are given by B1=[.15 .15 2] and B2=[.15 .15 .15 2]. As is evident, the max for both cases is 2, however the NOR formulation yields the maximum as .45 and .53 respectively. This observation clearly highlights the bagsize-bias associated with NOR. Such a problem is critical for cases where bag sizes might differ across training examples and ours is such a case since the number of frames per sequence varies from 60 to 600. Thus in this work we have addressed this problem by employing another soft-max function called generalized mean (GM), which is known to be a better approximating function than NOR [3].The instance probabilities (pij) for instance xijare obtained by the application of a sigmoid function over the raw classifier score hij:(9)pij=σhxij.As described in Section 3.1, the negative gradient of the loss-function (for instance xij) is obtained as:(10)wij=−∂L∂hij.We can easily calculate wijby exploiting the chain rule of differentiation and calculating each component as:(11a)wij=−∂L∂hij=−∂L∂pi∂pi∂pij∂pij∂hij(11b)∂L∂pi=1piti=11−ti1−piti=0(11c)∂pi∂pij=∂g(pij)∂pij(11d)∂pij∂hij=∂σhij∂hij=σhij1−σhij.Next we explain the rationale behind referring to the negative instance-wise gradients (wij) as weights, using the NOR softmax function as an example. From Table 1, wijfor the NOR soft-max function is defined aswij=1−pipipijfor a positive bag and wij=−pijfor a negative bag. Thus these weights describe (1) the label of the bag containing instance xijand (2) the importance of the instance in learning procedure, by being high for an instance that lies in a positive bag but has a low classifier score and vice-versa. The idea of weighting instances during learning is common in boosting procedure [12].As described in Section 3.1, the next step involves finding a new weak learner h(xij) that has the highest correlation with the weights wijusing the projection step (Eq. (4)). This work employs binary decision stumps as weak learners, which perform classification by assigning a threshold to a single feature and are a common choice in boosting frameworks [35]. ThusH80.99 belongs to the class of decision stumps. A simple mathematical formulation has been provided in Borris et al. [3] on how Eq. (4) (the projection step) can be transformed into:(12)ht=argminh∑ijhxij≠sgnwijw'ijwhere [.] is the Iverson bracket,w'ij=wij∑ij|wij|and sgn(l) is the signum function.Eq. (12) is a general formulation for any learning algorithm that has training data with binary labels sgn(wij) and weights w'ij. Thus we can easily find a function ht(xij) at tthiteration that has the highest correlation with wijby using training procedure for a decision stumps. All the steps of the MilBoost algorithm are mentioned in a sequential order in Algorithm 1.Algorithm 1MilBoost algorithm

@&#CONCLUSIONS@&#
