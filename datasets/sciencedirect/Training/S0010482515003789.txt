@&#MAIN-TITLE@&#
Multilayer descriptors for medical image classification

@&#HIGHLIGHTS@&#
Method presented for building an n-layer image using preprocessing methods.Performance of 2D descriptors improved using n-layer images.Multilayers and texture descriptors can be combined to enhance performance.

@&#KEYPHRASES@&#
Texture descriptors,Ensemble,Local binary patterns,Local phase quantization,Support vector machine,Multilayer descriptors,

@&#ABSTRACT@&#
In this paper, we propose a new method for improving the performance of 2D descriptors by building an n-layer image using different preprocessing approaches from which multilayer descriptors are extracted and used as feature vectors for training a Support Vector Machine. The different preprocessing approaches are used to build different n-layer images (n=3, n=5, etc.). We test both color and gray-level images, two well-known texture descriptors (Local Phase Quantization and Local Binary Pattern), and three of their variants suited for n-layer images (Volume Local Phase Quantization, Local Phase Quantization Three-Orthogonal-Planes, and Volume Local Binary Patterns). Our results show that multilayers and texture descriptors can be combined to outperform the standard single-layer approaches. Experiments on 10 datasets demonstrate the generalizability of the proposed descriptors. Most of these datasets are medical, but in each case the images are very different. Two datasets are completely unrelated to medicine and are included to demonstrate the discriminative power of the proposed descriptors across very different image recognition tasks.A MATLAB version of the complete system developed in this paper will be made available at https://www.dei.unipd.it/node/2357.

@&#INTRODUCTION@&#
The amount of multidimensional visual information (e.g., 2D images, videos, 3D surface models of objects, and 3D tomographic images) uploaded to the internet on a daily basis has increased enormously in the past few years. The video hosting website YouTube, for instance, received more than 100h of new video every minute in 2014. In the field of medical imaging, individual radiology departments routinely produce an enormous amount of multidimensional information that is warehoused in private and public medical databases. Such large quantities of data are difficult to manually label for further access and reuse. There is, as a result, an urgent need to develop sophisticated, efficient, and accurate image classification algorithms that are able to provide best matches for specific classification tasks. Many applications require the ability of discriminating among classes of images, to distinguish, for instance, pedestrians from objects for advanced driver assistance, for disease recognition from medical images, for landmark recognition for touristic purposes, and many other applications. In these cases, computer vision plays a central role. Key to successful classification is the ability of representing images based on visual characteristics such as texture, color, and shape [1].Automated visual tasks such as detection, localization, categorization, and recognition by visual features are important subjects of study in computer vision and image analysis. These tasks often prove difficult, however, due to high within-class variability that can be caused by a variety of factors: noise, distortions, illumination, scale changes, occlusion, and so on. Extracting features within visual databases is one of the most important steps in the classification process as success depends on the method adopted for extracting features from a given set of images. Particularly important are invariant image descriptors since they extract information from images which is invariant to noise, illumination, distortion, etc.One class of invariant descriptors is color composition, which in a visual scene is robust to noise, image degradations, changes in size, resolution, and orientation. Most existing systems use various color descriptors [2,3] in order to retrieve relevant samples. Unfortunately, the classification performance of color descriptors is negatively affected by their lack of discriminative power.Descriptors based on shape are considered more generic than color and are the most widely used descriptors in many application areas such as object detection and action recognition. There are many ways to represent shapes. Some examples include axial representation [4], primitive-based representation [5], histograms of oriented gradients [6], contour-based representation [7], and probability density function [8]. Most shape descriptors are robust to rigid transformations, noise, occlusions, and missing data.Another class of descriptors is texture. Texture has received considerable attention [9–11] and has proven valuable in medical imaging, image retrieval, object recognition, and industrial product identification (e.g., classifying types of marble, ceramic tiles, parquet slabs, etc.). Various methods have been introduced to analyse texture in digital images. The most well-known methods are based on statistical approaches, such as histograms of gray-level pixel values and the Gray-Level Co-occurrence (GLC) matrix [12], wavelet transforms and Gabor filters [13], and the Local Binary Pattern (LBP) operator [14,15]. The LBP operator, in particular, has proven an efficient method for describing texture in 2D. It has received much attention from the scientific community, and many powerful variants have recently been proposed [16].Much work has also been done exploring the integration of more than one feature to improve classification accuracy [17]; however, the performance of these combinations are strictly application dependent. In some cases less efficient features can degrade more efficient features with a loss of overall system accuracy.Most recently, there has been a move to explore color in three dimensions. In [18] the RGB space is considered as three planes in a 3D image, and a 3D version of the Local Ternary Pattern (LTP) is then proposed that extracts features directly from the 3D image, instead of from each color space (i.e., separately from R, G, and B). Moreover, they show that it is possible to generate a multilayer image starting from gray value images using a 2D circular symmetric Gaussian filter. Please note that in some works (e.g. [18]) the authors use the term “volumetric descriptors” to denote descriptors extracted from a RGB image or any kind of images represented by more than a layer. In this work we use the term multilayers to denote such a concept to avoid misunderstanding with the the notion of “volumetric descriptor” that in the context of medical image analysis refers to some measure of volume in anatomically meaningful regions on a medical image.In this paper we start from the idea in [18] that considered the RGB image as a three-layer image and extend this representation to preprocessed images, showing that it is possible to improve the performance of single-layer descriptors by building an n-layer image (n=3, n=5, etc.) using different preprocessing approaches.Formally speaking if a single layer image can be considered as mappings from a spatial coordinate system into a value space, i.e. a mapping from [1,2,…,m]×[1,2,…,l]→ ℜ for an m×l image, a n-layer image is a mapping to a n dimensional space: [1,2,…,m]×[1,2,…,l]→ ℜn. An RGB color image is a particular case of a multilayer image where n=3. This definition is different from that used in the literature for existing multilayer descriptors (e.g. [18,22]) since they were proposed for dynamical textures.The n-layer images are here described using multilayer descriptors, and these feature vectors are fed into a Support Vector Machine (SVM) [19]. We test both color and gray-level images, two well-known texture descriptors (LBP and Local Phase Quantization), and three of their variants suited for multilayer images (viz., Volume Local Phase Quantization, Local Phase Quantization Three-Orthogonal-Planes, Volume Local Binary Patterns). Our results show that multilayer and texture descriptors can be combined to outperform standard single-layer approaches. The proposed approach is applied to ten datasets to demonstrate the generalizability of this approach. In this work, we do not deal with the segmentation problem, since all the datasets considered in the experiments contain only segmented image. The segmentation of large images that can contain parts of different textures is out of the scope of this paper.The remainder of this paper is organized as follows. In Section 2 we describe the base approaches used in our system, and in Section 3 we present our system. We conclude in Section 4 by summarizing the significance of our work and by highlighting some future directions of exploration.In this section we provide an overview of the methods used for building the ensemble of descriptors. We describe the multilayer descriptors in Section 2.1. A very brief description of the preprocessing methods employed to generate the n-layer images is available in Section 2.2.In this subsection we briefly explain the methods used for building the ensemble of multilayer descriptors.VLBP, introduced by [22], is an extension of LBP, with the notion of self-similarity, central to conventional image texture, extended to the spatiotemporal domain. VLBP deals with dynamic texture analysis on the 2D time series and not on the full 3D data; therefore, it only provides rotation invariance towards rotations around the z-axis.VLBP extends standard LBP to handle dynamic texture analysis by considering the joint distribution v of the gray-levels of 3P+3 pixels (P>1 is the number of local neighboring points around the center in one frame):V=v(gtc−L,c,gtc−L,0,…,gtc−L,P−1,gtc,c,gtc,0,…,gtc,P−1,gtc+L,c,gtc+L,0,…,gtc+L,P−1)wheregt,pt∈[tc−L,tc,tc+L],p∈[0,…,P−1]corresponds to the gray value of P equally spaced pixels on a circularly symmetric neighbor of the center pixel c, in the center frametcand its previous/posterior frames with time interval L.To obtain the gray-scale invariance as in LBP, the gray value of the centergtc,cis subtracted from the gray values of the circularly symmetric neighborhood of each frame. To achieve invariance with respect to the scaling, the resulting code is binarized by considering only the signs of the differences. Finally, a unique VLBPL,P,Rcode is obtained by binary factorization (i.e., by weighting each pixel difference by a different binomial factor). The histogram obtained from the VLBPL,P,Rcode is normalized with respect to size variations by setting the sum of its bins to unity. Rotation invariance is obtained by rotating the neighbor set in three separate frames clockwise synchronously so that a minimal value is selected. Here we have used the following parameters R=1; P=4; L=1.VLPQ, introduced in [23], is based on the binary encoding of the phase information of the local Fourier transform at low frequency points and is an extension to the LPQ operator [21] used for spatial texture analysis. Since dynamic texture consists of a sequence of texture in the spatio-temporal domain, the Fourier transform estimation is performed locally using Short-Term Fourier Transform (STFT). Given a sequence f(x), STFT is computed over an M-by-M-by-N neighborhoodNxof x (where M is the spatial size and N the size in the temporal domain):F(u,x)=wuTfx,where wuis the basis of the 3D DFT at frequency u, and fxis a vector containing all pixels from the neighborhoodNx. The computation is efficient since STFT can be evaluated for each pixel using 1D convolutions for each dimension, due to the separability of the basis functions.The transform matrix is obtained using the thirteen lowest non-zero frequency points (see [23] for details) and by separating the real and imaginary parts of each component. This results in a 26-by-M2N transform matrix:W=[Re{wu1},Im{wu1},…,Re{wu13},Im{wu13}].Hence, the vector form of the STFT for all frequencies u1, …, u13 can be written asFx=Wfx.In order to reduce the length of the resulting descriptor, dimension reduction by PCA and scalar quantization to a binary value is performed. Finally, the quantized coefficients are represented as integer values, using simple binary coding, and codified as a histogram of size L (where L is the dimension retained by PCA). Here we use windows of size=3 and L=10.LPQ-TOP, proposed by Zhao and Pietikainen [24], is an extension of LPQ that, similarly to VLBP [22], takes into consideration dynamic texture. With LPQ-TOP the basic LPQ features are extracted independently from three orthogonal planes: XY, XT and YT (the XY plane provides the spatial domain information while the other two planes provide temporal information). The phase information is computed locally in a window for every image position in the three directions. The final descriptor, LPQ-TOPWx,Wy,Wt, is the concatenation of the three normalized histograms obtained by accumulating the occurrence of quantized phase code in each direction (XY, XT and YT). Wx, Wy,Wtare the window size parameters (i.e., the dimension of the neighborhood at each pixel position for computing LPQ). These parameters are set to different values in the spatial and temporal planes. Following [24], we employed LPQ-TOP3,3,3 and LPQ-TOP5,5,5; the two feature vectors are then concatenated and fed into a SVM.One of the aims of this work is to test the idea of improving the performance of texture descriptors by building a multilayer image using different preprocessing approaches. In this way, multilayer descriptors could describe the different nlayer images, and each feature set produced by each descriptor could then be fed into a SVM. In exploring this approach, we test the following preprocessing methods: decomposition by wavelets (Wave); Multi-Resolution by Gaussian filters (MRG); Gradient image (GR); color space transform for color images (RGB to HSV and RGB to YUV); image enhancement using exposure based subimage histogram equalization for gray-level images (ESI); and contrast enhancement based on layered difference representations of 2D histograms for color images (LDR). In some cases, when the approach (e.g., Wave, MRG, and GR) is applied to color images, it is applied separately to each space; thus, three processed images are obtained using these preprocessing methods. The remainder of this section describes each of aforementioned preprocessing methods in more detail.WAV works well in many computer vision methods that detect and recognize objects of interest. In order to use WAV[25] for 2D decomposition, a 2D scaling function,φ(x,y)and three 2D wavelets functions,ψi(x,y)are needed, whereirepresents the three possible intensity variations along horizontal, vertical, and diagonal edgesi={H,V,D}.In this paper, we use the Daubechies [26] wavelet family (Wa) with four vanishing moments and a single scale decomposition. The four preprocessed images are thus the approximation coefficients matrix and the three details coefficients matrices (horizontal, vertical and diagonal). The processed images are resized to the size of the original image. In the experimental section, we label each of these as follows:•Wave1: the approximation coefficients matrix;Wave2: the horizontal details;Wave3: the vertical details;Wave4: the diagonal details.In this paper, we test the Gaussian scale-space representation, where the original image is filtered to obtain two smoothed versions. This is accomplished using a 2D symmetric Gaussian lowpass filter of size k pixels (k=3 and k=5 in this work) with standard deviation 1.Gradient is an image operation that is commonly used to detect edges. GR calculates the magnitude of the gradients of each pixel in the x and y directions based on its neighbors. The processed image at coordinates(x,y)is given by the magnitude of the gradient vector at the same coordinates. Thus, each pixel of a gradient image measures the change in intensity in a given direction of the corresponding point in the original image.A color space transform converts images from one color space representation to another. The input images in the tested databases are given in the RGB color space. In this paper, the original images are transformed into the following two color spaces: YUV and HSV.The type of image enhancement applied depends on whether the images are colored or gray-level:•ESI: For gray-level images, we apply a novel Exposure-based Sub-Image Histogram Equalization method, proposed in [27], that enhances the contrast of low exposure gray-scale images. Exposure thresholds are computed that divide the original image into subimages of different intensity levels, and the histogram is clipped using a threshold value (as an average number of gray-level occurrences) to control enhancement rate. The individual histograms of the subimages are then equalized independently, with all subimages finally integrated into one complete image that is used for analysis.LDR: For color images, we apply a novel contrast enhancement algorithm, proposed in [28], that is based on the layered difference representation of 2D histograms. The basic idea behind this method is to enhance image contrast by amplifying the gray-level differences between adjacent pixels. Enhancement is viewed as a constrained optimization problem that is based on the observation that the gray-level differences occurring more frequently in the input image should be emphasized more in the output image. This is accomplished by first taking a given RGB image and transforming it into the YUV space, where the Y band is enhanced. The YUV image is then transformed back to the RGB space.Below is a simple description of our proposed algorithm:1.INPUT IMAGEThe proposed method accepts both gray-level and color images.DO COLOR SPACE CONVERSIONThis step is performed only on color images and is aimed at analyzing the image from different color perspectives (see Section 2). The original RGB image is converted into HSV and YUV, thus obtaining six additional layers.PREPROCESS IMAGESImage preprocessing techniques are usually used to improve the quality of an image before processing into an application. In this approach we use some well-known preprocessing methods to highlight distinctive aspects of the images before performing feature extraction. The preprocessing techniques are outlined in Section 2.MAKE n-LAYER IMAGEIn the case of a gray-level input image, the output of the preprocessing step is a 9-layer image (the original image plus eight preprocessed layers), otherwise the output is a 33-layer image (resulting from the concatenation of 24 layers produced from the preprocessing step, i.e. 8 approaches × 3 color channels, plus three layers from the original RGB image, plus six color layers from step 1).EXTRACT FEATURESThe feature extraction is performed on both the input image and the n-layer image:41.TEXTURE DESCRIPTOR EXTRACTIONThe following two texture descriptors are extracted from the gray-level image (note: RGB images are converted to gray-level images) and their processed images: canonical LBP and LPQ.n-LAYER DESCRIPTOR EXTRACTIONThe following descriptors (which are the n-layer versions of the previous ones) are extracted from the n-layer image (or k-layer images, if a reduced number of layers is considered): VLBP, VLPQ, and LPQ-TOP.CLASSIFYClassification is performed separately for each descriptor using SVMs as the base classifier, with both linear and radial basis function kernels. For each dataset, the best kernel and the best set of parameters are chosen using a 5-fold cross-validation approach on the training data (note: we are only determining here which descriptors to combine). The SVM is implemented using the widely used tool LibSVM (available at http://www.csie.ntu.edu.tw/~cjlin/libsvm/). The features used to train and test the SVMs are normalized to [0, 1] using the training data (the testing set is always completely blind).NORMALIZE AND FUSE;Before fusion, the scores obtained by each classifier are normalized to mean 0 and standard deviation 1. The final decision is obtained by combining the resulting scores from each n-layer descriptor and its related texture descriptor (VLPQ+LPQ, LPQTop+LPQ, and VLBP+LBP) using the weighted sum rule with the same settings applied in all datasets (see Section 4).Fig. 1 illustrates the steps 1–6 above. The steps of an input image vary depending on its type as indicated by the color of the arrows: gray for gray-level images and white for colored images. It should also be noted that the dimension of the layer image is different in these two cases.

@&#CONCLUSIONS@&#
The aim of this work is to show the feasibility of building an n-layer image using different preprocessing approaches. These images are then described using multilayer descriptors, which form feature vectors that are used to train a set of SVMs. For assessing the performance of this approach, we run tests on several datasets, on both color and gray-level images, using both LPQ and LBP texture descriptors, and three multilayer descriptors based on LPQ and LBP: viz. VLBQ, VLBP, and VLB-TOP. Our results show that n-layer and single-layer descriptors can be combined for outperforming the standard single-layer approaches. This conclusion is confirmed using the Wilcoxon signed rank test with a p-value 0.1.Since the performance of n-layer descriptors is strictly related to the order of the considered layers, we plan in the future to study how this order influence performance with the aim of designing a selection/ranking approach for selecting/ranking the layers most useful for classification purposes. Another feasible way to improve classification accuracy is to cluster layers into several groups, extract descriptors from each group, and then exploit fusions to improve classification performance.None declared.