@&#MAIN-TITLE@&#
An improved AEA algorithm with Harmony Search(HSAEA) and its application in reaction kinetic parameter estimation

@&#HIGHLIGHTS@&#
An improved AEA algorithm with Harmony Search (HSAEA) is proposed.Estimation of distribution algorithm is used to generate the compared population in AEA.The moving step length in AEA is improved to vary with different phases of the iteration.Harmony search is introduced to improve the quality of population.

@&#KEYPHRASES@&#
Alopex,EDA,Step length,Harmony search,Parameter estimation,

@&#ABSTRACT@&#
Alopex-based evolutionary algorithm (AEA) is one kind of evolutionary algorithm. It possesses the basic characteristics of evolutionary algorithms as well as the advantages of gradient descent methods and simulation anneal algorithm, but it is also easy to trap into a local optimum. For the AEA algorithm, the unreasonably settings of compared population and step length are two typical drawbacks, which lead to the lack of communication between individuals in each generation. In this paper, estimation of distribution algorithm (EDA) is employed to generate the compared population. Then the moving step length in AEA is improved to vary with different phases of the iteration during the actual operation process. And more importantly, harmony search algorithm (HS) is introduced to improve the quality of population of every generation. By compared with original AEA, the performance of the improved algorithm (HSAEA) was tested on 22 unconstrained benchmark functions. The testing results show that HSAEA clearly outperforms original AEA for almost all the benchmark functions. Furthermore, HSAEA is used to estimate reaction kinetic parameters for a heavy oil thermal cracking three lumps model and Homogeneous mercury (Hg) oxidation, and satisfactory results are obtained.

@&#INTRODUCTION@&#
Alopex (Algorithm of Pattern Extraction), firstly proposed by Harth in 1974, is a stochastic and parallel optimization algorithm used to solve combinatorial optimization and pattern matching problems [1]. It has the characteristic of both stochastic and heuristic searching methods. The algorithm is inspired by the influence of previous change of independent variables with respect to the objective function. Meanwhile, with the effect of the process parameter called “temperature”, the algorithm changes gradually from the stochastic search process into a deterministic one, which helps to find the best solution quickly. In recent decades, much attention has been paid to this algorithm. The heuristic idea of Alopex has been successfully applied to the field of engineering optimization. Such as, it was used to make all the interconnection strengths update synchronously and optimize neural network controller [2]. Alopex had also been applied to the sports medicine domain [3].However, the basic Alopex algorithm is a single point iterative algorithm, namely only one point can be updated every iteration, which makes the algorithm work less efficiently [4]. Xue et al. had used Alopex strategy to improve other evolutionary algorithms. Such as, a hybrid genetic-alopex algorithm (GAA) [5] was proposed to enhance the algorithm to converge at the global optimum. The GAA had been used to optimize waste interception and allocation networks [5]. Li et al. developed a PSO-Alopex algorithm by introducing Alopex into particle swarm optimization (PSO) algorithm to train a BP neural network [6]. Xu et al. improved differential algorithm (DE) based on Alopex to overcome the disadvantages of DE [7]. Inspired by the traditional evolutional algorithm, especially swarm intelligence, Li and Li proposed an Alopex-based evolutionary algorithm (AEA), with combination of the computation method of Alopex and the concept of swarm evolutionary [8]. In AEA, two populations are generated every generation, and then they evolve through Alopex operation. In order to overcome the demerit of basic Alopex that it was somewhat blind to set the annealing temperature artificially, a new strategy about adaptive adjustment of the annealing temperature was proposed. The annealing temperature calculated and changed automatically with the parameters of the evolutionary process. The testing results [8] on benchmark functions showed that the AEA was approximately equal to DE and better than HGA3 (an improved GA algorithm by Deep and Das [9]) and PSO.In this paper, an improved algorithm (HSAEA)which combines HS and AEA is developed. Experiment results show that the new algorithm is an effective optimizing method. The remainder of the paper is organized as follows. Theoretical backgrounds including AEA algorithm, EDAs and HS algorithm are reviewed in Section 2. Section 3 details the improved algorithm, HSAEA. Section 4 analyzes the testing results with 22 benchmark functions of HSAEA by comparing with AEA. Then the applications of the parameters estimation for a heavy oil thermal cracking three lumps model and Homogeneous mercury (Hg) oxidation are demonstrated in Section 5. Lastly, Section 6 outlines our conclusion and future work.The Alopex-based evolutionary algorithm (AEA) was firstly proposed by Li and Li [8]. This algorithm merges the advantages of evolutionary algorithms and Alopex. A trial individual is generated by adding (or subtracting) the weighted difference vector between two chosen individuals to (or from) the first individual according to a probability. If the trial individual yields a better result than the first original individual, it will substitute for the first one. The key idea of AEA is that the moving direction of trial individual is determined by the cross-correlation of the two chosen individuals.Take a minimization of the function as an example. Suppose that there are two populationsG1tandG2twhich have the same dimension of the individuals at iterative time t.Xit=xi1t,xi2t,⋅⋅⋅,xiNtandYit=yi1t,yi2t,⋅⋅⋅,yiNtare the i-th individuals inG1tandG2t, respectively. The equations related can be described as follows:(1)ΔFit=FXit−FYitwhere, i=1,2,…,K (K is the individual number in one population). F(·) is the objective function of optimizing problem.ΔFitis the difference of the two objective function values.(2)Cijt=xijt−yijt⋅ΔFitwhere, j=1,2,…,N (N is the number of variable dimension),Cijtis the correlation-coefficient between the variables and the corresponding function value.(3)Tt=1KN∑i=1K∑j=1NCijt(4)Pijt=11+eCijt/Tt(5)Δijt=1,ifPijt≥rand(0,1)−1,otherwisewhere, Ttis the annealing temperature andPijtis the probability. “e” is the Natural base.Δijt, determined by the probabilityPijt, denotes the variable changing direction. “rand(0,1)” is a random number which subjects to uniform distribution between 0 and 1.(6)xijt′=xijt+xijt−yijt⋅rand0,1⋅Δijt(7)xijt+1=xijt′,ifF(Xijt′)<F(Xijt)xijt,otherwisewhere,xijt′is the trial variable, andxijt+1is the variable in G1 at iterative time (t+1).AEA is one kind of swarm intelligence evolutionary algorithms. The two populations are generated every generation, and executed the operations according to Eq. (1)–Eq. (7). The computation procedure of the algorithm is described as follows: SupposeG1tis a population at time t.G2t, with the same dimension of the individuals asG1t, is generated by rearranging the individual order in populationG1t. Calculate a correlation-coefficient by multiplying the difference between the two variables and their objective function differences. The probabilityPijtwhich determines the directions where all variables of every individual in populationG1tmove is calculated. Because of the probability, each variable of individual does not always move to the gradient descent direction, and it may have the chance to move to the opposite direction. In this case, the algorithm owns better ability to climb the hill, which may give the algorithm more chance to escape from local minima. The detail steps of the algorithm can be referenced in [8].Estimation of distribution algorithm (EDA) was first proposed by Mühlenbein and Paaß in 1996 [10]. It is a population-based stochastic heuristics search strategy, and used as an extension of genetic algorithms. In EDAs, a number of individuals are created and computed every generation until an optimal solution is achieved. Unlike other evolutionary algorithms, EDAs have some unique advantages which are described as follows. Firstly, EDAs evolve from one generation to the next by estimating the probability distribution of candidate solutions and sampling the built model. Then the evolutionary trends of all individuals can be described at a macro level. Secondly, EDAs provide new tools to solve some complex optimum problems. Because EDAs can clearly represent the dependencies between individuals by using a probability model, they are more efficient to solve the multi-variable coupled and nonlinear optimum problems. Thirdly, EDA is a combination of statistical learning theory and random optimization algorithm. The hybrid of EDAs with other algorithms will provide a new idea for the research of optimization algorithm [11].Generally, the evolutionary process of EDA can be described as follows. Firstly, an initial population is generated with a fixed number of individuals. The objective function values of all individuals are calculated. Then a subset of individuals is selected from the initial population to form an intermediate population. The probability distribution of individuals given in intermediate population is estimated. Finally, a new population is generated by sampling from the distribution of the intermediate population. The steps above are repeated until a stop criterion is verified.The two major problems in EDAs are how to build a probability model and how to sample individuals. Bayesian networks and multivariate Gaussian distribution are common probabilistic models which are used in EDAs [12]. And Monte-Carlo method [13] is one of the most widely used sampling strategies. This method will sort all the individuals in intermediate population by their objective function values. Then the individuals with better functions values are in front and have more chance to be selected. In this way, the data dependencies and the structure found in the best individuals can be transferred into new population. EDAs have a wide range of applications, such as in the graph matching [14] and the Bayesian networks [15]. In this paper, it is used to generate the compared population in AEA.Harmony Search algorithm, imitating the improvisation process of musicians, was firstly introduced and developed by Geem et al. [16]. When a musician improvises, the aesthetic value referred to as the fitness function is determined by a set of pitches produced by the music instruments (variables) involved. The musician seeks to produce aesthetically pleasing harmony (the optimum solution) as determined by his/her aesthetic perception inferred from rehearsals (iterations). In spite of its youth, HS has been used in many practical applications and is attracting more attentions. A global HS creates new harmonies using the global best harmony in the HM [17]. A local HS periodically regroups the HM, applies HS independently within each group, and uses the group-best harmony to create new harmonies for each group [18]. In addition, HS has also been successfully hybridized with other metaheuristic methods to collaboratively boost the optimization performance. Such as, Chakraborty et al. proposed an improved variant of the classical Harmony Search metaheuristic by blending with it a difference vector-based mutation operator borrowed from the DE family of algorithms [19]. The pseudo-code of HS can be described as follows:Harmony search (HS)BeginCreate and randomly initialize a harmony memory (HM) with HM size (HMS)Repeat MaxImp timesBeginImprovise a new harmonyXnewtfrom the HM as followingfor (j=1 to N) doif (r1<HMCR) thenXnew,jt=Xat(j)if (r2<PAR) thenXnew,jt=Xnew,jt+r3BWendifelseXnew,jt=LBj+r(UBj−LBj)endifendforUpdate the HM asXworstt=Xnewtiff(Xnewt)<f(Xworstt),worst=argmaxifXitEnd RepeatEnd HSIn the pseudo-code of HS, the variable a is an integer number in the range of 1 to HMS. The variables r1, r2, r3, and r are random numbers between 0 and 1. Pitch adjusting rate (PAR) and bandwidth (BW) have a profound effect on the performance of the HS. Mahdavi and co-workers [17] proposed an improved harmony search (IHS) which dynamically updates PAR and BW according to Eq. (8) and Eq. (9):(8)PAR(t)=PARmin+PARmax−PARmintmaxt(9)BW(t)=BWminexp(ln(BWmin/BWmax)tmaxt)where, t is the current number of iterations and tmax is the maximum number of iterations. PARmin and BWmax are the minimum and maximum adjusting rates, respectively. BWmin and PARmax are the minimum and maximum bandwidths, respectively. A large number of experiments and studies show that the IHS has better optimization performance than the HS in most cases [20]. In this paper, the IHS was introduced to improve the AEA algorithm.As mentioned above, two populationsG1tandG2tare generated at each iteration time. The heuristic information used in AEA is obtained by computing the two populations. So the quality of the two populations has an effect on searching efficiency. In AEA, the initial populationG1tis randomly generated according to the population size and variable dimensions. The compared populationG2tis generated by rearranging the individuals order in populationG1t. The individuals in two populations are the same. Due to the randomness and finiteness of the individuals inG1t, it is easy to trap into local optima.In this paper, EDA is employed to generate the compared populationG2t. In order to explain the process clearly, take a minimization of the function as an example. Firstly, the function values of all individuals inG1tare calculated. The individuals are sorted by their objective function values in ascending order. The sorted individuals are numbered consecutively from 1 to K. Secondly, estimate the probability distribution of the individuals given inG1t. The probability value of one individual inG1tis calculated by using its number to divide the sum of the numbers. This calculation can be described as follows:(10)PM(i)=NUM(i)∑i=1kNUM(i)where, NUM(i) and PM(i) are the number and probability value of the i-th individual inG1trespectively. By this model, the individuals with smaller objective function values will have larger probability values.Finally, generate the compared populationG2t. The initial and compared populations can be described asG1t=(DX1t,DX2t,…DXjt,…,DXNt)andG2t=(DY1t,DY2t,…DXjt,…,DYNt). Where,DXjtandDYjtdenote the variables in the i-th dimension of the two populations, respectively. The variables inDYjtare generated by sampling from the correspondingDXjt. Each variable inDXjthas the same probability value with its corresponding individual. The Monte-Carlo method is used to sample the variables inDXjt, so the variables with large probability values will have more chance to be selected into the compared population. For example, the variables inDY1tare generated by sampling from theDX1t. The variables inDX1tare selected with different chances. The operations above are repeated until all the variables inG2tare generated.The individuals inG2tare different from those inG1t. EDA transfers the better variables which are found in the best individuals into the compared population. The diversity of the compared population is increased. On the other hand, the compared populationG2tmay miss the best individuals inG1t. In order to avoid this problem, some of the worst individuals inG2tare replaced by the best individuals of the same number inG1t. The number of the replacement is K/2. This method is generally known as “elitist strategy” [21–22]. The pseudo-code of generating the compared populationG2tis described as follows:EDABeginGenerate an initial populationG1trandomlyCalculate the objective functions values of all individuals inG1tSort the individuals inG1tin ascending order and number them from 1 to KCalculate the probability values PM(i) of each individual given inG1tGenerate the variables inDYjtby sampling from the correspondingDXjtRepeat until all variables inG2tare generatedReplace K/2 worse individuals inG2twith the same number of better individuals inG1tReturnG2tEndIn AEA, the searching process is similar to Simulated Annealing (SA) algorithm to some extent. Two parameters “T” and “P” are used to control the step length and directions. Take a minimization of the function as an example, if the objective function value decreases at the previous moving direction, it is expected that the variables can keep moving along the same direction. This operation makes the algorithm have greater opportunity to accelerate convergence. Meanwhile, because of the probability, there is still opportunity to search the opposite direction of the anterior search to avoid trapping in local minima.But it is not reasonable that the range of step length is the same for the variables moving forward and backward. Actually, AEA is a hybrid algorithm of deterministic search and stochastic search. Each of the two searches plays a key role at a certain moment, which is determined by the probability “P”. The deterministic search is similar to the gradient descent method. In order to accelerate the convergence process and search more efficiently, individuals are expected to walk along the gradient descent direction continually. In this case, the algorithm searches in a smaller range to avoid missing the global optimum. It must be noted that the gradient descent direction is not always the right direction to the global optimum but to a local optimum. In order to help the algorithm jump out of local minima as soon as possible, the search direction would have a certain probability to be the opposite of gradient descent. In this case, a larger range for individuals moving would be provided to ensure that they can escape from local minima. Otherwise, it is easy to swing back and forth in the bottom of one valley, and finally converge to a local minimum. The individuals can also jump out of a local minimum with more iterations, but it is not beneficial to global search. In this paper, the change of independent variables with respect to the objective function is considered. The Eq. (5) can be set as follows:(11)Δijt=αt,ifPijt≥rand(0,1)−βt,otherwisewhere, αtand βtare both real number, and 0≤αt≤1, βt≥1.Furthermore, at the beginning of the evolutionary process, individuals in population are scattered in a large search scope. They move towards the direction of global optimum fast. An individual may step backward for continuous several times with the probability. The factor βtwhich is used to control the stepping backward should be a little larger. At the same time, the factor αtwhich is used to control the stepping forward should be a little smaller. In the late stage of evolutionary process, the majority or all individuals have gathered around an optimum. In order to speed up the convergence, the factors βtand αtare respectively changed to be smaller and larger. In this way, the individuals not only tend to search along the direction of global optimum finely, but also still have opportunity to jump out of the local minima.In this paper, αtand βtare set as follows:(12)αt=αLt+(αHt−αLt)⋅t/tmax(13)βt=βLt+(βHt−βLt)⋅(1−t/tmax)where, t is iteration time, tmax is the maximal iteration times.αLtandαHtare the low bound and high bound of αt, respectively.βLtandβHtare the low bound and high bound of βt, respectively. After a number of experiments, we setαLt=0.1,αHt=0.7;βLt=1.3,βHt=1.7. It is clear that in the whole iterative process, αt increases from 0.1 to 0.7, while βtdecreases from 1.7 to 1.3. Both of them can be calculated by using the Eq. (12) and Eq. (13), respectively.In the original AEA, the heuristic information is obtained from the change of independent variables with respect to the objective functions. In order to generate a trial individual, the weighted difference vector between two individuals is added (or subtracted) to (or from) the first individual with a probability. In this way, the convergence speed is improved. But how, the individuals are easy to trap into local optima. In this paper, the HS algorithm is employed to overcome the problem.In the process of generating a vector, the different evolutionary algorithms always have the same characteristic that only seldom parent vectors are considered. For example, in genetic algorithm (GA), only two individuals from old population are considered to produce a new vector [23]. In particle swarm optimization (PSO), each particle adjusts its flying according to its own flying experience and its companions’ flying experience [24]. In Differential Evolution (DE), a donor vector is generated by adding the weighted difference of the two parameter vectors [25]. All of them consider seldom information. In this paper, the HS algorithm is employed to generate a new vector through considering all of the existing vectors. It contrasts sharply with the conventional evolutionary approaches. The use of HS algorithm will increase the robustness and flexibility of the underling search mechanism and ensures better solutions [26].Hence, HS algorithm is employed to improve the search mechanism in AEA. In this paper, it is used to improve the swarm after Alopex operator in every generation. As discussed above, the detailed implementation steps of HSAEA algorithm can be described as follows:Step 1: Set the number of individuals in population (K), the number of variable dimension (N), function optimization goal, maximal iteration times, and HS parameters: MaxImp, HMCR (Harmony memory consideration rate), PARmax, PARmin, BWmax and BWmin.Step 2: Set iteration time t=1 and initialize the populationG1twithin the search scope randomly.Step 3: Calculate the individual's objective function values in populationG1t.Step 4: Estimate the probability distribution of individuals given inG1tby Eq. (10). The initial population and compared population can be respectively described asG1t=(DX1t,DX2t,…DXjt,…,DXNt)andG2t=(DY1t,DY2t,…DXjt,…,DYNt). The variables inDYjtare generated by sampling from the correspondingDXjt. Repeat the operations above until all variables inG2tare generated. Then replace K/2 worst individuals inG2twith the same number of best individuals inG1t.Step 5: SupposeG1t=(X1t,X2t,…XKt)andG2t=(Y1t,Y2t,…YKt)denote the populationsG1tandG2t. For the variablexijt(i=1,2,…,K;j=1,2,…,N)of the individualXit(i=1,2,…,K)from populationG1tand the corresponding variableyijt(i=1,2,…,K;j=1,2,…,N)of the individualYit(i=1,2,…,K)from populationG2t, calculate the difference(xijt−yijt), and then compute difference[F(Xit)−F(Yit)]of the two objective function values.Step 6: Calculate the cross-correlationCijt(i=1,2,…,K;j=1,2,…,N)between the variablexijt(i=1,2,…,K;j=1,2,…,N)and its corresponding variableyijt(i=1,2,…,K;j=1,2,…,N)according to Eq. (2).Step 7: According to Eq. (3) and Eq. (4), calculate the annealing temperature Ttand the probabilityPijt(i=1,2,…,K;j=1,2,…,N).Step 8: Calculate the value of αtand βt, according to Eq. (12) and Eq. (13), respectively.Step 9: Determine the walking direction ofΔijt(i=1,2,…,K;j=1,2,…,N)according to Eq. (10), and then generate a trial variable(xijt)′(i=1,2,…,K;j=1,2,…,N)according to Eq. (6).Step 10: Update the population according to Eq. (7). All the newly generated trial variables form a new trial individual(xit)′(i=1,2,…,K). Calculate the function value of(xit)′(i=1,2,…,K), if the objective value of(xit)′(i=1,2,…,K)is better than that of corresponding individualXit(i=1,2,…,K)inG1t, the trial individual(xit)′(i=1,2,…,K)will replace the old corresponding oneXit(i=1,2,…,K). Otherwise,Xit(i=1,2,…,K)will remain in the populationG1t.Step 11: RegardG1tas HM. Generate a uniform random number, r1, in the rang [0,1]. If r1 is bigger than HMCR, go to Step 13, else generate a integer random number r2 from {1,2,…,K}. A new harmony(Xnewt)generates.Xnewt=[xr2,1t,…,xr2,Nt].Step 12: Calculate PAR and BW according to Eq. (8) and Eq. (9), respectively. Generate a uniform random number, r3, in the rang [0,1]. If r3<PAR,xnew,jt=xnew,jt+(2⋅rand(0,1)−1)⋅BW. Go to Step 14.Step 13: Generatexnew,jt(j=1,2,…,N)by re-initialization in the search space.Step 14: Update theG1tasXworstt=XnewtifF(Xworstt)>F(Xnewt). Where,F(Xworstt)=max[F(Xit)].Step 15: If the MaxImp meets, go to Step 16, otherwise, go to Step 11.Step 16: Output optimizing result if termination criterion meets, otherwise, t=t+1 and go to step 4.The pseudo-code of HSAEA algorithm can be described as follows:HS-AEABegin:Initialize all related parametersInitialize the populationG1tRepeat until terminal condition meetsBeginGenerate the compared populationG2taccording to EDACalculate cross-correlationCijtCalculate the annealing temperature Ttand the probabilityPijtCalculate αtand βt, and then determine the walking direction ofΔijtUpdate the populationRegardG1tas HM, calculate the objective function value of each harmony vectorRepeat MaxImp timesBeginImprovise a new harmonyXnewtfrom the HMUpdate the HMEnd RepeatEnd RepeatEnd HSAEAIn literature [8], the researchers conducted a set of tests with one improved Genetic Algorithm (HGA3) [9], Particle Swarm Optimization (PSO) [27], Differential Evolution (DE) [28] and AEA. The results in literature [8] show that AEA is approximately equal to HGA3 and DE on the successful runs. As for ABFV (the average best function value) which is the characterization of individual quality, AEA is better than PSO, DE and HGA3. In this paper, the performance of HSAEA is tested and compared only with AEA. Table 1lists a set of 22 unconstrained benchmark functions selected from the literature [9]. In Table 1, both the mathematical formulas and the bounds of variables for each function are described. In fact, these benchmark functions are commonly used for comparing evolutionary algorithms because they have many different types of characteristics which can help to evaluate the performance of the algorithms from different aspects. In this paper, all the algorithms used for the experiments are programmed in MATLAB. And all the experiments are carried out on a LENOVO computer (T400) with an Intel(R) Core(TM) 2 Duo CPU, P8400 with frequency 2.26GHz, 3GB RAM under Windows 7 operation system.In order to make a fair comparison, a group of experiments are conducted as the same as literature [8], in which the variable dimension for all the benchmark functions are fixed at 10 (N=10). And all the numbers of individuals in population for different functions are fixed at 100. Additionally, HS parameters are shown in Table 2.Here, a total of 100 runs are conducted with randomly initial populations for each test function. According to the literature [9], a run is regarded as a successful one if the value obtained by the algorithm is within 1% accuracy of the known optimal individual. But particularly for three functions including Rosenbrock, Schwefel, and Neumaier, the accuracy is set as 15, 0.4 and 2, respectively. The termination condition is a maximum of 2000 generations or there is no improvement observed in the best individual in consecutive 100 generations. In Table 3, there lists the rate of success (RS), the average best function value (ABFV) and standard deviations (SD) of 100 independent runs of the two algorithms. Particularly, the ABFV and SD are accounted for successful runs only, while unsuccessful runs are not considered and the never succeed one finally is denoted as (-) instead. Note that the results obtained by AEA are cited from the Li and Li's literature [8].From Table 3, the best results for these functions are assigned by the bold-face. To evaluate the overall performance, for the two algorithms, the numbers of no worse results (NNWR) compared with the other are listed at the bottom of the table. It is clear that HSAEA algorithm outperforms AEA algorithm on most functions. The number of no worse ABFV obtained by HSAEA is 20, while that by AEA is 10 only. Then the standard SD which represents the stability of the two algorithms is considered. The numbers of no worse SD obtained by HSAEA and AEA are 19 and 8, respectively. These results show that HSAEA is more efficient and more stable than AEA in optimization. At last but not least, the numbers of no worse RS obtained by the two algorithms are 21 and 18, which show that HSAEA has a higher successful rate to find solutions.Meanwhile, HSAEA especially works better on three functions which are difficult for AEA to find the global optimum. The characters of the three functions are described as follows. Ackley function (f1) is a continuous, multimodal test function, which is obtained by modulating an exponential function with cosine wave of moderate amplitude [29]. Due to the reason that it traps into local optima easily, this function can be used to test algorithms [30]. Griewank function (f4) has a global minimum value 0 and a lot of local minima [31]. It is generally used to test the convergence of optimization algorithms. Rastrigin function (f8) is generated by adding cosine modulation to the sphere function. Then it is highly multimodal and difficult to find the global optimum for algorithms. All of the three functions are suitable to be benchmark functions. On the other hand, because the global optimum of the Rosenbrock function (f9) is in a long, narrow and parabolic shaped flat valley [32], it is difficult for both HSAEA and AEA to find an optimal solution of the function.In order to deeply verify the significant difference between HSAEA and AEA, the Wilcoxon signed ranks test is employed [33]. It is a nonparametric test for pairwise comparisons. In this paper, the ABFVs represent the characterization of individuals quality and are used as the performance scores of the two algorithms. The main steps of Wilcoxon test are described as follows. Firstly, diis calculated as the difference of ABFV between the two algorithms on i-th out of 21 functions (the function f17 can’t be found an optimal solution by using HSAEA and AEA, so it is ignored). Secondly, the differences are ranked according to their absolute values. If some differences have the same absolute value, they will use the average rank of them. For example, there are three differences which have the same absolute value and they are assigned ranks from 1 to 3. Then they will use the average rank 2. The differences of ABFVs between HSAEA and AEA, and their ranks are shown in Table 4.From Table 4, the number of the differences which have the same absolute value (0) is 9. One of them will be ignored [33] and the surplus use the average rank of them, which is equal to 4.5. Then the other differences are ranked from 9 to 20. Finally, the ranks of the functions in which the HSAEA outperforms the AEA are summed. This sum is denoted as R+. In the same way, R− denotes the sum of the ranks for the opposite. In addition, the ranks of the functions in which the two algorithms have the same performances are split evenly among the two sums. The equations for the calculation can be set as follows:(14)R+=∑di>0rank(di)+12∑di=0rank(di)(15)R−=∑di<0rank(di)+12∑di=0rank(di)In this paper, the R+ and R− are 172 and 38, respectively. Then the TS(test statistic) is assigned the smaller of the two sums, TS=38. It will be compared with the value of the distribution of Wilcoxon for 20 degrees of freedom [34]. If the value of TS is smaller, the HSAEA outperforms the AEA with the p-value associated. The p-value can be calculated as 0.017 by using the R software package [35]. As a result, HSAEA has a significant improvement over AEA, with a level of significance α=0.05.HSAEA has achieved good results on the 22 unconstrained benchmark functions. Then the practical performance of HSAEA will be tested in reaction kinetic parameter estimation. In chemical process, many parameters which are important to the modeling process are non-measurable. Meanwhile, the number of them will increase with the complexity of the physic-chemical model [36]. In this case, a method which can give the best estimation of the non-measurable parameters is needed. HSAEA is a stochastic heuristic optimization algorithm which has ability to search in a large space. In order to test the search precision and search efficiency of HSAEA, two types of parameter estimation are employed in this paper.Cracking of heavy oil is a very complex chemical process. In order to simplify the reaction system, a heavy oil thermal cracking three lumps model is given by Song et al. [37]. The model is set up merging a large number of single compounds in the reaction system into a number of virtual components, according to the principle of similar kinetic characteristics.The material transformation process of heavy oil thermal cracking three lumps model is shown in Fig. 1. There are two hypothesizes as follows:(1) Heavy oil raw material H is methylbenzene in the residue with temperature more than 510°C. The heavy middle fractions W generated from Heavy oil pyrolytic reaction can be converted to pyrolysis gas, light fractions and condensation product L, simultaneously.(2) The secondary reaction that heavy middle fractions W are converted to condensation product L(W→L) is first order reaction.Given the condition of the same temperature, and the overall reaction order is 1, according to the reaction mechanism, dynamic equation of reaction can be derived as follows:(16)xL=KLP0e−ELP/TnL1−1−xnL+KWP0KWLP0e−EWP+EWLP/TnW−KWLP0e−EWLP/T1KWLP0e−EWLP/T1−1−xKWLP0e−EWLP/T+1nW1−xnW−1where x and T are the input variables and xLis the system output. There are eight parameters of the model to be estimated: KLP0, ELP, KWP0, KWLP0, EWP, EWLP, nW, nL.In literatures, researchers employed many algorithms to implement on parameter estimation of this model, including SGA, EGA [37], SPSO, CPSO [38], APSO [39] and so on. In order to make a fair comparison, all of 56 groups of data provided by Song et al. [37] are used as modeling samples and test samples, the maximal generation of HSAEA is set as 500, and the objective function is selected as:(17)minArer=1m∑i=0m−1xLi−xLi^/xLi^where,xL^iis obtained by using Eq. (16), m is the number of samples, Arer is the average self-checking relative error.In Table 5, the parameter estimation results given by various algorithms, including the values of the eight parameters and the average self-checking relative errors, are listed. From Table 5, it can be seen the average self-checking relative error (Arer) gained by HSAEA is 5.27%, smaller than all the other algorithms mentioned, which means that HSAEA can offer a fairly high fitting precision of the model.Mercury emissions from coal-fired power plants are highly dependent upon mercury speciation [40]. Thus, much attention has been paid to research efficient means to remove Hg from power plant flue gases.Commonly, Mercury in the flue gas is classified in three forms: elemental mercury (Hg0), oxidized mercury (Hg2+) and particulate bound mercury (HgP). The particulate bound mercury is usually trapped by ash collection devices within power plants, such as electrostatic precipitators (ESP), mechanical hoppers, or bag houses. Elemental mercury is relatively inert and difficult to capture because of its non-reactivity. It is also volatile at high temperatures and insoluble in water. Conversely, oxidized mercury is very water soluble and has an affinity for adsorbing onto particulate matter such as fly ash or on metal surfaces in the duct. As a result of these physical and chemical properties of Hg0 and Hg2+, the removal of mercury is enhanced when elemental Hg is converted to its oxidized form [41].In detail, the reaction mechanism, according to Agarwal and Stenger [41], is a five-reaction system to analyze mechanism of Hg oxidation. During the five reactions, two of them are reversible and three are irreversible.In order to model or optimize the process, ten parameters need to be determined. The ten parameters are A1, A2, A3, A4, A5, E1, E2, E3, E4, E5 which are used in five reaction kinetic equations. where, Aiis the pre-exponential factor, Eiis the activation energy of reaction kinetic equation i (i=1,2,…,5). The main purpose of this optimization problem is to determine the best values of these parameters so that difference between the values of reaction rates calculated from the five reaction kinetic equations, and those measured experimentally is minimized. Thus the objective function is described by(18)minEQS=1m∑i=1mc¯i−ci2where, EQS denotes the average of the squares of differences; ciis the experimental Hg conversion;c¯iis the Hg conversion calculated according to the five reaction kinetic equations; m is the number of data points.Then, HSAEA is employed to determine the kinetic parameters for homogeneous mercury oxidation in flue gas with the data measured experimentally and reported by Agarwal et al. [42]. To take a fair comparison, the maximal number of generations is also set as 1000. The objective function is used as a guideline to determine the best optimal parameters from the data set. The results are compared with those obtained by “fmin search algorithm”, a deterministic method which EQS uses the Nelder–Mead simplex algorithm [41], and ISDE [43] are listed in Table 6. The minimum obtained by HSAEA is 69.709. It is better than the reported results, 88.134 by Agarwal et al. [41] and 73.746 by ISDE. Hence, HSAEA is found to be more profitable operating than those reported in earlier literatures.In general, from the promising results of the two applications above, it can be concluded that HSAEA is competent to solve these real complex nonlinear cases.

@&#CONCLUSIONS@&#
AEA is proved to be an efficient and a powerful search and optimization algorithm in scientific and engineering fields. In the AEA algorithm, the compared population and the moving step length play key roles in the algorithm's performance. However the setting methods of them are not reasonable. In this paper, the compared population is generated by EDA. The moving step length is improved to be gradually adjusted during different phases of evolution. And more importantly, HS is introduced to improve the quality of swarm of every generation. Some benchmark functions and two real cases on parameter estimation are used to test the performance of AEA. The promising results demonstrate that HSAEA is superior to AEA in almost all indicators related.Despite of these promising results, there is still much room for improvement, such as, our future work is mainly focused on more reasonable controlling of the annealing temperature “T”, further improvement of the generation method of the compared population G2, how to solve the variable-interdependent problems such as Rosenbrock function, and so on.