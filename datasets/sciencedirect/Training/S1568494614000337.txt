@&#MAIN-TITLE@&#
Multi-criteria optimization classifier using fuzzification, kernel and penalty factors for predicting protein interaction hot spots

@&#HIGHLIGHTS@&#
An improved FKP-MCO classifier based on fuzzification method, kernel technique, and penalty factors is proposed and is used for predicting protein–protein interaction hot spots.A fuzzy contribution of each input point is introduced to MCO classifier for soft separation.The penalty factors are used to trade-off overfitting for the majority and underfitting for the minority in dataset.FKP-MCOC avoids solving the quadratic programming problem so as to gain efficiency.FKP-MCOC obtains better performance of predicting active compounds in bioassay and protein interaction hot spots than MCOC and other classifiers in stability, separation and generalization.

@&#KEYPHRASES@&#
Data mining,Fuzzy set,Multi-criteria optimization,Classification,Protein interaction,

@&#ABSTRACT@&#
In order to understand the patterns of various biological processes and discover the principles of protein–protein interactions (PPI), it is important to develop effective methods for identifying and predicting PPI and their hot spots accurately. As for multi-criteria optimization classifier (MCOC), it can learn a decision function from different classes of training data and use it to predict the class labels of unknown samples. In many real world applications, owing to noises, outliers, imbalanced class distribution, nonlinearly separable problems, and other uncertainties, the predictive performance of MCOC degenerates rapidly. In this paper, we introduce a fuzzy contribution to each instance of training data, the unequal penalty factors to the samples in imbalanced classes, and kernel method to nonlinearly separable dataset, then a novel multi-criteria optimization classifier with fuzzification, kernel and penalty factors (FKP-MCOC) is constructed so as to reduce the effects of anomalies, improve the class imbalanced performance, and nonlinear separability in classification. The experimental results of predicting active compounds and protein interaction hot spots and comparison with MCOC, support vector machines (SVM) and fuzzy SVM, the conclusion shows that FKP-MCOC significantly increases the efficiency of classification, the partition of active and inactive compounds in bioassay, the separation of hot spot residues and energetically unimportant residues in protein interactions, and the generalization of predicting active compounds and hot spot residues in new instances.

@&#INTRODUCTION@&#
Protein–protein interactions play critical roles in the elucidation of cellular activities, drug design, cellular engineering, and other biological processes. Many researchers [13,27] have showed that it is currently difficult to fully understand the principles dominating protein interactions because a great deal of experimental effort is required in practice. However, mutation studies [4,9] indicate that only a fraction of hot spots in protein interfaces is responsible for most of the binding free energy of the protein complexes. At the same time, advanced high-throughput technologies are providing a large amount of new PPI data to facilitate discovering the characteristics of PPIs.Thus a variety of methods have been proposed to address this prediction problem of protein interactions. Some researchers propose that the computational methods can be employed to predict hot spot residues based on chemical and physical features in protein interactions. These methods (e.g. [8,17,21,25,31,32]) mainly include computing energetic changes and identifying sequential or structural residues, but it is evident that there are certain limitations in the predictive performance of precision, efficiency and generalization.Recently, in order to effectively deal with the problem of predicting hot spot residues in protein interfaces, some data mining and machine learning techniques are used to build prediction models, for instance, Bayesian network [22], neural network [15], decision tree [12], logistic regression, SVM [6,19], fuzzy SVM [14], other methods [33,41], and so on.SVM based on statistical learning theory and optimization methods recently grew in popularity owing to its higher generalization power than that of some traditional methods [10,30,42]. The main idea of the SVM classifier is to separate instances from different classes by fitting a separating hyperplane that maximizes the margin among the classes and minimizes the misclassification simultaneously. For the linearly separable case, the hyperplane is located in the input space. Besides, for the nonlinearly separable case, kernel techniques are used to map the data from the input space into the feature space, and the hyperplane is positioned in the feature space.When SVM is employed to solve classification problems, each input point is treated equally and assigned certainly to one of different classes. However, in many practical applications, SVM is very sensitive to noise, outliers and anomalies in data so that the separating hyperplane severely deviates from the right position and direction. Thus several methods have been proposed to solve the problem by introducing a proper fuzzy membership function to SVM model [11,23,28,40,43,45,47]. Additionally, in real world applications, it is very usual that one class is more important than others, and that class distribution is imbalanced, resulting in a rapidly degenerating classification precision and accuracy. In order to effectively deal with the class imbalance problem, some penalty techniques based on cost-sensitive learning are used [24,44,48,49].MCOC is a new method which is mainly used to solve the classification in data mining and machine learning. The classifier considers the trade-off between the overlapping degree of different classes and the total distance between the input points and decision boundary, where the former should be minimized and the latter should be maximized simultaneously. Subsequently, a linear MCOC based on a comprise solution was proposed for the behaviour analysis of credit cardholders [37–39]. A multiple phase fuzzy linear programming approach was provided for solving classification problem in data mining [20]. And a penalized MCOC was presented for solving the imbalanced data problem in credit cardholder behaviour analysis [26]. Then a quadratic MCOC was proposed and used for credit data analysis [34]. A rough set-based MCOC was put forward and used for the medical diagnosis and prognosis [53,54], and a MCOC with fuzzy parameters is used to improve the its generalization, where the appropriate fuzzy membership function is introduced to MCOC, and the objective functions and the constraints are transformed into the fuzzy decision set, then the new MCOC with fuzzy parameters are constructed [53,54]. A kernel-based MCOC was given just like the use of the kernel method in SVM [51,52]. MCOC was used to analyze the behaviour of VIP E-mail users [51,52]. The above rough set-based MCOC was applied to prediction of the protein interaction hot spots [7]. In these applications, MCOC performed better than some traditional methods in data mining methods. MCOC and related models and algorithms are gradually developed as powerful tools for solving classification, regression and other problems.However, in many applications, for example, in bioinformatics and intelligent information processing, noise, outliers and anomalies are ubiquitous in datasets, where datasets may be also imbalanced class distribution or nonlinearly separable case at the same time. These uncertainties often lead to the result that some input points are difficult to correctly classify to one of the predefined classes. Consequently, when MCOC was trained on these datasets with uncertainties, it will degenerate into an inefficient, instable, and inaccurate one. In a word, MCOC has no ability to deal with the situation of noises, outliers, anomalies, class imbalance, nonlinear separable case and other uncertainties effectively.In this paper, by introducing a fuzzy contribution to each input point, kernel function to constraints, and penalty factors to objective functions of imbalanced classes in MCOC simultaneously, MCOC was reformulated into a new MCOC with fuzzification, kernel and penalty factors (FKP-MCOC). Thus the proposed method can improve the performance of the original MCOC in terms of stability, efficiency and generalization by reducing the effects of anomalies, imbalance and nonlinear structures significantly.The rest of this paper is organized as follows: First, Section 2 is a brief review of the SVM and fuzzy SVM classifiers. And Section 3 is the basic principle of MCOCs. Then the new FKP-MCOC is detailed in Section 4. The experiment on prediction of protein interactions and its hot spots and the results are demonstrated in Section 5. Finally, discussion and conclusions will be given in Sections 6 and 7 respectively.In this section the theory of SVM and fuzzy SVM classifiers is briefly reviewed. For a binary classification problem, suppose a training set T1={(x1, y1), …, (xn, yn)} is given, each input pointxi∈Rdbelongs to either of two classes with a label yi∈{−1, 1}, d is the dimension of the input space, and n is the sample size. If the dataset Tiis linearly or approximately separable, the separating hyperplane is defined as:(1)wTx+b=0wherew(w∈Rd)is the weight vector and b is a scalar.Besides, the supporting hyperplanes corresponding with two classes are denoted as the following formulation. The nonnegative slack variables ξiare introduced so as to deal with the case of constraint violations.(2)yi(wTxi+b)≥1−ξi,ξi≥0.In order to find an optimal separating hyperplane the margin between the two supporting hyperplanes should be maximized, at the same time, the total errors of misclassifications should be minimized, thus the optimization problem of the SVM classifier is expressed as:(3)minf(w,b)=12w22+C∑i=1nξis.t.yi(wTxi+b)≥1−ξi,ξi≥0.where C is a constant and called as a regularization parameter which makes a trade-off between margin maximization and misclassification minimization.The classification problem in Eq. (3) is often transformed into the corresponding dual problem by constructing a Lagrange function and the SVM classifier is expressed as:(4)ming(α)=12∑i=1n∑j=1nαiαjyiyj(xi⋅xj)−∑i=1nαis.t.∑i=1nαiyi=0,0≤αi≤C.whereα=(α1, …, αn) is the Lagrange multipliers and C is a parameter as a upper bound of the variables αi(i=1, …, n). For the optimal solutionα*of the problem (4) and its Karush–Kuhn–Tucker (KKT) conditions, the input pointsxiassociated with0<αi*<Care called as support vectors which lie on the supporting hyperplanes. In the case ofαi*=0the corresponding points are correctly classified while the input points withαi*=Care misclassified.For a new input pointxthe decision function below is used to predict its class label.(5)f(x)=sign∑i=1nαiyixi⋅x+bFor the nonlinearly separable case, the kernel function K(xi,xj), which is discussed in Section 4.2, is introduced to Eq. (4) and the SVM classifier can be denoted as:(6)ming(α)=12∑i=1n∑j=1nαiαjyiyjK(xi,xj)−∑i=1nαis.t.∑i=1nαiyi=0,0≤αi≤C.The corresponding decision function is defined as:(7)f(x)=sign∑i=1nαiyiK(xi,x)+bMany studies have shown that SVM is very sensitive to noise, outliers and anomalies, so fuzzy SVM was proposed to reduce the effects of anomalies in data [11,23,28,40]. Among these methods fuzzy SVM model is reformulated by associating the importance or contribution of each input point with a fuzzy membership si. Then decision function is constructed based on fuzzy support vector. Generally these methods can enhance the classification performance of SVM considerably.In the model of the fuzzy SVM classifier the fuzzy membership is defined as the ratio between the distance from an input point to its class representative point and the class maximum distance and the class mean is chosen as the representative point.For a binary classification problem, ifx¯yiis define as the mean of class yi, it can be written as:(8)x¯yi=1Nyi∑j=1Nyixj,yi∈{−1,1}whereNyiis the number of input point which belongs to class yi.Then the radius of class yiis denoted as:(9)ryi=maxxi−x¯yi2Let the fuzzy membership siof each input point be the linear function of the mean and radius of one class, we have(10)si=1−xi−x¯yi2(ryi+δ)where δ (δ>0) is an sufficiently small constant and used to avoid si=0.Thus a new training set T2={(x1, y1, s1), …, (xn, yn, sn)} with a fuzzy membership si(τ<si≤1) is obtained, where input pointxi∈Rdis partitioned by target variable yi∈{−1, 1} and the parameter τ (τ>0) is a user-defined threshold.The above Eq. (3) can be rewritten as the fuzzy SVM classifier below:(11)minf(w,b)=12w22+C∑i=1nsiξis.t.yi(wTxi+b)≥1−ξi,ξi≥0.where the term siξiis the weighted error with respect to an input pointxi. Obviously an input pointxiwith a smaller siis less important, that is, it can be considered as noise, outliers and anomalies.The fuzzy SVM classifier is transformed into the dual problem below by constructing a proper Lagrange function, we have(12)ming(α)=12∑i=1n∑j=1nαiαjyiyj(xi⋅xj)−∑i=1nαis.t.∑i=1nαiyi=0,0≤αi≤siC.Similarly, in the nonlinearly separable case the kernel function K(xi,xj) is used to replace the dot productxi·xj, thus the fuzzy SVM classifier is denoted as:(13)ming(α)=12∑i=1n∑j=1nαiαjyiyjK(xi,xj)−∑i=1nαis.t.∑i=1nαiyi=0,0≤αi≤siC.After support vector machines derived from statistical learning theory and optimization are successfully exploited in various domains, recently the MCO approaches are paid more attention to, especially in computational intelligence. The two methods share the same advantages of using the flexible objectives and constraints to fit decision function for separating the data points of different classes.Thus a general problem of data classification using MCOC can be described as follows: given the training set T1, in order to separate the different classes, some researchers have chosen the two measures: the overlapping degree deviating from the separating hyperplane, and the distance departing from the decision hyperplane [16]. In the first case an instance may lie in the wrong side of the hyperplane and be misclassified, while in the second case it may be in the right side and be correctly classified. Subsequently, Glover considered the above two factors in the classification models simultaneously [18].Let αi(αi≥0) be the distance where an input pointxideviates from the separating hyperplane, and the sum of αiis characterized by the functionf(α)=αpp(p≥1) which should be minimized with respect to αi, so we have(14)minf(α)=αpps.t.wTxi−b≥−yiαi,αi≥0,∀iObviously if αi=0, thexican be correctly classified. If αi>0, the input pointxiis misclassified.Similarly, let βi(βi≥0) be the distance where the input pointxideparts from decision hyperplane, then the sum of βiis characterized by the functiong(β)=βqq(q≥1) which should be maximized with respect to βi, and we get(15)maxg(β)=βqqs.t.wTxi−b≤yiβi,βi≥0,∀iAnd if βi=0, thexiis misclassified. If βi>0, the input pointxiis correctly classified.If we take the two measures in the above model (14) and (15) into account simultaneously, MCOC is defined as:(16)minf(α)=αppandmaxg(β)=βqqs.t.wTxi−b=yi(βi−αi),αi≥0,βi≥0,∀iIf C is the penalty factor of the objective function f(α), we may rewrite Eq. (16) as a new MCOC with the hybrid objective function h(α,β) with respect to αiand βias follows:(17)minh(α,β)=Cf(α)−g(β)=Cαpp−βqqs.twTxi−b=yi(βi−αi),αi≥0,βi≥0,∀iwherexiare given training data,wand b are unrestricted variables, i=1, 2, …, n.According to the above Eq. (17), we can calculate the term b (b∈R) and the weight vectorw(w∈Rd)of the decision hyperplane which is defined as:(18)wTx=bThus during the testing phase we may use the decision function to predict the class label of an input pointxas below:(19)f(x)=sign(wTx−b)If p=1 and q=1 in Eq. (17), we get a linear MCOC with the linear objective function which can be rewritten as:(20)minh(α,β)=C∑i=1nαi−∑i=1nβis.t.wTxi−b=yi(βi−αi),αi≥0,βi≥0,∀iSimilarly, if p=1, p=2 and q=1 in Eq. (17), and the term(1/2)w22is also added to the objective function, we get a quadratic MCOC with the quadratic objective function and the linear constraints which can be denoted as:(21)minh'(w,α,β)=12(w22+α22)+C∑i=1nαi−∑i=1nβis.t.wTxi−b=yi(βi−αi),αi≥0,βi≥0,∀iBesides, based on Eq. (20), the compromise solution approach has been used to improve the performance of the linear MCOC in business practices [37–39].Owing to its advantages: simplicity, high efficiency, and interpretability, MCOC has become more popular than some traditional methods in solving practical problems in recent years. But MCOC has the significant limitations of the unstability, poor generalization and insufficient flexibility, especially for the case of anomalies, class imbalance, nonlinear separability, and other uncertainties in data. Therefore, it is necessary that we do not use crisp but soft method to restructure MCOC so as to remarkably increase their robustness, prediction accuracy and efficiency in finding optimal decision function of classification problem.Finally, for multi-class classification problem, we may transform Eq. (20) or (21) into multiple one-against-all or pairwise binary classification problems of MCOC in the real world applications.The fuzzy characteristics of data, class imbalance, and the kernel tricks are firstly demonstrated and introduced to MCOC, and then FKP-MCOC and its algorithm are explained in details in this section.We know that each input point should be correctly assigned to one of classes using MCOC. However, in many applications, because of noises, outliers, and anomalies in data, some input points may be misclassified. Thus the contribution of each input point is very different, some points are more important than others in classification.According to the above viewpoint, an input pointxibelongs to one of classes with uncertainty. That is to say, there are three cases: (i) the input pointxibelongs certainly to one class; (ii) the input pointxidoes not belong determinately to one class; (iii) the input pointxibelongs to one class with the degree of fuzzy membership si(0<si<1). In the first case we have si=1, and si=0 is hold for the second case. Apparently for a specified constant τ (τ>0) which is a predefined threshold, if si≤τ, the input pointxiis considered to be unimportant and may be ignored or discarded from MCOC. Otherwise, it is regarded as the important part of constructing MCOC.Many real world datasets are not linearly separable. An appropriate basic function ϕ(x) is chosen to transform the input space where the dataset is not linearly separable into a higher dimensional feature space where the dataset is linearly separable [5,46]. In order to compute the dot product of basic function, (ϕ(x)Tϕ(y)), we may replace it by the kernel function K(x,y)=(ϕ(x)Tϕ(y)) of input points in the original input space.The kernel functions are often chosen as such functions: (i) the linear kernel K(x,y)=xTy; (ii) the polynomial kernelK(x,y)=(xTy+1)d(d≥2); (iii) the radial basis function kernel (RBF)K(x,y)=exp(−γx−y22)(γ>0); (iv) the sigmoid kernel K(x,y)=tanh(axTy+r) (a, r∈R). In this paper, the above four types of kernel function are used in experiments.In data mining and machine learning, the class imbalance problem is very common, many studies show that in that case a classifier tend to overfit the large class, at the same time, and underfit the small one [1,48,49]. Generally speaking, there are two ways to solve the problem: (i) the cost-sensitive learning, this kind of strategy used different penalty factors for different classes of data to misclassify minority-class samples costlier than majority-class samples. (ii) The synthetic minority over-sampling technique, the methods employed the over-sampling of minority-class to shift the decision boundary towards majority-class. In this paper, the cost-sensitive learning is used to form the penalized MCOC.Similar to SVM, MCOC is very sensitive to anomalies. We proposed FKP-MCOC by introducing an appropriate fuzzy membership function which is based on the distance between an input point and its class representative point where the mean of class is used as a representative point.Thus given the training set T2={(x1, y1, s1), …, (xn, yn, sn)} with the degree of fuzzy memberships si(τ<si≤1, τ>0). From Eq. (10) we know that the pointxiwith a fuzzy membership simay be unimportant one and be considered as noise, outlier and anomaly if si≤τ, otherwise the pointxiis regarded as positive contribution to classification. Besides, the parameter αi(αi≥0) is a measure of the misclassified input point in Eq. (20), so the term siαiis an error measure with different weight. By this way, the effects of noises, outliers and anomalies in data are reduced remarkably, at the same time, the stability of MCOC is improved considerably. Thus in the linearly separable case Eq. (20) is written as:(22)minC∑i=1nsiαi−∑i=1nβis.t.wTxi−b=yi(βi−αi),αi≥0,βi≥0,∀iwhere C(C>0) is a penalty constant of the misclassified input points.For the class-imbalanced case, if yi=−1, let C1(C1>0) be the misclassification cost or penalty factor of the negative class. Similarly if yi=1, let C2(C2>0) be the misclassification cost of the positive class. We can write Eq. (22) as:(23)minC1∑yi=−1siαi+C2∑yi=1siαi−∑i=1nβis.t.wTxi−b=yi(βi−αi),αi≥0,βi≥0,∀iwherexiare given,wand b are unrestricted variables, C1>0, C2>0, τ<si≤1, τ>0, i=1, 2, …, n.For the nonlinearly separable case, we suppose that ϕ(x) is a basic function mapping the input data into a higher dimensional feature space. According to the ideas of the kernel method, given the dataset T3={(ϕ(x1), y1, s1), …, (ϕ(xn), yn, sn)}, the weight vectorwcan be denoted as the linear combination of ϕ(xj) and yiwith respect to the positive coefficient λj(λj≥0), that is(24)w=∑j=1nλjyjϕ(xj)Plugging the above weight vectorwinto Eq. (23), we have(25)minC1∑yi=−1siαi+C2∑yi=1siαi−∑i=1nβis.t.∑j=1nλjyjϕ(xj)Tϕ(xi)−b=yi(βi−αi),αi≥0,βi≥0,∀i0≤λj≤C1,foryj=−1,0≤λj≤C2,foryj=1In the case the dot product (ϕ(x)Tϕ(y)) of basic function is replaced by the kernel function K(x, y), we get FKP-MCOC(26)minC1∑yi=−1siαi+C2∑yi=1siαi−∑i=1nβis.t.∑j=1nλjyjK(xj,xi)−b=yi(βi−αi),αi≥0,βi≥0,∀i0≤λj≤C1,foryj=−1,0≤λj≤C2,foryj=1Moreover, let α∗(α∗>0) be the initial value of f(α) (see Eq. (14)) which is decision maker's minimum expectation for misclassification, the difference|α*−(−∑i=1nαi)|should be minimized. Ifdα+(dα+≥0)is the positive deviation of the forenamed difference, anddα−(dα−≥0)is the negative deviation of the difference, according to Eq. (26) we have(27)α∗+C1∑yi=−1siαi+C2∑yi=1siαi=dα+−dα−Similarly, let β∗ (β∗>0) be the initial values of g(β) (see Eq. (15)) which is decision maker's maximum expectation for correct classification, the difference|β*−∑i=1nβi|should be minimized. Ifdβ+(dβ+≥0)is the positive deviation of the forementioned difference, anddβ−(dβ−≥0)the negative deviation of the difference, according to Eq. (26) we get(28)β*−∑i=1nβi=dβ+−dβ−Thus FKP-MCOC (see Eq. (26)) can be rewritten as:(29)mindα++dα−+dβ++dβ−s.t.α∗+C1∑yi=−1siαi+C2∑yi=1siαi=dα+−dα−β*−∑i=1nβi=dβ+−dβ−,dα+,dα−,dβ+,dβ−≥0,∑j=1nλjyjK(xj,xi)−b=yi(βi−αi),αi≥0,βi≥0,∀i,0≤λj≤C1,foryj=−1,0≤λj≤C2,foryj=1By solving Eq. (29), we can obtain the coefficients λj(j=1, 2, …, n). Plugging the coefficients λjinto Eq. (24), the weight vectorwcan be obtained. For all training pointsxi(i=1, 2, …, n) which satisfy αi=0 or βi>0, according to the decision hyperplanewTϕ(xi)=b, we haveb=∑j=1nλjyjK(xj,xi), then an average ofbis taken. The decision function is denoted as:(30)f(x)=sign(wTϕ(x)−b)=sign(∑j=1nλjyjK(xj,x)−b)According to the above descriptions, the overall process of the FKP-MCOC learning algorithm can be summarized into the following four steps:Step 1: Computing the fuzzy membership sifor each input pointxi(i=1, 2, …, n) with training set (see Eq. (10)):si=1−xi−x¯yi2/(ryi+δ), wherex¯yiandryias (8) and (9), and δ (δ>0) is a sufficiently small constant.Step 2: Solving Eq. (29) and getting the optimal solution λj(j=1, …, n) of FKP-MCOC based on training set.Step 3: Constructing decision functionwTϕ(x)=bby λj(j=1, …, n) with training set (see Eq. (30)):w=∑j=1nλjyjϕ(xj)andb=∑j=1nλjyjK(xj,xi), where the input pointxisatisfies αi=0 or βi>0. Thus the decision function is written asf(x)=sign(∑j=1nλjyjK(xj,x)−b), where b is the average ofb.Step 4: Testing an unknown samplexby the above decision function:If f(x)≥0,xis classified as positive class; otherwise,xis classified as the negative class.In this section a simulated dataset is designed and characterized by anomalies, imbalance class, and nonlinear separability so as to test the new FKP-MCOC. In this paper, since the linear MCOC and quadratic MCOC are only fit for the linearly or approximately separable case, FKP-MCOC, SVM, and fuzzy SVM with the RBF kernels are tested on the dataset. The dataset is used for binary classification with 83 points for negative class and 54 points for positive class as shown by diamonds and crosses in Fig. 1.In Fig. 1 the boundaries and margins found by the RBF kernel for SVM and fuzzy SVM with C=1000 and γ=0.5, respectively. At the same time, the RBF kernel for FKP-MCOC generates a better separating boundary with C1=1500, C2=1000 and γ=0.02, which shows that FKP-MCOC can achieve an excellent accuracy for uncertain data.In this section a systematic experiment to predict active compounds in a screening bioassay and PPI hot spot residues is presented, which include datasets used, experiment design, experimental results, comparison analysis and discussion.In the experiments, the first dataset D1 sourced from the UCI Machine Learning Repository (http://archive.ics.uci.edu/ml/) where the high-quality datasets are provided for researchers developing and evaluating their computational models. The dataset is from the differing types of screening that can be performed using HTS technology, where 21 datasets were created from 12 bioassays including both primary and confirmatory bioassays. Among them, AID604 is a primary screening bioassay for Rho kinase 2 inhibitors from the Scripps Research Institute Molecular Screening Centre [36]. The bioassay contains activity information of 59,788 compounds with a ratio of 1 active compound to 281 inactive compounds (1.4%). 57,546 of the compounds have known drug-like properties. And AID644 is selected as the first dataset, which is confirmatory screen of AID604. This dataset is partitioned into the training set and the test set. The former is composed of 54 active compounds (positive instances) and 111 inactive compounds (negative instances) respectively, while the latter comprises the corresponding13 and 28 compounds. Besides, each input point in the two datasets is composed of 100 conditional attributes and 1 decision attribute. The dataset is a mixture of Boolean, integer and real values and is listed in Table 1.Other two datasets are derived from crystallographic structures of 28 protein–protein complexes which are collected and stored in Protein Data Bank [2,3]. The dataset of protein interface residues is calculated from the 28 protein–protein complexes. Then 904 surface residues are collected as hot spots (positive instances) or energetically unimportant residues (negative instances). Here we used two definitions for hot spots, ΔΔG≥1.0 kcal/mol and ΔΔG≥2.0 kcal/mol, where ΔΔG is expressed as the change of the binding free energy. If ΔΔG≥1.0 kcal/mol is defined as hot spots, the interface residues are divided into 255 hot spots and 649 non-hot spots and form the second dataset D2. Similarly, the interface residues are divided into 107 hot spots and 797 unimportant residues and form the third dataset D3 if ΔΔG≥2.0 kcal/mol is considered as hot spots. At the same time, the two datasets are composed of 14 conditional attributes for describing the attributes of protein–protein interactions and 1 decision attribute for indicating whether the protein interface residue is hot spot or not.In the two datasets D2 and D3 the attribute includes the solvent-accessible surface area (ΔASA), the mutative rate of the solvent-accessible surface area (ΔASA%) when proteins change from monomer to complex, the polar (PB) and non-polar atoms (NPB) for a residue in bound state, the relative size of side chain of a residue (SSC), the type of the chemical nature of amino acid (CA, CB, CC), the type of secondary structure of the fragment where a residue is located (SSH, SSS, SSN), the number of hydrogen bonds (NHB) between surface residues across protein–protein interfaces, the contact degree of atoms (ACD), the number of the salt bridges (SB) across interfaces. If the types of conditional attributes are taken into account, there are 8 continuous attributes and 6 nominal attributes, so the datasets are the mixture of the different types of attributes. The two datasets are the benchmark for analyzing and predicting of protein interaction hot spots and are listed in Table 1.The three datasets are highly class-imbalanced, the number of the inactive compounds is 2.1 times more than that of active compounds in the dataset D1. The number of the energetically unimportant interface residues is 2.5 times more than that of the hot spot residues in the dataset D2. And the proportion of non hot spots to hot spots reached 7.4 times in the dataset D3. Owing to the inevitable errors in computing the values of above attributes, the three datasets may contain potential noise, outliers and anomalies.In the experiments, the training set Tr1 and the test set Ts1 can be directly obtained from the original dataset D1. Besides 200 hot spots and 200 energetically unimportant residues from the dataset D2 are randomly selected as the training set Tr2 and the remaining 504 residues are used for the independent test set Ts2. Similarly, 70 hot spots and 70 energetically unimportant residues from the dataset D3 are randomly selected as the training set Tr3 and the remaining 764 residues are held for the independent test set Ts3, respectively.Then the method of feature selection based on rough set is applied to the two training sets Tr2 and Tr3, and we get the minimal conditional attribute subsets. The 10-fold cross-validation method is employed to train FKP-MCOC based on three training sets, respectively. To evaluate classification performance, we tested these classifiers with the independent test sets and the averages of classification accuracies are calculated and reported. Besides, for the comparison analysis, MCOC, the SVM classifier, and the fuzzy SVM classifier are also trained with the same training sets and tested them with the same test sets.In the experiments five accuracy measures are used to evaluate the performance of these classifiers [8,12]: sensitivity (active compound or hot spot accuracy), specificity (inactive compound or non-hot spot accuracy), total accuracy, F1 Score, and MCC. These measures are defined respectively as following:Total accuracy (the total classification accuracy):(31)Total accuracy=TP+TNTP+FN+TN+FPSensitivity (the catch rate of the active compounds or the hot spots of interacting protein pairs, also called as Recall):(32)Sensitivity=TPTP+FNSpecificity (the catch rate of the inactive compounds or the non-hot spots of protein interactions):(33)Specificity=TNTN+FPF1 Score (the mixed measure of sensitivity and precision):(34)F1Score=2SN∗PRSN+PR=2TP2TP+FN+FPMatthew's Correlation Coefficient (the adjusted impacts of imbalanced dataset, MCC):(35)MCC=TP∗TN−FP∗FN(TP+FN)(TP+FP)(TN+FP)(TN+FN)1/2where true positive (TP) is the number of active compounds or protein interaction hot spots that are correctly predicted. False negative (FN) is the number of active compounds or hot spots of interacting protein pairs that are predicted as inactive compounds or non-hot spots, respectively. True negative (TN) is the number of inactive compounds or protein interaction non-hot spots that are correctly predicted. False positive (FP) is the number of inactive compounds or non-hot spots that are predicted as active compounds or hot spots. The F1 Score is the harmonic mean of recall and precision, where it takes into accounts effects of the two quantities of sensitivity and precision. Besides, the value of MCC is between −1 and 1, and higher MCC corresponds to better predictive performance of classifier. Finally, the area under curve (AUC) statistic is also a performance measure of classifier based on the area under an ROC (receiver operating characteristic) curve, where a classifier is preferred if its ROC curve is closer the upper-left corner, that is to say, with a large AUC value. The value of AUC lies in the interval [0,1], and a larger AUC means a better predictive performance of the classifier.All of the experiments are carried out using Matlab 7.0. The linear programming problem of linear MCOC and FKP-MCOC, and the convex quadratic programming problem of quadratic MCOC, SVM and fuzzy SVM are solved by utilizing Matlab optimal tools.Finally, the feature selection results, prediction results, and comparison analysis based on three different datasets of active compounds and protein interaction hot spot residues are presented separately in the following subsections.After the Johnson's algorithm of feature selection based on rough set [29,35,50,53,54]) is conducted on the training sets Tr2 and Tr3, respectively, the number of conditional attributes in the protein interface residues datasets is reduced, and the feature structures of the original datasets and reduced datasets are described in Table 2, respectively.Table 2 shows that 11 attributes or features are selected from the training set Tr2, while 10 attributes are selected from the training set Tr3. Besides, we find that the common three features (CC, SSN, SB) in two datasets are removed from the original datasets. Obviously the corresponding test sets Ts2 and Ts3 should be reduced to the same conditional attributes as the two training sets.Based on the training set Tr1 and the two reduced training sets Tr2 and Tr3, FKP-MCOC algorithms and other classifiers are trained using 10-fold cross-validation method, and then these classifiers are tested on the independent test sets Ts1, Ts2 and Ts3, respectively.For SVM, fuzzy SVM, MCOC and FKP-MCOC the different accuracies on the independent test set Ts1 are shown in Fig. 2. By using grid search the following parameters are set to: d=2 for the polynomial SVM (Poly. SVM), fuzzy SVM (Poly. FSVM), and FKP-MCOC (Poly. FKP-MCOC). γ=0.01 for the RBF SVM (RBF SVM) and FKP-MCOC (RBF FKP-MCOC), and γ=0.1 for fuzzy SVM with the RBF kernel (RBF FSVM). a=0.01and r=−0.01 for the sigmoid SVM (Sig. SVM), fuzzy SVM (Sig. FSVM), and FKP-MCOC (Sig. FKP-MCOC). Besides, C=10 for the penalty factor of SVM and fuzzy SVM with the respective linear (Linr. SVM, Linr. FSVM), polynomial, and sigmoid kernels and the quadratic MCOC (Quad. MCOC), and C=1000 for the penalty factor of the SVM and fuzzy SVM classifiers with the RBF kernel, respectively. C=1,000,000 for the linear MCOC (Linr. MCOC). Similarly, the parameters of FKP-MCOC (including Linr. FKP-MCOC) are set to: α∗=0.01 and β∗=10, 000 for the initial input values, C1=100 and C2=200 for the penalty factors, respectively.From the experimental results shown in Fig. 2, we can find that the predictive performance of our proposed FKP-MCOC is slightly better than others. According to the statistical results FKP-MCOC has the highest averages of total accuracy (88.97%), sensitivity (79.35%), specificity (93.16%), F1 Score (0.81), MCC (0.73), and AUC (0.84). The fuzzy SVM has the averages of total accuracy (83.79%), sensitivity (71.90%), specificity (89.58%), F1 Score (0.74), MCC (0.64), and AUC (0.80). SVM has the averages of total accuracy (83.95%), sensitivity (70.82%), specificity (90.36%), F1 Score (0.73), MCC (0.62), and AUC (0.79) while MCOC has the averages of total accuracy (81.71%), sensitivity (62.31%), specificity (88.32%), F1 Score (0.67), MCC (0.56), and AUC (0.79). Besides, the linear and RBF kernels for FKP-MCOC totally achieve better accuracy than SVM and fuzzy SVM classifiers with the corresponding kernels. Owing to the majority of inactive compounds in the screening bioassay, the linear, polynomial and RBF kernel for fuzzy SVM and SVM have better performance in specificity and AUC, and there is no statistically significant difference between them and KFP-MCOC.The predictive accuracies for SVM, fuzzy SVM, MCOC and FKP-MCOC on the test set Ts2 are shown in Tables 3–6, respectively. By using grid search the parameters are set to: d=2 for the polynomial SVM, d=3 for fuzzy SVM, γ=0.01 for the RBF SVM and fuzzy SVM, a=0.001 and r=−0.0001 for the sigmoid SVM and fuzzy SVM, C=200 for the penalty factor of SVM, C=100 for the penalty factor of fuzzy SVM, respectively. Similarly, the parameters of FKP-MCOC are set to: d=2 for the polynomial kernel, γ=0.01 for the RBF kernel, a=0.0001 and r=−0.0001 for the sigmoid kernel, α∗=0.01 and β∗=10000 for the initial input values, C1=30 and C2=60 for the penalty factors, respectively.From the experimental results shown in Tables 3–6, we can find that the performance of our proposed FKP-MCOC is slightly better than others. Generally FKP-MCOC has the highest sensitivity averaged 83.15%, F1 Score averaged 0.365, and MCC averaged 0.3175. Besides, the linear kernel for FKP-MCOC totally achieves better accuracy than the linear kernel for SVM and fuzzy SVM. Owing to the majority of energetically unimportant residues, the polynomial and sigmoid kernel for fuzzy SVM and the RBF kernel for SVM have better performance in total accuracy and specificity than that of FKP-MCOC.When SVM, fuzzy SVM, MCOC, and FKP-MCOC are tested with the test set Ts3, the different accuracies are shown in Tables 7–10. By using grid search the parameters are set to: d=2 for the polynomial SVM and fuzzy SVM, γ=0.01 for the RBF SVM and fuzzy SVM, a=0.01 and r=−0.0001 for the sigmoid SVM and fuzzy SVM, C=200 for the penalty factor of SVM, C=100 for the penalty factor of fuzzy SVM, respectively. Similarly, the parameters of FKP-MCOC are set to: d=2 for the polynomial kernel, γ=0.01 for the RBF kernel, a=0.001 and r=−0.0001 for the sigmoid kernel, α∗=0.01 and β∗=10000 for the initial input values, C1=60 and C2=120 for the penalty factors, respectively.Comparing the experimental results shown in Tables 7–10, we can see that FKP-MCOC is slightly better than others in predicting protein interaction hot spot residues with average of 84.72%. For linear and sigmoid kernel, the total accuracy and specificity of FKP-MCOC is slightly superior to that of other classifiers. However, because of the majority of non-hot spot residues in data the polynomial kernel for SVM has the best performance for total accuracy, specificity, F1 Score, and MCC, while the RBF kernel for fuzzy SVM achieves the best the total accuracy and specificity.

@&#CONCLUSIONS@&#
In this paper, we proposed an improved MCOC method FKP-MCOC based on fuzzification, kernel and penalty factors for predicting active compounds in bioassay and PPI hot spot residues. FKP-MCOC extends the capacities of MCOC and avoids solving the convex quadratic programming problem of SVM and fuzzy SVM. FKP-MCOC is characterized by introducing a fuzzy membership to each input point. The improved classifier can reduce the effects of noise, outliers, and anomalies in data. The class-imbalanced penalty factors are used to trade-off overfitting majority-class and underfitting minority-class. At the same time, kernel function can transform a nonlinearly separable problem into a linearly separable one. FKP-MCOC was tested with the simulation data and three real world datasets. The experimental results show that FKP-MCOC is a more effective classifier for prediction of protein interaction hot spots and active compounds in bioassay, and has great potential as a prospective classification approach for other applications.