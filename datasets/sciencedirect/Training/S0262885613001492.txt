@&#MAIN-TITLE@&#
Bi-modal biometric authentication on mobile phones in challenging conditions

@&#HIGHLIGHTS@&#
We examine bi-modal (face/speaker) authentication in challenging mobile environment.We release new protocols (and data) with significant mismatch conditions (MOBIO).We study bi-modal and multi-algorithm fusion using generative modelling techniques.Multi-algorithm and multi-modal fusion provides a consistent performance improvement.The proposed bi-modal system significantly outperforms the state-of-the-art.

@&#KEYPHRASES@&#
Face authentication,Speaker authentication,Bi-modal authentication,Gaussian mixture model,Session variability,Inter-session variability,Total variability,I-vector,Fusion,

@&#ABSTRACT@&#
This paper examines the issue of face, speaker and bi-modal authentication in mobile environments when there is significant condition mismatch. We introduce this mismatch by enrolling client models on high quality biometric samples obtained on a laptop computer and authenticating them on lower quality biometric samples acquired with a mobile phone. To perform these experiments we develop three novel authentication protocols for the large publicly available MOBIO database. We evaluate state-of-the-art face, speaker and bi-modal authentication techniques and show that inter-session variability modelling using Gaussian mixture models provides a consistently robust system for face, speaker and bi-modal authentication. It is also shown that multi-algorithm fusion provides a consistent performance improvement for face, speaker and bi-modal authentication. Using this bi-modal multi-algorithm system we derive a state-of-the-art authentication system that obtains a half total error rate of 6.3% and 1.9% for Female and Male trials, respectively.

@&#INTRODUCTION@&#
Mobile phones have become an integral part of many people's daily life. They are used not just for telephonic communication, but also to send and receive emails, take photos or even have video conversations. This has led to the mobile phone being an inherently multimedia device, which often has a front-facing camera in addition to the standard microphone. Hence, it forms an exciting new device that allows researchers to explore the applicability of bi-modal (face and speaker) authentication in challenging mobile phone environments.This exciting challenge of bi-modal authentication in the mobile phone environment has begun to receive more attention. An international competition was organised in 2010 [1], where researchers evaluated state-of-the-art algorithms for face and speaker authentication using Phase I of the MOBIO database [2]. In this evaluation, enrolment was exclusively performed with mobile phone data. It was shown that a combination of these systems produced an impressive bi-modal authentication system. Since then other researchers have examined methods to perform face [3,4], speaker [5,6] and bi-modal [7,8] authentication in the challenging mobile phone environment.A theme common to some of the prior work on biometric authentication in a mobile environment is the idea of session variability modelling [5,4], which achieves state-of-the-art results for bi-modal authentication [8]. Session variability modelling aims to estimate and suppress any variability such as audio or image noise that may cause confusion between different observations of the same biometric identity. In [5] session variability modelling was used to cope with audio channel variability, while [4] introduced this concept to face authentication, where its application was supposed to reduce the impact of pose and illumination variation. Finally, in [8] state-of-the-art face and speaker authentication systems that used inter-session variability (ISV) modelling were combined to derive a state-of-the-art bi-modal authentication system. However, this prior work applied ISV modelling in the case of matched acquisition conditions, i. e., where biometric test samples are acquired using the same device as employed for client model enrolment. Furthermore, they did not use the most recent advances such as total variability (TV) modelling, which has been applied to speaker [9] and face [10] authentication.In this paper we explore three issues of applying bi-modal authentication to the challenging mobile phone environment. First, we examine the issue of mismatched conditions between enrolment and testing. In particular, we examine the effect of enrolling users on high quality biometric samples acquired with a laptop computer and then authenticating them using lower quality biometric samples acquired with a mobile phone. As a significant contribution, we develop three new protocols for the MOBIO database [2] with respect to prior work [1,2,4,8] that was exclusively using mobile phone data both for enrolment and testing.Second, we extend the work of [8] by examining the effectiveness of TV modelling for bi-modal authentication. Third, we show the effectiveness of multi-algorithm fusion to further improve the results for face, speaker and bi-modal authentication in the mobile phone environment. The final outcome of this work is the development of a state-of-the-art bi-modal (face and speaker) authentication system that improves upon the previous state-of-the-art [8] with a relative performance gain of 35% for Female and 27% for Male trials on the MOBIO database.The remainder of this paper is structured as follows: In Section 2 we outline the employed face and speaker authentication systems, while Section 3 combines these into bi-modal and multi-algorithm authentication systems. Section 4 presents the new protocols for the MOBIO database that are used in our experiments, which we discuss in Section 5. Finally, Section 6 concludes the paper.We examine the effectiveness of state-of-the-art Gaussian mixture model (GMM) based approaches for face, speaker and bi-modal authentication. GMMs have formed the basis of state-of-the-art speaker authentication systems for over a decade [11,9] and it was recently shown that incorporating session variability modelling into a GMM system produces state-of-the-art results for face authentication [12]. Also, the combination of GMM-based systems that use session variability modelling produces a state-of-the-art bi-modal (face and speaker) authentication system [8].When using GMMs and session variability for speaker and face authentication, the same underlying approach is taken. The main difference is how the feature vectors are extracted from the image (face) and audio (speech) samples. Below we describe the feature extraction process for both face and speaker authentication followed by a description of the GMM and the associated session variability modelling approaches that we examine.Two separate feature extraction processes are used for image (face) and audio (speech) data. For both modalities, a biometric sampleO(image or audio) is decomposed into a set O of K feature vectors (O={o1,o2,…,oK}), where each feature vector is of dimensionality M. This decomposition is performed in the spatial domain for the image data, and in the time domain for the audio data.For the image data, we rely on parts-based features that were proposed for the task of face authentication in [13]. These features have since been successfully employed by several researchers [14,15]. The key idea is to decompose the face image into a set of overlapping blocks before extracting a feature vector from each of them. The feature vectors extracted from these blocks are then considered as observations of the same signal (the same face), and can be modelled in a generative way.The feature extraction process is similar to the approach described in [16]. First, each image is rotated, scaled and cropped to 64×80 pixels such that the eyes are 16 pixels from the top and separated by 33 pixels. Second, to reduce the impact of illumination, each cropped image is preprocessed with the multi-stage algorithm of Tan & Triggs [17], using their default parameterisation. Third, 12×12 blocks of pixel values are extracted from the preprocessed image using an exhaustive overlap, leading to K=3657 blocks per image. Fourth, pixel values of each block are normalised to zero mean and unit variance, prior to extracting the M+1 lowest frequency 2D discrete cosine transform (2D-DCT) coefficients [13] and removing the zero frequency coefficient as it is redundant. Fifth, the resulting 2D-DCT feature vectors are normalised to zero mean and unit variance in each dimension with respect to the other feature vectors of the image. As in previous work [16,8], M was set equal to 44.For the audio data, observations are extracted at equally-spaced time instants using a sliding window approach. First, audio segments are denoised using the Qualcomm-ICSI-OGI front end [18]. Second, voice activity detection (VAD) is performed jointly using the normalised log energy and the 4Hz modulation energy [19]. The aim of the 4Hz modulation energy is to discriminate speech from other audio sources such as noise and music. An adaptive threshold is applied on both the 4Hz modulation energy and the normalised log energy. In our experiments, this approach provided a relative improvement of up to 16% compared to the common energy-based VAD. Third, 19 mel frequency cepstral coefficient (MFCC) and log energy features together with their first- and second-order derivatives are obtained by computing 24 filter bank coefficients over 20ms Hamming windowed frames every 10ms. This results in acoustic feature vectors of dimensionality M=60. Finally, feature normalisation based on cepstral mean and variance normalisation (CMVN) is applied on the remaining speech. The number of feature vectors K extracted from each audio sample depends on the duration of the sample and the number of segments that the VAD classifies to be speech.We use the same generative probabilistic framework that models the observed feature vectors using Gaussian mixture models (GMMs) for both image (face) and audio (speech) modalities. GMMs have been successfully applied first to speaker authentication [20,11] and then to face authentication [13,21,14–16]. One of the main challenges with GMMs is to reliably estimate a client model with limited enrolment data. This enrolment process is sensitive to the conditions, in which the data was captured. To address this issue, several session variability modelling techniques built on the GMM baseline have been proposed that constrain client models to be in a restricted subspace. In this work, we consider two approaches to session variability modelling, inter-session variability (ISV) modelling [22] and total variability (TV) modelling [9]. Both methods were initially proposed for speaker authentication [9,22] before being applied to face authentication [10,12]. In the remainder of this section, we first describe the GMM baseline system, followed by the more advanced ISV and TV techniques.The distribution of the observed feature vectors (face or speech) is modelled using a GMM. A GMM is the weighted sum of C multi-variate Gaussian componentsN:(1)poΘgmm=∑c=1CωcNo;μc,∑c,where Θgmm={ωc,μc,∑c}c={1,…,C} are the parameters of this distribution: the weights, the means and the covariance matrices, respectively.To use GMMs for authentication we need to learn a GMMSifor each subject i from a set of enrolment samples. One of the main challenges is that the number of enrolment images or audio recordings per client is usually limited, possibly to a single sample. In practise, it has been shown that for both speaker [11] and face authentication [14,15] an efficient enrolment method is to use a subject-independent prior GMMM, called the universal background model (UBM), and to adapt this prior to the enrolment samples of the subject i to generate the client modelSi. The UBMMis learnt beforehand by maximising the likelihood of observations extracted from a large independent training set of several identities using the iterative expectation–maximisation (EM) algorithm [23]. Afterwards, adaptation is achieved by using maximum a posteriori (MAP) estimation [20], where only the means of the UBM are updated, as this has been shown to be efficient for both modalities [14,15,20]. As in previous work [11,14–16] GMMs are assumed to have diagonal covariance matrices.A convenient and compact representation of mean-only MAP adaptation and other session variability modelling techniques is the GMM super-vector notation. This notation consists of grouping the parameters of the various Gaussian components of a GMM (weights, means or covariance matrices) into single large vectors or matrices. For instance, the mean super-vector m of the UBMMis obtained by concatenating the means μcof all its components: m=[μ1T,μcT,…,μcT]T. In [22] it was shown that mean-only MAP adaptation can then be written as:(2)si=m+di,where siis the mean super-vector of the GMMSiand diis a client-specific offset for subject i. This offset diis given by:(3)di=DziwithD=ΣT,where Σ is the variance super-vector of the UBM (recalling that covariance matrices are assumed to be diagonal),Tis a relevance factor that provides a weight for the prior UBM when performing MAP adaptation [20] and ziis a latent variable, which is assumed to be normally distributedN0I. In the following, since only the means of the UBM are adapted, we will use siand m to describe the client modelSiand the UBMM, respectively, by abusing the notation.Once a client model is enrolled, a test sampleOt(also called a probe sample) is authenticated against the model siby calculating a log-likelihood ratio (LLR) score with respect to the UBM m[11]:(4)hgmmOtsi=∑k=1Ktlogpotksi−logpotkm.With higherhgmmOtsivalues, the probability increases that the observations Otextracted from the sampleOtwere produced by the client model si.Recently, a linear approximation of Eq. (4), known as linear scoring[24], has been adopted in the speaker authentication literature and has also been applied to face authentication [16]. It relies on the mean centralised first order sufficient statistics of the UBM given the observations Ot, as given in Eq. (9) of [12]. This approximation was shown [24] to be orders of magnitude more efficient with no significant degradation in performance.As a final step, we also perform zt-score normalisation[25] due to the consistent performance improvements that this gives for both face [16] and speaker authentication [26].One problem of the GMM mean-only MAP adaptation approach is that the client model sican be difficult to estimate reliably with limited enrolment data as it is sensitive to the conditions in which the data was captured. Part of the reason for this is that there is no explicit model to capture and suppress detrimental variations such as audio or image noise. Session variability modelling aims to estimate and suppress the effects of within-client variations in order to create more discriminant client models. For the face modality, within-client variations include variations of pose, illumination or expression of samples of a given subject, whereas for the speaker modality variations are, amongst others, caused by the sensor (microphone) or the environment (background noise or acoustic conditions).Inter-session variability (ISV) modelling [22] and joint factor analysis (JFA) [27] are two session variability modelling techniques, used in the context of a GMM-based system, that have been successfully applied to both speaker [22,26,28] and face authentication [4,12]. Both techniques aim to estimate session variation like audio or image noise in order to compensate for it. This compensation for the estimated session variation is the key difference between ISV and the classic mean-only MAP adaptation. Note that for these experiments we have not examined JFA as it was shown empirically that ISV outperforms JFA for both speaker [8] and face [8,12] authentication.As in [22] it is assumed that session variability results in an additive offset to the mean super-vector siof the client model. This offset can be added directly to the normal mean-only MAP adaptation representation. Given the j-th biometric sampleOi,jof subject i the mean super-vector μi,jof the GMM that best represents this biometric sample is:(5)μi,j=m+Uxi,j+Dziwhere U is a subspace that constrains the possible session effects, xi,jis its associated latent session variable (xi,j∼N0I), while D and zirepresent the client-specific offset in the same manner as for mean-only MAP adaptation, which is given in Eqs. (2) and (3).The model enrolment of a client is performed in the following manner. Given a session subspace U, which is learnt by maximising the likelihood of the training data, the latent variables xi,jand ziare jointly estimated using MAP. Afterwards, the session varying part is suppressed by retaining only the client-specific information:(6)siisv=m+Dziisv.For details on how to jointly estimate the latent variables and how to train the subspace U, readers are referred to [12].ISV relies on a LLR score similar to Eq. (4). The main differences are that the session offsets of the enrolment samples have been compensated while generating the client model, and that session offsets of the probe sampleOtare estimated prior to scoring. This means that the latent session variables xi,jand xubm,jof the observed feature vectorsOt=ot1ot2…otKtextracted from a biometric sampleOtare first estimated, before computing a LLR score:(7)hisvOtsi=∑k=1Ktlogpotksi+Uxi,t−logpotkm+Uxubm,t.In practise, simplifications have been proposed to speed up the process in [24], which consists of first approximating the session offset Uxi,tof the client model, with the session offset Uxubm,t, and then using the linear scoring approximation as for the GMM-baseline.Finally, as with the GMM baseline, we perform zt-score normalisation.In [29] it was shown that JFA can fail to separate between-client and within-client variations into two different subspaces. This is potentially caused by the high dimensionality of the GMM mean super-vector space.To address this issue, an alternative technique called total variability (TV) modelling was developed for speaker authentication [30,31] and later applied to face authentication [10]. This framework is built on the GMM approach and relies on the definition of a single subspace that contains both identity and session variabilities. In particular, it aims to extract low-dimensional factors wi,j, so-called i-vectors, from biometric samplesOi,j. More formally, the TV approach can be described in the GMM mean super-vector space by:(8)μi,j=m+Twi,jwhere T is the low-dimensional total variability subspace and wi,jthe low-dimensional i-vector, which is assumed to follow a normal distributionN0I.The TV subspace T is learnt by maximising the likelihood over a large training set. This algorithm is similar to the one used to estimate the identity (between-class) subspace in JFA [32], with one major difference: while JFA jointly consider the samples coming from a given subject, TV treats them as if they have been produced by different identities, which is an advantage when large unlabelled training datasets are used. In addition, the extraction of i-vectors requires the estimation of a covariance matrix ΣTto model the residual variability that is not captured by the subspace T.In contrast to ISV, TV does not explicitly perform session compensation. TV is just a front-end that extracts a low dimensional i-vector wi,jfrom each sampleOi,jbased on the total variability of the training set. As such, it is likely to capture both client-specific and session-specific information. Hence, TV requires to use separate session compensation and scoring techniques after the extraction of i-vectors. Additionally, a set of preprocessing algorithms have been proposed to map i-vectors into a more adequate space [33,34,10]. Possible variants of preprocessing, session compensation and scoring methods have been employed and combined in different manners [30,31,10]. Some of them are summarised in Fig. 1and described in the remainder of this section.First, i-vector whitening was proposed in [33,10] and shown to boost classification performance. Whitening consists of normalising the i-vector space such that the covariance matrix of the i-vectors, of a training set, is turned into the identity matrix. This is performed by applying:(9)wi,jwhitened=WTwi,j−w¯,wherew¯is the mean of a training set of i-vectors, wi,j(whitened) the whitened i-vector, and W the whitening transform. This transform W is computed as the Cholesky decomposition ofΣ¯−1=WWT, whereΣ¯is the covariance matrix of a training set of i-vectors.Another efficient preprocessing technique is i-vector length normalisation [34,10], which aims at reducing the impact of a mismatch between training and test i-vectors. It consists of mapping the i-vectors into a unit hypersphere:(10)wi,j1−norm=wi,jwi,jwhich is very effective when using session compensation or scoring methods that assume Gaussian-like distributions.A set of session compensation techniques have been proposed for both speaker [31] and face [10] authentication. Linear discriminant analysis (LDA) [35] is a popular algorithm that aims at learning a linear projection maximising between-class variations while minimising within-class variations. The projection matrix A is learnt from a training set of i-vectors extracted from samples coming from several identities, by first computing the between-class and within-class scatter matrices:(11)SW=∑i∑j=1Jiwi,j−w¯iwi,j−w¯iT(12)SB=∑iJiw¯i−w¯w¯i−w¯Twhere Jiis the number of i-vectors from client i,w¯iis the mean of this client-specific i-vectors, andw¯the means of all i-vectors in the training set. Next, LDA maximises the ratio of the determinants of these two scatter matrices. The solution is found by solving the generalised eigenvalue decomposition SBv=λSWv. We then retain the n1da eigenvectors with the greatest eigenvalues to build the projection matrix A. An i-vector wi,jis projected into the LDA space by:(13)wi,j1da=ATwi,j.Within-class covariance normalisation (WCCN) is a technique initially introduced for SVM-based speaker authentication [36]. It has since been successfully applied to i-vectors for both speaker [30] and face authentication [10]. It aims to normalise the within-class covariance matrix of a training set of i-vectors. Given the within-class scatter matrix from Eq. (11) and the number of identities N in the training set, the WCCN linear transform B can be computed using the Cholesky decomposition of:(14)1NSW−1=BBT.An i-vector wi,jis projected into the corresponding WCCN space by:(15)wi,jwccn=BTwi,j.Once session compensation has been performed, any scoring technique might be employed for authentication purposes. Cosine similarity scoring[9,10] is a simple and efficient method used to estimate how close a (normalised) i-vector wtextracted from a probe sampleOtis to the i-vector wirepresenting a client i:(16)hcosinewtwi=wt×wi∥wt∥∥wi∥.Another technique commonly applied in the i-vector space is probabilistic linear discriminant analysis (PLDA) [37–39]. PLDA is a probabilistic framework that incorporates both between-class and within-class information and, therefore, performs session compensation. In addition, considering the authentication problem, this probabilistic approach allows the generation of LLR scores.More formally, PLDA assumes that the j-th i-vector of client i is generated by:(17)wi,j=Fhi+Gki,j+∈i,j,where F and G are the subspaces describing the between-class and within-class variations, respectively, hiand ki,jare the associated latent variables, which are assumed to be normally distributedN0I, and ∈i,jrepresents the residual noise, which is supposed to follow a Gaussian distributionN0Σϵ.The parameters Θplda={F,G,Σϵ} of this model are learnt using an EM algorithm over a training set of i-vectors. Once the model has been trained, given an i-vector wtextracted from a probe sampleOtand an i-vector wirepresenting a client i, authentication can be achieved by computing the LLR score:(18)hpldawtwi=pwt,wi|ΘpwtΘpwiΘ.Here, p(wt,wi|Θ) is the log-likelihood that the i-vectors wtand wishare the same latent identity variable hiand, hence, are coming from the same client, whereas p(wi|Θ)p(wi|Θ) is the log-likelihood that the i-vectors wtand wihave different latent identity variables htand hiand, therefore, are from different clients. For details on how to estimate these likelihoods and how to train the parameters Θplda, readers are referred to [37–39].Finally, recent work [40] on speaker recognition at NIST SRE 201211http://www.nist.gov/itl/iad/mig/sre12.cfm.has shown that the duration mismatch between enrolment and test speech segments can tremendously affect the accuracy of the system. To cope with this variability, [40] proposed to truncate the speech signal into shorter segments. Then the i-vectors of the truncated versions together with the i-vectors of the original signals are used to train the PLDA. We also evaluate this technique on the MOBIO database (see Section 5.6).Several fusion strategies are known in the literature [41]. They can be classified into three main categories:Low-level fusion which is also known as data fusion, combines multiple sources of raw data to produce new raw data. The major problem of this fusion method comes with non-balanced dimensionalities of data from the multiple sources.Intermediate-level fusion or feature level fusion combines various features that might come from several raw data sources or even from the same raw data. The drawback of feature-level fusion is that synchronisation between modalities [42] is required, which is not provided in the MOBIO database.High-level fusion which is also called decision level fusion, late fusion or score fusion, combines decisions from several systems. This fusion strategy is very flexible and can be used for multi-modal (face and speaker) or multi-algorithm (for instance combining GMM and ISV) fusion. High-level fusion methods include majority voting methods, fuzzy logic based methods [43], and statistical methods.In this work we choose the high-level fusion approach due to its ease of use for both multi-modal [8] and multi-algorithm [44–46] fusion.We take the well-known statistical linear logistic regression approach, which has been successfully employed for combining heterogeneous speaker and face authentication classifiers [44–46] and for bi-modal (face and speaker) authentication [8].Linear logistic regression combines a set of Q classifiers using the sum rule. Let the probeOtbe processed by Q classifiers, each of which produces an output scorehqOtsi. These scores are fused using a linear combination:(19)hfusionOtsi,β=β0+∑q=1QβqhqOtsi,where β=[β0,β1,…,βQ] are the fusion weights (also known as regression coefficients).The coefficients β are computed by estimating the maximum likelihood of the logistic regression model on the scores of the development set. LetXtruebe the set of true client access trials, i. e., the set of pairsx=Otsi, where the identity of the test sampleOtand of the client siis the same. Let furthermoreXimpbe the set of impostor trials, i. e., the set of pairsx=Otsi, where the identities of the test sampleOtand of the client siare different. LetX=Xtrue∪Ximp. The objective function to maximise is:(20)Lβ=−∑x∈Xlog1+exp−yxhfusionxβ,where:(21)yx=+1,ifx∈Xtrue−1,ifx∈Ximp.The maximum likelihood estimation procedure converges to a global minimum. In our work, this optimisation is done using the conjugate-gradient algorithm [47].This approach performs best when the scores of the classifiers are statistically independent of each other. For this reason we measure the independence and, therewith, the complementary nature of our classifiers. We use the scatter plots (see Fig. 8) and the relative common error (RCE):(22)RCE=CE×max1TE1,1TE2,…1TEQ,where CE is the number of common errors between the Q classifiers and TEqis the total number of errors of the qthsubsystem. The lower RCE is, the more independent the classifiers are.In this work we evaluate the effectiveness of both bi-modal and multi-algorithm fusion. This leads to a number of different system combinations, which we outline in Fig. 2. The top row of Fig. 2 displays the three different bi-modal fusion systems, while the bottom row shows the two different multi-algorithm fusion systems and the bi-modal multi-algorithm fusion approach that we examine.The MOBIO database [2] is a unique bi-modal, face and speaker, database as it was captured almost exclusively using mobile phones. It consists of over 61h of audio-visual data of 150 people captured within twelve distinct sessions that are usually separated by several weeks. The users answered a set of questions, which varied in type, including:1short response questions (p) such as “what is your address”,free speech questions, where the user speaks about any subject for approximately 10s (f) or about 5s (r), andpre-defined text (l) that the user read out.All of this data was captured on a mobile phone, except for the first session, where data was obtained using both a mobile phone and a laptop computer. One of the unique attributes of this database is that the acquisition device was held by the user, rather than being in a fixed position. As such, the microphone and camera are not fixed and used in an interactive and uncontrolled manner. This presents several challenges such as high variability of pose and illumination conditions, high variations in the quality of speech, and variability in terms of acoustics as well as illumination and background. Exemplary images of one identity are given in Fig. 3.This challenging mobile phone database has been used to evaluate several face and speaker authentication systems [1] as well as bi-modal authentication systems [7,8]. The database provides a well defined protocol, which was initially described for the full database in [4]. This protocol separates the clients of the database into three non-overlapping partitions for training, development (DEV) and evaluation (EVAL). The performance is measured in a gender-dependent manner (Female and Male, respectively). An overview of this initial protocol, which we refer to as mobile-0, is provided in Table 1. A limitation of this previously defined protocol is that only the lower quality biometric data acquired from the mobile phone was used, while the higher quality laptop data were ignored.In this work we extend the MOBIO protocol [2] and define three novel protocols that explore mismatched conditions by making use of the laptop data.22The MOBIO database (videos, still images, eye locations and the four evaluation protocols) are available for free at http://www.idiap.ch/dataset/mobio.The mismatched conditions that we wish to investigate are the specific cases of enrolling a user with high quality biometric samples (for instance acquired from a laptop computer) and then compared, or tested, using lower quality biometric samples obtained using a mobile phone.mobile-1 is identical to a mobile-0, except that it includes the laptop data in the training set. This ensures that the same training data is being used for mobile and laptop evaluation (the next protocol that we present). It provides an additional 1050 training samples compared to mobile-0. Enrolment and testing is conducted using only mobile phone data. Please see Table 2for more details.laptop-1 contains the same training data as mobile-1, but enrolment is performed exclusively using laptop data, while testing is conducted exclusively with mobile phone data. See Table 3for details on the kind of data used in this protocol.laptop-mobile-1 also consists of the same training data as mobile-1. Here, enrolment is performed using both mobile and laptop data, while testing is still conducted exclusively on mobile phone data, see Table 4for details.To measure the accuracy of the presented authentication systems, we use two different evaluation criteria that were previously defined in [2]. These measures are the half total error rate (HTER) and detection error trade-off (DET) plots.The HTER is used to represent the performance of an authentication system on the unbiased evaluation partition as a single number. To compute the HTER, a threshold θ is defined on the development partition at the intersection point of the false acceptance rate (FAR) and the false rejection rate (FRR). The corresponding FAR (or FRR) value of the development partition at this threshold θ is known as the equal error rate (EER). The threshold is applied to the evaluation partition (DEVAL) to obtain the HTER:(23)HTER=FARθDEVAL+FRRθDEVAL2,which is the average of the FAR and the FRR at θ. Finally, we provide a complete overview of the system performances using a DET plot [48], which outlines the miss probability (FRR) versus the probability of false acceptance (FAR) on the evaluation set.

@&#CONCLUSIONS@&#
In this paper, we studied the problem of face, speaker and bi-modal authentication in the challenging mobile environment. The study was carried out on the MOBIO database, for which we proposed three new protocols. One of these new protocols, laptop-1, presents a significant challenge for both speaker and face authentication as there is a significant mismatch between enrolment and test conditions. Empirically, we found that both face and speaker authentication are adversely affected by this condition mismatch with the relative performance of the best uni-modal and uni-algorithm systems F-ISV and S-ISV degrading by 47% and 37%,respectively, for Male trials. The impact of this condition mismatch was extended to the bi-modal system, whose relative performance degraded by as much as 80% for Male trials.We also examined several aspects of bi-modal and multi-algorithm fusion in the challenging mobile environment. We developed a state-of-the-art bi-modal multi-algorithm fusion system (B-ALL) that outperformed the state-of-the-art system of [8] obtaining a relative performance improvement of 35% and 27% on Female and Male trials, respectively. We found that multi-algorithm fusion provides a consistent performance improvement, particularly for the audio modality with average performance improvements of 3% for face authentication and 19% for speaker authentication across Male and Female trials for all of the protocols. In addition to this we showed empirically that ISV consistently outperforms not only GMM, but also TV with a limited amount of training data for both face and speaker authentication. We further explored this performance difference and found that TV provided improved performance for short utterances (less than 5s) and ISV provided better performance for medium length utterances (between 5s and 10s).