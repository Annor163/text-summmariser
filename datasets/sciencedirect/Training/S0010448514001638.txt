@&#MAIN-TITLE@&#


@&#HIGHLIGHTS@&#
Anℓ1-regression based subdivision scheme is proposed to handle noisy curve/surface data with outliers.A fast numerical optimization method named dynamic iterative reweighted least squares is proposed to solve this problem.The most advantage of the proposed method is that it removes noises and outliers without any prior information about the input data.

@&#KEYPHRASES@&#
ℓ,1,-Regression,Data restoration,Outlier detection,Subdivision scheme,Iterative reweighted least squares,

@&#ABSTRACT@&#
Fitting curve and surface by least-regression is quite common in many scientific fields. It, however cannot properly handle noisy data with impulsive noises and outliers. In this article, we studyℓ1-regression and its associated reweighted least squares for data restoration. Unlike most existing work, we propose theℓ1-regression based subdivision schemes to handle this problem. In addition, we propose fast numerical optimization method: dynamic iterative reweighted least squares to solve this problem, which has closed form solution for each iteration. The most advantage of the proposed method is that it removes noises and outliers without any prior information about the input data. It also extends the least square regression based subdivision schemes from the fitting of a curve to the set of observations in 2-dimensional space to ap-dimensional hyperplane to a set of point observations in(p+1)-dimensional space. Wide-ranging experiments have been carried out to check the usability and practicality of this new framework.The curve and surface fitting is the process of constructing curve and surface that has the best fit to the data. In statistics and machine learning, overfitting occurs when a statistical model describes random error or noise instead of the underlying relationship. Overfitting generally occurs when a model is excessively complex. Situation becomes more worse when some data points deviate so much from the other data points as to arouse suspicions in data. Such type of data points are called outliers. Outliers are also referred to as abnormalities, discordants, deviants, or anomalies in the data mining and statistics literature. Outlier detection and dealing with noisy data have been extensively studied in the past decades in different disciplines. Here is the brief survey of the related work.One of the most widely used statistical technique to detect outlier is weighted least squares regression  [1]. In 1997, Sohn and Kim  [2] proposed an algorithm for detection of outliers in weighted least squares regression using Studentized weighted residuals. Multiple case outlier detection in least squares regression model using quantum inspired evolutionary algorithm has been introduced by Khan and Aktar  [3]. Iteratively reweighted minimization algorithms for sparse recovery and matrix rank minimization have been discussed by  [4–6]. Bissantz et al.  [7] offered convergence analysis of generalized iteratively reweighted least squares algorithms on convex function spaces by using quantile,ℓq,q∈[1,2)logistic and isotonic regressions. Lai et al.  [8] presented improved iteratively reweighted least squares for unconstrained smoothedℓq,q∈(0,1]minimization. However, the detection of outlier in high dimensional case is a challenging task. This is because in high dimensionality, the data becomes sparse and the sparsity behavior makes all points look very similar and almost equidistance to one another. A practical algorithm to outlier detection and data cleaning for the time-dependent signal is proposed by Pan et al.  [9]. It is claimed that proposed algorithm is good for bioinformatic application. Nikolova  [10] introducedℓ1data-fidelity based variational method for the processing of image corrupted with outliers and different kinds of impulse noise. Avron et al.  [11] introduced anℓ1-sparse method for the reconstruction of a piecewise smooth point set surface. Optimal fitting may alternatively be obtained using simpler functions and theℓ1andℓ∞norms  [12].The class ofℓ1-regularized optimization problems has received much attention recently because of the introduction of compressed sensing  [13], which allows images and signals to be reconstructed from small amounts of data. Despite this recent attention, manyℓ1-regularized problems still remain difficult to solve, or require techniques that are very problem-specific  [14]. Yang and Zhang [15] studied the use of alternating direction algorithms for severalℓ1-norm minimization problems arising from sparse solution recovery in compressive sensing. A new nonlocal total variation regularization algorithm for image denoising has been introduced by Liu and Huang  [16]. Xiao et al.  [17] proposed, analyzed and tested primal and dual versions of the alternating direction algorithm for the sparse signal reconstruction from its major noise contained observation data. The algorithm minimizes a convex non-smooth function consisting of the sum ofℓ1-norm regularization term andℓ1-norm data fidelity term. Candès et al.  [18]presented a novel method for sparse signal recovery that in many situations outperformsℓ1-minimization in the sense that substantially fewer measurements are needed for exact recovery. The algorithm consists of solving a sequence of weightedℓ1-minimization problems where the weights used for the next iteration are computed from the value of the current solution.Subdivision schemes (i.e. classical schemes) are widely used for curve and surface fitting from few decades. An intensive study and literature along with mathematical descriptions and formulations are available now  [19–23]. A major advantage of subdivision schemes is that they can be easily applied to virtually any data type. However, early work in subdivision schemes does not deal with noisy data with impulsive noises and outliers. A downside of subdivision algorithm is that they are sensitive to outliers. One outlier can damage whole model and in most of the cases, schemes give overfitted model for noisy data with outliers. One algorithm has been reported by  [24] for fitting a Catmull–Clark subdivision surface model to an unstructured, incomplete and noisy stereo data set by using quasi-interpolation technique but this work does not deal with outliers. Recently, Dyn et al.  [25] have presented univariate subdivision schemes based on least squares minimization to deal with noisy data. They have compared their schemes with least squares minimization problems with kernel weights. The main purpose of their work was to address an open question: How to approximate a function from its normally distributed noisy samples by subdivision schemes?The purpose of our article is to address the open question: How can we approximate a function by subdivision technique from its noisy samples with impulsive noises and outliers? To address this question: we use theℓ1-regression to construct subdivision schemes with dynamic iterative reweighted weights. Numerical results show that new schemes have the ability to remove outliers and give better fitted models as compared to the other subdivision schemes when data is contaminated with noises and outliers. A major advantage of our schemes is that they give best fit to any type of data with and without added noise and outliers in high dimensional spaces. Throughout this paper,LSscheme means subdivision scheme based on least square regression with constant weights whileℓ1scheme means subdivision scheme based onℓ1-regression with dynamic iterative reweighted weights.The paper is structured as follows: In Section  2, the construction ofℓ1scheme for curve fitting is discussed in detail along with its variants. Numerical examples for curve fitting are also presented in this section. In Section  3, we introduceℓ1scheme for surface fitting. We also introduce new least square regression surface schemes in this section as special cases ofℓ1schemes. Comparison of fitting surfaces byℓ1andLSschemes is also presented in this section. Section  4 summarizes the topics discussed in this article and outlines further research directions.In this section, we propose a class ofℓ1-regression based subdivision schemes with dynamic iterative reweighted weights for curve fitting. We also discuss its generalization and variants along with some numerical experiments.

@&#INTRODUCTION@&#


@&#CONCLUSIONS@&#
