@&#MAIN-TITLE@&#
Exploiting Universum data in AdaBoost using gradient descent

@&#HIGHLIGHTS@&#
We address a novel boosting algorithm by taking advantage of Universum data.A greedy, stagewise, functional gradient procedure is taken to derive the method.Explicit weighting schemes for labeled and Universum samples are provided.Practical conditions to verify effectiveness of Universum learning are described.This algorithm obtains superior performances over AdaBoost with Universum data.

@&#KEYPHRASES@&#
AdaBoost,Gradient boost,U,AdaBoost,Universum,U,-SVM,

@&#ABSTRACT@&#
Graphical abstract

@&#INTRODUCTION@&#
Conventional machine learning algorithms take labeled data, unlabeled data or both of them for learning. Vapnik [1] proposed the third kind of data: Universum data. The Universum data contains the data that belongs to the same domain as the classification problem but it does not belong to any class of the problem. For example, in handwritten digits recognition problems, if the samples of handwritten digits ‘5’ and ‘8’ are prepared for learning, then other handwritten digit samples can be naturally treated as Universum data since they belong to the same domain but cannot be assigned to any of the two classes.It is a common case that large labeled training data is included in order to obtain good quality of training. However, it is quite costly or sometime even impossible to have very large training data. To deal with such problem, semi-supervised learning is a common option when unlabeled data is available since unlabeled data helps model data distribution of the whole data. On the other hand, without unlabeled data, Universum data is still able to provide the supports to maintain the training quality with relatively small labeled data set. The reason is Universum data can be generated through a lot of ways from labeled data only [2] (mentioned later). Moreover, Universum data can carry additional valuable prior information from the domain of the problem into the training process. To the best of our knowledge, there is no comparison between semi-supervened learning and Universum based learning. But in our opinion, Universum based learning can better model the whole data set since Universum data stays in the same domain of learning problem with which we are concerned [1] while the unlabeled data may be too general and stay outside of the domain. In terms of data acquisition, Universum data can be obtained more widely.Vapnik first discussed transductive learning with Universum since transductive learning provides prior information to estimate the upper bound of inductive inference [1]. However, the classifier trained by inductive learning is more practical to classify unknown data. Weston et al. [2] proposed an inductive algorithm, Universum Support Vector Machines (U-SVM).U-SVM contains an additional regularization term for Universum data in addition to conventional SVM. The regularization is based on this assumption: the decision values on the Universum data should be close to zero. That is Universum data should fall inside the margin of the classifier since it does not belong to any class. The Universum samples which meet such assumption are called contradictions because the goal of learning is putting labeled data outside of the margin. Thus the margin should contain more Universum data to achieve better learning performance. More Universum data means more contradictions. Therefore the learning criterion for Universum based learning is called Maximal Contradiction on Universum (MCU) [1]. Two learning problems: common semi-supervised and training based on Universum are demonstrated in Fig. 1. The unlabeled data should be away from the margin like labeled data while Universum data should fall between margins.Sinz et al. [3] analyzed theU-SVM for inference and they showed thatU-SVM would give the hyperplane which had its normal lying in the orthogonal to the principal directions of Universum data. They also discussed the connection of least squared version ofU-SVM with Fisher discriminant analysis and oriented principal component analysis. They showed thatU-SVM outperformed SVM with carefully selected Universum data. In addition to SVM, Universum data has also been extended to other learning problems, such as semi-supervised learning [4], linear discriminant analysis [5], twin support vector machine [6], cost-sensitive learning [7], linear programming [8], and domain adaptation [9]. In terms of application, besides the handwritten digits recognition problem mentioned above, it is also applied into medical imaging [10], document clustering [11], pose recognition [12,13], etc.Universum data is always obtained from the domain of the classification problem mentioned above. Weston et al. [2] proposed four kinds of Universum data: random noise, the rest of the training data (e.g. the other digits in the handwritten digits recognition problem), artificial data from the same distribution of training data and random average of training data. Bai & Cherkassky [14] applied Universum data into gender classification and they took three kinds of Universum data: random average, empirical distribution and animal faces. In the field of Universum data selection, Sinz et al. [3] suggested that a good Universum set should contain invariant directions and be positioned “in between” the two classes of the classification problem. Chen & Zhang [15] proposed a guided formulation to pick the informative ones, i.e., in-between Universum (IBU) samples.Boosting family contains a series of well-known algorithms with a large number of applications. Motivated by the success ofU-SVM, Shen et al. [16] proposedUBoost by adding Universum data to boosting algorithms and showed that they can benefit from Universum data as SVM did.UBoost is derived from AdaBoost-CG [17] which is another view of boosting. Compared with AdaBoost-CG, AdaBoost is a stagewise method [18] which is more general and popular. The whole training procedure of AdaBoost is also much faster. Although Universum data has shown its power onU-SVM [2] andUBoost [16], to our knowledge, its importance on AdaBoost has not been evaluated.In this paper, we propose a new boosting algorithm calledUAdaBoost to improve the classification performance of AdaBoost with the help of Universum data. The learning is not straight forward since Universum data belongs to neither positive nor negative data. Stagewise AdaBoost keeps the pre-selected weak classifiers unchanged in the following training. It pays more attention on misclassified samples in the next training iteration. The weights for training samples and coefficients for the pre-selected weak classifiers are obtained according to gradient descent. Involving Universum data into AdaBoost framework needs to take these properties of AdaBoost into account. Instead ofUAdaBoost,UBoost takes Universum as a conventional convex optimization problem and solves it by column generation as AdaBoost-CG does.To tackle the above challenges, we propose explicit weighting schemes for both labeled data and Universum data which are both involved in AdaBoost training procedure. The rationale of updating weights in common AdaBoost training is to enforce the training to focus on hard samples. In this paper, this rationale is revisited and further extended on Universum data in the proposedUAdaBoost.The major contributions of this paper are as follows:1)Given Universum data, a newUAdaBoost learning based on the framework of AdaBoost is proposed. We proposeUAdaBoost using the same functional gradient descent as AdaBoost. By taking advantage of AdaBoost,UAdaBoost is much easier and more practical to apply thanUBoost [16] sinceUAdaBoost only needs one parameter to tune.The whole training procedure ofUAdaBoost is efficient. The time cost forUAdaBoost is less thanUBoost. Our experimental results demonstrate such improvement.UAdaBoost provides a better framework for investigating the benefits of Universum data in boosting approaches. It is known that AdaBoost is a popular algorithm in boosting algorithm family. In recent years, researches have contributed significant efforts to investigate AdaBoost in order to improve its performance. To our best knowledge,UAdaBoost is the only framework using the same approach (i.e. stagewise) as AdaBoost and integrating Universum data, so the performance evaluation on integrating Universum data into AdaBoost is more precise and convincing. In contrast,UBoost follows column generation approach [17] which is different from AdaBoost so we cannot useUBoost framework in evaluating the benefits of Universum data to AdaBoost.Also, in this paper, we discuss a method for selecting effective and informative Universum data in order to better take the advantages of Universum data in AdaBoost framework. This will benefit several applications in computer vision area.The paper is structured as follows. In Section 2, we discuss the related work aboutU-SVM,UBoost and the motivation toUAdaBoost. In Section 3, we propose the novel boosting formulationUAdaBoost based on the Universum data and compare it with semi-supervised boosting, AdaBoost andUBoost. In Section 4, we analyze the practical conditions forUAdaBoost. In Section 5, the performance of our model will be demonstrated with several public data sets. In Section 6, we conclude the paper.In this paper, our focus is only on binary classification problems, while our method can be extended to the multi-class scenario. LetXL=x1y1x2y2…xmymbe the set of m labeled examples, where yi∈{−1,1} is the class label. LetXU=x1*x2*…xn*represent the Universum data with n samples. wiand wj∗ represent the weights of the labeled sample (xi,yi) and xj∗ during boost training phase respectively.Weston et al. [2] proposedU-SVM and treated it as an inductive learning problem. The Universum examples are considered to be close to the separating hyperplane selected by SVM. The optimization objective should minimize the cumulative loss on the Universum examples. Given the Hinge loss Ha[t]=max{0,a−t} for the standard SVM (a=1) and ε-insensitive loss Iε[t]=H−ε[t]+H−ε[−t] for Universum data, the learning problem can be formulated as:(1)minw,b12∥w∥2+CL∑i=1mH1yifw,bxi+CU∑j=1nIεfw,bxj*.The first two terms are for the standard SVM and the last one is for Universum data.CLandCUare parameters for regularization. fw,b(x) is the learned classifier. Parameter ε controls the margin of Universum data. Sinz et al. [3] gave the least squares version ofU-SVM and showed that this kind ofU-SVM can also show similar performances with the originalU-SVM with less parameters (there is no ε in Eq. (2)).(2)minw,b12∥w∥2+CL2∑i=1mQyifw,bxi+CU2∑j=1nQ0fw,bxj*where Qa[t]=∥t−a∥22 is the quadratic loss for labeled data (a=yi) and Universum data (a=0).Shen et al. [16] proposed a boosting algorithmUBoost using Universum data. They applied squared loss of Universum data in the optimization objective. They formulated this problem with the same framework of AdaBoost-CG [17] by adding the regularization term for Universum data.(3)minw1m∑i=1mexpzi+c2n∑j=1nzj*2+D1⊤ws.t.zi=−yiHiw,∀i;zj∗=Hj*w,∀j;w≽0.D controls the regularization of the weighting coefficients of boosting algorithm. c controls the trade-off between the errors of labeled data and unlabeled data. H is the prediction of all available weak classifiers on the training data. H={hk(x):x→{1,−1}}, Hik=hk(xi). hkdenotes the k-th weak classifier. Hiis the i-th row of H which denotes the output of all weak classifiers on example xi. Likewise, H⁎ is the prediction of Universum data. Two additional variables z and z⁎ are used for generating the dual problem. Based on this formulation, they obtained its corresponding dual problem and solved it with column generation. Experiments on a variety of handwritten digits recognition problems and computer vision problems showed thatUBoost outperformed AdaBoost and AdaBoost-CG.

@&#CONCLUSIONS@&#
