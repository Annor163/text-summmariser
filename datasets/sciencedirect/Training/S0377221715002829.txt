@&#MAIN-TITLE@&#
Heuristic approaches for biobjective mixed 0–1 integer linear programming problems

@&#HIGHLIGHTS@&#
Biobjective mixed 0–1 integer linear programming problems are considered.A variable neighborhood search based heuristic is presented.A local branching based heuristic is presented.An algorithm is presented to find Pareto line segments.Several experiments are conducted and results are presented.

@&#KEYPHRASES@&#
Multiobjective programming,Biobjective mixed 0–1 integer linear programming,Variable neighborhood search,Local branching,

@&#ABSTRACT@&#
In this study, biobjective mixed 0–1 integer linear programming problems are considered and two heuristic approaches are presented to find the Pareto frontier of these problems. The first heuristic is a variant of the variable neighborhood search and explores the k-neighbors of a feasible solution (in terms of binary variables) to find the extreme supported Pareto points. The second heuristic is adapted from the local branching method, which is well-known in single objective mixed 0–1 integer linear programming. Finally, an algorithm is proposed to find Pareto segments of outcome line segments of these heuristics. A computational analysis is performed by using some test problems from the literature and the results are presented.

@&#INTRODUCTION@&#
In a linear programming problem if there are two challenging objective functions, the problem needs to be evaluated in the biobjective linear programming (BOLP) context. Adding 0–1 requirements to all variables yields the biobjective 0–1 integer linear programming problem (0–1 BOILP). In a more general form, a biobjective mixed 0–1 integer linear programming problem (0–1 BOMILP) arises when some variables are real-valued and others are binary. Although there is no difference in the notion of computing the extreme supported Pareto points of these three problems, the properties of Pareto frontiers are different. It is well-known that the set of Pareto points of BOLP is convex, hence connected and all Pareto points are supported (see Ehrgott, 2005; Isermann, 1974, 1977; Steuer, 1985; Yu & Zeleny, 1976; Zeleny, 1974). In 0–1 BOILP, Pareto points are isolated and some of these Pareto points might be nonsupported which makes the problem challenging. Additionally, the number of Pareto points may grow exponentially depending on the problem size (see for instance, Hamacher & Ruhe, 1994; Visée, Teghem, Pirlot, & Ulungu, 1998). Since 0–1 BOMILP have common characteristics with both BOLP and 0–1 BOILP, the set of Pareto points might be partially connected as well as might include isolated points (Belotti, Soylu, & Wiecek, 2012; Mavrotas & Diakoulaki, 2005; Stidsen, Andersen, & Dammann, 2014; Vincent, Seipp, Ruzika, Przybylski, & Gandibleux, 2013). Even if selecting very small ɛ values in the ɛ-constraint method (Haimes, Lasdon, & Wismer, 1971), the profile of the Pareto frontier of BOMILP is generally not generated exactly. Instead we need to apply BOLP methods, such as Parametric Simplex Algorithm (Ehrgott, 2005), to construct connected parts on the Pareto frontier.0–1 BOMILPs have been used to model decision making problems that take place in sophisticated systems such as production (Sayın & Karabatı, 1999), logistics (Pishvaee, Farahani, & Dullaert, 2010), hub-location (Köksalan & Soylu, 2010), supply chains (Olivares-Benitez, González-Velarde, & Ríos-Mercado, 2012) etc.In the single objective programming literature, although there are many exact algorithms to solve mixed 0–1/integer linear programming problems (MILPs), there are only a few heuristic approaches, in particular, for extremely difficult MILPs (see Lodi, 2010). Hansen and Mladenović (2001) proposed the variable neighborhood search (VNS), which searches neighbors of an incumbent solution for a local optimum. If this local optimum solution is better than the incumbent, then the incumbent solution is updated. Another approach is local branching (LB) (Fischetti & Lodi, 2003), where the solution space is divided into two parts in terms of neighbors of an incumbent solution as similar to the classical branching idea. Although LB is an exact method, if a time limitation is imposed on the solution stages of subproblems, then LB will behave as a heuristic. Whenever an improving solution is found, it becomes the new incumbent and exploration is reapplied to the unsearched neighbors of the incumbent. Hansen, Mladenović, and Urošević (2006) developed a VNS heuristic where the definition of neighborhoods around the incumbent solution is the same as in LB. Recently, successful approaches combining heuristics and exact methods have been proposed. Lazić, Hanafi, Mladenović, and Urošević (2010) developed a hybrid approach for 0–1 MILPs, which combines VNS and an exact solution method.Even though the single objective MILP literature is extensive both in theory and in methodology, unfortunately this has not led to much improvement in the multi-objective MILP literature. Although the exact algorithms (such as Belotti et al., 2012; Mavrotas & Diakoulaki, 1998, 2005; Stidsen et al., 2014; Vincent et al., 2013) to find the Pareto frontier of multi-objective MILPs have advanced in recent years, they are still complex even in the biobjective case. It is well-known that the multiobjective 0–1 integer linear programming problems #P-hard in general. That is the number of Pareto solutions is intractable and counting them is difficult (see Ehrgott, 2000). This behavior is especially observed in multiobjective combinatorial optimization problems (see for example, Ehrgott, 2000; Serafini, 1986; Visée et al., 1998). Adding real-valued variables to these problems would not decrease the computational difficulty. Computation time and memory limitations on exact methods make promising heuristic approaches which perform well in approximating the Pareto frontier. However, in the literature there is no heuristic method to approximate the Pareto frontier of BOMILPs using line segments except for some attempts to approximate the extreme supported Pareto points of BOMILPs. Although some metaheuristics, such as evolutionary algorithms, have been applied in these problems, (except additional efforts) they are insufficient to generate the profile of the Pareto frontier since they are able to find only some representative points on the Pareto frontier.This paper modifies the VNS and LB approaches to the biobjective context for the purpose of approximating the Pareto frontier. Both methods are based on the neighborhood search where a neighbor is a solution that is close to the current solution in terms of a metric, such as the Hamming distance (Hamming, 1950) between them in terms of binary variables. Once the neighborhood size is defined, new feasible solutions can be searched by using exact methods or heuristics. In the biobjective variant of VNS, which is called BOSS, the k-neighbors of each feasible solution are explored with the Parametric Simplex Algorithm (Ehrgott, 2005) to find extreme supported Pareto points and (if available) the line segment connecting them. Then an attempt is made to construct the Pareto frontier with the Pareto points found. The other proposed heuristic, which is called BOLB, is based on the local branching approach where two subproblems are generated from each node. One corresponds to the k-neighbors of an incumbent solution and the other corresponds to the non-k-neighbors of this solution. At each level the k-neighbors of a solution are explored with a dichotomic search (Aneja & Nair, 1979; Cohon, 1978) by making “Tabu” previously searched solutions.In Section 2, a general 0–1 BOMILP is defined and basic definitions are provided. In Section 3, a biobjective neighborhood search heuristic, BOSS, is proposed. In Section 4, a biobjective local branching heuristic, BOLB, is proposed. In Section 5, computational results are presented. Finally, the conclusion is given in Section 6.A 0–1 BOMILP can be defined as follows:P:Maxz1(x)=c1TxMaxz2(x)=c2TxSubjecttox∈XwhereX={x∈Rp×{0,1}n−p:Ax≦b}n>1,0≤p≤nis the set of feasible solutions. There are p continuous and n − p binary variables. Then the image of the set X isY={y∈R2:y1=c1Tx,y2=c2Tx,x∈X}called the set of attainable vectors. The spacesRp×{0,1}n−pandR2are referred to as the decision space and the objective space, respectively. Consider two feasible solutionsx1,x2 ∈ X and their imagesy1,y2 ∈ Y. We use the following notation:y1≧y2 ifyi1≥yi2for i = 1, 2; andy1 ≥y2 ify1≧y2 andy1 ≠y2; andy1 >y2 ifyi1>yi2for i = 1, 2. Here the inequality ( ≥ ) refers to the componentwise ordering on the spaceR2, and inequalities (≧) and (>) refer to weak and strict componentwise ordering, respectively. Ify1 ≥y2, then it is said thaty1 dominatesy2. If the inequality is strict (>) then it refers to strict dominance. A feasible solutionx^∈Xis said to be efficient (weakly efficient) for problem P if there is no feasible solutionx∈ X such thaty= z(x) dominates (strictly dominates)y^=z(x^)where z = (z1, z2). Ifx^is (weakly) efficient, theny^is called (weakly) Pareto point. The set of efficient solutions is XE⊂X and the set of Pareto points is YN⊂Y. Note that the set of weakly Pareto points includes all Pareto points and some special dominated points. The reader may refer to (Ehrgott, 2005; Steuer, 1985) for a detailed coverage of multiobjective programming.Efficient solutions can be categorized basically into three classes: extreme supported, nonextreme supported and nonsupported. When we aggregate two objectives with some λ ∈ (0, 1), we get the following single objective weighted-sum problem (Geoffrion, 1968).Pλ:Maxz(x)=λc1Tx+(1−λ)c2TxPλ:Subjecttox∈XThe optimal solutionxof Pλis a supported efficient solution of the problem P and the correspondingyis the supported Pareto point of the problem P. Ifyis an extreme point of Conv(Y), thenyis an extreme supported Pareto (ExSP) point. Ifyis on the boundary of Conv(Y) but not extreme, thenyis a nonextreme supported Pareto (NExSP) point. Ifxis an efficient solution but not the optimal solution of Pλ, then its imageyis said to be a nonsupported Pareto (NSP) point.A slice problem of P is defined as follows (Belotti et al., 2012):P(x¯j):Maxz1(x)=c1TxP(x¯j):Maxz2(x)=c2TxP(x¯j):Subjecttox∈X(x¯j)whereX(x¯j)={x∈Rp×{0,1}n−p:Ax≦b,xi=x¯i−pj∀i>p,x¯j∈{0,1}n−p}defines a slice of the set X. Here thex¯vector refers to the binary variables of a solutionx∈ X. In a slice, all the binary components ofxare fixed tox¯j. Therefore,P(x¯j)is a BOLP and hence its Pareto frontier is piece-wise linear and convex. The setY(x¯j)={y∈R2:y1=c1Tx,y2=c2Tx,x∈X(x¯j)}is the set of attainable vectors of problemP(x¯j). LetXE(x¯j)andYN(x¯j)denote the set of efficient solutions and Pareto points of problemP(x¯j), respectively. Each linear component ofYN(x¯j)is called a line segment. Let [ya,yb] be a line segment such thaty1a<y1bandy2a>y2b. Ifya=ybthen it is called an isolated point. A line segment can be open ( ), half open (], [) or closed [ ] depending on whether the end points are dominated or not. Several examples are given in Fig. 1.Fig. 2illustrates the Pareto frontier of some slices of an example 0–1 BOMILP. The mathematical model of the example is provided in the Appendix. In this example, variables x1, x2, …, x7 are defined as binary, and variables E11, E12, …, E72 are defined as continuous. Although this is a specially designed example, we observe similar profiles in more general 0–1 BOMILPs or in our test problems. For instance, in the marked sliceX(x¯9), the binary variables x2, x4 and x7 are set to 1 and other binary variables are set to 0. Note that some line segments of several slices construct the Pareto frontier of the example problem. Exactly five slices (X(x¯1),X(x¯2),X(x¯3),X(x¯4)andX(x¯6)) contribute to the Pareto frontier, so we also call them as Pareto slices. Although it is not available in this figure, isolated points may also be observed in the Pareto frontier of BOMILPs.Before defining the k-neighborhood problem, we present the definitions of Hamming distance and the k-neighborhood constraint. The Hamming distance (Hamming, 1950) between two vectors including binary variables is the number of flipped variables. Accordingly, the Hamming distance between solutionxand the binary vectorx¯jcan be computed asH(x,x¯j)=∑i>p|xi−x¯i−pj|. The k-neighborhood constraint is defined asH(x,x¯j)≤k,which ensures that the Hamming distance fromx¯jshould be at most k where 0 ≤ k ≤ n − p. This constraint is also known as the local branching constraint (Hansen et al., 2006). Although this constraint is nonlinear, it can be linearized by defining a setSj={i:i>p,x¯i−pj=1}. Then the linearized version of the constraint is∑i∈Sj(1−xi)+∑i∉Sj(xi)≤k. In Fig. 2, slicesX(x¯1)andX(x¯7)(two left slices) are in the 1-neighborhood of each other, while slicesX(x¯1)andX(x¯3)are in the 2-neighborhood of each other.The k-neighborhood problem of a given sliceX(x¯j)is defined as follows:P(x¯j,k):Maxz1(x)=c1TxP(x¯j,k):Maxz2(x)=c2TxP(x¯j,k):Subjecttox∈X(x¯j,k)whereX(x¯j,k)={x∈Rp×{0,1}n−p:Ax≦b,H(x,x¯j)≤k,x¯j∈{0,1}n−p}. LetY(x¯j,k)={y∈R2:y1=c1Tx,y2=c2Tx,x∈X(x¯j,k)}is the set of attainable points of problemP(x¯j,k)and the setYN(x¯j,k)is the set of Pareto points.Result 1When k = 0, the problemP(x¯j,0)is equal to the problemP(x¯j).Assume that the setSj={i:i>p,x¯i−pj=1}. Then∑i∈Sj(1−xi)+∑i∉Sj(xi)≤0. This constraint is only satisfied whenxi=x¯i−pj∀i>p. When replacing theH(x,x¯j)≤0constraint ofP(x¯j,0)withxi=x¯i−pj∀i>p, we obtain the problemP(x¯j).□The decision space of the k-neighborhood problem covers both the decision space of the slice problem itself and all k-neighboring slice problems. LetX¯¯(x¯j,k)={x∈Rp×{0,1}n−p:Ax≦b,H(x,x¯j)=k,x¯j∈{0,1}n−p}be the set of solutions which is exactly k away from the sliceX(x¯j)andY¯¯(x¯j,k)={y∈R2:y1=c1Tx,y2=c2Tx,x∈X¯¯(x¯j,k)}. ThenX(x¯j,k)=X(x¯j)∪X¯¯(x¯j,1)∪X¯¯(x¯j,2)∪⋯∪X¯¯(x¯j,k)whereX(x¯j)=X¯¯(x¯j,0)=X(x¯j,0). Similarly,Y(x¯j,k)=Y(x¯j)∪Y¯¯(x¯j,1)∪Y¯¯(x¯j,2)∪⋯∪Y¯¯(x¯j,k). Here we can come up with the following equalities;X(x¯j,k+1)=X(x¯j,k)∪X¯¯(x¯j,k+1)Y(x¯j,k+1)=Y(x¯j,k)∪Y¯¯(x¯j,k+1)Finally, the setYN(x¯j,k+1)is equal to the Pareto points of the setY(x¯j,k+1)or the Pareto points of the union setY(x¯j,k)∪Y¯¯(x¯j,k+1).Remark 2Although the setY(x¯j,k+1)covers the setY(x¯j,k), the setYN(x¯j,k+1)does not have to cover the setYN(x¯j,k). Similarly, the set of supported efficient solutions of problemP(x¯j,k+1)does not have to cover the set of supported efficient solutions of problemP(x¯j,k).We refer to the relaxed problems of all the defined problems above (except for the slice problem since it is a BOLP) asP˜..wherex∈Rp×{0,1}n−pis replaced withx∈Rnand the constraint set 0 ≤ xi≤ 1, ∀i > p is added. The sets of efficient solutions and Pareto points of all relaxed problems are also denoted asX˜E.. andY˜N.., respectively. Additionally, Pλ, .. refers to the single objective weighted-sum problem of P...The main idea of this heuristic is to search the neighboring slices of a given sliceX(x¯j)for the potential extreme efficient solutions of problem P. For exploration purposes, the Parametric Simplex Algorithm (PSA) (Ehrgott, 2005) is applied to the problemP˜(x¯j,k). During the search if we catch one point belonging to another sliceX(x¯l), its binary vector is added in a setX^ESL. This search procedure is repeated for all elements ofX^ESL. After completing the search procedure, the Pareto frontier of the slices available in theX^ESLset is constructed by PSA. Finally, their upper envelope is computed to construct the profile of the Pareto frontier of problem P.The pseudo code of the BOSS algorithm is given in Algorithm 1. The inputs of the algorithm are problems P andP˜, an initial feasible solutionx0 and lower and upper limits over the parameter k, while the set of approximating line segmentsYN*is output. Lines 5–14 search all k-neighbors of all elements of the setX^ESL. For this purpose, ExSP points of the relaxed k-neighborhood problem are found by applying PSA. During this process all slices found are recorded inX^ESL. The ConstructSlicesYNprocedure computes all ExSP points of all slice problems available by the PSA and each line segment computed is recorded in the setY^N. Finally the ConstructPareto procedure finds the upper envelope of all line segments available.In the BOSS algorithm, the selection of the parameter k is also important. In our case, all k values between kmin and kmax are applied for each of the slices currently available. Here kmin and kmax are user defined parameters where 0 ≤ kmin ≤ kmax ≤ n − p. Although larger values of the parameter k allow us to explore farther slices, it is possible to miss the slices contributing to the Pareto frontier but located at convex regions of the Pareto frontier (i.e., nonsupported Pareto points of P). By choosing small values of k we can catch such slices since a supported Pareto point of this restricted problem may be the nonsupported Pareto point of problem P (see Fig. 2). However, in the latter case we can also waste time by searching close neighbors who do not contribute to the Pareto frontier of problem P.Algorithm 2finds ExSP points of the relaxed k-neighborhood problem by using PSA. It starts with the solution of the problemP˜(x¯j,k)with one objective function, i.e. the solution ofP˜λ(x¯f,k)for λ1 = 1. This solution is optimal for all λ ∈ [λ2, 1]. As soon as λ ≤ λ2, new solutions become optimal. The PSA stops whenever λs = 0 or the problem is unbounded. In the end, the PSA returns a sequence of λs, s = 1, 2…, h values and solutions which are optimal to the problemP˜(x¯j,k)for all values of λ ∈ [λs, λs − 1]. The reader may refer to Ehrgott (2005) for details of the PSA.Alternatively, the dichotomic search (DS) (Aneja & Nair, 1979; Cohon, 1978) can be used to obtain the ExSP points of the problemP˜(x¯j,k). DS solves the problemP˜λ(x¯j,k)repeatedly for different λ weights obtained according to the dichotomic principle. It is also possible to explore theP(x¯j,k)problem instead of its relaxation. The DS algorithm then helps to find ExSP points ofP(x¯j,k), of course taking considerable time. In our experiments we also perform a comparison for that case.Algorithm 3computes ExSP points of each slice problemP(x¯j)wherex¯j∈X^ESLby using PSA. Whenever two neighboring extreme points (ys − 1 and ys) ofP(x¯j)found, the line segment [ys − 1, ys] connecting them is added into the setY^N. Finally it returns the set of extreme supported efficient solutions and the set of line segments which might contribute to the Pareto frontier, YN, of problem P.The steps up to this point are not sufficient to generate the Pareto frontier of the problem P due to its nonconvex structure. Although some candidate line segments are available at hand, their Pareto segments are not known. The last stage of the BOSS algorithm is related to finding the upper envelope of all candidate line segments. The problem of finding the upper envelope of n line segments (see Fig. 3(a)) (problem A) is known in computational geometry literature and reduces to the problem of finding Pareto segments of n line segments problem (problem B) in BOMILP. Both problems can be solved by considering subintervals constructed on the y1 axis and computing the upper envelope (for maximization problems) of each subinterval. However, in its current form, the union of the upper envelopes of subintervals for problem A does not correspond to the set of Pareto line segments of problem B (see Fig. 3). Since each subinterval is independently evaluated, a point is unaware of points dominating it and belonging to another subinterval. However, when we introduce dominated line segments from end points of each line segment, then the connection between subintervals is conducted (see Fig. 3(b)). Here the dominated line segments delimit the dominance cone of an end point within the ranges ofY^N. Since the end point of a line segment dominates the set of points on the dominated line segment, it also dominates points dominated by the dominated line segment. Now, the upper envelope of each subinterval corresponds to either a Pareto line segment or a dominated line segment. If the output is a dominated line segment, then a conventional value null can be assigned. As a result of introducing dominated line segments, the finding upper envelope of line segments problem reduces to finding Pareto segments of line segments problem.The most important advantage of evaluating the problem within this context is to allow parallel processing. The jobs of finding upper envelopes of different subintervals can be distributed to different processors without needing any communication and the final outputs can immediately be merged.In the computational geometry literature, there exist more computational efficient algorithms such as the Atallah algorithm (Atallah, 1983) or the Hershberger algorithm (Hershberger, 1989) which finds the upper envelope of line segments. Recently, for the purpose of constructing the nondominated set Vincent et al. (2013) propose an algorithm which makes the pair-wise comparison of outcome edges and apply a dominance test by using some propositions explaining dominance relations. Although their algorithm and the proposed algorithm return the nondominated set finally, both algorithms are different since the upper envelope of a subinterval constructed in this study might not be the same with the output line segments of a pair-wise comparison in Vincent et al. (2013) study even if both methods use the same intersection point. This is because more than two line segments might appear in a subinterval. Interested readers might devise both different perspectives.In the straightforward application of the divide-and-conquer approach, segments are partitioned into two subsets, the upper envelope of each subset is computed and finally two results are merged. For this purpose, the y1 axis can be divided into small intervals and then the Pareto line segments of each interval can be found. However, this is not an easy job, if the line segments are intersecting (see Fig. 3(a)). To handle this situation we can divide the interval from the intersection points again until no two segments intersecting in a subinterval. Basically there are two possible intersections. One is that line segments might intersect as in Fig. 3(a), the other is that dominated line segments (emanating from end points) might intersect others as in Fig. 3(b). Note that there is no need to consider open end points since there exists a closed end point dominating it. One extreme case is that parallel line segments might intersect. In this case, although there are infinitely many intersection points, taking any one of them as the intersection point is enough for constructing subintervals in Algorithm 4. Subintervals are constructed on the y1 axis by using adjacent intersection points. Then, the Pareto line segment (upper envelope) is computed within each subinterval. Algorithm 4 gives the pseudo-code of an upper envelope algorithm. The input of the algorithm is the set of line segments,Y^N, while the output is the set of Pareto line segments,YN*. Intersection points are stored in a set IP. First, dominated line segments are introduced into the setY^N. Then each line segment is compared with others to determine intersections. Finally, subintervals are constructed and Pareto line segments of all subintervals are computed.It is possible to make more efficient Algorithm 4. We can observe from Fig. 3(b) that all (blue) dominated line segments (except the (red) horizontal dominated line segment emanating from the north-west end point of a line segment) never contribute to the upper envelope or the construction of subintervals and they can be removed from the setY^N. Since all end points are included in the set IP, the (blue) vertical ones can be eliminated. Since the (blue) horizontal ones cannot contribute to the upper envelope, they can also be eliminated.Proposition 1Algorithm 4computes the Pareto segments of outcome line segments for BOMILP by using subintervals.LetY^Nbe the set of outcome line segments andYN*be the set of Pareto line segments ofY^N. All line segments are located in the interval[minyj∈Y^Ny1j,maxyj∈Y^Ny1j] of y1 axis. Let divide this interval from any point y′1 into two adjacent subintervals such thatinterval1=[minyj∈Y^Ny1j,y′1]andinterval2=[y′1,maxyj∈Y^Ny1j]. In this case, letY^N1,Y^N2be the set of outcome line segments, respectively andYN1*,YN2*be the set of Pareto line segments, respectively. It is easy to observe thatY^N=Y^N1∪Y^N2. Then introduce dominated line segments from end points (see Fig. 4). Now, the upper envelope of interval 1 is equal toYN1*and the upper envelope of interval 2 is equal toYN2*. Additionally, it can be observed thatYN*=YN1*∪YN2*. Here all dominated points are located in the interior of the dominance cone generated by the endpoint or the Pareto line segment. In a similar manner, interval 1 can be divided into two parts and interval 2 can be divided into two parts. If we set this division point y′1 as the y1 coordinate of intersection points yi∈ IP, we get exactly |IP| + 1 intervals given in Algorithm 4. ThenYN*=YN1*∪YN2*….∪YN(|IP|+1)*.□In Fig. 5, we illustrate the search idea of BOSS by using the example given in the Appendix. Let us assume thatx0 is an initial feasible solution and kmin = kmax = 5. That means we are searching for slices within 5-neighborhood of the sliceX(x¯0). The binary vectorx¯0is included inX^ESL. After solving the problemP˜(x¯0,5)with the PSA, the resulting Pareto frontier (bold frontier) is obtained. The red points show ExSP points of the problemP˜(x¯0,5), while the bold line shows NExSP. During the PSA we found a feasible solutionx1 which belongs to theX(x¯1)slice. Then the binary vectorx¯1is also included inX^ESL. Since another slice is now explored, its k-neighbors will also be searched during the algorithm. After completing the search (lines 5–14 in Algorithm 1), the setYN(x¯j)is computed for allx¯j∈X^ESL. Assume thatX^ESL={x¯0,x¯1}. In line 15 of Algorithm 1 (Algorithm 3 is called), ExSP points of each slice problemP(x¯j)wherex¯j∈X^ESLare found by using PSA. The outputsYN(x¯0)andYN(x¯1)of Algorithm 3 are given in Fig. 5. Finally, in line 16 of Algorithm 1 (Algorithm 4 is called), the upper envelope of these Pareto slices is computed.The Local Branching Algorithm was developed by Fischetti and Lodi (2003) for single objective mixed 0–1 problems. Similar to the branch-and-bound algorithm, two subproblems are generated from each node as illustrated in Fig. 6. One problem defines the k-neighbors (left branch) and the other defines the non-k-neighbors (right branch) of a solution. On the other hand, unlike the branch-and-bound algorithm the solution space division is not based on fractional variables. A general-purpose MIP solver is used to explore each subproblem. The left branch is expected to produce new solutions while the right branch produces new subproblems to explore. The search is continued until a point where the incumbent solution cannot improve. If there is no time or memory restriction during the exploration, then the local branching acts as an exact method. Otherwise, it behaves as a local search heuristic. There is a tradeoff between the node exploration time and the neighborhood parameter k where 0 ≤ k ≤ n − p. Here the parameter k is a fixed value and defined at the beginning of the algorithm. As the parameter k increases, the node exploration time also increases but it is then more likely to cover better solutions. With the selection of small k values, the incumbent solution might not improve at early stages and the tree might not deep enough as so leaving the most of the search space to the last node problem. As to the selection of the parameter k, a steady state point for the incumbent solution may be observed. An analysis about this issue is performed inSection 5.In this study, the idea of local branching is adapted for biobjective mixed 0–1 problems. Given a sliceX(x¯0), the current node problem P can be partitioned into two subproblems asP1′andP1′′. Let Xt,Xt+1′andXt+1′′be the feasible sets of problemsPt′′,Pt+1′andPt+1′′, respectively. ThenXt+1′={x∈Xt:H(x,x¯t)≤k}andXt+1′′={x∈Xt:H(x,x¯t)≥k+1}Here the left branch is used to explore new slices which are expected to contribute to the Pareto frontier of problem P and the right branch generates new subproblems. Fig. 6 illustrates the implementation of the local branching strategy where the node exploration implies the application of a dichotomic search (Aneja & Nair, 1979; Cohon, 1978) to the problemPt′. At each level of the tree the kth neighbor of a slice is searched by making “Tabu” previously searched slices. Actually the constraintH(x,x¯t)≥k+1is known as the “Tabu” constraint (Fischetti & Lodi, 2003) since it does not allow the search of k-neighbors of previously visited slices. During the exploration, all ExSP points of problemPt′are computed by using the DS algorithm. If any of these points belong to a new slice, then it is stored for the purpose of node exploration later. Before starting to explore each node we need to check whether it is promising or not. If it can be proven that at the end of the node exploration we will not obtain any ExSP point belonging to the Pareto frontier of problem P, then this subproblem can be discarded before exploration. For this purpose, we can define an upper bound related to the problemPt′. A well-known upper bound is the ideal point (z*) of problemP˜t′wherez1*=max{z1(x):x∈X˜t′}andz2*=max{z2(x):x∈X˜t′}. If the ideal point of problemP˜t′is dominated by any pointy∈Y^, then this node is closed. Whenever k-neighbors of all slices available at hand are searched, the algorithm reaches the last level. It stops after searching the last right node problem.The BOLB heuristic and the node exploration procedure are given in Algorithms 5 and 6, respectively. Algorithm 5 starts with an initial solutionx0. Then the left node problem is generated and checked whether it is promising to explore or not. If it is promising, the DS algorithm starts to compute ExSP points of the subproblemP1′. Whenever a point belonging to a new slice is found, it is added into the setX^ESL. If there is no slice to restrict the problem, the while loop ends. But one more exploration is performed for the last generated subproblemPl′′. The last two procedures (lines 18 and 19) are the same as the BOSS algorithm. After completing the search procedure, the Pareto frontier of slices available in theX^ESLset is constructed by PSA. Finally, the upper envelope of them is computed to construct the profile of the Pareto frontier of problem P.Proposition 2When there is no time or memory limitation, the BOLB finds all the ExSP points of P.If there are l levels in the tree, thenX=X1′∪X2′…∪Xl′∪Xl′′. Note that subregionsX1′,X2′,…,Xl′,Xl′′are disjoint and any ExSP point of problem P only belongs to one subregion. Since the union of all subregions constructs the set X, all ExSP points of P are found by searching all subregions separately.□The BOLB algorithm also helps to find the nonsupported points of problem P while it is searching for ExSP points of a subproblem. An ExSP point of a restricted subproblem may be the nonsupported Pareto point of problem P (see Fig. 7).As stated above, both the BOSS algorithm and the BOLB algorithm require an initial feasible solutionx0. A good initial feasible solution is expected to accelerate the algorithm. It could be found heuristically or exactly solving the problem. For both algorithms we solve the problem P only for the first objective function and we set the resultingxasx0.In Fig. 7, we illustrate the idea of BOLB. Let us assume thatx0 is an initial feasible solution and k = 1. That means in the node problemP1′we will search for slices within the 1-neighborhood of the sliceX(x¯0). The binary vectorx¯0is included inX^ESL(line 3 in Algorithm 5). After adding the constraintH(x,x¯0)=(1−x1)+x2+x3+(1−x4)+(1−x5)+(1−x6)+x7≤1into the problem P, the problemP1′is obtained (line 7). Since the ideal point ofP1′is not dominated, NodeExploration subprocedure is called (line 11). The black dots in Fig. 7(a) show the ExSP points found by DS during the node exploration subprocedure. It is clear that some points belonging toX(x¯1),X(x¯2)andX(x¯3)slices are caught and these slices are also added intoX^ESL. After exploration, the right hand side of theH(x,x¯0)is replaced with ≥ 2 and the problemP1′′is obtained. At the second iteration,x¯1∈X^ESLis taken and the constraintH(x,x¯1)=(1−x1)+x2+(1−x3)+(1−x4)+(1−x5)+(1−x6)+x7≤1is added into the problemP1′′(line 7). Since the ideal point is not dominated (line 11), the NodeExploration subprocedure is called. The red dots in Fig. 7(b) show the ExSP points found by DS. Since some points belonging toX(x¯4)andX(x¯5)slices are caught, they are added intoX^ESL. The search is repeated for all elements ofX^ESLand also for the final node problemPf′′. Assume thatX^ESL={x¯0,x¯1,x¯2,x¯3,x¯4,x¯5}. In line 18 of Algorithm 5 (Algorithm 3 is called), ExSP points of each slice problemP(x¯j)wherex¯j∈X^ESLare found by using PSA. The outputsYN(x¯0),YN(x¯1),…,YN(x¯5)are given in Fig. 7(c). Finally, in line 19 of Algorithm 5 (Algorithm 4 is called), the upper envelope of these Pareto slices is computed.As regard to the theoretical comparison of both heuristics, it is easy to observe the advantage of BOLB over BOSS since the prior one searches for the Pareto points of problem P instead of the relaxed problem. Additionally, due to Proposition 2, the BOLB algorithm guarantees to find all ExSP points of P. InSection 5, we also developed the alternative version of the BOSS algorithm (called BOSS_x) by changing the search mechanism for the purpose of finding the Pareto points of problem P instead of the relaxed problemP˜. On the other hand we expect an advantage of alternative version of the BOSS algorithm over the BOLB algorithm since different size k-neighbors (kmin ≤ k ≤ kmax) are searched in the BOSS algorithm while only one setting of k-neighbors is searched in the BOLB algorithm. Note that non-k-neighbors of the last slice are also searched by the BOLB algorithm as different from the BOSS algorithm.The proposed algorithms were coded in C using the Cplex 12.1 callable library (Cplex, 2010) and run on a 2.4 GHz workstation with 4 GB of RAM memory. All accelerating mechanisms (cuts, cliques and heuristics) of Cplex are turned off at the beginning. Algorithms were tested on instances of 0–1 BOMILP test problems of different size and some combinatorial problems (biobjective knapsack (Gandibleux & Fréville, 2000) and set covering (Gandibleux, Vancoppenolle, & Tuyttens, 1998)). The test problems are generated using the interval ranges of Mavrotas and Diakoulaki (1998, 2005). The elements of the matrix A and the vectorsbandcare randomly generated floating point numbers within the following intervals:-[−10, 10] for the elements ofcassociated with continous variables;[−200, 200] for the elements ofcassociated with 0–1 variables;[50, 150] for the elements ofb;[−1, 20] for the elements of A.In all test problems,the number of constraints and the number of variables are equal, i.e. (m = n). The size of the problems gradually increases up to 500 constraints and 500 variables. The problems were coded as n × m × p and five instances were generated for each problem.The ExSP points of each problem were found by using a dichotomic search (Aneja & Nair, 1979; Cohon, 1978) and these points were used to compare the performances of algorithms. Additionally, the ɛ-constraint method (Haimes et al., 1971) was employed to find several slices belonging to the Pareto frontier of problem P. For this purpose, the range of the second objective function was divided into 10,000 subintervals equally and the problem was solved by using the first objective only over these intervals. The results of the algorithms were compared with that of found slices. In addition, the hyper-volume measure (HV measure) (maximum is better), one of the most frequently applied measures for comparing the results of multiobjective optimization algorithms, proposed in Zitzler (1999) is used. Let HV(S) denotes the measure of a generic approximation set S with respect to the HV measure. Let YNandYN*denote the set of Pareto points found by the ɛ-constraint method with 10000 subintervals and the set of approximating line segments yield by proposed algorithms, respectively. The following abbreviations are used to denote the performance measures.CPU: the CPU time measured in seconds.#ExSP: the number of ExSP points of problem P (found by the DS algorithm).#PS: the number of Pareto slices of problem P (found by the ɛ-constraint method with 10000 subintervals).%ExSPcatch: the percentage of ExSP points of problem P caught by algorithms.%PScatch: the percentage of Pareto slices of problem P caught by algorithms.HV(S): the hyper-volume measure of the set S (over the scaled objective values).Deep: number of subproblems explored in the BOLB algorithm.Closed: number of subproblems closed in the BOLB algorithm since their ideal point is dominated.Moreover, in both algorithms we experimented with three levels of the kmax parameter as 1, 3 and 5 and kmin = 0.Table 1 presents the performancestatistics of the BOSS algorithm. Although the solution time of the algorithm is very promising, the %ExSPcatch and %PScatch statistics are less than 50 percent in most of the instances. The third column presents the hyper-volume of the set of Pareto points of problem P. As the outcomes of algorithms close to the Pareto frontier, the hyper-volume of outcomes closes to that of Pareto frontier. According to results the hyper-volumes of outcomes of the algorithms are relatively better than the %ExSPcatch and %PScatch statistics indicating that although ExSP points and slices found by algorithms are not belong to the Pareto frontier, they are close to the Pareto frontier. As the problem size increases the performance statistics become worse, as expected. As the k increases, the CPU time increases.It is interesting to note that all except the CPU performance results of kmax = 3 and kmax = 5 are the same. To analyze this case we selected one feasible solutionx0 belonging to each problem. We applied the k-neighbor search to this solution and recorded performance statistics during the search. The results are presented in Fig. 8. According to this figure after a certain point, increasing k does not affect the performance. For the instance belonging to the 20 × 20 × 10 problem, after k = 3, no new supported efficient solution or no new Pareto slice are found. For the other problem instances, after k = 0, no new supported efficient solution is found aroundx0. This might explain why the results of kmax = 3 and kmax = 5 are the same in Table 1.To analyze the effect of finding the ExSP of problemP(x¯0,k)exactly during the k-neighborhood search, we changed the PSA, which solves the problemP˜(x¯0,k), with the DS. We call the new version BOSS_x and present the results in Table 2. According to this, finding the ExSP exactly instead of relaxation results in significant improvements in performance statistics but the CPU time also increases considerably. The number of ExSP and PS of problem instances are also given in the third and sixth columns of Table 2. As the problem size increases #ExSP also increases, however, we cannot observe the same trend in #PS. There is less number of Pareto slices in the 500 × 500 × 250 problem and so the performance statistics are unusually high. One reason for that is the right hand side vectorb∈ [50, 150]. If we relaxbinto the range [50, 200], the number of Pareto slices increases and the performance decreases as given in the last row of Table 2.To better understand the effect of the initial solutionx0, we changed it with a very poor one, which is the 0 vector. The new version is called the BOSS_0. The experimental results are given in Table 3. We observe that in general the performance improves in small-size problems while the reverse happens in large-size problems.Table 4presents the performance results of the BOLB heuristic. Since the BOLB heuristic is able to find all ExSP points of problem P, the %ExSPcatch statistic is 100 for all instances. The best performance is again observed when kmax = 3. When compared to the BOSS heuristic results, the BOLB algorithm performs significantly better, however, the CPU time of the BOLB algorithm is also higher than the BOSS algorithm. The number of subproblems (level + 1) in the tree is given in the Deep column. In general, as the problem size increases, the tree size increases. According to the Closed column, the number of subproblems closed due to dominance (see Section 4) increases as the parameter k increases and it decreases as the problem size increases.Fig. 9illustrates the %ExSPcatch and %PS statistics at each subproblem of a mid-size problem. We can infer from this figure that as the parameter k increases, ExSP points are found at early levels of the tree. A similar trend is observed for the second graph as well.We also performed experiments on proposed algorithms with the biobjective knapsack and set covering test problems. The instances are available in the web site of http://xgandibleux.free.fr/MOCOlib/. In the biobjective knapsack problems, instance sizes increase up to 300 items starting with 50 items. The weights and cost coefficients are uniformly generated (Gandibleux & Fréville, 2000; Visée et al., 1998). In the biobjective set covering problems, there are three problem sizes, which are 10 × 100, 40 × 200 and 60 × 600 (represented as |rows|×|colums|). For each problem four variants are given: A, B, C and D (Gandibleux et al., 1998). Since these problems are combinatorial, we use the %Pareto statistic defined below instead of %PS statistic.%Pareto: the percentage of Pareto points of combinatorial problems found by algorithms.According to the results given in Table 5, the BOLB heuristic is better than BOSS and BOSS_x heuristics in %ExSPcatch statistic since the BOLB heuristic is able to find all supported efficient solutions of the problem. In terms of hyper-volume and %Pareto statistics, BOSS_x and BOLB represent a similar performance. Here we also observe that hyper-volumes of BOSS_x and BOLB are very close to the hyper-volume of Pareto frontier. For instance, Pareto frontier and the outcomes of algorithms for the knapsack instance 2KP300-1A are illustrated in Fig. 10. Although this instance is one of the worst performing instances in terms of %Pareto statistic, we observe that outcomes of algorithms are very close to the Pareto frontier and thus result with a high hyper-volume. In Table 5, one interesting result is due to the hyper-volume measures of the 2KP50-92 problem, which are 0.0. The hyper-volume measure actually computes the volume bounded by the Nadir point. In this instance, since there exist only two Pareto points that also yield the Nadir point, the volume of the dominated space is 0.0. In terms of CPU time statistic, the solution time of BOLB could be extremely high. Although the CPU time of the BOSS heuristic is very low, its performance is worse than others.When we evaluate the performances of algorithms over combinatorial problems in general we observe that although their performances are promising with respect to performance statistics, they have very high CPU times. This is due to the fact that they are especially designed to find Pareto slices of 0–1 BOMILPs rather than Pareto points of combinatorial problems. Actually they are applying the slice exploration process to each point of these problems. Therefore, they are performing high effort at each point of these problems.

@&#CONCLUSIONS@&#
