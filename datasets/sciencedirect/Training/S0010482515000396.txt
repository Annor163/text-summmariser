@&#MAIN-TITLE@&#
New feature selection for gene expression classification based on degree of class overlap in principal dimensions

@&#HIGHLIGHTS@&#
A feature selection based on degree of class overlap is proposed.Sorting the degrees of overlap for selecting the candidate dimensions.The minimum degree of overlap must be used as the relevant features.Using forward feature selection with SVM classification.The PMDO method selected fewer candidate features.

@&#KEYPHRASES@&#
Micro-array data,Dimension reduction,Feature selection,Feature extraction,principal component,Analysis,Support Vector Machine,

@&#ABSTRACT@&#
Micro-array data are typically characterized by high dimensional features with a small number of samples. Several problems in identifying genes causing diseases from micro-array data can be transformed into the problem of classifying the features extracted from gene expression in micro-array data. However, too many features can cause low prediction accuracy as well as high computational complexity. Dimensional reduction is a method to eliminate irrelevant features to improve the prediction accuracy. Typically, the eigenvalues or dimensional data variance from principal component analysis are used as criteria to select relevant features. This approach is simple but not efficient since it does not concern the degree of data overlap in each dimension in the feature space. A new method to select relevant features based on degree of dimensional data overlap with proper feature selection was introduced. Furthermore, our study concentrated on small sized data sets which usually occur in reality. The experimental results signified that this new approach can achieve substantially higher prediction accuracy when compared with other methods.

@&#INTRODUCTION@&#
Classification techniques have been used for analysis and interpretation of micro-array gene expression data. The information from the pattern of micro-array gene expression data has provided valuable insights for many biological problems. The main challenge in classifying gene expression data is the curse of dimensionality problem. There are a large number of genes (in biological context) or features (in classification context) compared to small sample sizes [1]. In the aspect of computation, the number of features can be treated as the number of dimensions in the feature space. Mathematically, if the number of dimensions is larger than the number of data, then no unique solution exists and achieving the high prediction accuracy is impossible. To alleviate this curse of dimensionality, some irrelevant features must be eliminated from the prediction or classification process.Dimension reduction usually consists of two types of methods, i.e. feature selection and feature extraction [2]. Feature selection chooses a subset from original features according to classification performance. Feature selection can be organized into two categories: filter and wrapper methods. The filter method basically uses a criterion relating to rank and selects key genes for classification such as Pearson correlation coefficient method [3,4], t-statistics methods [5,6], signal-to-noise ratio method [7,8]. The wrapper method integrates the feature selection process with the evaluation of the selected features for classification such as forward feature selection [9,10], sequential backward selection [11,12], genetic algorithm [13,14]. Feature extraction projects the whole data into a low dimensional space and constructs the new dimensions (components) by analyzing the statistical relationship hidden in the data set. Many transformation methods have been applied to reduce the dimension of the data such as independent component analysis [15,16], principal component analysis [17,18], wavelet analysis [19,20].Another popular technique is based on the evolutionary computation or algorithm. Evolutionary algorithm (EA) finds optimal solution in a complex high dimensional space. Dimensional reduction was applied to solve this problem. Genetic Algorithm (GA) [21] generates acceptable solutions by using techniques inspired by natural evolution such as selection, crossover and mutation. A population of chromosomes representing encoded candidate solutions to the optimization problem is evolved toward the better solutions as some examples reported in [22,23]. Rough set theory [24] is another concept used for dimensional reduction in large data. Rough set is a formal approximation of a crisp set similar to fuzzy set [25]. Rough set can be combined with other methods such as GAs and multi-objective genetic algorithms (MOGA) to search for optimal solutions [26,27]. Particle swarm optimization (PSO) [28] based on the behavior of swarm is another popular method for finding an acceptable solution. PSO was adopted to the problem of identifying important gene from gene expression [29,30].The most widely used dimensional reduction method is principal component analysis (PCA) [31]. PCA [32] rotates the bases of the feature space to the positions so that the data variance of each dimension is minimum. PCA is a popular unsupervised statistical method to find useful eigenassays [33]. One goal of PCA is to find a “better” set of eigenassays, so that the PCA coefficients are uncorrelated on the new basis, which cannot be linearly predicted from each other [34]. However, PCA does not consider the degree of overlap among classes in each dimension. For any dimension, if there is no overlap among classes, then the feature in this dimension can obviously be used as one of the principal features for achieving maximum classification accuracy.To improve efficiency of PCA for use in feature selection, we proposed a new method called principal feature analysis of minimum distribution overlap with forward feature selection based on Support Vector Machine. The degree of class overlap is measured and used to determine the candidate bases in the feature space. The propose method was tested with four gene expression data sets and compared with the recently proposed methods.The rest of this paper is organized as follows. Section 2 summarizes the background and related work. Section 3 discusses the concept of our proposed method. Section 4 explains the data set and experimental results. Section 5 concludes the paper with some discussions.Our proposed method selects each feature based on the degree of overlap between classes and also the result of preliminary classifying process. Recently, Luo [9] adopted the statistical measure similar to t-test with different distance measures, e.g. Euclidean, to improve the feature selection in the original feature space. Zhang [34] proposed a weight measure which can improve the discriminant ability and preserve the local neighborhood structure of the original data. Hence the methods of Luo׳s and Zhang׳s with the concept of support vector machine are briefly summarized in the following section.Luo [9] proposed two forward feature selection methods named as FFS-ACSA1 and FFS-ACSA2 in dichotomy classification. The target value of each feature vector is in{+1,−1}. The concept of this approach is to deploy the regular technique of average ensemble classifiers with a new feature selection. The proposed feature selection of Luo׳s is based on the measure of signal-to-noise ratio of each feature and a cost function defined as follows. Suppose that the considered feature corresponds to dimension i. Let aibe the value of the ith feature:(1)f(ai)=sign[si(ai−bi)](2)si=μi(A)−μi(B)σi(A)+σi(B)(3)bi=(μi(A)+μi(B))/2whereμi(A),μi(B),σi(A), andσi(B)are the mean and standard deviation of data distribution in dimension i of classes A and B, respectively. Functionf(ai)is the cost function and siis the signal-to-noise ratio for the ith feature. The difference between the values off(ai)and the values of actual targets is compared by using some distance measures such as Euclidean distance and loss function. Those features with the differences less than a predefined threshold are selected.Zhang [34] proposed an improved supervised orthogonal discriminant projection (SODP). The concept of this approach is based on the nearest neighbor classifier with a manifold learning. The proposed manifold learning of Zhang׳s maximizes the weighted difference between the non-local scattering and the local scattering. Zhang defined the weightwi,jbetween two data pointsxiandxjbased on local information and class information as follows:(4)wi,j={exp(−‖xi−xj‖2β2)[1+exp(−‖xi−xj‖2β2)]if(xi∈K(xj))or(xj∈K(xi))and(ti=tj)exp(−‖xi−xj‖2β2)if(xi∈K(xj))or(xj∈K(xi))and(ti≠tj)0otherwisewhere‖xi−xj‖is the Euclidean distance betweenxiandxj;K(xi)is the set of k nearest neighbors ofxi;tiis the class label ofxi; andβis a tuning parameter. The local scattering and the non-local scattering are defined as follows. LetAbe a linear transformation matrix. The local scatteringJL(A)is defined as(5)JL(A)=ATSLAwhereSL=(1/2n2)∑i=1n∑j=1nwi,j(xi−xj)(xi−xj)T. The non-local scatteringJN(A)is defined by(6)JN(A)=ATSNAwhereSN=(1/2n2)∑i=1n∑j=1n(1−wi,j)(xi−xj)(xi−xj)T. The optimal transformation matrix A can be obtained by(7)argmaxJ(A)=argmaxAT((1−γ)SN−γSL)Awhere the constantγis an adjustable parameter. Then the optimal linear features of all dataxi∈Xcan be obtained by(8)Y=ATXFor anyyi∈Y, a data pointy⁎∈Ycan be assigned to the same class asyiif the following condition is satisfied:(9)y⁎=arg∀jmin(‖yj−yi‖)Support Vector Machine (SVM) [35, 36] has been very popular in solving dichotomy classification problems. It mapped the data in feature space onto a higher dimensional feature space by a set of kernel functions so that the data are absolutely separable. The values of targets are in{+1,−1}. The data are separated by a hyperplane and the class of each datum is determined by this condition. Letxjand b be a datum and a bias, respectively. If(wxj+b)≤−1, then it is in class −1. But if(wxj+b)≥+1, then it is in class +1. Any datum whose value(wxj+b)in between −1 and +1 is indeterminate. The most commonly used kernel function is inner product of two feature vectors.Forward feature selection method selects one feature at a time which gives the best prediction accuracy in combination with the previously selected features. In our method this selection is implemented with SVM as follows:1.Add one feature with the previously selected features to form a new set of candidate features.Use the candidate set of feature with SVM in training process.Measure prediction accuracy from testing samples.Choose the added feature which gives the highest prediction accuracy.The work of Luo׳s [9] suggested an interesting t-test observation. However, each feature vectorxiis still in its original feature space. No natural data distribution in the feature space is seriously considered and involved during feature selection process. This may lead to the wrong selection of each feature since global relationships between all feature pairs are ignored. We introduced a new approach by taking the natural data distribution into account. The actual bases of the feature space must be defined by the direction of data distribution in every dimension. Those bases whose degree of data overlap of classes is minimum must be used as the relevant features and the other bases must be excluded as irrelevant features. There are two major parts in our proposed method. The first part is to identify the direction of data distribution in each dimension. The second part is to measure the degree of class overlap in each dimension and select those dimensions or features with minimum degree of overlap. To ensure that those selected features possibly achieve highest classification accuracy, those preliminarily selected features must be as the input of SVM classifier. Thus our method consists of the following steps. Letxi(α)=[xi,1(α),xi,2(α),…,xi,m(α)]Tbe the ith feature vector of class αin m dimensions. In this study, only dichotomy classification is concerned, i.e.α∈{A,B}. We assume thatX(A)={x1(A),x2(A),…,xN(A)}is a set of feature vectors in class A andX(B)={x1(B),x2(B),…,xM(B)}is a set of feature vectors in class B of the given feature space.Algorithm 1Principal feature analysis of minimum distribution overlap (PMDO).1. Compute the covariance matrixCofX(A)∪X(B).2. Compute the set of eigenvectors{u1,…,um}ofC.3. Foreachxi(A)∈X(A)do4.Foreachxi,j(A)∈xi(A)do5.Transformxi,j(A)to new locationx^i,j(A)=ujTxi(A).6.EndFor7. EndFor8. Foreachxi(B)∈X(B)do9.Foreachxi,j(B)∈xi(B)do10.Transformxi,j(B)to new locationx^i,j(B)=ujTxi(B).11.EndFor12. EndFor13. Foreach dimension ido14. Compute meanμi(A)=1N∑k=1Nxi,k(A).15. Compute standard deviationσi(A)=∑k=1N(xi,k(A)−μi(A))2N.16. Compute meanμi(B)=1M∑k=1Mxi,k(B).17. Compute standard deviationσi(B)=∑k=1M(xi,k(B)−μi(B))2M.18. Compute degree of overlap bydi=|μi(A)−μi(B)|σi(A)+σi(B).19. EndFor20. LetD={d1,…,dm}be a set of all degrees of overlap sorted in descending order.21. Use d1 as the first selected feature and apply forward feature selection from setDwith SVM to obtain the proper set of features with high accuracy.To verify the classification ability of our algorithm, it was applied to tumor classification using four data sets of gene expression data with different properties in dimensionality of samples and set size. Our results were compared with the results from FFS-ACSA1, FFS-ACSA2 [9] and SODP [34] methods.We applied our proposed method with four public gene expression data sets to evaluate its performance. There are non-small cell lung cancer (NSCLC) data sets of Coldren et al. [37], Chronic Myeloid Leukemia (CML) data sets of McWeeney et al. [38], Chronic Myeloid Leukemia (CML) data sets of Crossman et al. [39], Breast Cancer data sets of Gluck et al. [40]. The detailed descriptions of four data sets. are as follows:1.NSCLC data set: The data are from the web site of ncbi.nlm.nih.gov, GEO Data Sets number: GSE4342 containing Affymetrix HG-U133A 45 samples with 22283 genes in each sample. According to the original literature, there are 11 training samples and 34 testing samples. In our experiment, we used 23 samples that are sensitive or resistant to gefitinib. Coldren et al. [37] classified sensitive to gefitinib withIC50≤0.5μmol/Land resistant to gefitinib withIC50≥4.5μmol/LCML(1) data set: The data are from the web site of ncbi, GEO Data Sets number: GSE14671 containing Affymetrix HG-U133_Plus_2 59 samples with 54675 genes in each sample. According to the original literature, McWeeney et al. [38] classified response to imatinib of patients with a complete cytogenetic response within 12 months and non-response to imatinib of patients lacking of a major cytogenetic response within 12 months.CML(2) data set: The data are from the web site of ncbi, GEO Data Sets number: GSE2535 containing Affymetrix HG_U95Av2 28 samples with 12625 genes in each sample. According to the original literature, Crossman et al. [39] classified response to imatinib with complete cytogenetic response (CCR) within 9 months and non-response to imatinib without major cytogenetic response (MCR) after 1 year.Breast Cancer data set: The data are from the web site of ncbi, GEO Data Sets number: GSE22358 containing 158 samples with 44290 genes in each sample. Gluck et al. [40] classified samples to pathologic complete response (pCR), near-complete response (npCR) and partial response (PR). In our experiment, we used 26 samples that used trastuzumab treatment and classified response to trastuzumab with pathologic complete response (pCR) and near-complete response (npCR) and non-response to trastuzumab with partial response (PR).

@&#CONCLUSIONS@&#
