@&#MAIN-TITLE@&#
Quantitative systematic analysis of vocal tract data

@&#HIGHLIGHTS@&#
Quantitative comparison of vocal tract profiles considering multiple regions.Normalised differences allowing intra- and inter-speaker comparisons.Analysis of speech production considering multiple speakers/realisations.Abstract visual representation to support analysis of the computed difference data.

@&#KEYPHRASES@&#
Vocal tract analysis,Quantitative comparison,RT-MRI,

@&#ABSTRACT@&#
Articulatory data can nowadays be obtained using a wide range of techniques, with a notable emphasis on imaging modalities such as ultrasound and real-time magnetic resonance, resulting in large amounts of image data.One of the major challenges posed by these large datasets concerns how they can be efficiently analysed to extract relevant information to support speech production studies. Traditional approaches, including the superposition of vocal tract profiles, provide only a qualitative characterisation of notable properties and differences. While providing valuable information, these methods are rather inefficient and inherently subjective. Therefore, analysis must evolve towards a more automated, replicable and quantitative approach.To address these issues we propose the use of objective measures to compare the configurations assumed by the vocal tract during the production of different sounds. The proposed framework provides quantitative normalised data regarding differences covering meaningful regions under the influence of various articulators. An important part of the framework is the visual representation of the data, proposed to support analysis, and depicting the differences found and corresponding direction of change.The normalised nature of the computed data allows comparison among different sounds and speakers in a common representation.Representative application examples, concerning the articulatory characterisation of European Portuguese vowels, are presented to illustrate the capabilities of the proposed framework, both for static configurations and the assessment of dynamic aspects during speech production.

@&#INTRODUCTION@&#
Speech production studies are currently served by a wide range of technologies that allow research on the dynamic aspects of speech. Methods such as ultrasound (US) and real-time magnetic resonance imaging (RT-MRI) (Scott et al., 2014) provide data regarding the position and coordination of the different articulators over time (Hagedorn et al., 2011). Furthermore, they offer the possibility to improve on the studies based on information regarding a static sustained production by reducing the hyperarticulation effect (Engwall, 2003).After image acquisition, the different regions of interest must be segmented (e.g., Bresch and Narayanan, 2009; Silva and Teixeira, 2015), or points of interest identified, often resulting in contours delimiting the vocal tract or specific structures such as the tongue or velum.Analysis of different vocal tract contours is typically performed visually by characterising the position of the different articulators or by describing articulator differences between different sounds (e.g., Delvaux et al., 2002; Shadle et al., 2008). This is often done by superimposing contours and performing qualitative analysis of the main differences (Martins et al., 2008; Cleland et al., 2011; Badin et al., 2014). Adding to the subjective nature of such analysis, when the database is large, e.g., as happens when RT-MRI is used (Niebergall et al., 2013), it becomes an almost infeasible task to explore all available data.Beyond the sheer amount of data made available by current technologies, the field of speech production faces several challenges that should be addressed to allow further advances, harnessing the full potential of the data available. A framework should be proposed that tackles the large amounts of data addressing, among others, the following aspects:•Objectivity — The subjective nature of the methods used to describe articulatory differences, for example, results in variability among researchers that precludes true comparison among works in the literature describing the same phenomena.Intra-speaker assessment — The analysis of articulatory features for different sounds produced by one speaker lacks methods to profit from multiple repetitions and common grounds for comparison among sounds.Inter-speaker assessment — The comparison among speakers lacks common grounds for comparison, e.g., a common normalised measure of difference, without losing sight of the contributions provided by each articulator.Variability — Not only average behaviour is relevant for the researchers, there is also a strong need to have information on variability (across repetitions, across speakers, etc.).Inter-language comparison — Data from multiple speakers of one language could be jointly used to provide overall quantitative characterisation of its main features. This would allow new ways of comparing sounds in multiple languages, advancing on the current status of inter-language comparisons mostly based on qualitative assessment of data from different speakers.Multimodality — Data provided by different modalities and concerning similar phenomena or providing complementary data might benefit from joint analysis. For example, several technologies that support speech production studies (e.g., EMA, Kim et al., 2014 and ultrasound, Laprie et al., 2014) are used in combination with MRI (Scott et al., 2014). Regardless of how the different data is analysed, if their individual contributions to the understanding of specific phenomena could be gathered in joint representations it might motivate a generalisation of multimodal studies and an easier interpretation of the data.One important route to attain a systematic analysis addressing these issues is to move towards quantitative methods that allow it to be performed automatically, in an expedite and replicable way, resulting in data providing a summary of the most important features which researchers can analyse. In the work presented here we consider these challenges in the scope of real-time MRI data.

@&#CONCLUSIONS@&#
This article proposes a quantitative framework for vocal tract profile comparison. To the best of our knowledge, this is the first time a quantitative analysis framework is presented encompassing the assessment of per sound intra-speaker variability, inter-speaker differences and overall inter-sound differences using data from all speakers in a single representation. The presented application examples show the potential of the proposed framework to take advantage of the increasing amounts of articulatory data available and move towards quantitative inter-speaker and inter-language comparison, both for static and dynamic analysis scenarios.The application of the proposed methods to other classes of sounds (e.g., laterals, Martins et al., 2010) and the study (and inclusion in the proposed visualisations) of landmark trajectories and their variability, considering all available speakers, should provide further insight into the usefulness of the proposed methods.Adding to the possible improvements already mentioned in the discussion, there are a few other lines of work that deserve further attention. Grounded on the principles and methods encompassed by the proposed framework, promoting systematic analysis of the available vocal tract data, the presented work can evolve to provide support to the use of high-level models of articulator organisation and control (e.g., TAsk Dynamics Application (TADA), Nam et al., 2015). Regarding the identification of relevant gestures, the analysis based on multiple realisations of each sound can already be helpful, and the instantiation of the framework considering other features (e.g., tongue body and tip constriction location and degree) can bring the outcomes closer to the variables required by the models. The aspect that needs to be further developed is the computation of temporal aspects that, despite being present in the dynamic analysis, is not explicitly addressed. In this regard, the long-term goal is the generation of gestural scores, from the data, that can serve as input, for example, to articulatory synthesisers (e.g., Teixeira et al., 2002; Birkholz et al., 2011)The use of multi-planar (e.g., Proctor et al., 2012) or 3D imaging (e.g., Martins et al., 2011; Zhu et al., 2013) of the vocal tract assumes importance for studying sounds exhibiting important characteristics not observable in the sagittal plane (Zhu et al., 2013). While the work presented here does not explicitly address the application of the framework to these kind of MRI data, there is no impediment to it, as long as the new features comply with the requirements set in section 2 and analysis follows the systematic procedure inherent to the framework. Instead of one midsagittal vocal tract contour, several contours can be considered, in different planes of interest. For instance, regarding lateral sounds (e.g., /l/), a comparison feature might be used accounting for the asymmetric nature of the lateral channels forming on the tongue sides (Martins et al., 2010), based on their section area, in the coronal plane, and their length.One route we are considering for further development of the proposed framework, addressing the multimodality challenge discussed earlier, is its use with data from different/multiple imaging modalities. We are currently considering the analysis of RT-MRI and ultrasound data simultaneously.The presented methods (and envisaged steps regarding computational analysis) are computationally demanding and generate large amounts of data. Therefore, their deployment in a cloud environment would provide a more suitable scenario for further developments and an important first step towards their validation and use by third parties such as phoneticians. The use of the proposed framework and its developments by other research groups, over their data, would pave the way for comparisons between dialects and languages. We are currently starting this migration process in the scope of projects Cloud Thinking11http://cloudthinking.web.ua.pt/ieeta/.and IRIS.22www.microsoft.com/pt-pt/mldc/iris/default.aspx.