@&#MAIN-TITLE@&#
Effect of segmentation on financial time series pattern matching

@&#HIGHLIGHTS@&#
We evaluate financial time series pattern matching and segmentation methods.PIP, PAA, PLA, and TP segmentation methods are analyzed.TB, RB, HY, DT, and SAX pattern matching approaches are evaluated.PIP achieves better performance and is especially superior when used with RB and HY.

@&#KEYPHRASES@&#
Segmentation,Pattern matching,Financial time series,Technical pattern,Head-and-shoulders,

@&#ABSTRACT@&#
In financial time series pattern matching, segmentation is often performed as a pre-processing step to reduce the data points from the input sequence. The segmentation process extracts important data points and produces a time series with reduced data points. In this paper, we evaluate the effectiveness and accuracy of four approaches to financial time series pattern matching when used with four segmentation methods, the perceptually important points, piecewise aggregate approximation, piecewise linear approximation and turning points methods. The pattern matching approaches analysed in this paper include the template-based, rule-based, hybrid, decision tree, and Symbolic Aggregate approXimation (SAX) approaches. The analysis is performed twice, on a real data set (of Hang Seng Index prices from the Hong Kong stock market) and on a synthetic data set containing positive and negative cases of a technical pattern known as head-and-shoulders.

@&#INTRODUCTION@&#
In financial trading, two types of analysis are usually used to predict future price movements. The first type, called “technical analysis”, involves forecasting a trend in the financial market based on historical data such as the daily prices and volumes traded. The historical data are commonly represented in the form of a time series. The second type is called “fundamental analysis”, which is the prediction of price movement based on the financial strength (health) of the company or on developments in the social, political and economic situation. In technical analysis, one of the crucial steps performed by traders is locating interesting patterns in the time series that can help to forecast future price trends. These patterns are commonly referred to as technical patterns (or chart patterns) in the financial domain. Some of the well-known technical patterns include head-and-shoulders (H&S), double top, triple top, cup-with-handle and range breakout [1].Pattern matching is one of the most important tasks for the analysis of financial time series and many novel pattern matching approaches have appeared in recent years. Zapranis and Tsinaslanidis [2] proposed a new approach based on neural networks to identify a technical pattern known as H&S. Zhou et al. [3] proposed a geometrical similarity measure approach that is invariant to shifting and scaling, and which calculates the angle between two vectors after shift-eliminated transformation. In [4], a kernel regression estimator of a given time series is constructed. The extrema on the original time series is identified based on the local minimum or maximum in the regression line. The extremas in the original time series are then used to determine whether or not a pattern has occurred. Rao and Principe [5] proposed a generalized eigendecomposition algorithm using two step Principal Component Analysis (PCA) process for segmenting speech signals. In [6], Ge et al. proposed an approach to model time series with Hidden semi-Markov Model (HSMM) to detect specific waveform patterns. Symbolic Aggregate approXimation (SAX) [7] allows a time series of arbitrary length to be reduced to a string of arbitrary length. Barnaghi et al. [8] proposed an enhanced SAX which uses K-means clustering method to determine the zones of the symbols. To calculate the similarity measure, Damerau–Levenshtein distance [9,10] is also used to find the edit distance between two strings. Kullback–Leibler divergence [11], Jensen–Shannon divergence [12] or Bhattacharyya distance [13] are also used to compare the difference between two probability distributions.A number of comprehensive surveys on time series pattern matching are also reported in literature. Fu [14] gives a review on time series data mining and categorizes the time series data mining research into representation, indexing, similarity measure, segmentation and visualization. Xing et al. [15] survey sequence classification methods in terms of methodologies and application domains. A comprehensive survey of control-chart pattern-recognition methods is also reported in [16].Dynamic time warping (DTW) is one of the most popular similarity measure based approaches for time series pattern matching [17]. Some researchers have applied an extended version of dynamic time warping (DTW) for pattern matching. For instance, Li et al. [18] proposed a novel similarity measure approach based on piecewise linear approximation and derivative DTW. Junkui and Yuanzhen [19] accelerated the DTW process by terminating the calculation earlier when the values of the neighbour cells in the cumulative distance matrix exceeded the tolerance level. Chen et al. [20] proposed a new distance calculation method called DTW-D that combines DTW and Euclidean distance (ED) for time series semi-supervised learning algorithms. In [21], string kernels are used to measure similarity of strings in linear time by using annotated suffix trees. However, computing string kernels using suffix trees does not scale well to problems with large data size [22]. To alleviate this problem, Teo et al. [22] compute string kernels by designing a space efficient and scalable algorithm using Enhanced Suffix Arrays [23]. None of these pattern-matching approaches (DTW, ED, string kernels) require the size of the query pattern to be the same as the size of the sub-sequence.A number of pattern-matching approaches require segmentation as a pre-processing step. These methods include the template-based (TB) [24], rule-based (RB) [24], hybrid (HY) [25] and decision tree (DT) approaches, which are all discussed in Section 2.4. All of these pattern matching approaches require the size of the query pattern to be the same as the size of the sub-sequence.To reduce the number of data points in the original time series, segmentation methods have been commonly used as a pre-processing step in time series analyses. These segmentation methods include the perceptually important points (PIP) method [26], the piecewise aggregate approximation (PAA) method [27], the piecewise linear approximation (PLA) method [28] and the turning points (TP) method [29]. Three variations of the PIP method were compared by Fu et al. [24]. They considered the vertical distance PIP (PIP-VD) to be the best choice.In using these segmentation methods, analysts must consider that each method involves a different process for the selection of data points from the input time series. Therefore, the resulting time series can be significantly different, depending on the segmentation method used. Such differences in segmented time series can have a profound effect on the pattern matching results. A number of studies have been conducted on the effects of segmentation on pattern matching methods. Chen et al. [30] compared the PIP-based evolutionary pattern discovery approach with the discrete wavelet transformation (DWT) approach (which is based on the pattern discovery approach). Their proposed approach solved the problems of information loss, distortion of segments and generation of meaningless patterns that had been associated with the DWT-based approach. Fu et al. [24] compared efficiency and effectiveness among several pattern matching approaches, namely the TB approach (which uses the PIP-VD segmentation method), the RB approach (which also uses the PIP-VD) and the PAA-based approach proposed by Keogh et al. [28]. The two approaches based on PIP performed better than the PAA-based approach. The TB approach provided an effective method, but the RB approach showed a better ability to describe query patterns. Zhang et al. [25] compared the processing time and accuracy of the proposed HY approach with that of two other pattern matching approaches the ED-based method and the slope-based method. The experimental results showed that the HY approach was more effective and efficient than the ED-based method or the slope-based approaches.Comparisons of the PLA, PAA and PIP segmentation methods in terms of on-line use, representation interval and complexity were discussed by Si and Yin [29]. They reported that the PAA approach could be directly used for on-line representation, but the PIP and PLA approaches were unsuitable. PAA is based on segments with identical length calculations, but PIP and PLA are based on the degree of fluctuation in the time series. The complexity rating of PAA is O(n), and the ratings of PIP and PLA are both O(n2). PAA is similar to the operation of removing redundant information from triangle meshes [31].Si and Yin [29] also compared a segmentation method based on TPs with two common segmentation methods, PLA and PIP, in terms of capacity for reconstructing error and ability to keep trends. The PLA approach produced the least amount of errors and the fewest trends, and the proposed TP approach preserved more trends than the PLA or the PIP approaches. All of these studies, however, investigated only one or two pattern matching approaches each. To the best of our knowledge, no comparative analysis has been ever performed on all of the well-known methods of data segmentation methods and pattern matching.In this paper, we evaluate the effectiveness and accuracy of four well-known approaches to pattern matching (the TB, RB, HB and the DT approaches) when used with four segmentation methods as the pre-processing step. These four segmentation methods are the PIP method [26], the PAA method [27], the PLA method [28] and the TP method [29]. We use the technical pattern known as H&S as a query pattern to better understand how these segmentation methods affect the pattern matching approaches.The remainder of the paper is organised into four sections. We briefly review the algorithms used for segmentation and pattern matching in financial time series in Section 2. In Section 3, we report the experimental results obtained from evaluation of segmentation and pattern matching algorithms applied to price data from the Hong Kong stock market. In Section 4, we summarise our findings and discuss directions for future research.The term “time series” is defined as an ordered list T=[(t1, x1), (t2, x2), …, (tn, xn)]. Len(T) represents the number of points in T. As the value tiis sequential (e.g., 1, 2,…, n), T can be simplified to T=[x1, x2, …, xn] and Tiis often used to denote the element xi. Accordingly, the sub-sequence S of T is Ti,j=[xi, xi+1, …, xj], where i and j are the start and end points of the sub-sequence.The aforementioned four pattern matching approaches (the TB, RB, HY and DT approaches) all require a pre-processing of the input sequence to reduce the number of data points until the length of the input sequence is the same as the query pattern. The well-known segmentation method, PIP, was first introduced by Chung et al. [26]. The variants of PIP are PIP-ED, PIP-VD and perpendicular distance PIP. Fu et al. [24] considered the PIP-VD method to be the best choice among these three variants in terms of efficiency and effectiveness. Therefore, we choose the PIP-VD method for our experiment. The generic algorithm of PIP is described in Algorithm 1[24]. With the time series T, the first and the last data point in the time series are the first two PIPs. The third PIP is the point in T with maximum vertical distance to the line joining the first two PIPs. The fourth PIP is the point in T with maximum vertical distance to the line joining its two adjacent PIPs, either between the first and second PIPs or between the second and the last PIPs. This process continues until the length of the segmentation sequence SP is equal to the input sequence Q. An illustration showing the selection of five PIPs from a time series is shown in Fig. 1.Algorithm 1Pseudocode of the PIP identification [24]Function: PIP Identification (T, Q)Input: sequence T of Len(T) = m, template Q of Len(Q) = nOutput: pattern SP of Len(SP) = nSet SP1=P1, SPn=PmrepeatSelect point Tjwith maximum distance to the adjacent points in SP (SP1 and SPninitially)Add Tjto SPuntil all SP are all filledreturn SPThe PAA method was proposed by Keogh and Pazzani [27]. In PAA, a time series T of length n is represented by the compressed time series T′ of length N. That is, T=(x1, …, xn) is represented by T′=(x′1, …, x′N). The time series T is divided into N equal-sized parts and each part is represented by the mean value of the data points in that part. The ith element of T′ can be calculated by using Eq. (1).(1)x′i=Nn∑j=sieixjwhere siand eidenote the start point and end point of the ith part, respectively. An illustration showing the selection of five points with PAA is shown in Fig. 2.The PLA method uses several straight lines to segment a time series T. PLA can be obtained through the sliding window, top-down or bottom-up methods [28]. In our experiment, we choose the bottom-up method for obtaining PLA. The generic bottom-up algorithm [28] for PLA is described in Algorithm 2. In this bottom-up method, the time series is represented by a number of segments in the first FOR loop. The costs of merging the neighbour segments are calculated in the next FOR loop. In the WHILE loop, the two neighbouring segments with the lowest merge-costs are combined until the minimum merge cost is less than the threshold. In this experiment, the merging process continues until the number of data points in the sequence is equal those in the query pattern. Fig. 3gives an illustration showing the selection of five points with PLA.Algorithm 2The generic algorithm of the PLA-bottom up [28]Function: Seg_TS = Bottom_UP(T, max_error)for i = 1: 2: Len(T) {Create initial fine approximation.}doSeg_TS = concat (Seg_TS, create_segment (Ti,i+1));end forfor i = 1: Len(Seg_TS)-1 { Find the cost of merging each pair of segments.}domerge_cost (i) = calculate_error ([merge (Seg_TS(i), Seg_TS(i +1))]);end forwhile min (merge_cost)< max_error {While not finished.}doi = min (merge_cost); { Find the cheapest pair to merge.}Seg_TS(i) = merge (Seg_TS (i), Seg_TS(i + 1))); {Merge them.}delete (Seg_TS(i + 1)); {Update records.}merge_cost (i) = calculate_error (merge (Seg_TS(i), Seg_TS(i + 1));merge_cost (i-1) = calculate_error (merge (Seg_TS(i-1), Seg_TS(i));end whileThe TP method [29] comprises an identification phase and an evaluation phase. The pseudocode for identifying and evaluating TPs is described in Algorithm 3. First, all of the local maximum and minimum points are marked as the TPs from the time series. Next, the importance of all of these points is evaluated. For retrieval, all of the TPs are then stored in a stack according to their importance. In our experiment, a certain number of TPs are pop out from the stack to represent a sequence. An illustration showing the selection of five TPs from a time series is shown in Fig. 4.Algorithm 3Pseudocode for identifying and evaluating TPs [29]Function: TPs_identification_phaseInput: T[1..n]Output: TP[1..m]for each point i in T[1..n]doif T[i-1] < T[i] and T[i] > T[i+1]thenput T[i] into TPend ifif T[i-1] > T[i] and T[i] < T[i+1]put T[i] into TPend ifend forFunction: TPs_evaluation_phaseInput: Sequence TP[1..m]Output: TPstack TPS[1..m]Calculate each TPs importance value.repeatPush the TP with lowest importance value into the stack TPS.if a point that is no longer minimum or maximum is found in TP[1..m]thenPush it into TPSUpdate the neighbour TPs importanceend ifuntil all point in TPs are pushed into TPSreturn TPSSymbolic Aggregate approXimation (SAX) [7] transforms a time series into a sequence of symbols. A time series is firstly segmented using PAA into several parts and then each part is represented by a symbol. The total number of symbols (α) to represent a time series is determined in advance. Breakpoints are a sorted list of numbers B=β1, …, βα−1 such that the area under a standard Gaussian curve from βito βi+1=1/α where β0 and βαare defined as −∞ and ∞, respectively.After the PAA step, each part segmented by PAA in the time series is represented by a mean value. These mean values are mapped into the regions in the look-up table and represented by the symbol in the corresponding region. The look-up table of breaking points which divide the Gaussian curve into regions of equal area is shown in Table 1. For example, to represent a time series with 3 characters (i.e. when α=3), we divide the Gaussian curve into 3 regions of equal area. As shown in the Table 1, two break points β1 and β2 are needed and each region is represented by a symbol. The mean values within the three intervals (−∞, −0.43], (−0.43, 0.43] and (0.43, ∞) can now be represented as symbol A, B and C, respectively.After the PAA step, two time series P=p1, …, pnand Q=q1, …, qn, are transformed into symbolic sequencesP′=p′1,…,p′wandQ′=q′1,…,q′wwhere n andware the lengths of the time series and the symbolic sequence. The minimum distance between two symbolic sequences is calculated by [32]:(2)MINDIST(P′,Q′)=nw∑i=1w(dist(pi,qi))2where dist(·) is a function for calculating the distance between two symbols and it can be defined as:(3)dist(ci,cj)=0|i−j|≤1βj−1−βii<j−1βi−1−βji>j+1Most pattern matching approaches (such as the TB, RB, HY and DT approaches) can perform their operations after the time series are segmented to reduce the data points.The TB approach [24] measures the similarity between the defined pattern templates and the sequences by calculating their point-to-point amplitude distance (AD) and temporal distance (TD). These distances are conditional expected deviations. The pattern template and the segmented sequence may have different amplitudes. Therefore, the data points need to be rescaled so that the comparison between sequences in different amplitudes can be performed. Rescaling can be performed by normalizing all the sequence values to a given range. The amplitude distance (AD) is defined as:(4)AD(SP,Q)=1n∑k=1n(spk−qk)2To address the horizontal distortion of the segmented sequence against the pattern templates, rescaling of the time dimension should be performed. The temporal distance (TD) is defined as:(5)TD(SP,Q)=1n−1∑k=2n(spkt−qkt)2The similarity measure is(6)D(SP,Q)=w1×AD(SP,Q)+(1−w1)×TD(SP,Q)where Q denotes the pattern template and SP denotes the segmented sequence. Variablew1∈[0,1]is the weight for compensating between vertical and horizontal variations among different kind of patterns. The variables spkand qkdenote the points in SP and the pattern template, respectively. The variablesspktandqktdenote the time coordinates of the points spkand qk. As with the weights configuration described by Fu et al. [24], we setw1=0.5for our experiment.The RB approach [24] uses predefined rules to identify patterns. A sequence is recognised as a matching pattern if its segmented sequence complies with the rules of a given pattern. The rules for the H&S pattern are defined as follows:sp4 > sp2 and sp6sp2 > sp1 and sp3sp6 > sp5 and sp7sp3 > sp1sp5 > sp7diff(sp2, sp6) < 15%diff(sp3, sp5) < 15%The HY approach [25] first calculates the Spearman's correlation coefficient of the sequence. The Spearman's correlation coefficient is calculated by(7)ρ=1−6×∑i=1ndi2n(n2−1)The variable diis the difference between the predefined rank of a data point on the template pattern and the rank of a corresponding data point on the segmented sequence. The Spearman's correlation coefficient of the sequence is compared with the threshold of eight technical patterns (H&S, double top, triple top, spike top, H&S reversed, double top reversed, triple top reversed and spike top reversed) to get the post patterns. The post patterns that pass the rules of the given patterns are identified as the matching patterns. In our experiment, we test the similarity between the sequence and the technical pattern H&S (H&S). The rules of this technical pattern are defined as follows:Rule 1:SP2–SP6<15%,Rule 2:SP3–SP5<15%,Rule 3: the ranking of SP4 is first,Rule 4: the ranking of SP2 and SP6 must be 2 and 3,Rule 5: the ranking of SP1 and SP7 must be 5 or 6 or 7.As in the RB and HY approaches, the relative positions of the points on the pattern can be used to define the rules. Based on these rules, a DT approach is proposed in this paper. From our testing of the previously proposed pattern matching approaches, we find that the relative position of the points in each pattern can be adopted to define the rules. For example, there are seven points in H&S. The fourth point must be the highest point among the six. Therefore, the relative position of the fourth point must be 1. By analysing the possibilities for the relative positions of each point in H&S, we can define the rules as shown in Table 2.In Table 2, each rule represents one possible combination of seven points, and P1, P2, …, P7 denote the first point, second point, …, seventh point and so on. The number in each entry in the table represents the relative position of that point. For example, 1 represents the highest point, and 7 represents the lowest point.After defining these eight positive cases, we define the negative cases for training a DT. For generating negative cases, we simply generate all permutations of 1, 2, …, 7. Except for the eight rules shown in Table 2, all other generated rules are defined as negative training examples. After these positive and negative cases are created, a DT can be trained.There are other pattern matching methods which do not require segmentation as a pre-processing step. These methods include the Euclidean distance (ED) and the dynamic time warping (DTW) approaches. Both of these approaches measure the similarity between the original time series and the query sequence directly, without segmentation. In the following paragraphs, we briefly review these two approaches. A pattern matching approaches for symbolic strings and a similarity measure between two probability distributions are also reviewed.The ED approach measures the similarity of time and query sequences by calculating the point-to-point ED between the two sequences. This distance between the two sequences X(x1, …, xn) and Y(y1, …, yn) can be represented by the following equation [33]:(8)ED(X,Y)=∑i=1n(xi−yi)2The two sequences are said to be similar if the ED is less than the threshold.Berndt and Clifford [17] proposed DTW to measure the similarity between time series with different lengths and to match the similar sequences that are out of phase. Given two sequences X(x1, …, xn) and Y(y1, …, ym), an n-by-m matrix M is constructed. The elements d(xi, yj) in the matrix represent the ED between the points xiand yj. The warping pathW=w1,w2,…,wk(max(m, n)≤K<m+n−1) is a neighbouring set of elements in M. The warping path follows three constraints, that is, the boundary conditions, continuity and monotonicity. The boundary conditions arew1=(x1,y1)andwK=(xn,ym). Continuity means that givenwk=(a,b), thenwk−1=(a′,b′), where a−a′≤1 and b−b′≤1. Monotonicity means that a−a′≥0 and b−b′≥0. The optimal warping path DTW(x, y) is defined as(9)DTW(x,y)=min∑k=1k=KwkThe optimal warping path DTW(x, y) that minimises the warping cost is calculated by dynamic programming. The cumulative distance γ(i, j) is defined as the distance d(xi, yj), which is found in the current cell, and the minimum of the cumulative distances from the adjacent elements.(10)γ(i,j)=d(xi,yj)+min{γ(i−1,j−1),γ(i−1,j),γ(i,j−1)}Damerau–Levenshtein distance [9,10] can be used to calculate the edit distance between two strings (i.e. symbolic sequences). The edit distance between two strings A and B is defined as the minimum number of edit operations needed in converting strings A into B or vice versa. Edit operations are defined as insertion, deletion, substitution and transposition in Damerau-Levenshtein distance. Given two strings A and B of length m and n (m≤n), the edit distance between A and B is computed by filling an (m+1)×(n+1) matrix D. Each cell D(i, j) in matrix D is filled by following recurrences [34]:D[i,0]=iD[0,j]=j(11)D[i,j]=D[i−1,j−1],ifA[i]=B[j].D[i−1,j−1],ifA[i−1..i]=BR[j−1..j]andD[i−1,j−1]>D[i−2,j−2].1+min(D[i−1,j−1],D[i−1,j],D[i,j−1]),otherwise.where A[i] denotes the ith character in A, A[i..j] denotes the substring of A and the superscript R denotes the reverse string. The cell D[m+1, n+1] is the edit distance between A and B. A financial time series is represented by real numbers. Therefore, we need to transform a financial time series to a symbolic sequence if we apply the Damerau–Levenshtein distance to calculate the similarity.Kullback–Leibler divergence [11], Jensen–Shannon divergence [12] and Bhattacharyya distance [13] are commonly used to compare the difference between two probability distributions in information theory and statistics. In [35], the similarity between two time series is calculated by the Kullback–Leibler distance between the transition matrixes of the two time series. To obtain the transition matrix of a time series, the time series should be transformed into a Markov chain. Given a time series x=(x0, …, xi, …), where each xibelongs to one of the m states of a variable X, the generating process of the sequence is a Markov chain if the conditional probability that the variable xtvisits state i at time t is independent of (x0, …, xt−2), that is, p(xt=i|(x0, …, xt−1))=p(xt=i|xt−1). A Markov chain is represented by the m×m transition matrix P and each element in P is pij=p(xt=i|xt−1=j). The Bayesian estimate of pijcan be defined as follows [35]:(12)pijˆ=αij+nijαi+ni(13)ni=∑jnij(14)αi=∑jαijwhere nijdenotes the transition frequency from state i to state j and αij=1mis a hyper-parameter. In [36], Kullback–Leibler distance is used to calculate the distance between two categorical sequence and each time series can be represented as a sequence of states (i.e. a Markov chain). For two transition probability matrix P1 and P2 for two Markov chains, let P1ijand P2ijdenotes the probabilities of the transition from state i to state j in P1 and P2. The asymmetric Kullback–Leibler distance of P1 and P2 is computed by [35]:(15)d(P1i,P2i)=∑j=1mP1ijlogP1ijP2ijThe symmetric version of Kullback–Leibler distance is defined as:(16)D(P1i,P2i)=d(P1i,P2i)+d(P2i,P1i)2The average distance between P1 and P2 is:(17)D(P1,P2)=∑iD(P1i,P2i)m

@&#CONCLUSIONS@&#
The TB pattern matching approach measures the similarity of the absolute positions of the data points between the pattern template and the segmentation result of the sequence. The RB, HY and DT approaches measure similarity according to the relative fluctuations of the data points in the segmentation result of the sequence. From our experiments, we find that the PIP segmentation method can perform well to preserve the overall shape of the sequence. As a result, the PIP method achieves better performance than the other segmentation methods and is especially superior when used with the RB and HY pattern matching approaches. The goal of the PLA method is to minimise reconstruction errors. We find that PLA works well with all four pattern matching approaches, especially with the TB and DT approaches. Although the TP method can preserve more trends, it is inferior to the other segmentation methods when the length of the pattern is known or predefined by the user. The PAA method is designed to smooth out the input sub-sequence. As a result, PAA cannot preserve the up and down trends of the original sequence. The PAA method performs poorly with the RB, HY and DT approaches.SAX transforms a real value time series into a symbolic sequence and MINDIST function is used to calculate the distance between two symbolic sequences. The synthetic data experiment shows that Case 1 has higher accuracy than Case 2 while Case 2 has higher precision than Case 1. When it is compared to other approaches, TB with PIP has higher accuracy than SAX in Case 1. The real data experiment shows that SAX is just as good as other pattern matching approaches.When SAX is used to represent a query time series, it smoothes the time series since PAA is used as a preprocessing step. Therefore, the outcome of PAA can affect the overall process of SAX approach. When SAX is used to represent a template pattern, analysts need to decide the number of symbols that are used to represent the template and how to assign the symbols to the real value points from the template. Therefore, any bias in decision making can significantly affect the results. Although SAX approach is selected for evaluation in our experiments, other approaches can be used to measure the similarity between two symbolic sequences (e.g. edit distance of two strings). We can also model each symbol as a state and then use a Markov model to calculate the probability of the time series.For future work in this area, we plan to develop an adaptive pattern matching system for streaming on-line data with appropriate pattern matching algorithms and segmentation methods.