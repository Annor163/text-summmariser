@&#MAIN-TITLE@&#
A linear programming decomposition focusing on the span of the nondegenerate columns

@&#HIGHLIGHTS@&#
We identify the limits of the existing improved primal simplex (IPS).We revise every step of the dynamic reduction implemented in IPS.We show how basic solutions can be built to warm start the solution of each problem.A simple and fast procedure can test the potential for improvement of the algorithm.The algorithm outperforms IPS and CPLEX’s primal simplex on a large benchmark.

@&#KEYPHRASES@&#
Linear programming,Degeneracy,Improved primal simplex,Decomposition,Primal algorithms,

@&#ABSTRACT@&#
The improved primal simplex (IPS) was recently developed by Elhalaloui et al. to take advantage of degeneracy when solving linear programs with the primal simplex. It implements a dynamic constraint reduction based on the compatible columns, i.e., those that belong to the span of a given subset of basic columns including the nondegenerate ones. The identification of the compatible variables may however be computationally costly and a large number of linear programs are solved to enlarge the subset of basic variables. In this article, we first show how the positive edge criterion of Raymond et al. can be included in IPS for a fast identification of the compatible variables. Our algorithm then proceeds through a series of reduction and augmentation phases until optimality is reached. In a reduction phase, we identify compatible variables and focus on them to make quick progress toward optimality. During an augmentation phase, we compute one greatest normalized improving direction and select a subset of variables that should be considered in the reduced problem. Compared with IPS, the linear program that is solved to find this direction involves the data of the original constraint matrix. This new algorithm is tested over Mittelmann’s benchmark for linear programming and on instances arising from industrial applications. The results show that the new algorithm outperforms the primal simplex of CPLEX on most highly degenerate instances in which a sufficient number of nonbasic variables are compatible. In contrast, IPS has difficulties on the eleven largest Mittelmann instances.

@&#INTRODUCTION@&#
We consider a linear program (LP) in standard form:(P){mincTxs.t.Ax=bx≥0,wherex,c∈Rn,b∈Rm,andA∈Rm×n. We assume thatAis of full rank m with m ≤ n and that the feasible domainFP={x≥0:Ax=b}is nonempty. A basis is a set of m independent columns ofA, and the associated variables are said to be basic. Starting from the indicesBof the basic variables andNof the remaining nonbasic variables, the associated basic solution is obtained by setting(1)xB=A·B−1bandxN=0,where for any set of indicesJ,A·Jis the set of columns ofAindexed byJ,andxJis the corresponding subvector of variables. More generally, the submatrix ofAcontaining the rows indexed byIand the columns indexed byJwill be denotedAIJ. The basic solution is feasible if and only ifxB≥0. If{j∈B:xj=0}is not empty, the solution is said to be degenerate, and all the variables indexed by this set are degenerate. The remaining nonzero basic variables are the nondegenerate variables.Starting from a basic feasible solution, the primal simplex algorithm (see Dantzig, 1955) monotonically improves the objective value by going through a sequence of neighboring feasible bases until optimality is reached. One theoretical limitation of the algorithm is that an iteration may not lead to any progress in the objective value if the solution is degenerate. Geometrically, a degenerate vertex of the n-dimensional feasible polytope of an LP is the intersection of more than n constraints of this LP. In terms of the simplex algorithm, this means that a single vertex can correspond to several bases. The difficulty is that sometimes many iterations move from one basis to another associated with the same vertex. As a consequence, the theoretical convergence of the simplex cannot be guaranteed without a pivoting rule such as those described in Bland (1977) and Charnes (1952).Although cycling is rarely an issue in practice, the risk of stalling is real. Several techniques have been developed to limit the negative effects of degeneracy (Benichou, Gauthier, Hentges, & Ribiere, 1977; Charnes, 1952; Gal, 1993; Gill, Murray, Saunders, & Wright, 1989; Greenberg, 1978), but recently there has been a growing interest in methods that take advantage of degeneracy. These studies all rely on the idea that degeneracy corresponds to a local excess of information, since degenerate basic variables are not needed to characterize a vertex of the polytope. Perold (1980) exploits this to develop a degeneracy structure in the LU decomposition of the basis, which involves fewer calculations when performing degenerate pivots. Pan (1998) took another important step in this direction by generalizing the concept of a basis. He defines a deficient basis to be a set of less than m independent columns ofAwhose range containsb. If the current solution is degenerate, it is sufficient to consider the deficient basis that contains only the positive variables. Degeneracy therefore becomes a potential opportunity to solve smaller linear systems at each iteration. Using deficient bases, Pan develops a simplex-like algorithm (Pan, 2008) and a dual projective algorithm (Pan, 2005) that show promising results in an experimental comparison with MINOS 5 (Murtagh & Saunders, 1983).Elhallaoui, Metrane, Soumis, and Desaulniers (2010); Elhallaoui, Villeneuve, Soumis, and Desaulniers (2005) also take advantage of degeneracy to speed up the solution of set partitioning problems by aggregating the original constraints into clusters of constraints. The feasibility of the solution is ensured by keeping only the variables that are compatible with the clusters, i.e., the variables that are either present in or absent from every constraint of each cluster. When no improvement can be made by considering the compatible variables, some clusters are broken up or combined to include new improving directions in the aggregated problem. The strength of this dynamic constraint aggregation is the focus on a problem with many fewer constraints than the original one. The improved primal simplex (IPS) (Elhallaoui, Metrane, Desaulniers, & Soumis, 2011) extends this approach to general linear programming. A reduced problem is formed by keeping only the nondegenerate and compatible variables. In this context, a variable is compatible when the corresponding column ofAis in the range of the p nondegenerate columns. With the incompatible variables removed, m − p constraints are redundant and thus ignored. Once the optimal solution of the reduced problem has been found, a complementary problem is solved to prove the optimality of the original LP or to identify a sequence of pivots ending with an improvement in the objective value. The authors report that IPS significantly outperforms CPLEX11CPLEX is freely available for academic and research purposes under the IBM academic initiative: http://www-03.ibm.com/ibm/university/academic.on flight assignment (FA), combined vehicle and crew scheduling (VCS), and uncapacitated facility location (UFL) problems, and Raymond, Soumis, and Orban (2010) describe implementation techniques that improve the performance of the algorithm.One important limitation of IPS is that compatible variables are identified through costly algebraic operations similar to those performed when computing a simplex tableau. As highlighted by Omer, Rosat, Raymond, and Soumis (2014), these operations are also useful when solving the complementary problem since they allow us to search for an improving direction in a reduced space, as is done in reduced gradient methods (Murtagh & Saunders, 1978). However, their tests on a diversified benchmark show that these operations cause IPS not to perform well on every highly degenerate LP. Raymond, Soumis, Metrane, and Desrosiers (2010) address this issue with a stochastic test requiring as many operations as the computation of a reduced cost to identify all the compatible variables. The authors apply this test to develop a partial pricing algorithm focusing on the compatible variables first. They report good results on the aforementioned VCS and FA instances, but their procedure struggles with two families of instances represented in Mittelmann’s benchmark. Based on this test, Towhidi, Desrosiers, and Soumis (2014) implement the positive edge pricing criterion within COIN-OR LP solver22https://projects.coin-or.org/Clp.(CLP). Their results show significant improvement with regards to the devex pricing criterion (Harris, 1973) for the most degenerate Mittelmann instances, but their comparison focuses on CLP, which is known to be less efficient than most commercial LP solvers. More importantly, the articles by Towhidi et al. (2014) and Raymond, Soumis, Metrane et al. (2010) show how a fast compatibility test can be used to cope with degeneracy, but they do not take advantage of degeneracy, since the size of the linear system solved at each simplex pivot is not reduced.Although the dual simplex and barrier algorithms often solve LPs more efficiently than the primal simplex, the latter has a strong advantage when a good feasible solution is available. As a consequence, the primal simplex is still used for reoptimization after modifications in the objective function, or after adding columns in the master problem in a column-generation procedure. Our work thus focuses on improving the primal simplex by taking advantage of degeneracy.Our main contribution is a new dynamic reduction algorithm that overcomes the difficulties that IPS encounters on large instances. This algorithm not only yields substantial improvement on many degenerate instances but also provides a fast procedure to test the potential for improvement in advance. To achieve this, we modify IPS to include the fast compatibility test described in Raymond, Soumis, Metrane et al. (2010). One negative effect is that the complementary problem cannot be reduced without doing the algebraic operations that we are trying to avoid. The algorithm thus focuses on a complementary problem involving the original constraints ofP,and it involves a new mode of alternation between the reduced and the complementary problems that is more efficient on large LPs. We then show how good basic solutions can be built to warm-start both the reduced and the complementary problems. The practical impact of these modifications is studied on a large benchmark including the VCS, FA, and UFL instances used in Omer et al. (2014) and 45 Mittelmann instances. The purpose is to evaluate our new algorithm by comparing it with IPS and the primal simplex of CPLEX, and to show that it is possible to identify quickly the instances that offer a strong potential for faster solution. The results show the potential of the algorithm for an implementation as an adaptive strategy in a state-of-the-art primal simplex code.In Section 2 we describe IPS as a necessary background for the rest of the article. The new algorithm based on a fast compatibility test is developed in Section 3. The results of the experimental tests are presented and analyzed in Section 4, and in Section 5 we discuss directions for future research.In this section, we summarize the theoretical foundations and the practical implementation of IPS as a background for the new algorithm developed in Section 3. Although efficient implementations of linear programming algorithms should focus on LPs with bounded variables, we consider an LP in standard form to clarify and shorten the presentation. Omer et al. (2014) show the generalization to an LP with bounded variables, and the implementations tested in Section 4 use this generalization.Letx∈FPbe a basic feasible solution ofP. The variables’ indices can be partitioned into two setsP={j:xj>0}andL={j:xj=0}. Sincexis a basic solution, the variables indexed byPare basic, and the columns ofA·Pare linearly independent. The cardinality ofP,p=|P|,thus satisfies p ≤ m. Moreover,xL=0,so the range ofA·Pcontainsb.Remark 1If p < m,A·Psatisfies the definition of a deficient basis given by Pan (1998).Assuming thatx∈FP,a primal algorithm iteratively improves the objective value by following a sequence of feasible improving directions as introduced below.Definition 1feasible directiond∈Rnis a feasible direction atxif there exists ρ > 0 such thatx+ρ·d∈FP.d∈Rnis an improving direction ifcTd< 0, i.e., if taking a positive step alongdyields an improvement in the objective value.An improving vector is characterized by any pair (d, ρ) such thatdis a feasible improving direction and ρ > 0.dcan thus be normalized at will as long as ρ is chosen to satisfyx+ρ·d∈FP. One way to make a step toward optimality is to follow a normalized feasible improving direction that maximizes the rate of improvement in the objective value. The greatest normalized improvement (GNI) program finds one of these directions:(Gni){mindcTds.t.Ad=0wLTdL≤1dL≥0,wherewL>0is a normalization vector.Proposition 1Gnihas an optimal solutiond⋆. Moreover, denotingI−={i∈P:di☆<0},•xis an optimal solution ofPif and only ifcTd⋆ = 0;d⋆is an optimal ray ofPif and only ifcTd⋆ < 0 andI−=∅;ifcTd⋆ < 0 andI−≠∅,the maximal feasible step alongd⋆is performed by settingx←x+ ρmax  ·d⋆, whereρmax=mini∈I−{xidi}. This step leads to an improvement ρmaxcTd⋆in the objective value.d=0is feasible, soGniis feasible. Assuming thatGniis unbounded, it admits a feasible rayrwithcTr< 0. ∀λ ≥ 0, λ ·ris feasible, soλwLTrL≤1,withrL≥0andwL>0,hencerL=0. It follows thatA·PrP=0,withA·Pof full rank, sorP=0. This contradictscTr> 0.Gniis thus bounded and has an optimal solutiond⋆.Next, assume thatcTd⋆ < 0.d⋆ is an optimal ray ofPif and only ifd⋆ ≥0, or equivalently⇔I−=∅. IfI−≠∅,it is easy to verify that ρmax  > 0 is the largest step such thatx+ ρmaxd⋆ ≥0is feasible, and that it leads to an improvement ρmaxcTd⋆ < 0 in the objective value.xis not an optimal solution ofP,soxoptimal ⇒d⋆ ≥ 0.Finally, ifcTd⋆ ≥ 0, no improving feasible direction exists atx, soxis an optimal solution ofPif and only ifcTd⋆ = 0.□To take advantage of degeneracy, Omer et al. (2014) decomposeGniby referring to the following concept of compatibility.Definition 3compatible variableA variable xjis compatible with a deficient basisPif and only if the corresponding column inA, aj, is inSpan(A·P).The zero variables can then be partitioned into compatible and incompatible variables whose sets of indices are respectively denotedCandI. Based on this partition,Gniis decomposed into the reduced-Gni(R−Gni) and the complementary-Gni(C−Gni) that respectively focus on the compatible and incompatible variables:(R−Gni){min(dP,dC)cPTdP+cCTdCs.t.A·PdP+A·CdC=0wCTdC≤1dC≥0(C−Gni){min(dP,dI)cPTdP+cITdIs.t.A·PdP+A·IdI=0wITdI≤1dI≥0.The validity of this decomposition is justified by the following theorem of Omer et al. (2014), which extends to general LPs a result proved by Rosat, Elhallaoui, Soumis, and Lodi (2013) for set partitioning problems.Theorem 1LetzGni☆,zR−Gni☆,andzC−Gni☆be the optimal objective values ofGni,R−Gni,andC−Gni; then we havezGni☆=min{zR−Gni☆,zC−Gni☆}.Theorem 1 ensures that an optimal solution ofGnican be found by solvingR−GniandC−Gniseparately.One key feature of the decomposition is thatR−Gnican be further reduced. If the current solution is degenerate, i.e., if p < m, the rank of the constraint matrix ofR−Gni(p) is lower than the number of constraints (m), so m − p constraints ofR−Gniare redundant. Assume that the rows and columns are permuted so that the first p columns correspond to the nonzero variables and the first p rows correspond to independent rows ofA·P; thenR−Gniis equivalent to{min(dP,dC)cPTdP+cCTdCs.t.APPdP+APCdC=0wCTdC≤1dC≥0The practical benefit of the decomposition is that an efficient solution process can focus on the easy subproblemR−GniuntilzR−Gni☆=0,and then solveC−Gnito find an improving direction involving several incompatible variables or to prove the optimality of the current solution.One important step of the decomposition is the identification of the compatible variables. In IPS, the deficient basisA·Pis completed with m − p independent vectors to form a basisBofRm. LetP¯={1,…,m}∖P. The compatible variables are then identified using the following result.Proposition 2v∈Rmis compatible withPif and only if(B−1v)P¯=0.v∈Rmis written uniquely in the basisBas((B−1v)P,(B−1v)P¯).(B−1v)P∈Span(A·P)gives the coordinates ofvcorresponding toA·P,sov∈Span(A·P)if and only if(B−1v)P¯=0.□A computationally efficient completion ofA·Puses vectors of the canonical basis ofRm. Assuming a convenient reordering of the rows,Band its inverse are given byB=[APP0AP¯PIm−p]⇔B−1=[APP−10−AP¯PAPP−1Im−p].The compatible variables ofLare thus identified by searching for the zero columns ofA¯=−AP¯PAPP−1APL+AP¯L.A¯is a restriction of the simplex tableau to the m − p rows ofP¯,and it is well known that the tableau version of the simplex had to be abandoned because of these algebraic operations. A practical implementation of IPS thus requires us to limit the number of times that the compatible variables are identified, and to take advantage of the knowledge ofA¯.Omer et al. (2014) show that solvingR−Gniand following the resulting improving direction is equivalent to performing a simplex pivot in the original LP restricted to the nondegenerate and compatible variables,PC. The redundant constraints ofPCare identified by performing a LU decomposition ofA·Pusing UMFPACK (Davis & Duff, 1997). DenotingUthe upper triangle matrix provided by the decomposition, a set of dependent rows ofA·Pis deduced from the rows ofUwith at least one nonzero element. After removing the redundant constraints, this reduced LP is(PC){mincPTxP+cCTxCs.t.APPxP+APCxC=bPxP≥0,xC≥0The variable pivoted into the basis minimizes the normalized pricing criterionc¯i/wi,wherec¯T=cT−cPTAPP−1APC. As a consequence, if the partition(P,C,I)is not updated after the solution ofR−Gni,this problem may be solved several times by performing as many simplex pivots inPC. The drawback is that some basic variables may take a zero value, in which case degenerate pivots may occur. However, this risk is compensated for by the time saved on the update ofA¯. Unless the optimal solution is found, the solution ofPCis interrupted after an arbitrary number of pivots, and(P,C,I)is updated if two conditions that formulate the expected benefit of a new partition are satisfied. In Omer et al. (2014), IPS interrupts the solution ofPCevery m iterations, and in order to update(P,C,I)it requires that at least 10 percent of the basic variables ofPCare degenerate and 30 percent of the nondegenerate variables do not belong toP.Once an optimal solution ofPChas been found,C−Gniis solved to find a new improving direction or to prove the optimality of the current solution. In C-Gni, the linear constraintsA·PdP+A·IdI=0can be interpreted as follows: the weighted combination of the columnsA·IdIthat potentially enter the basis must be compatible. According to Proposition 2, another way of putting this condition isA¯·IdI=0. For a givendIsatisfying this condition, the unique vectordPsatisfyingA·PdP+A·IdI=0is then given bydP=−APP−1APIdI. SinceA¯·Iwas computed when the compatible variables were identified,C−Gnican be modified without additional operations to search for an optimal solution by solving a smaller LP that involves only the variables ofdI:(Cr−Gni)zC−Gni☆=mindIc¯ITdIs.t.A¯·IdI=0wITdI≤1dI≥0LetdI☆be an optimal solution ofCr−Gni. If(P,C,I)is not updated beforeCr−Gniis solved, the compatibility of the combinationA·IdIdoes not necessarily imply that the optimal solution ofCr−Gnicorresponds to a feasible direction forP. This is not a real issue in IPS, because this solution is not used to produce a strict improvement in the objective value but instead to select columns that should be added to the reduced problem. SincedI☆indicates an interesting direction in most cases, we select every variable indexed byI+={j∈I:dj☆>0}.Cr−Gniis then updated by removing the variables indexed byI+and solved several times until at least 10 percent of the variables ofIare selected.Algorithm 1 summarizes the overall procedure. In this procedure, IPS solvesPusing a dynamic reduction of the problem. The algorithm performs major iterations (steps 4–22) composed of a reduction phase (steps 4–12) and an augmentation phase (steps 13–22), until optimality is reached. At each major iteration, IPS performs simplex pivots on the reduced problemRed,and, depending on the state of the solution, the number of rows and columns inRedmay increase and/or decrease. The motivation for developing the dynamic reduction is that the solution may be accelerated if the majority of the pivots can be performed on a problem smaller than the original one.In this implementation,Redis always set toPCafter an update of(P,C,I),and it is gradually augmented with new columns after each solution of the complementary problem. Rows are added together with the columns to ensure that the constraints ofPthat do not appear inRedare the redundant ones (this does not appear explicitly in the algorithm).To ensure that the interruptions and subsequent warm starts are done efficiently, every LP is solved with a simplex algorithm. To be specific,Redis solved with the primal simplex andCr−Gniis solved with the dual simplex. Raymond, Soumis, and Orban (2010) justify the choice of the dual simplex forCr−Gniby a large number of computational tests. A more qualitative reason is that the initial solution ofCr−Gniis usually degenerate. Using the dual simplex offers an opportunity to escape from a highly degenerate vertex without going through a long sequence of degenerate pivots. From this perspective, IPS is a primal–dual algorithm that switches to the dual simplex when no improvement can be achieved in the subspace spanned by the variables ofP.In this section, we develop a new dynamic reduction process to include a fast compatibility test that does not require the computation ofA¯. This new algorithm, called IPSO, maintains the global structure of IPS: reduction and augmentation phases are iteratively performed until optimality is reached. However, the reduction and augmentation phases are both revised to operate with no access to the information contained inA¯. To characterize the differences between IPS and IPSO, we may bring out an analogy with the differences between Dantzig’s primal simplex and the revised simplex method. In its initial description, the primal simplex computes the simplex tableauA¯and updates it with each pivot. To avoid spending too much time in the computation ofA¯,the revised simplex records only the indices of the basic variables and it solves two linear systems to identify the entering and exiting variables at each iteration. Similarly to the revised simplex, IPSO records only the indices of the compatible variables and updates them by solving linear systems. One issue with this new process is that IPS also usesA¯to reduce the complementary problem. As a consequence, one important difference between IPSO and IPS is that the augmentation phase is completely redesigned to limit the time spent in the solution of the complementary problem.Algorithm 2 summarizes the new dynamic reduction process. The key features of IPSO include the adaptation of the positive edge criterion for fast identification of the compatible variables (steps 6 and 13) and a heuristic augmentation procedure that requires only one solution of the complementary problem at each phase (steps 13–27). Warm-starting the reduced and complementary problems is another essential contribution for an efficient transition between the phases (steps 16, 19 and 27). These important steps of the algorithm are detailed below.In steps 6 and 13 of Algorithm 2, we adapt the positive edge criterion of Raymond, Soumis, Metrane et al. (2010) for a fast identification of the compatible variables. More generally, this criterion can be viewed as a stochastic test of membership in the range of a set of independent vectors. As for IPS, Raymond, Soumis, Metrane et al. (2010) complete the deficient basisA·Pwith vectors of the canonical basis ofRmto form a basisBofRm. Let j ∈ {1, …, n}, anda¯j=(B−1aj)P¯. The essence of the positive edge criterion is then given by the following proposition.Proposition 3Letvbe a vector of m − p continuous random variables. Then, eitherajis compatible withPandvT·a¯j=0,orajis incompatible andP(vTa¯j=0)=0.Ifajis compatible withP,Proposition 2 states thata¯j=0,hencevTa¯j=0. Otherwise,vTa¯jis a nonzero continuous random variable, which implies that the probability it takes a particular value is zero.□Givenv∈Rm−p,letw∈Rmbe the solution ofwTB= (0, vT), i.e.,wT=vT(B−1)P¯.vTa¯j=wTaj,so the compatible variables may be identified by solving one linear system of m equationswTB= (0, vT) and computing one matrix/vector productwTA·L,which requires the same number of operations as the computation of a reduced cost vector. Compared with the operations performed by IPS, this test divides the complexity of the update of(P,C,I)by n − p.Remark 2In practice, the bit representation of floating points implies that the probability of mistakenly identifying a variable as compatible is greater than zero. This case is problematic if a false-compatible variable is positive in the solution ofRed,because it may result in this solution being outsideFP. Since the primal feasibility ofxis an essential assumption for the convergence of Algorithm 2, it is necessary to check that the solution ofRedis inFP. Ifx∉FP,feasibility is recovered by performing a few dual simplex pivots onPCwith every constraint ofP.In steps 4–12 of Algorithm 2,Redis solved to optimality, and reduced if needed. An augmentation phase is then executed from step 13 to step 27. Two slightly different augmentation phases may be executed depending on whether or notRedhas been reduced at step 7 since the last augmentation phase.Since constraints were removed during the reduction,Redincludes|P0|<mrows. Referring to the test at step 14, the complementary problemC−Gniis then used to select the variables ofIthat should be included inRed. Since the positive edge criterion is used to identify the compatible variables,A¯is no longer available to form the reduced complementary problemCr−Gni. While this implies that the complementary problem may take longer to solve, it also means that(P,C,I)may be updated before we solveC−Gni. With this update, the following proposition holds.Proposition 4The solution ofC−Gniat step 19 ofAlgorithm 2provides a feasible improving direction forPifzC−Gni☆<0,or proves that the current solutionxis optimal ifzC−Gni☆=0.Let(P0,C0,I0)be the partition computed during the last execution of step 6,(P,C,I)the partition computed at step 13, andR−GniandC−Gnithe complementary and reduced problems corresponding to(P,C,I).P⊂P0∪C0,soA·P⊂Span(A·P0)andC⊂P0∪C0. Therefore, (Redis solved to optimality)⇒zR−Gni☆=0. From Theorem 1, it follows thatzGni☆=zC−Gni☆. Proposition 1 then gives the result.□In IPSO, to avoid spending too much time in the augmentation phase, we solve the complementary problem just once before focusing again on the reduced problem. Assuming thatzC−Gni☆<0,the primal solution ofC−Gnithen provides an improving feasible direction atx. Let(dP☆,dI☆)be an optimal solution ofC−Gni; we may complete it with zeros to obtaind⋆. The solution is then updated viax←x+ ρmax  ·d⋆ for an improvementρmaxzC−Gni☆in the objective value, with ρmax  computed as in Proposition 1.The augmentation itself then relies on the dual solution ofC−Gni. Let (π, λ) be the dual variables ofC−Gni. The dual of the complementary problem is(Cd−Gni){max(π,λ)λs.t.ajTπ+wjλ≤cj,∀j∈IajTπ=cj,∀j∈Pλ≤0Proposition 5Let (π⋆, λ⋆) be the optimal solution ofCd−GniandI−={j∈I:cj−ajTπ☆<0}. Then1.I−=∅ifzC−Gni☆≥0.I−⊃{j∈I:dj☆>0}(≠∅)ifzC−Gni☆<0.Recall thatwj>0,∀j∈L. Since λ is maximized inCd−Gni,λ☆=zC−Gni☆. The first set of constraints ofCd−Gnithus implies thatcj−ajTπ☆≥wjzC−Gni☆,∀j∈I. It follows thatI−=∅ifzC−Gni☆≥0.Assume thatzC−Gni☆<0. Forj∈Isuch thatdj*>0,the complementary slackness ensures thatcj−ajTπ=wjzC−Gni☆<0,soj∈I−. As a consequence,{j∈I:di☆>0}⊂I−.□The optimal solution ofCd−Gniprovides a dual solution ofP,π⋆, that satisfies the complementarity conditionsxj·(cj−ajTπ)=0,j = 1, …, n, and maximizes the minimum normalized reduced cost(cj−ajTπ)/wj,j = 1, …, n. Proposition 5 thus suggests a heuristic augmentation strategy that selects the variables that will potentially lead to the greatest improvements in the objective value. We initialize the columns ofRedwith those indexed byP0∪C0at step 7, and we enlarge the problem by including the variables ofI−at each augmentation phase. Proposition 5 then ensures that every incompatible variable taking a positive value in the improving directiond⋆ is included inRed.One last important issue of this augmentation phase is the update of the rows ofRed. SinceI−may contain a large number of variables, the restriction ofPto the variables indexed byP∪C∪I−does not necessarily have redundant constraints. The operations that would be necessary to identify the redundant rows are thus skipped, and all the rows ofPare included inRed.Since every row is added to the reduced problem at the end of an augmentation phase (step 27),|B|=m. The test performed at step 14 thus concludes thatGniwill be used to select the variables ofIthat should be included inRed.To explain this alternative in Algorithm 2, we denote by(P,C,I)the partition computed at step 13. Nothing was done during the previous augmentation phase(s) to ensure that the columns compatible withPwere included inRed. If the columns indexed byCare not included inRed,solving Red to optimality does not necessarily imply thatzR−Gni☆=0,so Proposition 4 does not hold. As a consequence, the complete direction search problemGniis solved instead ofC−Gni. After we replaceC−GniwithGni,Proposition 4 holds and the remainder of the augmentation is unchanged.SinceC−GniandGniplay the same role in Algorithm 2, they are both referred to as the complementary problem. This simplifies the presentation and emphasizes the correspondence with the complementary problem of IPS.As in IPS,Redis solved with the primal simplex, andC−Gniis solved with the dual simplex. An essential point for the efficiency of the overall algorithm is a good dual feasible basic solution to warm-startC−Gni(orGni). We thus describe how such a basis is computed at steps 16 and 19 of Algorithm 2. In the following discussion, we extend the term basis to the set of indicesBwhen the corresponding set of columnsA·Bis a basis.Proposition 6LetBbe a basis ofP,xandc¯the corresponding basic solution and reduced cost vector, and(P,C,I)the partition corresponding tox.1.Letb=argminj∈L{c¯jwj}; thenB∪{b}is a dual feasible basis ofGniwhose objective value iszGni=minj∈L{c¯jwj}.LetbI=argminj∈I{c¯jwj}; thenB∪{bI}is a dual feasible basis ofC−Gniwhose objective value iszC−Gni=minj∈I{c¯jwj}.A proof of the second point is given in Omer et al. (2014); it can easily be adapted to show the first point.□At step 16 of Algorithm 2,Redhas m rows, so its optimal basis is also a basis ofP,hence Proposition 6 shows thatB∪{b}can be used to warm-startGni. IfRedcontains|P0|=p0rows, its optimal basis,B0,must be completed with m − p0 columns to form a basis ofP. In IPS,B0is completed with m − p0 vectors of the canonical basis ofRmto form a basisB1ofP. This corresponds to adding slack variables toPand enforcing zero lower and upper bounds. These slack variables are systematically added by CPLEX, so no additional operation is required in our implementation. The dual feasible basisB1D=B1∪{bI}is then a valid candidate to warm-startC−Gni.The basisB1Drepresents a neutral but poor initial guess forC−Gnisince it reflects only the dual information gathered when solvingRed. In particular, the dual solution does not take into account the rows indexed byP¯. Another completion is thus considered.Proposition 7LetBbe a feasible basis ofPand(P,C,I)be the partition associated with the corresponding basic solution. LetA·B0be a set of independent columns spanningA·P,and letP¯=B∖P. ThenA·B0∪P¯is a basis ofRm. Moreover, ifB0is a feasible basis ofPC0,thenB0∪P¯is a feasible basis ofP.A·Bis a basis ofRm,soSpan(A·P)⊕Span(A·P¯)=Rm,which implies thatA·B0∪P¯is a basis ofRm.IfB0is a feasible basis ofPC0,thenb=A·B0xB0withxB0≥0. As a consequence, any completion ofB0is a feasible basis ofP.□Since Algorithm 2 starts with a basic feasible solution, a complete basisBofPis always known. At step 19, a completion ofBbased on Proposition 7 is thus possible. LetB2be this basis ofP,andB2D=B2∪{bI}. ThenB1DandB2Dare two valid candidates to warm-startC−Gni. SinceCd−Gniis a maximization problem, the basic solution with the higher objective value is chosen.As for the complementary problem, it is important to start solvingRedwith a basis corresponding to the current solution. We must therefore find a basis corresponding to the updated solution after the augmentation (step 28).Proposition 8Assume thatPis bounded and that the current solutionxat step 13 is not optimal. LetB☆andd⋆be the optimal basis and solution of the complementary problem, andb∈{j∈P:xj+ρmaxdj☆=0}. ThenB=B☆∖{b}is a feasible basis ofPand the corresponding basic solution isx+ ρmaxd⋆.According to Proposition 1,xis not optimal implies thatd⋆ ≠0andPbounded implies that ρmax  < +∞, which shows that{j∈P:xj+ρmaxdj☆=0}is not empty. Next,d⋆ is the basic solution ofGniassociated withB☆,soA·B☆d☆=0,andA·BdB☆=−db☆ab. Since db≠ 0,ab∈Span(A·B)and the rank ofA·BandA·B☆are the same. Moreover,A·B☆is obtained by removing one row from the basic matrix ofGni,whose rank is equal to m + 1, so the ranks ofA·B☆andA·Bare equal to m. As a consequence,Bis a basis ofP.To see thatx+ ρmaxd⋆ is the corresponding basic solution, first recall that the initial basis ofGniincludesPand that the variables ofdPare unbounded. These variables cannot be chosen as the exiting variable in a dual pivot and thus remain in the basis until optimality, soB☆⊃{j:xj>0}∪{j:dj☆≠0}. As a consequence,A·B(x+ρmaxdB☆)=b+0andB⊃{j:xj+ρmaxdj☆>0},which completes the proof.□With this last proposition, it is finally possible to address the convergence of the overall algorithm.Theorem 2LetPbe a feasible LP. Assuming that the primal and dual simplex are finite, Algorithm 2 is finite and returns either an optimal solution ofPif it is bounded, or an optimal ray if it is unbounded.If Algorithm 2 is finite, it either ends becauseRedis found to be unbounded, or because the optimal solution ofGnisatisfies eitherzGni☆=0ord⋆ ≥0. If an optimal ray ofRedis found, it can be completed with zeros to form an optimal ray ofP. Otherwise, letx⋆ be the result of the algorithm. IfzGni☆=0ord⋆ ≥0, Proposition 1 ensures thatx⋆ is respectively an optimal solution or an optimal ray ofP.To prove that the algorithm is finite, we first notice that each step is finite if the primal and dual simplex are assumed to be finite. For the same reason, the loop covering steps 4 –12 eventually end with an optimal solution ofRed,so the algorithm performs finite major iterations starting at step 4 and ending at step 27. Letxbe the current solution before solvingC−GniorGni; Proposition 4 shows that ifxis not optimal, thend⋆ is an improving feasible direction. Moreover, Proposition 8 ensures thatx+ ρmax  ·d⋆ is a basic solution ofP. As a consequence, each major iteration ends with a basic feasible solution providing a strict improvement in the objective value. Since the number of basic solutions is finite, so is Algorithm 2.□In this section, we evaluate IPSO by comparing it to the primal simplex of CPLEX 12.4. The pricing criterion is systematically set to an approximate version of the steepest edge described by Goldfarb and Reid (1977) (CPX_PARAM_PRIIND=3), because it has similar performance to that of the automatic default pricing of CPLEX but allows for a consistent analysis of the results (see Omer et al., 2014). The other CPLEX parameters are set to their default values. We finally compare the performance of IPSO with that of IPS and with that reported by Towhidi et al. (2014) for the primal simplex with the positive edge pricing criterion.The tests are all performed on an OpenSuse operating system with an Intel(R) Core(TM) i7-3770 CPU @ 3.40 gigahertz processor, and all the LPs, includingRed,C−Gni,andCr−Gni,are solved with the simplex of CPLEX. The initial basic feasible solutions used by every algorithm are obtained by the primal simplex phase I of CPLEX. We set the time limit to 10 hours for every tested algorithm.Unless otherwise specified, CPLEX refers to the primal simplex of the LP solver in the rest of the article.In their analysis of the performance of IPS, Omer et al. (2014) emphasize that the algorithm was designed with some particular classes of highly degenerate problems in mind. It would be unrealistic to expect that it would achieve the impressive performance described in Elhallaoui et al. (2011); Raymond, Soumis, and Orban (2010) on every LP. Omer et al. (2014) identify three major reasons that IPS may not perform well on some instances. The first two relate to degeneracy. If only a few simplex pivots are degenerate (<30percent), then interrupting the primal simplex may be counterproductive. Otherwise, if only a few variables are degenerate (<20percent), then reducingPis unlikely to lead to a faster solution of the problem. Moreover, after a reduction ofRed,the problem contains only the positive and compatible variables. IfRedcontains only a few compatible variables, it is again likely that the whole process of dynamic reduction will not be advantageous. Referring to Omer et al. (2014), IPS does not perform well when|C0|≤0.5|P0|. In IPS, identifyingCis time-consuming, but this is not the case if the positive edge criterion is used. As a consequence, one important contribution of IPSO is that we can test its potential for improvement in advance.In practice, Algorithm 3 is run before Algorithm 2 to take into consideration the conclusions of Omer et al. (2014). This algorithm iteratively tests the potential of IPSO and perform simplex pivots onPuntil the potential of IPSO is shown or optimality is reached. A few pivots are done before the first time that the numbers of degenerate and compatible variables are tested to estimate the percentage of degenerate pivots. The maximum number of pivots M is then increased after each interruption of the simplex to limit the number of these interruptions when the tests are unsuccessful.If Algorithm 3 reaches step 14, IPSO will potentially improve the primal simplex, so the current solutionx0 is used to initializexat step 2 of Algorithm 2. Otherwise, the complete solution of the problem is done in Algorithm 3 with primal simplex pivots onP. In the latter case, the difference with an ordinary execution of the primal simplex is that the algorithm is interrupted a limited number of times to make unsuccessful tests. The only costly operation in the tests of Algorithm 3 is the identification of the compatible variables when building the partition(P0,C0,I0). As is stated in Section 3.1, this requires a similar number of operations as computing the reduced costs, so the complexity of the tests is the same as that of a simplex pivot. For a very small computational cost, Algorithm 3 may thus prevent from starting the dynamic reduction of IPSO on instances that should be solved more efficiently with the primal simplex.Another improvement ensures that IPSO performs well on highly degenerate LPs with only a few compatible variables. When the number of compatible variables is not sufficient to start IPSO, but the simplex performs a large number of degenerate pivots, it is interrupted every time it remains stuck on the same vertex for more than some large number of pivots. No reduction is done, butGniis then solved to find a feasible improving direction. In our tests, this process is triggered when more than 95 percent degenerate pivots are performed overall and the objective value stalls for more than 1000 pivots.The benchmark includes the VCS, UBFA, and UFL instances used in Omer et al. (2014), and 45 instances of Mittelmann’s benchmark33These instances are available online: http://plato.asu.edu/ftp/lpcom.html(last visited on 14.01.15).. For Mittelmann’s benchmark, we exclude only two instances that take less than 1 second to solve. As in Omer et al. (2014), we added the dual formulation of four LPs (neos1, neos2, neos3, rlfprim) whose number of constraints is much larger than the number of variables. The characteristics of the benchmark appear in Table 1, which includes the dimensions (n × m) and the density (ρA) ofA, and the main statistics for the phase II of the primal simplex of CPLEX, i.e., the number of pivots (piv), the percentage of degenerate pivots (degen piv), and the computational time (cpu). The CPLEX statistics result from two separate optimization runs for each model: the number of pivots and the total runtime are obtained with the default options, but the perturbations had to be disabled to compute the fraction of degenerate pivots. The column “phase I” indicates how much time is spent in phase I to find an initial feasible solution. Since the time and pivots spent in this phase I are common to every algorithm, they are excluded from the subsequent comparisons.In our first evaluation of IPSO we present detailed statistics on its execution and compare it with the primal simplex of CPLEX. The tests described in Algorithm 3 are performed before starting the dynamic reduction. The results of the instances solved with the dynamic reduction are reported in Table 2. The instances that are not degenerate enough or have too few compatible variables appear in Table 3. In both tables, we do not report the time spent in testing the potential of IPSO, because it is negligible compared to the solution times of IPSO and CPLEX.We first focus on Table 2, where “aug” is the number of augmentation phases, andm−pmand|C||P|are the average percentages of degenerate and compatible variables. The next four columns detail the time spent and the numbers of pivots performed when solving the reduced and the complementary problems. The remaining runtime is that spent in identifying the compatible variables and the redundant constraints ofPC; we do not report it in Table 2, because it is negligible when compared to the solution time of the reduced and complementary problems. The last two columns contain the ratios of the total number of pivots and runtime of CPLEX relative to those of IPSO, thus indicating improvement factors. The cpu improvement column shows that IPSO significantly (≥30 percent) outperforms CPLEX on 33 of 38 instances while CPLEX never outperforms IPSO, which means that Algorithm 3 successfully filters out the unfavorable cases. Moreover, IPSO divided the computational time by a factor larger than two on all the VCS and UBFA instances and on nine Mittelmann instances. Most of the computational time was spent in the reduced problem, which is consistent with the fact that only one complementary problem is solved at each augmentation phase. Finally, the six instances buildingen, neos1, neos2, neos3’, rlfprim’, and stat96v1 have very low|C||P|,soPis never reduced during the solution with IPSO. The improvement in runtime is achieved by solving one or more complementary problems to find improving feasible directions, as described in Section 4.1.Table 3 details the degeneracy and compatibility values when the tests in Algorithm 3 are not successful. The instances are organized according to the test that fails, and we report the highest observed values. IPS and IPSO are designed to take advantage of degeneracy, so it seems logical that the first 12 instances should not be tested. On the other hand, when solving the last ten instances, the primal simplex struggles with degeneracy, and the number of degenerate variables would allow for an advantageous reduction ofP. IPSO was thus executed without testing the number of compatible variables to evaluate the relevance of the chosen threshold. For eight instances out of ten, IPSO is outperformed by CPLEX although the improvement factor is significant in only five cases. This shows that the decomposition based on compatibility does not allow us to take advantage of degeneracy for every degenerate LP (only 21 of 34 degenerate Mittelmann instances), but it is possible to test the potential efficiency of IPSO before starting the dynamic reduction. Moreover, this test is fast, since the positive edge criterion allows us to find the number of compatible variables within a computational time that compares to that of a simplex pivot. As a consequence, the algorithm could be used as an adaptive strategy integrated in an efficient primal simplex.For a more general view on the available LP solvers, we compared IPSO and the primal simplex of CPLEX with the barrier and dual simplex algorithms of CPLEX. These comparisons focus on the solution times from scratch. The results show that the dual simplex and the barrier algorithm respectively outperform IPSO by an average 1.55 and 2.15 factor, whereas they outperform the primal simplex by an average 2.90 and 4.10 factor. Even though IPSO is clearly more competitive than the primal simplex, the improvement is not sufficient to make it the most efficient algorithm when solving an LP from scratch. Despite this, IPSO provides the best solution time for 10 percent of the instances, whereas the primal simplex outperforms the other three algorithms for only one instance. IPSO may then be useful as one of the concurrent methods used in the default behavior of CPLEX. Moreover, as highlighted in the introduction, the main benefit of the primal simplex is that it can be used to efficiently optimize an LP when a good feasible solution is available, so IPSO should be the best option in a large number of such situations.In this section we compare IPSO with the latest version of IPS described in Omer et al. (2014) and with the results reported by Towhidi et al. (2014) for the primal simplex of CLP including the positive edge pricing criterion (PE).Table 4 gives the computational time spent identifying the compatible variables (“PCI”) and solving the reduced problems (“Red”) and the complementary problem (“Cr−Gni”) during the execution of IPS, and total runtime of IPS (“total”). The last three columns give the cpu improvement of IPS, IPSO, and PE (when available in Towhidi et al. (2014)). The cpu improvement of IPS and IPSO are the ratios of the runtime of the primal simplex of CPLEX relative to those of IPS and IPSO, respectively. The cpu improvement of PE is the ratio of the runtime of CLP’s primal simplex relative to that of PE. In our tests IPS is started from the same phase I feasible solution as IPSO, whereas PE uses a different solution generated by CLP in Towhidi et al. (2014). Only the Mittelmann instances with sufficient degeneracy are included, and they are divided into two different groups depending on whether or not the compatibility test fails (|C||P|<50percent). The instances are then organized by increasing number of rows.The improvement factors show that IPS and IPSO have similar performance on the VCS and UFL problems. IPS outperforms IPSO on the UBFA instances, but the improvement factors are still high. Focusing on the first group of Mittelmann instances, IPS matches the performance of IPSO in four of sixteen cases but is significantly outperformed in the other twelve cases. Moreover, IPS experiences serious difficulties on the largest instances, while there is no obvious correlation between the performance of IPSO and the size of the instance. To explain the difficulties of IPS, we observe that the identification of compatible variables alone takes more time than the complete solution by CPLEX for buildingen, dbic1, neos3, pds-100, storm_125, storm_1000, watson_1, and watson_2. Moreover,Cr−Gnimust be solved a large number of times during the augmentation phases to select a significant number of incompatible variables. Finally, IPS does not perform better on the instances that would fail the compatibility test, but this test is time-consuming ifA¯has to be computed.Towhidi et al. (2014) use the positive edge criterion as a pricing criterion to select the compatible variables. More precisely, ifjC=argmin{c¯j:j∈C}andjI=argmin{c¯j:j∈I},the entering variable isxjCifc¯jC<ψc¯jIandxjIotherwise (0 < ψ < 1). They thus use the concept of compatibility to reduce the number of degenerate steps, but they do not take advantage of degeneracy by removing constraints from the LP. The comparison with PE is presented simply to indicate global trends, because the available data is partial and, more importantly, because the improvement factors of PE refer to CLP. Nevertheless, if we focus on the instances that satisfy the compatibility test, we see that IPSO’s improvement factors range from two to six for rail4284, neos1, neos2, and watson-2 whereas PE fails to improve the computational time of CLP. For rail4284 and watson-2, the success of IPSO is due to the reduction ofP,but for neos1 and neos2, solvingGnito find improving feasible directions makes the difference. On the other hand, PE clearly outperforms IPSO on all the instances that failed the compatibility test. In these cases, the advantage of PE is its flexibility. In contrast with IPSO, it does not exclusively focus on the compatible variables and does not need to rely on a heuristic augmentation phase. PE selects a compatible variable when its reduced cost is negative enough and an incompatible variable otherwise.

@&#CONCLUSIONS@&#
