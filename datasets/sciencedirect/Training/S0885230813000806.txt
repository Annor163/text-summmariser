@&#MAIN-TITLE@&#
Effect of acoustic and linguistic contexts on human and machine speech recognition

@&#HIGHLIGHTS@&#
We compared the automatic speech recognition (ASR) with that of humans (HSR).HSR improves dramatically when given some preceding words context.More contexts improve human word prediction, but do not recognition so much.ASR without any language model were much inferior to HSR.Thus the acoustic models in ASR should be improved rather than language models.

@&#KEYPHRASES@&#
Continuous speech recognition,Human speech recognition ability,Acoustic model,Language model,

@&#ABSTRACT@&#
We compared the performance of an automatic speech recognition system using n-gram language models, HMM acoustic models, as well as combinations of the two, with the word recognition performance of human subjects who either had access to only acoustic information, had information only about local linguistic context, or had access to a combination of both. All speech recordings used were taken from Japanese narration and spontaneous speech corpora.Humans have difficulty recognizing isolated words taken out of context, especially when taken from spontaneous speech, partly due to word-boundary coarticulation. Our recognition performance improves dramatically when one or two preceding words are added. Short words in Japanese mainly consist of post-positional particles (i.e. wa, ga, wo, ni, etc.), which are function words located just after content words such as nouns and verbs. So the predictability of short words is very high within the context of the one or two preceding words, and thus recognition of short words is drastically improved. Providing even more context further improves human prediction performance under text-only conditions (without acoustic signals). It also improves speech recognition, but the improvement is relatively small.Recognition experiments using an automatic speech recognizer were conducted under conditions almost identical to the experiments with humans. The performance of the acoustic models without any language model, or with only a unigram language model, were greatly inferior to human recognition performance with no context. In contrast, prediction performance using a trigram language model was superior or comparable to human performance when given a preceding and a succeeding word. These results suggest that we must improve our acoustic models rather than our language models to make automatic speech recognizers comparable to humans in recognition performance under conditions where the recognizer has limited linguistic context.

@&#INTRODUCTION@&#
The performance of automatic speech recognition (ASR) systems using large-vocabulary continuous speech recognizers (LVCSR) is sufficient for read speech, and such systems are used every day in dictation systems, broadcasting systems (Imai et al., 2000, 2007; Matsoukas et al., 2006; Boulianne et al., 2006; Ortega et al., 2009), etc. Good recognition of spontaneous speech, such as dialogs or lectures, is also required, but ASR system performance is still insufficient for these tasks because spontaneous speech involves phenomena such as much greater variation in the speed of speech, higher rates of speech, and wider variation in pronunciation, caused by coarticulation, than read speech. In this paper we use the term “coarticulation” to describe the influence of preceding and following sounds on pronunciation.ASRs use local acoustic information about sub-word units such as phones and syllables, along with the local linguistic context. Similar to the way in which ASRs function, human speech recognition (HSR) is also believed to use sub-word acoustic knowledge to discriminate the phones or syllables, and local linguistic information about the target word and its contexts, to predict the acoustic phenomena. In this paper we focus on evaluating the performance of automatic speech recognition based on conventional HMM acoustic and n-gram language models, by comparing them with human speech recognition based on a combination of acoustic knowledge and information about local context. More specifically, we compare the performance of an automatic speech recognition system using n-gram language models, HMM acoustic models, and combinations of the two, with the word recognition performance of human subjects who have access to only acoustic information, have information only about local linguistic context, or have access to a combination of both. All of the speech recordings used are taken from Japanese narration and spontaneous speech corpora.For large-vocabulary speech recognition, hidden Markov models (HMM) and n-gram models (typically with small n values, such as bigrams (n=2) or trigrams (n=3)) are used as acoustic and language models, respectively.HMM statistical models focus on acoustic features of sub-words, such as phones or syllables, and are usually designed to consider just the preceding or following sub-words in order to deal with short-term coarticulation.On the other hand, n-gram statistical models use local linguistic properties such as a conditional probability when given the preceding n−1 words. N-gram models work well with machine speech recognition algorithms, and can easily constrain the search space in an algorithm.Lippmann (1997) compared the overall performance of machine recognition systems with human recognition under various conditions, and revealed that machines were quite inferior to humans, especially under adverse conditions such as recognition with background noise, etc. For example, the human transcription error rates of read sentences from the Wall Street Journal, and of spontaneous conversations recorded over the telephone, were less than 0.4% and 4%, respectively (Ebel and Picone, 1995; Lippmann, 1997), error rates are far beyond the ability of machines. The recognition performance of humans and machines was also compared in a digit string recognition task (Lippmann, 1997). The string recognition error rate of the machine recognizer was 0.72% for wide-band speech, while that of humans was 0.105% for LPC synthesized speech, and humans recognized digit strings almost perfectly (the error rate was 0.009%). These results suggest that enough acoustic information is present in the feature parameters for recognition. Lippmann (1997) concluded that further work was needed to explore the differences between spontaneous and read speech, but he did not clearly explain the difference in linguistic-level performance.Braida et al. investigated the factors which determine clarity of speech for hearing-impaired listeners by comparing clear and conversational speech (Picheny et al., 1985), and acoustically analyzed these types of speech (Picheny et al., 1986). Then they compared HMM consonant recognition with human recognition under various noisy conditions (Sroka and Braida, 2004). Their research, however, dealt only with isolated sub-word recognition and did not treat continuous speech recognition.In May 2007, a special issue of “Speech Communication” was published with the theme “Bridging the gap between human and automatic speech recognition”, and comparison of human and machine speech recognition was revisited. In that issue, Scharenborg (2007) reviewed research comparing human and machine speech recognition, and pointed out that acoustic and linguistic models could be improved by incorporating knowledge obtained from human speech recognition research.Shinozaki and Furui (2003) also studied human and machine recognition of words in short word contexts and concluded that the major factors in machine misrecognition, which were caused by estimation errors of language models and acoustic models, included acoustic variation, ambiguity, etc. Our aim in this study is to discover the weaknesses of acoustic and linguistic models in current speech recognizers, especially for spontaneous speech, by comparing human and machine recognition in a way similar to their research, using speech recognition algorithms to indicate more clearly what we should focus on in state-of-the-art speech recognition research.Dusan and Rabiner (2005) compared the general characteristics of both human and automatic speech recognition from the viewpoints of architecture, analysis methods, information processing methods, etc., and concluded that radical modifications were needed before automatic speech recognition could approach human levels. But no clear direction for such modification has been found. For example, humans apparently use more global information, such as longer contexts, co-occurrence information with target words, and domain knowledge. Humans also use syntax and lexical, semantic, and interpretative contexts (Tyler and Frauenfelder, 1987; Tanenhaus and Lucas, 1987). Thus, efforts have been made to model such information, but a general method for using such global information has not yet been developed. One way of thinking about this approach to improving machine speech recognition performance is that the improvement of acoustic and/or language modeling would be a quick (and wonderful) short-cut.Sommers and Danielson (1999) and Pichora-Fuller et al. (1995) compared human identification scores for words presented in isolation and in low- and high-predictability sentences, and showed that predictability greatly affects identification/recognition scores. Nittrouer and Boothroyd (1990) also noted the contribution of word and sentence contexts on the perception of phonemes within words, and of words within sentences, respectively. N-gram models raise the prediction ability of machine recognizers in a similar manner. Our experiments in this paper assess the capability of n-grams by comparing them with human speech recognition.In this paper, as mentioned above, we focus on evaluating the performance of widely used HMM acoustic and n-gram language models by comparing them with a combination of human acoustic knowledge and information about local context. The comparisons are done under various conditions by altering context lengths, target word lengths, and speaking styles, such as read speech vs. spontaneous speech, and we discuss the performance of such local acoustic and linguistic models. By examining the differences between ASR and human speech recognition (HSR), and by investigating the strengths and weaknesses of automatic speech recognizers, we hope to understand how ASR could profit from more knowledge about HSR. We then discuss how our experiments described in this paper contribute to this understanding. Through experimentation, we evaluate the impact of acoustic and linguistic information on human and machine speech recognition, and reveal where the models used in machine recognition can be improved. Finally, we point out that we must improve acoustic models first, because the difference in the capability of acoustic models vs. human use of acoustic information is larger than the capability gap of language models. These experiments highlight the weak points in such frameworks, and help us determine the best direction for future research in automatic speech recognition.This paper is organized as follows: in Section 2, we evaluate human speech recognition performance when given (or not given) local linguistic context and acoustic information. Section 3 describes the performance of an automatic speech recognizer when given almost the same information as given to humans in Section 2. In Section 4, we briefly summarize the experimental results shown in Sections 2 and 3, and discuss the capabilities of acoustic and language models from the viewpoint of information theory. Section 5 concludes the paper.

@&#CONCLUSIONS@&#
