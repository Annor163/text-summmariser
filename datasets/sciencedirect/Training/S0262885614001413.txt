@&#MAIN-TITLE@&#
Model-based graph-cut method for automatic flower segmentation with spatial constraints

@&#HIGHLIGHTS@&#
An improvement of the classical energy function (graph-cut) is proposed.Integration of spatial priori and gradient information improves segmentation result.A new “coarse-to-fine” flower segmentation method is presented and implemented.

@&#KEYPHRASES@&#
Automatic flower image segmentation,Graph-cut,Spatial prior,

@&#ABSTRACT@&#
Graphical abstract

@&#INTRODUCTION@&#
Automatic flower classification systems are important for a wide range of application including pharmacy research, environment protection and perfume production. Thanks to computer vision, image processing and pattern recognition techniques, automatic recognition systems make the identification of the flower category easier by analyzing color images. Image segmentation is generally considered an important component of the recognition or classification processes, and affects the quality of the image analysis. Automatic flower segmentation allows the extraction of the object of interest (foreground) from the rest of the image (background) without any manual interaction.The majority of flower images present natural scenes with complex background. The areas surrounding the flowers have generally large variety of colors and textures. It can contain several entities distributed separately or together such as stones, leaves, turf grass, green foliage and soil. Fig. 1illustrates different types of elements that can be contained in the area surrounding the flower. As the flowers from different species may look very similar both in shape and color, the use of the background information to generate the image features can increase this similarity and consequently reduce the classification accuracy.Therefore, we believe that the extraction of features only from the object of interest provides more meaningful and accurate information than the one obtained from the whole image. Although many flower image segmentation methods have been proposed in the literature, it remains difficult to find a general solution that is applicable to all types of flowers and gives accurate results. In the next paragraph, we present the state of the art on flower image segmentation.Das et al. [2] proposed an iterative segmentation algorithm using color and spatial domain knowledge-driven feedback. Their method mapped the RGB color space to commonly used color names in order to delete pixels which belong to background color classes like black, brown, green or gray. The foreground region represented by the remaining colors is accepted if it is included in the flower area. In order to define the flower region, some hypotheses were made such as the flower centroid should fall within the “central region” of the image. Saitoh et al. [3] presented the Normalized Cost (NC) method to extract flower regions. It is based on a Dynamic Programming method known as the intelligent scissors [4] for extracting the boundary of the object of interest. The image is represented as a directed weighted graph where nodes are pixels and arcs between neighboring pixels represent the 8-connectivity information. This method consists in computing the local minimum cost given by a path between two seeds. The obtained cost is normalized by the length of this path. The shortest path in the graph gives the object edges. In this work, the authors assume that the flower is at the center of the image and the background occupies the peripheral area. Based on this hypothesis, the authors determine some local minimum points of each local cost profile along the straight line from the starting point to all the middle points of four sides. Then, they extract the boundary for each local minimum point based on the NC and they select the one that has the smallest normalized cost and contains the center point. Another interesting automatic algorithm can be found in [1]. The first step of this algorithm aims to obtain an initial flower segmentation using the contrast dependent prior MRF (Markov Random Field) cost function, which is optimized by using graph-cut [5] based on general foreground and background color distributions. These distributions are learned by labeling pixels in few training images for each flower class as foreground or background. Then, these distributions are averaged over all classes. A generic flower-shaped model is then fitted to this initial segmentation in order to detect petals which have a loose geometric consistency using an affine invariant Hough-like procedure. The image region corresponding to the petals is used to obtain image-specific foreground and background color distributions which will be employed to obtain new color distributions by blending them with general ones. A new segmentation using MRF-based graph-cut is repeated using these new color distributions. The shape-model fitting and the segmentation will be iterated until convergence. In [6], Aydin and Ugur proposed a flower image segmentation algorithm based on ant colony optimization (ACO) [7]. First, RGB color space is converted to CIE-LAB color space in order to provide accuracy and perceptual approach in color difference determination. Second, the histogram of each color component is built (n bins for each component) to determine the center points of each bin and three “bin-centers” are obtained to generate all possible combinations of center points (n3) which constitute the potential candidate cluster's center points. Then, ant colony optimization algorithm is applied to select optimum cluster center points. Finally, each pixel is classified in the nearest cluster. Recently, Fukuda et al. [8] proposed an automatic flower segmentation model based on graph-cut. They set the region penalty in graph-cut based on posterior probability, and not on likelihood as in the classical graph-cut. The posterior probabilities are determined by using AdaBoost [9] and saliency map [10]. In fact, the AdaBoost classifier provides a rectangular window representing the approximate flower location used to compute foreground and background color distributions (likelihoods). Then, the saliency map is used to provide prior probabilities. Using the Bayes' theorem, posterior probabilities are computed and integrated in graph-cut. More recently, Chai et al. in [11] addressed the co-segmentation of flowers which consists of segmenting a similar object (flower) from a pair of images. Their algorithm iterates at two levels: segmenting each image independently with GrabCut [12] at the pixel level and classifying the obtained superpixels into foreground and background using SVM learning algorithm.To conclude, all of these approaches aim the automatic segmentation of flower images but there are some differences between them in terms of quality and time-consuming. In fact, the approach proposed by [2] doesn't extract all flower regions in the image, but isolates a region as the best description of the flower color. Besides, the fact of eliminating pixels belonging to non-flower color classes will fail with the images containing brown or gray flowers which will be considered in the evaluation of our algorithm. All these factors can reduce the segmentation performance of this method [2]. As for the method proposed by [3], the main disadvantage is the high computational time due to the research of the local cost paths. As demonstrated in [1], the method of Nilsback et al. achieves good performances (93% in quality measure [13]) but it is very slow to be run in real-time applications. In this method [1], fitting a generic shape model needs the determination of the corners, the petals and the center of the flowers which requires high computational time. The method proposed by [6] yields accurate segmentation (87% in quality measure) but we have no information about the processing time of the algorithm. The method reviewed by [11] achieves about 94% segmentation accuracy not on the original data split of the Oxford-17 flower dataset. As cited in [14], the algorithm proposed by iteChai11 needs more than 30s as run-time for segmentation. Finally, in [8], it is demonstrated that the proposed method can reduce the error rate in segmenting flower images with no mention of the consuming time. In terms of evaluation settings, all aforementioned flower segmentation methods have been performed on different datasets and evaluated with different accuracy measure as shown in Table 1.After this study, one can say that the model proposed by Nilsback et al. [1] yields the best performance of automatic flower image segmentation even if it is very time-consuming. That is why we take it as a state-of-the-art in order to improve it or to propose a new fast algorithm offering similar accuracy.Recently, it has been an important interest in image segmentation approaches based on graph-cut [15–18]. Many works use graph-cut technique since it gives positive results on both medical and natural images [19,12]. Unfortunately, the standard graph-cut algorithm suffers from some limits: (i) it fails to give a smooth segmentation result by labeling some object pixels as background, or vice versa. (ii) It fails to define the desired boundary of the object. In fact, if object pixels have color distribution similar to the image background then these pixels will be labeled as background, and they won't be considered as part of the object. This failure can be explained by the fact that the traditional energy function to be minimized uses only the color information. In order to alleviate these problems, some solutions have been proposed to integrate a priori information other than color information. It has been demonstrated that the segmentation results of graph-cut methods can be improved by introducing novel constraints in the segmentation process such as shape constraints [20,21] and spatial constraints [22]. For example, authors in [21] proposed to incorporate a generic star-shaped prior in the energy function minimized by graph-cut. The shape prior is represented as an object mask form using the distance transform of the star shape. Although this method [21] gives encouraging results with the assumption that the center of the shape is given by the user, the extracted object shape tends to be star-aligned. While shape-based graph-cut works can be robust, they have a shortcoming to segment object with high shape variability [23].In this paper, our aim is to modify the standard graph energy function to enhance the segmentation result. Therefore, we introduce spatial constraints through an additional term in the formulation of the graph-cut energy function. Thus, we obtain a better classification of pixels as foreground despite their high probabilities of belonging to the background if we consider only their colors and vice versa. We integrate also gradient information in the traditional graph energy function to better express boundary constraints. Consequently, true boundaries can be determined based on both gradient and color information. The rest of the paper is organized as follows: In Section 2, first, we present the mathematical theory of graph-cut technique and its use for energy minimization. Then, we introduce our proposed new energy function to be minimized by graph-cut. Finally, our automatic flower segmentation algorithm is described. In Section 3, we present the experimental results and the evaluation of the proposed segmentation method.The graph-cut optimization technique proposed by [5] is one of the energy minimization algorithms which solve the object segmentation problem [24]. The algorithm defines a non-oriented graph G=(V,E) where V is a set of nodes corresponding to the image pixels and E is a set of non-oriented edges that connect those nodes. There are two terminal nodes called source s and sink t added to V in order to represent “foreground” and “background”, respectively. Each pixel node is connected to each one of the two terminal nodes by a link called t-link and each pair of neighboring pixels is connected by an edge called n-link. A cut is a partition of V into disjoint subsets S and T such that the source s is in S and the sink t is in T as shown in Fig. 2.The cost of the cut is defined as the sum of weights of all edges that are severed by the cut. The optimal segmentation is obtained by finding the cut that has the minimum cost among all cuts [25,26]. This graph-cut technique is used to minimize the standard energy function formulated as shown in Eq. (1).(1)Ef=∑p∈VRpfp+λ∑p,q∈CBp,qfpfqwhere C is the set of pairs of adjacent pixels representing the 4- (or 8-) neighborhood system, f=fp∈Vis the labeling function which associates each pixel p with a label fp (fp=0 if it belongs to the foreground and fp=1 otherwise). The first term Rp is called the region or data dependant term which evaluates the penalty for assigning any label to a pixel p and represents the weight of t-link edges. The second term Bp,q is the boundary or smoothness term that measures the cost for two neighboring pixels p and q being different and represents the weight of n-link edges. The constant λ in Eq. (1) controls the relative importance of the boundary term versus the region term. Since Ip is the intensity of pixel p, the terms Rp(“Obj”) and Rp(“Bkg”) are equal to negative log-likelihood of foreground and background intensity models, respectively (Eqs. (2) and (3)).(2)Rp“Obj”=Rpfp=0=−lnPrIp|“Obj”(3)Rp“Bkg”=Rpfp=1=−lnPrIp|“Bkg”.The smoothness term Bp,q is commonly expressed by the Eq. (4).(4)Bp,q=1distpqexp−Ip−Iq22σ2where σ is the standard deviation of the norm of the image gradient [27]. For grayscale images, Ip and Iq are the intensities of pixels p and q. For color images, they can be the RGB color vectors of pixels p and q. The dist(p,q) is the Euclidean spatial distance between the pixels p and q.We propose a new modeling of the energy function to be optimized by graph-cut. As we search to consider not only color information, but also the spatial one, we add a new term to the energy function. We also improved the boundary constraints by modifying the standard smoothness term. The proposed energy function consists of two new terms, namely, data-consistent term Dp and modified boundary termB˜p,qas illustrated in Eq. (5).(5)Ef=∑p∈PDpfp+λ∑p,q∈CB˜p,qfpfqwhere(6)Dpfp=βRpfp+1−βSpfp.The first term Dp is a barycentric combination (Eq. (6)) of a priori color-dependent term Rp (Eqs. (2) and (3)) and a priori spatial-dependant term Sp (Eqs. (7) and (8)). In fact, we add the spatial distribution to the color distribution in the segmentation process in order to improve the quality. The spatial terms Sp(“Obj”) and Sp(“Bkg”) represent respectively the penalities for assigning a pixel p to the classes “foreground” and “background” according to spatial distributions (Prs(p|“Obj”) and Prs(p|“Bkg”)). Thus, the spatial term Sp is defined as the negative log-likelihood of foreground and background spatial distributions such as described in Eqs. (7) and (8).(7)Sp“Obj”=−lnPrsp|“Obj”(8)Sp“Bkg”=−ln1−Prsp|“Obj”.The spatial distribution of foreground is determined by the approximate spatial location Rsof the flower zone in the image. Inside the region Rs, the probability to consider a pixel as an object is maximum. Whereas, for every pixel outside the region Rs, this probability decreases according to a Gaussian of the distances dist(p,Rs) that separate the pixel p to the boundary of the region Rs. The formulation of these probabilities is given by Eq. (9).(9)Prsp|“Obj”=1ifp∈Rs.exp−distpRs22σRS2ifp∉Rs.where σRSis the standard deviation of the distances dist(p,Rs).In standard graph-cut, the expression of the smoothness term is not enough to describe the boundary constraints. In fact, the area surrounding the flower is characterized by a large variety of colors and texture, but it is generally a repetitive texture pattern or elements. Using only the intensity difference in the boundary term (Eq. (4)) penalizes the discontinuity between two adjacent pixels within the same region. So, it is likely that the two neighboring pixels will not have the same label not only when they belong to different regions but also when they belong to the same textured area. Since adding the gradient norm difference will provide the intensity variation within a window of pixels, both of the two pixels will have a high value of gradient norm which will reduce the difference and increase the smoothness term in order to avoid the cut. The modified boundary termB˜p,qis then given by Eq. (10).(10)B˜p,q=1distpqBIp,q+BGp,qiffp≠fq.0otherwisewhere BIp,q and BGp,q are two neighborhood interaction functions that can penalize intensity difference and gradient norm difference between two neighboring pixels p and q, respectively. By performing an optimization of a parameter combining BIp,q and BGp,q, we have not obtained an efficient estimation. So we chose to minimize the proposed energy function with no parameter between BIp,q and BGp,q. The term BIp,q is obtained using the old boundary term (Eq. (4)) and is expressed by Eq. (11).(11)BIp,q=exp−Ip−Iq22σI2.The additional term BGp,q used to improve boundary regularity is defined by Eq. (12).(12)BGp,q=exp−∇p−∇q22σI2where ‖∇p‖ and ‖∇q‖ are gradient norms of the image at neighboring pixels p and q.Since we have optimized this new energy function using graph-cut, we have called it “Extended graph-cut”.We propose a new coarse-to-fine method which devises a two-level scheme to segment flower image. In our work, we use general foreground and background color distributions (Gfgand Gbg) which are learned as in [1] using the Gaussian mixture model [28]. At the first level, our goal is to perform a coarse segmentation based on the minimization of the proposed energy function by the graph-cut. At the second level, we enhance the obtained result by refining color distributions in order to apply the standard graph-cut. In the next section, we will detail the components of the proposed algorithm followed by the complexity analysis. The proposed automatic segmentation framework is depicted in Fig. 3.At this first level, a huge part of the background is eliminated in order to approximate the location of the flower zone. The idea is to select automatically several background seeds: two seeds are selected in each window corner of the image as mentioned in Section 2.3.1.2, and then we gather all pixels similar to those seeds in terms of color. The obtained area will correspond to a large part of the background. The remaining area of the image will contain necessary the flower region and will be served to determine the spatial model for applying extended graph-cut. The process of this level is described by Algorithm 1.Algorithm 1 Coarse segmentation ()Input: general background Gbgand foreground Gfgcolor models-Input image IimgOutput: coarse segmentation result.1.Estimate background probability density function PDFbgof the border of Iimg.For each corner={pixel(x, y)} in Iimgdosetseed1xy^=argmaxpixelxyPDFbgpixelxysetseed2xy^=argmaxpixelxyGbgpixelxyEnd forSet Iq=Quantization(Iimg, 12).Label the pixels from Iq having the same color as background seeds to 0 and the other pixels to 1.Compute Euclidean distance map EDM of the previous result (step 7).Build object and background spatial models using EDM.Use graph-cut algorithm to segment Iimgusing spatial and color models.Return the coarse segmentation result Rescoarse.We have estimated a specific background distribution calculated from the image to be segmented. So, as illustrated in Fig. 4, we divide our input image into two areas, an internal area (IA) representing the kernel of the image and an external one (EA) representing the border. We can estimate the specific background distribution by computing the multivariate Gaussian distribution in EA because we made the assumption that the internal area will contain the flower region and the external one will correspond to the background.In each corner of the input image, we choose two pixel seeds within 20×20 window, as shown in Fig. 4. The first and the second seeds are chosen among the pixels contained in the window corner as they have the maximum background likelihood value based on background color distributions, respectively, learned by GMM (Gbg) and computed on the external area EA (PDFbg).Due to complex mixtures of colors in natural scenes and the fact that texture features tend to be ambiguous and not discriminative enough [29], image color quantization is performed in order to reduce the color number. Consequently, we can easily reduce the visual difference between pixels having similar colors as shown in Fig. 5.In order to perform the color image quantization, we compute the class-map of the image by changing each original pixel color to his corresponding quantized value. Typically, the number of levels needed for the quantization of natural scenes is between 10 and 20 [30]. We have used an empirical value of quantization level which is fixed to 12. Fig. 5 shows some results of 12-level color quantization on four images from “Daffodil”, “Iris”, “Pansy” and “Fritillary” classes.This step consists in gathering the pixels having the same color level as the background seeds. As shown in (Fig. 6b), the gathered pixels represent a background area Zbgwhile the remaining part Zocontains the flower and it is considered its spatial location.A distance map [31] of the previous obtained result is computed (Fig. 6c) in order to provide a prior probability of the spatial object model which will be incorporated in the energy function minimized by graph-cut. This map labels each background pixel with the shortest Euclidean distance to the nearest foreground pixel, unlike the foreground pixels which will take the value zero as a label. Fig. 6c illustrates the Euclidean distance map of the background pixels gathering result.The final step at the coarse segmentation is the application of the extended graph-cut. Fig. 6d shows an example of the obtained result on two flower images, namely “crocus” and “iris”. The result may not be perfect, but it is sufficient to detect a large part of the object with some parts of the background and a few object pixels misclassified as background. In the next section, we will explain in simple steps how to refine this result. The modified boundary term of the energy function optimized by the extended graph-cut enhances the boundary smoothness.Fig. 7shows how the integration of the gradient information improves the coarse segmentation result. Using the new boundary term, we obtain better coarse segmentation (Fig. 7c) than the result obtained using the standard boundary term (Fig. 7b).At this level, we try to refine the color distributions used at the coarse segmentation level. Then, we apply standard graph-cut using these updated color models. The pseudocode of the fine segmentation is shown in Algorithm 2.Algorithm 2 Fine segmentation()Input: coarse segmentation result RescoarseOutput: fine segmentation result1:Compute color histogram BCH of the pixels classified as background in Rescoarse.Threshold BCH to identify dominant background colors.Find the set of object pixels OP from Rescoarse that has a dominant background colors.Modify the labels of all pixels in OP in order to be assigned as background.Reestimate the object hobject and background hbackground color models according to the labeling result obtained in step 4.Use graph-cut algorithm to segment Iimgusing the updated color models hobject and hbackground.Return the fine segmentation result Resfinal.Considering the result of the coarse segmentation, we identify the background dominant colors by computing the background color histogram BCH and thresholding it. These colors will be used to rectify the classification of background pixels labeled as foreground in the coarse segmentation result. So, we modify the labels of these pixels classified as foreground and having a dominant background color. The obtained result is denoted by “imagecleaned” where the foreground pixels form a region denoted by “objectcleaned”.In order to apply standard graph-cut segmentation, we have to re-estimate color distributions. These distributions should be finer than those used at the coarse segmentation level. We can rectify the color likelihoods used at the coarse segmentation level by updating general color models. The new foreground color model hobjectis a barycentric linear combination of the color distribution hOCof the objectcleanedand the general foreground distribution Gfgas mentioned in Eq. (13).(13)hobject=γ∗hOC+1−γ∗Gfg.In some cases, the flower is not accurately cut at the coarse segmentation level. In fact, the foreground pixels marked as background inhibit the fine segmentation to extract the flower object accurately if we include them in the background color model. To avoid this, we consider an uncertainty zone around the objectcleanedby applying morphological dilation using a 5×5 disk-shaped structuring element. We called objectcleaned−dilatedthe new object region. The background model hbackground is, therefore, a barycentric linear combination of the general background distribution Gbgand the color distribution hNOCof the pixels which are not part of the region objectcleaned−dilated(Eq. (14)).(14)hbackground=γ∗hNOC+1−γ∗Gbg.Since the new color distributions provide more accurate modeling of both background and foreground than those used at the first level, we give them more confidence with a high weight. In our experiments, the γ empirical value used is 0.8.The standard graph-cut is then performed using the updated color models. At this step, the use of the extended graph-cut makes hard to precisely segment the flower even if the object and background models have been updated. In fact, the background pixels contained in the flower zone will be used to compute the object spatial model and they will be classified again as foreground due to their high object probabilities. Therefore, we use the standard graph-cut algorithm at the fine segmentation level. Fig. 8shows how the fine segmentation accurately cleans the coarse segmentation result.In fact, it is difficult to segment the center pixels of some flower (e.g. Sunflower), having a dark background color (e.g. brown and black), as foreground. That's why we fill the center hole into the foreground.Our method applies, at each segmentation level, the graph-cut technique. Since we have two levels, the computational complexity of our algorithm will be 2×O(m×n2×|c|). It is twice the running time complexity of the max-flow min-cut algorithm in the worst case; where n is the number of nodes, m is the number of edges and |c| is the cost of the minimum cut in the graph. However, the complexity of the algorithm in [1] is NB∗O(m×n2×|c|) where NB is the number of iterations required until convergence of the algorithm. In [1], it was mentioned that the algorithm can need five iterations to converge. This means that NB is greater than or equal to 5 in the worst case. Since the flower images in the Oxford database vary in size, at least 500×500pixels, the constant NB which is relatively high (≥5) increases the computational time for large images. Thus, our proposed algorithm is faster than that proposed in [1].

@&#CONCLUSIONS@&#
