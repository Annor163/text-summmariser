@&#MAIN-TITLE@&#
Non-myopic information theoretic sensor management of a single pan–tilt–zoom camera for multiple object detection and tracking

@&#HIGHLIGHTS@&#
Detailed derivation of an information theoretic framework for real PTZ management.Introduction and implementation of a non-myopic strategy.Large experimental validation, with synthetic and realistic datasets.Working demonstration of myopic strategy on an off-the-shelf PTZ camera.

@&#KEYPHRASES@&#
Pan–tilt–zoom camera,Multiple object tracking,Sensor management,Markov decision process,

@&#ABSTRACT@&#
Automatic multiple object tracking with a single pan–tilt–zoom (PTZ) cameras is a hard task, with few approaches in the literature, most of them proposing simplistic scenarios. In this paper, we present a novel PTZ camera management framework in which at each time step, the next camera pose (pan, tilt, focal length) is chosen to support multiple object tracking. The policy can be myopic or non-myopic, where the former analyzes exclusively the current frame for deciding the next camera pose, while the latter takes into account plausible future target displacements and camera poses, through a multiple look-ahead optimization. In both cases, occlusions, a variable number of subjects and genuine pedestrian detectors are taken into account, for the first time in the literature. Convincing comparative results on synthetic data, realistic simulations and real trials validate our proposal, showing that non-myopic strategies are particularly suited for a PTZ camera management.

@&#INTRODUCTION@&#
Visual tracking of multiple objects in realistic outdoor scenarios is often performed in wide areas. In these viewing conditions a stationary fixed focal length camera has typically too limited field of view and image resolution with respect to the scene extent. Therefore, a network of cameras is used to sufficiently cover the area at the required resolution [1–5]. However, this may be unfeasible for the cost associated to the setup and maintenance of the camera network, as well as for the practical impossibility to provide all the necessary resolutions for target biometric recognition at a distance. Similarly, in the case of a vehicle mounted camera [6–9] it would be difficult to cover a wide area at adequate resolution due to the limited acceleration at which the camera may be moved. Active vision [10] and specifically active pan tilt zoom (PTZ) cameras, have promised to solve these limitations, permitting at least in principle the monitoring of a large space at variable image resolutions [11,4]. However, letting a large number of stationary or PTZ cameras operate in a cooperative way is still an expensive and complex solution [12,13]; for this reason, exploiting a single zooming sensor could be a more reasonable and worthy goal. According to this, in this paper, we propose and show the benefits of an active sensing approach to multiple object detection and tracking using a single pan tilt zoom camera.Despite the high exploitable potential, when applied for the task of multiple object tracking in world coordinates, a single PTZ camera induces a number of complex problems that must be solved to obtain effective results [14,15]. Specifically, camera calibration solutions adopting natural landmarks [16,17] should be preferred with respect to others adopting domain specific scene landmark geometry as in [11,15,18]. Since the PTZ camera must also undergo rapid and unpredictable motions to rapidly gaze at any part of the field of view, real time tracking of camera motion should not be based on recursive filtering, but on keyframe based methods [16,19]. At the same time, the scene background appearance must be continuously updated [16,20,21]. Moreover, due to the fact that monitoring is performed in a large area, accurate objects localization in a common 3D world reference frame is needed to track targets at a distance. This requires some form of online camera calibration since the camera parameters change dynamically. The framework we developed in [22] is conceived to support all these requirements and is therefore suitable to be used in task-driven active surveillance of scenes with multiple moving objects.Starting from this framework [22], we propose a solution for sensor management (i.e. determine the best way to control the visual sensor) in order to enhance multiple target detection and tracking in a wide area. Here the focus is on non-myopic sensor management where the long-term ramifications of taking a particular sensing action are accounted for decision making. A sensing action may consist of choosing a particular image processing modality (e.g. pedestrian detection or motion detection), a particular camera pose and focal length, or a combination of the two. Information gain [23] is chosen as performance indicator of decision making, since it has the desirable property that different inhomogeneous sensing actions can be simultaneously optimized in a single metric. This requires to maintain the probability density which capture uncertainty in the current state estimate. In our setting there are multiple actions that can be tasked by evaluating a single global metric, some of which contribute better than others to tracking. The PTZ camera sensor is used to gain information about the kinematic state (e.g. position and velocity) and objects detectability.1Image object measurements are obtained according to a detector that have a time-varying object response characteristics. For example in the case of pedestrian detection as processing modality, the response characteristic varies depending on the imaged size of the object and on how much the object is occluded.1There are many objectives that the sensor manager may be tuned to meet, e.g. minimization of track loss, probability of target detection, minimization of track error/covariance, and identification accuracy. Each of these different objectives taken alone may lead to a different sensor allocation strategy. As detailed in Section 4, we jointly optimize over all these objectives by maximizing the expected amount of information extracted from the scene, namely the expected information gain between the current objects state estimate and the state estimate after a measurement has been made. Since the best sensing action must be selected before actually executing it, what is practically maximized is the expected reduction in entropy (i.e. the expected information gain) that a sensing action will produce. Fig. 1shows the three main components of the complete multi target tracking system for a single PTZ camera.The sensor management problem can be approached in a principled way with the Markovian Decision Process (MDP) formalism [24]. However, the long-term (non-myopic) planning solution suffers from combinatorial explosion and may be defined, as in our case, in a continuous state space. Approximate solutions are therefore required and will be discussed in the next section. This work extends the preliminary results we obtained in [25] where we have analyzed the myopic (i.e. greedy) aspects of sensor management. Experimental results show that the non-myopic strategy provides a substantial performance improvement by better capturing the complex space–time trade-off between objects and camera motion. Two motivating examples for which the non-myopic will outperform the myopic strategy are: (1) the case in which an object is repeatedly measured before it gets occluded so as to sharpen its uncertainty when it reappears and (2) the case in which objects are measured exploiting the calibrated zoom2Calibrated zoom allows increasing measurement accuracy in world plane coordinate object localization. In Appendix A a formal proof of this result is presented.2so as to sharpen their uncertainties. The underlying assumption is that if the operative scenario evolves with reasonable temporal coherence, it is possible to predict the ability of gathering information of a future action.Synthetic and real experiments are shown confirming the suitability of our approach for realistic scenarios. Fig. 2shows few frames from the three sets of experiments.The rest of the paper is organized as follows. We give and overview of related work in Section 2 while we summarize our contributions in Section 3. The information theoretic formulation based on MDP for the myopic version is presented in Section 4, and the modeling of the real world challenges such as missed detections and occlusions is presented in Section 5. The non-myopic version is described in Section 6. In Section 7 we give a detailed discussion about how the proposed solution can be extended to a network of multiple cameras. Some implementation and evaluation details are given in Section 8. Experiments for the myopic framework are reported in Section 9 while experiments for the non-myopic version are reported in Section 10. Finally the conclusions are drawn in Section 11.

@&#CONCLUSIONS@&#
In this paper, we propose a novel solution to perform sensor management of a single PTZ camera for multiple target tracking. Such solution considers the detector performance at different image resolutions and occlusion ratios. Moreover, it considers the effects of a different camera pose to targets localization. To further improve the tracking performance we apply a non-myopic approach which considers future occlusions among targets in selecting the next actions. We analyze the characteristics and demonstrate the effectiveness of our approach through, synthetic experiments, realistic simulations and effective real-time trials on a real PTZ camera.Assuming a constant image measurement error and perfect calibration, world coordinate localization is much more accurate if the backprojection is performed using zoomed views (i.e. long focal length).We derive analytically the expression that allows to appreciate how zoom affects the measurement equation in the recursive filtering formulation. Measurement uncertainty is mainly oriented along the direction of instantaneous depth because of the panning camera capability. According to this, uncertainty can be quantified assuming a 1D projective camera parametrized by the tilt angleθin the instantaneous plane rotating around the vertical camera axis and focal length f.Without loss of generality let’s consider that the principal point lies at the image center:(A.1)K=f001.We further have:(A.2)R=cosθ-sinθsinθcosθ.The 1-D camera projection matrixP=K[R|t]results in:(A.3)P=fcosθ-fsinθ0sinθcosθ-d,wheret=[0-d]⊤and d is the camera distance with respect to the scene plane. Being the scene planeZ=0, the 1D homography from world to image can be computed from Eq. (A.3):(A.4)H=fcosθ0sinθ-d.The inverse:(A.5)H-1=1fcosθ0sinθfcosθd-d-1,can be used to compute the back-projected uncertainty of a given noise uncertainty∊assumed in the camera image sensor and compute its backprojectionδ. Without loss of generality and for the sake of simplicity, let’s define:(A.6)x1=01,x2=∊1,their corresponding backprojected points:(A.7)x1′=H-1x1=0-d-1,(A.8)x2′=H-1x2=∊fcosθsinθ∊fcosθd-d-1,are used to compute:(A.9)δ=x2′-x1′=∊dsinθ∊-fcosθ.Given∊,θand d, the value ofδcan be increased by increasing the focal length (i.e. performing zoom-in).Supplementary data associated with this article can be found, in the online version, at http://dx.doi.org/10.1016/j.cviu.2014.12.001.