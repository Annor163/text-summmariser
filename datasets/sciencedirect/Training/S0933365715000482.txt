@&#MAIN-TITLE@&#
An empirical evaluation of supervised learning approaches in assigning diagnosis codes to electronic medical records

@&#HIGHLIGHTS@&#
Supervised learning approaches are evaluated for assigning diagnosis codes to EMRs.Different dataset characteristics (EMR size, number of codes) warrant different learning approaches.Classifier chaining works best for more focused datasets with correlated codes and shorter narratives.Feature selection and training data selection offer best performance gains for smaller datasets of general EMRs.Binary relevance with label calibration and candidate re-ranking is ideal for larger and more general datasets.

@&#KEYPHRASES@&#
Multi-label text classification,Learning to rank,Label calibration,Diagnosis code assignment,

@&#ABSTRACT@&#
BackgroundDiagnosis codes are assigned to medical records in healthcare facilities by trained coders by reviewing all physician authored documents associated with a patient's visit. This is a necessary and complex task involving coders adhering to coding guidelines and coding all assignable codes. With the popularity of electronic medical records (EMRs), computational approaches to code assignment have been proposed in the recent years. However, most efforts have focused on single and often short clinical narratives, while realistic scenarios warrant full EMR level analysis for code assignment.ObjectiveWe evaluate supervised learning approaches to automatically assign international classification of diseases (ninth revision) – clinical modification (ICD-9-CM) codes to EMRs by experimenting with a large realistic EMR dataset. The overall goal is to identify methods that offer superior performance in this task when considering such datasets.MethodsWe use a dataset of 71,463 EMRs corresponding to in-patient visits with discharge date falling in a two year period (2011–2012) from the University of Kentucky (UKY) Medical Center. We curate a smaller subset of this dataset and also use a third gold standard dataset of radiology reports. We conduct experiments using different problem transformation approaches with feature and data selection components and employing suitable label calibration and ranking methods with novel features involving code co-occurrence frequencies and latent code associations.ResultsOver all codes with at least 50 training examples we obtain a micro F-score of 0.48. On the set of codes that occur at least in 1% of the two year dataset, we achieve a micro F-score of 0.54. For the smaller radiology report dataset, the classifier chaining approach yields best results. For the smaller subset of the UKY dataset, feature selection, data selection, and label calibration offer best performance.ConclusionsWe show that datasets at different scale (size of the EMRs, number of distinct codes) and with different characteristics warrant different learning approaches. For shorter narratives pertaining to a particular medical subdomain (e.g., radiology, pathology), classifier chaining is ideal given the codes are highly related with each other. For realistic in-patient full EMRs, feature and data selection methods offer high performance for smaller datasets. However, for large EMR datasets, we observe that the binary relevance approach with learning-to-rank based code reranking offers the best performance. Regardless of the training dataset size, for general EMRs, label calibration to select the optimal number of labels is an indispensable final step.

@&#INTRODUCTION@&#
Assigning codes from standard terminologies is a regular and indispensable task often encountered in medical and healthcare fields. Diagnosis codes, procedure codes, cancer site and morphology codes are all manually extracted from patient records by trained human coders. The extracted codes serve multiple purposes including billing and reimbursement, quality control, epidemiological studies, and cohort identification for clinical trials. In this paper we focus on assigning international classification of diseases, clinical modification, 9th revision (ICD-9-CM) diagnosis codes to electronic medical records (EMRs), and the application of supervised multi-label text classification approaches to this problem.Diagnosis codes are the primary means to systematically encode patient conditions treated in healthcare facilities both for billing purposes and for secondary data usage. In the US, ICD-9-CM (just ICD-9 henceforth) is the coding scheme still used by many healthcare providers while they are required to comply with ICD-10-CM, the next and latest revision, by October 1, 2015. Regardless of the coding scheme used, both ICD code sets are very large, with ICD-9 having over 14,000 diagnoses while ICD-10 has nearly 68,000 diagnosis codes [1] and as will be made clear in the rest of the paper, our methods will also apply to ICD-10 coding tasks. ICD-9 codes contain 3 to 5 digits and are organized hierarchically: they take the form abc.xy where the first three character part before the period abc is the main disease category, while the x and y components represent subdivisions of the abc category. For example, the code 530.12 is for the condition reflux esophagitis and its parent code 530.1 is for the broader condition of esophagitis and the three character code 530 subsumes all diseases of esophagus. Any allowed code assignment should at least assign codes at the category level (that is, the first three digits). At the category levels there are nearly 1300 different ICD-9 codes.The process of assigning diagnosis codes is carried out by trained human coders who look at the entire EMR for a patient visit to assign codes. Majority of the artifacts in an EMR are textual documents such as discharge summaries, operative reports, and progress notes authored by physicians, nurses, or social workers who attended the patient. The codes are assigned based on a set of guidelines [2] established by the National Center for Health Statistics and the Centers for Medicare and Medicaid Services. The guidelines contain rules that state how coding should be done in specific cases. For example, the signs and symptoms (780-799) codes are often not coded if the underlying causal condition is determined and coded. Given the large set of possible ICD-9 codes and the need to carefully review the entire EMR, the coding process is a complex and time consuming process. Hence, several attempts have been made to automate the coding process. However, computational approaches are inherently error prone. Hence, we would like to emphasize that automated medical coding systems, including our current attempt, are generally not intended to replace trained coders but are mainly motivated to expedite the coding process and increase the productivity of medical record coding and management.The main focus of this paper is supervised text classification approaches to automatically assign11Throughout the article we use the verb assign to indicate the prediction of diagnosis codes by multi-label classification of the EMR narratives because the codes (or the corresponding English names) may not be present in the EMR for straightforward extraction through named entity recognition.ICD-9 codes to clinical narratives using a large EMR dataset of patients discharged during a two year period (2011–2012) from the University of Kentucky (UKY) Medical Center. We will refer to this dataset as UKLarge in the rest of the paper. The correct codes for the EMRs in this dataset are those assigned by one of the trained coders in the UKY medical records office. To study the affect of scale, we also use two other smaller datasets: a gold standard dataset created for the BioNLP 2007 shared task [3] by researchers affiliated with the Computational Medicine Center (CMC: http://computationalmedicine.org/) and a subset of the UKLarge dataset, called UKSmall, that is comparable to the number of records used in BioNLP dataset. The CMC dataset is a high quality, but relatively small and simple dataset of clinical reports that covers the pediatric radiology domain. The gold standard correct codes are provided for each report. To experiment with a more general dataset with a comparable number of records to the CMC dataset, we curated UKSmall, a subset of the UKLarge dataset.In supervised classification, multi-label problems are generally transformed into several multi-class (where each object belongs to a single class) or binary classification problems. In this effort, we explore these different transformation techniques with different base classifiers (support vector machines, naive Bayes, and logistic regression). Large label sets, high label cardinality (number of labels per instance), class imbalance, inter-class correlations, large feature sets are some of the factors that negatively affect performance in multi-label classification problems. We experiment with different state-of-the-art feature selection, training data selection, classifier chaining, and label calibration approaches to address some of these issues. We achieve comparable results to the state-of-the-art on the CMC dataset. Our experiments reveal how the differences in the nature of our three datasets significantly affect the performance of different supervised approaches. To our knowledge, our current contribution is the first of its kind in terms of the total number of codes considered and the realistic nature of the multi-document EMRs (Section 3). We design a sophisticated end-to-end pipeline involving novel features that depend on output code co-occurrences to rerank codes and to predict the correct number of codes per EMR (Sections 4.6 and 4.7). While our current task is diagnosis code assignment, our methods can also be used by the AI community to situations where coded biomedical information needs to be extracted from textual narratives.The rest of the paper is organized as follows: In Section 2 we discuss related work on automated diagnosis code assignment and provide background on multi-label classification approaches. We elaborate on the statistics and characteristics of the three different datasets used in Section 3. In Section 4, we present details of different multi-label text classification learning components used in our experiments. After a brief discussion of evaluation measures in Section 5, we present our results in Section 6. We present a qualitative error analysis of our results and identify opportunities for improvement in Section 7.In this section we discuss related work and prior efforts in assigning ICD-9 codes and present a brief general background for multi-label classification techniques.Several attempts have been made to automatically assign ICD-9 codes to clinical documents since the 1990s. Advances in natural language and semantic processing techniques contributed to a recent surge in automated coding. de Lima et al. [4] use a hierarchical approach utilizing the alphabetical index provided with the ICD-9-CM resource. Although completely unsupervised, this approach is limited by the index not being able to capture all synonymous occurrences and also the inability to code both specific exclusions and other condition specific guidelines. Gunderson et al. [5] extracted ICD-9 codes from short free text diagnosis statements that were generated at the time of patient admission using a Bayesian network to encode semantic information. However, in the recent past, concept extraction from longer documents such as discharge summaries has gained interest. Especially for ICD-9 code assignment, recent results are mostly based on the systems and the CMC dataset developed for the BioNLP workshop shared task on multi-label classification of clinical texts [3] in 2007.The CMC dataset consists of 1954 radiology reports arising from outpatient chest x-ray and renal procedures and is observed to cover a substantial portion of pediatric radiology activity. The radiology reports are also formatted in XML with explicit tags for history and impression fields. Finally, there are a total of 45 unique codes and 94 distinct combinations of these codes in the dataset. The dataset is split into training and testing sets of nearly equal size where example reports for all possible codes and combinations occur in both sets. This means that all possible combinations that will be encountered in the test set are known ahead of time. The top system obtained a micro-average F-score of 0.89 and 21 of the 44 participating systems scored between 0.8 and 0.9. Next we list some notable results that fall in this range obtained by various participants and others who used the dataset later. The techniques used range from completely handcrafted rules to fully automated machine learning approaches. Aronson et al. [6] adapted a hybrid medical subject heading (MeSH) assignment program called the medical text indexer (MTI) that is in use at the National Library of Medicine (NLM) and included it with SVM and k nearest neighbor classifiers for a hybrid stacked model. Goldstein et al. [7] applied three different classification approaches – traditional information retrieval using the search engine library Apache Lucene, Boosting, and rule-based approaches. Crammer et al. [8] use an online learning approach in combination with a rule-based system. Farkas and Szarvas [9] use an interesting approach to induce new rules and acquire synonyms using decision trees. Névéol et al. [10] also model rules based on indexing guidelines used by coders using semantic predications to assign MeSH heading-subheading pairs to indexing biomedical articles. A recent attempt [11] also exploits the hierarchical nature of the ICD-9-CM terminology to improve the performance achieving comparable performance to the best scores achieved during the competition. Although not applicable/practical in all situations, researchers have also tried to predict codes purely based on structured resources in the EMR [12].In terms of dataset size, there are two recent efforts similar to ours. The first is by Ruch et al. [13] who worked on French EMRs. They used a k-NN approach to obtain codes from training documents. However, their results are very inconclusive as they focused on example-based recall R20, the recall at top 20 codes, without the accompanying precision values. Instead, they report P0, which is precision of the top most code. The second effort is by Perotte et al. [14] conducted independently around the same time our research was conducted. They use flat and hierarchical SVM approaches with a dataset of nearly 22,000 discharge summaries of intensive care unit patients and the corresponding diagnosis codes. They achieve an F-score of 39.5% over a set of 5000 codes where the definitions of true positives and false negatives are relaxed based on the ICD-9 hierarchy in a way that is broader than our treatment of codes at the fourth digit level (see Section 3). Their effort also considers single summaries while we consider full EMRs based on in-patient visits. Hence our findings are not directly comparable with their results.Next, we provide a brief review of the background and state-of-the-art in multi-label classification, the general problem of choosing multiple labels among a set of possible labels for each object that needs to be classified. A class of approaches called problem transformation approaches convert the multi-label problem into multiple single-label classification instances. A second class of methods adapts the specific algorithms for single-label classification problems to directly predict multiple labels. Both problem transformation and algorithm adaptation techniques are covered in this recent survey by [15]. Recent attempts in multi-label classification also consider label correlations [16–18] when building a model for multi-label data. An important challenge in problems with a large number of labels per document is to decide the number of candidates after which candidate labels should be ignored, which has been recently addressed by calibrated ranking [19] and probabilistic thresholding [20]. Feature selection is an important aspect when building classifiers using machine learning. We request the readers to refer to Forman [21] for a detailed comparative analysis of feature selection methods. Combining the scores for each feature using different feature selection methods has also been applied to multi-label classification [22]. When dealing with datasets with class imbalance, methods such as random under/over-sampling, synthetic training sample generation, and cost-sensitive learning were proposed (see [23] for a survey). In contrast with these approaches, Sohn et al. [24] propose an alternative Bayesian approach to curate customized optimal training sets for each label. In the next section, we discuss the characteristics of the three datasets used in this paper.We already introduced the CMC dataset in Section 2 when discussing related work. Here we will give some additional details to contrast it with our private datasets. From the 1954 reports in the CMC dataset, 978 are included in the training dataset with their corresponding ICD-9 codes; the remaining documents form the testing set. All labels sets that occur in the testing set occur at least once in the training dataset. At least 75% of the codes appear less than 50 times in the training set. Almost 50% of the 45 labels appear less than ten times. For each instance in the CMC dataset there are two fields that contain textual data, ‘clinical history’ and ‘impression’. Clinical history field contains textual information entered by a physician before a radiological procedure about the patient's history. The impression field contains the textual narrative entered by a radiologist after the radiological procedure about his/her observations of the patient's condition as obtained from the procedure. Many of the textual entries contained in these two fields are very short. An example of the clinical history field is “22 month old with cough.” The corresponding impression is just one word “normal.” The average size of a report is 21 words.We created two datasets from EMRs of the UKY medical center in-patient visits with discharge dates in the 2011–2012 two year period. They have been approved by the UKY IRB for use in research projects (protocol #12-0139-p3h). The first dataset UKLarge is the largest and consists of all in-patient visits during this time period. We also collected the ICD-9 codes for these EMRs assigned by one of the trained coders at the UKY medical records office. There are a total of 71,461 EMRs that have a total of 7,485 unique ICD-9 codes. The average number of codes is 9.76 per EMR with a median of 8 codes. For each in-patient visit, the original EMR consisted of several documents, some of which are not conventional text files but are stored in the RTF format. There were approximately 921,000 physician authored documents in the entire dataset, so an average of 13 documents per EMR. Since many of the codes in the UKLarge have very few examples, we decided to consider predicting codes at the fourth digit level. That is, all codes of the form abc.xy for different ‘y’ are mapped to the four digit code abc.x, but codes that were already coded at the fourth or third digit level were retained in that form. With this mapping we had 4,723 unique codes. The average size of each EMR (that is, of all textual documents in it) in the UKLarge dataset is 5303 words. Even when truncated to 4 digits, there were still many codes that had too few examples to apply supervised methods. Hence we resorted to using 1231 codes (at the fourth digit level) that had at least 50 EMRs in the dataset. That is, our predictions are going to be only on these 1231 codes, and not all of the 4,723 codes. This subset of codes accounts for 76% of the total diagnoses in the dataset. There are 60,238 unique combinations of these 1231 codes in the data.Label-cardinality is the average number of codes per report (or EMR in the UKY dataset). To use consistent terminology we refer to the single reports in the CMC dataset as EMRs that consist of just one document. Let m be the total number of EMRs and Yibe the set of labels for the i-th EMR. Then we haveLabel-Cardinality=1m∑i=1m|Yi|.The CMC training dataset has label cardinality 1.24 and the UKLarge dataset has label cardinality 7.4. Another useful statistic that describes the datasets is label-density, which divides the average number of codes per EMR by the total number of unique codes q. We haveLabel-Density=1q·1m∑i=1m|Yi|.The label-density for the CMC dataset is 0.03 and for UKLarge is 0.006. Unlike label-cardinality, label-density also takes into account the number of unique labels possible. Two datasets with the same label cardinality can have different label densities and might need different learning approaches tailored to the situations. Intuitively, in this case, the dataset with the smaller density is expected to have fewer training examples per label.As we can see, the datasets have significant differences: the CMC data set is coded by three different coding companies and final codes were consolidated from these three different assignments. As such, it is of higher quality compared to UKLarge, which is coded by only one of the eight trained coders from the UKY medical records office. On the other hand, the CMC dataset does not have the broad coverage of the UKLarge, which models a more realistic dataset at the EMR level. The CMC dataset only includes radiology reports and has 45 codes with 94 code combinations and has on an average 21 words per EMR. In contrast, even with the final set of 1231 codes (at the four digit level) that have at least 50 examples that we use for our experiments, the number of combinations for the UKY dataset is 60,238 with the average EMR size two orders of magnitude more than the average for the CMC dataset.We created a subset of UKLarge, called UKSmall, with 1000 EMRs corresponding to a randomly chosen set of 1000 in-patient visits from February, 2012. Although in terms of the nature of the EMR documents this dataset shares the same traits as the UKLarge dataset, it has fewer codes and also fewer examples per code when considering absolute count. We also removed all EMR documents whose names did not contain any of the words “report”, “summary”, “note”, or “portion” based on coding specific knowledge that important notes for coding tasks contain these words. This halves the size of our EMRs in this dataset and from an automatic code extraction perspective has been shown to be detrimental for code recall (more in Section 7). For this dataset, we made prediction only for those codes that had at least 20 examples; although this number is less than the minimum requirement of 50 examples for the UKLarge dataset, considering the size of the datasets, 20 examples out of a total of 1000 examples constitute a 2% presence. However, 50 examples in a dataset of over 71,000 examples, constitute only 0.07% of the UKLarge dataset, which is an order of magnitude fewer examples than that for the UKSmall dataset. This is also reflected by the label density for the two datasets as shown in the summary of all three datasets in Table 1. This crucial difference translates to high class imbalance in the UKLarge dataset compared with the other two datasets. This was the original dataset used in the conference version of the paper [25] and offers a different perspective on suitability of different learning components and performance variations based on differences in size and nature of the datasets available (Section 7).The main motivation for our effort is to assess the suitability of feature selection, data selection, label calibration, and different problem transformation methods for extracting diagnosis codes from EMRs on datasets at different scale (the three datasets in Section 3). The learning components we use fall into these four categories that are arranged in the order they are used.1.First, we use a problem transformation approach and transform the multi-label classification problem into multiple binary classification problems. We also use different approaches that take into account label correlations expressed in the training data (Section 4.3).After problem transformation, we utilize feature selection and training data selection as additional components of the binary classifiers (Sections 4.4 and 4.5).Using the binary classifier outputs for each label, we use the learning-to-rank approach to rerank top label predictions from the binary classifiers based on output code co-occurrences (Section 4.6).Finally, we also use label calibration methods to predict an appropriate number of labels for each EMR instead of a choosing a constant number for all (Section 4.7).We used unigram and bigram counts as the common features in all experiments. Stop words (determiners, prepositions, and so on) were removed from the unigram features. For the CMC dataset where the report size is very short, we used the binary features, but for the other two datasets we used the popular tf-idf transformation [29, Section 4] of word counts. In addition to these syntactic features, we also used semantic features such as named entities and binary relationships (between named entities) extracted from text, popularly called semantic predications, as features for the smaller datasets. Due to computational resource availability constraints, we could not use named entities or relationships as features for the UKLarge dataset. To extract named entities and semantic predications, we used software made available through the Semantic Knowledge Representation (SKR) project by the NLM. The two software packages we used were MetaMap and SemRep. MetaMap [30] is a biomedical named entity recognition (NER) program that identifies concepts from the Unified Medical Language System (UMLS) Metathesaurus, an aggregation of over 160 biomedical terminologies. When MetaMap outputs different named entities, it associates a confidence score in the range from 0 to 1000. We only used concepts with a confidence score of at least 700 as features. Each of the concepts extracted by MetaMap also contains a field specifying if the concept was negated (e.g., “no evidence of hypertension”). We used negated concepts that capture the absence of conditions/symptoms as different features from the original concepts. We used SemRep, a relationship extraction program developed by Thomas Rindflesch [31] and team at the NLM that extracts semantic predications of the form C1→relationType→C2 where C1 and C2 are two different biomedical named entities and relationType expresses a relation between them (e.g., “Tamoxifen treats Breast Cancer”). If there is more than one document in an EMR, features are aggregated from all documents authored by a physician.With this introduction, we present the overall code prediction pipeline in Fig. 1. As shown in the figure, we first build the best models using problem transformation, feature selection, and data selection methods. Using the top ranked codes from these models for each EMR, we learn a ranking function that uses several novel features (Section 4.6) including the classifier scores based on a validation dataset. Subsequently, we also use the validation dataset to learning a function that predicts the correct number of codes for each EMR, which is used to select case specific top few codes after the learning-to-rank component reranks the original binary classifier based positions. NLM tools MetaMap and SemRep are not only used as features for the binary classifiers but also as features and filters in ranking the codes and predicting the correct number of labels.We initially experimented with three base classifiers on codes in the UKSmall dataset: Support Vector Machines (SVMs), Logistic Regression (LR), and Multinomial Naive Bayes (MNB). We used the MNB classifier that is made available as part of the Weka framework. For LR, we used LibLINEAR [32] implementation in Weka/Scikit-Learn and for SVMs we used LibSVM [33] in Weka. We experimented with three different multi-label problem transformation methods: binary relevance (BR), copy transformation, and ensemble of classifier chains.Let T be the set of labels and let q=|T|. BR learns q binary classifiers, one for each label in T. It transforms the dataset into q separate datasets. For each label Tj, we obtain the dataset for the corresponding binary classifier by considering each document–label-set pair (Di, Yi) and generating the document-label pair (Di, Tj) when Tj∈Yiand generating the pair (Di, ¬Tj) when Tj∉Yi. When predicting, the labels are ranked based on their score output by the corresponding binary classifiers and the top k labels are considered as the predicted set for a suitable k. The copy transformation transforms multi-label data into single-label data. Let T={T1, …, Tq} be the set of q possible labels for a given multi-label problem. Let each document Dj∈D, j=1, …, m, have a set of labels Yj⊆T associated with it. The copy transformation transforms each document–label-set pair (Dj, Yj) into |Yj| document–label pairs (Dj, Ts), for all Ts∈Yj. After the transformation, each input pair for the classification algorithm will only have one label associated with it and one can use any single-label method for classification. The labels are then ranked based on the score given from the classifier when generating predictions. We then take the top k labels as our predicted label set.One of the main disadvantages of the BR and copy transformation methods is that they assume label independence. In practical situations, there can be dependence between labels where labels co-occur very frequently or where a label occurs only when a different label is also tagged. Classifier chains [16], based on the BR transformation, try to account for these dependencies that the basic transformations cannot. Like BR, classifier chains transform the dataset into q datasets for binary classification per each label. But they differ from BR in the training phase. Classifier chains loop through each dataset in some order, training each classifier one at a time. Each binary classifier in this order will add a new Boolean feature to the subsequent binary classifier datasets to be trained next. For further details of the chaining approach and the ensemble of chains modification that overcomes dependence on the chaining order, we request the reader to refer to the paper by Read et al. [16].An important issue in classification problems with a large number of classes and features is that the features most relevant in classifying one class from the rest might not be the same for every class. Furthermore, many features are either redundant or irrelevant to the classification tasks at hand. In the domain of text classification, Bi-Normal Separation (BNS) score was observed to result in best performance in most situations among a set of 12 feature selection methods applied to 229 text classification problems [21]. We employ this feature scoring method for our current effort. Let T={T1, …, Tq} be the set of q possible labels. For each label Ti∈T and for each feature, we calculate the number of true positives (tp) and false positives (fp) with respect to that feature – tp is the number of training EMRs (with label Ti) in which the feature occurs. Similarly, fp is the number of negative examples for Tiin which the feature occurs. Let pos and neg be the total number of positive and negative examples for Ti, respectively. With F−1 denoting the inverse cumulative probability function for the standard normal distribution, we define the BNS score of a given feature for a particular class asBNS=|F−1(tpr)−F−1(fpr)|wheretpr=tpposandfpr=fpneg.Because F−1(0) is undefined, we set tpr and fpr to 0.0005 when tp or fp are equal to zero. For each of the q binary classification problems, we pick an appropriate top k ranked features for each label. In our experiments k=8000 gave the best performance out of a total of 68,364 features for the UKSmall dataset. The number of features was very small for the CMC dataset, a total of 2296 features, feature selection did not improve the performance. The UKLarge dataset had around a million features and feature selection did not improve performance.In the case of multi-label problems with a large number of classes, the number of negative examples is overwhelmingly larger than the number of positive examples for most labels. We experimented with the synthetic minority oversampling approach [34] for the positive examples which did not prove beneficial for our task. Deviating from the conventional random under/over-sampling approaches, we adapted the ‘optimal’ training set (OTS) selection approach used for medical subject heading (MeSH terms) extraction from biomedical abstracts by Sohn et al. [24]. The OTS approach is a greedy Bayesian approach that under-samples the negative examples to select a customized dataset for each label. The greedy selection is not technically optimal but we stick with the terminology in [24] for clarity. Our adaptation is described in detail in the conference version of our current contribution [25, Section IV(D)]. Essentially, the method ranks negative examples according to their content similarity to positive examples and iteratively selects negative examples according to this ranking and finally selects the negative instance subset that offers the best performance on a validation set for that label. The intuition behind this approach is that the negative examples that are ‘closest’ to the positive examples in terms of their content are the hardest to classify. Hence, choosing a subset that is the harder to distinguish from the positive examples provides the best outcome instead of using all the negative examples that could be easily distinguished but nevertheless add significant noise in the training process.As discussed in Section 4.3, the BR approach of building a binary classifier for each label does not take label correlations into account. Although the classifier chain transformation we discussed in that section helps account for label dependence, it is impractical with a large number of labels owing to the exponential number of chaining permutations for the ensemble. Other approaches such as ensemble of pruned label sets (EPS) [18] consider sets of labels as new special labels. Even this approach leads to combinatorial explosion for large label spaces. From Table 1 we have over 60,000 unique label combinations for UKLarge. Hence we need a different and computationally feasible way of accounting for label dependencies to rerank the original ranking of codes that is based on the binary classifiers. Originally, this reranking was done in an unsupervised way where empirical evidence is used to manually assign weights to different features that are pertinent to the ranking task. This is tedious and also impractical if the number of features used in the ranking is large. To handle this, as a collaboration between information retrieval and machine learning communities, learning-to-rank [35] has emerged as an automated way of learning functions that can rank a list of documents in response to an input query based on different query-specific features extracted from the documents. Although the original motivation was to rank documents returned from a search engine, this approach can be adapted to our current situation in a straightforward way. We first outline the approach in the information retrieval context.A learning-to-rank algorithm follows a supervised approach and in its training phase, takes as input a training dataset of queries and the corresponding ranked lists of documents:(1){(Qi,R(Di)):i=1,…,n},where Qiis a query, Diis the set of documents associated with Qi,R(Di)is the ground truth ranking on the documents for Qi, and n is the size of the training dataset. The algorithm then learns a ranking model that minimizes an appropriate loss function that pertains to the ranking. We note that the training process actually extracts featuresfj(Dir), r=1, …, t(i), for each documentDir∈Diwhere t(i) is the number of documents provided to the i-th query instance for training and j is an index for the particular features used. Note that features extracted from the documents are heavily based on the specific query to tightly constrain the ranking based on information available in the query. Finally, given a new query and a specific set of documents as input, the learned model imposes a ranking on the document set based on query specific document features. This is a general description of the problem; for a more detail discussion of the pointwise, pairwise, and listwise variants see [35, Section 1.3.3].Next, we map our problem of ranking ICD-9 codes to a listwise variant based on random forests that is implemented in the RankLib library, an open source collection of learning-to-rank implementations part of the Lemur project: http://sourceforge.net/p/lemur/wiki/RankLib/. We experimented with several of the algorithms in RankLib and chose the random forests based implementation that gave the best results on a validation dataset with normalized discounted cumulative gain (NDCG) [36] as the optimization metric. In Eq. (1), our queries Qiare the EMRs (the text from the EMRs) and the documents Diare the top 200 candidate ICD-9 codes when ranked solely based on the binary classifier rankings. We chose 200 candidates because the top 200 terms had close to 90% of all possible correct terms on a validation dataset for the UKLarge dataset. Next we discuss the features for the the learning-to-rank.An obvious feature of a candidate code c isfb(c,E)=score(∈[0,1])output by the corresponding binary classifier forcfor EMR E. Next, we introduce a Boolean feature based on NER.As introduced in Section 4.1, MetaMap is an NER tool that identifies biomedical concepts from over 160 source terminologies incorporated in the UMLS Metathesaurus (see NLM resource: http://www.ncbi.nlm.nih.gov/books/NBK9684/). ICD-9-CM is one of these terminologies and hence concepts in ICD-9-CM also have a concept unique identifier (CUI) in the Metathesaurus. As part of its output, for each concept extracted, MetaMap also gives the source vocabulary and the string representation of the code in the source terminology. Thus when MetaMap is run on an EMR text document set, we also obtain a set of ICD-9 codes, sayN(E), by filtering the non-negated concepts to only those that are from the ICD-9-CM terminology. Note that codes inN(E)may not all be coded for the EMR by trained coders because mere mentions in the EMR text does not necessarily warrant inclusion. These NER extracted codes can nevertheless be included in learning-to-rank as a Boolean featurefn(c,E)=1ifc∈N(E);0otherwise.Besides these two features, we also exploit ICD-9 code sets from historical records at the UKY medical center to obtain two additional features. From the time when the UK medical center moved to electronic record keeping to late 2011, we have a total of around 2 million visits and each visit has a set of diagnosis codes. From this, we can obtain code co-occurrence counts independent of the actual EMR texts. Intuitively, a predicted candidate code that highly co-occurs with the ICD-9 codes inN(E)(the ones extracted from EMR text using NER) is probably a more relevant code for the EMR over a predicted code with low co-occurrence with codes extracted using NER. We demonstrated that this co-occurrence based score is useful for unsupervised multi-label classification in the context of indexing biomedical citations with MeSH terms [37]. Here we use the co-occurrence score of a candidate term with the contextual codes extracted using NER from the EMR text as a feature for learning-to-rank. The co-occurrence score is computed by first building a square matrixM[i][j]=number of code sets containing bothi-th andj-th ICD-9 codesnumber of code sets containing thei-th codethat holds the normalized co-occurrence scores with size equal to the number of all unique ICD-9 codes that occurred in the 2 million code sets. Note thatM[i][i]=1because the numerator would be equal to the denominator. With this definition,M[i][j]is an estimate of the probability P(j-th code|i-th code). Finally, we introduce the co-occurrence frequency based feature for a candidate code c asfF(c,E)=∑t∈N(E)M[t][c].Our final feature is based on measuring relatedness between any given pair of codes using distributional semantics [38], a well known methodology traditionally used to identify implicit relatedness between words in a corpus of documents. Although the co-occurrence based feature fF(c, E) captures direct association between the candidate code and the context codes of a test set EMR, it does not capture latent or implicit associations between codes that might not have co-occurred frequently in historical data but are nevertheless strongly associated from a distributional semantics perspective. We map each of the 2 million code sets to a document and the constituent ICD-9 codes to terms to build a distributional semantic index using the term based reflective random indexing (TRRI) approach by Cohen et al. [39], which was shown to successfully predict implicit associations between words. Finally, the TRRI feature for a candidate code c is defined asfR(c,E)=∑t∈N(E)R(t,c),whereR(t,c)is the TRRI index based similarity score of t with c. We used the semantic vectors package [40] to construct the MeSH term vectors using TRRI. Both the co-occurrence and TRRI based features when combined helped us obtain improved performance in MeSH term prediction for indexing biomedical citations over using only one of them or neither of them. Hence we also chose to conduct experiments with both these features for our current task of code assignment.The conventional approach in binary classification using an LR model is to predict a positive response if the value of the logistic function (whose input is a function of the input variables) is greater than 0.5. However, in the BR model, it is possible to have testing instances where too many or too few labels are predicted with the 0.5 threshold when compared with the correct number of labels. Thus in multi-label classification, it is also important to consider the effect of the number of labels predicted per document on the performance of the approaches used. A quick fix that many employ is to pick a threshold of top r (r-cut) labels (when ranked either using just the LR classifier scores or using more sophisticated ranking approaches discussed in Section 4.6) where the r picked is the one that maximizes the example-based F-score on the training data. However, using this method always results in the same number of labels for each document in the testing set. We used an advanced thresholding method, Multi Label Probabilistic Threshold Optimizer [20] (MLPTO), for choosing a different number of labels per EMR that changes for each EMR instance. The optimizer we employed uses 1−(Example-based-F-score) as the loss function and finds the r that minimizes the expected loss function across all possible example-based contingency tables for each instance. For specific details of this strategy, please see [20]. This strategy is computationally expensive and hence we also experimented with a simpler approach of predicting the number of labels based on linear regression. The idea is to use a validation dataset where the numerical output is the correct number of labels for the EMRs. We used two numerical features: 1. the number of labels for the EMR for which the corresponding binary classifiers output a probability at least 0.1; 2. the number of ICD-9 codes identified in the EMR text by named entity recognition (NER) using MetaMap. We also tried other features including the size of the EMR (numbers of documents and words in the EMR) which did not seem to predict well on the validation dataset.Before we discuss our methods and results, we establish notation to be used for evaluation measures. Since the task of assigning multiple codes to an EMR is the multi-label classification problem, there are multiple complementary methods [41] for evaluating automatic approaches for this task. Recall that Yi, i=1, …, m, is the set of correct labels (here codes) in the dataset for the i-th EMR, where m is the total number of EMRs. Let Zibe the set of predicted labels for the i-th EMR. The example-based precision, recall, and F-score are defined asPex=1m∑i=1m|Yi∩Zi||Zi|,Rex=1m∑i=1m|Yi∩Zi||Yi|,andFex=1m∑i=1m2|Yi∩Zi||Zi|+|Yi|,respectively.For each label Tjin the set of labels T being considered, we have label-based precision P(Tj), recall R(Tj), and F-score F(Tj) defined asP(Tj)=TPjTPj+FPj,R(Tj)=TPjTPj+FNj,andF(Tj)=2P(Tj)R(Tj)P(Tj)+R(Tj),where TPj, FPj, and FNjare true positives, false positives, and false negatives, respectively, of label Tj. Given this, the label-based macro average F-score isMacro−F=1|T|∑j=1|T|F(Tj).The label-based micro precision, recall, and F-score are defined asPmic=∑j=1|T|TPj∑j=1|T|(TPj+FPj),Rmic=∑j=1|T|TPj∑j=1|T|(TPj+FNj),andMicro−F=2Pmic·RmicPmic+Rmic,While the macro measures consider all labels as equally important, micro measures tend to give more importance to labels that are more frequent. This is relevant for our dataset because we have a very unbalanced set of label counts and in such cases micro measures are considered more important.

@&#CONCLUSIONS@&#
