@&#MAIN-TITLE@&#
Character n-gram application for automatic new topic identification

@&#HIGHLIGHTS@&#
We used the character n-gram method to predict topic changes in search engine queries.We obtained more successful estimations than previous studies, and made remarkable contributions.We compared the character n-gram method with the Levenshtein edit-distance method.We analyzed ASPELL, Google and Bing search engines as pre-processed spelling correction methods.We conclude that Google could be used as a pre-processed spelling correction method.

@&#KEYPHRASES@&#
Content-ignorant algorithms,The character n-gram method,New topic identification,The Levenshtein edit-distance,Pre-processed spelling correction methods,

@&#ABSTRACT@&#
The widespread availability of the Internet and the variety of Internet-based applications have resulted in a significant increase in the amount of web pages. Determining the behaviors of search engine users has become a critical step in enhancing search engine performance. Search engine user behaviors can be determined by content-based or content-ignorant algorithms. Although many content-ignorant studies have been performed to automatically identify new topics, previous results have demonstrated that spelling errors can cause significant errors in topic shift estimates. In this study, we focused on minimizing the number of wrong estimates that were based on spelling errors. We developed a new hybrid algorithm combining character n-gram and neural network methodologies, and compared the experimental results with results from previous studies. For the FAST and Excite datasets, the proposed algorithm improved topic shift estimates by 6.987% and 2.639%, respectively. Moreover, we analyzed the performance of the character n-gram method in different aspects including the comparison with Levenshtein edit-distance method. The experimental results demonstrated that the character n-gram method outperformed to the Levensthein edit distance method in terms of topic identification.

@&#INTRODUCTION@&#
In recent years, applied researchers have become increasingly interested in correctly estimating the behaviors of search engine users to improve the performance of search engines. In order to determine these behaviors, researchers have investigated search engine query logs and classified them as topic continuation or topic shifts. For example, if a user continues to search on the same topic or content, the previous query is labeled as a topic continuation. Conversely, if a user changes the topic, the previous query is labeled as a topic shift. Using this approach, researchers have to improve the accuracy of search engine results.Numerous studies based on content-based (semantic) and content-ignorant (non-semantic) methods have been developed to determine topic shifts and continuations in the transaction logs of search engine users. Content-based approaches, which use dictionaries and thesauruses, are complicated, because these tools must be created and stored (Leung, Ng, & Dik, 2008). Although content-based methods identify new topics more successfully than content-ignorant methods, the content-ignorant methods have the advantages of speed and low memory requirements, which are essential in real-time applications. Further, these methods yield satisfactory results for new topic identification problems during search engine user sessions (Leung et al., 2008). Therefore, in this study content-ignorant methods were used to identify new topics.Content-ignorant methodologies consider the statistical characteristics of consecutive queries, such as the duration, and the set of common and different terms. Spink, Ozmutlu, and Ozmutlu (2002) and Goker and He (2000) conducted some of the first studies linking the statistical characteristics of queries to topic changes. Various content-ignorant methods have been developed to identify new topics using the same datasets and performance parameters. Some of these methods include the Dempster-Shafer theory and genetic algorithms (He, Goker, & Harper, 2002; Ozmutlu & Cavdur, 2005a; Ozmutlu, Cavdur, & Ozmutlu, 2006), conditional probabilities (Ozmutlu, Ozmutlu, & Buyuk, 2007), Monte-Carlo simulations (Ozmutlu, Ozmutlu, & Buyuk, 2008a), and neural networks (Ozmutlu & Cavdur, 2005b; Ozmutlu, Cavdur, & Ozmutlu, 2008b; Ozmutlu, Cavdur, Spink, & Ozmutlu, 2004a; Ozmutlu, Ozmutlu, & Cosar, 2011).These methods are limited in their ability to detect correct topic changes when consecutive queries have synonymous words or spelling errors. For example, content-ignorant methodologies cannot estimate topic continuation when subsequent queries such as “X Hotel” and “Y Inn” are included in a user session, so they generate a topic shift estimate as expected. The content-ignorant methodologies cannot recognize that the synonymous words are related, because they do not use a dictionary or a thesaurus. Further, if consecutive queries contain spelling errors, no new topic identification algorithm, whether it is a content-based or a content-ignorant algorithm, can recognize these terms are related which causes errors in topic shift estimates. For example, when content-ignorant methodologies must process two consecutive single-term queries, such as “cybersc@n” and “cyberscan”, which are the actual subsequent queries found in the FAST dataset, the two terms are not considered to be related because they do not exactly match. As a result, the two subsequent queries will be considered to be unrelated.Topic continuations cannot be detected without a dictionary when consecutive queries contain synonymous words; however, if consecutive queries contain spelling errors, topic changes can be detected by using the typing information contained in the queries, which has been ignored in previous content-ignorant methods.In consecutive queries, the majority of the characters in the words with spelling errors remain the same. In nearly all spelling errors, a single character has been mistyped, or two consecutive characters have been switched. Since the majority of the characters in the mistyped words are correct (the “cybersc@n” example given above), the use of a similarity measure could improve the performance of new topic identification algorithms by eliminating incorrect topic shift estimates due to spelling errors. The error model approach has been used to correct spelling with a dictionary, with correction probabilities and with an edit distance matrix. To eliminate the complexity of spelling error detection methods and to avoid the use of a dictionary, we chose the character n-gram method. Although this method detects spelling errors using typing information from consecutive queries, it does not perform well regardless of the statistical characteristics of queries. Therefore, the best previous method is combined with the character n-gram method to make use of the statistical characteristics and typing information of consecutive queries.Based on the previous studies about the new topic identification problem, the artificial neural network algorithm is chosen to be integrated into the character n-gram algorithm because it generates the most accurate estimates when used in the new topic identification problem (Ozmutlu et al., 2011). In this study, a hybrid algorithm based on the character n-gram and artificial neural network methods have been developed to eliminate the incorrect estimates caused by spelling mistakes while enhancing the performance of the existing algorithms. Comparison of the results from the new hybrid methodology and the artificial neural network algorithm shows that the proposed algorithm offers significant improvements.In addition, the performance of the suggested method is compared with one of the well-known similarity measurement method; the Levenshtein edit-distance. Moreover, original datasets are updated by different spelling correction methods (Bing, Google and ASPELL) to evaluate the effectiveness of the character n-gram method. Comparison of the results demonstrates that the character 2-grams method outperforms to the Levenshtein edit-distance, and the Google search engine can be used as a pre-processed spelling correction method.The remainder of this paper is organized as follows: in Section 2, previous studies on the new topic identification problem and character n-gram methodologies are reviewed. In Section 3, the data sources, study notation and definition of the character n-gram are presented with examples. Experimental results from the character n-gram method and the proposed method are described in Section 4. In Sections 5, a detailed analysis of the character n-gram method is presented, and in Sections 6 and 7, the study is summarized and the limitations and future applications are discussed.Web search engine data logs have been used in many studies by researchers such as Silverstein, Henzinger, Marais, and Moricz (1999), Cooley, Mobasher, and Srivastava (1999), Spink, Bateman, and Jansen (1999), Spink, Wolfram, Jansen, and Saracevic (2001), Spink, Jansen, Blakely, and Koshman (2006), Ozmutlu, Ozmutlu, and Spink (2004b), Ozmutlu et al. (2006) which are based on statistical or linguistic characteristics of web search queries (Pu, Chuang, & Yang, 2002).As usual, content-based algorithms work with dictionaries and thesaurus; they are more expensive and difficult to be implemented due to the requirement of creating and storing these dictionaries. Thus, there is only limited number of content-based studies in literature for the new topic identification in user sessions; mostly they are used for natural language processing, classification and clustering.Spink, Wolfram, Jansen, and Saracevic (2001), aimed to determine the behaviors of search engine users by examining more than one million queries from the Excite search engine. They ignored logic expressions in the queries, and they used the Intelligent Concept Extraction which is a method of making connection between terms. In this study, some useful data were gathered such as, number of total users, queries, repeated queries and unique queries. The usage ratio of similar pages was determined with content-based methods, the classification of queries and the distributions of these classes were determined. According to the results, 16.9% of the queries were included in entertainment subject, 16.8% of them were included in sexual subject contents, and 10% of the queries were related with science and health. Most of the queries had simple content and less than 3 words, few of them had complex content because most of the users did not go beyond to second result page. Further, the frequency of terms used during search was low because query language was very rich so the sessions were mostly formed by unique queries.One of the studies about personalization of search engines belongs to Leung et al. (2008). To improve the queries for more efficient researches, popular and advanced search engines propose various search alternatives related with search topics. Although search engines offer useful alternatives, they cannot detect the interest of users. Thus, the authors developed a method based on personalized concept-based clustering techniques to generate personalized proposals for the users. The authors used clickthrough data to generate query suggestions according to user preferences. They defined the relationship between queries, users and documents by expanding the scope of graph-based clustering algorithm which was developed by Beeferman and Berger (2000). They developed a Google interface which tracks the user clicks to collect clickthrough data. This interface recorded the data whenever user clicks on a search result. They tried to identify similar queries with content-based methods using the database of records. The study results showed that the improved method could generate personalized query proposals considering users’ conceptual requirements. Further, it was observed that the new clustering algorithm was able to yield better results compared to the previous clustering algorithms.Besides their complexity in real time applications, content-based studies are troublesome and costly. Although successful results can be acquired through these methods, researchers prefer content-ignorant statistical methods in real time applications because of the disadvantages of content-based methods. Content-ignorant methods are simpler and less expensive than content-based methods, and they can obtain realistic results by interpreting collected data statistically.Web researchers use web search engines for various purposes such as classification, clustering or new topic identification. Jansen, Booth, and Spink (2007) tried to classify search engine user intent; informational, navigational and transactional intent. They found that more than 80% of web queries were informational and about 20% of web queries were navigational and transactional queries. Similar to the classification, some researchers performed content analysis of Web search engine data logs at the term level; they observed that the highest ranking terms were related to topics of entertainment, pornography, and education (Spink et al., 2001).Further, exploring the behavioral changes throughout the day could provide important data about the behaviors of search engine users. For this purpose Ozmutlu, Ozmutlu, and Spink (2004b) performed statistical and topical analysis of about 1 million queries from the Excite and the FAST search engines. They found that the popularity of topics was diverse throughout the day. For example, topics such as finance, business and education were more popular during the earlier hours of the day whereas entertainment and pornography were more prevalent during the evening. Also Beitzel, Jensen, Chowdhury, Grossman, and Frieder (2004) reached similar results in their study.In addition to diversity of interested topics, search engine users can be interested in multiple topics at the same user session. Spink et al. (2002) called this research behavior multitasking and defined multitasking as “the process of searches over time in relation to more than one, possibly evolving, set of information problems including changes or shifts in beliefs, cognitive, affective, and/or situational states”. Researchers observed that 1.4% of the Excite search engine users and 31.8% of the FAST search engine users performed multitasking searches.Numerous studies have been developed based on query clustering models and algorithms to investigate search engine users behaviors. Beeferman and Berger (2000) and Wen, Nie, and Zhang (2002) applied the query clustering methods to search engine query logs, including clickthrough data. Leung et al. (2008) explored a methodology to develop personalized search engines based on Beerferman and Berger’s graph-based clustering algorithm. Muresan and Harper (2004) proposed a topic modeling system for developing mediated queries. The terms in a set of documents were analyzed statistically and then represented as a lexicographic model of the query. The context analysis was applied on the mediated queries based on the similarity of terms to specific topics. Giacomo, Didimo, Grilli, Liotta, and Palladino (2007) affirmed that classical search engines had a weak point in the presentation of results, and alternatively new generation search engines which are called web clustering engines have been designed. According to Giacomo et al. (2007) web clustering engines organized search results into set of clusters, and each cluster contained web pages that were semantically related to each other. They used various clustering algorithms to achieve this classification.Alternatively content-ignorant methods can be used for query clustering or new topic identification in a search session. In such an approach, queries can be categorized in different groups of topics respect to their statistical characteristics, such as time intervals between subsequent queries. He et al. (2002) proposed a topic identification algorithm based on Dempster-Shafer theory (Shafer, 1976). The algorithm identified topic changes automatically using statistical data obtained from Web search logs. The probabilities and the weights (importance) of events and a threshold were used by Dempster-Shafer theory to detect topic shifts. Probabilities of a topic shift (or continuation) based on a single factor (the time interval or the search pattern of the subsequent queries) were easily obtained through the analysis of the data log. To set the required weights and the threshold, the authors applied a genetic algorithm. The researchers concluded that the Dempster-Shafer theory could be used for the purpose of new topic identification successfully. Moreover, this approach was replicated by some researchers such as, Ozmutlu and Cavdur (2005a) and Ozmutlu et al. (2006).Due to the advantages of content-ignorant methodologies, numerous studies have been performed for the purpose of new topic identification. For example, Ozmutlu et al. (2004a) and Ozmutlu and Cavdur (2005b) used artificial neural networks to identify the topic changes automatically by content-ignorant methods. In addition to these studies Ozmutlu et al. (2007) proposed conditional probabilities for automatic new topic identification, and Ozmutlu et al. (2008a) applied Monte-Carlo simulation for the same reason to the same data. Ozmutlu et al. (2011) developed another neural network approach for the same data and they reached better results than the other methods. The statistical characteristics of the query log were the inputs of the neural network, whereas the binary response of topic shift/continuation was the output of the neural network. In this study, the data were cleaned from regular words found in the transaction logs, such as “AND”, “OR”, and then regarding to the statistical characteristics of queries, such as time intervals and search patterns, the neural network method was applied to the queries with the aim of identifying topic changes in a user session (Ozmutlu et al., 2011).In previous studies by Ozmutlu et al., 2004a,b, 2006, 2007, 2008a,b; Ozmutlu & Cavdur, 2005a,b, the same datasets and comparison parameters were used to logically compare different approaches. Based on these comparisons as presented in Table 1for the Excite dataset and in Table 2for the FAST dataset, the neural network application generated better estimate results than the previous studies (Ozmutlu et al., 2011).Content-ignorant methodologies do not improve upon the results given in previous studies. Further, two important problems that cannot be solved by content-ignorant methods have been identified (Ozmutlu et al., 2011). The first problem arises from spelling differences in subsequent queries, and the second problem arises from content-based mistakes, such as synonymous words. Because content-ignorant methodologies do not consider the meanings of queries, they cannot recognize topic continuation between sequential queries that include synonymous words. Conversely, in sequential queries that contain spelling differences, some portions of the queries often remain unchanged. This finding prompted us to identify a methodology that can detect similarities within words.Many researchers have used error model approach for spelling correction. Brill and Moore (2000) developed a new error model using the position probabilities of words’ partitions, which were calculated based on a dictionary, for noisy channel spelling correction. They used a training set consists of string pairs, spelling errors and correct spellings of the words. They calculated distances between letters, and after editing operations, they found the probabilities of each substitution. Using a dictionary (includes 200,000 entries) and the probabilities, they tested their model with a 10,000-word corpus of common English spelling errors, paired with their correct spelling. They concluded that the new error model had a significant improvement in performance compared to the previous studies. Cucerzan and Brill (2004) developed an iterative approach for spelling correction of search engine queries. They used a modified Damerau-Levenshtein edit distance (Damerau, 1964) and a prior probability information of correction. They performed two approaches with and without a trusted lexicon. They developed a successful approach for spelling correction task of search query logs using their collective information. This study demonstrated that the Levenshtein edit-distance method without lexicon could be used as a content-ignorant method to detect spelling mistakes in search engine query logs for the purpose of new topic identification. However, detecting only spelling mistakes would not be sufficient for the new topic identification, hence this situation prompted us to search more effective tools that can detect similarities within words including spelling mistakes.The n-gram methodology is widely used in statistical language modeling for the purpose of predicting the next word given previous words. The n-gram language models make an assumption that the probability of the next word depends upon the last n−1 words. Shannon (1951) tried to guess next letter in a text with Shannon game. After Shannon’s studies, many estimate methods have been developed, but the n-gram method is remained the simplest and the most successful method in language modeling (Huang, Peng, An, Shuurmans, & Cercone, 2003). Damashek (1995) used the n-grams for measuring topical similarity in an unrestricted text. Huang et al. (2003) identified boundaries of sessions using n-grams in a large collection of Livelink log data. Canvar and Trenkle (1994) researched electronic documents, and they calculated the frequency of n-grams in terms of textual errors, such as spelling and grammatical errors. Roark, Saraclar, and Collins (2007) used a discriminative n-gram approach for speech recognition. Briefly, the n-gram language modeling can be used for speech or optical character recognition, spelling correction, handwriting recognition, and statistical machine translation.Although the n-gram method works with word sequences in a huge amount of text, spelling errors cannot be detected without considering words in characteristic review. The spelling errors in sequential queries can be detected by character n-grams. Therefore, using the character n-grams for predicting topic continuations in search engine queries is more logical than using the word n-grams.The use of character n-grams in language modeling dates back to Shannon (1951). The researcher made contributions to the information retrieval theory, and he also described a sequence of the character n-gram and the word n-gram approximations to English (McNamee & Mayfield, 2004). After this research, implementations of the character n-gram method increased rapidly. McNamee and Mayfield (2004) used the character n-gram method for multilingual text retrieval. They aimed to demonstrate that the character n-gram tokenization can provide retrieval accuracy better than the other language-specific approaches. Liu and Keselj (2007) studied about automatic classification of web user navigation patterns, and they implemented the character n-gram method for capturing textual content of web pages. Kamaris and Stamatatos (2007) studied about webpage genre identification for improving the quality of search engines, and they applied the character n-gram method to identify of webpage genres. Chau, Lu, Fang, and Yang (2009) researched the character usage of Chinese search logs from Chinese search engines. Since the character n-gram method is independent from language, they implemented this method to their study without any difficulty. Vilares, Vilares, and Otero (2011) used the classic stemming-based methods and the character n-gram method for the purpose of identifying spelling mistakes and make corrections in Spanish. They compared these methods and showed performance results in their study. In addition to these studies, the character n-gram method also has been used in handwriting recognition (El-Nasan A. & M., 2002; Senda & Yamada, 2001).Researchers have developed various content-ignorant methods for the purpose of new topic identification. These methods have focused primarily on the statistical characteristics of queries rather than on typing information. As a result, recently developed methods cannot detect spelling errors or similarities between words in consecutive queries, which are important in topic identification. The aim of this study is to reduce the number of topic identification failures by using typing information from consecutive queries. We develop a character n-gram method that uses similarities between sequential queries and apply it to datasets that have been used in previous studies. Comparing the results of the character n-gram method with results from previous methods, we show that using typing information from the queries rather than the statistical characteristics of the queries is not sufficient for new topic identification. Thus, we combine the best previous method with the character n-gram method to make use of the statistical characteristics and the typing information of the queries. The results show that combining these methods ensures the accurate identification of new topics. Moreover we also compare our proposed method with well-known spelling detection methods, and we present that our methodology outperforms to these methods in terms of accurate identification of new topics.

@&#CONCLUSIONS@&#
