@&#MAIN-TITLE@&#
On allocation of redundant components for systems with dependent components

@&#HIGHLIGHTS@&#
We consider the problem of allocation of a redundant component for systems with dependent components.We develop multivariate extensions of the bivariate joint stochastic orders as a tool to provide the new results.Extensions of known results in the case of independent components are given for the case of dependent components.

@&#KEYPHRASES@&#
Standby and active redundancy,Joint stochastic orders,k-out-of-n systems,Dependent components,

@&#ABSTRACT@&#
In this paper we consider the problem of optimal allocation of a redundant component for series, parallel and k-out-of-n systems of more than two components, when all the components are dependent. We show that for this problem is naturally to consider multivariate extensions of the joint bivariates stochastic orders. However, these extensions have not been defined or explicitly studied in the literature, except the joint likelihood ratio order, which was introduced by Shanthikumar and Yao (1991). Therefore we provide first multivariate extensions of the joint stochastic, hazard rate, reversed hazard rate order and next we provide sufficient conditions based on these multivariate extensions to select which component performs the redundancy.

@&#INTRODUCTION@&#
Allocation of redundant component in a system in order to optimize, in some sense, the lifetime of the system is an important problem in reliability, from an applied and theoretical point of view. In the literature we can found a great number dealing with this subject. In Boland and Proschan (1994) we can find several results and references. More recent results can be found in the papers by Mi (1999), Valdés and Zequeira (2003), Romera, Valdés, and Zequeira (2004), Yalaoui, Chu, and Châtelet (2005), Valdés and Zequeira (2006), Ha and Kuo (2006), Li and Hu (2008), Hu and Wang (2009), Misra, Dhariyal, and Gupta (2009), Li and Ding (2010), Valdés, Arango, Zequeira, and Brito (2010), Ding and Li (2012) and Zhao, Chan, and Ng (2012), among others. Whereas this problem has been extensively treated for the case of independent components, the case of dependent components has not received too much attention. Only the papers by Kotz, Lai, and Xie (2003), da Costa Bueno (2005) and da Costa Bueno and Martins do Carmo (2007) deals with this problem. Some of these results can be found also in Section 10.3 in Lai and Xie (2006). Recently Belzunce, Martínez, and Ruíz (2011) have considered the problem of allocation of redundant components for series and parallel system in the case of two dependent components. In this paper two commonly used redundancies are discussed: active redundancy, which stochastically leads to consideration of the maximum of random variables, and standby redundancy, which stochastically leads to consideration of the convolution of random variables.The purpose of this paper is to extend the results given by Belzunce et al. (2011) to the case of systems of more than two components, when all the components are dependent. Let us consider a system with n components with random lifetimes T1,T2,…,Tnand an additional component, with random lifetime S, that can be put in active or standby redundancy with anyone of the n components. Then, we can consider n possibly different systems, depending on which component we perform the redundancy, and the problem is which one of the n system has a larger lifetime in some probabilistic sense. We initially consider series and parallel systems, and later we consider the problem of allocating a redundant spare in a k-out-of-n system of n components. A k-out of-n system is a system of n components which functions if, and only if, at least k of its components function and whose lifetime we will be denote by τk{T1,…,Tn}. Series and parallel systems are particular cases of k-out-of-n and more specifically, we have that τn{T1,…,Tn} and τ1{T1,…,Tn} are the lifetimes of the resulting series system and parallel system, respectively. For k-out of-n system, one may refer to Barlow and Proschan (1981) for a comprehensive discussion.For a treatment of this problem, previously it has been necessary to consider multivariate extensions of the joint bivariate orders introduced by Shanthikumar and Yao (1991) and Shanthikumar, Yamazaki, and Sakasegawa (1991). For the moment we have been able only to applied the joint stochastic orders for the case of one redundant component. It remains as an open problem the case of s>1 redundant components, like in Mi (1999), Hu and Wang (2009) or Ding and Li (2012).The structure of this paper is as follows. In Section 2 we consider several notions of stochastic orders, we study multivariate extensions of the joint stochastic, hazard rate and reversed hazard orders. We establish also some results which will be used in the proofs of Section 3. The main results are presented in Section 3 and they will be compared to the existing ones in the case of independence. We conclude, in Section 4, with examples where the results can be applied.In this paper, for any random variable X and an event A, we denote by {X∣A} any random variable whose distribution is the conditional distribution of X given A. The random variables considered in Section 3 are assumed to be continuous and nonnegative. Given an n-dimensional random vector (X1,…,Xn) with joint distribution function F, we denote byF¯(x1,…,xn)≡P(X1>x1,…,Xn>xn)its joint survival function.In order to select, among two systems, which one has a “larger” random lifetime, one of the most important tools is the comparison of random lifetimes in terms of several notions of stochastic orders. Several criteria have been defined to compare the “magnitude” of two random variables, we recall first the definition of some of these stochastic orders (see Müller & Stoyan, 2002; Shaked & Shanthikumar, 2007 for definitions, properties and references).Definition 2.1Let X and Y be two random variables, we say that X is smaller than Y in the usual stochastic order, denoted by X⩽stY, ifF¯(t)=P[X>t]⩽P[Y>t]=G¯(t),forallt∈R, or equivalently, if E[ϕ(X)]⩽E[ϕ(Y)] for all increasing function ϕ for which previous expectations exist.The definition of the stochastic order is a way to formalize the idea that the random variable X is less likely than Y to take on large values. For the case of two random vectors we have the following extension.Definition 2.2LetXandYbe two n-dimensional random vectors, we say thatXis smaller thanYin the usual stochastic order, denoted byX⩽stY, if E[ϕ(X)]⩽E[ϕ(Y)] for all increasing functionϕ:Rn→R, for which previous expectations exist.Two properties that will be used along the paper are the preservation under increasing transformations and under mixtures.Lemma 2.1LetXandYbe two n-dimensional random vectors.(a)IfX⩽stYthen ϕ(X)⩽stϕ(Y), for any increasing functionϕ:Rn↦Rk.LetΘbe an m-dimensional random vector, if [X∣Θ=θ]⩽st[Y∣Θ=θ], for allθin the support ofΘ, thenX⩽stY.Some other notions have been found of interest to compare random variables.Definition 2.3Let X and Y two random variables with distribution functions F and G, respectively, we say that X is smaller than Y in the hazard rate order, denoted by X⩽hrY, ifF¯(x)G¯(y)⩾F¯(y)G¯(x)forallx⩽y. Additionally we say that X is smaller than Y in the reversed hazard rate order, denoted by X⩽rhY, if F(x)G(y)⩾F(y)G(x) for all x⩽y.In the case of absolutely continuous random variables we can consider also the following criteria.Definition 2.4Let X and Y two absolutely continuous random variables with density functions f and g, respectively, we say that X is smaller than Y in the likelihood ratio order, denoted by X⩽lrY, if f(x)g(y)⩾f(y)g(x) for all x⩽y.Among these orders we have the following relationships:X⩽lrY⇒X⩽[hr,rh]Y⇒X⩽stY.Another criteria to compare random lifetimes used in this context is the following.Definition 2.5Let X and Y two random variables, we say that X is smaller than Y in the preference order, denoted by X⩽prY, if P(X>Y)⩽P(Y>X).No relationships are known among this notion and the hr, rh and lr orders, and it is well known (see Blyth, 1972) thatX⩽stY⇏(⇍)X⩽prY.Except for the preference order, the stochastic orders considered previously only take into account the information provided by the marginal distributions to compare the random variables. Shanthikumar and Yao (1991) and Shanthikumar et al. (1991) introduce some stochastic orders that take into account the dependence among the random variables. To give these definitions we recall first, the definition of the following sets of functions.Definition 2.6Let us denote by D the set of functionsD={g|g:R2↦R}, we consider the following sets:(a)Glr={g∈D: g(x, y)⩽g(y, x), for all x⩽y}.Ghr={g∈D: g(y, x)−g(x, y) is increasing in y, for all x⩽y}.Grh={g∈D: g(x, y)−g(y, x) is increasing in x, for all x⩽y}.Gst={g∈D: g(y, x)−g(x, y) is increasing in y, for all x}.We recall now the following definitions.Definition 2.7Given two random variables X and Y, we say that X is smaller than Y in the joint usual stochastic [hazard rate, reversed hazard rate, likelihood ratio] order, denoted by X⩽st:j [hr:j, rh:j, lr:j]Y, if E[g(X, Y)]⩽E[g(Y, X)], for all g∈Gst[hr, rh, lr].These joint stochastic orders have been used by Belzunce et al. (2011) to provide results for the allocation of redundant components in series and parallel systems with two dependent components. The purpose of this paper is to extend these results to the general case with more than two dependent components and also to provide results in the general case of k-out-of-n systems. Therefore it is natural to consider extensions of the joint stochastic orders from the bivariate to the multivariate case. However, these extensions have not been defined or explicitly studied in the literature, except for the joint likelihood ratio order, which was introduced by Shanthikumar and Yao (1991). To give this extension, we need to recall the following definitions.Definition 2.8Letx=(x1,…,xi,…,xj,…,xn) andx′=(x1,…,xj,…,xi,…,xn) be two vectors inRn. We said that x′ is a simple transposition of x, denoted by x′⩽tx, if xi<xj, with i<j∈{1,…,n}.Let x and y be two vectors inRn. We said that x⩽ay if there exist x1,x2,…,xr∈Rnsuch that x⩽tx1⩽tx2⩽t⋯⩽txr⩽ty, that is, x can be obtained from y by a finite sequence of simple transpositions.A functionf:Rn↦Ris said to be arrangement increasing, denoted by f∈AI, if x⩽ay∈Rnimplies f(x)⩽f(y). If f is arrangement increasing then −f is arrangement decreasing.Shanthikumar and Yao (1991) propose the following definition of the lr:j order for the case of n-dimensional random vectors. We assume that the random vectors are absolutely continuous.Definition 2.11Given n random variables X1,…,Xn, we say that X1⩽lr:j⋯⩽lr:jXnif the joint density function f of the random vector (Xn,…,X1) is AI.It is not difficult to see that this property is preserved under marginalization. Equivalently this definition can be given as follows (see Theorem 5.2 by Shanthikumar & Yao, 1991).Definition 2.12Let us denote by C the set of functionsC={g|g:Rn↦R}, we consider the following set:Glr={g∈C:g(x1,…,xi,…,xj,…,xn)⩽g(x1,…,xj,…,xi,…,xn),forallxi⩽xj}.Given n random variables X1,…,Xn, we say that X1⩽lr:j⋯⩽lr:jXn, ifE[g(X1,…,Xi,…,Xj,…,Xn)]⩽E[g(X1,…,Xj,…,Xi,…,Xn)],for all g∈Glr, and for all i<j, or equivalently if,g(X1,…,Xi,…,Xj,…,Xn)⩽stg(X1,…,Xj,…,Xi,…,Xn),for all g∈Glr.Additionally we need to consider multivariate extensions of the joint stochastic, hazard rate and reversed hazard orders. To give these extensions, previously, we define the following sets of functions.Definition 2.13Under the same notation given in Definition 2.12, we define the following sets:(a)Ghr={g∈C: g(x1,…,xj,…,xi,…,xn)−g(x1,…,xi,…,xj,…,xn) is increasing in xj, for all xi⩽xj, and for all i<j}.Grh={g∈C: g(x1,…,xi,…,xj,…,xn)−g(x1,…,xj,…,xi,…,xn) is increasing in xi, for all xi⩽xj, and for all i<j}.Gst={g∈C: g(x1,…,xj,…,xi,…,xn)−g(x1,…,xi,…,xj,…,xn) is increasing in xj, for all xi, and for all i<j}.For these new classes it is not difficult to prove the following relationships:(2.1)Gst⊆Ghr[rh]⊆Glr.In terms of the sets Gst [rh, hr], we propose the following definitions of the joint multivariate stochastic, hazard rate and reversed hazard rate orders.Definition 2.14Given n random variables X1,…,Xn, we say thatX1⩽st:j[rh:j,hr:j]⋯⩽st:j[rh:j,hr:j]Xn,ifE[g(X1,…,Xi,…,Xj,…,Xn)]⩽E[g(X1,…,Xj,…,Xi,…,Xn)],for all g∈Gst[rh, hr], and for all i<j.Therefore, from (2.1), we have the following relationships between the orders defined above:(2.2)X1⩽lr:j…⩽lr:jXn⇒X1⩽hr:j[rh:j]…⩽hr:j[rh:j]Xn⇒X1⩽st:j…⩽st:jXn.For the new joint multivariate orders we derive some results that will be useful for the rest of the paper.First we provide a result for the joint stochastic order.Theorem 2.2Let us consider n random variables X1,…,Xn. If X1⩽st:j⋯⩽st:jXnthen(X1,…,Xi,…,-Xj,…,Xn)⩽st(X1,…,Xj,…,-Xi,…,Xn),for all i<j.For all increasing functionϕ:Rn↦R, for which previous expectations exist, let us consider the real valued functionψ(x1,…,xi,…,xj,…,xn)=ϕ(x1,…,xi,…,-xj,…,xn).Is easy to prove that ψ(x1,…,xi,…,xj,…,xn)∈Gst. Therefore, from the hypothesis, we have thatE[ψ(X1,…,Xi,…,Xj,…,Xn)]=E[ϕ(X1,…,Xi,…,-Xj,…,Xn)]⩽E[ϕ(X1,…,Xj,…,-Xi,…,Xn)]=E[ψ(X1,…,Xj,…,Xi,…,Xn)],that is (X1,…,Xi,…,−Xj,…,Xn)⩽st(X1,…,Xj,…, −Xi,…,Xn), for all i<j.□Now we provide some results for the joint hazard rate order. First we provide a technical result that will be used in the proof of the main result for the joint hazard rate order.Lemma 2.3Let us consider a random vector (X1,…,Xn), with absolutely continuous joint distribution F and joint density function f. Let us consider i<j∈{1,…,n}, s∈{1,…,n}−{i, j} and δ>0, for all xi⩽xj, let us consider the real valued function:g(u1,…,ui,…,uj,…,un)=1,ifxi-δ⩽uj<xi,xj⩽uiandxs⩽us,0,otherwise.Then, we have:(a)E[g(X1,…,Xi,…,Xj,…,Xn)]=F¯(x1,…,xj,…,xi-δ,…,xn)-F¯(x1,…,xj,…,xi,…,xn).E[g(X1,…,Xj,…,Xi,…,Xn)]=F¯(x1,…,xi-δ,…,xj…,xn)-F¯(x1,…,xi,…,xj,…,xn).(a)E[g(X1,…,Xi,…,Xj,…,Xn)=∫x1+∞⋯∫xj+∞⋯∫xi-δxi⋯∫xn+∞f(u1,…,ui,…,uj,…,un)dun…duj…dui…du1=∫x1+∞⋯∫xj+∞⋯∫xi-δ+∞⋯∫xn+∞f(u1,…,ui,…,uj,…,un)dun…duj…dui…du1-∫x1+∞⋯∫xj+∞⋯∫xi+∞⋯∫xn+∞f(u1,…,ui,…,uj,…,un)dun…duj…dui…du1,that is,(2.3)E[g(X1,…,Xi,…,Xj,…,Xn)]=P⋂p(≠i,j)=1n(Xp>xp)∩(Xj>xi-δ)∩(Xi>xj)-P⋂p(≠i,j)=1n(Xp>xp)∩(Xj>xi)∩(Xi>xj)=P⋃p(≠i,j)=1n(Xp⩽xp)∪(Xj⩽xi)∪(Xi⩽xj)-P⋃p(≠i,j)=1n(Xp⩽xp)∪(Xj⩽xi-δ)∪(Xi⩽xj).Now, we recall the following equality for finite unions:(2.4)P⋃p=1nAp=∑p=1nP(Ap)-∑p<s=1nP(Ap∩As)+∑p<s<k=1nP(Ap∩As∩Ak)+⋯+(-1)n+1P⋂p=1nAp.Denoting by FI, with I={i1,…,ik}⊆{1,…,n}, the marginal distribution function ofXI={Xi1,…,Xik}. Then, from (2.3) and (2.4), we have(2.5)E[g(X1,…,Xi,…,Xj,…,Xn)]=Fj(xi)-Fj(xi-δ)+∑{i,j}≠p=1nFpj(xp,xi-δ)-Fpj(xp,xi)+Fij(xj,xi-δ)-Fij(xj,xi)-∑p<s=1,p≠s≠{i,j}nFpsj(xp,xs,xi-δ)-Fpsj(xp,xs,xi)-∑{i,j}≠p=1nFpij(xp,xj,xi-δ)-Fpij(xp,xj,xi)+⋯+(-1)n+1(F(x1,…,xj,…,xi,…,xn)-F(x1,…,xj,…,xi-δ,…,xn)).Noting that the joint survival function of a random vector (X1,…,Xn), can be expressed as follows(2.6)F¯(x1,…,xn)=1-∑p=1nFp(xp)+∑p<s=1nFps(xp,xs)-∑p<s<k=1nFpsk(xp,xs,xk)+⋯-(-1)n+1F(x1,…,xn),the result follows from (2.5) and (2.6).The proof is similar to that of (a).□Next we provide the following properties of the joint multivariate hazard rate order.Theorem 2.4Let us consider a random vector (X1,…,Xn), with absolutely continuous joint distribution function F. Let us consider the following statements:(a)X1⩽hr:j⋯⩽hr:jXn.∂∂xiF¯(x1,…,xi,…,xj,…,xn)⩽∂∂xiF¯(x1,…,xj,…,xi,…,xn),forallxi⩽xj.F¯(t,…,t︷i,…,t-x︷j,…,t)⩽F¯(t,…,t-x︷i,…,t︷j,…,t),forallx>0.Then, we have that (a)⇒(b)⇒(c).Let us see that (a)⇒(b). Let us consider the function g(u1,…,ui,…,uj,…,un) defined in Lemma 2.3. It is easy to verify that g(u1,…,ui,…,uj,…,un)∈Ghr. Hence, from the hypothesis and Lemma 2.3, we have that(2.7)F¯(x1,…,xj,…,xi-δ,…,xn)-F¯(x1,…,xj,…,xi,…,xn)⩽F¯(x1,…,xi-δ,…,xj,…,xn)-F¯(x1,…,xi,…,xj,…,xn).Therefore considering the ratio by any δ>0 and taking limδ→0 in (2.7), we have that∂∂xiF¯(x1,…,xi,…,xj,…,xn)⩽∂∂xiF¯(x1,…,xj,…,xi,…,xn).Let us prove now that (b)⇒(c). Given t and x>0 and integrating with respect to xi, we have thatF¯(t,…,t,…,t,…,t)-F¯(t,…,t-x,…,t,…,t)=∫t-xt∂∂xiF¯(t,…,xi,…,t,…,t)dxi⩽∫t-xt∂∂xiF¯(t,…,t,…,xi,…,t)dxi=F¯(t,…,t,…,t,…,t)-F¯(t,…,t,…,t-x,…,t),and therefore,F¯(t,…,t,…,t-z,…,t)⩽F¯(t,…,t-z,…,t,…,t).□An analogous result for the joint multivariate reversed hazard rate order can be proved. We state the result and the proof is omitted given that the proof is similar to the one in previous result.Theorem 2.5Let us consider a random vector (X1,…,Xn), with absolutely continuous joint distribution F. Let us consider the following statements:(a)X1⩽rh:j⋯⩽rh:jXn.∂∂xjF(x1,…,xj,…,xi,…,xn)⩽∂∂xjF(x1,…,xi,…,xj,…,xn),forallxi⩽xj.F(t,…,t,…,t−x,…,t)⩽F(t,…,t−x,…,t,…,t), for all x>0.Then, we have that (a)⇒(b)⇒(c).In this section we give some sufficient conditions under which the lifetime of series system, or parallel system or k-out-of-n system is stochastically maximized. We consider a set of n components with random lifetimes T1,T2,…,Tn, additionally we have a redundant component with random lifetime S. The random lifetimes considered previously can be possibly dependent. And by T1(x),T2(x),…,Tn(x) we will denote the components of the random vector (T1,T2,…,Tn∣S=x). We consider first the optimal allocation in a series system for an active and a standby redundant. Later we consider the problem for a parallel system with an standby redundant component. And to finish we consider the problem for a general k-out-of-n system with an active redundant component.Let us consider n+1 components as described previously. In this case the redundant component can be put in active redundancy with each one of the other components and in particular we can consider n systems,U1=min{max(T1,S),T2,…,Tn}U2=min{T1,max(T2,S),…,Tn}⋮Un=min{T1,T2,…,max(Tn,S)},depending on which component we perform the active redundancy.In the case of independent components Boland, El-Neweihi, and Proschan (1992) provide a more general result for the case of k-out-of-n systems (see Theorem 3.6). Next we describe sufficient conditions under which previous systems can be ordered in the stochastic and preference orders, and therefore to determine which one has a “larger” random lifetime.Theorem 3.1Let us consider n+1 components described as above. Then we have the following:(a)If T1(x)⩽st:j⋯⩽st:jTn(x) for all x in the support of S, thenUn⩽pr⋯⩽prU1.If T1(x)⩽lr:j⋯⩽lr:jTn(x) for all x in the support of S, thenUn⩽st⋯⩽stU1.(a) From the hypothesis and Theorem 2.2, we have(3.1)(T1,…,Ti,…,-Tj,…,Tn|S=x)⩽st(T1,…,Tj,…,-Ti,…,Tn|S=x),for all i<j and for all x in the support of S. For a fixed value x in the support of S, let us consider the real valued functionϕ(x1,…,xi,…,xj,…,xn)=min(x1,…,xi,…,xj-1,xj+1,…,xn,x)+xj,which is clearly increasing in (x1,…,xi,…,xj,…,xn).Now from Lemma 2.1a and (3.1), we have that, for all x in the support of S,(min(T1,…,Ti,…,Tj-1,Tj+1,…,Tn,x)-Tj|S=x)⩽st(min(T1,…,Ti-1,Tj,Ti+1,…,Tj-1,Tj+1,…,Tn,x)-Ti|S=x)and in particular,Pmin(T1,…,Ti,…,Tj-1,Tj+1,…,Tn,x)-Tj>0|S=x⩽Pmin(T1,…,Ti-1,Tj,Ti+1,…,Tj-1,Tj+1,…,Tn,x)-Ti>0|S=x.Therefore, denoting by FSthe distribution function of S, we have that(3.2)P[min(T1,…,Ti,…,Tj-1,Tj+1,…,Tn,S)>Tj]=∫Pmin(T1,…,Ti,…,Tj-1,Tj+1,…,Tn,x)-Tj>0|S=xdFS(x)⩽∫Pmin(T1,…,Ti-1,Tj,Ti+1,…,Tj-1,Tj+1,…,Tn,x)-Ti>0|S=xdFS(x)=P[min(T1,…,Ti-1,Tj,Ti+1,…,Tj-1,Tj+1,…,Tn,S)>Ti].Now, we recall the following equivalences among events provided by Romera et al. (2004):(3.3)min{T1,…,max(Ti,S),…,Tj,…,Tn}>min{T1,…,Ti,…,max(Tj,S),…,Tn}⇔Ti<min(T1,…,Ti-1,Ti+1,…,Tj,…,Tn,S)and(3.4)min{T1,…,Ti,…,max(Tj,S),…,Tn}>min{T1,…,max(Ti,S),…,Tj,…,Tn}⇔Tj<min(T1,…,Ti,…,Tj-1,Tj+1,…,Tn,S),From these two equivalences and (3.2), we haveP[Uj>Ui]⩽P[Ui>Uj],that is, Uj⩽prUi, for all i<j.(b) From the hypothesis, we have that for any x in the support of S, that(3.5)g(T1,…,Ti,…,Tj,…,Tn|S=x)⩽stg(T1,…,Tj,…,Ti,…,Tn|S=x),for all g∈Glr, and for all i<j.For a fixed value x in the support of S, let us consider the real functiongx(t1,…,ti,…,tj,…,tn)=min(t1,…,ti,…,max(tj,x),…,tn).It is easy to verify that gx(t1,…,ti,…,tj,…,tn)∈Glr, and therefore from Lemma 2.1a and (3.5), we have thatmin(T1,…,Ti,…,max(Tj,x),…,Tn|S=x)⩽stmin(T1,…,max(Ti,x),…,Tj,…,Tn|S=x),and from Lemma 2.1b, we conclude that Uj⩽stUi, for all i<j.□Let us consider now that the redundant component can be put in standby redundancy with each one of the other components. Now we can consider n systems:V1=min{T1+S,T2,…,Tn}V2=min{T1,T2+S,…,Tn}⋮Vn=min{T1,T2,…,Tn+S},depending upon which component we perform the redundancy.In the case of independent components, the following results are well known.Theorem 3.2Let T1,…,Tnand S independent random variables. Then, we have:(a)(Singh & Misra, 1994) If T1⩽st⋯⩽stTnthenVn⩽pr⋯⩽prV1.(Boland et al., 1992)T1⩽hr⋯⩽hrTn⇔Vn⩽st⋯⩽stV1.Next we provide similar results for the case of dependent components.Theorem 3.3Let us consider n+1 components described as above. Then we have:(a)If T1(x)⩽st:j⋯⩽st:jTn(x) for all x in the support of S, thenVn⩽pr⋯⩽prV1.If T1(x)⩽hr:j⋯⩽hr:jTn(x) for all x in the support of S, thenVn⩽st⋯⩽stV1.(a) From the hypothesis and Theorem 2.2, we have, for all x in the support of S and for all i<j, that(T1,…,Ti,…,-Tj,…,Tn|S=x)⩽st(T1,…,Tj,…,-Ti,…,Tn|S=x).Given a fixed value x in the support of S, let us consider now the functionϕ(x1,…,xi,…,xj,…,xn)=min(min(x1,…,xi,…,xj-1,xj+1,…,xn)+xj,x),which is increasing, then, as in the proof of Theorem 3.1a, we obtain that(3.6)P[min(T1,…,Ti,…,Tj-1,Tj+1,…,Tn)>Tj]⩽P[min(T1,…,Ti-1,Tj,Ti+1,…,Tj-1,Tj+1,…,Tn)>Ti].Now taking into account the following equivalences among events (see Singh & Misra, 1994):{min{T1,…,Ti+S,…,Tj,…,Tn}>min{T1,…,Ti,…,Tj+S,…,Tn}}⇔Ti<min(T1,…,Ti-1,Ti+1,…,Tj,…,Tn)andS>0,and{min{T1,…,Ti,…,Tj+S,…,Tn}>min{T1,…,Ti+S,…,Tj,…,Tn}}⇔Tj<min(T1,…,Ti,…,Tj-1,Tj+1,…,Tn)andS>0,the result now follows from (3.6).(b) For any x in the support of S, let us denote the joint survival function of (T1,…,Tn∣S=x) byF¯x(u1,…,un)and by FSthe distribution function of S. Then,P[Vi>t]=P[T1>t,…,Ti+S>t,…,Tj>t,…,Tn>t]=∫P[T1>t,…,Ti+x>t,…,Tj>t,…,Tn>t|S=x]dFS(x)=∫F¯z(t,…,t-x,…,t,…,t)dFS(x).In a similar way,P[Vj>t]=∫F¯z(t,…,t,…,t-x,…,t)dFS(x).Now from the hypothesis and Theorem 2.4c, we have thatF¯x(t,…,t,…,t-x,…,t)⩽F¯x(t,…,t-x,…,t,…,t),for any t and for any x>0. Therefore,P[Vj>t]=∫F¯x(t,…,t,…,t-x,…,t)dFS(x)⩽∫F¯x(t,…,t-x,…,t,…,t)dFS(x)=P[Vi>t],forallt,that is, Vj⩽stVi, for all i<j.□Let us observe that in the case of independent components, Theorem 3.3a is equivalent to Theorem 3.2a provided by Singh and Misra (1994) and Theorem 3.3b gives us the sufficient conditions of Theorem 3.2b provided by Boland et al. (1992).Let us consider now that the n components with random lifetimes T1,…,Tnperform a parallel system and let us assume that the additional component, with random lifetimes S, is put in standby redundancy with anyone of the n components. Again we have n systems:W1=max{T1+S,T2,…,Tn}W2=max{T1,T2+S,…,Tn}⋮Wn=max{T1,T2,…,Tn+S},and in this section we discuss sufficient conditions in order to compare stochastically these systems. Let us observe, that in this case, when the lifetime perform a parallel system, it makes no sense to consider the problem when the redundant component is put into parallel with anyone of the components, because in this case the resulting systems are the same.In the case of independent components, the following result is well known.Theorem 3.4Boland et al., 1992Let T1,…,Tnand S be independent random variables. ThenT1⩽rh⋯⩽rhTn⇔W1⩽st⋯⩽stWn.Now we can state the following result for dependent components.Theorem 3.5Let us consider n+1 components described as above. If T1(x)⩽rh:j⋯⩽rh:jTn(x) for all x in the support of S, thenW1⩽st⋯⩽stWn.For any x in the support of S, and following the notation in the proof of Theorem 3.3, we have thatP[Wi⩽t]=∫P[T1⩽t,…,Ti+x⩽t,…,Tj⩽t,…,Tn⩽t|S=x]dFS(x)=∫Fx(t,…,t-x,…,t,…,t)dFS(x).In a similar way,P[Wj⩽t]=∫Fx(t,…,t,…,t-x,…,t)dFS(x).Now from the hypothesis and according to Theorem 2.5c, we have that:P[Wj⩽t]=∫Fx(t,…,t,…,t-x,…,t)dFS(x)⩽∫Fx(t,…,t-x,…,t,…,t)dFS(x)=P[Wi⩽t],that is, Wi⩽stWj, for all i<j.□Note that, in contrast with series system, is more favorable to allocate to the strongest component, according to the joint multivariate order considered.In the case of independent components, the above theorem give us the sufficient conditions of Theorem 3.4 provided by Boland et al. (1992).In this section we extend some of the previous results to the case of k-out-of-n systems. Again let us consider n+1 components described as in previous sections, where the redundant component can be put into parallel with anyone of the remaining components, in this case we can consider n systems:M1=τk{max(T1,S),T2,…,Tn}M2=τk{T1,max(T2,S),…,Tn}⋮Mn=τk{T1,T2,…,max(Tn,S)}.In the case of independent components we can recall the following result.Theorem 3.6Boland et al., 1992Let T1,…,Tnand S be independent random variables. If T1⩽st⋯⩽stTnthen Mn⩽st⋯⩽stM1.Now we provide a result for the case of dependent components.Theorem 3.7Let us consider n+1 components described as in previous sections. If T1(x)⩽lr:j⋯⩽lr:jTn(x) for all x in the support of S, then Mn⩽st⋯⩽stM1.Let us consider i<j(∈{1,…,n}) and the random vector Z=(T1,…,Ti−1, Ti+1,…,Tj−1,Tj+1,…,Tn) and let us consider the following events:Zr,t>={AtleastrcomponentsofZaregreaterthant}andZr,t={ExactlyrcomponentsofZaregreaterthant}.It is not difficult to see that{Mi>t}=Zk,t>∪{Zk-1,t∩max{max{Ti,S},{Tj>t}}}∪{Zk-2,t∩min{max{Ti,S},{Tj>t}}},where the events in the union are disjoint. A similar result holds for the event {Mj>t}.Therefore(3.7)P[Mi>t]-P[Mj>t]=P[Zk-2,t∩min{max{Ti,S},{Tj>t}}]-PZk-2,t∩min{max{Tj,S},{Ti>t}}].Now we observe that we can writeP[Zk-2,t∩min{max{Ti,S},{Tj>t}}]=∑AP[min{Ts1,…,Tsk-2,max{Ti,S},Tj}>t],where the sum is extended over the set A of all subsets {s1,…,sk−2} of size k−2 of the set {1,…,i−1, i+1,…,j−1, j+1,…,n}, and a similar result holds for P[Zk−2, t∩{min{max{Tj, S}, Ti}>t}].Therefore we can writeP[Mi>t]-P[Mj>t]=∑AP[min{Ts1,…,Tsk-2,max{Ti,S},Tj}>t]-P[min{Ts1,…,Tsk-2,max{Tj,S},Ti}>t].Let us consider the random variablesUj(k)=min{Ts1,…,Ti,…,max(Tj,S),…,Tsk-2}andUi(k)=min{Ts1,…,max(Ti,S),…,Tj,…,Tsk-2}.From the hypothesis, the preservation of the AI property by marginalization and Theorem 3.1b, we have thatUj(k)⩽stUi(k),foralli<j. Therefore,P[min{Ts1,…,Tsk-2,max{Ti,S},Tj}]-P[min{Ts1,…,Tsk-2,max{Tj,S},Ti}]⩾0and therefore P[Mi>t]−P[Mj>t]⩾0, for all t. That is, Mj⩽stMi. □We observe that is more favorable to allocate to the weakest component, according to the joint multivariate likelihood ratio order.In the case of independent components, the condition considered in Theorem 3.7 is not equivalent to the condition considered in Theorem 3.6 provided by Boland et al. (1992). Therefore it would be interesting to see if the lr:j order condition in Theorem 3.7 can be replaced by the st:j order. We left this an open problem.In order to provide examples where previous results can be applied, we need the following result. The proof is similar to the proof of Theorem 2.3 by Belzunce, Ortega, Pellerey, and Ruiz (2007) and is omitted.Theorem 4.1Let X1,…,Xnbe n random variables with absolutely continuous joint distribution. If X1⩽lr:j⋯⩽lr:jXnthen(X1|Xi=x)⩽lr:j…⩽lr:j(Xi-1|Xi=x)⩽lr:j(Xi+1|Xi=x)⩽lr:j…⩽lr:j(Xn|Xi=x),for any i=1,…,n and any x in the support of Xi.Now if we consider a system of n components, with random lifetimes T1,…,Tn, and a redundant component with random lifetime S. Based on Definition 2.11 and Theorem 4.1, if the joint multivariate density of (S, Tn,…,T1) or (Tn, S, Tn−1,…,T1) or ⋯ or (Tn,…,T1, S) is arrangement increasing, then we have:T1(x)⩽lr:jT2(x)⩽lr:j…⩽lr:jTn(x),forallxinthesupportofS.Hence, by the relationships between the joint orders (see (2.2)), we have also:T1(x)⩽st:j,hr:j,rh:jT2(x)⩽st:j,hr:j,rh:j…⩽st:j,hr:j,rh:jTn(x)forallxinthesupportofS.So from previous observation we can provide examples of the sufficient conditions provided in Theorems 3.1, 3.3, 3.5 and 3.7. Next, we recall some multivariate parametric models with arrangement increasing densities.Example 4.2The following examples of arrangement increasing densities were provided by Hollander, Proschan, and Sethuraman (1977) and Belzunce et al. (2011).(1)Multivariate dirichlet distribution:f1(x1,…,xn)=Γθ+∑i=1nλiΓ(θ)∏i=1nΓ(λi)1-∑i=1nxiθ-1∏i=1nxiλi-1,wherexi⩾0;∑i=1nxi⩽1;θ>0;λi>0,λi⩽λj,∀i<j∈{1,…,n}.Multivariate inverted dirichlet distribution:f2(x1,…,xn)=Γ(θ+∑i=1nλi)Γ(θ)∏i=1nΓ(λi)∏i=1nxiλi-11+∑i=1nxiθ+∑i=1nλi,where xi⩾0; θ>0; λi>0, λi⩽λj, ∀i<j∈{1,…,n}.Multivariate F distribution:f3(x1,…,xn)=Γ(λ)∏i=0nλiλi∏i=0nxiλi-1∏i=0nΓ(λi)(λ0+∑λixi)λ,where xi⩾0;λ=∑i=0nλi; λi>0,λi⩽λj,∀ i<j∈{1,…,n}.Multivariate pareto distribution of type I:f4(x1,…,xn)=a(a+1)⋯(a+n-1)∏i=1n1θi∑i=1nθi-1xi-n+1-(a+n),where xi>θi>0,θi⩽θj,∀ i<j∈{1,…,n}; y a>0.Marshall–Olkin multivariate exponential distribution:f6(x1,…,xn)=∏i=1,i≠snλi(λs+λn+1)exp-∑i=1,i≠snλixi-(λs+λn+1)xs,where xs=max{x1,…,xn};xi,λi>0;λn+1>0,∑i=1n+1λi=λ,λj+λn+1⩽λi,∀i<j∈{1,…,n}.Moran–Downton multivariate exponential distribution:f7(x1,…,xn)=∏i=1nθi(1-ϱ)n-1exp-11-ϱ∑i=1nθixiSnϱ∏i=1nθixi(1-ϱ)n,whereSn=∑i=0∞zi(i!)n; xi>0; θj⩽θi, ∀i<j∈{1,…,n}; 0<ϱ<1.Recall that, from the comments provided in Section 2, previous examples satisfy that Xn⩽lr:jXn−1⩽lr:j⋯⩽lr:jX1.Next we provide an example where the AI property is not satisfied but the hr:j order is satisfied and Theorem 3.1a and Theorem 3.3 can be applied.Example 4.3Let us consider a random vector (X1, X2, X3) with joint density function given byf(x1,x2,x3)=6exp[-3x1-2(x2-x1)-(x3-x2)],for 0<x1<x2<x3 (this is a particular case of the multivariate Weinman distribution, see Kotz, Balakrishnan, & Johnson, 2000, pp. 388–391).If we consider the random vector (X2, X3∣X1=x1), the joint density function is given byf(x2,x3|x1)=2exp[-2(x2-x1)-(x3-x2)],for 0<x1<x2<x3.Is easy to see, from Definition 2.11, that −f(x2, x3∣x1) is not AI and therefore (X2∣X1=x1)≰lr:j(X3∣X1=x1), for any x1 in the support of X1.The joint survival function of (X2, X3∣X1=x1) is given byF¯(x2,x3|x1)=1ifx2⩽x1andx3⩽x1,exp[-2(x2-x1)]ifx1⩽x2andx3⩽x2,2exp[-(x3-x1)-2(x2-x1)]-exp[-2(x3-x1)]ifx1⩽x2andx2⩽x3,2exp[-(x3-x1)]-exp[-2(x3-x1)]ifx2⩽x1andx2⩽x3.In the bivariate case, conditions (a) and (b) in Theorem 2.4 are equivalent (see Shanthikumar & Yao, 1991). Therefore we will check the condition (X2∣X1=x1)⩽hr:j(X3∣X1=x1) through the partial derivatives ofF¯(x2,x3|x1). Let us consider that x2⩽x3 then∂∂x2F¯(x2,x3|x1)=0ifx2⩽x3⩽x1,0ifx2⩽x1⩽x3,-2exp-(x3-x1)-2(x2-x1)ifx1⩽x2⩽x3,and∂∂x2F¯(x3,x2|x1)=0.Therefore condition (b) in Theorem 2.4 is clearly satisfied.

@&#CONCLUSIONS@&#
In this paper we consider the problem of allocation of a redundant component for series, parallel and k-out-of-n-systems in the case of dependent components. We show that a basic tool to derive results in this context is the joint stochastic orders introduced by Shanthikumar and Yao (1991) for the bivariate case and extended here for the general case. The results are consistent with those obtained for the case of independent components.