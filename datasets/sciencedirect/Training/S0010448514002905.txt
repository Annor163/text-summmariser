@&#MAIN-TITLE@&#
An effective algorithm for constrained optimization based on optics inspired optimization (OIO)

@&#HIGHLIGHTS@&#
Introducing a new algorithm for constrained optimization inspired from Optics.The mechanism of algorithm is simple which allows its implementation easily.Investigating the application of the algorithm on mechanical engineering design.The algorithm is capable to find the global optimum of many investigated problems.The algorithm behaves constantly and performs more reliable than other algorithms.

@&#KEYPHRASES@&#
Optics,Optics inspired optimization,Constrained optimization,Engineering design optimization,

@&#ABSTRACT@&#
Due to the law of reflection, the converging and diverging behavior of concave and convex mirrors causes that curved mirrors show different image types. The optics inspired optimization (OIO) is a recently proposed algorithm for unconstrained optimization which treats the surface of the function to be optimized as a wavy mirror in which each peak is assumed to reflect as a convex mirror and each valley to reflect as a concave one. Each individual is treated as an artificial light point that its glittered ray is reflected back by the function surface, given that the surface is convex or concave, and the artificial image (a new solution) is formed based on mirror equations adopted from Optics. There are several constraint handling techniques which have been proposed for handling infeasible solutions. However, these techniques may suffer from problem dependency, no unique way for designing their operators, no unique way for updating their internal parameters, increasing the computational complexity, etc. To equip OIO with a mechanism to handle constraints and to avoid the drawbacks of typical techniques, a feasibility measure is used beside the objective function value to bias the search toward feasible regions. Such a consideration requires to modify several modules in the basic OIO algorithm. To increase the probability to generate better solutions, a number of alternative solutions are produced from each individual and one is selected based on the sequential use of modified Deb’s tournament selection. Besides, Deb’s tournament selection rule is used in place of the greedy selection in basic OIO, along with allowing the survival of individuals with a good value of the objective function, regardless of their feasibility. Performance of the proposed algorithm is compared with a number of noticeable algorithms such as COPSO, ECHT-EP2,αSimplex etc, on CEC 2006 and CEC 2010 set of benchmark problems and on a set of mechanical design optimization problems. Results demonstrate that the proposed algorithm performs the global optimization task very well and competitive. Such an outcome encourages that further developments and applications of OIO would be worth to realize its full potency in the future studies.

@&#INTRODUCTION@&#
Constrained optimization problems, especially nonlinear optimization problems, are of scientists’ great interests in recent decades. Structural optimization and engineering design are just a few fields in which constrained optimization problems are met. A general constrained optimization problem can be defined as follows:(1)minimizef(X→)gj(X→)≤0,j=1,…,qhj(X→)=0,j=q+1,…,mL→≤X→≤U→whereX→=[x1x2…xn]∈Rnis ann-dimensional decision vector,L→=[l1l2…ln]andU→=[u1u2…un]are the lower and upper bound vectors with conditionld≤xd≤ud,d=1,…,n.f(X→)is a numerical function to be minimized,gj(X→)≤0,j=1,…,q, areqinequality constraints andhj(X→)=0,j=q+1,…,m, arem−qequality constraints.The main challenge in constrained optimization is how to balance the search between feasible and infeasible regions effectively, i.e., to design an efficient constraint handling technique to locate the global optimum in the feasible region  [1]. Several trends for handling infeasible solutions have been emerged in the area of evolutionary computation. Among numerous methods that have been proposed for handling constraints, the most popular methods are the penalty function method, special representations and operators, repair methods, methods based on the separation of objectives and constraints, etc.  [2,3]. There are many algorithms that have used one of the mentioned constraint handling mechanisms (e.g., SAFF  [4], COPSO  [5], ISR  [6], ATMES  [7], SMES  [8], ECHT-EP2  [9], HCOEA  [10],αSimplex  [11], ECHT-DE  [12], ANT-β[13], ICA-4  [14], MPSO  [15], MBFOA  [16], DSS-MDE  [1], Sic-PSO  [17], RSPSO  [18], PSRE  [19], CPSO-DD  [20], IHS  [21], COPSO  [5], CPSO  [22], CDE  [23], DES  [24], FSA  [25],εPSO  [26],V(μ+1)-ES  [27], (μ+λ)-ES  [28], IPSO  [29], S&C  [30], SES  [31], Co-evolutionary penalty  [32]). Recently, Mezura and Coello  [33] have done an analysis of the most relevant types of constraint-handling techniques that have been adopted with nature-inspired algorithms.To tackle a certain difficult problem for which a common representation scheme might not be appropriate, a typical way would be to develop special representations and consequently design special operators to enforce feasibility of solutions at all time. However, this approach is problem dependent and remains applicable for problems in which locating the feasible solutions is extremely difficult  [3]. Repair methods are also applicable when it is relatively easy to repair an infeasible solution to obtain a feasible one (for example, many combinatorial optimization problems admit such repairs). The difficulty with this method is that there is no unique way for designing the intended repair operator  [3].The most common approach for handling constraints is the method of penalty function. The idea is to transform a constrained optimization problem into an unconstraint one by adding a certain value to the objective function value based on the amount of constraint violation presented in a solution  [3]. The constraint violation for thejth constraint in (1) is defined as follows:(2)cvj(X→)={max{0,gj(X→)},j=1,…,qmax{0,|hj(X→)−ε|},j=q+1,…,mwhere,εis a small positive tolerance value to convert an equality constraint into an inequality constraint. The formulation of the penalty function is as follows:(3)φ(X→)=f(X→)+∑j=1mrj×cvj(X→)whereφ(X→)is the penalized objective function andrjis a positive coefficient called “penalty factor”. As the infeasible solutions are penalized by the function in (3), the search ability in feasible regions will be enhanced. The most critical challenge with this method is how to set appropriate penalty weightsrj[3]. It is often required to reset the penalty factors for different kinds of optimization problems and this sounds a main drawback for such methods.The method of multi objective optimization is one of the widely used methods working based on the separation of objectives and constraints. In such methods, the objective functionf(X→)and the constraint violation measurescvj(X→)constitute a (m+1)-dimensional vector. Using some multi-objective optimization methods, the attempt is to minimize the components of the composite vector. Therefore, an ideal solutionX→∗would havecvj(X→∗)=0,j=1,…,m, andf(X→∗)≤f(Y→)for allY∈F[2]. One of the main drawbacks of such methods is that while the computational complexity increases, the problem difficulty is not decreased.There have been proposed several approaches that make the use of penalty functions unnecessary. Deb  [34] introduced a tournament selection operator for evolutionary algorithms, where two solutions are compared at a time and the following selection criteria are always enforced:•Between 2 feasible solutions, the one with better fitness value wins.If one solution is feasible and the other one is infeasible, the feasible solution wins.If both solutions are infeasible, the one with the lowest sum of constraint violations is preferred.In Deb’s approach, feasible solutions are always preferred to infeasible ones. Therefore, this approach will have difficulties in problems in which the global optimum lies on the boundary between the feasible and infeasible regions  [35]. To cover the lack of a diversity preserving mechanism in Deb’s approach, Mezura et al.  [36] used the Deb’s approach while allowing the survival of those individuals with a good value of the objective function, regardless of their feasibility. Through introducing a user-defined parameter calledSr, with probabilitySrthe selection between two solutions is only based on the objective function value and with probability1−Srthe selection is by using Deb’s selection criteria. Runarsson and Yao  [37] proposed a new constraint handling technique called stochastic ranking (SR). In SR, the balance between the objective and penalty functions is achieved through a ranking procedure based on the stochastic bubble-sort algorithm. Like method of Mezura et al.  [36], SR uses a comparison probability (Pf) for comparing the feasible and infeasible solutions which enables the user to specify an agreeable bias toward the objective function in ranking individuals in evolutionary algorithms. Takahama and Sakai proposed theαconstrained method  [38] andεconstrained method  [39], which adopt a lexicographic ordering with relaxation of the constraints. The authors called these methods algorithm transformation methods, because these methods convert an algorithm for unconstrained optimization into an algorithm for constrained optimization by replacing the ordinal comparisons with theαlevel and theεlevel comparisons.There are several deterministic algorithms which are efficient to solve numerical constrained optimization problems, e.g., the recursive quadratic programming, projection method and the generalized reduced gradient method  [40]. However, efficiency of these methods is under assumptions of differentiability and continuity of the objective function, which may be rarely met in real world applications. Beside deterministic algorithms, metaheuristics, e.g., evolution strategies, particle swarm optimization, ant colony optimization, differential evolution, etc., which are stochastic optimization techniques do not require any assumption on the objective function. However, such methods lack a mechanism to bias the search efficiently toward the feasible region in constrained search spaces. To cover this deficiency, considerable amount of researches have been devoted and a wide variety of approaches have been suggested in the last few years to handle constraints efficiently during the search  [3,9,36,37,39].There are several nature inspired algorithms which adopt their source of inspiration from Physics, e.g., Ray optimization  [41], Spiral Dynamics inspired optimization  [42], Central force optimization  [43], etc. It has been observed that concave surfaces/mirrors reflect the light rays toward the principal axis (a line passing through the center of the mirror and perpendicular to the mirror). Such a mirror causes light rays to converge (see Fig. 1(a)). On the other side, by a convex surface/mirror, light rays coming in parallel to the principal axis are reflected away from the principal axis so that they appear to be diverging (see Fig. 1(b)). Such an optical observations have been metaphorically modeled into the searching process of numerical unconstrained optimization by the so called optics inspired optimization (OIO) algorithm. OIO which have been introduced by Husseinzadeh Kashan  [45], treats the surface of the numerical function to be optimized as a reflecting surface wherein each peak is assumed to reflect as a convex mirror and each valley to reflect as a concave one. Each point in the joint search/solution and objective space (a subset inRn+1) which is mapped as a solution within the search space (a subset inRn) is assumed to be an artificial light point. In this way, the artificial ray glittered from an artificial light point is reflected back artificially by the function surface, given that the reflecting surface is a part of a peak or a part of a valley, and the artificial image point (a new point in the joint search and objective space which is mapped as a new solution in the search/solution space) is formed upright (toward the light point position in the search space) or inverted (away from the light point position in the search space). Such a model gives the ability to carry out both exploration and exploitation tasks during the search for optimum. In optics, spherical mirrors (concave or convex mirrors) suffer from spherical aberration phenomenon which results in an imperfection of the produced image. Such a phenomenon can also be observed artificially in OIO which may result in a less converging behavior of the algorithm.OIO has been originally proposed for unconstrained continuous optimization. To enforce OIO to search for the feasible global optimum of a constrained optimization problem, we need to equip it with a constraint handling technique and a number of auxiliary mechanisms. In this paper, the proposed OIO based algorithm for constrained optimization•uses the notion of Deb’s constraint-handling method with a diversity mechanism in a probabilistic manner during “generation of a new solution” step of OIO,is adapted in such a way that a feasibility based criterion is used beside the objective function criterion to decide indirectly upon getting acceleration toward better solutions or outward worse solutions. To include the feasibility based criterion into the searching process of OIO, it is required to completely modify two main steps of basic OIO, e.g., “mirror type determination” and “correction of spherical aberration”,generates a number of alternative solutions from each individual, among them, one is selected based on the sequential use of Deb’s tournament selection with a modification on one of selection rules. This increases the probability to generate better solutions,uses a truncated geometric distribution to set the number of changes in a given solution dynamically (to obtain a new solution) with more emphasis given to the smaller/larger rate of changes.In this section, we briefly review the Optics related materials which will be used metaphorically in OIO. For more details please see Husseinzadeh Kashan  [45]. Optics is a branch of physics which involves the behavior and properties of light, including its interactions with matter and the construction of instruments that use or detect it. A curved or spherical mirror is a mirror with a curved reflective surface, which may be either convex (bulging outward) or concave (bulging inward). The behavior of light reflected by a curved mirror is subject to the two laws of reflection.1.The incident ray, the reflected ray, and the normal all lie on the same plane.The angle between the incident ray and the normal is equal to the angle between the reflected ray and the normal.A concave mirror has a reflecting surface that bulges inward (away from the incident light). Concave mirrors reflect light inward to focal point. Concave mirrors show different image types depending on the distance between the object and the mirror. The image on a concave mirror is virtual, upright and larger than the object if it is placed between focal point and mirror. If the object is placed beyond the focal point, its image is always real and inverted. But the image size depends on the position of the object (see Fig. 2(a)).A convex mirror is a curved mirror in which the reflective surface bulges toward the light source. Convex mirrors reflect light outwards and always form a virtual, upright and smaller image than the object. The ray diagram in Fig. 2(b) represents how an image is formed by a spherical convex mirror.The spherical mirror model can be used to develop a simple equation for spherical mirrors. Letfbe the focal length,rbe the radius of curvature(r=2f),pbe the object position, andqbe the image position (see Fig. 3). The mirror equation is approximately given by [46]:(4)2r=1p+1q⇒q=rp2p−r.All distances are measured from the vertex; the point where the axis meets the mirror. Both ofr(orf) andqare negative for a convex mirror and onlyqis negative for a concave mirror just when the object lies between the vertex and the focal point.Magnification (m) is another property of a spherical mirror and is the ratio of the image height to the object height  [46].(5)m=−qp=HIHO⇒HI=−HOqp.A negative magnification means that the image is inverted relative to the object.It should be noted that Eq. (4) is approximately correct for a spherical mirror. To form an image, this equation uses only rays that are close to and almost parallel with the principal axis. In reality rays that are far from principal axis do not converge to a single point. The fact that a spherical mirror does not bring all parallel rays to a single point is known as spherical aberration (see Fig. 4).The extent of the ray divergence(κ)from the focus is called the lateral aberration and is given by  [47]:(6)κ=r22r2−HO2−r2.Spherical aberration can be minimized by using a mirror whose height is small compared to the radius of curvature. Eq. (6) predicts that when HO is kept constant and the radius of curvature gets larger, the lateral aberrationκdecreases. OIO uses such a mechanism for correcting artificial spherical aberration.The optics inspired optimization (OIO) algorithm is a population based evolutionary algorithm, which has been recently proposed by Husseinzadeh Kashan  [45]. OIO is suitable for unconstrained optimization with continuous variables. The idea behind OIO for search is to preserve both exploration and exploitation via imitating the image formation process by spherical mirrors. In OIO it is assumed that a number of artificial light points (points inRn+1whose mapping inRnare potential solutions to the problem) are sitting in front of an artificial wavy mirror reflecting their images. OIO treats the surface of the function to be optimized as the reflecting mirror composed of peaks and valleys. Each peak is treated as a convex reflective surface and each valley is treated as a concave reflective surface. In this way, the artificial ray glittered from the artificial light point is reflected back artificially by the function surface, given that the reflecting surface is a part of a peak or a part of a valley, and the artificial image point (a new point inRn+1which is mapped inRnas a new solution in the search domain) is formed upright (toward the light point position in the search space) or inverted (outward the light point position in the search space).Fig. 5demonstrates the idea behind OIO to generate new solutions in the one dimensional search space. In this figure it is assumed that an artificial light point (object) in the joint search and objective space is in front of the function surface (mirror) in a particular distance from vertex (values on theX-axis form the search/solution space and values on thef(X)-axis form the objective space. The set of all points in theX−f(X)-coordinate system forms the joint search and objective space). The artificial image is formed in the joint search and objective space, with its position and height determined through the equations of mirror (Eq. (4)) and magnification (Eq. (5)). Mapping the artificial image position into the search space results the position of the new candidate solution in the search space which may enter to the population after a greedy selection phase. Depending on the type of the reflecting part of the function surface (convex or concave) and depending on the position of the artificial light point (object) in the joint search and objective space, there are four different situations under which new solutions are generated (see Fig. 5). The idea behind OIO is thus simple. Given an individual solutionOin the population, a different solutionF(vertex point) is picked randomly from the population. IfFis worse thanO, in terms of function/objective value, then it is assumed that the surface is convex and the new solution is generated upright somewhere towardO, on the line connectingOandF(see Fig. 5(a)). IfFis better thanOthen it is assumed that the surface is concave and the new solution is generated upright toward (see Fig. 5(b)) or inverted outward (see Fig. 5(c) and (d))O, on the line connectingOandFin the search space. The above logic to generate new solutions is able to serve exploration and exploitation during the search for an optimum. Exploration may be controlled relatively via allowing a larger jump in the solution space (see Fig. 5(b) and (c)) while exploitation may be carried out by allowing a relatively smaller jump over the base solutions (see Fig. 5(a) and (d)).Letf(X→),X→=[x1x2…xn]1×n∈Rn, be annvariable scalar function that should be minimized over thendimensional decision spaceX→∈S⊆Rn, defined byld≤xd≤ud,d=1,…,n. The joint search and objective space is introduced by the set of all vectors[x1x2…xnf(X→)]1×(n+1)∈Rn+1. The following notations are used to develop OIO’s process. To have a visualized perception of these notations, the interest reader may refer to Figs. 6 and 7in  [45].–O→jt=[oj1toj2t…ojnt]1×nis the position of artificial light pointjin thendimensional search space in iterationt(i.e., thejth solution in the population),F→ikt=[fik1tfik2t…fiknt]1×nis an individual in the population, different fromO→jt, which passes the artificial principal axis through itself. Indexikis drawn randomly from{1,…,NO}. NO is the number of artificial light points (i.e., the population size).I→j,ikt=[ij,ik,1tij,ik,2t…ij,ik,nt]1×nis the image position of the artificial light pointjin the search space in iterationt. The artificial image is formed by the artificial mirror whose principal axis passes throughF→ikt.sj,iktis the position of the artificial light pointj(whose image is formed by the artificial mirror) on the function/objective axis (objective space) in iterationt. The position of artificial light pointjin the joint search and objective space is thus given by the vector[oj1toj2t…ojntsj,ikt]1×(n+1),pj,iktis the distance between the position of artificial light pointjon the function/objective axis and the position of artificial mirror vertex on the function/objective axis (i.e.,f(F→ikt)) in iterationt,qj,iktis the distance between the image position of the artificial light pointjon the function/objective axis and the position of artificial mirror vertex on the function/objective axis (i.e.,f(F→ikt)) in iterationt,riktis the radius of curvature of the artificial mirror whose center of curvature is on the principal axis which passes throughF→ikt,miktis the position of the center of curvature on the function/objective axis (objective space),HOj,iktis the height of the artificial light pointjfrom artificial principal axis in iterationt,HIj,iktis the image height of the artificial light pointjfrom artificial principal axis in iterationt,κj,iktis the value of lateral aberration relevant to the artificial mirror which is reflecting the image of the artificial light pointjin iterationt.The overall mechanism of OIO can be formulated as follows. At first, NO numbers of individuals are generated randomly to form the initial position of the artificial light points in the search space. Thereafter, in iterationt, each artificial light pointj(j=1,…,NO)with positionO→jt=[oj1toj2t…ojnt]in the search space sits (in the joint search and objective space in position[oj1toj2t…ojntsj,ikt]) in front of the artificial mirror (function surface) in distancepj,iktfrom the mirror vertex and its artificial image is formed in the joint search and objective space in distanceqj,ikt(on the function/objective axis) from the vertex, given that the principal axis passes through pointF→iktin the search space (F→iktis selected randomly from the current population with this condition thatf(F→ikt)must differ fromf(O→jt)) and the artificial mirror radius of curvature isrikt. Mapping the artificial image position into the solution space produces an artificial image positionI→j,iktin the search space which can be treated as a new solution to the problem.To develop OIO’s equation for generation of a new solution in the search space, consider the artificial light pointj,O→jt, and pick an individualF→iktfrom the current population such thatik∈{1,…,NO},ik≠jandf(F→ikt)≠f(O→jt). Iff(O→jt)>f(F→ikt)it is concluded that the function surface is concave and iff(O→jt)<f(F→ikt)it is concluded that the function surface is convex.Now, assume that the function surface in front of the artificial light pointjis concave. The positionsj,iktof the artificial light pointjon the function/objective axis (objective space) is somewhere betweenf(O→jt)and positive infinity (please see Fig. 6 in  [45]). That is:(7)sj,ikt=U[f(O→jt),f(O→jt)+d∞]where,U[a,b]is a random value distributed uniformly betweenaandb.d∞is assumed to be the physical infinity and can accept any positive value. Starting withd∞=|maxj=1,…,NO{f(O→j1)}|, we updated∞whenever we correct the artificial spherical aberration in OIO (see Section  3.1). Based on our definition ofpj,iktwe can write:(8)pj,ikt=sj,ikt−f(F→ikt).In a similar way the positionmiktof the center of curvature of the artificial concave mirror on the function/objective axis (objective space) is somewhere betweenf(O→jt)and positive infinity. That is:(9)mikt=U[f(O→jt),f(O→jt)+d∞].Based on our definition ofriktwe can write:(10)rikt=mikt−f(F→ikt).Now, assume that the function surface in front of the artificial light pointjis convex. The positionsj,iktof the artificial light pointjon the function/objective axis (objective space) is somewhere betweenf(F→ikt)and positive infinity (please see Fig. 7 in  [45]). That is:(11)sj,ikt=U[f(F→ikt),f(F→ikt)+d∞].Similar to the case of an artificial concave mirror, in case of an artificial convex mirror,pj,iktis obtained by Eq. (8). The positionmiktof the center of curvature of the artificial convex mirror on the function/objective axis (objective space) is somewhere between negative infinity andf(O→jt). Therefore:(12)mikt=U[f(O→jt)−d∞,f(O→jt)]where it is assumed thatf(O→jt)−d∞is the physical negative infinity. Just similar to the case of an artificial concave mirror, in case of a convex mirror,riktis obtained by Eq. (10). However, hereriktis negative. Once the values ofpj,iktandriktwere determined, the mirror equation can be used to obtainqj,iktas follows:(13)2rikt=1pj,ikt+1qj,ikt⇒qj,ikt=riktpj,ikt2pj,ikt−rikt.Depending on the type of the function surface (convex or concave) and the relative position of artificial light pointjon the function/objective axis to the position of the center of curvature,qj,iktcan be either positive or negative. Given the fact that:(14)HOj,ikt=‖O→jt−F→ikt‖based on the magnification equation (Eq. (5)), the image height of the artificial light pointjis obtained as follows:(15)HIj,ikt=−HOj,iktqj,iktpj,iktwhich can be treated as the step length to generate a new solution. We can now generate the image position of the artificial light pointjin the search space in iterationtas follows:(16)I→j,ikt=F→ikt+HIj,ikt(O→jt−F→ikt)‖O→jt−F→ikt‖=F→ikt−HOj,iktqj,iktpj,ikt(O→jt−F→ikt)‖O→jt−F→ikt‖=F→ikt−qj,iktpj,ikt(O→jt−F→ikt)=F→ikt−rikt2pj,ikt−rikt(O→jt−F→ikt).In this way,I→j,iktmay be entered as a new solution (search direction) in the population if it produces a better function value thanO→jt. However, to preserve more diversity in the population, it seems beneficial to use a consequent of multiple search directions instead of a single search direction generated by Eq. (16) in OIO. LetI→j,iktbe a typical search direction obtained by Eq. (16). Based on the notion of using multiple search directions to generate a new consequent image position (solution) we write:(17)I→jt=∑k=1KwktI→j,iktwhereI→jtis the weighted linear combination of different search directions and is treated as the consequent of different images of the artificial light pointj. The random weightswktare generated such that∑k=1Kwkt=1,0<wkt≤1. GenerallyK=1, 2, 3 would produce satisfactory results (for eachI→jtthe value ofKis set randomly equal to 2 or 3 in all of our experiments). Eq. (17) says that to generate a new solution in OIO, we may generate multiple artificial image pointsI→j,ikt,∀kand introduce their weighted mean as the new solutionI→jt.We use from Roulette wheel selection strategy to select the individual solutionF→iktfrom the current population. However, since the function to be optimized is a minimization problem, we have to convert the objective function values to fitness function values of maximization form. One way is to use the simplest procedure to get the fitness value relevant to solutionj(fit(O→jt))from its function value(f(O→jt))and the maximum function value in the population as follows:fit(O→jt)=maxi=1,…,NO{f(O→it)}−f(O→jt).As was already mentioned, spherical aberration occurs in a spherical mirror when the rays are far from the principal axis relative to the radius of curvature. Large values of lateral aberrationκindicates that the width (HO) of incoming ray is large compared to the radius of curvature (r).In analogy with what happens in optics, spherical aberration may occur artificially in OIO, which results in a weak convergence of algorithm. Correction of spherical aberration (or decreasing the lateral aberration) in OIO is related to modify the value ofrikt.In course of search for optimum by OIO, it is possible to repeatedly obtainHOj,ikt>|rikt|(sinceriktis negative for an artificial convex mirror, its absolute value is used), which means that the artificial light pointjis at the height that cannot be seen by the artificial mirror (and the lateral aberration amount is undefined). To be diagnosed in the mirror, the mirror absolute radius of curvature(|rikt|)must be greater thanHOj,ikt. Therefore, when computingriktin Eq. (10), if we getHOj,ikt>|rikt|, we must correctrikt. We correctriktvia increasing the length of the artificial mirror radius of curvature. Given an artificial light pointj, with position[oj1toj2t…ojntsj,ikt]in the joint search and objective space sitting in front of the function surface, we say aberration has occurred if the value of lateral aberrationκj,ikt(see Eq. (18)) be greater than a given threshold. The threshold value is a given constant and equal to 0.01. Using Eq. (6) as the basis to calculate the value of lateral aberration relevant to the artificial mirror which is reflecting the image of the artificial light pointjin iterationt, we can write:(18)κj,ikt=(rikt)22(rikt)2−(HOj,ikt)2−|rikt|2.If for an artificial light pointjwe come out toκj,ikt≥0.01, we correct the occurred aberration via increasing the length of the artificial mirror radius of curvature. To correct the occurred aberration, we repeatedly do the following steps until gettingκj,ikt<0.01. We first increase the absolute length of the physical infinity and setd∞←2d∞. Then, the new value of the radius of curvatureriktis calculated via Eq. (10), given thatmikt=mikt+d∞(if the artificial mirror is concave) ormikt=mikt−d∞(if the artificial mirror is convex). The value ofκj,iktis then computed based on the new value ofriktand the whole process is repeated until gettingκj,ikt<0.01. The final value ofriktis then used to calculateqj,iktvia Eq. (13).The entire flowchart of OIO for minimizing an unconstrained numerical function has been demonstrated in Fig. 6.The optics inspired optimization of Section  3 is only applicable for unconstrained optimization where there is no hard constraint except the boundary constraints. To adapt OIO to solve constrained optimization problems we need to adjust:1. The mirror type determination step of OIO (see Fig. 6) to include a feasibility based criterion beside the objective function criterion to decide upon the type of the artificial mirror. Such an adjustment may speed up steering of search toward feasible regions. To adapt the mirror type determination step of OIO we use from cv(X) criterion besidef(X)criterion. If bothO→jtandF→iktare feasible, we use fromf(X)criterion to decide upon the type of mirror; otherwise we use from cv(X) criterion. Fig. 7 revises the mirror type determination step in the adapted OIO algorithm for constrained optimization, whose idea is completely similar to that was used in Fig. 6. Accordingly, the spherical aberration correction step in the adapted OIO algorithm for constrained optimization is depicted in Fig. 8. In this figure we haved∞1=|maxj=1,…,NO{f(O→j1)}|andd∞2=max{10−4,maxj=1,…,NO{cv(O→j1)}}. We may updated∞1andd∞2whenever we correct the artificial spherical aberration.2. To prevent the population converges too early resulting in being suboptimal, and to heal the premature convergence, we can add a random perturbation term to the right side of Eq. (17) to probably improve the exploration ability of OIO for constrained optimization. The relevant equation is thus obtained as follows:(19)I→jt=∑k=1Kwk(I→j,ikt+rj×δjt×berjt×e→)where,e→=[1,1,…,1]1×nandδjtis a parameter which controls the perturbation magnitude and its value is set equal tomax(0,lnHOjt). As the population evolves, we expectδjtgets smaller and therefore the perturbation magnitude gets smaller.rjis a random number with value equal toU[−0.75,0.75].berjtis a Bernoulli random number which is equal to 1 with probability1−NO×t+j#eval, where#evalis the maximum number of function evaluations. As can be seen, the probability value decreases as the search proceeds, and this makes the effect of random perturbation less and less during the evolution process.By “POIO” we address to a version of OIO algorithm which uses Eq. (19) to generate a new solution (“OIO” itself uses Eq. (17) to generate a new solution). The perturbation term in Eq. (19) is employed just after POIO finds a solution in the feasible region.3. The way of updating the current population to attain a new population during the selection process (the Generation of a new solution step in the flowchart of Fig. 6). Once a complete solution was generated by Eq. (17) or (19), and before any evaluation being carried out on the quality of the solution, its feasibility is first checked with respect to the range constraints and values outside of the ranges are inserted back into their range randomly. To handle hard constraints, our constraint handling mechanism avoids the use of penalty function. Instead, we use the notion of Deb’s constraint-handling method  [34] in the selection process. Deb’s method uses a tournament selection operator, where two solutions are compared at a time based on three selection criteria described in Section  1. However in Deb’s approach, feasible solutions are always considered better than infeasible ones and therefore, this approach may have difficulties with problems in which the global optimum lies on the boundary between the feasible and infeasible regions  [35]. To remedy this deficiency, similar to the approach followed by Mezura et al.  [36], we try to preserve diversity by allowing the infeasible solutions having good value of the objective functions remain in the population. To select betweenI→jtandO→jt, we employ Deb’s constraint-handling method in a probabilistic manner (diversity mechanism). Based on the value of a parameter calledSr(selection ratio), the selection will be performed either based only on the value of the objective function, regardless of feasibility, or based on Deb’s criteria  [36]. Typically at the beginning of the search when the algorithm seeks for fruitful feasible areas, using larger values ofSrmakes more sense since it allows accepting infeasible solutions located in promising areas of the search space. However, as the population evolves and feasible areas are detected the weight given to the selection of a better infeasible solution with a great sum of constraint violations value should be decreased in comparison with the one with a smaller sum of constraint violations value. Therefore, using a decreasing updating strategy forSrseems to be reasonable. We use a decreasing strategy to update the value ofSras follows:(20)Srt+1=max{0,Sr0(1−(αt/tˆmax))}wheretˆmax=#evalNOis an estimation of the number of iterations passed by the algorithm, #eval is the maximum allowable number of objective function evaluations to solve a problem; NO is the size of population andαis a coefficient. Following Mezura et al.  [48] we useSr0=0.55as the initial value ofSr.Based on the strategy followed by Eq. (20) to updateSr, at the beginning of the search process, there is a significant chance for updating the current population only based on the value of the objective function, regardless of the feasibility. However as the time proceeds, this chance decreases and selection based on Deb’s criteria precedes.In order to increase the probability to generate better solutions, each individual(O→jt)in the population is allowed to generate a number of alternative solutions(I→jt)in each iteration i.e., multiple offspring are generated from a single solution  [36]. The number of generated solutions(nf)is a user defined parameter. Among thenfalternative solutions generated for each individual, we select only one of them (as the finalI→jtsolution) based on the sequential use of Deb’s tournament selection with a modification on the third condition (that is, when both solutions are infeasible) as follows:Ifboth solutions are infeasible thenIfU[0,1]≤SrtConduct a greedy selection between the two solutions based onf(X)criterion.Else ifConduct a greedy selection between the two solutions based on cv(X) criterion.End ifEnd ifThe idea behind the above selection strategy is that at the beginning of the search when the algorithm searches in infeasible areas, neither the sum of constraint violations value nor the objective function value precedes directly the other one. However, as the value ofSris decreased (i.e., when the above if-then-else statement terminates more likely at the else part) and the feasible regions are detected, the feasibility issue becomes more important. The above modification has a significant role in preventing the search to approach unfavorable feasible regions too early. As an evidence, the modification has a very significant effect on the rate of successfully hitting the global optimum of problem g13 of CEC 2006 test suite (see Section  5.1) by OIO.4. The feasible solutionI→jtgenerated by Eq. (17) or (19) differs withO→jtin all dimensions. However on many functions, due to the early convergence of algorithm to the local optima, it may not be a good choice to make changes in all dimensions ofO→jt. For some functions, even preserving a small amount of change inO→jtmay produce more satisfactory results. Letcjt​denote the number of changes made inO→jt.To simulate the number of changes(cjt), we use a truncated geometric distribution  [44,49,50]. Using a truncated geometric distribution, we can set the number of changes dynamically with more emphasis given to the smaller/larger rate of changes. The following formula simulates the random number of changes made inO→jtto get a new solution.(21)cjt=⌈ln(1−(1−(1−pc)n)r)ln(1−pc)⌉,cjt∈{1,2,…,n}wherer=U[0,1]andpc<1,pc≠0is a control parameter. The greater positive (negative) value ofpc, the smaller (greater) number of changes is recommended. Let us setU→jt←O→jt.cjtcomponents are selected randomly fromI→jtand their values are assigned to their relevant components inU→jt. Finally we conduct selection betweenU→jtandO→jt.

@&#CONCLUSIONS@&#
