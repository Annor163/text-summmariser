@&#MAIN-TITLE@&#
A new Reinforcement Learning-based Memetic Particle Swarm Optimizer

@&#HIGHLIGHTS@&#
A Reinforcement Learning-based Memetic Particle Swarm Optimizer (RLMPSO) is proposed.Each particle is subject to five possible operations under control of the RL algorithm.They are exploitation, convergence, high-jump, low-jump, and local fine-tuning.The operation is executed according to the action generated by the RL algorithm.The empirical results indicate that RLMPSO outperforms many other PSO-based models.

@&#KEYPHRASES@&#
Memetic algorithm,Particle Swarm Optimization,Reinforcement learning,Local search,

@&#ABSTRACT@&#
Developing an effective memetic algorithm that integrates the Particle Swarm Optimization (PSO) algorithm and a local search method is a difficult task. The challenging issues include when the local search method should be called, the frequency of calling the local search method, as well as which particle should undergo the local search operations. Motivated by this challenge, we introduce a new Reinforcement Learning-based Memetic Particle Swarm Optimization (RLMPSO) model. Each particle is subject to five operations under the control of the Reinforcement Learning (RL) algorithm, i.e. exploration, convergence, high-jump, low-jump, and fine-tuning. These operations are executed by the particle according to the action generated by the RL algorithm. The proposed RLMPSO model is evaluated using four uni-modal and multi-modal benchmark problems, six composite benchmark problems, five shifted and rotated benchmark problems, as well as two benchmark application problems. The experimental results show that RLMPSO is useful, and it outperforms a number of state-of-the-art PSO-based algorithms.

@&#INTRODUCTION@&#
Memetic-based optimization algorithms have been used successfully in many applications, e.g. DNA sequence compression [1], flow shop scheduling [2], multi-robot path planning [3], wireless sensor networks [4], finance applications [5], image segmentation [6], and radar applications [7]. The main objective of developing memetic-based algorithms is to exploit the benefits of both global and local search methods and combine them into a single model. As an example, the Particle Swarm Optimization (PSO) algorithm is an effective global optimizer, and has been integrated with different local search methods to produce a number of memetic PSO-based models [1,2,8–11]. The resulting models combine the global search strength of PSO and the refinement capability of local search methods into a unified framework.In the literature, many successful applications of memetic PSO-based models have been reported. In [1], a memetic model integrating PSO and an Intelligent Single Particle Optimizer (ISPO) [12] to solve the DNA sequence compression problem was presented. In [11], an adaptive memetic algorithm with PSO was developed and applied to the Latin hypercube design problem. Specifically, the standard PSO algorithm was adopted to perform the global search operations. It was integrated with a Lamarckian algorithm to perform the refinement operations. A hybrid model of PSO and a pattern-based local search method was studied in [10]. The resulting model was useful for parameter tuning of the Support Vector Machine (SVM). On the other hand, some studies indicate that PSO can be used for performing the local search operations in memetic models [5,13,14]. In [5], a hybrid model of PSO and genetic algorithm was introduced, whereby the PSO algorithm acted as a local search method. A hybrid shuffled frog-leaping algorithm and modified quantum-based PSO local search method was described in [13]. Recently, a hybrid model combining the differential evaluation algorithm and PSO was introduced. Again, PSO functioned as a local search method [14].There are a lot of challenges in developing an effective memetic-based PSO model. The key challenges include when the local search method should be called, the frequency of calling the local search method, and which particle should undergo the local search operations. Indeed, the findings in [1] indicate that efficient management of the local search method in terms of time and frequency of calling has a significant impact on the performance. Besides these challenges, the standard PSO algorithm also suffers from several weaknesses, primarily the premature convergence and high computational cost problems. The first weakness is related to its fast premature convergence condition [15,16]. As pointed in [15,16], PSO can be trapped quickly in local optima at the beginning of the search process. The second limitation of PSO comes from its high computational cost. While a large particle population size gives a better swarm diversity capability, the computational cost becomes intensive too, e.g. each particle needs to undergo the fitness evaluation in every search cycle. This limitation of PSO has been reported in [17,18].To mitigate the aforementioned problems, this study introduces a new reinforcement learning-based memetic PSO (RLMPSO) model. RL has been employed with standard PSO and other evolutionary algorithms [3,19]. An integration of RL and PSO was proposed by Piperagkas et al. [19]. Another recent study [3] employed RL for parameter tuning a differential evolution algorithm. On the other hand, RL worked independently from PSO in [6], whereby it was adopted to enhance the estimation of the objective function in noisy problems.Comparing with the existing work in the literature, this study differs in the aspect that RL is embedded in RLMPSO to control the operation of each particle during the search process. Each particle, under the control of RL, performs one of the five possible operations [20], i.e. exploration, convergence, high-jump, low-jump, and fine-tuning. Moreover, each operation is given a reward or penalty according to its achievement. The proposed RMLPSO model has the following advantages:(1)RLMPSO works with a small population size (typically 3 particles). It utilizes the ISPO (i.e. Intelligent Single Particle Optimizer) algorithm [12]. Additionally, it is enhanced with a total of five operations, i.e. exploration, convergence, high-jump, low-jump, and fine-tuning.The RL algorithm is embedded into RLMPSO to control the operation of each individual particle in the swarm. Specifically, RL adaptively switches the particle from one operation to another in accordance with the particle's achievement. Positive rewards are given to particles that have performed well, while penalties are imposed to non-performing particles.Each particle in RLMPSO evolves independently, e.g. one particle executes exploration, while others perform their respective operations.To minimize the computational cost of fine-tuning, two parameters are introduced i.e. delay (D) and cost (C). The delay parameter prevents fine-tuning (i.e., for local search) to be initiated at the beginning of the search process. The cost parameter controls the duration between each consecutive call of the fine-tuning operation.Similar to RL, the idea of selecting the best performing operators from a set of alternatives has been comprehensively studied in the literature [21–24]. As an example, four PSO velocity updating strategies were used in [21]. A probability execution variable was assigned for each strategy, and the best operation was given a higher probability of selection. An evolutionary-based optimization algorithm with an ensemble of mutation operators was introduced in [22]. Each individual in the population would select a mutation strategy according to a probability distribution. Improved results were achieved with the ensemble strategy as compared with the single mutation strategy [25].Differential Evolution (DE)-based methods with ensemble strategies were studied in [23,24,26]. In [23], an evolving DE model with an ensemble mutation strategy was presented. During the search process, DE randomly selected a mutation strategy with a random set of parameters to generate a new offspring. If the produced vector was better than the parent, the strategy would be retained; otherwise a new random mutation strategy with a new set of parameters would be generated [23]. The multi-objective DE algorithm with a pool of Neighbourhood Size (NS) parameter was presented in [24]. In particular, DE was developed using k NS candidates. The best NS value was adaptively selected from k candidates according to their historical performances. Improvements were achieved using k NS candidates as compared with only one candidate. Another DE-based model with an ensemble mutation strategy was presented in [26]. In particular, the population was randomly divided into three small sub-populations and one large sub-population. The three small sub-populations were executed for a specific number of Fitness Evaluations (FEs). Each sub-population was executed with a different mutation strategy, i.e. “current-to-pbest/1” and “current-to-rand/1”, and “rand/1” [26]. A reward was computed as the ratio of fitness improvement to the total number of fitness calls consumed by each sub-population. After that, the large sub-population was executed with the setting of the best performing small sub-population. This process was repeated until the maximum number of FEs is met. In this case, the best mutation strategy could be selected dynamically during run time. The proposed model was able to outperform other DE variants.The rest of this paper is organized as follows. In Section 2, an overview of PSO and its variants is given. The proposed RLMPSO model is explained in Section 3. In Section 4, a series of experiments to evaluate the effectiveness of RLMPSO using benchmark optimization problems is described. A summary of the research findings is presented in Section 5.PSO was introduced by Kennedy and Eberhart about two decades ago [27]. The motivation of PSO is to mimic social interaction and search behaviours of animals, such as bird flocking and fish schooling. In general, PSO is represented by a swarm of N particles. Each particle in the swarm is associated with two vectors, i.e., the velocity (V) and position (X) vectors, as follows:(1)Xi=di1,di2,di3,…,xiD(2)Vi=vi1,vi2,vi3,…,viDwhere D represents the dimension of the optimization problem and i denotes the particle number in the swarm. During the search process, the velocity and position vectors are updated as follows:(3)Vi+1=w∗Vi+c1∗randuniform(pBest−Xi)+c2∗randuniform(gBest−Xi)(4)Xi+1=Xi+Vi+1where w is the inertia weight, c1 is the cognitive acceleration coefficient, c2 is the social acceleration coefficient, randuniformis a uniformly distributed random number within0,1, pBest is the local best position achieved by a particular particle, and gBest is the global best position achieved by the whole swarm.As can be seen in Eq. (3), each particle's movement is affected by three components, namely its particle velocity (Vi), the distance from its local best (pBest−Xi), and the distance from the global best (gBest−Xi) in the swarm. Therefore, to control each component in Eq. (3), three parameters are used, i.e., w, c1, and c2. The suggested range of the inertia weight isw∈[0.4,0.9][27]. It has been pointed out that w must be high in the exploration stage and low in the convergence stage [20]. On the other hand, the settings of c1 and c2 need to strike a balance between pBest and gBest. As suggested in [20,21], c1 must be higher than c2 in the exploration stage, and the opposite in the convergence stage.Since the introduction of the original PSO algorithm, many PSO variants have been put forward to improve its performance [1,2,4–11,13,14,17,18,28–48]. The main PSO-based algorithms available in the literature can be divided into five categories i.e. modified-based, hybrid-based, cooperative-based, micro-based, and memetic-based algorithms. A discussion of each category is presented, as follows.The main aim of this category is to enhance the performance of PSO by controlling its parameters [20,45,48], balancing between the exploitation and exploration operations [46,49], or modifying the swarm topology connectivity [44,47,50–53]. An adaptive PSO algorithm was introduced in [20]. The aim was to improve the search performance by efficiently controlling its parameters, i.e. the cognitive acceleration parameter, social acceleration parameter, and inertia weight parameter. These parameters were adaptively tuned by using fuzzy rules [20]. However, the method relied on the distribution of the swarm particles at run time, which was not suitable for PSO with small populations. The PSO variant proposed in [45] assigned independent parameters for each particle in the swarm. Specifically, each particle was given its own parameters (i.e. cognitive acceleration, social acceleration, and inertia weight), and they were tuned adaptively according to the behaviour of the particle during the search process [45]. However, managing these parameters independently would increase the complexity of PSO. Inspired by control theory, a new strategy for controlling the PSO parameters was suggested in [48]. The strategy adopted the concept of the peak time and overshoot in its search process. Nevertheless, the strategy worked with a large population size (i.e. 250 particles); therefore increasing the computational cost owing to fitness evaluation for each particle during each search cycle.Other researchers attempted to improve PSO by balancing exploitation and exploration operations at run time [46,49]. According to [54], exploration was concerned with spreading the swarm particles to visit the whole search space of the optimization problem, while exploitation was concerned with searching around those visited regions found during the exploration process [54]. The idea of evolving two concurrent swarms as a master–slave model was presented in [46]. The master particles were responsible for exploration while the slave particles performed exploitation. Again, evolving two swarms simultaneously increased the complexity of the proposed model [46]. An intelligent scheme which divided the whole swarm into two groups i.e. exploration and exploitation, was proposed in [49]. Two metrics were developed to split the population, i.e. population spatial diversity and population fitness spatial diversity. Besides its model complexity, the reported results [48] were not competitive on difficult, complex benchmark problems as compared with those from other PSO-based models.Methods to modify the swarm topology were examined in [44,47,50–53]. The swarm topology is related to the information link between each particle and its neighbour. These links produce different types of topologies, such as fully connected topology [52], ring topology [47], and wheels topology [50]. The dynamics of these topologies can be either static or dynamic. In the former, the topology is fixed during the search process. The latter has dynamically changing topologies at run-time. The main advantage of a dynamic topology over a static one is its ability to prevent the swarm from the premature convergence problem [51] at the beginning of the search process. However, it increases the computational cost required to manage the swarm topology during the search process. In [51], a dynamic topology was proposed. The connection between particles started with one particle, i.e. each particle was connected to another randomly selected particle in the swarm. Then, the connection was linearly increased with time until it reached a fully connected topology, i.e. all particles in the swarm were connected together so that learning from the global best particle in the swarm could take place [51].One of the drawbacks of modified PSO-based algorithms is the lack of optimization refinement capabilities. In other words, these methods cannot fine-tune the individual dimension of the particles independently without affecting other dimensions.Enhancing the effectiveness of PSO by hybridization with other meta-heuristic optimization algorithms has been studied in the literature [34–39,55–58]. PSO has been used to form hybrid models with Ant Colony Optimization (ACO) [37,55,58], GA [34,35,56], Artificial Bee Colony (ABC) [36], and Harmony Search (HS) [38,39]. The main aim of hybrid PSO-based algorithms is to combine the strengths of the constituents into one integrated model. An integration of PSO and HS was proposed in [39]. The PSO swarm was divided into several dynamic multi-swarm particle optimizers, and each sub-swarm was managed by HS. The sub-swarms communicated with one another and exchanged their knowledge after a pre-defined number of FEs. Nevertheless, the model [39] worked with a large population size (i.e. 10 sub-swarms), and required the FEs operation for each search iteration. Another hybrid model of PSO and HS was proposed in [38]. The pitch adjustment operation in HS was replaced with the particle velocity addition operation. The resulting model was applied to dynamic load dispatch optimization problems. As PSO and HS both performed global search [38], the search refinement capabilities were inadequate.Hybrid PSO and ACO models were studied in [37,55,58]. The hybrid model in [58] comprised two sequential phases, i.e. the ant colony phase and the PSO phase. In addition, a global best exchange operation was added after each search cycle. The proposed model [58] was computational expensive owing to the execution of two swarms simultaneously. Another hybrid PSO and ACO model was developed in [55]. The model [55] comprised four hybridization strategies, i.e., sequential, parallel, sequential with an enlarged table, and a global best exchange strategy. It was proposed to tackle data clustering problems, and the results [55] showed that the hybrid PSO-ACO model outperformed its constituents (i.e. PSO and ACO).The main challenge of hybrid-based PSO algorithms is threefold: (i) simultaneous managing multiple swarms and exchanging information between them; (ii) the computational cost of the FEs operation for each swarm; (iii) hybrid-based PSO models mainly comprise global search methods [59], and they lack the capability of performing search refinement.Unlike the aforementioned categories where all dimensions of the particle are evolved together, cooperative PSO-based algorithms split the optimization process into several sub-problems. This strategy was examined in [28–33,43], in which the task was split into K sub-problems for simultaneous optimization before combining the results.A cooperative PSO-based algorithm with application to large-scale optimization problems was proposed in [32]. A new position update scheme was introduced to identify the sub-component size in an optimal manner. Another cooperative PSO-based algorithm was developed in [28] to tackle FPGA (Field Programmable Gate Array) placement problems. The placement task was divided into two sub-problems: logic blocks and I/O (Input/Output) blocks. A cooperative-based algorithm was proposed in [30], in which a new statistical strategy was used to decompose the optimization problem. The aim was to estimate the degree of inter-dependencies pertaining to the optimization variables, and then to include the dependent variables in the same sub-problem [30].In [43], where the original micro PSO algorithm [17] was used with a new master–slave model. At the beginning of the search process, the problem was decomposed into several sub-problems with small dimensions. Then, each sub-problem was solved by an independent micro PSO population [17], and all sub-populations were executed in parallel. As such, the proposed model [43] could be classified as a cooperative PSO-based model. The results in [43] showed its effectiveness in tackling high dimension problems as compared with those from other PSO variants.One of the key challenges of cooperative-based PSO algorithms is identifying the best sub-problem size and finding the independent variables to be placed in different sub-problems.To minimize the computational cost of PSO with large population sizes, i.e. the cost associated with the FEs operation of each particle during the search process, micro PSO-based algorithms have been introduced [17,18,41–43]. A micro PSO algorithm was introduced in [17] for tackling high dimension optimization problems. The results [17] showed the capability of the developed optimizer to achieve competitive performance as compared with those from the standard PSO algorithm. On the other hand, the application of micro PSO to image reconstruction problems was investigated in [18]. The limitations of the micro PSO algorithm in [17,18] included prevention from exploitation of possible promising search regions due to the repelling force, as well as the high computational cost of re-starting the whole micro swarm during the search process.The application of a micro PSO algorithm to real-world design problems was discussed in [41,42]. PSO with a small population size was developed for tuning the parameters of power system stabilizers [42]. A re-generation scheme was used to improve diversity of the micro PSO swarm, where the position and velocity vectors were randomized after a pre-defined number of FEs. The reported results [42] revealed the usefulness of the re-generation strategy in enhancing the diversity property.In summary, the main advantage of micro PSO-based algorithms is the ability to overcome the high computational cost per each particle in a standard PSO algorithm. However, it lacks population diversity due to the small number of particles employed. It also suffers from the problem of premature convergence, i.e., the population converges rapidly towards the global best particle at the early stage of the search process [17].As the PSO algorithm is generally a global search optimizer [1], it lacks the capability of refining its local search space. Therefore, a number of investigations to integrate PSO with local search methods to produce memetic-based PSO models have been studied [1,2,4,5,7–11,13,14,60]. A memetic model of PSO and two local search methods, i.e. cognition-based search and random walk, was introduced in [9]. The local search methods were able to enhance the performance of the standard PSO algorithm. Another memetic-based PSO model was studied in [10]. In particular, a probabilistic selection scheme to determine which particle should undergo local search was developed [10]. Then, those particles with better fitness were given a higher probability to be fine-tuned [10]. A hierarchical-based memetic model was developed in [8]. The model contained two (top and bottom) layers. The bottom layer comprised M swarms while the top layer comprised one swarm with M best particles from the bottom layer. In addition, the search process consecutively switched from the bottom layer to the top layer. A Latin hypercube sampling optimizer was embedded for the fine-tuning operation, which was triggered every ten search generations [8]. Nevertheless, managing local search methods in terms of when to call the local search method, as well as the frequency of calling posed as a difficult problem in memetic-based PSO models.Another memetic-based PSO algorithm was reported in [13]. An improved Quantum-based PSO optimizer was developed to act as a global optimizer, while the shuffled complex evolution technique was adopted for local search operations. The numerical results [13] indicated that the local search method was able to improve the PSO performance as compared with that from the standard PSO algorithm. A recent study of a memetic-based PSO algorithm with application to a large-scale Latin hypercube design problem was presented in [11]. Specifically, PSO was integrated with multiple local search methods to tackle the hypercube design problem. However, incorporating multiple local search methods increased FEs in each local refinement operation [11].The synergy of PSO with local search methods was discussed in [60]. In particular, an enhanced PSO algorithm was integrated with gradient-based and derivative-free local search methods. The gradient-based methods were used for numerical optimization problems, while the derivative-free local search methods were adopted for real-world problems. The reported results revealed that the employed local search methods were able to improve the search performance of PSO [61].Real-world applications of memetic-based PSO algorithms were reported in [1,2,4,5,7]. A memetic-based PSO model for undertaking flow shop scheduling problems was described in [2]. Several local search methods were developed, e.g. nawaz-enscore and simulated-annealing, and they were embedded in the memetic-based PSO model. A memetic PSO optimizer for tackling DNA sequence compression problems was presented in [1]. The search space was clustered into several regions for facilitating the local search operations. The frequency of calling the local search method was suggested to be low [1]. For financial applications, an integrated model of GA and PSO was developed in [5]. GA and PSO were used as a global optimizer and a local search method, respectively. The application of a memetic-based PSO algorithm to wireless sensor networks was described in [4]. Operating as a global search optimizer, PSO was integrated with an active-set local search method [4]. The proposed model [4] was used to maximize the quality of transmitted video streams by visual sensors. A recent study of a memetic-based PSO optimizer for radar applications was presented in [7]. The combination of PSO as a global search optimizer and a gradient-free local search method was proposed [7]. A memetic-based PSO optimizer for SVM parameter tuning was examined in [10]. In particular, the standard PSO optimizer was integrated with a pattern-based local search method for refinement operations. The results showed the effectiveness of the memetic-based model in tuning SVM parameters, as compared with those from the standard PSO as well as other optimizers reported in [10].The proposed RLMSPO model integrates RL into the memetic PSO operations. The detailed explanations are as follows.RL [62] stems from research in machine learning and artificial intelligence. It has been widely studied in game theory [63,64]. The main components of RL include a learning agent, an environment, states, actions, and rewards. To implement RL in this study, the Q-learning algorithm [65] is adopted.LetS=S1,S2,…,Snbe a set of states of the learning agent,A=a1,a2,…,anbe a set of actions that the learning agent can execute, rt+1 be the immediate reward acquired from executing action a, γ be the discount factor within [0,1], α be the learning rate within [0,1], Q(st, at) be the total cumulative reward that the learning agent has gained at time t, and it is computed as follows:(5)Qt+1(st,at)=Q(st,at)+α[rt+1+γmaxaQ(st+1,a)−Q(st,at)]A numerical example is presented to clarify Eq. (5), as follows. Assuming a learning agent with sthas to perform one of the four possible actions, i.e. move up, move down, move left, or move right, as shown in Fig. 1. After executing the “move right” action with a reward of 1 (i.e., r=1), the next state is st+1, as shown in Fig. 1(b).Assume that the parameter settings are as follows: the pervious value stored in the Q-table for Q(st, at) is 10, i.e. Q(st, at)=10; the discount factor is 0.1, i.e. γ=0.1; and the learning rate parameter is 0.9, i.e., α=0.9. Then, the new value in the Q-table updated toQt+1(st,at)=10+0.9∗[1+0.1∗max(20,30,100,90)−10]=10.9Then, update the state st→st+1The search steps of the Q-learning algorithm are illustrated in Algorithm 1. One of the main characteristics of Q-learning is how the learning rate (i.e., α) determines the extent of which the newly learned information overrides the existing, old information. As an example, when α is close to 1, this means that a higher priority is given to the newly gained information, and Q-learning performs more exploration for all defined states. On the other hand, a small α value gives a higher priority for the existing information in the Q-table to be exploited. This puts Q-learning in the exploitation mode. For this reason, α normally is set to a high value at the beginning of the search process, and is decreased at each time step, in order to switch to the exploitation mode, as follows [3]:(6)α(t)=1−0.9∗tMaxFEswhere, MaxFEs is the maximum number of FEs. The discount factor γ is responsible for penalizing the future reward. When γ=0, Q-learning considers the current reward only. When γ=1, Q-learning looks for a higher, long-term reward. It is suggested to set γ=0.8 [3].Fig. 2shows the overall RLMPSO structure that integrates RL and PSO. The PSO particles acts as the RL agents. The environment is characterized by the search space of the particles. The states represent the current operation of each particle, i.e., exploration, convergence, high-jump, low-jump, or fine-tuning. The action is defined as it changes from one state to another. As can be seen in Fig. 2, RL controls the operation of each particle in the PSO swarm. Specifically, RL adaptively switches the particle from one operation (state) to another according to the particle's achievement. Positive rewards are given to particles that have performed well, while penalties (negative rewards) are given to non-performing particles.In a standard PSO algorithm, the exploration operation is initiated at the beginning for the whole swarm particles. Then, the operation gradually switches to the convergence state towards the end of the search process. In RLMPSO, the choice of the most suitable search operation for each particle is selected adaptively using RL. Algorithm 2 illustrates the proposed RLMPSO search procedure. The procedure is repeated until the maximum number of FEs is met.The main interaction between Q-learning and the five possible search operations can be summarized in three steps, as follows:(i)Obtain the best operation to be executed based on the Q-table value for the current particle.Execute the selected operation and compute the fitness function. The immediate reward is computed, i.e.,(7)r=1iffitnessisimproved−1otherwiseUpdate the Q-table for the current particle using Eq. (5).In Algorithm 2, after calling the fine-tuning operation, a cost (i.e. a negative reward) is given to penalize the execution of this operation, in order to give a higher priority for other operations to be executed (as further clarified in Section 3.7).The Q-table is shown in Fig. 3. It is an M×M matrix, where M is the number of states. In RLMPSO, each particle has its own Q-table. Therefore, to minimize the computational cost of managing the Q-tables, a micro PSO model with a small population size (i.e., 3-particles) has been used throughout this research.To delay the execution of the fine-tuning operation (F) at the beginning of the search process and to give a higher priority for other operations to be executed, the initial Q-table entry for state F is set to a negative value, as indicated in Fig. 3. In addition, RLMPSO has to be executed N times (i.e., a minimum lapse of N FEs is required) before RL activates fine-tuning. Finally, the maximum negative value is considered as the initial value of F, as shown in Fig. 4.During the execution of RLMPSO, the best action for the current state is retrieved from the Q-table, as follows:(8)bestaction=Max[Q(currentstate,allactions)])A numerical example of the Q-table entries for Particle 1 is shown in Fig. 5. Assuming that the current state of Particle 1 is exploration (E). When Eq. (8) is applied, the next state is C, as indicated in Fig. 6.To update the content of the Q-table, Eqs. (5) and (6) are used. The new content of the Q-table is shown in Fig. 6. As can be seen, after executing the exploration (E) operation, Particle 1 receives a penalty because it cannot improve the search process.In PSO, there are four possible boundary conditions, i.e. reflecting wall, damping wall, invisible wall, and absorbing wall, as shown in Fig. 7. The details are as follows:(i)Reflecting wall: When a particle exceeds the limit of the search space in any dimension Xi, the sign of its velocity (i.e., Vi) is changed, and Xiis reflected back to the search space.Damping wall: This case is similar to the reflecting wall except that the particle is reflected with a small random value.Invisible wall: The particle is allowed to jump out of the pre-defined search space; however, the fitness function is not computed.Absorbing wall: When the particle exceeds the limit of the search space in any dimension Xi, its velocity Viis set to zero, and Xiis set to the boundary limit.In this research, the damping wall, which has been used in a number of PSO variants [18,66], is adopted.The exploration and convergence operations are normally executed at the beginning and towards the end of the search process, respectively. However, some studies recommend switching adaptively at any time from exploration to convergence, and vice versa. Motivated by the findings in [20], RLMPSO can execute any state, i.e. exploration, convergence, high-jump, low-jump, and fine-tuning, at any time during the search process. RL is responsible to keep track of the best executed operation pertaining to each particle.As stated earlier, particle Ximoves in the search space guided by the global best particle, gBest, its current velocity, Viand the local best particle, pBesti, as indicated in Eq. (3). Parameters w, c1 and c2 control the direction and movement of particlei, as shown in Figs. 8 and 9. Therefore, in the exploration state, w should be high to allow the particle to make a large movement to explore the search space. Moreover, as stated in [20], c1 should be higher than c2 in the exploration mode, in order to move the particle far away from the global best particle, as shown in Fig. 8.The convergence operation is similar to the exploration operation, except that all particles converge slowly towards the global best particle, gBest. Therefore, w should be low, in order to prevent particle Xifrom oscillating around the gBest location. Moreover, the settings of c1 and c2 should be the opposite of those in the exploration mode. In this study, c1=0.5 and c2=2.5, as used in [67]. Fig. 9 shows the location of particle Xi+1 after applying Eq. (3).The high and low jump operations have been used in many PSO-based variants [20,49–51]. The main idea of these jump operations is to enable the local best particle, pBesti, to escape from possible local optima. Specifically, a random value is added to each dimension of pBesti, as follows:(9)Xi=pBesti+randnormal(Rmax−Rmin)where Rmax, and Rminare the maximum and minimum boundaries of the search space, respectively,randnormal∈0,1is a normal distributed random number, i.e., N∼(u, σ2) with mean u=0 and standard deviation σ. Note that σ=0.9 helps the escape with a high jump while σ=0.1 helps the escape with a low jump.The fine-tuning operation aims to fine-tune each dimension, di, of particle pbestiindependently from other dimensions, as indicated in Fig. 10.In this study, the ISPO model [12] is adopted for the fine-tuning operation. The details of fine-tuning are shown in Algorithm 3. The fine-tuning operation iterates through all dimensions of pBesti,d, and it tunes each dimension independently. As indicated in Algorithm 3, the search process continues for J times. In ISPO, the velocity is computed as follows [12]:(10)Vi,d=ajpr+Li,dwhere a is the acceleration factor, p is the descent parameter that controls the decay of the velocity, r is a uniformly distributed random number within [−0.5,0.5], and j is the current FEs number. Variable Li,drepresents the learning rate that controls the jumping size. Its value is doubled if the fitness value improves; otherwise it is decreased. As such, Li,dis updated as follows:(11)Li,d=2Vi,diffitnessimprovedLi,d2otherwiseThe value of pBesti,dis updated as follows:(12)pBesti,d=pBesti,d+Vi,diffitnessimprovedpBesti,dotherwiseFine-tuning is useful for exploiting promising search regions. However, it has been given a low priority because it consumes a high number of FEs as compared with other operations which take only a single FEs per call, as indicated in Algorithm 2. The execution of fine-tuning must be delayed until the global operations i.e. exploration operation, convergence operation, and jumping operations, have been performed. This allows the fine-tuning operation to perform exploitation of the regions that have been explored by the global search operations. Therefore, to prevent RLMPSO from executing the fine-tuning operation at the beginning of the search process, fine-tuning in the Q-table is initialized with a negative value, in order to delay its execution (i.e. after a minimum lapse of N FEs, as discussed in section 3.3). Moreover, a cost parameter is introduced to provide an internal delay between consecutive fine-tuning operation calls, as shown in Fig. 11.Three benchmark optimization problems were investigated in this experiment, i.e. unimodal, multi-modal, composite problems, shifted, and rotated problems. In addition, two real-world problems were studied. The details are as follows.Table 1shows the parameter settings of RLMPSO. For performance evaluation, the mean fitness value was computed from the best fitness values obtained from different runs. The convergence curve was computed during the RLMPSO search process.A total of four commonly used unimodal and multi-modal problems, i.e. Sphere, Schwefel, Ackely, and Griewank, were examined. These benchmark problems were studied previously using PSO in [1,68,69]. Table 2shows the mathematical formula of each benchmark problem as well as the associated search range. The maximum number of FEs was set to FEmax=2.5×105, as in [1].The Centre Composite Design (CCD), a useful design-of-experiment method [70,71], was employed to analyze the effects of the delay (D) and cost (C) parameters pertaining to the RLMPSO performance. During the experimental run, the value of each parameter was set at three different levels, i.e. low, medium, and high. The possible combinations of the experimental parameters at each level were generated, in order to study the interaction between these parameters. A total of 100 runs for each of the five cases were carried out with different levels of both D and C settings. Table 3and Fig. 12show the detailed parameter settings and experimental configurations.As can be seen in Table 4, for Exp. 4 (D=1000 FEs, and C=−2), the cost (penalty) of executing fine-tuning was set at −2 by RL. On the other hand, D=1000 indicated that the fine-tuning operation was delayed by a negative value of −10.78 (explained in Section 3.3). In other words, a minimum lapse of 1000 FEs was required before fine-tuning could be executed.For each benchmark problem, the experiment was repeated 100 times. The averages and standard deviations are reported in Table 4. Better results were achieved from two configurations, i.e., D=1000 FEs and C=−2 as well as D=10 FEs and C=−2. This implied that RLMPSO could produce better results with small penalty values. The worst result was produced with the highest cost value. On the other hand, the best result was produced by Exp. 4, where the delay value was the highest while the cost value was the lowest. The results in Table 4 reveal that it is better to delay the execution of fine-tuning rather than calling it in the early stage, where it requires a large computational cost for FEs especially in high-dimensional problems.For further analysis, the average number of calls pertaining to each operation and the average number of FEs for each operation were computed. Fig. 13shows the results plotted in the logarithm scale. The fine-tuning operation accumulated the lowest number of calls, owing to the restriction of the cost and delay parameters embedded in the RL algorithm. The minimum number of calls pertaining to fine-tuning occurred in Exp. 3, in which the cost and delay parameters were the highest. Other operations showed a similar average number of calls. Fig. 14shows the average numbers of FEs for each operation. The fine-tuning operation showed the highest value. Exp. 1 and Exp. 4 required the highest FEs in all benchmark problems. This was because Exp. 1 and Exp. 4 had the minimum cost (C=−2).The effect of the population size on the RLMPSO performance was evaluated. Table 5shows the results of varying the number of particles. A larger population size degraded the results in all benchmark problems. This was owing to the increase in complexity of RLMPSO, i.e., the increase of the number of Q-tables and the FEs pertaining to the fine-tuning operation.The importance of each individual operation in RLMPSO was analyzed. RLMPSO was executed 100 times, each with one of its operations omitted. As can be seen in Table 6, the most important operations affecting RLMPSO were convergence and fine-tuning. This was because of the nature of the benchmark problems, i.e. small numbers of local optima [1]. The least important operations were exploration and high jump. However, both operations would be useful for complicated benchmark problems with high numbers of local optima, such as the composite benchmark problems in Case Study II.To trace the sequence of each RLMPSO operation at run time, Table 7shows the detailed information of each particle as well as the identification (denoted as ID) of the global best particle. The following abbreviations are used in the illustration:•(H) Particle executed the high jump operation, and improved the local best value.(h) Particle executed the high jump operation, and could not improve the local best value.(–) Particle was not selected.(L) Particle executed the low jump operation, and improved the local best value.(l) Particle executed the low jump operation, and could not improve the local best value.(C) Particle executed the convergence operation, and improved the local best value.(c) Particle executed the convergence operation, and could not improve the local best value.(E) Particle executed the exploration operation, and improved the local best value.(e) Particle executed the exploration operation, and could not improve the local best value.(F) Particle executed the fine-tuning operation, and improved the local best value.(f) Particle executed the fine-tuning operation, and could not improve the local best value.As an example, at the fifth FEs, particle 2 successfully executed the convergence operation, and became the global best particle. On the other hand, particle 3 successfully executed the exploration operation at FEs=9. While its local best particle was updated, it was inferior to that of particle 2. However, particle 3 was able to achieve the best results at FEs=14, and became the global best particle.In addition, it can be concluded from Table 7 that when a particle had executed an operation successfully, the operation was accorded a higher priority to be executed in the next FEs. Notice that the fine-tuning operation was delayed until a minimum lapse of 1000 FEs, as shown in Table 7.To investigate the effect of the key RLMPSO control parameters, i.e., N (population size), D (minimum lapse of RLMPSO FEs before executing fine-tuning), C (cost of the local search operation), J (number of fine-tuning FEs), and V (the range of velocity), a graphical sensitivity technique as used in [72] was followed. The main idea was to measure the influence of each parameter independently with respect to the RLMPSO performance. The effects of five RLMPSO control parameters are shown in Table 8. During the experiment, the remaining RLMPSO parameters were set to those in PSO and ISPO models recommended in the literature, i.e., [67], [20,49–51] and [12] as explained in Sections 3.5, 3.6, and 3.7.During this experiment, each parameter was independently changed from low (i.e. −1) to high (i.e. +1) as shown in Table 8, and the settings of the remaining parameters followed the suggested values in Table 1. As an example, when parameter N was studied, other parameters (i.e. D, C, J, and V) were set according to Table 1. For each parameter analysis, RLMPSO was executed 100 times for each of the three levels (i.e. −1, 0, and +1). After that, the mean fitness value was computed, as shown in Fig. 15. The main benefit of the sensitivity analysis is to show the change of each parameter graphically, i.e., having an increasing effect, a decreasing effect, or no effect with respect to the RLMPSO performance.From the graphical plot in Fig. 15, it can be seen that the most important parameter that affected the performance of RLMPSO in all benchmark problems was J (the fine-tuning FEs). By increasing J, the number of FEs calls consumed by the fine-tuning operation is increased. As can be seen in Fig. 15, when J was high (i.e. at state +1 with 100 FEs), the RLMPSO performance for the unimodal benchmark problems (i.e. Sphere and Schwefel) improved, but became worse for the multimodal benchmark problems (i.e. Ackely and Griewank). Because of many local optima in multimodal problems, the fine-tuning operation was less effective, as compared with the unimodal problems with only a single global solution. The least important factor was V (the range of velocity) in all benchmark problems, as shown in Fig. 15.A further analysis has been carried on by evaluating the effect and relative effect measures as defined in [72]:(13)relativeeffect=100×effectglobalmean(14)effect=maxabslog(f(x))0¯−log(f(x))+1)¯,abslog(f(x))0¯−log(f(x))−1)¯,abslog(f(x))+1¯−log(f(x))−1)¯where the global mean is the mean effect among all parameters,log(f(x))0¯is the log mean fitness function at parameter setting (0),log(f(x))+¯is the log mean fitness function at parameter setting (+1), andlog(f(x))−¯is the log mean fitness function at parameter setting (−1).The effect and relative effect measures are reported in Table 9. Parameters C (the cost of local search) and J (the local search FEs) showed the highest impact on the RLMPSO performance, as compared with those from other parameters. This implied that managing the local search method efficiently constituted one of the most important issues in developing an effective memetic-based algorithm.An analysis of the diversity curve generated by RLMPSO during the execution time of a 2-D sphere optimization function was conducted. The 2-D sphere optimization function is defined as:(15)Minimizef(x)=x12+x22,xi∈[−10,10]Following [69], the diversity analysis measure is defined as:(16)Diversity(t)=1N|L|∑i=1N∑j=1DXij−Xj¯2where t is the current search FEs, N is the total number of particles, L is the longest diagonal length in the search space, D is the dimension of the search space,Xijis the value of particle i at dimension j, andXj¯is the mean value of the whole swarm particles at dimension j.For comparison purpose, PSO [61] was employed with a total of 30 particles, and the maximum number of FEs was set to 1000. Fig. 16shows the diversity measures for both PSO and RLMPSO. The RLMPSO diversity curve moved up and down during the search operation. This was owing to the dynamic behaviour of RLMPSO, where each particle evolved independently and could execute any search operations under the control of RL. However, in general, the diversity curve decreased from high to low as the search process progressed.Further analyses were carried on by plotting the locations of the particles at four different search FEs (i.e. FEs=1, 200, 500, and 900), as shown in Fig. 17. The particles started with the exploration operation at the beginning of the search process and then shifted gradually to the convergence state, i.e., the swarm particles became crowded around the global best particle.Table 10reports the average fitness values from 100 runs of RLMPSO. For comparison purposes, the reported results in [1,68,69] are included in Table 10. Note that the results in Table 10 were generated using the same number of FEs in [1], i.e. FEmax=2.5×105, and the configuration for all benchmark problems were set at 30-D, and each experiment was repeated 100 times as in [1]. In [68,69], the benchmark problems were set at the same dimension (i.e. 30-D) while the experimental runs were 30 times, and the FEs in [68,69] were 3×105and2×105, respectively.Therefore, the maximum FEs of RLMPSO (i.e., FEmax=2.5×105) was the same as that in [1] (i.e., FEmax=2.5×105), lower than that in [68] (i.e., FEmax=3×105), but higher than that in [69] (i.e., FEmax=2×105).As can be seen in Table 10, RLMPSO outperformed the memetic-based PSO variant in [1] for all four benchmark problems and the methods in [68] and [69] for three benchmark problems. However, RLMPSO yielded inferior results than those reported in [68,69] for the Ackley function. In addition, it should be noted that since the method in [69] was executed with fewer number of FEs as compared with that of RLMPSO, it could outperform RLMPSO if it was executed with FEmax=2.5×105. As such, it was not surprising that the result from the method in [69] was better than that in [68] for the Ackley function, since more FEs were consumed [73].To quantify the achieved results statistically, the 95% confidence intervals of the RLMPSO results were computed using the bootstrap method [74], as shown in parentheses in Table 10. Statistically, RLMPSO significantly outperformed other methods, except the Ackley benchmark problem. The refinement capability of RLMPSO allowed it to outperform other methods studied in this comparison.In this case study, six composite functions in [1] were examined. They constituted more challenging benchmark problems as compared with the unimodal and multi-modal functions in Case Study I. As an example, composite function five (cf5) is composed of ten benchmark functions comprising two rotated Rastrigin functions, two rotated Weierstrass functions, two rotated Griewank functions, two rotated Ackley functions, and two sphere functions.The same benchmark composite problems were studied in [1] with three PSO variants i.e., CLPSO [53], ISPO [12], and POMA [1]. The results from [1] are included in Table 11for comparison purposes. The maximum number of FEs was set to FEmax=2.5×105, as used in [1], and the dimension of all benchmark problems was 10-D. The experiment was repeated 100 times. The mean fitness values are shown in Table 11.RLMPSO yielded the best results as compared with those from other PSO variants, except POMA in cf3. One of the reasons pertaining to the good performance of RLMPSO was because of fine-tuning, whereby each particle in the micro swarm had the chance to undergo the refinement operation. In addition, the capability of each particle to change from convergence to exploration at any time provided RLMPSO a better chance to escape from local optima. As a result, RLMPSO outperformed other methods in most of the benchmark problems.The 95% confidence intervals are shown in parentheses in Table 11. Note that the upper limit of the 95% bootstrapped confidence interval is smaller than the reported mean fitness values of the related methods in all functions, except for cf3, whereby the POMA result resides within the 95% confidence interval of RLMPSO.To investigate the effectives of RLMPSO in solving shifted and rotated benchmark problems, a total of ten functions from CEC 2005 [75] were used. These problems have been widely studied in the literature [50,68,76,77]. Four models were evaluated using the same CEC 2005 functions, i.e., CLPSO [53], CPSO [31], ANS [78], and GWO [79]. The settings of these models are shown in Table 12. The mathematical formulae of the employed benchmark functions are defined, as follows [75].F1:Shifted sphere functionF1(x)=∑i=1DZi2+f_bias, Z=X-O, X=[x1, x2, …, xD]D: dimensions, X∈[−100, 100]DO: the shifted global optima O=[o1,o2,…,oD]f_bias: the bias valueF2:Shifted Schwefel's Problem 1.2F2(x)=∑i=1D∑j=1iZj2+f_bias,Z=X-O,X=x1,x2,…,xDD: dimensions, X∈[−100, 100]DO: the shifted global optima O=[o1,o2,…,oD]F3:Shifted Schwefel's Problem 1.2 with noise in fitnessF4(x)=∑i=1D∑j=1iZj2∗(1+0.4N(0,1))+f_bias,Z=X-O,X=x1,x2,…,xDD: dimensions, X∈[−100, 100]DO: the shifted global optima O=[o1,o2,…,oD]F4:Shifted rotated Weierstrass functionF9(x)=∑i=1D∑k=0kmaxakcos2πbkZi+0.5−D∑k=0kmaxakcos2πbk0.5+f_biasa=0.5, b=3, a=0.5, b=3, kmax=20Z=(X-O)*M,X=x1,x2,…,xDD: dimensions, X∈[−5, 5]DO: the shifted global optima O=[o1,o2,…,oD]M: linear transformation matrix for function rotationF5:Shifted Rastrigin's FunctionF8(x)=∑i=1D(Zi2−10cos(2πZi)+10)+f_biasZ=(X-O),X=x1,x2,…,xDD: dimensions, X∈[−5, 5]DO: the shifted global optima O=[o1,o2,…,oD]The mean fitness values achieved by RLMPSO and other methods for the rotated and shifted benchmark problems are shown in Table 13. Each method was executed 30 times with a total of 30×104FEs at 30-D using the parameter settings in Table 12. It can be seen in Table 13 that RLMPSO compared favourably with other methods. In particular, RLMPSO achieved the highest accuracy scores for F1, F2, and F5.To investigate the characteristics of RLMPSO at run time for the shifted and rotated benchmark problems, a graphical comparison analysis technique was used by plotting the convergence curves of RLMPSO and other methods. Specifically, the base-10 logarithmic mean values of the fitness function from a total of 30 runs were computed, as shown in Fig. 18. The convergence speed of RLMPSO was slower than those from other models. This was because of the small population size (i.e. 3 particles) of RLMPSO. However, RLMPSO could escape from local optima owing to the benefit of the jumping operations, as well as its capability of changing from exploration to convergence at any time during the search process. As an example, RLMPSO started with a slow convergence rate in Fig. 18(a), (b), and (e), but was able to converge rapidly once the global optima regions were identified.To analyze the computational time, the evaluation criteria in [75] were adopted. The general steps of the criteria are explained, as follows:Step 1: Run and compute the time consumed by the code segment in Fig. 19. This code segment was suggested in [75] to measure the time required for executing different mathematical operations such as summation division, multiplication operations. The time consumed is represented by variable T0.Step 2: Compute the time required to evaluate function F3 (i.e. Shifted Rotated High Conditioned Elliptic Function) from [75] with a dimension of 50-D for 200,000 FEs according to [75]. The time consumed is represented by variable T1.Step 3: Compute the time required by the entire model with function F3 at 50-D for 200,000 FEs according to [75]. This step is conducted independently for each model, i.e. PSO, CLPSO, DE, BAT, Harmony, GWO, and RLMPSO. The time consumed is represented by variable T2.Repeat Step 3 for five times and compute the mean of T2, i.e.,T2¯=mean(T2). The time complexity is represented by T0,T1, T2, and(T2¯−T1)/T2, as indicated in Table 14. The computational time required by each model (i.e. T2) was similar except RLMPSO which required a slightly shorter time (i.e. 21.89s). This was owing to the small population size of RLMPSO (i.e. 3 particles), as compared with other methods that worked with a large population size of 30. Additionally, RLMPSO with a large population size (i.e. 30 particles) was experimented, and the results are shown in Table 15. It should be noted that the CPU time consumed by each method is affected by several factors such as programming language and programming skill, as well as hardware configuration.A detailed analysis was conducted to investigate the time consumed by the Q-table operations of RLMPSO, i.e. updating the Q-table and obtaining the best action from the Q-table. The computational times required for both operations are reported in Table 15. With three particles, the time consumed was very small as compared with the total time consumed by the entire model (i.e. T2). Note that the Q-table size was small (i.e. 5×5) and was independent from the dimension of the problem. Moreover, a large population size of RLMPSO (i.e. 30 particles) was experimented, with the maximum FE set to 200,000. As can be seen in Table 15, the computational time of RLMPSO increased (i.e. T2=29.87s) owing to extra memory requirements.The t-test [80] was conducted to statistically evaluate the achieved results by RLMPSO as compared with other methods in solving the shifted and rotated functions of CEC 2005. For comparing two methods (X and Y, where X=RLMPSO and Y=the compared method), the null hypothesis H0 claimed that X and Y performed equally well. The alternative hypothesis H1 assumed that X outperformed Y. The significance level of p-value was set at 0.05, i.e., the alternative hypothesis H1 would be accepted if the p-value was less than 0.05 (i.e., 95% confidence level). Table 16presents the p-values from the paired t-test between RLMPSO and other methods. All the p-values were smaller than 0.05, except for the test between RMLPSO and CLPSO for F4.Two real-world engineering design optimization problems were examined, i.e., train gear design and pressure vessel design, as follows.The problem of designing train gears was studied in [81], and was further examined in [49,51,68]. The main objective of the problem was to optimize the gear ratio of a compound train gear containing three gears, as shown in Fig. 20. The optimization problem is defined as follows:(17)f(x)=16.931−ADBC2where A, B, C and D are the decision variables that represent the number of gear teeth and their range, i.e., 12=<A, B, C, and D≤60, as described in [81].The main objective was to find the optimal values of A, B, C and D that could produce a gear ratio as close to 1/6.931 as possible. The formulation of the gear ratio is as follows:(18)Thegearratio=angularvelocityofoutputshaftangularvelocityofinputshaftIn this study, the performance of RLMPSO was compared with the reported results in [49,51,68]. The parameters used in this experiment were the same as those in [49,51,68], where the maximum FEs was fixed at FEmax=3×105. As indicated in Table 17, RLMPSO achieved the best results. Again, the capability of performing fine-tuning was useful to tackle this train gear design optimization problem. Furthermore, from the statistical point of view, RLMPSO significantly outperformed other methods in [49,51,68], as indicated by the 95% confidence intervals.The pressure vessel design problem [83–85] aimed to find the minimum manufacturing cost of designing the cylindrical compressed air storage with pre-defined conditions and constrains, as shown in Fig. 21. The complexity of this design problem was higher than that of the train gear design problem as a total of four design constraints (i.e. g1, g2, g3, and g4) were involved. The problem is defined as follows.Considerx→=Ts,Th,R,L(19)Minimizef(x→)=0.6224TsRL+1.7781ThR2+3.1661Ts2L+19.84Ts2Rsubjecttog1(x→)=0.0193R−Ts≤0g2x→=0.00954R−Th≤0g3x→=1296000−πR2L−43πR3≤0g4x→=L−240≤0where 0≤Ts≤99, 0≤Th≤99, 10≤R≤200, 10≤L≤200, and L is the length of the cylinder, R is the cylinder radius, Tsis the cylinder thickness, and This the thickness of cylinder head, as shown in Fig. 21.This experiment was conducted using the same settings in [85], where the maximum FEs was set to FEmax=5×104, and the experimental run was repeated 30 times. The reported results in [83–85] are shown in Table 18for comparison purposes. RLMPSO yielded the best mean results as compared with those from other methods. From the statistical point of view, RLMPSO significantly outperformed the methods in [83,84], as indicated by the 95% confidence intervals.

@&#CONCLUSIONS@&#
