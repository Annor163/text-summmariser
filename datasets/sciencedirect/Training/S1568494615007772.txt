@&#MAIN-TITLE@&#
Automatic clustering using nature-inspired metaheuristics: A survey

@&#HIGHLIGHTS@&#
Sixty-five clustering methods based on nature-inspired metaheuristics are reviewed.Codification and validity index are basic items in automatic clustering algorithms.Evolutionary computation is the most popular paradigm used in automatic clustering.A strong tendency in using multiobjective and hybrid algorithms is found.Research directions and challenges for automatic clustering problem are formulated.

@&#KEYPHRASES@&#
Cluster analysis,Automatic clustering,Nature-inspired metaheuristics,Single-objective and multiobjective metaheuristics,

@&#ABSTRACT@&#
In cluster analysis, a fundamental problem is to determine the best estimate of the number of clusters; this is known as the automatic clustering problem. Because of lack of prior domain knowledge, it is difficult to choose an appropriate number of clusters, especially when the data have many dimensions, when clusters differ widely in shape, size, and density, and when overlapping exists among groups. In the late 1990s, the automatic clustering problem gave rise to a new era in cluster analysis with the application of nature-inspired metaheuristics. Since then, researchers have developed several new algorithms in this field. This paper presents an up-to-date review of all major nature-inspired metaheuristic algorithms used thus far for automatic clustering. Also, the main components involved during the formulation of metaheuristics for automatic clustering are presented, such as encoding schemes, validity indices, and proximity measures. A total of 65 automatic clustering approaches are reviewed, which are based on single-solution, single-objective, and multiobjective metaheuristics, whose usage percentages are 3%, 69%, and 28%, respectively. Single-objective clustering algorithms are adequate to efficiently group linearly separable clusters. However, a strong tendency in using multiobjective algorithms is found nowadays to address non-linearly separable problems. Finally, a discussion and some emerging research directions are presented.

@&#INTRODUCTION@&#
Cluster analysis is an unsupervised learning technique aimed at discovering the natural grouping of objects according to the similarity of measured intrinsic characteristics [1]. The two fundamental problems in automatic clustering are determining the optimal number of clusters and identifying all data groups correctly. In this sense, the number of combinations in assigning N objects into K clusters is11S(N, K) is known as the Stirling numbers of the second kind.:(1)S(N,K)=1K!∑i=0K(−1)K−iKiiN.On the other hand, the search space size in finding the optimal number of clusters is22B(N) is known as the Bell numbers.:(2)B(N)=∑K=1NS(N,K).Besides, the clustering (or grouping) problem of finding an optimal solution is NP-hard when K>3 [2]; hence, even for moderate-sized problems, the clustering task could be computationally prohibitive [3].To limit the search space size, many clustering methods described in the literature assume a fixed number of clusters, which is unknown a priori in many clustering practices. To overcome this inconvenience, automatic clustering approaches aimed at finding the adequate number of clusters within the range [Kmin, Kmax] have been developed.The principal clustering techniques developed in the last 50 years were reviewed by Jain [4], who presented the evolution and trends in data clustering. Also, Xu and Wunsch [5] focused on algorithms for grouping data sets that are used in statistics, computer science, and machine learning. In the last decade, developments in automatic clustering have been strengthened [6–8]. In particular, nature-inspired metaheuristics have been applied to obtain satisfactory suboptimal solutions to the automatic clustering problem in an acceptable timescale [9]. These kinds of metaheuristics model the behavior of natural phenomena, which exhibit an ability to learn or adapt to new situations to solve problems in complex and changing environments [10].Some review articles on clustering analysis that use nature-inspired metaheuristics have been published by Handl and Meyer [11], Sheikh et al. [12], Hruschka et al. [9], Rana et al. [13], Bong and Rajeswari [6], Nanda and Panda [14], and Alam et al. [15]. A review of ant-based and swarm-based clustering techniques was presented by Handl and Meyer [11]. A survey on genetic algorithms applied to clustering was summarized by Sheikh et al. [12]. Hruschka et al. [9] presented a brief summary of evolutionary algorithms and reviewed the initialization procedure, encoding scheme, crossover, mutation, and fitness evaluation for single and multiobjective cases. Bong and Rajeswari [6] investigated multiobjective nature-inspired clustering techniques applied to image segmentation. Recently, Nanda and Panda [14] surveyed some nature-inspired metaheuristics focused on the partitional clustering paradigm. Reviews on particle swarm optimization algorithms and their applications to data clustering were presented by Rana et al. [13] and Alam et al. [15].Despite the relevance of these review articles, to the best of our knowledge, no review paper about nature-inspired metaheuristics for automatic clustering has been published. Therefore, we present an in-depth review of nature-inspired metaheuristics for automatic clustering that have been reported in the last two decades. This paper contributes in the following two main aspects: (i) it presents an up-to-date overview on single-solution, single-objective, and multiobjective metaheuristics applied to automatic clustering, and (ii) it provides a review of important aspects, such as encoding schemes, validity indices, data sets, and applications.The outline of this paper is as follows. Section 2 describes the basic terms and concepts related to automatic clustering analysis. Section 3 presents single-solution metaheuristics that use a single agent or solution, which moves through the search space in a piecewise style. Section 4 reviews single-objective metaheuristics, in which a population of potential solutions cooperate to optimize a unique cost function. Section 5 presents multiobjective metaheuristics, which optimize distinct cost functions and consider a trade-off among them. Section 6 discusses relevant automatic clustering algorithms useful for solving specific data sets and applications. Finally, future tendencies and conclusions are given in Sections 7 and 8, respectively.The following terms and notation are used throughout this paper:•A pattern (or object) is a single data item represented by a vector of measurements x={x1, x2, …, xD}T, wherexi∈ℝis a feature (or attribute), and D denotes the dimensionality.A data set is denoted asX={x1,x2,…,xN}∈ℝD, where N is the total number of patterns in the data set.A cluster (or group) can be defined as high-density regions separated by low-density regions within the feature space.Clustering, denoted as C={ck|k=1, …, K}, refers to the set of mutually disjoint clusters that partitions X into K groups.The number of objects in cluster ckis denoted by nk=|ck|.The centroid of cluster (or prototype) ckis expressed asc¯k=1/nk∑xi∈ckxi, whereas the centroid of data setX isX¯=1/N∑xi∈Xxi.A distance measure is a metric (or quasi-metric) used to quantify the proximity between patterns.A cluster validity index uses a distance measure to quantitatively evaluate the obtained clustering.The specialized literature on cluster analysis commonly classifies clustering techniques into partitional and hierarchical [1,4,14], which are detailed in the following subsections.Partitional clustering can be performed in two different modes: hard (or crisp) and fuzzy. Hard clustering assumes that the membership between patterns and clusters is binary; thus, each pattern belongs to exactly one cluster. On the other hand, fuzzy clustering assigns different degrees of membership to the patterns for each cluster to build a non-binary relationship between them.Hard clustering divides a data set directly into a prespecified number of clusters without a hierarchical structure [16], where a data set X is partitioned into K nonoverlapping groups C={c1, c2, …, cK}, such that the following three conditions should be satisfied:•ci≠∅, i=1, …, K;⋃i=1Kci=X;ci∩cj=∅, i, j=1…, K and i≠j.Perhaps the most fundamental algorithm related to hard clustering is the k-means algorithm, which attempts to minimize the sum-of-squared-error criterion [17,18]. Fifty years after its formulation, k-means is still popular and widely used because of its simplicity and low computational complexity [4]. However, a predefined number of clusters is required at the beginning of the algorithm, which is unknown in several real-world clustering applications. Hence, the k-means algorithm has been extended to automatically find the number of clusters; some of these extended approaches include the X-means [19] and the G-means algorithm [20].Fuzzy clustering is an alternative definition given in terms of fuzzy sets, in which each pattern belongs to more than one cluster simultaneously, with a certain degree of membership uj∈[0, 1]. The membership value of the ith pattern in the jth cluster should satisfy the following two conditions:•∑j=1Kuj(xi)=1, i=1, …, N;∑i=1Nuj(xi)<N, j=1, …, K.The most well-known fuzzy algorithm is fuzzy c-means [21], which is essentially a fuzzy extension of the k-means method.Hierarchical clustering algorithms produce a hierarchy of clustering called a dendrogram (or tree structure), which represents the nested grouping of the objects in a data set. The procedure builds N successive clustering levels, in which the current clustering is based on the solution obtained at the previous level. Therefore, hierarchical clustering does not require a priori knowledge about the number of clusters; however, the obtained groups are static because the objects assigned to a given cluster cannot move to another one.Agglomerative and divisive approaches are the two main categories of hierarchical clustering, of which single-link and complete-link [4] algorithms are the most well-known.Clustering algorithms measure the proximity between objects to form groups [1]. The selection of the appropriate proximity measure is important because memberships are defined for every object in data set X. Depending on the kind of proximity measure, different groupings can be created [22]. A proximity measure can be either a distance (dissimilarity) or a similarity between a pair of objects, between an object and a prototype, or between a pair of prototypes. The most common proximity measures used in the automatic clustering techniques described herein are detailed below.•The Minkowski metric [16], or Lp-norm, is a dissimilarity measure defined as(3)dp(x,y)=∑i=1D|xi−yi|p1/p,where x and y are D-dimensional data vectors. Note that when p=2, the Minkowski metric becomes the well-known Euclidean distance (or L2-norm), which is denoted as de(x, y). Two other common special cases of the Minkowski metric are the Manhattan distance (or L1-norm), when p=1, and the Chebyshev distance (or L∞-norm), when p→∞, which is computed as(4)d∞(x,y)=max1≤i≤D|xi−yi|.The similarity between two vectors x and y can be measured by the cosine of the angle between them:(5)cos(x,y)=xTy∥x∥∥y∥,where ∥·∥ denotes the L2-norm. There is a relationship between cosine similarity and Euclidean distance. If x and y are unit vectors, the cosine similarity can be transformed into a dissimilarity measure as [23](6)dcos(x,y)=1−cos(x,y)=12de2(x,y).The point symmetry distance between the pattern xiand the cluster ckis defined as [24](7)dps(xi,ck)=minxj∈X\xide(xi−c¯k,xj−c¯k)de(xi,c¯k)+de(xj,c¯k).The modified point symmetry distance between the object xiand the cluster ckis expressed as [36](8)dps*(xi,ck)=12∑minxj∈ck(2){de(2c¯k−xi,xj)}.The point2c¯k−xiis called the symmetrical (reflected) point of xiwith respect to the prototypec¯k. The function ∑min(n) computes the sum of the n lowest arguments.A cluster validity index (CVI) evaluates the goodness of a particular clustering structure determined by a clustering algorithm by using only information inherent in the data [1]. A CVI should satisfy the following requirements: (i) it should have intuitive meaning, (ii) it should be easy to compute, and (iii) it should be mathematically justifiable. Also, the CVI should be capable of imposing an ordering of the clusters in terms of its goodness [26]. Regarding the kind of clustering algorithm, the CVI could be either hard or fuzzy.Commonly, a CVI defines a relationship between cluster cohesion (within-group scatter) and cluster separation (between-group scatter) to estimate the quality of a clustering solution [27]. This characteristic has been exploited by clustering algorithms based on metaheuristics, in which CVIs are used as objective functions to be optimized.The most common CVIs used by the automatic clustering algorithms reviewed in this study are depicted below. The following notations are used: an abbreviation of the CVI name followed by two subscript symbols, a letter to indicate if the CVI is hard (H) or fuzzy (F) and an arrow to denote if the CVI is maximized (↑) or minimized (↓) by the metaheuristic.•Within-group scatter (WGSH↓) [18]. The within-cluster dispersion by the sum of the squared distances between points in a cluster to the centroid is measured as(9)WGS(C)=∑ck∈C∑xi∈ckde2(xi,c¯k).Between-group scatter (BGSH↑) [1]. The separation between a pair of cluster prototypes is defined as(10)BGS(C)=∑ck,cr∈Cde(c¯k,c¯r).Connectivity index (ConnH↓) [7]. This reflects the cluster connectedness by evaluating the degree to which neighboring objects have been placed in the same cluster. It is computed as(11)Conn(C)=∑i=1N∑j=1Lxi,nij,wherexr,s=1j,if¬∃ck:r∈ck∧s∈ck;0,otherwise.The variable nijis the jth nearest neighbor of the ith object. The parameter L determines the number of neighbors that contribute to the connectivity measure.Dunn index (DIH↑) [28]. This is a ratio-type index in which the cohesion is estimated by the nearest neighbor distance and the separation by the maximum cluster diameter. It is defined as(12)DI(C)=minck∈C{mincr∈C\ck{δ(ck,cr)}}maxck∈C{Δ(ck)},whereδ(ck,cr)=minxi∈ckminxj∈cr{de(xi,xj)},Δ(ck)=maxxi,xj∈ck{de(xi,xj)}.Besides, there are distinct variants of DI, called generalized Dunn indices [29], which involve different measures of the criteria cohesion δ and separation Δ.Calinski–Harabasz index (CHH↑) [30]. This is a ratio-type index in which the cohesion is estimated by the sum of the distances of the patterns to their respective centroid and the separation is measured by the sum of the distances from each centroid to the global prototype. It is defined as(13)CH(C)=N−KK−1×∑ck∈Cnkde(c¯k,X¯)∑ck∈C∑xi∈ckde(xi,c¯k).Davies–Bouldin index (DBH↓) [31]. In this index, the cohesion is estimated by the mean distance of the objects to their respective centroid and the separation quantifies the distance between centroids. It is expressed as(14)S(C)=1K∑ck∈Cmaxcr∈C\ckS(ck)+S(cr)de(c¯k,c¯r),whereS(ck)=1nk∑xi∈ckde(xi,c¯k).FCM index (FCMF↓) [21]. This is the cost function originally used in fuzzy c-means algorithm, which minimizes the within-group scatter measure defined as(15)FCM(C)=∑ck∈C∑xi∈Xukimde2(xi,c¯k),where U(X)={uki} is the fuzzy membership matrix, and m denotes the fuzzifier parameter.Silhouette index (SIH↑) [32]. This is a normalized summation-type index in which the cohesion is measured by the sum of the distances between all the points in the same cluster and the separation is based on the nearest neighbor distance between points in different groups. It is defined as(16)SI(C)=1N∑ck∈C∑xi∈ckb(xi,ck)−a(xi,ck)max{b(xi,ck),a(xi,ck)},wherea(xi,ck)=1nk∑xj∈ckde(xi,xj),b(xi,ck)=mincr∈C\ck1nr∑xj∈crde(xi,xj).Xie–Beni index (XBF↓) [26]. This is the ratio of the total variation to the minimum separation of the clusters defined as(17)XB(C)=∑ck∈C∑xi∈Xuki2de2(xi,c¯k)Nminck,cr∈C{de2(c¯k,c¯r)}.Note that the numerator is the FCM index with m=2.Turi index (TIH↓) [33]. In this ratio-type index, the inter-cluster separation is estimated by the minimum distance between centroids and the intra-cluster dispersion is computed by the average distance between each object and its respective centroid:(18)TI(C)=(c×N(μ,σ)+1)×intra(C)inter(C),where c is a user-specified parameter,N(μ,σ)is a normal distribution with mean μ and standard deviation σ, and the terms intra(C) and inter(C) are defined asintra(C)=1N∑ck∈C∑xi∈ckde(xi,c¯k),inter(C)=mincr∈C\ck{de(c¯k,c¯r)}.PBM index (PBMF↑) [34]. This index is defined as the product of cohesion and separation measures. The former is calculated by the sum of all pattern distances in a cluster to its respective centroid, whereas the later is estimated by the maximum distance between centroids. The PBM index is expressed as(19)PBM(C)=1K×E(C)FCM(C)×maxck,cr∈C{de(c¯k,c¯r)}2,where the denominator in the second factor involves the FCM index with m=1, and the numerator is computed asE(C)=∑xi∈Xde(xi,X¯).CS index (CSH↓) [35]. This is a ratio-type index that estimates the cohesion by using the cluster diameters and the separation measurement is based on the nearest neighbor distance between prototypes. It is computed as(20)CS(C)=∑ck∈CΔ(ck)∑ck∈Cmincr∈C\ck{de(c¯k,c¯r)},whereΔ(ck)=1nk∑xi∈ckmaxxj∈C{de(xi,xj)}.Sym-index (SymH↑) [36]. This ratio-type index is an adaptation of the PBM index [34]; the cohesion is estimated by the sum of the point symmetry distances in the same cluster and the separation is quantified by the maximum Euclidean distance between centroids. It is defined as(21)Sym(C)=maxck,cr∈C{de(c¯k,c¯r)}K∑ck∈C∑xi∈ckdps(xi,ck).Weight of Evidence Information (WIMH↑) [37]. This is an uncertainty measure based on mutual information, which estimates the amount of evidence information between an object and a cluster at a given confidence level. The value of WIM for an object xibelonging to cluster ckis given by(22)WIM(C)=I(ck:xi)−I(≠ck:xi),where the mutual information is defined in terms of the joint probability distribution of xigiven ckand the marginal probability of ckasI(ck:xi)=logP(ck∣xi)P(ck).The clustering problem can be formulated as an optimization problem that could be solved by single-objective and multiobjective metaheuristics [38].Let Ω={C1, C2, …, CB(n)} be the set of all feasible clusterings, in which the elements are clustering solutions of a given data set X, and f is a single criterion function, for instance, one of the validity indices listed in Section 2.4. Then, the goal of single-objective clustering problem (Ω, f) is to determine the clustering C* for which(23)f(C*)=min{f(C)∣C∈Ω}.Note that f(·) is minimized without loss of generality. On the other hand, the goal of multiobjective clustering problem (Ω, f1, f2, …, fm) is to determine the clustering C* for which(24)f(C*)=min{ft(C)∣C∈Ω},t=1,2,…,mwhere ft, t=1, …, m, is a set of m (single) criterion functions. Usually, multiobjective problems have multiple optimal solutions, which are commonly identified by using the principle of Pareto dominance. For two clustering solutions C1, C2∈Ω, C1 is said to dominateC2 (denoted as C1≺C2), where the following two conditions should be satisfied:(25)ft(C1)≤ft(C2),∀t∈1,2,…,mand(26)ft(C1)<ft(C2),∃t∈1,2,…,m.The set of all Pareto nondominated solutions is referred to as the Pareto-optimal set and its corresponding set of objective function values is referred to as the Pareto-optimal front.Any iterative metaheuristic, whether single-objective or multiobjective, requires a representation or an encoding of a solution, which is directly related to the objective function to be optimized [40]. The encoding schemes play a relevant role in the efficiency and effectiveness of any metaheuristic and constitute an essential step in its design. Besides, the efficiency of an encoding is also related to the search operators applied in this representation, such as mutation, recombination, neighborhood, etc. [41].Encoding schemes can be categorized into binary, integer, and real, which have all been used in metaheuristics that attempted to solve the automatic clustering problem. Because the number of clusters is unknown a priori, the encoding scheme should be designed for varying the number of clusters within a predetermined range [Kmin, Kmax], where Kmin=2 andKmax=round(N)[42]. Fig. 1illustrates the didactic 10-point data set used to exemplify binary, integer, and real encodings when varying the number of clusters in the representation.Each clustering solution is represented as a binary string of length equal to N; that is, each position in the binary string corresponds to a particular object in the data set. The value of the ith position is “1” if the ith object is considered as a cluster prototype, and “0” otherwise. Fig. 2illustrates the clustering solutions of the data set in Fig. 1 when K=3 and K=5.Similarly to binary encoding, the integer representation codifies all objects in the data set into an integer array. The codification of clustering solutions can be done by two kind of representation: label-based and graph-based encodings, which are detailed below.•Label-based encoding. Every object in the data set takes an integer value (or cluster label) in the alphabet {1, …, Kmax}, where Kmax is the maximum number of possible clusters. This integer encoding is naturally redundant, because Kmax! different labelings represent the same clustering solution. For instance, for a 5-point data set with K=2, the clustering solution C={11122} is the same as C={22211}. Therefore, the size of the search space explored by the clustering algorithm is much larger than the original space of solutions [9]. An alternative solution to this problem is to use a renumbering procedure [2]. Fig. 3a shows the labeled data set in Fig. 1 when K=3 and K=5.Graph-based encoding. Each object in the data set can take a single value from the set {1, …, N}. A value j assigned to the ith position means that there is a link between these objects, which are placed in the same cluster. Then, the clustering solution is recovered by identifying all connected components of the directed graph [7]. Fig. 3b illustrates the clustering solution of the data set in Fig. 1 when K=3 and K=5.In real encoding approaches, a clustering solution represents the location of the cluster prototype in the D-dimensional feature space. Let us consider a set of N objects to be clustered into K groups, in which the prototypes of each cluster are codified into a real vector of size D×K. The population-based metaheuristics used in automatic clustering commonly used fixed- or variable-length encoding.•Fixed-length encoding. All clustering solutions encode a predefined maximum number of prototypes, Kmax. Hence, all members of the population maintain the same length throughout the optimization process; that is, D×Kmax. Additionally, to determine which prototypes participate in the clustering process at each iteration of the algorithm, a mask vector or activation thresholds are commonly used [43,44]. Fig. 4shows a fixed-length encoding when Kmax=5 but considering the activation of K=3 prototypes.Variable-length encoding. Consider a set of P clustering solutionsC={Ci|i=1,…,P}. Then, the ith clustering solution encodes a particular number of prototypes Ki. Hence, the length of vector solutions is variable; that is, D×Ki. The population-based metaheuristics that use this variable scheme should adjust the search operators to cope with members of different sizes [45,46].Nature-inspired automatic clustering algorithms have been successfully applied to distinct areas of engineering and science. Researches commonly use standard data sets, which could be either synthetic or real-life measurements, for validating and comparing the performances of different automatic clustering algorithms.Synthetic data sets are widely preferred because it is feasible to control distinct features, such as number of clusters, density, size, shape, dimensionality, overlapping, and noise. Moreover, an important feature is the linear separability between clusters, which mostly determines the complexity of the data set. Three cases are clearly identified: (i) linear separability without overlapping, (ii) linear separability with overlapping, and (iii) non-linear separability without overlapping. The two first cases consider globular or elliptical clusters, such asS˜1,S˜3, andS˜7data sets in Table 1, whereas the third case involves arbitrary cluster shapes, such asS˜9,S˜10, andS˜11data sets in Table 1.On the other hand, real-life data sets consider features obtained from sensed or measured real-world data, which could be associated, in some cases, to class labels. Well-known data sets, such as the Iris, Breast Cancer, Wine, or Glass, have become standard data sets in the literature. Besides, many authors define particular data sets related to the particular problem that they are attempting to solve with a clustering algorithm designed to cope with specific requirements.Furthermore, principal real-life applications areas of automatic clustering based on nature-inspired metaheuristics include image segmentation, such as satellite imagery [51,8,52–55], medical image [57,49], and textured images [47,43,96,97], and gene expression microarray analysis [37,56–58,25].It is worth mentioning that synthetic data sets are useful to evaluate distinct characteristics and limitations of clustering algorithms. Besides, real-world problems aid to demonstrate the generalization capabilities of the clustering algorithms to distinct research fields. Hence, a complete evaluation of an automatic clustering algorithm should involve synthetic and standard real-life data sets as well as an application field.Tables 1 and 2summarize the characteristics of common synthetic and real-life data sets, respectively, used by the automatic clustering algorithms reviewed herein. In such tables, “ID” identifies the data set, “Data set” refers to the original name of the data set, “N” denotes the number of patterns, “D” is the feature space dimensionality, and “K” is the number of groups.This section presents a review of single-solution-based metaheuristics for the automatic clustering problem. These kinds of metaheuristics improve a single point solution, evaluated by a single criterion function, and could be viewed as search trajectories through the search space. Such trajectories are updated by iterative procedures that move from the current solution to another one in the search space. Hence, they are mainly oriented toward intensifying the search in local regions (exploitation) [41]. In the next subsections, simulated annealing (SA) and tabu search (TS) approaches applied to automatic clustering are reviewed.Simulated annealing (SA) is a probabilistic method proposed by Kirkpatrick et al. [59] to find the global minimum of a cost function. SA emulates the physical process whereby a melted solid material (initial state) is gradually cooled until the minimum energy state is reached, that is, when the material structure is “frozen.” An overview of the theoretical and practical aspects of SA is provided in [60].Bandyopadhyay et al. [61] proposed an algorithm called SAKM-clustering, in which objects are redistributed among K clusters probabilistically. In this scheme, points farther away from the cluster center have higher probabilities of migrating to other clusters. SAKM-clustering minimizes the WGS index, which is also used by the k-means algorithm [17]. The result of experiments, which considered three artificial and two real-life data sets, indicated that SAKM-clustering surpassed the performance of k-means.Later, Bandyopadhyay [44] proposed an unsupervised fuzzy clustering algorithm called SA-RJMCMC. This approach automatically discovers the number of clusters by using the homogeneous reversible jump Markov chain Monte Carlo (RJMCMC) kernel; thus, this algorithm is able to jump between different dimensions. SA-RJMCMC does the clustering by optimizing the XB index. Its effectiveness, in comparison with the FCM algorithm, was shown in experiments that considered three artificial and two real-life data sets, in which the SA-RJMCMC technique determined the appropriate number of clusters automatically and provided better values of the XB index in almost all the evaluated cases.Tabu search (TS), a metaheuristic search procedure proposed by Glover [62], uses flexible-structure memories conditions to strategically constrain and free the search process (embodied in the tabu restrictions and aspiration criteria) and applies memory functions of varying time spans to intensify and diversify the search [63,64]. TS has obtained optimal and near optimal solutions to a wide variety of classical and practical problems. Although it has not been used directly to solve the automatic clustering problem, TS has been used to guide other methods in escaping local optima by using a tabu memory, such as in the method proposed by Pan and Cheng [65], which is detailed in Section 4.1.3.

@&#CONCLUSIONS@&#
