@&#MAIN-TITLE@&#
The iPICEA-g: a new hybrid evolutionary multi-criteria decision making approach using the brushing technique

@&#HIGHLIGHTS@&#
We propose a new method to model decision-maker preference, the brushing technique.Using the brushing technique, a new decision-making method iPICEA-g is proposed.Experimental results show iPICEA-g performs well.

@&#KEYPHRASES@&#
Preference articulation,Decision making,Multi-objective optimization,Evolutionary algorithms,

@&#ABSTRACT@&#
Various preference-based multi-objective evolutionary algorithms have been developed to help a decision-maker search for his/her preferred solutions to multi-objective problems. In most of the existing approaches the decision-maker preferences are formulated either by mathematical expressions such as the utility function or simply by numerical values such as aspiration levels and weights. However, in some sense a decision-maker may find it easier to specify preferences visually by drawing rather than using numbers. This paper introduces such a method, namely, the brushing technique. Using this technique the decision-maker can specify his/her preferences easily by drawing in the objective space. Combining the brushing technique with one existing algorithm PICEA-g, we present a novel approach named iPICEA-g for an interactive decision-making. The performance of iPICEA-g is tested on a set of benchmark problems and is shown to be good.

@&#INTRODUCTION@&#
Multi-objective problems (MOPs) regularly arise in many real-world decision problems, that is, the solutions are required to meet multiple performance criteria (or objectives) simultaneously. These objectives are often conflicting, wherein an improvement in one objective cannot be achieved without detriment to another objective. In this case, there is no single solution to a MOP that can be selected objectively; rather a set of solutions exists, representing different performance trade-offs between criteria. In this setting, the subjective preferences of a decision maker (DM) have to be considered so as to identify a single satisfied solution.According to when decision maker preferences are incorporated, i.e. before, during or after the search, hybrid evolutionary multi-criteria decision making approaches can be divided into three classes – a priori, interactive and a posteriori, respectively. In an a priori decision making approach, the DM preferences are incorporated prior to the search process. When the DM preferences can be faithfully captured in a mathematical model, an a priori method would be effective and efficient. However, this is rarely the case. In an interactive decision making approach, the DM preferences are incorporated progressively during the optimization process. This enables a DM to learn about the problem and fine-tune his/her preferences if needed, effectively guiding the search towards regions of interest and away from exploring non-interesting solutions. The main limitation of this scheme is that the DM may need to be involved intensively during the search process. In an a posteriori decision making approach, the DM preferences are incorporated after the search; an approximation of the Pareto optimal front is found first followed by selection of a preferred solution by the DM from the set of trade-off solutions presented.An a posteriori approach can be effective for MOPs with 2 or 3 objectives – a good approximation of the Pareto optimal front can be obtained and easily be presented to the DM, enabling him/her to confidently select a preferred solution. However, a posteriori schemes become less effective on MOPs with higher number of objectives, sometimes termed many-objective problems (MaOPs, Purshouse and Fleming, 2003). Not only does the computational burden for solving these problems become very expensive, the approaches become more inefficient since the DM often is only interested in particular regions of the Pareto front. Furthermore, the number of Pareto optimal solutions required for describing the entire Pareto optimal front of a MaOP is usually very large. Selecting one preferred solution from all these solutions is cognitively difficult. Thus, to facilitate the process of decision making, the alternative is to consider incorporating DM preferences a priori or interactively into the evolutionary multi-objective (EMO) approaches. Such hybrid approaches might take advantages of both EMO and multi-criteria decision making methods.To date, considerable effort has been spent on developing efficient hybrid evolutionary multi-criteria decision making approaches, including, for example, MOGA (Fonseca and Fleming, 1998a), G-MOEA (Branke, 2001), reference point based NSGA-II (Deb and Sundar, 2006), reference direction based NSGA-II (Deb and Kumar, 2007a), utility function (pair wise comparison) based MOEA (Phelps and Köksalan, 2003). Amongst these methods, some elicit a direct model of preferences, e.g. reference point (aspirations), reference direction, and trade-off information. Other methods construct a preference model indirectly based on elicitation of some examples of holistic judgements, such as utility function.Eliciting direct preference information from the DM requires a high cognitive effort, and so can be counter productive in real-world decision making situations. Eliciting indirect preferences (i.e., utility functions) tends to be less demanding in terms of cognitive effort. However, it is argued that constructing an appropriate utility function, e.g., via pair wise comparisons (Branke et al., 2009; 2010), may not be easy (Köksalan et al., 2013). This study proposes a new method called the brushing technique which enables a DM to specify his/her11Throughout this paper, we use he instead of he/she, and his instead of his/her for brevity.preferences simply by drawing, that is, brushing his/her preferred region in the objective space, see Fig. 1(a). According to the brushed region, the preference information is elicited and then is incorporated into an algorithm to search for solutions that are of interest to the DM, see Fig. 1(b). Combining the brushing technique with the preference-inspired co-evolutionary algorithm using goal vectors (PICEA-g) (Purshouse et al., 2011; Wang et al., 2013a), a new hybrid evolutionary multi-criteria decision making approach, denoted as iPICEA-g, is proposed, which provides users an alternative approach to conduct multi-criteria decision making.The remainder of this paper is structured as follows: in Section 2 a review of hybrid evolutionary multi-criteria decision making approaches is provided. Following that, Section 3 elaborates the iPICEA-g. Section 4 empirically studies the performance of iPICEA-g on different problems. Section 5 concludes this paper.This section reviews some representative hybrid evolutionary multi-criteria decision making approaches, classified as direct preference mode based, and indirect preference model based. There are other approaches to those presented here, such as the Interactive Surrogate Worth Trade-off method (Vira and Haimes, 1983), the NIMBUS approach (Miettinen and Mäkelä, 1995) and others proposed in Steuer and Choo (1983),Korhonen and Laakso (1986),Korhonen et al. (1984) and Köksalan and Sagala (1995). However they have yet to be combined with EMO approaches and are not reviewed here.There is a large body of approaches using a direct preference model, including reference point (aspirations) based, weights related information based (e.g. lexicographical ordering, relative importance order, reference direction and light beam search), trade-off information based and other forms.Perhaps MOGA developed by Fonseca and Fleming (1993) is the earliest such approach. The DM preference is specified as aspirations and the non-dominated ranking mechanism is extended to accommodate aspiration levels, enabling the search to be gradually guided towards the DM region of interest. MOGA was further extended by introducing a preferability operator, with which both goals and priorities can be accommodated in the ranking scheme (Fonseca and Fleming, 1998a). This new ranking scheme provides a unification of Pareto optimality, the lexicographic method, goal programming, constraint satisfaction and constrained optimization. MOGA has been successfully used in optimising a low-pressure spool-speed governor of a Pegasus gas turbine engine and many other applications (Fleming et al., 2005; Fonseca and Fleming, 1998b). The main weakness of this approach is that it requires a DM to know the ranges of objective values so as to initialize coherent aspiration levels.Another representative approach that uses aspirations was proposed by Molina et al. (2009). A dominance relation called g-dominance (g refers to goals) is defined; solutions satisfying all the aspirations and solutions fulfilling none of the aspirations are preferred over solutions satisfying some aspirations. An approach called g-NSGA-II that combines g-dominance and NSGA-II is proposed to search for solutions satisfying the specified aspirations. This algorithm works regardless of whether the specified goal vector is feasible or infeasible. However, it is demonstrated in Said and Bechikh (2010) that g-NSGA-II faces difficulties when the provided goal vector is close to the true Pareto front (as the approach does not preserve a Pareto based ordering). Handling of multiple ROIs by g-NSGA-II is not considered. Intuitively, the g-dominance relation is not easy to extend to handle multiple ROIs as an individual can g-dominate one goal vector, and simultaneously, be g-dominated by another goal vector.Deb and Sundar (2006) proposed a reference point based NSGA-II (R-NSGA-II) for searching for solutions close to a DM specified reference point. The reference point is not applied in a classical way, i.e. together with an achievement scalarizing function (Wierzbicki, 1980), but rather by establishing a biased crowding scheme. Solutions near reference points are emphasized by the selection mechanisms. The extent and the distribution of the solutions is maintained by a user defined parameter ɛ. The efficiency of R-NSGA-II is demonstrated on MOPs with up to ten objectives. R-NSGA-II can also handle multiple ROIs simply by using multiple reference points. In addition to the NSGA-II, (Allmendinger et al., 2008) hybridized the reference point approach with particle swarm optimization (PSO) approach.Thiele et al. (2009) hybridized the reference point with an indicator based evolutionary algorithm (PBEA). The reference point is applied to an achievement scalarizing function and this is then incorporated into the binary indicator function, the ɛ-indicator (Zitzler et al., 2003) (which is Pareto dominance preserving). The spread range of the obtained solutions is controlled by an additional parameter which might be not easy to configure.Said and Bechikh (2010) proposed another reference point based approach, the r-NSGA-II . In their study, the reference point is employed to modify the usual dominance principle, resulting in a new dominance relation, named r-dominance, which can be used to create a strict partial order over non-dominated solutions. The r-dominance relation prefers solutions that are closer to the specified reference point, and simultaneously preserves the order induced by Pareto dominance relation. The approach r-NSGA-II is derived from NSGA-II by replacing the Pareto dominance relation with the r-dominance relation. The algorithm has other two additional parameters δ and w. δ ∈ [0, 1] is used to control the range of the ROIs, and w expresses the bias of the DM. The performance of r-NSGA-II is assessed on a set of benchmarks ranging from 2 to 10-objective problems and is shown to be good in guiding the search towards both single and multiple ROIs. However, as pointed out by the authors, r-NSGA-II faces difficulties on multi-modal problems, such as ZDT4.Deb and Kumar (2007a) combined the reference direction with NSGA-II. The reference direction is incorporated into an achievement scalarizing function which is used to guide the search towards a preferred region. Multiple ROIs are obtained by specifying multiple reference directions. The efficiency of this approach is demonstrated on MOPs with up to ten objectives. Again, the spread range of the ROI is controlled by a user defined parameter.Deb and Kumar (2007b) also hybridized the light beam search method with NSGA-II. The hybridized approach is able to search for part(s) of Pareto optimal fronts illuminated by the light beam emanating from a starting point to the reference point with a span controlled by a threshold. This approach also performs well on MOPs with up to ten objectives. The light beam search is also hybridized with MSPSO algorithm (Wickramasinghe and Li, 2008; 2009). Again, the issue is how to appropriately control the spread range of the obtained solutions.Branke (2001) proposed a guided evolutionary multi-objective optimization approach, denoted as G-MOEA. In G-MOEA the DM preferences are manifested through a modification of the dominance relation, specifying an maximally acceptable trade-off rate between objectives, i.e. one unit improvement in objective fiis worth at most aijunits in objective fj. G-MOEA works well for two objectives. However, providing all pair-wise information for a problem with many objectives is cognitively intensive and needsM2−M2comparisons.Branke and Deb (2005) suggested a modified and controllable biased crowding approach. Their approach aims to search for a set of solutions that are parallel to an iso-utility function defined by a specified reference direction. Specifically, a parameter is applied to control the range of ROI along the Pareto optimal front. This parameter is defined as the ratio of the real distances between neighbouring solutions on the Pareto optimal front and the projected distance of the same solutions on a plane defined by a linear utility function.Zitzler et al. (2007) integrated weight preferences in the calculation of hypervolume indicator. The weighted hypervolume indicator serves as a means of integrating the DM preferences. Auger et al. (2009) applied this idea to HypE and proposed the weighted hypervolume based HypE. W-HypE is demonstrated to perform well in searching for preferred solutions for both bi and many-objective problems. The only issue is that the spread range of the ROI is controlled by a deviation parameter in the weight distribution function. Defining a proper value for this parameter is not easy for a decision maker.Karahan and Köksalan (2010) proposed a steady-state elitist evolutionary algorithm, named the territory defining evolutionary algorithm (TDEA). Similar to ɛ-MOEA (Deb et al., 2003), TDEA defines a territory around each individual so as to prevent crowding. A smaller territory corresponds to a denser coverage of solutions (i.e. more neighbouring solutions), and a larger territory corresponds to a sparser coverage of solutions. Based on TDEA, the authors developed an a priori approach, named prTDEA, in which the DM specifies his/her preferred region by a weight set. Solutions in the preferred region and non-interesting region are then assigned different territories such that more solutions are obtained in the preferred region(s).Most of the previously mentioned methods (e.g. MOGA, R-NSGA-II, G-MOEA) can be turned into interactive approaches simply by allowing the DM to adjust preferences and continue the optimization interactively. For example, Köksalan and Karahan (2010) proposed iTDEA as an interactive extension of the TDEA. In the iTDEA, the DM is asked to choose his/her preferred solutions from a set of representative solutions at each interaction. A territory is then defined around those preferred solutions so as to obtain more solutions around them, obtaining denser coverage of these interesting regions. This procedure continues till the algorithm finds a satisfactory solution. The iTDEA is tested on three problems using three different utility function types to simulate the DM responses. Experimental results show that iTDEA converges well to the DM simulated preferred regions.Indirect preference model in this study refers to that the DM preferences are not specified directly as numerical values like reference point, reference direction but are elicited by comparisons of solutions. Results of the comparison are then used to build a utility function to model the DM preference. In an indirect preference model based approach, the DM preferences are modelled based on obtained solutions. Thus, such approaches are often used interactively.Phelps and Köksalan (2003) proposed to elicit the DM preference by pairwise comparisons of solutions. This preference information is further used to obtain a ‘most compatible weight vector via linear programming methods, resulting in a linearly weighted sum utility function. This aggregate objective is optimized in the subsequent generations using an evolutionary algorithm till new comparisons of solutions are provided. This approach is one of the earliest interactive evolutionary multi-objective algorithm which paved a way to current interactive EMO approaches. Similar to Phelps and Köksalan (2003),Fowler et al. (2010) used a more general quasi-concave utility function to form the DM preference as a preference cone consisting of inferior solutions. Their approach is tested on multi-objective knapsack problems and is found to perform well.Jaszkiewicz (2007) also proposed to elicit the DM preference from pairwise comparison of solutions. However, this strategy does not aim to identify a single most likely utility function but, rather, simultaneously maintains a range of utility functions compatible with the elicited preferences. In other words, the preference information is not applied to create a single compatible weight vector but to reduce the set of possible weight vectors.Greco et al. (2008a) proposed another utility function based approach in which a logical preference model is built using the Dominance-based Rough Set Approach (DRSA). DRSA (Greco et al., 2001; 2008b) is a methodology of multiple criteria decision analysis which is used for structuring the DM preferences in terms of the most general and understandable ‘if …, then …’ decision rules. Their approach works as follows: once an approximation of solutions is obtained, the DM is then asked to indicate those relatively good solutions. Having this information, a preference model structured in terms of ‘if …, then …’ decision rules is induced using DRSA. This preference model is then applied to refine the obtained solutions, cutting off non-interesting solutions. The procedure continues until a satisfactory solution is found. Following the study of Greco et al. (2008a), Two other approaches, called DRSA-EMO and DRSA-EMO-PCI, are proposed in Greco et al. (2010). In these two approaches, the DM preferences are elicited by sorting some solutions in the current population into ‘relatively good’ and ‘others’, or by pairwise comparison of solutions, respectively. The main advantage of these three approaches is that the preference model is composed of a set of user-friendly decision rules.In Deb et al. (2010), the DM is asked to order a given set of alternatives from best to worst. This preference information is then used to model a strictly increasing polynomial value function (PVF). The constructed PVF is utilized to redefine the dominance principle, and drive the EMO approach (NSGA-II is applied in Deb et al., 2010) to search for preferred solutions for the subsequent iterations until the next ‘DM call’. Following the study of Deb et al. (2010),Sinha et al. (2010a) augmented the polynomial value function into a generalized polynomial value function that fits a wider variety of quasi-concave preference information. Also, in Sinha et al. (2010b), a polyhedral cone is used to construct the DM preference.Having reviewed these approaches, we can observe that (i) in direct preference model based approaches, the DM has to articulate his/her preferences by numerical values which, in some sense, might not be cognitively easy, and (ii) in indirect preference model based approaches, instead of using numerical values, the DM preferences are elicited via comparisons of solutions. This can be effective for 2 and 3-objective problems. However, when applied to MaOPs, the number of comparisons times required for ranking the solutions may increase significantly. Based on these observations, this study proposes a new method, namely, the brushing technique. Instead of specifying the numerical values or performing comparisons of solutions, it allows a DM to express his preference simply via drawing, i.e., brushing his preferred region in the objective space.This section describes our proposed method, wherein the brushing technique is used. Specifically, we elaborate how the PICEA-g, an EMO approach, can be turned into a hybrid evolutionary multi-criteria decision making approach, iPICEA-g, and how the brushing technique is used in iPICEA-g.PICEA-g is a co-evolutionary approach in which the usual population of candidate solutions is co-evolved with a set of goal vectors during the search (Wang et al., 2013a; 2013b). In this algorithm, candidate solutions gain fitness by Pareto dominating a particular set of goal vectors in the objective space, but the fitness contribution is shared between other solutions that also satisfy those goals. Goal vectors only gain fitness by being Pareto dominated by a candidate solution, but the fitness is reduced the more times the goals are dominated by other solutions in the population. Please refer to Appendix A for more details on the fitness calculation mechanism in PICEA-g.Inspired from this fitness assignment scheme, it is easy to imagine that if goal vectors are exclusively generated in an area, for example, the shaded region in Fig. 2, then candidate solutions inside this area would be encouraged in the evolution. This is because that these candidate solutions can dominate more goal vectors and so are more likely to gain higher fitness, while candidate solutions outside this area can only dominate few goal vectors and so are likely to have lower fitness. Therefore, candidate solutions will exhibit a tendency to approach to the Pareto optimal front enclosed by this area. Having known this, we can simply generate goal vectors in appropriate areas so as to guide solutions towards a region of interest to a DM. Fig. 2 illustrates that different sets of goal vectors lead to different solutions.Next we describe how the brushing technique is used in the iPICEA-g. Assuming that a set of solutions has been obtained, and is presented to the DM. Then, a region in the objective space is brushed by the DM. Fig. 3(a) illustrates the brushed region (the shaded ellipse) in Cartesian coordinate system. As the Cartesian coordinate system is not suitable for displaying high-dimension data, parallel coordinate system is used in MaOPs. Fig. 3(b) illustrates how a brushed region (the shaded band) is in the parallel coordinate system.From Fig. 3 shows that the brushing technique enables a DM to avoid the use of numerical values, instead, visually brush his preferred region. For bi- and tri-objective problems where solutions can be presented using the Cartesian coordinate system, the DM not only can visually express his preferences but also knows the location of his brushed region in objective space.22It is worth mentioning that brushing on the Cartesian coordinate system for tri-objective problems is not as straightforward as that is done for bi-objective problems due to the fact that we often do not use a three dimensional spherical display system, e.g., spherical computer monitor (Ritchey, 1992).For solutions that are presented using the parallel coordinate system, although the DM cannot know the location of his brushed region in the objective space, he knows the location of the brushed region in comparison to other solutions. By knowing such information, the DM could feel more confident on his selected region.Having obtained the brushed region, the next step is to build an area for generating goal vectors such that solutions can be guided towards the preferred region. For the convenience of illustration, this is discussed using a bi-objective case in the Cartesian coordinate system, see Fig. 4. Assuming that the DM brushes a region A, goal vectors are to be generated in the shaded area enclosed by z* and gmaxas shown in Fig. 4(a). z* is the estimated ideal point. gmaxis a vector that is estimated based on the objective vectors of the current non-dominated solutions f(S*), where S* refers to the current non-dominated solutions. Specifically, gmax= β × f(S*), β > 1. β is suggested as 1.5 through a set of preliminary tests. In order to describe this shaded area, two other parameters– w and θ, are needed, see Fig. 4(b). w describes a reference direction from the z* to the centre of A. Parameter θ measures the angle between the reference direction and the direction from the ideal point to the extreme point. An extreme point is defined as an objective vector that has the largest value in one objective. For example, P1 and P2 are two extreme points of the region A. θ can be calculated byarccos(Pz*→·P1′z*→). It is easy to understand that in parallel coordinate systems, the reference direction w and parameter θ can be calculated the same way.In addition, the area can also be constructed in other forms. For example, we can simply build the area as a rectangle (or a hyper-cube in MaOPs) using z* and gmax, see Fig. 5. However, as this study focuses on illustrating the integration of the brushing technique and the PICEA-g, effects of different methods are not discussed specifically, and will be left as a future study.Instead of using the Pareto dominance relation, the Pareto cone-dominance (Batista et al., 2011) is employed in iPICEA-g to determine whether a goal vector g is met by a candidate solution s or not. Specifically, g is said to be Pareto cone-dominated by s, if and only if the angle between the vectorf(s)g→and the vectorf(s)z*→is not larger than θ. For example, in Fig. 6, g1 is Pareto cone-dominated by candidate solution s1 while g2 is not Pareto cone-dominated by s1. The mathematical definition of the Pareto cone-dominance is presented in Appendix B. Compared with Pareto dominance, the use of Pareto cone-dominance can further emphasise the solutions along the reference direction, w, i.e., assigning higher fitness to these solutions. For example, in terms of Pareto dominance, s2 can satisfy some goal vectors and therefore might be retained in the evolution. However, s2 is not in the ROI. When using Pareto cone-dominance, s2 cannot satisfy any goal vector, having the lowest fitness and then would be more likely to be disregard in the evolution.Intuitively, driving the search towards a narrow region at the beginning of optimization process will cause a lack of population diversity and result in low search efficiency or converge to a local optimum (Coello et al., 2007, pp. 131–143). In order to avoid this problem, we start the search with a large search range θ′ and gradually, decrease θ′ to the preferred θ gradually. The decrease process is set in Eq. (1)(1)θuse=θ′−(θ′−θ)×(genmaxGen);where gen is the current generation, maxGen is the maximum generations, The benefits of this strategy will be illustrated in the subsequent section.Finally, the interaction of the analyst and the DM when using iPICEA-g is as follows:33The source code of iPICEA-g is available at https://drive.google.com/folderview?id=0B1Tvn88FrkeFdXpOY1NDY2RscVE&usp=sharing.(i)Run iPICEA-g without any preferences for a number of generations and to gain some basic information of the problem, e.g., the range of the Pareto front.The analyst elicits the DM preferences by asking him to brush his preferred region(s). According to the brushed region(s), goal vectors are generated in an appropriate area.Next, the algorithm sets the population size of candidate solutions and the number of goal vectors and the stopping criterion. Subsequently, iPICEA-g is performed till the stopping criterion is met.The obtained solutions are presented to the DM. If the DM is satisfied with the provided solutions then stop the search process. Otherwise, ask the DM to brushes new region(s), then return to (ii).

@&#CONCLUSIONS@&#
Incorporation of DM preference is an important part of a real-world decision support system. However, current methods for preference formulation are all based on numerical values, e.g., aspirations, reference direction/point or utility functions. This study proposes a method named the brushing technique which enhances the DM-friendliness by allowing preferences to be expressed simply via drawing, that is, the DM brushes his/her preferred regions directly in the objective space. Combining the brushing technique with an effective MOEA, i.e., the PICEA-g, a new hybrid evolutionarily multi-criteria decision-making approach, namely, iPICEA-g is proposed. The performance of iPICEA-g is examined on a set of test problems and is shown to be good.We have also identified three potential directions for future research. First, since decision-making is often a group rather than individual activity, it would be useful to develop a methodology to support a group decision making process. Also, since the DM preferences are often expressed in fuzzy linguistic terms (Jin and Sendhoff, 2002; Rachmawati and Srinivasan, 2010), it would be interesting to investigate how fuzzy preferences can be incorporated in our method. Concluding, the application of iPICEA-g to a real-world decision-making problem would be interesting as it has the potential of revealing issues that can further foster improvement of our method. A user friendly interface for using iPICEA-g is currently under development.