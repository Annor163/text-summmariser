@&#MAIN-TITLE@&#
Regularized generalized canonical correlation analysis for multiblock or multigroup data analysis

@&#HIGHLIGHTS@&#
Multiblock and multigroup data analysis methods are reviewed in the introduction.We propose a new version of the RGCCA algorithm so that it can be used for both multiblock and multigroup data.We remind that many multiblock data analysis methods are special cases of RGCCA.We show how to use RGCCA for multigroup data analysis.We illustrate RGCCA for multiblock and multigroup data on two real datasets.

@&#KEYPHRASES@&#
Multiblock data analysis,Multigroup data analysis,Regularized generalized canonical correlation analysis,

@&#ABSTRACT@&#
This paper presents an overview of methods for the analysis of data structured in blocks of variables or in groups of individuals. More specifically, regularized generalized canonical correlation analysis (RGCCA), which is a unifying approach for multiblock data analysis, is extended to be also a unifying tool for multigroup data analysis. The versatility and usefulness of our approach is illustrated on two real datasets.

@&#INTRODUCTION@&#
In this paper, we consider a data matrix X structured in groups (partition of rows) or in blocks (partition of columns). Rows of X are related to individuals and columns to variables. Multiblock data analysis concerns the analysis of several sets of variables (blocks) observed on the same set of individuals. Multigroup data analysis concerns the analysis of one set of variables observed on several groups of individuals. Note that there is no established consensus in the literature on the use of the terms “multiblock” and “multigroup”. Therefore these two terms are clearly defined in this paper.In the multiblock framework, a column partition X=[X1, …, Xj, …, XJ] is considered. In this case, each n×pjdata matrix Xjis called a block and represents a set of pjvariables observed on n individuals. The number and the nature of the variables usually differ from one block to another but the individuals must be the same across blocks. The main aim is to investigate the relationships between blocks. The data might be preprocessed in order to ensure comparability between variables and blocks. To make variables comparable, standardization is applied (zero mean and unit variance). To make blocks comparable, a possible strategy is to divide each block bypj(Wold, Hellberg, Lundstedt, Sjostrom, & Wold, 1987). This two-step procedure leads toTraceXjtXj=nfor each block.In the multigroup framework, a row partitionX=X1t,…,Xit,…,XIttis considered. In this framework, the same set of variables is observed on different groups of individuals. Each ni×p data matrix Xiis called a group. The number of individuals of each group can differ from one group to another. The main aim is to investigate the relationships among variables within the various groups. Following the proposal of Kiers and Ten Berge (1994) variables are centered and normalized (i.e. set to unit norm) within each group. This preprocessing leads toTraceXitXi=pfor each group.Many methods exist for multiblock and multigroup data analysis.Two families of methods have come to the fore in the field of multiblock data analysis. These methods rely on correlation-based or covariance-based criteria. Canonical correlation analysis (Hotelling, 1936) is the seminal paper for the first family and Tucker’s inter-battery factor analysis (Tucker, 1958) for the second one. These two methods have been extended to more than two blocks in many ways:(1)Main contributions for generalized canonical correlation analysis (GCCA) are found in Horst (1961), Carroll (1968a), Kettenring (1971), Wold (1982, 1985) and Hanafi (2007).Main contributions for extending Tucker’s method to more than two blocks come from Carroll (1968b), Chessel and Hanafi (1996), Hanafi and Kiers (2006), Hanafi, Kohler, and Qannari (2010, 2011), Hanafi, Mazerolles, Dufour, and Qannari (2006), Krämer (2007), Smilde, Westerhuis, and de Jong (2003), Ten Berge (1988), Van de Geer (1984), Westerhuis, Kourti, and MacGregor (1998), Wold (1982, 1985).Carroll (1968b) proposed the “mixed” correlation and covariance criterion. van den Wollenberg (1977) combined correlation and variance for the two-block situation (redundancy analysis). This method is extended to a multiblock situation in this paper.Regularized generalized canonical correlation analysis (Tenenhaus & Tenenhaus, 2011) includes many of these references as particular cases.For multigroup data analysis, we may distinguish three families of methods:(1) Several methods combine the covariance matrices Sior the correlation matrices Rirelated to the various groups.In a seminal paper, Levin (1966), considering the problem of simultaneous factor analysis, proposed the diagonalization ofR‾=(1/I)∑i=1IRi. The acronym SUMPCAcfor the Levin method was proposed by Kiers (1991). Kiers and Ten Berge (1994) proposed several simultaneous component analysis methods (see paragraph 3 below): one of them (SCA-P) leads also to the diagonalization ofR‾. Krzanowski (1984) proposed to carry out a multigroup PCA (MGPCA) by diagonalizing eitherT=∑i=1ISior the within group covariance matrixS=∑i=1I(ni/n)Si.Krzanowski (1979) proposed to use an approach similar to Carroll’s GCCA (correlation criterion) for comparing group correlation matrices R1, …, RI. Since GCCA on these matrices yields a trivial solution, Krzanowski replaced each matrix Riby its k first eigenvectors and then applied GCCA on the obtained matrices.Flury (1984) proposed a method called common principal component analysis (CPC) for I groups by supposing a special structure on the covariance matrices Σ1, …, ΣIdefined at the population level. In CPC, the I covariance matrices have the same eigenvectors but the eigenvalues are specific to each group: Σi=AΛiAtwhere A is orthogonal and Λidiagonal. Flury (1987) also proposed a partial common principal component analysis (PCPC) where only q eigenvectors of Σiare common to all populations. Flury’s approach is based on normal-theory maximum likelihood and a complicated iterative algorithm is required. Two alternative algorithms have been proposed: (1) Krzanowski (1984) showed empirically that MGPCA and PCPC give very close results; (2) a sequential least squares solution to PCPC can be obtained by using the CCSWA algorithm (Common components and specific weight analysis) described in Hanafi et al. (2006). It is also worth mentioning that CCSWA and HPCA (Hierarchical principal component analysis described in Westerhuis et al. (1998)) are two equivalent methods (Hanafi et al., 2010).(2) It is always possible to use a multiblock method on multigroup data by considering the transpose of each group. Eslami, Qannari, Kohler, and Bougeard (2013a) proposed to use an approach similar to Carroll’s GCCA (correlation and covariance criteria) on the transpose groupsX1t,…,XIt. This approach has later been extended to a multiblock/multigroup situation in Eslami, Qannari, Kohler, and Bougeard (2013b).(3) Kiers and Ten Berge (1989, 1994) and later Timmerman and Kiers (2003) proposed a generalization of PCA to a multigroup situation under the generic name of Simultaneous Component Analysis (SCA). Each data group Xiof dimension ni×p is modeled by a lower rank ni×p matrixX^i=XiWiPitwhere Wiis a p×q (q<p) weight matrix and Pia p×q pattern matrix. A factor matrix Fi=XiWiand a loading (or structure) matrix Li=RiWiare also introduced. PCA and SCA methods are about minimizing∑i=1IXi-XiWiPit2subject to specific constraints on the weight/pattern/structure/factor matrices which are summarized in Table 1.The reconstructed matrixX^i=XiWiPitis invariant up to an orthogonal (rotation) matrix A:XiWiPit=XiWiAtAPit. This invariance can be used to improve interpretation. Various rotation methods are described in Niesing (1997). Moreover, Niesing shows in a comparative study that SCA-P gives the best practical results.De Roover, Ceulemans, and Timmerman (2012), De Roover, Ceulemans, Timmerman, and Onghena (2013) and De Roover, Timmerman, Van Mechelen, and Ceulemans (2013) have developed a clusterwise approach to SCA-P, SCA-PF2, SCA-IND and SCA-ECP for tracing structural differences and similarities between different groups of individuals.Finally, Van Deun, Smilde, van der Werf, Kiers, and Van Mechelen (2009) proposed a simultaneous component analysis framework for multiblock and multigroup data analysis.In this paper, a modified version of RGCCA, which can be applied to multiblock and multigroup data, is described. The acronym RGCCA will be kept for this more general method. This paper is organized as follows: the general optimization problem for both multiblock and multigroup data analysis is presented in Section 2. A monotonically convergent algorithm is presented in Section 3. An overview of applications of RGCCA for multiblock and multigroup data analysis is given in Sections 4 and 5. The versatility and usefulness of our approach is illustrated on two real datasets in Sections 6 and 7.RGCCA for multiblock and multigroup data analyses is based on a single optimization problem that we present in this section. We consider I matrices Q1, …, QI. Each matrix Qiis of dimension m×pi. We also associate to each matrix Qia weight column vector wiof dimension piand a symmetric definite positive matrix Miof dimensions pi×pi. Moreover, a design matrix C={ciℓ} is defined with ciℓ=1 if matrices Qiand Qℓ are connected, and ciℓ=0 otherwise. The core optimization problem considered in this paper is defined as follows:(1)Maximizew1,…,wI∑i,ℓ,i≠ℓciℓg(〈Qiwi,Qℓwℓ〉)s.c.witMiwi=1,i=1,…,Iwhere 〈x, y〉=xty is the usual scalar product and g stands for the identity, the absolute value or the square function. By settingvi=Mi1/2wiandPi=QiMi-1/2optimization problem (1) becomes(2)Maximizev1,…,vI∑i,ℓ,i≠ℓciℓg(Pivi,Pℓvℓ)s.c.vitvi=1,i=1,…,IA monotone convergent algorithm can be developed for optimization problem (2). This algorithm will be presented in detail in the next section. It is worth mentioning that all the multiblock and multigroup methods to be presented in this paper are special cases of optimization problem (2).The following Lagrangian function related to optimization problem (2) is considered:(3)F(v1,…,vI,λ1,…,λI)=∑i,ℓ,i≠ℓciℓg(〈Pivi,Pℓvℓ〉)-φ∑i=1Iλi2vitvi-1where λ1, …, λIare the Lagrange multipliers, where φ=1 when g is the identity or the absolute value and φ=2 when g is the square function. We may suppose that〈Pivi,Pℓvℓ〉is different from 0, because if this were not the case, we would just set the design coefficient ciℓ to zero. Therefore, we may also consider the derivative g′ when g is the absolute value.Canceling the derivatives of the Lagrangian function with respect to viand λiyields the following stationary equations:(4)Pit1φ∑ℓ=1,ℓ≠iIciℓg′(Pivi,Pℓvℓ)Pℓvℓ=λivi,i=1,…,Iwith the normalization constraint(5)vitvi=1,i=1,…,I.These stationary equations have no closed-form solution, but they can be used to build a monotonically convergent algorithm for optimization problem (2).Wold (1982, 1985) proposed a PLS (partial least squares) algorithm for component-based structural equation modeling. This approach can be applied to multiblock data (Tenenhaus, Esposito Vinzi, Chatelin, & Lauro, 2005). This PLS algorithm has a severe drawback: its convergence properties are unknown, except for some specific situations. The RGCCA algorithm proposed in Tenenhaus and Tenenhaus (2011) is a slightly modified PLS algorithm with known convergence properties. The RGCCA algorithm is extended in this paper in order to cover the multigroup situation.It is useful to introduce the function w(x)=(1/φ)g′(x). The function w(x) is equal to 1 when g is equal to the identity, to x when g is equal to the square function and to sign(x) when g is equal to the absolute value. This function w(x) has a useful property: w(x)x=g(x) for g equal to the identity, to the square or to the absolute value. Indeed, if g is the identity w(x)x=x=g(x); if g is the square function w(x)x=x2=g(x); and if g is the absolute value w(x)x=sign(x)×x=∣x∣=g(x).Using the “PLS path modeling” terminology, viis a vector of outer weights, yi=Piviis called an outer component, and an inner component ziis defined as follows:(6)zi=∑ℓ=1,ℓ≠iIciℓw(〈Pivi,Pℓvℓ〉)PℓvℓThe inner component zirepresents the quantity between brackets in the stationary Eq. (4). For the various types of g functions, we obtain the following inner components:g=identity:zi=∑ℓ=1,ℓ≠iIciℓPℓvℓg=squarefunction:zi=∑ℓ=1,ℓ≠iIciℓ〈Pivi,Pℓvℓ〉Pℓvℓg=absolutevalue:zi=∑ℓ=1,ℓ≠iIciℓsign(〈Pivi,Pℓvℓ〉)PℓvℓThe inner component ziis useful to simplify stationary Eq. (4). Using the normalization constraint (5), stationary Eq. (4) become(7)vi=Pitzi/Pitzi,i=1,…,IIt is worth noting that stationary Eq. (7) are also obtained by considering the maximum of 〈Pivi, zi〉 with respect to a vector visubject to the normalization constraint ∥vi∥=1. This result is central to the proof of Proposition 2 below. The following proposition (similar to Proposition 1 in Tenenhaus & Tenenhaus (2011)) specifies the role of the inner components in the criterion to be maximized.Proposition 1For g equal to the identity, to the square, or to the absolute value, we obtain the following result:(8)∑i,ℓ=1,i≠ℓIciℓg(〈Pivi,Pℓvℓ〉)=∑i=1I〈Pivi,zi〉Equality (8) is obtained from the identity w(x)x=g(x) for g equal to the identity, to the square or to the absolute value:(9)∑i=1I〈Pivi,zi〉=∑i=1IPivi,∑ℓ=1,ℓ≠iIciℓw(〈Pivi,Pℓvℓ〉)Pℓvℓ=∑i,ℓ=1,i≠ℓIciℓw(〈Pivi,Pℓvℓ〉)〈Pivi,Pℓvℓ〉=∑i,ℓ=1,i≠ℓIciℓg(〈Pivi,Pℓvℓ〉)□It is possible to construct a monotonically convergent algorithm related to optimization problem (2); that is to say, the bounded criterion∑i,ℓ=1,i≠ℓIciℓg(〈Pivi,Pℓvℓ〉)to be maximized in (2) increases at each step of the proposed iterative procedure. Stationary Eq. (7) and Proposition 1 suggest an iterative algorithm for optimization problem (2):•Begin with arbitrary normalized outer weights vi, i=1, …, I.Compute the inner component ziaccording to formula (6).Compute new normalized outer weights using formula (7).Iterate this procedure.To obtain a monotonically convergent algorithm, it is necessary to use a sequence of operations similar to the ones used for RGCCA in Tenenhaus and Tenenhaus (2011). This PLS algorithm for optimization problem (2) is described in Algorithm 1. The procedure begins by an arbitrary choice of initial normalized outer weights vectorsv10,v20,…,vI0(step A in Algorithm 1). Suppose outer weights vectorsv1s+1,v2s+1,…,vi-1s+1have been constructed. The outer weight vectorvis+1is computed by considering the inner componentzisgiven in step B in Algorithm 1, and the formula given in Step C in Algorithm 1. The procedure is iterated until convergence of the bounded criterion∑i,ℓ=1,i≠ℓIciℓg(〈Pivi,Pℓvℓ〉)which is due to proposition 2 below.Proposition 2Letv1s,…,vIs,s=0,1,2,…be a sequence of outer weight vectors generated by the PLS algorithm for optimization problem(2). Let f be the function defined by(10)f(v1,v2,…,vI)=∑i,ℓ=1,i≠ℓIciℓg(〈Pivi,Pℓvℓ〉)The following inequalities hold:(11)∀sfv1s,v2s,…,vIs⩽fv1s+1,v2s+1,…,vIs+1The monotone convergence ofAlgorithm 1is guaranteed by inequality(11).See appendix. □A PLS algorithm for optimization problem (2).A. InitializationChoose I arbitrary normalized vectorsv10,v20,…,vI0.Repeat (s=0, 1, 2, …)Fori=1, 2, …, IB. Computing the inner componentzisCompute the inner component according to the selected type of g function:zis=∑ℓ<iciℓwPivis,Pℓvℓs+1Pℓvℓs+1+∑ℓ>iciℓwPivis,PℓvℓsPℓvℓswhere w(x)=1 when g is the identity, x when g is the square function and sign (x) when g is the absolute value.C. Computing the outer weight vectorvis+1Compute the outer weight vectorvis+1=Pitzis/PitzisEnd ForUntilfv1s+1,v2s+1,…,vIs+1-fv1s,v2s,…,vIs⩽ε.In the multiblock framework, a column partition X=[X1, …, Xj, …, XJ] is defined. Each n×pjdata matrix Xjis called a block and represents a set of pjstandardized variables observed on the same set of n individuals. In the event of unbalanced block size, it is possible to divide each block bypjto make them comparable. We associate with each block Xj, a block component Xjwj(where wjis a weight column vector of dimension pj) and a pj×pjpositive definite symmetric matrix Mj. The role of Mjwill be clarified later in the paper. Moreover, a design matrix C={cjk} is defined with cjk=1 if blocks j and k are related, and cjk=0 otherwise. The following optimization problem is considered:(12)Maximizew1,…,wJ∑j,k,j≠kcjkg(cov(Xjwj,Xkwk))s.c.wjtMjwj=1,j=1,…,Jwhere g stands for the identity, the absolute value or the square function. The block components being centered, cov(Xjwj, Xkwk) is equal to (1/n)〈Xjwj, Xkwk〉. Therefore optimization problem (12) is a special case of optimization problem (2) with Qi=Xiand m equals to the number n of individuals.For each block, higher dimension components can be obtained by using orthogonality constraints on the previous block components. Orthogonality is insured by considering the residual matrices in the regression of each block Xjon the previous block components (deflation process). Optimization problem (12) is then applied to these residual matrices to obtain higher dimension components.RGCCA, initially described in Tenenhaus and Tenenhaus (2011), is a special case of problem (12) withMj=τjI+(1-τj)1nXjtXj, the shrinkage constant τjvarying between 0 and 1. When τj=1 problem (12) gathers covariance-based multiblock methods; when τj=0 problem (12) gathers correlation-based multiblock methods; and when 0<τj<1 problem (12) insures a continuum between these two approaches. To follow the nomenclature of PLS path modeling in this paper, a choice of τj=1 is also called “mode A” and a choice of τj=0 “mode B”.All the various methods described in Tables 2–5are special cases of problem (12). It is quite remarkable that a single, very simple monotone convergent algorithm can be used for all those methods. A full description of these methods is given in Tenenhaus and Tenenhaus (2011) and references therein.In the multigroup framework, a row partitionX=X1t,…,Xit,…,XIttof the data matrix is considered. In this framework, the same set of variables is observed on different groups of individuals. Each ni×p matrix Xiis called a group and represents a set of p centered and normalized (i.e. unit norm) variables observed on a set of niindividuals. This normalization implies that (i) the correlation matrix Rirelated to Xiis equal toXitXiand (ii) the correlation matrix R related to the whole data matrix X is equal toR¯=(1/I)∑i=1IRi.The SCA approach for multigroup data analysis is global: for each group, q components are found in one step. However the weight matrix W (in SCA-W) or the pattern matrix P (in SCA-P, SCA-PF2, SCA-IND, SCA-ECP) must be the same for all groups, or column-wise proportional for the loading matrix L (in SCA-S).In this paper, we propose to use RGCCA for multigroup data analysis. This approach is more flexible than SCA as regards group-loading vectors in that they do not have to be the same across groups. The objective of RGCCA for multigroup data analysis is to find group-loading vectors with small angles and high norms. However, the group-loading vectors are obtained sequentially: the related group components are constrained to be orthogonal to the previous ones. A p×p positive definite symmetric matrix Miis associated with each Xi. A design matrix C={ciℓ} and a function g are defined as previously in problem (12).The following optimization problem is considered:(13)Maximizew1,…,wI∑i,ℓ,i≠ℓciℓg(XitXiwi,XℓtXℓwℓ)s.c.witMiwi=1,i=1,…,IThis optimization problem is a special case of optimization problem (2) withQi=XitXiand m and piboth equal to the number p of variables. VectorXitXiwiis called a group-loading vector. We also introduce the standardized group-loading vectorXitXiwi/‖Xiwi‖. Due to the unit norm of each column ofXi,XitXiwi/‖Xiwi‖represents the vector of correlations between the columns of Xiand the group component Xiwi.We now consider three typical situations which give a good idea of the generality and powerfulness of the optimization problem (13) for multigroup data analysis.We consider a situation where all groups are related: ciℓ=1 for all i≠ℓ. The matrixMi=XitXiis considered for each group. Therefore, in this situation, matrices Xiare supposed to be full rank (Miis supposed to be positive definite, see Section 2). For g equals to the identity, problem (13) becomes:(14)Maximizew1,…,wI∑i,ℓ,i≠ℓcos(XitXiwi,XℓtXℓwℓ)×XitXiwi×XℓtXℓwℓs.c.‖Xiwi‖=1,i=1,…,IAs the maximization ofXitXiwis.c. ∥Xiwi∥=1 yields the normalized first principal component of group Xi, problem (14) leads to a compromise between separate one dimension PCA’s of the various groups Xi’s and group-loading vectors with small angles. Due to the chosen normalization, the group-loading vectorsXitXiwirepresent the vectors of correlations between the various columns xijof group Xiand the group component Xiwi. Due to cosines being as high as possible between the group-loading vectors, interpretations of the various group components are expected to be similar.Higher dimension solutions are searched for by deflation on the previous components. Let us examine in detail the procedure for the second dimension. We denote bywi1,i=1,…,Ithe optimal solution for problem (14). The second dimension components are constrained to be orthogonal to the first dimension components. The second dimension solution is thus obtained by considering the following optimization problem:(15)Maximizew1,…,wI∑i,ℓ,i≠ℓcos(XitXiwi,XℓtXℓwℓ)×XitXiwi×XℓtXℓwℓs.c.Xiwi=1andwi1tXitXiwi=0,i=1,…,IAs any vector Xiwiorthogonal toXiwi1can be written asXi1wiwhereXi1is the residual matrix of the regression of XionXiwi1, problem (15) can also be written as(16)Maximizew1,…,wI∑i,ℓ,i≠ℓcos(XitXi1wi,XℓtXℓ1wℓ)×XitXi1wi×XℓtXℓ1wℓs.c.Xi1wi=1,i=1,…,IAsXi1is orthogonal toXi-Xi1, we obtain the following equality:XitXi1=Xi1tXi1. Therefore optimization problem (16) becomes equivalent to optimization problem (17):(17)Maximizew1,…,wI∑i,ℓ,i≠ℓcos(Xi1tXi1wi,Xℓ1tXℓ1wℓ)×Xi1tXi1wi×Xℓ1tXℓ1wℓs.c.Xi1wi=1,i=1,…,ITherefore, searching for the second dimension group-loading vectors boils down to replacing in optimization problem (14) each matrix Xiby the residual matrixXi1of the regression of Xion the first dimension group componentXiwi1. The same procedure is used for higher dimensions.Dataset matrices Xiare not necessarily full rank. When this full rank condition is not satisfied, we may modify problem (14) by using Mi=I for each group and consider this new problem:(18)Maximizew1,…,wI∑i,ℓ,i≠ℓcosXitXiwi,XℓtXℓwℓ×XitXiwi×XℓtXℓwℓs.c.‖wi‖=1,i=1,…,IAs the maximization ofXitXiwis.c. ∥wi∥=1 also yields the first principal component of group Xi, problem (18) also leads to a compromise between separate one dimension PCA’s of the various groups Xi’s and group-loading vectors with small angles. So, what is the difference between problems (14) and (18)? The criterion used in (18) can be re-written using standardized group-loading vectors and this yields the following expression:(19)∑i,ℓ,i≠ℓninℓcosXitXiwi,XℓtXℓwℓ×Var(Xiwi)×Var(Xℓwℓ)×∑j=1pcor2(xij,Xiwi)×∑j=1pcor2(xℓj,Xℓwℓ)Therefore, group sizes are taken into account in problem (18) and not in problem (14).Higher dimension solutions will be sought by deflation as explained above.We consider a super-group XI+1 equal to the row concatenation of the various groups Xifollowed by a normalization:XI+1=X1t,…,XItt/I. This insures that each column of XI+1 has a norm equal to 1. From this we deduceRI+1=XI+1tXI+1=1IX1tX1+⋯+XItXI=1I[R1+⋯+RI]=R‾We consider a situation where all groups 1 to I are only related to the super-group XI+1: ciℓ=1 for i=1, …, I, ℓ=I+1 and 0 otherwise. MatrixMi=XitXiis considered for each group and for the super-group.For g equals to the square function, optimization problem (13) becomes:(20)Maximizew1,…,wI+1∑i=1Icos2XitXiwi,XI+1tXI+1wI+1×XitXiwi2×XI+1tXI+1wI+12s.c.‖Xiwi‖=1,i=1,…,I+1.Optimization problem (20) leads to a compromise between the various one-dimension PCA’s of the Xi’s, the one-dimension PCA of XI+1 and small angles between the group-loading vectors and the super-group-loading vector. Due to the chosen normalization, the loading vectorsXitXiwirepresent the vectors of correlations between the various columns of each group Xiand their group component Xiwi. The solution of optimization problem (20) and useful properties are given in proposition 3 below.Proposition 3Letw1, …,wI+1be the solution of optimization problem(20). The following results are obtained:(a)The super-group-weight vectorwI+1is the eigenvector ofRI+1associated with the largest eigenvalue.The group-weight vectorwiis equal towI+1/∥XiwI+1∥.The super-group componenttI+1=XI+1wI+1is the normalized first principal component ofXI+1.The group componentti=Xiwiis equal toXiwI+1/∥XiwI+1∥; this means thattiis the normalized fragment oftI+1related to group i.The cosine between the group-loading vectorXitXiwiand the super-group-loading vectorXI+1tXI+1wI+1is positive. This means that all group-loading vectors are pointing out in the direction of the super-group-loading vector.Setting ti=Xiwi, i=1, …, I+1 optimization problem (20) can also be written as(21)Maximizet1,…,tI+1∑i=1IXitti,XI+1ttI+12s.c.‖ti‖=1,i=1,…,I+1.For a fixed vector tI+1, maximization ofXitti,XI+1ttI+1s.c. ∥ti∥=1 yields(22)ti=XiXI+1ttI+1/XiXI+1ttI+1and consequently(23)Xitti,XI+1ttI+1=XiXI+1ttI+1Therefore problem (21) becomes(24)MaximizetI+1tI+1tXI+1XI+1t2tI+1s.c.‖tI+1‖=1.The solution of problem (24) is obtained for the normalized first principal component of XI+1. This gives points (a) and (c). Using Eq. (22), we deduce points (b) and (d). Eq. (23) leads to point (e). □Remarks(1)It is quite remarkable that optimization problem (20) produces group-weight vectors wiall proportional to the super-group-weight vector wI+1, although this was not a constraint of the problem.Optimization problem (20) has already been considered in Eslami et al. (2013a).SCA-P and optimization problem (20) give the same results for q=1. This property is also verified for q>1 provided that the deflation procedure is carried out in optimization problem (20) on the super-group only.If orthogonality is required for higher dimension solutions at group and super-group levels, the RGCCA algorithm can be used. This will be illustrated in Section 7.2. SCA-IND and SCA-ECP yield also orthogonal components at the group level.RGCCA for multiblock data is illustrated on a dataset describing a tasting experiment on various red Loire wines. This dataset has already been analyzed using PLS path modeling in Tenenhaus and Esposito Vinzi (2005) and in Tenenhaus and Tenenhaus and Hanafi (2010).This dataset is fully described in Pagès, Asselin, Morlat, and Robichet (1987). It concerns a set of 21 red wines with Bourgueil, Chinon and Saumur origins, produced on 4 soils, and described by 27 variables grouped into four blocks (Olfaction at rest, View, Olfaction after shaking and Tasting). The description of the four blocks is given below:X1=Olfaction at restRest1=olfaction intensity at rest, Rest2=aromatic quality at rest, Rest3=fruity note at rest, Rest4=floral note at rest, Rest5=spicy note at rest.X2=ViewView1=visual intensity, View2=shading (from orange to purple), View3=surface impression.X3=Olfaction after shakingShaking1=olfaction intensity, Shaking2=olfaction quality, Shaking3=fruity note, Shaking4=floral note, Shaking5=spicy note, Shaking6=vegetable note, Shaking7=phenolic note, Shaking8=aromatic intensity in the mouth, Shaking9=aromatic persistence in the mouth, Shaking10=aromatic quality in the mouth.X4=TastingTasting1=intensity of attack, Tasting2=acidity, Tasting3=astringency, Tasting4=alcohol. Tasting5=balance (acidity, astringency, alcohol), Tasting6=mellowness, Tasting7=bitterness, Tasting8=final intensity in the mouth, Tasting9=harmony.Furthermore, a global quality measure is available for each wine. This is an illustrative variable to be used for interpreting the results of the multiblock data analysis.The main aim of multiblock data analysis is to investigate the relationships between blocks. We propose in this paper a two-step methodology. The first step consists in searching a set of sub-blocks of the various blocks such that all the variables belonging to this set of sub-blocks are well correlated. This set of sub-blocks is found by considering variables well correlated with the first principal component of the whole data table. The second step consists in applying RGCCA for multiblock data analysis to this set of sub-blocks (one of the methods given in Table 2) in order to modeling the relationships between the sub-blocks. Then we can repeat this methodology on the variables which have not been selected at the first step.Multiblock principal component analysis (MBPCA) applied to blocks X1, X2, X3, X4 (see Table 5, footnote c) plays an implicit central role in this step. It corresponds to the following optimization problem:(25)Maximizew1,…,w5∑j=14Cov2(Xjwj,X5w5)s.c.‖wj‖=1,j=1,…,5.where X5 is the super-block [X1, X2, X3, X4].It is a special case of RGCCA with mode A (special case of problem (12) with Mj=I) for the four original blocks and for the super-block and g equals to the square function. It is worth pointing out that the RGCCA algorithm has not to be used because problem (25) is similar to problem (20) above. For a fixed vector w5, the solution of optimization problem (25) is obtained forwj=XjtX5w5/XjtX5w5.Using the following expression(26)∑j=14Cov2(Xjwj,X5w5)=∑j=14w5tX5tXjXjtX5w52/XjtX5a52=∑j=14w5tX5tXjXjtX5w5=w5tX5tX52w5optimization problem (25) boils down to maximizingw5tX5tX52w5s.c. ∥w5 ∥=1.(1)The super-block component X5w5 is exactly the first principal component of the super-block X5 and the vectors wj’s are the normalized fragments of w5 related to block Xj. In fact, an “average” one-component PLS regression between the super-block and each original block is equivalent to separate one-component PLS regressions between the first principal component of the super-block and each original block.Optimization problem (25), multiple co-inertia analysis (MCoA) (Chessel & Hanafi, 1996) and consensus PCA (version CPCA-W by Smilde et al., 2003) have similar solution: the super-block component is the first principal component of the super-block (usual one for problem (25) and normalized one for MCoA and CPCA-W); block components are the same for the three methods.We may consider that the variables well-correlated with the first principal component of the super-block are defining sub-blocks which are interconnected.The set of related sub-blocks is well summarized by the first principal component of the “super-sub-block”. This is intuitive, but also justified by MBPCA which takes into account the block structure.Results of PCA on the whole dataset are given in Table 6. We decide to study in this section the set of sub-blocks built with variables correlated to the first principal component and to global quality.We may use the SUMCOV method (see Table 2) to relate the sub-blocks. The following optimization problem is considered:(27)Maximizew1,…,w4∑j,k=1,j≠k4Cov(Xjwj,Xkwk)s.c.‖wj‖=1,j=1,…,4.RGCCA with mode A for all blocks and g equals to the identity is used to solve optimization problem (27). This problem (27) can be considered as an “average” one-component PLS regression between the various pairs of blocks Xjand Xk.The multiblock analysis has been carried out with the RGCCA R package (Tenenhaus & Guillemot, 2013). RGCCA is also available in the PLS-PM option of the XLSTAT software (Addinsoft, 2013). Using the SUMCOV criterion on the set of sub-blocks selected in step 1, we obtain the results given in Fig. 1. Based on a bootstrap strategy, all correlations which appear in Fig. 1 are significant.We summarize all the variables appearing in Fig. 1 by their standardized first principal component. Using this component (labeled Factor 1) and global quality, we obtain the graphical display shown in Fig. 2: wines are visualized using soil markers (actually, appellation is not a good indicator of quality for these Loire wines). High quality wines coming from the reference soil are well recognized by the experts. The reader interested in wine can even detect that the Saumur 1DAM is one of the best wines from this sample. We can testify that we drank outstanding Saumur-Champigny produced at Dampierre-sur-Loire.

@&#CONCLUSIONS@&#
