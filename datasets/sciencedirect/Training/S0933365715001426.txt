@&#MAIN-TITLE@&#
The feature selection bias problem in relation to high-dimensional gene data

@&#HIGHLIGHTS@&#
We analyze seven gene datasets to show the feature selection bias effect on the accuracy measure.We examine its importance by an empirical study of four feature selection methods.For evaluating feature selection performance we use double cross-validation.By the way, we examine the stability of the feature selection methods.We recommend cross-validation for feature selection in order to reduce the selection bias.

@&#KEYPHRASES@&#
Feature selection bias,Convex and piecewise linear classifier,Support vector machine,Gene selection,Microarray data,

@&#ABSTRACT@&#
ObjectiveFeature selection is a technique widely used in data mining. The aim is to select the best subset of features relevant to the problem being considered. In this paper, we consider feature selection for the classification of gene datasets. Gene data is usually composed of just a few dozen objects described by thousands of features. For this kind of data, it is easy to find a model that fits the learning data. However, it is not easy to find one that will simultaneously evaluate new data equally well as learning data. This overfitting issue is well known as regards classification and regression, but it also applies to feature selection.Methods and materialsWe address this problem and investigate its importance in an empirical study of four feature selection methods applied to seven high-dimensional gene datasets. We chose datasets that are well studied in the literature—colon cancer, leukemia and breast cancer. All the datasets are characterized by a significant number of features and the presence of exactly two decision classes. The feature selection methods used are ReliefF, minimum redundancy maximum relevance, support vector machine-recursive feature elimination and relaxed linear separability.ResultsOur main result reveals the existence of positive feature selection bias in all 28 experiments (7 datasets and 4 feature selection methods). Bias was calculated as the difference between validation and test accuracies and ranges from 2.6% to as much as 41.67%. The validation accuracy (biased accuracy) was calculated on the same dataset on which the feature selection was performed. The test accuracy was calculated for data that was not used for feature selection (by so called external cross-validation).ConclusionsThis work provides evidence that using the same dataset for feature selection and learning is not appropriate. We recommend using cross-validation for feature selection in order to reduce selection bias.

@&#INTRODUCTION@&#
Microarray technology has been one of the most important technological developments in biology in recent years [1]. It aims at the measurement of mRNA levels in particular cells or tissues for many genes at once. The application of microarray technology is very wide, ranging from the understanding of underlying biological mechanisms [2], the examining of drug response [3], the discovering of novel subgroups of diseases [4,5], the classifying of patients into disease groups [4] to the predicting the outcomes of a disease [6,7] and many other potential application areas. One of the major problems with microarray datasets is their very high dimensions. Each object of the dataset is usually described by thousands of genes. The classification or any other exploration task of high-dimensional datasets can be difficult due to the “curse of dimensionality” [8]. The feature selection can help in these cases.Feature selection is a technique widely used in data mining, and the aim is to select a subset of features from all the available features that are relevant to the problem being considered [9,10]. The best subset should contain the minimum number of features that have the greatest impact on the quality of the model relating to the problem. Other features should be rejected as being irrelevant. In the case of a classification problem, the feature selection algorithm should select a subset of features to obtain the best properties of the classifier, for example, allowing one to obtain the best separation of decision-making classes measured on the basis of the established criterion.There are three major categories of feature selection methods—filter, wrapper and embedded. Filter methods use some of the internal properties of the data to select and score features, which is usually done on an individual basis. Examples of such properties are feature dependence, redundancy and the entropy of distances between data points. In wrapper methods, the feature selection is connected with another data analysis technique, usually classification. The accompanying technique helps in evaluating the quality of the selected features sets. Embedded methods of feature selection integrate the selection in model building. An example of such a method is the decision tree induction algorithm, where a feature has to be selected at each node [10]. When feature subspace is selected the next step is to build a classifier and evaluate its performance.It is not appropriate to use the same data to build a model and to evaluate its performance. The performance measure obtained that way constitutes a positive bias. For new data, we would probably see a worsening in performance. In the case of feature selection, it is necessary to select features from the whole dataset and then build the decision model (e.g. classifier) and estimate its performance vis-à-vis the selected features. Even if cross-validation is used to measure model quality, it is too optimistic because we look at the data before building the model—in this case conducting feature selection. In the literature, it is sometimes called feature selection bias [11] and it also can be seen as an example of so called “data snooping” [12]. Data snooping occurs when a given set of data is used more than once for purposes of inference or model selection—in this case twice, first for feature selection and later for classification. Here, we analyse several commonly used high-dimensional gene datasets to show the feature selection bias effect for quality measure. We measure unbiased accuracy using external cross-validation where the whole process of feature selection is repeated.The quality of the subset of selected features is usually measured based on the performance of classifiers [10]. A relatively often neglected issue is the stability of feature selection. Such stability could be described as the sensitivity to perturbations in the data. The interest in the stability of feature selection algorithms is because biomarker experts usually prefer small subsets of features. These subsets of features should be relatively non-sensitive to slight changes in the data. Sometimes, classifier performance might be of less interest to them.The goal of this paper is to present an empirical study where the feature selection bias is shown in these datasets, which are related to medical applications. We propose a framework of calculation that would avoid feature selection bias and present a series of experiments in which we use the latest feature selection methods to analyse genetic datasets. In addition, because our proposed framework of calculation repeatedly uses the same feature selection method to analyse slightly different datasets, we have complemented the study of feature selection bias with a stability analysis in order to verify the quality of solutions: a solution that drastically changes in the presence of a very minor variation of data can be less reliable.To summarize, in this paper:•We empirically study the feature selection bias in high-dimensional datasets. We analyse seven gene datasets and four feature selection methods to show the feature selection bias effect on the accuracy measure.We propose a calculation scheme that uses a double cross-validation in order to avoid feature selection bias.We examine the stability of the feature selection methods.The rest of the paper is organized as follows: In Section 2, a short overview of related works on feature selection, its stability and bias is given. In Section 3, we detail a feature selection technique based on the minimization of the convex and piecewise linear (CPL) criterion function, the smaller details of the other feature selection algorithms included in the study and also the feature selection stability measure. In Section 4, we describe the datasets used and the experimental setup. The proposed approach is experimentally evaluated on real gene expression data. In Section 5, we present the results of the experiments and an interpretation. Finally, the concluding remarks outline the main findings of this work.

@&#CONCLUSIONS@&#
