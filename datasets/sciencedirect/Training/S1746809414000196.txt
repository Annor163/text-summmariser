@&#MAIN-TITLE@&#
Early detection of liver disease using data visualisation and classification method

@&#HIGHLIGHTS@&#
An automated diagnostic method for early-stage liver disease in the general population was proposed.A principal model which combined support vector data description (SVDD) with data visualisation techniques is established to improve diagnostic accuracy.We solve the classification problem in which the normal samples greatly outnumber the abnormal ones for dataset from the general population.A visualisation based on dimensionality reduction is also provided in the medical field.

@&#KEYPHRASES@&#
Machine learning,Liver disease,Classification,SVDD,Visualisation,GSO,

@&#ABSTRACT@&#
Detection of early-stage liver diseases is a challenge in medical field. Automated diagnostics based on machine learning therefore could be very important for liver tests of patients. This paper investigates 225 liver function test records (each record include 14 features), which is a subset from 1000 patients’ liver function test records that include the records of 25 patients with liver disease from a community hospital. We combine support vector data description (SVDD) with data visualisation techniques and the glowworm swarm optimisation (GSO) algorithm to improve diagnostic accuracy. The results show that the proposed method can achieve 96% sensitivity, 86.28% specificity, and 84.28% accuracy. The new method is thus well-suited for diagnosing early liver disease.

@&#INTRODUCTION@&#
Liver disease is one of the most common diseases. Most of the risk factors for liver disease and liver cancer are discussed in [1]. Worldwide, Liver cancer is the fifth most commonly diagnosed cancer in men and the seventh in women [2].According to Department of Health statistics, liver disease has consistently been among the top 10 fatal diseases in Beijing. In 2010, liver cancer accounted for 11.3% of cancer mortality in men and is second only to lung cancer. Symptoms are easily overlooked in the initial stage, and by the time they appear, the patient has missed the best opportunity for treatment. Effective early diagnosis is therefore of paramount importance, and automation of this diagnosis is highly desirable. Much effort has been devoted to solving this problem.Automated diagnostics for liver disease have been intensively researched in recent years, and many methods have been proposed and applied [3,4,1,5–8]. Data mining (DM) techniques, such as artificial neural networks (ANN), intelligent algorithms, fuzzy sets, and support vector machines (SVMs), have recently found medical application. Most of this work falls into two broad categories: diagnostics for patients and early detection for the general population.On the one hand, Nakano were the first to propose a machine learning technique in which artificial neural networks differentiate between the two subtypes of chronic active hepatitis, mild and severe, based on five blood biochemical parameters [3]. The ANN network correctly diagnosed 78% of the cross validation control group with data from 31 patients. Onisko used a probabilistic causal model that included 16 liver disorders and 40 features in her initial report of the diagnostic performance test results [4]. In the long term, the model may be unsuitable if it misses major disorders that require immediate attention, such as liver cancer. Growth curve analysis using logistic regression, decision tree, and neural networks showed that liver disease was accurately diagnosed in 72.55% of the cases and that the sensitivity was 78.62% when a neural network was used [1]. In the pattern recognition literature, Comak presented a hybrid method based on combining least square support vector machines (LSSVM) with fuzzy weighting preprocessing to diagnose liver disorders using the standard BUPA Liver Disorders Dataset benchmark [5]. The highest classification accuracy yet reported for this dataset is 94.29%. Based on the same dataset, Polat proposed a classification algorithm based on a fuzzy artificial immune recognition system (AIRS) for liver diagnostics [6]. The accuracy was 83.36%, but the algorithm gained an advantage by means of a shorter classification time. In-depth research was carried out both for patient diagnosis and for early prevention in the general population.On the other hand, Lin applied CART (classification and regression tree) and CBR (case-based reasoning) techniques to the diagnosis of early stage liver disease [7], achieving a diagnostic accuracy of 90%. Lin further constructed an intelligent liver disease diagnostic model capable of distinguishing among the various types of liver disease [8]. Ramana evaluated four classification algorithms, which include a naïve Bayesian classifier, C4.5 (the decision tree learner), back-propagation neural network, and SVMs, for the classification of two liver patient datasets based on four criteria [9]. The results of an exploratory analysis of USA and INDIA liver disease datasets were reported in [10].Most previous research on the development of liver disease diagnosis models uses datasets in which the normal and abnormal samples are comparable in sizes. However, in early diagnosis for realistic populations, we often need to solve the classification problem when the normal samples greatly outnumber the abnormal ones. By data visualisation, we refer to the reduction of number of features (14 in our dataset) into 2 or 3 dimensions, so that the samples can be plotted on a graphical representation. In such a manner, the two classes of samples (normal and abnormal) can be visually distinguished. Data visualisation combined with traditional machine learning plays a central role in an intelligent diagnostic model. This model requires an inter-play between automated analysis and human judgement.Principal component analysis (PCA) and Independent Component Analysis (ICA) are linear methods for dimensionality reduction. Manifold learning is a nonlinear dimensionality reduction approach. Locally linear embedding (LLE) and isometric feature mapping (ISOMAP) have been frequently applied for dimensionality reduction. Recently, various dimensionality reduction methods in the machine learning were reported such as self-organizing maps (SOMs), Hessian LLE, diffusion maps, Laplacian eigenmaps, and others [11–15]. Visualisation methods that reflect this inherent structure to support the user during the process of dimensionality reduction are therefore becoming extensively used in more and more fields [16–18]. Nevertheless, data visualisation has yet to find wide application in the medical field.The present paper presents a data visualisation and classification method for early-stage diagnosis of liver disease in the general population based on a realistic dataset. A dimensionality reduction technique is first applied to visualise the essential signals after a reduction to two or three dimensions. The method then employs a classification algorithm to distinguish between healthy and diseased livers based on a realistic dataset used for machine learning in medical diagnostics.This paper is organised as follows. Section 2 describes our clinical dataset, the dimensionality reduction methods used for visualisation, and the diagnostic methods. Section 3 describes the two steps in liver disease diagnostics: visualisation (Section 3.1) and actual diagnosis (Section 3.2). Section 4 has the discussion. In this section, the new method is applied to a realistic and compared with some other liver diagnostic methods. Section 5 summarises our conclusions.Data from liver function tests for 1000 patients in a community hospital in Beijing was collected. It comprises of 225 records containing 14 features (Table 1) and 775 records containing 4 features. Patients with abnormal results in first step will be required to do further checking. In this study we selected the 14 features 225 records as our investigation objects. The data contains long-term tracking records for 150 healthy individuals and 25 patients with liver disease.Following the assistance of medical experts, we reconstructed the dataset (Table 2) that includes 175 samples with 13 features and one class label; there are 130 male and 45 female cases; there are 150 normal and 25 abnormal samples. Compared with Table 1, the dataset omits CHOL and TRIG but adds Age and Gender as criteria.PCA [19] is based on the assumption that important signals contribute most to variance and are captured by the first few principal components with a relatively high contribution ratio. PCA is a linear coordinate transformation that rotates a coordinate system in a particular way that does not suffice to extract all of the important signals. ICA [20] comprises a broad field of techniques that aim to decorrelate and maximise the independence of the data with linear mixing of a suitable underlying matrix whose rows are mutually independent and not normally distributed.Two novel nonlinear methods have been proposed to tackle the dimensionality reduction problem, namely LLE [21] and Isomap [22]. Both of these methods attempt to preserve the local neighbourhood of each object. An unsupervised learning algorithm, LLE, is based on the intuition that even though the columns are points in n-dimensional Euclidean space, the points might actually lie on a much lower dimensional manifold. Isomap [22] determines which points are neighbours on the manifold, finds the intrinsic geometry of the data (as captured in the geodesic manifold distances between all pairs of data points), and finally, applies some classical techniques of MDS (multidimensional scaling), PCA, LDA (linear discrimination analysis), and others to the matrix of graph distances. To permit visualisation, Isomap maps an input data set onto a two- or three-dimensional space that preserves the intrinsic manifold as far as possible.It is a challenge in dimensionality reduction to explain the mapping from the input matrix to the output matrix [23]. We present four visualisation methods that assist the interpretation by comparing their classification results in the two- or three-dimensional space after dimensionality reduction.The SVDD [24,25] is inspired by the global optimisation problem and SVMs first presented at the Fifth Annual ACM Workshop on Computation Learning Theory (COLT) for the first time. SVDD is a method for one-class classification to obtain an accurate estimate of a set of observations [26]. In general, the data description problem differs from classification problems with dual-class or multi-class classification. SVDD uses a single type in a normal data set to separate the set from a few abnormal data (outliers) and is a popular kernel method for outlier detection. The method tries to fit one-class data with a super-sphere. SVDD is an effective classification algorithm. The method is made robust against outliers in the training set and is capable of tightening the description by using negative examples. SVDD is dominantly used in datasets with a small portion of abnormal data. We select SVDD because liver disease dataset is mostly of this kind. Our empirical results show that SVDD work on the dataset with a good performance.To begin, we assume vectors x are column vectors and have a data set(1){xi}⊆χ,χ⊆ℜdwhere i=1,…, n. Here, n is the vector index, and d is the dimension of the vector. The data set is mapped to a high dimensional feature space, where we try to search for a minimal enclosing hyper-sphere by means of a Gaussian kernel function. Soft constraints are described by(2)Φ(xj)−a2≤R2+ξj∀j,ξj≥0,where |||| is the Euclidean norm, a is the centre of the hyper-sphere, R is the radius, and ξ is a slack variables.The minimum hyper-sphere can be taken in the form(3)R2+C∑jξj,where C is a penalty constant. This optimisation problem is solved using Lagrange multipliers. We set the derivative of Lagrangian with respect to R, a, and ξ equal to zero, and transform it into the Wolfe dual form using the Lagrange multipliers αj:(4)W=∑jΦ(xj)2αj−∑i,jαiαjΦ(xi)⋅Φ(xj)with0≤αj≤C,∑jαj=1,a=∑jαjΦ(xj)We represent the dot products by a positive definite kernel, or Mercer kernel K(xi, xj). In this paper, we use the Gaussian kernel:(5)Φ(xi)⋅Φ(xj)=K(xi,xj)=e−qxi−xj2,with width parameter q. The Wolfe dual form becomes(6)W=∑jK(xj,xj)αj−∑i,jαiαjK(xi,xj)In view of (2), (4) we have:(7)R2(x)=K(x,x)−2∑jK(xj,x)αj+∑i,jαiαjK(xi,xj)We see from (7) that when the radius R(xi) is equal to R, xiis a support vector. Therefore, some support vectors (SVs) lie on the sphere (0<αj<C), the bounded support vectors (BSVs) are outside the sphere (αj=C), and the other points are inside the sphere (αj=0).The GSO algorithm is a new nature-inspired heuristics for optimisation problems. Krishnanand proposed it in 2005 as a derivative-free meta-heuristic algorithm that mimics the glow behaviour of glowworms [27]. The algorithm combines rules and randomness to imitate some natural phenomena. Each artificial glowworm, namely agent, carries a light on two-dimensional spaces and has its own local decision range that depends on the number of neighbours. The agents are assumed to carry a luminescence quantity called luciferin along with them. Briefly, the algorithm involves in three phases: luciferin update phase, movement phase, and decision range update. The brighter agent has a better position. The higher luciferin intensity the neighbour has, the more attraction it gains within the local decision range. While the neighbour-density is low, the range is enlarged in order to find more neighbours.The algorithm shares some common features with ant colony optimisation (ACO) and particle swarm optimisation (PSO), but it has an efficient running algorithm, which is a significant advantage.The basic procedure of the proposed method is shown in Fig. 1. There are three steps:(1)Feature selection. Feature selection chooses distinguishing features from the data samples. In this paper, we use input sets with 13 features.Data visualisation. In this paper, we choose four visualisation methods with PCA, ICA, LLE and Isomap.GSO algorithm and SVDD. The width q of the Gaussian kernel and the penalty constant C are the two adjustable parameters in the SVDD algorithm, which employs the GSO algorithm for parameter optimisation.The proposed method includes two key steps: data visualisation, and SVDD.

@&#CONCLUSIONS@&#
