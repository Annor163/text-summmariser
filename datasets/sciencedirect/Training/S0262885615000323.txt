@&#MAIN-TITLE@&#
Local color transformation analysis for sudden illumination change detection

@&#HIGHLIGHTS@&#
Illumination change detection is carried out without assuming a specific color transformation.The method is designed to improve existing standard foreground detection algorithms.Backup procedures are designed to recover from detection failures.The method is applicable to a wide range of varying illumination conditions.

@&#KEYPHRASES@&#
Background modeling,Foreground detection,Illumination invariance,Color transformation,

@&#ABSTRACT@&#
Graphical abstract

@&#INTRODUCTION@&#
The segmentation of the foreground objects in a scene is a fundamental task which lays at the foundation of many computer vision systems. Most approaches to this task create a model of the background that is updated progressively as time passes. Consequently sudden illumination changes are challenging problems, since the appearance of the background no longer matches the past observations. This issue has been studied for many years [6,9,10,20] due to its relevance to applications. Practical computer vision systems are placed in environments where the illumination conditions vary through time. For example, indoor scenes frequently exhibit light switching, while passing clouds affect outdoor cameras.There are different ways to address this problem. On one side there are pixel-level algorithms that work by analyzing the scene on a pixel by pixel basis, so that an independent decision is made for each pixel. On the other hand, there are other approaches which work at a higher level, namely block-level (where the decision for one pixel depends on the information from several nearby pixels) or frame-level (where all the pixels of the frame might be taken into account to decide whether a pixel belongs to the foreground). It is often the case that methods use multiple levels simultaneously to analyze the video. A representative case of this approach is the Wallflower system 28, which uses the three levels already mentioned. One of the downsides attributed to this method is that it uses a non flexible criterion in real situations because it selects a set of background scene models representing different situations and each frame has to be assigned to the model which produces the fewest foreground pixels. This implies that each possible situation must be predicted in advance, and that a representative background model of each situation must be found. Hence, the approach is less adequate for scenes where unpredictable events occur which affect the background, such as a left object or a parked car which starts moving.There are other methods using region-level analysis as in the case of [22]. This approach analyzes the image at block and pixel levels. The main idea is that each pixel belongs to several overlapping blocks, so that it is determined whether it belongs to the background or not depending on how it has been classified in each of these blocks.A more recent algorithm is [14], which uses pixel-level and image-level elements. The proposal consists in using a two-layer architecture based on a Gaussian Mixture Model to represent the background. Then the result is optimized using a Markov random field decision framework.On the other hand, there are many algorithms that work at the pixel level. For example [27] is based on applying a homomorphic filter, while [12] performed an analysis with stereo vision and employs a disparity model created offline to mitigate the consumption of extra CPU time required for this type of processing. Also, [8] uses discriminative texture features to capture background statistics, by means of texture operators named local binary patterns (LBP). Finally, [6] presents an adaptive algorithm that uses multiple feature subspaces and Principal Component Analysis to capture and learn different lighting conditions.Our aim here is to develop an illumination change detection system which works along an existing foreground detection algorithm. A previous example of an add-on illumination change detection algorithm can be found in [32]. Unlike our proposal, they assume that the order of pixel values is preserved in local neighborhoods when illumination changes occur, and they root their proposal on the analysis of physical properties, namely the radiance. In contrast to this, we are not interested in the particular form of the color transformation, but in its smoothness properties. This way, we consider that any color transformation which is not smooth can not correspond to a lighting change, i.e. it is due to a foreground object. Hence, we are able to detect variations in illumination irrespective of the particular features of the color transformation at hand. Moreover, the procedure is largely independent from the baseline background model, so it can be used to improve a number of well known methods which are not specifically designed to work under illumination changes.This paper is organized as follows. Section 2 presents the illumination change detection method. Section 3 presents some experimental results to demonstrate the ability of our approach to manage complex scenes. The main features and properties of our proposal are discussed in Section 4. Finally, Section 5 is devoted to conclusions.The illumination change management procedure that we present here has two parts. The first one classifies the pixels of the current frame according to its current state with respect to illumination changes (Subsection 2.1). The second part uses this information to decide which pixels must undergo a reset because an illumination change has rendered their background models outdated (Subsection 2.2).Here we must estimate the illumination state of each pixel of the current video frame. The illumination state of pixel i is formed by three fuzzy variables Rough, Difference, and Baseline; their values (membership degrees) will be noted αi,βi,γi∈[0,1], respectively. The interpretation of the variables is as follows:•Rough indicates whether the transformation of the colors in the previous frame to the colors in the current frame is not smooth in the vicinity of pixel i. If αiis high, then it is unlikely that an illumination change is happening, since illumination changes produce smooth changes in the colors of the background and the foreground objects.Difference indicates whether the color of pixel i in the current frame is very different from that stored in the background model for pixel i. If βiis high, then either an illumination change is happening or a foreground object is present.Baseline indicates whether the baseline background model has detected a foreground object. Please note that γi∈{0,1} for background models that do not output a degree of confidence for the foreground detection.Next we describe how to compute the fuzzy membership values αiand βi; please note that γiis simply the output of the baseline background model.Let us center our attention in a small neighborhood Wiof pixel i. In our experiments we have considered square windows of size 5×5pixels, which offer a good tradeoff between efficiency and accuracy. Now we define a vector field which represents the transformation of the colors of the neighborhood Wiin the background model to the colors of Wiin the current frame:(1)fi:ℝ3→ℝ3where tristimulus color values are assumed, but any color space could be used. Now, our task is to detect those vector fields fiwhich are not smooth. A simple and fast way of measuring the smoothness of fiis based on the computation of the following quotients:(2)qijk=aijkbijk(3)aijk=fixj−fixk2(4)bijk=xj−xk2where xjand xkare the colors in the background model of two pixels j,k∈Wi, and fi(xj) and fi(xk) are the colors of those pixels in the current frame. The vector field fiis non smooth whenever qiattains high values for some pairs j,k∈Wi. As seen in Fig. 1, a smooth transformation is one that maps similar colors in the background model to similar colors in the current frame, even if the colors change considerably from the background model to the current frame. That is, the distances ‖xj−fi(xj)‖ are irrelevant to the smoothness of fi. This way we can manage the switching of lights of any color, for example yellow or blue lights. It must be pointed out that low values of qiare not interesting because they are usually associated with homogeneous foreground objects passing in front of textured backgrounds, which are adequately managed by standard foreground detection algorithms.In practice pixel noise can lead to large errors in the estimation of qi, in particular if the denominator bicontains noise. We alleviate this by considering a filtered quantity ϕi:(5)ϕijk=aijkBlowifbijk<Blow0ifbijk>Bhighqijkotherwisewhere Blowand Bhighare suitable lower and upper thresholds for the denominator value bi, with Blow<Bhigh. Please note that Blowmanages low lighting conditions, where pixel values have little precision. On the other hand, Bhighensures that highly textured backgrounds (which are associated to high values of aiand bi) are not mistaken as non smooth color transformations. Finally we use the highest ϕi(j,k) as a measure of the roughness of fi:(6)αi=min1,1Kαmaxj,k∈Wiϕijkwhere the min function ensures that αi∈[0,1], and Kαare a suitable scaling parameter.On the other hand, the second membership value βiis obtained from the squared Euclidean distance between the color of pixel i in the current frame and the color stored in the background model for pixel i:(7)βi=1Kβx0−fix02where x1 and f (x1) are the colors of the pixel at hand in the background model and in the current frame, respectively; and Kβis another scaling parameter.The possible values of the three fuzzy membership variables are mapped to eight possible illumination states for pixel i according to Table 1. The standard fuzzy set operations are used to compute the fuzzy memberships of the eight states from αi,βi,γi. Then a defuzzification is carried out by declaring pixel i to be in the state with the maximum membership.Pixels in state ‘dubious’ must be corrected to either ‘Foreground object’ or ‘Background with illumination change’. That is, given a pixel with a smooth color transformation and a large difference with respect to the background model color, which is classified as foreground by the baseline algorithm, we cannot tell whether it is a real foreground object or a background pixel subject to an illumination change. At the pixel level this ambiguity cannot be solved. Hence we propose a frame level procedure for this task. From each dubious pixel i we trace lines to the four directions up, down, left, and right, and to the four diagonals. If any of these lines hits a pixel in the ‘Foreground object’ or ‘Background with illumination change’ states, then we count a vote for that state. If a line exits the frame without having hit a pixel in any of these two states, then that line does not cast any vote. Finally, pixel i is changed to the state with the most votes. If there is a tie, then no correction is made. The rationale behind this procedure is that dubious pixels belonging to foreground objects are usually located in the interior of the objects. Hence, for these pixels most of the eight lines hit some pixels in the ‘Foreground object’ state which lie in the borders of the object. It must be noted that dubious pixels are more frequent in objects with a homogeneous interior. On the other hand, dubious pixels belonging to the background are usually surrounded by some pixels in the ‘Background with illumination change’ state.Fig. 2exemplifies the illumination state estimation procedure described in this subsection. Please note that the states are represented in the pseudocolors specified in Table 1. As shown, the dubious pixels inside the person are corrected to the ‘Foreground object’ state, while those which lie in his shadow are corrected to the ‘Background with illumination change’ state. Additionally, Fig. 3shows the flowchart of our proposal for the k-th frame.Once the illumination state of every pixel has been computed by the procedure explained in the previous subsection, a decision must be made about which pixels must be reset. A reset should be carried out whenever the system has failed to detect an illumination change in the background, so that the background model is not updated and pixels stay wrongly in the foreground state during many frames. We keep a counter cifor each pixel i which counts how many consecutive frames the pixel has been declared as foreground by the baseline background model. If cisurpasses a prespecified limit Climit, then we assume that the baseline algorithm has failed, so that the pixel must be reset. A pixel is reset by application of the initialization procedure of the baseline algorithm to it.The illumination state of the pixels is used to perform an early reset of some pixels by increasing their counters ciwhen an illumination change has been detected on them. Two rules are applied on each video frame. The first one (‘soft reset’) says that pixels i which are in the ‘Background with illumination change’ state in the current frame modify their counters as follows:(8)ci=maxciCsoft.The second rule (‘hard reset’) says that pixels i which have been in the ‘Background with illumination change’ state continuously for the last Khardframes modify their counters as follows:(9)ci=maxciChardwhere Chard>0 and(10)0<Csoft≤Chard≤Climit.The soft and hard reset rules model situations where the likelihood of an illumination change is low and high, respectively. The longer a pixel stays in the ‘Background with illumination change’ state, the most likely that the illumination change is real.

@&#CONCLUSIONS@&#
We have presented a new approach to illumination change detection. Our proposal is designed to be attached to an existing foreground detection algorithm. It is based on the study of the local color transformation which the pixels undergo when a lighting change occurs. One of the advantages is that we only need to tune one parameter and, using a few different values, good results can be obtained. Therefore it is easily usable and does not require a vast expertise.In order to demonstrate its performance, we have considered a set of eight real sequences with different complex environments, representing indoors and outdoors situations. All of them can be found in public repositories. Five state-of-the-art methods have been modified with our proposal. This amounts to a total of 40 benchmarks, and our proposal overcomes the original algorithm in 35 of them, while it achieves competitive results in the other five. There are also three background subtraction and shadow detection methods that we have tested; they achieve poor results compared to our proposal.From the above, it can inferred that our proposal increases the performance of existing background segmentation methods in a wide range of illumination change conditions, which demonstrates its potential to enhance computer vision systems which include a foreground detection subsystem.