@&#MAIN-TITLE@&#
From expert-derived user needs to user-perceived ease of use and usefulness: A two-phase mixed-methods evaluation framework

@&#HIGHLIGHTS@&#
This framework leverages the complementarity of usability experts and end-users.Mixed-methods allow comprehensive data collection and triangulation.Mixed-methods improve understanding of vague or underspecified user needs.

@&#KEYPHRASES@&#
Evaluation studies,Clinical trials,Workflow,Needs assessment,Medical informatics,

@&#ABSTRACT@&#
Underspecified user needs and frequent lack of a gold standard reference are typical barriers to technology evaluation. To address this problem, this paper presents a two-phase evaluation framework involving usability experts (phase 1) and end-users (phase 2). In phase 1, a cross-system functionality alignment between expert-derived user needs and system functions was performed to inform the choice of “the best available” comparison system to enable a cognitive walkthrough in phase 1 and a comparative effectiveness evaluation in phase 2. During phase 2, five quantitative and qualitative evaluation methods are mixed to assess usability: time-motion analysis, software log, questionnaires – System Usability Scale and the Unified Theory of Acceptance of Use of Technology, think-aloud protocols, and unstructured interviews. Each method contributes data for a unique measure (e.g., time motion analysis contributes task-completion-time; software log contributes action transition frequency). The measures are triangulated to yield complementary insights regarding user-perceived ease-of-use, functionality integration, anxiety during use, and workflow impact. To illustrate its use, we applied this framework in a formative evaluation of a software called Integrated Model for Patient Care and Clinical Trials (IMPACT). We conclude that this mixed-methods evaluation framework enables an integrated assessment of user needs satisfaction and user-perceived usefulness and usability of a novel design. This evaluation framework effectively bridges the gap between co-evolving user needs and technology designs during iterative prototyping and is particularly useful when it is difficult for users to articulate their needs for technology support due to the lack of a baseline.

@&#INTRODUCTION@&#
Evaluation is important to all innovations [1], including Health Information Technology (HIT) interventions. However, there are significant barriers for evaluating HIT, such as the lack of a reference HIT gold standard [2], the paucity of knowledge regarding user needs prior to the development of an HIT intervention [3], and the complexity of socio-technical systems and multi-stakeholder teams, which can affect the intended outcomes of the HIT intervention [4].Five levels of usability evaluation have been described in the literature: task-based, user-task, system-task, user-task-system, and user-task-system-environment [5]. The first three levels occur early in prototype development, focusing on task identification, how users perform their tasks, and if a system supports the task it was designed for [5]. The fourth level addresses how users perform a set of tasks using the system and how users perceive the usefulness of the system [5]. Building on these, the fifth level evaluates how the task, user, and system interact within the workplace environment [5]. The fifth level usually occurs after system deployment [6], while the fourth level occurs during the prototype development stage.Often, HIT prototypes are not fully comparable with existing systems because of their inherent novelty and uniqueness. Identifying an appropriate baseline or comparison system (when a baseline is lacking) for evaluation purposes is difficult for emerging HIT interventions [7]. However, it is important to overcome this problem and select the “best available” system as a reference standard for comparing the usability and effectiveness of various systems against.Various evaluation methods and strategies have been developed [8]. Evaluations that mix methodologies are considered robust [9,10] and particularly useful in the medical setting [11,12]. There are many ways to combine methods, such as mixing qualitative and quantitative methods [13], involving users of varying perspectives for data collections [14], or using various data collection methods to achieve greater data validity. The mixed-methods approach is superior to either qualitative or quantitative research methods alone [15] because it ensures comprehensive data collection and avoids unnecessary a priori assumptions often made by researchers [13]. In a mixed-methods evaluation, qualitative data can be used to identify unmet needs [16–18], while quantitative data can measure workflow impact [18–22]. Data triangulation further allows verification of derived user needs [23].In addition, evaluation designs can involve different types of evaluators, including usability experts and intended users. Several evaluation methods utilize usability experts. Cognitive task analysis (CTA) is an evaluation method performed by usability experts for assessing usability and has been successfully applied in healthcare settings [24]. Cognitive walk-through also involves usability experts but is less intensive than CTA. In a cognitive walk-through, an expert who is already familiar with the system performs a set of predefined tasks and notes the number of steps required by certain tasks and any usability and design problems with the interface [25]. Other evaluation methods make use of the intended end-users themselves. Time-motion analysis is a quantitative method that measures the amount of time users spend performing a task [26]. Results of time-motion analyses provide insight into the likelihood of system adoption and can be used to identify areas in users’ workflow amenable to an informatics intervention [27]. The advantages of surveys, emails, and think-aloud protocols in evaluating informatics interventions is well-established [28]. Software log analysis is another useful evaluation method that can capture behind-the-scenes interactions with the system and is not intrusive to evaluators [29]. Questionnaires can assess users’ perceptions of a system’s usability [30] and the likelihood of acceptance of the technology [31]. Qualitative information in the form of unstructured interviews and think-aloud protocols are especially useful during system evaluation because they allow users to provide additional information not specified a priori via a structured questionnaire [32]. Mixing qualitative and quantitative evaluation methods to further enhance the evaluation result is a well-established approach [15].To address the evaluation challenges with emerging HIT, where user needs are vague and clinical workflow is complex, we describe a two-phase mixed-methods evaluation framework to bridge the gap between co-evolving user needs and technology designs during iterative prototyping. This novel evaluation framework enables an integrated assessment of both expert-derived user needs satisfaction and the user-perceived usefulness and ease of use of emerging HIT interventions [33]. It supports formative evaluation of HIT before the release of a fully-fledged system. We applied our methodology to evaluate the prototypes of a novel clinical research decision support system called Integrated Model for Patient Care and Clinical Trials (IMPACT), which is designed to provide decision support for scheduling research visits [34]. We followed the STAtement on Reporting of Evaluation studies in Health Informatics (STARE-HI) guideline for reporting evaluation studies where applicable [35] since our framework was ideally suited for formative evaluations of software prototypes. We then describe this evaluation framework and its use in evaluating IMPACT prototypes.Our evaluation framework consists of two phases. In phase 1, a usability expert collects user needs and compares the intervention with related systems by aligning system functions with derived user needs for each system. This enables the selection of a suitable comparison system followed by a cognitive walk-through involving a task analysis and a comparison of interface design differences between the innovation and the comparison system. Phase 2 involves the system’s end-users, Clinical Research Coordinators (CRCs) to collect quantitative and qualitative data. Fig. 1illustrates our mixed-methods evaluation framework.Table 1shows the types of data collected at each phase. Two measures are assessed during phase 1: the number of steps required by each task and interface features used while performing each task (e.g., screen transitions and pop-ups). Analysis during phase 1 allows developers to assess how well the system performs in a laboratory setting. If phase 1 identifies many critical system functions that require improvement, the system can be refined prior to testing with end-users. This approach prevents end-users from being adversely affected by a system requiring critical improvements. Since phase 1 of the IMPACT evaluation revealed no such deficiencies, we were able to proceed directly to phase 2 of the evaluation.Columbia University Medical Center (CUMC) is an academic medical center where many patients are also research participants. The IMPACT system, developed at CUMC, was designed to integrate information from both patient care and clinical research to facilitate the scheduling of research visits and coordination of patient care and research workflows. It incorporates temporal constraints from the research protocol’s visit schedule and availability of research resources (e.g., rooms, equipment, and personnel) into a calendar interface. Designed for use by CRCs and schedulers, IMPACT automatically calculates resource availability and recommends suitable dates and times for the next research visit. IMPACT’s complete functionality has been published elsewhere.We recruited a usability expert to derive comprehensive user needs for scheduling decision support. This usability expert was independent from the design team but was present in the participatory design meetings to understand user needs. To guide user needs identification, the expert surveyed existing scheduling systems and anticipated problems that the user is likely to encounter using knowledge of CRCs’ workflow. Each system’s features (including those of IMPACT) were compared to this set of usability-expert derived user needs. Four relevant systems currently being used for scheduling at CUMC were included to quantify how well user needs were satisfied by each system: Microsoft Outlook Calendar, AllScripts Study Manager [36], Velos eResearch [37], and WebCAMP [38].The cross-system feature alignment was used to identify a competent system to compare IMPACT with. This was done by comparing IMPACT’s key features – user support during scheduling and calendar visualization – with those of other systems to identify the “best available” system for comparison with the intervention during phase 2.Based on the previously reported CRC workflow [34], the usability expert developed scenarios for eight tasks: logging in, locating a participant, scheduling a screening visit, scheduling a randomization visit, viewing visit details, moving a visit on the calendar, updating a visit’s status, and rescheduling a visit. These scenarios ensure that the iterative evaluations stay focused on these important functionalities and remain relevant throughout the lengthy software design cycle. Appendix 1 provides sample scenarios.The usability expert counted the number of steps required by each task for both systems and noted their design differences. During phase 2, these same scenarios were used by end-users to evaluate each system.We recruited CRCs and schedulers from various clinical research settings to participate in scenario-based evaluations, 30min or so each. This evaluation was conducted after IMPACT had undergone a 2-year participatory design process and after all key features had been implemented. We provided evaluators with a task-based scenario (the same scenario used by the usability expert in phase 1) and asked them to complete each task using IMPACT and the comparison system. We asked all evaluators to use both systems. Columbia University Medical Center’s Institutional Review Board approved this study (IRB-AAAK6000).Our evaluation framework integrated three quantitative and two qualitative methods to assess four research constructs (Table 1). We studied usability and user acceptance at three conceptual levels: (1) human–computer interaction design, (2) team work and workflow, and (3) the socio-technical systems around Health Information Technology. We identified four research constructs and mapped them to these three levels as follows: user anxiety and user-perceived ease of use were mapped to the first level, human–computer interaction design; workflow was mapped to the second level, team work and workflow; and function integration was mapped to the third level, the socio-technical system issues. We selected each of the five methods because they contribute data for a measure that was not assessable by other methods. For instance, time-motion analysis contributes information regarding task-completion-time that could not be assessed by any of the remaining four methods. Similarly, only unstructured interviews allowed users’ “wish-list” to be elicited from evaluators. Two measures were assessed by multiple methods, namely usability and user acceptance.Specifically, we used a think-aloud protocol to identify and record usability and interface problems [21]. Using unstructured interviews, we elicited user feedback and needs [39]. In addition, we used time-motion analysis to assess the impact of HIT on workflow [40,41] by comparing the time needed to perform a set of tasks with and without using IMPACT, since task-completion-time is related to user satisfaction [42]. We also used questionnaires to assess perceived usability [30] and user acceptance [31] and a software log to record users’ interactions with IMPACT [29]. We then triangulated data [43] from these diverse data sources.Evaluators were asked to “think aloud” while completing the scenario. Evaluators vocalized their difficulties with the system and in some cases recommended changes to IMPACT’s interface design and functionalities. At the end of the think aloud session, an unstructured interview was conducted during which users were asked about their overall impression of the application and their suggestions for improving IMPACT or the comparison system. Both the think aloud session and unstructured interview were recorded and transcribed by a professional transcription service.1www.synergytranscriptionservices.com.1During each evaluation, we collected the time spent per task using an iPad v.2.0 (Apple Inc., Cupertino, CA) application called: ATracker [44]. User tasks were defined using prior knowledge of CRCs workflow [34]. The complete list of tasks included logging in, locating participant’s visit schedule in system, scheduling a regular visit, scheduling a PRN (Pro Re Nata: Latin meaning “as needed”) visit, viewing visit details, tabbing through visit pages, updating visit status, rescheduling a visit from the visit details page, rescheduling a visit using drag-and-drop from the main calendar page, viewing reminders, changing account settings, searching for note paper, talking on phone, using REDCap [45,46], and miscellaneous activities. Data were collected for all evaluators interacting with either system.After each scenario and subsequent interview, each evaluator completed two questionnaires: the System Usability Scale (SUS) [30] and the Unified Theory of Acceptance of Use of Technology (UTAUT) [31]. The UTAUT enhances the well-known Technology Acceptance Model [47]. Users ranked their responses on a scale of one (strongly disagree) to five (strongly agree). SUS scores were normalized so that four indicated the optimal response while zero indicated the lowest possible response.Additionally, we analyzed IMPACT’s software log of user activities recorded during each evaluation session. Ten actions were logged: logging in, changing password, viewing calendar, viewing visit, scheduling a visit, interacting with resource optimizer, rescheduling a visit using drag-and-drop, scheduling a personal event, viewing user reminder(s), and logging out. We analyzed the action transition frequencies [41] to assess the integration of functions within IMPACT and as another assessment of IMPACT’s effect on workflow. Software logs were unavailable for the comparison system.We triangulated data [43] across evaluation methods to compare results obtained across methods [13] and assess their complementarity and look for convergence. We also compared results obtained across evaluation phases (usability expert vs. end-users).

@&#CONCLUSIONS@&#
We present a two-phase evaluation framework that combines evaluations by usability experts (phase 1) and evaluations by end-users (phase 2). Our framework is particularly relevant for early prototype evaluations for emerging HIT interventions, when users are unclear about their needs and when a baseline is lacking, both frequently encountered problems. Our framework enables an integrated assessment of user needs identification by usability experts (phase 1) and user needs refinements by end-users (phase 2). By triangulating results from mixed-methods, our framework measures the ease-of-use, integration of functions, user anxiety, and workflow impact.The research described was supported by grants R01LM010815 and R01LM009886 from the National Library of Medicine, R01HS019853 from the Agency for Healthcare Research and Quality, and UL1 TR000040 from the National Center for Advancing Translational Sciences.The authors declare that they have no competing interests.CW conceptualized the evaluation framework, supervised the evaluation study, and wrote the manuscript. MRB performed data collection and analysis and wrote an initial draft of the manuscript. AR participated in the evaluation research and edited the manuscript. YS developed the software log used in this evaluation. RCS, CL, LB contributed ideas to the design and provided feedback on the manuscript. JTB and SB provided substantive methodological contributions to the manuscript.