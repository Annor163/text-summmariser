@&#MAIN-TITLE@&#
Evolving soft subspace clustering

@&#HIGHLIGHTS@&#
Two online soft subspace clustering are proposed by adopting online learning strategy.We extend online subspace clustering and present two streaming soft subspace clustering.Our methods receive the benefit of online learning scheme and scalable clustering strategy.Online soft subspace clustering achieves better results than the batch versions.Streaming soft subspace clustering obtains results as good as online and batch ones.

@&#KEYPHRASES@&#
Subspace clustering,Data stream clustering,Online clustering,Scalable clustering,

@&#ABSTRACT@&#
A key challenge to most conventional clustering algorithms in handling many real world problems is that, data points in different clusters are often correlated with different subsets of features. To address this problem, subspace clustering has attracted increasing attention in recent years. In practical data mining applications, data points may arrive in continuous streams with chunks of samples being collected at different time points. In addition, huge amounts of data often cannot be kept in the main memory due to memory restriction. Accordingly, a range of evolving clustering algorithms has been proposed, however, traditional evolving clustering methods cannot be effectively applied to large-scale high dimensional data and data streams. In this study, we extend the online learning strategy and scalable clustering technique to soft subspace clustering to form evolving soft subspace clustering. We propose two online soft subspace clustering algorithms, OFWSC and OEWSC, and two streaming soft subspace clustering algorithms, SSSC_F and SSSC_E. The proposed evolving soft subspace clustering leverages on the effectiveness of online learning scheme and scalable clustering methods for streaming data by revealing the important local subspace characteristics of high dimensional data. Substantial experimental results on both artificial and real-world datasets demonstrate that our proposed methods are generally effective in evolving clustering and achieve superior performance over existing soft subspace clustering techniques.

@&#INTRODUCTION@&#
Clustering has long been a hot research topic in various disciplines as an important data processing technique. It has been widely utilized as a fundamental tool for data analysis and visualization in areas such as data mining and machine learning [1]. Clustering aims to categorize unlabeled input data vectors into different groups, called clusters, such that data points within a cluster are more similar to one another than they are to data points belonging to different clusters, i.e., by maximizing the intra-cluster similarity while minimizing the inter-class similarity [2,3]. However, a key challenge to most conventional clustering algorithms is that, in many real world problems, data points in different clusters are often correlated with different feature subsets. For example, given a subset of data points identified in a cluster, it is possible that these points exhibit the same characteristics as points from other clusters when a certain subset of dimensions are observed [4,5].The difficulty that traditional clustering algorithms encounter in dealing with this challenge has inspired the invention of subspace clustering, or projected clustering, which has been studied extensively in recent years. The goal of subspace clustering is to locate clusters with their own associated dimensions that are embedded in different subspaces of the original data space [4,5]. Based on the ways that the cluster subspaces are determined, subspace clustering can be generally classified into two main categories: hard subspace clustering and soft subspace clustering [5,6]. In this paper, we focus on soft subspace clustering, which measures the importance of each dimension to a particular cluster in the clustering process by automatically assigning different weightings to different dimensions. Soft subspace clustering algorithms can be grouped into two categories: fuzzy weighting subspace clustering (FWSC) [7] and entropy weighting subspace clustering (EWSC) [6]. Both of them use batch processing to update cluster centers, only after all data points’ membership degrees have been obtained. However, in real-life environments, streaming data continuously arrives as being collected with chunks at different time intervals. Moreover, it may not be possible to retain huge amounts of data in the main memory due to the memory limit [8,9]. To address these problems, it is necessary to generate an effective evolving version of soft subspace clustering which has the ability to incrementally update models for high dimensional data streams.During the last few years, a number of research efforts to cluster streaming data have been made. One traditional way to replace the batch update of cluster centers is based on an online learning strategy, and a large number of competitive learning-based online clustering algorithms [10–12] have been introduced. Banerjee and Ghosh combined frequency sensitive competitive learning and introduced an effective clustering technique called frequency-sensitive spherical K-means (fs-spkmeans) [10]. The online spherical K-means (OSKM) algorithm [12] investigates an online version of spherical k-means algorithm to cluster large text datasets. In addition, the extension of learning vector quantization [11] was developed based on the Winner-Take-More scheme that updates the recursion formulas of a cluster center by incorporating them with the membership degree of each cluster center. In general, the online version of clustering algorithms can not only achieve significantly better results than batch algorithms but can also incrementally update the cluster models with different learning strategies, which is an important and effective way of analyzing data streams, building on the understanding of the underlying distribution of data flow. By considering the advantages of competitive learning theory, we first introduce two online soft subspace clustering algorithms, OFWSC and OEWSC, which adopt an online learning strategy to modify the traditional batch soft subspace clustering approaches.Although performance studies have shown that using an online clustering technique can achieve significantly better results than the batch version, it is still necessary to go through all data points several times, making it impossible to store large data streams. Recently, scalable clustering has been proposed to partition large-scale data or streaming data, by means of dividing data points into chunks and processing them continuously, segment by segment [9,13–17]. Bradley et al. first presented a scalable clustering technique [13], called ScaleKM, which selectively retains or compresses the samples that are important and discards the insignificant samples. By using a simple compression scheme, Farnstrom et al. proposed a simplification of ScaleKM [14], the simple single pass K-means algorithm, for large databases. In addition to these works addressing the crisp case of scalable clustering, Hall et al. generalized the scalable strategy to the fuzzy case and introduced two fuzzy scalable clustering algorithms: single-pass fuzzy C-means (SPFCM) [15] and online fuzzy C-means (OFCM) [16,17]. In summary, scalable clustering can not only handle streaming data efficiently, but can also produce partitions as good as online and batch subspace clustering methods. Motivated by these ideas, we propose evolving soft subspace clustering, which identifies clusters by assigning various weighting values to different dimensions of clusters in an incremental, step-wise clustering procedure. Specifically, by extending the online soft subspace clustering, we present two streaming soft subspace clustering algorithms, SSSC_F and SSSC_E, for partitioning data streams and revealing the important local subspace characteristics in data stream clustering procedures.To the best of our knowledge, the proposed evolving soft subspace clustering is the first work to address the synthesis of the merits of soft subspace clustering with the beneficial properties of an online learning scheme, as well as providing a scalable subspace clustering strategy for streaming data. Comprehensive performance studies demonstrate that OFWSC and OEWSC are more effective in subspace clustering than the traditional batch methods, and SSSC_F and SSSC_E can obtain data stream clustering results as good as online and batch algorithms. The rest of this paper is organized as follows. In Section 2, a brief overview of existing approaches for soft subspace clustering is given. Section 3 provides a detailed description of online soft subspace clustering, OFWSC and OEWSC. The proposed streaming soft subspace clustering algorithms, SSSC_F and SSSC_E, are presented in Section 4. In Section 5, the clustering performance of four evolving subspace clustering methods on both synthetic and real-life datasets in comparison with other clustering techniques is reported. Finally, conclusions and future work are discussed in Section 6.Soft clustering aims to group a set of given data points X={x1, x2, …, xN}⊂RDinto a set of clustering centers V={vi, 1≤i≤C}. Let uijdenote the membership degree of xibelonging to vi, then we can define the fuzzy C-partition matrix U of the given dataset, i.e., U={uij|1≤i≤C, 1≤j≤N}. To discover clusters from different subspaces, it is vital that a soft clustering algorithm has the capability to cluster data points by automatically weighting features in its clustering procedure. For this reason, a weighting wikis assigned to each dimension based on the importance of the kth dimension to the formation of the ith cluster. The subspaces of clusters can be identified by the weighting matrix W={wik|1≤i≤C, 1≤k≤D} after soft subspace clustering [2,6,7].Fuzzy weighting subspace clustering (FWSC) [7] seeks to find clusters from fuzzy weighting subspace. In all the fuzzy weighting subspace clustering algorithms, a fuzzy weightingwikτis assigned to each feature of clusters with a fuzzy weighting index τ. The objective function of FWSC is generally formulated as:(1)JFWSC=∑j=1N∑i=1Cuijm∑k=1Dwikτ(xjk−vik)2s.t.0≤uij≤1,∑i=1Cuij=10≤wik≤1,∑k=1Dwik=1.By minimizing (1) using Lagrange multipliers, the updating equations for estimating center vi, fuzzy weighting wikand membership degree uijcan be derived by the theorem below.Theorem 1Assume m>1 and τ>1, the necessary condition for the minimum of the objective function of FWSC in (1) yields the following update equations:(2)uij=(dij)−1/m−1∑s=1C(dsj)−1/m−1dij=∑k=1Dwikτ(xjk−vik)2(3)vik=∑j=1Nuijmxjk∑j=1Nuijm(4)wik=(qik)−1/τ−1∑s=1D(qis)−1/τ−1qik=∑j=1Nuijm(xjk−vik)2The entropy concept, which is used to represent the certainty of dimensions in the identification of a cluster, is also introduced into soft subspace clustering. Since the weightings in the extended subspace clustering methods are controllable by entropy, this type of algorithm is referred to as entropy weighting subspace clustering (EWSC) [6]. In general, the objective function of EWSC can be defined as:(5)JEWSC=∑j=1N∑i=1Cuijm∑k=1Dwik(xjk−vik)2+γ∑i=1C∑k=1Dwiklogwiks.t.0≤uij≤1,∑i=1Cuij=10≤wik≤1,∑k=1Dwik=1.The Lagrange multiplier is also employed to minimize the objective function in Eq. (5). The following theorem results in the equations for updating the estimations of center vi, entropy weighting wikand membership degree uij.Theorem 2Assume m>1 and γ>0, the necessary condition for the minimum of the objective function of EWSC in (5) yields the following update equations:(6)uij=(dij)−1/m−1∑s=1C(dsj)−1/m−1dij=∑k=1Dwik(xjk−vik)2(7)vik=∑j=1Nuijmxjk∑j=1Nuijm(8)wik=exp(−qik/γ)∑s=1Dexp(−qis/γ)qik=∑j=1Nuijm(xjk−vik)2By observing the existing soft subspace clustering algorithms, we can see that both FWSC and EWSC have been mainly used in batch mode. That is, each cluster center is updated only after all the membership degrees of the data points have been being obtained as the mean of all the data points weighted by their membership degrees of belonging to each cluster. However, data points may arrive as continuous streams with chunks of data being collected at different times in certain real-life scenarios [8,9]. Moreover, huge amounts of data often cannot be kept in main memory because of memory restrictions. It is therefore necessary to generate an effective evolving version of subspace clustering which has the ability to incrementally update models for large-scale high dimensional data and data streams.During the last few years, various algorithms for clustering large-scale data and data streams have been proposed. In general, these approaches can be divided into two categories: adaptive methods and scalable methods [8,9]. Motivated by these ideas, we bring online learning strategy and the scalable clustering technique into soft subspace clustering and propose evolving soft subspace clustering algorithms, as detailed in the following sections.In this section, online soft subspace clustering is proposed. This technique identifies clusters by employing an optimal variable weight vector in each cluster with an online clustering strategy. In particular, we present two online soft subspace clustering algorithms, OFWSC and OEWSC, based on soft competitive learning theory.Online learning strategy has attracted significant research attention in the past few decades because it provides a compact description of data flows. There is a large number of competitive learning-based online learning algorithms, such as vector quantization [18,19] and self-organizing maps (Kohonen map) [20]. Unlike the traditional batch mode, competitive learning theory allows multiple nodes to compete for an incoming instance and the winner earns the right to adjust itself to respond more strongly to the input example.Competitive learning strategy can be divided into two categories, namely WTA (Winner-Take-All), which is a hard competitive learning, and WTM (Winner-Take-More), which is a soft competitive learning [21,22]. Based on the WTA rule, there is only one centralized data point in the dataset. Standard competitive learning and K-means clustering use this strategy. When applied to online clustering study, it leads to several WTA rule-based online clustering methods. For example, the frequency-sensitive spherical K-means (fs-spkmeans) [10] and online spherical K-means (OSKM) [12] update the closest cluster center iteratively. The abstract recursion equation for the WTA rule-based online clustering can be generally formulated as:(9)vi*(t)=vi*(t−1)+η×D(vi*(t−1),xNt)i*=argminid(vi(t−1),xNt)where vi(t−1) is the reference cluster center representing ith cluster at t−1 time, xNtis the Ntth instance arriving at t time, and d(vi(t−1), xNt) is the distance between xNtand vi(t−1). Since each clustering algorithm may have its own special metric property, a general distortion error between xNtand its closest cluster centervi*(t−1)can be normalized as:D(vi*(t−1),xNt)=ρ(d(vi*(t−1),xNt)), where ρ′(x)≥0. Like most gradient descent methods, η in (9) is a learning rate, which usually decreases with time in order to avoid oscillations and enforce the convergence of the algorithms.Notably, the WTA mechanism is a simple competitive learning rule which allows only one winner for every incoming sample. However, the WTA rule suffers from a serious problem, namely, that with different initial values, so-called dead points or under-utilization points may appear during the learning process. Thus the Winner-Take-More (WTM) strategy has been proposed to relax the WTA rule and weaken the dependence of the initial values by introducing fuzzy membership degrees [22]. Based on the WTM scheme, soft competitive learning methods allow multiple winners to adjust, with the rate of adjustment being inversely proportional to the distortion error between each winner and the newly arriving sample. When applied to online clustering study, it also leads to several WTM rule-based online clustering algorithms. For example, the extension of learning vector quantization [11] computes the membership degree for each update cluster according to the relative inverse distance from the incoming example. More generally, the abstract recursion formula for WTM rule-based online clustering is updated according to:(10)vi(t)=vi(t−1)+η×ui(Nt)×D(vi(t−1),xNt)where ui(Nt) denotes the membership degree of the newly arriving sample xNtbelonging to ith cluster center vi.To some extent, online soft subspace clustering, proposed in the following subsection, is based on soft competitive learning theory, and the WTM strategy is applied to update the recursion formulas of cluster center and fuzzy weighting by incorporating the membership degree.To incorporate the scheme of soft competitive learning into FWSC, we first define the objective function for online fuzzy weighting subspace clustering (OFWSC) in (11), as follows:(11)JOFWSC(t)=∑j=1Nt∑i=1Cuijm∑k=1Dwikτ(xjk−vik)2=JOFWSC(t−1)+∑i=1Cui(Nt)m∑k=1Dwikτ(x(Nt)k−vik)2s.t.0≤uij≤1,∑i=1Cuij=10≤wik≤1,∑k=1Dwik=1.Thus, with reference to Theorem 1 and (2), the membership degree ui(Nt) can be computed in (12), when the Ntth instance xNtarrives at t time.(12)ui(Nt)=(di(Nt))−1/m−1∑s=1C(ds(Nt))−1/m−1di(Nt)=∑k=1Dwikτ(t−1)(x(Nt)k−vik)2where wik(t−1) denotes the cluster weighting obtained at t−1 time.Next, by referring to Theorem 1 and (3), we have the following cluster center recursion formula:(13)vik(t)=∑j=1Ntuijmxjk∑j=1Ntuijm=∑j=1Nt−1uijmxjk+ui(Nt)mx(Nt)k∑j=1NtuijmSincevik(t−1)=∑j=1Nt−1uijmxjk∑j=1Nt−1uijm, we can obtain:(14)∑j=1Nt−1uijmxjk=vik(t−1)×∑j=1Nt−1uijmSubstituting (14) in (13), we have the following updating equation after rearranging the terms.(15)vik(t)=∑j=1Nt−1uijm×vik(t−1)+ui(Nt)mx(Nt)k∑j=1Ntuijm=vik(t−1)−ui(Nt)m∑j=1Ntuijm×(vik(t−1)−x(Nt)k)=vik(t−1)−ηNt×ui(Nt)m×(vik(t−1)−x(Nt)k)(16)ηNt=1∑j=1Ntuijm=1∑j=1Nt−1uijm+ui(Nt)m=11/ηNt−1+ui(Nt)mBy comparing Eq. (15) with (10), we observe that: (1) the cluster center recursion formula (15) for OFWSC is based on soft competitive learning strategy, because the membership degreeui(Nt)mwith a fuzziness exponent m is considered as the weight in the cluster center update procedure; (2) the term (vi(t−1)−xNt) is basically the difference between the newly arriving instance xNtand the old cluster center vi(t−1) at t−1 time, which suggests that OFWSC is essentially a stochastic gradient descent method; (3) ηNtof (16) is the learning rate, which is inversely proportional to the growing number of data points. Since the denominator of (16)increases with the increment of Nt, ηNtusually decreases with time to avoid oscillations, as well as enforcing the convergence of the approach.It is worth noting that the learning rate schedule of (16) may be sensitive to the initial membership degree ui(Nt) which may possible to cause bad clustering results in practice. Therefore, a simple exponential learning factor is applied in this study, which is the same as the decreasing learning rate used in [12]. Based on the annealing effect, the exponentially decreasing rate η(t) at t time is defined as:(17)η(t)=η0ηfη0t/NMwhere t is the online iteration index (0≤t≤NM), N is the number of data points, and M is the number of passes (supposing we go through all data points M times). In our experiments, the initial learning rate η0 and the final learning rate ηfare set as 1.0 and 0.01, consistent with previous studies [12].With reference to Theorem 1 and (4), the recursion formula of fuzzy weighting wik(t) can then be updated with (18), as shown by:(18)wik(t)=(qik(t))−1/τ−1∑s=1D(qis(t))−1/τ−1where(19)qik(t)=∑j=1Ntuijm(xjk−vik)2=∑j=1Nt−1uijm(xjk−vik)2+ui(Nt)m(x(Nt)k−vik(t))2=qik(t−1)−ui(Nt)m(vik(t)−x(Nt)k)2Note that the numerator of (18) is also based on soft competitive learning strategy, since the membership degreeui(Nt)mwith a fuzziness exponent m is considered in (19), and the denominator of (18) is a normalization term to ensure that the cluster weightings satisfy the equality constraint,∑k=1Dwik(0)=1, 1≤i≤C.In OFWSC, the initial weightings wik(0) are set to random numbers, which are uniformly distributed and must meet the equality constraints∑k=1Dwik(0)=1, 1≤i≤C. The initial centers vi(0) are C different data points chosen by K-means++ [23], which specifies a procedure for choosing the initial cluster centers before proceeding with the standard K-means iterations.Based on the above description, the proposed OFWSC algorithm is described in Fig. 1.To incorporate the strategy of soft competitive learning into EWSC, the following objective function in (20) is minimized for online entropy weighting subspace clustering, termed OEWSC, as shown by:(20)JOEWSC(t)=∑j=1Nt∑i=1Cuijm∑k=1Dwik(xjk−vik)2+γ∑i=1C∑k=1Dwiklogwik=JOEWSC(t−1)+∑i=1Cui(Nt)m∑k=1Dwik(x(Nt)k−vik)2s.t.0≤uij≤1,∑i=1Cuij=10≤wik≤1,∑k=1Dwik=1.Thus, with reference to Theorem 2 and (6), the membership degree ui(Nt) can be updated with (21).(21)ui(Nt)=(di(Nt))−1/m−1∑s=1C(ds(Nt))−1/m−1di(Nt)=∑k=1Dwik(t−1)(x(Nt)k−vik)2where wik(t−1) denotes the cluster weightings obtained at t−1 time.Then, we obtain the recursion equation of cluster center vik(t) at t time by adopting a similar derivation of OFWSC, i.e.,(22)vik(t)=vik(t−1)−η(t)×ui(Nt)m×(vik(t−1)−x(Nt)k)(23)η(t)=η0ηfη0t/NMSimilarly, based on Theorem 2 and (8), the recursion formula of entropy weighting wikat t time is derived below:(24)wik(t)=exp(−qik(t)/γ)∑s=1Dexp(−qis(t)/γ)(25)qik(t)=qik(t−1)−ui(Nt)m(vik(t)−x(Nt)k)2The proposed online version of OEWSC algorithm is summarized in Fig. 2.Based on Winner-Take-More competitive learning theory, we introduce the OFWSC and OEWSC algorithms that adopt online learning strategy to modify traditional batch subspace clustering methods. Given a newly arriving sample, online soft subspace clustering updates each cluster center incrementally, without the requirement that the membership degrees of all samples are obtained. Meanwhile, it is necessary to go through all data points several times to achieve superior clustering performance. The re-training of online soft subspace clustering is not always a feasible option, because it is not possible to store all the incoming data streams [8,9]. To address this problem, streaming soft subspace clustering is proposed, which locates clusters in different subspaces in data stream clustering applications. By combining the efficient online soft subspace clustering method with fuzzy scalable clustering framework, we present two streaming soft subspace clustering techniques, i.e., fuzzy weighting streaming soft subspace clustering, termed SSSC_F, and entropy weighting streaming soft subspace clustering, termed SSSC_E.The basic idea of scalable clustering is to divide large-scale data or streaming data into chunks, and process them continuously, chunk by chunk. A key challenge for most scalable clustering algorithms is to retain sufficient statistics for history data and to summarize the contribution of past chunks incrementally. Consequently, based on different scalable learning strategies and various basic clustering techniques, a wide range of scalable clustering algorithms have been successfully developed from both theoretical and practical perspectives [9,13–17,24].Although scalable clustering was initially proposed for large-scale data to overcome the problem of memory limitations [13,14], it was also used to cluster data streams. Aggarwal et al. proposed an effective method [24], called CluStream for clustering large evolving data streams. CluStream divides the clustering process into two components: an online module that collects detailed summary statistics from the history data, and an offline module that analyses these summary statistics to provide a quick understanding of the clusters in data streams. To partition large streaming text data, Zhong investigated an online version of the spherical K-means algorithm [9], called OSKM, and then extended this to streaming OSKM. Experimental results indicate that streaming OSKM is generally effective in text data stream clustering.In addition to these works addressing the crisp case of scalable clustering, Hall et al. recently generalized the scalable clustering framework to the fuzzy case and introduced two fuzzy scalable clustering algorithms, i.e., single-pass fuzzy C-means (SPFCM) [15] and online fuzzy C-means (OFCM) [16,17]. Since an example may not completely belong to a particular cluster in fuzzy clustering, SPFCM and OFCM retain sufficient statistics for past data points by involving the fuzzy membership matrix. This is a crucial difference between their scalable summarization and crisp cases. Specifically, SPFCM initially partitions the incoming chunk of data using FCM. Next, each cluster is represented by its cluster center, weighted by summing the membership degrees of all samples. The weighted cluster centers, serving as sufficient statistics for historical chunks, are partitioned in the next scalable clustering process together with the newly arriving chunk of data. This process is repeated until the end of the data stream or until the entire dataset has been scanned once. In the case of OFCM, the large-scale data is divided into chunks, after which each data segment is clustered chunk by chunk using FCM. The cluster centers for each data segment are then weighted by summing the membership degrees. Finally, all the weighted centers are clustered to obtain the final cluster centers of the entire dataset. Overall, both SPFCM and OFCM are not only able to handle large amounts of data points efficiently, they also produce partitions that are very closely approximate to FCM.Inspired by the scalable clustering schemes above, we combine online soft subspace clustering with a scalable clustering strategy and present two streaming soft subspace clustering algorithms, SSSC_F and SSSC_E, for partitioning high dimensional data streams. The proposed methods can also reveal their local subspace characteristics in data stream clustering procedure.Assuming large high dimensional data arrive as a continuous stream, streaming soft subspace clustering divides the streaming data into chunks, and processes each data segment continuously. The size of each chunk is dependent upon the amount of available memory and the speed of the data streams. Even though the scalable clustering strategy does not process the data points as a continuous stream, it is a reasonable approach in practice because we always make changes in the cluster distributions after a certain number of incoming samples. Processing in chunks also takes advantage of the re-training of online soft subspace clustering that makes it is possible to achieve superior performance in data stream clustering. Similar to [15–17], sufficient statistics for a previous chunk are represented by weighted cluster centers, which are obtained by involving the fuzzy membership matrix. This indicates that our streaming soft subspace clustering is derived from fuzzy scalable clustering strategy with a crucial difference in scalable summarization from the crisp cases.As an example, consider a data stream of N examples. When the tth chunk of data are retrieved, the weights of historical cluster centers vi(t−1) are denoted as pi(t−1), i=1, …, C, obtained by the previous clustering process, and the weights of Ntincoming data streams pi(t)=1, j=1, …, Ntall the time.For the first data segment, since vi(0) are initially empty and pi(0)=0, N1data points xj(1) are partitioned using online soft subspace clustering and condensed into C weighted cluster centers vj(1). The weights of vj(1) can be calculated as follows:(26)pi(1)=∑j=1N1(uij)qj(1)+∑i′=1C(uii′)pi′(0)=∑j=1N1(uij)qj(1)=∑j=1N1uij,1≤i≤Cwhere uijis the membership degree of sample xj(1) belonging to vj(1).Next, the C weighted centers vj(1) along with the N2 incoming examples xj(2) are clustered when the second chunk of data arrives. After applying online soft subspace clustering, the N2+C samples are condensed into C new weighted cluster centers vi(2) by taking into account the weights of each sample. Similar to (26), the weights of vi(2) are obtained by summing the membership degrees of N2+C samples, as shown by:(27)pi(2)=∑j=1N2(uij)qj(2)+∑i′=1C(uii′)pi′(1)=∑j=1N2uij+∑i′=1C(uii′)pi′(1),1≤i≤Cwhere qj(2)=1;pi′(1)is the weight ofvi′(1)obtained from the previous clustering in (26); uijanduii′are the membership degrees of xj(2) andvi′(1)belonging to vi(2).Similarly, for the third data segment, the C weighted centers vi(2) together with the N3 newly arriving examples xj(3) are condensed into C new weighted cluster centers vj(3). In general, the sufficient statistics for historical data are represented by the cluster centers vj(t), which can be weighted by summing the membership degrees of the C weighted centers vi(t−1) along with the Ntincoming samples xj(t), i.e.,(28)pi(t)=∑j=1Nt(uij)qj(t)+∑i′=1C(uii')pi′(t−1)=∑j=1Ntuij+∑i′=1C(uii′)pi′(t−1),1≤i≤Cwhere qj(t)=1;pi′(t−1)are the weights ofvi′(t−1); uijanduii′are the membership degrees of xj(t) andvi′(t−1)belonging to vi(t).It is easy for us to prove that∑i=1Cpi(1)=N1,∑i=1Cpi(2)=N1+N2. As a corollary,∑i=1Cpi(t)=N1+N2+⋯+Nt. As we know, vi(t) are obtained by partitioning Ntsamples xj(t) together with the previous weighted centers vi(t−1). Therefore, we can infer that if we use vi(t−1), it means we employ not only Nt−1 samples xj(t−1), but also weighted centers vi(t−2). Similarly, vi(t−2) involve the information of Nt−2 samples xj(t−2), as well as centers vi(t−3), and so on. Thus the sufficient statistics for past data segments are propagated between chunks from one point in time to the next, and our streaming soft subspace clustering stores the summarization of the historical data through the whole data stream clustering procedure.Since we know weighted cluster centers retain sufficient statistics in scalable clustering, OFWSC or OEWSC need to handle weighted cluster centers to fit in with our streaming soft subspace clustering. Thus we modify the recursion equations of cluster center vikand fuzzy weighting wikto take into account the weights pNtof the incoming condensed centers cNt. When the condensed centers cNtarrive at t time, the recursion equations of vikand wikin SSSC_F can be reformulated as follows:(29)vik(t)=vik(t−1)−η(t)×ui(Nt)m×pNt×(vik(t−1)−c(Nt)k)(30)wik(t)=(qik(t))−1/τ−1∑s=1D(qis(t))−1/τ−1(31)qik(t)=qik(t−1)−ui(Nt)m×pNt×(vik(t)−c(Nt)k)2Similarly, considering the weights pNtof the newly arriving condensed centers cNt, the recursion equations of vikand wikin SSSC_E are updated by:(32)vik(t)=vik(t−1)−η(t)×ui(Nt)m×pNt×(vik(t−1)−c(Nt)k)(33)wik(t)=exp(−qik(t)/γ)∑s=1Dexp(−qis(t)/γ)(34)qik(t)=qik(t−1)−ui(Nt)m×pNt×(vik(t)−c(Nt)k)2To make good use of the summarization of past chunks, the initial cluster centers and weightings for the incoming data segment are obtained by the previous online soft subspace clustering, such that the streaming soft subspace clustering results between different chunks may respond to the changes smoothly. In the end, the final set of cluster centers as well as weightings are provided at the end of the data stream or after the entire dataset has been scanned once in SSSC_F or SSSC_E.Fig. 3shows the procedure of the proposed streaming soft subspace clustering algorithm.In [17], Hall et al. discussed the convergence of single-pass fuzzy C-means (SPFCM) and online fuzzy C-means (OFCM) algorithms. When the weightings of each feature dimension equal each other, and OFWSC or OEWSC are replaced by FCM to partition each chunk of data, SSSC_F and SSSC_E can degenerate into the SPFCM algorithm. Therefore, streaming soft subspace clustering algorithms can easily be proved to be convergent using similar proof to that in [17]. In the same way, the proposed online soft subspace clustering algorithms are indeed an extension of SPFCM. By employing a strategy similar to that used in [17], it can be inferred that the proposed OFWSC and OEWSC approaches, at least along some subsequences, converge toward either local optimal or saddle points of their objective functions.Next, we discuss the basic operations and the computational complexity of the streaming soft subspace clustering algorithm. We use the following notation in our analysis.D: number of feature dimensions;C: number of clusters in the data stream;S: size of chunk dependent upon the amount of available memory and the speed of data stream;s: number of chunks that have been scanned;M: number of passes for each chunk of data partitioned by OFWSC or OEWSC.Given the complexity of one recursion of cluster center vik(t) and feature weighting wik(t) in OFWSC or OEWSC, computing the membership degree ui(Nt), vik(t) and wik(t) need O(CD) operations for each sample xNt. As there will be S incoming samples xj(t) along with C weighted centers vi(t−1), the time complexity for one pass over a chunk of data is O(CD(S+C)). Since we go through each chunk of data M times, the complexity of the OFWSC or OEWSC methods partitioning each data segment needs O(CDM(S+C)) operations. Therefore, assuming that the data stream has been scanned for s chunks, the total computational complexity of SSSC_F or SSSC_E algorithms is O(sCDM(S+C)).To evaluate the performance of the proposed online soft subspace clustering and streaming soft subspace clustering algorithms, a number of experiments on both synthetic and real-life datasets are conducted. Experimental results are reported as follows. The parameter settings and experimental setup for different approaches are first given. Next, three metrics used for clustering performance evaluation are described, followed by detailed comparative studies of OEWSC and OFWSC on synthetic datasets, UCI datasets and gene expression datasets. Finally, two sets of experiments on both a synthetic dataset and text data streams are reported to demonstrate the effectiveness of SSSC_E and SSSC_F.For the online soft subspace clustering study, OEWSC and OFWSC are compared with five approaches: two soft subspace clustering algorithms, EWSC [6] and FWSC [7]; one online spherical K-means, OSKM [12]; and two batch clustering methods, SPKM (batch spherical K-means) [25,26] and FCM [27]. Different parameters are used in the seven approaches. The feature entropy weighting index γ in OEWSC, EWSC and fuzzy weighting index τ in OFWSC, FWSC are set to 5 and 2 respectively. Similar to [2,3,28], the value of fuzziness exponent m in our experiment is determined by a simple but effective approach. Suppose N and D are the size of the data and features, if min(N, D−1)≥4, m should be within the range as specified by the inequality1<m≤min(N,D−1)min(N,D−1)−2; otherwise, m is set to 2. As the authors [12] did in their papers, we go through all data points 5 times for online clustering approaches and set the maximum iteration number to 100 for batch clustering algorithms.In streaming soft subspace clustering experiments, SSSC_E and SSSC_F are each compared with the following nine methods: two data stream clustering methods, namely SOSKM (streaming OSKM) [9] and SPFCM (single-pass FCM) [15,17]; three online clustering approaches, OEWSC, OFWSC, and OSKM [12]; and four batch clustering algorithms, EWSC [6], FWSC [7], SPKM [25,26], and FCM [27]. The size of chunk for the four data stream clustering methods is determined with loaded fixed percentages (10%, 20% and 50%) of the datasets, whereas seven online or batch clustering algorithms are run on the whole data stream that is not possible in practical data stream clustering environments. Consistent with the previous studies, the experimental settings of these approaches that partition each data chunk in data stream clustering, SSSC_E, SSSC_F, SOSKM and SPFCM, are the same as those of OEWSC, OFWSC, OSKM and FCM respectively.The experimental evaluations are conducted on five datasets which are grouped into two categories: online soft subspace clustering, which contains synthetic datasets I, UCI datasets and gene expression datasets, and streaming soft subspace clustering, which consists of synthetic dataset II and text streams. All datasets are preprocessed by normalizing the features in each dimension into the interval [0 1]. The clustering process for all algorithms is repeated 20 times at each setting, and the experimental evaluations are compared in terms of the means and standard deviations of their clustering results. All experiments are run on a computer with Intel Xeno (R) CPU 2.53-GHz, Microsoft Windows 7, and MATLAB.Since the external class labels are available for all experimental datasets, three evaluation indices, Clustering Accuracy (CA) [29], Normalized Mutual Information (NMI) [30] and Rand Index (RI) [31] are used to make an extensive comparison among different clustering algorithms. CA measures the percentage of data points that are correctly classified by a clustering method that compares with known class labels, defined as follows,(35)Clustering Accuracy=∑l=1CnlNwhere nlis the number of data points with the majority class label in cluster l, which is relabeled with the class label of the majority of data points that come from that cluster. N is the number of data points in the dataset.NMI calculates the average mutual information between every pair of clusters and their class, which is defined as follows,(36)NMI=∑i=1C∑j=1Cnijlog((N⋅nij)/(ni⋅nj))∑i=1Cnilog(ni/N)⋅∑j=1Cnjlog(nj/N)where nijis the number of agreements between cluster i and class j, niis the number of data points in cluster i, njis the number of data points in class j, and N is the total number of data points in the dataset.RI evaluates the agreement between two data partitions by computing the number of data points that exist in the same and different clusters, which is defined as follows,(37)RI=n00+n11N(N−1)/2where n00 is the number of pairs of data points having different class labels and belonging to different clusters; n11 is the number of pairs of data points having the same class labels and belonging to the same clusters; N is the size of the whole dataset.Clearly, CA, NMI and RI take values within the interval [0 1]. The higher the values, the better the clustering performance. All these metric values are equal to 1, when the clustering result and the original class label completely match [32].In this experiment, we compare the performance of the proposed OEWSC and OFWSC against EWSC, FWSC, OSKM, SPKM and FCM on six simulated datasets acquired from the same generation algorithm presented in Jing et al. [6] and Lu et al. [33]. The cluster structures of these datasets are controlled by three parameters: the subspace ratio ɛ, which is the average dimension ratio of the subspace to that of the whole space; the dimension overlap ratio ρ, which is the dimension ratio of the overlapping subspace; and the data overlap ratio α, which is the overlapping rate between two Gaussian clusters. In synthetic dataset I, we generate 6 datasets with different values of ρ and α, taking from the set {0.5, 0.8} and {0.2, 0.5, 2}. The subspace ratio ɛ is fixed at 0.375 for all datasets, as the authors did in their papers [6,33].Each of the synthetic datasets has 500 data points, 100 dimensions and 10 clusters, with each cluster containing 50 samples. For each cluster, the distribution of data points in the relevant dimension is normally distributed, where the corresponding means are specified in the range [0, 100] and the corresponding variance is set to 10. The distribution in the irrelevant dimensions is uniformly distributed in the range [0, 100].The means (mean) and standard deviations (std) of the CA, NMI and RI values achieved by each algorithm are shown in Tables 1–3respectively. It is evident that our proposed online soft subspace clustering algorithms perform much better than the other five methods. In fact, OEWSC demonstrates the best performance on the six synthetic datasets. We also find that FCM performs less effectively than other approaches, which is due primarily to the feature weightings having equal importance, as well as the fact that FCM cannot reveal the important local subspace characteristics of high dimensional data.To compare the overall performance and stability of the proposed OEWSC and OFWSC algorithms, the average clustering results and the corresponding standard deviations of CA, NMI and RI are plotted in Fig. 4. In the left-hand graph, the vertical axis represents the means of three performance metrics, and in the graph on the right, it represents the standard deviations of the clustering results. It can readily be seen that: (1) OEWSC and OFWSC obtain the highest and second highest average values of the three performance metrics; (2) OEWSC, OFWSC and OSKM generate better and more stable clustering results than EWSC, FWSC and SPKM respectively. Hence, it follows that the online learning strategy-based clustering methods indeed perform better than the traditional batch versions. To sum up, online soft subspace clustering shows greater ability to partition the high dimensional data and is more stable in subspace clustering analysis.The average running times on synthetic dataset I, in seconds, are as follows: OEWSC: 0.4063, OFWSC: 0.5353, EWSC: 0.3407, FWSC: 0.4764, OSKM: 0.1278, SPKM: 0.0587, and FCM: 0.2951. It can be seen that the running time of online soft subspace clustering is a little longer than that of the other methods, since much more computational complexity is needed to calculate the cluster feature weightings; in addition, they have to go through all data points more than one time.In this section, we present the experimental results on UCI datasets. Six real-life datasets taken from the University of California, Irvine (UCI) Machine Learning repository [34] are used to compare the performances of the proposed OEWSC and OFWSC algorithms. The datasets are summarized in Table 4.Tables 5–7show the means and standard deviations of CA, NMI and RI values conducted with the UCI datasets by executing each of the seven algorithms respectively. The results indicate that OEWSC and OFWSC perform better than all other algorithms in most cases. In fact, OEWSC and OFWSC demonstrate the best performance on five of the six UCI datasets. However, even though FCM is in general inferior to other methods, it achieves the best clustering performance for the Breast-diagnostic dataset. It is possible that the good performance of FCM is due in part to the geometrical shape in the Breast-diagnostic dataset, as well as the fact that FCM can easily detect the spherical clusters. This also suggests that it is necessary to choose the appropriate clustering method, since there is no single algorithm that is always superior to others for all datasets.The subspace detection results of the various subspace clustering methods performed on Iris and Wine datasets are reported in Table 8, which illustrates the top feature weightings detected in each cluster. From Table 8, we see that the sequence numbers of features detected by OEWSC, OFWSC and EWSC in each corresponding cluster are similar, whereas the sequence numbers achieved by FWSC are different from the detected results of other algorithms. We also observe that the experiment results of FWSC in Tables 5–7 are less effective than OEWSC, OFWSC and EWSC. Similar results are achieved with other UCI datasets.In this subsection, five benchmarking gene expression datasets [35–39], as shown in Table 9, are used to test the performance of the proposed OEWSC and OFWSC methods. Like most bioinformatics datasets, the gene expression data are characterized with few instances but many features, leading to the curse of dimensionality.In terms of the means and standard deviations of the CA, NMI and RI values, Tables 10–12show the clustering results achieved by using the gene expression datasets. From the comparison, we observe that OEWSC and OFWSC outperform all other approaches in most cases, since they adopt the online learning strategy and integrate the WTA scheme into the soft subspace clustering analysis. By comparing of Tables 10–12, we see that sometimes an algorithm showing the best clustering performance as indicated by CA may not have the highest values by NMI or RI. Thus, it is necessary to evaluate the clustering performances with different metrics.In addition to the three rigorous measures of clustering results, we also calculated the P-value of online soft subspace clustering produced by t-test at the 5% significance level, which is often used to illustrate the statistical significance of the proposed algorithms. Tables 13 and 14report the P-values of OEWSC and OFWSC compared to other algorithms in respect of the NMI results. As a null hypothesis, it is assumed that there is no significant difference between the NMI results of the two groups, whereas the alternative hypothesis is that there is a significant difference in the final clustering solutions. From these tables, we find that most of the P-values are less than 0.05 (5% significance level), indicating that better clustering results produced by OEWSC and OFWSC are statistically significant and have not occurred by chance.In summary, experimental comparisons on synthetic datasets I, UCI datasets and gene expression datasets demonstrate that, by extending the online learning strategy to soft subspace clustering, OEWSC and OFWSC achieve better clustering results than the batch versions, and can also be highly competitive among state-of-the-art clustering approaches.In this experiment, we adopt a synthetic dataset to conduct our data stream clustering experiment, in which the dataset is derived from the same generation algorithm as that in [3]. The dataset has 1750 data points, 41 dimensions and 7 clusters. The seven clusters locate at different subspaces with controlled cluster structures. The distribution of the various dimensions is presented in Fig. 5. For each cluster, the feature distributions in the relevant dimension are normally distributed, while the feature distributions in the irrelevant dimensions are uniformly distributed. Thus, different clusters have different important dimensional features. For example, the 1st–6th features are important for Cluster 1 and 36th–41st features are the important features for Cluster 7. Subspace clustering is an efficient approach for performing clustering analysis for this kind of data.To investigate the performance of SSSC_E and SSSC_F for streaming data, we randomly divide synthetic dataset II into several chunks and test SSSC_E and SSSC_F on the data streams, whereas online or batch clustering methods are run once on the entire dataset for demonstration. The means and standard deviations of the CA, NMI and RI values achieved by each algorithm are presented in Table 15, where the size of chunk for the data streams take 10%, 20% and 50% of the dataset, respectively.From Table 15, a number of observations can be made by analyzing the results. First, SSSC_E and SSSC_F attain excellent performance metrics on the streaming data, and all measurements show there is almost no great difference between streaming soft subspace clustering and online or batch subspace clustering algorithms. For SSSC_E and SFCM especially, the streaming clustering methods achieve better results than the online and batch algorithms. Second, there is a clear trend in the performance of the data stream clustering methods that coincides with variations in the size of the data segment: the data stream clustering performances increase with the fixed percentages of the dataset. Third, the clustering results obtained by online clustering, OEWSC, OFWSC and OSKM, are superior to their batch versions, EWSC, FWSC and SPKM, consistent with our online soft subspace clustering experiments in Section 5.3. Furthermore, as is evident from the last column of Table 15, the average run time (seconds) of the streaming soft subspace clustering algorithms is usually a little longer than that of the online or batch clustering approaches due to the computation of online learning strategy and scalable clustering process. We will attempt to address the time consumption issue by taking advantage of the appropriate sampling technique in future.Table 16shows the subspace detection results of the six soft subspace clustering algorithms. It illustrates the 6 features detected that correspond to the top 6 feature weightings of each cluster. The detection results of SSSC_E and SSSC_F are shown with a chunk size determined at 20% of the dataset. The boldface numbers denote the sequence numbers of irrelevant features detected incorrectly. The results indicate that the subspace detection accuracy of streaming soft subspace clustering is competitive with that of other subspace clustering methods.To demonstrate the effectiveness of SSSC_E and SSSC_F for text data stream clustering, we extract four different structured text datasets from the 20-Newsgroups corpus [40]. The 20-Newsgroups corpus is a collection of 20,000 messages, collected from 20 different newsgroups with each group containing 1000 messages. The raw text data is first preprocessed to eliminate the common words using a stop list and to stem all the words using Porter's suffix-stripping algorithm. The words are then represented by the conventional tf-idf term weighting model, and each text document is considered as a vector in the term-space. In our experiment, we preprocess the original raw text corpus using the Bow toolkit [41], and obtain 18,846 documents, each of which is represented by 26,214-dimensional tf-idf feature vectors in the resulting dataset.Table 17summarizes the characteristics of four structured text datasets extracted from the 20-Newsgroups corpus. To ensure diversity in the datasets, we generate them from different group categories, e.g., Dataset A2 contains 1772 documents from alt.atheism, comp.graphics; Dataset B2 contains 1715 documents from talk.politics.mideast, talk.politics.misc; and so on. The categories in datasets A2 and A4 are semantically different since different categories share few relevant dimensions (words), whereas the categories in datasets B2 and B4 are semantically close to each other because the relevant dimensions of different categories overlap significantly.We randomly divide the four text datasets into several chunks to compare the clustering results of SSSC_E and SSSC_F for text stream datasets. Tables 18–21show the means and standard deviations of the CA, NMI and RI values of the data stream clustering algorithms with loaded fixed percentages (10%, 20% and 50%) of the datasets, compared to online or batch clustering approaches that are run once on an entire datasets.The results reported in these tables indicate that of the eleven algorithms, SSSC_E performs better than all other algorithms. OEWSC and EWSC also achieve high clustering accuracy since both of them are inferred from the entropy weighting subspace clustering scheme. From Tables 18–21, we find that although the average performances obtained by SOSKM, OSKM and SPKM are in general inferior to SSSC_E, OEWSC and EWSC, they achieve the second best clustering results for the text datasets. It is evident that the document vectors have directional characteristics. In our experiments, we also observed that streaming soft subspace clustering obtains equally good results as clustering the full data streams at once, which suggests that streaming soft subspace clustering derived from fuzzy scalable clustering strategy handles text data streams effectively. We must note, however, that the performances of SSSC_F, OWWSC and FWSC are not as good as entropy weighting subspace clustering approaches. This may be the result of our failure to find optimal values for the algorithm parameters; we will attempt to analysis this problem for text data stream clustering in the near future. In addition, the running time in seconds for the eleven different algorithms on text datasets is shown in the last column of each table. Unsurprisingly, the running times of the proposed streaming soft subspace clustering algorithms are a little longer than the running times of online and batch methods.The important relevant words (dimensions) in each cluster are reported in Fig. 6, which illustrates the feature weightings of each dimension for text dataset A2 detected by SSSC_E and SSSC_F with the size of chunk determined as 20% of the text dataset. In the graphs, the x-axis represents the feature (word) index and the y-axis represents the feature weighting. From the feature weightings, we can easily identify the important relevant dimensions of each cluster, which represent the keywords of the corresponding topic, i.e., jpeg, pixel, gif, visual for category comp.graphics and bibl, theist, jesus, religi for category alt.atheism. The keywords with high weightings can be used to understand the topics in document clustering analysis.In general, the experimental results on both synthetic dataset II and text data streams indicate that, by combining the efficient online soft subspace clustering methods with fuzzy scalable clustering, SSSC_E and SSSC_F obtain clustering performances as good as online and batch subspace clustering methods for high dimensional data streams.

@&#CONCLUSIONS@&#
