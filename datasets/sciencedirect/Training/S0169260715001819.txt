@&#MAIN-TITLE@&#
Non-invasive real-time prediction of inner knee temperatures during therapeutic cooling

@&#HIGHLIGHTS@&#
Non-invasive real-time prediction of inner body temperature variables during cryotherapy.Validated simulation model of the cryotherapeutic treatment.Machine learning methods on the simulated data to construct a predictive model.Feature ranking with the RReliefF method.Using only skin temperatures as input attributes gives excellent prediction.

@&#KEYPHRASES@&#
Biomedicine,Cryotherapy,Computer simulation,Predictive modeling,Feature ranking,

@&#ABSTRACT@&#
The paper addresses the issue of non-invasive real-time prediction of hidden inner body temperature variables during therapeutic cooling or heating and proposes a solution that uses computer simulations and machine learning. The proposed approach is applied on a real-world problem in the domain of biomedicine – prediction of inner knee temperatures during therapeutic cooling (cryotherapy) after anterior cruciate ligament (ACL) reconstructive surgery. A validated simulation model of the cryotherapeutic treatment is used to generate a substantial amount of diverse data from different simulation scenarios. We apply machine learning methods on the simulated data to construct a predictive model that provides a prediction for the inner temperature variable based on other system variables whose measurement is more feasible, i.e. skin temperatures. First, we perform feature ranking using the RReliefF method. Next, based on the feature ranking results, we investigate the predictive performance and time/memory efficiency of several predictive modeling methods: linear regression, regression trees, model trees, and ensembles of regression and model trees. Results have shown that using only temperatures from skin sensors as input attributes gives excellent prediction for the temperature in the knee center. Moreover, satisfying predictive accuracy is also achieved using short history of temperatures from just two skin sensors (placed anterior and posterior to the knee) as input variables. The model trees perform the best with prediction error in the same range as the accuracy of the simulated data (0.1°C). Furthermore, they satisfy the requirements for small memory size and real-time response. We successfully validate the best performing model tree with real data from in vivo temperature measurement from a patient undergoing cryotherapy after ACL reconstruction.

@&#INTRODUCTION@&#
Measurements in biomedicine are often difficult to perform because human subjects are involved. Many examples can be found, particularly in clinical procedures, where in vivo measurements are often not as accurate as desired [3], difficult, dangerous or even impossible to perform [46], especially if deep tissues or vital organs are in question [15,38]. Moreover, non-invasive medical procedures are emerging in the search for more reliable, less expensive, and risk-free medical technology for the future [31,34,40].Computer simulations provide safe and inexpensive insight into physiological processes. In recent decades, computer simulations have significantly helped to better understand and solve a variety of problems in science. Advances in computer technology enable simulation of natural phenomena that cannot be subject to experiments in reality because of ecological, hazardous or financial obstructions [18]. With the use of computer simulation, it is possible to calculate, analyze, and visualize both stationary and time-dependent temperature fields in living biological tissues [11]. The temperature of the human tissue is an important factor in many fields of physiology [51], sports [14], cryotherapy [19], etc.In the context of the issues addressed above, computer simulations can be useful to estimate an immeasurable variable or a variable difficult to be measured based on other variables of the system/process whose measurement is more feasible – a concept known as soft or virtual sensing [2]. In the case of biomedical systems, if the variable of interest cannot be measured because of the non-invasive nature of the system – hidden system variable, then we should measure other noninvasive variables that can be used to estimate the hidden one. However, simulations are usually resource- and time-consuming which is not acceptable in real-time systems [42]. We assume that real-time biomedical systems, for example, for the purpose of controlling the variable of interest, require a solution usually deployed on a mini on-board computer that cannot process large amount of data nor perform computationally demanding operations.To this end, we use machine learning methods [20] to construct a predictive model that will provide a prediction for the hidden variable in a much shorter time with satisfying accuracy. Namely, we use simulation to generate a substantial amount of data for different input simulation parameters: capture the correlation between the hidden system variable and the non-invasive measurable ones. The data generated in this way is then used to construct a predictive model. There are two main advantages of using predictive models: (1) a predictive model can be used to simplify the simulation model and elucidate the most important correlations between the measurable (input) variables and non-measurable (output) variables, and (2) the predictive models are typically more efficient in terms of memory and computational complexity. Various machine learning techniques have proven to be adequate for extracting knowledge from data resulting from simulation models in various areas like medicine [5,10], ecology [9], social sciences [21], etc.In this paper, we apply this approach on a real-world problem in the domain of biomedicine – non-invasive real-time prediction of inner knee temperature during cryotherapeutic treatment after anterior cruciate ligament (ACL) reconstructive surgery of a knee. It is known, mostly from empirical evidence, that cryotherapy following reconstruction contributes to reduced tissue edema, inflammation, hematoma formation and pain, reducing the need for pain medication and enabling faster rehabilitation [35]. Various cooling modalities are routinely used in postoperative treatment in orthopedics, traumatology, facial surgery, pain prevention in sport [11], etc. In a previous study, we have shown that computer-controlled cryotherapy with pre-programmed protocols in terms of heat extraction intensity and treatment time is more effective and controllable than a conventional cooling with gel-packs [29]. We measured in vivo temperatures of the inner and outer knee parts and assessed the effectiveness of both methods. Moreover, we confirmed delayed and less severe pain in patients with the computer-controlled cryotherapy. However, a lack of uniformity in patients’ response to the cooling was confirmed, which raised the need for “smart” cooling devices, i.e. personalized cryotherapy. Different patients need different cooling protocols, depending on their constitution and regulatory systems, and on environmental conditions. A “smart” cooling device would be able to perform cooling adapted to the individual patient's response. Therefore, we propose an upgrade of the method for computer-controlled cryotherapy by introducing automatic control of the temperature inside the knee by changing the cooling temperature in the pad [28]. A few non-invasive temperature sensors on the knee surface, providing data about the actual heat transfer and the physiological response of the patient, should give a feedback for the process of control. To control the deep knee temperature successfully in an arbitrary knee without measuring it, we need to estimate the deep temperature from the non-invasively measured data using predictive models – the main goal of this paper. The light-weighted predictive models will then correspond to the mini architecture of the computer that controls the cryotherapy. Our previous work in the field of heat transfer in biological tissues forms the ground base for providing the simulated data [30].The machine learning tasks that we address in this paper belong to the predictive modeling setting where the goal is to predict the value of one property of the examples (called a target attribute or output) using the values of the remaining properties (called descriptive or input attributes) [17]. Considering that the solution requires efficient predictive models, we investigate and evaluate several state-of-the-art machine learning methods. The machine learning methods that we will consider include simple methods, such as linear regression and regression trees, as well as more complex methods, such as model trees, and ensembles of regression and model trees. The simple methods will also provide efficient and understandable predictive models, i.e., with these models we will be able to infer the most important correlations between the input and output variables. The complex methods, on the other hand, will be black-box models and will offer state-of-the-art predictive power. The methods will be evaluated based on the predictive power and the time/memory efficiency of the models they construct. At the end, we will select the best performing method that can be further used, for example, for control of the hidden output variable in real-time. Moreover, we validate the best performing method with real data from in vivo temperature measurement from a patient undergoing cryotherapy after ACL reconstruction. Furthermore, we use methods for feature ranking to provide an ordered list of the input variables by their relevance or importance for the output/target variable [16]. We use these methods to investigate which input variables are sufficient for making a good prediction and to which extent we need to measure them in time. The expected benefit would be the reduction of the number of sensors and construction of even smaller and more efficient predictive models.The rest of the paper is organized as follows. In Section 2, we present the background knowledge from computer simulations of topical cooling and the design of the simulation scenarios in order to generate the data set for the machine learning tasks. The simulation model has been previously validated on a set of measurements from therapeutic knee cooling [42,45]. Next, in Section 3, we present the machine learning tasks considered – feature ranking and predictive modeling methods. In Section 4, the results from feature ranking, and the evaluation of the predictive performance and time/memory efficiency of the constructed predictive models, are presented and discussed. Finally, the paper concludes with a summary of the results and directions for further work in Section 5.In this section, we first present the computer simulation model for therapeutic knee cooling. After that, we give the settings for various simulation scenarios used to generate a large amount of data for the machine learning tasks.In computer simulations, a physical system is represented by a mathematical model that cannot be solved analytically. Thus, an adequate numerical approach is required to obtain the numerical solution [13]. For this purpose, the continuous physical domain is replaced by its discrete form, usually by imposing a grid and partitioning the physical domain into many small sub-domains. Solving the mathematical model over discretized domain gives the value of a certain physical quantity at every grid point for each time interval. In the majority of simulations, the classical numerical methods, such as the Finite Difference Method (FDM) or the Finite Element Method (FEM) [42], are used.In this paper, the interest is in computer simulations of heat transfer in biological tissues. The basis of the simulation model for heat transfer in biological tissues is the well-known Pennes’ Bio-heat equation that incorporates classical heat transport and the heat generated from the biological response of the tissues, i.e., the heat transfer between the arterial blood and the tissues, and the heat production by tissue metabolism [26,48]:(1)∂T∂t=1ρcp∇⋅(λ∇T)︸heat transport+ρbcpbρcpVr(T)(Ta−T)︸blood perfusion+1cphm(T)︸metabolic heat production︷biological response,where ∇=(∂/∂x, ∂/∂y, ∂/∂z) is differential operator in terms of Cartesian coordinates (x, y, z), T is the temperature of a tissue, t is time, ρ, cpand λ are density, specific heat, and thermal conductivity of a tissue, ρbandcpbare density and specific heat of the blood. The function Vr(T) is the blood flow rate and the function hm(T) is the metabolic heat production per unit mass. The temperature Ta=36.8°C stands for the reference tissue temperature.The total blood flow is separated into regional blood flows Vr(T) for each tissue:(2)Vr(T)=vrV(T).The regional blood flow rate Vr(T) is modeled with the exponential function V(Ts) multiplied with dimensionless regional blood flow rate fraction vr[43]:(3)V(Ts)=(5.142⋅10−5e0.322Ts+0.705)ml(100ml)−1min−1,where Tsis the skin temperature. The fraction vris introduced to enable different blood flow rates in different tissues and it was not included in the original Pennes’ equation. The vrvalues for different tissues were determined based on measured ratios between blood flow rates in different tissues [39] using an iterative procedure while maintaining the mean blood flow rate [43].The rate of metabolic heat production per unit mass hmis assumed to obey the Q10 rule [41], which is expressed as a function of the tissue temperature:(4)hm(T)=hr2(T−Tr)/10,where hrstands for the reference metabolic heat production of a tissue at reference temperature Tr=35°C. For muscles at rest, hrwas taken to be about half the human basal metabolic rate, which equals 0.6W/kg [37].The heat flow between the skin surface and the ambient air is simulated using convective boundary conditions [4], based on the continuity of the heat flow perpendicular to the surface of the simulated body part:(5)λ∂T∂n=H(Tam−T),where Tamstands for the ambient temperature. The convection coefficient H=16Wm2/°C is determined by numerical experiments, based on simulation of the thermo-neutral conditions (Tam=26°C and Ts=33°C) [43], maintained also by Pennes’ measurements. ∂/∂n denotes derivative along the outward normal to the face of the surface voxel in contact with the air.Outside biological tissues, the Bio-heat equation is reduced to diffusion equation as both additional source terms (heat transfer between blood and tissues, and metabolism) vanish. No flow is allowed in the tissue and only incompressible fluids are considered. A substantial amount of work on closed form and numerical solutions of the Bio-heat equation has been published [4,8,22–24,26,50]. Our previous work in this field includes computer simulation of heat transfer in different body parts [30]. Spatial models of human body parts, in particular, a human heart [38,46], knee [45] and forearm [43], along with parallel programs for simulation of their cooling during and after surgery, have been developed.The paper concerns the simulation of topical cooling (cryotherapy) of a human knee after ACL reconstructive surgery. The computational domain of the simulation is composed of the knee model and surrounding layers: protective bandage, cooling layer, isolating blanket and ambient air. The knee is modeled as a 3D closed cavity. The model is obtained through expert assisted segmentation of human body parts from the Visible Human Dataset (VHD) [1]. The body parts are composed of different tissues (muscles, bones, cartilage, fat, vessels and nerves). Considering the complexity of the modeled body part, spatial dependent thermo-physical properties are employed. The final geometric domain is composed of small cubic voxels, resulting in millions of discretization points.The initial conditions of the simulation comprise the initial temperatures of the modeled tissues. The temperature of the blood in the artery is set to a constant value of 36.8°C with Dirichlet boundary conditions on the artery walls, assuming well-mixed blood with a significant flow. Moving air is not simulated because of its significant contribution to the calculation complexity [44]. The ambient air is assumed to be well-mixed and thus has a constant temperature. The heat flux from the first and last slices is kept constant in order to imitate the influence of the leg not exposed to the cooling. Simulations are performed on the described model using realistic process parameters gathered from measurements (Table 1).The governing system of PDEs is numerically solved on parallel computers using the Explicit Finite Difference Method (EFDM) [25]. It is a simple implementation of a numerical scheme for inhomogeneous tissue with low calculation complexity and straightforward parallelization. Using FDM, the geometric domain is first discretized in space with a rectangular mesh of points with four neighbors in 2D or with six neighbors in 3D. The spatial derivatives in the mathematical model are replaced by finite differences. An equation is formed for each voxel using collocation, which states that the model equations should be satisfied exactly at all points. After spatial discretization, the PDEs are transformed into a system of ordinary differential equations (ODEs) that can be solved using time discretization. The time discretization method used is the explicit Euler's method, because the approximate solution value in the next time-step is obtained from already known solutions in the previous time-step. The explicit methods are simple and require just matrix-vector multiplication at each time-step, while implicit methods require solution of a system of equations at each time-step in order to proceed to the next simulation time-step. Thus, by approximating the time derivatives with finite differences, a system of algebraic equations is obtained, which can be solved analytically. Starting with the initial condition and stepping in time, we can get the approximate solution of the system for each voxel at any time step.The method described has several limitations. Spatial models differ with different persons and with time, and consequently the simulated results can differ. Minor errors in tissue segmentation and inaccurate thermodynamic constants could also produce small errors in the simulated results. Personal regulatory mechanisms have not been included in the simulation model but could easily be incorporated. All such limitations could have some impact on the simulated temperatures; however, previous validations of the simulation model for different cooling or heating methods of different body parts have confirmed that the used model and numerical procedure are appropriate for simulation of heat transfer in bio-tissues [30,43,45]. For example, in Fig. 1, the simulated steady-state temperature profile for a naked knee and the temperature profile after 90min of simulated cooling at 9°C are shown for the model slice z=101 along the y (anteroposterior) axis for x=105 (the center of the knee, i.e. in the intercondylar notch). The figure shows that the gradients in the temperature profiles after cooling are much more pronounced compared to the steady-state.Computer simulation data of human knee temperatures during cooling after surgery provides the basis for the predictive modeling tasks. A substantial amount of diverse data is needed for the predictive model to capture the relation between the inner knee temperatures and the outer knee temperatures with satisfying accuracy for a wide range of human knees and cooling settings. Therefore, using the presented simulation model, the simulation data is generated from different simulation scenarios with variation of the simulation parameters. Previous evaluations [30] have shown that variations in the knee dimension and the blood flow rate have the most important impact on the temperature profiles. Moreover, analyses showed that the difference between the simulations in 3D and in 2D under the same conditions is not significant. We have thus performed 2D computer simulation of two hours of knee cooling after surgery for 1400 different settings resulting from:•10 knee dimension settings, starting from normal knee dimension and then scaling down to a small knee and up to a big knee. The variations in knee dimension were set in the range of real-value human knee dimensions.10 blood flow rate settings, starting from normal blood flow rate with the blood flow parameters for different tissues set as in Table 1, and then scaling linearly up and down by 20%.14 cooling temperatures ranging from 2°C to 15°C. The range and the accuracy of 1°C for the cooling temperature was set according to the specifications of today's cryotherapeutic devices, namely, the CTS100 by Waegener [47].Simulation of arthroscopic surgery and resting after surgery, starting from steady-state, preceded the simulation of cooling for each setting. The model slice 101 from the central knee region was taken as computational domain in 2D. For each setting, the temperature distribution for the whole computational domain was saved for two hours every second (7200s) with an accuracy of 0.1°C. The data resulting from the simulation totaled 10×10×14×7200=10,080,000 temperature distribution files. Each file had size of approximately 1MB, which resulted in approximately 10TB of raw simulation data. This data was further processed to obtain the dataset for the tasks of predictive modeling.In this section, we first describe the pre-processing of the simulation results to obtain the dataset for the machine learning tasks. Next, we present the two machine learning tasks used to analyze the data: we first present the RReliefF feature ranking algorithm and then overview several methods for learning regression predictive models.The main machine learning task considered here belongs to the predictive modeling setting, i.e., building a predictive model that will predict the inner knee temperature as output or target attribute based on other outer knee temperatures as descriptive or input attributes. The simulated raw data was pre-processed to obtain the dataset for the predictive modeling tasks. The positions of the voxels whose temperature was extracted as input and output attributes are schematically shown in Fig. 2on the geometric model of the knee used in the simulation.The input temperature attributes are temperatures on four locations on the knee skin (SK1, SK2, SK3, SK4), and on four respective locations on the protective bandage (BA1, BA2, BA3, BA4). The output temperature is the temperature in the intercondylar notch (ICN). Furthermore, the knee dimension (KD) and the temperature of the cooling liquid (CT) are also considered as input attributes. The dataset also includes a short history of each input temperature attribute: the values of the temperature attributes are all taken in the current and in five previous time steps with the resolution of 1s, meaning that the data extracted from a temperature distribution file are accompanied with data extracted from five previous temperature distribution files. The notations of the input temperature attributes for the training dataset are given in Table 2.An instance for the resulting dataset was extracted from each of the 10,080,000 temperature distribution files. The dataset was further checked for duplicate instances and they were excluded from further analysis because such samples could influence the distribution of the target variable and hence disturb the learning of the predictive model. The pre-processing resulted in downsize of the data from around 10TB raw simulation data to 591MB large dataset.The first task performed on the dataset is feature ranking using the RReliefF algorithm [32]. The major competitive advantage of the Relief family of algorithms (including RReliefF), compared to other methods for estimating the importance of features, is that they are able to capture the relevance for features involved in feature interactions [33]. Moreover, these algorithms are efficient, aware of the contextual information, and able to correctly estimate the variable relevance in domains that contain strong dependencies between the variables.The RReliefF algorithm begins with a random selection of an example from the training data. Next, it looks for its k-nearest neighbors and calculates the importance of each attribute. The importance is calculated using a probabilistic definition about the distance between the predicted values. Moreover, the neighbors closer to the selected example influence more on the overall score than the ones that are more distant (exponentially decreasing influence defined with the exponent sigma parameter of the algorithm).The RReliefF algorithm has two parameters that need to be instantiated depending on the application domain: sample size and number of nearest neighbors. The number of randomly selected examples (sample size), used to estimate the importance of the features, corresponds to the iterations parameter of the algorithm (as called within the WEKA [49] tool). In Ref. [33], it is shown that the estimates of attributes’ importance stabilize relatively fast, i.e., after only several tens of iterations; nonetheless, this value is application dependent. The second parameter of the RReliefF algorithm is the number of nearest neighbors considered for the estimates of the attributes importance. This parameter controls the amount of local or global information that the algorithm exploits. The RReliefF algorithm is relatively robust to the number of nearest neighbors as long as this number remains relatively small compared to the sample size.For selecting the proper parameter values for the algorithm, we have employed the recommendations given in Ref. [33]. For the sample size, i.e. the iteration parameter of the algorithm, we have set the value to 1000 examples. We have also performed experiments with smaller and larger sample sizes; however, the results were quite similar to each other, which indicates that the estimates of attributes’ importance are stable. For the number of nearest neighbors, we have set the value to 70 neighbors with exponentially decreasing influence (the value of the sigma parameter was set to 20).To further obtain more stable results, we use 10-fold cross-validation. This procedure in the context of feature ranking is as follows. First, the training dataset is randomly partitioned into 10 subsets with equal size (each subset does not contain parts from the other subsets). Next, using 9 of the subsets, a new training dataset is constructed. This is repeated for 10 times, each time leaving one subset aside. On each of the 10 new training datasets, a feature ranking is then performed; thus, yielding 10 feature rankings. Finally, the estimates of attributes’ importance and the ranks of the features are averaged across the 10 new training datasets and they are reported as average merit and average rank, together with the respective standard deviation.The second machine learning task is the learning of predictive models based on specific scenarios (datasets from a subset of input attributes) regarding the feature ranking results. For each of the defined scenarios, we evaluate the performance of several methods for predictive modeling: linear regression, regression trees and model trees for different pruning parameters (i.e., variation of the minimal number of examples per leaf), and ensembles of regression and model trees. We briefly present these methods and the evaluation methodology in the remainder of this section.Linear regression is one of the simplest methods for solving regression tasks and it has been used for decades. The main rationale behind this method is to calculate the target variable as a linear combination of the descriptive attributes using weights that are calculated on the training dataset. Although linear regression is a very simple and efficient method, the produced models suffer from the fact that they look for linear relations that possibly exist in the data [49]. However, the data in a large majority of real-life applications exhibit some non-linarites. Thus, the linear model with the best-fitted straight line (best in the sense of least squared difference) may not fit very well on the data. Nonetheless, these models can be used as components of more complex methods, such as model trees.Regression and model trees are very similar predictive models. Both regression and model trees are decision trees for predicting the value of a continuous/numeric target variable [7]. Regression and model trees are hierarchical structures where the internal nodes contain tests on the input attributes. Each branch of an internal test corresponds to an outcome of the test, and the prediction for the value of the target attribute is stored in a leaf. The only difference between regression and model trees is how they calculate the prediction. Namely, leafs of a regression tree contain constant values as predictions for the target variable; therefore, a regression tree represents a piece-wise constant function. On the other hand, leafs of a model tree contain a linear function calculated using the examples belonging to the given leaf [27]; hence, a model tree represents a piece-wise linear function. In this paper, we use the M5P implementation of these models from the WEKA machine learning tool. The tree construction algorithm belongs to the group of greedy divide-and-conquer algorithms. The construction algorithm is known as Top-Down Induction of Decision Trees (TDIDT).After a regression/model tree is constructed, it is common to prune it in order to improve its predictive power and interpretability [12]. The pruning protects against over-fitting the constructed trees to the training data at hand and improves the predictive performance on unseen examples. The pruning methods are typically integrated into the tree construction algorithm as stopping criteria and they are called pre-pruning methods.The predictive performance of the regression and model trees can be further improved if they are combined into an ensemble [6]. An ensemble is a set of base predictive models whose predictions are combined to obtain an overall prediction. The performance of the ensembles is better if the base predictive models have good predictive performance and are different among each other. Most famous ensemble methods for tree-based predictive models are bagging, random forests and boosting [36], with bagging being the most widely used method. Bagging constructs its base predictive models using bootstrap replicates of the training data. To produce a prediction for an example, the ensemble sorts the example through each base predictive model and the overall prediction is obtained by averaging the predictions of the base predictive models.Considering that the solution requires efficient predictive models that perform in real-time, the methods are evaluated based on the time/memory efficiency of the models they construct and their predictive performance. The models’ time efficiency is measured as time needed to produce a prediction for a new instance, and the models’ memory efficiency is measured as the size of the model file produced by the WEKA modeling suite (note that this size is related also to the model size in terms of tree nodes and/or equation terms). Given the design request for the models to perform real-time on a small on-board computer (with limited memory and computing power), we will aim to select models that have fast response and are light-weighted.As measures for the models’ predictive performance, we used correlation coefficient (CC) and mean absolute error (MAE) between the real and the predicted values of the target variable. The predictive performance of a predictive model is the estimation of the models’ performance on new and previously unseen examples. Typically, the predictive performance is estimated using hold-out testing set of examples or by using k-fold cross validation (i.e., its most famous variant – 10-fold cross validation). The latter is a standard statistical procedure for performance estimation widely used by the machine learning research community and also in the paper. This procedure in the context of predictive modeling is as follows. First, the original dataset is randomly partitioned into 10 equal-size subsets. Next, a single subset is retained for validation of the model, and the remaining 9 subsets are used as training data. The cross-validation process is then repeated 10 times (the folds), with each of the 10 subsets used exactly once as validation data. Finally, the 10 results from each fold are averaged (or otherwise combined) to produce a single estimation.

@&#CONCLUSIONS@&#
The paper implements and evaluates a method for real-time prediction of hidden variables during therapeutic cooling or heating where the inner body temperature cannot be measured either because the system is non-invasive, like most of the examples in biomedicine, or for any other reason. Therefore, other measurable variables of the system are used to estimate the hidden output variable of interest. The paper leverages two research areas from computer science and engineering for that purpose: computer simulations – to provide safe and inexpensive insight into the studied process/system, and machine learning – to provide methods for predicting the hidden output variable from the measurable ones based on the data generated from the simulations. The proposed approach is applied on a real-world problem in the domain of biomedicine – non-invasive real-time prediction of the inner knee temperature during cryotherapy after knee surgery.Using the dataset from the simulated results, we have performed predictive modeling. First, feature ranking using the RReliefF method revealed that the most important features for predicting the inner knee temperature are the skin temperatures that correspond to the temperatures of skin sensors placed anterior and posterior to the knee. Based on the feature ranking results, we have investigated several scenarios for the performance of several methods for predictive modeling: linear regression, regression trees, model trees, and ensembles of regression and model trees. The model trees performed the best with prediction error in the same range as the accuracy of the simulated data (0.1°C). Moreover, they satisfy the requirements for small memory size and real-time response. Results have shown that using only temperatures from skin sensors as input attributes gives excellent prediction for the temperature in the knee center. Moreover, satisfying predictive accuracy is also achieved using short history of temperatures from just two skin sensors (placed anterior and posterior to the knee) as input variables.The need for non-invasive prediction of inner knee temperature during therapeutic cooling is based on the finding that the cryotherapy should be controlled depending on the individual patient's response [29], which raises the need for “smart” cooling devices for personalized therapy. Therefore, the proposed method for non-invasive prediction of inner knee temperature during therapeutic cooling can be included in a framework for real-time control of inner knee temperature based on the feedback control loop that uses predicted, instead of measured, inner temperatures because measurements are not feasible or would introduce invasiveness into the system [28].The work presented in this paper can be extended in several directions. First, for achieving even better predictive performance in real-life therapeutic cooling, the dataset can be extended with simulated data for a wider range and/or higher resolution of already varied input parameters (cooling temperature, knee dimension and blood flow rate), or variations of other input simulation parameters. Next, the geometric model of the knee for the simulation can be personalized for each patient using, for example, the Magnetic Resonance Imaging (MRI) of the knee made before surgery. Finally, the proposed method for non-invasive real-time prediction of hidden inner body temperature variables can be applied for prediction of inner temperatures during therapeutic cooling or heating of any part of the body. The application includes using already developed valid simulation model of bio-heat transfer in the treated body part. The modular design and implementation allow for easy inclusion of different simulation or predictive models.