@&#MAIN-TITLE@&#
An enhanced one-dimensional SPIHT algorithm and its implementation for TV systems

@&#HIGHLIGHTS@&#
In the hybrid coding scheme, DPCM–VLC is used for the low-pass band and 1D SPIHT is used for the high-pass band.DPCM–VLC is improved by the additional sub-schemes.Target bit length of each block is determined by the block-based bit allocation scheme.Block complexity is estimated by using the coding information of the upper block.The proposed 1D SPIHT algorithm is implemented in hardware design.

@&#KEYPHRASES@&#
Color image coding,Discrete wavelet transform (DWT),Set partitioning in hierarchical trees (SPIHT),Differential pulse code modulation (DPCM),Golomb–Rice coding,Bit allocation,

@&#ABSTRACT@&#
In general, to achieve high compression efficiency, a 2D image or a 2D block is used as the compression unit. However, 2D compression requires a large memory size and long latency when input data are received in a raster scan order that is common in existing TV systems. To address this problem, a 1D compression algorithm that uses a 1D block as the compression unit is proposed. 1D set partitioning in hierarchical trees (SPIHT) is an effective compression algorithm that fits the encoded bit length to the target bit length precisely. However, the 1D SPIHT can have low compression efficiency because 1D discrete wavelet transform (DWT) cannot make use of the redundancy in the vertical direction. This paper proposes two schemes for improving compression efficiency in the 1D SPIHT. First, a hybrid coding scheme that uses different coding algorithms for the low and high frequency bands is proposed. For the low-pass band, a differential pulse code modulation–variable length coding (DPCM–VLC) is adopted, whereas a 1D SPIHT is used for the high-pass band. Second, a scheme that determines the target bit length of each block by using spatial correlation with a minimal increase in complexity is proposed. Experimental results show that the proposed algorithm improves the average peak signal to noise ratio (PSNR) by 2.97dB compared with the conventional 1D SPIHT algorithm. With the hardware implementation, the throughputs of both encoder and decoder designs are 6.15Gbps, and gate counts of encoder and decoder designs are 42.8K and 57.7K, respectively.

@&#INTRODUCTION@&#
Recently, a resolution of TVs has increased rapidly, and the trend is expected to continue for a while. As high resolution requires a large memory and a high transmission bandwidth, video/image compression is becoming more and more important. The area of video/image compression which has been used in TV systems is diverse and it often requires high-fidelity. For example, an LCD overdriving technique requires a video frame to be stored in memory and then used to adjust the driving voltage according to the difference between current and previous frames. To reduce frame memory size, each frame is compressed by a compression ratio (CR) of between 3 and 6. Loss of information in the compressed frame degrades the image quality displayed on an LCD. Therefore, a compression technique used for LCD overdriving demands high-fidelity [1,2]. A frame memory compression (FMC), used for storing the reference frame in standard video compression formats such as H.264 or HEVC, also requires a high-fidelity technique [3,4].Video/image compression can be classified as lossy or lossless compressions [5]. Lossless compression is suitable for high-fidelity applications. However, lossless compression requires a large memory, a high transmission bandwidth, and high power consumption because its CR is low. Also, it is hard to restrict the frame memory size in the lossless compression as the CR is irregular. Therefore, lossless compression is not suitable for the commercial TV applications. On the other hand, as the CR in lossy compression is high, a small memory size, a low transmission bandwidth, and low power consumption can be achieved. Also, the size of the memory to store the compressed data can be fixed in lossy compression. The length of the encoded bitstream is not allowed to exceed the target bit length (TBL). In order to achieve high image quality under the condition of a limited memory size, the encoded bit length should be close to the TBL. There are many ways to fit the encoded bit length close to the TBL in lossy compression. For example, a Golomb–Rice coding, which is a kind of variable length coding (VLC), increases quantization granularity until the encoded bit length approximates the TBL [3,4]. However, this coding scheme requires iterative computation, and precise fitting of the encoded bit length to the TBL is difficult. The set partitioning in hierarchical trees (SPIHT) compression algorithm fits the encoded bit length to the TBL precisely [6]. In SPIHT, wavelet coefficients are encoded in a descending order of bit-planes. When every encoded bit is generated, the encoded bit length is compared with the TBL. If the encoded bit length equals to the TBL, the remaining data in the lower bit plane are discarded.In many compression algorithms, the input image is decomposed into small blocks and each block is compressed independently of the other blocks. The block to be encoded is temporarily stored in an internal buffer, the size of which may increase as the block size increases. When the image is received from a camera (or any other device), the input pixels are generally transmitted in the raster scan order. In order to encode an image block, the entire set of lines including the block needs to be stored in an internal buffer. For example, a 16×16 block in a full HD image (size 1920×1080) requires at least 16 lines (1920×16 pixels) to be stored prior to encoding. In order to reduce memory size, it is desirable to reduce the height of the block. For example, compression of 1×64 block may require only a single line of memory. Thus, a number of compression algorithms that compress blocks with a height equal to one have been proposed [7–9].The 1D SPIHT algorithm, a modification of the original 2D SPIHT algorithm, compresses a block with a height equal to one [8,9]. Thus, a line memory size of the 1D SPIHT algorithm is much smaller than that required by the original 2D SPIHT algorithm. A basic operation within the 1D SPIHT is almost the same as that within the original 2D SPIHT because 1D SPIHT encodes wavelet coefficients in the descending bit-plane order. However, unlike the original 2D SPIHT, 1D SPIHT cannot make use of the redundancy in the vertical direction, thus its compression efficiency is substantially reduced when compared with the original 2D SPIHT. In the previous 1D SPIHT presented in [9], the compression unit is an entire line of an image, thus its memory size is still large because it stores the image data of that line. To reduce memory requirement, research on the 1D block compression unit including a development of a block-based bit allocation scheme is needed.In order to improve compression efficiency, this paper proposes hybrid coding and bit allocation schemes for 1D SPIHT algorithm. To this end, wavelet coefficients in the low-pass band are encoded by a differential pulse code modulation (DPCM)–VLC algorithm while those in the high-pass band are encoded by 1D SPIHT. This approach is used because 1D SPIHT is not very effective in low-pass band compression. In order to allocate more bits to a complex block than that allocated to a simple block, a bit allocation scheme that differentiates the amount of bits allocated to each block based on its complexity is proposed.In this paper, the proposed schemes are implemented in hardware with Verilog HDL. As a resolution of recent displays has increased to UHD, a high-throughput hardware design is required. To this end, the speed of input and output data is assumed by two pixels per clock cycle and the data is assumed to come in and out continuously.When the target CR, the block size, and the DWT decomposition level are 3, 1×64, and 3, respectively, the proposed algorithm achieves an average peak signal to noise ratio (PSNR) of 46.95dB that is improved by 2.97dB compared with the conventional 1D SPIHT. The implemented hardware design achieves the throughput of 6.15 Gbps while running at 125MHz. Hardware logics of encoder and decoder designs are 42.8K and 57.7K gates, respectively.The remainder of this paper is organized as follows. In Section 2, previous works are introduced and in Section 3 the proposed 1D SPIHT algorithm that comprises the hybrid coding and block-based bit allocation schemes is explained. Section 4 presents experimental results, and the hardware implementation for the proposed algorithm is presented in Section 5. The conclusion is presented in Section 6.The SPIHT compression algorithm is based on a bit-plane coding and a DWT [6]. Let the initial threshold denote the highest one among the bit-planes that contain ‘1’. Since bit-planes are coded in a descending order from the initial threshold, the significant bits are coded first and the bits of the lower bit-planes are truncated when the TBL is not large enough to cover the lower bit-planes. Thus, SPIHT is effective in meeting the TBL with relatively low computational complexity and a small amount of image distortion.Fig. 1(a) shows a 2D spatial orientation trees (SOT) structure of wavelet coefficients when the block size is 16×16 and the DWT decomposition level is 3. Arrows represent the relationship between parents and their offspring forming a set. The SPIHT algorithm performs a significance test on each set which is classified as a significant set when the maximum coefficient of the set is larger than the threshold. Otherwise, the set is classified as an insignificant set. LetTdenote a set to be tested. A coefficient(i,j)inTis denoted byci,j. The threshold,Th, of the n-th bit-plane is denoted by2n(n=first non-zero bit-plane (FNZB), FNZB-1, …, 1, 0). The significance test is formulated as (1).(1)Sn(T)=1,max(i,j)∈T{|ci,j|}≥Th0,otherwiseA set classified as insignificant is coded with the symbol ‘0’. A set classified as significant is divided into subsets and the test in (1) is performed on each subset. If a subset is significant, it is divided into subsets again. When a subset has a single coefficient, the coefficient of a pixel is tested by the significance test in which the coefficient is compared with the threshold. When the coefficient is larger than the threshold, the magnitude bit ‘1’ and a sign bit are generated. This pixel is classified as significant that outputs a magnitude bit for all the subsequent bit-planes (refined). When the pixel is smaller than the threshold, the pixel outputs magnitude bit ‘0’ and the pixel is classified as insignificant. An insignificant pixel is retested in subsequent bit-planes until the pixel is classed as significant. The SPIHT process starts with the initial threshold and ends when all coefficients are coded or when the encoded bit length becomes the TBL. Note that one example of the conventional SPIHT by the author is shown in [10].In conventional SPIHT, coefficients of the low-pass band are initialized as a list of insignificant pixels (LIP). In this case, the significance test is performed for all bit-planes of those coefficients and magnitude bits are coded for every bit-plane until the encoded bit length becomes the TBL. Therefore, the compression efficiency of the low-pass band is low. Hybrid coding schemes in which different compression algorithms are used for the low-pass and high-pass bands have been proposed in various image compression studies [11,12]. Iano et al. proposes a hybrid fractal wavelet image coder in which the low-pass band is coded by a 2D fractal image coder and the high-pass band is coded by a 2D SPIHT [11]. However, the fractal image coder involves a lot of computations and resources, thus that approach is difficult to be implemented in a real-time system with limited resources.Conventional SPIHT encodes a 2D wavelet tree structure as shown in Fig. 1(a). In 2D SPIHT, memory size and latency increase when the input data is presented in raster scan order. To reduce memory size and latency, 1D SPIHT algorithms have been proposed [8,9]. The operation of the 1D SPIHT is similar to that of the 2D SPIHT, but the 1D SPIHT encodes a 1D wavelet tree structure as shown in Fig. 1(b). In the 1D SPIHT algorithm, compression efficiency is low because 1D DWT that cannot make use of the redundancy in the vertical direction lowers the compression performance compared to that in the 2D SPIHT. Therefore, the compression efficiency of 1D SPIHT needs to be improved while maintaining its small memory size and short latency. In particular, the compression efficiency of 1D SPIHT in the low-pass band is low and requires improvement.The 1D SPIHT codec proposed by Zhi-hui and Jun uses a line as the compression unit [9]. However, such compression unit leads to increase in memory size and latency. Consider a case where the compression unit is a line. In such a case, a line memory is needed to store wavelet-transformed coefficients before processing the 1D SPIHT. The size of the line memory depends on the horizontal resolution of the image. When the compression unit is a block that is obtained by dividing a line, both memory size and latency are reduced. In the block-based coding, each block has a TBL, which is determined by using (2).(2)TBL=blocksize×pixel widthCR,where CR=total bits before compressiontotal bits after compressionIn (2), assuming that the pixel width, block size, and target CR are 24 bits, 1×64, and 3, respectively, then the TBL of a block is 512.It may not be proper to use the same TBL for all blocks because block complexities differ. Several bit allocation schemes are proposed for use in other applications. Lee et al. uses a simple bit allocation which passes unused bits to the next block [3], but that approach is ineffective because the characteristics of the blocks are not considered. Seo et al. proposes a bit allocation and rate control algorithm for hierarchical video coding that allocates bits for macroblocks based on the calculated complexity of each macroblock [13]. This algorithm needs coding information on previous frames and requires complex calculations to obtain a variance of difference and a mean absolute difference. Therefore, a less complex bit allocation algorithm for block based 1D compression is needed.Conventional SPIHT codes wavelet coefficients in a dynamic order, which causes difficulties in hardware implementation. No-list SPIHT (NLS) based design increases the processing speed by using the fixed scanning order [14]. Bit-plane Parallel SPIHT encodes each bit-plane in parallel and pipelined manner, thus the throughput of the encoder achieves a very high throughput rate [15]. Block-based pass-parallel SPIHT reorganizes passes in SPIHT and pre-calculates the length of the bitstream to decode the passes in a parallel and pipelined manner, thus it achieves high throughputs for both the encoder and the decoder [16]. The previous hardware designs enhance the processing speed of SPIHT, but those are designed for 2D compression. Therefore, a novel hardware design for 1D SPIHT is needed.In order to improve compression efficiency, coefficients in the low-pass band should be coded more efficiently because those are added to LIP in the initialization step and output magnitude bits to every threshold even though those are added to list of significant pixels (LSP) during the compression. To this end, a hybrid coding scheme that uses a different algorithm for the low-pass band is proposed. Moreover, a block-based bit allocation scheme that allocates bits differently depending on block complexities by using information in the high-pass band is presented.In this section, a hybrid coding scheme that uses two independent compression algorithms for low-pass band and high-pass band is proposed. As shown in Fig. 2, the low-pass band is encoded by using DPCM–VLC, whereas 1D SPIHT is used for encoding the high-pass band. The DPCM–VLC is a simple algorithm, thus it is easy to be implemented in hardware. If coefficients in the low-pass band are similar each other, the DPCM–VLC can show a high coding efficiency.In DPCM that is used in the low-pass band, the difference between adjacent coefficients is calculated by applying (3), of whichcoef(n)is the value of the n-th coefficient (1⩽n<width of low-pass band). Usually, the magnitude of difference is small because of the spatial correlation. Thecoef(0)is the value of the leftmost pixel and is used as the reference coefficient which is encoded without subtraction.(3)diff(n)=coef(n)-coef(n-1)In (3),diff(n)is coded by Golomb–Rice coding in which the computational complexity is low and the CR is high [3,4,17]. To represent a negativediff(n)without a sign bit,diff(n)is converted tosource(n)by applying (4).(4)source(n)=2|diff(n)|,diff(n)≥02|diff(n)|-1,diff(n)<0In Golomb–Rice coding,source(n)is divided by a divisor k and an output codeword is generated by using the quotient and the remainder. The quotient is expressed in a unary notation and the remainder is represented in k bit-sized binary notation. For example, fordiff(n)of −29 with k of 4,source(n)becomes 57 and it is divided by 16 (=24) resulting in a quotient and remainder of 3 and 9, respectively. In this example, the unary notation of the quotient is 0001 and the k bit-sized binary notation of the remainder is 1001. Thus, the codeword becomes 00011001.In order to improve the performance of the DPCM–VLC encoding, three sub-schemes are proposed. In the conventional DPCM–VLC, every block has the reference coefficient. When the block size becomes small, the number of blocks in a line increases and the number of reference coefficients also increases. As reference coefficients are not compressed, compression efficiency decreases in block-based coding. To improve compression efficiency, the last coefficient of the previous block is stored and, then, it is used to compress the reference coefficient by DPCM–VLC in whichcoef(-1)fordiff(0)in (3) is defined as the stored coefficient. This sub-scheme is referred to as a reducing reference coefficient.The second sub-scheme uses an adaptive divisor k as shown in (5), in whichk(n)represents the n-th divisor expressed as2n[18].(5)k(n+1)=k(n)+source(n)2When k is large, the bits for the quotient decrease but the bits for the remainder increase. In contrast, when k is small, the bits for the quotient increase and those for the remainder decrease. Ifsource(n)value is large, a large value of k is efficient. Otherwise, a small value of k is desirable. Therefore, an adaptive divisor k is used to improve compression efficiency.Divisor k is updated by applying (5). However, the update of divisor k in (5) may decrease compression efficiency because new k is determined only by using the previousk(n)andsource(n)values. For instance, the updated divisor may be small because of the presence of continuous smallsource(n)values. If thesource(n)value increases rapidly, compression efficiency decreases because the divisor k is small and cannot increase rapidly as fast. In order to solve this problem, divisor k is reset as the default value when the quotient exceeds a predefined value. This sub-scheme is referred to as updating and resetting divisors.The length of the bitstream by DPCM–VLC is various. Especially, the length can be even larger than that of total bits before compression even though the updating and resetting divisors sub-scheme is used. In hardware implementation, the bitstream failed in compression causes a bitstream buffer size problem resulted from the low compression efficiency. As shown in Fig. 3, the proposed algorithm stores original coefficients of the low-pass band, and compares the length of total bits before compression (represented as original length in Fig. 3) and the length of generated bitstream. If the length of generated bitstream is larger than original length, original coefficients are included in the final bitstream with the syntax ‘1’. On the other hand, if the length of generated bitstream is equal to or less than the original length, the generated bitstream is included in the final bitstream with the syntax ‘0’. In the experiment by using twenty-four Kodak test images [19], the average ratio of blocks in which the length of generated bitstream is larger than original length is 5.23% that is small. In hardware implementation, the size of buffer in DPCM–VLC can be restricted easily by using this sub-scheme.In order to use DPCM–VLC for the low-pass band, initialization of the conventional SPIHT is modified. To that end, the proposed 1D SPIHT applies the initialization scheme of Iano et al. [11]. For self-containment, a description of the scheme used by Iano et al. is discussed. First, as coefficients in the low-pass band are not coded by using 1D SPIHT, LIP is initialized as an empty list. However, coefficients in the high-pass band are still coded by using 1D SPIHT, thus initializations of LSP and list of insignificant sets (LIS) are the same as those of conventional SPIHT. LSP is initialized as an empty list, and descendants of the root are added to the LIS as type A. Second, initial threshold that is used in SPIHT is modified. In conventional SPIHT, the initial threshold is calculated by using all of coefficients in low-pass and high-pass bands. However, in the proposed hybrid coding scheme, coefficients in the low-pass band are not considered when the initial threshold is calculated. As the initial threshold is determined by using only coefficients in the high-pass band, it represents a characteristic of the high-pass band properly. Typically, the high-pass band includes detailed information (like edges), thus complexity of the block can be approximated by using the initial threshold. In next section, a bit allocation scheme to block-based compression that uses the initial threshold is proposed.In block-based compression, a TBL is given to each block and the coding is terminated when the coded bit length becomes the same as the TBL. As the image complexity of each block may be different, it is desirable to allocate more bits to more complex blocks. In order to allocate bits to each block efficiently, the relative complexities of all blocks in the image are calculated and the TBL of each block is determined according to its relative complexity. This method results in a large memory size and long latency because the whole set of data for an image needs to be required before calculating relative complexities. To reduce the memory size and latency, relative complexity within a line is determined. Subsequently, a TBL is allocated to each block in the line. The relative complexity of each block in a line can be calculated once all data for that line are obtained, which means that there is still a need to store all data for a line. Thus, a line sized memory is still needed.In order to reduce the memory size and latency, a scheme for estimating the relative complexity of each block in a line by using the coding information of the upper block is proposed. As there is a strong spatial correlation between current and upper blocks, the bit-rate of the current block is similar to that of the upper block. Fig. 4(a) and (b) show the similarity in bit-rates between adjacent 1D blocks in the vertical direction. In Fig. 4, the horizontal axis represents the block number and the vertical axis represents the bit-rate of the high-pass band that is coded by the lossless 1D SPIHT. In this example, the block size is 1×64 and the number of blocks in a line is 12. Vertical lines in Fig. 4 indicate changes in a line. Fig. 4(a) and (b) use Kodak03 and Kodak08 images, which are the simplest and most complex images, respectively, among twenty-four Kodak images [19]. In both Fig. 4(a) and (b), the bit-rates of two vertically adjacent blocks are similar. Thus, the complexity of a block is estimated by using the information of the upper block.The bit length of the upper block is unknown because the encoding process of a block is terminated when the bit length reaches the TBL. Thus, a scheme to estimate the complexity by using coding information of the 1D SPIHT is proposed. When the complexity of a block is high, the block has many edges. Since the edges increase the magnitude of the coefficients in the high-pass band, the initial threshold,Thinitial, of the block also increases. Therefore,Thinitialis used to estimate the complexity of a block. Fig. 5showsThinitialvalues that are obtained under the same conditions as those for Fig. 4. TheThinitialgraph in Fig. 5 is similar to the bit-rate graph presented in Fig. 4. This similarity shows that the complexity of a block is estimated by usingThinitial. LetTBLM(i)denote the TBL of the i-th block in a line when the proposed bit allocation scheme is used, andTBLPrepresent the conventional TBL that is obtained by dividing the predefined bit length of a line by the number of blocks. The number of blocks in a line is represented byBN, whereasTBLRis the saved bit length in upper lines. TheThinitialof the i-th block in the upper line is represented byThinitial(i). Then,TBLM(i)is determined by using (6).(6)TBLM(i)=TBLP×rate+TBLP×BN×(1-rate)+∑jTBLR(j)×log2Thinitial(i)∑klog2Thinitial(k)Theratein (6) controls the effect ofTBLPand the relative complexity of the block. When therateis close to 1,TBLMapproachesTBLP. When therateis close to 0, theTBLMapproaches the bit length determined by the relative complexity. In this proposal, therateis chosen as 0.3. Suppose that image width, block size, and target CR are 256, 1×64, and 3, respectively. Then,BNandTBLPare set by 4 and 512. Also, suppose that initial thresholds of 4-blocks in the upper line are 64, 128, 32, and 256, respectively, and the sum ofTBLRin upper lines is 100. By using (6), TBLs of the first, second, third, and fourth blocks in the current line is 507, 566, 448, and 625, respectively. It shows that the fourth block that is predicted as the most complex block gets the largest TBL while the third block that is predicted as the simplest block gets the smallest TBL.However, in hardware implementation, the size of bitstream buffer can be excessively increased as theTBLMin (6) increases. Also, a real-time operation of the encoder becomes difficult because of the output bandwidth. Therefore, the size ofTBLMneeds to be restricted for some hardware constraints. The restricted TBL is shown in (7) and (8).(7)TBLM′(i)=TBLP×rate+{TBLP×BN×(1-rate)}×log2Thinitial(i)∑klog2Thinitial(k)+∑jTBLR(j)(8)TBLM″(i)=TBLM′(i),TBLM′(i)<TBLMAXTBLMAX,otherwiseIn (7), the sum ofTBLRis only used by next blocks in the line and is reset when coding for the line is over. Also, it is used irrelevantly to initial thresholds of upper blocks.TBLMAXin (8) represents the restricted TBL. IfTBLM′in (7) is equal to or larger thanTBLMAX, the final TBL,TBLM″, is set byTBLMAX. Otherwise,TBLM″is set byTBLM′as shown in (8). The largeTBLMAXassures an effective bit allocation but the size of bitstream buffer is increased. On the other hand, the smallTBLMAXassures a small size of the bitstream buffer but the effects of the bit allocation decrease. Therefore, a properTBLMAXis required to balance hardware costs and coding efficiency. This paper setsTBLMAXexperimentally, and the value is 640 (see Fig. 8in Section 4).The proposed bit allocation scheme results in improved reconstructed image quality while additional hardware costs and computations are small because the coding information of the upper block is exploited.

@&#CONCLUSIONS@&#
In order to improve compression efficiency, this paper proposes an algorithm that includes a hybrid coding scheme and a block-based bit allocation scheme. In the hybrid coding scheme, DPCM–VLC is used for the low-pass band and 1D SPIHT is used for the high-pass band. The compression efficiency of the proposed DPCM–VLC is improved by the addition of sub-schemes that reduce the bits needed for the reference coefficient in each block and that update the divisor needed for Golomb–Rice coding. The proposed block-based bit allocation scheme improves the PSNR by determining the TBL of each block based on the estimation of block complexity. In order to reduce memory size, latency, and computation for the bit allocation scheme, the complexity of a block is estimated by using the coding information of the upper block.The proposed algorithm improves the average PSNR by 2.97dB compared with the conventional 1D SPIHT algorithm when the target CR is set by 3. The proposed algorithm is implemented with Verilog HDL. The throughput of the proposed design is 6.15Gbps, and gate counts of encoder and decoder designs are 42.8K and 57.7K, respectively.