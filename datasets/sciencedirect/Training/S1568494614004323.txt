@&#MAIN-TITLE@&#
Liver fibrosis staging using CT image texture analysis and soft computing

@&#HIGHLIGHTS@&#
Liver fibrosis staging is an important problem in medical informatics.Currently, liver biopsy is the gold standard; however, it is invasive and expensive.We utilize image analysis (feature extraction and classification) on CT images.Automatic stage classification into 7 stages is a really challenging problem.Our non-invasive approach can be used in especially pairwise stage comparisons.

@&#KEYPHRASES@&#
Liver fibrosis staging,Texture features,Feature selection,k-Nearest neighbor,Support Vector Machines,

@&#ABSTRACT@&#
Liver biopsy is considered to be the gold standard for analyzing chronic hepatitis and fibrosis; however, it is an invasive and expensive approach, which is also difficult to standardize. Medical imaging techniques such as ultrasonography, computed tomography (CT), and magnetic resonance imaging are non-invasive and helpful methods to interpret liver texture, and may be good alternatives to needle biopsy. Recently, instead of visual inspection of these images, computer-aided image analysis based approaches have become more popular. In this study, a non-invasive, low-cost and relatively accurate method was developed to determine liver fibrosis stage by analyzing some texture features of liver CT images. In this approach, some suitable regions of interests were selected on CT images and a comprehensive set of texture features were obtained from these regions using different methods, such as Gray Level Co-occurrence matrix (GLCM), Laws’ method, Discrete Wavelet Transform (DWT), and Gabor filters. Afterwards, sequential floating forward selection and exhaustive search methods were used in various combinations for the selection of most discriminating features. Finally, those selected texture features were classified using two methods, namely, Support Vector Machines (SVM) and k-nearest neighbors (k-NN). The mean classification accuracy in pairwise group comparisons was approximately 95% for both classification methods using only 5 features. Also, performance of our approach in classifying liver fibrosis stage of subjects in the test set into 7 possible stages was investigated. In this case, both SVM and k-NN methods have returned relatively low classification accuracies. Our pairwise group classification results showed that DWT, Gabor, GLCM, and Laws’ texture features were more successful than the others; as such features extracted from these methods were used in the feature fusion process. Fusing features from these better performing families further improved the classification performance. The results show that our approach can be used as a decision support system in especially pairwise fibrosis stage comparisons.

@&#INTRODUCTION@&#
Chronic liver diseases (CLD) is a major public health problem worldwide. Some of CLD conditions are caused by the infection with B, C or delta hepatic viruses, whereas other cases include non-alcoholic fatty liver disease, alcoholic hepatopathy and other rare genetic, metabolic or immunological conditions. Liver fibrosis can be defined as the accumulation of collagen, proteoglycans, and other macromolecules within the extracellular matrix, which is a common feature of almost all causes of CLD. Hepatocarcinoma is one of the most common malignancies in patients that are affected by these diseases [1]. The fibrosis is the scarring response formed in the chronic injury of any cause. It is a dynamic process, with a possibility of reversibility. Hepatic dysfunction, portal hypertension, and cirrhosis are detected respectively due to progressing liver fibrosis. Therefore, in our study we have focused on the assessment of fibrosis in medical images, as it is the unchanging marker for the diagnosis, prognosis, and treatment follow-up of all CLD conditions.At present, liver biopsy is considered to be the gold standard in evaluating fibrosis. Using the liver biopsy one can establish the diagnosis with some certainty, assess the severity of necro-inflammation and fibrosis, and indicate the existence of simultaneous liver diseases, if there is any. On the other hand, it is an invasive procedure with possible side effects such as pain in 30–40% of the cases, hemorrhage, biliary peritonitis, penetration of abdominal vessels, pneumothorax (3/1000) or even death (2/10,000) [2,3]. Furthermore, sampling errors (either by the fragmentation of the biopsy specimen or by removing an inadequate volume) can be seen in 24% of the cases [2]. Fibrosis distribution is not always homogenous inside the liver, hence biopsy specimen has usually a very small volume (in average it has the shape of a cylinder with a diameter of 1mm and a length of 1.5cm) when compared to the whole liver volume. In addition, intra- and inter-operator variability may be seen. Such situations are reported in 10–20% of the cases. The two combined factors (lack of representativity of the biopsy specimen and the variability in the assessment of fibrosis) lead to a cirrhosis false negative rate of 24%. Therefore, in a significant percentage of cases, biopsy does not yield precise diagnosis results for liver fibrosis.Clinical assessment of fibrosis can also be made using non-invasive tests (e.g., serologic markers or elastography) and medical imaging techniques, e.g., ultrasonography (US), computed tomography (CT), and magnetic resonance imaging (MRI). However, the sensitivity and the specificity of non-invasive tests still remain insufficient [3]. Although imaging tests may detect manifestations associated with fibrosis, e.g., portal hypertension, splenomegaly or cirrhosis, they currently fail to diagnose parenchymal fibrosis itself particularly in early stages of the disease [4,5]. Nevertheless, there are various studies in the literature about staging of the liver fibrosis using image analysis, mostly applied on ultrasound images [6–11]. Therefore, we believe that the appeal of non-invasive techniques in characterization of liver tissue will be remain to be an active area of research in the coming years as well.Some of the suggested approaches have been based on different image features such as the first-degree and second-degree gray level statistics and the texture features measured from fractal size estimator in these studies [6–9]. Zhang et al. tried to determine the stage of the fibrosis by using 2 liver shape features and 5 gray level co-occurrence matrix texture features (contrast, angular second moment, entropy, mean and inverse difference moment), and two first-order statistics (mean and standard deviation) features [10]. Sun et al. defined the cirrhosis disease type by using the pyramid plexus and discrete wavelet transform from liver ultrasound images. They classified the disease into 4 classes: Healthy, stage 1, stage 2, and “very serious” [11].In this study, using several supervised machine learning algorithms, we have tried to classify liver fibrosis stages using various texture features obtained from CT liver images. The reasons behind using/analyzing ‘CT liver images’ will be discussed in Section 2. Our study is comprised of two parts, namely data collection/preparation and analysis (classifier design) phases.The first phase involved building a database of CT liver images and associated fibrosis stage information obtained through pathological analysis performed by the expert physicians in our team. Further, in this phase we have selected some suitable ‘Regions of Interest’ (ROIs) or sub-images from the CT images (i.e., slices of volume data) for each subject, again with the help of physician colleagues.In the second phase of the study, we have extracted and selected texture features to be used in designing classifiers for predicting ‘fibrosis stage.’ We have used the following approaches to extract some suitable texture features to be used in classifier design:•Gray Level Co-occurrence Matrix (GLCM) [12],Gray Level Run Length Matrix (GLRLM) [13],Gray Tone Difference Matrix (GTDM) [14],Laws’ Texture Features (LTF) [15],Discrete Wavelet Transform (DWT) [16],Discrete Fourier Transform (DFT),Gabor filters [17], andFirst Order Statistics (FOS).We have employed sequential floating forward selection (SFFS) [18] and exhaustive search methods for feature selection. As for classification methods, k-nearest neighbor (k-NN) [19] and Support Vector Machines (SVM) methods [20] were utilized. Further details of our ‘classification experiment design’ will be given in ‘Materials and Methods’ (Section 2.6).In a preliminary study [21], we have used the features obtained by GLCM, DWT, and DFT methods in classifying liver fibrosis stage. After determining classification performance individual features, we have combined or fused better performing features to be used in the final classifier design. We have obtained relatively better results using the features obtained through GLCM and DWT methods. In pairwise group classification, we were able to reach classification accuracy rates of 86% and 90%, using k-NN and SVM classifiers respectively. Different from our previous study [21] and other studies in the literature [6–11], in this study we have included a very comprehensive set of texture features obtained through different approaches. These texture feature extraction approaches will be described briefly in the next section; detailed explanation of these methods is given in Appendix. Specifically, in order to raise classification accuracy figures to clinically acceptable levels, we have included features based on GLRLM, GTDM, LTF, Gabor filters, and FOS methods in classifier design, in addition to previously employed feature extraction methods. Furthermore, in this current study we have also investigated the possible effects of gray level scaling and quantization in obtaining GLCM features and the way partitioning of the sample set into test and train sets (i.e., fold value in the cross validation) on the classification performance.In parallel with the noticeable development of CT technology over time, CT imaging of liver has gained importance and speed. CT is a standard method for liver imaging today and it is used in detecting and characterizing the primary and secondary liver masses and in reviewing the diffuse liver diseases [22].Although MRI could be more advantageous in soft tissue imaging, its use in liver imaging is not common practice yet, mostly due to long data acquisition times that lead to motion (respiration) artifacts in the images and economic considerations (MRI is still an expensive procedure and is not available at every center). As for the US imaging, although it is practical (easy to operate/use) and less harmful (it does not involve any radiation), it suffers from problems such as somewhat subjective (operator-dependent) imaging quality and interpretation, degraded imaging quality in obese patients and in cases where there is intensive abdominal gas. As such, US is not a viable alternative to CT either in evaluating or staging liver fibrosis.In CT imaging/evaluation of liver, an uncontrasted screening may cause misdiagnosis of hypodense vascular structures as lesions. Therefore, the main principle includes the optimum imaging of the vascular structures and review of the liver parenchyma and surrounding soft tissues. This information can be obtained by using contrast agents in appropriate volume and concentration. The contrast agents in CT are extracellular and injected intravenously and they directly associated with radiographic attenuation rise [23].For an ideal liver CT imaging process, it is absolutely necessary to use intravenous contrast. Multiphasic study gains importance because of the vascular hemodynamic of liver. Approximately 60–75% of blood build-up of liver is derived from portal vein, and the remaining 25–40% is derived from hepatic artery. Multiphasic liver CT process consists of arterial, portal venous and late venous phases. Liver parenchyma shows opacification in portal venous phase mostly [24].In this retrospective study, the cases of patients who have undergone needle biopsy since 2006 were examined by pathologists and radiologists in our team at Erciyes University Medical School Hospital. The fibrosis in liver needle biopsy specimens were histopathologically evaluated on the hematoxylin-eosin-stained and mason trichrome-stained parts and fibrosis stage was determined (or scored) in accordance with Ishak Staging System [25], ranging from 0 (no fibrosis) to 6 (cirrhosis) (see Table 1).The images of 116 patients were acquired by using the multi-slice CT scanner (GE Light Speed 16, Milwaukee, Wisconsin, USA) at the Radiology Department of Erciyes University Medical School, in Kayseri, Turkey. Considering the possible change/progress in patients’ fibrosis stages, patient imaging dates were arranged or limited to be within ±2 months from the date of the biopsy.The patients were administered nonionic 300/100cc contrast substance to and images of the venous phase upper abdominal sections were taken with 512 pixels by 512 pixels resolution at 1.25mm thickness (along longitudinal axis). A sample liver CT image and a selected ROI are shown in Fig. 1. Number of subjects in each fibrosis stage is given in Table 2.Texture is a measure of intensity change of an image and associated with features such as smoothness, roughness and regularity. For an excellent discussion of this somewhat controversial issue, we refer the reader to [26].Different families of approaches exist for measuring texture features/characteristics. The first-degree texture features do not consider the pixel neighborhood relations, however, the second-degree texture features generally take these relationships into account. In this study, we have utilized many of the ‘texture feature extraction’ methods approaches that have been utilized in the image processing literature. These include GLCM, GLRLM, GTDM, LTF, 2-D DWT, 2-D DFT, Gabor filters, and FOS. In order to increase the readability of the paper, we have found it suitable to move the cumbersome mathematical formulations of these texture features to the end of the paper, into Appendix; aforementioned methods are respectively discussed in Sections A.1–A.8. Here in this section, we will suffice by shortly describing these methods.Gray Level Co-occurrence Matrix (GLCM) defines the relationship between the neighbor pixels, and shows the occurrence frequency of the brightness levels on the image in a definite distance and direction [12,27–29]. Gray Level Run Length Matrix (GLRLM) is one of the ways to detect the second-degree statistical texture features. The set of sequential pixels that have the same gray level value in the same direction forms the gray level run. Run length is the pixel quantity in the run and run length value is the occurrence number of the runs on the image [13,30–32]. The existence of many neighbor pixels with the same gray level represents a rough texture, and the lower quantity of such neighbor pixels represents a thinner texture with a faster change. Gray Tone Difference Matrix (GTDM) has been suggested by Amadasun and King by which visual properties of the texture can be extracted [14]. Laws’ texture features (LTF), which were developed by Kenneth Ivan Laws [15,33,34], were used in many different practices. For these features, first, small convolution kernels are applied onto the image, and then they are extracted using a nonlinear windowing operation. In Discrete Wavelet Transform (DWT) of an image is generally calculated by convoluting the image with two filters, which are low-pass and high-pass, throughout the rows and columns [16]. In this approach we use some suitable transform coefficients as texture features. The 2-D Discrete Fourier Transform (DFT) is relatively well known but for a good refresher we still refer the reader to the book by Demirkaya, Asyali, and Sahoo [35]. In this approach, we compute 2-D DFT of ROIs and use the relative power in certain frequency bands (corresponding to low and high frequencies in horizontal and vertical directions) as features. In Gabor filtering, in order to accentuate features that could lie in certain orientations, the image is filtered by a suitable linear 2D filter which is a Gaussian kernel function modulated by a sinusoidal plane wave. Depending on the variances (width) of the Gaussian kernel in horizontal and vertical directions and the frequency of the sinusoidal, differed pattern are emphasized at the output image and then this filtered image is averaged to produce a final single texture feature [17,36]. By using several Gabor filters with different orientations, multiple features can be obtained.In this study, k-nearest neighbor (k-NN) and Support Vector Machines (SVM) methods were used in the classification stage. The k-NN method is one of the most commonly known and used classification methods. Though it lacks modeling capability, it is frequently used as a benchmark method, due to its simplicity and relatively high performance. In contrast, the SVM method is a powerful, state of the art, and relatively complex method with superior learning/modeling capability. Both of these classifier design approaches are nonparametric, i.e., there is not any assumption about the underlying statistical model (or distribution) for the data. In our case, since the number of samples was limited, employing a parametric classifier (such as a Bayesian classifier [37]) would not be a sensible choice, as estimating model/distribution parameters from a limited number of samples would result in parameter estimates with high bias and variance.In the k-NN method, a test data/feature vector x is simply assigned to class of most occurring data among x's k-nearest neighbors, using a proper distance metrics such as Euclidean distance [19]. SVM is a supervised learning method that was developed by Vapnik [20]. SVM is commonly used in various pattern recognition applications such as object, handwriting, speech and face recognition [20,38–40]. The main objective of SVM is to find a hyperplane that can separate the two classes in the most suitable way (i.e., with the possible largest margin on both sides) in the feature space. SVM optimizes the position of the hyperplane in order to maximize the distance between the hyperplane and data samples in both sides during learning. SVM carries out this process by defining the samples (support vectors) that determine the boundary of each class [19].Although SVM method is mainly a binary classifier, several of them can be combined using generally one-versus-one (1-1) or one-versus-rest (1-r) approaches, in order to handle multi-class cases. In this study, SVM (1-1) method was preferred to classify the multi-category data. SVM (1-1) is the method where the classes in data set are compared in a pair-wise fashion. In this method, for a data set with c class, c(c−1)/2 binary classification processes are carried out with SVM. A test data is assigned to the most voted class by analyzing the results of these binary classifiers. If there are multiple classes that have the most votes, the test data is assigned to the class with the lowest index or to the class with the highest prior probability density function among these classes [41,42].The assessment of classifier performance is an important issue and it deserves a long and involved elaboration. For a detailed discussion of this topic, we refer the reader to an extensive review by Asyali et al. [43]. However, we will suffice by noting that empirical prediction of classification accuracy (i.e., the ratio of correct decisions to the total number of cases studied) remains to be the popular and practical performance measure, especially for nonparametric classifiers. The estimate of ‘classification accuracy’ is a random variable as it depends on the particular training and test samples used. If there are enough number of data samples available, one can partition this set into two subsets and use one for ‘training’ and the other for ‘testing.’ Literature [44,45] shows that if ‘classification accuracy’ is estimated by randomly splitting the set of all available samples into two parts, its bias decreases and variance increases as the size of training (testing) set increases (decreases).A common technique to assess classifier performance is to m-fold cross validation (CV), where the overall set of n data samples is randomly divided into m approximately equal size and balanced (i.e., the distribution of samples into different classes is similar) subsets Then m classification trials are carried, each time one of these subsets is excluded from the overall set and used as a test set (for the classifier that is trained based on the rest of the samples). This is repeated over the m subsets and the resultant ‘classification accuracies’ are averaged to obtain the so-called m-fold CV accuracy rate. Typical choices for m are 5 or 10, as these correspond to a reasonable tradeoff between the variance and bias in estimation of classification accuracy rate and computational burden. As an extreme case, m can also be set equal to n, size of the overall data set, in which case we will have what is known as ‘leave one-out cross-validation’ (LOOCV). Although the LOOCV accuracy rate estimator will have a lower bias, it will have a relatively higher variance (as the size of the test set is just 1) and computationally it be very costly. In our case, for the estimation classification accuracy, we chose to try 2- or 3-fold CV only due to our limited sample size within each group, especially for the case of pairwise group (liver fibrosis stage) classification trials.The objective of the selection methods is to choose the best subset that will represent the original data set. Feature selection not only minimizes the search space, but also increases the quality of classification process [46]. While there are many studies in the literature about feature selection methods, we used exhaustive search (ES) and sequential floating forward selection (SFFS) methods for their advantages in our study.In the Exhausted Search (ES) method, when an original set with M elements is to be represented with a subset with N elements,MNsubsets are formed and the subset yielding the best classification performance is selected as the optimal subset. Because it checks all possible combinations, ES is guaranteed to find the best subset with N elements to represent the original set. However, it is very costly computationally, i.e., when M and N are relatively large, ES becomes unacceptably slow. In the sequential forward selection (SFS) method, the optimal feature subset starts with an empty set and each iteration/trial the relatively better performing features are added to the set [47]. In SFS, the features included in the optimal set cannot be removed is subsequent steps. In contrast, the Sequential Forward Floating Selection (SFFS) method allows removal of included features. Hence, it is possible to obtain comparatively better performing optimal subsets representing the original feature set. Nevertheless, neither SFS nor SFSS is guaranteed to the find the global optimum [18].Our overall classification approach is summarized in detail in Fig. 2. All algorithms were implemented using Matlab R2010a (Mathworks Inc., Natick, MA) software [48]. Further details of the processing steps will be discussed next.Liver parenchyma that does not contain big blood vessels is selected from the CT images as the ROI. In order to avoid selecting the wrong area, 2 ROIs of size 32 by 32 pixels were manually selected from 5 images (slices) of each patient as shown in Fig. 1. Therefore, in a way, we have thoroughly scanned available imaged liver area in 3-D to obtain some good representative areas of liver fibrosis condition. In subsequent feature extraction steps, we have used these selected 10 ROIs for each patient. Before feature extraction process (the first step in Fig. 2), the Hounsfield values on the ROIs were linearly scaled/normalized and quantized to be 32 and 128 gray levels. For 32 (128) level scaling, the minimum and maximum pixel values were assigned as 1 (1) and 32 (128), respectively. Scaling was necessary to moderate the cost of GLCM and GLRLM computations. In order to assess the effect of gray level quantization on the final classification performance, we have repeated the whole scheme (feature extraction, selection, and classification) at these two quantization levels. Different studies in the literature selected/used various quantization levels such as 8, 16, 32, 64, 128, and 256. We tested these levels and decided to omit 8, 16 and 256 levels due to either low performance or high computational cost, and set lower and upper bound as 32 and 128 respectively.In the second step, GLCMs of 10 ROIs were computed symmetrically at 0, 45, 90, and 135 degrees up to 8-pixel distance. Thus, we obtained 32 GLCMs (4 orientations times 8 distances) for one ROI and 320 for 10 ROIs for each patient. Later, 20 features mentioned in Section 2.3 were extracted from these GLCMs using the approach followed in [49]. Same features with the same distance in different angles were averaged, and thus 20 features were obtained for each distance getting a total of 20×8=160 features using the GLCM method for each ROI.A single representative feature to be used in the further analysis was obtained by taking the median of the features extracted from 10 ROIs. When the underlying distribution of the parameters is not known it is safer to use the median as the measure of central tendency, rather than the mean, as it is more immune to outliers.Similarly, 11 GLRLM texture features were extracted at 0, 45, 90, and 135 degrees up to runs in length of 8 using GLRLM Toolbox v1.0 [50]. In order to make the features rotation invariant, the feature values were determined by taking the means of the same features with the same run length in different angles as we have done in finding GLCM features. Totally, 11×8=88 features were obtained in runs in length of 8 using this particular method.Using the LTF method by means of applying 3×3, 5×5, and 7×7 windows with length-5 kernels, 14 texture images were obtained for each window. The window size with the best performance among these 3 windows determined the features of Laws’ texture feature family. Therefore, 14×5=70 features were extracted for each window by extracting the five FOS features (mean, standard deviation, average energy, skewness, and kurtosis; see Appendix A.8 for details) out of these images. This process was also carried out for the length-3 kernels, and 5×5=25 features were obtained for each window. The window size features with the highest classification success were also selected in this method.Another feature extraction approach we used was the Gabor filters. We created Gabor filters to be applied on the ROI images by using variances as 1, 2, 3, 4, and frequencies as 0, 2, 4, 8, 16, 32, at the angles 45, 90, 135, and 180 degrees in combination (see Appendix for the formulation of Gabor filters). For each resultant/filtered image, 5 FOS features were extracted. The features were made rotation invariant by averaging the features obtained through the use of the filters with the same variance and frequency combination in different angles. For each variance combination (16 combination) 6×5=30 features (5 FOS features for each frequency) were obtained using Gabor filters.The DWT was applied as another feature extraction method. Matlab's wavelet functions db1, db2, db3, coif1, coif2, coif3, sym2, sym3, sym4, dmey, bior1.1, bior2.2, bior3.1, bior4.4, rbio1.1, rbio2.2, rbio3.1, and rbio4.4 were used as the filter banks [48]. DWTs were computed using these different wavelet filter banks and decomposition coefficients were obtained at 4 levels. Then, 5 FOS of the DWT coefficients were obtained for each level.In addition to these features, 5 features were extracted by using the GTDM method. Furthermore, using DFT 4 texture features, corresponding to the relative power in certain bands, were obtained. Finally, 5 FOS features were directly computed from each ROI. For ease of referral, Table 3lists/summarizes all of the feature extraction methods that we have utilized, along with the number of features computed in each category.Once we have obtained all the features, we have normalized them to be between 0 and 1, in order to balance their influence in the subsequent fusing step. After feature extraction and normalization steps, we have performed the “dimension reduction” or “feature selection” step. In this step, we have basically reduced the number of features by selecting the most discriminative features from the set of all texture features. In order to include best possible features in the analysis, we have started with a very extensive/comprehensive set of features and eliminated less performing features at this critical step. By doing so, we have also overcome the “curse of dimensionality” problem [51]. This problem refers to the fact that, when the number of features is too many compared to the number of samples in a classification problem/setup, due to increasing sparsity of matrices that are involved in the analysis, numerical problems may arise and statistical power may be lost, which in effect hinder the significance of the estimated parameters [37]. In our case, as Table 3 shows, the total number of features (obtained using different texture measurement approaches) is 467, which is too many compared to the number of samples (116 samples from 7 different classes). In most practical applications, the number of features should be around one tenth of the number of samples per class [37].We did not want to resort to feature combination methods such as principal component analysis [37], because such methods combine features in somewhat inexplicable ways. In our case, we wanted to identify/pinpoint texture features that are relatively highly relevant to fibrosis characterization. As such, combining texture features that indicate totally different texture characteristics in an uncontrolled way would not be suitable for our purposes.In feature selection (i.e., dimension reduction), as performance metric we have used ‘classification accuracy rates’ that are returned by k-NN and SVM classifiers at each trial/step, as these two were the methods of our choice in the final classification step. The dimension reduction was carried out in two phases. In the first phase, the best features were selected from each family using the SFFS method (see Section 2.5). In this phase, exhaustive search was not preferred as it is computationally costly. For example, to reduce 160 features obtained from GLCM family to 3 features requires1603=665,680trials. Because the feature values were not monotonic the branch and bound method [52] could not be applied. Since the cost of selecting few features out of such high number of features with the backward selection methods [18,53] is higher than with the forward selection, backward selection methods were not preferred either. In the second phase of feature selection, that is, after the best features were obtained from each feature family, the resultant features were fused and “exhaustive search” was performed on the fused set of features, in order to determine the best performing features in the final classification stage.We have proceeded with selection of 3, 4, or 5 best performing features from each family according to their classification accuracy. As we will discuss in the next section, GLCM, DWT, GF and LTF methods consistently yielded a mean classification accuracy of larger than 75%. Therefore, only the features coming from these methods were used in the fusing stage. Otherwise, for the case of 4 features, for instance, if we had included 4 best features from all of the 9 feature families, we would have total of 4×9=36 features in the fused set and an exhaustive search on this set for overall 4 best features would require almost 60,000 trials. On the other hand, if we just include aforementioned 4 feature families in the final search (in the fused feature space), again for the case of 4 features for instance, we will have164=1820trials, which is a reasonable number.After fusing, a final set of 3, 4, or 5 features were selected from the set of fused features by using exhaustive search where all feature combinations are examined in terms of classification performance. Although not reported here, we have also experimented with the case of 6 features (i.e., included 6 features in the feature fusing step and in the final round of feature selection) and noticed that performance did not improve significantly whereas computation time increased significantly. For this reason, we have decided not to include the case of 6 features in the analysis.

@&#CONCLUSIONS@&#
