@&#MAIN-TITLE@&#
A similarity metric designed to speed up, using hardware, the recommender systems k-nearest neighbors algorithm

@&#HIGHLIGHTS@&#
kNN algorithm performance.Collaborative filtering hardware similarity measure.Low-cost recommender systems hardware circuits.

@&#KEYPHRASES@&#
Similarity measure,Metric,Collaborative filtering,Memory based,Recommender system,

@&#ABSTRACT@&#
A significant number of recommender systems utilize the k-nearest neighbor (kNN) algorithm as the collaborative filtering core. This algorithm is simple; it utilizes updated data and facilitates the explanations of recommendations. Its greatest inconveniences are the amount of execution time that is required and the non-scalable nature of the algorithm. The algorithm is based on the repetitive execution of the selected similarity metric. In this paper, an innovative similarity metric is presented: HwSimilarity. This metric attains high-quality recommendations that are similar to those provided by the best existing metrics and can be processed by employing low-cost hardware circuits. This paper examines the key design concepts and recommendation-quality results of the metric. The hardware design, cost of implementation, and improvements achieved during execution are also explored.

@&#INTRODUCTION@&#
Recommender Systems (RS) provide a relevant tool which helps to mitigate part of the information overload generated via the use of Web 2.0 applications. RS provide personalized recommendations to users about items (books, music, films, gadgets, holiday destinations, etc.) [26,20,23,4,8,24].When the RS is solely based on the information stored in the array of votes it is called memory-based collaborative filtering [1,16,14]. A variety of Collaborative Filtering (CF) exists which obtains information from additional sources to the array of votes, such as the social relations between users or the contents of posts in blogs; in these cases (memory-based+additional information) the additional information is used to improve the quality of the recommendations, but its use is only applicable to the subset of RS where that type of additional information exists. All scientific progress in the area of memory-based CF has the virtue of being applicable in the different types of CF-based RS: pure CF and Hybrid filtering (CF+social, CF+demographic, CF+knowledge-based, etc.).As the size of CF RS increase, the system response times also increase. The model-based CF [19] helps to reduce recommendation times, but this reduction causes the system to operate with obsolete data and requires continuous offline updates of the models. As a result, commercial RS usually utilize memory-based CF algorithms, such as the k-nearest neighbor (kNN) algorithm. This algorithm calculates a set of k neighbor users whose similarities are most comparable to the user for whom a recommendation is sought (the active user). The similarity between users is determined by applying metrics that act on the existing set of data (memory-based CF).The kNN algorithm requires lengthy computation times because of the need to calculate the similarity of each active user comparing them with the votes cast by each of the existing users in the database. Using Netflix as an example, each recommendation created for an active user requires calculations of the similarities between the active user and approximately 480,000 other users. A maximum of 17,000 votes must be compared. A typical similarity metric is the Pearson correlation. Using this metric, the process of creating recommendations for a user requires the processing of approximately 480,000 correlations of 17,000 values.Traditionally, the similarity metrics and measures used in RS come from those used in the statistics area or some of those used in various fields of information retrieval, such as Pearson correlation, cosine, adjusted cosine and Spearman rank correlation [1,16]. Recent studies have shown that it is possible to improve the quality of the prediction and recommendation results [16,10,17] by using new memory-based CF similarity metrics and measures [8,9,10,11,13] specifically designed to make the most of the special feature inherent to RS and its more complex operating modes, such as high levels of sparsity [3] and cold-start situations [2].By relegating the performance issue to marginal commentaries or brief comparisons of execution time, research in the field has focused on improvements in the accuracy, precision, and recall of predictions and recommendations. At present, a midrange modern computer is able to process the recommendation of a Netflix user in 50ms. Real-time systems are not considered because users do not require a defined response time. In this context, improvements in the accuracy provided are more appealing than improvements in system performance.In spite of the previous considerations, three factors contribute significant value to research aimed at improving processing times for recommendations:1.The memory-based CF algorithms are not scalable. The processing time increases in quadratic proportion with the number of users and items (elements to recommend).The number of users and items increases rapidly in commercial RS.The distribution of the recommendation requests is not completely uniform: large demand spikes occur on certain days and at certain hours (for example, Sunday evenings).The development of a mechanism that is aimed at reducing the execution times of CF RS is necessary to ensure viable processes for both current and future operations, particularly during high-demand periods and for large RS.Although more attention has been focused on the recommendation quality of RS, there is a research topic that considers processing time for the kNN algorithm: the Nearest Neighbors (NN) classifiers and, more specifically, the NN graphics classifiers. There are four different techniques aimed at accelerating the classical kNN method:•Accelerate the sort operation: In [7] they replace the sort operation with calculating the order statistics, making the kNN method more efficient; they also increase the method’s stability. In the case of RS, the sort operation requires much less execution time than the processing of all similarity measurements; therefore, significant reductions in execution time do not exist in this portion of the algorithm. Furthermore, stability problems in the sort operation do not occur in RS because ordering is performed on sets of real values. In NN graphics classifiers, sorting algorithms order the records with equals keys in different ways.Parallel approaches: In [5] they propose a parallel Graphics Processing Units (GPUs) implementation for the kNN algorithm. In [6] they use the parallel implementation to accelerate brute force searching algorithms for metric-space databases. There are several factors that make the adoption of a kNN based CF parallel approach unappealing: sparsity and the centralized nature of RS databases, the fine granularity of the similarities metrics, and the absence of specific processors.Using kd-trees and k-means clustering when using NN matching in high-dimensional spaces: In [21,25] they use hierarchical k-means trees and multiple randomized k-d trees to provide the best performance results. In the case of RS, each 〈user, item〉 dimensional space datum is defined by a unique real value. Its processing does not require a matching complex; therefore, it is usually resolved by processing a statistical similarity measure, such as the Pearson correlation.Decreasing the number of elements that should be compared in the kNN algorithm: In [15] they present a technique for quickly eliminating most templates from consideration as possible neighbors. This method is applicable when the number of features is large and the type of each feature is binary. In [18] they weight all the training instances in the generalization phase; an instance having zero weight can be removed from the training set. In the case of RS, the amount of time required for filtering (or weight processing) each item or user should be similar to the amount of time required for the similarity metric, for which reason its use in CF RS is inadequate. In [22] they use Pareto dominance to perform a pre-filtering process eliminating less representative users from the k-neighbor selection process while retaining the most promising ones.In a kNN-based CF RS, the repetitive evaluation of the similarity metric is the phase in which nearly all of the processing time is utilized. Any significant improvements in performance (time consumption) of the similarity metric will have a significant impact on any improvements in recommendation times for the entire system.This paper introduces a similarity metric that is designed for simple and cost-effective implementation through the use of low-cost hardware. The proposed metric slightly reduces the quality of the obtained results (in comparison with the latest generation of metrics); however, its “recommendation quality/performance” relation makes it the current CF metric with the greatest advantages among large RS.The proposed similarity measure allows to face new and exciting possibilities into the RS collaborative filtering based applications: (1) RS in which users recognize, in real time as they add new ratings, changes in the recommendations that they receive (RS push technology); (2) Direct and high performance support to RS based on binary ratings; these RS are becoming more and more popular due to the typical like/dislike GUI options of social applications running on mobile devices; and (3) RS servers devoted to process incoming recommendation requests from users of different companies services; that is, hardware circuits containing tens of thousands of parallel bit processing units, running requests from users of films, music, news, posts, etc. RS companies.The paper is structured in the following way: Section 2 explains the method followed to design the similarity measure, Section 3 shows the hardware design and implementation approaches, Section 4 presents the experiments carried out and the results obtained. Finally, Section 5, sets out the most relevant conclusions.JMSD is a metric with a very simple formulation. Its motivation and foundations can be found in [9]. JMSD combines: (a) numerical similarity information between the 2 users compared (Mean Squared Differences “MSD”) and (b) non-numerical information (structural information) of similarity between the 2 users compared (Jaccard).JMSD provides better recommendation results than traditional metrics; in addition, its design facilitates the creation of variants that can be easily implemented via hardware circuits. For these 2 reasons, JMSD was used as a reference for the creation of the proposed similarity metric: HwSimilarity. JMSD combines the Jaccard and MSD metrics. HwSimilarity combines specific versions of these metrics: BitJaccard and BitMSD, which are adapted to the hardware implementation.In order to define HwSimilarity (running on hardware circuits) from JMSD (defined over real numbers), we design an intermediate metric: BitJMSD. By means of binary data and logic operations, BitJMSD obtains similar results to those of JMSD. This can be summarized according to the following:JMSD=Jaccard*MSD; JMSD uses Jaccard and MSD metrics, dealing with real numbers and real numbers operations.BitJMSD=BitJaccard−BitMSD; BitJMSD uses BitJaccard and BitMSD metrics, dealing with Boolean data and logical operations. Finally, a natural numbers sustraction operation will be necessary.HwSimilarity=AND AND XNOR; HwSimilarity uses AND and XNOR Boolean functions, dealing with Boolean data and logical operations. Finally the Boolean function AND is used to get the result.Fig. 1shows the evolution from JMSD to the proposed HwSimilarity. The evolution of Jaccard and MSD have been discussed, respectively, in Sections 2.2 and 2.3. BitJaccard and BitMSD are discussed in Section 2.4. The proposed HwSimilarity (sequential and parallel) is explained in Section 3. The recommendations quality improvements are showed in Section 4.1. Finally, performance (speed up) improvements are discussed in Section 4.2.Let I the set of the RS items. LetV={min⋯max}∪{•}the range of the ratings and the lack of rating.Let L the set of relevant ratings, and γ the relevance threshold: L={γ⋯max}|γ∊{min⋯max}.Let R the set of ratings from users to items: R={〈u, i, r〉>|∀u∊U, ∀i∊I}.Metrics in this paper utilize only three possible states to define each rating ru,i: (non-relevant: 0, relevant: 1, not voted:•):Let(1)R∗={〈u,i,r∗〉}|∀u∀i,ru,i∗=0⇔ru,i∈V-L1⇔ru,i∈L•⇔ru,i=•Jaccard represents the proportion of items that has been rated in common by the two users that were compared: u, v(2). BitJaccard does not use proportions; instead, it utilizes only the Jaccard numerator (3). Although this simplification greatly reduces the complexity of the required hardware, it leads to a loss of information. It is reasonable to consider the similarity of 2 users who have rated approximately 40 items each, of which 30 items were rated in common. It is not as reasonable to consider the similarity of 2 users who have rated approximately 2000 items each, of which only 30 items were rated in common.Let Iube the set of items voted by user u: Iu={i∊I|ru,i≠•}(2)Jaccard(u,v)=|Iu∩Iv||Iu∪Iv|,Jaccard(u,v)∈[0⋯1](3)BitJaccard(u,v)=∑i∈Iαi|αi=0⇔ru,i∗=•∨rv,i∗=•1⇔ru,i∗≠•∧rv,i∗≠•,BitJaccard(u,v)∈{0⋯|I|}In order to explain each metric and the decisions made to change the similarity measures to fit the hardware proposal, this papers provides a scenario of use showed as a running example: Given a RS with 8 items (I=8), users u and v have voted as follows: u: 5 1•2 1•4 1, v: 4 2••2 1 5 5. We apply the relevancy threshold value γ=4. Fig. 2shows the internal operations of the Jaccard and BitJaccard similarity measures.Jaccard, on the left side of Fig. 2, needs to process the following operations: (1) to calculate, using real numbers, the quantity of times that both user u and user v have rated the same items (numerator); (2) to calculate, using real numbers, the number of times that the user u or the user v have rated any item (denominator); and (3) to calculate a division between real numbers. BitJaccard, on the right side of Fig. 2, simplifies the Jaccard process: (1) It operates just using logical values and (2) It does not need a real numbers division: Instead it uses a simple count of the logical “1” values.To assess how disregarding the Jaccard denominator impacts the quality of the results, the Mean Absolute Error (MAE) of the original JMSD and the MAE of the JMSD without the Jaccard denominator are calculated. Fig. 3shows the resulting loss of accuracy. The results are labeled JMSD and BitJaccard+MSD; COR refers to the Pearson correlation and is incorporated in the figure as a baseline value.The percentage of average accuracy loss is 2.7%. This value is acceptable in the context of a considerable improvement in the response times of RS.The MSD presents a simple measurement of the average value of the discrepancy between the votes cast by the two users who are being compared (4). If the same vote had been cast for each of the items that were mutually voted for, the value of MSD will be zero (maximum similarity).(4)MSD(u,v)=∑i∈Iu∩Iv(ru,i-rv,i)2|Iu∩Iv|where Iurepresents the items rated by the user u.BitMSD(5) and BitJaccard both disregard the information provided by the denominator (in this case, the average value). BitMSD also disregards the numerical values of the votes; it only records the cases in which discrepancies exist for the relevance (1) of the same votes. In [9,11], an experiment is presented that examines how the “relevant/not relevant” assessment not only does not worsen the precision/recall measurements, but rather it improves them both, particularly the precision, when the number of recommendations (N) is high.(5)BitMSD(u,v)=∑i∈Iβi|βi=0⇔αi=0∨ru,i∗=rv,i∗1⇔αi=1∧ru,i∗≠rv,i∗,BitMSD(u,v)∈{0⋯|I|}Fig. 3 shows a negligible accuracy loss between the results of the original JMSD and the combination of the Jaccard with HwDiff BitMSD. This result offers an alternative formulation to the JMSD without apparent loss of quality and with a lower cost of hardware and implementation.Following Fig. 2 running example, Fig. 4shows the internal operations of the MSD and BitMSD similarity measures. MSD, on the left side of Fig. 4, needs to process the following operations: (1) to calculate, using real numbers, the squared errors between the items that both user u and user v have rated (numerator) and (2) to calculate a division between real numbers. BitMSD, on the right side of Fig. 4, simplifies the MSD process: (1) It operates just using logical values and (2) It does not need a real number division: Instead it uses a simple count of the logical “1” values.Finally, the proposed metric, BitJMSD(6), computes the number of items that have been rated in common and with the same significance (relevant or not relevant). BitJMSD generates results in the range {0⋯|I|}; a value of 0 indicates no similarity, and a high value of the metric indicates a high similarity between the two users that are being compared (u, v). Eq. (6) is based on the following facts: (1) BitJaccard↑⇒BitJMSD↓ and (2) BitMSD↓⇒BitJMSD↑.(6)In the next section (Section 3), the running example clarifies the way the proposed similarity measure (HwSimilarity) removes the state • (not rated item), making possible the use of hardware circuits.To estimate the items predictions, the set of k neighbors obtained with BitJMSD is utilized in the aggregation approach phase. The execution time of the aggregation approach is negligible with respect to the time required for calculating the set of neighbors; thus, its hardware implementation is not required. Nevertheless, given that BitJMSD operates with values of 0, 1, and •, we formulate the Deviation From Mean (DFM) aggregation approach (8) by considering the following condition:Letr¯u∗the average rating of the user u:(7)r¯u∗=|{i∈I|ru,i∗=1}||{i∈I|ru,i∗≠•}|LetKu,ithe set of neighbors of u that have rated the item i.Letpu,i∗the prediction of the item i for the user u using the range {0⋯1}:(8)pu,i∗=r¯u∗+∑v∈Ku,iBitJMSD(u,v)(rv,i∗-r¯v∗)∑v∈Ku,iBitJMSD(u,v)Letpu,ithe prediction of the item i for the user u using the range {min⋯max}:(9)pu,i=pu,i∗(max-min)+minAn RS that takes advantage of the proposed metric will allow users to vote on the items with only 2 options: positive and not positive. An operating RS can obtain these 2 values by applying a relevance threshold γ. For both of these cases, we begin with 2 packages of I bits for each user to begin the hardware processing:•Package 1: items “rated/not rated” by the user.Package 2: items “rated as relevant/(not rated or rated as irrelevant)” by the user.Let up1 the user’s u package 1, and up2 the user’s u package 2:(10)BitJaccard is implemented with the AND function applied to package 1 of u and v; BitMSD is implemented with the XNOR function applied to package 2 of u and v. Fig. 5shows the running example hardware operative process. BitJMSD=BitJaccard−BitMSD(6). That is: Number of items both users have voted−Number of items that have been voted using a different “relevant/not relevant” assessment.Upper side of Fig. 5 shows the HwSimilarity implementation of BitJaccard: AND operation applied to the packages_1 bits. Bottom side of Fig. 5 shows the HwSimilarity implementation of BitMSD: XNOR operation applied to the packages_2 bits. Finally, right side of Fig. 5 shows the HwSimilarity implementation of the BitJMSD subtraction: AND operation applied to the previous results.

@&#CONCLUSIONS@&#
Recommender systems that are based on the kNN collaborative filtering algorithm are common to commercial and academic domains. The latent problems of recommender systems include the following issues: the algorithm is not scalable, its execution requires considerable processing time, the size of the databases continually expands, and the recommendation requests from users do not conform to a uniform distribution. As an alternative to the kNN algorithm, the model-based collaborative filtering improves response time; however, it has the disadvantage of continuously downgrading the models.With the goal of accelerating the execution of the kNN algorithm, this paper proposes a new similarity metric: HwSimilarity. This metric yields a recommendation quality that is extremely similar to those of the best existing metrics, and it can be processed using low-cost hardware circuits (as little as $1). The proposed metric is approximately twice as fast as traditional metrics, such as the Pearson correlation. In addition to this intrinsic advantage, it allows hardware implementations that can reduce execution time by half (approximately, in the sequential version) and by a maximum of 2n times (parallel implementation, with n being the number of processing bit units used).This paper provides designs for low-cost hardware implementations; these designs implement the HwSimilarity metric (sequential and parallel). For future studies, we suggest that the following issues be explored: (a) the creation of hardware devices (coarse-grained parallelization) that will help resolve simultaneous recommendation requests from users of one or more recommender systems and (b) the development of a prototype recommender system in which users recognize, in real time as they add new ratings, changes in the recommendations that they receive (recommender systems push technology).