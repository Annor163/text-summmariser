@&#MAIN-TITLE@&#
Adaptive facial point detection and emotion recognition for a humanoid robot

@&#HIGHLIGHTS@&#
We propose a robust landmark detector to deal with pose variation and occlusions.SVRs and NNs are respectively used to estimate intensities of 18 selected AUs.Fuzzy c-means clustering is employed to detect seven basic and compound emotions.Our unsupervised facial point detector outperforms other supervised models.The overall development is integrated with a modern humanoid robot platform.

@&#KEYPHRASES@&#
Facial point detection,Action unit,Pose variation,Occlusion,Emotion recognition,Human robot interaction,

@&#ABSTRACT@&#
Automatic perception of facial expressions with scaling differences, pose variations and occlusions would greatly enhance natural human robot interaction. This research proposes unsupervised automatic facial point detection integrated with regression-based intensity estimation for facial action units (AUs) and emotion clustering to deal with such challenges. The proposed facial point detector is able to detect 54 facial points in images of faces with occlusions, pose variations and scaling differences using Gabor filtering, BRISK (Binary Robust Invariant Scalable Keypoints), an Iterative Closest Point (ICP) algorithm and fuzzy c-means (FCM) clustering. Especially, in order to effectively deal with images with occlusions, ICP is first applied to generate neutral landmarks for the occluded facial elements. Then FCM is used to further reason the shape of the occluded facial region by taking the prior knowledge of the non-occluded facial elements into account. Post landmark correlation processing is subsequently applied to derive the best fitting geometry for the occluded facial element to further adjust the neutral landmarks generated by ICP and reconstruct the occluded facial region. We then conduct AU intensity estimation respectively using support vector regression and neural networks for 18 selected AUs. FCM is also subsequently employed to recognize seven basic emotions as well as neutral expressions. It also shows great potential to deal with compound and newly arrived novel emotion class detection. The overall system is integrated with a humanoid robot and enables it to deal with challenging real-life facial emotion recognition tasks.

@&#INTRODUCTION@&#
In order to build robots that interact in a more human-like and intuitive manner, perception of human emotions is essential [1–3]. Automatic face and expression recognition has greatly benefited such multimodal agent-based interface development. However, detecting emotions from spontaneous facial expressions during real-life human robot interaction could still be challenging because of various pose and subject variations, illumination changes, occlusions and background clutter. Especially, for automatic face analysis, the original vision APIs provided by the robot’s SDK employed in this research were not capable of dealing with such challenging facial emotion recognition tasks.Optimal, robust and accurate automatic face analysis is thus important to deal with such challenging real-life applications since the performance of advanced applications such as facial action and emotion recognition relies heavily on it. Many parametric and mode specific feature extraction approaches in the computer vision field have been proposed to estimate head pose and detect facial landmarks from real-life images to benefit subsequent automatic facial behavior perception to address the above issues. However, many of the above applications found it difficult to balance well between high quality feature extraction and low computational requirement, which is essential in real-time applications.For example, although feature extraction techniques, Active Appearance Models (AAM) and Constrained Local Models (CLM), have been widely used for medical imaging and computer vision research, they rely heavily on intensive training to adapt to diverse pose variations and head movements [4]. Although various approaches have been used to improve their robustness and efficiency, they usually require intensive computational convergence time. Thus, other computational-wise optimal unsupervised methods for automatic face analysis are required in order to deal with challenging real-life (spontaneous) facial emotion recognition tasks.This research is thus motivated to develop a facial emotion recognition system for a humanoid robot to deal with emotion detection from images with pose variations, illumination changes, occlusions and background noise. A cost-effective optimal unsupervised learning scheme for facial point detection is proposed as the first step of this research and implemented incorporating a 2D Gabor filter, a novel feature descriptor, BRISK (Binary Robust Invariant Scalable Keypoints), an Iterative Closest Point (ICP) algorithm and fuzzy c-means (FCM) clustering. In order to deal with images with occlusions effectively, the proposed facial point detector applies ICP to first recover neutral landmark points for an occluded facial region. Then FCM is applied to further inference the shape of the occluded element by taking the prior knowledge of the attributes of the non-occluded facial regions into account. The following post processing is subsequently used to further re-construct the occluded facial region. After we have applied FCM to obtain the shape cluster of the occluded facial element, we select the top five image outputs with the highest correlations to the test image in the cluster and average them to re-construct the best fitting geometry for the occluded facial element. The re-constructed set of landmarks with shape information embedded for the occluded facial region is then used to adjust the neutral landmarks generated by ICP. This background robust facial feature point detector is able to detect 54 facial points for images or real subjects from challenging real-time human robot interaction with great efficiency to fulfil the robot’s computational constraints.In comparison to several typical supervised facial point detection models, e.g. AAM, CLM and a recent state-of-the-art face alignment method, Gauss–Newton Deformable Part Models (GN-DPMs) [5], our proposed unsupervised model is with optimal computational cost and great efficiency for landmark detection to deal with real-life applications with pose variations, scaling differences, and occlusions. It also allows us to go beyond the constraints of individual databases to perform high quality feature extraction. Evaluated with five well-known image databases, our proposed facial point detector outperforms AAM and CLM greatly in terms of landmarks detection accuracy and computational efficiency and also achieves comparable performance in comparison to the state-of-the-art GN-DPM model. It has also laid a solid foundation for subsequent automatic facial behavior perception.Moreover, Facial Action Coding System (FACS) has been used as an intermediate channel to bridge raw motion-based facial representations with emotional facial behavior recognition in many applications [6]. It provides an objective approach to describe the truth of human behavior and is closely related to physical indicators of emotional facial expressions. It employed 32 action units (AUs), which represent the muscular activities to describe and score facial expressions. It also provides a versatile method to describe a wide range of facial behaviors, e.g. facial punctuators in conversation and emotional facial expressions [7,8]. FACS is also capable of describing emotion intensities and compound emotions, and distinguishing fake from real emotional expressions. Thus many computational facial emotion recognition studies employed FACS and AUs [6].According to FACS, the intensity of an AU can be scored on a five-point ordinal level from A to E. Level A refers to a trace of an action. Level B indicates slight evidence. Level C describes pronounced or marked evidence. Level D represents severe or extreme actions with Level E indicating maximum evidence. Each intensity level refers to a range of appearance changes. Despite intensive studies of facial AU detection, automatic AU intensity measurement still posed great challenges to automated recognition systems since the differences between some AUs’ intensity levels could be subtle and subjective during emotional or conversational behavior, and the physical cues of one AU might vary greatly when it occurs simultaneously with other AUs. This research also aims to deal with such challenges to measure AU intensities of those AUs closely related to emotional facial expressions.Furthermore, in cognitive research, perception of facial emotions was regarded to be based on a categorical model [9], which has been intensively employed in the machine learning field. In comparison with the categorical model, neuroscience research suggested that the perception of facial emotions was best to be described as a continuous model [10], where each emotion was described using characteristics common to all emotions in a multidimensional space. Although this model showed advantages in explaining emotion intensities compared to the categorical model, it was still not easy to use it to describe compound emotions. Therefore, Martinez and Du [11] proposed a new theoretical model for the description of multiple compound emotion categories such as happy or angry surprise. Their model aimed to overcome the difficulty that both of the categorical and continuous models encountered. Their proposed method was to define N distinct continuous spaces and linearly combine these several spaces to recognize compound emotion categories for facial expressions. This new theoretical model pointed out directions for building new computational models for the recognition of compound facial behavior.This research is also motivated by the above theoretical research on compound emotion modeling and anatomical knowledge of FACS. In order to incorporate anatomical truth of emotional facial behaviors into our research, intensities of 18 selected facial AUs closely associated with emotional expressions are respectively estimated using support vector regression (SVR) and neural networks (NNs) based on the above derived 54 facial points. FCM clustering algorithm is also subsequently used to recognize seven basic emotions and neutral expressions based on the derived intensities of the 18 AUs. The seven basic emotions detected include happiness, anger, sadness, disgust, surprise, fear and contempt. This FCM-based emotion clustering technique also shows great potential to deal with compound and newly arrived unseen novel emotion classification. In comparison to basic rigid clustering algorithms (e.g. partitioning, hierarchical and density-based methods), which force an object to belong to only one cluster, FCM clustering offers the possibilities to cluster an object into multiple clusters. That is, it calculates the degree of membership (i.e. the likelihood) for a test data point belonging to each fuzzy emotion cluster. Therefore this probability based clustering is able to allow us to go beyond rigid classic clustering to recognize a compound emotion, which may belong to more than one emotion cluster.The overall system has been integrated with the C++ SDK of the latest NextGen H25 NAO robot in order to benefit real-life human robot interaction. The employed NAO robot has a powerful CPU processor and cameras sensors to allow for real-time image processing and better low light perception. Computer vision packages have been provided for the robot to perform basic object and face detection. Especially, the vision API of NAO also allows it to perform basic face recognition by extracting 31 two-dimensional geometric points automatically. But experiments indicated these facial landmark points proved to be sometimes not sufficient enough to capture physical cues of subtle facial behaviors [12]. Especially, they suffered from poor performances when dealing with facial emotion recognition tasks from real-life images with pose variations, illumination changes and occlusions.Therefore, this research has further extended the NAO’s vision APIs and extends landmark generation from 31 to 54 two-dimensional points. It also allows NAO to deal with challenging facial point detection and emotion recognition tasks with large pose variations (at least up to 60 deg), illumination changes, scaling differences and the presence of occlusions and background clutter from dynamic real-life images and interactions. The overall system architecture with facial point detection for non-occluded images is provided in Fig. 1whereas the system architecture with facial point detection for images with occlusions is presented in Fig. 2.Overall, the paper is organized in the following way. Section 2 reviews state-of-the-art research on automatic facial point detection, AU intensity estimation and emotion recognition. The overall system architecture is presented in Section 3. The proposed unsupervised facial point detection is discussed in Section 4. The regression and NN based AU intensity estimation and fuzzy clustering based emotion recognition are respectively discussed in Section 5. Evaluations conducted using several well-known image databases for diverse challenging real-life cases and related discussions are provided in Section 6. We draw conclusion and identify future work in Section 7.

@&#CONCLUSIONS@&#
In this research, we have developed an unsupervised facial point detector (a rarely explored topic), regression-based AU intensity estimation and emotion clustering for the recognition of the eight basic and compound emotions from posed and spontaneous facial expressions. They are integrated with NAOs C++ SDK under Ubuntu to benefit real-life robot human interaction. The proposed facial point detection model is able to perform robust landmark extraction from images with illumination changes, head rotations, pose variations, scaling differences, partial occlusions and background clutter. Our facial point detector has achieved an averaged accuracy rate of 80%, 73%, 78%, 85% and 85% respectively for the evaluation of 200 diverse images taken from CK+, LFW, PUT, LFPW and Helen databases. On average, it has outperformed AAM and CLM respectively by 13% and 9% and achieved comparable performance in comparison to the state-of-the-art supervised model, GN-DPM, for facial point detection across the five databases with posed and spontaneous facial expressions. It also has optimal computational cost and is significantly fast than AAM and CLM with comparable computational costs to GN-DPM. Moreover, the AU intensity estimation and emotion clustering are also evaluated using images from the CK+ database. The SVR-based AU intensity estimation outperformed the NN based method. The average MSE of the SVR-based intensity estimation for the 18 AUs is 0.0397. In comparison to NNs, the SVR-based method performs well for the intensity measurement for the following AUs: AU1, 2, 4, 5, 6, 10, 12, 14, 15, 17, 20, 23, 25, and 26/27. Moreover, FCM clustering also not only enables the robot to recognize the seven basic emotions and neutral expressions but also shows great potential to detect compound and newly arrived novel emotion classes. Evaluated with CK+, it achieves an average accuracy of 90.38% for the recognition of the eight basic emotions. It also outperforms other state-of-the-art research in the field. Initial experiments also indicate its efficiency in tackling compound emotions with an average detection accuracy of 82.83% for surprised compound emotion recognition.In future work, we aim to extend our system to deal with emotion recognition for 90-deg side-view images of spontaneous expressions, which pose challenges to many state-of-the-art applications. We also aim to employ other techniques (e.g. Grey-level Co-occurrence Matrices and evolutionary optimization [69]) to extract appearance deformations embedded in textures to inform affect analysis when geometric features are not able to provide a thorough full view of emotional behaviors. Compound and spontaneous emotional expressions from databases and more testing subjects will also be employed to further evaluate the system’s efficiency. Min-margin based active learning techniques will also be explored to deal with emotion annotation ambiguity of the clustering results of FCM for challenging real-world emotion recognition tasks. Moreover, according to Kappas [1], human emotions are psychological constructs with notoriously noisy, murky, and fuzzy boundaries. Therefore, in the long term, we also aim to incorporate affect indicators embedded in body gestures and dialogue contexts with facial emotion perception to draw a more reliable conclusion on affect interpretation to better deal with open-ended challenging robot human interaction.