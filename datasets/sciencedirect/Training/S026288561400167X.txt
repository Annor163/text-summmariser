@&#MAIN-TITLE@&#
Simultaneous high-dimensional clustering and feature selection using asymmetric Gaussian mixture models

@&#HIGHLIGHTS@&#
We introduce the multidimensional asymmetric Gaussian mixture (AGGM).We propose two novel inference frameworks for unsupervised non-Gaussian feature selection.We have used these frameworks for challenging applications.

@&#KEYPHRASES@&#
Asymmetric Gaussian distribution,Mixture modeling,Expectation-maximization (EM),Rival penalized EM (RPEM),Feature selection,Model selection,Minimum message length (MML),Scene categorization,Facial expression recognition,

@&#ABSTRACT@&#
Finite mixture models are broadly applied in a wide range of applications concerning density estimation and clustering due to their sound mathematical basis and to the interpretability of their results. Indeed, they permit the incorporation of domain knowledge which allows the provision of better insight into the nature of the clusters and then uncovers application-specific desirable patterns that the practitioner is looking for. However, most of the works done on mixture models, when applied to computer vision tasks, assume that per-component data follow a mixture of Gaussians which may not hold as data are generally non-Gaussian (for instance, it is well-known that the distribution of natural images is highly non-Gaussian). The effect of the Gaussian mixture is analogous to the deployment of Euclidean or Mahalanobis type distances for discrimination purposes. Thus, this mixture cannot be applied efficiently in several applications involving asymmetric shapes. In this paper, we overcome this problem by using the asymmetric Gaussian mixture (AGM) model. The AGM can change its shape to model non-symmetrical and heavy tailed real world data which make it a good choice for modeling data with outliers. Modern computer vision applications generally generate complex high-dimensional data and usually, some features are noisy, redundant, or uninformative which may affect the speed and also compromise the accuracy of the used learning algorithm. Therefore, this paper addresses also the problem of unsupervised feature selection when considering AGM models. We propose two approaches for learning the resulting statistical framework. The first approach is based on the minimization of a message length objective and the second one considers rival penalized competitive learning. Our extensive simulations and experiments involving two challenging tasks namely visual scene categorization and facial expression recognition indicate that the method developed in this paper is efficient and has merits.

@&#INTRODUCTION@&#
Mathematical models in general and statistical approaches in particular have been widely used for the development of useful computer vision, signal and image processing algorithms [1–3]. Many of these approaches are based on finite mixture models (i.e. a weighted sum of distributions) which have been the topic of extensive research in the past [4] and have been applied in several applications such as content-based images categorization and retrieval [5]. In the field of finite mixtures, Gaussian mixture model (GMM) has been widely considered, studied and used [6–8]. However, the Gaussian assumption is rarely justified and met in practice [9] and this is especially true in the case of natural images as shown by several studies and research works [10]. Gaussian density has several drawbacks such as its symmetry around the mean and the rigidity of its shape, which prevent it from having a good approximation to data with outliers. Therefore, in order to model data with different shapes many researchers have considered the generalized Gaussian density (GGD) [11,12]. The GGD is able to model data with various shapes thanks to its shape parameter that controls the tail of the distribution: the larger the value of this parameter is, the flatter is the distribution; the smaller is, the more peaked is the distribution. Despite the higher flexibility that GGD offers, it is still a symmetric distribution inappropriate to model non-symmetrical data. In this article, we suggest the consideration of the asymmetric Gaussian distribution (AGD) capable of modeling heavy and short tailed data [13,14]. The AGD uses two variance parameters for left and right parts of the distribution, which allow it not only to approximate a large class of statistical distributions but also to include the asymmetry. An important part of the mixture modeling problem concerns learning the model parameters and determining the number of consistent components (M) which best describes the data.Concerning parameters estimation, the most popular approach is perhaps the one based on the maximization of the likelihood function through the expectation maximization (EM) framework [15]. It is well-known that the EM algorithm needs an appropriate predefined number of clusters. Therefore, in the past decades, a lot of research has been devoted to the automatic selection of the number of clusters which best describes a given data set and a lot of selection criteria have been proposed such as Akaike's information criterion (AIC), minimum description length (MDL), Laplace empirical criterion (LEC), and minimum message length (MML) [4,16]. In particular, the MML criterion has been shown to outperform the majority of existing selection criteria. Thus, we shall consider it in this work by comparing it to another approach based on the rival penalized competitive learning (RPCL) algorithm which has received a lot of attention [17,18]. The RPCL algorithm allows automatic selection of the number of clusters during learning via penalizing the rival in competition. Its basic idea is that for each input not only the winner of the input sample is updated to adapt to the input, but also its rival is de-learned by a smaller de-learning rate. Many experiments have shown that the RPCL can indeed automatically select the correct cluster number by gradually driving extra seed points far away from the input data set. However, its performance is sensitive to the selection of the de-learning rate, such that if it is not well selected, the RPCL may completely break down. In order to overcome this problem, the rival penalized controlled competitive learning (RPCCL) was introduced in [19]. This algorithm sets the de-learning rate at the same value as the learning rate, then dynamically adjust it based on the relative distance of the winner to the rival and the current input, respectively. In Ref. [20], the rival penalized EM (RPEM) algorithm was proposed for mixture-based clustering. The RPEM learns the model parameters by making the mixture components compete with each other at each time step; this can be done by not only updating the winning density component parameters to adapt to the input but also all rivals's parameters are penalized with the strength proportional to the corresponding posterior density probabilities. Therefore, the RPEM is able to automatically select an appropriate number of densities by fading out the redundant densities from a density mixture which can save the computing time. Thus, we propose to use the RPEM algorithm to perform model selection and parameters learning together in a single step for the AGM model.Modern computer vision application generates high-dimensional vectors. Handling data defined in high-dimensional feature spaces is a difficult problem [21]. Theoretically, the more information we have about each pattern, the better a learning algorithm is expected to perform. However, in many cases, some features can be noisy or uninformative which can degrade clustering efficiency [22]. Thus, in order to achieve a good performance of data modeling, irrelevant features have to be discarded. An accurate feature selection (FS), the task of choosing the best feature subset, allows the improvement of the understandability, scalability, and accuracy of the resulting learned models that generalize better to unseen data. Indeed, several recent studies have shown that selecting relevant features allows more meaningful modeling results [23,24]. However, the problem is challenging especially in unsupervised settings because of the absence of class labels that could guide the selection process [25]. Therefore, there have been only few feature selection techniques that have been applied in mixture-based clustering [26–28] since the aim is to identify simultaneously two inter-related unknowns that are optimal feature subset and optimal number of clusters. In this article, and following recent approaches (see, for instance [26–28]), we perform unsupervised feature selection approach by casting it as an estimation problem, thus avoiding any combinatorial search. For each feature, we associate a relevance weight which measures the degree of its dependence with class labels.The remainder of this paper is organized as follows: After the introduction we first describe our AGM model and then detail our feature selection approach in Section 2. In Section 3, we address the issue of identifying the model's order using the minimum message length approach. In Section 4, we integrate the concept of feature saliency into the RPEM algorithm for the AGM model. The subsequent Section 5 demonstrates some computer simulation and experimental results on challenging applications. Finally, the paper closes with a summary of the work and concluding remarks.Formally we say that a D-dimensional random variable, the image feature vector in our case,X→=X1…XDTfollows a M-component mixture distribution if its probability density function can be written in the following form:(1)pX→|Θ=∑j=1MpjpX→|ξjwhere ξjis the set of the parameters of the jth component, {pj} are the mixing proportions which must be positive and sum to one, Θ={p1,…,pM,ξ1,…,ξM} is the complete set of parameters fully characterizing the mixture, M≥1 is the number of components in the mixture. For the AGM, each component densitypX→|ξjis an asymmetric Gaussian distribution (AGD):(2)pX→|ξj=∏d=1D2π1σljd+σrjd×exp−Xd−μjd22σljd2ifXd<μjdexp−Xd−μjd22σrjd2ifXd≥μjdwhereξj=μ→jσ→ljσ→rjis the set of the parameters of component j whereμ→j=μj1…μjD,σ→lj=σ→lj1…σ→ljD, andσ→rj=σ→rj1…σ→rjDare the mean, the left standard deviation, and the right standard deviation of the D-dimensional AGD, respectively. The AGD is chosen to be able to fit, in analytically simple and realistic way, symmetric or non-symmetric data by the combination of the left and right variances.LetX=X→1…X→Nbe a set of N independent and identically distributed vectors, assumed to arise from a finite AGM model with M components. Thus, its likelihood function can be expressed as follows:(3)pX|Θ=∏i=1N∑j=1MpjpX→i|ξjWe introduce stochastic indicator variables, Zi=(Zi1,…,ZiM), one for each observation, whose role is to encode to which component the observation belongs. In other words, Zij, the unobserved or missing vector, equals 1 ifX→ibelongs to class j and 0, otherwise. The complete-data likelihood for this case is then:(4)pX,Z|Θ=∏i=1N∏j=1MpjpX→i|ξjZij.where Z={Z1,…,ZN}. Taking the logarithm of Eq. (4) we can get the complete data log-likelihood by:(5)logpX,Z|Θ=∑i=1N∑j=1MZijlogpjpX→i|ξjThe expectation-maximization (EM) algorithm is the main framework to find the maximum likelihood estimate of the parameters of a probabilistic data generation process characterized by the presence of incomplete (or missing) data in general and the parameters of an underlying finite mixture model in particular.It is noteworthy that the previous model in Eq. (1) supposes actually that the D features have the same importance and carry pertinent information which is not generally the case, since many of which can be irrelevant for the classification task. In order to take into account the potential presence of irrelevant features, it is possible to represent irrelevant features by background Gaussian distribution with parameterλ→=η→δ→for all classes, whereη→=η1…ηDandδ→=δ1…δDrepresent the mean and standard deviation of the Gaussian distribution, respectively. We adopt the feature relevancy approach suggested in Ref. [26] in the case of the finite Gaussian mixture, because it is suitable for unsupervised learning. The main idea is to consider the dth feature as irrelevant if its distribution is independent of the class labels and can follow our common Gaussian density p(Xd|λd). Then, the mixture density in Eq. (1) can be written as:(6)pX→|Θ,λ→,φ→=∑j=1Mpj∏d=1DpXd|ξjdφdpXd|λd1−φdwhere λd=(ηd,δd) andφ→=φ1…φDTis a set of binary parameters, such that φd=1 if the dth feature is relevant and φd=0, otherwise. Note that, {φd} can be considered as missing variables. Thus, the resulting model can be given by Ref. [26]:(7)pX→|ΘM=∑j=1Mpj∏d=1DωdpXd|ξjd+1−ωdpXd|λdwhereΘM=Θω→λ→is the complete set of parameters fully characterizing the mixture. We suppose that not all the features of an observation are important, through the weight relevancy of these features. That is, the weight is denoted as ω=[ω1,…,ωD]Twith 0≤ωd≤1, where ωdrepresents the probability that the dth feature is relevant to all the clusters (ωd=p(φd=1)). Therefore, the irrelevant features have little contribution to a given cluster in the subspace, thus their distributions are common to all the clusters in this case. We finally note that the previous model is reduced to the one in Eq. (1) when all the features are considered as relevant.First, we suppose that the number of mixture components M is known and we use the EM algorithm to estimate the model's parameters. Then, we use the MML criterion to choose the optimal number of classes M.By treating M as known, we can derive the following EM algorithm for parameters estimation:•Expectation step:(8)hj|X→i,ΘM=pj∏d=1Dζijd∑j=1Mpj∏d=1Dζijdwhere ζijd=ωdp(Xid|ξjd)+(1−ωd)p(Xid|λd)Maximization step:(9)pjnew=∑i=1Nhj|X→i,ΘMN(10)μjdnew=∑i=1NωdpXid|ξjdζijdhj|X→i,ΘMXid∑i=1NωdpXid|ξjdζijdhj|X→i,ΘM(11)σljdnew=σljdold−∂2LXΘMZφ∂σljd2−1∂LXΘMZφ∂σljd(12)σrjdnew=σrjdold−∂2LXΘMZφ∂σrjd2−1∂LXΘMZφ∂σrjd(13)ηdnew=∑i=1N∑j=1M1−ωdpXid|λdζijdhj|X→i,ΘM]Xid∑i=1N∑j=1M1−ωdpXid|λdζijdhj|X→i,ΘM(14)δd2new=∑i=1N∑j=1M1−ωdpXid|λdζijdhj|X→i,ΘMXid−ηd2∑i=1N∑j=1M1−ωdpXid|λdζijdhj|X→i,ΘM(15)ωdnew=∑i=1N∑j=1MωdpXid|ξjdζijdhj|X→i,ΘMNwhereLXΘMZφis the model's complete data log-likelihoodLXΘMZφ, and we have∂LXΘMZφ∂σljd=∑i=1NωdpXid|ξjdζijdhj|X→i,ΘMϑlijd∂2LXΘMZφ∂σljd2=∑i=1NωdpXid|ξjdζijdhj|X→i,ΘMϑlijdρlijd+ϑlijd1−ωdpXid|λdζijd∂LXΘMZφ∂σrjd=∑i=1NωdpXid|ξjdζijdhj|X→i,ΘMϑrijd∂2LXΘMZφ∂σrjd2=∑i=1NωdpXid|ξjdζijdhj|X→i,ΘMϑrijdρrijd+ϑrijd1−ωdpXid|λdζijdwhereϑlijd=Xid−μjd2σljd3−1σljd+σrjdifXid<μjd−1σljd+σrjdifXid≥μjd;ϑrijd=−1σljd+σrjdifXid<μjdXid−μjd2σrjd3−1σljd+σrjdifXid≥μjdρlijd=1σljd+σrjd2−3Xid−μjd2σljd4ifXid<μjd1σljd+σrjd2ifXid≥μjdρrijd=1σljd+σrjd2ifXid<μjd1σljd+σrjd2−3Xid−μjd2σrjd4ifXid≥μjdNote that the gradients with respect toσljdandσrjdare non-linear, therefore we have decided to use the Newton–Raphson method, based on the first and second gradients ofLXΘMZφ, for estimation.Generally, the maximum likelihood estimate favors higher values for M which leads to overfitting. Therefore, a model selection criterion is needed to estimate the number of components of a mixture model. The MML approach is based on evaluating statistical models according to their ability to compress a message containing the data (minimum coding length criterion). High compression is obtained by building a short code for your data. Therefore, in the case of MML, the optimal number of classes in the mixture is obtained by minimizing the following cost function [29]:(16)MessLens≈−logpθM+c21+log112+12log|IΘM|−logpX|θMwhere p(θM), I(ΘM), andpX|θMdenote the prior distribution, the Fisher information matrix, and the likelihood, respectively. The constant c=M+D+3DM+2D represents the total number of parameters, and |.| denotes the determinant. Note that the information matrix of the model is very difficult to obtained analytically, therefore, we assume the independence of the different groups of parameters, which allows the factorization of both p(θM) and |I(ΘM)|. Furthermore, we approximate the Fisher information |I(ΘM)| using the complete likelihood which assumes labeled observations. Additionally, since we have no knowledge about the parameters, we adopt the uninformative Jeffrey's prior for each group of parameters as prior distribution. From this, we obtain the following objective:(17)MessLens≈c21+log112+c2logN+3M2∑d=1Dlogωd+3D2∑j=1Mlogpj+∑d=1Dlog1−ωd−logpX|θMwhich we minimize under the constraints 0<pj≤1, 0<ωd≤1, and ∑j=1Mpj=1 in a manner similar to the one followed in Ref. [26]. In order to use the MML approached the EM algorithm undergoes a minor modification in the calculation of the mixing proportions pjand the feature relevancy ωd:(18)pjnew=max∑i=1Nhj|X→i,ΘM−3D2,0∑j=1Mmax∑i=1Nhj|X→i,ΘM−3D2,0(19)ωdnew=max∑i=1N∑j=1Maijd−3M2,0max∑i=1N∑j=1Maijd−3M2,0+max∑i=1N∑j=1Mbijd−1,0where(20)aijd=hj|X→i,ΘMωdpXid|ξjdζijd(21)bijd=hj|X→i,ΘM1−ωdpXid|λdζijdThe following script summarizes the main steps of the algorithm used for the AGM parameters estimation and model selection1.Initialize ΘM:•The feature relevancy is set to ωd=0.5.The number of parameters M=Mmax=10.The AGM parameters Θ are initialized using the Fuzzy C-means. Note that, we initialized both the left and right standard deviations with the standard deviation values obtained from the Fuzzy C-means.Perform the common Gaussian densityλ→parameters estimation to cover the whole data.Implement the EM+MML approachWhileM<Mmaxdo {(a)While not converged do {i.Perform E-Step according to Eq. (8)Perform M-Step according to Eqs. (10) to (14), (18) and (19).Ifpj=0, Then the jthcomponent is eliminated.Ifωd=0, Then the (Xid|ξjd) is eliminated.Ifωd=1, Then the p(Xid|λd) is eliminated.}End WhileCalculate the associated message length Eq. (17).Remove the component j with smallest pj.}End WhileReturn the model parameters with the smallest message length.The main learning purpose is to estimate the parameters ΘMfrom N observations, denoted asX=X→1…X→N. Then, the rival penalized EM (RPEM) algorithm [20,30] is developed from the maximum weighted likelihood framework via maximizing the following weighted likelihood:(22)QΘMX=1N∑i=1NℳΘMX→iwithMΘMX→i=∑j=1Mgj|X→i,ΘMlnpj∏d=1DωdpXid|ξjd+1−ωdpXid|λd−∑j=1Mgj|X→i,ΘMlnhj|X→i,ΘMwheregj|X→i,ΘMis a designable weight, which can embody the model selection in the learning process, satisfying the two constraints:∑j=1Mgj|X→i,ΘM=1and∀jgj|X→i,ΘM=0ifhj|X→i,ΘM=0. Thus, the weightgj|X→i,ΘMcan be expressed by:(23)gj|X→i,ΘM=1+εIj|X→i,ΘM−εhj|X→i,ΘMwhere ε is a small positive quantity which we took as 1. Moreover,(24)Ij|X→i,ΘM=1ifj=c0ifj≠candc=argmax1≤j≤Mhj|X→i,ΘM. Thus, the feature weighted RPEM (FW-RPEM) algorithm for the AGM can be implemented in the following steps:1.Initialize ΘM:•The feature relevancy is set to ωd=0.5.The number of parameters M=Mmax=10.The AGM parameters Θ are initialized using the Fuzzy C-means algorithm. Note that, we initialized both the left and right standard deviations with the standard deviation values obtained from the Fuzzy C-means.Perform the common Gaussian densityλ→parameters estimation:(25)ηd=1N∑i=1NXid(26)δd2=1N∑i=1NXid−ηd2Repeated until convergence for eachX→i,i=1,…N•Expectation step(27)hj|X→i,ΘM=pj∏d=1Dζijd∑j=1Mpj∏d=1Dζijdgj|X→i,ΘM=2−hj|X→i,ΘMifj=c−hj|X→i,ΘMifj≠cMaximization step(28)βjnew=βjold+γβ∂MΘMX→i∂βjΘMold=βjold+γβgj|X→i,ΘM−pjold(29)μjdnew=μjdold+γ∂MΘMX→i∂μjdΘMold=μjdold+γgj|X→i,ΘMωdoldζijd∂pXid|ξjdold∂μjd(30)Sljdnew=Sljdold+γ∂MΘMX→i∂SljdΘMold=Sljdold+γgj|X→i,ΘMωdoldζijd∂pXid|ξjdold∂Sljd(31)Srjdnew=Srjdold+γ∂MΘMX→i∂SrjdΘMold=Srjdold+γgj|X→i,ΘMωdoldζijd∂pXid|ξjdold∂Srjd(32)ωdnew=ωdold+γω∂MΘMX→i∂ωdΘMold=ωdold+γω∑j=1Mgj|X→i,ΘMυijdζijdifωd>1thenωd=1ifωd<0thenωd=0where(33)∂pXid|ξjdold∂μjd=pXid|ξjdoldκijd;∂pXid|ξjdold∂Sljd=pXid|ξjdoldτlijd;∂pXid|ξjdold∂Srjd=pXid|ξjdoldτrijd(34)Sljd=1σljd2Srjd=1σrjd2;pj=expβj∑j=1Mexpβjfor1≤j≤M(35)υijd=pXid|ξjd−pXid|λd;κijd=SljdXid−μjdifXid<μjdSrjdXid−μjdifXid≥μjd(36)τlijd=12Srjd1/2SljdSljd1/2+Srjd1/2−Xid−μjd2ifXid<μjdSrjd1/22SljdSljd1/2+Srjd1/2ifXid≥μjd(37)τrijd=Sljd1/22SrjdSljd1/2+Srjd1/2ifXid<μjd12Sljd1/2SrjdSljd1/2+Srjd1/2−Xid−μjd2ifXid≥μjdNote that, we have used the inverse variances (Sland Sr) in order to ensure that the learning of the model standard deviations are more stable. Also, the learning rates γβ, γ, and γωare taken as 0.0001, 0.001, and 0.0001, respectively.

@&#CONCLUSIONS@&#
