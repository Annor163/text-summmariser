@&#MAIN-TITLE@&#
Transcoding resilient video watermarking scheme based on spatio-temporal HVS and DCT

@&#HIGHLIGHTS@&#
Four criteria based on HVS and DCT domain are used to embed a robust watermark.We apply a quantization index modulation scheme to embed and detect the watermark.Watermark scheme is robust to transcoding video codec and low bit rate conversions.Watermark scheme is robust to signal processing and video frame-based attacks.Better performance than four recently reported video watermarking schemes.

@&#KEYPHRASES@&#
Video watermarking,Video transcoding,Human Visual System,Motion distortion threshold,Visual attention region,

@&#ABSTRACT@&#
Video transcoding is a legitimate operation widely used to modify video format in order to access the video content in the end-user's devices, which may have some limitations in the spatial and temporal resolutions, bit-rate and video coding standards. In many previous watermarking algorithms the embedded watermark is not able to survive video transcoding, because this operation is a combination of some aggressive attacks, especially when lower bit-rate coding is required in the target device. As a consequence of the transcoding operation, the embedded watermark may be lost. This paper proposes a robust video watermarking scheme against video transcoding performed on base-band domain. In order to obtain the watermark robustness against video transcoding, four criteria based on Human Visual System (HVS) are employed to embed a sufficiently robust watermark while preserving its imperceptibility. The quantization index modulation (QIM) algorithm is used to embed and detect the watermark in 2D-Discrete Cosine Transform (2D-DCT) domain. The watermark imperceptibility is evaluated by conventional peak signal to noise ratio (PSNR) and structural similarity index (SSIM), obtaining sufficiently good visual quality. Computer simulation results show the watermark robustness against video transcoding as well as common signal processing operations and intentional attacks for video sequences.

@&#INTRODUCTION@&#
With the rapid advance of multimedia and networking technologies, multimedia services such as teleconferencing, video on demand and distance learning have become more popular in our daily life. In these applications the video format is often required to be converted in order to adapt to several channel capacities (e.g., network bandwidth) as well as end-user's terminal capabilities (e.g., computing and display capacity) [1]. The transcoding is one of the key technologies to fulfill this challenging task. Using a transcoder we can convert a previously compressed video bit-stream into another bit-stream with different bit-rates, different spatial resolutions and/or different compression standards, etc. In the copyright protection issue, the transcoding poses new challenges on video watermarking technologies since it performs complex conversion operations that generate problems regarding the preservation of embedded copyright information. Then malicious users can perform the transcoding to obtain copyright-free video sequence with similar quality as the original ones and they can distribute them illegally [2]. Considering the above mentioned situation, watermark robustness against video transcoding must be considered to design an efficient video watermarking algorithm, however in almost all video watermarking techniques proposed in literature, the embedded watermark is not robust against transcoding and therefore the copyright protection is not sufficiently done.Recently several robust watermarking schemes have been proposed in the literature [3–6], in which the resilience to transcoding is also considered. Lee et al. [3] propose a real-time video watermarking robust against transcoding, in which notable results against spatial reduction are shown, obtaining robustness against conversion of spatial resolution from High-Definition Television (HDTV) to Quarter Video Graphics Array (QVGA). This scheme is performed on MPEG-2 video bit-stream directly in order to satisfy the real-time requirements, however it generates vulnerability against the conversion of other video compression standards with low bit rates. Chen et al. [4] propose a robust video watermarking algorithm using the singular value decomposition (SVD) and slope-based embedding technique, in which synchronization information, together with the watermark sequence, is embedded to combat frame attacks, however re-synchronization mechanism of this scheme is not sufficient for the frame rate reduction caused by some aggressive transcoding. In [5], the Human Visual System (HVS) is used to adapt the watermarking energy of the quantization based video watermarking scheme in DWT domain. This scheme shows watermark robustness to some signal processing attacks; however combined attacks caused by common transcoding tasks remove the embedded watermark sequence. Ling et al. [6] propose a video watermarking algorithm robust mainly to geometrical distortions using Harris-Affine interest point detector. The watermark robustness of this scheme strongly depends on an accurate detection of interest points and generally an aggressive transcoding causes an inaccurate detection of many interest points, reducing the performance of this scheme.The watermark embedding domain is an important aspect to design a video watermarking scheme robust against video transcoding. Video watermarking algorithms proposed in the literature can be classified into three main categories from embedding domains points of view: base-band domain algorithms [7,8], watermarking during video coding process [9,10] and watermark embedding directly on the encoded video sequence [11,12]. We consider that the base-band domain technique is more suitable for a watermarking scheme robust against aggressive video transcoding, because it is not focused on any video compression standard and also in this domain the watermark embedding energy can be adjusted easily according to its robustness and imperceptivity requirements, since the whole spatial information is available.In this paper we propose a video watermarking scheme robust against video transcoding which performs in base-band domain using Quantization Index Modulation (QIM) algorithm [13]. To design a video watermarking scheme robust against an aggressive video transcoding task, first the key aspects of video transcoding, such as quality degradation caused by low bit-rate coding, similarities and difference among video compression standards, and effects of the temporal/spatial resolution change, are analyzed in detail. To embed a watermark sequence as robust as possible keeping the watermark imperceptibility, in the proposed scheme the quantization step size of the QIM algorithm is adaptively calculated using spatial and temporal HVS criteria. In the image watermarking techniques, QIM algorithms with adaptive quantization step size based on the spatial HVS properties have been proposed in order to obtain watermark imperceptibility and robustness simultaneously [14,15]. The proposed scheme also considers and exploits temporal information in order to get advantage on deficiency of the HVS to follow regions with high motion speed, using the spatio-temporal contrast sensitivity function and influence of eye movement. Additionally in the proposed scheme, the visual attention region is segmented in each video frame using Information Maximization to obtain more adequate quantification step size. So the watermark embedding process is performed combining four HVS criteria; texture and luminance sensitivity, a motion distortion threshold and visual attention region. The performance of the proposed scheme is compared with four recently reported robust video watermarking schemes [3–6], showing a better performance of the proposed scheme, especially in robustness against video transcoding. We consider that the main contribution of the proposed scheme is robustness to transcoding which is obtained by a detailed analysis of transcoding task and exploiting the spatio-temporal HVS properties in order to obtain adaptively the quantization step size of the QIM algorithm. In our best knowledge there is no another video watermarking scheme that exploits the spatio-temporal HVS criteria and visual attention region estimation to improve watermark robustness against aggressive attacks.The rest of the paper is organized as follows: In Section 2, we analyze key aspects of video transcoding process to design an efficient watermarking scheme. Section 3 provides a detailed explanation of the watermark energy adaptation based on spatio-temporal HVS-based criteria and the visual attention region segmentation. In Section 4 the proposed scheme is described in detail. The evaluation results of the proposed scheme are compared with four recently reported video watermarking schemes in Section 5 and finally Section 6 provides the conclusions of this research.Digital video can be dynamically adapted according to the available resources of the end-user's devices, such as computing power and display capability, as well as the channel capacity for transmission of the video sequence, such as channel bandwidth. One common example is the delivery of a high-quality multimedia source, such as a Digital Versatile Disc (DVD) or High Definition TV (HDTV), to a receiver with lower resources, such as Smart Phone, Tablets and Personal Computer. The video transcoding is one of the core technologies to satisfy this challenging task, creating adequate bit-stream directly from the original video sequence without user's conscious about decoding and re-encoding process [16]. The transcoding is defined as the operation of converting a video encoded in some format to another one with different characteristics [17]. In practice, the main characteristics of a digital video which can be transformed by a transcoder are the frame rate, bit-rate, video compression standard and spatial resolution as shown inFig. 1. The transcoding task can be classified as homogeneous and heterogeneous. A homogeneous transcoder performs the conversion between video bit-streams of the same video compression standard, while a heterogeneous transcoder provides conversions between video bit-streams with different video compression standards.Frame rate reduction, also known as temporal resolution reduction, is necessary when the user's end-system has less processing capability that cannot support the same frame rate as that of the original video. There are several strategies to avoid the loss of the embedded watermark signal due to this operation. One possible solution is to limit the watermark embedding process to intra frames (I-frames), since the transcoder discards inter-frames (P or B-frames) mainly to obtain frame rate reduction. However, in a blind watermark detection scheme, the parameters such as the group of pictures (GOP) or frame rate are unknown, and then the I-frame cannot be identified in the detection stage. Although a redundant embedding of the watermark signal in every frame of video sequence can solve this problem regardless of the frame rates of the target video, this method becomes vulnerable to intra-video collusion attacks, which can obtain the watermark sequence from several frames with different scenes [2].According to the applications, several video compression standards, such as MPEG-2, Motion Picture Expert Group 4 Part 2 (MPEG-4), Windows Media Video 9 Codec (VC-1), Motion Picture Expert Group 4 Part 10 (H.264 AVC) and On2 True-Motion VP6 (VP6) are employed [16,17]. The necessity of the conversions between different video compression standards is generally associated to obtain acceptable visual quality with lower bit rate. In the heterogeneous transcoding the change of the frame format, as a result of change of the video compression standard, can be considered as an aggressive operation for many watermarking schemes, because a significant loss of information caused by quantization process may damage the integrity of the embedded watermark signal. The required bit-rate of the target video determines the quantization parameters and it is responsible for maintaining video quality while satisfying bandwidth, delay and memory space constraints [17,18].Spatial resolution reduction is required when the display capability of the end-user's device is lower than that of the original one. For example, generally a teleconferencing system uses a Common Intermediate Format (CIF) with a resolution of 352×288 pixels. This resolution must be downscaled if the receiver system has a lower display capability, such as smartphone with Quarter CIF (QCIF) format (176×144 pixels). In this situation, a transcoder can carry out the spatial resolution reduction using different techniques [16].Analyzing all processes performed by a heterogeneous transcoder, we conclude that in order to obtain an efficient watermarking scheme, the following two parameters must be considered: (a) the 2D-DCT coefficients that result more resilient to the quantization process, in which the watermark is embedded and (b) the maximum watermark energy that allows the embedded watermark to be robust against all attacks mentioned in this section, while preserving the watermark imperceptibility by the HVS.As mentioned in Section 2, in order to obtain the robustness against aggressive heterogeneous transcoding, we need to embed a sufficiently strong watermark signal, considering the trade-off between watermark robustness and imperceptibility. To achieve this difficult task, the watermark energy is adapted using four HVS criteria, which takes an advantage of video watermarking schemes performed in base-band domain. The first two criteria are based on the relationship between the sensibility of the HVS to the visual quality degradation and spatial characteristics of each frame of the video sequence. The watermarking energy is calculated using the texture and luminance masking [19]. The third criterion is based on the failure of the HVS to follow regions with high motion speed. We use a concept based on the just noticeable distortion (JND) adapted for video as a model of the observer's eye movement [20]. As the fourth HVS criterion, the visual attention region is obtained for each video frame in order to generate less distortion in regions where observer's attention is attracted [21].The most well-know HVS properties used in image coding fields are frequency sensitivity, texture and luminance masking. The texture masking property suggests that the human eye's sensitivity to error is low in the highly textured image areas, while the luminance masking property suggests that in the bright and dark image regions, the error sensitivity by human eye is low [19]. The luminance space of each frame of a video sequence is segmented into non-overlapped blocks of 8×8 pixels, in which the texture and luminance masking are calculated using the algorithm proposed by [19]. Both criteria are combined to obtain a maximum imperceptible distortion m(n,k) by the HVS for k-th block of n-th video frame, which is given by(1)m(n,k)=TexMask(n,k)×LumMask(n,k)whereTexMask(n,k)andLumMask(n,k)are the texture and luminance masking, respectively. To calculate the texture maskingTexMask(n,k), first each block of 8×8 pixels is classified into texture, plain and edge blocks according to their spatial characteristics. To perform this classification, each block is transformed by 2D-DCT and the block in DCT domain is furthermore segmented into four areas as shown inFig. 2(a), where the absolute sum of coefficients values of each area is denoted by DC, L, E and H, respectively [19]. The classification process can be summarized by the following conditions: a block is classified as plain block if E+H≤μ1 (μ1=125), otherwise if (L+E)/H>γ (γ=4) then the block is considered as edge block and it is classified as texture block if E+H>κ (κ=290). Subsequently the texture maskingTexMask(n,k)is assigned according to the type of k-th block. If the block is plain thenTexMask(n,k)=1, considering that the error in the plain block is most noticeable by the HVS. If the block belongs to the edge block andL+E≤400thenTexMask(n,k)=1.125, otherwiseTexMask(n,k)=1.25[22]. Finally, for texture blocks the texture masking is obtained by(2)TexMask(n,k)=(FmaxT−1)×TexE(n,k)−MinMax−Min+1whereTexE(n,k)=E+His the energy value for the k-th texture block of the n-th video frame; Max and Min represent the maximum and minimum energy for texture blocks, whose values are 1800 and 290, respectively [19], and FmaxTis the maximum elevation value used for fine adjustment of the model whose value is set to 2.25.In the case of luminance masking, the method is divided into two parts: a linear model for middle and high luminance which is based on the Weber's law and a nonlinear model for low luminance where Weber's law is invalid [19]. Fig. 2(b) shows an approximation of the Weber's luminance law, where Lminand Lmaxvalues denote the luminance range for the linear model, which are determined as 90 and 255, respectively [19]. FmaxLrepresents the maximum luminance factor whose value is set to 2 [19]. The luminance maskingLumMask(n,k)of the k-th block of the n-th video frame is given by(3)LumMask(n,k)={(FmaxL−Fref)×DC(n,k)−meanLmax−mean+1ifDC(n,k)>mean1if25≤DC(n,k)≤mean1.125if15≤DC(n,k)<251.125ifDC(n,k)<15whereDC(n,k)is the DC coefficient of k-th block of n-th frame, mean is mean luminance value of n-th frame and Frefis the reference factor corresponding to mean luminance value in linear model.Fig. 3 shows the first frame from “Foreman” video sequences and its texture and luminance masking representation. In the texture masking (Fig. 3(b)), “black” represents plain blocks, where the HVS sensitivity to distortion is larger, while “white” and “gray” represent edge and texture blocks, respectively. In the luminance masking (Fig. 3(c)), regions with higher and lower brightness (darkness) suggest a lower sensitivity to distortion by the HVS than other regions.The third criterion is based on the deficiency of the HVS to follow regions with high motion speed. Taking in account the perceptual motion speed of each 2D-DCT block, the motion distortion thresholdT(n,k,i,j)can be computed. This threshold value indicates the maximum distortion that can be tolerated in the (i,j)-th sub-band of the k-th 2D-DCT block of the n-th video frame, because a distortion with smaller value than this threshold cannot be perceived by the HVS. To compute this threshold, we use a DCT-domain JND-based method for video sequence proposed in [20], which is given by(4)T(n,k,i,j)=1G(n,k,i,j)⋅Mφiφj(Lmax−Lmin)⋅10.6+0.4cos2θi,j,i,j=0,1,…L−1where L is the sub-block size,G(n,k,i,j)is the spatio-temporal contrast sensitivity function (CSF), which is described later,LmaxandLminrepresent the maximum and minimum display luminance values whileMis the number of gray levels, which is set to 256,ϕiandϕjare 2D-DCT normalization factors, which areφu=1/Lfor u=0 andφu=2/Lfor u≠0;θi,j=arcsin(2ρi,0ρ0,j/ρi,j2)is the visual angle, whereρi,j=(i/ωx)2(j/ωy)2/2is the spatial sub-band frequency, whereωℏ=2⋅arctan(Λℏ/2l),ℏ=x,ydenotes either the horizontal or vertical sizes of a pixel in degrees of visual angle, which are represented by the viewing distance l and display sizeΛx×Λy[20].The spatio-temporal CSFG(n,k,i,j)in (4) is defined by [20](5)G(n,k,i,j)=c0(k1+k2|log(ε⋅v(n,k)3)|3)⋅v(n,k)⋅(2πρi,j)2⋅exp(−2πρi,jc1(ε⋅v(n,k)+2)/k3)wherek1,k2,k3andεare constants which are empirically set to 6.1,7.3, 23 and 1.7, respectively [20], andc0andc1are constant values that control the magnitude and the bandwidth of a CSF whose best-fitted values are 7.126 and 0.565, respectively [20].v(n,k)is the perceptual motion speed given byv(n,k)=vI(n,k)−vE(n,k), which corresponds to the motion speed without eye movement,vI, compensated by eye movement speed,vE, where(6)vI(n,k)=f⋅(MVx(n,t)⋅ωx)2+(MVy(n,t)⋅ωy)2represents the motion speed of k-th block without eye movement, f is the video frame rate and(MVx(n,t),MVy(n,t))is motion vector, whilevE(n,k)=min[g⋅vI(n,t)+vmin,vmax]is eye movement speed [20], where g is the gain factor related to the object tracking efficiency of eye,vminandvmaxare minimum and maximum eye movement speed, whose values are set to 0.92, 0.15 and 80.0deg/s, respectively [20].Visual attention is a mechanism that filters out redundant visual information and detects the most relevant parts of our visual field. It is one of the fundamental properties of the HVS that can be used in several applications, such as image and video coding, and computer vision. Due to that, many research groups are currently investigating computational modeling of the visual attention system [21,22]. In the watermarking schemes, especially in video watermarking schemes where the duration of observation of each frame is short, the detection of the visual attention regions allows us to determine an adequate watermarking energy, strong enough to survive transcoding attacks, while keeping a minimum perceptual distortion.The visual Attention based on Information Maximization (AIM) [21] is one of the methods most supported by the mechanism of the HVS, in which some orthogonal basis functions are generated using Independent Component Analysis (ICA) from a great amount of visual patches of T×T pixels randomly obtained from a considerably wide range of natural images. After generation of the basis functions, which are matrices with T×T elements, their pseudo-inverse matrices are calculated. Each patchCi,jof T×T pixels with a center (i, j) of the input frame is analyzed in terms of similaritySi,j,rwith the r-th basis function, where r=1,…,R.(7)Si,j,r=∑p=0T−1∑q=0T−1Ci,j(u,v)⋅Ψr(p,q)whereΨris inverse matrix of r-th basis function,u=i−⌊(T−1)/2⌋+p,v=j−⌊(T−1)/2⌋+qand R is the number of basis functions. ThenSi,j,ris normalized in order that the range of normalizedS¯i,j,ris [0, 1], where the highest and lowestS¯i,j,rare 1 and 0, respectively. The higher similarity between the (i,j)-th patchCi,jand r-th basis function produces the higher value ofS¯i,j,r. Considering that the observer's attention is attracted in the regions with unexpected objects or patterns compared with their surround, and then to obtain this unexpectedness, the self-information of the (i,j)-th patchCi,jfor r-th basis function is calculated, which is1/log(Pri,j(r))=−log(Pri,j(r)), wherePri,j(r)is the probability density function of the normalized similarityS¯i,j,r. For example, if the similarityS¯ik,jk,rbetween a patchCik,jkand r-th basis function is high and also many other patches of the frame have same high similarities with the r-th basis function, then thePrik,jk(r)is high meaning that the patchCik,jkis a common pattern in the frame, so its self-information is low. The self-information of the patchCi,jfor all basis functions is a sum of the self-information of each basis function, which is(8)SMap(i,j)=−∑r=1Rlog(Pri,j(r))The resultant SMap is visual attention map, whose values indicate the unexpectedness grades and then using these values the visual attention regions can be segmented from the rest of the frame.Fig. 4 shows the visual attention region obtained applying the AIM-based method [21] to the first frame of “Coastguard” video sequence. Once the visual attention regions are obtained, the rest of image is segmented by blocks of 8×8 pixels, denominated as Region of No Interest (RONI) blocks.

@&#CONCLUSIONS@&#
In this paper, we proposed a video watermarking technique robust against several signal processing distortions, frame-based attacks and especially video transcoding. To improve robustness and accuracy in the detection, and at the same time obtain a good quality of video sequences, the watermark sequence was embedded and detected using a quantization index modulation (QIM) algorithm with adaptive step size, which is calculated using four HVS-based criteria, such as texture and luminance masking, a motion threshold and visual attention region. The experimental results show fairly good watermark imperceptibility since the obtained PSNR values are near to 47dB and the SSIM is close up to 1, taking in account that the SSIM index is a good indicator of the perceptual quality degradation. The experimental results show the watermark robustness to the five most common video compression standards encoding, such as MPEG-2, MPEG-4, H.264 AVC, VP6 and VC-1 with different bit rates, also the embedded watermark is robust to signal processing, intentional video frame-based attacks and changing spatial resolution. The watermark robustness is evaluated in two practical situations, which are homogeneous and heterogeneous transcoding cases. In both cases, the BERs of the proposed scheme are smaller than 0.1, which provides a reliable copyright protection over the digital video contents in the practical situations. The robustness performance of the proposed scheme is compared with four recently reported robust video watermarking schemes [3–6] under the same conditions. The comparison results show the better performance of the proposed scheme especially in heterogeneous transcoding, which is applied commonly in our daily life.