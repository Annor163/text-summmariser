@&#MAIN-TITLE@&#
Adapting content-based image retrieval techniques for the semantic annotation of medical images

@&#HIGHLIGHTS@&#
Proposed a method for annotating 3D computed tomography images of the liver.Leveraged content-based image retrieval techniques to derive semantic labels from a collection of similar images.Adapted support vector machines and nearest-neighbour search.The method achieved high annotation accuracy than other well-established methods.The method had the best result at ImageCLEF 2014 Liver Annotation challenge.

@&#KEYPHRASES@&#
Image annotation,Content-based image retrieval,Computed tomography,Liver,ImageCLEF,

@&#ABSTRACT@&#
The automatic annotation of medical images is a prerequisite for building comprehensive semantic archives that can be used to enhance evidence-based diagnosis, physician education, and biomedical research. Annotation also has important applications in the automatic generation of structured radiology reports. Much of the prior research work has focused on annotating images with properties such as the modality of the image, or the biological system or body region being imaged. However, many challenges remain for the annotation of high-level semantic content in medical images (e.g., presence of calcification, vessel obstruction, etc.) due to the difficulty in discovering relationships and associations between low-level image features and high-level semantic concepts. This difficulty is further compounded by the lack of labelled training data. In this paper, we present a method for the automatic semantic annotation of medical images that leverages techniques from content-based image retrieval (CBIR). CBIR is a well-established image search technology that uses quantifiable low-level image features to represent the high-level semantic content depicted in those images. Our method extends CBIR techniques to identify or retrieve a collection of labelled images that have similar low-level features and then uses this collection to determine the best high-level semantic annotations. We demonstrate our annotation method using retrieval via weighted nearest-neighbour retrieval and multi-class classification to show that our approach is viable regardless of the underlying retrieval strategy. We experimentally compared our method with several well-established baseline techniques (classification and regression) and showed that our method achieved the highest accuracy in the annotation of liver computed tomography (CT) images.

@&#INTRODUCTION@&#
Medical imaging is a fundamental component of modern healthcare with roles in patient diagnosis, treatment planning, and assessment of response to therapy. A direct consequence of this is the rise in medical imaging informatics research, including content-based image retrieval  [1,2], modality-classification and case-based retrieval  [3], classification  [4,5], and annotation  [5–7]. Semantic image annotation is also emerging as a research question, in which the main research challenge is to detect subtle differences in low-level image features and to relate them to higher-level labels derived from a standard terminology. Ultimately the goal is to apply the annotation technologies for the automatic generation of structured imaging reports  [8,9].Annotation is also considered to be a prerequisite for semantic medical search engines that enable radiologists to find medical images, reports, and associated publications more efficiently  [7]. Automatic semantic annotation is needed because it is difficult, time-consuming and expensive to manually annotate the rich contents of these items. The annotation and image markup use case of the caBIG project  [10], which described a software library that could be used for the annotation of large collections of images, provides an example of the ponderous nature of manual annotation processes. Wennerberg et al.  [7] improved the efficiency of this manual annotation process using an ontology modularisation tool that identifies and ranks fragments of an ontology that are relevant to the annotation task; this relevance is based upon the specific domain (e.g., lymphoma) and hierarchical relationships of terms already annotated. However, these manual annotation approaches require physicians to subjectively determine the labels that are relevant to a particular image based on the physicians’ expertise and prior experience.In contrast, automatic image annotation is conducted on the basis of quantifiable image features. The combination of features present in each image suggests the annotations that are relevant. Many existing approaches described in the summary paper by Deselaers et al. [11] only annotated the images with the properties of the image, such as the image modality, body orientation, body region and biological system being examined. Setia et al.  [5] extracted local feature descriptors from the most salient (interesting) points on each image to capture the geometric relationships present in the image; a hierarchical classification method was used to annotate each image by the image properties listed earlier. In a similar application, Tommasi et al.  [6] proposed a method that extracted global and local features using three classification strategies that emphasised feature fusion at different stages of the annotation process. Ko et al.  [12] presented a method that utilised a random forest classifier together with a predefined body relation graph to identify and annotate the body region shown in the image.A more difficult objective is to annotate the images with clinically relevant content, such as the presence of calcification, mass effect, etc. In the general (i.e., non-medical) domain, image annotation tasks have moved rapidly from object identification to sentence generation, where the aim is to describe the images through words, in the same way in which a human witness might describe a scene that they have observed; several such methods have been described in a recent summary paper  [13]. Kulkarni et al.  [14] used computer vision based object detection to construct a graph of the objects and labeled the graph based upon statistics mined from large corpora of descriptive text; the labels and graph relationships could then be used to generate descriptive sentences.One of the major hurdles in achieving this objective for medical images is that there are likely to be thousands of semantic labels to learn and often very few labeled training samples  [15]. Thus a major challenge of such research is the development of categorisation and annotation techniques that are less hindered by lack of training samples  [16]. To reduce problems caused by lack of training data, Gimenez et al. [17] avoided classification methods and instead annotated liver CT images using logistic regression, through the least absolute shrinkage and selection operator (LASSO). However, their method only annotated binary semantic outcomes that could be presented by positive or negative observations, e.g., whether or not a lesion was homogeneous. In a follow-up study, Depeursinge et al. [18] learned semantic terms describing the visual appearance of liver lesions derived from a linear combination of multi-scale wavelet features. This allowed their method to model each annotation at the most relevant image scale. The method predicted the probability that a particular semantic description (e.g., irregular lesion margin) was applicable to the lesion in the image but did not annotate the effects on anatomical structures, e.g., the proximity of the lesion to the hepatic vasculature.The recognition of image content also falls within the scope of another important area of medical imaging informatics research called content-based image retrieval (CBIR)  [1]. In CBIR, low-level visual features such as intensity, texture, shape, and the spatial arrangement of objects are used to determine which images are similar to a given query  [19]. A key challenge for CBIR is the semantic gap, which is the difference between machine-computed similarity and a human's interpretation of similarity  [19]. Many different CBIR algorithms have been investigated for this purpose; a summary can be found in the recent review by Kumar et al.  [2]. Well-established CBIR techniques are therefore designed to relate low-level image features to higher-level semantic concepts. We hypothesise that the problem of automatic semantic image annotation could be addressed in a related fashion, by adapting the ability of CBIR techniques to leverage low-level image features in the search for images with similar high-level semantic concepts.Thus in this paper, we present a method for the automatic annotation of medical images that is derived from CBIR techniques. Given an image to annotate, we propose to identify or retrieve a collection of semantically similar images that have already been labelled and use this collection to determine the best semantic annotations for the unlabelled image. Our annotation method is designed for limited training data compared to the number of annotations that need to be automatically recognised. We suggest that the technique would be applicable regardless of the underlying retrieval strategy and therefore we describe two ways of identifying the best annotations: either through multi-class classification and nearest-neighbour search, both of which are well-established CBIR methods. We evaluated our work on the annotation of liver CT images by comparing our annotation method to several other well-established techniques. We also compared our method to the state-of-the-art techniques submitted to the Imaging track of the Conference and Labs of the Evaluation Forum (ImageCLEF)  [20] Liver Annotation Challenge  [21]; the outcomes were reported at the CLEF workshop  [22]. In this paper we expand upon the report by including: (i) detailed definitions of the classification and nearest neighbour methods for annotation, and (ii) a more comprehensive evaluation, which includes comparison with well-established techniques that were not submitted to the ImageCLEF Liver Annotation Challenge.We employ the following terminology in the remainder of this paper. A question refers to a specific annotation task, i.e., an element of the structured report that needs to be automatically filled. A label is an annotation that could possibly be assigned to a question. An answer is the label that our method automatically assigns to the question based on the analysis of the image features; the answer is chosen from a set of labels that are unique to each question. The term query refers to a single un-annotated image volume that will be annotated using our approach.We also use the following notation. Let Ω be a question andLΩbe the set of labels for Ω withLΩ=l. During annotation, we also letLΩ+⊆LΩdenote a possible set of answers (needed only in case of ties) andL∈LΩ+denotes the final answer. Note that since only one label is chosen as the final answer (i.e.,L=1) thenL=LΩ+⇔LΩ+=1(there were no ties).We used a public dataset of volumetric (3D) computed tomography (CT) images of the liver from the ImageCLEF 2014 Liver Annotation Challenge  [21]. The dataset contained 50 CT volumes cropped to the region around the liver; the volumes had varied resolutions (x: 190–308 pixels, y: 213–387 pixels, slices: 41–588) and pixel spacings (x, y: 0.674–1.007mm, slice: 0.399–2.5mm). A mask of the liver pixels and the bounding box for a selected lesion were provided for each image.The data also included a set of 60 well-established image features (with a total dimensionality of 458) that had been extracted from the images in the dataset. We refer to these as computer generated (CoG) features and they are further described in Section 2.3.The dataset also contained 73 ground truth annotations. These annotations were independently determined by an experienced clinician based upon the semantic labels in the ONtology of Liver for Radiology (ONLIRA)  [23]. The ontology described the different sets of possible labelsLΩfor different questions Ω. Several label sets have one or two special labels with the following meaning:•other: none of the other labels inLΩfully answer the question.N/A: the question Ω is not relevant to this image.Our aim was to derive the annotations of a query based upon similarity to other images. We adapted two state-of-the-art approaches, classification of image similarity using support vector machines (SVMs)  [24] and weighted nearest-neighbour (WNN) search  [25] to show that our method is applicable regardless of the underlying retrieval strategy.Multi-class SVM classification was used because SVMs are effective in high-dimensional spaces where the dimensionality is higher than the sample size. SVMs are also versatile as they can use different kernel functions for different classification tasks. These characteristics of SVMs make them appropriate for our particular dataset, in which there were a wide variety of annotation labels with a small number of training samples. The WNN method was used because it uses the most similar samples for labelling, which is important when the distribution of samples is not known. Thus, it gives higher annotation priority to more similar samples than to those (dissimilar samples) that are further away. This was appropriate for our dataset, where different annotations had varying distributions of labels. In addition, WNNs are also useful in cases where there are no clear class boundaries, such as between related terms with very subtle differences.We applied similar image annotation processes for both methods. Visual features were first extracted from the images. The classification or similarity model was then trained using the extracted visual features; feature selection was also performed in this phase. After the model is developed, new queries can then be annotated. This is done by comparing the features extracted from these images to the model as described in Sections 2.4 and 2.5.Table 1summarises the features that were used in our experiments. These features are detailed in the following subsections.The dataset contained computed generated (CoG) image features that had already been extracted from the liver, the hepatic vasculature, and the marked (primary) lesion. Image features were also extracted from all lesions in the image and accumulated into a global feature value (e.g., mean intensity). The features described 3D object shape properties (e.g., volume, surface area, sphericity, solidity, convexity, Hu shape invariants  [26]), texture information (e.g., Haralick et al.  [27], Gabor  [28], Tamura et al.  [29], Haar  [30]), and pixel intensity information. The ImageCLEF 2014 task documentation  [21] provides a detailed list of the CoG image features.We cleaned the CoG feature data by removing feature dimensions with missing values (i.e., given a not-a-number or NaN value) or that were used to scale other features and had no variation across all samples. These feature dimensions were excluded from all samples:1.Group: Lesion. Feature: Anatomical Location (5 dimensions). Reason: All dimensions were missing (a NaN value) for one of the images.Group: Lesion. Feature: Hu Moments (3 dimensions). Reason: All dimensions were missing (a NaN value) for one of the images.Group: Lesion. Feature: Histogram (only the first two dimensions). Reason: These features were the upper and lower bounds and were the same for all samples. They were not needed after normalisation.Group: All Lesions. Feature: HistogramOfAllLesions (only the first two dimensions). Reason: These features were the upper and lower bounds and were the same for all samples. They were not needed after normalisation.We also created a bag of visual features (BoVF) representation of the image. This was derived from scale invariant feature transform (SIFT) descriptors  [31] extracted from key points detected in the 2D slices of the CT images. The SIFT descriptors were extracted from all of the axial slices on an image. We randomly sampled 5% of the descriptors extracted from the training dataset. We generated a visual codebook by grouping the subsampled descriptors using k-means clustering with k=1000. This value of k has been successfully used in other medical image retrieval projects on diverse imaging data, e.g., X-rays, magnetic resonance, CT, etc.  [32]. The subsampled clustering process was much faster than clustering on the full set of descriptors and created codebooks of similar quality  [33]. The visual code for each cluster was the cluster centroid, i.e., the mean of all descriptors within that cluster.We then assigned a single visual code to every key point in an image. For each key point, we calculated the Euclidean distance between its descriptor and all of the visual codes (cluster centroids) in the codebook; the codebook entry with the lowest distance (i.e., most similar descriptor features) was assigned as the visual code for the key point. This process was repeated for all of the key points in all of the axial slices in an image. We then created a BoVF descriptor using a k-bin histogram representing the frequency of the visual codes in that image  [32]. The visual codes from all of the axial slices were pooled into a single descriptor.The features we extracted had a variety of ranges. This difference in range would cause some feature dimensions to have a higher influence on distance computation (see Section 2.5) compared to others. We therefore normalised the features to the range0,1by linearly scaling them to a random variable with zero mean and unit variance and shifting the values so they were within the desired range. Let x be the value of a feature f, and μfand σfbe the mean and standard deviation of f in the dataset. The normalised valuex˜of x was determined as follows:(1)x˜=(x−μf)/3σf+12According to Aksoy and Haralick  [34], this equation normalises 99% of the feature values to the range0,1; normalised values lower than 0 were set to 0, while those higher than 1 were set to 1. This normalisation scheme was shown to be effective in prior image retrieval research  [35].SVMs  [24] are a well-established classification technique with many applications in medical CBIR  [36,37]. SVMs are supervised learning models that can be used for binary classification. An SVM divides labelled training data into two categories and classifies new samples into one of these categories. Multi-class problems are usually solved with banks of multiple SVMs.We adapted such an approach for our multi-class, multi-label annotation problem as shown in Fig. 1. The core idea was to divide the problem into two stages for each question. The first stage identified a collection of labels that represented groups of images with similar image features (LΩ+). The second stage was used to evaluate each of the elements of this collection to select the best answer (L).Each stage in our classification approach comprised a bank of several SVM classifiers. For every labelA∈LΩ, we trained an A-vs-rest (1-vs-all) SVM classifier, hence forming l 1-vs-all SVMs. We also trained A-vs-B (1-vs-1) SVMs for every pair of labelsA,B∈LΩwhere A≠B, forming a total ofl2−l/21-vs-1 SVMs. For every question, our first stage was composed of the 1-vs-all SVMs and the second stage was composed of the 1-vs-1 SVMs. This two stage approach was repeated separately for each question.After training, we annotated the queries using the following procedure. We first extracted the same features from the query as described in Section 2.3. The query image was then classified using the first stage. If only one of the 1-vs-all SVMs returned a positive classification (i.e., there was no tie) then the label corresponding to that classifier was adopted as the answer. If the classifiers in the first stage assigned multiple labels (i.e., multiple 1-vs-all classifiers returned positive results) then the second stage was activated. The output of the first stage was the set of labels given a positive response by their associated 1-vs-all SVM, i.e.,LΩ+. During the second stage, we classified the query using the 1-vs-1 classifiers for all the labels inLΩ+(i.e., the 1-vs-1 classifiers for the tied labels). A majority voting scheme was used to select the answer.Two tiebreaker situations remained after both classification stages. The ties included the case where the first stage did not return a positive label and when there was a tie in the vote during the second stage (multiple labels had the highest majority vote). In both of these situations, we set the answer to “other”. For such ties in questions Ω where “other”∉LΩ, we selected the label “N/A” if it was available or “false” for questions that expected a Boolean answer.Our design of the two stage classification scheme was due to the unbalanced training dataset. We expected that the classifiers for labels with few samples would have relatively low accuracy and thus the two stage approach introduced further discriminative power, especially in the case of ties.Our WNN approach for annotation used the most similar training images to select the answers for the query. The core idea was that the query would be annotated with the same label assigned to a collection of images with similar features. An overview of the method is shown in Fig. 2. We created two variations of this approach: the first variation used the entire feature space to determine the nearest-neighbours while the second variation used forward sequential feature selection  [38] to define a unique optimal feature space for each individual Ω. Thus, in the first variation, the process in Fig. 2 was only performed once; in the second variation, the process was repeated for each Ω individually.To locate the nearest-neighbours, we calculated the dissimilarity (s) of the features of each training image from the features of the query, using the Euclidean distance:(2)sQ,T=∑i=0dqi−ti2where Q was the feature vector of the query image (Q), T was the feature vector of a training image (T), qiwas the ith dimension of Q, tiwas the ith dimension of T, and d was the dimensionality of the feature set. Under this formulation, higher values of s indicated greater dissimilarity;sQ,T=0implied that Q and T were exactly similar.The set of n images in the training set with the lowest values of s were chosen as the nearest neighbours because the most similar images are generally expected to be retrieved within the first few results (referred to as early precision)  [32]. LetS=s1,…,snbe the dissimilarity values of these images sorted in ascending order. A weighted voting scheme was used to select the answer for each question using this set of dissimilarity values. The weighted votevifor the ith most similar image was given by:(3)vi=s1+ϵsi+ϵwhere si∈S was the dissimilarity value of the ith most similar image and ϵ=1.18×10−38 was used to avoid divisions by zero.The weighted vote VAfor a labelA∈LΩwas given by:(4)VA=∑i=1nλA,iviwhere(5)λA,i=1ifAisthelabelofTi0otherwiseand Tiwas the ith nearest-neighbour (training image).In the case of a tie (multiple labels having the same weighted vote), we set the answer to “other”. When “other”∉LΩ, we selected the label “N/A” if it was available or “false” for questions that expected a Boolean answer.We accounted for the small training dataset by weighting the value using Eq. (3). Neighbours with a higher similarity would thus have a stronger vote compared to images with a lower similarity. The weighting scheme ensured that the emphasis was placed on the labels of the neighbours that were closest to the query, even if there were a larger number of neighbours of a different label that were further away. This is in contrast to a majority voting scheme, where labels that had a higher frequency in the dataset would have a higher chance to be selected as the answer (depending on the value of n).

@&#CONCLUSIONS@&#
In this paper we provide a new concept for extending CBIR methods to automatically annotate liver CT images, by deriving the annotations from the most semantically relevant images within the already labelled collection. Our methods had higher annotation accuracy on small datasets when compared to several baseline techniques. This was because the underlying CBIR technologies we extended were effective in high dimensional spaces where the dimensionality was larger than the sample size. Our work also scored the best results at the ImageCLEF Liver Annotation Challenge.Our methods enable the annotation of the semantic content of the image and not simply annotation of the image modality or body region, which was prevalent in earlier work  [11]. Our methods annotate more than just binary observations  [17] and are also capable of annotating the characteristics of the anatomical structures in the image  [18]. We have released an implementation of our method11http://sydney.edu.au/~engineering/it/~ashnil/code/liverannot.htmlto encourage future research in semantic image annotation using our work as a baseline.In future work, we plan to improve our approach by an optimised fusion of methods in which each question is annotated using the best performing method and feature set combination. We will examine data augmentation techniques to boost the number of training samples to ensure that annotations by our method are significantly more accurate compared to baseline methods. As part of this, we will also introduce methods to make our approach robust to missing values in the data. We will also investigate recent CBIR work that incorporates complementary non-image information  [43], e.g., the semantic distance  [44] between related terms in an ontology. We will also examine emerging deep learning techniques that have shown great promise in image recognition and classification  [45,46]. We have already begun adapting deep learning methods for modality  [47] and body region  [48] annotation.