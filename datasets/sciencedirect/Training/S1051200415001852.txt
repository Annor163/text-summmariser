@&#MAIN-TITLE@&#
Analysis of the TDLMS algorithm operating in a nonstationary environment

@&#HIGHLIGHTS@&#
An accurate stochastic analysis of the algorithm is presented;The model considers a nonstationary environment, i.e., a time-varying plant;Model expressions describing the algorithm behavior in steady state are obtained;The impact of the algorithm parameters on its behavior is discussed;Simulation results for different scenarios confirm the accuracy of the model.

@&#KEYPHRASES@&#
Adaptive filtering,Nonstationary environment,Stochastic analysis,TDLMS algorithm,Time-varying plant,

@&#ABSTRACT@&#
This paper presents a stochastic analysis of the transform-domain least-mean-square (TDLMS) algorithm operating in a nonstationary environment (time-varying plant) with real-valued correlated Gaussian input data, from which the analysis for the stationary environment follows as a special case. In this analysis, accurate model expressions are derived describing the transformed mean weight-error behavior, learning curve, transformed weight-error covariance matrix, steady-state excess mean-square error (EMSE), misadjustment, step size for minimum EMSE, degree of nonstationarity, as well as a relationship between misadjustment and degree of nonstationarity. Based on these model expressions, the impact of the algorithm parameters on its performance is discussed, clarifying the behavior of the algorithm vis-à-vis the nonstationary environment considered. Simulation results for the TDLMS algorithm are shown by using the discrete cosine transform, which confirm the accuracy of the proposed model for both transient and steady-state phases.

@&#INTRODUCTION@&#
One of the most powerful and well-known adaptive algorithms is the least-mean-square (LMS) algorithm [1]. Due to its features of low computational complexity and robustness for a wide range of operating conditions, this algorithm has been successfully used in many practical applications [2–5]. However, it is well known that, depending on the correlation level of the input data (i.e., eigenvalue spread of the input autocorrelation matrix), the LMS algorithm has its convergence rate compromised [1–5]. Aiming to overcome this drawback of the LMS algorithm, different strategies have been proposed and studied in the open literature [2–19]. Such strategies generally lead to better performance at the expense of a substantial increase in the computational load (for details, see [5]). Nevertheless, in many practical applications, it is important to consider the tradeoff between computational complexity and convergence rate. To meet this challenge, the transform-domain LMS (TDLMS) algorithm [6] arises as an interesting alternative to the LMS algorithm. The TDLMS is obtained by including a preprocessing stage into the LMS [2,4,6]. This preprocessing performs an orthogonal transformation on the input data followed by power normalization [2,17]. Such an approach aims to reduce the correlation level of the input data, thus enhancing the algorithm convergence rate for correlated data [5,6]. Hence, taking into consideration the practical applicability of the TDLMS algorithm, its stochastic model becomes an interesting tool to predict the algorithm behavior under different operating conditions.In short, stochastic models can provide a comprehensive understanding as well as useful insights into the algorithm behavior [20], permitting one to identify undesired behavior and to modify the algorithm aiming to overcome such a problem or even improve its performance under specific operating conditions [21]. Moreover, having model expressions at hand, the use of extensive Monte Carlo (MC) simulations for tuning the algorithm parameters as well as assessing its performance can be avoided [19–25], saving time spent on simulations. However, the analysis of adaptive algorithms has brought to light some mathematical challenges, which are mostly circumvented by the use of simplifying assumptions [5,25]. In general, accurate models require as few as possible simplifying assumptions, implying more mathematical difficulties during their development. Thereby, depending on the kind of assumptions used, there is a tradeoff between the difficulty of the involved mathematics and the accuracy of the obtained model.Concerning the TDLMS algorithm, it is important to mention that most of the analyses presented so far in the literature focus exclusively on the case of a stationary environment [6,8,9,14,26,27]. On the other hand, the analysis of the TDLMS algorithm considering a nonstationary environment (i.e., a time-varying plant) has only been addressed in a few research works [15,25]. In [15], a modified power estimator is used in the TDLMS, leading to a model particularized for such a modified algorithm. Moreover, [15] assumes that the output of each transformation branch is a sequence of statistically independent samples [16], which significantly compromises the accuracy of the obtained model (especially, in the transient phase). In [25], model expressions for predicting the steady-state excess mean-square error (EMSE) of some key algorithms (including the TDLMS) have been derived. Nevertheless, particularly for the TDLMS algorithm, the model expressions presented there are not accurate (see [25, Fig. 11]). In addition, even with respect to the steady-state algorithm behavior, the results obtained in [25] have not been properly discussed. Therefore, a complete and accurate analysis of the TDLMS algorithm operating in a nonstationary environment remains an open problem.Another important aspect in the analysis of the TDLMS algorithm resides in the determination of expected values in the form(1)E[Dˆ−1(n)Rˆ(n)]whereDˆ(n)denotes a diagonal normalization matrix [2] andRˆ(n)represents an estimate of the input autocorrelation matrix. In the open literature, several approximations have been used to analytically determine (1), each of them having distinct accuracy levels [14,15,26,27]. In particular, better results are obtained in [27], where the Averaging Principle (AP) [28] is used to split (1) into two expected values, i.e.,(2)E[Dˆ−1(n)Rˆ(n)]≅E[Dˆ−1(n)]E[Rˆ(n)]whereE[Rˆ(n)]is obtained from the input autocorrelation matrix andE[Dˆ−1(n)]is determined by using an approximate method for computing the integrals arising from the calculus of such an expected value, which are known as hyperelliptic integrals [29]. Although, the use of the AP results in simpler model expressions, its accuracy is satisfactory only under restrictive operating conditions, i.e., when a large window is used to estimateDˆ(n)and when the input data is weakly correlated. Therefore, the determination of accurate analytical expressions for computing such expected values is still an open issue.In this context, focusing on the analysis of the TDLMS algorithm operating in a nonstationary environment (time-varying plant), the present research work has the following goals:i)to determine analytical solutions for expected values in the form of (1) considering real-valued correlated Gaussian input data;to obtain model expressions describing the transformed mean weight-error behavior, learning curve, and transformed weight-error covariance matrix;to derive model expressions for the EMSE in steady state, misadjustment, step size for minimum EMSE, degree of nonstationarity as well as a relationship between misadjustment and degree of nonstationarity; andto discuss how the algorithm parameters affect its performance.The paper is organized as follows. Section 2 describes the rule used to obtain time-varying plants characterizing the nonstationary environment considered and introduces the TDLMS algorithm. Section 3 derives the proposed model and discusses how the algorithm parameters affect its performance. In Section 4, simulation results are shown and discussed, aiming to assess the accuracy of the proposed model under different operating scenarios. Finally, Section 5 presents concluding remarks.Throughout this paper, the mathematical notation adopted follows the standard practice of using lower-case boldface letters for vectors, upper-case boldface letters for matrices, and both italic Roman and Greek letters for scalar quantities. In addition, superscript T stands for the transpose,E(⋅)denotes the expected value, andtr(⋅)characterizes the trace operator.In this section, a brief description of the nonstationary environment considered in the development of the proposed model is firstly presented. Next, the general expressions describing the TDLMS algorithm are introduced.A fundamental feature of adaptive filters is its capability to track variations of the underlying signal statistics in the surrounding environment. Hence, it is important to develop stochastic models characterizing the tracking capability of adaptive algorithms in nonstationary environments [5]. In this context, a plant (system to be identified) exhibiting a time-varying behavior is used here to describe the nonstationary environment considered [1–5]. For simplicity, we assume that the plant undergoes random variations according to a first-order Markov process, which is described by the following rule [4,5,32]:(3)wo(n+1)=αwo(n)+ϕ(n)where0<α≤1,wo(0)characterizes an N-dimensional arbitrary starting vector, andϕ(n)=[ϕ0(n)ϕ1(n)⋯ϕN−1(n)]Tdenotes a plant perturbation vector (see Assumption A3 further ahead).In the literature, it is usually assumed that the value of parameter α in (3) is very close to 1 [3–5]; thus, the first-order Markov process [given by (3)] can be simplified (makingα=1) to the commonly used random-walk model [3,33]. On the other hand, instead of simplifying (3) to the random-walk model as done in [15] and [25], we have preferred to keep α as a free parameter in the development presented here. Thereby, since the proposed model is not restricted to a specific value of α, the condition in which (3) reduces to the random-walk model (i.e.,α=1) follows as a special case. Note also that, makingα=1andϕ(n)=0∀n(σϕ2=0)in (3), the plant weight vector becomes time invariant, i.e.,wo(n)=wo∀n; as a consequence, one returns to the stationary environment commonly considered in the analysis of adaptive algorithms.In order to track variations of the plant weights, an adaptive filter of the same order is used (see Fig. 1). This adaptive filter has its weights updated by the TDLMS algorithm (see Fig. 2), which is given by [2,6](4)w˜(n+1)=w˜(n)+2μDˆ−1(n)e(n)x˜(n)wherew˜(n)=[w˜0(n)w˜1(n)⋯w˜N−1(n)]Tis the transformed adaptive weight vector,x˜(n)=Tx(n)=[x˜0(n)x˜1(n)⋯x˜N−1(n)]Tdenotes the transformed input vector resulting from applying a real-valuedN×N-orthogonal transform T[2] to the input vectorx(n)=[x(n)x(n−1)⋯x(n−N+1)]T,e(n)represents the error signal, μ is the step-size control parameter, andDˆ(n)characterizes a diagonal normalization matrix whose elements are determined by(5)σˆi2(n)=1Mx˜iT(n)x˜i(n)with(6)x˜i(n)=[x˜i(n)x˜i(n−1)⋯x˜i(n−M+1)]Tdenoting an M-dimensional (window length) vector that contains the output of the ith transformation branch. Note that, in practical implementations, the variance of the output of the ith transformation branchσˆi2(n)is usually estimated through(7)σˆi2(n)=βσˆi2(n−1)+(1−β)x˜i2(n)with0≪β<1[2,5,26]. Nevertheless, since the algorithm exhibits the same behavior considering either (5) or (7), the former expression has been used here for the sake of mathematical simplicity.By considering the system identification problem depicted in Fig. 1, the error signal can be expressed as(8)e(n)=d(n)−w˜T(n)x˜(n)with(9)d(n)=woT(n)x(n)+z(n)=w˜oT(n)x˜(n)+z(n)wherew˜o(n)=Two(n)characterizes the time-varying transformed plant weight vector andz(n)denotes the measurement noise (see Assumption A2 further ahead). Note that the adaptive filter output and the error signal are in the time domain, althoughx˜(n),w˜o(n), andw˜(n)are in the transform domain [2].Now, we proceed to derive model expressions describing the algorithm behavior, taking into account the nonstationary environment considered and the particulars of the TDLMS algorithm. Specifically, in this section, model expressions for the transformed mean weight-error behavior, learning curve, and transformed weight-error covariance matrix are firstly derived. Then, model expressions for the steady-state EMSE, misadjustment, step size for minimum EMSE, degree of nonstationarity, as well as a relationship between misadjustment and degree of nonstationarity are also obtained. Based on these expressions, we still discuss how the algorithm parameters affect its performance.Before proceeding further with the model development, let us firstly introduce the simplifying assumptions considered [2–5]:A1.The input signalx(n)is obtained from a zero-mean correlated Gaussian stationary process with varianceσx2and autocorrelation matrixR=E[x(n)xT(n)].The measurement noisez(n)is obtained from a zero-mean stationary process with varianceσz2, which is assumed uncorrelated with any other signal in the system.The plant perturbation vectorϕ(n)is obtained from a process with zero mean, varianceσϕ2, and autocorrelation matrixΦ=E[ϕ(n)ϕT(n)], which is uncorrelated with any other signal in the system. Also, it is assumed thatϕ(k)andϕ(l)are uncorrelated fork≠l, i.e.,E[ϕ(k)ϕT(l)]=0fork≠l.For a slow adaptation condition (small step size),w(n),wo(n), andx(n)are assumed statistically independent11For a different point of view about the independence assumption, see [34]..In this section, an expression for the transformed mean weight-error behavior is derived. To this end, we first substitute (9) into (8) and the resulting expression into (4), which leads to(10)w˜(n+1)=w˜(n)+2μDˆ−1(n)x˜(n)[x˜T(n)w˜o(n)+z(n)−x˜T(n)w˜(n)].Then, using (3) and defining the transformed weight-error vector as(11)v˜(n)=w˜(n)−w˜o(n)the transformed weight-error update equation becomes(12)v˜(n+1)=[I−2μDˆ−1(n)x˜(n)x˜T(n)]v˜(n)+2μDˆ−1(n)x˜(n)z(n)+(1−α)w˜o(n)−ϕ˜(n)where I is anN×Nidentity matrix andϕ˜(n)=Tϕ(n)denotes the transformed plant perturbation vector.Now, taking the expected value of both sides of (12) and considering Assumptions A2–A4, we get(13)E[v˜(n+1)]≅(I−2μP)E[v˜(n)]+(1−α)E[w˜o(n)]with(14)E[w˜o(n)]=αnw˜o(0)and(15)P=E[Dˆ−1(n)x˜(n)x˜T(n)].The key point now is to determine a solution for (15), which depends on the statistical characteristics of the input data. Then, considering real-valued correlated Gaussian input data (see Assumption A1) and using the expected value definition, the elements of matrix P can be written as(16)p(i,j)=M∫−∞∞⋯∫−∞∞︸M+1foldx˜i(n)x˜j(n)x˜iT(n)x˜i(n)fx(x˜i,j)dx˜i,jwhere(17)fx(x˜i,j)=1(2π)M+1det⁡(R˜i,j)e−12x˜i,jT(n)R˜i,j−1x˜i,j(n)represents the Gaussian probability density function [33] of(18)x˜i,j(n)=[x˜i(n)x˜i(n−1)⋯x˜i(n−M+1)x˜j(n)]TandR˜i,j=E[x˜i,j(n)x˜i,jT(n)]is the corresponding autocorrelation matrix. In Appendix A, we present in detail the procedure followed to compute (16), which yields(19)p(i,j)=Mq˜iTHiQ˜i−1r˜i,jwhereQ˜idenotes the eigenvector matrix arising from the eigendecomposition ofR˜i=E[x˜i(n)x˜iT(n)],q˜iis a vector containing the first row ofQ˜i,r˜i,j=E[x˜i(n)x˜j(n)]represents a vector of cross-correlation betweenx˜i(n)[given by (6)] andx˜j(n), andHiis a diagonal matrix whose elements are given by(20)hi(l,l)≅12λi,lGi[∑q=1M/2Ai,l,qln⁡(λi,q′)+Bi,lln⁡(λi,l)]with(21)Gi=∏k=1Mλi,k,(22)Ai,l,q=(λi,q′)M/2λi,lλi,q′−λi,l∏j=1j≠qM/2λi,j′λi,q′−λi,j′,and(23)Bi,l=∏q=1M/2λi,lλi,q′λi,l−λi,q′for(24)λi,q′=λi,(2q−1)λi,(2q),q=1,2,…,M/2characterizing the geometric mean of adjacent pairs of eigenvaluesλi,kofR˜i.Therefore, from the model expression (13), along with (14) and (19)–(24), the transformed mean weight-error behavior of the TDLMS algorithm can be predicted.For the TDLMS algorithm, an expression describing the learning curve can be obtained by substituting (9) into (8), and using (11), which results in(25)e(n)=−v˜T(n)x˜(n)+z(n).Then, squaring (25), taking the expected value of both sides, and considering Assumptions A2 and A4, one obtains [27](26)E[e2(n)]=σz2+tr[R˜K˜(n)]whereR˜=E[x˜(n)x˜T(n)]is the transformed autocorrelation matrix and(27)K˜(n)=E[v˜(n)v˜T(n)]characterizes the transformed weight-error covariance matrix. Thereby, the algorithm learning curve can be predicted ifK˜(n)is known.In this section, an expression describing the transformed weight-error covariance matrixK˜(n)is obtained. Thus, calculating the outer product of (12) [i.e.,v˜(n)v˜T(n)] and taking the expected value of both sides of the resulting expression, we get(28)E[v˜(n+1)v˜T(n+1)]=E[v˜(n)v˜T(n)]+(1−α)E1(n)+(1−α)E1T(n)−E[v˜(n)ϕ˜T(n)]+2μE[v˜(n)z(n)x˜T(n)Dˆ−1(n)]−2μ(1−α)E2(n)−2μ(1−α)E2T(n)−2μE3(n)−2μE3T(n)+4μ2E4(n)−4μ2E[Dˆ−1(n)x˜(n)x˜T(n)v˜(n)z(n)x˜T(n)Dˆ−1(n)]+2μE[Dˆ−1(n)x˜(n)x˜T(n)v˜(n)ϕ˜T(n)]−E[ϕ˜(n)v˜T(n)]+2μE[Dˆ−1(n)x˜(n)z(n)v˜T(n)]−(1−α)E[ϕ˜(n)w˜oT(n)]−4μ2E[Dˆ−1(n)x˜(n)z(n)v˜T(n)x˜(n)x˜T(n)Dˆ−1(n)]+4μ2E5(n)+2μ(1−α)E[Dˆ−1(n)x˜(n)z(n)w˜oT(n)]−2μE[Dˆ−1(n)x˜(n)z(n)ϕ˜T(n)]+2μ(1−α)E[w˜o(n)z(n)x˜T(n)Dˆ−1(n)]+(1−α)2E[w˜o(n)w˜oT(n)]−(1−α)E[w˜o(n)ϕ˜T(n)]+2μE[ϕ˜(n)v˜T(n)x˜(n)x˜T(n)Dˆ−1(n)]−2μE[ϕ˜(n)z(n)x˜T(n)Dˆ−1(n)]+E[ϕ˜(n)ϕ˜T(n)]with(29)E1(n)=E[v˜(n)w˜oT(n)],(30)E2(n)=E[Dˆ−1(n)x˜(n)x˜T(n)v˜(n)w˜oT(n)],(31)E3(n)=E[v˜(n)v˜T(n)x˜(n)x˜T(n)Dˆ−1(n)],(32)E4(n)=E[Dˆ−1(n)x˜(n)x˜T(n)v˜(n)v˜T(n)x˜(n)x˜T(n)Dˆ−1(n)],and(33)E5(n)=E[Dˆ−1(n)x˜(n)z2(n)x˜T(n)Dˆ−1(n)].Then, due to the statistical characteristics ofz(n)andϕ˜(n)(see Assumptions A2 and A3), all terms containing such variables in (28) are equal to zero except those involvingz2(n)andϕ˜(n)ϕ˜T(n). In addition, taking into account thatv˜(n)andw˜o(n)are assumed statistically independent ofx˜(n)(see Assumption A4), the remaining nonzero terms in (28) [i.e., (29)–(33)] can be rewritten as(34)E1(n)=K˜′(n)−K˜o(n),(35)E2(n)≅P[K˜′(n)−K˜o(n)],(36)E3(n)≅K˜(n)PT,(37)E4(n)≅2PK˜(n)PT+Str[R˜K˜(n)],and(38)E5(n)=Sσz2with(39)K˜o(n)=E[w˜o(n)w˜oT(n)],(40)K˜′(n)=E[w˜(n)w˜oT(n)],and(41)S=E[Dˆ−1(n)x˜(n)x˜T(n)Dˆ−1(n)].Therefore, from (34)–(41), (28) reduces to(42)K˜(n+1)=K˜(n)−2μK˜(n)PT−2μPK˜(n)+4μ2{2PK˜(n)PT+Str[R˜K˜(n)]}+4μ2σz2S+(1−α)[K˜′(n)+K˜′T(n)]−2μ(1−α)P[K˜′(n)−K˜o(n)]−2μ(1−α)[K˜′(n)−K˜o(n)]TPT−(1−α2)K˜o(n)+Φ˜with(43)Φ˜=E[ϕ˜(n)ϕ˜T(n)].Note now that the model expression (42) depends on the knowledge of matricesK˜o(n)[given by (39)] andK˜′(n)[given by (40)], which are related to the nonstationary environment (time-varying plant) considered. Hence, it is required to determine auxiliary model expressions for computing recursively such matrices. Particularly, a recursive expression for the plant autocorrelation matrixK˜o(n)can be obtained by pre-multiplying both sides of (3) by T, determining the outer product of the resulting expression [i.e.,w˜o(n)w˜oT(n)], taking the expected value of both sides, and considering Assumption A3. Thereby,(44)K˜o(n+1)=α2K˜o(n)+Φ˜.Likewise, a recursive expression describingK˜′(n)can be obtained from (3) and (10) along with Assumption A4 as(45)K˜′(n+1)=α(I−2μP)K˜′(n)+2μαPK˜o(n).To complete the model derivation, we still need to determine the matrix S given in (41). Then, following a way similar to the one used to obtain P, we derive S as shown in Appendix B. Specifically, the diagonal elements of S are obtained according to(46)s(i,i)=M2q˜iTUiq˜iwhereq˜idenotes a vector containing the first row ofQ˜iandUiis a diagonal matrix whose elements are(47)ui(l,l)≅−14Gi{∑q=1M/2Ai,l,qλi,q′[1+ln⁡(2λi,q′)]+Bi,lλi,l[1+ln⁡(2λi,l)]}withGi,Ai,l,q,Bi,l, andλi,q′given by (21), (22), (23), and (24), respectively. On the other hand, aiming to make the derivation mathematically tractable and since the off-diagonal elements of S have less impact on the overall solution than those on the main diagonal, the off-diagonal elements are computed by using the AP [28], yielding(48)s(i,j)≅M2E[1x˜iT(n)x˜i(n)]E[x˜i(n)x˜j(n)]E[1x˜jT(n)x˜j(n)]withE[x˜i(n)x˜j(n)]given by the(i,j)thelement ofR˜andE{[x˜iT(n)x˜i(n)]−1}obtained as in [27]. Thereby, despite the use of the AP for obtaining the off-diagonal elements, matrix S is computed here in a more accurate way than that given in [27] (see Section 4.3).Finally, taking into account the proposed solutions for P given by (19)–(24) and S given by (46)–(48), along with the model expressions (26), (42), (44), and (45), the algorithm learning curve is completely characterized.Here, an expression describing the steady-state EMSEξex(∞)is derived. To this end, let us first recall the definition of the EMSE [4,5](49)ξex(n)=E{[e(n)−z(n)]2}=E{[v˜T(n)x˜(n)]2}.Thereby, the steady-state EMSE can be obtained by determining the productv˜T(n+1)Dˆ(n)v˜(n+1)from (12), taking the expected value of both sides of the resulting expression, lettingn→∞, considering (15), (34), (49),(50)limn→∞⁡E[v˜T(n+1)Dˆ(n)v˜(n+1)]=limn→∞⁡E[v˜T(n)Dˆ(n)v˜(n)]and Assumptions A2–A4 (for details, see [5, Ch. 15–21]). Thus,(51)ξex(∞)=14μ(1−α)tr{E[Dˆ(∞)][K˜′(∞)−K˜o(∞)]}+14μ(1−α)tr{E[Dˆ(∞)][K˜′(∞)−K˜o(∞)]T}+14μ(1−α)2tr{E[Dˆ(∞)]K˜o(∞)}+14μtr{E[Dˆ(∞)]Φ˜}+μσz2tr(P)+μE6(∞)−12(1−α)tr{R˜[K˜′(∞)−K˜o(∞)]}−12(1−α)tr{R˜[K˜′(∞)−K˜o(∞)]T}with(52)E6(∞)=E[x˜T(∞)v˜(∞)v˜T(∞)x˜(∞)x˜T(∞)Dˆ−1(∞)x˜(∞)].Then, noting that(53)tr{E[Dˆ(∞)][K˜′(∞)−K˜o(∞)]}=tr{E[Dˆ(∞)][K˜′(∞)−K˜o(∞)]T}and(54)tr{R˜[K˜′(∞)−K˜o(∞)]}=tr{R˜[K˜′(∞)−K˜o(∞)]T}(51) can be simplified to(55)ξex(∞)=12μ(1−α)tr{E[Dˆ(∞)][K˜′(∞)−K˜o(∞)]}+14μ(1−α)2tr{E[Dˆ(∞)]K˜o(∞)}+14μtr{E[Dˆ(∞)]Φ˜}+μσz2tr(P)+μE6(∞)−(1−α)tr{R˜[K˜′(∞)−K˜o(∞)]}.Now, taking into account that(56)K˜o(∞)=limn→∞⁡K˜o(n)=11−α2Φ˜and(57)K˜′(∞)=limn→∞⁡K˜′(n)=2μα1−α2[(1−α)I+2μαP]−1PΦ˜(55) becomes(58)ξex(∞)=μσz2tr(P)+μE6(∞)+tr({αE[Dˆ(∞)]+(1−α)R˜P−1}[(1−α)I+2μαP]−1PΦ˜])1+α.Finally, considering(59)E6(∞)≅tr(P)ξex(∞)and(60)R˜P−1≅E[Dˆ(∞)]the steady-state EMSE is obtained according to(61)ξex(∞)≅μσz2tr(P)1−μtr(P)+tr{E[Dˆ(∞)][(1−α)I+2μαP]−1PΦ˜}(1+α)[1−μtr(P)].Firstly, let us recall that the algorithm misadjustmentMis defined as [4,5](62)M=ξex(∞)ξminwhereξex(∞)denotes the steady-state EMSE whileξmincharacterizes the minimum MSE attainable in steady state. Thereby, substituting (61) into (62) and makingξmin=σz2, we can write the misadjustment as a sum of two portions [2–4], i.e.,(63)M=M1+M2with(64)M1=μtr(P)1−μtr(P)characterizing the misadjustment portion present in both stationary and nonstationary cases, and(65)M2=tr{E[Dˆ(∞)][(1−α)I+2μαP]−1PΦ˜}σz2(1+α)[1−μtr(P)]denoting the portion related to the nonstationary environment considered (time-varying plant). Note from these model expressions that, except for the special case of uncorrelated input data22For uncorrelated input data, (63)–(65) can be simplified toM≅μN1−μN+σx2tr(Φ)σz2(1−μN)(1+α)(1−α+2μαN); consequently, in this case, the misadjustment does not depend on the transformation matrix T (see [2, pp. 479–483])., the misadjustment of the TDLMS algorithm is affected by the choice of the transformation matrix T.According to [3–5], theM1portion [given by (64)] is due to the adaptive weight vector fluctuations (termed estimation misadjustment) while theM2portion [given by (65)] is due to the plant weight vector tracking lag (termed lag misadjustment). Thereby, since theM2portion is equal to zero only for the case of stationary environment (α=1andΦ˜=σϕ2Iwithσϕ2=0), the misadjustmentM[given by (63)] attained in a nonstationary environment is always larger than the one obtained in the stationary case. Also, observe from (63)–(65) that, except for the common term1−μtr(P), theM1portion is directly proportional to μ while theM2portion is inversely proportional to μ, ratifying the existence of a tradeoff between these two misadjustment portions mediated by adjusting the step size.In order to clarify the impact of the plant variation speed on the adaptive algorithm, Fig. 3plots the model expressions (63)–(65) as a function of the parameters α [Fig. 3(a)] andσϕ2[Fig. 3(b)]. (The remaining parameter values are the same as used later in Example 1 of Section 4.1, considering 40 dB SNR.) Notice from these figures that the misadjustmentMis a convex function with respect to μ; consequently, there is a step-size value that leads to the minimum misadjustmentM. Particularly, observe from Fig. 3(a) that, for values of α close to 1, small misadjustment values are achieved by using intermediate step sizes; in contrast, as α is decreased, small misadjustment values are attained with small step sizes (μ tending to 0). In turn, notice from Fig. 3(b) that, for α close to 1, intermediate step sizes are required to attain the minimum misadjustmentMover a wide range of values ofσϕ2; such a characteristic gradually vanishes asσϕ2decreases.For more details about the algorithm misadjustment in a nonstationary environment, Fig. 4plots separate curves of (63), (64), and (65) forα=0.999andσϕ2=5×10−7[Fig. 4(a)], andσϕ2=10−5[Fig. 4(b)]. From Fig. 4(a), observe that theM1portion is an increasing function of μ and, hence, small misadjustment values are achieved with small step sizes; on the other hand, due to the characteristics of theM2portion, small step sizes lead to high misadjustment values. Therefore, under certain conditions, it is important to consider an intermediate step-size valueμmethat satisfies this tradeoff between theM1andM2portions, which then leads to the minimum misadjustmentM[see Fig. 4(a)]. In turn, notice from Fig. 4(b) that for a fast time-varying plant (obtained by increasingσϕ2) the overall misadjustmentMis dominated by the lag misadjustmentM2. Thereby, since theM2portion is a convex function with respect to μ, a step sizeμmeM2that leads to the minimum misadjustmentMcan also be determined for this condition [see Fig. 4(b)].Here, since the misadjustment (equivalently, the steady-state EMSE) is a convex function with respect to μ, we derive approximate expressions for the optimum step size that minimizesξex(∞). To this end, taking into account that intermediate step-size values are required to attain the minimum misadjustment especially for values of α very close to 1 [see Fig. 3(a)], expression (61) is first rewritten as(66)ξex(∞)≅μσz2tr(P)1−μtr(P)+tr{E[Dˆ(∞)]Φ˜}4μ[1−μtr(P)].Then, differentiating (66) with respect to μ, i.e.,(67)∂ξex(∞)∂μ=σz2tr(P)[1−μtr(P)]2−[1−2μtr(P)]tr{E[Dˆ(∞)]Φ˜}4μ2[1−μtr(P)]2setting the resulting expression equal to zero, and solving forμ=μme, we get(68)μme=tr{E[Dˆ(∞)]Φ˜}4σz2(1+4σz2tr{E[Dˆ(∞)]Φ˜}tr(P)−1).Note that, forΦ˜=σϕ2I, (68) reduces to(69)μme=Nσx2σϕ24σz2(1+4σz2Nσx2σϕ2tr(P)−1).Aiming to explain the impact of parameterσϕ2on (69), Fig. 5shows the step size for minimum EMSEμmeas a function ofσϕ2. From this figure, one verifies that (69) is an increasing function ofσϕ2, presenting saturation for both very low and very high values ofσϕ2. Such a behavior can be interpreted as follows: firstly, for very low values ofσϕ2, the plant is approximately stationary, implying thatμmetends to zero; second, asσϕ2increases, the plant variation becomes faster and, hence, (69) leads to larger values ofμme. In other words, for plants that undergo slow variation (mainly for small values ofσϕ2), the algorithm needs smaller step-size values to track the plant weight vector with small misadjustment. On the other hand, for plants that experience fast variation (mainly for large values ofσϕ2), larger step-size values are required to properly track the plant weight vector. In particular, for plants with very fast variation (very large values ofσϕ2), the overall misadjustment is dominated by theM2portion [see Fig. 4(b)]. In this condition, the step size for minimum misadjustment (or steady-state EMSE) can be obtained from(70)∂M2∂μ=tr{E[Dˆ(∞)]Φ˜}[2μtr(P)−1]4σz2[μ−μ2tr(P)]2which results in an expression for the step-size value that depends only on the input data statistics, i.e.,(71)μmeM2=12tr(P).In this section, aiming to clarify the concepts of slow and fast variation of the plant weight vector, an expression for the degree of nonstationarity is derived. This metric is formally defined as [3–5,32](72)ψ(n)=E[yo,inc2(n)]E[z2(n)]where(73)yo,inc(n)=[w˜o(n+1)−w˜o(n)]Tx˜(n)is the incremental output variation. Note that the numerator of (72) characterizes the average power introduced by the plant variation and the denominator denotes the minimum MSE [3].Regarding the numerator of (72), pre-multiplying both sides of (3) by T, substituting the resulting expression into (73), taking the expected value of both sides, and considering Assumptions A3 and A4, we obtain(74)E[yo,inc2(n)]=α2n(1−α)2tr[K˜o(0)R˜]+2−α2n(1−α)1+αtr(Φ˜R˜).Then, substituting (74) into (72) and recalling thatσz2denotes the variance of the measurement noisez(n), the degree of nonstationarity reads as(75)ψ(n)=α2n(1−α)2σz2tr[K˜o(0)R˜]+2−α2n(1−α)σz2(1+α)tr(Φ˜R˜).Note from (75) that the degree of nonstationarityψ(n)presents the following characteristics:a)Exponential evolution, starting with the maximum value(76)ψ(0)=(1−α)2tr[K˜o(0)R˜]+tr(Φ˜R˜)σz2and approaching(77)ψ(∞)=2tr(Φ˜R˜)(1+α)σz2.The degree of nonstationarityψ(n)is larger forα<1than forα=1.Forα=1, (76) and (77) become(78)ψ(n)|α=1=ψ(0)=ψ(∞)=tr(Φ˜R˜)σz2which is a result commonly found in the open literature [3–5,32].Here, aiming to provide some insights on the conditions in which the adaptive filter would be able to properly track variations of the plant weight vector, we obtain an expression relating the degree of nonstationarityψ(n)and the misadjustmentM. Thus, substituting (11) into (49), using (3), and considering Assumption A3, the EMSE is firstly rewritten as(79)ξex(n)=E{[x˜T(n)w˜(n)−αx˜T(n)w˜o(n−1)]2}+tr(Φ˜R˜).Similarly, multiplying both sides of (49) byα2, using (11), considering(80)αw˜o(n)=w˜o(n+1)−ϕ˜(n)and taking into account Assumption A3, a second relationship is established as(81)α2ξex(n)=E{[αx˜T(n)w˜(n)−x˜T(n)w˜o(n+1)]2}+tr(Φ˜R˜).Then, adding (79) and (81), and rearranging the resulting expression, we have(82)ξex(n)=11+α2[E({x˜T(n)[w˜(n)−αw˜o(n−1)]}2)+E({x˜T(n)[αw˜(n)−w˜o(n+1)]}2)+2tr(Φ˜R˜)].From (82), it is possible to show that the following inequality holds:(83)ξex(n)≥21+α2tr(Φ˜R˜)≥21+αtr(Φ˜R˜).Now, dividing both sides of (83) byσz2and taking into account thatξmin=σz2, we get(84)ξex(n)ξmin≥2(1+α)σz2tr(Φ˜R˜).Finally, considering (62) and (77), we find [from (84)] that the misadjustment of an adaptive filter operating under the nonstationary environment considered is lower bounded by(85)M≥ψ2(∞).From this inequality, we can draw out the following important features [3,5,32]:a)Ifψ(∞)≫1, variation of the plant weight vector is too fast for the filter to be able to track, leading to a larger misadjustment.Ifψ(∞)≪1, the adaptive filter would generally be able to track variation of the plant weight vector, resulting in smaller misadjustment.Now, aiming to assess the accuracy of the proposed model, some examples are presented considering a system identification problem (see Fig. 1). In these examples, MC simulations obtained by averaging 200 independent runs are compared with results obtained from model predictions. For these examples, a zero-mean correlated Gaussian input signal with unit variance is used (see Assumption A1), which is obtained from an AR(2) process given by(86)x(n)=0.18x(n−1)−0.85x(n−2)+v(n)wherev(n)is a white noise with varianceσv2=0.2748, resulting in an eigenvalue spread of the input autocorrelation matrix ofχ=45.9forN=8. (Note that, irrespective of the process used to generate the input data, the algorithm behavior can be adequately predicted through the proposed model if the input autocorrelation matrix R is known.) Here, the orthogonal transform T used to reduce the correlation level of the input data is the discrete cosine transform (DCT)33Although the DCT has been considered here, any other real-valuedN×N-orthogonal transform could be used instead., since it approximates very well the optimal Karhunen–Loève transform [4].In our simulations, unless otherwise stated, two different values of signal-to-noise ratio (SNR) are used, i.e., 40 dB and 60 dB SNR. For the sake of simplicity, the initial condition for plant weight vector, used in (3), is obtained as(87)wo(0)=wauxwauxTwauxwithwaux=[sinc(0)sinc(1/N)⋯sinc(N−1/N)]Tdenoting a vector that contains samples taken from the sinc function. In the presented examples, the adaptive filter weights are initialized in different ways, namely [3]:a)w˜(0)=Two(0), aiming to evaluate the tracking behavior in the nonstationary case; andw˜(0)=0, in order to assess the algorithm behavior during the acquisition mode in the stationary case.In the following, four examples are presented aiming to verify the accuracy of the proposed model under different operating conditions.Example 1In this example, the proposed model is assessed under a nonstationary environment in order to confirm the accuracy of the model expressions describing the transformed mean weight-error behavior (13) and the learning curve (26). To this end, the following parameter values are used:N=8,M=8, andμ=0.1μme=0.00257. In addition, to obtain the time-varying plant, we consider in (3) thatα=0.999and that the elements of the plant perturbation vectorϕ(n)consist of samples taken from a white noise with autocorrelation matrixΦ=σϕ2I, whereσϕ2is adjusted so thatψ(∞)=0.2for each SNR value considered. For this operating scenario, Fig. 6shows curves describing the transformed mean weight-error behavior for only four of the weights (for the sake of clarity). Specifically, Figs. 6(a) and (b) depict the case of40dBand60dBSNR, respectively. In addition, Fig. 7shows the learning curves obtained for both SNR values. From these figures, a very good match between MC simulations and the proposed model is verified for both transient and steady-state phases.Example 2In this example, considering also a nonstationary environment, the accuracy of the proposed model is assessed for different filter lengths and eigenvalue spreads of the input autocorrelation matrix. To this end, we now consider filters withN=32andN=64weights forM=16andM=32, respectively. In particular for this operating scenario, the eigenvalue spreads of the input autocorrelation matrix areχ=581.13forN=32andχ=941.17forN=64, both obtained by changing −0.85 to −0.95 in (86) withσv2=0.0966. Here, the step size isμ=0.000257, aiming to assure the algorithm stability, with the remaining parameter values being the same as in Example 1. The results obtained for this scenario are depicted in Fig. 8, in which a very good match between MC simulations and model predictions can be verified for these larger filter lengths and both SNR values considered. Note also that the proposed model performs well even for higher eigenvalue spreads, ratifying the accuracy of the methodology used to compute matrices P and S.Example 3Here, the accuracy of the model expression describing the steady-state EMSE is verified in a nonstationary environment for different step-size values. To this end, results predicted from the model expression (61) are compared with those obtained by MC simulations, averaging the last 100 EMSE values in steady state. Particularly for this example, we use three distinct values for the plant perturbation variance, i.e.,σϕ2=5×10−6,σϕ2=5×10−8, andσϕ2=5×10−10. The remaining parameter values are the same as in Example 1. In this context, Fig. 9illustrates the steady-state EMSE as a function of μ. From this figure, we verify that the model expression (61) predicts very well the steady-state EMSE, irrespective of the SNR andσϕ2value considered. Furthermore, observe from Figs. 9(a) and (b) that fast time-varying plants (obtained by increasingσϕ2) lead to higher steady-state EMSE values due to the tracking lag. On the other hand, slow time-varying plants (obtained by decreasingσϕ2) lead to lower steady-state EMSE values [see Figs. 9(e) and (f)]. Specifically in Figs. 9(c) and (f), notice that under certain conditions there exists an intermediate step-size value for minimum EMSE. Therefore, these results come from confirming the discussion presented in Sections 3.5 and 3.6.Example 4This example has three main goals, namely: a) to assess the accuracy of the proposed model (via learning curve) in a stationary environment (α=1andσϕ2=0); b) to verify the impact of the value of M on the model accuracy; and c) to compare the proposed model with that given in [27], which is derived exclusively using the AP. To this end, two different values of M are used, i.e.,M=8andM=64, whereas parameters N and μ are the same as in Example 1. The results obtained for this scenario are shown in Fig. 10. Notice that the model given in [27] does not properly predict the algorithm behavior during the transient phase for small values of M; according to [27], satisfactory results are obtained forM>30. On the other hand, the proposed model exhibits very good accuracy for both transient and steady-state phases, irrespective of the value of M. Moreover, observe that the steady-state EMSE is smaller than that achieved in the nonstationary case (see Fig. 7), ratifying the discussion presented in Section 3.5.In this section, simulation results are shown aiming to verify the validity of approximations (35)–(37). Such results are obtained by considering the scenario described in Example 1 for the case of 40 dB SNR, usingM=8andM=64. These results are depicted in Figs. 11–13; in particular, Fig. 11 illustrates the accuracy of the first four elements on the main diagonal of (35) for bothM=8andM=64. From the presented plots, we verify the reasonability of the approximation used, which is confirmed by the very good match between the curves obtained by MC simulations of{E[Dˆ−1(n)x˜(n)x˜T(n)v˜(n)w˜oT(n)]}i,iand model{P[K˜′(n)−K˜o(n)]}i,i. In Figs. 12 and 13, the same presentation pattern of Fig. 11 is repeated for the approximations given by (36) and (37), respectively. Regarding Fig. 13(a), notice that (37) is less accurate forM=8, becoming better as the value of M is increased [see Fig. 13(b)]. Therefore, observe that the approximations used in the model development exhibit satisfactory accuracy for both values of M considered.Here, simulation results are presented in order to assess the accuracy of the proposed solutions for matrices P [given by (19)–(24)] and S [given by (46)–(48)] in comparison with those solutions obtained by using the AP in [27]. Such results, shown in Fig. 14, are obtained by considering the same parameter values as in Example 1. From this figure, one verifies that the proposed approach for computing both P and S exhibits very good accuracy, i.e., a very good match between the results obtained by MC simulations and the proposed solutions. However, the same accuracy cannot be verified by means of the results obtained from [27].In this paper, a stochastic model for the TDLMS algorithm operating in a nonstationary environment (time-varying plant) was presented, from which a model for a stationary environment follows as a special case. Specifically, we derived model expressions for the transformed mean weight-error behavior, learning curve, transformed weight-error covariance matrix, steady-state EMSE, misadjustment, step size for minimum EMSE, degree of nonstationarity, as well as a relationship between misadjustment and degree of nonstationarity. Based on these model expressions, we have also discussed how the algorithm parameters affect its performance and which conditions must be fulfilled so that the algorithm can properly track a time-varying plant. It is important to highlight that such expressions have been obtained by using a small number of simplifying assumptions and approximations, leading to a model that accurately predicts the algorithm behavior over a wide range of operating conditions. This accuracy is mainly due to the strategy used for computing some expected values arising from the model development. Moreover, in contrast to other models so far presented in the open literature, the proposed model is valid irrespective of the window length used to estimate the signal power at each transformation branch. Simulation results showed a very good match between MC simulations and model predictions, ratifying the effectiveness of the proposed model for both transient and steady-state phases.

@&#CONCLUSIONS@&#
