@&#MAIN-TITLE@&#
Focused crawling enhanced by CBP–SLC

@&#HIGHLIGHTS@&#
A heuristic-based approach, CBP–SLC, is presented for enhancing focused crawling.A weighted voting classifier using TFIPNDF feature weighting approach is built.1-DNFC identifies more reliable negative documents from the unlabeled examples set.

@&#KEYPHRASES@&#
Focused crawling,DOM tree,TFIPNDF,CBP–SLC,WVC,Tunneling,

@&#ABSTRACT@&#
The complexity of Web information environments and multiple-topic Web pages are negative factors significantly affecting the performance of focused crawling. In a Web page, anchors or some link-contexts may misguide focused crawling, and a highly relevant region also may be obscured owing to the low overall relevance of that page. So, partitioning Web pages into smaller blocks will significantly improve the performance. In view of above, this paper presents a heuristic-based approach, CBP–SLC (Content Block Partition–Selective Link Context), which combines Web page partition algorithm and selectively uses link-context according to the relevance of content blocks, to enhance focused Web crawling. For guiding crawler, we build a weighted voting classifier by iteratively applying the SVM algorithm based on a novel TFIDF-improved feature weighting approach. During classifying, an improved 1-DNF algorithm, called 1-DNFC, is also proposed aimed at identifying more reliable negative documents from the unlabeled examples set. Experimental results show that the performance of the classifier using TFIPNDF outperforms TFIDF, and our crawler outperforms Breadth-First, Best-First, Anchor Text Only, Link-context, SLC and CBP both in Harvest rate and Target recall, which indicate our new techniques are efficient and feasible.

@&#INTRODUCTION@&#
With the rapid growth of information and the explosion of electronic text from the complex World Wide Web, more and more knowledge you need is included. But, the massive amount of text also takes so much trouble to people for finding useful information. For example, users often visit a search engine site to find relevant information. But, the results may be mixed with a great deal of irrelevant Web pages which include a lot of noise. The main reason of above problem is the WWW information overload. Therefore, an appropriate method to obtain relevant information can solve the problem effectively. Focused crawlers [5,30] (also called topical crawlers) are designed to collect pages on specific topic, which are widely used today. Focused crawlers carefully decide which URLs to scan and in what order to pursue based on previous downloaded pages information, which rely on the fact that pages about a topic tend to have links to other pages on the same topic. Focused crawlers have been used in a variety of applications such as search engines [16], competitive intelligence [6], and digital libraries [33]. They allow a higher-level application to gather from the Web, a focused collection rich in information about a topic or theme.To design an efficient focused crawler collecting relevant pages from the WWW, the choice of strategy for prioritizing unvisited URLs is crucial. In general, texts and links about the same topic are usually grouped into one region in a Web page. Motivated by this, we partition Web pages into some content blocks in this paper. Our solution approach works with the DOM (Document Object Model) trees, rather than raw HTML markup. Instead of treating a whole Web page as a unit of relevance calculating, we evaluate each content block, respectively. Anchor text is a key component of the Web page. It is a powerful navigation for people browsing the WWW, and it also helps search engines understand the relationship among Web pages. But, anchor text is not informative enough, so, it sometimes needs to be expanded to link-context. Link-context is the critical contextual information of anchor text for retrieving domain-specific resources. While some link-contexts may misguide focused crawling and obtain the inappropriate Web pages, because several relevant anchor texts become irrelevant or several irrelevant anchor texts become relevant after calculating the relevance (e.g., our previous work on selectively using link context presented in IAJIT). For irrelevant content blocks, we adopt a heuristic-based approach to deal with anchor text and the link-context to make the topic of out-link page clearer and more accurate. Although there are many advantages using link-context to crawl the Web pages, it is not known whether every anchor text should combine with several bytes link-contexts in a text window. Therefore, in the light of this, this paper presents a novel method combining Content Block Partition and Selective Link Context (CBP–SLC) to enhance focused crawling guided by text classifier.The strategy of text classification is also another direct impact on the performance of focused crawling. In order to select links that lead to documents of interest, and avoid links that lead to off-topic regions, focused crawlers use the page classifiers to guide the search. For more domain-specific information, focused crawler gives priority to links that belong to pages classified as relevant. However, it is generally known that the primary challenge of text classification problem is that no labeled reliable negative documents are available in the training example set, and it is quite costly, exhausted and difficult to obtain training data. So, PU (learning from Positive and Unlabeled examples) text classifying is a very effective strategy. Text classification is the task of automatically applying labels to new documents, which is used widely [18,25,36,41], based on the classifier learnt from training examples. This paper also presents a TFIDF-improved approach, TFIPNDF (Term Frequency Inverse Positive–Negative Document Frequency), for weighting the terms in the positive and negative training example set, respectively. According to TFIPNDF, during the process of training classifier, a term plays different roles in training set. That is, the more documents a positive feature appears in, the more significant it is in positive training set. Also, our method has higher requirements on the quality of the training set, especially the negative data set. Therefore, how to identify more reliable negative documents is also studied in this paper.The rest of the paper is organized as follows. We review the related work in Section 2. Section 3 illustrates how to represent and use CBP–SLC to enhance the focused crawling. Section 4 describes how to identify more reliable negative documents from the unlabeled example set using 1-DNFC and presents the process of our text classifying. The whole crawling procedure is proposed in Section 5. Several comprehensive experiments are performed to evaluate the effectiveness of our method in Section 6 in which experimental settings, performance metrics and results are provided. Section 7 draws the conclusions.

@&#CONCLUSIONS@&#
In this paper, we presented a novel focused Web crawling approach enhanced by CBP–SLC for domain-specific information retrieval guided by text classifier. The methods presented in this paper and the experimental results draw the following conclusions:TFIDF does not take into account the difference of term IDF weighting in the positive and negative example sets when building text classifier. And, TFIPNDF can be considered to make up for the defect of TFIDF in text classification. And the experimental result proved that the performance of classifying using TFIPNDF weighting outperforms TFIDF for each dataset. We also built a weighted voting classifier by iteratively applying the SVM algorithm using TFIPNDF. Furthermore, for comparing the performance of the text classifying based on different techniques clearly, we also implement OCS and PEBL methods used for comparison. And, 1-DNFC effectively ensures and enhances the performance of classifying.CBP–SLC method, which works with the DOM tree as opposed to raw HTML markup, makes Web page content more refined, and the highly relevant regions in an excessive clutter Web page are not obscured. Treating anchor text and its link-context respectively can both improve the efficiency and bring the error rate down caused by the misguidance of crawler. The comparison between using anchor text, link-context and content block partition also shows that many Web pages can be retrieved only using anchor text. Accordingly, partitioning the Web pages into smaller blocks and selectively using anchor and link-context improve the focused crawling performance significantly.