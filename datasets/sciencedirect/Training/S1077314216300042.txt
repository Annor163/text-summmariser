@&#MAIN-TITLE@&#
Interactive multiple object learning with scanty human supervision

@&#HIGHLIGHTS@&#
Efficient approach for learning and detecting multiple objects in image sequences.Interactive object learning using scanty human supervision.Computation of multiple online classifiers using human-robot interaction.Real-time performance in diverse recognition problems.

@&#KEYPHRASES@&#
Object recognition,Interactive learning,Online classifier,Human-robot interaction,

@&#ABSTRACT@&#
We present a fast and online human-robot interaction approach that progressively learns multiple object classifiers using scanty human supervision. Given an input video stream recorded during the human-robot interaction, the user just needs to annotate a small fraction of frames to compute object specific classifiers based on random ferns which share the same features. The resulting methodology is fast (in a few seconds, complex object appearances can be learned), versatile (it can be applied to unconstrained scenarios), scalable (real experiments show we can model up to 30 different object classes), and minimizes the amount of human intervention by leveraging the uncertainty measures associated to each classifier.We thoroughly validate the approach on synthetic data and on real sequences acquired with a mobile platform in indoor and outdoor scenarios containing a multitude of different objects. We show that with little human assistance, we are able to build object classifiers robust to viewpoint changes, partial occlusions, varying lighting and cluttered backgrounds.

@&#INTRODUCTION@&#
Over the last decade, we have witnessed the enormous progress in the field of object recognition and classification in images and video sequences. At present, there are methods that produce impressive results in a wide variety of challenging scenarios corrupted by lighting changes, cluttered backgrounds, partial occlusions, viewpoint and scale changes, and large intra-class variations (Ali and Saenko, 2014; Felzenszwalb et al., 2010; Hinterstoisser et al., 2011; Malisiewicz et al., 2011; Schulter et al., 2014; Tang et al., 2012; Villamizar et al., 2012a).This progress in object recognition has had a positive impact in many application fields such as robotics, where computer vision algorithms have been used for diverse robotics tasks such as object recognition and grasping (Alenyà et al., 2014; Amor-Martinez et al., 2014), detection and tracking of people in urban settings (Bellotto and Hu, 2009; Merino et al., 2012; Portmann et al., 2014), human-robot interaction (Ragaglia et al., 2014; Tamura et al., 2014), and robot localization and navigation (Corominas et al., 2008; Ferrer et al., 2013; Hornung et al., 2010).The standard method for recognizing objects in images consists in computing object specific classifiers during an offline and time-consuming training step, where large amounts of annotated data are used to build discriminative and robust object detectors (Felzenszwalb et al., 2010; Malisiewicz et al., 2011). However, there are situations in which offline learning is not feasible, either because the training data is obtained continuously, or because the size of the training data is very cumbersome, and a batch processing becomes impractical. This is particularly critical in some robotics applications, specially those related to human-robot interaction, where the robots need to compute object detectors on the fly, in real time, and with very little training data.In these cases, online learning methods which use their own predictions to compute and update a classifier have been proposed (Godec et al., 2010; Grabner and Bischof, 2006; Moreno-Noguer et al., 2008). Yet, although these approaches have shown great adaptation capabilities, they are prone to suffer from drifting when the classifier is updated with wrong predictions. This has been recently addressed by combining offline and online strategies (Gall et al., 2010; Kalal et al., 2010), semi-supervised boosting methods (Grabner et al., 2008), or by using human intervention during learning so as to assist the classifier in uncertain classification cases (Villamizar et al., 2012b; Yao et al., 2012).In preliminary versions of this work, we already proposed online object detection approaches in which the human assistance is integrated within the learning loop in an active and efficient manner (Garrell et al., 2013; Villamizar et al., 2012b, 2015). In Garrell et al. (2013); Villamizar et al. (2012b) the proposed approach was focused for single object detection, and further extended in (Villamizar et al., 2015) to multiple instances using an adaptive uncertainty classification threshold that reduces the human supervision. In this paper, we unify the formulation of these previous works and perform a more in-depth analysis of the method presented in Villamizar et al. (2015) through additional experiments in synthetic and real scenarios, while providing more comparisons against competing approaches.More precisely, we propose a fast and online approach that interactively models several object appearances on the fly, using as few human interventions as possible, and still keeping the real-time efficiency (Villamizar et al., 2015). At the core of our approach, there is a randomized tree classifier (Criminisi et al., 2011; Ozuysal et al., 2010; P. Geurts and Wehenkel, 2006) that is progressively computed using its own detection predictions. Yet, to avoid feeding the classifier with false positive samples (i.e, drifting), we propose an uncertainty-based active learning strategy (Lewis and Gale, 1994; Settles, 2010) that gradually minimizes the amount of human supervision and keeps high classification rates. Note that this issue is critical in order to maintain long-term interactions with robots, as if the robot keeps asking for annotating images insistently, people tend to quickly give up the interaction (Garrell et al., 2013; Rani et al., 2006).To make the proposed approach scalable for various object instances, multiple object specific classifiers are computed in parallel, but sharing the same features in order to maintain the efficiency of the method and to reduce the computational complexity in run time Villamizar et al. (2010).As an illustrative example, Fig. 1shows the operation of the proposed interactive method to learn and detect multiple object instances through human-robot interaction (Fig. 1a). Each time the human user seeks to model a new object of interest, he/she marks a bounding box around the object in the input image, via a mouse, keyboard or touchscreen (see Fig. 1b). The robot initializes a model for this new object and runs a detector on subsequent frames for this, and the rest of objects in the database ( Fig. 1c). When the robot is not confident enough about the detections and class predictions, it requests the human assistance to provide the true class labels, which, in turn, are used to update the classifier, observe Fig. 1d. This procedure is performed continuously, and at each iteration, the performance and confidence of the classifier is increased whereas the degree of human intervention is reduced significantly.The remainder of the paper is organized as follows: Section 2 describes the related work and our contributions, while Section 3 explains the proposed approach with all its main ingredients. Section 4 describes the experiments conducted to evaluate the proposed learning approach. We report results using synthetic an real data. The former are used to thoroughly assess the limits of the method in terms of number of classes it can handle or classification rate. Real experiments demonstrate the applicability of the method for diverse perception tasks in challenging scenarios.In this section, we show and discuss our contributions along with the related work on the three main topics concerned with the proposed approach: human-robot interaction, the computation of online classifiers, and interactive learning techniques.Computer vision techniques for human-robot interaction have been mainly focused on recognizing people in urban scenarios (Bellotto and Hu, 2009; Merino et al., 2012; Portmann et al., 2014) as well as identifying human gestures and activities (den Bergh et al., 2011; Nickel and Stiefelhagen, 2007) to establish contact with people and perform particular robotics tasks such as guiding people in museums and urban areas (Garrell and Sanfeliu, 2010; Rashed et al., 2015; Thrun, 2000), providing information in shopping malls (Gross et al., 2002), or recognizing human emotions through classifying facial gestures (Bartlett et al., 2003). Although, these techniques have endowed the robot with remarkable interaction skills, they are commonly computed offline and using a potentially large training time. As a result, the robot is limited to perform tasks only for which it has been trained previously, missing the opportunity to learn and improve its perception skills through the interaction with humans.Conversely, in this work we propose a very efficient interactive approach that combines human assistance and robot’s detection predictions so as to build object detectors which can be applied for a wide range of robotics tasks. The approach exploits the interplay between robots and humans in order to compute and improve progressively the robot’s perception capabilities.Despite showing impressive results, standard methods for object detection compute the classifiers using intensive and offline learning approaches applied to large annotated datasets (Felzenszwalb et al., 2010; Malisiewicz et al., 2011; Ozuysal et al., 2010; Villamizar et al., 2012a). Therefore, most of these offline approaches are not suitable for some particular applications requiring computing the classifier on the fly, either because the training data is obtained continuously, or the size of the training data is so large that it needs to be loaded progressively. To handle these situations, several online alternatives allowing to sequentially train the classifiers have been proposed (Avidan, 2007; Babenko et al., 2011; Grabner and Bischof, 2006; Hall and Perona, 2014; Santner et al., 2010).In this work, the classifier we use is based on an online random ferns formulation (Kalal et al., 2010; Krupka et al., 2014; Ozuysal et al., 2010; Villamizar et al., 2012b, 2015), which has been showing excellent results, both in terms of classification rates as computational efficiency. In Fig. 2, we show the overall schemes of the proposed interactive method and the online classifier. In essence, this classifier computes several sets of intensity-based pixel comparisons (see Fig. 2b) to build the randomized trees which are then used to estimate the posterior class probabilities.Most previous online versions are focused to single object modeling and tracking (Avidan, 2007; Babenko et al., 2011; Grabner and Bischof, 2006; Villamizar et al., 2012b). In order to learn multiple models we propose computing simultaneously and in parallel multiple classifiers, one for each object class (Fig. 2a), and with specific configurations like the spatial distribution of ferns or the particular object size (observe Fig. 2c). This also differs from other state of the art classifiers, that when applied to multiclass problems they require objects with constant aspect ratios and to know the number of object classes in advance (Torralba et al., 2007). In this respect, our method scales better for multiple objects since each object is learned by separated at the time in which the user selects a new object model in the video stream. This allows learning and detecting up to 30 object classes in an efficient and dynamic manner.Active learning techniques have been extensively used in computer vision to reduce the number of training samples that need to be annotated when building a classifier (Settles, 2010). Approaches such as “query by committee” (Abe and Mamitsuka, 1998; H.S. Seung and Sompolinsky, 1992), and “uncertainty-based sampling” (Lewis and Gale, 1994) close the learning loop using human assistance. In these works, the human user acts as an oracle that annotates/labels those samples that the classifier is not quite confident about their class prediction.In this paper, we propose an interactive learning strategy in which the robot plays a more active role, that is, the discriminative classifiers are built using a combination of the robot predictions with the human assistance (see Fig. 2f–h). Additionally, we also propose a methodology based on an adaptive uncertainty threshold that progressively reduces the amount of human assistance, making a more ”enjoyable” human-robot interaction. This is also another difference with respect to our own previous works (Garrell et al., 2013; Villamizar et al., 2012b). As it will be shown in the experimental section, using an adaptive threshold we can learn and detect several object instances without decrementing the intra-class classification rates.After having discussed the related work, we can summarize the main contributions of our approach as follows: (1) Proposing an online approach to learn and detect multiple object instances in images; (2) Designing an interactive learning strategy that incrementally improves the discrimination power of the classifiers using human assistance; (3) An adaptive learning scheme to reduce gradually the human interventions; and (4) A real-time implementation of the algorithm, which can cope with multiple objects at several frames per second.In this section, the main components of the proposed learning and detection strategy are described in more detail. Fig. 2 shows an schematic of how these elements are related.To perform online learning of object instances, we consider a scenario in which the classifiers are learned using a computer onboard a mobile robot, equipped with devices such as a keyboard, mouse, and a screen that enable the interaction with the human. We refer the reader to Fig. 1 for an illustration. Specifically, the keyboard and mouse are used to annotate the object of interest that the user wants to learn, and also to attend the robot in situations where the robot is not confident in its predictions. On the other hand, the touch screen is used to display the output of the detector and to show the performance of the detection system.In those situations where the robot is uncertain about its detection hypotheses, the robot will formulate to the user a set of concise questions, that expect for a ‘yes’ or ‘no’ answer. Table 1shows a few examples of such questions that are used to label the detection hypotheses and to update the classifiers with them. Note that the classifiers are computed only with difficult samples which require human assistance (i.e, active learning). In the experiments section, we will show that this strategy improves the classification performance using less human annotations.In order to make the human-robot interaction efficient and dynamic, the robot has been programmed with behaviors that avoid having large latency times (Table 1), specially when the human does not know exactly how to proceed. Strategies for approaching the person in a safe and social manner, or attracting people’s attention have been designed for this purpose (Feil-Seifer and Mataric, 2005; Garrell et al., 2013; Villamizar et al., 2012b; Wilkes et al., 1997).The proposed approach performs object detection by scanning a fixed-size sliding window over an input image I, observe Fig. 2d. At every image location, the classifier is tested over an image sample x (local image window defined by the object size Bu× Bv) and returns the probability that such window contains a particular object instance (Fig. 2e and f). The size of the object is defined at the time of selecting the object of interest by the user using the computer mouse. This scanning procedure is carried out over the entire image, and once it is finished non-maximal neighborhood suppression is applied to remove multiple overlapped detections. Observe that Fig. 2i shows just a single detection (green bounding box). Additionally, the scanning process is repeated for different window sizes so as to deal with scale changes.Next, we describe each of the main ingredients of the online classifier used for object detection: the computation of random ferns on pixel intensities, the shared pool of ferns parameters to reduce the computational complexity of the method, the building of the classifier and the process to update this classifier with new samples.We compute object classifiers using a particular version of the extremely randomized trees (Criminisi et al., 2011; Ozuysal et al., 2010; P. Geurts and Wehenkel, 2006), which are the so-called random ferns (Kalal et al., 2010; Krupka et al., 2014; Ozuysal et al., 2010; Villamizar et al., 2012b). Specifically, random ferns consist of sets of random and simple binary features computed over image pixel intensities Ozuysal et al. (2010). Fig. 2b shows for instance three different random ferns, each with three binary features (i.e., pairs of colored dots).More formally, each feature f corresponds to a Boolean comparison of intensity image values at two pixel locations a and b within a square subwindow s of size S × S where the fern is computed. Refer to Fig. 3for a descriptive example showing the computation of a random fern and its binary features. Then, a binary feature can be written as:(1)f(x;u,ω)=I(x(a)>x(b))whereI(e)is the indicator function11The indicator function is defined by:I(e)=1if e is true, and 0 otherwise., x is the image sample or window, u is the center position of the subwindow s inside the image window,ω={a,b}are two randomly chosen pixel locations, and x(a) indicates the intensity-pixel value at location a.Following the same spirit of the random ferns Ozuysal et al. (2010), we define a fern f as the combination of M different binary features in the image subwindow s. This can be formulated as,(2)f(x;u,ω)=[f(x;u,ω1),f(x;u,ω2),⋯,f(x;u,ωM)]whereω={ω1,…,ωM}is the set of parameters (pixel locations) for M different binary features.The fern output f(x; u, ω) is an M-dimensional binary vector, which at the implementation level, is represented by an integer valuez∈{1,…,2M}. Fig. 3c and d shows an example of how a fern is computed on a subwindow s centered at u. In this case,M=3intensity-based pixel comparisons are considered, with individual outputs 1,1,0. The combination of these Boolean features responses determines the fern outputz=(110)2+1=7.In order to compute efficiently multiple online object classifiers from scratch, we propose to use recursively a small set of fern features among all classifiers. By doing this, the computation of the fern features, which is the most computation costly part of the algorithm, is shared by all classifiers. This provides a remarkable speed up compared to when we train each classifier with a different subset of ferns, while classification rates are shown to remain high Villamizar et al. (2012a, 2010).To this end, a small set of R random ferns parameters,Ω={ω1,⋯,ωR},is computed in advance, such that each object classifier can then be computed as a combination of ferns evaluated at different image locations but sharing the same parameters. Again in Fig. 2c, we show an example where two different object classifiers are trained for two object instances. Note that each classifier is computed using five random ferns with onlyR=3features parameters, but with a specific spatial distribution and ferns probabilities which makes it discriminative for that object class.Since both classifiers use the same fern features parameters Ω, the feature computation is shared and done in advance to testing the object classifiers. This results in a U × V × R lookup table with a dense sampling of all possible fern responses, being U × V the size of the input image I. Consequently, the sharing strategy makes the overall computational cost to be just a function of the number of ferns parameters R (a typical value isR=10), and to be independent on the number of classifiers (K) and the amount of ferns in each classifier (J). In fact, since the complexity of the rest of computations involved in the test is negligible compared to the cost of these convolutions, we can roughly approximate aKJR-fold speed-up achieved by the sharing scheme Villamizar et al. (2010). With this, two classifiers withJ=200ferns andR=10distinct features parameters, yields speed-ups of up to 40×.The classifier Hk(x) for an object instance k is built by random sampling with replacement of J random ferns, from the shared pool Ω, computed at multiple image locations. The response of this classifier Hk(x) over the sample x is:(3)Hk(x)={+1ifconfk(x)>β−1otherwise,where confk(x) is the confidence of the classifier on predicting that x belongs to the object k, and β is a confidence threshold whose default value is 0.5. Thus, if the output of the classifier isH(x)=+1,the sample x is considered as an object or positive sample. Otherwise, this sample is assigned to the background or negative class.The confidence of the classifier is defined according to the following posterior:(4)confk(x)=p(y=+1|Fk(x),ηk),whereFk(x)={f(x;uj,ωj)}j=1J,withωj∈{ω1,⋯,ωR}anduj∈{u1,…,uL},makes reference to the set of J ferns for the classifier k,y={+1,−1}indicates the class label, andηkare parameters of the classifier. The set{u1,…,uL}corresponds to all possible 2D pixel coordinates within a window x where a fern can be tested. For each fern j, the binary features parametersωjand fern location ujare chosen at random and kept constant during the learning and run-time steps.In turn, the posterior probabilityp(y=+1|Fk(x),ηk)is computed by combining the posterior of the J ferns:(5)p(y=+1|Fk(x),ηk)=1J∑j=1Jp(y=+1|f(x;uj,ωj)=z,ηkj,z),where z is the fern output andηkj,zis the probability that the sample x belongs to the positive class in the k-th classifier with output z in the fern j. Since these posterior probabilities follow a Bernoulli distribution,p(y|f(x;uj,ωj)=z,ηkj,z)∼Ber(y|ηkj,z),we can write that(6)p(y=+1|f(x;uj,ωj)=z,ηkj,z)=ηkj,z.The parameters of these distributions are computed through a Maximum Likelihood Estimate (MLE) over the input samples and their corresponding labels, provided by the human user during the interaction with the robot. That is,(7)ηkj,z=Nk,+1j,zNk,+1j,z+Nk,−1j,zwhereNk,+1j,zis the number of positive (object) samples with output z for fern j. Similarly,Nk,−1j,zcorresponds to the number of negative samples for the fern j with output z in the classifier k.During the learning stage, once a sample has been labeled by the human user, this sample is used to recompute the probabilitiesηkj,zof Eq. (7), and update the online classifier. For instance, let us assume that a sample x is labeled asy=+1,and it activates the output z of the fern f(x; uj,ωj). We will then update the classifier by adding one unit to the z-th bin of the histogram ofNk,+1j,z. This process is repeated for all J ferns of the classifier k. With these new distributions, we can recompute the priorsηkj,zand update the classifier.The computation of the online classifier is summarized in Algorithm 1. Note that the first part of the algorithm (lines 1–3) corresponds to convolve the ferns, with the shared pool of fern features parameters Ω, on the input sample x. This process is done in advance to updating the classifier, and hence, it reduces drastically the computational cost since the size of Ω is much lower than the number of ferns (R < <J).The online learning strategy to train a specific classifier k is shown again in Fig. 2d. As mentioned before, the object detection is carried out using a sliding window approach Viola and Jones (2001), where the classifier Hk(x) is tested at every image location and multiple scales over an input image I. At each location, the image sample x is evaluated on all J ferns of the classifier to obtain the confidence confk(x) (Eq. (4)). Subsequently, the class label for this sample,y={+1,−1},is estimated according to the classifier response and the threshold β (Eq. (3)).To reduce the number of false positives and avoid drifting problems (produced when updating the classifier with erroneously labeled samples), frequent in non- and semi-supervised learning approaches Avidan (2007); Grabner and Bischof (2006), we use an uncertainty-based active learning strategy Lewis and Gale (1994); Settles (2010) that reduces gradually the amount of human assistance in combination with an adaptive uncertainty threshold. Active learning minimizes the risk of misclassification by updating the classifier only with samples which have been annotated/labeled by the user.Therefore, in situations where the classifier is not certain about the class estimate y, because the confidence over the sample x is ambiguous (close to the threshold β), the system opts for requiring the human help so as annotate the true class of the sample. This request q can be written as:(8)q(x)=I(β+θk/2>confk(x)>β−θk/2),where θkcorresponds to the uncertainty threshold for the classifier k. Fig. 4displays a clarifying example. If q(x) is true the system asks for human assistance. Otherwise, this sample is discarded and not used to update the classifier. Note that by doing this we are just feeding the classifier with labeled samples that are close to the decision boundary, improving thus, its discriminability power.With the aim of adapting the human assistance in accordance to the performance of the classifier, we define an adaptive threshold that depends on the incremental classification rate over the requested samples. That is,(9)θk=1−ξλk,where ξ is a sensitivity parameter assigned by the user, and λkmeasures the performance of the classifier k. In turn, this performance rate can be computed by(10)λk=Mkc/MkqbeingMkqandMkcthe numbers of requested samples and correctly classified samples, respectively. A sample x is correctly classified when the class label y coming from the classifier agrees with the true class label given by the user.Algorithm 2shows the proposed interactive learning approach to compute online classifiers with small human supervision. Observe that each classifier is only computed with difficult samples falling close to the decision boundary β and which require human assistance.

@&#CONCLUSIONS@&#
In this work, we have presented a novel approach for human-robot interaction to interactively learn the appearance model of multiple objects in real time using scanty human supervision. The proposed method uses efficient and reliable random trees classifiers to compute object detectors on the fly and which are progressively refined with the human assistance. The proposed method also includes an uncertainty-based active learning strategy that reduces the amount of human intervention while it maintains high recognition rates. The method has been evaluated extensively in both synthetic and real-life different scenarios such as 2D classification, face recognition, and the learning and detection of contextual objects in urban settings using an autonomous mobile robot.