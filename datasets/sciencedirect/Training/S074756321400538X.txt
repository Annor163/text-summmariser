@&#MAIN-TITLE@&#
Carrot and stick 2.0: The benefits of natural and motivational prosody in computer-assisted learning

@&#HIGHLIGHTS@&#
Users of tutorial systems are affected by auditory modulations in system feedback.Users learn faster with naturally spoken than with synthesized computer feedback.Users’ learning rates benefit from system feedback with motivational prosody.

@&#KEYPHRASES@&#
Computer-supported learning,Speech-based communication,Feedback,Motivation,Praise and blame,Prosody,

@&#ABSTRACT@&#
For acquiring new skills or knowledge, contemporary learners frequently rely on the help of educational technologies supplementing human teachers as a learning aid. In the interaction with such systems, speech-based communication between the human user and the technical system has increasingly gained importance. Since spoken computer output can take on a variety of forms depending on the method of speech generation and the employment of prosodic modulations, the effects of such auditory variations on the user’s learning achievement require systematic investigation. The experiment reported here examined the specific effects of speech generation method and prosody of spoken system feedback in a computer-supported learning environment, and may serve as validational tool for future investigations of spoken computer feedback effects on learning. Learning performance in a basic cognitive task was compared between users receiving pre-recorded, naturally spoken system feedback with neutral prosody, pre-recorded feedback with motivating (praising or blaming) prosody, or computer-synthesized feedback. The observed results provide empirical evidence that users of technical tutoring systems benefit from pre-recorded, naturally spoken feedback, and do even more so from feedback with motivational prosodic modulations matching their performance success. Theoretical implications and considerations for future implementations of spoken feedback in computer-based educational systems are discussed.

@&#INTRODUCTION@&#
Today’s learning is not anymore restricted to the classroom where human students are instructed by human teachers. Learners frequently employ educational technologies like e-learning platforms, smartphone applications or console games to improve their skills and further their knowledge. Tools like computer-assisted instruction and intelligent tutoring systems (e.g., Cognitive Tutor,1http://www.carnegielearning.com/secondary-solutions/cognitive-tutor/.1Andes,2http://www.andestutor.org/.2or AutoTutor3http://www.autotutor.org/.3) have been specifically developed to simulate human teachers’ and tutors’ behavior and support learners in reaching their study objectives by giving targeted assistance and adaptive feedback customized to their users’ individual knowledge and performance (Larkin & Chabay, 1992; Anderson, Corbett, Koedinger, & Pelletier, 1995; Bayraktar, 2001; Shute & Towle, 2003; Woolf, 2010; Graesser, Conley, & Olney, 2012).In the interaction with such educational technologies, like in any interaction between human users and technical systems, a multitude of dialogue forms can occur, ranging from simple commands to information retrieval dyads to elaborate ‘conversations’ (Allen et al., 2001). Besides the content of the system’s contributions to such dialogues, an important issue to consider is the form in which the system output is generated. In principle, a tutoring system’s contributions to the dialogue can be transported in a variety of ways. They can be presented through visual as well as auditory channels, and in both modalities verbal (written text or speech; e.g., AutoTutor) and non-verbal output (symbols, colors, or tones; e.g., Andes) are possible. In certain (non-tutorial) applications, even tactile feedback has been proven useful (Akamatsu, Mackenzie, & Hasbroucq, 1995). However, due to its closeness to the predominant inter-human dialogue form and based on the ever increasing demand for hands-free and eyes-free interfaces, there is a continuing trend towards speech-based system interfaces, regarding both user input and computer output (Cohen & Oviatt, 1995; Nass & Gong, 2000; Allen et al., 2001; Graesser, VanLehn, Rosé, Jordan, & Harter, 2001; Nass & Brave, 2005).When considering this development, it becomes crucial to take a closer look at the potential effects of auditory variations in such ‘spoken’ system output. Especially for developers of computer-based tutoring systems it is highly relevant to know not only with which words the technical tutor should respond to the learner’s input, but also how the speaking system’s voice should sound to optimally support the user’s learning progress. Therefore, besides the content of the feedback given by the program, it is essential to empirically validate the effects of the way in which the feedback is spoken on users’ learning performance.One highly important aspect of speech is its prosody, i.e., the rhythm, stress, and intonation of the produced utterances. In human-to-human interaction, prosody is employed as a linguistic means serving a variety of purposes. Prosodic variations are employed by human speakers to separate the speech stream into structural units (syntactic prosody), but also to express emotions and intentions (emotional prosody). Human recipients then use these prosodic cues to analyze the syntactic structure of the sentence (parsing) and to assess the intentions and feelings of the speaker. In an educational context, teachers may employ emotional prosody to underline evaluative feedback and motivating comments which are part of the standard initiation–response–feedback (IRF) exchange (cf. Sinclair & Coulthard, 1975).While in the last few decades a substantial amount of research has been conducted with regard to the production and processing of syntactic as well as emotional prosody in the interaction between humans (see, e.g., Frick, 1985; Price, Ostendorf, Shattuck-Hufnagel, & Fong, 1991; Baum & Pell, 1999; Scherer, 2003; Friederici & Alter, 2004; Wildgruber, Ackermann, Kreifelts, & Ethofer, 2006; Wolff, Schlesewsky, Hirotani, & Bornkessel-Schlesewsky, 2008), little effort has been directed towards enlightening the role of prosody in the context of human–computer interaction, especially regarding the effects of emotional prosody produced by the computer. Available data from this field rather focused on the recognition and classification of human prosody by technical systems (cf. Cowie et al., 2001; Schuller, Rigoll, & Lang, 2003; El Ayadi, Kamel, & Karray, 2011) or on the problem of how to simulate prosodic variations in synthesized speech (e.g., Murray & Arnott, 1993; Schröder, 2001, 2009; Burkhardt & Stegmann, 2009), but in how far prosodic variations employed by a technical system might influence a human user attempting to interact with the system remains a largely open question. Even though several basic emotions are now prosodically implemented in various speech synthesizing programs (for a continuously updated overview, see Burkhardt, 2014), thereby rendering an examination of their impact feasible, controlled empirical studies in this regard are still pending.Computer-assisted learning can be expected to be particularly susceptible to the impact of prosodic variations in the system output, since it constitutes an environment in which feedback given by the technical system plays an important role for the user’s learning progress and the user is thus effectively dependent on the system feedback. Therefore, it is especially interesting to examine whether prosodic modulations in system feedback can enhance users’ learning success in the task they are trying to complete with the system’s help.In the context of learning, it is opportune to select prosodic variations that are known to have a motivating effect in human learning, since—besides other factors like intelligence, task-specific skills, or attention—the learner’s motivation has long been known to have a substantial effect on learning success (Stipek, 2001; Pintrich, 2003), especially in situations where learning is self-regulated (Zimmerman & Schunk, 2012, chap. 1) and computer-assisted (Song & Keller, 2001).An individual’s motivation for a particular behavior is partially based on dispositional sources which are relatively stable and unaffected by external factors, e.g., the individual’s need for achievement (Murray, 1938; McClelland, Atkinson, & Clark, 1953; McClelland, 1985; Thrash, Elliot, & Schultheiss, 2007), esteem needs (Maslow, 1954), need for competence (Deci & Ryan, 1985, 2002), causal attributions for success and failure (Weiner, 1979, 1985, 2005, chap. 5), or goal orientation (Dweck, 1986; Ames & Archer, 1988; Elliot, 2005, chap. 4), and partially stems from the intrinsic interest and pleasure in the behavior itself (e.g., Csikszentmihalyi, 1975; Renninger, 2000). However, the desired behavior can also be extrinsically motivated by the prospect of desirable outcomes (Ryan & Deci, 2000a, 2000b).In educational initiation–response–feedback exchanges, praise and blame are frequently consulted for this purpose: Here, positive feedback (i.e., informative feedback following a correct response) is often accompanied by praising comments, while negative feedback (i.e., feedback following an incorrect response) often entails blame or criticism in addition to its informational content. It is noteworthy that there may also be paradoxical effects of praise and blame on students’ motivation; however, these appear to be limited to specific circumstances, like excessive praise for success in a task perceived as very easy, or in a task that other students did not receive praise for (e.g., Meyer, 1992; Miller & Hom, 1996; Kaspar & Stelz, 2013). Similarly, negative effects of praise on students’ achievement have been reported in cases where praise has been administered unsystematically, i.e., independent of the correctness of the students’ responses (Brophy, 1981). If, on the other hand, praise and blame are employed in a contingent fashion focusing on the student’s individual mastery of the task, they can be considered effective tools of operant conditioning (cf. Skinner, 1953; see also O’Leary & O’Leary, 1977; Brophy, 1981; Henderlong & Lepper, 2002; Pintrich, 2003; Hattie & Timperley, 2007). Couched in this framework, praise (or more generally, positive reinforcement) is employed to enhance a desired behavior (i.e., correct responses), while blame (or more generally, punishment) serves to reduce the occurrence of undesired (i.e., incorrect) behavioral responses.At this point, it is useful to consider how the assumed effects of praise and blame may translate to feedback given by a technical system. Since there is ample evidence that users tend to treat computers like human beings – attributing them with emotions and intentions even though they are fully aware that computers are not human (i.e., the CASA—Computers Are Social Actors—Paradigm; Reeves & Nass, 1996; Nass & Moon, 2000; Lee & Nass, 2010, chap. 1), it is reasonable to assume that such conditioning mechanisms may have similar effects if employed by a technical system. Following the effective usage of praise and blame in human education, it therefore appears suitable to examine the implementation of praising and blaming prosody into the feedback given by a technical tutoring system. If computer-assisted learning is in fact susceptible to the usage of prosodic praise and blame, we should thus be able to observe an improved learning performance with prosodically motivational feedback in comparison to prosodically neutral feedback.On a semantic level (i.e., regarding the textual content of the utterances, not their prosody), at least the effects of computer-generated praise have been examined before (Fogg & Nass, 1997). While the authors did not report any effects on actual task performance, they showed that (written) praise given by a computer can have beneficial effects on the users’ subjectively perceived performance, their mood, and their evaluation of the computer. Since this was the case with “sincere” praise (i.e., praise described to participants as contingent upon correct responses) as well as with “flattery” (i.e., praise described as independent of response correctness), the authors suggested that “computers should praise people frequently—even when there may be little basis for the evaluation” (Fogg & Nass, 1997, p. 559). Similarly, Mumm and Mutlu (2011) observed an increase in self-reported motivation and willingness to continue with the task when written praise was given irrespective of participants’ actual task performance (speed in a letter-counting task).However, while such non-contingent praising feedback may improve psychological parameters such as the user’s affect and feelings of self-efficacy (cf. Bandura, 1977, 2012), the question of potential effects on actual task performance remains unanswered. It is conceivable that, in a computer-supported learning environment, unconditional praise should not be able to enhance actual performance since—not being contingently combined with informative feedback on performance—it cannot be effectively capitalized on for learning (see above; Brophy, 1981; Henderlong & Lepper, 2002; Pintrich, 2003; Hattie & Timperley, 2007). By comparison, feedback that is semantically contingent with respect to the correctness of the user’s input, i.e., positive feedback (e.g., “yes”) for correct responses and negative feedback (e.g., “no”) for incorrect responses, provides a dependable information source for learners to rely on, and it can be experimentally combined with the application of either neutral or praising/blaming prosody to examine the motivational effects of those variations on actual user performance. Consequently, this approach was chosen for the experiment presented here.A second point of interest regarding the optimization of spoken feedback in speech-based tutorial systems addresses the question in how far user performance may be affected not only by motivational prosody but also by the employed means of feedback generation. Here, one needs to differentiate between utterances that are pre-recorded from a human speaker and later played back by the system and utterances that are computer-generated by means of speech synthesizing software like Mary4http://mary.dfki.de/.4or Festival5http://www.cstr.ed.ac.uk/projects/festival/.5. While it is certainly intuitive for software developers to aspire toward increasingly natural-sounding levels of speech synthesis, it is not at all clear whether—beyond long settled initial issues of intelligibility—there is an actual advantage of natural-sounding speech over synthetic-sounding speech, e.g., in the form of user performance benefits.In a previous experiment by Nass, Foehr, and Somoza (2001), happy and sad stories that were presented in synthesized speech were rated as more likeable and more credible than the same stories presented in pre-recorded natural speech. However, from these results it is not possible to derive how the usage of synthesized vs. natural speech may affect the user’s performance in a learning task. Furthermore, Nass et al. (2001) used (natural and synthesized) sad and happy voices that were matched or mismatched with the sad and happy stories, and the authors acknowledge that the synthesized speech may have been perceived as less emotional, so the differences observed between synthesized and recorded speech might in fact rather be based on differences in emotionality/neutrality than on the speech modality per se. To investigate the effects of speech generation type in isolation, a comparison of prosodically neutral, non-emotional utterances in natural vs. synthesized speech would therefore constitute a valuable addition to the data available so far.While the predictions regarding the direction of a potential effect of motivational prosody are based on a vast literature on reinforcement learning and are therefore rather straightforward (i.e., if there are any prosodic influences, we expect a better learning performance with motivational than with neutral prosody, see Section 1.1), forming a directed hypothesis for the effects of pre-recorded vs. synthesized feedback on a user’s learning performance is more difficult. Important experimental differences notwithstanding, the results reported by Nass et al. (2001; see above) may suggest an advantage for computer-supported learning employing synthesized feedback. On the other hand, the CASA paradigm (Reeves & Nass, 1996; Nass & Moon, 2000; see Section 1.1) tells us that computers are perceived as social actors and are attributed with human properties by their users. As a consequence, users tend to prefer in computers what they prefer in human interaction partners. One such preference that has been shown to occur in the interaction with machines as well as with humans is the preference for similarity (social identification; Tajfel & Turner, 1979). For instance, users tend to prefer working with systems using same-gender voices (Lee, Nass, & Brave, 2000) and with systems displaying a similar “personality” to their own (Nass & Lee, 2001). Should this preference for more user-like machines translate into a better learning performance when interacting with a tutoring system, one could assume a beneficial effect of system feedback if it sounds more human (and therefore user-like), i.e., if it is pre-recorded instead of computer-synthesized. Then again—though this seems rather unlikely considering the abundance of natural speech-based appliances in today’s environment—one should not neglect the possibility of contrary effects comparable to the ‘uncanny valley’ effects in robotics and 3D computer animations, which refer to results showing that robots and avatars resembling humans are only accepted up to a certain degree of humanness and are rejected should they appear too human—until they are not at all recognizable as technical systems anymore and therefore accepted as humans (Mori, 1970; MacDorman & Ishiguro, 2006; Moore, 2012; for an equivalent rejection of “partially human” auditory computer interfaces, see Huang, Lee, Nass, Paik, & Swartz, 2000). In sum, the comparison of naturally spoken vs. synthesized machine feedback is at this point best characterized as an exploratory endeavor not warranting one-sided predictions in either direction.To examine the effects of feedback generation mode (pre-recorded vs. synthesized) and prosody (neutral vs. motivational), a simple computer-controlled experiment was conducted in which participants were required to learn the categorization of sounds according to certain acoustic features. More precisely, participants were presented with frequency modulated tones of varying duration, intensity, frequency range, modulation direction, and modulation speed, and were asked to indicate after listening to each sound whether it constituted a “target” or a “non-target” sound. In the beginning of the experiment, the relevant stimulus properties for this categorization (in this experiment, duration and modulation direction; e.g., short/rising sounds as targets) were not known to the participants and therefore had to be inferred from the system feedback. To this avail, contingent positive or negative feedback was presented auditorily following each participant response. One of the experimental groups received this feedback as computer-synthesized speech with a neutral prosody (Group SYNTH), one as pre-recorded natural speech with a neutral prosody (Group NEUT), and one as pre-recorded natural speech with a motivational (i.e., praising or blaming) prosody (Group MOTI).6Note that, while several types of emotional prosody have been implemented by current speech synthesizing software (see Section 1.1), these are to date mostly limited to major emotional categories like joy, anger or sadness, and do not include such specific prosodies as praise or blame. Due to this present lack of synthesized speech with motivational prosodies, it is currently not possible to examine the questions at hand in a 2×2 design completely crossing the factors generation method (pre-recorded vs. synthesized) and prosody (neutral vs. motivational).6We analyzed data collected from 46 participants (23 female) aged 19–35years (mean age 25.9years) with normal hearing. The data from a further eleven participants (five SYNTH, two NEUT, four MOTI) were excluded from the statistical analysis based on the results of a post-experimental questionnaire which showed that they had failed to learn the required categorization correctly.The experimental stimuli consisted of 240 different frequency-modulated (FM) tones. The relevant stimulus properties for the discrimination task were duration (400 vs. 800ms) and direction of frequency modulation (rising vs. falling), resulting in four basic categories of tones: short/rising, short/falling, long/rising, and long/falling. For each participant one of these categories was the target category, while the other three categories were non-targets (counter-balanced across participants). In addition, the tones also differed in intensity (low intensity tones ranging from 76 to 81dB vs. high intensity tones ranging from 86 to 91dB), speed of modulation (0.25 vs. 0.5 octaves per second), and frequency range (low frequency tones ranging from 500 to 831Hz vs. high frequency tones ranging from 1630 to 2639Hz).The feedback stimuli consisted of four positive and four negative feedbacks as well as one time-out feedback, all spoken in standard German (positive: ja, “yes”; richtig, “right”; ja, richtig, “yes, right”; stimmt, “correct”; negative: nein, “no”; falsch, “wrong”; nein, falsch, “no, wrong”; stimmt nicht, “not correct”; time-out: zu spät, “too late”).Each participant received these feedbacks in one of three different types of prosody, depending on experimental group. Group NEUT(n=16)received neutrally spoken natural feedback, Group MOTI(n=16)received motivationally spoken natural feedback (with a praising prosody for positive feedback and a blaming prosody for negative feedback), and Group SYNTH(n=14)received neutrally spoken computer-synthesized feedback.Both types of naturally spoken feedback (Groups NEUT and MOTI) were digitally recorded from a female German professional speaker. The recordings were taken from the evaluated motivational feedback corpus MOTI (Wolff & Brechmann, 2012) which includes neutral, praising, blaming, and sympathetic prosodies for the evaluative feedback utterances employed here (i.e., ja, richtig, etc.; see above) as well as for six pseudowords (words without semantic content) and was evaluated by 24 German native speakers (12 female) aged 20–35years. For the feedback stimuli employed here, recognition rates were 78%, 97%, and 94% for the neutral, praising, and blaming prosody, respectively.The synthetic feedback was computer-generated with the Mary text-to-speech synthesizing system (version 3.6.0; German Research Center for Artificial Intelligence, Saarbrücken, Germany; see Footnote 4), employing the neutral female voice profile mbrola-de-5. It needs to be mentioned that, due to an error in the stimulus construction process, the experimental timing was not identical in all three conditions: While all audio files of naturally spoken feedbacks contained the respective recording preceded and followed by 20ms of silence, the files of synthesized feedbacks contained an average silence of 307ms (SE 26ms) before and 327ms (SE 36ms) after the core of the utterance. While a replication of the experiment with equally timed stimulus presentation is certainly warranted, the data obtained with the utilized stimulus materials were still considered viable for analysis for several reasons. First, although such a difference is above the threshold for just noticeable computer feedback delays when subjects are instructed to attend to seldom delays in feedback (Miller, 1968; Kohrs, Angenstein, Scheich, & Brechmann, 2010), the human–computer dialogue is only disturbed if such a delay is unexpected (Shneiderman & Plaisant, 2005). This was clearly not the case here since the timing was consistent throughout the run of the experiment for each of the experimental groups. Second, the divergent timing in Group SYNTH does not affect any of the conceivable effects of motivational prosody (Group MOTI vs. Group NEUT), thus rendering the respective comparison fully interpretable.Table 1shows the acoustic properties of the synthesized and recorded feedback stimuli (fundamental frequency mean and range, intensity, and duration of the core utterances).

@&#CONCLUSIONS@&#
