@&#MAIN-TITLE@&#
A hyper-heuristic based framework for dynamic optimization problems

@&#HIGHLIGHTS@&#
We propose a novel hybrid strategy for applicability of hyper-heuristic techniques on dynamic environments.Performance of our method is validated with the dynamic generalized assignment problem and the moving peaks benchmark.Our approach outperforms the related work for various problem instances with respect to quality of solutions.

@&#KEYPHRASES@&#
Dynamic optimization problems,Hyper-heuristics,Generalized assignment problem,Moving peaks benchmark,Memory search technique,

@&#ABSTRACT@&#
Most of the real world problems have dynamic characteristics, where one or more elements of the underlying model for a given problem including the objective, constraints or even environmental parameters may change over time. Hyper-heuristics are problem-independent meta-heuristic techniques that are automating the process of selecting and generating multiple low-level heuristics to solve static combinatorial optimization problems. In this paper, we present a novel hybrid strategy for applicability of hyper-heuristic techniques on dynamic environments by integrating them with the memory/search algorithm. The memory/search algorithm is an important evolutionary technique that have applied on various dynamic optimization problems. We validate performance of our method by considering both the dynamic generalized assignment problem and the moving peaks benchmark. The former problem is extended from the generalized assignment problem by changing resource consumptions, capacity constraints and costs of jobs over time; and the latter one is a well-known synthetic problem that generates and updates a multidimensional landscape consisting of several peaks. Experimental evaluation performed on various instances of the given two problems validates that our hyper-heuristic integrated framework significantly outperforms the memory/search algorithm.

@&#INTRODUCTION@&#
Hyper-heuristics are meta-heuristic techniques for automating the process of selecting, combining, generating multiple simple low-level heuristics to solve hard combinatorial optimization problems [1,2]. Basically, a hyper-heuristic repeatedly chooses an appropriate low-level heuristic to apply without using problem specific information until a stopping criterion is satisfied [3,4]. Therefore, hyper-heuristics can be thought as a black box, which takes a problem instance and a set of low-level heuristics as input [4]. Hyper-heuristics target to find optimal or quasi-optimal solutions for the problem addressed in the search space of heuristics rather than search space of solutions. They have been applied for solving a large number of static combinatorial optimization problems. A list of applications from different domains is already presented in a recent survey [5].In a stationary optimization problem, the fitness landscape does not change during the course of the optimization and the main goal is to find an optimal solution for the given problem. Many real world problems in different domains including telecommunications, transportation, finance and information society may change over time. Shortest path routing in mobile ad hoc networks (MANETs) is an example problem where network topology changes over time [6]. Dynamic multicast routing problem in MANETs is another dynamic optimization problem for the same domain [7]. Online arrival of customer requests during an operation is the most common source of dynamism for dynamic vehicle routing problems in transportation and logistics [8].A dynamic optimization problem is the one in which one or more elements of the underlying model including the objective function, the problem constraints, the decision variables or even the environmental parameters may change in time [9]. The main motivation in a dynamic optimization problem is to track the global optimum value, since the fitness landscape may change over time.Dynamic optimization problems can be broadly categorized in two classes, synthetic problems and real world applications [10]. Since the degree of dynamism and the complexity of the objective function is viewable and controllable for synthetic problems, they are widely used in the literature. Dynamic Onemax function [11], Dynamic Deceptive functions [12], Dynamic Royal-Road function [12] Moving peaks benchmark (MPB) [9] and dynamic multidimensional knapsack problems [13,14] are the example synthetic problems used in the literature. On the other hand, real-world problems demonstrating dynamic behavior emerge during last few years. Aerospace design [15], pollution control [16], car distribution system for “off-lease” cars [16] and wireless sensor networks [17] are examples of different domains which have problems showing dynamic behavior.In this study, we present a novel hybrid technique by integrating various hyper-heuristic techniques with the memory/search algorithm. To the best of our knowledge, this work is one of the first attempts of providing hyper-heuristic based solutions for dynamic optimization problems. The memory/search algorithm [18] is an important and widely used technique that addresses various dynamic optimization problems on different domains. In order to validate the effectiveness of our proposed solution, we conduct experiments with two different problems. The generalized assignment problem (GAP) is an NP-complete problem that assigns each job to exactly one agent in order to minimize the total cost without exceeding each agent's resource capacity [19]. Dynamic instances of the GAP (i.e. the instances of the DGAP) are generated by changing the costs of jobs, resource consumption of jobs and capacity constraints of agents. On the other hand, moving peaks benchmark (MPB) is a widely-used benchmark that generates a multidimensional landscape consisting of several peaks with changing the width, the height and the position [9].Experimental study on the given two problems with various problem instances are conducted in order to present effectiveness of our hybrid approach. The results clearly show that our proposed approach is very successful for tracking the global optima. Five different problem types by considering each of which with six cases generates a total of 30 problem instances for the DGAP problem. Our hybrid approach outperforms the reference study for all 30 instances by delivering up to 13% improvement. Amount of improvement of our approach reaches up to 20% for the moving peaks benchmark.The rest of this paper is structured as follows. Section2 gives a brief information on the hyper-heuristic approaches. The details of our methods that hybridize the memory/search algorithm with the hyper-heuristics are presented in Section3. Section4 presents the dynamic optimization problems considered in this paper. Section5 and Section6 present the details of our experimental evaluation for the dynamic generalized assignment problem and the moving peaks benchmark, respectively. Finally, Section7 summarizes our main conclusions.Meta-heuristic techniques utilized for a particular problem domain may not be directly applicable to another problem domain or even new instances of the problem that has been already solved. The development and deployment process requires sufficient knowledge on both the problem domain and the meta-heuristic techniques. Additionally, tailor-made metaheuristics require fine tuning of a large set of parameters and algorithmic choices [5].Hyper-heuristic techniques are emerging fast and generic methods that work at a higher level of abstraction in order to avoid those drawbacks of metaheuristics [2]. They require only a set of low-level heuristics and an objective function to be maximized or minimized. They select and adapt several low-level heuristics during the search for the given combinatorial optimization problem. Hyper-heuristics operate on search space of heuristics rather than search space of solutions. They have been successfully applied to many stationary problems including personnel scheduling, timetabling, cutting stock and inventory problems [2–5,19,20].Hyper-heuristics can be considered into two main categories: heuristic selection, and heuristic generation[1]. Based on the first category, hyper-heuristics choose and select existing heuristics, where they can be called as “heuristics to choose heuristics” [21]. The second category includes hyper-heuristics that can generate new heuristics by using components of existing heuristics.When the nature of low-level heuristics is concerned, hyper-heuristics can be classified into constructive hyper-heuristics and perturbative hyper-heuristics[1]. Constructive hyper-heuristics apply low-level heuristics to build a complete solution gradually from an empty initial solution; while perturbative hyper-heuristics improve quality of a candidate solution through selecting and applying heuristics. In this paper, we consider only perturbative hyper-heuristics; and hyper-heuristics term refers to perturbative hyper-heuristics for the remaining part of the paper.Two major components of a perturbative hyper-heuristic technique are the heuristic selection mechanism and the criteria for move acceptance[22]. Although there are a large number of heuristic selection mechanisms proposed in the literature, we consider five of them in this study, which are the Simple Random (SR), the Random Descent (RD), the Random Permutation (RP), the Random Permutation Descent (RPD), and the Choice Function[2,4].A Simple Random (SR) heuristic selection method chooses a low-level heuristic randomly based on a uniform distribution and the selected heuristic is applied once. The Random Descent (RD) approach is a variant of the SR, where each selected low level heuristic is applied continuously until there is no improvement in the solution. The Random Permutation (RP) approach generates a random permutation of all low-level heuristics; and each low-level heuristic is applied once in predefined order cyclically. The Random Permutation Descent (RPD) approach is a variant of the RP, where each low level heuristic is applied continuously until there is no improvement.The choice function approach [2,4] ranks the low-level heuristics based on a overall score (Eq. (4)), which is a combination of recent improvements of each low-level heuristic (Eq. (1)), recent improvements for consecutive pairs of low-level heuristics (Eq. (2)) and the amount of time elapsed since the low-level heuristic was last called (Eq. (3)). Individual performance of a low-level heuristic Njis defined with the following equation,(1)f1(Nj)=∑nαn−1×In(Nj)Tn(Nj)where In(Nj) and Tn(Nj) are the change in the objective function and the amount CPU time used in the nth last time heuristic was called, respectively.Collective performance of a low-level heuristic Njis defined with the following equation,(2)f2(Nk,Nj)=∑nβn−1×In(Nk,Nj)Tn(Nk,Nj)where In(Nk, Nj) and Tn(Nk, Nj) are the change in the objective function and the amount CPU time used in the nth last time heuristic was called immediately after the low-level heuristic Nk, respectively. The parameters α and β at the above equations are set between 0 and 1, which show greater importance attached to the recent performance.In order to explore other regions of the search space and diversification, a low-level heuristic which has not been recently applied can be selected and employed. Therefore, the last part of the choice function for each low-level heuristic Njis defined with the following equation,(3)f3(Nj)=τ(Nj)where τ(Nj) is the amount of CPU seconds which have elapsed since the low-level heuristic Njwas last called. The parameter δ is set between 0 and 1.The resulting choice function of a low-level heuristic Njis given in Eq. (4) by assuming that Njhas been called after Nk. It should be noted that the parameters α, β, δ are updated by using an adaptive procedure defined in [3].(4)f(Nj)=αf1(Nj)+βf2(Nk,Nj)+δf3(Nj)The second component of a hyper-heuristic is the move acceptance mechanism, where all moves (AM), only improvements (OI), and improving and equal (IE) are examples of simple acceptance mechanisms presented in [2,4]. The AM case accepts all moves, the OI case accepts only improving moves, and the IE move acceptance mechanism rejects only the worsening moves. There are also sophisticated ones including Great Deluge [23], Simulated Annealing method as a move acceptance [24], and Monte Carlo method [25,26]. The Great Deluge method accepts a non-improving move (frequently at the early stages and occasionally at the end), if it is better than a dynamically changing threshold value, which depends on current time and total time of the experiment. On the other hand, Monte Carlo (MC) method accepts all improving solutions and it accepts non-improving ones with a dynamically changing probability function adapted from the classical simulated annealing function without reheating. In this study, we consider the AM, the OI and the MC cases in our hybrid solutions, where the AM, the OI are the deterministic ones and the MC case is the non-deterministic one.In a dynamic optimization problem (DOP), the objective function, problem formulation, one ore more constraints or even a part of the problem change over time, where those changes may generate a change in the objective value. Specifically, a non-optimum or sub-optimum solution may be an optimum solution after a change. Therefore, a good optimization method for a given dynamic problem targets to track the global optimum by adapting itself to the changes.Dynamic optimization problems have different characteristics of changes. Some of these characteristics are frequency of change, severity of change, predictability of change and cycle length/cycle accuracy[9]. Frequency of change is a property that defines how often an environment changes. Severity of change is a property that defines the strength of changes in an environment. Predictability of change is a measure of correlation between consecutive changes. Finally, cycle length/cycle accuracy is a property that defines whether the optimum returns to a previous location or it gets a close location.Evolutionary approaches for dynamic optimization problems are classified into four categories [27,28]. The algorithms in the first category target to increase the diversity after a change. Hyper-mutation [9] and Variable Local Search (VLS) [29] are two examples of this category. In VLS, the mutation rate is increased gradually after a change. The population are kept after a change and the mutation rate is increased drastically in hyper mutation technique.The algorithms in the second category target to maintain diversity throughout the run. An example technique of this category is Random Immigrants (RI) [30]. This approach replaces some individuals in the population with randomly generated individuals, at each generation. The third category includes the memory-based approaches, where evolutionary algorithms are enhanced with some sorts of memory which can be either implicit or explicit, where the good individuals are stored in the memory and reused later. Memory/search algorithm[9,18] is an example of this category, which is explained in detail at the following subsection.The last category includes the multi-population approaches, where Self-Organizing Scouts (SOS) [9] and Multinational Genetic Algorithm [31] are two examples of this category. In SOS, the population is divided into one base population and many scout populations. While scout population exploits small search space, the base population continues to explore the new search space [9]. In addition to the four categories, a new category represents hybridization and combination of the leading methods. Hybridizing the Self-Organizing Scouts with a local search technique provided by the crossover hill-climbing operator, and the combination of Multinational GA with the Random Immigrants method are the two examples of this category [32].Memory-based techniques store good or partial solutions from the current environment, which can be used later in a new environment. They can either support implicit memory through redundant representations (such as a diploid genome) or explicit memory in terms of extra storage space with its own storing and retrieving policies [18]. The memory/search algorithm[9,18] is a memory-based technique which divides a population of individuals into two sub-populations each having n individuals: a memory population and a search population. In addition to both a memory population and a search population, a separate explicit memory of k individuals is also considered, which is empty at the beginning of the algorithm.In the memory/search algorithm, the next generation of memory population and search population is equivalent to a single generation of standard evolutionary algorithm. While the memory population exploits the memory and it maintains minimum jumps, the search population explores new areas of the search space and it submits new peaks to the memory [18]. It should be noted that search population does not retrieve any information from memory.The memory/search algorithm starts with an empty memory. The best individual selected from the memory population and the search population is stored into the memory at every x generation, where x is a predefined number that represents the memory update frequency. When the memory is full, the best individual is replaced with another one according to mindist (minimum distance) replacement strategy [18]. This strategy determines two individuals (i and j) in the memory that has the minimum distance between each other and it selects the less fit individual (say j). The new individual is replaced with j in the memory, if it is better than j.Search population is evolved as the memory population. Whenever a change in the environment occurs, the search population is re-initialized in order to increase the diversity of individuals. In case of a change, the memory population and the explicit memory are merged which provides a total of n+k individuals. The best n individuals among them constitute the memory population; and the explicit memory remains unaffected. The individuals in both populations are reevaluated when the environment is changed.In this paper, we present a novel strategy by integrating hyper-heuristic techniques into the memory/search algorithm. The hyper-heuristic approaches are utilized in order to construct the next generation of the search population. Since a hyper-heuristic has a set of low-level heuristics with different characteristics, the diversity in a hyper-heuristic based search population may become very high. Therefore, in our strategy, the search population is not re-initialized after a change in the environment; it is evolved based on the selected hyper-heuristic strategy, instead.The pseudo-code of the our hyper-heuristic based hybrid algorithm is presented in Fig. 1. Although this figure provides an outline of the algorithm, there can be problem-specific extensions or modifications. Specifically, the initial population generation phase of our solution may differ based on the problem considered. For the dynamic generalized assignment problem (DGAP), 90% of both the memory and the search populations are initialized randomly and the remaining 10% of the both populations are initialized by using the constraint-ratio heuristic (CRH) [33]. The CRH technique provides more meaningful solutions for the generalized assignment problem and it takes the capacity constraints of the agents into account in order to find a feasible solution; therefore it is considered for generating part of each initial population. On the other hand, the initial population is generated randomly for our second problem, the moving peaks benchmark.Another extension of our algorithm for the DGAP case is performed at each memory update generation. After the best member/solution of the generation is determined, it is refined by using the heuristic improvement algorithm proposed by Chu and Beasley [34]. Their algorithm attempts to repair the best solution which may violate capacity constraints. It also targets to reduce the total cost of the best solution. This refinement process is the Step 19 of the algorithm given in Fig. 1. Problem-specific extensions presented in our hybrid solution provide feasible results for all problem instances of the DGAP. The other details including crossover and mutation operators are presented as part of the experimental study given at Sections5 and 6.One of the five different heuristic selection mechanisms is applied for the search population (given at Step 14 of Fig. 1), which are the Simple Random (SR), the Random Descent (RD), the Random Permutation (RP), the Random Permutation Descent (RPD), and the Choice Function (CF). The selected mechanism is applied throughout a single execution of our algorithm.In the SR case, a low-level heuristic is selected and applied on all individuals of the search population, at each generation. The RD case is the same as the SR, but each low level heuristic is applied continuously until there is no improvement. In the RP, a random permutation of all low-level heuristics is determined. At each generation, the same low-level heuristic (based on the predefined order cyclically) is applied once to all individuals in the search population. The RPD case is the same as the RP case, but the low level heuristic is applied continuously until there is no improvement. Therefore, number of times that a selected low-level heuristic applied may vary from one individual to another.In the CF case, the low-level heuristic that maximizes the choice function is selected. Then, the same selected heuristic is applied to each individual in the search population. Although the same α, β and σ values (given in Eqs. (1)–(4)) are used for all low-level heuristics as stated in the literature, we consider separate α and β values for each low level heuristic to reflect relative weights more accurately. Initially, α and β values for all heuristics are set to 0.5. The value of α for a low-level heuristic Njis updated with the following equation(5)αNj=αNj+In*(Nj)et*,whereIn*(Nj)is the mean value of changes in objective function of all individuals in the population, which is different than the In(Nj) term given in Eq. (1). In this equationet*=max{eτ,eτ+1,…,et}, where etis the tth evaluation and τ is the time of the last change. The value of β is updated with a similar equation by using the termIn*(Nk,Nj), which is the mean value of changes to represent collective performance of the low level heuristic Nj. On the other hand, we consider a single σ value for all low-level heuristics, which is set to 0.5 initially. Then, it is increased or decreased by a constant value of 0.01.We omit Tnfrom the choice function, since running time of low-level heuristics is very small in most of the cases. Therefore, Eqs. (1) and (2) are replaced with Eqs. (6) and (7) in our implementation, respectively. Additionally, Eq. (3) is preserved and Eq. (4) is replaced with Eq. (8).(6)f1(Nj)=∑nαNjn−1×In*(Nj)(7)f2(Nk,Nj)=∑nβNjn−1×In*(Nk,Nj)(8)f(Nj)=αNjf1(Nj)+βNjf2(Nk,Nj)+δf3(Nj)Three different move acceptance criteria (which are AM, OI and MC cases) is considered in our hyper-heuristic based framework (see step 16 of Fig. 1). For all move (AM) case, the new solution is inserted into the next search population in any case. For only improving (OI) case, the new solution is inserted into the next search population only if it is better than the previous one.For Monte Carlo (MC) case [25,26], improving solutions are always accepted and non-improving solutions are accepted with a probability based on the simulated annealing function without reheating. Specifically, if new solution is worse than the current solution, a random number is generated. If the generated random number is smaller than the Monte Carlo value (the MC term given in Eq. (9)), the new solution is accepted; otherwise it is discarded.(9)MC=e(δ×i)/ΔEq. (9) is an adapted version of the classical simulated annealing function in order to represent dynamic changes in the environment, where the temperature parameter and reheating concept are not used; frequency of change and the distance from the last change terms are considered, instead. It represents minimization where δ is the difference between the current solution's fitness and the new solution's fitness. In this equation, the i term is the number of generations elapsed since the last environmental change and Δ represents frequency of change for the problem considered.Although there are many dynamic optimization problems (DOPs) presented in the literature, most of them are synthetic problems which provide controllable characteristic for the degree of dynamism and the complexity of the objective function [10]. The Moving peaks benchmark (MPB) [9], Dynamic Onemax function [11], Dynamic Deceptive functions [12] and Dynamic Royal-Road function [12] are a few example synthetic problems. Additionally, some DOPs can be generated from a given binary encoded stationary problem by using XOR generator [35]. On the other hand, real-world problems demonstrating dynamic behavior emerge during last few years [15–17].To validate our framework, we consider the dynamic generalized assignment problem and moving peaks benchmark in our experimental study. The first problem is generated by introducing environmental changes on the generalized assignment problem, which is a stationary real-world problem; and the second one is a widely used synthetic problem in the literature. The details of the problems are given in the remaining part of this section.The generalized assignment problem (GAP) is an NP-complete combinatorial optimization problem, where the goal is to find the minimum cost of assigning a set of jobs to a set of agents such that each job is assigned to exactly one agent without exceeding its resource capacity.Given m agents I={1, 2, …, m} and n jobs J={1, 2, …, n}, the GAP can be formulated as follows:(10)minimize∑i=1m∑j=1ncij*xijsubject to(11)∑j=1nrij*xij≤bi,∀i∈I(12)∑i=1mxij=1,∀j∈J(13)xij∈{0,1},∀i∈I,∀j∈J.where xijis set to 1 if job j is assigned to agent i and 0 otherwise. The term rijis the resource consumption of job j for agent i, cijis the cost of assigning job j to agent i, and biis the resource capacity of agent i. The equality constraints in Eq. (12) guarantee that each job is assigned to exactly one agent; and the capacity constraints in Eq. (11) state that the total resource requirement of the jobs of one agent does not exceed its capacity.We generate dynamic instances of the GAP by changing the costs of jobs, the resource consumptions and the capacity constraints, based on the procedure given for the dynamic knapsack problem presented in the literature [13]. They are updated by multiplying with a normally distributed random variable for every change (see Eq. (14)).(14)cij+=cij*(1+N(0,σc))rij+=rij*(1+N(0,σr))bi+=bi*(1+N(0,σb))Additionally, the cost of jobs, the resource consumptions and the capacity constraints are restricted to intervals based on the following inequalities [13]:(15)0.8*cij≤cij≤1.2*cij0.8*rij≤rij≤1.2*rij0.8*bi≤bi≤1.2*biIf the lower or upper bounds are exceeded after values are updated, they are bounded back from the interval and set to a corresponding value with the interval. In dynamic version of the GAP, the changes are known in advance; i.e., it provides changes in every x generation, where the value of x is assigned from a predefined set. We do not encounter infeasible results for the GAP problem types (given in Section5.2) in our experimental study, when dynamic instances are created by using Eq. (14). It is due to the problem-specific repair steps of our hybrid framework, which are already explained at Section3.3. Feasible instances of the dynamic GAP can also be generated by updating cijand rijwith the given equations and calculating bibased on the formula related with the problem type considered (see Section5.2 for original problem types).Moving peaks benchmark (MPB) generates a multidimensional landscape which consists of several peaks with changing the width, the height and the position of each peak in an n dimensional space. The cost function for n dimensions and m peaks has the following form [9]:(16)F(x→,t)=maxi=1,..mHi(t)1+Wi(t)×∑j=1n(xj−Pij(t))2wherex→∈Rnis a particular solution,P→∈Rm×nis the set of m peak locations.H→andW→are vectors for storing heights and widths of peaks, respectively.The coordinates, the heights and the widths of peaks are randomly initialized. During the search, the height, the width and the location of each peak are changed according to the following equations [9]:(17)σ∈N(0,1)Hi(t)=Hi(t−1)+height_severity×σWi(t)=Wi(t−1)+width_severity×σPi→(t)=Pi→(t−1)+vi→(t)wherevi→(t)is the shift vector that is the linear combination of the previous shift vectorvi→(t−1)and a random vectorr→. Both the random and shift vector are normalized to s. Additionally, λ is the parameter which defines the correlation between movements of a peak. The shift vector at time t is calculated with the following equation [9]:(18)vi→(t)=s|r→+vi→(t−1)|×((1−λ)r→+λvi→(t−1))Table 1gives the parameters of the moving peaks benchmark which are considered for all experiments in this study, unless otherwise stated.In this section, we present the details of our hyper-heuristic based formulation and low-level heuristics considered for the dynamic GAP. We represent each individual with an n dimensional vector, S=(S1, S2, …, Sn), where n is the number of jobs. For each job j, its assignment is denoted by Sj. This representation ensures the equality constraint given in Eq. (12). Each individual in the memory and search populations is randomly generated. The individuals are evaluated with the following fitness function [36]:(19)f(s)=∑j∈JcSjj+∑i∈Iψi·Pi(S)where ψ is the penalty weight of agent i; and Pi(S) is the penalty of agent i calculated by the following equation:(20)Pi(S)=max0,∑j∈J,Sj=irij−bi.Penalty weight of each agent is set to 1000 in our computational experiments, unless otherwise explained.To compute the next generation of the memory population, the binary tournament selection is used for selecting two parents for reproduction. After parent selection, one-point crossover and swap mutation are performed. The generational replacement strategy is considered and the elitism of the best individual is performed. The search population is handled based on hyper-heuristics (see Section3.3 for details).For dynamic GAP, we consider six different low-level heuristics, where each of which is a mutational heuristic that performs a single local search move. The low-level heuristics are based on state-of-the-art single shift and double shift neighborhoods given in [36].Single-shift low-level heuristics. In a single shift neighborhood, a new solution is obtained by shifting a job to another agent. Three single-shift heuristics are considered in our study:•Shifting a job to another random agent. In this method, a job is selected randomly and it is removed from the corresponding agent. Then, it is assigned to a randomly selected agent among the remaining ones.Shifting a job to the agent that has the minimum fitness value. A randomly selected job is removed from the corresponding agent. Then, it is assigned to the agent that has the minimum value of the fitness function.Shifting a job to another agent with the smallest relative cost-resource index. In this case, a randomly selected job is assigned to the agent which has the smallest relative cost-resource index, γij=cij×(rij/bi), which was presented in [33].Double-shift low-level heuristics. In this category, a new solution is obtained by performing two shift moves. We consider the following three variants:•Swapping two jobs assigned to different agents. This low-level heuristic exchanges assignments of two randomly selected jobs.Cyclic double shift. It is the cyclic double shift neighborhood given in [36]. A randomly selected job j0 is removed from the assigned agentSj0. Then, the avail(j) term (proposed in [36]) is computed for each job j by using Eq. (21).(21)avail(j)=rSj,j−PSj(S)ifrSj,j>PSj(S)rSj,jotherwiseIn this equation,PSjis the penalty of the agent where the job j is assigned. Among the jobs whose resource requirements are not more than the value of the avail(j) term for the job j0, i.e., among the jobs jksatisfyingrSj0,jk≤avail(j0), the most profitable job j1 is selected and it is moved to agentSj0. Finally, the job j0 is assigned to the agentSj1. The profit of a job is defined with−cij+c(Sj)j.Changing the assignment of two jobs that are assigned to different agents. This method is similar to the previous one. The removed job j0 is assigned to the agent which minimizes the fitness function, instead.In this study, all algorithms are coded in C programming language and the computational experiments are conducted on a set of machines, each of which has Intel Core 2 – 2.83GHz. processor running Linux operating system. We present the performance of the algorithms with respect to both the quality of solutions and computational efforts, at the following subsections. In order to compare the algorithms with respect to quality of solutions, the results are reported in terms of offline performance, which is defined byx*=1/T∑t=1Tet*whereet*=max{eτ,eτ+1,…,et}, etis the tth evaluation and τ is the time of the last change. On the other hand, computational effort of each algorithm is presented in terms of the average running time.In this work, five types of problem instances for GAP are considered, which are called types A, B, C, D, and E. The first four types are available at OR-Library [37] and the Type E is available at [38]. They are created as follows:Type A:rijare random integers selected from the range [5, 25], cijare random integers selected from the range [10, 50]; and bi=0.6(n/m)15+0.4R whereR=maxi∈I∑j∈J,Ij=irijand Ij=min{i|cij≤ckj}, ∀k∈I.Type B:rijand cijare set as in the Type A; but biis set with bi=0.7{0.6(n/m)15+0.4R}.Type C:rijand cijare set as in the Type A; but biis set with bi=0.8∑j∈Jrij/m.Type D:rijis a random integer from the range [1, 100]; cij=111−rij+e where e is a random integer from the range [−10, 10] and bi=0.8∑j∈Jrij/m.Type E:rijis set with rij=1−10lne2 where e2 are random numbers on (0, 1]; cij=1000/rij−10e3 where e3 are random numbers on [0,1] and bi=0.8∑j∈Jrij/m.For each problem type considered, a problem instance is created for each agent/job combination. Specifically, the number of jobs is equal to either 100 or 200; and the number of agents can be equal to 5, 10, and 20. Therefore, each problem type contains 6 problem instances, which generates a total of 30 instances in our experimental study. As explained in Section4.1, the dynamic instances of the GAP for the given problem types are created by using Eq. (14).The termination condition is set to 10,000 iterations, for both memory/search algorithm and our hyper-heuristic based approaches. Both the memory and search populations have 46 individuals and the memory has 10 individuals, as given in the related work. The changes are applied at every 50 generations, unless otherwise explained; and the memory is updated at every 10 generations, i.e., x=10. The standard deviations of the normal distributions for updating costs of jobs, resource consumption and capacity constraints (given in Eq. (14)) are set to 0.05. The crossover probability is taken 0.7 and the mutation rate is set to 0.2.

@&#CONCLUSIONS@&#
In this paper, we present a novel hybrid approach by integrating hyper-heuristic techniques with the memory/search algorithm for solving dynamic optimization problems. Computational study is conducted by considering two well-known dynamic optimization problems, which are the dynamic generalized assignment problem (dynamic GAP) and the moving peaks benchmark.For the dynamic generalized assignment problem, empirical results indicate that our hyper-heuristic based algorithms significantly outperform the memory/search algorithm with respect to quality of solutions. Specifically, our methods provide significantly lower offline performance values for all 30 problem instances that are generated from 5 different problem types. The most effective one is the choice function hyper-heuristic by considering the only improving acceptance criteria (i.e., the CF-OI case), where it outperforms the related work for all cases and it outperforms our remaining hyper-heuristic based alternatives for 16 out of 30 cases. The RPD-OI case is the second choice which provides best results for 8 cases. The results of one-way ANOVA tests reveals that most of our hyper-heuristic based solutions provide statistically significant results when compared with the reference algorithm for the problem instances of the dynamic GAP.On the other hand, our hyper-heuristic based algorithms generate up to 20% better quality of solutions (i.e., lower offline error values) than the reference algorithm for the moving peaks benchmark. Based on conducted set of experiments, most of the only-improving (OI) based hyper-heuristics perform better than other alternatives. Specifically, the CF-OI case yields either statistically significant results or better results than all other algorithms considered in the experimental study including the related work for various dynamic characteristics of changes on the problems addressed. The RD-OI case is the second best algorithm among the other alternatives for the moving peaks benchmark.Based on the overall results of experimental study on two different problems, it can be concluded that choice function hyper-heuristic with the only improving acceptance criteria (the CF-OI case) is capable of tracking changes in dynamic environments and it can react rapidly to different type and severity of changes. For solving dynamic optimization problems, all moves acceptance criteria (the AM case) can be considered as the worst strategy for all heuristic selection methods considered.We believe that this is the first study that investigates the effectiveness and efficiency of hyper-heuristics with the memory/search algorithm in solving the dynamic optimization problems. One future work is to utilize the proposed hybridization for solving other selected dynamic optimization problems including dynamic scheduling problems and dynamic vehicle routing problem.