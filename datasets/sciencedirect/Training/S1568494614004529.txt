@&#MAIN-TITLE@&#
Training algorithms for Radial Basis Function Networks to tackle learning processes with imbalanced data-sets

@&#HIGHLIGHTS@&#
We present a study about the performance of two classical weight training methods of Radial Basis Function Networks (RBFN), Least Mean Square (LMS) and Singular Value Decomposition (SVD), applied to classification problems, when the data-sets are imbalanced.These methods are tested with representative RBFN design paradigms: Clustering, Incremental, Genetic and CO2RBFN (a cooperative–competitive method proposed by the authors).The results obtained, statistically validated, show that SVD outperforms LMS, when the imbalance ratio of data-sets is low but when the imbalance ratio of these data sets grows, LMS outperforms SVD.

@&#KEYPHRASES@&#
Imbalanced data-sets,Radial Basis Function Networks,Training algorithms,Evolutionary computation,

@&#ABSTRACT@&#
Nowadays, many real applications comprise data-sets where the distribution of the classes is significantly different. These data-sets are commonly known as imbalanced data-sets. Traditional classifiers are not able to deal with these kinds of data-sets because they tend to classify only majority classes, obtaining poor results for minority classes. The approaches that have been proposed to address this problem can be categorized into three types: resampling methods, algorithmic adaptations and cost sensitive techniques.Radial Basis Function Networks (RBFNs), artificial neural networks composed of local models or RBFs, have demonstrated their efficiency in different machine learning areas. Centers, widths and output weights for the RBFs must be determined when designing RBFNs.Taking into account the locally tuned response of RBFs, the objective of this paper is to study the influence of global and local paradigms on the weights training phase, within the RBFNs design methodology, for imbalanced data-sets. Least Mean Square and the Singular Value Decomposition have been chosen as representatives of local and global weights training paradigms respectively. These learning algorithms are inserted into classical RBFN design methods that are run on imbalanced data-sets and also on these data-sets preprocessed with re-balance techniques. After applying statistical tests to the results obtained, some guidelines about the RBFN design methodology for imbalanced data-sets are provided.

@&#INTRODUCTION@&#
Many real applications are associated to data-sets with an implicit imbalance of the existing classes. This fact implies that the number of instances of certain classes is much lower than the instances of the other classes. The importance of these data-sets resides in the fact that a minority class usually represents the concept of interest, for example patients with illness in a medical diagnosis problem; whereas the other class represents the counterpart of that concept (healthy patients). For this kind of problems standard classifier algorithms have a bias toward the majority class. This is due to the fact that the mechanisms inside these classifiers are positively weighted to predict the majority class defined by the accuracy metrics. The different methods proposed for addressing this problem can be categorized into three groups [1]: resampling methods [2], algorithmic adaptations [3], and cost sensitive techniques [4].Radial Basis Function Networks (RBFNs) [5] are one of the most important Artificial Neural Network paradigms in the field of Machine Learning. RBFNs have important features such as: a simple topological structure; the possibility of extracting rules [6,7]; a universal approximation capability [8]; and that each neuron/RBF has a characteristic locally-tuned response. RBFNs have been successfully applied in most important machine learning areas [9] such as classification [10,11], regression [12,13] or time series forecasting [14,15]. Examples of application fields where RBFNs have been demonstrated their good behavior are: engineering problems [16–18], medical diagnosis [19,20] or web mining [21,22], among others. They have also obtained outstanding results in imbalanced problems [23]. Typically, RBFNs are designed in two stages: First, RBF parameters (center and width) are calculated with different techniques [9]; and then, weights are obtained with local or more global training algorithms. A global training algorithm considers all the data-set instances at the same time in a matrix that is resolved using numerical methods, whereas a local training algorithm separately trains each instance, only modifying weighs of the nearest RBFs.From these premises, the objective of this paper is to carry out a study that characterizes the most suitable RBFN training methodology (local and global) taking into account the known locally-tuned response of the RBFs and avoiding the above mentioned bias toward the majority class on imbalanced data-sets.Thus, this paper analyzes the characteristics of Least Mean Square (LMS) [24] and Singular Value Decomposition (SVD) [25], as local and global weights training representative methods respectively. In this way, the behavior of both training algorithms is studied with different RBFN design methods: an Incremental algorithm, a Clustering method, a traditional Evolutionary Computation method (Pittsburgh codification) and CO2RBFN [10], an evolutionary cooperative–competitive RBFN design method.During experimentation, the RBFN design algorithms were applied to a largest group of data-sets with different imbalance ratio (IR) and to the same group of data-sets preprocessed in order to re-balance them. The analysis of the results let us to obtain a set of guidelines for the use of weights training algorithms in imbalanced frameworks.The paper is organized as follows: First, Section 2 introduces RBFNs. Section 3 describes LMS and SVD, the two weights training algorithms for RBFNs. Section 4 details the most important paradigms for RBFN design. In Section 5, the imbalance problem is analyzed. The experimental framework of this research is specified in Section 6 and the results and their analysis are shown in Section 7. Finally, the conclusions of the paper are outlined in Section 8.From a structural point of view, an RBFN is a feed-forward neural network with three layers: an input layer with n nodes, a hidden layer with m neurons or RBFs, and an output layer (Fig. 1).The m neurons of the hidden layer are activated by a radially-symmetric basis function, ϕi:Rn→R, which can be defined in several ways [26], the Gaussian function being the most widely used (Eq. (1)):(1)ϕi(x→)=ϕi(e−(∥x→−ci→∥/di)2)whereci→∈Rnis the center of basis function ϕi, di∈R is the width (radius), and ∥∥ is typically the Euclidean norm on Rn. This expression is the one used in this paper as the Radial Basis Function (RBF). The output node implements the following function, where weightswijshow the contribution of an RBF to the output node (Eq. (2)):(2)fj(x→)=∑i=1mwijϕi(x→)The objective of any RBFN design process is to determine centers, widths and the linear output weights connecting the RBFs to the output neuron layer. The most traditional learning procedure has two stages [27–29,13,30]: first, learning of centers and widths, and then, training of output weight. In order to obtain the weights in the second stage, two paradigms are usually followed: gradient descent based methods (with a local behavior) [29,31,30] and global linear regression methods [12,32,13,31,33].One early technique to adjust the centers and widths is Clustering [34]. K-means algorithm [35] is a well-know algorithm based on clustering, which has been used for the RBFN design in [36]. For this kind of methods the center of the clusters are established as the center of the RBFs and RBF widths are typically set to the width of the previously calculated clusters or as the average distance between RBFs.RBFNs were initially used for function approximation. For this reason, most of the RBFN design methods are based on traditional optimization techniques such as regularization [37], orthogonalization of regressors [38], gradient-based [39], or Levenberg–Marquardt [40]. These techniques can be used to decide if the RBFs aggregate or eliminate and may be considered as forward or backward selection methods [41]. The RAN algorithm [42] is one of the most well-known methods based on an incremental (aggregation) scheme.Another important paradigm for RBFN design is Evolutionary Computation (EC) [43–45], which uses natural evolution and stochastic searching to design optimization algorithms. Reviews of EC applied to RBFN design can be found in [46,9]. In most of the proposals within this evolutionary paradigm, an individual represents a whole RBFN, and different operators are applied to the entire population to improve individual fitness [46,47]. This approach is known as the Pittsburgh representation scheme. An alternative to this evolutionary approach is the cooperative–competitive evolutionary or cooperative–coevolutionary strategy [48,49]. It provides a framework within which an individual of the population represents only a part of the solution, as it evolves in parallel and competes to survive, but at the same time cooperates to find a common solution (the complete RBFN).In the engineering field, it can be found other RBFN design methods [17]. In [50–52] control systems are modeled using RBFNs. To developed them, first genetic algorithms are employed to determine the initial parameters of the networks. Then, the parameters of the decoupled adaptive neural network controllers are updated regarding some stability theories, such as the Lyapunov theory, and boundary-layer functions that guarantee the convergence of the state errors within a specified error bound. In the experimentation, the success of the design methods are demonstrated testing the stability and the error convergence of the modeled adaptive control laws.Pérez-Godoy et al. developed an evolutionary cooperative–competitive RBFN design method, CO2RBFN [10]. The standard version of CO2RBFN, which uses the LMS algorithm for training weights, was applied to the classification problem of imbalanced data-sets [23] and achieved significant results. Other conclusions deduced in [23] were the good behavior of some RBFN design methods dealing with imbalanced data-sets.As mentioned in this paper a more general objective is established, specifically to characterize the weights training phase inside a RBFN design process for imbalance problems. Thus, we will analyze the behavior of local and global training algorithms with different RBFN design methods such as an Incremental algorithm, a Clustering method, a traditional Evolutionary Computation method (Pittsburgh codification) and CO2RBFN. Theses methods are described in Section 4.During the training phase for RBFN weights a known set of input and output data pairs were delivered to the RBFN to compute the output layer weights. Taking into account Eq. (2), the least squares recipe is then to minimize the sum-squared-error.(3)S=∑i=1p(yi−f(xi))2,∀i=1,…,nwhere yiis the desired output and f(xi) is any output of the network.In order to calculate the weights that reduce the error of Eq. (2), different methods are normally used. These methods can been categorized as local or global paradigms. Gradient descent based methods [29,31,30,15] have a local behavior and separately trains each instance, only modifying weighs of the nearest RBFs. Global linear regression methods [27,12,32,13,31] consider all the data-set instances at the same time, solving the above equation by means of linear least squares in matrix forms.LMS was selected, as representative of local weights training algorithms, because it is the reference algorithm within the gradient descent based methods. With respect to global weights training algorithms there are several well known techniques to solve the matrix that results when linear regression methods are applied, for instance: the Cholesky decomposition [25], the orthogonal least squares (OLS) method [38], or the Singular Value Decomposition (SVD) [25]. SVD, the selected global training method for the experimentation, is one of the most widely used because it avoids possible ill-conditioning in the resolution matrix.The Least Mean Square (LMS) algorithm [24] can be used to calculate the RBF weights. This technique exploits the local information that can be obtained from the behavior of each RBFs. Eq. (4) shows the update of the weights.(4)w¯k+1=w¯k+αekx¯k|x¯k2|where k is the number of iterations,w¯k+1is the next value of the weight vector,w¯kis the present value of the weight vector andx¯kis the value of the actual input pattern vector. The present linear error, ek, is defined as the difference between the desired output and the output network before adaptation. The α value is the speed of learning, which measures the size of the adjustment to be made. The choice of α controls stability and speed of convergence.This weight training algorithm is easier to explain if the RBFN performance is expressed in a matrix form:y¯=Θw¯, wherey¯is the vector of the desired training outputs,w¯is the weight vector, and Θ is a non-square matrix with components which are equal to the output of the radial basis functions evaluated in the training points.Minimization of the sum-squared error function yields the well-known least-squares solution for the weights:Aw¯=ΘTy¯, from this equation we can extract the weight vector:w¯=A−1ΘTy¯. A−1, the variance matrix, is:A−1=ΘTΘ−1.Thus, the network weights can be computed by fast linear matrix inversion techniques using SVD.SVD is chosen because it avoids possible ill-conditioning of A. This method provides a decomposition of A that allows the calculation ofw¯and an optimal solution to the least mean squares equation.This section describes the algorithms which represent some of the most significant paradigms in the RBFN design: a method based on the clustering technique (Clustering), a procedure based on the incremental technique (Incremental), an evolutionary algorithm based on the Pittsburgh representation scheme (Genetic), and CO2RBFN, an evolutionary cooperative–competitive algorithm. All the algorithms were implemented by the authors of this paper.Traditionally, clustering has been associated with classification tasks. The objective of the cluster analysis is to capture the natural structure of the data by dividing them into groups (clusters) that are meaningful, useful or both. These meaningful groups have been assigned to classes and often a representative or prototype of each group is often determined. As has been previously mentioned, clustering techniques have been the first stage in the classical design of RBFNs, as they relate each cluster to an RBF. Therefore, the RBFN will have the same number of RBFs as the calculated clusters and the geometric center of the clusters are established as the center of the RBFs. RBF widths are typically set to the width of the previously calculated clusters or may be established as the average distance between RBFs.One of the most well-known clustering techniques is K-means [53]. This technique defines a prototype in terms of a centroid, i.e., the mean of a group of points. K initial centroids are chosen, where K is a user specified parameter and represents the desired number of clusters. Each point is then assigned to the closest centroid, and each collection of points assigned to a centroid is a cluster. The centroid of each cluster is then updated using the points assigned to the cluster. The assignment and update steps are repeated until the centroids remain the same, i.e., no point changes clusters.Following this paradigm, the Clustering method for RBFN design was implemented. The structure of this algorithm is shown in Algorithm 1. Thus, given the input data vectorX=x→i:i=1,…,n, the objective was to obtain K clusters with their corresponding prototype. In order to calculate the cluster to which each vector belongs, it was necessary to define a distance measure and to minimize the objective function D (Eq. (5)):(5)D=∑j=1c∑i=1n∥x→i−p→j∥wherex→1,…,x→nare the vectors to group,p→1,…,p→care the prototypes for each group and ∥∥ is the Euclidean distance. Moreover, membership functionμpj(x→i)of the input vectorx→ito the cluster represented by the prototypep→jis defined in Eq. (6). The clustering achieved by the K-means algorithm can be described by the partition matrix U shown in Eq. (7).(6)μpj(x→i)=1if∥x→i−p→j∥2<∥x→i−p→l∥2∀l≠j0in another case(7)U=μ∈{0,1}|∑i=1cμPi(x→k)=1∀kand0<∑k=1nμPi(x→k)<n∀iThe Clustering method, described in Algorithm 1, begins with random initialization of the prototypesp→jto existing input vector and sets the distortion δ to infinite where δ measures changes in the clusters. Following the main loop will determine the clusters. The stop condition of this loop is to check, whether in this iteration the distortion has changed over a threshold, ϵ, with respect to the distortion of the previous iteration δprev. The two main steps of the loop are: first, determining to which partition each vector belongs by calculating its memberships functionμpj(x→i)(Eq. (6)). Second, recalculating prototypes of each partition using Eq. (8). In the last step of the main loop, δ is calculated using Eq. (9) where δjis distortion calculated for cluster j. Finally, once the clusters are determined, the weights are trained using LMS or SVD.Algorithm 1Main steps of the Clustering methodδ=∞Random initialization of the first set of prototypesp→jwhile |δant−δ|/δ>=ϵdoδant=δCalculate membership functionsμpi(x→k)Calculate prototypesp→jCalculate δend whileTrain Weights (LMS or SVD)(8)p→j=∑i=1nμpj(x→i)x→i∑i=1nμpj(x→i)(9)δ=∑j=1cδj(10)δj=∑x→i∈Pj∥x→i−p→j∥2Incremental algorithms are a traditional paradigm for RBFN design [42,54,41]. One of the first and most well-known algorithms is the RAN (Random Allocation Network) algorithm [42]. This algorithm starts with an empty RBFN (without RBFs) and, iteratively, identifies patterns that are not currently well represented by the network, allocating new units that memorize these patterns. Thus, in each iteration of the main loop, a random pattern of the training set is analyzed in order to check whether two conditions are fulfilled. These conditions are firstly whether the pattern is placed at a distance of more than a δ threshold from any RBF, and secondly whether the difference for this pattern, between the desired output and the output of the network, is larger than an ϵ threshold. If both conditions are met, a new RBF is inserted, where the RBF center is set to the pattern, and the RBF width is set to the minimum distance between the new RBF and the other RBFs in the RBFN. If these conditions are not fulfilled, the weights of this RBF are adapted using the LMS algorithm.To test this important paradigm, the Incremental method was implemented. This method includes two versions of the RAN algorithm, one for each weights training algorithm (LMS and SVD). The main steps of the algorithm are detailed in Algorithm 2.The main loop of the implemented algorithm begins with the random determination of the instance I from the training data-set (TD). Then, the output of the RBFN, f(x), and the error committed for this instance, e, are calculated. Next, the distance, d, from I to the nearest RBF in the RBFN is determined. Having e and d, the typical conditions of the RAN algorithm are checked, i.e., If e is higher than ϵ And d is higher than δ Then a new RBF is placed on I. As in the original RAN algorithm, the RBF center is set to I, and the RBF width is set to the minimum distance between the new RBF and the other RBFs. For the first RBF, the RBF width is set to δ. After that, weights are trained using SVD or LMS.This is the main difference with regards to the original RAN algorithm, because RAN only trains the weight of the instance I when the conditions are not met. This change was introduced for two reasons: SVD cannot be applied only for a weight of one RBF, and most algorithms for RBFN design train the whole set of the weights of the RBFN. Finally, the stop condition of the loop checks the number of times that RAN conditions are not met. If this number cont is higher than the total number of instances of the training data-set, then the algorithm ends.Algorithm 2Main steps of Incremental methodcont=0while cont < nInstances(TD) doI=random(TD)Evaluate the output of the RBFN f(x)Compute error e=|y−f(x)|Find distance, d, from I to nearest RBFif (e>ϵ)and(d>δ) thenInsert a new RBF with center=Iif is the first RBF thenRBF width=deltaelseRBF width=dend ifTrain Weights (LMS or SVD)cont=0elsecont=cont+1end ifend whileEvolutionary computation is a widely used paradigm in optimization and particularly in RBFN design [9]. Thus, the Genetic method has been implemented following the design lines of genetic algorithms for RBFNs learning [46].This method follows the traditional Pittsburgh evolutionary approach for the design of RBFNs. In this approach each individual is a whole network and therefore contains the coordinates of the center, widths and weights for each RBF. A real codification of the individuals was used considering a variable number of RBFs. The objective of the evolutionary process is to minimize the classification error. The best individual will be the final solution.Algorithm 3Main steps of Genetic methodInitialize RBFNwhile Number of Generations not Achieved doSelectionRecombinationMutationTrain Weights (LMS or SVD)Evaluation RBFNend whileThe main steps of this algorithm are shown in Algorithm 3.As an initialization step, for each individual, a random number of neurons is allocated to the different patterns of the training set. Thus, each RBF center,c→i, is randomly established to a pattern of the training set. The RBF widths, di, will be set to half of the average distance between the centers. Finally, the RBF weights,wij, are set to zero.A tournament selection mechanism is applied to the whole group in order to determine the new population. The diversity of the population is promoted by using a low value for the tournament size (k=3).Recombination and mutation operators are applied to the new RBFNs population. With the crossover (recombination) operator two individuals (RBFNs) parents are chosen to obtain an RBFN offspring. The number of RBFs of the new individual will be delimited between a minimum and a maximum value. The minimum value is set to the number of RBFs of the parent with fewest RBFs. In the same way, the maximum value is set to the number of RBFs of the parent with most RBFs. In order to generate the offspring, RBFs will be chosen from the parents at random.Six mutation operators, usually considered in the specialized bibliography [46], were implemented. They can be classified as random operators or biased operators. The random operators are:•DelRandRBFs: randomly eliminates k RBFs, where k is a pm percent of the total number of RBFs in the RBFN.InsRandRBFs: randomly aggregates k RBFs, where k is a pm percent of the total number of RBFs in the RBFN.ModCentRBFs: randomly modifies the center of k RBFs, where k is a pm percent of the total number of RBFs in the RBFN. The center of the basis function will be modified in a pr percent of its width.ModWidtRBFs: randomly modifies the center of k RBFs, where k is a pm percent of the total number of RBFs in the RBFN. The width of the basis function will be modified in a pr percent of its width.Biased operators which exploit local information are:•DelInfRBFs: deletes the k RBFs of the RBFN with a lower weight. k is a pm percent of the total number of RBFs in the RBFN.InsInfRBFs: inserts the k RBFs in the RBFN outside the width of any RBF present in the RBFN. k is a pm percent of the total number of RBFs in the RBFN.After applying mutation operators, weights are trained using the LMS or the SVD algorithm. The fitness for each individual/RBFN is defined as the geometric mean, see Eq. (15), classification error.The search space of this method was limited in order to establish similar operating conditions for the algorithms. As it is well known, with Pittsburgh genetic algorithms, where the only objective to optimize is the classification error, the complexity of the individuals (i.e. number of RBFs) grows in an uncontrolled way (because an RBFN with more RBFs usually gives a lower error percentage than an RBFN with fewer RBFs). In this way a maximum complexity (chromosome size) was established.CO2RBFN [10], is an evolutionary cooperative–competitive hybrid algorithm for the design of RBFNs. In this algorithm each individual of the population represents, with a real representation, an RBF and the entire population is responsible for the final solution. The individuals cooperate toward a definitive solution, but they must also compete for survival. In this environment, in which the solution depends on the behavior of many components, the fitness of each individual is known as its “credit assignment”. In order to measure the credit assignment of an individual, three factors were proposed: the RBF contribution to the network output, the error in the basis function radius, and the degree of overlapping among RBFs.There are four evolutionary operators that can be applied to an RBF: an operator that eliminates the RBF, two operators that mutate the RBF, and finally, an operator that maintains the RBF parameters in order to explore and exploit the search space and to preserve the best RBF, respectively.The application of the operators is determined by an fuzzy rule base system. The inputs of this system are the three parameters used for credit assignment and the outputs are the operators’ application probability.The main steps of CO2RBFN, explained in the following subsections, are shown in the pseudocode in Algorithm 4.Algorithm 4Main steps of CO2RBFN methodInitialize RBFNwhile Not Stop doTrain RBFN (LMS or SVD)Evaluate RBFsApply operators to RBFsSubstitute the eliminated RBFsSelect the best RBFsend whileIn the RBFN initialization step, to define the initial network, a specified number m of neurons (i.e. the size of population) is randomly allocated to the different patterns of the training set. To do so, each RBF center,c→i, is randomly assigned to a pattern of the training set. The RBF widths, di, will be set to half of the average distance between the centers. Finally, the RBF weights,wij, are set to zero.In the RBFN training step, LMS or SVD training algorithm is used.For the RBF evaluation, a credit assignment mechanism is required in order to evaluate the role of each RBF ϕiin the cooperative–competitive environment. For an RBF, three parameters, ai, ei, oiare defined:•The contribution, ai, of the RBF ϕi, i=1, …, m, is determined by considering the weight,wi, and the number of patterns of the training set inside its width, npii. An RBF with a low weight and few patterns inside its width will have a low contribution:(11)ai=|wi|ifnpii>q|wi|*(npii/q)otherwisewhere q is the average of the npiivalues minus the standard deviation of the npiivalues.The error measure, ei, for each RBF ϕi, is obtained by counting the wrongly classified patterns inside its radius:(12)ei=npibcinpiiwhere npibciand npiiare the number of wrongly classified patterns and the number of all patterns inside the RBF width respectively.The overlapping of the RBF ϕiand the other RBFs is quantified by using the parameter oi. This parameter is computed by taking into account the fitness sharing methodology [44], whose aim is to maintain the diversity in the population. This factor is expressed as:(13)oi=∑j=1moijoij=(1−∥ϕi−ϕj∥/di)if∥ϕi−ϕj∥<di0otherwisewhere oijmeasures the overlapping of the RBF ϕiy ϕjj=1…m.In CO2RBFN four operators have been defined in order to be applied to the RBFs:•Operator Remove: eliminates an RBF.Operator Random Mutation: randomly modifies the coordinates of the center and the width of an RBF.Operator Biased Mutation: modifies the width and the coordinates of the center using local information of the RBF environment.Operator Null: in this case all the parameters of the RBF are maintained.The operators are applied to the whole population of RBFs. The probability of choosing an operator is determined by means of a Mandani-type fuzzy rule based system [55]. This system represents expert knowledge about the operator application in order to obtain a simple and accurate RBFN.The inputs of this system are parameters ai, eiand oiused for defining the credit assignment of the RBF ϕi. These inputs are considered linguistic variables vai, veiand voi. The outputs, premove, prm, pbmand pnull, represent the probability of applying Remove, Random Mutation, Biased Mutation and Null operators, respectively.The rule base system aims to evolve RBFs with a good behavior (high contribution, low error and low overlapping) and to eliminate RBFs with a bad behavior (low contribution, high error and high overlapping).In the introduction of new RBFs step, the eliminated RBFs are substituted by new RBFs. The new RBF is located in the center of the area with maximum error or in a randomly chosen pattern with a probability of 0.5 respectively.The width of the new RBF will be set to the average of the RBFs in the population plus half of the minimum distance to the nearest RBF. Its weights are set to zero.The replacement scheme determines which new RBFs (obtained before the mutation) will be included in the new population. To do so, the role of the mutated RBF in the net was compared with the original one to determine the RBF with the best behavior in order to include it in the population.Firstly, this section introduces the problem of imbalanced data-sets in classification. Secondly, it describes the pre-processing technique that was applied in order to deal with the imbalanced data-sets: the SMOTE algorithm [56].The number of instances of each class can be very different in imbalanced classification problems [57]. Standard classifier algorithms usually have a bias toward the majority class during the learning process in favor of the standard accuracy rate metric, which does not take into account the class distribution of the data. Consequently, the instances belonging to the minority class are misclassified more often than those belonging to the majority class.We used the imbalance ratio (IR), defined as the ratio of the number of instances of the majority class to the minority class, to organize the different data-sets. In the bibliography [58], research usually focuses on binary imbalanced data-sets, where there is only one positive and one negative class. In the same way, in order to make the analysis of the results easier, data-sets were categorized into two groups according to the IR level. The first group, called “high”, comprises data-sets with IR levels higher than 9, where there are no more than 10% of positive instances in the whole data-set compared to the negative ones. The second sub-group is “low” and it is composed of data-sets where the instances of the positive class are between 10% and 40% of the total instances (IR between 1.5 and 9).The measures of the quality of classification were defined from a confusion matrix (shown in Table 1), which recorded correctly and incorrectly recognized examples for each class.The most extensively used empirical classification measure, accuracy rate (Eq. (14)), does not distinguish between the number of correct labels of different classes. This fact may lead to erroneous conclusions in the context of imbalanced problems. For example, a classifier that obtains an accuracy of 99% in a data-set with a distribution of 1:100 might not be accurate if it does not extensively cover any minority class instance.(14)Acc=TP+TNTP+FN+FP+TNA widely used metric for imbalanced data-sets, also used in this study, is the geometric mean of the true rates [59], which can be defined as:(15)GM=TPTP+FN·TNFP+TNThis metric attempts to maximize the accuracy of each of the two classes with a good balance. It is a performance metric that links both objectives.As already mentioned, the approaches previously proposed for dealing with the class-imbalance problem can be categorized into three groups [1]: resampling methods which pre-process or resample the data in order to diminish the effects of their class imbalance [60,61,2]; algorithmic adaptations which create new algorithms or modify existing ones to take the class-imbalance problem into consideration [59,62,63,3]; and cost-sensitive learning solutions [4] which incorporate the data and algorithmic level approaches and which assume higher misclassification costs with samples in the minority class and seek to minimize the high cost errors [64–66].The advantage of the approaches based on pre-processing the data is that they are independent of the classifier used. In [60] these methods are classified into three groups:•Under-sampling methods that eliminates some of the examples of the majority class in order to decrease the imbalance.Over-sampling methods that creates new synthetic examples of the minority class.Hybrid methods that combine the two previous methods.In this study the choice was an over-sampling method which is widely-used in this area: the SMOTE algorithm [56].With this approach, the positive class is over-sampled by taking each minority class sample and introducing synthetic examples along the line segments joining any/all of the k minority class nearest neighbors. Depending upon the amount of over-sampling required, neighbors from the k nearest neighbors are randomly chosen. This process is illustrated in Fig. 2, where xiis the selected point, xi1 to xi4 are some selected nearest neighbors and r1 to r4 the synthetic data points created by the randomized interpolation. The implementation of this method uses only one nearest neighbor with the Euclidean distance, and balances both classes to 50% distribution.In short, the main idea is to form new minority class examples by interpolating between several minority class examples that lie together. More specifically, synthetic samples are generated in the following way: Take the difference between the feature vector (sample) which is under consideration and its nearest neighbor, multiply this difference by a random number between 0 and 1, and add it to the feature vector under consideration. This causes the selection of a random point along the line segment between two specific features. This approach effectively forces the decision region of the minority class to become more general.A high number of data-sets with different IR was selected to analyze the performance of training algorithms for RBFNs in an imbalanced scenario. The objective was to analyze the influence that weight training algorithms with global and local behavior has on the design of RBFNs models for imbalanced data-sets. Thus, different RBFNs design methods and two representative weight training algorithms were considered. To include in the analysis the effect on preprocessed data-sets the experimentation was divided into two subsections: performance analysis with original data-sets (without preprocessing) (Section 7.1), and the same package of data-sets but preprocessed with SMOTE (Section 7.2). As previously mentioned, SMOTE was chosen because nowadays it is one of the most widely used methods that shows the best performance.Two versions, LMS and SVD, were implemented respectively for the RBFN design methods and therefore eight different algorithms were obtained.First, this section describes the collection of imbalanced data-sets selected for the study (Section 6.1). Second, it shows the algorithms selected for comparison in the experimental study and the corresponding parameters (Section 6.2). Finally, it presents the statistical tests used in the analysis (Section 6.3).This study used forty four binary data-sets with different IR from KEEL (Knowledge Extraction based on Evolutionary Learning) [67] repository.Table 2summarizes the data selected in this study and shows, for each data-set, the number of examples (#Ex.), number of attributes (#Atts.), class name of each class (minority and majority), class attribute distribution and IR. This table is ordered according to the IR, from low to highly imbalanced data-sets.A five fold cross validation approach was used to estimate precision. This is five partitions for training and test sets, 80% for training and 20% for testing, where the five test partitions form the whole set. For each data-set the study considered the average results of the five partitions by five run repetitions.As stated above, alternative paradigms in the RBFN design field were selected to evaluate the training methods. The paradigms selected are: a procedure based on the incremental technique (Incremental method), a method based on the clustering technique (Clustering method), a genetic algorithm based on the Pittsburgh representation scheme (Genetic method) and the evolutionary cooperative–competitive method, CO2RBFN.Table 3summarizes the parameters for the different approaches used in the experimental study. The parameters used in the experimentation are typical for these algorithms. A population, number of RBFs, of 5 was established for CO2RBFN as in [23].In the same way, to establish models with similar complexity, the maximum number of RBFs for RBFN (Chromosome length) for the Genetic method was established at 6. Thus, the experimentation demonstrates that the average number of RBFs of the models obtained is about 5.5.Regarding the use of the SMOTE pre-processing method [56], only the 1-nearest neighbor (using the Euclidean distance) was considered to generate the synthetic samples. Furthermore both classes were balanced to the 50% distribution.This study used statistical techniques for the analysis of the results, in order to provide a correct empirical study [68–70]. Specifically, non-parametric tests were considered. The reason for this was that as the initial conditions that guarantee the reliability of the parametric tests might not have been satisfied and that would decrease the credibility of the statistical analysis [68]. Specifically, a Wilcoxon signed-rank test was employed as a non-parametric statistical procedure to perform pairwise comparisons between two algorithms, the RBFN design method trained with LMS and the same method trained with SVD. This test is explained in Appendix A. Furthermore, any interested reader can find additional information on the website http://sci2s.ugr.es/sicidm/, together with the software to apply the statistical tests.

@&#CONCLUSIONS@&#
Nowadays, research on imbalanced data-sets is receiving more attention because they are present in many real applications. However, traditional classifiers are often unable to handle these kinds of data-sets adequately. This is because they are biased toward a correct classification of the majority classes and leave aside minority classes.The present study identifies the most suitable RBFN weights training methodology to avoid the above mentioned bias toward the majority class. To do so, the study considered the two main weights training paradigms (local and global) used in the RBFN design. For these typical training paradigms used to calculate output weights, two representative algorithms were chosen: LMS, a gradient descent-based algorithm that makes more local transformations; and SVD, a matrix computation algorithm that achieves a global solution. In order to test these training algorithms, different RBFN design methods belonging to the main paradigms in the field were chosen.The results, statistically validated, show:•For data-sets with the highest IR, i.e., data-sets with high IR and without preprocessing, all the RBFN design methods trained with the LMS algorithm outperformed, with significant differences, RBFN design methods trained with the SVD algorithm.For data-sets with low IR and also without preprocessing, RBFN design methods trained with the LMS algorithm often outperformed RBFN design methods trained with the SVD algorithm, but not always with significant differences.For data-sets with an original high IR, but rebalanced with SMOTE, SVD methods outperformed LMS methods. However, it was difficult to find significant differences.Finally, for the data-sets with the lowest level of IR, i.e., data-sets with low IR and preprocessed with SMOTE, SVD methods outperformed LMS methods, even with frequently significant differences.A general conclusion is that the higher the data-set IR, the better the results are achieved by a local weights training methods (such as LMS). This fact can be explained because SVD obtains more global optimization models, which improves the classification performance of the majority class and disfavors the minority class. Nevertheless, LMS with a more local (per instance) optimization is able to achieve best results. When the IR of the data-set is lower or the data-set is balanced with methods like SMOTE, the SVD algorithm improves its performance.