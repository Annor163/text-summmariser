@&#MAIN-TITLE@&#
Automatic abstraction controller in reinforcement learning agent via automata

@&#HIGHLIGHTS@&#
Reinforcement learning is a branch of machine learning in which the agent must learn through interaction with the environment.One of the major challenges the scaling up of reinforcement learning faces with is the well-known “curse of dimensionality” problem.One way to address this problem is hierarchically learning.

@&#KEYPHRASES@&#
Reinforcement learning,Hierarchical reinforcement learning,Cluster,Multi-agent learning,

@&#ABSTRACT@&#
Reinforcement learning (RL) for solving large and complex problems faces the curse of dimensions problem. To overcome this problem, frameworks based on the temporal abstraction have been presented; each having their advantages and disadvantages. This paper proposes a new method like the strategies introduced in the hierarchical abstract machines (HAMs) to create a high-level controller layer of reinforcement learning which uses options. The proposed framework considers a non-deterministic automata as a controller to make a more effective use of temporally extended actions and state space clustering. This method can be viewed as a bridge between option and HAM frameworks, which tries to suggest a new framework to decrease the disadvantage of both by creating connection structures between them and at the same time takes advantages of them. Experimental results on different test environments show significant efficiency of the proposed method.

@&#INTRODUCTION@&#
RL is a branch of machine learning in which the agent must learn through interaction with the environment. RL has two main unique features: (1) Learning is based on trial and error, and (2) Signals of rewards may be delayed. One of the major challenges the scaling up of reinforcement learning faces with is the well-known “curse of dimensionality” problem or combinatorial explosion (In dealing with large environments, the state space grows exponentially).To cope with the curse of dimensionality, the state space reduction techniques can be used. In this ways, the original model is mapped to another model with fewer states. In other words, the new structural model is modified to obtain an efficient solution so that the transformed model is an approximate of the main model. This approach of reinforcement learning has a better ability to deal with the curse of dimensionality in solving problems and introduces methods such as Monte Carlo, temporal difference backups and functions approximation. Another approaches use techniques of aggregation/disaggregation [1]. The method is used in cases where the system model can be viewed as an interacting set of tightly coupled subsystems. The solution method is generally an iterative one in which sub-models are solved, then the obtained results are used to improve the sub-models repetitively until the optimal convergence is achieved.Recent attempts to address the challenge of the curse of dimensionality in RL tend to methods based on abstraction which are indeed based on the aggregation and disaggregation techniques. Using these techniques leads to hierarchical control architectures and their associated learning algorithms known as hierarchical reinforcement learning (HRL). In most cases, hierarchical solutions provide near optimal solutions in their performance, less cost at runtime and also at the learning time and required solving space in comparison with mere RL techniques [2].HRL is wide and active branch of RL that applies the hierarchical structure in the state, action and strategy space. Some frameworks based on HRL use temporal abstraction techniques. Making a decision at each step is not necessary, but instead the temporally extended actions (the macro actions) or sub-tasks can be selected to reach the goal [3]. Indeed, HRL is a general framework for scaling up RL to problems with large state spaces by using the task structure (action) to restrict the space of policies. The key principle considered by HRL is that the development of learning algorithms does not need to learn a policy from scratch, but instead it reuses available policies for simpler subtasks (macro) and uses the divide-and-conquer strategy. HRL models to apply the temporal abstraction must be able to make use of actions with variable lengths. A statistical model known as semi-Markov decision processes (SMDP) is used to perform behaviors with actions of variable lengths. These works on SMDP approach have led to the development of powerful HRL models, including Hierarchical Abstraction Machines (HAMs) [4,5], options [3], and MAXQ [6].The goal of the option approach is to learn global policies, being given completely partial polices to do subtasks; but HAMs emphasize on limiting policies that can be performed instead of developing the actions. In option framework, the set of agents’ actions are increased by produced actions instead of being simplified or even reduced. In the framework of reinforcement learning using option, choosing temporally extended actions is too expensive due to their large number of time steps. Now the issue is to create a smart choice of temporally extended actions to increase the learning speed.The main innovation introduced in this paper is the use non-deterministic automata like HAM as a high-level controller to select temporally extended actions in low level. The proposed approach tries to present a framework which uses the capabilities of option and HAMs, and also reduces the disadvantages of both methods by building communication structures between them.Option and HAM frameworks have certain operational features. Obviously, a structural design based on the two approaches in such a way that they cover each other's weaknesses leads to a flexible structure with high performance. These approaches are strong methods but, as mentioned before, have some disadvantages the purpose of this paper is to address.The rest of the paper is divided into the following sections: Section 2 gives a review of the background of the work. Section 3 discusses multi-agent learning. In Section 4, we describe the algorithm and its steps. Section 5 defines a framework of the proposed method and that how it is used in RL. Section 6 offers some computational experiments to evaluate the performance of the proposed algorithms on single-agent and multi-agent environments separately. Finally, we conclude our work in Section 7.

@&#CONCLUSIONS@&#
