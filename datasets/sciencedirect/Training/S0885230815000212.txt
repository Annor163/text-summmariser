@&#MAIN-TITLE@&#
Improving translation quality stability using Bayesian predictive adaptation

@&#HIGHLIGHTS@&#
Bayesian predictive adaptation (BPA) is presented for model adaptation in SMT.Results are presented for a standard adaptation task (JHU SummerWorkshop 2012).Comparison between different sampling strategies.In-depth analysis of stability of most common optimisation algorithms in SMT.Computational cost comparison among the methods presented.

@&#KEYPHRASES@&#
Bayesian methods,Adaptation,Natural language processing,Machine translation,

@&#ABSTRACT@&#
We introduce a Bayesian approach for the adaptation of the log-linear weights present in state-of-the-art statistical machine translation systems. Typically, these weights are estimated by optimising a given translation quality criterion, taking only into account a certain set of development data (e.g., the adaptation data). In this article, we show that the Bayesian framework provides appropriate estimates of such weights in conditions where adaptation data is scarce. The theoretical framework is presented, alongside with a thorough experimentation and comparison with other weight estimation methods. We provide a comparison of different sampling strategies, including an effective heuristic strategy and a theoretically sound Markov chain Monte-Carlo algorithm. Experimental results show that Bayesian predictive adaptation (BPA) outperforms the re-estimation from scratch in conditions where adaptation data is scarce. Further analysis reveals that the improvements obtained are due to the greater stability of the estimation procedure. In addition, the proposed BPA framework has a much lower computational cost than raw re-estimation.

@&#INTRODUCTION@&#
Adaptation has become a very popular issue in natural language processing (Kuhn and De Mori, 1990; Huo et al., 1995; Koehn and Schroeder, 2007), and more specifically in statistical machine translation (SMT) (Koehn, 2010). Typically, the adaptation problem arises when two very different sets of training data are available, yielding two different sets of model parameters. The first set of data, the training dataT(e.g., obtained from the European Parliament or the United Nations) is often very large and rather generic in domain. The second set of data, the adaptation dataA, belongs to the specific task of interest, such as printer manuals or medical diagnoses, and is usually overwhelmingly smaller thanT. Then, the challenge is to modify the SMT system appropriately by taking into consideration bothTandA: on the one hand,Tis should provide robustness in the estimation of the model parametersθ, and on the other handAshould introduce a certain bias towards the specific task.This definition of adaptation is specially appropriate for the Bayesian learning paradigm, where the model parametersθare treated as (hidden) random variables governed by some kind of a priori distribution p(θ). This distribution represents our prior knowledge about what values forθshould be good estimates. Estimating p(θ) by using a sufficiently large collection of dataTallows us to obtain a canonical model with parametersθT, and it can be assumed that such estimation is a robust estimation. As further evidence arrives in form of adaptation dataA, that such estimations are revised so that they reflect the newly arrived data. ConsideringAwithin the Bayesian predictive distribution leads precisely to a scenario in which the decision regarding the output sentence includes a bias towardsA, but is still guided byp(θT)(i.e., the prior distribution givenT). Hence, under the Bayesian predictive adaptation (BPA) framework, the final translation is not computed by considering only the topic-specific data (i.e.,A), which could lead to over-trained estimations ofθ: if the amount of data available is small, the parameter prior p(θ) will compensate this, providing robustness (Duda et al., 2001). However, the effect of this prior knowledge fades when incorporating further evidence, until a point in which the contribution of the parameter prior towards the complete model distribution is negligible. In addition, the Bayesian learning paradigm does not attempt to obtain a single best point estimate ofθ, but rather relies on considering all possible parameter values, allowing uncertainty regarding what the best estimations of such parameters might be. In this paper, we focus on the Bayesian adaptation of the weights of the log-linear combination of features present in state-of-the-art SMT systems. Even though these weights are not very numerous (generally in the range of 10 or 20), providing the system with appropriate estimates for these weights is critical (Clark et al., 2011).The rest of this paper is structured as follows: the related literature is reviewed in Section 2. The formal derivation of Bayesian predictive adaptation for SMT is presented in Section 3. Since the equation obtained is very costly to apply in practise, different sampling strategies are presented in Section 4. The experiments performed are detailed together with their results and the related analysis in Section 5. Finally, conclusions are presented in Section 6.

@&#CONCLUSIONS@&#
In this paper, Bayesian predictive adaptation has been thoroughly analysed for its application to log-linear weight adaptation in statistical machine translation. On the one hand, the theoretical framework for adapting the scaling factors present in most state-of-the-art SMT systems has been developed. On the other hand, experimental results analysing the effectiveness of such adaptation procedures have been reported.Results show that BPA is able to provide consistent improvements in translation quality over the baseline systems, as measured by TER, with as few as 10 adaptation samples, and up to an amount of adaptation data that allows a complete re-estimation of the model parameters. In addition, BPA proves to be more stable than most re-estimation strategies, which rely heavily on the amount of adaptation data. It should be emphasised that an adaptation technique, by nature, is only useful whenever the amount of adaptation data is low, and BPA proves to behave well in such context. Whenever the amount of adaptation data is high, the best thing that one can do is to re-estimate the model parameters from scratch, although such re-estimation is often very costly. From a computational point of view, the Bayesian adaptation technique presented does not imply a significant computational overhead, and most terms can be precomputed in the case of heuristic sampling. Hence, we consider that it could be easily implemented within the decoder itself without a significant increase in computational complexity. Nevertheless, it must be taken into account that the search space explored by a given n-best list is much more restrained than the one that the decoder will take into account. This means that, if BPA is to be implemented within the decoder (instead of by re-scoring n-best lists), the number of n-best considered by BPA in the termp(A∣λ;T)must be sufficiently large. We plan to explore this in future work. Different parameter sampling strategies have been studied when applying BPA to the adaptation of the scaling factors, such as the theoretically sound Markov chain Monte Carlo and an ad-hoc heuristic sampling strategy and the Viterbi approach. It emerges that the heuristic sampling strategy performs slightly worse than MCMC, but is computationally less expensive and most terms can be precomputed. In addition, MCMC yields slightly larger confidence intervals.As future work, we plan to analyse the possibility of adapting the log-linear features of the translation model, and to extend the current BPA implementation so that it is able to deal with more feature-rich SMT models.