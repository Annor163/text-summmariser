@&#MAIN-TITLE@&#
Simplifying words in context. Experiments with two lexical resources in Spanish

@&#HIGHLIGHTS@&#
We developed the first lexical simplification for Spanish.Human-informed evaluation of the system.Comparison of two WSD strategies.Comparison of two lexical resources.Software and dataset made available for testing and verification.

@&#KEYPHRASES@&#
Lexical simplification,Spanish,Text simplification,Evaluation,

@&#ABSTRACT@&#
In this paper we study the effect of different lexical resources for selecting synonyms and strategies for word sense disambiguation in a lexical simplification system for the Spanish language. The resources used for the experiments are the Spanish EuroWordNet, the Spanish Open Thesaurus and a combination of both. As for the synonym selection strategies, we have used both local and global contexts for word sense disambiguation. We present a novel evaluation framework in lexical simplification that takes into account the level of ambiguity of the word to be simplified. The evaluation compares various instances of the lexical simplification system, a gold standard, and a baseline. The paper presents an in-depth qualitative error analysis of the results.

@&#INTRODUCTION@&#
Automatic text simplification is a technology to adapt the content of a text to the specific needs of particular individuals or target populations so that the text becomes more readable and understandable for them. The adapted text will most probably suffer from information loss and a too simplistic or boring style, which is not necessarily a bad thing if the original message can in the end be transmitted to the reader. Text simplification has also been suggested as a potential pre-processing step for making texts easier to handle by generic text processors such as parsers, or to be used in specific information access tasks such as information extraction. But our research is more related to the first objective of making texts more accessible to specific users. This is certainly more challenging than the second use of simplification because the output will necessarily be evaluated with the same yardstick that human written texts are evaluated with. The interest in automatic text simplification has grown in recent years and in spite of the many approaches and techniques proposed, there is still space for improvement. The growing interest in text simplification is evidenced by the number of languages which are targeted by researchers around the globe. Simplification systems and simplification studies do exist at least for English (Chandrasekar et al., 1996; Siddharthan, 2002; Carroll et al., 1998), Brazilian Portuguese (Aluísio and Gasperin, 2010), Japanese (Inui et al., 2003), French (Seretan, 2012), Italian (Dell’Orletta et al., 2011; Barlacchi and Tonelli, 2013), and Basque (Aranzabe et al., 2012). Text simplification, as a general task, is similar to other NLP tasks, such as machine translation, paraphrasing, text summarization or sentence compression. The mixed nature of the task and the specific requirements of each aspect, however, make text simplification not fully comparable to the mentioned NLP problems. While text summarization, for example, tries to select the most relevant information from input texts, text simplification rather concentrates on the elimination of superfluous details, by either deleting them or re-phrasing them in a more general way. Text summarization techniques for simplification can be found in Drndarevic and Saggion (2012a) and Stajner et al. (2013).Our research, concerned with simplification in the Spanish language (Saggion et al., 2011), has produced a text simplification system made up of components for reducing the syntactic complexity of sentences, deleting unnecessary information, rewriting numbers, normalizing reporting verbs, and substituting difficult words by their simpler synonyms (Bott et al., 2012; Drndarevic et al., 2013; Bott and Saggion, 2014). It is this last technology, lexical simplification, which is the focus of the present work. Lexical Simplification aims at replacing difficult words with easier synonyms, while preserving the meaning of the original text segments. Lexical simplification requires the solution of at least two problems: First, the finding of a set of synonymic candidates for a given word, generally relying on a dictionary or a lexical ontology and, second, replacing the target word by a synonym which is easier to read and understand in the given context. For the first task, lexical resources such as WordNet (Miller et al., 1990) can be used. For the second task, different strategies of word sense disambiguation (WSD) and simplicity computation are requited.Even if there is a considerable number of approaches to lexical simplification in different languages, an estimation of how different lexical resources and WSD strategies impact the task have not yet been studied. There is also no previous work which addresses the question in how far the level of ambiguity of a word influences the degree of success in automatic lexical simplification. The goal of this paper is to address these gaps presenting LexSiS (Bott et al., 2012), a system for Spanish lexical simplification which is parametrized in a way it can use different lexical resources which provide word senses and lists of synonyms. Hence, the main contributions of this paper are:•A description of a lexical simplification procedure for the Spanish language.A comparison of the performance of our lexical simplification system with two different lexical resources (Open Thesaurus and EuroWordNet), in addition to a combined version of the two.A comparison of two different strategies for word sense disambiguation, one which only considers the local context of a target word and another which assumes that each target word has only one meaning per text and takes all local contexts for a given target into account.An evaluation that assesses the performance of the system depending on different levels of the ambiguity of target words.A quantitative and qualitative analysis of the results.The rest of the paper is organized as follows: In Section 2 we discuss the related work and the context in which our proposal has to be seen. In Section 3 we present a corpus study which provided insights in the human production of lexical simplifications and guided the development of our system. In Section 4 we describe our system, including the alternative resources it can work with and alternative strategies to perform word sense disambiguation. Section 5 explains the evaluation framework and presents the experimental results, while Section 6 draws some conclusions on the use of different lexical resources and disambiguation methods. In Section 7 we present an in-depth error analysis, which complements the quantitative analysis. Section 8 concludes the paper with a summary of the main results and an outlook on future work.Lexical simplification requires, at least, two things: a way of finding synonyms (or, in some cases, hyperonyms), and a way of measuring lexical complexity (or simplicity). Many approaches to lexical simplification (Carroll et al., 1998; Lal and Ruger, 2002; Burstein et al., 2007) used WordNet in order to find appropriate word substitutions. Bautista et al. (2011) instead use a dictionary of synonyms. As a measure of lexical simplicity most of the cited approaches (Carroll et al., 1998; Lal and Ruger, 2002; Burstein et al., 2007) have relied on word frequency, with the exception of Bautista et al. (2011), who use word length as a predictor for lexical simplicity. Since both word frequency and word length have been shown to correlate to the cognitive effort in reading (Rayner and Duffy, 1986) they are combined in several works (Keskisärkkä, 2012).Concerning the measurement of word complexity, recent studies show that both frequency and length influence readability and understandability at least for specific conditions such as dyslexia. For instance, in an eye-tracking study with 46 participants (23 with dyslexia) (Rello et al., 2013), the authors found that more frequent words improved the readability of text for people with dyslexia and the presence of shorter words improved their comprehension score. However, it is worth mentioning that it is not possible to fully dissociate word length from word frequency in language because both parameters are naturally related. In natural language longer words tend to be less frequent (Jurafsky et al., 2001).As for the overall simplification method, De Belder et al. (2010) apply explicit word sense disambiguation, with a Latent Words Language Model, in order to tackle the problem that many of the target words to be substituted are polysemic. Given a word and a set of possible substitutes, their language model is used to identify which synonyms best fit the context of the word to be replaced. Additionally, a second factor is taken into account: the probability that the word is “easy” – which the authors claim can be calculated based on factors such as word length, word frequency, or information on word use from a psycholinguistic database.In Bautista et al. (2011) a method is proposed where a readability measure (e.g. Flesch Reading Ease, (Flesch, 1949)) is used to guide the selection and replacement of words in the text. Words which are considered complex either in length or in number of syllables are looked up in a lexical database (e.g. WordNet) in order to find a list of possible replacements. A replacement which is shorter or has a lower number of syllables is chosen as a substitute.More recently, the availability of the Simple English Wikipedia (SEW) (Coster and Kauchak, 2011), in combination with the “ordinary” English Wikipedia (EW), made a new generation of text simplification approaches possible, which use primarily machine learning techniques (Zhu et al., 2010; Woodsend et al., 2010; Woodsend and Lapata, 2011; Coster and Kauchak, 2011; Wubben et al., 2012). This includes some new approaches to lexical simplification, which are the most important points of reference for our work.Yatskar et al. (2010) use edit histories from the SEW to identify pairs of possible synonyms and the combination of SEW and EW in order to create a set of lexical substitution rules of the form x→y. Given a pair of edit histories eh1 and eh2, they identify which words from eh1 have been replaced in order to make eh2 “simpler” than eh1. A probabilistic approach is used to model the likelihood of a replacement of word x by word y being made because y is “simpler”. In order to estimate the parameters of the model, various assumptions are made, such as considering that word replacement in SEW is due to simplification or normal edition and that the frequency of editions in SEW is proportional to that of editions in EW. In their evaluation, subjects are presented with substitution pairs (x→y) obtained using different methods including a man-made dictionary of substitutions and random and frequency-based baselines obtained from all possible substitution pairs. Subjects are asked to evaluate how x is when compared to y – more simple, more complex, equally simple/complex, not a synonym or “?”. Although the proposed approach performs worse than the dictionary, it is better than the two baselines.Biran et al. (2011) also rely on the SEW/EW combination without paying attention to the edit history of the SEW. They use context vectors to identify pairs of words which occur in similar contexts in SEW and EW (using cosine similarity). WordNet is used as filter for possible lexical substitution rules (x→y). In their approach a word complexity measure is defined which takes into account word length and word frequency. Given a pair of “synonym” words (w1,w2), their raw frequencies are computed on SEW (freqsew(wi)) and EW (freqew(wi)). A complexity score for each word is then computed as the ratio between its EW and SEW frequencies (i.e.complexity(w)=freqew(w)/freqsew(w)). The final word complexity combines the frequency complexity factor with the length factor in the following formula:final_complexity(w)=complexity(w)*len(w). As an example, for the word pair (canine, dog) the following inequality will hold: final_complexity(canine)>final_complexity(dog). This indicates that canine could be replaced by dog but not vice versa. During text simplification, they use a context-aware method comparing how well the substitute word fits the context, filtering out possibly harmful rule applications which would select word substitutes with the wrong word sense. Their work is interesting because they use a Vector Space Model to capture lexical semantics and, with that, context preferences. In our work we also rely on a formula that combines frequency and length, however for Spanish there is no such a parallel dataset as the SEW and EW.Finally, there is a recent tendency to use statistical machine translation techniques for text simplification (defined as a monolingual machine translation task). Coster and Kauchak (2011) and Specia (2010), drawing on work by Caseli et al. (2009), use standard statistical machine translation machinery for text simplification. In this case, lexical simplification is treated as an implicit part of the machine translation problem. The former uses a dataset extracted from the SEW/EW combination, while the latter is noteworthy for two reasons: first, it is one of the few statistical approaches that targets a language different from English (namely Brazilian Portuguese); and second, it is able to achieve good results, although for a limited range of phenomena, with a surprisingly small bi-dataset of only 4483 sentences.It is worth noting that a lexical simplification task was recently proposed in the SemEval evaluation (Specia et al., 2012) where given a word and a set of possible substitutes, systems have to identify the simpler synonym(s). The task was in fact rather complex as demonstrated by the evaluation results: only one of the participating systems was able to perform better than a baseline.In order to study the problem at hand, we have gathered a corpus of text simplification in Spanish, consisting of 200 informative texts obtained from a Spanish news agency Servimedia. The articles have been classified into four categories: national news, international news, society and culture. We then obtained simplified versions of the said texts, courtesy of the DILES (Discurso y Lengua Española) group of the Autonomous University of Madrid. Simplifications have been created manually, by trained human editors, following easy-to-read guidelines and methodology suggested by Anula (2008, 2009). The corpus has been automatically annotated using the FreeLing toolkit for part-of-speech tagging and named entity recognition (Padró et al., 2010). Furthermore, a text aligning algorithm based on Hidden Markov Models has been developed to obtain sentence-level alignments (Bott and Saggion, 2011). The automatic alignments have then been manually corrected through a graphical editing tool within the GATE framework (Cunningham et al., 2002). Any of the following correlations between original and simplified sentences is possible: one to one, one to many or many to one, as well as cases where there is no correlation (cases of information compression or expansion). Examples of alignments in the corpus are shown in Table 1. The corpus has been used to carry out several studies in text simplification (Rello et al., 2013; Stajner and Saggion, 2013; Bautista and Saggion, 2014) and although it has not been formally released, it can be obtained directly by contacting the first author of this paper.We have conducted an empirical analysis of 40 document pairs from the corpus (590 sentences with 246 and 324 in the original (O) and simplified (S) sets respectively). Our methodology, explained more in depth in Drndarevic and Saggion (2012b), consists in observing lexical changes applied by trained human editors. In addition to that, we carried out quantitative analyses on the word level in order to compare frequency and length distributions in the sets of original and simplified texts. Earlier work on lexical substitution has largely concentrated on word frequency, with occasional interest for word length as well (see Section 2). Our analysis is motivated by the desire to test the relevance of these factors in the text genre we treat and the possibility of their combined influence on the choice of the simplest out of a set of synonyms to replace a difficult input word.We observe a high percentage of named entities (NE) and numerical expressions (NumExp) in our corpus, due to the fact that it is composed of news articles, which naturally abound in this kind of expressions. NEs and NumExps have been discarded from the frequency and length analysis because they are tagged as a whole by FreeLing, and this presents us with two difficulties. First, some expressions, such as 30 millones de dólares (‘30 million dolars’) or Programa Conjunto de las Naciones Unidas sobre el VIH/sida (‘Joint United Nations Programme on HIV/AIDS’), are extremely long words (some exceed 40 characters in length) and are not found in the dictionary; thus, we cannot assign them a frequency index. Second, such expressions are not replaceable by synonyms, but require a different simplification approach.We conduct word length and frequency analysis from two angles. First, we analyze the totality of the words in the parallel corpus. Second, we analyze all lexical units (including multi-word expressions, e.g. complex prepositions) that have been substituted with a simpler synonym. These pairs of lexical substitutions (O–S) have been included in the so-called Lexical Substitution Table (LST) and are used for evaluation purposes. A small sample of the LST can be appreciated in Table 2.Analyzing the total of 10,507 words (6595 and 3912 in the original and simplified sets respectively), we have observed that the most prolific words in both sets are two character words, the majority of which are function words (97.61% in O and 88.97% in S). Two to seven-character words are more abundant in the S set, while longer words are slightly more common in the O set. The S set contains no words with more than 15 characters. Analysis of the pairs in the LST has given us similar results: almost 70% of simple words are shorter than their original counterparts.On the whole, we can conclude that in S texts there is a tendency toward using shorter words of up to ten characters, with one to five-character words taking up 64.10% of the set and one to ten-character words accounting 95.54% of the content.To analyze the frequency, a dictionary based on the Referential Corpus of Contemporary Spanish (Corpus de Referencia del Español Actual, CREA)11http://corpus.rae.es/creanet.html.has been compiled for our research. Every word in the dictionary is assigned a frequency index (FI) from 1 to 6, where 1 represents the lowest frequency and 6 the highest. We use this resource for the corpus analysis because it allows easy categorization of words according to their frequency and elegant presentation and interpretation of results. However, in Section 4 this method is abandoned and relative frequencies are calculated based on occurrences of given words in the training corpus, so as to ensure that words not found in the above mentioned dictionary are also covered.In the parallel corpus, we have documented words with FI 3, 4, 5 and 6, as well as words not found in the dictionary. The latter are assigned FI 0 and termed rare words. This category consists of infrequent words such as intransigencia (‘intransigence’), terms of foreign origin, like e-book, and a small number of multi-word expressions, such as a lo largo de (‘during’). The latter are recognized as multi-word expressions by Freeling, but are not included in the dictionary as such. The ratio of these expressions with respect to total is rather small (1.08% in O and 0.59% in S), so it should not significantly influence the overall results, presented in Table 3.We observe that lower frequency words (FI 3 and FI 0) are around 50% more common in O texts than in S texts, while the latter are somewhat more saturated in highest frequency words. As a general conclusion we observe that simple texts (S set) make use of more frequent words from CREA than their original counterparts (O set).In order to combine the factors of word length and frequency, we have additionally analyzed the length of all the words in the category of rare words. We have found that rare words are largely (72.44% in O and 77.44% in S) made up of seven to nine-character words, followed by longer words of up to twenty characters in O texts (39.42%) and fourteen characters in S texts (29.88%).We are, therefore, lead to believe that there is a degree of connection between the factors of word length and word frequency, and that these are to be combined when scores are assigned to synonym candidates. In Section 4.2 we propose criteria for determining word simplicity exploiting these findings.LexSiS tries to find the best substitution candidate (a word lemma) for every word which has an entry in a lexical database, such as the Spanish Open Thesaurus or the Spanish WordNet. The substitution operates in two steps: first the system tries to find the most appropriate substitution set (a SWN synset or its equivalent in SOT) for a given word, and then it tries to find the best substitution candidate within this set. Here the best candidate is defined as the simplest and most appropriate candidate word for the given context. As for the simplicity criterion, we apply a combination of word length and word frequency, and for the determination of appropriateness we perform a simple form of word sense disambiguation in combination with a filter that blocks words which do not seem to fit in the context.In the first step, we check for each lemma if it has alternatives in the lexical database. If this is the case, we extract a vector from the surrounding 9-word window, as described in Section 4.3. Since each word is a synonym to itself (and might actually be the simplest word among all alternatives), we include the original word lemma in the list of words that represent the word sense. We construct a common vector for each of the word senses listed in the thesaurus by adding all the vectors (resulting from Section 4.3) to the words listed in each word sense. Then, we select the word sense with the lowest cosine distance to the context vector. In the second step, we select the best candidate within the selected word sense, assigning a simplicity score and applying several thresholds in order to eliminate candidates which are either not much simpler or seem to differ too much from the context.As already mentioned, some approaches to lexical simplification make use of WordNet (Miller et al., 1990) in order to measure the semantic similarity between lexical items and to find an appropriate substitute. Spanish is one of the languages represented in EuroWordNet (Vossen, 2004), although its scope is more modest.22The Spanish part of EuroWordNet contains only 50,526 word meanings and 23,370 synsets, in comparison to 187,602 meanings and 94,515 synsets in the English WordNet 1.5.We have tried three lexical resources in LexSiS: the Spanish Open Thesaurus33We used version 2. The Spanish Open Thesaurus is included in the distribution of OpenOfficeOrg and can be found at http://openoffice-es.sourceforge.net/thesaurus/.(SOT), the Spanish EuroWordNet (SWN), and combination of SWN and SOT (SWN+SOT). We describe each of them below.The Spanish Open Thesaurus lists 21,831 target words (lemmas) and provides a list of word senses for each word. Each word sense is, in turn, a list of substitute words (and we shall refer to them as substitution sets hereafter). There is a total of 44,353 such word senses. The substitution candidate words may be contained in more than one of the substitution sets for a target word. The entry in SOT for the word hoja is as in (a).(a)hoja | 3-| acero | espada | puñal | arma_blanca-| bráctea | hojilla | hojuela | bractéola-| lámina | plancha | placa | tabla | rodaja | película | chapa | lata | viruta | loncha | lonja | capa | …The first line of the entry represents the target word and states that there are three different meanings. The three lines that follow list synonyms for the three word meanings (blade, leaf and sheet in English).A second resource we use is the Spanish EuroWordNet. However, for its use with LexSiS we extracted a dictionary from WordNet which represented synset of SWN in the same format as the Spanish Open Thesaurus. We additionally enriched each entry with hyperonyms (e.g. organo_de_una_planta/plant organ in the last sense below) of the word.44We expected the inclusion of hyperonyms to be beneficial. In the Open Thesaurus we observed that hyperonyms were often listed as possible substitutes, alongside synonyms. We also observed that LexSiS in combination with the Open Thesaurus could often produce good simplifications which were hyperonyms. In the error analysis based on the human evaluation (cf. Section 7) we found that, on the other hand, the inclusion of hyperonyms can also lead to over-simplifications in cases where WordNet only provides very general top-level categories as immediate hyperonyms.The SWN entry for hoja is given in (b).55It can be seen in this example that SWN lists many multi-word expressions. At the moment we do not have a module that can detect the same kind of multi-word expressions in the linguistic pre-process, so we have to ignore these entries. In the future we plan to include the treatment of such expressions, which we expect to lead to a better coverage of the system.(b)hoja | 4-| instrumento_cortante-| folio | cuartilla | pliego | hoja_de_papel | papel-| folio | folio | cuartilla | pliego | hoja_de_papel-| follaje | órgano | órgano_de_una_planta | órgano_vegetalThe word hoja is also semantically ambiguous here and can mean blade, leaf or sheet of paper. The sense for sheet of paper is represented by two synsets (second and third lines), a distinction which only captures a subtle difference in meaning.Finally, we are interested in whether a combination of SWN and SOT is able to produce better substitutions since this combination provides more substitution candidates to choose from. For this end we used a union of SWN synsets and SOT substitution sets and let LexSiS choose freely from the alternative lists of synonym words stemming from the two resources. The combined (SOT+SWN) representation for hoja contains all the lines contained in (a) and in (b).According to our discussion in Section 3, we calculate simplicity as a combination of word length and word frequency. The task of combining them, however, is not entirely trivial, considering the underlying distribution of lengths and frequencies. In both cases simplicity is clearly not linearly correlated to the observable values. We know that simplicity monotonically decreases with length and monotonically increases with frequency, but a linear combination of the two factors not necessarily behaves monotonically as well. What we need is a score for simplicity, such that for all possible combinations of word lengths and frequencies of two words,w1andw2,score(w1)>score(w2)iffw1is simpler thanw2. For this reason, we try to approximate the correlation between simplicity and the observable values at least to some degree to a linear distribution.In the case of length, our corpus study showed that a word with length wl is simpler than a word with length wl+1. But the degree to which it is simpler depends on the value of wl. The corresponding difference decreases with longer values for wl. For words with a very high wl value, a difference in simplicity between wl words and wl−1 words is not perceived any more. In our corpus, we found that very long words (10 characters and longer) were always substituted with much shorter words with an average length difference of 4.35 characters. In medium length range (from 5 to 9 characters), the average difference was only 0.36 characters, and very short original words (4 characters or shorter) did not tend to be shortened in the simplified version at all. For this reason we use the following formula, assuming that words which are shorter than 5 characters do not display a real difference in simplicity66The formula for scorewlresulted in quite a stable average value forscorewl(woriginal)−scorewl(wsimplified)for the different values of wl in the range of word lengths from 7 to 12, when tested on the gold standard (cf. Section 5). For longer and shorter words this value was still over-proportionally high or low, respectively, but the difference is less pronounced than with alternative formulas we tried, and much smoother than the direct use of wl counts. In addition, 74% of all observed substitutions fell into that range.:scorewl=wl−4ifwl≥5,0otherwise.In the case of frequency, we make the standard assumption that word frequency is distributed according to Zipf's law (Zipf, 1935); therefore, simplicity must be similarly distributed (when we abstract away from the influence of word length). In order to get a score which associates simplicity to frequency in a way which comes closer to linearity, we calculate the simplicity score for frequency as the logarithm of the frequency countcwfor a given word:scorefreq=logcwNow the combination of the two values isscoresimp=α1scorewl+α2scorefreqwhere α1 and α2 are weights. We determined values for α1 and α2 in the following way: we manually selected 100 good simplification candidates proposed by Open Thesaurus for given contexts taken from a corpus of simple non-parallel text.77It should be noted that for the estimation of these weights we did not use parallel corpus data. The use of parallel data for this purpose is problematic because the parallel corpus we used for the corpus study described in Section 3 only contained one substitution solution for any lexically simplified word, where other alternatives might have been correct, too. Often these human generated alternatives were also not listed in the lexical resources.We only considered cases which were both indisputable synonyms and clearly perceived as being simpler than the original. Then we calculated the average difference between the scores for word length and word frequency between the original lemma and the simplified lemma, and took these averaged differences as being the average contribution of length and frequency to the receivable simplicity of the lemma. This resulted in α1=−0.3988Note that word length is a penalizing factor, since longer words are generally less simple. For this reason, the value for α1 is negative.and α2=1.11.With this formula, the simplicity score is mainly determined by the word's frequency, but there are also examples where the word length determines the final decision of one candidate over another. An example can be seen in (1). The two most frequent substitution candidates from SOT for the word “especie” (“species”) are “tipo” (“type”) and “grupo” (“group”). The frequencies of the two candidates in the training corpus are 2499 and 5437, respectively, while the frequency of “especie” is 427. With this, “grupo” is more frequent than “tipo”, but since the latter is shorter, it receives a final simplicity score of 3.77, as opposed to 3.76 for the former. As a result, the metric prefers “tipo”, which is also the more adequate choice for the given example.(1)Descubren en Valencia un nuevo ESPECIE de pez prehistórico.‘A new SPECIES of prehistoric fish is discovered in Valencia.’In order to measure lexical similarity between words and contexts, we used a Word Vector Space Model (Sahlgren, 2006), a type of Vector Space Model (Salton et al., 1975) which represent words in very local contexts. Word Vector Space Models are a good way of modeling lexical semantics (Turney and Pantel, 2010), since they are robust, conceptually simple and mathematically well defined. The ‘meaning’ of a word is represented as the contexts in which it can be found. A word vector can be extracted from contexts observed in a corpus, where the dimensions represent the words in the context, and the component values represent their frequencies. The context itself can be defined in different ways, such as an n-word window surrounding the target word. Whether two words are similar in meaning can be measured as the cosine distance between the two corresponding vectors. Moreover, vector models are able to represent word senses and to discriminate between them. For example, vectors for word senses can be built as the sum of word vectors which share one meaning. Standard measures, such as cosine distance, can then be used to determine the distances between a given context and different word senses.We trained a vector model on a 8M word corpus of Spanish on-line news. We lemmatized the corpus with FreeLing, version 3 (Padró et al., 2010) and for each lemma type in the corpus we constructed a vector, which represents co-occurring lemmas in a 9-word (actually 9-lemma) window (4 lemmas to the left and to the right). Stop words, such as articles, pronouns and auxiliary verbs and conjunctions were excluded. The vector model has n dimensions, where n is the number of lemmas in the lexicon. The dimensions of each vector in the model (i.e. the vector corresponding to a target lemma) represent the lemmas found in the contexts, and the value for each component represents to number of times the corresponding lemma has been found in the 9-word context. In the same process, we also calculated the absolute and relative frequencies of all lemmas observed in this training corpus.The 20 most dominant dimensions of the vectors for three sample words can be found in Table 4. The dimensions with the strongest extension for “museo” (“museum”) correspond to words like “Prado” (a famous museum in Madrid), “director”, “national” and “Picasso”. “Jurisdicción” (“Jurisdiction”) is a low frequency word, as can be seen from its vector representation and “competencia” (‘competence”) is a possible simplification substitute for it. The words that can be found frequently in the context of “jurisdicción” and “competencia” center around entities that can have competence, such as states (“estado”) and communities (“comunidad”), and the adjectives that express such entities, like national (“nacional”), Spanish (“español”), European (“europeo”) or constitutional (“constitutional”) competence. Jurisdiction and competence can also be recognized, respected or transferred and the corresponding Spanish verbs (“reconocer”, “respetar” and “transferir”) can be found here. The overlap among the context words of the two target concepts is higher than appreciated by looking at only the most dominant dimensions in Table 4. Out of the 84 dimensions that have a non-zero count in the vector for “jurisdicción”, 49 also have a non-zero count in the vector for “competencia”.We implemented two different methods to carry out word sense disambiguation, which we call the local and the global method. The local method only looks at the local context of a target word assuming that the local context provides enough information for disambiguation (Krovetz, 1998), while the global method takes all the occurrences of each target word within a text and constructs a combined representation of the contexts in which they are found, assuming the one sense per discourse hypothesis (Gale et al., 1992).For the local method we extract a vector from the surrounding 9-word window. As already mentioned, we include the original word lemma in the list of words that represent the word sense. We construct a common vector for each of the word senses listed in the thesaurus by adding all the vectors of the words listed in each word sense. Then, we select the word sense with the lowest cosine distance to the context vector. In the second step, we select the best candidate within the selected word sense, assigning a simplicity score to all candidate words and applying several thresholds in order to eliminate candidates which are either not much simpler or seem to differ too much from the context.The global method works largely like the local method, with one difference. We assume that each target word has only one meaning in each text it appears. So, instead of extracting a local context vector for each target instance of a wordw, we extract all of the local vectors forwfound in the text. Then we sum over all of these local vectors, and obtain a global vector forwand compare it to the vectors representing word senses.There are several cases in which we do not want to accept an alternative for a target word, even if it has a high simplicity score. First of all, we do not want to simplify frequent words, even if Open Thesaurus lists them. So we set a cutoff point for frequent words, such that LexSiS does not try to simplify words with a frequency higher than 0.001% (calculated on the corpus we used to train the vector model in Section 4.2). We also discard substitutes where the difference in the simplicity score with respect to the original word is lower than 0.5, because such words can be expected not to be significantly simpler. We achieved this latter value through experimentation by comparing the simplicity scores of pairs of target words and proposed substitutions in which we could not observe a clear difference in word complexity.Many of the alternatives proposed by the lexical resource are in reality not acceptable substitutes. We try to filter out words that do not fit into the context by discarding all candidates whose word vector has a distance with a cosine inferior to 0.013, another value achieved through experimentation in a similar way in which we estimated the frequency threshold: we applied LexSiS to a series of non-parallel input texts and manually annotated those substitutions which clearly did not fit into the context because they did not preserve the original word sense. We also annotated cases of good and clearly meaning-preserving substitutions. Then we set the cosine filter to a value which minimized the cases which did not match the context while still not filtering out too many clearly synonymous substitution candidates.Finally, there are two cases in which the system does not propose a substitute. First, there are cases where none of the substitution candidates have low enough cosine distance to the context vector (with the threshold of 0.013), and second, there are also cases where the highest scoring substitute is the same as the original lemma. In both cases the original word is preserved.

@&#CONCLUSIONS@&#
Lexical simplification, an essential component in a text simplification system, aims at replacing difficult words with easier synonyms, while preserving the meaning of the original text. Most works on lexical simplification have been undertaken for the English language and rely on the WordNet lexical database as a resource to find synonyms. A handful of approaches also rely on the availability of huge parallel or comparable datasets such as the English and the Simple English Wikipedias.In this paper we have presented our approach for the simplification of vocabulary in Spanish texts. The approach relies on a lexical resource to find possible replacements for a word to be simplified, a vector model to represent “word meaning” and perform word sense disambiguation, and a simplicity criteria for choosing the simplest synonym. Here we have compared the effect of using different lexical resources and disambiguation strategies. In particular we have instantiated experiments with the Spanish WordNet and the Spanish Open Thesaurus as lexical resources. Where disambiguation methods are concerned, we have tried local and global disambiguation strategies.The comparison of two different lexical resources shows how far the quality of the resource used influences the quality of the lexical simplifications the system produces. Since Open Thesaurus is an open collaborative effort, the quality of the thesaurus entries is not strongly controlled, a factor which we could see reflected in poorly separated word senses and even missing representation for some senses. We could find differences in the performance depending on the lexical resource used, but it was surprisingly low and not statistically significant. This is a good result because the main bottleneck for the most language dependent part of a lexical simplification system like LexSiS is the availability of lexical resources. Our evaluation suggests that thesauri may be a good substitute for more sophisticated lexical ontologies.Another contribution of this paper is the comparison of two WSD methods: one based on local context and the second global method based on summed local context on the text level. We could show that the global method performs better for the lexical substitution task. The choice of the lexical resource is only one of a list of possible optimizations for the LexSiS system. There are other possibilities we would like to explore in the future, such as the use of TF*IDF weights and the investigation of in how far the size of the window which represents the context influences the system performance. After developing LexSiS, a new lexical resource for Spanish lexical simplification called CASSA (Rello, 2014) has been produced which is based on Google n-grams and Open Thesaurus; we plan to compare LexSiS to CASSA in the near future. Our lexical simplification system could also help to normalize paraphrases to the simplest word choice, which could be useful in plagiarism detection (Barrón-Cedeño et al., 2013).In our future work we will extend the functionalities of our simplification system to cover other languages starting with English. The availability of lexical resources in English and huge textual datasets to model lexical knowledge will facilitate porting LexSiS to other languages.