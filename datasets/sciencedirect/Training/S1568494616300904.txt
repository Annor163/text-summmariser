@&#MAIN-TITLE@&#
Multi layer ELM-RBF for multi-label learning

@&#HIGHLIGHTS@&#
This paper introduces weight uncertainty into ELM-AE and presents WuELM-AE.WuELM-AE extracts useful features that constitute higher level representation.ML-WuELM stacks WuELM-AE to create a multi layer network.Proposed ML-ELM-RBF achieves satisfactory results on multi-label data sets.

@&#KEYPHRASES@&#
Multi-label learning,Extreme learning machine,Radial basis function,k-Means clustering,Extreme learning machine based auto encoder (ELM-AE),

@&#ABSTRACT@&#
Many neural network methods such as ML-RBF and BP-MLL have been used for multi-label classification. Recently, extreme learning machine (ELM) is used as the basic elements to handle multi-label classification problem because of its fast training time. Extreme learning machine based auto encoder (ELM-AE) is a novel method of neural network which can reproduce the input signal as well as auto encoder, but it can not solve the over-fitting problem in neural networks elegantly. Introducing weight uncertainty into ELM-AE, we can treat the input weights as random variables following Gaussian distribution and propose weight uncertainty ELM-AE (WuELM-AE). In this paper, a neural network named multi layer ELM-RBF for multi-label learning (ML-ELM-RBF) is proposed. It is derived from radial basis function for multi-label learning (ML-RBF) and WuELM-AE. ML-ELM-RBF firstly stacks WuELM-AE to create a deep network, and then it conducts clustering analysis on samples features of each possible class to compose the last hidden layer. ML-ELM-RBF has achieved satisfactory results on single-label and multi-label data sets. Experimental results show that WuELM-AE and ML-ELM-RBF are effective learning algorithms.

@&#INTRODUCTION@&#
Recently, multi-label learning has become an important issue in machine learning fields. In multi-label learning, each sample is associated with multiple labels, and the objective is to predict the label sets of unseen samples through analyzing training set. Such learning issue is also suitable for many practical applications. For example, each gene may have more than one function in yeast gene functional analysis [1], each image may be associated with a set of labels in natural scene classification [2], and each document may belong to several predetermined topics in automatic web page categorization [3].Currently, there are many multi-label learning algorithms. Overall, these algorithms can be divided into two categories: problem transformation methods and algorithm adaptation methods [4]. It is problem transformation approach that transforms a multi-label classification problem into one or more single-label regression or classification problems. Binary relevance (BR) [4], classifier chain (CC) [5], label power-set (LP) [6], random k-labelsets (RAKEL) [7], and hierarchy of multi-label classifiers (HOMER) [8] are typical problem transformation methods. Many scholars have adapted the traditional machine learning methods to be able to handle multi-label problem. Schapire and Singer [3] proposed the famous BoosTexter which is based on an improved family of boosting algorithms. Rank-SVM proposed by Elisseeff and Weston [1] was based on a large margin ranking system that shares a lot of common properties with SVMs. Zhang and Zhou [9] presented backpropagation for multi-label learning (BP-MLL), which is derived from the popular backpropagation (BP) algorithm through employing a novel error function capturing the characteristics of multi-label learning. Zhang et al. extended k-nearest neighbor (KNN) to multi-label learning problem, ML-KNN. Ml-KNN employs maximum a posteriori (MAP) principle which is based on statistical information gained from the label sets of these neighboring samples, i.e. the number of neighboring instances belonging to each possible class, to determine labels for the test sample [10]. By performing k-means clustering on samples coming from each possible class and each clustering centers being regarded as the prototype vectors of a basis function in the hidden layer, Zhang [11] extended traditional radial basis function (RBF) to derive radial basis function for multi-label learning (ML-RBF). Kong et al. [12] studied the problem of transductive multi-label learning and propose TRAM which estimated the label sets of the unlabeled instances by utilizing the information from both labeled and unlabeled data. Recently, Kongsorot et al. [13] proposed two approaches for multi-label classification in extreme learning machine named as LSCCA1-ELM and LSCCA1-ELM Ensemble, which were faster than the previous multi-label classification methods.Extreme learning machine (ELM) proposed by Huang et al. was a simple and efficient single layer feed-forward neural networks (SLFNs) learning algorithm [14,15]. ELM output weights can be determined analytically, while the input weights and biases of the hidden layer were randomly generated. ELM can obtain only one optimal solution by one-time learning, and ELM had the advantage of fast training rate and better generalization. Many researchers regarded ELM as a learning method for multi-class classification, regression, and pattern clustering [16–18]. Chen et al. [19] and Lin et al. [20] presented the theoretical analysis of ELM. Optimally Pruned Extreme Learning Machine (OP-ELM) is based on ELM with additional steps to make it more robust and generic [21]. Regularized ELM (RELM) has been developed for classification and regression [22]. Tikhonov-regularized OP-ELM (TROP-ELM) used a cascade of two regularization penalties: first a L1 penalty to rank the neurons of the hidden layer, followed by a L2 penalty on the regression weights for numerical stability and efficient pruning of the neurons [23]. Weighted ELM (WELM) was used for the data with imbalanced class distribution [24]. Bai et al. [25] presented sparse ELM which can reduce storage space and testing time significantly. Kasun et al. [26] presented multi layer extreme learning machine (ML-ELM) which combined ELM with deep learning. ML-ELM stacked extreme learning machines based auto encoder (ELM-AE) to create a multi layer neural network. The outputs of ELM-AE were equal to the inputs. ML-ELM not only possesses good performance but also achieves fast training time.However, ELM-AE can’t solve the over-fitting problem in neural networks elegantly. And weight uncertainty was common method solving the over-fitting problem [27], which treated the weight matrix as random variables, such as each random variable in each layer weight matrix can be assumed as following Gaussian distribution, so that the network can be seen as an ensemble of many sub networks. We can introduce weight uncertainty into ELM-AE and propose weight uncertainty ELM-AE (WuELM-AE), which treats the input weights as random variables following Gaussian distribution. This paper introduces radial basis function for multi-label learning (ML-RBF) into WuELM-AE and presents a neural network named multi layer ELM-RBF for multi-label learning (ML-ELM-RBF), which also takes the advantages of ML-RBF, deep leaning, and ELM. ML-ELM-RBF first stacks WuELM-AE to learn the samples features because the representation capability of WuELM-AE may provide a good solution to ML-ELM-RBF. After that, the last hidden layer of ML-ELM-RBF is formed by conducting clustering analysis on samples features of each possible class. Finally, output weights of ML-ELM-RBF can be analytically calculated using regularized least squares. In order to assess the performance of the proposed algorithms, we compare ML-ELM-RBF with other algorithms on real-world single-label and multi-label learning problems.The rest of this paper is organized as follows. In Section 2, we review the ELM method, and introduce the principle of ELM-AE, then describe the model of ML-RBF. In Section 3, principles of WuELM-AE and models of ML-ELM-RBF are detailed. In Section 4, experimental results prove the feasibilities of WuELM-AE and ML-ELM-RBF. Finally, some conclusions and the intending work are given in the last section.

@&#CONCLUSIONS@&#
In this paper, we introduce weight uncertainty into ELM-AE and present WuELM-AE. WuELM-AE can extract useful features that will constitute higher level representation. ML-WuELM stacks WuELM-AE to create a multi layer network. Although ML-WuELM spends more times than ML-ELM to train a same network, the classification performance of ML-WuELM seems better than the ML-ELM. Next, we introduce ML-RBF into WuELM-AE and presents a neural network named ML-ELM-RBF. Experimental results show that ML-ELM-RBF has achieved satisfactory results in the quality of the classification results and the algorithm running time on single-label and multi-label data sets. However, the number of hidden layers and the number of neurons per layer of ML-ELM-RBF are decided arbitrary with heuristics. And the parameters of this algorithm are determined by the grid searching method, too. Therefore, these two points may be the major topics of the future research.