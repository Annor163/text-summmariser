@&#MAIN-TITLE@&#
Fast image processing for optical metrology utilizing heterogeneous computer architectures

@&#HIGHLIGHTS@&#
Image processing applications can benefit from heterogeneous computing architectures.Using FPGAs, GPUs and CPUs together enables a fast image processing pipeline.FPGA architectures within smart cameras can increase throughput and decrease latency.The image processing pipeline was demonstrated at the example of optical metrology.

@&#KEYPHRASES@&#


@&#ABSTRACT@&#
Industrial image processing tasks, especially in the domain of optical metrology, are becoming more and more complex. While in recent years standard PC components were sufficient to fulfill the requirements, special architectures have to be used to build high-speed image processing systems today. For example, for adaptive optical systems in large scale telescopes, the latency between capturing an image and steering the mirrors is critical for the quality of the resulting images. Commonly, the applied image processing algorithms consist of several tasks with different granularities and complexities. Therefore, we combined the advantages of multicore CPUs, GPUs, and FPGAs to build a heterogeneous image processing pipeline for adaptive optical systems by presenting new architectures and algorithms. Each architecture is well-suited to solve a particular task efficiently, which is proven by a detailed evaluation. With the developed pipeline it is possible to achieve a high throughput and to reduce the latency of the whole steering system significantly.

@&#INTRODUCTION@&#
Image processing is a very complex and time-consuming task. Therefore, a well-considered choice of the applied architectures for the realization of algorithms is necessary. Often, only standard PCs with a serial processing scheme are used for the required tasks in industrial applications. This results in a low throughput and in high latencies.However, image processing tasks are often easily parallelizable [1]. Taking a closer look at current CPUs, parallel computing is a rising market. While in 2006 only 2 cores (e.g. Core2Duo) were placed on a chip die, nowadays up to 8 cores work together as in the Intel Sandy Bridge EP. Additionally, the processors are internally extended via parallel computing units such as Streaming SIMD Extensions (SSE) or Intel HT-Technology. But are these new processor architectures sufficient for high-speed image processing algorithms? – It depends. Modern image processing algorithms, as used in optical metrology, contain numerous individual tasks with different granularities and complexities. While the raw data amount is quite high after capturing an image, the processing steps are very basic. On the contrary, postprocessing an image often requires complex floating point computation on small data packets. Hence, a standard multi-core CPU is not suitable for solving all these image processing tasks efficiently. Therefore, different architectures specialized for specific image processing tasks should be combined in order to achieve a high performance.A further problem is the high communication requirement in image processing systems. For high resolutions and high frame rates standard communication solutions such as Ethernet or USB are not sufficient. Other solutions, for example camera link, can achieve a higher bandwidth but require special and expensive frame grabber cards inside a PC. However, for real-time and high-speed image processing tasks with often more than 500 frames per second (FPS), these solutions are insufficient [2]. The only way to overcome the communication problem is to process or at least to preprocess the captured images close to the image sensor, directly inside the camera system. In many applications, a data reduction after the preprocessing in the image processing operation can be achieved. Thereafter, the data is small enough to allow a sufficiently fast transfer to postprocessing units by low-cost standard communication.Therefore, for fast processing, heterogeneous architectures that are well-suited for the different tasks in the image processing operation are indispensable. A promising approach is the use of Field Programmable Gate Arrays (FPGAs) which allow the implementation of custom hardware architectures. Thereby, especially fine-grained tasks can be implemented as so-called processor arrays. Because of the low complexity of these tasks, a high degree of parallelism can be achieved. It is also possible to use soft-IP cores for FPGAs instead of custom designed hardware components. An example of a soft-IP core is a full-featured CPU offered in a hardware description language like VHDL, to be implemented in an FPGA or ASIC. This shortens the development time for the image processing system and allows a modification for individual requirements. Therefore, FPGAs are highly preferred for use in preprocessing captured images directly inside the camera – with added intelligence, the camera becomes a so-called smart camera. If the granularity of the tasks becomes more complex and, for example, floating point units are required, then GPUs are a better choice. Traditionally, these were designed for rendering geometrical objects, but the high degree of parallelism and the availability of fast floating point units make them valuable devices for accelerating complex tasks in image processing systems.In this paper, the superiority of heterogeneous computer architectures is demonstrated using the example of an image processing pipeline, which consists of custom FPGA architectures, multicore CPUs and finally GPUs for fast processing. As an application, 3D wavefront measurements utilizing the Hartmann–Shack (HS) principle in optical metrology are used. This paper contains 5 sections. Section 2 explains the application of 3D wavefront measurements utilizing an HS sensor. Section 3 presents work related to this paper. In Section 4 our architectures and algorithms are presented, where we build the complete processing pipeline and evaluate its results. Finally, Section 5 entails the conclusion.First of all, the optical metrology application will be presented in more detail. The basic idea is to measure 3D wavefronts utilizing an HS sensor for astronomic or ophthalmological applications. For our example, we used an astronomic application, which is used in a so-called adaptive optical system as shown in Fig. 1(a). The basic idea is to capture an image (6) from an examined object in space (1). Because of turbulences in the Earth’s atmosphere, the image is aberrated (2). This problem is well-known in physics and called wavefront aberration. Hence, in the following we will call the light rays of such an object wavefront instead of image. To correct a wavefront (5), a deformable mirror (DM) (3) can be used, which requires an inverted wavefront profile. This is typically determined with the help of an HS sensor (7), which measures the derivation of the incoming wavefront. A beam splitter (4) is used to divide the wavefront between the sensor and camera (6). After the HS Sensor has captured the actual wavefront, the information is sent to a real-time processing system (8) where the input is processed. After the actual wavefront is reconstructed, its inversion can be calculated to control the DM (9).Now we want to take a closer look inside an HS sensor as shown in Fig. 1(b). An HS sensor consists of an image sensor, such as a CMOS or CCD sensor. Instead of a normal lens, a so-called micro-lens array consisting of several lenses is placed in front of the sensor. The area of the image sensor, which corresponds directly to a micro-lens, is called a sub-aperture. If a planar wavefront reaches the micro-lens array, a spot is projected behind each lens on the image sensor, directly in the centroid of the sub-aperture. This spot is called the reference point. If an aberrated wavefront reaches the micro-lens array, the projected spots move away from the reference points. By measuring the aberration between a reference point and a captured point, the wavefront can be reconstructed. If the wavefront is inverted afterwards, a deformable mirror can be controlled to correct the telescope image.Such an adaptive optical system is also called a closed-loop system because of the control loop between DM, HS sensor and real-time processing. In such a system hard requirements have to be met. A low latency of only a few milliseconds and a high throughput of several hundred FPS is required to work sufficiently. Moreover, the sensor resolution should be at least one mega-pixel with about 50×50 micro-lenses. The largest HS sensors currently available consist of 240×160 micro-lenses with an image sensor resolution of 16 mega pixel. But the problem of large image sensors is that they deliver only 3-4 FPS, which is not suitable for our intended fast image processing.Table 1summarizes the processing tasks to be conducted. The basic idea is to measure the spot distance between the measured position and the reference point, thus determining the aberration. With that information, the measured wavefront can be reconstructed. Therefore, it is necessary to localize the captured spots in an image, which is done in Task 1, spot exploration. As shown in Fig. 1(b), each spot has a local, maximum gray value in the center. Since there is noise present in the image, it is necessary to introduce a threshold for the maximum. This is called spot detection (Task 1a) and works on integer data. Because of the necessity for maximal accuracy for the adaptive optical system, the spot positions are required in sub-pixel precision. This can be achieved because each spot consists of up to 9×9 gray valued pixels each. These values can be used to determine the spot centroids in sub-pixel precision (Task 1b) As a next step follows the so-called spot correlation (Task 2). For measuring the distance between the captured and reference points, it is mandatory to know which spot belongs to which sub-aperture. In most implementations for HS sensors the spots are not allowed to leave their corresponding sub-aperture, limiting the application to slight aberrations only. Task 3 is responsible for the reconstruction of the wavefront based on the measured derivations in each sub-aperture. The reconstruction problem can be traced back to a least squares problem of the measured derivatives. This can be solved with iterative approaches such as the Jacobi method, the Gauss–Seidel (GS) method or the Successive Over Relaxation method (SOR), with increasing complexity and decreasing number of required iterations until convergence. Task 4 delivers the reconstructed wavefront values, which are then visualized. Finally, the deformable mirror is controlled with this information.As described in the section before, each processing task has its own granularity and complexity. Taking a closer look at Task 1, the whole image from the image sensor – normally a few megabytes – has to be processed. After this step, only the spot positions on the sensor are necessary for the successive tasks, requiring at most some hundred kilobytes. Moreover, the processing steps for Task 1 are very simple. For example, no floating point units are necessary and there are only local operations to perform. In contrast to Task 1, data amount of Task 3 is very small, but reconstruction of the wavefront is computationally intensive. Several hundreds of iterations on the whole input with considerable double-precision floating point operations have to be performed.Therefore, we developed a heterogeneous image processing pipeline to solve all tasks as efficiently as possible and to achieve an increased throughput while decreasing the latency.

@&#CONCLUSIONS@&#
