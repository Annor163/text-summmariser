@&#MAIN-TITLE@&#
Model identification and Q-matrix incremental inference in cognitive diagnosis

@&#HIGHLIGHTS@&#
Joint estimate the Q matrix, item parameters and attribute-mastery-pattern.Online estimate the Q matrix, item parameters and attribute-mastery-pattern.The modified online estimation algorithm can further improve the recover ratio.The algorithms can recover the true Q matrix based on a good or a bad input.Relative to the original method, the revised algorithms can be more likely to use.

@&#KEYPHRASES@&#
Cognitive diagnosis,Q-matrix,DINA model,Incremental inference,T matrix,

@&#ABSTRACT@&#
Q-matrix is the intermediary between attribute mastery patterns and responses in cognitive diagnostic assessment; therefore, Q-matrix plays a very important role in the assessment. Currently, lacking of reliable method of inferring and validating the expert-specified Q-matrix is the main problem. Based on the algorithm of Liu et al. (2012), three modified algorithms are proposed. There are two major differences between the algorithm of Liu et al. and the modified algorithms, one is to modify the item parameters from fixed to unfixed, the other is to use an “incremental” Q-matrix estimation, which some items named as “base items” have been correctly prespecified, others (or called as new items or raw items whose attributes have not been specified) need to be specified. The modified algorithms “incrementally” add new items to the “base items” one by one, estimate the item parameters and Q-matrix jointly, rather than estimate all of the items simultaneously which would bring more “noise” to affect the accuracy of estimation. Simulation studies showed that the modified algorithms could get satisfactory results, and the empirical study showed that the proposed algorithms could offer useful information about the Q-matrix specification.

@&#INTRODUCTION@&#
Based on responses, cognitive diagnostic assessment (CDA) infers the attribute mastery patterns (AMPs) of examinees by means of cognitive diagnostic models (CDMs) [1,2]. However, the responses are observable while the AMPs are potentially and unobservable. In the diagnostic process, the Q-matrix plays an important role in test development in that it embodies the attribute blueprint or cognitive specifications for test construction [3]. Many CDMs were built around the Q-matrix, such as DINA (Deterministic Input Noisy “AND” gate) model [4,5], DINO (Deterministic Input Noisy “OR” gate) model [6], NIDA (Noisy Input Deterministic output “AND” gate) model [7], RSM (Rule Space Model) model [8–10], AHM (Attribute Hierarchy Method) model [3,11], G-DINA (generalized deterministic inputs, noisy “and” gate) model [12] (de la Torre, 20011), etc. An integrative review about some important CDMs can be found in the book wrote by Rupp, Templin and Henson [13], and the book contains a comprehensive summary of many classical CDMs and their recent developments.Typically, Q-matrix is specified by field experts based on their experiences and domain knowledge. However, due to the different experience of experts, as well as the differences of their awareness and understanding towards an identical test knowledge, the built Q-matrixes are not exactly the same and usually there are disagreements. Until today, the Q-matrix of the famous “fraction Subtraction data” [14] is still highly controversial [15,16]. Some researchers have conducted extensive studies on Q-matrix theory. A successful skills diagnosis critically depends on high quality Q-matrix development [17]. Having a correctly specified Q-matrix is crucial for parameter estimation and for the identification of underlying attributes of examinees [18], and a misspecified Q-matrix may lead to substantial lack of fit and, consequently, erroneous attribute identification [18,19].In this case, it is imperative to study more efficient and objective methods to specify the Q-matrix, because Q-matrix will seriously hamper the implementation of CDA in large-scale and affect the validity. Unfortunately, this area has not attracted enough attention and only a few studies such as de la Torre [26] proposed an empirical method to validate the Q-matrix, and the method needs to set a cut point to determine the appropriate item-vector, but this may be a problem in real conditions; DeCarlo [15,16] recognized the uncertainty in the Q-matrix via a Bayesian extension of the DINA model, and his method can be used to validate the partial elements of item-vectors, not the whole item vector; Liu et al. [19,20] proposed a Q-matrix inference algorithm, put forward the relevant theoretical foundation and inferred the Q-matrix from responses. Liu et al. algorithm is an objective definition approach of Q-matrix. On the one hand, the algorithm of Liu et al. [19,20] was based on a good initial Q matrix, on the other hand, it also needed to know item parameters at advance, and its application was limited by these two assumptions. Chiu [21] introduced a Q-matrix refinement method based on the nonparametric classification method (namely based on hamming distance), and Chiu’s method relied heavily on item numbers as well as the test blueprint; Huo and de la Torre [22] was targeted to extend the δ2-based method to derive the Q-matrix of the subsequent test form, and this method also needed to set a cut point to determine the appropriate item-vector; Xiang [23] considered the elements of Q-matrix as continuous variables and studied a nonlinear penalized estimation of Q-matrix, and then set a cut point to convert to 0 and 1. Relative to the method of Liu et al., Xiang’s estimation method has no advantages in terms of accuracy. Chung [24] estimated the Q-matrix for cognitive diagnosis models in a Bayesian framework, and Chung’s method was an exploratory method. From other perspectives, such as mathematics and biology, Xiang et al. [25] considered the Markov chain inversion approach to identify the Q-matrix (transition rates of ion channels), but their method could not be applied to CDA.The algorithm of Liu et al. assumed that there was already a prespecified Q-matrix at hand, which might have a few errors such as 3 error-specified items in all the 20 items, while the remaining 17 items were correctly specified. Liu et al. [19] considered when the attributes numbers were 3, 4 and 5, the fixed item parameters (slip and guessing parameter) was 0.2, 3 error items in all the 20 items, the algorithm had a high probability to recover the correct Q-matrix, when the number of examinee was 2000 or more, the successful recovery rate was 100%, when the number was 1000, the successful recovery rate was greater than 98%, even if the number was 500, the algorithm was also very likely to recover the correct Q-matrix.On the one hand, the algorithm of Liu et al. considers item parameters as fixed values and this assumption may not hold in practice. Therefore, the authors in this article consider modifying the algorithm of Liu et al., removing the assumption and estimating item parameters and Q-matrix simultaneously, so that the modified algorithms are more suitable for practical applications; on the other hand, the algorithm of Liu et al. assumes that most of the items in the prespecified Q-matrix have been correctly defined, which also may limit the application of the algorithm. Therefore, based on the algorithm of Liu et al., this study considers only a few items (notates asQBase) are correctly defined, and more items (notates asQRaw) are not specified or prespecified without certainty to their validity, and this will reduce the preconditions. The modified algorithms are based on theQBase, add one item (notated asQOne,QOnebelongs toQRaw) toQBaseeach time and then jointly estimate the item parameters and theQOne. Repeat this process until all items inQRaware specified. This incremental estimation and inference process is very similar to “Q-matrix online calibration”, and improves the recovery ratio of Q-matrix.This article is organized as follows: the section of theoretical background is a presentation of the notation, estimation procedures and the corresponding algorithms. The section on simulation includes simulation studies to assess the performance of the proposed estimation methods. The empirical study is conducted to investigate the performance of the proposed algorithm based on “fraction subtraction data”. Some discussions are given in the last section.Suppose a situation that N examinees are participating in a CDA consisting of J items and K attributes. The paper is based on DINA model and the algorithm of Liu et al. It may be noticed that the proposed algorithms are in fact generic in the sense that it is implementable to a large class of CDMs besides the DINA model. The concepts concerned, DINA model and Liu et al. algorithm are introduced first, and the notations and representations remain the same as those in the paper of Liu et al. [19] for the convenience of readers.Attribute: In CDA, the attribute usually refers to knowledge, skill or trait required to solve the item or problem correctly and it is a fine-grained description of knowledge mastery status.Attribute Mastery Pattern: Use a vectorα=(α1,…αk)Γto depict the knowledge mastery status, for each k,αkis a binary variable taking 0 or 1 and superscript Γ denotes transpose.αk=0or 1 indicates the absence or presence of the kth attribute.Q-matrix: Q-matrix is used to describe the link among the items and the attributes which is aJ×Kbinary matrix. Each row (also known as a row vector) specifies an item vector, each elementqjkis a binary variable taking 0 or 1 where 1 means item j testing the kth attribute while 0 means not.p vector: p vector describes the distribution of the attribute mastery pattern,p=(pα:α∈{0,1}K),pα∈[0,1]and∑αpα=1.Response to items: R vectorR=(R1,…RJ)Γis used to denote the response to J items, where, for each j,Rjis a binary variable taking 0 or 1 and superscript Γ denotes transpose.Rj=1means a positive response to item j,Rj=0means a negative response to item j.Ideal response: letηj(α,Q)denotes the ideal response which indicates whether an examinee with AMPαpossesses all the attributes required by item j. The ideal response assumes a value of 1 if the item-attribute relationship is specified by matrix Q, examineeαpossesses all the skills required for item j, and a value of 0 if the examinee lacks at least one of the required attributes.DINA model is a parsimonious model. For each item, it has only two parameters: guessing parameter and slip parameter [4]. The slip parameter is the probability that an examinee (with AMPα) responds negatively to an item with the ideal response to that itemηj(α,Q)=1; similarly, the guessing parameter refers to the probability that an examinee responds positively with the ideal responseηj(α,Q)=0. The calculation ofηj(α,Q)is(1)ηj(α,Q)=∏k=1KαkqjkFurther, matrix Q specifies the item-attribute relationships, the guessing parameter of item j isgjand the slip parameter issj, the AMP of the respondent isα, the positive probability of the respondent to item j can be calculated by the formula(2)P(Rj=1|Q,α,sj,gj)=(1-sj)ηj(α,Q)gj1-ηj(α,Q)In addition, under the condition of a givenα,(R1,…,RJ)are independent from each other. For more convenient to representation, we use the complement of the slip parametercj=1-sj, so (2) could be expressed as(3)P(Rj=1|Q,α,sj,gj)=cjηj(α,Q)gj1-ηj(α,Q)Throughout this article,Qcis used to denote the correct matrix that generates the data andQ′denote a generic J by K matrix with binary entries. When the Q-matrix, item parameters and p vector are given, the distribution of the response vector R would be(4)P(R|Q′,p,c,g)=∑αpα∏j=1JP(Rj|Q′,α,c,g)This is the expected distribution of the response vector R, and the empirical (observed) distribution is(5)P̂(R)=1N∑i=1NI(Ri=R)If the Q-matrix and the other parameters,(Q′,p,c,g), are correctly specified, the empirical distribution in (5) would be eventually converged to (4) as the sample size (the number of respondents) becomes large. The estimator is then constructed based on this observation.βvector:βvector describes the distribution of positive response numbers of all possible item combinations. For making it easier to understand, the calculation ofβvector is(6)β=P(R1=1|Q′,p,c,g)⋮P(RJ=1|Q′,p,c,g)P(R1=1,R2=1|Q′,p,c,g)⋮T-matrix: Liu et al. [19] introduced the T-matrix to infer the Q-matrix from response data. The T-matrix is central to the construction of the estimator, and it describes the relationship between item response distribution and AMPs’ distribution. For more detailed description, please see Liu et al. [19]. We only provide an illustrate example to introduce the process of constructing a T-matrix.Consider two attributesα1andα2in a test, the population is naturally divided into four strata. The corresponding contingency table of attributes would beα201α10p00p011p10p11Vectorp=(p00,p10,p01,p11)Γcontains all the corresponding probabilities in this particular order.p00is the ratio of the respondents who master neither ofα1andα2in the test population.p10is the ratio of respondents who masterα1but notα2. Consider three items in the test and admit the following Q-matrix.(7)Q=100111To simplify the discussion, consider thatcj=1andgj=0,j=1,2,3, namely there is no chance of slip or guessing. Thus, the response R is completely determined by the AMPα. Under this simplified situation, if the Q-matrix is correctly specified, the following identities are obtained:p10+p11=N1/Np01+p11=N2/Np11=N3/NwhereNj=∑r=1NI(Rrj=1)is the total number-correct response to item j. The corresponding T-matrix andβ-vector are created as follows:(8)(9)β=(N1/N,N2/N,N3/N,N1∧2/N,N1∧3/N,N2∧3/N,N1∧2∧3/N)ΓNj1∧j2=∑r=1NI(Rrj1=1∧Rrj2=1)is the total number-correct response to itemj1andj2simultaneously. Under these considerations, the following identities are obtained:(10)Tc,g(Q)p=βIn (10), vectorTandβare calculated in accordance with the response data and Q-matrix, and vector p is obtained by experts from historical data or empirical information. If the specified Q-matrix isQ′which contains errors, the equation (10) would not hold due to the difference between the left and right. Therefore, the objective function of inferring Q-matrix is(11)Sc,g,p=|Tc,g,p(Q′)p-β||·|refers to the Euclidean distance. If all the parameters are correctly specified, it is expected thatSc,g,p→0asN→∞, a natural estimator of the Q-matrix would be(12)Q̂=arginfQ′Sc,g,p(Q′)When parameters(c,g,p)are unknown, the objective functions are considered:(13)S(Q′)=arginfc,g,pSc,g,p(Q′)The minimization is subject to the natural constraints thatcj,gj,pα∈[0,1]and∑αpα=1. To obtain the Q-matrixQ, we only to findQ′which could minimize objective functionS. For more detailed description and related proofs, please refer to [20].Based on the algorithm of Liu et al., algorithm 1 relaxes the constraint of fixing item parameters and jointly estimates the item parameters and the Q-matrix. As mentioned above, notate the correct Q matrix asQc, a generalJbyKbinary matrix asQ′, letUj(Q′)be the set ofJ×Kmatrices that are identical toQ′except for the jth row (item). Then the algorithm is described as follows:Choose a starting pointQ(0)=Q0and j=1. For the mth iteration, given the matrix from the previous iterationQ(m-1), the following step are performed:(1)According to the starting pointQ(m-1), response data, then use EM algorithm [3] to estimate item parametersc⌢(Q(m-1))andĝ(Q(m-1)), calculateSc⌢(Q(m-1)),g⌢(Q(m-1))(Q(m-1)).ConsiderQ′∈Uj(Q(m-1)), response data, use EM algorithm to estimate item parametersc⌢(Q′)andg⌢(Q′), calculateSc⌢(Q′),g⌢(Q′)(Q′), and let(14)Qj=arginfQ′∈Uj(Q(m-1))Sc⌢(Q′),g⌢(Q′)(Q′)Letj∗=arginfjS(Qj).LetQ(m)=Qj∗and j=j+1, repeat step 2 to 4 until j=J.Repeat steps 1 to 4 untilQ(m)=Q(m-1).The algorithm considers updating one of the j items at each step. In particular, if the jth item is updated, the initial Q-matrix for the next step would be theQj. The optimization (Eq. (14)) consists of evaluating the functionSup to2K-1times, and estimating item parameters by using EM algorithm up to2K-1times. Thus, the total computation complexity of each iteration isJ×(2K-1)evaluations ofSand EM algorithm. Fig. A.1 is the flow chart of the mth iteration of Algorithm 1.Remark 1In algorithm 1, the quality of the starting pointQ0will affect the recovery ratio. Generally, most of the items inQ0are correctly specified and only a few items are error-specified. IfQ0contains too much error-specified items, the successful recovery ratio will decline quickly.Due to the drawback of the algorithm 1, if only a few items are correctly specified or only the specification of a small part of items are certain, consider modifying the algorithm 1 and adopting an incrementally joint estimation of item parameters and Q-matrix.Suppose there are only a few correctly specified items, and name these items as “base items” (notated asQBase) and those items need to be specified or defined as “raw items” (notated asQRaw).Firstly, Choose a starting pointQ(0)=Q0, the starting point of algorithm 2 is differ from algorithm 1, in algorithm 1, the starting pointQ(0)is generally provided by experts, there are only a few error-specified items, if any; but in algorithm 2, only a few items are correctly specified and more items are not specified or not certain. Select the first item fromQRaw, and notate the item asQOne, mergeQOnewithQRaw, and delete theQOnefromQRaw, and then the starting point would beQ(0)=QOneQBase.The algorithm 2 is described as follows:For each iteration m, given the matrix from the previous iterationQ(m-1),U1(Q(m-1))is the set of binary matrices that are identical toQ(m-1)except for the 1st row (item), the following step are performed:(1)According toQ′∈U1(Q(m-1))and response data, use EM algorithm to estimate item parametersc⌢(Q′)andg⌢(Q′), calculateSc⌢(Q′),g⌢(Q′)(Q′), and let(15)Qj=arginfQ′∈U1(Q(m-1))Sc⌢(Q′),g⌢(Q′)(Q′)Letj∗=arginfjS(Qj).Delete the first item inQRaw.Select the first item fromQRaw, LetQ(m)=QOneQj∗.Repeat steps 1 to 4 untilQRawis empty.The algorithm considers updating one of an item inQRawat each step. In particular, if the number of items inQRawisJRaw, one item is updated and merged withQBaseas the newQBaseat each time. The optimization (Eq. (15)) consists of evaluating the functionSup to2K-1times, and estimating item parameter by using EM algorithm up to2K-1times. Thus, the total computation complexity of each iteration isJRaw×(2K-1)evaluations ofSand EM algorithm. Fig. A.2 is the flow chart of Algorithm 2.Remark 2In algorithm 2, the number of the “base items” will affect the recovery ratio, especially when the number of examinees is small or moderate. When the number of “base items” and “examinees” are small, the inference Q-matrix may still contains a few error-specified items, if algorithm 1 is used to estimate the Q-matrix which is inferred by algorithm 2, the successful recovery ratio would be increased.Algorithm 3 is the same as algorithm 2 except for step (6) as follows:(1)Use the Q-matrix inferred by algorithm 2 as the starting point of algorithm 3, and use algorithm 1 to estimate the Q-matrix, thus the output of step (6) is the estimated Q-matrix.Fig. A.3 is the flow chart of Algorithm 3.Remark 3Relative to algorithm 2, algorithm 3 has one more step. When the number of “base items” or the number of examinees is small or moderate, the Q-matrix inferred by algorithm 2 will not be the same as the correct Q-matrix and it may contains several error-specified items. In this situation, using algorithm1 to estimate the overall items can improve the performance of algorithm 2.To evaluate the viability of the three algorithms systematically, and investigate the performance of the proposed algorithms, a simulation study is considered. The data are generated based on DINA model under different settings and a comparison is conducted between the estimated Q-matrix and the correct Q-matrix.Using the same method of Liu et al. [19] to generate data is considered, four 20-item Q-matrix (3,4 5 and 6 attributes for each), the slip parameters and guessing parameters are drawn from an uniform distribution, where the interval is [0.05,0.25]. In addition, the attributes are generated from a uniform distribution for the population of examinees, that is(16)pα=2-KFor each sample size N=500, 1000, 2000 and 4000, 100 data sets are generated under such a setting. To examine the impact on the robustness of the joint algorithm by different numbers of error items, considers 3, 4 and 5 error-specified items out of the total 20 items. For each of the error-specified items, the corresponding row of Q-matrix is sampled uniformly from all the possible K dimensional binary vectors excluding the true vector and the zero vector, namely each of these rows is a uniform sample of2K-2vectors. Thus, it is guaranteed thatQ(0)does not have zero vectors and is different from Q by precisely 3, 4 or 5 items. Fig. 1is the four correct Q-matrixesQ1,Q2,Q3andQ4.Under these settings, givenQ(0), use algorithm 1, 2 and 3 respectively to infer the correct Q matrix, and consider the successful recovery ratio as the evaluation index. The successful recovery ratio is calculated as follows:(17)SRR=DcDwhere D is the total number data sets, which is 100 here, Dc is the number of successful inferences (the estimated Q matrix matches the correct Q matrix) of Q matrix among all data sets.

@&#CONCLUSIONS@&#
