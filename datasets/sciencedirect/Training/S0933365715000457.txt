@&#MAIN-TITLE@&#
A label fusion method using conditional random fields with higher-order potentials: Application to hippocampal segmentation

@&#HIGHLIGHTS@&#
A label fusion method based on minimizing an energy using graph-cuts is presented.The discrete energy is defined on unary, pairwise and higher-order potentials.Two available databases of T1-weighted magnetic resonance images are used.We compare several label fusion methods in the hippocampal automatic segmentation.The scripts for running are available at https://www.nitrc.org/projects/lf_crf/.

@&#KEYPHRASES@&#
Atlas-based segmentation,Image registration,Label fusion,Graph cuts,Global optimization,Hippocampal segmentation,Magnetic resonance imaging,

@&#ABSTRACT@&#
ObjectiveThe objective of this study is to develop a probabilistic modeling framework for segmenting structures of interest from a collection of atlases. We present a label fusion method that is based on minimizing an energy function using graph-cut techniques.Methods and materialsWe use a conditional random field (CRF) model that allows us to efficiently incorporate shape, appearance and context information. This model is characterized by a pseudo-Boolean function defined on unary, pairwise and higher-order potentials. Given a subset of registered atlases in the target image for a particular region of interest (ROI), we first derive an appearance-shape model from these registered atlases. The unary potentials combine an appearance model based on multiple features with a label prior using a weighted voting method. The pairwise terms are defined from a Finsler metric that minimizes the surface of separation between voxels whose labels are different. The higher-order potentials used in our framework are based on the robust Pnmodel proposed by Kohli et al. The higher-order potentials enforce label consistency in cliques; hence, the proposed method can be viewed as an approach to integrate high-level information with images based on low-level features. To evaluate the performance and the robustness of the proposed label fusion method, we employ two available databases of T1-weighted (T1W) magnetic resonance (MR) images of human brains. We compare our approach with other label fusion methods in the automatic hippocampal segmentation from T1W-MR images.ResultsOur label fusion method yields mean Dice coefficients of 0.829 and 0.790 for the two databases used with mean times of approximately 80 and 160s, respectively.ConclusionsWe introduce a new label fusion method based on a CRF model and on ROIs. The CRF model is characterized by a pseudo-Boolean function defined on unary, pairwise and higher-order potentials. The proposed Boolean function is representable by graphs. A globally optimal binary labeling is found using a st-mincut algorithm in each ROI. We show that the proposed approach is very competitive with respect to recently reported methods.

@&#INTRODUCTION@&#
The automatic segmentation of subcortical structures in human brain magnetic resonance (MR) images plays a crucial role in clinical practice. The extraction of biomarkers from MR images is directed at variations in subcortical shapes and their volume measurements [1,2]. This task is very important but difficult to perform, even by hand. Neuroanatomists often develop and use complicated protocols in guiding the manual delineation process [3,4]. Specifically, hippocampal segmentation is an important tool for studying neurodegenerative diseases. Hippocampal volume and shape measures are commonly used as biomarkers for Alzheimer's disease, epilepsy and schizophrenia, among other [5]. This structure is difficult to segment because of its small size, high variability and low contrast in MR images.Many approaches have been proposed, and most segmentation methods use deformable surfaces or atlas-based techniques. Deformable methods tend to explicitly use learned shape variation as a priori information in segmentation. Various methods for representing shapes and their relationships with the appearances have been proposed, such as region growing [6], level-set within a Bayesian framework [7], probabilistic boosting tree [8] or using active shape-appearance models [9]. Atlas-based segmentation has become a standard technique for identifying structures from brain MR images. The atlas-based methods have been demonstrated to outperform other algorithms [10]. The atlas-based approaches are generally based on non-rigid registrations. In the context of this study, an atlas is an image in one modality with its respective labeling (typically generated by manual segmentation) [11]. The atlas-based methods allow a priori knowledge about the appearance and the shape of the anatomical structures to be introduced in a relatively simple way: only a registration method and a number of pre-segmented data sets are required. Barnes et al. [12] proposed registering the most similar atlas from an atlas set to segment the hippocampus. However, segmentations with a single atlas are intrinsically biased toward the shape and the appearance of a subject. Several studies have shown that approaches that incorporate the properties of a group of atlases outperform those that use a single atlas [11,13–16]. The primary benefit of the multi-atlas segmentation approach is that the effect of the errors associated with any single atlas propagation can be reduced in the process of combination. The transferred atlases are used to construct a model for segmenting the target image. This process is often called label fusion. Therefore, there are two steps in the multi-atlas based segmentation: (1) image registration and (2) label fusion. We focus on label fusion in this paper.The label fusion methods have been classified into two categories: global weighted voting and local weighted voting. Most existing label fusion methods are based on global weighted voting, such as majority voting (MV) [13], STAPLE [17] and weighted voting (WV) [18], which are widely used in medical image segmentation. In these approaches, each atlas contributes to the resulting segmentation with the same weight for all of its voxels. It is very sensitive to the registration errors because it does not take into account the relevance of each sample. Recent works have shown that local weighted voting methods outperform global weighted voting methods [18–20].Two approaches can be used to take into account the information of each voxel: (i) the atlases are registered non-rigidly into the target image [11,15,16] and (ii) the atlases are aligned in the target image and a patch-based label fusion method is applied [20–24]. The first approach has the advantage of forcing the resulting segmentation to have a similar global shape to those of expert-labeled structures in the atlases. There is a one-to-one mapping between the target image and each atlas. The label fusion methods of this approach generally calculate the labeling associated with the target image via maximum a posteriori (MAP) estimation [15,19,25]. In contrast to fusing label maps using non-rigid registrations, the second approach is based on the nonlocal mean principle [26]. This second approach increases the number of samples considered during the labeling estimation. The typical assumption of one-to-one mapping in non-rigid registration-based techniques is relaxed through the use of local search windows. However, the labeling is local and independent, without global constraints. In this article, we focus on the label fusion methods that use non-rigid registrations. We leave the patch-based labeling methods for future studies.The new segmentation algorithms attempt to integrate high-level information with image-based low-level features. At the low level, the appearance of an image patch leads to ambiguities in its labels. For example, in the case of the hippocampal head, the appearance of this structure is convoluted and blends with the amygdala. To overcome these ambiguities, it is necessary to incorporate extra information, such as a priori shape information and contextual information. In medical images, context plays a very important role because the anatomical structures are mostly constrained to relatively fixed positions. From the Bayesian perspective, context information is carried in the joint multivariate statistics in the posterior probability, which is often decomposed into likelihood and prior. In image processing, likelihood and prior often correspond to appearance and shape, respectively.Brain images present different structures of interest to be segmented. A region-wise approach is more appropriate [27], which can be achieved by dividing the image into multiple anatomically meaningful regions [28]. Therefore, our task is to segment a given 3D target image into K anatomical structures, where K is fixed. We assume that it is possible to define a region of interest (ROI) such that its voxels only belong to a k-structure or to the background. Partitioning the problem into ROIs improves the results of registrations and segmentations. Indeed, the multi-atlas approaches have greater accuracy when the registrations are only made near the object of interest and not in the entire image [28]. Furthermore, these approaches convert the complex multi-label problem into feasible binary segmentation problems. For each ROI, a segmentation is denoted as Sk, and the optimal solutionSk*can be obtained by the following Bayesian framework:Sk*=argmaxSkp(Ik|Sk;Θk)p(Sk;Θk)where Ikis the target image in the k-ROI and p(Ik|Sk;Θk) and p(Sk;Θk) define the image likelihood and the shape prior of the ROI, respectively. The classification model parameters for the k-ROI are denoted by Θk. In general, either a generative or discriminative model is used for the image likelihood, whereas the shape models use the transferred labels from the registered atlases combined with simple geometric constraints. In terms of appearance, generative models have explicit model parameters and are able to capture the global variability, but they often have simplified assumptions, which limits their ability to model inhomogeneous patterns. By contrast, in a discriminative appearance model, there are no explicit parameters to estimate. Discriminative models are able to combine many of the local statistics, which are insensitive to complex and inhomogeneous texture patterns [8]. However, these models have difficulty in taking the regional information into account. For this reason, discriminative appearance models are combined with shape prior models [8,29,30].Considerable research effort has been devoted to developing efficient algorithms for estimating the MAP solution. The simplest approach is to treat the segmentation as independent voxels and apply the standard classification algorithms. This approach is straightforward, but it loses the important interdependency information. The other extreme of the solution is to treat each instance of L as a single label and estimate its posterior probability. This implementation is infeasible because the space of the output labels grows exponentially with the size of the image. Conditional random fields (CRF) [31] have been widely used to model the correlations of the structured labels. The use of a CRF allows appearance, shape and context to be incorporated in a single unified model, although there are also other alternative approaches [29,32,33].However, CRF models are typically defined on the basis of a fixed neighborhood structure and make unrealistic conditional independence assumptions, thereby limiting their modeling capabilities. Segmentations using only unary-pairwise potentials tend to over-smooth, and they also have difficulties in capturing global shapes. To overcome these drawbacks, CRF models can be improved through the use of higher-order potentials defined on sets of voxels or cliques [34]. In this study, a CRF model is used for fusing the registered atlases, and this model is characterized by an energy function defined on unary, pairwise and higher-order potentials. The unary potentials of the CRF model are defined as the negative log of the likelihood of a label being assigned to a voxel. It is computed from an appearance model and a label prior. The pairwise edge potentials have the form of a spatial regularizer that minimizes the surface of separation between two different labels [35]. The conventional unary and pairwise cues are coupled with higher-order potentials that are defined on voxel sets generated using textons (a texton is a label given to a voxel that describes the local texture and associated through an appearance vector [36,37]). The higher-order potentials enforce label consistency in image regions, and the proposed method can be considered an approach to integrate high-level information with image-based low-level features.Although the experiments presented in this paper focus on hippocampal segmentation, the proposed concepts are generic and could be incorporated into other modalities and applications. We test different label fusion methods on publicly available MR images of human brains. We show that our approach produces segmentation results that are as good as or better than those of other label fusion methods.The remainder of this paper is organized as follows. In Section 2, the label fusion method is presented. The hippocampal segmentation experiments are described in Section 3. Finally, the discussion and conclusions are presented in Section 4.We present a label fusion method that is based on minimizing a pseudo-Boolean function using graph cuts with information on appearance, shape and context, which are estimated from the registered atlases in the target image. Other authors have previously used this framework [15,25,38,39]. Our label fusion method has the following differences: (a) a generative/discriminative appearance model based on multiple features extracted from each voxel and its neighborhood, (b) a label prior probability is estimated using a weighted voting method [18], (c) a spatial regularizer that minimizes the surface of separation between two different labels [35], and (d) higher-order potentials are used to obtain label consistency in the cliques.Given a ROI in the target image I, a set of N training atlases{Ai}i=1,…,N={Ii,Si}i=1,…,Nare used for the label fusion method, whereIi:Ωi⊂ℕn→ℝ, n=3, are the modality images andSi:Ωi⊂ℕn→{0,1}are the label maps. In the labeled images, voxels that belong to the k-structure are designated by the label S(x)=1, and background voxels are designated by the label S(x)=0. We denote Φi:Ω→Ωito be the spatial mapping from the target image coordinates to the coordinates of the ith atlas. For simplicity, we assume that{Φi}i=1,…,Nwas pre-computed using a pairwise registration procedure. This assumption allows us to simplifyA={S˜i=Si∘Φi,I˜i=Ii∘Φi}i=1,…,Nas the atlases in the coordinates of the target image. We seek to minimize an energy function under the Bayesian formulation, which defines the conditional probability as a discrete random field S with a neighborhood systemEand a clique set{Cb}b=1…,B, where a clique Cb⊂Ω is a connected set of voxels whose labels are conditionally dependent on each other. The neighborhood systemEis the set of edges that connect variables in the random field. The CRF model is defined by the following pseudo-Boolean function:(1)E(S)=∑x∈Ωψx(S(x);θ1(I,A))+∑x,y∈Eψxy(S(x),S(y);θ2(I))+∑b=1BψCb(S(Cb);θC(I,A))where Θ={θ1, θ2, θC} are the model parameters for this ROI. We next define the form of the three potential functions and their parameters.The unary potentialsψx(S(x);θ1(I,A))use the Bayesian formulation, which allows a priori information about the shape and appearance of structures to be segmented to be incorporated. As we experimentally demonstrate, unary potentials are the most powerful terms in the CRF model.We assume that the observed intensities of I are independent random variables. The image likelihoodp(I|S;A)can then be written as a product of the likelihoods of the individual voxels:p(I|S;A)=∏x∈Ωp(I(x)|S(x);A).The following two approaches are compared: (a) a generative appearance model in which a Gaussian quadratic classifier is defined for each voxel, and (b) a discriminative appearance model based on k-nearest neighbor voting. We start by developing the generative model.In general, the intensity distribution is modeled using a mixture of Gaussians [40,41]. Because there is a one-to-one mapping between the target image and each atlas, we alternatively use a multivariate Gaussian distribution for each voxel and for each label [27,42]. A pool of feature candidates are extracted from the training images, such as intensity, gradients, curvatures, and entropies. A feature selection process is applied, and GI(x) denotes the selected feature vector centered at x from I (for further details, see Section 3.1.1). The predicted appearance of a voxel is defined by(2)p(I(x)|l;A)∝1|Σl(x)|1/2·e−12GI(x)−μl(x)TΣl−1(x)GI(x)−μl(x),where l∈{0, 1}, μ is the mean vector, and Σ is the covariance matrix. The effect of sample size on the feature selection has to be considered. The means and covariance matrices are estimated using a variable number of samples #Ql(x), whereQl(x)={i|S˜i(x)=l}. A minimum number of observations is required from each of the two classes to ensure that the classification error is bounded relative to an infinite number of samples. This number depends on the dimension of the feature space. Let d be the dimension of the feature space. We haved≤15min(#Q0(x),#Q1(x))for d≤8 [43]. To obtain the least biased Gaussian parameters, a neighborhood system around the voxel is used to obtain more samples,N(x). The Gaussian parameters are computed fromA:(3)μl(x)=∑y∈N(x)∑i∈Ql(y)GI˜i(y)∑y∈N(x)#Ql(y)and(4)Σl(x)=∑y∈N(x)∑i∈Ql(y)(GI˜i(y)−μl(x))(GI˜i(y)−μl(x))T∑y∈N(x)#Ql(y)−1.Furthermore, d is variable in each voxel. The correlation matrix over the selected features is analyzed for each voxel. It only selects uncorrelated features during runtime.Then, a discriminative appearance model that requires low computational effort is presented. The registered atlas images are convolved using a filter-bank. A set of feature extraction kernels αj(for further details, see Section 3.1.1) is used to produce different feature maps:FI˜i(x)={I˜i(x)*αj(x)}j=1,…,fwhere f is the dimension of this feature vector andFI˜i(x)denotes the resulting feature vector ofI˜iat voxel x associated with the filter-bank{αj(x)}j=1,…,f. In this paper, derivatives of Gaussians are adopted to extract features [36,37]. The responses for all registered atlas image voxels are whitened separately (to provide zero mean and unit covariance). These feature vectors are used to train a k-nearest neighbor (k-NN) appearance model. For computational efficiency, we use the kd-tree algorithm [44] to perform the nearest neighbor search. A kd-tree model is constructed with{FI˜i(x),S˜i(x)}i=1,…,N,x∈Ω. The target image is also convolved and whitened. LetFl={FI˜i(x)/S˜i(x)=l}be the set of feature vectors extracted from the voxels belonging to the registered atlas images and whose labels are l. Let{Fr}r∈Rl(x)⊂Flbe the set whose elements are nearest neighbors to FI(x) and Rl(x) be the set of the indices of the feature vectors with label l that are nearest neighbors to FI(x). The image likelihoods of the individual voxels belonging to the target image are calculated using the following formula:(5)p(I(x)|l;A)∝∑r∈Rl(x)exp−∥FI(x)−Fr∥22The label prior probabilityp(S;A,I)models the joint probability of all voxels that belong to the ROI in a particular label configuration. Instead, we assume that the prior probability that voxel x has label l only depends on its position, the similarity between I andI˜iand the transferred atlas labeled images:p(S;I,A)=∏x∈Ωp(S(x);I,A).This assumption is not realistic, but we encode the correlations of the labels using pairwise and higher-order potentials. For each voxel x and each label l∈{0, 1}, we define:h(S(x)=l;I,A)=∑i∈Ql(x)m(I,I˜i,x)qwherem(I,I˜i,x)is a local or global similarity measure between the target image and the registered atlas image at x and q is an associated gain exponent [18]. The prior probability is defined as(6)p(S(x)=l;I,A)=h(S(x)=l;I,A)∑j∈{0,1}h(S(x)=j;I,A).Image likelihood and label prior terms are combined to define the unary potentialsψx(S(x);θ1(I,A)):ψx(S(x);θ1(I,A))=−logp(I(x)|S(x);A)p(S(x);I,A)p(I(x;A)).Following the work of Boykov and Kolmogorov [35], a smoothness term is added to the energy function. These authors decomposed this term, which is defined from a Finsler metric, into two elements. The first part minimizes the segmentation surface by a Riemannian metric, and the second one takes into account the orientation of the segmentation surface in the metric. We consider two types of Riemannian metrics from the image: (a)D(x)=g(∥∇I(x)∥)I, which is an isotropic metric, and (b)D(x)=g(∥∇I(x)∥)I+(1−g(∥∇I(x)∥))·u·uT, which is anisotropic, whereIis the identity matrix of size n, g(∥∇I(x)∥)=(exp(−∥∇I(x)∥/γ))1/3, γ is estimated by the average of ∥∇I(x)∥, andu=∇I(x)∥∇I(x)∥. The cubic root for the term g() is a mapping between the values in ∥∇I(x)∥ and g(). These pairwise potentials take the form of a contrast-sensitive Potts model,ψxyR(S(x),S(y);θ2(I))=0ifS(x)=S(y),detD(x)(xy→T·D(x)·xy→)2otherwise.The second term uses the flux of a vector fieldv→(x)through the segmentation surface. It is assumed that the neighborhood systemEof the CRF model is symmetric, that the vectorv→(x)can be decomposed into the set of edges belonging toEand that there are no local changes in the vector field. Under these constraints, the flux of the vector field defines the following pairwise potentials:ψxyf(S(x),S(y);θ2(I))=0ifS(x)=S(y),v→(x)·xy→∥xy→∥2ifS(x)=0,S(y)=1,−v→(x)·xy→∥xy→∥2ifS(x)=1,S(y)=0.A new higher-order potential is proposed based on the robust Pnmodel [34], which enforces label consistency softly. This potential uses a clique set from I, which is obtained using an unsupervised segmentation algorithm based on textons [36,37]. The target image is first labeled in textons, and then the cliques are formed by spatially connecting voxels that have equal textons. Not all cliques are equally good; some cliques contain voxels with different labels. The probabilities previously calculated for the unary potentials are used to define the label consistency of each clique. Let C={x1, …, xs|xi∈Ω} be a clique belonging to I. We define the probability that a clique belongs to a label l asp(Sl(C);I,A)=1#C∑xi∈Cp(S(xi)=l|I(xi);I,A).The non-dominant label of a clique and its probability are inferred from:(7)lmin=argminlp(Sl(C);I,A),plmin=p(Slmin(C);I,A).Assuming that all of the voxels belonging to a clique are equiprobable to obtain lmin, a new potential is defined as:(8)ψCS(C);θC(I,A)=0ifS(xi)=1−lmin,∀xi∈C,nlminifnlmin≤plmin#C,γmaxifnlmin≥plmin#C,wherenlminis the number of variables in the clique C whose labels are lminandγmax=plmin#C. This potential increases linearly withnlminup to γmaxas the robust Pnmodel [34]. However, our potential should be approximated to a step function aroundplmin#C, i.e., ifnlmin<plmin#C, then the label consistency cost is close to 0; otherwise, it is γmax(see Fig. 1). The issue is that this potential is not representable by graphs. To overcome this drawback, the voxels are weighted to preserve the consistency of the labeling of the clique. Letωil≥0be a weight that is used to specify the relative importance of label l in xi. Then, these weights are normalized asωil←ωil·#C∑iωil. This step allows an average voxel to not lose its unitary cost if the label changes.Now,nlminis replaced by∑iωilminδlmin(S(xi)), where δl(S(xi)) is the Kronecker delta function that returns 1 if S(xi)=l and 0 otherwise. Inserting the weight of each voxel on the consistency of the labeling in (8), the family of higher-order potentials can be written asψCS(C);θC(I,A)=min∑iωilminδlmin(S(xi)),γmax.This potential family can be viewed as a weighted version of (8). The weights can be used to specify the relative importance of different voxels. These higher-order potentials can be transformed to pairwise potentials through the addition of one auxiliary binary variable (see Fig. 2). In the case of lmin=0, we haveψCS(C);θC(I,A)=minz∈{0,1}γmax+−γmax+∑iωi0δ0(S(xi))z.If lmin=1, thenψCS(C);θC(I,A)=minz∈{0,1}γmax+−γmax+∑iωi1δ1(S(xi))(1−z).We also define the quality of the clique by the formulaG(C)=exp−∑xi∈C∥FI(xi)−ϒ∥22β·#Cwhere FI(xi) is the feature vector for labeling a voxel with a texton, β is an outlier measure of the dispersion of∥FI(xi)−ϒ∥22for all cliques (i.e.,{Cb}b=1…,B), and ϒ is the centroid of the texton that is assigned to the clique C. The function G(C) provides a measure of the compactness of the clique C. Consequently, γmaxandωil(which was normalized) have to be modified asγmax=plmin#C·G(C)ωil←ωil·G(C).A low level of G(C) indicates that the clique has little consistency and that this potential does not have an influence on the labeling of the voxels in C. Conversely, if G(C) is close to one, the voxels of the clique have a very similar appearance and these higher-order potentials will force a voxel subset of the clique to be labeled with the non-dominant label in relation toplmin.The proposed higher-order potential family only affects the cliques whose voxels have the same appearance. Precisely classifying the voxels in these cliques is more difficult. In these cases, the higher-order potentials usep(Slmin(C);I,A)and the weightsωilmin. For example, Fig. 2a shows the proposed graph when the non-dominant label in a clique is ‘0’. For the voxels whose weights tend to be smaller(ωi0→0), these potentials will favor their labelings as ‘0’ in relation top(S0(C);I,A). A clique with noise or that is not homogeneous in appearance makes G(C) tend to 0; thus, its higher-order potential does not affect its labeling.Introducing the three potential functions in (1), the weighting multipliers Λ={λR, λf, λC} are tuned such that the effects of the different model potentials are recombined to obtain the best segmentation results:(9)E(S)=∑x∈Ωψx(S(x);θ1(I,A))+λR∑x,y∈EψxyR(S(x),S(y);θ2R(I))+λf∑x,y∈Eψxyf(S(x),S(y);θ2f(I))+λC∑b=1BψCb(S(Cb);θC(I,A)).To evaluate the performance and the robustness of the proposed label fusion method, we employ two available databases of T1-weighted (T1W) MR images: (i) 18 modified images from the Internet Brain Segmentation Repository (IBSR) [45,46] and (ii) 50 images of epileptic and nonepileptic patients with hippocampal outlines (HFH) [47].The IBSR contains images of healthy patients with expert segmentation of 43 anatomical structures. The voxel size of these images is 0.9375×1.5×0.9375mm3. In contrast, HFH contains a total of 50 images that were randomly divided into 25 images used for the training set and 25 used for the test set. Manual segmentations are only available for the training images. Images were acquired using two MR imaging systems with field strengths of 1.5T and 3.0T; thus, these images have different resolutions (0.78×2×0.78mm3 and 0.39×2×0.39mm3, respectively). Fig. 3shows a coronal view of the T1W MR images together with the manual segmentations of similar brain locations for comparison. The intensity patterns and textures are quite different. The T1W MR images show large variations because not all of these images were acquired using the same scanner.In the pre-processing of the databases, non-brain regions are removed from all structural images. Removing non-brain tissue prior to registration is generally accepted as a means to simplify the inter-subject registration problem and thus increase the quality of the registrations [48,49]. The images are skull-stripped using BET [50]. Then, all images are spatially normalized to a reference atlas using an affine registration. For the IBSR database, CMTK's affine registration tool is used [51]. In HFH, an atlas (HFH_021) is selected as a reference to which all atlases are then co-registered with an affine transformation using FLIRT with 12 degrees of freedom [52].After spatial normalization for both of the databases, a region of interest is defined for each structure studied (left and right hippocampus) as the minimum bounding box containing the structure for all of the training atlases expanded by three voxels along each dimension. The patient image is also processed using a skull-striping filter and an affine transformation into the common reference space. Then, the normalized patient image is cropped around the structures of interest. For each ROI, the atlases are ranked based on their similarity according to the target image using the mutual information (MI) measure [53]. Then, the selected atlases are registered non-rigidly into the ROI of the target image. All non-rigid registrations are computed using Elastix[54], a publicly available package for medical image registration. The non-rigid registration of the images is based on the maximization of MI, in combination with a deformation field parameterized by cubic B-splines [55]. The MI is implemented according to [56], using a joint histogram size of 32×32 and cubic B-spline Parzen windows. A unique resolution is employed using a B-spline control point spacing of 3.0mm in all directions. To optimize the cost function, an iterative stochastic gradient descent optimizer is used [57]. In each iteration, 2000 random samples are used to calculate the derivative of the cost function. A maximum of 500 iterations of the stochastic optimization procedure is used. The above-described settings were determined through trial-and-error experiments on two image pairs. These parameters of the non-rigid registrations are equally applicable to both databases.The atlas-labeled images are modeled using the logarithm of odds (LogOdds) formulation, which is based on the signed distance transform [58]. This representation replaces the labels by the signed distances, which are assumed to be positive inside the structure of interest. We find that the LogOdds model produces more accurate results compared with trilinear interpolation or nearest-neighbor interpolation for transferring the atlas-labeled images [19].Fig. 4shows the relationship between the individual atlases and their performance in segmenting the target images. The DICE coefficient [59] is selected as a measure of the segmentation overlaps. The results are shown as the distributions ofDICE(S˜i,SR), where SRis the ground-truth segmentation of the target image and i is the order of the atlas in the database from the similarity to the target image.After the atlases are ranked and registered, we employ a leave-one-out validation strategy to determinate the number of atlases that are fused to the target image [11]. The number of fused atlases depends on the label fusion method. Section 3.1.2 shows how to determine the number of atlases to fuse. Finally, the registered atlases are fused, the labeling is calculated based on graph cuts, and an inverse affine transformation is applied to return the segmentation into the native space of the target image. Fig. 5shows a flow chart that summarizes the processing of the images.Given the CRF model in (9) and its parameters, the optimal labeling is found by applying the min-cut/max-flow algorithm of [60]. The parameters of the model for a ROI Θ={θ1, θ2, θC} are first learned by piecewise training and then recombined with the weighting multipliers Λ={λR, λf, λC}. The CRF model is only trained for voxels whose labels have uncertainty such that the computational burden is reduced. A voxel is uncertain in its label when the atlas-labeled images are transferred and this voxel receives votes from different classes.In the following subsections, we investigate the effects of different aspects of the model and then present the full quantitative and qualitative results for our approach and for other label fusion methods.Image likelihood: We compare a generative appearance model and a discriminative appearance model. Using the generative model of (2), a pool of candidate features has to be defined. For each T1W MR image, the following features are calculated: intensity, gradients, Laplacians, curvatures and local entropies in different scales. Spatial derivatives are implemented by Gaussian-derivative filters. Some of these features are not invariant in gray level, and thus, an intensity normalization is applied to the registered atlas images using the histogram matching algorithm [61]. Given a set of extracted features from each voxel, a feature selection process is required. Because we use Bayesian classifiers and the result of the assignment is two labels, i.e., binary classification, an estimation of the Bayes error is given by the Bhattcharyya distance. Due to the ability to predict error using the Bhattcharyya distance, it is possible to determine the minimum number of features required for the classification tasks [62]. Then, each proposed subset of features by the Bhattcharyya distance is tested. The subset that maximizes the Dice coefficient using the generative classifiers will be used as the selected feature vector. These features are the intensity, the gradient norm with derivatives of Gaussians at scale 2 and the local entropy using a neighborhood kernel of size 9×3×9. To estimate the statistical parameters of the generative appearance model and because the number of samples for any label is low, a neighborhood systemN(x)is tuned and applied to Eqs. (3) and (4).N(x)is defined by a sphere with center x and radius of 1mm. During runtime, a matrix of correlation coefficients is calculated in each voxel over the selected features. The scalar features, whose correlation coefficients are less than 0.6 in absolute value, are considered independents, and they are used in the unary potentials. Therefore, the dimension of the feature space is variable for each voxel and can be d=3, 2 or 1.Regarding the discriminative model, a 12-dimensional filter bank is applied to the registered atlases, generating a kd-tree model [44]. The registered atlas images are convolved with Gaussians at scales of 1, 2 and 4; derivatives of Gaussians at scales of 2 and 4; and Laplacians of Gaussians at scales of 1, 2 and 4 [37]. Given the 12-D responses of a voxel belonging to the target image, the training vectors in the kd-tree that are nearest are found. These vectors are used to calculate the distances to each label, and then Eq. (5) is applied to obtainp(I(x)|l;A). The amount of training data in the discriminative model is often biased toward the background class. A classifier learned using these data will have a prior preference for this class. To normalize for this bias, we weight each training example by the inverse class frequency. The classifiers trained using this weighting tend to provide better performance [63].Label prior: In the weighted voting method for estimating the label prior probabilities, similarity measures are needed between regions of the target image and each registered atlas image, i.e.,m(I,I˜i,x)in (6). Because a statistical relationship is assumed among the intensities of these images, MI is used as the similarity measure. The gain exponent is set to q=4 [18]. A semi-global strategy is used to calculate the weight for each registered atlas. This strategy is most appropriate when the contrast between neighboring structures is low, as in the case of the hippocampus [18]. A binary mask is used to measure this similarity between the target image and the registered atlases. This mask is constructed by joining all transferred labeled images. A voxel is considered in the binary mask if at least a vote of the foreground class is received.Spatial regularization: In the pairwise potentials and considering 3D grid-graphs with 6 neighborhood systems inE, ∇I(x) is calculated by derivatives of Gaussians at scale 1 and applied to the Riemann potentials. The two types of the image-based Riemannian metrics have been implemented. The anisotropic metrics provided better results than the isotropic one, but the improvements were insignificant. Therefore, we selected the anisotropic metrics in the results to be shown.The vector field, which is used to determine the orientation of the surface between labels, is the same used in the Riemannian metrics, i.e.,v→(x)=∇I(x). The flux requires determining whether the object is dark or bright according to the background. We estimate whether a voxel is dark by the sign of the Laplacian of Gaussian at scale 2, whereas unary potentials are used to infer whether this voxel belongs to the object. The flux term is implemented via edges to the terminals allowed given an arbitrary vector field, and this potential can be submodular [35].Higher-order potentials: The higher-order potentials require a clique set of the target image. The cliques are also obtained from voxels whose labels have uncertainty. For this purpose, we use textons [36,37]. An unsupervised clustering method is performed using the above 12-D responses from registered atlas image voxels. We employ the Euclidean distance k-means clustering algorithm, which can be made faster through the use of an accelerated version with simple patches [64]. This algorithm returns the cluster centroid locations. Each centroid is assigned to a texton. Next, each voxel of the target image is labeled with a texton using the nearest cluster centroid. A clique is formed by spatially connecting voxels that have equal textons. We investigated changes in the number of textons. The number of cliques remained approximately invariant when the number of textons is low. A high number of textons increased the computational cost without resulting in significant improvements. Thus, we used 10 textons in our experiments. Fig. 6shows an example of cliques obtained using a ROI belonging to HFH with 10 textons.The non-dominant label of each clique and its probability are inferred from (7). The weightsωilare obtained using the minimum Euclidean distance between the i-voxel of the clique and voxels whose labels do not have uncertainty, e.g.,ωi1is the minimum distance from i-voxel to another that belongs to the k-structure without uncertainty. Then, these weights are normalizedωil←ωil·#C∑iωilG(C).Finally, the four terms of the CRF model are combined by weighting multipliers Λ={λR, λf, λC}, and they are tuned by Dice evaluation. These multipliers are varied in certain ranges, and their effects are measured from the overlap between the resulting segmentation and the ground truth. The multiplier vector is adjusted to provide the highest Dice coefficient values.The number of fused atlases depends on the label fusion method. We compare five label fusion methods: STAPLE, MV, WV and the two methods that we derive from our proposal (i.e., with the generative or discriminative appearance model). STAPLE estimates the performance of each transferred atlas-labeled image iteratively. STAPLE treats the label fusion as a maximum-likelihood problem and solves it using the expectation–maximization algorithm [17]. In MV, the transferred atlas-labeled images are equally weighted. For each voxel, the label with largest agreement from all registered atlases is assigned as the final label. A natural extension of MV is to improve from simple averaging to adaptive weighted averaging. In [18], various weighting strategies were categorized into two groups: (i) global weighted voting and (ii) local weighted voting. These authors also showed that the global weighted method outperforms the local solution when segmenting low-contrast brain structures. We apply to WV the same parameters with respect to our proposal in label prior (semi-global, MI and q=4).Once the parameters of the label fusion methods are defined, we employ a leave-one-out validation strategy to determinate the number of atlases that are fused to the target image for each label fusion method [11]. In our label fusion methods, we only use the unary potentials in the proposed CRF model. The unary potentials depend on the number of fused atlases. By contrast, the pairwise potentials depend exclusively on the target image, and the higher-order potentials depend on the quality of the unary potentials, the cliques and the target image. Using this procedure, we decouple the determination of the number of the atlases to be fused and the tuning of Λ in our proposal.In all label fusion methods, the atlases are ranked using the MI measure between the target image and each atlas image. The i-first atlases are non-rigidly registered into the target image. Fig. 7shows how the segmentation accuracy varies with the number of fused atlases. Each plotted point shows the average DICE coefficient in segmenting all of the target images for the number of fused atlases. We observe a difference between fusion methods that only use the transferred labeled atlases (STAPLE and MV) and those that employ all information of the registered atlases and the target image (WV and our unary potentials). As previously reported in [18], the STAPLE-based fusion rule does not necessary lead to higher Dice coefficients compared to the majority voting rule. Our unary potentials outperform WV, particularly with the discriminative appearance model. We observe that our appearance models are able to work with the label prior model and improve the segmenting results.From this experiment, we fix the number of the fused atlases for each label fusion method as follows: (a) STAPLE: 5, (b) MV: 5, (c) WV: 15, (d) Generative CRF: 15, and (e) Discriminative CRF: 15.We investigate the segmentation results on the IBSR and HFH databases when the potentials of the proposed CRF model are modified. We compare between the generative and discriminative appearance models combined with the other potentials.Table 1presents the quantitative segmentation results for the generative and discriminative models on the IBSR and HFH databases. Five combinations of the proposed CRF model are evaluated: (a) only with unary potentials, (b) unary and pairwise potentials without the influence of flux, (c) unary and pairwise potentials with the influence of flux, (d) unary and higher potentials and (e) the full CRF model. The weighting multipliers are also reported for each case in the table. There are several conclusions. (1) The unary potentials are the most powerful in the classification task. (2) The pairwise and higher potentials improve the segmentations, making them more accurate and robust. These potentials result in the object contour being more accurately delineated. (3) The pairwise potentials that take into account the orientations of the segmentation surfaces do not result in significant improvements. (4) The performance of the proposed higher-order potentials is comparable to that of the pairwise potentials. (5) The full CRF model provides slight improvements to any of the previous combinations of the proposed CRF model. (6) To further generalize the model, the weighted multipliers Λ are tuned to the same values for both the left and right hippocampus. Note that the values of Λ are similar between the two databases: (i) with the generative appearance model, the values are ΛIBSR={0.5, 0.05, 0.5} and ΛHFH={0.75, 0.025, 0.25}), and (ii) with the discriminative appearance model, the values are ΛIBSR={0.2, 0.025, 0.2} and ΛHFH={0.4, 0.02, 0.6}). The proposed CRF model is shown to be robust to variability in the databases.Fig. 8shows the hippocampal segmentation results for the best, one mean, and the worst subject using the proposed CRF with the discriminative appearance model.

@&#CONCLUSIONS@&#
