@&#MAIN-TITLE@&#
Computational approaches for mining user’s opinions on the Web 2.0

@&#HIGHLIGHTS@&#
We carry out an empirical analysis to determine characteristics of social media channels.User generated content is “noisy” and contains mistakes, emoticons, etc.We evaluate text preprocessing algorithms regarding user generated content.Discussion of improvements to opinion mining process.

@&#KEYPHRASES@&#
Opinion mining,Noisy text,Text preprocessing,User generated content,Data mining,

@&#ABSTRACT@&#
The emerging research area of opinion mining deals with computational methods in order to find, extract and systematically analyze people’s opinions, attitudes and emotions towards certain topics. While providing interesting market research information, the user generated content existing on the Web 2.0 presents numerous challenges regarding systematic analysis, the differences and unique characteristics of the various social media channels being one of them. This article reports on the determination of such particularities, and deduces their impact on text preprocessing and opinion mining algorithms. The effectiveness of different algorithms is evaluated in order to determine their applicability to the various social media channels. Our research shows that text preprocessing algorithms are mandatory for mining opinions on the Web 2.0 and that part of these algorithms are sensitive to errors and mistakes contained in the user generated content.

@&#INTRODUCTION@&#
Opinion mining deals with analyzing people’s opinions, attitudes and emotions towards different brands, companies, products and even individuals (Balahur, 2013; Liu, 2012; Pang & Lee, 2008). Although related research areas to opinion mining such as natural language processing (NLP), information extraction and information retrieval have quite a considerable history, the research on mining people’s opinions has become quite popular in the last couple of years with the rise of the Web 2.0. User generated content on the Social Web can contain a variety of relevant market research information and deeply analyzing and exploiting it leads to more targeted business decisions (Guozheng, Faming, Fang, & Jian, 2008; Liu, 2008).Analyzing opinions on the Social Web is met with a variety of challenges: (i) the “usual” challenges known from natural language processing (such as word sense disambiguation, topic recognition and co-reference resolutions) and (ii) challenges arising from user generated content:•Noisy texts, language variations: User generated texts tend to be less grammatically correct and often use specific characters to express emotions (emoticons), abbreviations and unorthodox capitalization. (Abbasi, Chen, & Salem, 2008; Dey & Haque, 2009). Moreover, social media texts typically assume a higher level of knowledge about the context by the reader than more formal texts (Maynard, Bontcheva, & Rout, 2012).Relevance and boilerplate: When web texts and social media texts are gathered using a web crawler, the gained texts usually contain irrelevant content like advertisements, navigational elements or previews of other articles (Maynard et al., 2012; Petz et al., 2012; Yi & Liu, 2003).Target identification: Search-based approaches have to deal with the problem, that topics of retrieved documents do not necessarily match the mentioned sentiment object (Maynard et al., 2012).Big data challenges: That can be broken into several contexts such as temporal, spatial and spatio-temporal contexts (Derczynski, Yang, et al., 2013; Maynard, Dupplaw, & Hare, 2013).Due to these challenges, research papers usually deal with assumptions and constraints: Many of the approaches to analyze opinions assume linguistically correct texts (Dey & Haque, 2009), others focus on specific social media resources (e.g. Twitter as a basis for opinion mining Bollen, Mao, & Zeng, 2011; Davidov, Tsur, & Rappoport, 2010; Pak & Paroubek, 2010; or newswire text Balahur, Steinberger, van der Goot, Pouliquen, & Kabadjov, 2009; Sayeed, 2011; or Blogs Leshed & Kaye, 2006; Mishne & Glance, 2006; Zhang, Yu, & Meng, 2007). The utilization of text preprocessing steps prior to sentiment analysis approaches is quite important in order to achieve good results.The objectives of this paper are (i) to investigate the differences between social media channels regarding opinion mining and (ii) to evaluate the effectiveness of various text preprocessing algorithms as a subtask of opinion mining in these social media channels. To attain these objectives, we set up the research methodology as follows:(1)Identification of popular approaches and algorithms to carry out text preprocessing as a prior step to sentiment analysis.Identification of differences between social media channels and deduction of impacts on opinion mining and text preprocessing.Evaluation of the effectiveness and properness of several algorithms in order to determine their applicability.The rest of the paper is organized as follows: in the next section we discuss some related work in the field of opinion mining. We then report in Section 3 on the characteristics of user generated content in different social media channels. Section 4 discusses the impacts of these characteristics on some frequently used algorithms and evaluates their performance regarding noisy text.Pang and Lee (2008) and Liu (2012) present a detailed review of opinion mining. Liu defines an opinion as a quintuple (ei, aij, sijkl, hk, tl), where eiis the name of an entity, aijis an aspect of ei, sijklis the sentiment on aspect aijof entity ei, hkis the opinion holder and tlis the time when the opinion is expressed. An entity is the target object of an opinion; it is a product, service, topic, person, or event. The aspects represent parts or attributes of an entity (part-of-relation). The sentiment is positive, negative or neutral or can be expressed with numeric scores (such as star-ratings). The indices i, j, k, l indicate that the items in the definition must correspond to one another. (Liu, 2012; Wilson, Wiebe, & Hoffmann, 2009)There are several main research directions (Kaiser, 2009; Pang & Lee, 2008): (1) Sentiment classification: The main focus of this research direction is the classification of content according to its sentiment about opinion targets; (2) feature-based opinion mining (or aspect-based opinion miningHu & Liu, 2004b; Liu, Hu, & Cheng, 2005) is about analysis of sentiment regarding certain properties of objects (e.g. Hu & Liu, 2004a); (3) comparison-based opinion mining deals with texts in which comparisons of similar objects are made (e.g. Jindal & Liu, 2006a, 2006b). Other research directions focus on multilingual opinion mining (e.g. Banea, Mihalcea, & Wiebe, 2010; Steinberger, Lenkova, Kabadjov, Steinberger, & Goot van der, 2011) and on cross-domain sentiment analysis (e.g. Bollegala, Weir, & Carroll, 2011; Pan, Ni, Sun, Yang, & Chen, 2010).The classification of texts regarding sentiment polarity can be done at three different levels: (1) document level, (2) sentence level and (3) entity and aspect-level. There are several approaches to analyze opinions: (1) corpus-based approaches (e.g. Hatzivassiloglou & Wiebe, 2000; Turney, 2002; Wiebe & Mihalcea, 2006) and dictionary-based/lexicon-based approaches (e.g. Ding, Liu, & Yu, 2008; Hu & Liu, 2004a; Kim & Hovy, 2004; Popescu & Etzioni, 2005; Steinberger et al., 2012), (2) machine learning approaches. These approaches can be categorized as follows:(1)Supervised learning: Supervised learning (“classification”) is a machine learning task of inferring a function from labeled training data, where statistical methods are applied to construct prediction rules. This type of learning is widely used in real-world applications. Typical supervised learning algorithms are Naïve bayes classifiers, maximum entropy, support vector machines (SVM) and K-Nearest neighbor learning, amongst others (Liu, 2008; Zhang, 2010).Unsupervised learning: Unsupervised learning is often used when the user wants to find hidden structures in unlabeled data and is often called “clustering”. A variety of algorithms exist in this subject: k-means, mixture models, hierarchical clustering, etc. (Liu, 2008).Other approaches/algorithms: Since supervised learning needs a large number of labeled data for training and this task is often done manually and therefore is particularly time consuming, some researchers developed partially supervised learning approaches. The tasks include learning from labeled and unlabeled examples (“LU learning”) and learning from positive and unlabeled examples (“PU learning”). Exemplary algorithms include Expectation–Maximization (EM) algorithms, co-training and transductive support vector machines (Liu, 2008). One can identify several other algorithms used in opinion mining. E.g. aspect based opinion mining focuses on feature based approaches (e.g. Hu & Liu, 2004a; Liu et al., 2005; Moghaddam & Ester, 2010), while others apply latent variable models like the hidden Markov model (HMM, e.g. Jin, Ho, & Srihari, 2009; Wong, Bing, & Lam, 2011), conditional random fields (CRF, e.g. Choi & Cardie, 2010; Li et al., 2010), latent semantic association (e.g. Guo, Zhu, Guo, & Su, 2011; Guo, Zhu, Guo, Zahng, & Su, 2009; Hofmann, 2001) up to combinations and variations of existing algorithms (Airoldi, Bai, & Padman, 2006; Choi, Cardie, Riloff, & Patwardhan, 2005; Jakob & Gurevych, 2010; Nakagawa, Inui, & Kurohashi, 2010). One important part of the preprocessing steps in opinion mining is Part-of-Speech (POS) tagging; a variety of algorithms can be applied to implement this task: rule based approaches, Markov model approaches, maximum entropy approaches, etc. The majority of research work has focused on POS tagging in English, although there are research efforts under way to extend POS tagging to other languages as well (Güngör, 2010).Due to the number of different techniques, several researchers experimented with different algorithms and drew comparisons between them: (Chaovalit & Zhou, 2005; Cui, Mittal, & Datar, 2006; Moghaddam & Ester, 2012). Serveral researchers experiment with adaption of machine learning approaches to other languages and domains, e.g. (Boyd-Graber & Resnik, 2010), (Balahur & Turchi, 2013).Only little research work can be identified on preprocessing of noisy texts. The objective of preprocessing is to produce clean texts for further analysis processes. The tasks usually include identifying and correcting spelling errors, eliminating arbitrary sequences of whitespaces between words, detecting sentence boundaries, eliminating arbitrary use of punctuation marks and capitalization and are usually executed in a pipeline. One of the first research efforts in this area is from Kerninghan et al.; the authors describe a software program that corrects spelling mistakes and typos based on a noisy channel model using Bayesian algorithms (Kemighan, Church, & Gale, 1990). Mikheev presents an approach to detect and correct sentence boundary disambiguation, word disambiguation in terms of capitalization and identification of abbreviations (Mikheev, 2002). A couple of authors used machine learning approaches to deal with noisy texts (e.g. Clark, 2003; Gotoh & Renals, 2000; Kiss & Strunk, 2006). The cleaning approaches are seldom evaluated regarding Web texts or user generated texts in social media. Dey/Haque propose a framework for opinion extraction and mining from noisy text; the framework includes sentence boundary detection, improper case correction and context-dependent spelling correction. In order to determine opinions, the framework uses the cleaned text as input to POS tagging, dependency trees and other algorithms (Dey & Haque, 2009). Derczynski et al. focus on POS tagging for tweets and evaluate the performance of several widely used taggers (e.g. Brant’s TnT-tagger, Brill’s tagger TBL, Stanford tagger, etc.) that were trained with WSJ-corpus. Not surprisingly, the authors report a weak performance of these taggers when applied to tweets. The most frequent POS tagging errors result from internet slang words, common misspellings, genre-oriented phrases and unknown words. Based on this error analysis, the authors developed a POS tagger, which achieves significantly better results than the traditional POS taggers (Derczynski, Ritter, et al., 2013). In (Derczynski, Maynard, et al., 2013) the authors investigate the impact of the specific characteristics of tweets on several text preprocessing steps: language identification, tokenization, POS tagging and named entity recognition. All in all, the performance of machine learning methods suffers from noisy texts. Normalization approaches (basic resp. strong normalization) of noisy texts offer only small improvements, but are helpful and have to be further developed.Kessler/Nicolov collected 194 blog entries about cars and digital cameras and calculated some opinion mining relevant statistics (e.g. number of tokens between a sentiment expression and its target; target in the same sentence as their sentiment expression, etc.). (Kessler & Nicolov, 2009) We want to extend these statistics and find differences between social media channels in order to derive impacts on the opinion mining process.

@&#CONCLUSIONS@&#
