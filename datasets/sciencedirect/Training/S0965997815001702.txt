@&#MAIN-TITLE@&#
A novel nature-inspired algorithm for optimization: Virus colony search

@&#HIGHLIGHTS@&#
A new VCS algorithm is introduced to solve the mathematical and engineering optimization.Forty numerical optimization problems and three engineering design problems are tested.The results obtained by VCS are compared with other optimization methods.Obtained results confirm the excellent performance of the VCS method.

@&#KEYPHRASES@&#
Nature-inspired algorithms,Optimization,Virus colony search,Survival strategy,Unconstrained optimization problems,Engineering design problems,

@&#ABSTRACT@&#
This paper presents a new powerful nature-inspired method named Virus Colony Search (VCS). VCS simulates diffusion and infection strategies for the host cells adopted by virus to survive and propagate in the cell environment. With the strategies, the individual in the new algorithm explores and exploits the search space more efficiently. To verify the performance of our VCS, both the unconstrained classic and CEC2014 modern benchmark functions, and constrained engineering design optimization problems are employed. The experimental results, considering both convergence and accuracy simultaneously, demonstrate the effectiveness of VCS for global numerical and engineering optimization problems.

@&#INTRODUCTION@&#
Optimization is the process of searching for the global optima of a problem under a given circumstance. Lots of real-world complex optimization problems have emerged in many scientific fields such as engineering, economics and business which cannot be solved with a reasonable time or accuracy solutions by classical methods [1].Nature provides a rich source such as mechanisms, principles and concepts for designing artificial computational methods to solve such complex optimization problems. In recent years, researchers have developed many nature-inspired (NI) algorithms that mimic the specific phenomena or behaviors of nature for the different optimization problems [2]. For example, genetic algorithm (GA) proposed by Holland [3] represents a fairly abstract model of Darwinian evolution and biological genetics; particle swarm optimization (PSO) conceived by Kennedy and Eberhart [4] mimics social behaviors of bird flocking; ant colony optimization (ACO), proposed by Marco Dorigo [5], is inspired by the foraging behaviors of the ant colonies. Compared with the classical heuristic methods, these algorithms have been confirmed with the excellent performance, especially for optimizing the multimodal, non-differentiable and discrete complex optimization problems. Meanwhile, these NIs have been widely used in many scientific fields such as dynamic optimization [6], IIR filter design [7], image processing [8], mechanical design problems [9], task scheduling [10], data mining applications [11], and many other engineering problems [12–14].Generally, NIs can be divided into three main classes: evolutionary algorithms (EA), swarm intelligence (SI) algorithms and physics-based (PB) algorithms [2]. EAs are inspired by the genetic and evolutionary behaviors of creatures. GA and differential evolution (DE) [15] algorithm can be described as representative algorithms in EAs. The second main class of nature-inspired algorithms is swarm intelligence algorithms. Just like EAs, the SIs are usually inspired by the behaviors of intelligent nature creatures. Different from the EAs, the majority of SIs use genetic rules only and they always take full advantages of each solution in search space to provide better solutions to the problem. Some of the popular SIs are PSO, artificial bee colony (ABC) inspired by the honey-bees food searching behavior [16], cuckoo search (CS) inspired by parasitic bio-interactions in living creatures [17], animal migration optimization (AMO) algorithm inspired by the behavior of the animal migration [18], grey wolf optimizer (GWO) inspired by hunting mechanism of grey wolves [19] and dolphin echolocation algorithm (DEa) inspired by the behaviors of navigation and hunting of dolphin [20]. Physics-based algorithms are the third class of nature-inspired algorithms. Distinct from the EAs and SIs, the PBs are mostly inspired by physical rules in the nature. There are some popular PBs such as charged system search (CSS) [21], ray optimization (RO) [22], colliding bodies optimization (CBO) [23] and gravitational search algorithm (GSA) algorithm [24].These NIs share the same following characteristics: (1) make use of random variables; (2) do not require substantial gradient information; (3) have several parameters need to be fitted for the problem [25]. Each NI algorithm has unique advantages with respect to robustness and performance in noisy environments, in the presence of uncertain parameters, and in different problem spaces [26]. However, according to the ‘‘no free-lunch’’ theorem, there is no one NI algorithm to optimally solve all optimizing problems [27]. Therefore, new high-performance NI algorithms are continuously needed to solve specific optimizing problems.This paper introduces a new simple and powerful NI algorithm called Virus Colony Search (VCS).This algorithm simulates the virus infection and diffusion strategies for the host cells to survive and propagate in the cell environment. The main contribution of this paper is the presentation of a new algorithm with further insight into solving optimization problems. The proposed algorithm can achieve a solution that has the least (or at most, a small) error compared with the globally optimum solution within a minimal number of iterations, thus offering an improvement in terms of accuracy, convergence rate and simplicity of the operations.Preliminary studies show that the NIs such as CMA-ES, ABC, CS, GSA, CoBiDE, GWO, AMO, SOS and other well-known optimization algorithms are very promising and could be used as the compared algorithms for evaluating VCS's performance in solving unconstrained global optimization problems. For constrained problems, VCS is compared with the results in the previous lectures of different algorithms.The rest of this paper is organized as follows. Section 2 introduces the survival strategies of viruses. The detailed introduction of VCS is described in Section 3. The experiment sets and statistical results for different tests are shown in Section 4. Finally, Section 5 concludes the work and suggests some directions for the future.The proposed VCS algorithm simulates the infection and diffusion behaviors between viruses and host cells in the cell environment. In order to live and propagate, virus has to live on a host cell by diffusion and infection behaviors. Meanwhile, evolution behavior always occurs during the process of adapting to a changing cell environment. The following part in this section introduces the survival strategies of virus colony and describes the roles of virus, host cell and immune system.A virus is a small infectious agent that survives only inside the living cells of other organisms. Viruses are considered by some researchers to be a life form, because they carry genetic material, reproduce, and evolve through natural selection [28]. The origins of viruses in the evolutionary history of life are unclear, but their growth generally can be summarized as two processes in Fig. 1:viruses diffusion and host cells infection. Meanwhile, the evolution and immune response of the host immune system are also occurred along with above two processes.(1)Viruses diffusion: viruses randomly search for host cells to infect so as to absorb essential elements for growth (Diffusion process in Fig. 1). In this process, the random walk method of Gaussian walks [29] can be used to describe this behavior.Host cells infection: When found a host cell, virus infects and destroys it (Infection process in Fig. 1). Based on the essential elements of the host cell, virus can survive and reproduce itself until the death of the host cell. In other words, the host cell is “mutated” to be a virus with the process of reproduction. Inspired by evolutionary strategies with covariance matrix adaptation (CMA-ES) algorithm [30], this process is realized according to the CMA-ES operation to generate the new viruses.Immune response: Because the host immune system plays a major role to protect the host cell from infection or destruction, the virus will be selected during the growing process in order to survive. In other words, viruses that have better ability will be reserved for the next generation; otherwise they will be killed by the host immune system. It means evolution chance of the viruses which have not obtained a good ability will increase.According to the previous section, our Virus Colony Search algorithm employs three strategies including: Gaussian walks method for viruses diffusion, CMA-ES operation for host cells infection and evolution strategy for immune response. Actually, the first strategy is used to improve the exploitation properties. The second strategy is mainly to enhance the performance of exploration. For the third strategy, it is proposed to make full use of worse individuals so as to improve the search efficiency. Loosely speaking, VCS uses five simple rules to find a solution.1.Two different groups: virus colony Vpopand host cell colony Hpop, are used in the VCS.Each virus in the diffusion process creates a new random individual.Each virus infects one host cell.The reproduction of each virus is based on destroying the host cell to obtain nutrients.According to the protection of the host immune system, only some of the best viruses remain in each generation, and the rest of the viruses are evolved so as to survive.In general, viruses exist in some certain mediums such as air, water or circulation systems of some organisms. The random-walking phenomenon occurred in some mediums is the main activity of a virus in this stage until it searched a host cell. Gaussian random walk, which has a promising performance in finding global optimum [31], is often used as the diffusion rule to describe the phenomenon of random diffusion [32]. For improving the exploitation performance and avoiding local optimal, this method is selected and the viruses diffusion is designed as:(1)Vpopi′=Gaussian(Gbestg,τ)+(r1·Gbestg−r2·Vpopi)where i is the random index selected from [1,2,3,…, N] and N is the population size.Gbestgis the best solution of the generation g. r1and r2 are random values selected from [0, 1]. For the Gaussian parameters, the standard deviation ι is calculated bylog(g)/g·(Vpopi−Gbestg). In Eq. (1), the search direction(r1·Gbestg−r2·Vpopi)is used to avoid the local optimal which Vpopiis the ith current position of Vpop. And in order to improve the performance of local search, the term log (g)/g is designed so as to decrease the size of Gaussian jumps with the generation increased. From Eq. (1), it can be observed that the new individuals are generated around the global best solution by the Gaussian distributionGaussian(Gbestg,τ)and adjusted by the search direction(r1·Gbestg−r2·Vpopi). On the other hand, with the generation increased, the value of log (g)/g is decreased gradually. It leads to severe disturbance at the early generation while a slight disturbance at the late generation around the global best solution, which can help the individuals to get closer to the solutions.Once a host cell is infected, it will be invaded and destroyed by the virus until its death. Actually, this process can be explained as the process of interaction relations: the host cell provides the essential elements and the virus metabolizes the harmful substances leading to death of the host cell gradually. Finally, the host cell ‘mutates’ into a new virus. This process is mainly used to realize the information exchange and improve the exploration properties of the population. CMA-ES method is a famous evolutionary algorithm. It not only contains the mutation operation, but also considers the interactive relations between the individuals by covariance matrix adaptation (CMA), which adapts a full covariance matrix of a normal search (mutation) distribution [30]. Therefore this method is quite suitable for the behavior of the host cells infection and it is introduced in this paper. In order to reflect the interactive relations between the viruses and the host cells. The main steps of it can be summarized as follows.Step. 1. Update the Hpopby:(2)Hpopig=Xmeang+σig×Ni(0,Cg)where Ni(0, Cg) is a normal distribution with mean 0 and D×D covariance matrix Cg, g is the current generation, D is the dimension of the problem and σg> 0 is the step size.Xmeangis initialized by:(3)Xmean0=∑i=1NVpopiNStep. 2. Select the best λ individuals from the previous phrase as a parental vector, and the center of the selected vector is calculated based on:(4)Xmeang+1=1λ∑i=1λωi·Vpopiλbest|ωi=ln(λ+1)/(∑j=1λ(ln(λ+1)−ln(j)))whereλ=⌊N/2⌋, ωiis the recombination weight and i denotes the index of ith best individuals. Two so-called evolution paths are computed. They track the history of changes of the population mean with an exponential decay of the past.(5)pσg+1=(1−cσ)pσg+cσ(2−cσ)λw1σg(Cg)−1/2(Xmeang+1−Xmeang)(6)pcg+1=(1−cc)pcg+hσcc(2−cc)λw1σg(Xmeang+1−Xmeang)whereλw−1=∑i=1λwi2,(Cg)−1/2is symmetric, positive and satisfies(Cg)−1/2(Cg)−1/2=(Cg)−1. The cumulation parameters are generally set ascσ=(λw+2)/(N+λw+3),cc=4/(N+4)andhσ=1usually buthσ=0if∥pσg+1∥is large [30].Step. 3. Update the step size σg+1 and the covariance matrix Cg+1 by:(7)σg+1=σg×exp(cσdσ(∥pσg+1∥E∥N(0,I)∥−1))(8)Cg+1=(1−c1−cλ)Cg+c1pcg+1(pcg+1)T+cλ∑i=1λwiVpopiλbest−Xmeangσg·(Vpopiλbest−Xmeang)Tσgwheredσ=1+cσ+2max{0,(λw−1/N+1)−1}is usually close to 1 and c1, cλobey(9)c1=1λw((1−1λw)min{1,2λw−1(N+2)2+λw}+1λw2(N+2)2)(10)cλ=(λw−1)c1where 0 ≤ cλ≤ 1 is the updating rate for updating the covariance matrix C[30].As mentioned earlier, according to influence of the host cell immune system, the viruses with better performance are more likely to retain its ability to the next generation. However, worse viruses must be evolved themselves in case of being killed by the immune system. Thus, the following steps are used to realize the evolution of the viruses.Step. 1. Calculate the performance ranks Pr according to fitness value of virus colony Vpopby:(11)Prrank(i)=(N−i+1)Nwhere N is the population size of Vpop, rank(i) means the fitness rank of the ith individual in Vpop.Step. 2. Evolve the individuals of Vpopby:(12){Vpopi,j′′=Vpopk,j−rand·(Vpoph,j−Vpopi,j),ifr>Prrank(i)Vpopi,j′′=Vpopi,j,otherwisewhere k, i, h are random indexes selected from [1, 2, 3, … , N], i≠k≠h andj∈[1,2,3,…,d]. rand and r are the random values chosen from the [0, 1]. As shown in Eqs. (11) and (12), the better individuals are quite possible to remain their better performance to the next generation.In addition, the generation mechanisms of the new population obtained by above three behaviors processes may lead to crossing the search boundary. Thus, the boundary checking method is used to control the search boundary.Step. 1.if xi,j<Low, then xi,j=rand×(Up-Low)+Low.if xi,j>Up, then xi,j=rand×(Up-Low)+Low.By combining the three simulated behaviors of virus and the host cell, VCS is presented. The standard Virus Colony Search optimization algorithm can be described in Algorithm 1.At each generation, for each virus, a viruses diffusion vector Vpop’i is generated by the Gaussian random walk method (i. e. Eq. (1)). Then, check the search bounds of the new population and update the Vpopby objective function values. Afterward, the host cell infection vector Hpopig+1 is created by the individuals of Vpop(i. e. Eq. (2)). Then select the λ best individuals for the calculation of theXmeang+1(i. e. Eq. (4)) and other parameters (i. e. Eqs. (5–10)) according to CMA-ES method [30]. Again, check the search bounds of the new population Hpopand update the Vpop. Finally, immune response process makes a contribution to the evolution of the viruses. If the ranks of the individuals in Vpopare worse than the others, they are more likely to evolve themselves. The parameter Pr is used to calculate the performance ranks of the individuals (i. e. Eq. (11)). According to Eq. (11), the individuals with worse fitness values have the smaller value of Pr and the random value r may greater than Pr with high probability. In result, they are more likely to evolve by Eq. (12). Instead, the better individuals with greater Pr may remain unchanged with high probability.In this section, to study our new algorithm, experimental results are carried out based on the constrained and unconstrained benchmark functions. The 10 well-known classic benchmark functions [19] and 30 modern benchmark functions selected from CEC-2014 [33] are considered as the unconstrained benchmark functions. For constrained benchmark functions, three engineering design optimization problems commonly used in literature are employed in this paper. In addition, all the experiments in this section are performed on the computer with 3.20 GHz Intel(R) Core(TM) i5-3470 processor and 4GB of RAM using MATLAB 2013a.To evaluate optimization performance of our VCS for unconstrained benchmark functions, we compare VCS with eight state-of-the-arts nature-inspired optimization algorithms and Appendix A shows their download links.(1)Artificial bee colony algorithm (ABC) [16] simulates the behaviors of bees for foregoing foods, which is a popular swarm intelligence algorithm. It divides the population into three main groups: employed bees, onlooker bees and scout bees.Cuckoo search (CS) is one of the famous nature-inspired algorithms, developed in 2009 by Xin-She Yang and Suash Deb [17]. Based on brood parasitism of some cuckoo species, CS is enhanced by the Levy flights random walk method.Animal migration optimization (AMO) algorithm [18], proposed in 2014, is a new swarm intelligence algorithm, which is inspired by the animal migration behavior.Grew wolf optimizer (GWO) which borrows ideas from the leadership hierarchy and hunting mechanism of grey wolves in nature [19].Symbiosis organisms search (SOS) [35] that simulates the interactive behavior seen among organisms in nature. Just like our VCS, the processes of SOS for finding solutions contain three main phases: mutualism phases, commensalism phase and parasitism phase.Evolutionary strategies with covariance matrix adaptation (CMA-ES) algorithm [30] is a classical evolutionary algorithm. Meanwhile, it is introduced as an operator in the process of host cell infection in VCS.Differential evolution based on covariance and bimodal distribution (CoBiDE) algorithm is a novel DE variant [34]. It has two main components: covariance matrix learning and bimodal distribution parameter setting.The gravitational search algorithm (GSA) [24] is a well-known physics-based algorithm, which performs the optimization by using a collection of masses interacting with each other based on the laws of gravity and motion.In order to evaluate the performance of VCS for optimizing the classic benchmark functions, a series of 10 classic benchmark functions selected from [19] are used in this experiment. Appendix B shows the basic descriptions of these functions which can be divided into two main parts: unimodal functions (f01–f04) and multimodal functions (f05–f10). Among the multimodal functions, f05–f08 are the high-dimensional multimodal functions and f09–f10 are the low-dimensional multimodal functions. For each test function, 30 independent runs are performed with random seeds for each problem.Our VCS has been compared with six NIs in this experiment: CMA-ES, CoBiDE, CS, AMO, ABC and GSA. For all the test functions, maximum numbers of function evolution (MaxFEs) are given in Table 1. The setting values of the compared algorithms and VCS are given below.(1)CMA-ES:λ=⌊N/2⌋, σ = 0.3 as in [30];CoBiDE: pb = 0.4, ps = 0.5 and mutation strategies and crossover strategies as in [34];CS: b = 1.50, p0 = 0.25 as in [17];AMO: The number of animals in each group was set to 5 [18];GSA: Go= 100 and a = 20 [24];ABC: limit=(N•d)/2; Size of Employed-bee=Onlooker-bee=(Colony size)/2 as in [36].VCS:λ=⌊N/2⌋and σ = 0.3.To have a fair comparison, CS is adjusted based on fitness evolution numbers and ABC is modified according to [36]. In addition, in order to compare the above algorithms under the same number of maximum generation (MaxG) for each test function, the population size of VCS is set to 50 since it has three phases. The same as VCS, the population size of AMO and CS is 75 because there are two phases in each algorithm. As for the other compared algorithms, the population size is set to 150. As a result, every algorithm has the same function evolution numbers in each generation.Table 2reports the obtained mean results by the compared algorithms over the unimodal functions (f01–f04) based on 30 independent runs, where Best means the best solution, Worst stands for the worst solution, Mean represents the mean solution and SD means the standard deviation. The best results are set in bold in the coming tables. Meanwhile, the algorithms are sorted based on the mean solution from small to large. In addition, we calculate the mean rank and obtain the overall rank. From Table 2, it can be observed that VCS can find the global optimum for all the unimodal functions the specified MaxFEs (see Table 1). Absolutely, VCS ranks the first according to the mean value from the last row of Table 2. As we known, the convergence rate of search algorithm is more important for the unimodal functions than the final results since there are other specific methods aiming at the unimodal functions. Fig. 2shows the convergence rate of VCS and the other compared algorithms from a graphical point of view for four typical functions. As can be seen from Fig. 2, VCS has the faster convergence rate for the four unimodal functions compared with the other six algorithms.For the high-dimensional multimodal functions (f05∼f08) and the low-dimensional multimodal functions (f09–f10), Table 3summarizes the average results obtained by the algorithms based on 30 independent runs. The algorithms are ranked based on the mean value of each test function and the average ranks for the six functions are calculated. As these functions have many local minima, they are often used to scrutinize the ability of algorithms to escape from poor local optima and obtain the near-global optima. From Table 3, it can be found that VCS nearly finds all the global optimal values of the high-dimensional functions except for function f08. However, it has the best mean value for f08. As seen from the last row of Table 3, VCS ranks first for the classic multimodal functions, while CS performs the worst for these functions. Fig. 3shows the difference between VCS and other compared algorithms in terms of convergence rate in solving four classic high-dimensional multimodal functions. When examined Fig. 3, VCS algorithm converges more rapidly for the functions compared with the other algorithms.For the overall comparison on the 10 classic benchmark functions, the pair-wise comparison between the VCS and the other competitors based on the Wilcoxon signed ranks test [37] with a confidence level of 0.95 (α = 0.05) is employed. The Wilcoxon signed ranks test, which is a simple, yet safe and robust, nonparametric test for pairwise statistical comparisons, is usually used to detect the significant differences between two sample means, that is, the performance of two algorithms.Table 4summarizes the test results based on the best solution for each function with 30 independent runs, where ‘+’ in the tables means the reference algorithm performed better than the compared algorithms, ‘–’ indicates it performs worse and ‘=’ means the both two algorithms performed equally. R+ represents the sum of ranks for the problems in which VCS outperformed the compared one, and R− means the sum of ranks for the opposite. In addition, the multiple comparisons among all the algorithms are performed by using the Friedman test which is a multiple comparisons test that aims to detect significant differences between the properties of two or more algorithms [37]. It's necessary to emphasize that the Friedman test is accomplished in this paper by using the KEEL software [38]. The average rankings obtained by Friedman test can be used as indicators to illustrate how successful the algorithm is. In other words, the lower the rank, the more successful the algorithm is. Based on the average value and standard deviation for each test function, Table 5reports the ranks of Mean and SD among seven algorithms obtained by the Friedman's test with a confidence level of 0.95 (α = 0.05), in which Iman-Davenport's procedure is used as a post hoc procedure. In Table 5, the result of p-values of Iman-Davenport is obtained by the compared algorithm (i.e. CMA-ES, ABC) vs. the control algorithm (VCS) respectively. When examined the results of last row ‘+\≈\−’ in Table 4, it can be found that VCS has obviously high ‘+’ counts than the other competitors. It means that VCS exhibits the statistically superior performance than the six compared algorithms in the pair-wise Wilcoxon signed ranks test at the 95% significance level. From the results of Table 5, it can be observed that VCS ranks first according to the mean value and standard deviation obtained by the 30 independent runs for each classic benchmark.With regard to the time consumption for optimization of these 10 test functions, Table 6presents the results of mean time obtained by the VCS and other compared algorithms with 30 independent runs. We rank the algorithms from smallest mean time of the test functions to the highest mean time. As can be concluded from Table 6, CMA-ES ranks first among the algorithms. VCS ranks fifth, but it is better than GSA and AMO. As it significantly outperforms the other compared algorithms for the 10 benchmark functions, it is acceptable with a little more time consumption.We used 30 IEEE CEC2014 benchmark functions designed for the special issue and competition on single objective and complex real-parameter numerical optimization problems in this experiment. These benchmark functions have several novel features such as novel basic problems, composing test problems by extracting features dimension-wise from several problems, graded level of linkages, rotated trap problems, and so on. Table 7summarizes several features of the 30 benchmark functions and the detailed descriptions of these functions are provided in [30]. To evaluate the performance of VCS on optimizing the CEC2014 benchmark functions, the results of VCS are compared with six state-of-the-art algorithms: CS [17], GWO [19], GSA [24], CMA-ES [30], SOS [35] and CoBiDE [34]. The dimension of the problems is set to 30 and the MaxFEs is 300,000. For a fair comparison, the algorithms are performed in the experiment under the condition of same MaxFEs and MaxG. In result, the population size is set to 37 for SOS because it has four phases, and it is set to 50 for VCS. For CS, the population size is set to 75. For the other algorithms, the population size is set to 150. The parameters settings for CMA-ES, GSA, CS, CoBiDE and VCS are the same as previous experiment. There are no specific algorithm parameters in SOS [35] and we set a=2−1×(2/MaxG) of GWO as in [19]. In addition, the Matlab codes of the compared algorithms can be downloaded from Appendix A.Tables 8–11summarize the results of the objective function error values (f(x)−f(x*)) over 30 independent runs for each function, where x is the best solution in the population when the algorithm terminates and x* is the global optimal solution. We rank the algorithms from smallest mean solution to the highest solution. At last, the average ranks and the overall ranks obtained by algorithms are concluded.For the unimodal functions (f01(CEC)–f03(CEC)), Table 8 reports the results obtained by the algorithms with 30 independent runs. Fig. 4shows the convergence rate of the algorithms for two typical unimodal functions of CEC2014. The results from Table 8 obviously reflect that VCS performs better than the compared algorithms for the three CEC2014 unimodal functions. Meanwhile, it can find the global optimum for the three benchmark functions. The promising performance of convergence rate and accuracy can also be stressed in Fig. 4.Table 9 summarizes the results obtained by the algorithms with 30 independent runs for the simple multimodal functions (f04(CEC)–f16(CEC)). For a graphical point of view, the convergence rates of the algorithms for several typical multimodal functions are shown in Fig. 5and the graphical analysis results of the ANOVA tests are shown in Fig. 6.According to the results from Table 9, it is obvious that VCS presents much better performance than the other algorithms for most simple multimodal functions. More specifically, VCS ranks best based on the mean error value of 30 independent runs for f04(CEC), f06(CEC), f07(CEC), f10(CEC), f13(CEC), f15(CEC) and f16(CEC), and it can find the global optimum for f04(CEC) and f07(CEC). While GSA performs better than the compared algorithms on f05(CEC), f12(CEC) and f14(CEC). CMA-ES ranks first for f11(CEC) and GWO performs the best performance for f09(CEC). For f08(CEC), VCS performs the comparable with CoBiDE. Generally speaking, VCS ranks first for the CEC2014 multimodal functions compared with the other algorithms according to the overall rank in the last line of Table 9. From the graphical results of Fig. 5 and Fig. 6, it can be concluded that VCS has a better performance of convergence rate and global optimization than the compared algorithms for most listed typical functions, while CMA-ES performs better convergence rate than the VCS on f11(CEC).For the hybrid functions (f17(CEC)–f22(CEC)), Table 10 reports the results obtained by the algorithms with 30 independent runs. Figs. 7and 8show the convergence rates and ANOVA tests of the algorithms for several hybrid functions. As mentioned above, the CEC2014 hybrid functions are more complex than the above two groups and all the algorithms can hardly find the global optimum for these six hybrid functions. However, according to the results from Table 10, VCS performs more acceptable results than the other compared algorithms. Meanwhile, the best solutions of the VCS for the six functions rank first and it can be concluded that VCS has a promising ability for searching global optimization, but the stability of VCS is relatively poor. Based on the overall rank in the last line of Table 10, VCS performs the best among the compared algorithms on the hybrid functions. According to the graphical results of Figs. 7 and 8, VCS performs better preference for the four typical hybrid functions.Table 11 summarizes the results obtained by the algorithms with 30 independent runs for the composition functions (f23(CEC)–f30(CEC)). The graphical analysis results of the ANOVA tests are presented in Fig. 9. According to the results from Table 11, it can be concluded that VCS presents the satisfactory results, especially for f23(CEC)∼f25(CEC), f27(CEC) and f28(CEC). In addition, it can be observed that VCS can find the minimum best solutions for all composition functions. GWO performs better for optimizing the f26(CEC) and GSA shows better results for f29(CEC). Overall, VCS ranks first for the eight composition functions. From the graphical results of Fig. 9, VCS performs obviously better than the compared algorithms.For the overall comparison on the CEC2014 benchmark functions, we perform the pair-wise comparison between the VCS and the other competitors based on the Wilcoxon signed ranks test with a confidence level of 0.95 (α = 0.05). And the multiple comparisons by Friedman's test with a confidence level of 0.95 (α = 0.05) are employed, in which Iman-Davenport's procedure is used as a post hoc procedure for Friedman test and the VCS is used as a control algorithm [37]. Again, the Friedman test is accomplished in this experiment by using the KEEL software [38]. Table 12summarizes the test results based on the best solution for each function with 30 independent runs and Table 13reports the ranks of Mean and SD among seven algorithms obtained by the Friedman test. When examined the results of last row ‘+\≈\−’ in Table 12, it can be observed that VCS has obviously high ‘+’ counts than the other competitors. It means that VCS exhibits the statistically superior performance than the six compared algorithms in the pair-wise Wilcoxon signed ranks test at the 95% significance level. From the results of Table 13, it can be founded that VCS ranks first according to the mean value of the 30 independent runs for each CEC2014 benchmark. However, GWO ranks first on SD and VCS ranks third on it.According to above statistical results and analysis, it can be concluded that VCS performs extremely better optimization performance on convergence rate and accuracy compared with CMA-ES, GSA, GWO, CS, CoBiDE and SOS for CEC2014 benchmark functions.This section experimentally studies the influence of the three search phases, as well as two important parameters for the optimization performance of VCS. To evaluate the search strategies of VCS, we set three conditions: VCS without virus diffusion phase (VCS1), VCS without host cell infection phase (VCS2) and VCS without immune response phase (VCS3). These three conditions and standard VCS are tested on the several classic as well as CEC2014 benchmark functions. The f01, f03, f07 and f09 are used as the classic benchmark functions, and the f02(CEC), f07(CEC), f19(CEC) and f25(CEC) selected from CEC2014 are used as the modern benchmarks. In addition, the population for these three conditions is set to 75 because each of them has two phases. The other parameters are set according to its previous experiments.Table 14represents the average results of 10 runs for each condition and standard VCS. Fig. 10shows the convergence rate of three conditions and standard VCS. The obtained results from Table 14 show that different condition for different benchmark function presents different performance. It's clear that VCS performs best for the eight test functions. Without the Gaussian random walk in the virus diffusion phase which has a promising performance in convergence rate, VCS1 performs the worst performance on f01, f07 and f25CEC. VCS2 exhibits the worst performance on f03, f02CEC and f07CEC because the host cell infection phase can help the individuals to exchange the information so as to avoid local optimal. VCS3 shows the more proper performance compared with VCS1 and VCS2, but worse than VCS. It is because that the immune response phase is mainly used to adjust the individuals slightly so as to get closer to the solutions. Fig. 10 shows that VCS1 converges fast than the other conditions for f03 and f02(CEC). While VCS performs best for f07 and f19(CEC). Based on the results from Table 14 and Fig. 10, we can observe that each behavior in VCS can significantly affect the quality of final results, and they also show that three behaviors together make a coherent system to solve optimization problems.There are two important parameters in VCS namely the number of selected best individuals λ and the step size σ. In order to learn the influence of the two parameters on the performance of the algorithm, the following two experiments are performed on the above selected 8 benchmark functions. Tabled 15and 16summarize the results obtained by the different two parameter settings based on 30 independent runs. The population size N=50 and the other parameters are set according to its previous experiments. From Table 15, we can observe that there is no significant difference with the different λ for f01 and f25(CEC). For f03, f09 and f19(CEC), VCS performs the best whenλ=⌊N/2⌋. Meanwhile, it can be found that VCS exhibits the worse performance when λ=N orλ=⌊N/10⌋and it means that the larger or smaller λ value is not helpful to improve the performance of VCS. As we know, the primary design goal for λ is to make full use of best individuals as well as avoid local optimum. It means that larger λ value leads to useless of the best individuals and smaller λ value generates the high probability to run into partial optimization. In addition, because the number of the selected best individuals can directly affect the computational complexity, the more numbers, the larger time consumption. It can be proved from the results of Time in Table 15 that the time consumption becomes larger with the λ increased. Fig. 11shows the convergence rate on f07 and f02(CEC). It can be seen that VCS withλ=⌊N/2⌋converges faster than the other conditions. Considering the trade-off between time consumption and convergence accuracy, λ is set toλ=⌊N/2⌋in this paper.According to Eq. (2) in host cell infection behavior, we can observe that the parameter σ is used to control the amplitude for generating the new individuals. A larger σ is effective for global search, while a smaller σ is useful for local search. From the results of Table 16, it can be concluded that VCS exhibits the same convergence rate and accuracy for f01, f07, f02(CEC) and f25(CEC) with different settings of σ. However, VCS performs the best when σ = 0.1 for optimizing f03, which owes to its promising performance for local search if σ is smaller. Meanwhile, it shows the best performance when σ = 0.3 for optimizing f09, f07(CEC) and f19(CEC). In addition, VCS performs the worse optimization qualities if the algorithm has a larger σ from Table 16, especially for the complicated test function (i. e. f02(CEC), f07(CEC) and f19(CEC)). It is mainly because the larger σ leads to poor quality to get closer to the best solution. Fig. 12shows the convergence rate with different σ for VCS on the two test functions. Considering the balance between the global search and local search, we set σ=0.3 in VCS for giving full play to optimization performance according to the above results.Three well-known constrained engineering design optimization problems (see Appendix C) are used in this section: pressure vessel design problem, tension/compression spring problem and welded beam design problem. To evaluate the performance of VCS for optimizing the three constrained problems, the same constraint handling mechanism used in [39] is employed for VCS, and the results of 30 independent runs on each problem are summarized in Tables 17–22. In addition, the results of the compared algorithms for these three problems are directly taken from the related studies in the literature. For each problem, the population size of VCS is set to 20.Pressure vessel design problem is a mixed type of optimization problem. Fig. 13shows a cylindrical pressure vessel capped at both ends by hemispherical heads. The objective is to minimize the total cost (f(x)), including the cost of forming, material and welding. There are four variables in this problem: the thickness Ts (x1), thickness of the head Th (x2), the inner radius R (x3) and the length of the cylindrical section of the vessel L (x4).Many researchers have been previously solved this problem as a benchmark structural optimization problem such as Gandomi et al [39], who applied the bat algorithm; Bernardino et al. [40], who employed a hybrid genetic algorithm based on artificial immune system (AIS-GA); Huang et al. [41], who used the co-evolutionary differential evolution (CDE); Wang et al. [42], who employed the differential evolution with level comparison (DELC); Gao and Hailu [43], who used the proposed comprehensive learning particle swarm optimizer algorithm (CLPSO); Hsieh [44], who used the bacterial gene recombination algorithm (BGRA); water cycle algorithm (WCA) designed by Eskandar [45]; mine blast algorithm (MBA) proposed by Sadollah et al. [46] and gaussian quantum-behaved particle swarm optimization (G-QPSO) [47].Tables 17 and 18summarize the best results and statistical results obtained by VCS based on 30 independent runs and other different optimizer from the above literature for this problem. From Table 18, the results obtained by VCS with 36,020 function evaluations are better than the compared algorithms except for BGRA. However, VCS offers the competitive best solutions in much less number of function evaluations than offered by the BGRA. Meanwhile, the best cost value of VCS is much lower than that of AIS-GA, CDE, CLPSO, BA, DELC and G-QPSO.Spring design optimization problem is a well-known engineering design problem to investigate the superiority of an algorithm. The main objective of the problem is to design a spring for a minimum weight by achieving optimum values of the variables as shown in Fig. 14. It contains four nonlinear inequality constraints and three continuous variables: the wire diameter w (x1), the mean coil diameter d (x2) and the length (or number of coils) L (x3).This problem has been solved previously using BA, AIS-GA, CDE, DELC, BGRA, WCA, MBA, G-QPSO and hybrid evolutionary algorithm and adaptive constraint handling technique (HEAA) [48]. Table 19shows the comparisons of the best solutions obtained by VCS and other compared algorithms. Table 20lists the statistical results of this problem obtained by VCS with two sets and compares the results with solutions reported by previous researchers. The first set of results corresponds to 11,720 function evaluations and the second set corresponds to 2000 function evaluations using VCS. When examined Table 20, it can be found that the best function value is 0.012665222962643 with 11,720 function evaluations obtained by VCS. In addition, based on 2000 NFEs, VCS can archive the acceptable best result compared with G-QPSO.The welded beam design optimization problem (see Fig. 15) involves four design variables including the width h (x1) and length l (x2) of the welded area, the depth t (x3) and thickness b (x4) of the main beam. The main goal is to minimize the overall fabrication cost, under the bending stress σd(30,000 psi), appropriate constraints of shear stress τd(13,600 psi), maximum end deflection δd(0.25 in) and loading condition P (6000 lb).The optimization methods previously applied to this problem such as CDE, DELC, WCA, MBA, genetic algorithm based co-evolution model (GA3) [49], hybrid particle swarm optimization (HPSO) [50], hybrid nelder-mead simplex search and particle swarm optimization (NM-PSO) [51], hybrid genetic algorithm (HGA) [52], DE [53], and MGA [54].The comparison of best solution with previous methods is given in Table 21. Table 22lists the statistical results obtained by VCS and other nine algorithms. From Table 22, it can be observed that the best solution is obtained by NM-PSO with an objective function value of f(x) = 1.724717 after 80,000 function evaluations. By using the proposed algorithm, the best solution of f(x) = 1.724852308597364 is obtained with 36,020 function evaluations, which ranks second from the statistical results. It is worth to mention that the best solution of f(x) by applying the VCS method is 1.724852329051929 after 12,020 function evaluations with SD=8.35206e-07, which is also a satisfactory result with less NFEs compared with CDE, WCA, GA3, HPSO, DE and MGA.

@&#CONCLUSIONS@&#
