@&#MAIN-TITLE@&#
Evaluating the predictions of objective intelligibility metrics for modified and synthetic speech

@&#HIGHLIGHTS@&#
Algorithmically modified speech is used to assess objective intelligibility metrics.Reduced predictive power of the metrics for the given speech is demonstrated.Metrics show two opposite predictive patterns in fluctuating and stationary maskers.The glimpse proportion metric is extended.

@&#KEYPHRASES@&#
Objective intelligibility metric,Noise,Speech modifications,Synthetic speech,

@&#ABSTRACT@&#
Several modification algorithms that alter natural or synthetic speech with the goal of improving intelligibility in noise have been proposed recently. A key requirement of many modification techniques is the ability to predict intelligibility, both offline during algorithm development, and online, in order to determine the optimal modification for the current noise context. While existing objective intelligibility metrics (OIMs) have good predictive power for unmodified natural speech in stationary and fluctuating noise, little is known about their effectiveness for other forms of speech. The current study evaluated how well seven OIMs predict listener responses in three large datasets of modified and synthetic speech which together represent 396 combinations of speech modification, masker type and signal-to-noise ratio. The chief finding is a clear reduction in predictive power for most OIMs when faced with modified and synthetic speech. Modifications introducing durational changes are particularly harmful to intelligibility predictors. OIMs that measure masked audibility tend to over-estimate intelligibility in the presence of fluctuating maskers relative to stationary maskers, while OIMs that estimate the distortion caused by the masker to a clean speech prototype exhibit the reverse pattern.

@&#INTRODUCTION@&#
Spoken language applications using recorded natural11We use the term ‘natural’ to signify speech produced by a human talker as opposed to speech which is natural-sounding.or synthetic speech can be made more robust through algorithmic speech modification. Unlike traditional speech enhancement techniques (e.g., Hu and Loizou, 2004; Martin, 2005; Chen et al., 2006; Srinivasan et al., 2007) which focus on the noise-corrupted speech signal, the speech modification approach (e.g., Sauert and Vary, 2006; Bonardo and Zovato, 2007; Yoo et al., 2007; Brouckxon et al., 2008; Tang and Cooke, 2010) alters the clean speech signal prior to output or transmission. A recent evaluation (Cooke et al., 2013b) demonstrated that speech modification can result in intelligibility gains in noise equivalent to increases of more than 5dB in output level.A key ingredient in the design of effective modification strategies is the estimation of listener performance at frequent intervals during the development cycle. However, while subjective intelligibility scores remain the ultimate reference, continuous behavioural testing during algorithm design is usually infeasible. An alternative is to use objective intelligibility metrics (OIMs) to predict listener scores. OIMs not only avoid the need for extensive subjective testing, but can also be used at the core of the algorithm optimisation process. A number of speech modification algorithms (e.g., Sauert and Vary, 2010a; Tang and Cooke, 2011; Taal et al., 2013; Valentini-Botinhao et al., 2014) have been developed and optimised based on maximising intelligibility predictions made by OIMs such as the Speech Intelligibility Index (SII; ANSI, 1997) or the glimpse proportion metric (GP; Cooke, 2006).OIMs have been motivated by two distinct approaches to account for the effect of noise on speech. In addition to the aforementioned SII and GP metrics, the Articulation Index (AI; French and Steinberg, 1947; Fletcher and Galt, 1950; Kryter, 1962a,b), and the extended Speech Intelligibility Index (ESII; Rhebergen and Versfeld, 2005) focus on quantifying the masked audibility of speech in the presence of noise. On the other hand, techniques such as the Normalised-Covariance Measure (NCM; Holube and Kollmeier, 1996; Ma et al., 2009), the Christiansen–Pedersen–Dau metric (henceforth referred to as CPD for brevity; Christiansen et al., 2010) and the Short-Time Objective Intelligibility metric (STOI; Taal et al., 2010) correlate representations of the clean reference speech and the speech-plus-noise signal in an attempt to measure the distortion caused by the masker. Another distortion-based approach is the Coherence Speech Intelligibility Index (CSII) proposed by Kates and Arehart (2005). The CSII measures the similarity between clean and noisy speech using magnitude-square coherence (Carter et al., 1973; Kates, 1992) which quantifies the degree to which the output of a system is linearly related to its input.Both audibility- and distortion-based approaches target spectro-temporal regions least affected by the noise, but differ in their assumptions. While techniques based on audibility require separated estimates of speech and noise in order to estimate masking, distortion-based OIMs assume that human listeners possess a template of the clean speech which is compared to the incoming noisy speech.When an OIM is employed as the objective function to be maximised, the predictive accuracy of the OIM is critical in determining the validity and effectiveness of the optimisation process. Most of the OIMs mentioned above have been evaluated with recorded natural speech or speech processed by noise reduction techniques. Relatively few studies have investigated their predictive power for modified natural speech or synthetic speech in noise: most OIMs were originally proposed to predict the intelligibility of distorted natural speech, for distortions caused by additive noise together with artefacts introduced by suppression algorithms applied to the noisy speech signal.Predicting the intelligibility impact of modification algorithms is likely to be challenging since the most successful methods (in terms of improving masked intelligibility) modify the signal in diverse domains – durational and spectral/formant – and possibly through non-linear operations. While the alterations benefit intelligibility, they may also introduce artefacts to the speech signal, leading to degraded speech quality. Nevertheless, the relation between speech intelligibility and quality is complex, and factors such as listening effort and loudness interact. Intelligibility and quality are not simply negatively or positively correlated, especially across listeners (Preminger and Tasell, 1995). For synthetic speech it might be expected that the OIMs’ task is even more challenging because the natural speech reference signal is not available, i.e., distortions introduced by the text-to-speech (TTS) system cannot be taken into account. Consequently, predicting the intelligibility of poor quality synthetic speech may be even more difficult.In two initial studies, which concerned solely the ability of OIMs to predict the masked intelligibility of modified and synthetic speech regardless of the perceptual speech quality, we observed a large reduction in the predictive accuracy of several OIMs on modified and synthetic speech relative to unmodified speech (Tang and Cooke, 2011; Valentini-Botinhao et al., 2011). The current study extends these pilots to a larger range of objective intelligibility metrics and includes behavioural data from recent extensive evaluations of 30 forms of modified and synthetic speech (Cooke et al., 2013a,b). Specifically, we evaluate the performance of one standard (SII) and six recent objective intelligibility metrics (ESII, GP, NCM, CSII, CPD, STOI) in predicting subjective intelligibility scores for both modified and synthetic speech in additive noise. The evaluation makes use of three datasets which together contain 396 combinations of speech modification, masker type and signal-to-noise ratio (SNR). The seven metrics are introduced in Section 2 while Section 3 describes the evaluation datasets. The outcome of a comparison of model predictions against behavioural data from large-scale listening tests is presented in Section 4.SII and AI share a common underlying idea: speech intelligibility is dependent on the audibility of the signal in each frequency band. The AI can be expressed as a function of the masking level represented by the SNR (SNRfAI) in each frequency channel as(1)AI=∑f=1FWf·SNRfAI,∑f=1FWf=1where Wfdenotes the band importance function (BIF) in channel f andSNRfAIis a value in the interval [0, 1] based on a piecewise-linear transformation of the actual SNR level SNRfin band f(2)SNRfAI=min(15,max(−15,SNRf))+1530SII extends AI by taking into account the effects of the upward spread of masking and high presentation levels when calculating the effective SNR in each band(3)SNRfSII=Lf·min(15,max(−15,Ef−Df))+1530where Efand Dfdenote the equivalent speech spectrum level and the disturbance spectrum level and Lfis a factor accounting for speech level distortion when speech is presented at high levels. The final SII is calculated as for the AI(4)SII=∑f=1FWf·SNRfSIIIn this study, SII is computed using 21 critical bands (i.e., F=21) with the BIF for speech in noise from Table B.2 of ANSI (1997).ESII is an extension of SII designed to better predict intelligibility in the face of fluctuating maskers (Rhebergen and Versfeld, 2005). ESII computes the SII for each time frame (SIIlocal). Frame durations range from 35ms for the lowest frequency critical band to 9.4ms for the highest. The ESII model prediction is then based on the average SII across all frames:(5)ESII=∑t=1TSIIlocal(t)Twhere T denotes the total number of frames. The procedure to calculate the local SII follows the original SII calculation described above.Simpson and Cooke (2005) compared the masking effectiveness of N-talker babble noise on speech intelligibility for a range of N, showing that a single competing speaker or amplitude-modulated noise is much less effective as a masker than multi-talker babble or speech-shaped noise. These basic findings motivated the glimpsing model of speech perception in noise (Cooke, 2006) in which not only temporal but also spectro-temporal modulations play a role in defining those parts of the speech signal that escape masking. The glimpse proportion is intended to reflect the local audibility of speech in noise and is defined as the percentage of spectro-temporal regions in modelled auditory excitation patterns whose local SNR exceeds a threshold αdB:(6)GPoriginal=100TF∑f=1F∑t=1TH(Sf(t)>(Nf(t)+α))where T and F are the numbers of time frames and frequency channels, Sf(t) and Nf(t) denote the spectro-temporal excitation patterns (STEPs) of speech and noise at time t and frequency f, andH(·)is the Heaviside unit step function counting the number of ‘glimpses’ which meet the local masked audibility criterion α. The STEP is derived by a gammatone filterbank (Patterson et al., 1988) using an implementation introduced by Cooke (1993). The Hilbert envelope of each filter output is computed and smoothed by a leaky integrator with a 8ms time constant (Moore et al., 1988), downsampled and log-compressed.Glimpse proportion itself was not proposed originally as a dedicated intelligibility predictor but as an intermediate representation prior to a recognition component. However, with a number of simple extensions, GP has the potential to serve as an easily computed proxy for the amount of speech that survives energetic masking. The new metric (Eq. (7)) takes into account (i) the audibility of speech in quiet, (ii) the impact of durational changes, and (iii) ceiling performance, as detailed below:(7)GP=v1TorigF∑f=1F∑t=1TH((Sf(t)>(Nf(t)+α))∧(Sf(t)>HL))where ∧ is logical conjunction andv(·)is a quasi-log function defined as:v(x)=log(1+x/δ)log(1+1/δ),δ=0.01To model audibility in quiet, Sf(t) and Nf(t) represent STEPs that have been adjusted by a frequency-dependent gain (ISO 389-7, 2006), a weighting that permits the use of a frequency-independent value of hearing level (HL), which is set to 25dB here. To account for the decreased intelligibility of rapid speech, Torigdenotes the number of time frames in the unmodified speech STEP. The functionvcompresses GP scores to reflect the finding that subjective performance reaches a ceiling for GP values significantly lower than unity.Here, a 34-channel gammatone filterbank with filter centre frequencies covering the range 100–7500Hz linearly spaced on the equivalent rectangle bandwidth scale (Moore and Glasberg, 1983) was used to derive the STEPs. The local masked audibility threshold α was set to 3dB, a value which produced a high listener-model correlation (ρ=0.96) in Cooke (2006).Within the framework of the AI metric, the normalised-covariance measure (NCM) was motivated by the idea that noise both reduces and interferes with the temporal modulations of speech. Instead of measuring the SNR level, which is a relationship between speech and noise alone, the signal-to-distortion ratio (SDR) is used to quantify the degree of distortion. In frequency channel f the correlation coefficient rfbetween the downsampled Hilbert envelopes of clean Sfand noisy speech Yfis computed:(8)rf=∑t=1T(Sf(t)−S¯f)·(Yf(t)−Y¯f)∑t=1T(Sf(t)−S¯f)2·∑t=1T(Yf(t)−Y¯f)2where T denotes the length of the time series andS¯fandY¯fare across-time averages of the clean and noisy speech envelopes in channel f. The SDR in decibel of channel f is defined as:(9)SDRfNCM=10log10rf21−rf2Following the SII,SDRfNCMis then transformed to a normalised index NIfwhich lies in the range [0, 1], using Eq. (2) withSNRfAIreplaced bySDRfNCM. The final intelligibility index is computed using Eq. (1) as:(10)NCM=∑f=1FWf·NIfwhere Wfdenotes the BIF introduced along with the SII earlier. Ma et al. (2009) reported high correlations between listeners’ sentence recognition scores and predicted intelligibility in babble noise (ρ=0.94), car noise (ρ=0.85), street noise (ρ=0.88) and train noise (ρ=0.90).CSII replaces the correlation coefficient between the frequency-dependent Hilbert envelopes of the clean and noisy speech in Eq. (9) with the magnitude-square coherence γ2 to quantify the degree to which the noisy speech is linearly related to the clean speech(11)|γk|2=∑t=1TSk(t)Yk*(t)2∑t=1T|Sk(t)|2∑t=1T|Yk(t)|2where Sk(t) and Yk(t) are FFT spectra in frame t of the speech and speech-plus-noise signals, k is the bin index, and T is the total number of frames. The quantity |γk|2 is a value in the range [0, 1]. The SDR for a channel is defined as(12)SDRfCSII=10log10∑k=1KRf(k)|γk|2Y′(k)∑k=1KRf(k)[1−|γk|2]Y′(k)where f is the index of the simplified ro-ex filterbank R (Moore and Glasberg, 1983), K denotes the total number of FFT bins, and Y′ is the noisy speech power spectral density, estimated using the FFT. The CSII(t) of frame t is computed in each 16-ms Hamming window with a 50% overlap between windows using Eqs. (1) and (2) withSNRfAIsubstituted bySDRfCSII.To account for the differing degrees to which speech can be affected by the noise masker, frames are grouped into three levels – low, mid, high – according to the local root-mean-square energy (RMS in dB) relative to the overall RMS of the entire signal(13)RMS′(t)=20log10RMS(t)RMSoverallwhere RMS(t) and RMSoverallare the RMS of the amplitude of the signal waveform at frame t, and the entire signal, respectively. The low-level frames are those with a relative RMS range of −30dB to −10dB; −10 to 0dB count as mid-level; those with positive relative RMS are classified as high-level frames. For each level, the mean CSIIhigh,mid,lowis obtained by averaging across all frames falling into this level:(14)CSIIhigh=1Thigh∑CSII(t)whereRMS′(t)⩾0CSIImid=1Tmid∑CSII(t)where−10⩽RMS′(t)<0CSIIlow=1Tlow∑CSII(t)where−30⩽RMS′(t)<10The final model output, CSII, is obtained by a linear weighting plus offset of the three level scores:(15)CSII=−3.47+1.84·CSIIlow+9.99·CSIImid+0.00·CSIIhighThe weights shown minimise the mean-squared error between model predictions and listener scores, based on an unconstrained nonlinear minimisation method introduced by Nelder and Mead (1965). It can be seen that CSII only uses the information from the mid and low-level frames to make intelligibility predictions. In Kates and Arehart (2005), CSII showed good intelligibility prediction (ρ=0.94) for speech in additive noise and speech with peak-clipping and centre-clipping distortions.CPD uses a psychoacoustically validated model of auditory processing (Dau et al., 1996) to generate an internal representation of a signal. The signal is passed through 32 gammatone filters followed by half-wave rectification and five non-linear loops to model auditory nerve fibre adaptation. Denoting the internal representations of the reference speech signal and speech-noise mixture as s and y, a frame-based cross-correlation r(t) between s and y is then computed every 20ms with a 50% overlap:(16)r(t)=∑f=1F∑i=1I(yf(i)−y¯(t))(sf(i)−s¯(t))∑f=1F∑i=1I(yf(i)−y¯(t))2∑f=1F∑i=1I(sf(i)−s¯(t))2wheres¯(t)andy¯(t)denote the average across time and frequency of the internal representation of the reference signal s and corrupted signal y at frame t, respectively. F and I are the number of frequency bands and samples in a frame. Simultaneously, the frames are classified into three levels (low, mid, high) following the frame classification introduced in CSII. The overall intelligibility of each level (rlow, rmidand rhigh) can be calculated by Eq. (14), except that the CSII(t) of the frame t in Eq. (14) is substituted by the cross-correlation coefficients of that frame r(t) computed by Eq. (16). Finally, the objective score CPD is obtained by a linear weighting of the three level scores:(17)CPD=wlow·rlow+wmid·rmid+whigh·rhighwherewhigh,wmidandwloware the weights for each level. In CPD the final intelligibility score actually only reflects the contribution of the high-level frames (i.e.,wlow=0,wmid=0andwhigh=1). The intelligibility prediction by the CPD metric was reported in Christiansen et al. (2010) to have high correlation with subjective data in speech-shaped noise (ρ=0.96), cafe noise (ρ=0.94), car noise (ρ=0.97) and bottle noise (ρ=0.88) for enhanced speech processed by ideal time-frequency segregation (ITFS) techniques (Cooke et al., 2001; Wang, 2005).The STOI metric was initially designed to predict the intelligibility of speech processed by enhancement algorithms such as ITFS, and a high correlation (ρ=0.95) with subjective data was reported for this type of enhanced speech in noise (Taal et al., 2010). Both the corrupted speech signal and the reference signal are represented with time-frequency excitation spectra in 15 third-octave bands and 384-ms frames using the discrete Fourier transform. In order to de-emphasise the impact of regions in which noise dominates the spectrum, excitation spectra of the corrupted speech Y are further clipped by a normalisation procedure expressed in Eq. (18), where S is the excitation spectrum of the reference signal:(18)Y′=max(min(λ·Y,(1+10−β/20)·S),(1−10−β/20)·S)whereλ=∑Sf(n)2/∑Yf(n)2λ is a scale factor for normalising corrupted time-frequency bins in frequency band f and n is the time-frequency bin index. β=−15dB denotes the lower SDR bound. Finally, intelligibility is predicted by the correlation coefficient between Y′ and S averaged across all bands and frames:(19)STOI=1TF∑f=1F∑t=1Trf(t)where T and F denote the total number of one-third octave bands and the total number of frames, and rf(t) is the local correlation coefficient between Y′ and S at frequency f and time t.The OIMs described above were evaluated based on listeners’ responses to speech from three datasets (Table 1). One – natural – consists of unmodified and modified natural speech. A second dataset, tts, contains speech generated by an HMM-based synthesiser. The third dataset, hurricane, is made up of both natural and synthetic speech. Further details of the listening tests are provided in the articles mentioned in Table 1.The natural dataset (Tang and Cooke, 2011) was created to investigate modifications to sentences drawn from the GRID corpus (Cooke et al., 2006) with a sampling frequency of 25kHz. Twenty-four native British English speakers identified the spoken letter (‘A–Z’, except ‘W’) and digit (0–9) keywords in sentences such as ‘Place blue at C 3 now’. Listeners listened to the sentences embedded in noise over headphones, and chose letter and digit options from an on-screen keyboard. The percentage of keywords recognised correctly was taken as the measure of intelligibility.Six speech conditions were compared: unmodified speech+five modification strategies. In Tang and Cooke (2011) a sixth modification approach was also evaluated, but led to low subjective scores for the SNRs employed, and is not included in the current evaluation. Two techniques equalised SNR in each frame or frequency channel; a third approach transferred energy to time-frequency bins just below the threshold of audibility; another introduced a pause placed to avoid epochs of intense noise, while the final technique combined the latter two approaches. The unmodified and modified utterances were mixed with speech-shaped noise (SSN) and speech-modulated noise (SMN) at two global SNRs of −9 and −6dB, leading to a total of 24 listening conditions (unmodified+5 modifications×2 masker types×2 SNR levels). The SSN noise has the long-term spectrum of the corpus; the SMN noise is generated from the SSN by modulating with the envelope of a randomly concatenated utterance from the same corpus.The tts dataset, described in Valentini-Botinhao et al. (2011), is based on the responses of 88 listeners to text-to-speech (TTS) utterances presented to listeners in various noise conditions over headphones. After each sentence listeners typed the words they heard in the sentence. The subjective intelligibility in each condition was computed as the percentage of correctly identified words.TTS samples with a sampling frequency of 20kHz were generated using a HMM-based speech synthesis system (Zen et al., 2009), which was trained with 4000 sentences uttered by a male British English speaker in the ‘rjs’ corpus from the University of Edinburgh. The synthetic voice has a similar quality to that generated by the HTS2005 system in the Blizzard Challenge 2010 (King and Karaiskos, 2010), with a mean opinion score (MOS) of 2.5. Synthetic speech was further processed using four different approaches to simulate the acoustic properties of natural Lombard speech; each process had three parameter settings. These processes and accompanying settings are: spectral peak enhancement (no enhancement and two enhancement levels), fundamental frequency change (one decreasing and two increasing), shifts in the line spectral pairs domain (three different shifts amounts, always towards the high frequency region) and speech rate changes (one faster and two slower). Processed synthetic speech was then added to four maskers at four different SNRs chosen to produce word accuracies of approximately 20, 40, 60, and 80%: speech-shaped noise (SSN: −11.8, −8.8, −6.2 and −3.1dB), cafeteria babble noise (BAB: −9.5, −6.8, −4.6 and −1.9dB), car noise (CAR: −31.9, −28.4, −25.5 and −22.0dB) and high frequency noise (HiFQ: −43.5, −37.6, −32.7 and −26.8dB). Consequently, the tts dataset contains speech material for 192 listening conditions (4 processes×3 parameter settings×4 maskers×4 SNRs).The third dataset is based on the 2012 and 2013 Hurricane Challenges, the results of which are presented in Cooke et al. (2013b) and Cooke et al. (2013a), respectively. The subjective data was from 314 native British English speakers listening to the Harvard sentences (Rothauser et al., 1969) uttered by a male British English speaker over headphones. Each sentence contains five to six keywords such as ‘the juice of lemons makes fine punch’; as above, listener performance was assessed using the keyword identification rate.The speech type in this dataset consists of both natural (plain and Lombard) speech, modified speech and synthetic speech, at a sampling frequency of 16kHz. The synthetic speech has a MOS of 3.0 in terms of speech quality (Chen et al., 2014). In all, 30 types of speech were evaluated in two maskers: competing speech (CS) spoken by a female speaker, and SSN, both at three SNR levels chosen to produce recognition scores of about 25, 50 and 75% in each noise masker; specific SNRs are −21, −14, −7dB for CS and −9, −4, 1dB for SSN. The hurricane dataset thus represents 180 listening conditions (30 modifications×2 maskers×3 SNRs). Appendix A provides a brief summary of the 30 modification techniques.Long term average spectra (LTAS) of the maskers used in the three datasets are shown in Fig. 1. In the three experiments, the subjective intelligibility of each test condition was calculated as the mean word recognition rate across all listeners.All OIMs were evaluated by inspecting both the Pearson correlation coefficient ρ between mean listener scores and the raw output of the metric, and the standard deviation of the error σe, computed as(20)σe=σd·1−ρ2where σdis the standard deviation of subjective intelligibility scores for a given experimental condition. Statistical comparisons among dependent correlations were conducted using a method described in Meng et al. (1992) based on Chi-squared tests on z-transformed scores.Table 2reports correlations ρ and the standard deviations of the error σeacross all modifications and noise maskers for each of the three datasets. Since distortion-based OIMs require a reference signal – normally clean speech – as input to the metric, a choice must be made between using the clean unmodified or clean modified speech as the reference. For modified speech whose duration is altered by modification algorithm, the modified clean speech itself always has to be used as reference because the distortion-based OIMs require the reference signal to have the same duration as the tested signal. Outcomes using clean unmodified and clean modified speech as the reference signal are shown in the table. Since most OIMs make better predictions using modified clean speech as a reference, this is used in subsequent comparisons.For the natural and tts datasets, most of the chosen models performed significantly less well with modified natural speech and TTS speech than reported in previous studies for unmodified natural speech or noisy speech processed by noise reduction algorithms. The performance of the models varied significantly in the natural case [χ2(6)=64.895, p<0.001]: while GP, CPD, SII and ESII performed similarly [Z=0.731, p=0.465], audibility-based OIMs (i.e., GP, SII and ESII) outperformed those based on distortion (i.e., NCM, CSII and STOI) [Z=6.568, p<0.001], with the exception of CPD. For the tts dataset, the models also performed differently [χ2(6)=57.573, p<0.001]: the quality of predictions made by NCM, ESII, GP and CPD were statistically equivalent [Z=0.778, p=0.437] and superior to those of the remaining OIMs [Z=−9.109, p<0.001]. The performance of the metrics also differed for the hurricane dataset [χ2(6)=76.155, p<0.001]. Here, CSII and CPD were equivalent [Z=1.777, p=0.076] and superior to the other metrics [Z=6.422, p<0.001].Figs. 2–4present individual data points for each condition in the three datasets, coded by masker type. The CPD metric shows the most uniform performance across the datasets, but even this model lacks predictive power, especially for the tts dataset. For the hurricane dataset, all OIMs apart from CPD fare badly when predicting the intelligibility for both maskers combined: while ESII and GP predict higher-than-actual intelligibility for the competing speech masker, SII, NCM, STOI and to a lesser extent CSII show the converse pattern.Fig. 5shows the breakdown by noise type for the natural dataset, and reveals that the GP metric is most correlated with listeners (ρ>0.90) for both maskers, followed by ESII and CPD. Many of the metrics make reasonable (ρ>0.80) intelligibility predictions for speech in the presence of stationary maskers. Apart from GP, predictions were less good for the modulated masker than for stationary maskers, especially for NCM, CSII and STOI. Except for CPD, distortion-based OIMs produced lower correlations for the modulated maskers [Z=−6.701, p<0.001]. The poor performance of these three OIMs for this dataset may be due to them being unable to deal with the modification which introduced pauses into the speech signal.Correlations split by masker type in the tts dataset are shown in Fig. 6. As expected, correlations are generally higher for individual maskers than overall, suggesting a high degree of masker-specificity in the ability of metrics to predict intelligibility. Most metrics made reasonable predictions (ρ>0.85) for the high frequency noise masker. For the remaining maskers the pattern of correlation showed significant variation across metrics [for all maskers χ2(6)≥25.062, p<0.001].Table 3provides a more detailed look at correlations for the different processes applied during synthesis, relative to a synthetic speech baseline. The performance of most OIMs decreased as a result of processing. Apart from SII, all OIMs struggled to predict the effect of LSP shift. Other than GP, the performance of all OIMs degraded drastically in the face of linear expansion and contraction of duration.All metrics produced higher correlations in stationary noise than in the fluctuating competing speaker condition in the hurricane dataset (Fig. 7). OIMs were further evaluated for three groupings of conditions: natural speech only, synthetic speech only and for conditions that changed speech duration (Tab. 4). Except for CSII in for synthetic speech [Z=1.009, p=0.313] and modifications resulting in temporal change [Z=0.908, p=0.364], CPD demonstrated more robust predictive power [for remaining conditions Z≥2.477, p<0.05]. It is perhaps surprising to see that, in general, the subset of synthetic speech is better predicted than the natural speech subset. One possible explanation is that the degree of processing applied in the tts dataset was more extreme than seen in the hurricane dataset. Modifications involving temporal changes also appear less harmful in the hurricane dataset than in tts, particularly for those metrics – ESII, CSII, CPD and STOI – that suffered most. Again, this is likely to be due to the scale of temporal changes involved in the two datasets. In the hurricane dataset, speech duration was expanded by a smaller factor than in the tts data, and in no condition was duration decreased.

@&#CONCLUSIONS@&#
In the current study state-of-the-art OIMs that provide good predictions of natural speech performed less well for modified and synthetic speech, especially for those modifications introducing temporal changes. While many OIMs produced reasonable estimates for modified speech in the presence of single masker types, across-noise predictions were generally poor. Methods motivated by masked audibility tended to over-estimate intelligibility for fluctuating maskers and under-estimate intelligibility for stationary maskers, while for many metrics that computed the distorting effect of noise on clean speech the reverse pattern was evident. These findings suggest that further development of OIMs is required to enable their use in applications such as the offline development and online deployment of speech modification algorithms.