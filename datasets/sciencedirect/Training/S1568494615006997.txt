@&#MAIN-TITLE@&#
Instance variant nearest neighbor using particle swarm optimization for function approximation

@&#HIGHLIGHTS@&#
An approach to the function approximation problem using IVNN is presented.IVNN to apply less neighbors for noisy data and more neighbors for non-noisy data.IVNN solved by a PSO such that the prediction performance is maximized.IVNN requires no model/distribution assumption for variant numbers of neighbors.The prediction performance compared to conventional algorithms.

@&#KEYPHRASES@&#
Machine learning,Function approximation,Locally weighted regression,Instance variant nearest neighbor,Particle swarm optimization,

@&#ABSTRACT@&#
In this paper, a new approach called ‘instance variant nearest neighbor’ approximates a regression surface of a function using the concept of k nearest neighbor. Instead of fixed k neighbors for the entire dataset, our assumption is that there are optimal k neighbors for each data instance that best approximates the original function by fitting the local regions. This approach can be beneficial to noisy datasets where local regions form data characteristics that are different from the major data clusters. We formulate the problem of finding such k neighbors for each data instance as a combinatorial optimization problem, which is solved by a particle swarm optimization. The particle swarm optimization is extended with a rounding scheme that rounds up or down continuous-valued candidate solutions to integers, a number of k neighbors. We apply our new approach to five real-world regression datasets and compare its prediction performance with other function approximation algorithms, including the standard k nearest neighbor, multi-layer perceptron, and support vector regression. We observed that the instance variant nearest neighbor outperforms these algorithms in several datasets. In addition, our new approach provides consistent outputs with five datasets where other algorithms perform poorly.

@&#INTRODUCTION@&#
In this paper, we consider the function approximation problem where the aim is to estimate regression surface from data. We approach this problem with a k-nearest neighbor (kNN) based algorithm. Our main objective is to dynamically adjust the learning capacity to different local regions in the data, so as to approximate the original regression function with high accuracy. Local regions are defined as data patterns that may include few data, and outliers are one of the examples [5]. Aggarwal and Yu [6] refer to outliers as noisy data located outside a defined set or majority of clusters, which behave differently from them. Such noisy datasets can be seen in many different applications as the cause of outliers, human error, machine error, and changes in system behavior [7]. kNN regression is a locally weighted regression algorithm, which approximates a regression surface using the k nearest training data that are relevant to the new query data. The relevancy between the training and query data is measured by using a distance function, such as Euclidean [1]. The standard kNN requires a more principled way of choosing the parameter k than using a cross-validation. It is sensitive to noise when a new query data is predicted using its k neighbors that include noise [4]. Generally, this can be solved by using a larger k[4]. However, when a query with noisy data is to be predicted, a smaller k can improve the accuracy of prediction. Therefore, when kNN is applied to a noisy dataset, using a cross-validated kNN will end up with a trade-off k value that takes into account both the noisy and non-noisy data (i.e., trade-off between larger and smaller k values). Due to this reason, we employ a kNN regression with a set of variant k neighbors for each data instance to approximate the original regression function from the training data, which we call instance variant nearest neighbor (IVNN). Instead of using a single k value for a dataset, IVNN aims to find each k neighbors value for each data. More specifically, our approach attempts to apply smaller and larger k values to noisy and non-noisy data, respectively. Using these k neighbors for each data instance, we aim to maximize the prediction performance for the entire dataset.In this paper, the problem of selecting optimal k neighbors for each dataset is formulated as a combinatorial optimization problem, which will be solved using particle swarm optimization (PSO). PSO is chosen because of its advantages. Such advantages include simplicity with less parameters compared to other evolutionary optimization techniques and efficiency with reasonable optimization quality [20–24]. In addition, Escalante et al. [21] report that the way PSO searches for the optimal solution reduces over-fitting when this algorithm is used for model selection.The idea of using variant k neighbors for different data characteristics is investigated in the following research. Baoli et al. [8] apply variant k neighbors for imbalanced text classification datasets. The main assumption is that using kNN with a fixed k value to classify the categories of texts will result in a bias for large classes. For example, when k is so small, it may be unable to capture enough information from the training data that typically belong to large classes. They determine k neighbors for different categories based on the sample size of each category. The results from two different datasets show their approach is less sensitive compared to the standard kNN. Similarly, Ni and Nguyen [9] introduce the concept of variant k neighbors and apply it to an image interpolation application. Their method considers different k neighbors for each pixel. This turns out to be beneficial when high and low resolution patches within an image are non-uniformly scattered.The main concept of this research is closely related to Wang et al. [10]. They develop probability and confidence levels of misclassification errors to determine different k neighbors for each query data. The probability of error is derived with an assumption of either the Binomial or Beta prior model. We employ the idea of selecting different k for each query data. However, our approach does not involve making an assumption about a probability distribution model on the dataset. Instead, we empirically fit the original regression function using IVNN for each query data, such that the prediction performance is maximized. The proposed IVNN approach is formulated as a combinatorial optimization problem and is solved using PSO. The objective is to find a set of k neighbors for each query data such that a loss function is empirically minimized. We consider the mean square error between the target and prediction as the loss function.The rest of the paper is organized as follows. Section 2 briefly reviews related literature. We then introduce notations and formulate the problem in Section 3. We discuss results of our algorithm and compare with a number of function approximation algorithms in Section 4. In Section 5, we conclude the paper with suggesting future research directions.

@&#CONCLUSIONS@&#
