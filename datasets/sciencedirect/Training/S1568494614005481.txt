@&#MAIN-TITLE@&#
Artificial neural networks based modeling for solving Volterra integral equations system

@&#HIGHLIGHTS@&#
A combinative algorithm of approximating unknown functions in terms of truncated series involving Taylor polynomials method by applying an architecture of artificial neural networks is proposed here for solving linear Volterra integral equations system.The presented FNN in this study is a method for computing unknown constant coefficients in the Taylor expansions of solution functions.Excellent concurrence is seen to has been obtained between the exact and approximate solutions computed numerically by choosing high-order polynomial terms.

@&#KEYPHRASES@&#
Volterra integral equations system,Feed-backneural network,Taylor series,Approximate solution,

@&#ABSTRACT@&#
Properly designing an artificial neural network is very important for achieving the optimal performance. This study aims to utilize an architecture of these networks together with the Taylor polynomials, to achieve the approximate solution of second kind linear Volterra integral equations system. For this purpose, first we substitute the Nth truncation of the Taylor expansion for unknown functions in the origin system. Then we apply the suggested neural net for adjusting the numerical coefficients of given expansions in resulting system. Consequently, the reported architecture using a learning algorithm that based on the gradient descent method, will adjust the coefficients in given Taylor series. The proposed method was illustrated by several examples with computer simulations. Subsequently, performance comparisons with other developed methods was made. The comparative experimental results showed that this approach is more effective and robust.

@&#INTRODUCTION@&#
Integral equation methods offer an attractive alternative to conventional finite difference and finite elements for solving partial differential equations that arise in science and engineering. They offer several advantages: complex physical boundaries are easy to incorporate, the ill-conditioning associated with directly discretizing the governing equation is avoided, high-order accuracy is easier to attain, and far-field boundary conditions are handled naturally. Since these equations usually can not be solved explicitly, so it is required to obtain approximate solution [20]. In recent years, many different algorithms have been developed for solving numerically various classes of integral equations such as Taylor expansion approach [7,15,16], Variational iteration method [12], Homotopy analysis method [3,5,6,13,14], Legendre polynomial method [21,22], artificial neural networks approach [9–11], Bernstein polynomials method [8,17] and Adomian decomposition method [1]. In the most of introduced approaches the integral equation problem is transformed to a linear algebraic system associated to it, in order to determine the unknown coefficients of the approximate solution. But unfortunately in some cases the system corresponding to integral equation problem is ill-conditioned problem. Hence we have to use other valid methods. For this aim, we present artificial neural network approach. Using this method we can approximate solutions all of integral equations system.The goal of this paper is to show how to formulate a linear Volterra integral equations system problem of the general form:(1)∑i=1nAji(t)Fi(t)=fj(t)+∫at∑i=1nKji(s,t)Fi(s)ds,a≤t≤b,j=1,…,n,with the use of the combination of Taylor expansion method and feed-back neural networks approach to compute an approximate solution, and to establish the corresponding convergence analysis. Here, Aji(t) and Kji(s, t) are given real valued functions where assumed to be sufficiently many times differentiable and continuous with respect to all their arguments through the interval of discussion. Also, F(t)=[F1(t), …, Fn(t)] is the vector solution where to be determined. Our proposed algorithm is a method for constructing approximate solutions of the above system, first based on the replacement of unknown functions by truncated series of well known Taylor expansion of solutions, and then estimating the unknown constant coefficients with the use of artificial neural networks (ANNs) approach. To achieve our goal, we differentiate both sides of equations in the resulting system with respect to t, N times. This work reduces the given system into a set of linear algebraic equations. If more and more terms are used from the Taylor series, then the polynomial representations better and better approximate the unknowns. Then, we have aimed to employ a two layer feed-back neural network (FNN) with back-propagation supervised learning algorithm that is based on the gradient descent method to indirect numerical approximation of the resulting system, by starting with an initial guess. It is clear that the series solution converges to the exact solution if such a solution exists. The paper is organized as follows. Section 2 describes how to find a approximate solution of the given Volterra integral equations system by using a combination of Taylor series method and ANNS approach. In Section 3, computations have been done on standard model problems to illustrate the theoretical results. Finally Section 4 ends this paper with a brief conclusion.The main focus in this section will be directed to employ a series solution method wit the use of artificial neural networks approach for solving the mentioned system of linear Volterra integral equations of the second kind. The content of this part is divided into two essential steps, will be briefly outlined below. The first part handles the linear Volterra integral equations system by using the Taylor expansion method. In this part the origin problem is transformed to a linear algebraic system. The second part uses the artificial neural networks approach to complete the numerical approximation procedure. The unknown constant coefficients related to series solutions are estimated to any desired degree of accuracy.In recent decades, a considerable amount of research work has been invested in applying this method to a wide class of linear and non-linear integral equations as well [18,19]. Let us first recall the basic principles of the Taylor polynomial method for solving the present problem. Since these results are the key for our problems therefore we explain them [16,17]. For simplicity reasons, we will apply this process for n=2. In this special case Eq. (3) can be rewritten as following form:(2)A11(t)F1(t)+A12(t)F2(t)=f1(t)+∫at(k11(s,t)F1(s)+k12(s,t)F2(s))dsA21(t)F1(t)+A22(t)F2(t)=f2(t)+∫at(k21(s,t)F1(s)+k22(s,t)F2(s))ds,where the function fp(t) and the kernel Kpi(s, t) have been given and F(t)=[F1(t), F(t)] has to be evaluated. To obtain the solution of the given system in the form of(3)FpN(t)=∑i=0N1i!·Fp(i)(c)·(t−c)i,a≤t,c≤b,p=1,2,which is the Taylor polynomial of degree N at t=c. After substituting (3) into both sides of (1), differentiating each equation N times with respect to t and rearranging terms, we get:(4)∑i=0jji·Ap1(j−i)(t).F1(i)(t)+∑i=0jji·Ap2(j−i)(t)·F2(i)(t)=fp(j)(t)+∑i=0j−1∑r=0j−i−1j−r−1j−r−i−1·∂(r)Kp1(s,t)∂tr|s=t(j−r−i−1)·F1(i)(t)+∑i=0j−1∑r=0j−i−1j−r−1j−r−i−1·∂(r)Kp2(s,t)∂tr|s=t(j−r−i−1)·F2(i)(t)+∫at(∂(j)Kp1(s,t)∂tj·F1(s)+∂(j)Kp2(s,t)∂tj·F2(s))ds,p=1,2;j=0,...,N.Now, for the complete determination of coefficientsFp(i)(c)(for p=1, 2;i=0, …, N), we expand Fp(s) in Taylor series at c=a and then substitute its Nth truncation in (4). Without loss generality we assume that a=0. Because in the otherwise with doing change of variable x=s−a, the lower bound of the integration is transformed to 0. After some simplifications, we have:(5)∑i=0jTp1(j,i)·F1(i)(0)+∑i=0jTp2(j,i)·F2(i)(0)=fp(j)(0)+∑i=0j−1Wp1(j,i)·F1(i)(0)+∑i=0j−1Wp2(j,i)·F2(i)(0),+∑i=0NRp1(j,i)·F1(i)(0)+∑i=0NRp2(j,i)·F2(i)(0),whereTp1(j,i)=j!i!(j−i)!.Ap1(j−i)|t=0,Tp2(j,i)=j!i!(j−i)!.Ap2(j−i)|t=0,i=0,…,j,Vp1(j,i)=∑r=0j−i−1j−r−1j−r−i−1∂(r)Kp1(s,t)∂tr|s=t(j−r−i−1)|t=0,Vp2(j,i)=∑r=0j−i−1j−r−1j−r−i−1∂(r)Kp2(s,t)∂tr|s=t(j−r−i−1)|t=0,i=0,…,j−1,Rp1(j,i)=1i!.∫at∂(j)Kp1(s,t)∂tj|t=0·sids,andRp2(j,i)=1i!·∫at∂(j)Kp2(s,t)∂tj|t=0·sids,i=0,…,N;p=1,2;j=0,…,N.Solving the system (5) will lead to a complete determination of the coefficientsFp(i)(0)(for p=1, 2;i=0, …, N). Having determined these coefficients, the series solutions follow immediately upon substituting the derived coefficients into (3). It should be noted that, if exact solutions are not obtainable, then the obtained series can be used for numerical purposes. In this case, the more terms must be considered for higher accuracy level. The offered technique for determining the constant coefficientFp(i)(0)will be explained below.Artificial neural networks are usually organized into layers of information processing units. A view on this network has emerged that our understanding of the structure and function of the biological neural networks is a key to the success in this field. An ANN is more described as parallel and distributed processing. These kinds of networks are usually presented as systems of interconnected “neurons” that can compute values. These networks are capable of learning, organizing and representing the information. Since last decades, artificial neural networks have been used in many fields. Generally, they are implemented in almost every complicated function and technological field. Note that a vast literature is devoted to neural networks, and the reader can find them in [4].Since the unknown function F(s) in Eq. (1) must be integrable on interval [a, b], therefore we can approximate this with a hybrid neural network.Here, in order to get an iterative scheme for estimating the given non-linear equations system (5), a brief framework of the proposed neural network (FNN) architecture is offered. Consider the two-layer feed-back neural architecture shown in Fig. 1. Let us assume that two vectors X1=(X10, X11, …, X1N) and X2=(X20, X21, …, X2N) be initial values for the unknown Taylor numerical coefficients ofF1(i)(0)andF2(i)(0)(fori=0,…,N), respectively. The input–output relation of each unit of the proposed neural net can be written as follows:Input units:The input neurons make no change in their inputs, so:(6)ori=Xri,r=1,2;i=0,2,…,N.Output units:(7)Yj=f(netj),netj=∑i=0Nwjio1i+∑i=0Nwji′o2i,j=0,1,…,N.where Xriis real number. In above equationwji(wji′)denotes the connection weight from the input signal X1i(X2i) to the jth output unit and f(x) is identical activation function corresponding to output nodes. In this part, an architecture of ANNs has been structured for a particular application in which the network must be trained before it becomes useful. To start this procedure, the initial parameter (weight) must be chosen randomly. Then, optionally a training or learning begins. Note here that, the learning rule specifies how to adjust the network parameters for a given training pattern. In other words, the network needs to be trained with a learning rule which are described below.This architecture is a representing of the Taylor series corresponding to the unknown functions. The input signals are represented to the network and then the network outputs upon the presentation of Xpi(for p=1, 2;i=0, …, N) are calculated. Let us define the matrices Xri,wji,wji′and target vector Bp=(Bpo, …, BpN) as following:wji=Rp1(j,i),i>jRp1(j,i)−Tp1(j,i),i=jRp1(j,i)−Tp1(j,i)−Vp1(j,i),i<j,wji′=Rp2(j,i),i>jRp2(j,i)−Tp2(j,i),i=jRp2(j,i)−Tp2(j,i)−Vp2(j,i),i<j,Xri=Fr(i)(0),Bpj=−fp(j)(0),p,r=1,2;j,i=0,….,N.Suppose that Yp=(Yp0, …, YpN) be the output vector corresponding to the input vectors X1 and X2. Now, a suitable cost function that serves as a criterion function is designed so as to minimize an error measure between the networks output and the corresponding target output. We use a commonly error function namely the mean-squared error to transform the problem to minimizing on the input–output space as follows:(8)epj=(Bpj−Ypj)22,j=0,…,N.In general, the cost function for the given FNN is obtained as:(9)ep=∑j=0Nepj.Learning in artificial neural networks is viewed as a search for parameters which essentially drives the output error to zero. The goal, is to employ an appropriate learning algorithm such that for each input signal Xri, the error epmatches zero. Considering the above idea, we start by deriving a supervised gradient descent-based learning procedure which is a natural generalization of the delta learning rule, for adjusting the parameter Xri.(10)Xri(n+1)=Xri(n)+ΔXri(n),(11)ΔXri(n)=−η·∂ep∂Xri+α·ΔXri(n−1),r=1,2;i=0,…,N,where n is the number of adjustments, η and α are the small constant learning rate and the momentum term constant in which normally chosen between 0 and 1, respectively. The network parameters are updated in the manner that reduces the error and this work yields that the network output converges for each given input to the desired output. To do this, the new value for each input signal is found by taking the current value and adding an amount that is proportional to the slope of training. Now, the partial derivative (∂ep/∂Xri) is to be evaluated at the current input signal values. Using the chain rule for differentiation, one may express the present partial derivative as:(12)∂ep∂Xri=∂ep1∂Xrii+⋯+∂epN∂Xri,r=1,2;i=0,…,N.In other hand∂epj∂Xri=∂epj∂Ypj·∂Ypj∂netpj·∂netpjj∂Xri=(fp(j)(0)+Ypj)·∂netpj∂Xri,j=0,…,N,where∂netpj∂Xri=Rpr(j,i),i>jRpr(j,i)−Tpr(j,i),i=j,Rpr(j,i)−Tpr(j,i)−Vpr(j,i),i<j.Now, upon substituting above relations into (11) and using (10), the desired learning rule will be succeed.Considering the fact that regular nets are universal approximators, therefore the suggested architecture is obviously preferable to approximate solution of the resulting system (5) to any desired degree of accuracy.Here, we illustrate the mentioned method to achieve approximate numerical solutions of three linear Volterra integral equations systems of the second kind. The approximate solutions are compared with exact solutions numerically over a number of iterations as well as by plotting the cost function and difference between the approximate and exact solutions. Note that the empirical results obtained here will be compared with the ones achieved from the Taylor series method. A considerable amount of research work has been granted recently in employing this method to different class of integral equations. In the following simulations, we use the specifications as follows:1.Learning rate, η=0.01,Momentum constant, α=0.003,Stoping conditions, Emax<0.003.Example 3.1The presented iterative scheme is applied for solving the linear Volterra integral equations system:(2t2+3)F1(t)=f1(t)+∫0t(t2−2s)F1(s)ds+∫0t(s2−t)F2(s)ds+∫022sF3(s))ds(1−3t2)F2(t)=f2(t)+∫0ts(t+1)F1(s)ds+∫0tst(t2+1)F2(s))ds+∫0t(2s2+t3)F3(s))ds(3t2+6)F3(t)=f3(t)+∫0t(s−t)F1(s)ds+∫0t(s2−t3)F2(s)ds+∫0t(2ts+s2)F3(s)ds,withf1(t)=−115(6t5+35t4−95t3−270t2−225t−360),f2(t)=−130(15t7+10t6−33t5+275t4+115t3−390t2+150),f3(t)=160(40t6−66t5−175t4+250t3+780t2+360t+360).The exact solutions to this system are, F1(t)=5t+8, F2(t)=2t2−5 and F3(t)=t2+t+1. In this example, we illustrate the use of FNN technique to approximate the solution of this integral equations system. For this case, we used a polynomial of degree 2. Before starting network training, we set that the preliminary functions as:F1(t)=12t2+t+7,F2(t)=12t2+t−4andF3(t)=t2+2t+2.After 154 iterations the above approximate functions transformed to following forms:F1(t)=0.1619t2+5t+7.9837,F2(t)=1.9659t2+0.0879t−5.1337,F3(t)=1.0012t2+t+1.0005.The numerical results using presented method are shown in Table 1, in the case N=2. The error results for proposed method after n iterations are presented in Table 2. Figs. 2–4show the convergence behaviors for computed values of the parametersF1,n(i)(0),F2,n(i)(0)andF3,n(i)(0)for different numbers of iterations. Moreover, this example is going to show the difference between proposed algorithm and Taylor expansion method (TEM). From Table 2, it can be found that the obtained results are very satisfactory, in particular for large n.It is clear that, applying the Nth order Taylor series solution leads to the exact solutions, if the unknown functions be polynomials of degree up to N.Example 3.2Consider the following integral equations system:(3t−8)F1(t)+(−2t+5)F2(t)=f1(t)+∫0t(t+s)F1(s)ds+∫0ttsF2(s)ds4tF1(t)+(t−5)F2(t)=f2(t)+∫0t(2ts−1)F1(s)ds+∫0t(t−s)F2(s))ds,withf1(t)=5et−2t−9sin(t)−t2et+2tcos(t)−tet+3tsin(t),f2(t)=−3t−4et+sin(t)−t2et+tcos(t)+tet+4tsin(t)−1,where the exact solutions are F1(t)=sin(t) and F2(t)=et. In this example, we begin the training process with:F1(t)=12t5+12t4−12t3−12t2+12t+12,F2(t)=12t5+12t4+12t3+12t2+12t+12.After 44 iterations, the approximate solutions become:F1(t)=0.0066t5−0.0059t4−0.1782t3−0.0161t2+0.9892t−0.0009,F2(t)=0.0034t5+0.0313t4+0.15195t3+0.48816t2+1.0007t+1.0007.Numerical result can be found in Tables 3 and 4. Figs. 5 and 6show the convergence behaviors for computed values of the vectorsF1,n(i)(0)andF2,n(i)(0)where n is index of iterations. Similarly exact solution and the approximated solution are compared in Fig. 7.It is clear that to get the best approximate solutions, we must take more terms from the Taylor expansions of unknowns, that is, the truncation limit N must be chosen large enough.Example 3.3Our last test problem is the integral equations system:t2+t+110000F1(t)+−3t2+4t+11000F2(t)=f1(t)+∫0t(t2+s3)F1(s)ds(5t−3)tF1(t)+(t−1)F2(t)=f2(t)+∫0t2tsF2(s))dswithf1(t)=110000(4000t5−32500t4+60000t3−89990t2+49968t+5),f2(t)=−12(t5−4t4−2t3+28t2−68t+30).which has the exact solutions F1(t)=−2t+5 and F2(t)=t2−3t. Similarly, before starting calculations we assumed that:F1(t)=t2−3t+4,F2(t)=2t2−4t+1.After 104 iterations, the approximate functions F1(t) and F2(t) are estimated as:F1(t)=0.0889t2−1.8971t+5.0458,F2(t)=0.8964t2−3.05t−0.017.Similarly, numerical result can be found in Table 5. Figs. 8–10show the accuracy of the solution or the convergence behaviors for computed values of the vectorsF1n(i)(0)andF2n(i)(0)where n is index of iterations.In this case, the resulting linear equations system corresponding to the applying Taylor expansion method, is an ill-conditioned problem. Therefore it is so difficult for us to find its solution directly. But with using of the proposed FNN, we can obtain approximate solution of the given problem to any desired degree of accuracy. By comparing the numerical results indicated in Tables 1–5, we observe that the presented method converges to exact solutions of problem, when n grows.

@&#CONCLUSIONS@&#
A combinative algorithm of approximating unknown functions in terms of truncated series involving Taylor polynomials method by applying an architecture of artificial neural networks is proposed here for solving linear Volterra integral equations system. The presented FNN in this study is a method for computing unknown constant coefficients in the Taylor expansions of solution functions. Excellent concurrence is seen to has been obtained between the exact and approximate solutions computed numerically by choosing high-order polynomial terms. With the availability of this methodology, now it will be possible to investigate the approximate solution of ill-conditioned system of Volterra integral equations to any desired degree of accuracy. The analyzed examples illustrate the ability and reliability of the present method. The obtained solutions, in comparison with exact solutions admit a remarkable exactness.