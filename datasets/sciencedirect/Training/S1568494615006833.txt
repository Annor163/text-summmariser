@&#MAIN-TITLE@&#
An improved monkey algorithm for a 0-1 knapsack problem

@&#HIGHLIGHTS@&#
The 0-1 knapsack problem is a classic combinational optimization problem.The monkey algorithm (MA) is a novel swarm intelligent based algorithm.This paper proposed a binary version of the monkey algorithm for solving 0-1 knapsack problem.The result of the proposed algorithm provides better results in solving the 0-1 knapsack problem compared with the other solving methods.

@&#KEYPHRASES@&#
Monkey algorithm,Binary version of the monkey algorithm,Knapsack problem,Cooperation process,Greedy strategy,

@&#ABSTRACT@&#
The 0-1 knapsack problem is a classic combinational optimization problem. However, many exiting algorithms have low precision and easily fall into local optimal solutions to solve the 0-1 knapsack problem. In order to overcome these problems, this paper proposes a binary version of the monkey algorithm where the greedy algorithm is used to strengthen the local search ability, the somersault process is modified to avoid falling into local optimal solutions, and the cooperation process is adopted to speed up the convergence rate of the algorithm. To validate the efficiency of the proposed algorithm, experiments are carried out with various data instances of 0-1 knapsack problems and the results are compared with those of five metaheuristic algorithms.

@&#INTRODUCTION@&#
Knapsack problems (KPs) were first proposed by Dantzig in the 1950s [1] and subsequently attracted many scholars. Zou et al. described a novel global harmony search (HS) algorithm that includes two important operations, position updating and genetic mutation, for solving 0-1 knapsack problems [2]. The experiment shows the proposed algorithm has good performance for solving large-scale knapsack problems. Xiang et al. also considered a novel global-best harmony search algorithm that combines a two-phase repair operator to repair an infeasible harmony vector and to further improve a feasible solution [3]. Changdar et al. considered an improved ant colony optimization (ACO) approach to solve 0-1 knapsack problems in a fuzzy environment [4]. The proposed algorithm creates n candidate groups for n objects; each ant selects a candidate value from each group. Bansal et al. proposed a modified particle swarm optimization called MBPSO based on binary particle swarm optimization (BPSO) [5] for solving 0-1 knapsack problems and multidimensional knapsack problems [6]. Azad et al. proposed a simplified binary version of the artificial fish swarm algorithm (AFSA) for solving 0-1 quadratic knapsack problems [7]; the random heuristic drop_item procedure is used to make the points feasible, and the heuristic add_item is also implemented to improve the quality of the solutions. Glover used an improved greedy algorithm and surrogate constraints for linear and quadratic knapsack problems [8]. Sitarz studied the relations between multiple criteria dynamic programming (MCDP) and the multiple knapsack problem [9]; the paper also showed how to use MCDP methods to solve multiple knapsack problems. Gao et al. presented a novel quantum-inspired artificial immune system called MOQAIS, which is composed of a quantum-inspired artificial immune algorithm (QAIS) and an artificial immune system (BAIS) for multiobjective knapsack problems [10]. QAIS is responsible for exploration of the search space, and BAIS is applied for exploitation of the search space. García-Martínez proposed a tabu-enhanced destruction mechanism for an iterated greedy search in studying quadratic multiple knapsack problems [11]; the method records the last removed objects and avoids removing them again in subsequent iterations. Baykasoğlu et al. used a priority-based encoding technique for a firefly algorithm (FA) to construct feasible solutions and prevent infeasibility for solving dynamic multidimensional knapsack problems [12]. Hifi et al. used a dichotomous search-based exact method based on decomposing the original problem into a series of knapsack problems and introduced new upper bounds and incremental valid lower bounds in the interval search [13]. Levin et al. studied the stochastic behavior of the knapsack problem and defined a variant problem in which item values are deterministic and item sizes are independent random variables [14]. Zhao used a nonlinear reductive dimension approximate algorithm for the knapsack problem [15]. Liu et al. removed useless solution regions before applying simulated annealing (SA) to solve a knapsack problem and extracted the most possible part of the optimal solution space from the whole optimal solution space [16]. Saraç et al. used a genetic algorithm (GA) and a hybrid solution approach for solving a developed model called the generalized quadratic multiple knapsack problems (G-QMKP) [17]. The experiment shows the proposed hybrid solution approach can obtain good solutions in a reasonable time for a large-scale problem. Lin investigated using GA in solving the fuzzy knapsack problem [18]; this method simulates a fuzzy number by distributing it into partition points and uses GA to evolve the values in each partition point.The monkey algorithm (MA) is a new type of swarm intelligence based algorithm. It was proposed by Zhao and Tang in 2008 and is derived from simulation of the mountain-climbing processes of monkeys [19]. It consists of three processes: the climb process, watch–jump process and somersault process. The climb process is designed to gradually improve the objective function value. However, MA will spend considerable computing time searching for local optimal solutions in the climb process. To reduce the computing time and speed up the convergence rate, Chen and Zhou introduced the inertial step in the climb process and combined the simple method after the somersault process [20]. The watch–jump process can speed up the convergence rate of the algorithm, the purpose of the somersault process is to make monkeys find new search domains to avoid falling into local search. The algorithm has the advantages of simple structure, strong robustness, and not easy falling into local optimal solutions. Therefore, MA has been successfully applied in solutions to various optimization problems, such as transmission network expansion planning [21], intrusion detection technology [22], optimal sensor placement in structural health monitoring [23], the optimization of gas filling station project scheduling problem [24], the clustering analysis problem [25], etc. In this paper, the 0-1 knapsack problem will be studied, and a binary version of the monkey algorithm which combines cooperation process and greedy strategy (CGMA) is proposed. The algorithm improves the calculation accuracy and increases the convergence speed of the algorithm to a certain degree. The numerical experiment results show that the proposed algorithm has good performance in solving the 0-1 knapsack problem. It can be an efficient alternative for solving the 0-1 knapsack problem.The 0-1 knapsack problem is a typical NP-hard problem in operations research [26]. The problem is defined as follows:Given a set of items O={o1, o2, …, on}, each with a weightwiand a value pi, determine the number of each item to include in a collection so that the total weight WX is less than or equal to a given limit and the total value PX is as large as possible. It derives its name from the problem faced by someone who is constrained by a fixed-size knapsack and must fill it with the most valuable items. Its mathematical model is as follows:(1)maximizef(x1,x2,…,xn)=PX=∑i=1npixisubject toWX=∑i=1nwixi≤Vand xj∈{0, 1}, j=1, 2, …, n. Where P=(p1, p2, …, pn),W=(w1,w2,…,wn)represent the value vector and weight vector of all items. V is the maximum capacity of the knapsack. xi=1 indicates that item i is included in the knapsack and xi=0 that it is not.The monkey algorithm was first proposed to solve numerical optimization problems as a new swarm intelligence based algorithm stemmed from the mountain-climbing behavior of monkeys [19]. Assume that there are many mountains in a given field. At the beginning, the monkeys climb up from their respective positions to find the mountaintops (this action is called climb process). When a monkey get the top of its mountain, it will find a higher mountain within the sight and jump somewhere of the mountain from the current position (this action is called watch–jump process), then repeat the climb process. After repetitions of the climb process and the watch–jump process, each monkey will somersault to a new search domain to find a much higher mountaintop (this action is called somersault process).This paper proposed a binary version of the monkey algorithm where the greedy algorithm is used to correct the infeasible solutions and to improve the quality of the feasibility, the somersault process is modified to avoid falling into local search, the cooperation process is implemented to speed up the convergence rate, and the control parameter is used to keep the population diversity. The algorithm consists of 5 parts, the climb process, watch–jump process, greedy strategy repair process, cooperation process and somersault process.For the 0-1 knapsack problem, each item has two different status, namely the item has been included in the knapsack or it remains out. First, M is defined as the population size of monkeys. For monkey i, its position is denoted as a vector Xi=(xi1, xi2, …, xin), and this position will be employed to express a solution of the 0-1 knapsack problem, where xij∈{0, 1} and j=1, 2, …, n, n is the number of the items. xij=1 indicates the item j is included in the knapsack and xij=0 indicates it is not.In CGMA, the initial population is randomly generated. The random initialization process of M monkeys and n items is as follows:for i=1 to M dofor j=1 to N dox[i][j]=rand();If x[i][j]<0.5x[i][j]=0;elsex[i][j]=1;endifendforendforwhere xi,jrepresents the jth component in the vector Xi.According to the idea of pseudo-gradient-based simultaneous perturbation stochastic approximation (SPSA) [27], the climb process is a step-by-step procedure to improve the objective function by choosing a better one between two positions that are generated around the current position. For the monkey i, its position is Xi=(xi1, xi2, …, xin), i=1, 2, …, M, respectively. f(Xi) is the corresponding objective function value. The improved climb process is given as follows:(1)Randomly generate two vectorsΔx′i=(Δx′i1,Δx′i2,…,Δx′in)andΔx″i=(Δx″i1,Δx″i2,…,Δx″in), where(2)Δx′ij,Δx″ij=awith probability12−awith probability12j=1, 2, …, n, respectively. The parameter a(a>0), called the step of the climb process, can be determined by specific situations. Here, set the climb step a=1 for the 0-1 knapsack problem.Setx′ij=|xij−Δx′ij|andx″ij=|xij−Δx″ij|, j=1, 2, …, n, respectively, where |x| represents the absolute value of x. SetX′i=(x′i1,x′i2,…,x′in),X″i=(x″i1,x″i2,…,x″in).Calculatef(X′i)andf(X″i), i=1, 2, …, M, respectively.Iff(X′i)>f(X″i)andf(X′i)>f(Xi), setXi=X′i. Iff(X″i)>f(X′i)andf(X″i)>f(Xi),Xi=X″i.Repeat steps (1) to (4) until the maximum allowable number of iterations (called the climb number, denoted by Nc) has been reached.When the monkeys reach the top of the mountains, each monkey will take a look and determine whether there are higher points than the current one within its sight. If yes, it will jump somewhere of the mountain from the current position and then repeat the climb process. For the monkey i, its position is Xi=(xi1, xi2, …, xin), i=1, 2, …, M. The improved watch–jump process is given as follows:(1)Randomly generate a real yjin interval [xij−b, xij+b], j=1, 2, …, n, respectively, where b eyesight of the monkey can be determined by specific situations. Set Y=(y1, y2, …, yn).Because of the real yj∈[−1, 2], if yj<0.5, set yj=0; otherwise, set yj=1.Calculate f(Y), i=1, 2, …, M, respectively.If f(Y)>f(Xi), then Xi=Y, Xi=Y.Repeat the climb process steps (1) to (4) until the maximum allowable number of iterations (called the watch–jump number, denoted by Nw) has been reached.An abnormal encode individual (who does not meet the constraint conditions) may be obtained in solving the 0-1 knapsack problem using the metaheuristic algorithms. In [28], the local search strategy-greedy algorithm (GTA) is introduced to correct the infeasible solutions and to improve the quality of the feasibility. Assume that P=(p1, p2, …, pn) andW=(w1,w2,…,wn)represent the value vector and weight vector of all items. V is the maximum capacity of the knapsack. X=(x1, x2, …, xn) is an abnormal encode individual, and Y=(y1, y2, …, yn) is the adjusted normal solution. GTA is described as follows according to the 0-1 knapsack problem:Let Q(q1, q2, …, qn) be a sequence that sorts by value densitypi/wifrom large to small for all items xi=1in the vector X=(x1, x2, …, xn). GTA(X, W, P, V) Algorithm [28]:k=1;Temp=WQ[k];while Temp≤V doyQ[k]=1;k=k+1;Temp=Temp+WQ[k];endwhilefor j=k to n doyQ[j]=0;endforAfter the climb process and the watch–jump process, each monkey will arrive at the top mountain in its neighborhood. However, they differ among all the monkeys. The purpose of the cooperation process is to make the monkeys find a better solution by cooperating with the monkey that has the best position. The monkeys will go forward along the direction of the best monkey. This process can speed up the convergence rate. Assume that the optimal position isX*=(x1*,x2*,…,xn*). For the monkey i, its position is Xi=(xi1, xi2, …, xin), i=1, 2, …, M. The cooperation process is given as follows:(1)Randomly generate a real number α from the interval [0,1].If α<0.5, then yj=xj, otherwise,yj=xj*, j=1, 2, …, n, respectively.Update the monkeys’ position Xiwith Y.Monkeys will reach the maximal mountaintops around their initial points after repetitions of the climb process and the watch–jump process, respectively. To find a higher mountaintop, each monkey will somersault to a new search domain. The new position is not blind to choose, but limited within a certain region, which is determined by the pivot and the somersault interval. Fig. 1shows the somersault process of the original MA [8]. The points A, B, C, D represent monkeys. The point P is the center of all monkeys, the somersault interval [c,d]=[−1,1]. For example, the monkey A can reach any point (such as point P, A1, A2) within the circle r1 with the somersault interval [c,d]=[−1,1].The somersault process can effectively prevent monkeys falling into the local search. However, after many iterations, the somersault process may lose efficacy. The monkeys will fall into the local optima domain, and the population diversity will decrease. In the original MA, the monkeys will somersault along the direction pointing to the pivot, which is equal to the barycenter of all monkeys’ current positions. Here, we randomly choose a monkey's position as the pivot to replace the center of all monkeys and adopt a new somersault process. For the monkey i, its position is Xi=(xi1, xi2, …, xin), i=1, 2, …, M. The improved somersault process is given as follows:(1)Randomly generate real numbers θ from the interval [c,d] (called the somersault interval, which governs the maximum distance that monkeys can somersault).Randomly generate an integer k, k=1, 2, …, M, respectively. Let the location of the monkey k be the somersault pivot.Calculate(5)yj=xkj+θ(xkj−xij)Set yi=0, if yj<0.5; otherwise, set yj=1. Update the monkey's position Xiwith Y.After repetitions of the somersault process, monkeys may reach the same domain to make the somersault process lose efficacy. In case of this problem, we set a parameter called “limit” to control monkeys running into the local optima solution. If the global optimal solution is not improved by a predetermined number of trials, the monkeys are abandoned and then reinitialized.Following the climb process, watch–jump process, greedy method optimization process, cooperation process and somersault process, all monkeys are ready for their next actions. The condition for terminating CGMA iteration could either be when the optimal solution has been found or when a relatively large number of iterations have been reached. To summarize, the whole flowchart of the CGMA to find the optimal solution of the 0-1 Knapsack problem is shown in Fig. 2.In this section, the experiments were performed using a desktop computer with a 3.01GHz AMD Athlon(tm) II X4640 processor, 3GB of RAM, running a minimal installation of Windows XP. The application software was Matlab 2012a.To evaluate the performance of CGMA, it is compared with BPSO [6], MBPSO [6], NGHS [2], DGHS [3], and S-bAFSA [7]. To make a fair comparison, we use the maximum number of function evaluations (maxFEs) as the evaluation standard. In addition, the other specific parameters of the algorithms are given as follows:For BPSO and MBPSO, the population size is set to 5. The acceleration coefficients are c1=2, c2=2. The maximum velocity Vmax is set to 2.For NGHS, the harmony memory size HMS, namely, the population size, is set to 5; the mutation probability pm=2/N, and the penalty coefficient is set to 1020, which is the same as those utilized in [13].For DGHS, HMS=5. The maximum and minimum harmony memory consideration rate HMCRmin, HMCRmin are set to 0.95 and 0.3, respectively. The pitch adjusting rate PAR=0.75.For CGMA, the population size is 10. The climb number Nc=2, watch–jump number Nw=2, climb step a=1, eyesight b=1, and somersault interval [c, d]=[−1, 1].Here, two sets of small-scale knapsack problems are selected to test the performance of CGMA. The first set that contains 10 instances is taken from [2]. The second set comes from [6] and is tested in [4,29] also. The number of items in these instances range between 8 and 24, and the maxFEs are listed in the Table 2. For every instance, each algorithm is run 50 times individually with random initial solutions.Table 1shows the results of CGMA for the first set. Here, the control parameter is set as limit=10 (10 refers to iterations, and each iteration may need multiple function evaluations), the maxFEs is set to 1000 for all algorithms. As observed from the results, for the small-scale knapsack problems, CGMA can obtain the global optimal solution in all runs. Shi [32] proposed an improved ant colony algorithm to solve the knapsack problems (test problem 1 and 2), but the optimal solutions cannot be obtained in the paper. The optimal value of test problem 1 obtained by CGMA is 295, and the best value of test problem 2 is 1024. For the test problems F3, F4, and F5, the optimal solutions can be obtained by CGMA in all runs. For the test problem F6, Zou proposed a global harmony search algorithm to solve the problem in [2]. Unfortunately, the optimal solution of the problem cannot be found by NGHS [2]. The optimal solution for F6 obtained by CGMA is x=(0, 0, 1, 0, 1, 1, 1, 1, 1, 1), and the optimal value is 52. For the test problem F7, F8, F9, F10, the optimal solution can be found by CGMA, HS, and NGHS in all runs. The optimal values of the test problem F7, F8, F9 and F10 are 107, 9767, 130 and 1025, respectively.Table 2shows the results for all the algorithms for the second test set, which includes 25 test problems. “limit” is the control parameter. It is set to 10 for the test problems 11 to 20 and 20 for the other problems. The value of maxFEs is set to 1000 for the test problems 11 to 20 and 5000 for the other problems. The results of BPSO and MBPSO are from [6]. For the test problems F11 to F23, F26, F27, F32 and F35, CGMA can obtain the global optimal values in all runs, and the success rate (SR) can reach 100%. For other test problems, the mean values of CGMA are obviously better than BPSO, MBPSO, NGHS and DGHS. BPSO cannot find the global optimal value on all problems with SR=100%. MBPSO can find the global optimal values in all runs on F11 to F14, F20. NGHS can also find the global optimal values on five test problems with SR=100%, that is, F11 to F15. However, the mean values of NGHS on all the test problems are worse than that of MBPSO except F15. In addition, the mean values of MBPSO on all the test problems are better than that of BPSO. DGHS can find the global optimal values on 14 problems with SR=100%, that is, F11 to F15, F17, F19, F21 to F23, F26, F27, F32, F35. BPSO and NGHS are inferior to MBPSO. For the test problems F11 to F15, F17 to F23, F26, F27, F32, F35, S-bAFSA can obtain the global optimal values in all runs, and the success rate (SR) can reach 100%. Only the mean value of S-bAFSA on the test problem F34 is better than that of CGMA. Its performance is obviously better than BPSO, MBPSO, NGHS and DGHS. BPSO and NGHS are inferior to MBPSO and DGHS.In this subsection, for the small-scale knapsack problems, CGMA has a higher success rate than other algorithms on all the test problems. The proposed algorithm has better performance than BPSO, MBPSO, NGHS, DGHS and S-bAFSA.

@&#CONCLUSIONS@&#
This paper proposed a binary monkey algorithm for solving 0-1 knapsack problems. The algorithm used the greedy algorithm to correct the infeasible solutions and to improve the feasibility, introduced the cooperation process to speed up the convergence rate, modified the somersault process by randomly choosing one monkey as the pivot of another to avoid falling into the local optimal solutions, and reinitialized the population if the global optimal solution was not improved after a predetermined number of generations. The experiments show that the proposed CGMA algorithm has strong advantages in solving 0-1 knapsack problems for testing fixed and random problems and small- and large-scale problems. CGMA can be an efficient alternative for solving the 0-1 knapsack problem.