@&#MAIN-TITLE@&#
Multi-label feature selection based on neighborhood mutual information

@&#HIGHLIGHTS@&#
Different from the traditional multi-label feature selection, the proposed algorithm derives from different cognitive viewpoints.A simple and intuitive metric to evaluate the candidate features is proposed.The proposed algorithm is applicable to both categorical and numerical features.Our proposed method outperforms some other state-of-the-art multi-label feature selection methods in our experiments.

@&#KEYPHRASES@&#
Feature selection,Multi-label learning,Neighborhood,Neighborhood mutual information,

@&#ABSTRACT@&#
Multi-label learning deals with data associated with a set of labels simultaneously. Like traditional single-label learning, the high-dimensionality of data is a stumbling block for multi-label learning. In this paper, we first introduce the margin of instance to granulate all instances under different labels, and three different concepts of neighborhood are defined based on different cognitive viewpoints. Based on this, we generalize neighborhood information entropy to fit multi-label learning and propose three new measures of neighborhood mutual information. It is shown that these new measures are a natural extension from single-label learning to multi-label learning. Then, we present an optimization objective function to evaluate the quality of the candidate features, which can be solved by approximating the multi-label neighborhood mutual information. Finally, extensive experiments conducted on publicly available data sets verify the effectiveness of the proposed algorithm by comparing it with state-of-the-art methods.

@&#INTRODUCTION@&#
In classical supervised learning, each instance only belongs to one label relative to a number of candidate labels. In many real-world applications, however, one instance is usually associated with multiple concepts simultaneously [8,7,2,27,45,44]. For example, a newspaper article concerning the reactions of the scientific circle to the release of the Da Vinci Code film can be classified into any of the three classes: arts, science, and movies; an image showing a tiger in woods is associated with several keywords such as trees and tiger. As we know, one label per object is unable to fully describe such scenario, and therefore the research on multi-label classification task has attracted increasing interest [11,20,21,31,48,50,52,62]. In which, Zhang et al. [61] presented a profound review on multi-label learning algorithms, which includes the fundamentals on multi-label learning, eight representative algorithms, and several related learning settings.In many real-world applications, the multi-label data usually have thousands or even tens of thousands of features [15,22,61]. This is a more common characteristic in image annotation and text categorization. For example, millions of informative words are extracted from a collection of documents or web pages to represent their topics. Also, from an image thousands of features are extracted to reflect its all kinds of semantics. Generally speaking, many features are redundant and/or irrelevant for a given learning task, and high dimensional data may brings many disadvantages to learning algorithms, such as computational burden, over-fitting, and poor performance [4,14,17,18,29,33,46,47]. To solve this problem, a number of dimensionality reduction based multi-label learning methods have been presented. Those methods can be grouped into two categories: multi-label feature extraction and multi-label feature selection. Multi-label feature extraction is a method that converts original high-dimensional feature space into a new low-dimensional feature space through transforming or mapping, and the new constructed features are usually combinations of original features. However, it is difficult to link the features from original feature space to new features. At present, some popular feature extraction methods have been proposed, such as Partial Least Squares (PLS) [49], Linear Discriminant Analysis (LDA) [16], Canonical Correlation Analysis (CCA) [10], and multi-label informed latent semantic indexing (MLSI) [56]. To sum up, the characteristics of feature extraction include (1) the results of feature extraction are lack of interpretation; (2) feature extraction blurs the information of original features and loses physical interpretation.Different from multi-label feature extraction, multi-label feature selection selects the feature subset from the original feature space directly, and keeps the physical meaning for the selected features. Multi-label feature selection methods are usually classified into three main groups: filter, wrapper, and embedded [37,42,43]. The filter approach separates feature selection from classifier learning [39,58]. The wrapper approach uses the predictive accuracy of a predetermined learning algorithm to determine the quality of selected features [12,60]. The embedded approach achieves model fitting and feature selection simultaneously [15]. As we know, the key step of the filter approach is to design effective metrics to evaluate the quality of the candidate features, such as mutual information [6,23–25], dependency [58], and the classification margin [40,41]. As for mutual information, Lee et al. [23] proposed a multivariate mutual information based feature selection method for multi-label classification, which selects an effective feature subset via maximizing the multivariate mutual information between the selected features and labels. In [27], information gain between a feature and label set is exploited to measure the importance of the feature and label correlation. Yu et al. [57] proposed a multi-label feature selection algorithm based on mutual information and genetic algorithm. In addition, the mutual information measure is applied in [6] according to a modified LP approach [35], which considers label dependence.In order to compute mutual information for hybrid data, we should know the probability distributions of variables and their joint distributions. However, these distributions are not known in advance. In addition, the process of discretization easily loss useful information. Therefore, Hu et al. [17] presented an assumption that samples with the similar feature values should be classified into the same class or neighborhood class. Based on this assumption, the equivalent relation is extended into neighborhood relation [19,51,54], where neighborhood, computed with distance, is looked as the subset of instances which have the similar feature values with the centroid. Moreover, Hu et al. [17] integrated the concept of neighborhood into Shannon's information theory [38], and proposed a new information measure, called neighborhood information entropy. Then, joint neighborhood entropy, conditional neighborhood entropy, and neighborhood mutual information can be defined directly. However, these concepts cannot be used to multi-label learning directly. Different from single-label learning, each instance belongs to a set of labels in multi-label learning. Therefore, we need redefine the concept of neighborhood information entropy and its relative concepts. In this work, we generalize neighborhood entropy in single-label learning to fit multi-label learning, and propose three new measures of neighborhood mutual information, which can be used to evaluate the quality of the candidate features.Our work is focused on three problems. First, we introduce the margin of instance to granulate all instances under different labels. Meanwhile, we present three different cognitive viewpoints, i.e., optimistical viewpoint, neutral viewpoint, and pessimistic viewpoint. Based on these viewpoints, three kinds of neighborhood for multi-label learning are introduced, and the new definitions on neighborhood information entropy and neighborhood mutual information are proposed. Second, we discuss the problem how to use the proposed measures in multi-label feature selection. In which, we present an optimization objective function to evaluate the quality of the candidate features, which can be solved by approximating multi-label neighborhood mutual information. This solution has the potential of being a general strategy to multi-label feature selection. Finally, a comprehensive set of experiments is conducted to show the effectiveness of our proposed method. The main contributions of this paper can be summarized as follows:•Different from the traditional multi-label feature selection, the proposed algorithm derives from different cognitive viewpoints.A simple and intuitive metric to evaluate the candidate features is proposed.The proposed algorithm is applicable to both categorical and numerical features.Our proposed method outperforms some other state-of-the-art multi-label feature selection methods in our experiments.The rest of this paper is organized as follows. Section 2 introduces multi-label learning and neighborhood mutual information. Then, we present the multi-label feature selection based on multi-label neighborhood mutual information method and report on experimental evaluations in Sections 3 and 4, respectively. Finally, our conclusions are given in Section 5.In multi-label learning with m labels,X⊂Rddenotes a multi-label data set, and x∈X is represented as a d-dimensional vector x=[x1, x2, …, xd]. Let L={l1, l2, …, lm} be a set of labels. Each data point is associated with a subset of L, and this subset can be described as a m-dimensional vector y=[y1, y2, …, ym] where yj=1 only if x has label ljand 0 otherwise.In multi-label classification learning, the evaluation functions are different from the traditional single-label classification learning ones. In experimental evaluation, we select some measures proposed in [36]. Let T={(xi, yi)|1≤i≤N} be a given testing set where yi⊆L is a correct label subset, andYi′⊆Lbe the binary label vector predicted by a multi-label classifier for instance xi.Average Precision (AP): this measure evaluates the average fraction of labels ranked above a particular label γ∈yi, which is actually in yi. The formula for AP isAP=1N∑i=1N1|yi|∑γ∈yi|{γ′∈yi:ri(γ′)≤ri(γ)}|ri(γ)where ri(l) stands for the rank of label l∈L predicted by the algorithm for a given instance xi. The bigger the value, the better the performance.Coverage (CV): this measure evaluates how far, on average, we need to go down the label ranking list to cover all the ground-truth labels of the instance. CV is defined as followsCV=1N∑i=1Nmaxλ∈yirank(λ)−1where rank(λ) denotes the rank list of λ according to its likelihood, for example, if λ1>λ2, then rank(λ1)<rank(λ2). In the case of coverage smaller value shows better performance.Hamming Loss (HL): this measure evaluates how many times an instance-label pair is misclassified. HL isHL=1N∑i=1N|Yi′⊕yi|Mwhere ⊕ denotes the XOR operation. Here the smaller value denotes better performance.Ranking Loss (RL): this measure evaluates the fraction of reversely ordered label pairs. RL is expressed with the following wayRL=1N∑i=1N1|yi||yi¯||{(λ1,λ2)|λ1≤λ2,(λ1,λ2)∈yi×yi¯}|where λjis a real-valued likelihood between xiand each label li∈L based on a multi-label classifier, andyi¯denotes the complementary set of yi. Smaller value indicates better performance.Given a set of instances U={x1, x2, …, xn}, xi∈Rd, we can define a distance function Δ on U that satisfies Δ(xi, xj)≥0. The p-norm distance is then written as(∑k=1N|xik−xjk|p)1/p, such that p=1, p=2, and p=∝ denote Minkowski distance, Euclidean distance, and Chebychev distance, respectively. Given δ≥0, we claim the neighborhood of instance x by δ(x)={xi|Δ(x, xi)≤δ}. Intuitively speaking, different instances with similar feature values should be classified into the same class. In addition, the cardinality of δ(x) depends on the neighborhood size δ (in this paper, neighborhood size and granularity are equivalent terms).In data-driven learning, the probability distribution of each variable is usually unknown a priori. Therefore, Shannon's entropy and mutual information cannot be used to compute the relevance between numerical features due to the difficulty of estimating the probability density. To solve this problem, Hu [17] introduced the concept of neighborhood into information theory, and generalized Shannon's entropy for numerical information. Meanwhile, Lin [30] utilized neighborhood entropy to assess the quality of source in multiple information sources. Finally, the following definitions and theorems are better described in [17].Definition 1Given a set of instances U={x1, x2, …, xn} described by numerical or discrete features F, where f⊆F is a subset of features, the neighborhood of instance xiin f is denoted by δf(xi). The neighborhood uncertainty of the instance is then defined as(1)NHxiδ(f)=−log||δf(xi)||n,and the average uncertainty of the set of instances is(2)NHδ(f)=−1n∑i=1nlog||δf(xi)||n.If δ≤δ′,NHδ(f)≥NHδ′(f).If δ=0, then NHδ(f)=H(f), where H(f) is Shannon's entropy.Definition 2Suppose r, f⊆F are two subsets of features. When the neighborhood of instance xiin feature subspace f⋃r is denoted by δf⋃r(xi), the joint neighborhood entropy is computed as(3)NHδ(r,f)=−1n∑i=1nlog||δf⋃r(xi)||n.In particular, if r is a set of input variables and f is the classification feature c, we define δr⋃c(xi)=δr(xi)⋂δc(xi). Then,(4)NHδ(r,c)=−1n∑i=1nlog||δr(xi)⋂δc(xi)||n.NHδ(r, f)≥NHδ(r), NHδ(r, f)≥NHδ(f).Definition 3Suppose r, f⊆F are two subsets of features. The conditional neighborhood entropy of r to f is defined as(5)NHδ(r|f)=−1n∑i=1nlog||δr⋃f(xi)||||δf(xi)||.NHδ(r|f)=NHδ(r, f)−NHδ(f).Definition 4Suppose r, f⊆F are two subsets of features. The neighborhood mutual information of r and f is defined as(6)NMIδ(r;f)=−1n∑i=1nlog||δr(xi)||·||δf(xi)||n||δr⋃f(xi)||.Given two subsets of features r and f, let NMIδ(r;f) be their mutual information. The following equations then hold:(1)NMIδ(r;f)=NMIδ(f;r);NMIδ(r;f)=NHδ(r)+NHδ(f)−NHδ(r;f).For nominal data, a set of instances with the same attribute values is called an equivalent class or an equivalent granule. Similarly, for mixed data, a set of instances with the similar attribute values can be called a neighborhood class or a neighborhood granule. In which, mixed data refers to there coexist some numeric features, such as categorical and real-valued. To granulate all instances, we adopt margin [13] to set neighborhood size. Given an instance x, the margin of x with respect to a set of instances U, is defined as(7)m(x)=Δ(x,NS(x))−Δ(x,NT(x))where NS(x) and NT(x) are the nearest instances from different and the same labels, which called nearest miss (NS) and nearest hit (NT), respectively. Δ(x−NS(x)) denotes the distance between x and NS(x), and Δ(x−NT(x)) denotes the distance between x and NT(x). Therefore, we call δ(x)={y|Δ(x, y)≤m(x))} as a neighborhood granule with respect to x. In order to let all instances in the same neighborhood granule having the same class label, we set m(x)=0 if m(x)<0 for x, due to the principle of nearest neighbor classification. In addition, the size of the margin depends on the distance metric. In order to show the definition of margin of instance clearly, we depict x, NS(x), and NT(x) in Fig. 1.In multi-label learning, each instance is associated with a set of labels simultaneously, and each instance can be assigned to positive instances or negative instances with each label. As a result, we can transform the representation of multi-label data to differentiate instances in each label, and define the new concept of neighborhood entropy and neighborhood mutual information for multi-label learning. We now illustrate the process of transformation by Examples 1 and 2.Example 1Table 1illustrates a multi-label data set. There are four instances x1, x2, x3, and x4. In which, x1 belongs to the set of labels {l2, l3}, x2 belongs to the set of labels {l1}, x3 belongs to the set of labels {l3, l4}, and x4 belongs to the set of labels {l1, l3}, respectively.From Example 1, we can see that each instance basically belongs to a set of labels. Therefore, on the basis of the transformation of multi-label data, each instance can be divided into positive instance or negative instance for a given label.Example 2(Continued from Example 1). Table 2is the transformation of Table 1. In Table 2, there exist positive instances and negative instances with respect to each label. For example, the positive instances of l1 is {x2, x4}, and the negative instances of l1 is {x1, x3}.From Example 2, one instance may be positive or negative with respect to different labels. Therefore, we can obtain different granularity for a given instance under different labels, i.e., each instance has many granularity corresponding to different labels. Given an instance x and a label li∈L, the margin of instance x with respect to liis defined as(8)mli(x)=Δ(x,NSli(x))−Δli(x,NTli(x)),∀li∈L.From Eq. (8), we know that each instance has different granularity relative to its different labels. Therefore, it is an important issue to form multi-label granularity via combining all single-label granularity for a given instance. From the decision making viewpoint or cognitive viewpoint [26,32,53,55], different granularity combination represent different standpoints, and have their advantages and disadvantages. Therefore, we select the maximal granularity, average granularity, and minimum granularity as three representative granularity or viewpoints, and we claim these viewpoints as optimistical viewpoint, neutral viewpoint, and pessimistic viewpoint, respectively. Given an instance x and a label li∈L, the margin of x based on different viewpoints are formulated as follows:Optimistic viewpoint:(9)mopt(x)=ml(x),wherel=argmaxlimli(x).Neutral viewpoint:(10)mneu(x)=1|L|∑i=1Lmli(x).Pessimistic viewpoint:(11)mpes(x)=ml(x),wherel=argminlimli(x).From Eqs. (9)–(11), we can obtain three different definitions of neighborhood of instance from three different cognitive viewpoints.Definition 5Given a multi-label decision system MDS=〈U, F∪L〉, and x∈U, the optimistic neighborhood, neutral neighborhood, and pessimistic neighborhood with respect to x are defined respectively as follows:Optimistic neighborhood:(12)δopt(x)={y|Δ(x,y)≤mopt(x)}.Neutral neighborhood:(13)δneu(x)={y|Δ(x,y)≤mneu(x)}.Pessimistic neighborhood:(14)δpes(x)={y|Δ(x,y)≤mpes(x)}.From Eqs. (12)–(14), we can define the new concepts of neighborhood entropy, joint neighborhood entropy, conditional neighborhood entropy, and neighborhood mutual information for multi-label learning.Given a multi-label decision system MDS=〈U, F∪L〉, where U={x1, x2, …, xn} described by a feature space F, f⊆F is a subset of features, and L is the set of labels. Through optimistic viewpoint, neutral viewpoint, and pessimistic viewpoint, the three neighborhoods of instance xiinduced by f is denoted byδfopt(xi),δfneu(xi), andδfpes(xi), respectively. Then, the three kinds of multi-label average uncertainty of the set of instances are defined respectively as(15)NHδopt(f)=−1n∑i=1nlog||δfopt(xi)||n,(16)NHδneu(f)=−1n∑i=1nlog||δfneu(xi)||n,(17)NHδpes(f)=−1n∑i=1nlog||δfpes(xi)||n.For ∀f⊆F, we haveNHδopt(f)≤NHδneu(f)≤NHδpes(f).Proof∀xi∈U, we haveδfopt(xi)⊇δfneu(xi)⊇δfpes(xi), then∥δfopt(xi)∥≥∥δfneu(xi)∥≥∥δfpes(xi)∥. Therefore, we haveNHδopt(f)≤NHδneu(f)≤NHδpes(f). □Theorem 7For ∀f⊆F, if ∥L∥=1, thenNHδopt(f)=NHδneu(f)=NHδpes(f).ProofIf ∥L∥=1, then∥δfopt(xi)∥=∥δfneu(xi)∥=∥δfpes(xi)∥, we haveNHδopt(f)=NHδneu(f)=NHδpes(f).□Multi-label neighborhood entropy is a natural generalization of the neighborhood entropy if the cardinality of L equals to 1. As to single-label learning, there exists only one neighborhood for each sample when given fixed threshold δ. In this case, the multi-label neighborhood entropy equals to neighborhood entropy.Definition 7Suppose r, f⊆F are two subsets of features in multi-label learning. When the neighborhood of instance xiin feature subspace f⋃r is denoted byδf⋃ropt(xi),δf⋃rneu(xi), andδf⋃rpes(xi), respectively. The three kinds of multi-label joint neighborhood entropy are computed respectively as(18)NHδopt(r,f)=−1n∑i=1nlog||δf⋃ropt(xi)||n,(19)NHδneu(r,f)=−1n∑i=1nlog||δf⋃rneu(xi)||n,(20)NHδpes(r,f)=−1n∑i=1nlog||δf⋃rpes(xi)||n.(1)NHδopt(r,f)≥NHδopt(r),NHδopt(r,f)≥NHδopt(f); (2)NHδneu(r,f)≥NHδneu(r),NHδneu(r,f)≥NHδneu(f); (3)NHδpes(r,f)≥NHδpes(r),NHδpes(r,f)≥NHδpes(f).ProofFor ∀xi∈U, we haveδf⋃ropt(xi)⊆δropt(xi)andδf⋃ropt(xi)⊆δfopt(xi), then||δf⋃ropt(xi)||≤||δropt(xi)||and||δf⋃ropt(xi)||≤||δfopt(xi)||, thereforeNHδopt(r,f)≥NHδopt(r),NHδopt(r,f)≥NHδopt(f). Similarly, the conclusions of (2) and (3) are straightforward.□Definition 8Suppose r, f⊆F are two subsets of features in multi-label learning. The three kinds of multi-label conditional neighborhood entropy of r to f are defined respectively as(21)NHδopt(r|f)=−1n∑i=1nlog||δr⋃fopt(xi)||||δfopt(xi)||,(22)NHδneu(r|f)=−1n∑i=1nlog||δr⋃fneu(xi)||||δfneu(xi)||,(23)NHδpes(r|f)=−1n∑i=1nlog||δr⋃fpes(xi)||||δfo(xi)||.(1)NHδopt(r|f)=NHδopt(r,f)−NHδopt(f); (2)NHδneu(r|f)=NHδneu(r,f)−NHδneu(f); (3)NHδneu(r|f)=NHδneu(r,f)−NHδneu(f).ProofThe proof of them are straightforward.□Definition 9Suppose r, f⊆F are two subsets of features in multi-label learning. The three kinds of multi-label neighborhood mutual information of r and f are defined respectively as(24)NMIδopt(r;f)=−1n∑i=1nlog||δropt(xi)||·||δfopt(xi)||n||δr⋃fopt(xi)||,(25)NMIδneu(r;f)=−1n∑i=1nlog||δrneu(xi)||·||δfneu(xi)||n||δr⋃fneu(xi)||,(26)NMIδpes(r;f)=−1n∑i=1nlog||δrpes(xi)||·||δfpes(xi)||n||δr⋃fpes(xi)||.Given a multi-label decision system MDS=〈U, F∪L〉, r⊆F and f⊆F are two feature subsets, then the following equations hold:(1)NMIδopt(r;f)=NMIδopt(f;r);NMIδneu(r;f)=NMIδneu(f;r);NMIδpes(r;f)=NMIδpes(f;r).NMIδopt(r;f)=NHδopt(r)+NHδopt(f)−NHδopt(r,f);The proof of them are straightforward.□Definition 10Given a multi-label decision system MDS=〈U, F∪L〉, r⊆F is a subset of features, l⊆L is a subset of labels,lxidenotes xiis associated with the subset of labels l, and the neighborhood of instance xiinduced by the feature subset r is denoted by δr(xi). We say the decision of sample xiis neighborhood consistent ifδr(xi)⊆lxi, where δr(xi) is the subset of samples having the same class label aslxi.Given a multi-label decision system MDS=〈U, F∪L〉, r⊆F is a subset of features, and l⊆L is a subset of labels, thenNMIδopt(r;l)=−1n∑i=1nlog||lxi||n,NMIδneu(r;l)=−1n∑i=1nlog||lxi||n, andNMIδpes(r;l)=−1n∑i=1nlog||lxi||n.Proofδr⋃lopt(xi)=δropt(xi)⋂lxi, and we have thatδropt(xi)⊆lxiif xiis consistent. In this case,δr⋃lopt(xi)=δropt(xi). Then−log||δropt(xi)||·||lxi||n||δr⋃lopt(xi)||=−log||δropt(xi)||·||lxi||n||δropt(xi)||=−log||lxi||n. Similarly, the conclusions of neutral viewpoint and pessimistic viewpoint are straightforward.□Notes. For simplicity, we will not differentiate optimistic viewpoint, neutral viewpoint, and pessimistic viewpoint in Section 3.3.In this section, we will discuss how to select a good candidate feature subset, which has the highest dependency on the label set. Let S be the selected feature subset, and L be the label set. In this section, we use multi-label neighborhood mutual information to measure the dependency between S and L, and we have(27)NMIδ(S;L)=NHδ(S)+NHδ(L)−NHδ(S,L)If a candidate feature f+ arrived, then Eq. (27) is transformed as(28)NMIδ(f+,S;L)=NHδ(f+,S)+NHδ(L)−NHδ(f+,S,L).From Eqs. (27) and (28), we can easily obtain the optimization objective function to measure the candidate feature f+ as follows(29)argmaxf+J(f+,S,L)=NMIδ(f+,S;L)−NMIδ(S;L)subjecttoS⊂FTo maximize the objective function J(f+, S, L), we are trying to search for the candidate feature f+. Now, let us deduce the objective function for selecting the candidate feature f+, using Shearer's inequality [3], NHδ(f+, S, L)≤(1/2)(NHδ(f+, S)+NHδ(f+, L))+NHδ(S, L). Then we have(30)J(f+,S,L)=NMIδ(f+,S;L)−NMIδ(S;L)≤12(NHδ(f+,S)−NHδ(f+,L)+NHδ(S,L))−NHδ(S).As we know, NHδ(S, L), NHδ(L), and NHδ(S) are fixed values, then the objective function J(f+, S, L) can be approximated as(31)J˜(f+,S,L)∝12(NHδ(f+,S)−NHδ(f+,L)+NHδ(S,L))−NHδ(S)∝NHδ(f+,S)−NHδ(f+,L)=NMIδ(f+;L)−NMIδ(f+;S)+NHδ(H)−NHδ(L)∝NMIδ(f+;L)−NMIδ(f+;S)Eq. (31) measures the significance of each feature one by one, and ranks these features according to their significances by a descending order. Then some multi-label classification methods are used to check the best k features with respect to the classification performance, where k=1, …, d, d is the number of all candidate features.The proposed metric chooses the next candidate feature as the one that maximizes Eq. (31) under incremental selection. Then, the pseudocode of this algorithm can be described by the Algorithm 1.Algorithm 1Multi-label feature selection based on neighborhood mutual information (MFNMI)Input: F: a set of features; U: a set of instances; L: a set of labels.Output: S: Selected feature subset.1: ∀x∈U, compute δ(x) from different viewpoints;2: initialize S=∅ and F={f1, f2, ⋯, fN};3: while |S|<Ndo4:   find f∈F by maximizing Eq. (31);5:   S←S⋃{f};6:   F=F\S;7: end while8: return S;There are three key steps in Algorithm 1. The first step is the transformation of multi-label data, and its time complexity is O(|U|·|L|). The second step is to search the nearest hit/miss for each instance, and the related time complexity is O(|U|·|U|). The last step is to rank features with a heuristic search process, and its time complexity is O(|S|·|F|). Therefore, the computational complexity of this multi-label feature selection is O(|U|·|L|+|U|·|U|+|S|·|F|).

@&#CONCLUSIONS@&#
Multi-label feature selection is a hot topic, and has been widely studied for many years. In this paper, we presented a new multi-label feature selection method that selects distinguishing features based on multi-label neighborhood mutual information. We first used margin to granulate all instances with different viewpoints, and defined three different neighborhood mutual information for multi-label learning. Then, we proposed an optimization objective function to measure the candidate features. Finally, experiments validate the performance of our proposed method.Different from many other multi-label feature selection methods, MFNMI provides three multi-label feature selection methods via instance granulation under all labels with different viewpoints. Meanwhile, the label-specific features are another important factor. Therefore, in future research, we will study how to solve the multi-label feature selection problem including the label correlation and the label-specified features simultaneously.