@&#MAIN-TITLE@&#
Approximate dynamic programming for stochastic linear control problems on compact state spaces

@&#HIGHLIGHTS@&#
Proof of existence of optimal policies for stochastic linear control problems.Proof of existence of convex bounded solutions to Average Cost Optimality Equation.New ADP algorithm providing good policies and lower bounds to the optimal costs.Tests of algorithm on multiple sourcing where optimality gap can be bounded by 5%.On all tested instances algorithm performs better than state-of-the-art heuristic.

@&#KEYPHRASES@&#
Dynamic programming,Markov processes,Inventory,Dual sourcing,Multiple sourcing,

@&#ABSTRACT@&#
This paper addresses Markov Decision Processes over compact state and action spaces. We investigate the special case of linear dynamics and piecewise-linear and convex immediate costs for the average cost criterion. This model is very general and covers many interesting examples, for instance in inventory management. Due to the curse of dimensionality, the problem is intractable and optimal policies usually cannot be computed, not even for instances of moderate size.We show the existence of optimal policies and of convex and bounded relative value functions that solve the average cost optimality equation under reasonable and easy-to-check assumptions. Based on these insights, we propose an approximate relative value iteration algorithm based on piecewise-linear convex relative value function approximations. Besides computing good policies, the algorithm also provides lower bounds to the optimal average cost, which allow us to bound the optimality gap of any given policy for a given instance.The algorithm is applied to the well-studied Multiple Sourcing Problem as known from inventory management. Multiple sourcing is known to be a hard problem and usually tackled by parametric heuristics. We analyze several MSP instances with two and more suppliers and compare our results to state-of-the-art heuristics. For the considered scenarios, our policies are always at least as good as the best known heuristic, and strictly better in most cases. Moreover, by using the computed lower bounds we show for all instances that the optimality gap has never exceeded 5%, and that it has been much smaller for most of them.

@&#INTRODUCTION@&#
In this paper we address Markov Decision Processes (MDPs) with compact state and action space, linear dynamics as well as piecewise-linear and convex immediate costs. We denote this class of MDPs as Stochastic Linear Control Problems (SLCPs). SLCPs are a broad class of problems that represent many practically relevant problems in various areas, for instance in inventory management.We study existence and characterization policies that are optimal for the average cost criterion. In particular, we develop conditions that imply the existence of a convex solution to the Average Cost Optimality Equation (ACOE). Such solutions can be used to characterize optimal policies. In addition, we develop an Approximate Dynamic Programming (ADP) algorithm for SLCPs. The algorithm generates piecewise-linear and convex relative value function approximations, together with a non-decreasing sequence of lower bounds to the optimal average cost. The relative value functions approximation can then be used to derive a good policies. We show the effectiveness of this algorithm on the Multiple Sourcing Problem (MSP) known in inventory management and compare the results to the current best-known heuristics.The paper is organized as follows. Section 2 surveys the related literature for average-cost MDPs, ADP, and the MSP. In Section 3, we formally introduce SLCPs. Section 4 discusses the related concepts and results of MDP theory. In Section 5, we review and extend results about the existence of optimal policies and relative value functions for general state space MDPs. In particular, we prove the existence of a convex solution to the ACOE under rather mild assumptions that are typically fulfilled in practice. This motivates our approximation algorithm, which is given in Section 6. In Section 7, we apply our algorithms to various MSP instances, compare the results with state-of-the-art heuristics, and discuss the differences.In this section we review the relevant literature about MDPs, ADP and MSP, and discuss our contributions in the context of existing results and open questions.While average cost MDP with finite state and action spaces are well understood (Bertsekas, 2007), much less in known for MDPs with continuous state and action spaces. From the point of view of the present paper, the existing work on continuous state space MDPs can be classified into two groups, distinguished by continuity assumptions on the transition law.Most authors assume the transition law to be strongly continuous (Hernández-Lerma & Lasserre, 1990; Montes-de Oca & Hernández-Lerma, 1996; Kurano, Nakagami, & Huang, 2000). This simplifies the analysis of the existence of optimal policies and relative value functions. However, in the more general model considered here, we only assume a weakly continuous transition law, so these results cannot be applied.There are only very few papers that consider weakly continuous transition laws, most importantly Schäl (1993) as well as Hernández-Lerma and Lasserre (2002, chap. 12). Schäl (1993) follows the limiting discount factor approach to establish the existence of optimal policies. We apply these ideas to SLCPs and extend the results to the existence of a convex bounded solution to the ACOE.Hernández-Lerma and Lasserre (2002, chap. 12) follow the infinite dimensional Linear Programming (LP) approach. They provide conditions that imply existence of optimal policies and relative value functions. However, their results only hold almost everywhere with respect to an invariant measure. The conditions we propose in Section 5 imply that all their results are applicable as well.Arapostathis, Borkar, Fernández-Gaucherand, Ghosh, and Marcus (1993) provide a general survey about MDPs with the average cost criterion ranging from finite to Borel state and action spaces.Finally, we also note that convexity of value functions of MDPs has been an important object of study for a long time. However, most of these structural results have been established for finite horizon or discounted cost problems only (Dynkin, 1972; Hinderer, 1984; Hernández-Lerma & Runggaldier, 1994; Hernández-Lerma, Piovesan, & Runggaldier, 1995).ADP is an active research area with a variety of literature. Standard textbooks include Bertsekas and Tsitsiklis (1996) as well as Powell (2007).The typical approach in ADP is to approximate an optimal relative value function with a combination of a fixed set of basis functions. Such approaches were shown to work well for some applications (Schweitzer & Seidmann, 1985; De Farias & Van Roy, 2003, 2004; Farias & Van Roy, 2006, chap. 6). However, determining a good set of basis functions is often very difficult and problem-specific.As we will show in Section 5, there exists a convex solution to the ACOE for SLCPs, therefore we propose to use a piecewise-linear convex approximation of the relative value function instead. Thus, instead of fixing a set of basis functions, we take the maximum of a set of hyperplanes, which we generate in a particular way during the run of the algorithm.Piecewise-linear convex value function approximations were also considered by Lincoln and Rantzer (2006) and Shapiro (2011). Lincoln and Rantzer (2006) also focus on SLCPs, but instead of the average cost criterion, they consider discounted cost, for which the existence of optimal policies and relative value functions is well established. Lincoln and Rantzer introduce a “Relaxed Value Iteration” and exploit the fact that the Bellman operator can be expressed as a Multi-Parametric LP (MPLP) and thus preserves piecewise-linearity and convexity. In contrast to our approach, Lincoln and Rantzer (2006) have to solve an MPLP in every iteration to obtain the hyperplane representation of the next relative value function, which is a very complex operation (exponential in the worst case and practically solvable only in low dimensions). In order to reduce the complexity of the relative value function, some of the hyperplanes are dropped as long as given deviation bounds are respected. Satisfying the bounds implies performance guarantees for the resulting policies and allows to control the trade-off between computational effort and performance of the policy. However, to get meaningful performance guarantees, the bounds have to be chosen quite tight, which implies that most hyperplanes have to be kept. Unfortunately, solving an MPLP with many hyperplanes is impractical even in smaller dimensions. This drawback might be the reason why this approach has never been applied to problems of realistic size. For more details on MPLPs in control, including finite horizon approximation techniques, we refer to Jones, Baric, and Morari (2007) as well as Jones and Morari (2009).In this paper, we develop a different approach to cope with the inherent complexity of the exact value iteration step. Instead of solving an MPLP to find all hyperplanes in every step and then dropping some of them, we compute only a subset in the first place. We choose the subsets in such a way that the associated lower bounds are non-decreasing.Piecewise-linear convex value functions have also been used by Shapiro (2011) to compute a lower approximation of the optimal value function in the Stochastic Dual Dynamic Programming (SDDP) method, which originated in Pereira and Pinto (1991). SDDP is typically applied to the Sample Average Approximation problem. It is designed for finite horizon problems, whereas the technique proposed here targets infinite horizon problems.Inventory management with a single supplier is a well studied problem. Scarf (1960) and Iglehart (1963) proved that the optimal policy of an infinite horizon discounted cost problem with deterministic lead times and stationary stochastic demand is an(s,S)policy. Veinott and Wagner (1965) concluded that this holds also for infinite horizon average cost per stage problems.While single sourcing is well understood, there exist only few results about the structure of optimal policies if multiple suppliers are available. Fukuda (1964) proved that for two suppliers, a dual base stock policy is optimal when no fixed order costs are accounted, lead times are deterministic, and the difference of the lead times of the two suppliers is exactly equal to one. However, an optimal policy for a general MSP is highly state-dependent (Whittemore & Saunders, 1977). Therefore, parametric heuristics and the computation of their optimal parameters is an active field of research.Minner (2003) provided a detailed review of MSPs and discussed different heuristics and different setups. Scheller-Wolf, Veeraraghavan, and Van Houtum (2006) showed how the optimal parameters of the Single Index Policy (SIP) can be computed for a general dual sourcing problem with deterministic lead times. In Veeraraghavan and Scheller-Wolf (2008), the more complex Dual Index Policy (DIP) was analyzed and a simulation-based approach to determine the optimal parameters was presented, while Arts, Van Vuuren, and Kiesmuller (2009) provide a different way of approximating the optimal DIP parameters. Finally, Klosterhalfen, Kiesmuller, and Minner (2011) analyzed the Constant Order Policy (COP) and gave an overview of the three aforementioned heuristics and their performance based on numerical studies. The COP was also studied before, e.g. by Janssen and de Kok (1999) who provided an algorithm to compute the optimal parameters under service level constraints. Since the MSP is known to be a very difficult problem and since there are well-studied heuristics for it, it serves as a good test case for the new ADP approach proposed in this paper.In this paper, we provide results to the following open questions regarding SLCPs:•existence of optimal policies,existence and structure of the solution to the ACOE,suitable approximation of optimal relative value functions, andperformance evaluation of approximated policies and heuristics.We prove the existence of optimal policies as well as convex solutions to the ACOE under very mild assumptions. This significantly extends known results and constitutes the main theoretical contribution of the paper, stated in Theorem 6. These insights from the theoretical analysis motivate our new approximate Relative Value Iteration (RVI) algorithm based on piecewise-linear convex functions. Besides good control policies, the algorithm also provides a non-decreasing sequence of lower bounds to the optimal average costs. The lower bounds allow us to bound the optimality gap of any given policy.We apply this algorithm to MSP instances and compare the performance with state-of-the-art heuristics. MSP is a typical example of a seemingly simple stochastic control problem with simple dynamics and cost structure. However, nothing is known about the structure of optimal policies, and the only known way to find an optimal policy is via exact dynamic programming, which is usually intractable. Various heuristic policies have been proposed and shown to perform well on small instances by comparing them to the exact solution, but the lack of lower bounds has so far prevented to judge their performance on instances that are too large to be solved to optimality.We now formally introduce SLCPs, as for instance studied by Lincoln and Rantzer (2006).Let the state spaceSbe defined as(1)S≔x→∈Rn|Qx→⩽β→,whereQ∈Rs×nandβ→∈Rsdefine a polytope and the set of feasible actionsU(x→)for a given statex→∈Sbe defined as(2)U(x→)≔u→∈Rm|Ru→+Vx→⩽δ→,whereR∈Rr×m,V∈Rr×nandδ→∈Rrdefine a polytope for each statex→∈S. We assume thatU(x→)≠∅for allx→∈S. The set of all actionsUis defined asU≔⋃x→∈SU(x→).Using (1) and (2), the space of feasible state-action pairsWcan be defined asW≔x→,u→∈Rn+m|Qx→⩽β,Ru→+Vx→⩽δ,which implies thatWis also a polytope.For any(x→,u→)∈W, the random successor statex→′is given by(3)x→′≔Ax→+Bu→+C→where(A,B,C→)is a multi-dimensional random variable that takes values inRn×n×Rn×m×Rn. The support and distribution of(A,B,C→)are given assupp(A,B,C→)≔(Aj,Bj,C→j)|j=1,…,J,qj≔P(A,B,C→)=(Aj,Bj,C→j),j=1,…,J,whereJ∈N.In general, (3) does not guarantee that the resulting problem is consistent, because the successor statex→′does not have to be an element ofS. In order to ensure feasibility, we make the following assumption.Assumption 1FeasibilityFor every(x→,u→)∈Wwe assume thatAjx→+Bju→+C→j∈S,∀j=1,…,J.Given the previous definitions, it is possible to state the transition probabilities to a particular statex→′∈Sfor a given state-action pair(x→,u→)∈Was(4)Px→′|x→,u→=∑j=1Jqj·IAjx→+Bju→+C→j(x→′),whereI·(·)denotes the indicator function of a given set. Note that (4) defines an atomic transition law. In Section 5, we show that it is weakly continuous, but not strongly continuous. This is the reason why most existing results about continuous state space MDPs are not applicable, since they assume strongly continuous transition laws, as discussed in Section 2.1.The immediate costsc:W→R⩾0are assumed to be piecewise-linear and convex, non-negative, and defined as the maximum over a set of hyperplanes as(5)c:(x→,u→)↦maxw=1,…,Wζ→wTx→+η→wTu→+θwwhereζ→w∈Rn,η→w∈Rm,θw∈R, for allw=1,…,W.In this paper we focus on the infinite horizon average cost objective to evaluate the performance of a given policy. We consider only deterministic policies, thus a policy is a map from the state space into the action space. We call a policyπ:S→Ufeasible ifπ(x→)∈U(x→)for allx→∈S, and we denote the set of all feasible policies by Π.Suppose a stationary policyπ∈Π, or a sequence of policiesπs∈Π,s∈Nis given. We denote the random state that is reached after t periods byx→twhen applying the policy π, respectively the policiesπs,s=0,…,t-1, for a given initial statex→0∈S.The infinite horizon average cost per stage for a given initial statex→∈Sand a policyπ∈Πis defined asJπ(x→)≔limsupN→∞1NE∑t=0N-1c(x→t,π(x→t))x→0=x→,and the optimal average cost isρ∗≔infx→∈Sinfπ∈ΠJπ(x→).Later we will show that the optimal average cost does not depend on the initial statex→∈S.A common approach to study average cost behavior of MDPs is to analyze the corresponding discounted cost problems as the discount factor approaches one (Schäl, 1993). We also follow this approach here and define the discounted cost objective for a policyπ∈Π, discount factorα∈(0,1)and initial statex→∈SasVα,π(x→)≔limN→∞E∑t=0N-1αtc(x→t,π(x→t))x→0=x→.A basic tool from MDP theory is the Bellman operator, which we will need throughout the remainder of this paper. In the following, we provide a formal definition of the Bellman operator and the ACOE for SLCP and discuss some of their properties. For a more detailed discussion of its properties we refer to the standard literature for discrete state space problems (Puterman, 1994; Bertsekas, 2007), since a lot of results hold in the continuous state space setup as well.Definition 1Bellman OperatorGiven a functionh:S→Rand a discount factorα∈(0,1], the corresponding Q-functionQα:W→Rand Bellman operatorTαare given byQα(x→,u→|h)≔c(x→,u→)+α∑j=1Jqjh(Ajx→+Bju→+C→j),(x→,u→)∈W,Tαh(x→)≔infu→∈U(x→)Qα(x→,u→|h),x→∈S.Choosingα=1corresponds to the average cost per stage objective, in which case we usually drop the index and write just Q and T, respectively. We define the Bellman operator by using the infimum. However, whenever possible, we replace it by the minimum.For everyh:S→Rwe define a corresponding policyπh∈Πforx→∈Sby(6)πh(x→)≔argminu→∈U(x→)Qα(x→,u→|h),assuming that the minimum is achieved for allx→∈S.A scalarρ∈Rand a relative value functionh:S→Rare said to solve the Average Cost Optimality Equation (ACOE) ifρ+h(x→)=Th(x→)∀x→∈S.A relative value functionh:S→Rsatisfies the Average Cost Optimality Inequality (ACOI) ifρ∗+h(x→)⩾Th(x→)∀x→∈S.Studying ACOE and ACOI is of interest because of the following two theorems.Theorem 1Hernández-Lerma and Lasserre (2002, chap. 12)Supposeρ∈Randh:S→Rsatisfy the ACOE, and assume the policyπhis well-defined, thenJπh(x→)=ρ=ρ∗∀x→∈S.Suppose h satisfies the ACOI, and assume the policyπhis well-defined, thenJπh(x→)=ρ∗∀x→∈S.In other words, ACOE and ACOI imply thatπhis an optimal policy, and the optimal average cost is independent of the initial state.From standard convex optimization theory, it follows that the Bellman operator as defined here preserves convexity (Boyd & Vandenberghe, 1999, (2.11)). We state this in the following lemma for further reference.Lemma 1Given a convex functionh:S→R,Tαh:S→Ris again a convex function for allα∈[0,1].As noted for instance by Lincoln and Rantzer (2006) the Bellman operator of a piecewise-linear convex function can be written as an MPLP. This immediately implies that it also preserves piecewise-linearity in addition to convexity. We derive the corresponding MPLP since we need the notation later in this section in order to prove Lemma 2.Supposeh:S→Ris piecewise-linear and convex and given ash(x→)=maxk=1,…,Kϕ→kTx→+ψk.Using the provided definitions,Tαhcan be written asTαh(x→)=minu→∈U(x→)c(x→,u→)+α∑j=1Jqj·h(Ajx→+Bju→+C→j)=minu→∈U(x→)∑j=1Jqj·c(x→,u→)+α·h(Ajx→+Bju→+C→j)=minu→∈U(x→)∑j=1Jqj·maxk=1,…,Kw=1,…,Wζ→wTx→+η→wTu→+θw+α·φ→kT(Ajx→+Bju→+C→j)+ψk=minu→∈U(x→)∑j=1Jqj·maxk=1,…,Kw=1,…,Wζ→wT+αφ→kTAjx→+η→wT+αφ→kTBju→+θw+αφ→kTC→j+αψk.For readability, we merge the indicesk=1,…,Kandw=1,…,Wintoi=1,…,I≔K·Wand introduce the following notation:F→i,j≔ζ→wT+αφ→kTAjT,G→i,j≔η→wT+αφ→kTBjT,Hi,j≔θw+αφ→kTC→j+αψk.Using this notation, we can simplify the expression ofTαhto(7)Tαh(x→)=minu→∈U(x→)∑j=1Jqj·maxi=1,…,IF→i,jTx→+G→i,jTu→+Hi,j=minu→∈U(x→)∑j=1Jqj·minfj∈Rfj⩾F→i,jTx→+G→i,jTu→+Hi,jfj=Def.ofU(x→)minu→∈Rmf→∈RJRu→⩽δ→-Vx→G→i,jTu→-fj⩽-F→i,jTx→-Hi,j∑j=1Jqj·fj.For a fixed statex→∈Sthis is an LP and we refer to (7) as the primal LP formulation of the Bellman operator. This implies thatTαh(x→)is an MPLP with parameterx→∈S. Therefore, the Bellman operator preserves piecewise-linearity and convexity (Lincoln & Rantzer, 2006; Jones et al., 2007). Moreover, evaluating the Bellman operator for a given state can also return a supporting hyperplane as described in the following.Lemma 2EvaluatingTαh(x→)for a fixedx→∈Sby solving the dual LP corresponding to(7)also results in a supporting hyperplane ofTαhatx→, denoted byφ→[x→,h]Tx→+ψ[x→,h].In order to prove the lemma, we analyze the dual LP formulation of (7), but first we introduce the following notation to simplify matters:F≔F→1,1TF→1,2T⋮F→I,JT∈R(I·J)×n,G≔G→1,1TG→1,2T⋮G→I,JT∈R(I·J)×m,H→≔H1,1H1,2⋮HI,J∈R(I·J),J≔IJIJ⋮IJ∈R(I·J)×J,whereIJ∈RJ×Jdenotes the identity matrix of dimension J. This allows us to write (7) asTαh(x→)=minu→∈Rmf→∈RJ0→q→Tu→f→ROG-Ju→f→⩽δ→-Vx→-(Fx→+H→),where0→andOdenote the appropriate zero vector, respectively zero matrix. The dual LP can now be stated as(8)Tαh(x→)=maxΛ→∈Lφ→(Λ→)Tx→+ψ(Λ→),whereL≔Λ→∈R⩽0r+I·JROG-JTΛ→=0→q→,φ→(Λ→)≔Λ→T-V-F,ψ(Λ→)≔Λ→Tδ→-H→.We refer to (8) as the dual LP formulation of the Bellman operator. Thus, each vector of dual variablesΛ→∈Lleads to a hyperplaneφ→(Λ→)Tx→+ψ(Λ→)for eachx→∈S. Since L is a linearly constrained set independent ofx→∈S, andφ→(Λ→)as well asψ(Λ→)are linear inΛ→∈L, it follows that for everyx→∈S, the optimal solution of (8) will be an extreme point of L. Denoting the finite set of extreme points of L byext(L)allows us to writeTαh(x→)=maxΛ∈ext(L)φ→(Λ→)Tx→+ψ(Λ→).Therefore,Tαhis the maximum over a finite set of hyperplanes. Denotingφ→(Λ→)byφ→[x→,h]andψ(Λ→)byψ[x→,h]completes the proof. □From standard MDP theory it is known that the Bellman operator can be used to compute lower bounds to the optimal average cost (Bertsekas, 2007). This also holds for the compact case, and leads to the following theorem.Theorem 3Assumeh:S→Ris a bounded function. A non-decreasing sequence of lower bounds to the optimal average costρ∗is given byρ̲(Tnh)≔infx→∈STn+1h(x→)-Tnh(x→),n∈N.In this section we discuss the existence of stationary deterministic average cost optimal policies. We show that there exists a convex relative value function that satisfies the ACOE. If there exists a policy that achieves the minimum in the Bellman operator of such a relative value function, the ACOE implies that this policy is average cost optimal and achieves the optimal costρ∗for every initial statex→∈S.There are different approaches to study average cost MDPs, primarily the limiting discount factor approach (Schäl, 1993) and the infinite dimensional LP approach (Hernández-Lerma & Lasserre, 2002, chap. 12). The drawback of the infinite-dimensional LP approach is that its results might only hold for a subset of the state space. This subset will be an ergodic class under a resulting optimal policy, but the policy is only defined for this subset, so this does not yield any information about the remaining state space. In this paper we mainly follow the first approach.The proofs of theorems, lemmas, and corollaries are given in Appendix A. We skip them here, because most of them are very technical and do not provide many further insights.To begin with, we introduce the concept of Weak Accessibility (WA). This concept is known in discrete state space MDPs (Bertsekas, 2007), but the atomic transition law (4) allows us to extend it to SLCPs over compact state and action spaces.Definition 3Weak AccessibilityAn SLCP satisfies WA if there exist aT∈Nand aτ>0such that for everyy→,z→∈Sthere is a sequence of policiesπt,y→,z→∈Π,t∈N, such that(9)P∃s⩽T:x→s=z→|x→0=y→⩾τ.Next, Lemma 3 shows that the n-step cost differences between any two states remain bounded under WA and continuous terminal cost.Lemma 3If an SLCP satisfies WA andh:S→Ris continuous then there existsM⩾0such that for allα∈[0,1],n∈N, andy→,z→∈S(10)Tαnh(y→)-Tαnh(z→)⩽M.Next, we summarize results from Schäl (1993) on the existence of optimal policies for general state space MDPs.Definition 4Hernández-Lerma and Lasserre (2002, chap. 12)A transition lawP:W→M(S)is weakly continuous if for every continuous functionϕ:S→R(x→,u→)↦∫Sϕ(y→)P[dy→|x→,u→]is a continuous function onW, whereM(S)is the space of probability measures onS.LetF:X⇉Ybe a multi-valued mapping between topological spaces X and Y. F is said to be u.s.c. if for every openB⊂Y, the setF-1(B)≔x∈X|F(x)⊂Bis open as well.Schäl (1993) introduces a slightly weaker form of the following assumptions. We adapt them according to the assumptions already made throughout this paper.Assumption 2Schäl (1993), Condition (W)1.Sis compact;U(x→)is compact for allx→∈Sandx→∈S↦U(x→)is an upper semi-continuous (u.s.c.) multi-valued mapping;P:W→M(S)is weakly continuous;c:W→Ris continuous.Assumption 2holds for SLCPs.AssumeAssumption 2holds and let beα∈(0,1). Thus, there exists a discounted cost-optimal stationary policyπα∗and a value functionVα∗:S→Rsuch that(11)Vα∗(x→)=TαVα∗(x→)=Qαx→,πα∗(x→)∀x→∈S.Under the assumptions ofTheorem 4,Vα∗:S→Ris convex and continuous for SLCPs. In particular, for every bounded measurable functionV0:S→Rit holds thatVα∗(x→)=limn→∞TαnV0(x→)∀x→∈S,where the convergence is uniform.Moreover, Schäl (1993) introduces(12)mα≔minx→∈SVα∗(x→),wα(x→)≔Vα∗(x→)-mα.For everyα∈(0,1),wαis a non-negative convex function. In addition, Schäl (1993) makes the following assumption.Assumption 3Schäl (1993), Condition (B)supα∈(0,1)wα(x→)<∞∀x→∈S.In the following, we show that a slightly stronger condition holds.Lemma 5If WA holds,Assumption 3is satisfied and there existsM<∞such thatsupα∈(0,1)wα(x→)⩽M∀x→∈S.Lemmas 4 and 5 imply that the main result of Schäl (1993) also holds for SLCPs.Theorem 5Schäl (1993), Theorem 3.8Given thatAssumptions 2 and 3hold, there exists a stationary deterministic policyπ∈Πthat is average cost optimal with optimal average costρ∗independent of the initial state. In addition, for any sequenceαk≠arrow1,(k→∞),ρ∗=limk→∞(1-αk)mαk.We now use these results to prove the existence of a convex solution to the ACOE.Lemma 6Suppose an arbitrary sequenceαk↗1,k→∞, and define the two functionsh¯(x→)≔limsupk→∞wαk(x→),h̲(x→)≔liminfk→∞wαk(x→).For allx→∈S, it holds thath̲(x→)⩽h¯(x→), and further more we have(13)ρ∗+h¯(x→)⩽Th¯(x→),(14)ρ∗+h̲(x→)⩾Th̲(x→).In addition,h¯andh̲are bounded, andh¯is convex.Theorem 2 implies that any well-defined policy corresponding toh̲would be optimal, since (14) is equal to the ACOI. However, the limit inferior does not preserve convexity. Therefore, we will useh¯to show the existence of a convex function that satisfies the ACOE.Lemma 7|Tnh¯(x→)-nρ∗|is bounded uniformly for alln∈N,x→∈S.Since the Bellman operator preserves convexity, as stated in Lemma 1, we can define the following sequence of convex functions:hn(x→)≔Tnh¯(x→)-nρ∗.By Lemma 7, the sequence is uniformly bounded. In addition, Theorem 3 and Lemma 6 imply thatinfx→Thn(x→)-hn(x→)=ρ∗∀n∈N,since otherwise either the lower bound property stated in Theorem 3 or inequality (13) would be violated. In particular, this implies that for allx→∈Sρ∗+hn(x→)⩽Thn(x→),hn(x→)⩽hn+1(x→).Thus,hndefines a non-decreasing sequence of uniformly bounded convex functions and hence converges point-wise. We denote the limit ash∗(x→)≔limn→∞hn(x→).It follows thath∗is also convex and bounded and satisfies(15)ρ∗+h∗(x→)⩽Th∗(x→)∀x→∈S.By definitionhnsatisfiesρ∗+hn+1(x→)=Thn(x→).Since the Bellman operator is continuous we get(16)ρ∗+h∗(x→)=Th∗(x→).This allows us to state our main theorem.Theorem 6Existence of Convex Solution to ACOEIt exists a bounded convex functionh∗:S→Rand a scalarρ∗that satisfy the ACOEρ∗+h∗(x→)=Th∗(x→)∀x→∈S.In particular,h∗is continuous on the interior ofSand u.s.c. onS. If there exists a policy that achieves the minimum inTh∗for everyx→∈S, this policy is optimal.Even if a policy corresponding toh∗does not exist, the ACOE can be used to generate ∊-optimal policies for any∊>0.In the remainder of this section we show that our results immediately imply the existence of a continuous convex and bounded optimal relative value function in the simpler special case of a strongly continuous transition law. This also illustrates the difficulty of weakly continuous transition laws.Definition 6Gordienko and Hernández-Lerma (1995), Assumption 2.2(c)A transition lawP:W→M(S)is strongly continuous, if for each measurable and bounded functionϕ:S→Rthe map(x→,u→)↦∫Sϕ(y→)P[dy→|x→,u→]is a continuous function onW.Ifh∗andρ∗satisfy the ACOE as given inTheorem 6and the transition law is strongly continuous, thenh∗is a continuous, convex and bounded function andπh∗is a well-defined optimal policy.We know from the discussion in Section 4 that the Bellman operator preserves piecewise-linear convex functions and that we can get a supporting hyperplane by evaluating the dual LP (8). In addition, Theorem 6 implies the existence of a convex solution to the ACOE. These results enable us to propose an approximate version of RVI that uses piecewise-linear convex functions. Our algorithm produces a non-decreasing sequence of lower bounds to the optimal average cost, we therefore call it Monotone Approximate RVI (MARVI). We use the resulting relative value function to determine a corresponding policy as defined in (6). Thus, we can simulate the system under this policy to estimate the resulting average cost. The lower bounds allow us to bound the optimality gap.A lower bound for the optimal average cost per stage can be computed from every given relative value function as shown in Theorem 3. If the relative value function is piecewise-linear and convex, the lower bound can be computed efficiently as discussed in the following.Suppose a piecewise-linear convex relative value function is given ash(x→)=maxk=1,…,Kφ→kTx→+ψk.In (7) it is shown thatTh(x→)can be evaluated by solving an LP for a givenx→∈S. It is easy to see that(17)minx→∈STh(x→)can be evaluated as an LP as well, since there is only a linear dependence onx→in the right-hand side of the LP in (7), andSis linearly constrained.In order to compute the lower bound, we consider the state space decomposition(18)Sl≔x→∈S|φ→lTx→+ψl⩾φ→kTx→+ψk,∀k=1,…,K,l=1,…,K,which decomposes the state space into polyhedral regions, each defined by one active hyperplane among those used in the description of h. The lower bound corresponding to h, denoted byρ̲(h), can then be written asρ̲(h)=minx→∈STh(x→)-h(x→)=minl=1,…,Kminx→∈SlTh(x→)-h(x→)=minl=1,…,Kminx→∈SlTh(x→)-φ→lTx→+ψl.For everyl=1,…,Kthe corresponding inner minimization problem is an LP, since it can be constructed from (17) by adding a linear term to the objective and adding the required linear constraints to guaranteex→∈Sl. Thus we can compute the lower bound by solving K LPs. If such a sub-problem is infeasible, it means that the corresponding hyperplane is never active and can be deleted. Note that the sub-problems are independent and can be solved in parallel.The minimizer of the lower bound computation is a state-action pair(x→,u→)∈W, where the minimization over the action is hidden within the Bellman operator. We denote the minimizing state-action pair for a particular relative value function h by(x̲→(h),u̲→(h))∈W. If the relative value function has an index,hn, we write(x̲→n,u̲→n)instead.As stated in Lemma 2, evaluatingTh(x→)via the dual LP returns a supporting hyperplane of Th atx→, which we denote by(φ→[x→,h],ψ[x→,h]).In every iteration step, MARVI computes the current lower bound, checks the corresponding minimizer, and updates the current relative value function at all points that influence the value of the corresponding lower bound by computing the supporting hyperplanes. Using the introduced notation, we state MARVI in Algorithm 1. The algorithm always terminates as there is only a finite number of hyperplanes to be added in a particular step.Algorithm 1MARVI (N=number of steps)seth0(x→)=0for allx→∈Scomputeρ̲(h0)forn=1,…,Ndosethn(x→)=hn-1(x→)+ρ̲(hn-1)repeatcomputeρ̲(hn),x̲→n,u̲→ncompute(φ→[x̲→n,hn-1],ψ[x̲→n,hn-1])add hyperplane tohtif not redundantforj=1,…,Jdosetx→j=Ajx̲→n+Bju̲→n+C→jcompute(φ→[x→j,hn-1],ψ[x→j,hn-1])add hyperplane tohnif not redundantend foruntil no hyperplanes were added tohnshifthnby-ρ̲(hn)ifhn=hn-1thenbreak for-loopend ifend forreturn differential cost functionhN, lower boundρ̲(hN)As mentioned before, the sequence of lower bounds produced by MARVI is non-decreasing, as we prove in the following theorem.Theorem 8Monotonicity of MARVI Lower BoundsThe sequence of lower bounds generated by MARVI is non-decreasing.Suppose a piecewise-linear convex relative value functionhn-1generated by MARVI. The corresponding lower bound and minimizer areρ̲(hn-1)and(x→̲n-1,u→̲n-1). Note thatThn-1(x→)⩾hn-1(x→), for allx→∈S(Bertsekas, 2007). Therefore, we initializehnwithhn-1+ρ̲n-1, since it is the smallest function that satisfies this property. Suppose no further hyperplanes were added within an iteration step. This implies thatQ(x→̲n,u→̲n|hn)=Q(x→̲n,u→̲n|Thn-1)andhn(x→̲n)=Thn-1(x→̲n).This leads toρ̲(hn)=minx→∈SThn(x→)-hn(x→)=Q(x→̲n,u→̲n|hn)-hn(x→̲n)=Q(x→̲n,u→̲n|Thn-1)-Thn-1(x→̲n)⩾T2hn-1(x→̲n)-Thn-1(x→̲n)⩾minx→∈ST2hn-1(x→)-Thn-1(x→)=ρ̲(Thn-1)⩾ρ̲(hn-1)where the last inequality holds because of the monotonicity of lower bounds. □MARVI and Theorem 8 can also be applied to unbounded state spaces. However, the presented existence theory does not hold in its current form. Whether it can be extended to unbounded state spaces is subject of further research.In Sections 6.2.1 and 6.2.2, we provide two ways of speeding up the basic algorithm. First, we explain how the lower bound computation during the iteration steps can be sped up significantly. Second, we introduce a way to reduce the number of hyperplanes after every iteration step without decreasing the associated lower bound.The lower bound computation within MARVI can be sped up significantly. Within every step, MARVI adds some hyperplanes to the current relative value function approximation and then recomputes the lower bound. The lower bound computation results in an LP for every hyperplane. However, for earlier added hyperplanes the optimal solution is already known if the new hyperplanes are ignored. For the LP corresponding to an old hyperplane, adding hyperplanes translates to adding new constraints. Instead of solving the updated LP again, we check whether the old optimal solution stays feasible when the additional constraints are added. If this is the case, the old solution stays optimal and the LP does not have to be resolved.Suppose a piecewise-linear convex relative value functionh:S→Ris given ash(x→)≔maxk=1,…,Kφ→kTx→+ψk.We show how to construct a functionh̃:S→R, defined by a subset of the hyperplanes of h, which corresponds to a lower bound greater than or equal to the lower bound corresponding to h.Consider the sub-problem of the lower bound computation corresponding to hyperplanek∈1,…,K. Every hyperplanel≠kof h translates into constraints in the sub-problem. There are J constraints for every hyperplane to cover the piecewise-linear convex objective and one constraint to restrict to the feasible setSk⊂S. This follows from (7) and (18).Removing a hyperplane from h affects all lower bound sub-problems, where the corresponding constraints are active for the optimal solution, and the sub-problem corresponding to the hyperplane itself. We use this relation to construct a directed graphGon the set of hyperplanes, respectively sub-problems.Consider two nodesk≠l∈1,…,K. We add a directed edge(l,k)if the sub-problem corresponding to k has a constraint corresponding to l that is active for the optimal solution. If no such constraint exists, hyperplane l does not affect the optimal solution of sub-problem k.Next, we compute the strongly connected components ofG, i.e., the maximal strongly connected sub-graphs. We construct a second graphG′with the strongly connected components ofGas nodes. There is an edge between two nodes ofG′if there is at least one edge between the corresponding strongly connected components ofG.G′is called the condensation ofG.It is known that for every directed graph, its condensation is a Directed Acyclic Graph (DAG). Every DAG has at least one root node. We pick a root node ofG′that corresponds to a strongly connected component with minimal number of elements. We keep all hyperplanes that are elements of the chosen root node and drop the rest.By construction, the sub-problems of the kept hyperplanes are not affected by the removed hyperplanes, i.e., their optimal solutions stay the same. Since we reduce the number of sub-problems but do not change the remaining ones, the resulting lower bound is greater than or equal to the original lower bound.We apply this hyperplane reduction at the end of every MARVI iteration step. Since the lower bounds are never decreased by this method, Theorem 8 still holds.We focus on the MSP to apply our results and as a test case for MARVI. First, we formally introduce MSPs and show that they satisfy the assumptions made within this paper. Second, we investigate a set of dual sourcing instances where we increase the lead time of the slow supplier. We provide the results obtained with MARVI, compare them with the DIP, and discuss the differences.There are other parametric heuristics studied in the literature besides the DIP, such as SIP and COP (Scheller-Wolf et al., 2006; Veeraraghavan & Scheller-Wolf, 2008; Klosterhalfen et al., 2011). In our studies, DIP has always been superior to SIP and COP. Therefore, we only consider DIP in the forthcoming analysis. Since MARVI provides lower bounds to the optimal average cost, we can bound the optimality gap for different policies although the problems cannot be solved exactly.At last, we discuss an MSP instance with six different suppliers and a larger state and action space. We can solve this instance to optimality and show that the complexity does not necessarily correlate with the dimension of the state space, but with other problem parameters.We address the MSP as defined in Veeraraghavan and Scheller-Wolf (2008). Thus, we consider a periodic review problem withN∈Nsuppliers, where unmet demand is backlogged and satisfied as soon as possible. A positive inventory level represents stock on hand, and a negative one represents backlogged demand. The notation is provided in Table 1.With the notation in Table 1 we can state the MSP dynamics asyt+1=yt+∑i=1Nyi,t,0+∑i=1Nδ(Li=0)ui,t-dt,zi,t+1,j-1=zi,t,j,∀i∈1,…,N,j∈1,…,Li,zi,t+1,Li=ui,t,∀i∈1,…,N|Li>0,whereδ(·)is 1 if the given condition is satisfied and 0 otherwise.For readability we denote states asx→t≔yt,zi,t,j|j=1,…,Li,i=1,…,Nand actions asu→t≔ui,t|i=1,…,N, respectively. Therefore, state and action space are defined asS≔[y̲,y¯]×⊗i=1N[0,u¯i]Li,U≔⊗i=1N[0,u¯i].with corresponding dimensionsdimS=1+∑i=1NLianddimU=N.State and action space are illustrated in Fig. 1.The immediate costs are defined asc(x→t,u→t)=h(yt)++b(yt)-+∑i=1Nciui,t,where(·)+and(·)-denote the positive and negative part of the given number. Since(·)+and(·)-are piecewise-linear convex functions, it follows thatc:W→Ris piecewise-linear and convex as well. It can be easily seen that MSP is an SLCP.Without loss of generality we make the following assumption.Assumption 4Zero demand has positive probability.Otherwise, we could constantly place orders of the size of the minimal demand at the cheapest supplier. This will shift the demand distribution accordingly.In order to obtain a compact state space, we impose bounds on inventory level and order quantities. However, these bounds can interfere with Assumption 1. We have to make sure that the system remains feasible. Therefore, additional state-dependent constraints on the action space have to be added.First, we have to make sure that the inventory level upper bound is never violated. We achieve this by bounding the inventory position (the sum of inventory level and all open orders) by the upper bound for the inventory level:∑i=1Nui,t+yt+∑i=1N∑j=1Lizi,t,j⩽y¯.Second, we have to enforce a lower bound for the inventory level. We add a constraint that guarantees that the minimal inventory level at the next time step is greater than or equal to the lower bound:yt+∑i=1Nzi,t,1+∑i=1Nui,tδ(Li=0)⩾y̲+Dmax.However, this constraint might be infeasible if there is no supplier with lead time equal to zero. In this case, we add an artificial supplier with lead time zero that allows orders of the size of the maximal demand. This supplier is assumed to be very expensive and should never be used in practice. The artificial supplier can be interpreted as lost sales if the backlog is getting too big. If the unit order cost of the artificial supplier and inventory level lower bound are chosen properly, this constraint will never be binding except for transient states.These additional constraints make sure that the system is feasible, i.e. Assumption 1 holds.WA allows us to apply the existence results in Section 5 and we show that it holds for MSPs.Theorem 9Weak AccessibilityWA holds for MSPs.Proof ofTheorem 9. In order to prove the theorem, we have to show that there existT∈Nandτ>0, such that for any two statesy→,z→∈S, there are policiesπt,y→,z→∈Π,t=0,…,(T-1), such that(19)P∃t⩽T:x→t=y→|x→0=x→⩾τ,soz→can be reached fromy→with probability greater than or equal to τ in at most T periods. It can be assumed that the system starts with the maximal inventory level and no open orders. This state can always be reached by placing enough orders and assuming zero demand, which has positive probability by Assumption 4. Starting from this state, one can reach any inventory level with constant probability by assuming constant demand and adapting the orders accordingly. Similarly, the open orders of the target state can be reached by assuming zero demand again and placing the orders needed. Since the state space is compact, the needed number of periods can be bounded uniformly. □Corollary 2For every MSP instance as defined in this section, there exists a convex and bounded relative value functionh∗:S→Rand a scalarρ∗that satisfy the corresponding ACOE.In its standard form DIP is a heuristic for MSPs with two suppliers. DIP determines the order quantities as a function of the Inventory Position of the systemPtas well as the so-called Expedited Inventory PositionPte. The Inventory PositionPtis the sum of stock on hand and all open orders:Pt≔yt+∑i=1N∑j=1Liyi,t,j.The Expedited Inventory PositionPteis the sum of stock on hand and all open orders that will arrive within the lead time of the fast supplier, i.e.,Pte≔yt+∑i=1N∑j=1L2yi,t,j,whereL2denotes the lead time of the fast supplier. DIP depends on two parameters. No exact way to compute the optimal parameters is known, but efficient simulation-based approaches have been proposed (Veeraraghavan & Scheller-Wolf, 2008; Arts et al., 2009). DIP is defined as follows.Definition 7Dual Index PolicyDIP uses two parameterszr,ze∈Zand is defined asu2,t=max0,zr-Pte,u1,t=max0,ze-(Pt+u2,t).DIP can be extended to multiple suppliers by introducing one index per supplier and the corresponding order-up-to levels.We consider a set of MSP instances with two suppliers and increasing complexity. We vary the lead time of the slow supplier without changing the other parameters. The problem parameters are given in Table 2.We run MARVI with 15 steps to approximate the optimal policy and to obtain a lower bound. The resulting policy and DIP with optimal parameters are evaluated by simulation of 100,000 periods. All computations were carried out on 4 hexa-core Xeon X5660 CPUs (2.67gigahertz). Maximal memory usage was less than 1gigabytes. The detailed results are given in Table 3.Table 3 shows that the difference between the average costs of the MARVI policy and the DIP increases with the lead time difference of the two suppliers. In the remainder of this section, we analyze how the different cost factors contribute to the total costs and what differences there are between the policies. To this end, we show the average holding, backlogging, ordering and total costs for the different instances and policies in Fig. 2.Table 3 shows thatzeoptis equal to the maximal demand for the scenarios withL1greater than or equal to 4. Since the fast supplier can deliver immediately in the next period this implies that there is no backlogging at all under the DIP, as can be seen in Fig. 2.If the lead time difference is getting too big, the DIP reacts by preventing backlogging completely, which leads to high holding costs. On the other hand, the MARVI policy allows backlogging and usually uses the fast supplier more often to reduce the holding costs significantly compared to DIP.We now go beyond dual sourcing and consider an MSP instance with six suppliers with different lead times and unit order costs. We choose the unit order costs in such a way that all suppliers are used. The complete specification of the instance is given in Tables 4 and 5.We approximate the optimal policy for all six suppliers with 15 MARVI steps. Furthermore, we analyze how much is ordered on average at the different suppliers in a simulation run over 100,000 periods. The results of this experiment are given in Tables 4 and 5.Next, we compare dual sourcing instances where we keep the cheapest supplier and add one of the remaining suppliers. The results of this experiment are shown in Fig. 3. Note that the gap between simulated costs and the MARVI lower bounds has always been less than 1%, usually close to zero. Therefore we consider the resulting policies optimal and only report the optimal costs.The results of this experiment allow several conclusions. First, a strong portfolio effect can be observed. Using all six suppliers can decrease the costs by more than 4% compared to the best combination of the considered instances (the other possible combinations achieve about the same performance as the reported ones, the corresponding results are shown in Appendix B). In particular, the combination of supplier 3 and 6 achieve the best result (optimal average costs of 27.454) even though they are hardly used in the instance with all six suppliers.Second, the computation times vary quite a lot and are not at all monotone in the lead times and state space dimension, respectively. The dual sourcing instances where the average order quantity of the two suppliers is about the same are the easiest to solve, whereas the instances with a strong focus on one supplier are the hardest. In particular, the instance with all 6 suppliers was solved the fastest with the smallest number of hyperplanes.We introduced simple and realistic conditions that imply the existence of an optimal policy and a convex solution to the ACOE for SLCPs, a broad class of MDPs, which is particularly useful in inventory management.We exploited the insights to construct a new approximation algorithm, MARVI, to compute good policies for this broad class of problems. MARVI provides lower bounds of the optimal average costs, which can be used to bound the optimality gap of a particular policy.We first tested MARVI on different MSP instances with two suppliers and compared it to the DIP. The policy computed with MARVI consistently outperformed the DIP. In the considered dual sourcing scenarios, we found that a bigger lead time difference leads to a worse DIP performance.We also investigated a higher dimensional MSP instance with six suppliers. Even though the problem has larger state and action spaces than all the other dual sourcing instances, it was solved faster than most of them, in just a few seconds. This shows that the complexity does not only depend on state/action space dimension but also on other problem characteristics. Determining what drives the complexity is an open task for further research. Once this is understood, it might help designing new, more efficient policy structures, value function approximations, and solution algorithms.Another open point is related to the convergence of MARVI itself. We know that the sequence of lower bounds converges as it is monotonous and bounded, but it is not clear whether it converges to the optimal average cost. Nevertheless, all empirical evidence points towards this direction.There are several interesting extensions to the presented theory, algorithm, and application. First, the presented theory does not provide conditions under which the solution of the ACOE has to be continuous to the boundary of the state space as well. This would immediately imply the existence of a corresponding policy. Moreover, the restriction to compact state spaces is sometimes a bit artificial. Therefore it is an open task to extend the presented theory to unbounded state spaces. This could be one way to solve the problem of discontinuity of the relative value function on the boundary of the state space.It is straightforward to apply MARVI to discounted cost problems, which are much simpler than the average cost problem considered here. The resulting value function approximation would provide a lower bound to the optimal expected discounted cost for any initial state. Another extension could be to replace the expectation in the Bellman operator by the worst case. This is easy to implement, but it is not clear how to interpret this setup within an infinite horizon setting.Lastly, the MSP as introduced here can easily be extended to stochastic lead times as described, e.g., in Kaplan (1970) and Ehrhardt (1984). This will again yield an SLCP, and the theory and algorithm presented in this paper are applicable.To conclude, a promising new approach to approximate optimal policies for SLCPs was presented in this paper and successfully tested. There is still potential to further develop the corresponding theory, as well as to extend the algorithm to different settings.

@&#CONCLUSIONS@&#
