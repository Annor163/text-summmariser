@&#MAIN-TITLE@&#
Speech enhancement using Maximum A-Posteriori and Gaussian Mixture Models for speech and noise Periodogram estimation

@&#HIGHLIGHTS@&#
We use Gaussian Mixture Modelling (GMM) to model Periodograms of speech and noise.A method to find the proper size of GMMs is discussed.We propose a Maximum A-Posteriori (MAP) Periodogram estimation using GMMs.The GMMs are used to enhance the noisy speech using Wiener filter.

@&#KEYPHRASES@&#
Gaussian Mixture Model (GMM),Maximum A-Posteriori (MAP),Wiener filter,Speech enhancement,

@&#ABSTRACT@&#
In speech enhancement, Gaussian Mixture Models (GMMs) can be used to model the Probability Density Function (PDF) of the Periodograms of speech and different noise types. These GMMs are created by applying the Estimate Maximization (EM) algorithm on large datasets of speech and different noise type Periodograms and hence classify them into a small number of clusters whose centroid Periodograms are the mean vectors of the GMMs. These GMMs are used to realize the Maximum A-Posteriori (MAP) estimation of the speech and noise Periodograms present in a noisy speech observation. To realize the MAP estimation, use of a constrained optimization algorithm is proposed in which relatively good enhancement results with high processing times are attained. Due to the use of constraints in the optimization algorithm, incorrect estimation results may arise due to possible local maxima. A simple analytic MAP algorithm is proposed to attain global maximums in lower calculation times. With the new method the complicated MAP formula is simplified as much as possible to find the maxima, through solving a set of equations and not through conventional numerical methods used in optimization. This method results in excellent speech enhancement with a relatively short processing time.

@&#INTRODUCTION@&#
Speech is the simplest, most efficient and most frequent way of communication among human beings. There are lots of applications that transmit, amplify and understand these signals such as mobile communication, hearing aids and speech recognition systems. Due to the modern requirement of portability of these applications, they must perform in differing environments with different background noise types and levels. The background noise will highly degrade the performance of these applications, specially the quality and intelligibility of the output speech signal and hence speech enhancement algorithms can play a critical role in these applications.As discussed in Mohammadiha et al. (2013a), speech enhancement algorithms can be classified into two main categories: unsupervised and supervised algorithms. The simplest and most famous speech enhancement method is spectral subtraction (Boll, 1979) in which the spectral amplitude of noise is estimated from the spectral amplitude of noisy speech and subtracted from it to get to the spectral amplitude of clean speech. Since there are no considerations of the speech spectrum in spectral subtraction, it results in an artificial noise called musical noise. To reduce this musical noise some methods discussed in Sim et al. (1998) can be used which are simple but require an efficient Voice Activity Detector (VAD) to estimate the noise spectrum. In these methods the amplitude of the spectrum is used, but in Lu and Loizou (2008) the complex spectrum is considered. In Wiener filtering (Widrow and Mccool, 1975), speech and noise probabilistic properties are used and hence the enhanced speech suffers from less musical noise with respect to spectral subtraction methods. A method called Short Time Spectral Amplitude (STSA) is discussed in Ephraim and Malah (1984), which is based on Minimum Mean Square Error (MMSE) estimation with the assumption that the speech and noise spectral components are statistically independent and Gaussian random variables. The MMSE-STSA method is derived by minimizing a conditional mean square value of the short time spectral amplitude. As discussed in Wolfe and Godsil (2003), when PDFs of speech and noise spectrums are assumed to be Gaussian, the spectral gain will become the Wiener filter gain. This method is based on the a priori SNR estimation on a frame-by-frame basis by a decision directed approach and Maximum Likelihood (ML) estimator with the assumption that the noise variance is known or can be estimated during the silence intervals. There are different methods to calculate the a priori SNR. A decision directed method is discussed in Cappe (1994). A data driven approach to calculate a priori SNR is discussed in Suhadi et al. (2011) in which two trained artificial neural networks, one for speech and one for noise, is used. As confirmed in the literature, the MMSE spectral gain is superior to the spectral subtraction method but is computationally complicated to implement. To overcome this issue, a method discussed in Wolfe and Godsil (2003) called the Maximum A-Posteriori (MAP) method, which can result in relatively good enhancement results is used.In supervised speech enhancement algorithms we use some additional information about noise and speech such as noise type, speaker identity etc. to improve the enhancement. In supervised methods we create some offline models for speech and noise which are trained using large observed samples of each signal. Some examples of this class of algorithms include the codebook based approaches as discussed in Sreenivas and Kirnapure (1996), where LPC codebooks of speech are used and in Srinivasan et al. (2006), AR coefficient codebooks of speech and noise are used which leads to ML estimates of clean speech. Another well-known and high performance supervised speech enhancement methods are Hidden Markov Model (HMM) based systems and the state-of-the-art approaches are discussed in references (Ephraim, 1992; Sameti et al., 1998; Zhao and Kleijn, 2007). In these methods, the waveform signal is modelled as an autoregressive (AR) process, and hence the waveforms of speech and noise signals are modelled by HMMs. In recent HMM based methods as discussed in Mohammadiha et al. (2013b), Mohammadiha and Leijon (2013) and Veisi and Sameti (2013), distribution of the power spectral coefficients of speech and noise are modelled using HMMs using Gamma distribution. There are also other methods that are based on modelling the spectral amplitude of speech using Gaussian Mixture Modelling (GMM) in which estimates of speech are attained using a MAP criterion as discussed in Hao et al. (2009, 2010) and Fodor and Fingscheidt (2011) and a minimum mean-square error (MMSE) criterion as discussed in Burshteinand and Gannot (2002). The advantage of the supervised approaches such as the HMM based denoising algorithm is that it produces high quality enhanced speech signals. The reason for this is that for each noise type, a system is trained a priori. This is a tedious task in practice and is addressed in Mohammadiha et al. (2013a).In most supervised model based algorithms, GMM is used for the modelling of spectral amplitudes, log spectral amplitudes or Periodograms with their true power and some of these methods give excellent enhancement results. In this research we are going to use normalized Periodograms (with power equal to one) as the vectors to be modelled. In most Bayesian estimation criterions especially MAP estimation, due to the complexity of the estimation criterion formula, some mathematical distributions like Laplacian or Gamma are used to simplify the formula. Here we aim to use GMM to realize an explicit MAP estimate of speech signals. These are different methods from the previous work discussed in Wolfe and Godsil (2003) and Lotter and Vary (2005) (which use approximated distributions to simplify the MAP formula) and hence our aim is to realize the MAP with no approximations.In our previous research we used model-based speech enhancement methods such as codebook constrained Wiener filtering and Bayesian estimation. In codebook constrained Wiener filtering, we used the K-means algorithm to classify the large datasets of speech and noise normalized Periodograms into some centroids. In this way the Periodograms are classified based on their variety and shape with normal power (Chehresa and Savoji, 2009, 2010). Despite the large sizes of codebooks, poor enhancement results were attained. Later we used Gaussian Mixture Modelling (GMM) in which we modelled the Probability Density Function (PDF) of speech and noise Periodograms (Chehresa and Savoji, 2011). In this way we assumed that the PDF of speech and noise Periodograms could be represented by the sum of some Gaussians and in this way, better enhancement results and lower processing times were attained (Chehresa and Savoji, 2012a). In those methods the number of centroids was much less than the codebook centroids in codebook constrained methods. We also used these GMMs to realize the Bayesian estimation of speech and noise Periodograms. In this way we used MMSE and MAP criterions and used an implicit method to implement these two criterions (Chehresa and Savoji, 2012b, 2014). Coping with the fast changes of speech Periodograms from silent frames to active ones and vice versa, the MAP criterion offered better enhancement results than MMSE and hence we are focusing on this method to find more practical ways to realize it and use better GMMs to increase the enhancement results. In our previously introduced methods, we used an implicit realization of MAP which was based on some approximations on the PDF formulation of speech and noise using GMM. Here we are going to implement the MAP criterion without any approximations and hence propose an explicit method to realize it. We also previously used some GMMs with 6 Gaussians for speech and 9 Gaussians for noise in which the number of Gaussians were calculated by trial and error. However, in this paper we will propose a rigorous method to find the reasonable number of mixtures in the GMMs.In Section 2, we discuss the Gaussian Mixture Modelling of the PDF of Periodogram vectors. In Section 3, we realize explicit MAP using GMM with no approximation by means of an optimization algorithm. In Section 4, we discuss a method that could be used as a replacement of the optimization algorithm and gives an explicit formula for realization of MAP with no approximation. In Section 5, the proposed algorithms are tested and the enhancement results are reported.We assume that the noisy speech in the time domain is the sum of speech and noise as in x(t)=s(t)+n(t) where x, s and n are noisy speech, speech and noise respectively and t represents the discrete time index. As discussed in Chehresa and Savoji (2010), all these time domain signals are divided into some short time overlapping windowed frames to preserve the stationarity assumption. By applying an FFT of size 2Ω on each frame we have X(ω′)=S(ω′)+N(ω′) in which X, S and N are the spectrums of noisy speech, speech and noise respectively and ω′∈{−Ω+1, …, Ω} is the discrete frequency index. Using these calculated spectrums, we can calculate the Periodograms of speech, noise and noisy speech as Ps(ω′)=S(ω′)S*(ω′)=|S(ω′)|2, Pn(ω′)=N(ω′)N*(ω′)=|N(ω′)|2 and Px(ω′)=X(ω′)X*(ω′)=|X(ω′)|2 in which * shows the complex conjugate and the | |2 operator calculates the squared amplitude of each spectrum. Under the assumption that speech and noise are uncorrelated which discussed in section 9.3.1.1 of Loizou (2013) and by taking half band plus zero frequencies (due to the symmetric nature of the Periodogram), we can approximately say Px(ω)=Ps(ω)+Pn(ω) in which ω is the discrete frequency index as ω∈{0, 1, …, Ω}. In this way each Periodogram vector has Ω+1 elements. As mentioned in Chehresa and Savoji (2011), we have some sample speech files of different speakers saying different sentences and sample noise signals to train the GMMs of speech and different noise types. To train the GMMs of speech and different noise types Periodograms, we need large datasets of their corresponding Periodograms. All these training files of speech and different noise types, are divided into overlapping frames in time domain and for each frame the Periodogram is calculated. The speech frames contain all active and silent frames of speech. The power of each Periodogram is normalized and made equal to 1 by dividing by the sum of its elements and inserted into corresponding Periodogram datasets. In this way we create large Periodogram datasets for clean speech and each noise type. Each Periodogram dataset can be considered as a matrix with Ω+1 columns reflecting the size of each Periodogram vector and M rows which can be as large as hundred thousand. In each dataset, we will have a collection of Periodograms with their powers equal to 1 and different shapes and hence we can observe the changes in the shapes of Periodograms regardless of their powers. We can take each Periodogram (each row) of these datasets as a point in a Ω+1 dimensional space as symbolically shown in Fig. 1and each colony of Periodograms can be considered as a Gaussian distribution which are separated by circles. This figure not representing the real Periodogram clusters but an exaggerated symbolic illustration of the Periodogram distribution.The centre of each colony can be taken as the mean vector of that Gaussian, and the number of vectors in each class with respect to the total number of vectors shows the probability of that class. In the training procedure we used 32ms frames for the time domain signals. Using the Periodograms of these frames, we can have some repeating patterns of Periodograms of utterances based on their shapes in different speech signals while building the Periodogram datasets. Hence Fig. 1 can be thought of as a reasonable illustration of the distribution of Periodograms in the Ω+1 dimensional space.In Vaseghi (2008), we applied the EM algorithm on these large datasets of speech and noise Periodograms to generate GMMs. By setting the desired number of GMM mixtures as K for the EM algorithm, which is an iterative algorithm, it will result in K scalar probabilities, K mean vectors of size 1×Ω and K covariance matrixes of size Ω×Ω for the final GMM. We can show the PDF of a Periodogram asP=P(ω) as below:(1)f(P)=∑k=1KπkGk(P;μk,Σk),∑k=1Kπk=1where f is the PDF and πkis the probability of the kth mixture (Gaussian) of the GMM which means the number of Periodogram vectors in the kth cluster with respect to the total number of Periodograms that exist in the dataset. Gkis the kth Gaussian with mean vector ofμk=μk(ω) and covariance matrix of Σk. Also K is the total number of mixtures in the GMM. For simplicity of the formulas, the vector names with (ω) indexes are replaced with their bold version without frequency index. We can show Gk, which is a scalar, in terms ofμkand Σkas below:(2)Gk(P;μk,Σk)=exp−12(P/P¯)−μkT∑k−1((P/P¯)−μk)(2π)(Ω+1)/2|Σk|1/2where |Σk| denotes the determinant of the covariance matrix andP¯=∑ω=0ΩP(ω)is the power of P(ω). We divided P(ω) by its power since we are dealing with different powers for the input P(ω) while the power ofμkis equal to 1. To reduce the computational complexity, we assume that the covariance matrixes are diagonal. In this way, we can takeσk=σk(ω) as the elements of covariance matrix diagonal and take it as the variance vector. Rewriting (2) as we obtain:(3)Gk(P;μk,σk)=1(2π)(Ω+1)/2|σk|1/2exp−12∑ω=0Ω((P/P¯)−μk)2σkwhere|σk|=∏ω=0Ωσk(ω). A critical parameter in GMM is the number of mixtures which is K in (1). We used the Bayesian Information Criterion (BIC) to find a reasonable number of mixtures as discussed in Fraley and Raftery (2002) and Dasgupta and Raftery (1998). We changed the number of mixtures (K) from 2 to 26 in the EM algorithm and created the corresponding GMMs. We used the EM algorithm in MATLAB using the gmdistribution.fit command in which one of its outputs in addition to probabilities, mean vectors and covariance matrixes is the BIC of the estimated GMM. As discussed in Dasgupta and Raftery (1998), the number of mixtures that represent the maximum BIC, is the best number of mixtures. Since the BIC values calculated for speech and different noise types are different and we are just looking for maximums, to show them in one figure we map the values between 0 and 1 (the actual BIC values are negative and hence we take the minimum value as 0 and the maximum value as 1). The mapped values of BIC with respect to the number of mixtures for speech and different noise types are shown in Fig. 2.As can be seen from Fig. 2, in our created GMMs, by increasing the number of mixtures from 2 to 26, the BIC values are decreasing constantly and we cannot find the best number of GMM mixtures. This behaviour could be due to the large dimension of the modelled Periodograms which here are vectors with 257 elements. As in Nylund et al. (2007), we tried to calculate the Log-Likelihood of these GMMs and plot them with respect to the number of mixtures. When we are dealing with a Periodogram dataset with M vectors we can calculate the likelihood of one Periodogram of this dataset as below:(4)L(Pm(ω),m=1,…,M)=∏m=1Mf(Pm(ω))Since we are dealing with large Periodogram datasets, L may result in large values which most of the time are assumed as infinity in computer programming languages, hence in our calculations we use Log-Likelihood as below:(5)Llog(Pm(ω),m=1,…,M)=∑m=1Mlog(f(Pm(ω)))In our previous research, we set the number of mixtures K in (1) as 6 for speech and 9 for noise and these numbers were calculated by trial and error (Chehresa and Savoji, 2012a, 2012b). The larger number of mixtures can result in more accurate estimation of speech and noise Periodograms, since we have more Periodogram varieties among the mean vectors of the GMM, whilst increasing the complexity of calculations. This results in higher processing times. Hence we need to get to a compromise between the accuracy of estimations and the complexity of calculations. For the BIC criterion we calculated Llog for each GMM with the number of mixtures changed from 2 to 26. Since the range of calculated Log-Likelihoods for speech and different noise types are different and we wanted to analyze them in one figure, we mapped their range of variation between 0 and 1. These Log-Likelihoods with respect to the number of mixtures for speech and different noise types are shown in Fig. 3.As discussed in Nylund et al. (2007) and could be seen in Fig. 3, for small numbers of mixtures, the curves are quite steep and with increasing number of mixtures the slope is reduced. In all the plots at around 10 mixtures the slopes decrease and hence we take 10 as a reasonable number of mixtures for our experiments, which is not too small to decrease the accuracy and is not too big to make calculations complicated. We therefore created GMMs with 10 mixtures for speech and different noise types. As discussed in Nylund et al. (2007), this is not a deterministic method for finding the number of mixtures in GMMs but a way to select the number of mixtures with a degree of rigour.To attain MAP estimates of speech and noise Periodograms using GMMs, as discussed in section 5.6.2 of Vaseghi (2008) and by replacing the time domain noisy speech, speech and noise signals in equation (5.45) of Vaseghi (2008) with their corresponding Periodograms, we should solve the following equations:(6)PsMAP=argmaxPs[fn(Px−Ps)fs(Ps)]PnMAP=argmaxPn[fs(Px−Pn)fn(Pn)]where fs(Ps) and fn(Pn) are calculated by substituting Ps(ω) and Pn(ω) in (1) and (2). In (6) and as mentioned in Vaseghi (2008), fs(Ps) and fn(Pn) are assumed as the prior probabilities ofPs andPn, respectively and fn(Px−Ps) and fn(Px−Ps) as the corresponding likelihoods. Since we know thatPx=Ps+Pn, we can merge the two equations in (6) and rewrite them as:(7)PsMAP,PnMAP=argmaxPs,Pn[fs(Ps)fn(Pn)]Since there is no explicit solution for realization of this complicated algorithm (the sum of K=10 exponentials for the speech PDF multiplied by the sum of K=10 exponentials for the noise PDF, totally 100 exponential terms), we tried to use optimization algorithms to estimatePsMAPandPnMAPin a numerical basis. The optimization algorithms are classified as derivative based algorithms and genetic based algorithms in which no information about the optimum path is used. On the other hand, the derivative based algorithms can operate using the explicit derivative of the cost function or finding the best path by trial and error. The optimization algorithms can also be unconstrained or constrained. MATLAB software has a variety of optimization algorithms. The fmincon command in MATLAB is a derivative based algorithm to find the minimum of a cost function. It can input the explicit derivative equation and in the case of not having such an equation, it can calculate the best path using numerical methods. This algorithm performs within constraints which could be fed to it as simple equations. One of the disadvantages of this algorithm while dealing with large input vectors is long processing time. To use fmincon in MATLAB, an initial estimate of speech and noise Periodograms existing in the noisy speech Periodogram are entered to an iterative algorithm. Using predefined conditions, changes are made on these initial estimated Periodograms in such a way to maximize the multiplication of the PDFs of speech and noise Periodograms, which is actually done by minimization of a cost function, and then these new speech and noise Periodograms are entered into the next iteration. These iterations will continue while the changes made to the Periodograms are followed by PDF increase (which is actually the cost function decrease) and not violating the predefined constraints. The initial estimates of the noise Periodogram can be calculated using the Minimum Statistic (MS) method in which the minimums of smoothed noisy speech Periodogram in some successive frames is taken as the Periodogram of noise (Martin, 1994, 2001; Chang, 2013). By subtracting it from the noisy speech Periodogram, we can have an initial estimate of the speech Periodogram. Moreover, the estimated noise Periodogram resulting from the MS method is used to find the right noise codebook. The MS noise Periodogram is first normalized (making its power equal to 1) and then the distance between this noise Periodogram and all the mean vectors of different noise GMMs is calculated. The GMM that contains the mean vector that is the closest to the MS noise Periodogram, will be chosen as the appropriate GMM for the current noisy file. The MAP estimation using optimization is shown in Fig. 4.In Fig. 4, the resultedPs0 andPn0 from MS algorithm is fed to the optimization algorithm (fmincon). The i index represents the ith iteration and hencePsiandPniare the Periodograms of speech and noise estimated in the ith iteration. In the cost function, if the value of 1/(fs(Ps)fn(Pn)) is minimized, then it will result in the maximization of fs(Ps)fn(Pn). Since the fmincon optimization algorithm can accept just one vector as the input and here we aim to estimate bothPsMAPandPnMAP, we can concatenate the two Periodogram vectorsPsMAPandPnMAPtogether as one vector. If we assumePs=Ps(ω)=[Ps0,…,PsΩ]andPn=Pn(ω)=[Pn0,…,PnΩ]where the number of frequency bins is Ω+1, we can havePsn=Psn(ω)=[Ps(ω),Pn(ω)]=[Ps0,…,PsΩ,Pn0,…,PnΩ]and rewrite (7) asPsnMAP=[PsMAP,PnMAP]=argmaxPsn[fs(Ps)fn(Pn)]wherePsnMAPis a vector made up of the two vectorsPsMAPandPnMAP. We use some constraints with fmincon in the form of some input matrixes to the algorithm as below:(8)argminPsn1f(Ps)f(Pn)whereAPsn≤bAeqPsn=beqlb≤Psn≤ubThe matrixAis used to keep the power of estimated speech and noise Periodograms within a range close to the estimated power using the MS method. We takeP¯nMS=∑ω=0ΩPnMS(ω)as noise power andP¯sMS=P¯x−P¯nMSas speech power in which the MS superscript stands for minimum statistics method andP¯x=∑ω=0ΩPx(ω)is the power of noisy speech. We take 0<α<1 as a small permitted tolerance of these powers. MatrixAis defined in a way that if it is multiplied byPsnresults in the vector∑ω=0ΩPs,−∑ω=0ΩPs,∑ω=0ΩPn,−∑ω=0ΩPnand hence thebvector is[(1+α)P¯sMS,−(1−α)P¯sMS,(1+α)P¯nMS,−(1−α)P¯nMS]. TheAeqmatrix is defined in a way that if it is multiplied byPsnresults inPs+Pnand thebeqvector is taken asPx. Also thelbvector has the same length ofPsnand defines the lower bound onPsn. Since all the elements of the estimated Periodogram vectors cannot be negative, we setlbas an all zero vector of the same length asPsn. There are some other conditions for the fmincon optimization algorithm such as input tolerance or TolX, output tolerance or TolFun and constraint tolerance or TolCon. The iterations of fmincon will stop if the difference of the output of cost function in two consecutive iterations is less than TolFun. TolCon determines the least change in limitations. TolX determines the least change in the input vector. In our experiments the suitable values for these parameters are calculated through trial and error. The resultingPsMAPandPnMAPare used to construct a Wiener filter to enhance the noisy frame as below:(9)W(ω)=PsMAP(ω)PsMAP(ω)+PnMAP(ω)Sˆ(ω)=W(ω)X(ω)whereSˆωis the estimated spectrum of clean speech in the noisy frame. By applying inverse FFT, the estimate of the clean speech signal in the time domain frame can be recovered. The enhancement results of this algorithm are shown in Fig. 5.Due to the complexity of PDF formulation, using optimization algorithms to calculate MAP estimates of speech and noise Periodograms is too computationally time consuming. Hence finding an explicit solution to maximize the multiplication of speech and noise PDFs is of great interest. To find such a solution at first we should simplify the multiplication of speech and noise PDFs. Since the PDFs represent positive values, the maximization of the multiplication of the PDFs is equivalent to the maximization of the logarithm of their product as below:(10)PsMAP,PnMAP=argmaxPs,Pn[fs(Ps)fn(Pn)]≡argmaxPs,Pn[ln(fs(Ps)fn(Pn))]=argmaxPs,Pnln(fs(Ps))+ln(fn(Pn))︸TIn this way the maximization of fs(Ps)fn(Pn) could be replaced with the maximization of T=ln(fs(Ps))+ln(fn(Pn)). In (10) each PDF is the sum of 10 exponentials as in (1) and (2) and hence we can considerfs(Ps)=fs1(Ps)+⋯+fs10(Ps)andfn(Pn)=fn1(Pn)+⋯+fn10(Pn). From Jensen's inequality we know that:(11)ln(fs(Ps))≥ln(fs1(Ps))+⋯+ln(fs10(Ps))ln(fn(Pn))≥ln(fsn(Pn))+⋯+ln(fn10(Pn))T≥ln(fs1)+…+ln(fs10)+ln(fsn)+⋯+ln(fn10)where in the last equation we eliminated thePsandPnvariables for simplicity. As discussed in Beal and Ghahramani (2003), to maximize the T term in (10), we can maximize its lower bound as shown in the last equation of (11). Our experiments show that this maximization of the lower bound of T almost always results in the maximum of the T term. The exceptional cases can be considered as the estimation error. Hence, by the assumptions that we made, we can claim that the maximization of logarithm of sum of some positive exponentials in the PDF formula can be taken equivalent to the maximization of the sum of logarithms of those values. So we can insert the logarithm into the summations of exponentials of each PDF. In this way, by replacing the PDFs using (1) and (3), we can rewrite (10) as below:(12)PsMAP,PnMAP=argmaxPs,Pn∑k=1K[ln(πskGsk)+ln(πnkGnk)]=argmaxPs,Pn∑k=1KCk−12∑ω=0Ω(Qs−μsk)2σsk+(Qn−μnk)2σnkwhereQs=Qs(ω)=Ps(ω)/∑ω=0ΩPs(ω)andQn=Qn(ω)=Pn(ω)/∑ω=0ΩPn(ω)andGskandGnkare actuallyGsk(Ps;μsk,σsk)andGnk(Pn;μnk,σnk). Also Ckis a constant asCk=ln(πsk)−Ω+12ln(2π)−12ln|σsk|+ln(πnk)−Ω+12ln(2π)−12ln|σnk|. Now to maximize the final extracted function in (12), we calculate its first derivatives with respect toQsandQnand we take them equal to zero and hence equal to each other.(13)∑k=1K∑ω=1Ωμsk−Qsσsk=∑k=1K∑ω=1Ωμnk−Qnσnk=0One of our very basic considerations in getting to all these equations was Ps(ω)+Pn(ω)=Px(ω) and as we know since they are all Periodogram vectors, their elements just can be positive or zero. In this way, when we write Ps(ω)=Px(ω)−Pn(ω) or Pn(ω)=Px(ω)−Ps(ω) as we used in getting from (6) to (7), we consider that there is no negative elements in the resulting Ps(ω) and Pn(ω) from the subtractions. In this way we are treating each frequency bin independently and we have this consideration almost everywhere in this paper. In (13), the two summations (one on speech variables and the other on noise) are equal to 0. So we can write∑k=1K∑ω=1ΩZ(ω,k)=0where Z(ω, k) is a function of frequency and mixture number. With our assumption of independency of frequency bins we can say that for each specific frequency of ω∈{0, 1, …, Ω} we have∑k=1KZ(ω,k)=0. Hence we can say for each frequency, the value of Z(ω, k) is related to the same frequency from different mixtures. With this assumption, we can differentiate the two sides of (13) with respect to ω and get rid of the summations on ω and rewrite (13) as below:(14)∑k=1Kμsk−Qsσsk=∑k=1Kμnk−Qnσnk=0In the same way as discussed in Section 3, the MS method is used to determine the proper noise GMM. As discussed in Section 3, we can also use the MS estimates of noise Periodogram to find the right power for the final MAP Periodograms of speech and noise. To solve (14),QsandQnare replaced withPs/P¯sMSandPn/P¯nMSand thenPnis replaced withPx−Ps. We can then calculate the MAP estimate of the speech Periodogram as:(15)PsMAP=∑k=1Kμskσsk−μnkσnk+PxP¯nMS∑k=1K1σnk1P¯sMS∑k=1K1σsk+1P¯nMS∑k=1K1σnkWe can calculatePnMAPthroughPnMAP=Px−PsMAPand zero any resulting negative elements. Then using (9) we can enhance the noisy frame. The enhancement results of this algorithm are shown in Fig. 5.

@&#CONCLUSIONS@&#
Two different methods for the realization of explicit MAP estimates of speech and noise Periodograms using GMM, were proposed. One is based on optimization algorithms which due to the use of different constraints to control the algorithm, resulted in local maximums in the maximization process and hence poor enhancement results and long processing times. In another method called simple MAP, we try to simplify the MAP criterion as much as possible. Using this latter method can result to maximums equal or close to the global maximums in the MAP formula and hence better enhancement results. This method is suitable for many real-time applications. By creating GMMs for more noise types and using them and adding them to the enhancement algorithm we were able to improve the performance of the algorithm with a slight increase in the processing time.