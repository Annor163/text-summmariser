@&#MAIN-TITLE@&#
Improvement of RBF neural networks using Fuzzy-OSD algorithm in an online radar pulse classification system

@&#HIGHLIGHTS@&#
We introduce and examine a new methodology for training radial basis function (RBF) neural networks.We use fuzzy clustering in order to improve the functionality of the Optimum Steepest Descent (OSD) learning algorithm.We employed this new method in an online radar pulse classification system, which needs quick retraining of the network once new unseen emitters detected.

@&#KEYPHRASES@&#
Fuzzy C-Means,Optimum Steepest Descent (OSD),Radial basis function (RBF) neural network,Three-Phase OSD,Fuzzy-OSD,

@&#ABSTRACT@&#
In this paper a new methodology for training radial basis function (RBF) neural networks is introduced and examined. This novel approach, called Fuzzy-OSD, could be used in applications, which need real-time capabilities for retraining neural networks. The proposed method uses fuzzy clustering in order to improve the functionality of the Optimum Steepest Descent (OSD) learning algorithm. This improvement is due to initialization of RBF units more precisely using fuzzy C-Means clustering algorithm that results in producing better and the same network response in different retraining attempts. In addition, adjusting RBF units in the network with great accuracy will result in better performance in fewer train iterations, which is essential when fast retraining of the network is needed, especially in the real-time systems. We employed this new method in an online radar pulse classification system, which needs quick retraining of the network once new unseen emitters detected. Having compared result of applying the new algorithm and Three-Phase OSD method to benchmark problems from Proben1 database and also using them in our system, we achieved improvement in the results as presented in this paper.

@&#INTRODUCTION@&#
Radial basis function (RBF) neural networks introduced into the literature by Broomhead and Lowe in 1988 [1]. These feed-forward networks, which are trained using a supervised algorithm, have been extensively used for interpolation regression and classification [2,3]. This popularity is due to some advantages comparing to other neural network, such as better approximation characteristics, simpler network architecture and faster training procedures. For this reason, they have been widely used in interpolation regression and pattern classification problems and researchers have kept on working on improving performance of learning algorithms [4].These networks follow a different approach in designing a supervised neural network from other neural networks. In spite of the back propagation (BP) algorithm or the design of a multi-layer perceptron (MLP) which may be similar to a stochastic approximation method [5], design of the network in RBF neural networks could be referred as a curve-fitting (approximation) problem in a space with higher dimensionality than the input space [6].RBF neural networks have a three-layer feed forward architecture with a single hidden layer of units. The first layer, consisting of n input units, connects the input space to the environments. A hidden layer, which consists of NhRBF units with a basis function as the activation function, transforms the input space to the hidden space which is of higher dimensionality than the input layer, and the output layer of m linear units produces the final classification or output to the input pattern. Each hidden unit estimates similarity between the input pattern and its connection weights or centers, locally. These networks implement the mapping f:Rn→Rmthat:(1)Y=(y1,…,ys,…,ym):Rn→Rm:ys(X)=∑j=1NhwjsφX−Cjσjwhere X∈Rnis an input pattern, ysis sth network output, wjs∈R refers to the weight of the link between jth hidden neuron and sth neuron in the output layer, and finally Cjand σjare the center and width of the jth RBF unit in the hidden layer, respectively. Also the term φ denotes an activation function such as Gaussian function, defined by the equation [7]:(2)φ(r)=e−r2Besides being efficient function approximator, RBF networks are also capable to solve pattern classification task [8,9]. In such applications, RBF neural network should map continuous input space into a set of classes by assigning the input pattern to the class of the output unit with the maximum value. Throughout this mapping process, the hidden layer performs a nonlinear transformation on the input patterns into a set of corresponding patterns in high-dimensional space whereas the output layer does a linear one. The underlying principle of these transformations is Cover's theorem on separability of patterns. According to this principle, patterns mapped into a space with higher dimensionality than the input space are more likely to be classified linearly [9]. As a result, patterns which are the result of the hidden layer are more probable to be linearly separable.The learning process of RBF neural network, in the most applications, consists of two steps [10] which are an unsupervised learning stage for adjusting the parameters of the hidden layer including RBF centers Cj∈Rn, j=1, …,K, and widths of the units σj∈R and a supervised one for estimating weights for connectors of these units wjs∈R. Due to different applications of these networks, a large variety of learning algorithms, has been employed for training RBF networks, have been proposed by several researchers including randomly selection of fixed centers and supervised selection of centers [11], using a regularization approach to estimate [8], orthogonal least squares algorithm [12], employing Expectation–Maximization (EM) algorithm for optimizing the cluster center after an initial clustering phase [13], regression tree to find the centers and width of the RBF units [14], self-organized selection of centers containing K-mean clustering procedure and the self-organizing feature map clustering procedure [15], individual training of each hidden unit based on functional analysis [16] or initial selection of a large number of hidden units which is reduced as the algorithm proceeds [17]. Genetic algorithms are employed in [18] to select the structure of the network and the parameters simultaneously. Also the pseudo-inverse (minimum-norm) method [19], the Least-Mean-Square (LMS) method [20], the Steepest Decent (SD) method [21], and the Quick Propagation (QP) method [22] are used for calculating weights of the network.In our previous work [22], we proposed an optimized version of Steepest Descent method, called Optimum Steepest Decent (OSD), used an optimum learning rate in each epoch of the training process. In addition to the higher speed of learning process, this method also attains an absolute stability in network response. Although this improvement produced a better final result, randomly selection of the centers and widths of the RBF units was deficient. In order to improve the performance of the OSD, we introduced Three-Phase learning method which optimized the functionality of OSD learning method by attaining greater precision in initializing center and width of RBF units [23]. This method used K-Means clustering algorithm to calculate center of RBF units in the first phase and then their widths are estimated using K-Nearest Neighbor (KNN) algorithm. Adjusting the weights between hidden and output layer is performed in the third phase via OSD method. Despite the fact that these improvements guarantee that the global minimum in the weight space will be attained, there is a shortcoming that Three-Phase learning method may result in sub-optimal solutions. Because of being sensitive to the initial choice of cluster centers and widths, the K-Means algorithm and K-Nearest Neighbor algorithm may get stuck in a local optimum solution. This may cause learning process take more epochs to achieve a desired response.Some critical applications, like helicopter sound identification system [23] or our online radar pulse classifier, need to be assured that the same response would be produced in a reasonable time, if the training process is done on the same training set. In this paper we improve the functionality of the RBF neural network by a new learning method called Fuzzy-OSD. In this approach we have replaced the K-Means clustering algorithm with a fuzzy clustering algorithm which is not sensitive to the initial values of the cluster centers. In other words, whenever we retrain the network with parameters defined by the fuzzy clustering algorithm, we get the same result. This is highly desirable in applications where producing the same response on the same training set through the retraining process is essential, in addition to keeping performance and speed of learning at the same level.The organization of the paper is as follows. Section 2 describes in depth how fuzzy clustering is going to be used in the unsupervised learning stage. The Fuzzy-OSD method is presented in Section 3. Section 4 presents the implementation of the new method on several benchmark problems. Also the performance of this method in comparison with previous ones is discussed in this section. Experimental results of employing the proposed method in our online radar pulse classification system and a comparison of the mentioned methods is presented in Section 5. The last section, Section 6, concludes the previously presented sections.Cluster analysis is a technique for grouping a set of unlabeled patterns or objects into a set of clusters based on similarity or dissimilarity among them so that the similarity between patterns assigned to the same cluster are as much as possible while dissimilarity in objects from different clusters should as less. A clustering task aims to find previously hidden structure in the objects, assuming that a natural grouping exists in the data [24]. According to the way of assigning object to the clusters, partial assignment or full assignment, there are two main approaches in clustering techniques, hard (crisp) clustering and fuzzy clustering, which are discussed in the following sections.Some clustering techniques force the full assignment of the objects even if they are equally similar to two or more clusters. In other words, although an object may be in the same distance of two clusters, it should be assigned to one of them and this hard assignment does not reflect the uncertainty of membership of the objects in the clusters [25]. These methods are called crisp or hard clustering. In the classical clustering algorithm such as K-Means objects are divided into some partition so that every object should be assigned only to one of those pairwise disjoint partitions. A common objective function, which is going to be minimized in a crisp clustering procedure, defines as the sum of distances between objects and the cluster centers that is [26]:(3)JH=∑i=1KJi=∑i=1K∑kuk∈Ciuk−ci2where K is number of clusters, ukrefers to kth object of the ith cluster, Cidenotes the ith cluster and cirefers to the centroid of it.On the other hand, fuzzy cluster analysis provides uncertainty and ambiguity in assignment of members to the clusters which is gradual membership of objects to different clusters in [0,1]. These membership degrees express how ambiguously or definitely an object should belong to a cluster. The first use of fuzzy set concept in clustering was proposed by Ruspini in 1969 [20]. He mentioned that points in the center of a cluster could have a degree equal to 1, while membership degree of the boundary points depends on their distance to the cluster centers. The much closer an object to a cluster center, the closer degree of its membership to that cluster will be to 1 [24].In the field of fuzzy clustering two types of fuzzy cluster partitions, which are much richer means for representing cluster structure, have evolved: probabilistic and possibilitic [24]. The most widely used approach, the probabilistic fuzzy clustering, was firstly introduced by Bezdek in 1973. In order to produce an optimal probabilistic fuzzy partition of a given data set, this method aims to minimize the objective function [27]:(4)Jf(X,Uf,C)=∑i=1c∑j=1nuijmdij2,whereX=x→1,…,x→nis the set given of n objects, c denotes the number of clusters (1<c<n), and the parameter m, m>1, is called the fuzzifier or weighting exponent. Uf=(uij) refers to the membership matrix and we interpret uij∈[0,1] as the membership degree of thex→j,to the ith cluster relative to all other clusters. Finally,C=c→1,…,c→ccomprises the cluster centers and dijdenotes the distance between the jth object and center of the ith cluster. This method is based on two constraints on the membership matrix, the conditions on probabilistic fuzzy partitions [24]:(5)I.∑j=1nuij>0∀i∈1,…,cand(6)II.∑i=1cuij=1∀j∈1,…,nThe first constraint guarantees that no cluster is empty and the second one, called normalization condition, ensures that the sum of the membership degrees for each object equal 1 [28]. Since the parameters of objective function Jfdepend on each other, it cannot be minimized directly. Therefore, an iterative algorithm is used to alternatively optimize the membership degrees and the cluster parameters. Fuzzy C-Means (FCM) is one of the probabilistic fuzzy clustering algorithms which is described in the next section.Fuzzy C-Means, a probabilistic fuzzy clustering method, is a stable, reliable, simple, and fast clustering method with low computational demands. Since Euclidean distance between an object and the cluster prototype (the cluster center) is used as the similarity measure, FCM is able to recognize nearly same-sized clusters in hyper-spherical shape in a p-dimensional space. By minimizing the Eq. (4) with respect to the two constraints (5) and (6) on fuzzy partitions, the following update formula for the membership degrees and the cluster centers is obtained from Jf[29]:(7)c→i=∑j=1nuijmxj∑j=1nuijmand(8)uij=dij−2/(m−1)∑t=1cdtj−2/(m−1)where dijis the Euclidean distance between the jth object and the ith cluster's center. Fig. 1shows the FCM algorithm procedure which updates membership degrees and cluster centers frequently until their modifications gets less than a predefined threshold.Despite K-Means clustering algorithm, which is liable to get stuck in a local minima, the probabilistic FCM algorithm is quite insensitive to its initialization and in not likely to result in local solutions [24].Since the different layers in an RBF neural network have different roles, different parameter estimation procedures should be employed for each layer. This section describes our new approach in training RBF neural network in detail. The proposed method, Fuzzy-OSD learning algorithm, aims at lower deviation in network responses throughout retraining the network besides greater precision, consists three separate phases. In the first phase, fuzzy C-Means clustering algorithm is used for estimating RBF unit centers. Employing FCM algorithm in the first phase brings about better results in addition to smaller jitter range in them. Calculation of the hidden units’ widths is done in the second phase by utilizing K-Nearest Neighbor (kNN) algorithm. At the final step we employed the OSD learning method in order to determine the output layer weights aside taking advantage of optimum learning rate (ORL) over training procedure. Fig. 2exhibits the Fuzzy-OSD learning algorithm steps.In the first phase of this method the centers of RBF units are calculated using fuzzy C-Means clustering algorithm. In this phase we are going to cluster the input patterns into a number of groups equal to number of RBF units. According to the Cover's theorem [9] this number is better to be greater than the dimensionality of the input space. Steps of this algorithm are as follows:1.Choosing a proper value for c, number of RBF units which is equal to number of clusters, normally greater than dimensionality of input space.Initializing membership matrix with random normalized values.Calculating centers from Eq. (7).Updating membership degrees using Eq. (8).Repeat steps (3) and (4) until termination criterion is met.Having processed these steps, we adjusted RBF unit centers with centers calculated from Eq. (7).Centers’ width has a vital role in the performance of an RBF neural network and adjusting them is an important phase in designing the architecture of an RBF neural network. If widths are too large over-smoothing will occur in the estimated probability density. On the other hand, over-adaptation may happen in the particular data set, if widths have been determined by too small values. In addition, very small or large widths tend to cause numerical problems with gradient descent methods as their gradients vanish [30]. Different methods have been proposed in [10,31] for initial settings of center widths in RBF networks.To adjust centers’ widths in RBF units, we have employed the K-Nearest Neighbor algorithm. In this method the width for each center is set to the mean of distances from the K-Nearest prototypes of Cj[10], where the mean of squared distances between the center of cluster j and its K-Nearest Neighbors is calculated as parameter σj. All distances, dij=||Cl−Cj||, l=1, …,c. l≠j, are calculated and renumbered using a mapping function:(9)F=(l,j)→ll<j(l,j)→l−1l>jwhere j is the index of the current RBF unit and l is the index of clusters. Also, there is a permutation τ such that dτ(i)≤dτ(i+1)≤…≤dτ(c−1) and σjis set to:(10)σj=α1p∑i=1pdτ(i)where α>0 has to be set heuristically [10].To calculate the output weights of the network we use the OSD learning method, which uses an optimum learning rate in each iteration of the training process [22]. The Optimum Delta Weight Vector (ODWV) can be determined as:(11)ΔWopt=λoptΔW=(EΦ)(EΦ)TEΦ(EΦΦT)(EΦΦT)Thence(12)Wnew=Wold+(EΦ)(EΦ)TEΦ(EΦΦT)(EΦΦT)Twhich the initial value for W is set at random [22].To be assured of robustness and accuracy of our new method comparing other RBF learning methods, we applied it to five classic benchmark problems from Proben1 database from the UCI machine learning repository [32] and a real-time benchmark problem called Jaffe [33].Some information about data sets is shown in Table 1. Three-Phase OSD method and Fuzzy-OSD method have been applied to the mentioned problems. For classification problems the winner-takes-all strategy is used to determine the classified class. It means that the output of the network with highest response is taken, and its corresponding class is the resulting class for the input vector. Then the classification error is measured as the percentage of incorrectly classified samples. All experiments were run 30 times, and the average error and its standard deviation were computed. In the experiments at first the centers, widths and weights are computed using training set, 70% of data set, and then the remaining part of the data set, as the test set, is used to validate the trained network functionality. The average error and its standard deviation were computed on this set, too. Acquired result from different learning methods are compared and summarized in Table 2. In the Fuzzy-OSD technique, we set the fuzzifier, m, to 2.As shown in Figs. 3–5, the classification error in consecutive training epochs of Fuzzy-OSD method falls to a smaller value much faster than the other method in all the five data sets. Also, it is obvious that the Fuzzy-OSD method is faster than Three-Phase OSD method.Table 2 compares the results obtained by the two learning strategies on Iris data set. As shown in this Table, Fuzzy-OSD is usually able to achieve better results than the other method. Using the Three-Phase OSD algorithm resulted in 96.63% of correctly classified patterns, while Fuzzy-OSD learning method increased the performance of network to 97.08%. The same results are also drawn from the testing set. Similar results have been obtained for the other data sets.The most important improvement is attaining a lower deviation in the network responses which is vital in the online classification system. Table 2 also shows the standard deviation of the results of the two methods. Fuzzy-OSD method achieved smaller value which means that it is more reliable in producing the same result in the retraining process than Three-Phase OSD method. In all the five data sets, Fuzzy-OSD decreased standard deviation of the classification error of the test sets (these values have been shown in bold). In other words, if we retrain the network employing Fuzzy-OSD learning strategy, it is more likely to construct the same result as before than Three-Phase OSD.There are many problems in nature that time is a critical factor for them and decisions should be made in short time. These kinds of problems are called real-time. Due to the nature of this research, we applied the train methods in a benchmark real-time problem to validate the trained network functionality. In this research, the classification of the facial expression of the individuals performed based on the emotional state. This kind of classification has been applied in different areas such as e-learning environments. In this case, the system should offer suitable suggestions based on the classification results. Time is the critical factor in this problem and the system should be classified learners based on the emotional state in a short time [34]. According to the explanations, this classification is a real-time problem.There are different data sets about facial expressions. Jaffe is the most comprehensive facial expression data set that has been used in this research [33]. This data set includes 213 photos from 10 Japanese women with 7 emotional states for each of them.In this application, a photo selected randomly from the emotional states of each individual for the test set in each iteration. The rest of the implementation has been performed like the classic datasets.As shown in Fig. 6, the classification error in consecutive training epochs of Fuzzy-OSD method falls to a smaller value much faster than the other method. As shown in Table 3, the Fuzzy-OSD method is able to achieve better result than the other one in this application. The proposed method is more precise than the other. Also, the deviation of the Fuzzy-OSD method is lower than the Three-Phase OSD which means that it is more reliable in producing the same result in the retraining process.Today's radars have been extensively used to identify and track down any moving object such as aircrafts, ships, and so on. The main usage of radars in military operations is directing weapons. Electronic Warfare (EW) systems aim at detecting the presence of hostile emitters, analyzing and locating them accurately and quickly [35]. To achieve this goal, the system needs some equipments to measure pulses’ attributes Radio Frequency-RF (GHz), Pulse Width-PW (μs), Pulse Repetition Interval-PRI (μs), Pulse Amplitude-PA (dbm), Direction of Arrival-DOA (degree), etc. These values are put into a Pulse Description Word (PDW). Our system has two successive modules: deinterleaver module and classifier. A batch of PDWs is fed into the deinterleaver module in order to extract the interleaved pulses of the radars. Emitter Description Words (EDWs), which are output of the deinteleaver unit and contain features of detected emitters, comprise the input of the classifier unit. Because the receiver antenna is always changing in position and direction, a lot of EDWs of an emitter will be produced the deinterleaver module that should be merged and classified. On the other hand, new unseen emitters may not exist in our emitter's library, so whenever a new emitter is detected the classifier has to be retrained. So we need a classifier which is trained as fast as possible, produces results with great precision and it is so important that it should have very small deviation in responses throughout lots of retraining attempts. Fig. 7illustrates the organization of our online radar pulse identification system.Since RBF neural networks have strong abilities in classification and pattern recognition, so we have employed it for EDWs classification. In order to classify EDWs, we considered four main features of the simulated radars’ EDWs which are RF, PRI, PA and PW.We trained the RBF network with Three-Phase OSD and Fuzzy-OSD methods, which Fig. 8shows the error function of these methods over training iterations and Table 3 shows results in detail. As we hoped, the Fuzzy-OSD learning method attained better precision than Three-Phase OSD in both average and standard deviation of classification error. As Fig. 8 shows, employing the new method the convergence of training procedure is faster than the old one. In addition, according to the results presented in Table 4, we have fewer variance of error in Fuzzy-OSD method than Three-Phase OSD in both training and test sets. So we can consider this method to have better performance in our application.

@&#CONCLUSIONS@&#
