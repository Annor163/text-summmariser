@&#MAIN-TITLE@&#
Learning structured visual dictionary for object tracking

@&#HIGHLIGHTS@&#
A tracking algorithm which exploits both appearance and geometric informationTwo complementary features are adopted in building the appearance dictionary.A shape context approach to capture the stable geometric patterns of keypointsThe proposed tracking algorithm performs well in challenging conditions.

@&#KEYPHRASES@&#
Object tracking,Bag of features,Appearance model,Geometric relationship,

@&#ABSTRACT@&#
Graphical abstract

@&#INTRODUCTION@&#
Numerous effective tracking approaches have been proposed in the literature [1]. One of the most important factors for effective tracking algorithms is how to account for appearance variation with effective representations. Among the existing algorithms, some focus on holistic appearance models [2–6], whereas others resort to local parts or patches [7–12].In visual categorization, the bag-of-features (BoF) algorithm has been successfully applied to object and natural scene classification with different codebooks or decision criterion. While most existing works [13–17] use interest point detectors to select patches and SIFT descriptors [18] for object representation, Nowak et al. [19] show that random sampling with bag of features has more discriminative power than that with interest point detectors when a large number of patches are used.Aiming to develop an effective model with features extracted from image patches, we explore the BoF paradigm to collect representative features that best characterize an object and the background. We cluster a large set of patches into a compact number of groups as a visual dictionary which consists of the most distinctive features of both the foreground object and the background. Although Yang et al. [20] have applied BoF representation to tracking, their generative approach does not use negative samples or geometrical structures. The conventional BoF algorithm used in [20] treats an object image as a collection of independent patches without considering their spatial relationships. Numerous geometric descriptors have been developed to encode spatial information as features for object recognition. Cootes et al. [21] propose the active shape models (ASMs) to describe an object in terms of its contour. The ASMs consist of a set of landmark points representing the shape, which are analogous to the active contour model [22]. The active appearance models (AAMs) are developed to model both shape and texture information of an object [23]. Shape matching is carried out via an optimization process to minimize the difference between the current estimate of appearance and the target image. However, it is not applicable to real-time tracking since the optimization process is time-consuming. Recently, several algorithms have been proposed to incorporate spatial information with the patch collections [24–26] for object categorization and recognition. Nevertheless, these methods require significant numbers of training samples since they need to find statistically the most representative keywords to handle intra-class variance. Moreover, the time-consuming training procedures to describe the spatial relationships among the large training samples and build classification models make the previous works impractical for real-time object tracking. For object tracking, it is desirable to develop effective methods that exploit spatial relationships and constraints among features.Within the context of object tracking, numerous methods that utilize spatial information have been proposed due to its importance. Some approaches are formulated based on Markov networks [27,28] to encode the relationships of interest points or regions in the image space. However, [27] uses a fixed number of local patches and in [28] both appearance and spatial features entirely rely on the locations of detected keypoints. Therefore, they are not flexible enough to deal with complex scenarios. Similarly, [29,30] also use keypoints to extract local appearance patches, but no local spatial information among keypoints is utilized although [29] takes the overall geometry of the entire object into consideration. In addition, many approaches rely on the motion estimation of interest points (based on features or regions) between successive frames [31,28,32,33]. These methods first match features between two frames and then estimate their corresponding motion parameters. The global object movement is then computed from the motion parameters of local features. However, features in these methods are processed independently and thus rich geometric structures among them within the same image are ignored. Here we propose a method that focuses on statistical relationships of features and does not involve motion estimation.In this paper, we propose a tracking algorithm based on structured visual dictionary that exploits both appearance and geometric information. The contributions of this paper are summarized as follows. First, we extend the BoF representation scheme in the context of object tracking. We propose a discriminative tracking algorithm for learning an effective and compact visual dictionary of both foreground target and background. Second, we encode and exploit geometric constraints of local invariant features. Instead of computing movement of individual features to infer the global object motion, we model the statistical distribution of features in the 2D image plane. Third, our formulation facilities the integration of appearance and geometric information. Both appearance and geometric information are characterized as histograms so they can be combined in a unified and reasonable way. In addition, the proposed formulation accommodates an update scheme easily, thereby facilitating effective object tracking.In the first frame, the target object is initialized manually by a bounding box. We use the similarity transform to describe object motion with state variable xt. A small number of training images are collected to construct the structured visual dictionary for the target object as follows. For each training frame, a rectangular bounding box enclosing the target object is bootstrapped either by another tracker (with visual inspection) or by hand. Similar to the sampling scheme in [34], we crop out a set of patches P+={P| l(P)−l(xt)║<α} of random size within the object as positive samples, where l(P) and l(xt) denote the locations of small patch P and the object xtat time t. The distance α is a threshold to ensure all patches fall into the object region. For negative samples, we sample another set of patches P−={P|β<║l(P)−l(xt)║<γ} from the area outside the target. Here the distance β and γ are thresholds, and β is larger than α. These parameters are selected by imposing a range constraint on the central locations of patches such that these patches do not overlap with the object region and are not far away from the object. With several training frames, sufficient positive and negative samples are collected to build the structured visual dictionary at the outset of the tracking process.In this section, we describe the statistical properties learned from the sampled patch sets. In contrast to conventional BoF approaches that use only appearance or texture to represent objects, we incorporate geometric information into the visual vocabulary, and refer to it as the structured visual dictionary (SVD).Given two sets of patches P+ and P−, descriptors are extracted f(P+) and f(P−) where f is a feature function. Next, these features are grouped into clusters Kf(1)Kf=CfP+,CfP‐,whereCdenotes a clustering function that returns the center and Kf={Kf,m}m=1Mis a set of cluster centers that are used as appearance keywords of the dictionary, where M is the number of cluster centers. Note that positive and negative samples are clustered separately in order to better contrast the learned appearance keywords between positive and negative sets. We note that in [20] only positive samples are used to learn visual dictionary, which explains why the tracker is sensitive to visual drift.To better represent objects, the proposed SVD consists of two sets of appearance keywords obtained from the RGB descriptor and the local binary pattern (LBP) descriptor [35] which are extracted from randomly sampled patches instead of using interest point operators. Therefore, we describe an object from two different aspects which uncover as many representative features as possible. These collaborative sets of keywords derived from two descriptors often complement each other for more effective object representations, thereby facilitating the tracking process.Aside from appearance keywords, we incorporate geometric constraints of features in the proposed SVD. While some existing methods have utilized geometric information of features [24–26] in the context of object categorization and recognition, these methods are not efficient or effective for object tracking. Within the context of visual tracking, several methods [31,36,28,32,33] that use interest points for describing target objects have been proposed.Tracking is then carried out by matching individual interest points in consecutive frames. In other words, such approaches treat all the features independently and do not take the spatial relationships of neighboring features into account. However, the geometric information among neighboring features preserves important cues for feature matching and object tracking. Therefore, we utilize the relative positions of detected interest points in a 2D plane to incorporate geometric information for constructing a structured visual dictionary. Our motivation is that the spatial distribution of interest points within an object conforms with a stable pattern which can be statistically described by modeling the spatial relationships among these points. Note that the way of encoding spatial information does not depend on the correspondence of interest points in successive frames.We detect interest points from an image and extract their SIFT features [18]. As shown in Fig. 2, the relative positions of stable interest points extracted from an object in successive frames almost remain constant. The histograms of the five images from the target object in Fig. 2 are relatively similar. That is, the geometric structure of stable interest points are not affected much by smooth motion, scale change and rotation in successive frames. In contrast, interest points of the regions from the background cannot be modeled well by a specific distribution (See Fig. 2).Motivated by these observations, we propose an efficient method to exploit geometric structure as discriminative features for object tracking. Inspired by the shape context approach [37], we consider local statistics originating from the center c of the extracted object region to all the other interest points.Assume that we detect R interest points from a target object, we use Vrto represent a vector from c to the r-th point. The set of local statistics V={Vr}r=1Rdescribes the positions and distances of interest points with respect to c. To obtain a compact and robust representation, we quantize local statistics in V in the polar coordinate system as shown in Fig. 1. Suppose that we divide the plane of polar coordinates into Um×Uaregions where Umand Uarepresent different magnitudes and angles. For a specific region (um, ua), where umand uaare the indices of magnitude bin and angle bin respectively, we count the number of interest points within this grid, thereby constructing a matrix M(um, ua) which records the distribution of V (See Fig. 1). We then convert each matrix M(um, ua) to a histogram Hs(u), which preserves compact spatial information of interest points. Each bin u of the histogram represents a specific magnitude and angle region. In practice, we use 4 bins and 5 bins for quantization of magnitude and angle, respectively. The motivation to use histogram representation for spatial information is to facilitate update and fusion with appearance information in a simple and consistent way.We also use the same method to compute the spatial histogram of interest points from the background, as shown in Fig. 2. However, since interest points of sampled regions from the background may appear in any configurations, their spatial histograms do not contain representative information. The appearance keywords and the geometric correlations of interest points together are the integral components of the proposed SVD for object tracking.Once we have the SVD, an image region can be represented by two appearance histograms and a spatial histogram. Within each image region, a set of patches of random sizes P are randomly sampled. The appearance histogram is then created by assigning each patch in P to its nearest keyword in K and computing the occurrence frequency of corresponding keyword within this image region. The spatial histogram characterizes the spatial distribution of a set of interest points. The formulation of spatial histogram is mathematically consistent with appearance histograms, because both are histograms describing the statistical information (appearance or location) of a group of representative features.For a set of keywords Kf, we arrange positive and negative keywords separately, and then concatenate bins of indices from two kinds of keywords. Subsequently, the object is represented by an ordered appearance histogram Hf(u), where u indicates a bin of Hf(u). Assume that Hf(u) has U bins, so the first U/2 bins record the occurrence of positive keywords and the remaining U/2 bins represent that of negative ones. Moreover, we use the following equation rather than raw counts to compute Hf(u):(2)Hfu=∑n=1Nexp−dn/σ⋅δbPn−uwhere Hfdenotes the appearance histogram regarding feature f and Pndenotes a patch. In the equation above, N is the number of patches within a candidate, b(∙) indicates the index of bin, and δ(∙) is a delta response function that equals 1 when b(Pn) equals u. By multiplying a weighting term exp(−dn/σ), the patch more similar to the keyword contributes more to the corresponding bin. In the weighting term, dnis the minimal Euclidean distance between the feature of a patch f(Pn) and the closest keyword in Kf(σ is set to 0.2 in our experiments).At time t, we draw L candidate states Xt={xti}i=1Lwhere xti=(cxti, cyti, sti, θti) around the tracked object at time t−1 using random walks with factorized Gaussians in a way similar to [38]. cxti, cyti, stiand θtirepresent the center, scale and rotation angle of the state xti. Each state xticorresponds to a rectangular image region enclosed by a bounding box. Within each bounding box, we randomly sample small patches Pti={Pt,ni}n=1Nof random size and extract their RGB features f1(Pti) and LBP features f2(Pti). After computing the distances between features of patches f(Pti) and keywords Kf, the image patch region at state xtiis represented by two appearance histograms Hf1 and Hf2. We also detect interest points in the image patch at state xtiand compute the spatial histogram Hsof these points. Next, we compute the histogram similarity using the appearance and spatial histograms by:(3)Cw=maxexp−dχ2HwHw,∀w=f1,f2,s,where Hwis an appearance histogram or spatial histogram of the candidate sample, Hwis a set of corresponding histograms computed from the training images, anddχ2is a function that computes χ2-distance. Note that no similarity regarding individual patch is computed, and we focus more on the statistical properties of the group of randomly sampled patches. As summarized in Algorithm 1, we obtain similarities of each sampled candidate region with respect to the target object using appearance and spatial histograms.Algorithm 1SVD matching (w.r.t. one candidate region at xti)Given: Kf1 and Hf1 for RGB feature, Kf2 and Hf2 for LBP feature, spatial histograms Hs, SIFT features fsof detected interest points from a positive training sample (randomly selected) or a previous tracked object.Input: Set of patches P={Pn}n=1N1.for n=1 to N(a)Finddf1,n=minf1Pn,Kf1for RGB and index of the keyworduf1,nFinddf2,n=minf2Pn,Kf2for LBP and index of the keyworduf2,nend forConstruct appearance histograms Hf1 and Hf2 using (2)Detect interest points in candidate region at state xtiExtract SIFT features to perform interest point matching with fsConstruct spatial histogram of matched points HsDetermine appearance similarities Cf1 and Cf2 of two features and spatial similarity Csusing (3)Output: Similarities Cf1, Cf2 and Csbetween a candidate region and trained samples.We note that it is less effective to simply compute the spatial histogram from all detected points. If detected points are from the background but happen to have similar geometric relationships to those of the foreground, the computed similarity value Csof the candidate region may be high and leads to inaccurate results. Therefore, we add a constraint for feature matching. In a candidate region, only points that match those from the previous tracked object are used for constructing Hs, whereas other points lying in the background region are discarded. Despite its effectiveness, this strategy may discard some new interest points due to the appearance change of the target object. This issue is addressed by an update scheme discussed in Section 2.5.The appearance similarities Cf1 and Cf2, and spatial similarity Csfor a candidate region are integrated by:(4)Cj=expCf1/σf12+Cf2/σf22+Cs/σs2,whereσf1,σf2and σsare scaling terms that are set empirically to 0.02 and fixed in our experiments. The reliability of different features is computed by histogram similarities with the combination of appearance and spatial information. Thus, the proposed tracker is less likely to drift from the target as long as at least one feature is extracted to provide sufficient information of the target object, as shown in our experiments. A number of candidate regions with highest joint similarities are weighted and considered as the state of the target object at time t by(5)St=∑i=1L′Ct,jixti/∑i=1L′Ct,jiwhere L′ is used to determine how many candidates are used (e.g., L′ equals 5% of L candidate states in our experiments), and Ct,jiis the joint similarity for candidate xtiusing (4).Rather than a constant and off-line learned appearance model, all the histograms in the SVD are updated to account for appearance change of the target object.At each time t, we store the state Stof the tracked object and the corresponding accumulative similarity valueZt=∑i=1L′Ct,ji. Large Ztmeans that all selected candidates have high probabilities of being the target. Thus, during the period up to the next update at time t+T, we have a set of states for the tracked objects {St,Zt}tt+T, from which we choose the St∗ with highest Zt. From the image patch at St∗, we construct two appearance histograms based on the RBG and LBP features, and update the corresponding set of trained appearance histograms Hf1 and Hf2 by discarding the oldest histograms respectively. We choose the histogram that best characterizes the target object to account for the appearance change. The update period T is pre-determined to accommodate typical object motion so that this scheme is not quick to adapt to noise or too slow to respond to the change of the target objects. We show the change of appearance histograms that account for the foreground and background change of a test sequence in Fig. 3.The update process of a spatial histogram is slightly different from that of an appearance histogram. At each update, the matching process determines a number of interest points from a candidate which are used for computing a spatial histogram. Assume that p detected interest points are stored from the previously tracked object and q points from a candidate Stat time t, then r matched points (r≤min(p,q)) are determined. If r>ωp, where ω is a percentage (e.g., it is set to 80% in our experiments), it means that the tracking result does not deviate much from the target location. Thus, it is reliable to utilize the current tracking result to update the spatial histogram. Similar to the scheme used for the appearance histogram update, new histograms are added to the set and old ones are discarded. When spatial histograms are updated, the corresponding features of old interest points are replaced with those of the new ones for subsequent tracking process. The update process is illustrated in Fig. 4.In Fig. 5, we show some results of matched interest points for the spatial histogram update and tracked objects of the tom2 image sequence. The change of three similarity values is also presented in Fig. 7. This figure clearly demonstrates that tracking drifts result in few matched points, i.e., low spatial similarity values (see frame #289). Even when there are no matched interest points in some frames, our method is still able to compute the spatial histogram (all bins are 0) and similarity values used for the overall similarity in (4).Note that the appearance and spatial histograms are updated separately because they do not change simultaneously in most cases. If the appearance of the object changes but there are only few interest points, the spatial histograms do not contribute to the combined similarity much. Therefore, we update only the appearance histograms, but there is no need to update the spatial histograms. Experimental results validate the effectiveness of the update mechanism.

@&#CONCLUSIONS@&#
