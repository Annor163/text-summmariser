@&#MAIN-TITLE@&#
Proposal of a statistical test rule induction method by use of the decision table

@&#HIGHLIGHTS@&#
We propose a new rule induction method which drastically improves the method called LEM2 proposed by Jerzy Grzymala-Busse.The new rule induction method named STRIM statistically and directly inducts if-then rules without using the concept of approximation by the conventional method.The rules inducted by STRIM have statistical assurance of the confident coefficient of the p-value, and derive accuracy and coverage indexes used in the conventional method as by-products.An algorithm for STRIM described in C language style is developed into a piece of software, implemented in a PC and confirmed to be efficient and useful for the rule induction problem by a simulation experiment.

@&#KEYPHRASES@&#
Rough sets,Rule induction,Statistical test,Rule box,p-Value,

@&#ABSTRACT@&#
Rough sets theory is widely used as a method for estimating and/or inducing the knowledge structure of if-then rules from various decision tables. This paper presents the results of a retest of rough set rule induction ability by the use of simulation data sets. The conventional method has two main problems: firstly the diversification of the estimated rules, and secondly the strong dependence of the estimated rules on the data set sampling from the population. We here propose a new rule induction method based on the view that the rules existing in their population cause partiality of the distribution of the decision attribute values. This partiality can be utilized to detect the rules by use of a statistical test. The proposed new method is applied to the simulation data sets. The results show the method is valid and has clear advantages, as it overcomes the above problems inherent in the conventional method.

@&#INTRODUCTION@&#
Rough sets theory introduced by Pawlak [1] has been studied as an object of mathematical interest, and has provided many useful analytical methods in areas such as machine learning and data mining. One of the important tasks of rough sets is to compute the attribute reduct, which provides the construction of the decision table of interest by the minimum number of attributes, maintaining knowledge contained in it. Another task is to extract if-then rules from the decision table, by the use of approximations with indexes such as accuracy and coverage. This is useful for diagnosis systems for diseases, discrimination problems, and other aspects. Theoretical studies and practical algorithms for the above two tasks are available in the literature [2–7].This paper presents the results of a retest of the ability of the conventional basic rough set [4,6,7] by the use of simulation data sets, and notes that the conventional method has two main problems. The first problem is that the estimated rules are composed of a large number of subsets of the true rule set and/or indifferent rules. That is, they cannot estimate rule sets specified in advance. The second is the strong dependence of the estimated rules on the data set sampling from the population. The results from one sample will differ from those from another. Attempts have been made to solve these two problems in the conventional method, by proposing the variable precision rough set (VPRS) [5] and by studying the effects of sampling from statistical viewpoints [8–11]. However, these trials seem do not to be essential and /or direct studies to resolve these problems, and they only partially succeed in improvement of them.Consequently, we here propose a new method that directly estimates the if-then rules by the use of a statistical hypothesis test. Specifically, we regard the rule estimation problem by use of the decision table as the problem of identifying a black box containing rules. The inputs and outputs of the box are the condition part of a if–then rule and the decision part of the rule, respectively. We conduct a preliminary experiment in a white box which specifies if-then rules in advance, generates the input of the condition part randomly, and decides the output by use of the specified rules. From this preliminary experiment, we found that the decision table basically contains two types of data set. The first is the data set controlled by the rules in the rule box, and the second is the data set not to be applied to the rules, and the output is obtained by chance. Accordingly, we propose the following rule estimation strategy: (1) We set up the null hypothesis that a trying rule as a test does not exist in the black box. (2) We test the hypothesis using the data set of the decision table sampling the population, and decide whether the hypothesis is true or not. (3) We repeat procedures (1) and (2) changing the trying rule systematically and efficiently, and estimate all the rules in the black box. The validity and usefulness are confirmed by comparing the results from our proposed method with those by the conventional methods in a simulation experiment.The decision table shown in Table 1is first obtained. This is conventionally denoted with S = (U, A=C∪D, V, ρ). Here, U = {u(i)|i=1, …, N=|U|} is a sample set. A is an attribute set, C = {C(j)|j=1, …, |C|} is a condition attribute set, C(j) (j=1, …, |C|) is the member of C and a condition attribute, and D is a decision attribute. V is a set of attribute values and denoted with V = ⋃a∈AVaand is characterized by an information function ρ: U×A→V. In this example, if a = C(j)∈A (j=1, …, |C|=6) then Va= {1, 2, …, 6} and if a=D then Va= {1, 2}, and ρ(u(1), C(1)) = 1, ρ(u(2), C(1)) = 4 and so on.The decision table in Table 1 is obtained by generating the condition part of u(i) denoted by uC(i) with the uniform distribution with regard to Va=C(j) (j=1, …, |C|=6) and by deciding the decision part of u(i) denoted by uD(i) by use of the following specified rules R(1) and R(2), and decision assumptions (As1), (As2), and (As3):Specified rules:R(1):if C(1)=1 ∧ C(2)=1 ∨ C(3)=1 ∧ C(4)=1 then D=1.if C(1)=2 ∧ C(2)=2 ∨ C(3)=2 ∧ C(4)=2 then D=2.(As1):uC(i) can be applied to R(k) and uD(i) is uniquely determined as D=d(k).uC(i) cannot be applied to any R(k), and uD(i) can only be determined randomly.uC(i) can be applied to several R(k) (k=k1, k2, …) and their outputs of uC(i) conflict with each other. Accordingly, the output of uC(i) must be randomly determined from the conflicted outputs.Here, ∧ is conjunction and ∨ is disjunction. The case of (As1) is u(1) and u(4), (As2) is u(2), u(3) and u(N) and (As3) is u(N−1) in Table 1. The case of (As1) is a uniquely determined case, (As2) is what is known as an indifferent case, and (As3) is an inconsistent case.We generated three cases of the decision table using the model in Section 2 with N=10,000, applied two representative conventional methods to them, and retested their abilities of rule induction. The first method was the LEM2 algorithm [4], for which software could be downloaded [12], and the second was the FDMM algorithm [7] which was developed from the decision matrix method [2,6]. Table 2shows the results from the two methods using the lower approximation. Nactual is the net number of N deleting the same data. The row of the table denoted STRIM will be explained in Section 6. The experiment was conducted on a PC with a Celeron(R) CPU with 2.67GHz clock speed and 992MB of RAM memory. Reducing time in the table is the execution time of the rule induction for reference. Number of rule length is the number of the conjunction in the condition part of the estimated rules. For example, if the condition part is C(1)=1 then the length is 1 (hereafter this part is denoted with (100000) for convenience). If C(1)=1 ∧ C(2)=1 ((110000)) then the length is 2. The length of C(1)=1 ∧ C(2)=1 ∧ C(3)=1 ((111000)) is 3, the length of C(1)=1 ∧ C(2)=1 ∧ C(3)=1 C(6)=4 ((111004)) is 4, and so on. The number 48 at the rule length 3 of FDMM in Case 1 shows that FDMM inducted 48 rules of the rule length 3. Total is the sum of the numbers of all the estimated rules.Table 3incidentally shows one of the rules with the highest coverage index by every rule length (RL) inducted by LEM2 for three cases in Table 2. Here, (n1, n2) is the frequency of samples {uD=1(i)} and {uD=2(i)} respectively which satisfy the corresponding condition part ⋀j(C(jk)=VC(jk))of the inducted rules and is arranged them as (|{D=1}| = n1, |{D=2}| = n2), and accuracy and coverage indexes are defined asmax(n1,n2)n1+n2andmax(n1,n2)|{D=i}|and respectively. The column of kind of rule shows a category to which the corresponding inducted rule belongs. Sub denotes a sub-rule of true rules which are included in the true rules specified in advance, and Ind denotes an indifferent rule which is not included in the true rules, that is, not specified in the rule box in advance. For example, at the first row of Case 1, RL=3, the inducted rule is if (220300) then D=2, (n1, n2) was (0, 61), the accuracy is 61/61 = 1.0 since the lower approximation was used, the coverages is 61/4773 ≈ 0.013, and kind of rule is Sub since (220300) is included in R(2) specified in advance. From Tables 2 and 3, we can see that:(1)Regarding u(i) (i=1, …, Nactual) as a rule of D=1 or D=2, LEM2 and/or FDMM reduced the rule set, that is, the decision table to about its 45 [%] with more than the rule length 3. The rules with length 6 were not arranged at all.R(1) and R(2) specified in advance were rules with the length 2 and their total rules were 4. The rules induced by LEM2 and/or FDMM were confirmed to be included in either R(1) or R(2), or in different rules as shown in Table 3. Accordingly, LEM2 and/or FDMM estimated the sub-rules of the original rules and/or indifferent rules obtained by chance from the sample set, multiplying the number by several thousands. To say other words, the estimated results by both methods have scarce reliability since the coverage of those rules is very low though their accuracy is one.Both methods are thought to be highly dependent on the sample data set, since the estimated rule set of the three cases differed considerably.In this paper, we say (1) and (2) the diversification of the conventional method. Both of the conventional methods were though hardly to accomplish their purpose.Then, we consider the notion of the conventional rough sets theory and its rule induction method, specially through LEM2 algorithm as the representative of the conventional method. The conventional method inducts if-then rules R(k): if Rkthen D = Dk(∈VD) (k=1, 2, …), where Rk= ⋀jC(jk) =VC(jk), by use of the decision table S. Now, let U(Rk) = {u(i)|uC=Rk(i)} denote a set of u(i) applicable to Rkand U(Dk) = {u(i)|uD=Dk(i)} denote a set of u(i) with the decision part of Dk. The rule induction by the lower approximation finds the 2-tuple (Rk, Dk) satisfying the condition: U(Dk) ⊇ U(Rk). The algorithm is shown in Fig. 1. In the line number 0 (LN=0), U(Dk) with the lower approximation is set as B. In LN=5, t corresponds to C(jk) =VC(jk), T corresponds to Rkand [T] corresponds to U(Rk) = {u(i)|uC=Rk(i)}. All of Rk, that is, ⋁kRkcorresponds to τ in LN=15. In the processes to find the final rules, the redundant t and T are deleted in LN=14 and LN=17 respectively.From the above considerations, it has been shown that the conventional rule induction method is highly dependent on the sample set. The estimated rules will change with different sample data set, that is, different decision table as shown in Tables 2 and 3, and the diversification will occur by satisfying the restriction: U(Dk) ⊇ U(Rk) and U(Dk) = U(⋁kRk) (See LN=3 in Fig. 1).We propose a new rule estimation method to help overcome the above two problems described in Section 3, from the viewpoint that the decision table is a sample set obtained from the population Ω shown in Fig. 2. Here Ω is supposed to have a rule box as shown in Fig. 3. An input uC(i) of the rule box is obtained from Ω, and the output uD(i) is determined corresponding to the uC(i) by using some rules contained in the box, and the decision assumptions (As1)–(As3). Accordingly, the decision table like that in Table 1 is an arranged data set of the inputs and outputs, and the rule estimation problem is regarded as the problem identifying the rule box by use of the decision table.We used the data set of Case 1 in Table 2 to examine the way of estimating rules in the rule box. We first counted the frequency of the samples {uD=1(i)} and {uD=2(i)} respectively which have the condition part CP(k) = ⋀jC(jk) =VC(jk), and arranged them as (|{D=1}|=n1, |{D=2}|=n2). Table 4shows some examples of CP(k) and its frequency (n1, n2). For example, CP(1) in Table 4 means that if R=CP(1) (=(100000)) then it has (n1, n2) = (920, 653); CP(11) means that if R=CP(11) (=(110000)) then it has (n1, n2) = (256, 3); if R=CP(17) (=(110004)) then it has (n1, n2) =(61, 1), and so on. Table 4 shows that:(A1)The longer the rule length gets, the fewer the samples the rule can apply to.In this case, CP(k) containing the part of the rules existing in the rule box where R(1) and R(2) are specified causes a partiality of the distribution of (n1, n2). For example, see (n1, n2) of CP(1), CP(11) and CP(17), which result from (As1) and (As3).The partiality of the example in (A2) has a related tendency, since their rules are combined by the relationship; 100000⊇110000⊇110004.(n1, n2) = (256, 3) of 110000 and (61, 1) of 110004 have the inconsistent samples of three and one, respectively (cf. (As3)).A condition part CP(k) of an indifferent rule which does not have any part of the rules existing in the rule box does not cause severe partiality. For example, see (n1, n2) of CP(3), CP(27) and the like which result from (As2).To summarize (A2)–(A5), the partiality of (n1, n2) can be utilized to determine whether CP(k) contains the part of the rules in the rule box or not, regardless of the inconsistency of R(k).The above experimental knowledge leads to the following strategies for estimating rules in the rule box by use of the decision table:(1)Specify the following null hypothesis H0 and alternative hypothesis H1 (cf. (A6)): H0: a trying CP(k) is not the condition part of R(k) (k=1, 2, …). H1: a trying CP(k) is the condition part of R(k) (k=1, 2, …).Make a trying condition part of the rule length 1, and count its frequency (n1, …, nM), where M = |Va=D| = 2.Test H0 by use of (n1, …, nM). Specifically, select a proper statistic which depends on the problem under consideration. From the problem with two decision attribute values, the following z statistic can be adopted as one of the methods:z=(nmax+0.5−np)/np(1−p),heren=∑m=1Mnm, p=1/M (cf. (As2)), nmax = max(n1, …, nM). If pnm≥5 (m=1, …, M) then z obeys the standard normal distribution [13]. Then specify the statistical rule that if z≥3.0 the H0 is rejected and H1 is adopted; this specifies the level of significance less than 1 [%]. The value of z also is shown in Table 4. If ni = nmax = max(n1, …, nM), then the trying condition part is that of the rule for D=i.Reserve the trying rules r(k) consisting of CP(k) and D=i once when H0 is rejected (cf. (A6)).If procedures from (2) to (4) for all of CP(k) with the rule length 1 have been completed, repeat the same procedures changing CP(k) with the increased rule length until the condition pnm≥5 (m=1, …, M) does not hold (cf. (A1)).Classify the reserved rules into several rule sets by use of the order relation ⊇ (cf. (A3)). Denote the class with Pyramid(l) (l=1, 2, …), and then the following features hold in this class: ∀r(k)∈Pyramid(l) ⇒ ∃r(k1)∈Pyramid(l): r(k)⊆r(k1) or r(k)⊇r(k1). Accordingly, Pyramid(l) forms an upper semi-lattice.Find r(k) in Pyramid(l) which has the maximum of z in Pyramid(l) and denote it with rePyramid(l) (representation of Pyramid(l)).Let rePyramid(l′) with the maximum of z represent Pyramid(l) if the relationship ⊇ exists between rePyramid(l) (l=1, 2, …) and determine the final rule estimation result rePyramid(l) (l=1, 2, …).Figs. 4 and 5show the image of strategies (7) and (8) respectively. Fig. 5(a) shows an example of the same rePyramid, and Fig. 5(b) shows an example of rePyramids with relationship ⊇.Fig. 6shows an algorithm for the STIRM (statistical test rule induction method) described in C language style, based on the considerations in Section 4. Most of the lines have the comments corresponding to strategies from (1) to (8). Especially, strategies from (1) to (5) are executed in a function rule_check() respectively. This function explores trying CP(k) for tests satisfying the condition pnm≥5 (m=1, …, M) without overlapping. This part of the example is shown in Table 4. Strategies (7) and (8) correspond to the LN=14 and LN=17 of LEM2 algorithm in Fig. 1, which delete the redundant rules.We developed the algorithm shown in Fig. 6 into a piece of software using C language, implemented it in the PC used for the experiment in Section 3, and applied it to the three cases in Table 2. The results are shown in the rows of STRIM of Table 2. The reducing time is much faster than the two conventional methods, even though STRIM iterates the test many times while changing the condition part. The reason for the greater speed despite the number of iterations is that most of the STRIM computing time is consumed in counting the frequencies of the decision attribute values, while only accessing the main memories; STRIM hardly needs the set and/or logical operations that the two conventional methods require.With regard to the number of total and estimated rules classified with the its length, STRIM extracts the if–then rules in the number of one digit from about 9400 samples, and the extracted rules include the just true rules, as well as the extra rules of errors, as shown in Table 5. Generally, the results will depend on the sample data set given for the rule induction. However, STRIM is not affected by the sample so over-sensitively that it inducts the true rules and a small number of indifferent extra rules. In contrast, the two conventional methods could estimate only thousands of the sub-rules of the true rules and/or indifferent rules. Those rules cannot imagine the structure of the true rules specified in advance. The conventional methods cannot but be said to be over-sensitive for the sample data set. Table 5 also shows the values of (n1, …, nM) (M=2), p-value (z), the accuracy index (=max(n1,…,nM)/∑j=1Mnj) and the coverage index (=max(n1, …, nM)/|{D=i}|) as by-products corresponding to the estimated rules. Here, p-value means the probability of errors of rejecting H0 and adopting H1. It is also clear that the values of p-value, accuracy and coverage indexes of the errors are worse than those of the true rules. To compare Table 5 with Table 3, rules inducted by STRIM are statistically certified by p-vales though rules by the conventional method do not have. STRIM inducts rules with high indexes of accuracy and coverage, which coincides with the idea of VPRS (variable precision rough set method [5]). The validity and effectiveness of our proposed method is thus confirmed.We have shown our proposed idea and method to be superior to the conventional methods in the case of the simulation data set with |Va=D|=2 in Section 6.1. We furthermore confirmed the rule estimation ability of our method specifying the following rules with M=|Va=D|≥3 and generating the same data set of N=10,000 as shown in Section 2:Specified rules:R(m):if C(1)=m ∧ C(2)=m ∨ C(3)=m ∧ C(4)=m then D=m, (m=1, …, M) (M=3, …, 6).Table 6shows the results by STRIM corresponding to each M (=3, …, 6) with three cases in the same way as Table 2. The specified R(m) (m=1, …, M) have the total of 2M rules of the conjunction type with the length 2. The results by STRIM are also confirmed including the same rules specified in advance, and the extra rules of errors with the longer or equal rule length than the length 2. The results of M=6 and Case 1 as the representative of Table 6 are shown in Table 7. The differences between the specified and extra rules are clearly seen by the p-value and the accuracy and coverage indexes, in the same way as in Table 5. The validity and effectiveness of our proposed method has been also confirmed in more complex specifications than that of Section 6.1.

@&#CONCLUSIONS@&#
Rough Sets theory extracts if-then rules using the decision table obtained from its population. Accordingly, studies of the rule estimation problem by Rough Sets theory to date have investigated the effects of the sampling and the effects for the estimated rules on the accuracy and coverage indexes [8–11]. However, these studies are indirect ones to the effects. In this paper, we retested the rule estimation ability of the conventional method (LEM2 and FDMM) by a simulation experiment, and clarified that the lower approximation by the conventional method only estimated the large number of the sub-rules included in the true rules and/or indifferent rules not included in the true rules, and their estimated rules were highly dependent on the sampled data set. We then proposed a direct rule estimation method using the sample data and its algorithm based on the experimental knowledge obtained from a preliminary experiment, developed the algorithm into a piece of software, implemented this software in a PC, and conducted the simulation experiment. The results showed:(1)Our proposed method drastically improved the results by LEM2 and/or FDMM, estimating the true rules reproducibly, in spite of changing the decision table and adding a small number of extra rules.Our proposed algorithm statistically and directly extracted if–then rules without using the concept of approximation. Consequently, the rules extracted had the confident coefficient of the p-value, and also had accuracy and coverage indexes as by-products.To investigate superior statistics applicable to problems with more than three values of the decision attribute;To examine how many samples are needed for extracting rules with high precision, depending on the rule extraction problems;To study how many and which rules extracted in Tables 5 and 6 should be adopted as a family of rules, by use of a multiple comparison procedure;To study the relationship between STRIM and VPRS;To apply the method to real-life data, such as that held in the repository of the University of California at Irvine [14].