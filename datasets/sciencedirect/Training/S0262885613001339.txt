@&#MAIN-TITLE@&#
Integrating multiple character proposals for robust scene text extraction

@&#HIGHLIGHTS@&#
Proposed system separates text regions from images under unconstrained environment.Generalized clustering utilizes properties of scene text to detect text boundaries.Multiple image segmentations provide various interpretations on text regions.Two-step CRF approach models properties and relationship of text in graph structure.Character proposals are generated and integrated to find proper character regions.

@&#KEYPHRASES@&#
Scene text extraction,Two-stage CRF models,Multiple image segmentations,Component,Character proposal,

@&#ABSTRACT@&#
Graphical abstract

@&#INTRODUCTION@&#
Scene texts such as bottle labels, street signs, and license plates are text regions, which are present as integral parts of pictures (Fig. 1). The text is usually linked to the semantic context of the image, and it constitutes a relevant descriptor for content-based image indexing. Direct recognition of scene text from the images captured by mobile devices could facilitate the development of a variety of new applications, such as translation, navigation, and tour guide services.Scene text understanding refers to an attempt to recognize text in camera-captured images. It consists of two parts: scene text extraction and scene text recognition. The purpose of the extraction process is to separate text regions from the natural image. The purpose of the recognition process is to determine labels from the extracted text regions. Robust extraction of text from scene images is an essential step for successful scene text recognition. A very efficient text extraction method would enable the direct use of commercial OCR engines, which are normally optimized for binarized document images. However, errors due to a poor extraction method could be propagated in the recognition process. Therefore, we will focus on the extraction process in this paper.Extracting text from unconstrained images of natural scenes is difficult owing to the lack of any prior knowledge about the text regions, such as the color, font, size, orientation, or even the location of the text. In addition, scene images usually have uneven illumination, reflections on objects, and inter-reflection between objects owing to uncontrolled lighting conditions and the presence of shadows. These conditions make colors vary drastically, so the text regions may be fragmented, or the boundaries of the text region may be faint. It is also common for outdoor images to have complex layouts in which the content and background are mixed. Shapes in the background can be similar to characters, particularly for textured objects such as windows or buildings. Such complications make extracting text from scene images a persistent challenge.A two-stage conditional random field (TCRF) approach with multiple image segmentations is proposed to overcome these challenging problems and to extract proper character regions regardless of complex background and outdoor environments. One assumption in dealing with multiple segmentations is that there exists at least one correct segmentation for each character instance. Multiple image segmentations with different partition parameters provide alternative ways of grouping pixels to form homogeneous regions in the image, so having multiple image segmentations increase the possibility of some homogeneous regions matching character regions. Although some segmentations are inaccurate, others are accurate and would prove useful for the extraction task. Hence, having multiple segmentations can provide a more robust basis for character regions than any single image segmentation.The proposed TCRF approach finds coherent groups of correctly segmented character regions within a large pool of multiple candidate regions by utilizing the properties and hierarchical structure of the scene text. Commonly used structures of the scene text are the following: “the textline (composed of characters),” “the characters,” and “pixels”. An image contains one or more textlines; they are aligned horizontally, vertically or diagonally. These textlines have distinguishable texture patterns. Each textline does not vary in height too much, and it has sufficient width to contain more than a single character. Most characters (alphabet and numbers) are assumed to appear as single regions. Characters in the same textline exhibit common properties such as font and color. For instance, characters have no steep change in their thickness, and the insides of character regions have homogeneous colors, whereas the character boundaries show a distinction from the background. By satisfying these relationships of characters based on the scene text model, the proposed method can extract most plausible configurations of character regions among all possible combinations of segmented regions. As a result, the pixels in the character regions are instantiated as foreground, and the pixels in the other regions (non-text regions, i.e. noises) are indicated as background.The rest of this paper is organized as follows. Section 2 presents related works on the scene text extraction method. In Section 3, we briefly introduce the proposed extraction framework. The algorithm of partitioning an image into multiple segmentations and generating multiple character proposals is explained in Sections 4 and 5. We explain how to integrate character proposals into textlines in Section 6. The efficiency and performance of the suggested system are experimentally evaluated in Section 7. Finally, the paper is concluded in Section 8.

@&#CONCLUSIONS@&#
