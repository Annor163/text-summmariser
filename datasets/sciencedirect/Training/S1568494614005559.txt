@&#MAIN-TITLE@&#
A sinusoidal differential evolution algorithm for numerical optimisation

@&#HIGHLIGHTS@&#
The proposed SinDE uses sine-based formulas to define the DE parameter values.The proposed SinDE outperformed state-of the-art metaheuristics.28 benchmark functions have been used for validating the proposed approach.The proposed SinDE achieved very good results. It reduces the number of parameters.

@&#KEYPHRASES@&#
Differential evolution,Sinusoidal parameter adjustment,Exploration,Exploitation,Optimisation,

@&#ABSTRACT@&#
This paper presents a new variant of the Differential Evolution (DE) algorithm called Sinusoidal Differential Evolution (SinDE). The key idea of the proposed SinDE is the use of new sinusoidal formulas to automatically adjust the values of the DE main parameters: the scaling factor and the crossover rate. The objective of using the proposed sinusoidal formulas is the search for a good balance between the exploration of non visited regions of the search space and the exploitation of the already found good solutions. By applying it on the recently proposed CEC-2013 set of benchmark functions, the proposed approach is statistically compared with the classical DE, the linearly parameter adjusting DE and 10 other state-of-the-art metaheuristics. The obtained results have proven the superiority of the proposed SinDE, it outperformed other approaches especially for multimodal and composition functions.

@&#INTRODUCTION@&#
Differential Evolution (DE) is a powerful optimisation metaheuristic that was first proposed by Price and Storn in 1995 [1,2]. Since then, this population-based metaheuristic has attracted the attention of researchers in many fields. It was intensively used to solve academic and real life problems, especially in the fields of engineering and sciences. This evolutionary algorithm is considered as one of the most reliable and versatile optimisation techniques available today [3].Differential evolution uses rather a greedy selection and less stochastic approach to solve optimisation problems than other classical evolutionary algorithms. It differs from other evolutionary algorithms in the mutation and recombination phases. Unlike some metaheuristic techniques such as genetic algorithms and evolution strategies, where perturbation occurs in accordance with random quantities, DE uses weighted differences between solution vectors to perturb a given population [1,4].According to Price [5], differential evolution has the ability to find the true global optimum regardless of the initial parameter values, is fast and simple with regard to application and modification, requires few control parameters, has a parallel processing nature and offers fast convergence, capable of providing multiple solutions at a single run, effective on integer, discrete and mixed parameter optimisation and able to find the optimal solution for a non-linear constrained optimisation problem with penalty functions. It is clear that conventional DE is a bit overestimated by its inventors; new optimisation problems proved that DE sometimes fails at finding the global optimum, when failing at achieving a good balance between the exploitation of the already-found good solutions and the exploration of the non-visited regions of the search space. This weakness has led to the emergence of many variants of the basic algorithm.Basic variants of DE were first defined and used by its inventors, Price and Storn, to solve optimisation problems [2,5–7]. Later on, other variants have been proposed and thoroughly studied, by other researchers, to analyse their explorative and exploitive capabilities. For example, in [8], Lampinen proposed a DE algorithm for handling non-linear constraint functions. In [9], Gamperle et al. initiated the series of works studying the influence of parameters’ values on the performance of the DE algorithm; the authors gave some guidance about parameter values setting.Through comparing differential evolution with other evolutionary techniques, such as Particle Swarm Optimisation (PSO) [10], and with the emergence of many new variants of the DE algorithm, the problem of parameters setting and its impact on the overall performance of the algorithm appeared as a serious challenge for researchers interested in exploiting this metaheuristic (DE) for solving daily life problems. Hence, new variants working on the adjustment and adaptation of DE parameters, especially those based on hybridising DE with other evolutionary techniques, have emerged in recent years [11–16]. A rich review of some of these automatically parameter adjusting and adaptive variants of the DE algorithm can be found in the recent survey of Das and Suganthan about differential evolution [17].In its original version [1], the DE algorithm uses a special type of mutation in which a given individual is perturbed using the weighted difference between two other individuals chosen randomly from the population. The weight of this difference is known as mutation scaling factor or scaling parameter. In early generations of the algorithm, the individuals are relatively different from each others. So, small values of the scaling factor will be able to offer good diversity to the population. However, in later generations, the individuals would become very similar to each others, thanks to the crossover operator. So, small values of the scaling factor will be virtually incapable of offering diversity to the populations. Thus, the algorithm is likely to fall in local optima, especially in the case of high-dimension problems, where a big number of generations is generally needed to get good solutions. As a result, more freedom should be allowed to the algorithm to switch from exploration into exploitation, and vice versa, through an automatic adjustment of the values of parameters and so the change of search direction. In the same way, the crossover rate can dramatically influence the quality of solutions; a small value would lead to not getting profit from the mutant, while a big value will make big perturbations.In this perspective, we propose here a novel automatically parameter adjusting version of differential evolution: the Sinusoidal Differential Evolution (SinDE). This variant uses a sinusoidal framework to define the value of the mutation scaling factor and/or the crossover rate at each generation. Different patterns are proposed; some of them use the sinusoidal adjustment of only one parameter, the other parameter is fixed to the recommended value; other patterns adjust both parameters. This new scheme of calculating DE parameter values, as will be shown in the numerical results section, allows a good balance between exploration and exploitation, since the parameter value increases and decreases periodically, thanks to the periodicity of the Sine function. Moreover, the gradually-increasing (or decreasing) interval between maximum and minimum allowed values of the parameter being adjusted offers a good possibility to escape local optima.The proposed SinDE algorithm has been tested on the recently defined set of reference benchmark functions: the CEC-2013 testbed [18]. First, It has been compared using the Wilcoxon rank-sum test against the classical DE, the linearly parameter-adjusting DE (LADE), other four recent powerful variants of the DE algorithm (the SMADE, the DEcfbLS, the b6e6rl and the TLBSaDE), and the recently proposed CMA-ES-RIS algorithm. Next, the Holm–Bonferroni [19] statistical procedure has been used to further compare SinDE to these algorithms and five other state-of-the art algorithms. The obtained results show the superiority of the proposed SinDE in both statistics; it has largely outperformed the majority of these algorithms and has been slightly better than the others.The remainder of this paper is organised as follows. In Section 2, basic concepts of differential evolution and its standard variants are presented. The proposed sinusoidal DE, SinDE, and its different patterns are described in Section 3. Section 4 validates the performances of the proposed approach through comparing it against classical DE and other state-of-the-art metaheuristics. The obtained experimental results are discussed in the same section. Finally, conclusions and future directions of research are drawn up in Section 5.The Differential Evolution (DE) algorithm [1], follows the general procedure of an evolutionary algorithm: an initial population of individuals is created by random selection and evaluated; then the algorithm enters a loop of generating offspring, evaluating offspring, and selection to create the next generation [20], till a stop condition is met. Generating offspring consists of two operations: differential mutation and differential crossover.In basic DE, the mutation operation creates a new individualvicalled the mutant by adding the weighted difference between two individuals (vectors) chosen randomly from the population to a third one, also chosen randomly, as shown in Eq. (1) below [1]. In the equation, i, i1, i2, i3∈{1, ..., NP} are mutually different indices, NP is the size of population, and F is a real positive scaling factor of the differencedi=xi2−xi3.(1)vi=xi1+F.(xi2−xi3)The crossover operator implements a discrete recombination of the mutant,vi, and the parent vector, xi, to produce the offspring ui. This operator is formulated as shown in Eq. (2) below [21,22]; where: Uj(0, 1) is a random number in the interval [0,1],CR∈0,1is the crossover rate, and k∈{1, 2, ..., d} is a random parameter index, chosen once for each individual i to be sure that at least one parameter is always selected from the mutant. Popular values for CR are in the range [0.4, 1].(2)ui(j)=vi(j),ifUj(0,1)≤CRorj=k,xi(j),otherwiseThis type of crossover is called binomial crossover. Another type of crossover used in DE is the exponential crossover. Its principle is as follows. The starting position of crossover is chosen randomly from1,...,d, d is the problem dimension; then, L consecutive elements (counted in circular manner) are taken from the mutant vectorv[16]. In the present work, we are interested in binomial crossover.After applying differential crossover, the obtained individual, called trial vector, is compared to the parent individual (also called target vector). A greedy selection takes place at this point, i.e. the fittest of them will become the target vector. This selection operation, in the case of a minimisation problem, can be expressed as shown in Eq. (3); in the formula, xi,G+1 is the new target vector (at generation G+1), ui,G+1 is the trial vector obtained from the crossover operation, xi,Gis the target vector at generation G, and fit(*) is the fitness function. Of course, in the case of a maximisation problem, the opposite comparison is used, i.e. a ‘≥’ replaces the ‘≤’ in the equation.(3)xi,G+1=ui,G+1,ifeval(ui,G+1)≤eval(xi,G),xi,G,otherwise,As already stated, the differential evolution algorithm repeats these three operations of mutation, crossover and selection till a stop condition is met. The stop condition is generally chosen to be a predetermined number of generations. Another stop condition that can be used is achieving the global optimum, when its fitness is already known. The non-evolution of the best fitness among population individuals is an other widely-used stop condition. Generally, it is recommended to use a combination of two or three of these stop conditions.A number of variations to the basic DE algorithm have been developed in literature, mainly by its inventors [2,5–7]. DE strategies differ in the way the target vector is selected, the number of difference vectors used to perturb it, and haw to determine crossover points. In order to characterise these variations, a notation was adopted in differential evolution literature, namely DE/x/y/z notation [2,7,21]. In this notation, x refers to the method of selecting the target vector, y indicates the number of difference vectors used, and z indicates the type of crossover. The DE strategy already presented above is referred to as DE/rand/1/bin for binomial crossover.In addition to the DE/rand/1/bin variant, Storn and Price [2] propose four other DE variants which are expressed in Eqs. (4)–(7) below.(4)DE/best/1:_Vi,G=Xbest,G+F.(Xr1i,G−Xr2i,G)(5)DE/target-to-best/1:_Vi,G=Xi,G+F.(Xbest,G−Xi,G)+F.(Xr1i,G−Xr2i,G)(6)DE/best/2:_Vi,G=Xbest,G+F.(Xr1i,G−Xr2i,G)+F.(Xr3i,G−Xr4i,G)(7)DE/rand/2:_Vi,G=Xr1i,G+F.(Xr2i,G−Xr3i,G)+F.(Xr4i,G−Xr5i,G)One of the advantages of differential evolution is the limited number of parameters to be adjusted: the scaling parameter F, the crossover rate CR, and the population size NP. However, these control parameters and the learning strategies presented above (Eqs. (4)–(7)) are highly dependent on the optimisation problem to be solved [23]. Consequently, some new variants of DE have been proposed for the sake of finding a relationship between the problem dimension and the values of parameters [13]. Zaharie in [16] gives an interesting study of this relationship. Though, such a relationship cannot cover all types of problem requirements. For instance, one does not expect that the DE algorithm would perform similarly using the same parameter values for uni-modal and multi-modal problems for the same dimensions; the nature of the search space makes the difference. Hence, differential evolution, F and CR, need to be dynamic to cope with both the requirements of the problem at hand and the current state of population.With regard to the adopted parameter setting philosophy, DE variants can be categorised into three classes. In the first class, parameter values are set by the designer, the same values of parameters are used throughout the search process. In the second kind, the parameter values are continuously adapted or self-adapted according to feedback from the environment, which can be a fitness quality or other environmental constraints. Finally, in the automatically parameter-adjusting variants, some predefined formulas are used to automatically change the parameter value. Our proposed SinDE belongs to this third category. In the following paragraphs, we briefly present basic works that dealt with DE dynamic parameter setting.Chang and Xu [11] proposed one of the first, and most used, dynamic-setting variants of the DE algorithms. In their proposal, the authors use a linear adjustment of the parameters to make the crossover rate decrease from 1 to 0.7 and inversely the scaling factor increase from 0.3 to 0.5. This linear variant of the DE, called LADE in the following, is detailed in the next section.Another initiating work for dynamic setting of DE parameters is that of Abbass et al. [12]. The idea of this setting is that in each generation a new value of the scaling factor F is sampled from a Gaussian distribution, F∼N(0, 1). The same idea has been used in [24] with shifting the mean of the distribution to 0.5 and its standard deviation to 0.3 (i.e. F∼N(0.5, 0.3)). Later on, Abbass [25] applied the same formulation on the crossover parameter, i.e. CR∼N(0, 1). It is worth to mention that even if Abbass refers to these DE variants as being adaptive [12,25], we clearly see that they belong to the third category of parameter-adapting DE approaches: the automatically parameter-adjusting variants [21], since no information about the the already found solutions neither about current population individuals is exploited.As far as self-adaptation of DE parameters is concerned, Ali and Torn [26] proposed the use of information about the fitness of individuals to determine the value of the scaling parameter F at each generation. They have been motivated by the fact that the amount of difference between the best and worst individuals’ fitness is a key to know about population diversity, and so to compute the value of the F parameter, in order to favour either exploitation or exploration.In [27], Liu and Lampinen exploited the power of Fuzzy logics, on the basis of information about the population, to adapt the values of DE parameters. They proved that their variant of DE, called FADE, converges much faster than the classical DE. Brest et al. [28] went farther and devised new DE variants where the parameters are integrated to the individual structure. The basic idea was that good individuals are those found using good parameter values through generations, so it would be a good idea to define for each individual i at each generation t its specific parameters Ftiand CRti, so that the parameters also evolve with the other variables of the individual. The authors proved that their variant, called jDE, is better than both classical DE and the FADE algorithm. In [29], Price et al. draw the values of DE parameters from a Gaussian distribution. However, unlike the works in [12,24,25], the mean and standard deviation of the used distribution are not constant; they are themselves self-adapted through generations.The just-cited approaches are fundamental works on which other parameter-adjusting or adaptive DE variants have been based. A detailed review of other works related to dynamic parameter setting of the DE algorithm is given by Das and Suganthan in [17].As mentioned above, one basic scheme of dynamic setting of parameters in the DE algorithm is the linear adjustment. Eqs. (8) and (9) present the way these parameters are adjusted. In the equations, Fmin, Fmax, CRminand CRmaxare the lower and upper bounds of the scaling factor F and crossover rate CR, respectively. The inverse scheme, in which the scaling parameter F decreases and the crossover rate CR increases is also common [11].(8)Fit=Fmin+Fmax−Fminitmax*it(9)CRit=CRmax−CRmax−CRminitmax*itIn this linear adjustment of parameter values, only one direction of value change is permitted, either increasing or decreasing. This scheme also suffers from the relatively big number of parameters: Fmin, Fmax, CRminand CRmax; which implies additional effort from the user for their setting.In order to overcome these limits, the present work proposes the use of a sinusoidal-based pattern to calculate the parameter values. This allows changing the direction of adjustment: from increasing to decreasing and vice versa. In the following, we detail this new sinusoidal adjustment.The key idea of the proposed sinusoidal differential evolution is to use a new formula which does not only permit the adjustment of parameter values, but also permits to this adjustment to change its direction; from increasing to decreasing and vice versa. Such a possibility is well offered by the Sine, or the Cosine, function, see Fig. 1.What is remarkable in the case of sinusoidal functions is that the value of a given parameter increases and decreases periodically. This is exactly what is needed for permitting a certain flexibility in changing the direction of change for a given parameter.In the context of the present work, we use a sine-based formulation of parameters with different configurations. These configurations are presented in the sets of Eqs. (10)–(15). In the formulas, freq represents the frequency of the sinusoidal function, itmaxis the maximum number of generations and it is the current generation. According to the notation system used to identify DE variants, seen in Section 2.1, our new SinDE approach can be called SinDE/rand/1/bin.(10)Configuration 1:Fit=12*sin2π*freq*it*ititMax+1CRit=0.9(11)Configuration 2:Fit=12*sin2π*freq*it*ititMax+1CRit=12*sin2π*freq*it+π*ititMax+1(12)Configuration 3:Fit=12*sin2π*freq*it*itMax−ititMax+1CRit=12*sin2π*freq*it+π*itMax−ititMax+1(13)Configuration 4:Fit=12*sin2π*freq.*it*itMax−ititMax+1CRit=0.9(14)Configuration 5:Fit=0.5CRit=12*sin2π*freq.*it+*ititMax+1(15)Configuration 6:Fit=0.5CRit=12*sin2π*freq.*it+*itMax−ititMax+1As seen from these equations, the parameters F and CR are automatically adjusted in a way which allows them to change the direction of change with limited flexibility at first generations of the algorithm and more flexibility at final generations; and vice versa. The different configurations of parameter adjustment and setting are summarised in Fig. 2below.In order to find the best combination of formulas, all these combinations are compared according to their performances on the chosen set of benchmark functions, as will be seen in experimental design section. For the case of the sub-variants adjusting only one parameter, the value of the other parameter is set to the value recommended in literature [22].

@&#CONCLUSIONS@&#
