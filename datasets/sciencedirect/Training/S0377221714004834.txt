@&#MAIN-TITLE@&#
Likelihood estimation of consumer preferences in choice-based conjoint analysis

@&#HIGHLIGHTS@&#
An estimation method is proposed for value functions in choice based conjoint analysis.The method employs a likelihood function based on three sources of uncertainty.For individual choices we adopt multinomial logit with heterogeneous scale parameters.Interdependence of preferences is modeled using a multivariate normal distribution.The method exceeds average prediction performance of HB in 12 field data sets.

@&#KEYPHRASES@&#
Conjoint analysis,Utility function,Preference estimation,Maximum likelihood,Marketing research,

@&#ABSTRACT@&#
In marketing research the measurement of individual preferences and assessment of utility functions have long traditions. Conjoint analysis, and particularly choice-based conjoint analysis (CBC), is frequently employed for such measurement. The world today appears increasingly customer or user oriented wherefore research intensity in conjoint analysis is rapidly increasing in various fields, OR/MS being no exception. Although several optimization based approaches have been suggested since the introduction of the Hierarchical Bayes (HB) method for estimating CBC utility functions, recent comparisons indicate that challenging HB is hard. Based on likelihood maximization we propose a method called LM and compare its performance with HB using twelve field data sets. Performance comparisons are based on holdout validation, i.e. predictive performance. Average performance of LM indicates an improvement over HB and the difference is statistically significant. We also use simulation based data sets to compare the performance for parameter recovery. In terms of both predictive performance and RMSE a smaller number of questions in CBC appears to favor LM over HB.

@&#INTRODUCTION@&#
In marketing research the measurement of preferences and assessment of utility functions have long traditions. Often conjoint analysis (CA) is employed as the utility measurement instrument and the estimation takes place on segment or individual level. The world today seems increasingly customer or user oriented. In October 2013 ISI Web of Knowledge found 2758 hits for “conjoint analysis” (within title, abstract or keywords) with a significant share devoted to Operations Research/Management Science. References from the past five years indicate that research intensity in conjoint analysis is rapidly increasing in various fields, management science being no exception.This paper concerns choice based conjoint analysis (CBC) which appears the most popular type of CA today. For revealing individual preferences, estimation approaches and software for CBC analysis have developed significantly during past decades. The Hierarchical Bayes HB method for estimating individual utility functions (e.g., Allenby & Ginter, 1995) remains most popular, it is found to perform well and there is commercial easy-to-use software.1If segment-wise utility functions are adequate then latent class approaches also have turned out promising (DeSarbo, Ramaswamy, & Cohen, 1995).1Estimation results are frequently used to predict, for instance, market shares; see e.g., Natter and Feurstein (2002). The fine qualities of CBC analysis combined with HB estimation are acknowledged, for instance, by Karniouchina, Moore, van der Rhee, and Verma (2008) who compare CBC analysis with the ratings-based conjoint analyses, another commonly used method.The performance of alternative estimation approaches for CBC analysis (without adaptive question design) is compared in Halme and Kallio (2011) with HB as a benchmark. The study indicates that challenging HB is hard. However, based on likelihood maximization we propose in this article a new and highly promising method called LM (for Likelihood Maximization).The log-likelihood function in LM is based on three sources of uncertainty as follows. First, considering the choices of a respondent in individual questions of CBC we adopt the likelihood for each choice from the multinomial logit model (McFadden, 1974). Second, in terms of valuation errors the respondents are heterogeneous and we assume that the individual standard deviations of such errors are independent random draws from an inverse-gamma distribution. Third, as in HB, considering the interdependence of preferences among respondents we assume that parameter vectors defining individual utility functions are random draws from a multivariate normal distribution (e.g. Allenby & Ginter, 1995).Using twelve sets of field data LM achieves a statistically significant average improvement over HB in terms of predictive power. In these tests, some of the respondent’s questions are left for holdout. Having estimated the individual utility functions we attempt to validate the choices in the holdout question, i.e. test if the utility functions produced confirm the choices made by respondents. Predictive power is then measured by the share of confirmed choices among all holdout questions.Furthermore, we employ simulated data sets to compare utility function parameter recovery as well. With a small number of questions per respondent, LM outperforms HB in terms of RMSE (the root mean square error).Halme and Kallio (2011) also propose for CBC estimation an optimization based method CP with some similarities with LM: (i) both assume that individual part-worth vectors may be interpreted as random draws from a multivariate normal distribution; and (ii) in terms of valuation errors the respondents are heterogeneous. However, CP and LM differ in the methods of determining individual error levels (cross validation vs. likelihood maximization) as well as in choice models (projective penalty vs. multinomial logit). Numerical tests in Halme and Kallio (2011) indicate that average performance of CP is neither superior nor inferior to HB.Next, Section 2 introduces the CBC model and the likelihood method LM, Section 3 presents performance comparisons of LM and HB, and Section 4 concludes.We begin this section by introducing the choice model employed in preference estimation. Then the log-likelihood problem for estimation of part-worth vectors is defined. Finally, the approach is operationalized in a computer implementation.CBC analysis is a multi-attribute method where the value of a product or service stems from a given set of attributes which may only attain a small number of possible levels. Product concepts or profiles are defined by levels for each attribute. The utility function for a concept is assumed additive separable in attributes.In CBC analysis each respondent is presented a questionnaire including a set of Q questions (tasks). In each question, a set of P concepts (product/service profile specifications) is presented for evaluation and the respondent indicates the best one. Typically Q is in the range from 5 to 20 and P from 2 to 6. The questions often are different for each respondent or group of respondents, and they are defined employing, for example, fractional factorial, random experimental, or orthogonal question design (see Chrzan & Orme, 2000; Nair, 2013). In this article, we exclude adaptive question designs assuming that each task in the questionnaire is independent of responses to preceding tasks.We adopt additional notation from Halme and Kallio (2011) as follows:•i=respondent,i=1,2,…,Nj=question,j=1,2,…,Qk=profile alternative in a question,k=1,2,…,Pl=profile index to attribute levels,l=1,2,…,L(see example below)xijk=profile of alternative k in question j (row vector inRL) for person ixij1=preferred profile to person i in question jΔijk=xij1-xijk=preferred direction for alli,jand k.For notational convenience and without loss of generality, given data concerning the preferred choices, the alternatives of each question are reordered so thatk=1refers to the preferred one.For example, a profilex∈RLwith three attributes andL=5+5+4=14is depicted as follows:x=(01000︸00100︸0100︸)attribute1attribute2attribute3In this example, there are five possible levels of attribute 1, and the second level is chosen for this profile.For each respondent i and profile x, assume a utility function that is additive in attributes. Let part-worth vectorβi∈RLbe a column vector of weights such that the respondent’s utility function of profile x isvi(x)=xβi. Given preferred directionsΔijkof question j, the value margin of the preferred profilexij1with respect to a non-preferred profilexijkisvi(xij1)-vi(xijk)=Δijkβi. For person i, let∊ijkbe a random profile valuation error. Then profilexij1is conceived the most preferred profile by person i in question j, ifvi(xij1)+∊ij1⩾vi(xijk)+∊ijkforallk>1or equivalently, ifΔijkβi⩾∊ijk-∊ij1forallk>1. In the multinomial logit model (McFadden, 1974) we assume that the profile valuation errors∊ijkare independent and Gumbel distributed with location parameter zero, scale parameterγi. Then the standard deviation of∊ijkis given by(1)σi=π6γiand the probability of person i choosing profilexij1is(2)pij1=exp(γixij1βi)/∑kexp(γixijkβi)=1/∑kexp(-γiΔijkβi).In this section we formulate the log-likelihood function of LM to be used for estimation of the part-worth vectors. We employ a three-level hierarchy: (i) as in HB, we assume that the vectorsβiare independent random realizations from a multivariate normal distribution; (ii) the standard deviations of the profile valuation errors are independent draws from an inverse-gamma distribution; and (iii) the likelihood of choices made by respondents follows from (2). Given suitable independence assumptions below, the total log-likelihood is the sum of the log-likelihoods of the realizations of vectorsβi, the standard deviations of profile valuation errors, and individual choices. Next, we discuss the log-likelihood functions for each three levels.(i)Given that individual data tends to be scarce in the estimation of relatively many part-worth components inβi, we aim to borrow data from other respondents similarly as in HB. We take into account that the part-worth vectors among the respondents are correlated. For example, people generally prefer a low price to high and a high quality to low. In order to account for the interdependence of the respondents’ part-worths, we proceed as follows. Letβ̃be an L dimensional random vector with multivariate normal distributionN(α,V)whereα∈RLis the expected value ofβ̃andV∈RL×Lis the covariance matrix with∣V∣=1.2Normalizations∣V∣=κ≠1could be used equally well. This would just lead to rescaling the part-worth vectors as well as the standard deviations of the profile valuation errors.2We interpret part-worth vectorsβi, as independent realizations of the random vectorβ̃. The log-likelihoodci(βi)for a realizationβi(omitting a constant term) is(3)ci(α,βi,V)=-12(βi-α)TV-1(βi-α)-12log∣V∣.Recall that profile valuation errors∊ijkare Gumbel distributed with location parameter zero and scale parameterγi, for respondent i. We assume that the parametersγiare independent realizations of a random variableγ̃with a probability density functionγk-1exp(-γ/θ)/θkΓ(k)of a gamma distribution where k andθare positive parameters. Hence, omitting a constant term which depends on k andθ, the log-likelihood function of realizationγiis(4)gi(γi)=-γi/θ+(k-1)log(γi).Because the standard deviationσiof the profile valuation error in (1) is inversely proportional toγi,σirepresents a random draw from an inverse gamma distribution of a random variableσ. In a representative example of the test runs reported in Section 3 below, Fig. 1depicts such inverse gamma distribution corresponding tok=1andθ=0.25in (4). To illustrate the relative importance of the error term, in the horizontal axisσis scaled by a constant v, which represents an average absolute value of profile utility function. In this example, the average of the estimated standard deviationsσiis0.9×vrevealing the significance of the profile valuation errors.Employing the multinomial logit model (2) the log-likelihood for the choices of respondent i isUsing definitions (3)–(5) and given parameters k andθin (4), the likelihood problem is to find vectorsβi∈RL,α∈RL, a matrixV∈RL×Land scale parametersγi∈Rto(6)maxα,{βi}max{γi}∑i[hi(βi,γi)+gi(γi)]+maxV∑ici(α,βi,V)(7)s.t.log∣V∣=0.Note that the outer maximization overαand part-worth vectorsβiis a convex optimization problem if parametersγiand V are fixed, and optimality conditions imply(8)α=1N∑iβi.The inner problem is separable: maximizations over scale parametersγiand over the covariance matrix V. Given fixed vectorsαandβi, the former is a convex optimization problem and the latter is a non-convex problem with a globally optimal solution(9)V=1λH,where(10)H=∑i(βi-α)(βi-α)Tandλis the geometric mean of the eigenvalues of H; for a proof, see Appendix A.Our interest in this article is in the predictive performance of the part-worth vector estimatesβi. Next, as a measure for predictive power of the method we define the holdout hit-rate. We also introduce a cross-validation procedure for finding a suitable value for the parameterθof the gamma distribution in (4). Finally, we discuss a single iteration towards the solution of (6), (7) and specify the computer implementation of LM.Predictive performance of the part-worth vector estimatesβiis measured in terms of the holdout hit-rate. For this purpose, assume that we have one or more holdout questions in addition to the Q questions used for estimating the part-worth vectorsβi. Given vectorsβi, for all i, we define the confirmed holdout questions. Then the holdout hit-rate is the proportion of confirmed choices of holdout questions among all respondents.Cross validation aims to finding a suitable level for the parametersθ. We try several candidate values forθin (4). The performance of a candidate is measured following the leave-one-out procedure: For each questionj,j=1,2,…,Q, estimateβiexcluding question j, and check whether the preferred choicexij1is confirmed byβi. The cross-validation hit-rate is the share of confirmed choices among all respondents and all leave-one-out cases. We choose candidateθwhich yields the best cross-validation hit-rate.Given parameters k andθin (4), consider an iteration of problem (6), (7)3In the case there is only one attribute (brand) and ‘none’ alternative is present we proceed as follows. While iterating for the part-worth vectorsβi, we append sign constraints for componentsβil: for any task j of respondent i, if alternative l (a brand or ‘none’) is chosen thenβil⩾0, and if ‘none’ is chosen thenβil⩽0for all levels l in task j.3starting with some values forβi,α,γi,V. The iterations proceed in two alternating steps:Step1: With scale parametersγiand covariance matrix V fixed to current levels, solve the convex problem (6), (7) for optimalα,βi, for all i. For the initial iteration, we omit the covariance term (3), and for all i, setγi=1and∣βi∣⩽10to ensure existence of an optimum.Step2: With vectorsβiandαfixed to current levels, solve the convex problem (6), (7) for optimal parametersγi. Thereafter compute H in (10), find Cholesky factorizationH=CCT, determine the geometric meanλ=∣C∣2/Land update V using (9).Our primary interest is the predictive performance of the part-worth vector estimates. Therefore, we do not necessarily aim to find a global maximum for the non-convex log-likelihood problem (6), (7). After updating parametersγiand V inStep2, local optimality conditions for (6), (7) hold inStep1if the gradients of the objective function (after substitutingαusing (8)) with respect to all part-worth vectorsβiare zero. However, we only iterate as long as the sum of squared Euclidean norms of such gradients decreases or an iterations limit is met.The computational procedure involves two phases: cross-validation providing the bestθand final iterations for the part-worth vectorsβito be used for holdout hit-rate evaluation. The phases are implemented using AMPL (Fourer, Gay, & Kernighan, 2003) with solver option MOSEK for the convex optimization problems. For the parameter k of the gamma distribution used in (4), a suitable value is 1, and the resulting inverse gamma distribution of the standard deviation of profile valuation errors is illustrated in Fig. 1 above.Cross-validation. Try ten candidate values forθwithθQin the range[1,2,…,6]and determine cross-validation hit-rate for each candidateθ. Fixθto the value yielding the best cross-validation hit-rate.Final iterations. Withθfrom the cross-validation step, carry out iterations until the termination criterion holds. Thereafter, determine the holdout hit-rate.We illustrate the performance of LM by comparisons with HB using Sawtooth SoftwareCBC/HB4.2 (The HB/CBC System for Hierarchical Bayes estimation, 2009). Numerical comparisons are given in Section 4. However, we discuss first similarities and differences of the two approaches as well as the computational ideas used by HB.HB and LM share two features: first, the part-worth vectors are assumed to be random draws from a multivariate normal distribution with a covariance matrix accounting for the interdependence of preferences and the procedures pushing the parameter estimates towards the population mean; second, both methods employ the multinomial logit choice model. As for the differences HB produces a sample of estimates of theβivectors whose average is employed as final estimate whereas LM produces a point estimatesβi. The HB uses priors in the calibration of estimates and LM employs cross-validation for one parameter. The main difference is that HB employs the Bayes principle and uses MCMC (Markov Chain Monte Carlo) for computations while LM seeks to search for maximum likelihood estimates of the parameters. Furthermore, unlike HB, LM employs an inverse gamma distribution for the standard deviation of respondents’ profile valuation error.To understand HB, we now review its basic ideas. Consider a Markov chain in a continuous state space4The state space S in HB can be interpreted as an Euclidean space of suitable dimension. However, for convenience we define points in S by a set of vectors and a square matrix.4S where single step transition probability density function (pdf, transition kernel) from state z toz′is defined by an ordinary continuous pdf except possibly a spike atz′=z; i.e.,(11)P(z,z′)=p(z,z′)+r(z)δ(z′-z),wherer(z)is the probability forz′=z,δ(z′)is the Dirac delta function andp(z,z′)withp(z,z)=0is the transition pdf forz′≠z. Thenπ∗is a stationary pdf for the Markov chain if(12)π∗(z′)=∫SP(z,z′)π∗(z)dz=∫Sp(z,z′)π∗(z)dz+r(z′)π∗(z′).In MCMC simulationπ∗is given (at least up to a constant multiplier which cancels out in (12)), and we determineP(z,z′)to satisfy (12). While suchP(z,z′)need not be unique, a sufficient condition for (11) to satisfy (12) is the reversibility condition(13)π∗(z)p(z,z′)=π∗(z′)p(z′,z)∀(z,z′)∈S×S.For a simple proof, see Chib and Greenberg (1995). Using such kernelP(z,z′)MCMC simulates a single trajectory{zν}ν=1nin n steps starting with any given initial pointz0. In stepν⩾1,zν-1is given and we drawzνrandomly from pdfP(zν-1,zν)using Monte Carlo simulation. Then under mild regularity condition (of irreducibility) for any functionf:S→R+,(1/n)∑ν=0nf(zν)converges to∫Sf(z)π¯(z)dzasn→∞whereπ¯is the pdf of the stationary distribution; i.e.,π¯isπ∗scaled. (see e.g., Asmussen & Glynn, 2011). For example, for any integer k and a smallΔ>0, letfk(z)=I{kΔ⩽z⩽(k+1)Δ}be an indicator function. Then(1/n)∑ν=0nfk(zν)converges toπ¯(kΔ)Δasn→∞andΔapproaches zero. Therefore the sequence{zν}ν=1nforms a random sample of pdfπ¯as n increases without limit. Thus, using the sequence for large n we obtain, for instance, an approximation of the expected value of z with respect toπ∗– a result which is used by HB.In HB, the collection{βi}of part worth vectors of respondentsi=1,…,Nis a random sample from a conditional multivariate normal distributionN(α,V)givenαand V. In HB bothαand V are random. The prior distribution forαisN(0,A)withA-1=0and the prior distribution for V is an L dimensional inverse Wishart distributionW-1(LI,L)with L degrees of freedom andI∈RL×Lan identity matrix. Givenαand V, the conditional prior distribution ofβi,i=1,…,N, is multivariate normalN(α,V).Returning to MCMC, denotez=(α,V,{βi})and letR={Ri}denote the responsesRiof person i in all CBC questions used for estimation. Using Bayes theorem the posterior pdf for z given its prior pdff0(z)and responses R is(14)P{z∣R}=f0(z)P{R∣z}P{R}.HereP{R∣z}=P{R∣{βi}}=Πi=1NP{Ri∣βi}=Πi=1NΠj=1QPijwherePijis the probability given by (2) of the choice by respondent i in question j givenβi. While the denominator in (14) is difficult to evaluate, HB employs MCMC to obtain a random sample of z from the posterior distribution. For this purpose we defineπ∗(z)proportional to posterior pdfP{z∣R}as(15)π∗(z)=f0(z)P{R∣z}=fα(α)fV(V)Πi=1N[fβ(βi)Πj=1QPij],wherefα(α)is the pdf forαof the prior multivariate normalN(0,A),fV(V)is the pdf for V of the prior inverse WishartW-1(LI,L),fβ(βi)is the pdf forβiof the prior multivariate normalN(α,V)givenαand V, andPijis the choice probability in question j by respondent i in (2) givenβi.Next, givenπ∗in (15) we show how the transition kernelP(zν-1,zν)(for a Markov chain starting stepνwithzν-1and moving tozν) is chosen to meet the stationarity condition (12). Using “block-at-a-time” sampling,P(zν-1,zν)can be defined as a product of conditional transition kernels (for justification, see Chib & Greenberg, 1995) by(16)P(zν-1,zν)=Pα(αν-1,αν∣Vν-1,{βiν-1})×PV(Vν-1,Vν∣αν,{βiν-1})×Πi=1NPβi(βiν-1,βiν∣αν,Vν),wherePα,PVandPβi, fori=1,…,N, satisfy the stationarity condition (corresponding to (12)) with respect to conditional posterior probability distributionsπ∗(α∣Vν-1,{βiν-1}),π∗(V∣αν,{βiν-1}), andπ∗(βi∣αν,Vν), respectively.Let us now define each of the componetsPα,PVandPβi. First, it is simple to show thatπ∗(α∣Vν-1,{βiν-1})is the pdf ofN(β¯ν-1,Vν-1/N), whereβ¯ν-1=(1/N)∑iβiν-1. The stationarity condition is trivially met if we define(17)Pα(αν-1,αν∣Vν-1,{βiν-1})=π∗(αν∣Vν-1,{βiν-1}).Second, it is well known and straightforward but lengthy to show thatπ∗(V∣αν,{βiν-1})is the pdf of an L dimensional inverse Wishart distributionW-1(LI+H,L+N)withL+Ndegrees of freedom and H given by (10) withα=ανandβi=βiν-1fori=1,…,N. Again, the stationarity condition trivially holds if we define(18)PV(Vν-1,Vν∣αν,{βiν-1})=π∗(Vν∣αν,{βiν-1}).Finally, forPβiwe employ Metropolis–Hastings kernel (Hastings, 1970; Metropolis, Rosenbluth, Rosenbluth, Teller, & Teller, 1953; see also Chib & Greenberg, 1995). To begin, note thatπ∗(βi∣αν,Vν)∝πˆ(βi)≡exp-12(βi-αν)T(Vν)-1(βi-αν)×Πj=1QPij.For stepν, define candidateβi′=βiν-1+cdwhere c is a positive scale constant and d is a random draw fromN(0,Vν)whose pdf is denoted byfd(d). If the candidate is accepted thenβiν=βi′. Define an acceptance probability of candidateβi′by(19)ψ(βiν-1,βiν)=min[1,πˆ(βi′)/πˆ(βiν-1)].Then the Metropolis–Hastings transition kernel is(20)Pβi(βiν-1,βiν∣αν,Vν)=fd(d)ψ(βiν-1,βiν)+r(βiν-1)δ(βiν-βiν-1),wherer(βiν-1)is the probability forβiν=βiν-1; i.e., for candidate being rejected. It is simple to check that the first component of the sum in (20) referring to the transition toβiν≠βiν-1satisfies the reversibility condition.5Considering acceptance withβiν=βic; i.e.,βiν=βiν-1+cdandβiν-1=βiν-cd, suppose first thatπˆ(βiν)<πˆ(βiν-1). Thenπˆ(βν-1)×fd(d)ψ(βiν-1,βiν)=fd(d)πˆ(βiν)=fd(d)πˆ(βiν)min[1,πˆ(βiν-1)/πˆ(βiν)][asπˆ(βiν-1)/πˆ(βiν)>1]=πˆ(βiν)×fd(-d)ψ(βiν,βiν-1)[asfdis symmetric].This and a similar argument in caseπˆ(βiν)⩾πˆ(βiν-1)confirms reversibility.5Hence in view of (11) and (13), the transition kernel in (20) satisfies the stationarity condition. In stepν=1HB starts with scale constantc=0.1and adjusts its value dynamically so that the average acceptance frequency is about 0.3.To conclude, starting MCMC simulations withz0=(α0,V0,{βi0})=(0,I,{0}), in a single stepνof HB we drawανfromPα, thenVνfromPV(both referring to Gibbs sampler), and finally for eachi=1,…,Nseparately we drawβiνfromPβi(Metropolis–Hastings sampler). A large number (e.g., thousands) of steps are carried out until the initial starting point has no longer an impact (convergence occurs). From this point onwards, the draws ofαν,Vνandβiνrepresent sampling from posterior distribution (14) based on which an estimate of expected value ofβiis obtained for all i.In this section we use twelve test problems with field data for performance tests. We employ the holdout hit-rate as a performance criterion and compare the performance of LM and HB. Additionally, we consider simulated data sets and compare parameter recovery using the traditional metric of RMSE, the root mean square error.For comparisons we used Sawtooth SoftwareCBC/HB4.2 with the default settings. All the data files were run with 20,000 iterations. With one of our data sets nameddw7it was questionable whether or not proper convergence took place, but the prediction result turned out to be fairly good. However, additional iterations did not improve the result fordw7.Gensler, Hinz, Skiera, and Theysohn (2012) point out the importance of extreme behavior in data files; i.e., such respondents that always choose ‘none’ or never choose ‘none’. Among our field data onlydw7included the none-option, and the corresponding percentages for extreme behavior always selecting ‘none’ and never selecting ‘none’ were 0.4% and 44.2%.Tables 1 and 2present descriptive figures of the field-data problems.6The authors thank Jussi Fredrikson for csh and Leonhard Kehl for the brand choice modeldw7.6Our field data sets use random experimental question designs. For each problem and for each method we only use data on the first 250 respondents for the performance measurement.7Given a large number of test runs we restricted the number of responts somewhat arbitrarily to 250, which we felt is large enough for CBC analysis. However, we do not expect any problems with a much larger set of respondents.7We begin by choosing randomly and independently one holdout question for each respondent i. Then we use the remaining Q questions to estimate each individual part-worth vectorβi. The validation result is the holdout hit-rate. In order to get an idea of the variation in the validation results we perform ten independent repetitions of holdout selections, which we call samples. Thus, in each sample one question is randomly and independently chosen as a holdout task for each respondent i, and ten independent samples of holdout questions are used in comparisons of the methods.Table 3shows the holdout hit-rates of HB for twelve test problems and for ten samples of holdout selection. Average hit-rates over twelve problems are on the rightmost column, and the bottom rows indicate averages over the ten samples. The overall average hit-rate is 72.7 percent for HB.Table 3 also shows the hit-rate improvement of LM over HB. The overall average hit-rate is improved by 0.4 percentage points by LM. Given the twelve problems and ten sample runs for each, the average improvement is statistically significant(p<0.02). Comparing the two methods problem-wise, the average holdout hit-rate is larger with LM for seven problems and with HB for the other five problems. Sample averages are in nine out of ten cases favorable to LM.UnlikeHB,LMemploys an endogenous standard deviation of respondents’ individual valuation error. To see whether this makes a difference, we fixed the endogenous parameterγi=1in LM (the same value as HB uses) and reran all twelve field data cases (each with 10 scenario variations). In these tests, the average holdout hit-rate difference was 0.0%; i.e., there was no statistically significant difference in the prediction performance of LM and HB. Thus this factor offers part of the explanation of the difference in performance shown in Table 3.Using simulated true part-worths we compared LM against HB and tested the ability of these approaches to recover the part-worth vectors. Initially, we use the same test setting as in Halme and Kallio (2011) and which in turn was taken from Evgeniou, Pontil, and Toubia (2007). Thereafter, we modify the initial simulation setting by allowing a ‘none’ option among possible choices in each one of the initial tasks. To help our readers, we repeat the description of the initial simulation design below. Similar tests appear in several previous simulation studies.The number of attributes is 10, each of which has two levels, and each question consists of two alternatives. Each component of the true part-worth vectorβiis an independent random draw fromN(μ,κμ), where an increaseμleads to a decrease in the standard deviation of the response error andκaccounts for the heterogeneity of the part-worths. Bothμandκhave two possible values; forμthe value is either 0.2 or 1.2 (high and low response error), andκis either 1 or 3.5 (low and high heterogeneity).Initially, we generated either 16 or 32 questions for each respondent. Fractional factorial design was employed (Nair, 2013) with the two alternatives in each question such that the two never had any attribute at the same level. The choices of respondents were simulated based on the probabilities of the multinomial logit model. Again, 250 individuals were simulated, with 50 percent of the initially generated and randomly selected questions being left for holdout. We generated five samples by independent random choices of the 8 or 16 holdout questions. In conclusion, the number of possible parameter combinations was2×2(forμandκ), there are two alternatives for the number of questions Q (8 or 16) used for estimation, and there are 5 samples of holdout selections resulting in 40 part-worth sets of respondents.The traditional metric used in the comparison of LM and HB is the root mean square error (RMSE) of the true and estimated part-worths with both the vectors normalized to 1. We also report the holdout hit-rates to enable comparison with the results of the field data.The top half of Table 4shows for each method and for the eight variants the average holdout hit-rate and RMSE over five samples. With regard to RMSE, whenQ=8, LM performs significantly better than HB (p<0.001). Similarly for average hit-rate, ifQ=8LM outperform HB (p<0.025). ForQ=16, there is no significant difference, neither in RMSE nor in hit-rate, among the two methods. The overall average hit-rate improvement of LM over HB is 1.4% points. The results by sample are shown in Appendix B.1.Similarly, the bottom half of Table 4 shows the average holdout hit-rate and RMSE. Comparing the results with the previous one (without none-options), both hit-rates and RMSE are significantly weakened. This is the case even if the frequency of chosen none-options is low (seeLL8andLL16). The overall average hit-rate improvement of LM over HB is now increased to 3.9% points. However, there is large variation among the eight cases: in four cases the hit-rate improvement is over 6% points while in theHL16(high response error, low heterogeneity, 16 questions) it is −7% points. With regard to RMSE, in all eight cases average performance of HB does not exceed that of LM. Also for these cases Appendix B.2 shows the results by sample.

@&#CONCLUSIONS@&#
This paper proposes a likelihood based approach LM to CBC estimation. We compare the performance of LM with HB (the industry standard for CBC) using the field data from twelve problems. The criterion for comparison is the holdout hit-rate; i.e., share of correct predictions for holdout questions resulting from estimated utility functions. In these tests, the average predictive performance of LM exceeds that of HB and the differences are statistically significant. As indicated in Section 4, part of the explanation is that LM, unlike HB, employs an endogenous standard deviation of respondents’ individual valuation error the average impact of which turned out to be 0.4% in the hit rate.We also compared the methods using simulated true part-worths and tested the ability of these approaches to recover those values. In these tests when the number of questions per respondent was small, LM provided an improvement over HB. For a larger number of questions, no significant difference was found among the two methods.From our comparisons based on field data and the simulation tests one may identify characteristics of data sets that were linked with superior performance of either LM of HB. In particular, there is evidence (most notably from the simulation tests) that a smaller number of questions in CBC improves the chances that LM outperforms HB. The simulation studies including a none-option in each task indicate that LM is favorable if the response error is not too large, while large response errors together with a large number of tasks (per respondent) favor HB instead. Anyway, for a particular CBC analysis in practice, one might begin by testing the predictive power of the candidate methods, and based on the results, one chooses the method to be employed. While in practice HB has been a method without a competitive alternatives, our AMPL code is now available upon request to other researchers as well.Lemma 1Given vectorsαandβiin(3)and a matrix H in(10), if H is non-singular then the globally optimal solution for the problemminV∑i(βi-α)TV-1(βi-α)+Nlog∣V∣s.t.log∣V∣=0isV=(1/λ)H, whereλ>0is the geometric mean of eigenvalues of H.Optimality conditions implyV-1HV-1-(N+π)V-1=0, whereπis the dual variable of constraintlog∣V∣=0. Solving for V yieldsV=H/(N+π), which together withlog∣V∣=0yieldsV=H/λ. To show global optimality, let A be any symmetric matrix with∣A∣=1and row vectors denoted byAl. Consider a covariance matrixV=A-2. Thenlog∣V∣=0and∑i(βi-α)TV-1(βi-α)=∑i∑l(Al(βi-α))2=∑lAlHAlT=tr(AHA)⩾L∣AHA∣1/L=L∣H∣1/L=Lλproviding a lower bound for the objective function. ForV=H/λ, we haveA=(H/λ)-1/2and∑i(βi-α)TV-1(βi-α)=tr(AHA)=λtr(H-1/2HH-1/2)=λL, which attains the lower bound above and the proof is complete.□Table 5.Table 6.