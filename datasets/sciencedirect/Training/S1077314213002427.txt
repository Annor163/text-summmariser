@&#MAIN-TITLE@&#
Fast and accurate near-duplicate image search with affinity propagation on the ImageWeb

@&#HIGHLIGHTS@&#
We propose an efficient system for large-scale image search.We adopt document retrieval algorithms to improve the image search quality.We study the tradeoff strategy in search process to accelerate our algorithm.The proposed algorithm achieves the state-of-the-art search performance.The time and memory complexity of the algorithm is very low.

@&#KEYPHRASES@&#
Large-scale image search,ImageWeb,Graph propagation,Search process tradeoff,

@&#ABSTRACT@&#
Near-duplicate image search in very large Web databases has been a hot topic in recent years. In the traditional methods, the Bag-of-Visual-Words (BoVW) model and the inverted index structure are very widely adopted. Despite the simplicity, efficiency and scalability, these algorithms highly depends on the accurate matching of local features. However, there are many reasons in real applications that limit the descriptive power of low-level features, and therefore cause the search results suffer from unsatisfied precision and recall. To overcome these shortcomings, it is reasonable to re-rank the initial search results using some post-processing approaches, such as spatial verification, query expansion and diffusion-based algorithms.In this paper, we investigate the re-ranking problem from a graph-based perspective. We construct ImageWeb, a sparse graph consisting of all the images in the database, in which two images are connected if and only if one is ranked among the top of another’s initial search result. Based on the ImageWeb, we use HITS, a query-dependent algorithm to re-rank the images according to the affinity values. We verify that it is possible to discover the nature of image relationships for search result refinement without using any handcrafted methods such as spatial verification. We also consider some tradeoff strategies to intuitively guide the selection of searching parameters. Experiments are conducted on the large-scale image datasets with more than one million images. Our algorithm achieves the state-of-the-art search performance with very fast speed at the online stages.

@&#INTRODUCTION@&#
With more than twenty years’ efforts, content-based image retrieval (CBIR) has become a successful application in computer vision. It provides an effective way of bridging the intent gap by analyzing the actual contents of the query image, rather than the metadata such as keywords, tags, and/or descriptions associated with the image. With compact image representation, it is possible for the state-of-the-art Web image search engines such as Google and Bing to handle billions of images and process each query with real-time response.To search among a large corpus of images, the Bag-of-Visual-Words (BoVW) model [1] is widely adopted. The BoVW-based image search framework contains two major stages, i.e., offline indexing and online searching. At the offline stage, local descriptors [2] are extracted on the crawled images, quantized onto a large visual vocabulary [3], and indexed into the inverted structure [1]. At the online stage, local descriptors are also extracted on the query image, and quantized into visual words to access the corresponding entries in the inverted index. Finally, all the retrieved inverted lists are aggregated as ranked search result.Despite the simplicity, efficiency and scalability of the BoVW-based image search framework, the search results often suffer from the unsatisfied precision and recall. The main reasons arise from the limited descriptive power of low-level descriptors and the considerable information loss in the quantization step. In fact, the accurate matching between local features could be highly unstable especially in the cases of manual editing and geometric deformation or stretching, meanwhile there also exist a number of incorrect feature matches between some totally irrelevant images. This may cause some relevant images to be ranked after the irrelevant ones. To improve the quality of initial search results, various post-processing techniques are proposed to consider additional clues such as the geometric location of local features [4], the extra features from the top-ranked images [5], and the affinity values propagated between images [6]. Although all the methods are verified effective to improve the precision and/or recall of the search results, the handcrafted manners of re-ranking limit them from being generalized and scaled up in the Web environments.In this paper, we investigate the image search problem from a graph-based perspective, and discover a natural way of re-ranking the initial search results without using handcrafted tricks. First, we illustrate a small graph of near-duplicate images with distractors in Fig. 1. In the image graph, two nodes are linked with each other iff they share no less than 10 common features. Some relevant image pairs are not matched since the local descriptors are quantized into different visual words, whereas the low descriptive power of low-level features also causes the false matches found between some totally irrelevant pairs. Intuitively, given a near-duplicate query image, the missing links between query and the relevant images causes low recall, and the false matches between query and the irrelevant images result in low precision. To overcome, we claim that image-level matching is more reliable than feature-level matching, and make the following observations which are pivotal to promote the relevant images and filter the irrelevant ones.•It is possible to access the missing true positives, i.e., relevant images sharing few common features with the query, via constructing the indirect paths through other relevant images.For most queries, there are more true positives than false positives in the top-ranked search results. Therefore, it is possible to adopt majority voting or affinity propagation algorithms to filter the false candidates.Based on the intuitions above, we propose ImageWeb, a novel data structure to capture the image-level context properties. Essentially speaking, ImageWeb is a sparse graph in which each image is represented by a node. There exist an edge from nodeIato nodeIbif and only if imageIbappears among the top of the initial search result of imageIa. Since the links in ImageWeb actually imply the recommendation such as “IathinksIbis relevant”, it is straightforward to adopt the query-dependent link analysis algorithms, say, HITS [7], to re-rank the initial search results by propagating affinities through the links. We verify that, with the efficient discovery of image contexts, it is possible to achieve very accurate search results even without using the handcrafted rules such as spatial verification.The major contributions of this work are summarized as follows.1.We propose an efficient data structure, ImageWeb, to discover the high-level relationships of images. We carefully design the construction, insertion and deletion algorithms to make the data structure efficiency in real applications.We provide a tradeoff strategy to guide the parameter selection in the online searching process. By sacrificing the initial search accuracy which could be compensated in the post-processing, we achieve much better search performance with much lower time complexity compared to the baseline methods.A new near-duplicate dataset with 51 groups of famous car logos are provided. There are many (more than 200) samples in one near-duplicate group which is more similar to the Web environments. Our algorithm works very well in this dataset, demonstrating its good transportability to real-world applications.The remainder of this paper is organized as follows. First, we formulate a general pipeline for image search in Section 2. In Section 3, we introduce ImageWeb, an efficient data structure to capture the image-level relationships, and use ImageWeb to re-rank initial results in the large-scale image search problems. Section 4 follows to provide a tradeoff strategy for parameter selection in the online search process. After studying the search performance in two challenging databases in Section 5, we draw the conclusions in Section 6.In this section, we provide a brief overview of the image search pipeline based on the Bag-of-Visual-Words (BoVW) model and the inverted index structure.The Bag-of-Visual-Words (BoVW) model represents each image as a set of visual words. It starts with an imageI=(aij)H×W, whereaijis the pixel on position(i,j).Due to the limited descriptive power of raw image pixels, a number of regions-of-interest (ROI) are detected using operators such as DoG [2], MSER [8] or Hessian Affine [9], and the local descriptors are calculated on the ROIs to give a basic representation of local patches. Popular descriptors for image retrieval include SIFT [2,10], SURF [11], BRIEF [12] and so on. Each combination of detector and descriptor yields a set of descriptorsD:(1)D=(d1,l1),(d2,l2),…,(dM,lM),wheredmandlmdenote the description vector and the corresponding location of the mth descriptor, respectively. M is the total number of descriptors, which could be hundreds or even thousands in common images.After descriptors have been extracted, they are often quantized into visual words to be compact. There are generally two ways of transforming descriptors into visual words. The first method, Vector Quantization (VQ), requires training a codebook using clustering methods [1]. In image search tasks, K is often rather large, e.g., more than one million, and it could be computationally intractable to compute the accurate (exact) K-Means clustering. Therefore, the hierarchical [3] or approximate [13] versions of K-Means are often adopted for efficiency. After codebook construction, descriptors are quantized into visual words. Given a codebook with B codewords, hard quantization strategy produces a unique numberfm(from 0 toB-1), which is the ID of the nearest (most similar) codeword of descriptordm. Oppositely, soft assignment mapsdmonto a subspace spanned by several codewords [14], giving a few different word IDs as well as their weights. In most cases, soft quantization produces more accurate feature matches but also requires more computational resources. We denote the quantization result ofdmasfm, which is a sparse,ℓ1-normalized vector with K dimensions, and each nonzero dimension represents the weight of the corresponding codeword.The second approach, Scalar Quantization (SQ) [15], binarizes each descriptor bin directly without training a codebook. Given a SIFT descriptordm(D=128):(2)dm=dm,1,dm,2,…,dm,Da transformation function is defined to quantizedminto a bit vector (all elements are either 0 or 1)fmdirectly:(3)fm=fm,1,fm,2,…,fm,2DHere, the visual wordfmis twice the length of descriptordm, which implies that 2 bits are used to encode each bin ofdm. Following [15],fmis calculated in a bitwise manner. Forj=1,2,…,D,(4)fm,j,fm,j+D=(1,1),ifdˆmH<dm,j(1,0),ifdˆmL<dm,j⩽dˆmH(0,0),ifdm,j⩽dˆmLdˆmHanddˆmLare high and low thresholds, respectively, which are defined by sorting the elements ofdmin ascending order:(5)dmS=dm,1S,dm,2S,…,dm,DSand using statistics:(6)dˆmL=dm,D/2S+dm,D/2+1S2(7)dˆmH=dm,3D/4S+dm,3D/4+1S2.In the Scalar Quantization framework, the comparison of descriptors inℓ2-distance is captured by the Hamming distance between the corresponding2D-bit binary vectors.Of course, more quantization methods such as metric learning [16,17], dimension reduction [18,19] could also be adopted to encode the descriptors. After quantization, a set of visual words are obtained:(8)F=(f1,l1),(f2,l2),…,(fM,lM).Meanwhile, the set of local descriptors are not used in the later steps. Although the computation with visual words are much more effective, the quantization step might discard considerable information, which might cause some irrelevant descriptors projected onto the same visual word. This further weaken the descriptive power of local features.Besides local descriptors, there are also efforts to represent an image with their global properties. Such holistic features, such as GIST [20] or visual attributes [21], are verified effective to encode the images with relatively smaller number, usually less than a few thousand, of bits. It is acknowledged that global features could serve as a good compensation to local ones, and they are especially useful when retrieving images with close semantics but are not near-duplicate  [22–24]. Similar to the classification models combining multiple sources of features [25], local and holistic features have also been integrated in the image search tasks to boost the search performance [26,27]. Since this paper focuses on near-duplicate image search, we do not adopt holistic features in our system.In large-scale image search, the feature matching problem could be formulated as finding features’ approximately nearest neighbors. When the number of features becomes very large, say, over 1 billion, we need an efficient data structure to organize them so that we can search them in a very short time. To address the problem, we leverage from information retrieval [28] to use the inverted index [13,4] as a scalable indexing structure for storing a large corpus of images with their features. In essence, the inverted index is a compact representation of a sparse matrix, whose rows and columns denote features and images, respectively. The offline indexing stage maps each unique codeword onto an entry followed by a list of units, in which we store the ID of the image where the visual word appears, and some other clues for verification, e.g., location of the feature. Moreover, each feature could also be assigned with the IDF score [28] orℓp-IDF score [29], which is used to indicate the feature importance (discriminative power) in the whole corpus.The ways of indexing visual words in Vector Quantization and Scalar Quantization are also different. Vector Quantization produces a single ID or a group of (weighted) IDs for each descriptor, which could be used as the address of the inverted index entry directly. In contrast, the output of Scalar Quantization is a2D-bit binary vector. We take its first t bits as a natural codeword to access the entry, and store the remaining2D-tbits as well as the image ID in the indexed unit. It is worth noting that, although the total amount of possible codewords is2t, which could be as many as4Gwhent=32[15], the number of valid codewords (with non-empty lists) is much smaller (80Min experiments). Therefore it is possible to allocate an entry for each codeword and use hashing to map the codewords onto index entries.Given a query imageIq, local descriptors are also extracted and quantized into visual words. Then the searching process finds the corresponding entry of each encoded feature and retrieve the followed lists. Finally, the retrieved images are ranked by their frequencies of occurrence or accumulated IDF scores. The performance of such a naive searching process depends highly on the accuracy of feature matches. However, due to the limited descriptive power of local descriptors and the information loss in the quantization step, the simple counting method often suffers from unsatisfied precision and recall. To improve the quality of search results, various types of post-processing algorithms are used, including spatial verification, query expansion, and/or diffusion-based methods.It is verified in many works that spatial contexts are useful for filtering false positive results. Therefore, spatial verification [13,5,30] is proposed to boost the precision by checking the relative geometric locations of the matched features. For example, [5] considers weak geometric consistency to quickly filter potential false matches, [13] performs global spatial verification based on a variation of RANSAC, and [31,32] adopts a geometric coding scheme by encoding the relative spatial locations within a compact binary code. In [33,34], the authors propose visual phrases as more robust feature groups for filtering false matches.Query expansion [4,35,36], leveraged from text retrieval, reissues the initial top-ranked results to find valuable features which are not present in the original query. The enriched feature set is helpful to boost the recall of search systems. Typical query expansion techniques include transitive closure expansion [4], intra- and inter-expansion [35], automatic failure recovery [36] and so on. In the Scalar Quantization algorithm [15], a spacial approach is adopted for query expansion. The goal is to find the matched visual words (256-bit binary codes) with the Hamming distance no larger thanκ. Recall that each visual word in the database is indexed by the firstt=32bits. For each querying feature, the first t bits are also extracted as the querying codeword. The inverted lists with codewords no more than d Hamming distance to the querying codeword are visited, and each of the features in the lists are checked if the total Hamming distance (on 256 bits) to the querying feature are no more thanκ. It requires visitingQ=∑s=0dtsinverted lists. We name d andκthe codeword expansion threshold and Hamming threshold, respectively. Note that a larger expansion threshold often results in heavier enumeration (larger Q) and therefore higher time complexity.In image search, it is also straightforward to propagate affinities or beliefs via the connection of objects. Based on global image matching, it is feasible to establish an image network [6], and calculate the affinity scores of image candidates in the initial search results. For example, [37,6,38] explore the use of content-based features to improve the quality of text-based image search results, and [39] mines both context and content links in social media networks to discover the underlying latent semantic space. In [27], the relationship between images are presented using a graph-based data structure, which is also exploited in [40] for exploring image collections based on global topological analysis.The proposed ImageWeb algorithm is an efficient implementation of diffusion methods. Different from the related work, VisualRank [6], which adopts PageRank to improve text-based image retrieval, we leverage the query-dependent HITS algorithm into the content-based image retrieval task with clear intuitions. We shall also highlight the superior performance and fast online searching speed as our contributions.This section illustrates ImageWeb for re-ranking images to improve the initial search quality. We shall first introduce the intuition originating from the document retrieval community, and then present the construction, updating and searching algorithms of ImageWeb. Finally, we discuss the scalability of ImageWeb by analyzing its time and memory consumptions.Document search engines play an important role on the World Wide Web (WWW). A search engine typically consists of three major stages. First, webpages are found by a Web crawler, an automated Web browser which follows every link on the websites. The retrieved webpages are then stored in an index database for later use. When a user types in the querying keywords, the engine looks up the index and retrieves a list of best-matched web pages according to some criteria. While there may be millions of webpages that include the querying keywords, some pages may be more relevant, popular, or authoritative than others. Most search engines employ link analysis to rank the retrieved webpages and return the best ones to the users.To measure the quality of webpages, many link analysis algorithms explore the internal structure of the Web, and use random walk, a mathematical formulation to calculate the affinity values [41] of the pages. The most popular link analysis algorithms include PageRank [42] and HITS [7]. Both of them assume that high quality websites are likely to receive more links from other websites. and the quality of a webpage could be roughly estimated by accumulating affinities from the pages linking to it. Both PageRank and HITS could be computed either intuitively with iteration methods, or mathematically using eigenvectors.PageRank [42] assigns a numerical weightw∈(0,1)to each element of the Web. which is a probability representing the likelihood that a person randomly clicking on links will arrive at the particular page. In each iteration, each node distribute its weight through its outgoing links and re-collects weight from the incoming links. Since we set equal weights on each page at the beginning of the algorithm, the final results of PageRank is a reflection of the Web’s intrinsic structure, and therefore independent to any specified queries.As a precursor of PageRank, HITS [7] algorithm, or known as Hubs and Authorities, also assigns numerical weightsh,a∈0,1named hubs and authorities to each webpage, which stems from a particular insight into the creation of webpages when the Web was originally forming. In each iteration, the authority values are first updated using the hub values, and the hub values are then updated using the new version of authority values. Different from PageRank, HITS starts with a given query, and the hub and authority values of the query are set to 1 while others are 0. Therefore, HITS is a query-dependent algorithm.The key assumption of the link analysis algorithms in document retrieval is that one webpagePacontains a link to another onePbifPasuggests thatPbis of high quality. In order to leverage those algorithms into the image search problems, we organize the images in the database with a graph model, in which each image is represented by a node, and there exists a link from imageIatoIbifIbis ranked among the top in the (initial) search results ofIa. The same assumption also holds in the constructed network of images. One imageIacontains a link to another oneIbimplies thatIbis more likely to be relevant toIa.Formulating the basic idea above yields the definition of ImageWeb, a data structure for organizing image-level relationships. Suppose we have a large-scale image databaseI=I1,I2,…,IN, where N is the total number of images which could be over one million in real applications. ImageWeb is a directed graphG=V,E.V=v1,v2,…,vNis the set of nodes, andvnis the corresponding node of imageInforn=1,2,…,N.Eis the set of edges connecting between nodes, and there is an directed edge fromvn1tovn2iffIn2appears among the top-K (initial) search results ofIn1. Here K, an integer much smaller than N, is named the breadth of the ImageWebG. There is also a weight assigned to each edge of the ImageWeb, indicating the importance of the edge. In any time, the outgoing links of any node are ranked according to their weights, and the weights of the node are normalized so that their sum equals to 1. The algorithm of constructing ImageWeb is illustrated in Fig. 2.In real applications, the image database might change with time. Newly retrieved images might be added, while some outdated or illegal images should be removed from the database. Therefore, it is necessary for the ImageWeb to support efficient insertion and deletion operations. The main difficulties of image insertion and deletion arise from updating the outgoing links of images that are already in the database before insertion or preserved after deletion. Re-calculating the outgoing links for all the images is a natural solution, but it could be computationally intractable if we re-construct the whole ImageWeb after each modification of image database. Here, we adopt the approximated updating algorithm, in which we only check and update those images that are linked by the inserted/deleted images. The complete re-calculation of one image’s outgoing links is only called when necessary, i.e., the number of outgoing links is less than a threshold, say,0.8K. Since the approximated updating algorithm might cause the ImageWeb more and more inaccurate, we can re-construct the whole ImageWeb after a relatively long period of time. The insertion and deletion algorithms are illustrated in Figs. 3 and 4, respectively.Fig. 5illustrates two nodes in a toy ImageWeb (K=10) constructed on 1104 images. We present an example for the easy and difficult queries, respectively. It is worth noting that K is usually a very small number relative to the size of database, therefore we are actually not storing all the true positives to each query in the ImageWeb. The ImageWeb only provides a pre-calculated network structure for improving the quality of online search results.The link analysis algorithm on the ImageWeb is aimed at filtering the top-ranked false positives for each query. Here, we benefit from the fact that all the true positive images contain the same near-duplicate concept, and it is more probable to find effective feature matches between them, i.e., they are more likely to appear among others’ outgoing links (top-ranked candidates). Even when we consider the relatively more difficult case in Fig. 5, in which more false positives (6 out of 10) are found in the top-ranked results, we can see that 6 false samples come from 6 different near-duplicate groups, therefore they are not likely to be connected with each other very closely.The above observations inspire us to adopt the affinity propagation algorithms for image re-ranking. Given a query image, we extract all its visual words to look up the inverted index, and obtain a N-dimensional vector representing the number of feature matches between the query and each candidates. Normalizing the vector yields a probability distribution over all the imagesw0=w0,1,w0,2,…,w0,Nsatisfying∑nw0,n=1. We take this distribution as the initial hub values of all the images, and adopt the HITS algorithm [7] on the ImageWebGto update authority and hub values of all the images iteratively for R rounds. R is named the depth of link analysis, which means that after R iterations, we could retrieve all the candidates that the query-candidate shortest path is not longer than R in the ImageWeb. We do not always iterate the weighting till convergence for the consideration of time consumption. The algorithm is formulated in Fig. 6.In this part, we focus on the time and memory consumptions caused by the ImageWeb algorithms. We ignore the time used at descriptor extraction, quantization and indexing stages, and the memory cost for indexing millions of images into and inverted file structure.The time overhead of the ImageWeb algorithms consist of two parts, offline construction and online searching. The main part of offline time is used in the initial searching process, and is highly related to the BoVW model adopted to construct the ImageWeb. In practise, we use the Scalar Quantization [15] to generate the initial results. Suppose there are N images in the database, each image contains M features, and each feature is expanded Q times in initial search process, then each image requiresO(MQα)time to access the inverted index, andO(Nlog(N))time for sorting the candidates. Here,αis the average number of features indexed after each entry. In real applications, we haveMQ<Nandα∼log(NM)<2log(N), therefore the time complexity for constructing the ImageWeb isO(N2log(N)), Moreover, it requiresO(Nlog(N))time to insert an image into the ImageWeb or delete an image from the ImageWeb (ignoring the sparsely called re-search operations in image deletion). In our experiments (see Section 5), N is larger than106, and we use parallelization methods to accelerate the construction process. Considering the operation is required only once, the time complexity is affordable.The online stage requiresO(Nlog(N))operations for initial searching, and an R-round HITS algorithm consisting of2NKRfloating number calculations. With different settings of parameters, i.e., codeword expansion threshold d, Hamming thresholdκ(see Section 2.2), ImageWeb breadth K and depth R, the time cost of online stage could vary from tens of milliseconds to tens of seconds. It is a challenge to accelerate the algorithm without harming the search accuracy. We will cope with this problem in Section 4.The memory cost of the ImageWeb comes from the storage of each image’s K top-ranked candidates in the ImageWeb. It costs 8 bytes to store one candidate (4 bytes for image ID and 4 bytes for score). We setK=20for the best performance of searching process (see Section 4.2), therefore the total memory cost on a single image is 160 bytes. In total, we need 160 Megabytes to store the ImageWeb with one million images, which is much smaller than 4 Gigabytes used for loading the inverted index of one millions images.This section is aimed at finding a proper set of parameters for the online searching process. For this, we adopt two intuitive tradeoff strategies to guide the parameter selection, one between precision and recall, and the other between time complexity and search accuracy. Finally, accurate search results are achieved with very low time consumptions.We start with introducing the basic settings used in our experiments.We use Scalar Quantization (SQ) [15] as the baseline system. Based on the initial search results provided by SQ, we construct ImageWeb for post-processing. To make fair comparison, we keep the same settings as the baselines.•Descriptor extraction. We use the SIFT descriptors [2] calculated on the Regions of Interest detected by DoG operators [2]. All the images are greyscale, and resized so that the larger axis size is 300.Descriptor quantization. We use Scalar Quantization (SQ) [15] formulation (4) to encode each 128-D SIFT descriptor into a 256-bit binary code.Indexing. The first 32 out of 256 bits of each visual word are taken as the indexing address. Image ID as well as the remaining 224-bit binary codes are stored in the inverted index. We remove the features which appear in more thanN1/3images where N is the number of images in the whole corpus.Online searching. We follow the basic searching process of Scalar Quantization [15] to obtain the initial scores. The HITS algorithm is then performed on a pre-constructed ImageWeb. The impact of searching parameters will be discussed thoroughly in this section.Accuracy evaluation. We use the mean average precision (mAP) to evaluate the accuracies of all methods.There are several adjustable parameters that affect the online searching stage, including the codeword expansion thresholdd and the Hamming thresholdκin the initial searching process (Section 2.3), and the ImageWeb breadthK and depthR (see Section 3). The different sets of parameters could result in contrasting search accuracies and time costs, and our goal is to find a proper set of parameters producing accurate search results very efficiently. It is obviously very difficult to enumerate all the possible parameter combinations and choose the best one. Therefore, we adopt an intuitive tradeoff strategy to help us find a proper set of parameters.Throughout this section, we report the mAP values and time costs on the DupImage dataset [31] with one million distractors. The detailed information of this dataset could be found in Section 5.1.First we consider the setting of breadthK and depthR in the ImageWeb. A larger K causes more top-ranked candidates is stored in the ImageWeb structure, and a larger R implies that affinity values are propagated through more iteration rounds. Intuitively, the increase of K and R helps to improve the recall and precision, respectively. As shown in [43,5], there always exists a precision-recall tradeoff in both document and image search systems. For example, decreasing the stopword threshold, i.e., more visual words are stopped, produces more accurate feature matching to improve the search precision, but also rejects some correct matches and therefore degrades the recall.To be clear, we test different combinations of K and R, and plot the mAP values under different settings in Fig. 7. It is easy to observe that the mAP value does not always increase when the breadth and depth go up, although large breadth and depth help to improve the recall and precision of search results, respectively. In fact, if K becomes too large, a considerable number of false positives would be introduced into the ImageWeb, which would harm the precision of link analysis algorithm. On the other hand, an improper value of R could also cause the search accuracy drop. Therefore, we can conclude that individually maximizing precision or recall does not provide the best overall performance. The highest mAP value is achieved using the compromised parameters, i.e.,K=20andR=10.Next we configure the setting of codeword expansion thresholdd and Hamming thresholdκin initial searching. We report the search accuracy and query time obtained with different sets of parameters in our algorithm, where the time percentages used in initial search and post-processing are recorded, respectively. We list a part of the data in Table 1. WhenK=R=0(no post-processing), the search accuracy does increase significantly with the codeword expansion threshold d, since a larger expansion threshold helps to capture more possibly matched features in the expanded searching. The major shortcoming of a large d value is the considerable time use at the online searching stage, i.e., we need about500msto process a query whend=2, and even more than4swhend=3. Fortunately, the ImageWeb is verified as a good compensation to the inaccurate initial searching process. With ImageWeb re-ranking algorithms, we can obtain excellent performance even based on very poor initial results (see Table 1, from about0.15to higher than0.75). Moreover, ImageWeb bridges the accuracy gap between small and large d values. Under proper setting of breadthR=10and depthK=20, the mAP difference betweend=0andd=3decreases from about0.40to no more than0.01. Considering thatd=0only requires50msfor initial searching, which is merely1%of usingd=3, it is quite instructive to adopt it to shorten the time of online searching.We provide an explanation to the parameter selection using the well-known marginal utility. Suppose we have fixed the total time cost in the online searching process (including initial searching and post-processing), then it is unadvisable to spend too large fraction in either initial or post processing stage. When we choosed=0,κ=16andK=20,R=10, the initial and post processing stages would require50msand60msfor each query, respectively. The total searching time,110ms, is even much shorter than the baseline system (480ms). We shall inherit these parameters in the later experiments.

@&#CONCLUSIONS@&#
In this paper, we investigate the near-duplicate Web image search problem, and propose a novel solution for refining the initial search results. We observe that the shortcoming of BoVW-based image search framework arises from the limited descriptive power of local features, and claim that the image-level matches are more reliable than the feature-level matches. From a graph-based perspective, we propose ImageWeb, an efficient data structure to capture images’ context properties, and adopt HITS, a query-dependent re-ranking algorithm, to bring order into the ImageWeb. An intuitive tradeoff strategy is also introduced to guide the parameter selection at the online searching stage. Experimental results on the large-scale image search datasets reveal that our algorithm achieves significant accuracy improvement with even much lower time complexity compared to the baseline system. Moreover, our algorithm is highly scalable, as the time and memory overheads are both linear to the size of the image corpus.