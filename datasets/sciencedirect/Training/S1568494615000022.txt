@&#MAIN-TITLE@&#
Using group genetic algorithm to improve performance of attribute clustering

@&#HIGHLIGHTS@&#
Proposed an improved approaches for attribute clustering based on the GGA.It can speed up classification time and reduce cost by selecting a feature subset.It can replace missing values by other attributes in the same clusters.Experiments show that the proposed approach is efficient on a real dataset.

@&#KEYPHRASES@&#
Attribute clustering,Feature selection,Genetic algorithm,Grouping genetic algorithm,Data mining,

@&#ABSTRACT@&#
Feature selection is a pre-processing step in data mining and machine learning, and is very important in analyzing high-dimensional data. Attribute clustering has been proposed for feature selection. If similar attributes can be clustered into groups, they can then be easily replaced by others in the same group when some attribute values are missing. Hong et al. proposed a genetic algorithm (GA) to find appropriate attribute clusters. However, in their approaches, multiple chromosomes represent the same attribute clustering result (feasible solution) due to the combinatorial property, and thus the search space is larger than necessary. This study improves the performance of the GA-based attribute clustering process based on the grouping genetic algorithm (GGA). In the proposed approach, the general GGA representation and operators are used to reduce redundancy in the chromosome representation for attribute clustering. Experiments are also conducted to compare the efficiency of the proposed approach with that of an existing approach. The results indicate that the proposed approach can derive attribute grouping results in an effective way.

@&#INTRODUCTION@&#
Feature selection is an important pre-processing step in data mining and machine learning [8]. An appropriate subset of features not only reduces the execution time required for deriving rules [2], but also improves classification accuracy. Feature selection is also critical to data classification and data retrieval, and has been widely used in many research fields, such as pattern recognition, statistics, and data mining. Since feature selection is an optimization problem, there are many techniques could be utilized. Some well-known approaches like genetic algorithms [15,17], particle swarm optimization [20,21], ant colony optimization [13], and other bio-inspired optimization algorithms [9,10,22].There are many GA-based approaches which have been proposed for feature selection [15,17]. In addition, some PSO- or ACO-based algorithms have also been proposed for the feature selection problems [13,21]. For example, in [21], a multi-objective particle swarm optimization (PSO) approach was presented for feature selection. The goal of that approach was to generate a set of Pareto solutions (feature subsets) for classification. In [13], a hybrid metaheuristic named ant-cuckoo colony optimization was proposed, which was a hybrid of ant colony optimization and cuckoo search for feature selection in digital mammogram. In other words, the main purpose of those approaches was to provide a set of selected attributes for classification.However, in order to overcome the problem of high dimensionality, various feature selection techniques have been proposed [1,23]. A good feature subset can help the adopted learning algorithm get better results in less time. Finding an optimal feature subset has been shown to be an NP-hard problem [3]. In 2007, Hong and Liou proposed a feature selection approach based on the concept of feature clustering [12]. In their approach, the problem of selecting K features from input features could be considered as a K-clustering problem, with each cluster providing one feature as a member of the final feature subset. This approach can find an approximate feature subset for classification. In addition, the algorithm can find another feature to replace the selected feature if the feature value of the object is missing. Based on the same idea, Hong and Wang [16,17] proposed genetic algorithm (GA)-based clustering methods for attribute clustering to find an approximate feature subset for classification. As Falkenauer pointed out, the general GA has some weakness when solving grouping problems [6]. Because of the encoding scheme, multiple chromosomes would map to the same attribute clustering result (feasible solution) due to the combinatorial property, and thus the search space is larger than necessary.Furthermore, the GA operations cannot distinguish chromosomes that are mapped to the same result. In this case, two chromosomes with the same result may generate a new chromosome that is quite different from its parents, meaning that GA-based methods require more time to converge. Falkenauer [6] thus proposed the group genetic algorithm (GGA) to address this problem. The GGA has the same workflow as that of the GA, but uses different encoding schema and different operators. It has been demonstrated that the efficiency of the GGA is superior to that of the GA in some areas, especially with regard to grouping problems [4].The present study thus proposes a GGA-based attribute clustering approach. In other words, the main goal of the proposed GGA-based approach is to divide attributes into K groups for classification. Attributes in the same group mean they have similar properties. Then, we can select attributes from each group, and gather them together for classification. In addition, in case a selected attribute A has too many missing values, we can select attribute B from the same group to replace A. Thus, based on GGA, we define chromosome representation, genetic operations, and fitness function (please see Section 3) for the proposed approach for solving the attribute clustering problem. Besides, the contributions of this paper are stated as follows: (1) a GGA-based attribute grouping approach is proposed for solving attribute clustering problem for classification; (2) by utilizing the derived attribute clustering results, when an attribute A has many missing values or less data instances, instead of the attribute A, an attribute B in same group can be selected for classification.The rest of this paper is organized as follows. Some related studies are reviewed in Section 2. The proposed method based on the GGA for attribute clustering with some examples are also given to illustrate its use is described in Section 3. The experimental results are given and discussed in Section 4. Finally, the conclusions and suggestions for future work are stated in Section 5.This chapter reviews some related research, covering topics such as feature selection, attribute clustering, attribute dependency measurements, and the GA used to address grouping problems.Feature selection is an important pre-processing procedure in machine learning and data mining, especially when the learning process is executed on high-dimensional datasets. Dash et al. [5] defined feature selection for classification as finding the minimally sized subsets of features that could maintain the classification accuracy and the resulting class distribution. A good feature subset not only reduces the training time and I/O requirement, but also leads to a better understanding of the data and more accurate predictions. An input feature set usually has some features that are redundant or irrelevant to the objective. These features may cause the classifier to infer in the wrong direction for finding the result, and thus decrease the speed of the process. Irrelevant features cannot improve, or may even decrease, the performance of the target objective. Redundant features are relevant to the target objective, but can be replaced by other features. The purpose of feature selection is thus to find an appropriate subset of features that are relevant to the target concept. In other words, it is a procedure that removes irrelevant or redundant features to improve the efficiency of applications working with high-dimensional datasets. There are many GA-based approaches which have been proposed for feature selection [15,17]. In addition, some PSO- or ACO-based algorithms have also been proposed for the feature selection problems [13,21]. The main purpose of those approaches is to provide a set of selected attributes for classification. In our previous work, a GA-based algorithm has been proposed for attribute grouping [17], not only for feature selection.Dependency measurements are used to estimate the similarity between attributes. They were proposed by Han et al. [11] and Li et al. [14]. Hong and Liou used a dependency measure in their attribute clustering method based on feature selection approaches [10]. Attributes that provide a similar contribution to the classification have a high dependency on each other. Formally, given two attributes Aiand Aj, the relative degree of dependency of Aiwith regard to Ajis denoted as Dep(Ai, Aj), defined as formula (1):(1)Dep(Ai,Aj)=∏Ai(U)∏Ai∪Aj(U),where U is a set of training examples and∏Ai(U)is the projection of U on attribute Ai. The degree of dependency degree is not symmetrical. The average of Dep(Ai, Aj) and Dep(Aj, Ai) is thus used to represent the similarity of attributes Aiand Aj.Hong and Wang [16] presented a GA-based clustering method for attribute clustering to find approximate feature subsets for classification. They first proposed an approach which considers the average classification accuracy and the cluster balance of the attribute clusters, represented by chromosomes, as the fitness evaluation criteria. The average classification accuracy is used to calculate all the possible feature subset combinations from the chromosome clustering results, and this is then used to evaluate the classification ability of the selected feature subset with regard to the given dataset. The other measure used in this approach is the cluster balance, which is used to help the GA clustering algorithm to find clusters with similar numbers of attributes. If the clustering result represented by a chromosome is more balanced, then the value is larger. The fitness function then combines the two measures together to get a good trade-off between accuracy and cluster balance.Many methods based on GA have been proposed for solving grouping problems [12], although some challenges remain for standard GA. The two main weaknesses of GA in this respect are described below.First, the standard encoding scheme of GA is highly redundant with regard to grouping problems. Assume that there are N objects to be clustered into K clusters. Each chromosome may be represented as an N-gene sequence, with each gene being one of the K group symbols and representing the object of the gene belonging to that group. For example, assume that there is a chromosome, ABBAC, which represents five objects in three groups. The first and the fourth objects are in the group A, the second and third objects are in another group B, and the fifth object is in a singleton group C. This encoding scheme has K! distinct chromosomes to represent the same grouping result, and the genetic operators cannot identify them. Hence the search space increases significantly because of the duplicate encoding scheme, seriously impairing the efficiency of the GA.The second weakness of GA for grouping problems is that the classical GA crossover operator cannot ensure the inheritance property of the offspring from their parents. For example, consider the two parents shown below and execute the two-point crossover operation on the bar positions:AB|AB|Cthe first parent,BB|CA|Athe second parent.This yields the following offspring:AB CA Cone of the children,The result shows that the first object in the offspring is grouped with the fourth object, although neither parent has this property. The offspring is generated due to the relabeling of its parents’ symbols; the same situation occurs for the other objects in the offspring. Comparing the offspring with its parents, it can be observed that they are quite different, and this is because the crossover operator is object-oriented. The crossover operator only considers which group tag an object belongs to, and does not care about the current cluster relation of the objects. In fact, the most important thing of the grouping problem is which objects are grouped together and which are not. This means that using a GA-based algorithm to solve grouping problems is more like a random search, thus reducing the efficiency of the algorithm.Since the traditional GA approach has some weaknesses when applied to the grouping problems, as mentioned above, Falkenauer proposed the GGA. The GGA has successfully been applied to various grouping and clustering problems, such as the problems of bin packing and economies of scale [7]. In addition, Pankratz employed an adaptation of the GGA for vehicle routing problems [18], and Rekiek applied the GGA to the handicapped person transportation problem [19]. The results of Falkenauer's experiments showed that the GGA outperforms the GA with regard to solving these problems [6].Brown and Sumichrast [3] carried out some empirical tests to determine the performance of GA and GGA in different domains, and their results indicated that the latter was superior to the former for large grouping problems. The GGA and the GA have nearly the same procedures, but the former adopts a different encoding scheme and different genetic operators. In the following section, the GGA's encoding scheme and genetic operators are introduced.In Falkenauer's GGA representation, a chromosome consists of two parts, one each for the object and group. The object part stores the information about how the objects are grouped, and the group part is an ordered list of the groups. The object part is formed by a fixed-length string, with each gene in the string representing the group label of an object. For example, consider an object part in the chromosome: ACBBA. This chromosome represents that the first and the fifth objects belong to group “A”, the third and the fourth objects belong to group “B”, and the second object belongs to group “C”. The group part is the main difference from the traditional GA. It stores all the group names in a sequence of variable length. In the GGA, a chromosome represents how the given objects are grouped. The group tags are only used to differentiate which objects are in the same group, and which are not, and hence the same group tag of objects in different chromosomes does not mean that there are any relations between the groups. An example of a complete chromosome is shown below:ACBBA: ABC.In this chromosome, the object part is located before the semicolon, and the group part is after it. In the object part, there are five objects partitioned into three groups. The names of the groups are recorded as a sequence after the semicolon. For three groups, the sequence in the group part can be any combination of the three symbols ‘A’, ‘B’, and ‘C’. For example, it may be “ABC”, “BAC”, or other combinations. The sequence is randomly assigned when a chromosome is produced in the initial population generation process.Different from the GA crossover, the GGA crossover is based on the groups instead of on the objects. Falkenauer used the following five steps for his GGA crossover operator [6].1.Select the position of the insertion in the group part of the first parent, and then randomly select a section of the group part of another parent.Copy all the contents of the first parent to an empty chromosome, and then copy and insert the selected group section of the other chosen parent to the new chromosome.Eliminate duplicate objects which are copied from the first parent.Adjust the resulting groups according to the hard constraints of the problem to be solved.Apply Steps 1–4 to the pair of parents with their roles exchanged to generate another child.An example is given below to illustrate the crossover operation. Assume that there are two chromosomes:ADBBCCAD: ACBD, andaabbbccd: adbc.In the first step, the position of the insertion in the group part of the first parent and a random section of the second parent are selected. Assume that the position of the insertion in the first parent is the bar position shown below:A|CBD(position of the insertion is between A and C).Also assume that the selected section of the second parent is the area between the two bar positions:ad|bc|(selected section is “bc”).Then in the second step, the section “bc” in the second parent is inserted into the first parent, with the group part of the new chromosome written as:A bc CBD.Note that each letter represents a group in the group part. In the third step, duplicate objects in the new chromosome are checked. Take the above chromosome as an example. The group “b” includes the third, fourth, and fifth objects according to the original second parent. Group “B” includes the third and fourth objects according to the original first object. The third and fourth objects thus appear twice in the new chromosome. Object redundancy checking on the new chromosome is thus needed to make it feasible. The chromosome after Step 3 becomes:ADbbbccD: AbcD.After Step 3, the new chromosome may have more or fewer groups than previously defined. The adjustment of the chromosome to make the number of groups equal to the desired number is then executed in Step 4 if necessary. Fig. 1illustrates the above example.In a grouping problem, a redundant item in an inappropriate group may significantly effects the quality of the results. The GGA crossover operator emphasizes the relations within a single group, and the algorithm maintains the completeness of a group from the parent chromosomes. This concept can thus ensure that the quality of the ancestors is successfully passed on to their descendants.Mutation is adopted in the GGA using three strategies, namely creating a new group, eliminating an existing group, and exchanging items among groups. In addition, an inversion operator is also used in the GGA to change the order of the genes in the group part, in order to ensure that some groups have greater chances of being transmitted to offspring. These two operators are useful to avoid getting trapped in locally optimal solutions.The proposed method divides the whole feature set into K appropriate feature subsets, such that feature selection and replacement can be easily achieved with good performance. Here, K is a predefined constant. The proposed method adopts the GGA proposed by Falkenauer to find appropriate attribute groups [6]. A representation is first designed to encode each feasible attribute clustering result into a chromosome. The GGA approach is then executed to find the best chromosome, which is the final clustering result. In addition, the same fitness function is adopted in both the proposed GGA-based attribute clustering algorithm and the GA-based approach proposed by Hong and Wang [17] in order to fairly compare their efficiencies. The chromosome representation of the proposed approach is described below.In the proposed attribute clustering approach, each chromosome represents a possible attribute clustering result. Let a feature set A consist of n features, denoted as {a1, a2,…, an}. If the objective is to select K features from A, then a partition with K attribute groups is formed. The final feature set includes the features, with each selected from a group. Formally, let the i-th group of features be denoted as Gi. Then, G1∪G2∪…∪Gk=A, Gi≠ϕ and ∀i≠j, Gi∩Gj=ϕ. A chromosome includes the partition information of the K groups (G1, G2,…, GK). As mentioned above, and with a little modification, a GGA chromosome consists of two parts, one for the attributes and one for the groups. For the problem of attribute clustering, the attribute part stores the information about how the attributes are grouped, and the group part is an ordered list of the attribute groups. The attribute part is a fixed-length string. Each gene in the attribute part represents a group label of an attribute. For example, consider the attribute part: ABBAC. This chromosome represents that the first and fourth attributes belong to group “A”, the second and the third attributes belong to group “B”, and the fifth attribute belongs to group “C”. The other part of a GGA chromosome is the group part, which is the main part of a chromosome that the genetic operators of the GGA work on. The group part stores all the group names in a string of length K. An example for a chromosome is shown below:ABBAC: ABC.In the above chromosome, the attribute part is located before the semicolon, and the group part is after the semicolon. In the attribute part of the example, there are five attributes partitioned into three groups. The names of the groups are recorded as a sequence after the semicolon. For three groups, the sequence can be any combination of the three symbols ‘A’, ‘B’, and ‘C’. For example, it may be “ABC”, “BAC”, or other combinations. The sequence is randomly assigned when a chromosome is produced in the initial population generation process.In the GGA, a group name is used to differentiate which attributes are in the same group and which are not. Therefore, two genes with the same group name but in different chromosomes do not have any relation. In the practical implementation, the concept of a set is used instead of a string to store the group information in the attribute part. A simple example is given below to illustrate this idea.Example 3.1Assume that there are six attributes, {a1, a2,…, a6}, to be divided into three groups. Thus, K=3 and N=6. Suppose that there is a chromosome: ABBAC:ABC, shown in Fig. 2, for implementation. In this chromosome, the two attributes a1 and a4 are in the same group, the three attributes a2, a3, and a6 are grouped together, and a5 is in its own group. There are thus three sets ({1,4}, {5}, {2,3,6}) in Fig. 2 to represent the attribute part. The sequence “ABC” is stored to represent the group part of the chromosome.First, a population of chromosomes is randomly generated. Assume that we want to partition N features into K groups. First, K empty groups G={G1, G2, …, GK} are generated. The N features are then randomly assigned to the groups, with one feature to an arbitrary group. If there still exists an empty group after all the attributes are assigned, a group is selected at random from the set {Gi||Gi|>(N/K), Gi∈G}, and is split into two groups randomly. The above process is repeated until there are K non-empty groups.In the proposed approach, the fitness function proposed by Hong and Wang [17] is used to find appropriate feature subsets. The fitness function consists of two factors, cluster accuracy and cluster balance, which are briefly described below.The cluster accuracy is an evaluation of the average classification accuracy of all the possible attribute subsets of a chromosome for the given training dataset. An attribute subset that has high classification accuracy means that an object can be precisely classified to the correct class by the attributes in the feature subset. Formally, the accuracy of a chromosome Ciis defined as formula (2):(2)accuracy(Ci)=∑p=1NCsubAccuracy(Sp)NC,where NC is the number of attribute combinations resulting from chromosome Ci, and subAccuracy(Sp) is the accuracy of the p-th possible attribute combination Spfor the given training dataset.Another evaluation factor is the group balance. This factor is used to make the groups represented by the chromosome have as similar numbers of attributes as possible. If a chromosome is unbalanced, a new object with a missing value may not be correctly classified, since no other alternative attributes can be used in the group with the missing attributes. Formally, the factor of cluster balance for a chromosome Ciis defined as formula (3):(3)balance(Ci)=∑i=1K−|groupi|Nlog|groupi|N,where |groupi| represents the number of attributes in the i-th group. This basically follows the idea of entropy, and the more balanced a chromosome is, the higher is the value of the factor.According to the above discussion, the fitness function is defined as formula (4):(4)f(Ci)=accuracy(Ci)×[balance(Ci)]α,where parameter α is used to control the relative influence of the above two factors, and can be set according to the circumstances.The GGA crossover operator takes an arbitrarily chosen chromosome as a base chromosome, and then inserts some groups from another chromosome. It then eliminates duplicate attributes from the newly formed chromosome. Formally, assume that there are two chromosomes, C1 as a base chromosome and C2 as an inserted chromosome:C1=G11G21…Gp1Gp+11…GK1,C2=G12G22…Gs2Gs+m−12…GK2,where p is the point of insertion, s is the start group in the inserted segment, and m is the number of inserted segments. The parameters p, s, and m are generated randomly when a crossover operation is executed. The new offspring Cnewis then generated as follows:Cnew=G11′G21′…Gp1′Gs2…Gs+m−12Gp1′…GK1′,Ci1′=Gi1−(Gs2∪Gs+12∪…∪Gs+m−12),i∈[1,k]In addition,Gi1′is eliminated ifGi1′=ϕ. Thus, after the elimination process, there are three possible situations, namely |Cnew|<K, |Cnew|=K, or |Cnew|>K, where |Cnew| is the number of non-empty groups in the new chromosome. The second situation is the ideal result, and the other two need further adjustment to fit the constraint.If |Cnew|<K, then a new group is randomly selected from {Gi||Gi|>(N/K), Gi∈G}, and then split into two subgroups. This process is repeated until there are K non-empty groups. For the third case, where |Cnew|>K, the roulette wheel selection strategy is used to select the groups to be removed. When a group is selected for removal, the attributes inside it are randomly reassigned to another group. A group with fewer attributes has a higher probability of being selected for removal. This operation is designed to preserve the parents’ characteristics, as we want the offspring to be similar to their parents. If a group with more attributes is removed in the above operation, then the offspring will be less similar to their parents.The mutation operator works on the object part only. It randomly reassigns an attribute into another group. Another group genetic operator is the inversion operator. It is designed to help the crossover operator select different combinations of groups to exchange between two parents. This can be done with a random or purpose-driven rearrangement of the positions of groups. Groups that are closer with regard to a sequence position have a greater chance of being transferred together. In the proposed approach, the rearrangement is done randomly.Based on the above description, the proposed GGA-based algorithm for attribute clustering is designed as follows.Proposed GGA-based Attribute Clustering Algorithm:INPUT: A training dataset with N attributes and the K clusters.OUTPUT: An appropriate attribute clustering result.STEP 1. Randomly generate a population of P individuals, with each being a feasible attribute clustering result.STEP 2. Calculate the fitness value of each chromosome Ciusing the following substeps.STEP 2.1. Calculate the average accuracy of all possible attribute combinations of the chromosome by formula (2).STEP 2.2. Calculate the cluster balance of the chromosome, which is decided by the number of attributes in the group, by formula (3). A chromosome has a higher balance value when its clustering result is more balanced.STEP 2.3. Integrate the values from Steps 2.2 and 2.3 to evaluate the fitness of a chromosome by formula (4).STEP 3. Execute the GGA crossover operation.STEP 4. Execute the GGA mutation operation.STEP 5. Execute the GGA inversion operation.STEP 6. Calculate the fitness values of the new chromosomes.STEP 7. Select the chromosomes for the next generation using the roulette wheel selection strategy.STEP 8. Repeat Steps 3–7 until the termination criterion is satisfied.STEP 9. Output the chromosome with the best fitness value.In this section, an example is given to illustrate the proposed GGA-based attribute clustering approach. Assume that there are ten people with the following seven attributes: Sex (S), English (E), Country (C), Income (I), Age (A), Married (M), Buying Computer (B). The class name is Credit. The database is shown in Table 1.For the data shown in Table 1, the proposed algorithm proceeds as follows.STEP 1: P individuals are randomly generated as the initial population. In this example, P is set at 5. Each individual represents a possible clustering result. Assume that the ten initial chromosomes are generated as shown in Table 2.STEP 2: The fitness value of each chromosome is calculated using the following sub-steps.STEP 2.1: The accuracy of each possible attribute combination from the clustering result is calculated. Take the first chromosome, C1, as an example. It includes the following eight combinations: S1:{S, C, E, I}, S2:{S, C, E, A}, S3:{S, C, M, I}, S4:{S, C, M, A}, S5:{B, C, E, I}, S6:{B, C, E, A}, S7:{S, C, M, I}, and S8:{S, C, M, A}. Each of these is evaluated with regard to their accuracy. The results are shown in Table 3.STEP 2.2: The average accuracy of each chromosome is calculated. Take chromosome C1 as an example. It has eight possible attribute combinations. The sum of all the accuracies of its combinations is 5.7. The average accuracy of C1 is then calculated as follows:accuracy(C1)=6.98=0.863.The average accuracy for the other chromosomes can be similarly calculated.STEP 2.3: The cluster balance of each chromosome is calculated. For example, chromosome C1 has 2, 1, 2, and 2 attributes in its four clusters, respectively. Its cluster balance value is then calculated as follows:balance(C1)=−27×log27−17×log17−27×log27−27×log27=0.587.STEP 2.4: The fitness value of each chromosome is calculated. Take chromosome C1 as an example. Its average accuracy is 0.863, and its cluster balance is 0.587. Parameter α is set at 1. The fitness value of C1 is then calculated as follows:f(Ci)=0.863×[0.587]1=0.507.STEP 3: The crossover operation is executed on the population. Take C1 and C2 as an example. Assume that the two crossover sites are selected as shown in Fig. 3. Fig. 4shows the offspring after insertion.There are some duplicate attributes in the chromosomes, so the elimination procedure is executed. Fig. 5shows the offspring after elimination.The first offspring in Fig. 5 has three groups, which is less than K (=4 in the example). Assume that group C1 is chosen to be split into two groups. Fig. 6shows an example of splitting the group into two sub-groups.The second offspring in Fig. 5 has five groups. To satisfy the restriction on the number of groups, one group needs to be removed. Assume that group b2 is chosen for removal. Attribute M inside group b2 is then arbitrarily reassigned to another group. Fig. 7shows an example of a chromosome before and after the adjustment.STEP 4: The mutation operator is executed to generate possible offspring. The mutation operator adopted here arbitrarily reassigns an attribute to another group. The mutation procedure is illustrated in Fig. 8.STEP 5: The inversion operator is used to help the crossover operator select different combinations of groups for insertion. First, an arbitrary number between 0 and 1 is generated. If the number is larger than the inversion rate, then the group order is rearranged. The inversion procedure is illustrated in Fig. 9. Note that the inversion operator only changes the representation of the chromosome; it retains its essential nature.STEPs 6–8: The fitness values of the new chromosomes are calculated, and five chromosomes are selected as the next generation by the roulette wheel selection strategy. This procedure is repeated until the termination criterion is satisfied. The chromosome with the highest fitness value is then output as the result. After STEP 8, the attributes are divided into three clusters and the clustering results can then be flexibly used for classification.

@&#CONCLUSIONS@&#
