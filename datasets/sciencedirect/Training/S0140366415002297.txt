@&#MAIN-TITLE@&#
Can user privacy and recommendation performance be preserved simultaneously?

@&#HIGHLIGHTS@&#
HighlightPreserve privacy without loss of recommendation accuracy.Add factitious ratings to blur the privacy but intensify interests.Propose a new similarity metric of ratings.

@&#KEYPHRASES@&#
Privacy protection,Balance precision,Similarity metric,Recommendation,

@&#ABSTRACT@&#
In online systems of videos, music or books, users’ behaviors are disclosed to the recommender systems to learn their interests. Such a disclosure raises a serious concern in the public for the leak of users’ privacy. Meanwhile, some algorithms are proposed to obfuscate users’ historical behavior records to protect users’ privacy, at the cost of degradation of recommendation accuracy. It is a common belief that such tradeoff is inevitable. In this paper, however, we break this pessimistic belief based on the fact that people's interests are not necessarily limited to items which are geared to a certain gender, age, or profession. Based on this idea, we propose a recommendation-friendly privacy-preserving framework by introducing a privacy-preserving module between a recommender system and user side. For instance, to obfuscate a female user's gender information, the privacy-preserving module adds a set of extra factitious ratings of movies not watched by the given user. These added movies are selected to be those mostly watched by male viewers but interesting the given female user. Extensive experiments show that our algorithm obfuscates users’ privacy information, e.g., gender, efficiently, but also maintains or even improves recommendation accuracy.

@&#INTRODUCTION@&#
Recommendation service plays an important role in online video systems, but the possible disclosure of users’ privacy via recommendation raises serious concern in the public. The accurate recommendation is of cardinal significance for both the service providers and users. It helps users get satisfying products with ease and in turn increase the revenue for the providers [1]. Hence Netflix ever set $1 M Prize for 10% improvement of recommendation accuracy [2]. However, since a recommender system (RS) makes recommendations via mining user historical behavior to find their interests, it has been found that user personal information such as gender and age can be inferred as well. For instance, one's gender can be inferred from his or her records of watching videos [3,4], and one's age can be inferred from his or her web browsing history [5]. It worries some users that leak of personal information may lead to malicious attacks, e.g., prank calls, annoying advertisement. As a result, some users are even disgusted with personalized recommendation [6]. It is a burning issue of finding a solution to the dilemma of efficient recommendation and the protection of user identity information [7].Many existing schemes have protected user privacy successfully during the recommendation processes. And they commonly solve this problem via sacrificing the recommendation accuracy [3,6,8]. In contrast, in this paper, we set up a simple and effective framework to protect user privacy information from being inferred, and retain the recommendation accuracy without degradation meanwhile. The key to the success of this framework is a privacy-preserving (PP) module that makes the user privacy information blurred but the interest pattern undistorted and even more distinct towards RS. Since both the advertisers and malicious attackers can divide users into two groups, i.e., target group and non-target group, we exam PP module via binary classification of user privacy in detail.We illustrate its functionality with a simple example. In an online book system, to avoid old people being troubled with too many elderly-oriented advertisements, the PP module will add a set of extra factitious ratings of unread books which satisfy the following conditions: (1) their readers are mostly youth; (2) they are similar to the books that the user is interested in. Then, we set the ratings of selected books equal to the average ratings given by the youth. With the help of PP module, we can not only protect old people's age from being inferred but also provide them with accurate recommendations simultaneously. In Section 5, we introduce the work steps of PP module in detail.We take gender preservation as an instance to elaborate our algorithm as does the existing work on privacy preservation in RS [3]. Gender is one of the most basic personal information cared by not only users themselves but also advertisers or other third parties who want to make their commercials more targeted. We evaluate our framework on the MovieLens and Flixster datasets, which are commonly used in studies on RSs and privacy information obfuscation algorithms [3,9–12]. In particular, we use balanced precision [4] as the metric to evaluate the accuracy of gender inference to handle the serious imbalance between different user groups in this dataset, e.g., the number of male users is much greater than that of female ones. Such an imbalance is common in real systems [5]. And we also make a complexity analysis of our method in Section 6. The main contributions of this paper are as follows.•We treat the recommendation-friendly privacy protection (RFPP) problem as a multi-objective optimization problem and propose an efficient heuristic algorithm to solve it. As a result, we can preserve privacy without loss of recommendation accuracy simultaneously.We introduce a PP module, which serves between users and RSs to disguise user ratings before they are disclosed to the RS and the privacy inferrer (PI). With this module, the privacy information is blurred but the interest preference of user keeps undistorted.We propose a new similarity metric of movies based on the average difference of ratings and the number of users. Evaluation results show that our metric outperforms the existing ones.We add factitious ratings based on her/his interests and the average ratings from the opposite privacy group. Our scheme performs better both in the recommendation accuracy and the user privacy protection than the existing works.The rest of this paper is organized as follows. The related work is presented in Section 2. In Section 3, we describe the RFPP problem, and in Section 4, we give out the prerequisite knowledge. Section 5 introduces the algorithm framework, and Section 6 shows our experiment analysis. Finally, we make a conclusion in Section 7.

@&#CONCLUSIONS@&#
