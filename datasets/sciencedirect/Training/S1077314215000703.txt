@&#MAIN-TITLE@&#
Practical and accurate calibration of RGB-D cameras using spheres

@&#HIGHLIGHTS@&#
Practical, robust and accurate calibration of RGB-D cameras using spherical objects.Estimation of the intrinsic and extrinsic parameters of the RGB and depth sensors.Experimental comparison with existing RGB-D calibration algorithms.“RGB-D calibration toolbox” for MATLAB freely available on the Internet.

@&#KEYPHRASES@&#
RGB-D cameras,Camera calibration,Robotic vision,

@&#ABSTRACT@&#
RGB-Depth (or RGB-D) cameras are increasingly being adopted in robotic and vision applications, including mobile robot localization and mapping, gesture recognition, and at-home healthcare monitoring. As with any other sensor, calibrating RGB-D cameras is needed to increase their sensing accuracy, especially since the manufacturer’s calibration parameters might change between models. In this paper, we present a novel RGB-D camera-calibration algorithm for the estimation of the full set of intrinsic and extrinsic parameters. Our method is easy to use, can be utilized with any arrangement of RGB and depth sensors, and only requires that a spherical object (e.g., a basketball) is moved in front of the camera for a few seconds. Our image-processing pipeline automatically and robustly detects the moving calibration object while rejecting noise and outliers in the image data. Our calibration method uses all the frames of the detected sphere and leverages novel analytical results on the multi-view projection of spheres to accurately estimate all the calibration parameters. Extensive numerical simulations and real-world experiments have been conducted to validate our algorithm and compare its performance with that of other state-of-the-art calibration methods. An RGB-D Calibration Toolbox for MATLAB is also made freely available for the scientific community.

@&#INTRODUCTION@&#
RGB-D cameras consist of an RGB sensor and a depth sensor, which capture a color image (RGB) and per-pixel depth information (depth image), respectively [1,2]. These sensors generate depth estimates by relying on either structured infrared light[2,3] or Time-of-Flight (ToF)[4,5] technology. Low-cost RGB-D cameras (e.g., Microsoft’s Kinect[3]) have been recently adopted in an increasing number of applications, such as SLAM [6], 3-D scene parsing [7], semantic mapping and telepresence [8], stereoscopic augmented reality [9], 3-D shape scanning [10], construction of dense 3-D maps [11], body tracking [12,13], gait monitoring for tele-rehabilitation [14,15], and object and gesture recognition [16–18]. All of the aforementioned applications require an accurate knowledge of the calibration parameters, i.e., of the relative position and orientation between the RGB and depth sensors, as well as of their intrinsic parameters (focal length, principal point, and lens-distortion). As shown in the visual-odometry example in Fig. 1, calibrating RGB-D cameras is essential to improve their sensing accuracy, since the calibration parameters can differ in each model from those provided by the manufacturer.Calibrating RGB-D cameras still presents numerous challenges, especially when it comes to accuracy and ease of use.Most of the existing approaches [19–23] are tailored to the Microsoft’s Kinect since they make use of the infra-red (IR) image. As such, these approaches suffer from some important drawbacks: first, they are impractical as they require the user to adopt an external source of light (e.g., an halogen lamp) to improve the quality of the IR image. Second, the joint RGB-IR calibration can only be performed off-line, since for the Kinect, RGB and IR images cannot be acquired simultaneously.Other methods have been devised that make use of specific calibration objects simultaneously observed by the RGB and depth sensors. In [25], the authors use a large custom-made wood panel with tens of circular holes, which however requires an extensive manual procedure to associate the holes’ centers in each RGB- and ToF-image pair. Other approaches have been proposed in [24,26,27,29,30] that use planar surfaces with a printed checkerboard pattern. The method in [24] suffers from a major limitation, since it requires the user to provide an initial estimate of some of the calibration parameters, which might not be easily available for generic RGB-Depth sensor-pairs (e.g., RGB and ToF) other than the Kinect, thus affecting the accuracy of the overall calibration. The approaches in [26,27] have two major drawbacks: first, they are time consuming, as they require an expert user to play an active role in selecting corresponding features in the depth images. Second, this selection is unreliable due to the noisy nature of the depth image (in which corners and edges are not well-defined and cannot be accurately selected). The possible errors in the manual selection of features propagate through the calibration algorithm, thus resulting in a decreased accuracy. In [29], the authors automatically extract features from both the RGB image and the 3-D point cloud generated from the depth image. A major drawback of this method is that the corresponding features between the RGB image and the sparse 3-D point cloud are difficult to locate. Finally, building upon [26], in [28,30] more sophisticated distortion-correction models have been developed for the depth image of generic Kinect-like devices. In particular, in [28] the undistortion map is estimated using SLAM in a fully unsupervised way, i.e. the user is only required to record a few minutes of data from a natural environment. However, when compared with the checkerboard-based approaches above, the computational complexity of the method in [28] appears not to be negligible.In this paper, we present a novel calibration method for RGB-D sensors that addresses all of the aforementioned limitations: it is easy to use even for non-expert users, it is accurate, and can be adopted to calibrate any RGB-D arrangements (e.g., RGB-Lidar, RGB-ToF) which are frequently used in robotic and computer vision applications (cf. Table 1). In what follows, we highlight the major strengths of our method.(1)Our method is practical, since the user only needs to move a spherical object (e.g., a basketball) in front of the camera and no a priori knowledge of its size is required. A robust image-processing pipeline has been designed to automatically detect and track the spherical object in both the RGB and depth image, while discarding spurious features.Our calibration algorithm is accurate, since it leverages a novel (least-squares) analytic solution that provides an initial and robust estimate of the camera parameters. As a result, the accuracy of the final calibration is higher. It is also worth emphasizing here that our choice of spheres as calibration objects highly contributes to the accuracy of our method. In fact, the ellipse- and sphere-fitting algorithms adopted in our algorithm help in minimizing the measurement noise and the presence of outliers when compared, e.g., to methods that only use the corners of a standard calibration checkerboard. Spherical calibration objects were used in [31–35] for calibrating RGB sensors, but these methods could not be seamlessly extended to RGB-D sensors.Our approach is widely applicable, since it can be used to calibrate RGB and depth sensors in any 3-D relative configuration (i.e., not necessarily pointing in the same direction as for the Kinect).A new MATLAB calibration toolbox implementing our algorithm, called RGB-D Calibration Toolbox, has been developed and made available on the Internet.1http://ranger.uta.edu/∼gianluca/research/assistiverobotics_rgbdcalibration.html.1It is important to point out here some major differences between our approach, and other existing multi-camera calibration methods, such as those based on wands [36–38] (e.g., for Vicon systems [39]), or those specifically designed for RGB-Lidar and RGB-ToF sensor pairs [40,41]. Wand-based methods assume that each camera can detect tiny, reflective, 3-D fiducial markers attached to a wand, which might not be easily detectable by generic RGB-D sensors (e.g., the Kinect). Furthermore, the position of these markers must be known, while our method does not require any knowledge about the geometry of the observed calibration object. In [40], the authors extrinsically self-calibrate an RGB camera and a 3-D laser range finder without using a calibration object. However, this method is scarcely practical since it requires constant user intervention, e.g., to manually select corresponding features in both the RGB and range images. Closely related are the works in [41,42], where the authors proposed a method to calibrate multiple RGB-ToF cameras, ultimately adopting the standard checkerboard-based calibration approach in [43]. Nevertheless, these methods may be very demanding and time consuming for the end user, because the required features (such as corners and edges) are not well-defined in ToF images.This paper is built upon a previous conference contribution [44] over which we extend in several new directions: (1) we present here for the first time a novel formula that relates the projection of the spherical objects with the projection of the 3D sphere’s center. The adoption of this formula guarantees an accurate estimate of all the calibration parameters; (2) we provide more extensive simulation and experimental results in comparing our algorithm with more recent calibration methods; and (3) we experimentally evaluate our approach by also examining its accuracy in calibrating an RGB-D camera made of a ToF and RGB sensor in a generic relative configuration.The rest of this paper is organized as follows: Section 2 reviews some basic facts on pinhole camera modeling and image projection of spheres. Section 3 describes our calibration algorithm, and in Section 4 extensive numerical and real-world experimental results are presented and thoroughly discussed. Finally, in Section 5, conclusions are drawn and some possible directions for future research are discussed.Fig. 2(a) shows the general RGB-D camera configuration considered in this work. As mentioned in Section 1, our goal consists of calibrating RGB-D cameras, i.e., estimatingDK, the intrinsic calibration matrix of the depth sensor,RK, the intrinsic calibration matrix of the RGB sensor,(DRR,DRt), the rigid-body transformation between the two sensors’ frames, {R} and {D}, and the radial and tangential lens-distortion parameters for both the RGB and depth sensors. Differently from existing RGB-D calibration approaches, our method automatically estimates all of these quantities by using the live data acquired by the RGB and depth sensors, without assuming any a priori knowledge about them or about the observed scene. In what follows, we introduce the notation for both the pinhole camera model and for the image projection of spheres, which will be used in the remaining of this paper.As illustrated in Fig. 2(a), a generic 3-D pointDX0.25em0ex≜0.25em0ex[XD,0.12em0exYD,0.12em0exZD]Tin the depth-sensor frame {D} is projected at a pixel pointRu0.25em0ex≜0.25em0ex[uR,0.12em0exvR]Ton the image plane of the RGB-sensor frame {R} according to the pinhole model [45]. First,DX is expressed in {R} as:(1)RX=[DRR0.25em0ex|0.12em0exDRt]0.12em0exDX∼,where the tilde indicates the extension to homogeneous coordinates of a vector. The 3-D pointRX is then normalized asRxn0.25em0ex≜0.25em0ex[xnR,0.12em0exynR]T=[XR/ZR,0.12em0exYR/ZR]T. Similarly to [22,23,26], the lens-distortion is applied as:(2)Rxt=[2k3xnRynR+k4(r2+2(xnR)2)k3(r2+2(ynR)2)+2k4xnRynR],(3)Rxrt=(1+k1r2+k2r4+k5r6)Rxn+Rxt,wherer2=(xnR)2+(ynR)2andkc=[k1,k2,k3,k4,k5]Tis the lens-distortion vector. The pixel projection ofDX onto the RGB sensor in {R} is finally obtained as:(4)Rλ0.12em0exRu∼0.12em0ex=0.12em0exRK0.12em0exRx˜rt,whereRλ is an unknown scale factor. The intrinsic calibration matrices,RK andDK, of both the RGB and the depth sensors are defined as:(5)RK0.25em0ex≜0.25em0ex[fuRsRu0R0fvRv0R001],1em0exDK0.25em0ex≜0.25em0ex[fuDsDu0D0fvDv0D001],where (dropping the superscripts D and R for the sake of simplicity) fuand fvrepresent the focal lengths (in pixels) in both image-axes directions,[u0,0.12em0exv0]Tis the principal point in pixels, and s is the skew factor [45].We assume here that the depth sensor outputs a so-called depth image (map), i.e., a set of image points along with their corresponding depth value (in meters) on the z-axis. Each feature acquired by the depth sensor is thus defined asDu0.25em0ex≜0.25em0ex[uD,0.25em0exvD,0.25em0exZD]T. Differently from a featureRu in the RGB camera,Du can be used to uniquely retrieve the corresponding 3-D point coordinates,DX, as:(6)DX∼=[DK−103×101×31]Du¯,where03×1denotes a3×1matrix of zeros andDu¯is defined as a vector obtained fromDu as follows:(7)Du¯0.25em0ex≜0.25em0ex[uDZD,vDZD,ZD,1]T.A sphere with 3-D centerDOs, and radius rs, (see Fig. 2(b)) can be algebraically represented by a quadric [45]:(8)DQ=[I3−DOs−DOsTDOsTDOs−rs2],where I3 denotes the3×3identity matrix. In general, a sphere projects in each sensor’s image plane at an ellipse, and its projection in the RGB image,RC, is given by:(9)RC*=DRP0.12em0ex0.12em0exDQ*0.12em0exDRPT1em0exwhere1em0ex0.12em0ex0.35em0exDRP0.25em0ex≜0.25em0exRK[DRR|DRt],beingRC*=adj(RC)(the adjugate ofRC) the dual conic ofRC, andDQ*=adj(DQ)[46]. Note that the adjugate is equal to the inverse if the matrix is invertible. The conicsRC andDC will play a key role in the RGB-D camera-calibration algorithm described in Section 3.As illustrated in Fig. 2(b), the centers of the two ellipses in both {D} and {R} (Doe,Roe), do not correspond, in general, to the projected centers of the sphere, (Dos,Ros), in both images.2The center of the ellipse and the projected sphere center match when the 3-D sphere center lies on the camera’s z-axis.2A closed-form expression for converting oeinto os(dropping again the superscripts D and R for generality) is given in [35] and can be rewritten as:(10)os=oe(1−γ)+[u0,0.12em0exv0]Tγ,whereγ0.25em0ex≜0.25em0ex(rs/Os(3))2and Os(3) is the third component of the sphere center, Os. While the sphere radius rsis unknown, γ can be equivalently expressed as a function of the observed conic and of the sensor’s parameters as follows:(11)γ=1−(C12*−C13*v0−C23*u0)/C33*+u00.12em0exv0(C13*/C33*−u0)(C23*/C33*−v0),whereCij*the (i,j)-th entry of the dual conic C*.To the best of our knowledge, (11) is novel and introduced for the first time in this paper. The mapping in (11) is of particular importance as it allows to compute the projected center of the sphere, os, from the ellipse center, oe, by only knowing the conic and camera parameters and without assuming any knowledge about the sphere’s radius (see Appendix A for more details on the mathematical derivation of (11)).

@&#CONCLUSIONS@&#
In this work, we have presented a novel and easy-to-use calibration method for RGB-D cameras, which utilizes the image projection of a sphere (observed from multiple viewpoints) as the only calibration object. Our method relies on a novel closed-form solution for the calibration of the depth sensor, which is used to accurately initialize a nonlinear minimization strategy, providing a refined estimate of all the calibration parameters of the RGB-D camera (including lens distortion). The proposed algorithm is practical, since it does not need any user intervention in extracting the image features when compared to the existing checkerboard-based calibration methods, robust to outliers, and flexible since it works with generic arrangements of the color and depth cameras. Extensive simulation and real-world experiments validate our algorithm. A MATLAB toolbox implementing our method has been developed, and it has been made freely available for the research community.In future research we would like to adapt our method for the simultaneous calibration of a rig of RGB-D cameras, for which no algorithm exists in the literature to our knowledge. We also aim at showing that our calibration method can improve the localization accuracy of a mobile robot equipped with a RGB-D sensor, with respect to the non-calibrated case. Finally, as far as the Kinect sensor is concerned, we plan to extend our algorithm to include other calibration parameters such as the disparity-to-depth parameters [23,24,26] used to transform the disparity image to the depth map. These parameters are important as they can dictate the distance calculated for each pixel in the depth map.An important element of our calibration algorithm is the closed-form estimate of the projected sphere center,Ros, in (11), which uses the ellipse center,Roe, and the intrinsic calibration matrix,RK. Assuming that both the quadricRQ and the conicRC are expressed in {R}, we obtain:(A.22)RC*=P0.25em0exRQ*0.12em0exPT1em0exwhere1em0ex0.12em0ex0.35em0exP0.25em0ex≜0.25em0exRK[I3|03×1].The dual conic,RC*, can be written as:(A.23)C12*=u0Rv0R(rs2−(ZR)2)−XRYRfuRfvR−YRZRfvRu0R−XRZRfuRv0R,C13*=u0R(rs2−(ZR)2)−XRZRfuR,C22*=v0R(rs2−(ZR)2)−YRZRfvR,C33*=rs2−(ZR)2,whereCij*is the (i,j)-th entry ofRC*,XR,YR,ZRare the components of the 3-D centerROs, and rsis the sphere radius. From [35],Rγ0.25em0ex≜0.25em0ex(rs/ZR)2can be rewritten as:(A.24)Rγ=1−C12*−C13*v0R−C23*u0RC33*+u0Rv0Rαβ,whereα0.25em0ex≜0.25em0exC13*C33*−u0R,1em0exβ0.25em0ex≜0.25em0exC23*C33*−v0R.In the case thatα=0andβ=0, then the sphere is positioned directly in front of the camera (i.e.,XR=0andYR=0), and the projected center of the sphere has coordinates(u0R,v0R), i.e.,Rγ=1. Whenα=0andβ0.25em0ex≠0.25em0ex0, then the sphere 3-D center lies along the camera y-axis, (i.e.,XR=0). In this case, the projected center of the sphere has u-coordinateu0R, while its v-coordinate can be obtained from (10) by means of (A.23) withXR=0, as follows:C11*=rs2(fuR)2+u0R(rs2−(ZR)2),C13*=u0R(rs2−(ZR)2),C33*=rs2−(ZR)2,from whichRγ=1−(fuR)2C33*C11*−C13*u0R.An analogous result is found whenα0.25em0ex≠0.25em0ex0andβ=0.The RGB calibration method in [34] is used to estimateRK from at least three images of a sphere by using the image of the absolute conic (IAC) defined asω=RK−TRK−1. The IAC,ω, satisfies the orthogonal constraintl=ωv, where v is a vanishing point and l is a vanishing line [45] which can be determined from the eigenvectors of the homography conic [34] defined as:(A.25)RH=RCiRCj*,wherei0.25em0ex≠0.25em0exj,i,j=1,⋯,N. In particular, the vanishing line l is obtained as the eigenvector ofRH corresponding to a line passing through the center of both conics. Instead, the vanishing point, v, is given by the cross product of the remaining two eigenvectors. From the orthogonal constraint, a homogeneous equation is readily obtained as[l]×ωv=03×1where[x]×denotes the skew-symmetric matrix associated to a vector x ∈ IR3. ThenRK can be estimated by using the DLT method [45]. A check on the rank of the resulting homogeneous system rules out degenerate cases (e.g., conic centers close to each other).