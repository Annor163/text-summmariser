@&#MAIN-TITLE@&#
An accelerating CPU based correlation-based image alignment for real-time automatic optical inspection

@&#HIGHLIGHTS@&#
We propose an accelerating CPU based correlation-based image alignment.The image pyramid search scheme is combined with the parallel computation.Sub-pixel accuracy is used to attain the more accurate image alignment.It can align the accurate pose of the template image within the inspected image.

@&#KEYPHRASES@&#
Image alignment,Image pyramid search strategy,Streaming SIMD Extensions 2,Similarity measure,

@&#ABSTRACT@&#
This paper proposes an accelerating correlation-based image alignment using CPUs for time-critical applications in automatic optical inspection (AOI). In order to improve computation efficiency, the image pyramid search scheme is combined with the parallel computation. The image pyramid search scheme is employed first to quickly find certain objects in both monochrome and color images with rotation, translation and scaling. Sub-pixel accuracy is then used to attain the more accurate results at the sub-pixel level. In our experimental results, rotation accuracy is smaller than 0.218°, and the speed is increased between 277 and 20,841 times. According to translation, rotation and scaling tests, the errors of rotation, translation and scaling are 0.2°, 2.07pixel and 0.55%, respectively. These results show that the proposed method is suitable for dealing with correlation-based image alignment for time-critical applications in automatic optical inspection.

@&#INTRODUCTION@&#
Recently, the demands of automatic optical inspection (AOI) have increased significantly as the quality control of manufacturing process had become increasingly important. Template matching is particularly useful for machine vision tasks, where an object within an image must be aligned with a model of the object. In general, two-dimensional (2D) matching under rigid transformations, i.e., translation and rotation, is usually applied in applications of AOI in which the model of the object is generated from an image of the object. In order to account for rotation and multi-object detection, the similarity measure should be calculated with rotated version of the template. However, this induces a significant increment in computational cost because several similarity calculations considering the multi-object detection with rotated template version must be done at each position of the detected region.Based on correlation similarity measurement, the object model is systematically compared to the image using all allowable degrees of freedom of transformations in the matching phase. The most popular and robust correlation-based similarity measurement is the Normalized Cross Correlation (NCC), which is used to evaluate the similarity between the template image and the compared image. For template matching in AOI, the optimal matching results are indicated by the maximum NCC correlation, and the NCC coefficient is confined in the range between −1 and 1. The NCC coefficient is equal to 1 when there is perfect matching, and it also means the two compared images are same.To conduct the NCC, a template in a scene image by sliding the template window on a pixel-by-pixel basis, and the NCC coefficient is further computed. Therefore, it is a computationally intense problem that aims at finding an object in a scene image. As is well known, template matching based on a full search correlation metric is exhaustively time-consuming, so locating the best object in an image efficiently and quickly is always an important problem in matching process [1].To solve this problem, numerous approaches have been recently proposed to facilitate this task. Though the several efficient methods are used, the computation burden is still too expensive to calculate the similarity between a template and a block in search image. To simplify the computation procedure, an elimination strategy based on the upper bounds has been applied in the full exhaustive search template matching procedure to rapidly skip the dissimilar candidates in monochrome level images [2] and color images [3]. For the elimination strategy performance, tight bounds are proposed to speed up the correlation computation in [4]. And the authors further propose a coarse-to-fine scheme and two stage techniques for large and small templates based on a partial elimination algorithm [5]. Other efficient searches with the multilevel winner update scheme [6], the branch-and-bound scheme [7] and weak classifiers [8] are also proposed to efficiently accelerate inference by avoiding unnecessary calculation of the NCC metric.The other optimal manner considered to achieve parallel computing is single instruction multiple data (SIMD) by parallel computing CPUs. In the correlation-based similarity algorithm, SIMD executes repetitive arithmetic operations based on the multiplication and accumulation computations. In previous studies [9,10], the SSE and SIMD instructions are used to quickly calculate the multiplication and accumulation computations of NCC metric. Although efficient correlation computation based on parallel computing methods is proposed, the computation burden dramatically increases when the objects contain rotation and scaling of an image.Although optimal methods based on the elimination strategy or other efficient search algorithms are proposed, these approaches are unable to satisfy AOI applications if the objects have rotation and scaling. The NCC methods with rotated angle are introduced to solve this problem [11,12]. In [11], the pre-computed score set from rotated templates to the original template are used to recognize the target object by skipping unnecessary computation in the first step and then estimating rotation angle of the target object in the refinement step. In the refinement step, piecewise linear models are used to evaluate the rotation angle by straight lines. However, its alignment accuracy depends on the rotation step angle in the pre-computed score set. Furthermore, bi-cubic interpolation in the matching scores is used to obtain the sub-pixel level location and rotation angle in both steps [12], and accuracy of rotation depends on the interval of the interpolation, so, it cannot guarantee the optimal results after the two step interpolations. In [13], a two stage method is proposed to locate the certain object and estimate the rotation angle of the certain object. In the first stage, the rotation invariant method, ring-projection transform, is used to recognize the possible locations of a template in the scene image. In the second stage, the least second moment is employed to estimate the rotation angle. However, the computation burden is still high in matching process by using ring-projection transform.In contrast to there correlation-based methods, the gradient based NCC metric for detection of texture-less objects has been proposed [14–16]. Here, the efficient template matching method only considers the image gradients to detect objects, and further use a binary representation of the gradients. Then SIMD units are also applied to save the computations of gradient based NCC metric. The iterative closest point (ICP) is further employed to optimize the pose estimation in [17], but the ICP is impractical for the real-time application.As mentioned above, many studies have proposed optimizing the template matching method, but these methods cannot provide a precise result when the object involves rotation, translation and scaling in time-critical applications.In AOI applications, the geometric relationship between template image T and inspected image I is constricted, as shown in Fig. 1. There usually exists an indicated inspection pattern (such as the diamond patterns shown in Fig. 1) in the images to alleviate the computational burden for performing real-time template matching. Although the scaling variation between T and I is small because the working distance between the lens and the inspected target is fixed in AOI applications, the scaling still affects the accuracy of alignment results. Hence, the template matching problem for AOI can be formulated as finding the optimal 2-D rigid transformation between T and I, constrained by translation vectorT⇀, the rotation angle, and the scaling factor.This study is motivated by developing a real time correlation-based image alignment for monochrome and color images by which the target objects can be detected under rigid transformations and scaling. The method presented here not only can obtain the location of the template image within the inspection image but also can provide precise information on rotation and scaling. In addition, the proposed methods become not only very accurate, but also efficiently real-time oriented by a image pyramid search technique, Streaming SIMD Extensions 2 and sub-pixel accuracy estimation.The remaining sections of this paper are organized as follows: Section 2 reviews the normalized cross correlation method. Section 3 presents the architecture of this proposed method. Section 4 presents the key processes in detail. Section 5 presents the sub-pixel alignment. Various experimental results are then discussed in Section 6. Finally, the conclusions of this paper are given in Section 7.The correlation-based similarity measurement is applied to evaluate the degree of similarity between two compared images. The NCC metric is generally applied for both monochrome and color images, and the definitions of NCC for both monochrome and color images are first carried out [18]. In monochrome images, the NCC similarity coefficient can be used for image alignment between the template imageT(i,j)and the inspection imageI(x,y), is defined as:(1)δg(x,y)=∑i=-m/2m/2∑j=-n/2n/2[I(x+i,y+j)·T(i,j)]-m·n·μI·μTσI·σTwhere the size of the template image ism×n;uTanduIare the gray level averages of the template image and the windowed search image, respectively, i.e.,μT=1m·n∑i=-m/2m/2∑j=-n/2n/2T(i,j).T(i,j)μI=1m·n∑i=-m/2m/2∑j=-n/2n/2I(x+i,y+j)σI=∑i=-m/2m/2∑j=-n/2n/2I2(x+i,y+j)-m·n·μI2σT=∑i=-m/2m/2∑j=-n/2n/2T2(i,j)-m·n·μT2δg(x,y)is the degree of similarity between template image and the compared window image at coordinates(x,y)for gray level images.On the other hand, color images are typically represented with RGB vectors that correspond to the red (R), green (G), and blue (B) channels. The extension of NCC similarity coefficient from gray level images to color images is defined by:(2)δc(x,y)=∑i=-m/2m/2∑j=-n/2n/2[Ic(x+i,y+j)·Tc(i,j)]-3·m·n·μIc·μTcσIc·σTcwhereTc(i,j)=(TR(i,j),TG(i,j),TB(i,j))Ic(x+i,y+j)=(IR(x+i,y+j),IG(x+i,y+j),IB(x+i,y+j))μTc=13·m·n∑i=-m/2m/2∑j=-n/2n/2[TR(i,j)+TG(i,j)+TB(i,j)]μIc=13·m·n∑i=-m/2m/2∑j=-n/2n/2[IR(x+i,x+j)+IG(x+i,x+j)+IB(x+i,x+j)]σIc=∑i=-m/2m/2∑j=-n/2n/2‖Ic(x+i,y+j)‖2-3·m·n·μIc2σTc=∑i=-m/2m/2∑j=-n/2n/2‖Tc(i,j)‖2-3·m·n·μTc2The number of computations required to calculate the NCC similarity coefficient for a color image is three times that for a gray-level image.In NCC similarity measurement, the dot product term yielding the numerator of Eqs. (1) and (2) represents the cross correlation between the template and the compared window image, and its computation turns out to be the bottleneck in similarity measurement. In order to support both computation of speed and accuracy, the image pyramid search strategy, SIMD units and refinement adjustment are developed.Fig. 2shows an overview of the architecture, which consists of offline model generation and online matching process. There are three input parameters defined by the user: the threshold of similarityThmin; the detected range of rotation angle,RminandRmax; the detected range of scaling,SminandSmax.In offline model generation, the model is generated from the template image of the object to be matched in the online matching process. HereTlis the multi-resolution images; l is an index of image pyramid levels, L is the maximum index of image pyramid levels; andul,T,σl,Tandul,Tc,σl,Tcare the mean and the standard deviation of monochrome and color template images, respectively, on image pyramid level l.In the online matching process, the multi-resolution image of an inspection image,Il, is built by a fixed image pyramid level L from the offline model generation. Then the NCC similarity measurement with the image pyramid search strategy user defined parameters, the pre-calculated mean and standard deviation, of template to quickly find the objects. Meanwhile, the locations of objects with a similarity score exceeding the threshold of similarityThminare registered and stored in object list. The translation estimation is performed by using vectorT⇀between the centers of the target detected in T and I. Essentially, the image pyramid search strategy begins with an exhaustive search on the highest level and to extract a fixed number of objects from the best matches. Each object is then tracked down to the lowest level of the image pyramids, and the objects with the scores that exceed the threshold of similarityThminare reported as the final match. The multi-resolution NCC metrics based on this image pyramid search strategy are further described in Section 4.2. The sub-pixel alignment is done by fitting a second order polynomial to the similarity scores in a3×3×3neighborhood around the position which has maximum score.The proposed object selection is coarse-to-fine image alignment method. In the coarse level, rotation, translation and scaling of objects are detected by the image pyramid search strategy on a search image.To consider the objects with the rigid transformation involved in NCC similarity coefficient calculation as shown in Fig. 2, the transformed models are generated by rotating and scaling the compared window image. The similarity coefficient for monochrome and color NCC metric are re-defined from Eq. (1) to be further used to estimate the pose parameters of each candidate as:(3)δg(x,y,θ)=∑i=-m/2m/2∑j=-n/2n/2[I(x+î,y+ĵ)·T(i,j)]-m·n·μl,I·μl,T∑i=-m/2m/2∑j=-n/2n/2I2(x+î,y+ĵ)-m·n·μl,I2·∑i=-m/2m/2∑j=-n/2n/2T2(i,j)-m·n·μl,T2whereîĵ=S×cos(θ)S×-sin(θ)S×sin(θ)S×cos(θ)ij,θis rotation angle,Sis scaling ratio,îandĵare the shift position in coordinate (x,y) after rotation and scaling.The gray level averages of the compared window imageμl,Iis modified by(4)μl,I=1m·n∑i=-m/2m/2∑j=-n/2n/2I(x+î,y+ĵ)Subsequently, extension of the NCC coefficient from monochrome images to color images is re-defined by(5)δc(x,y,θ)=∑i=-m/2m/2∑j=-n/2n/2[Ic(x+î,y+ĵ)·Tc(i,j)]-3·m·n·μl,Ic·μl,Tc∑i=-m/2m/2∑j=-n/2n/2‖Ic(x+î,y+ĵ)‖2-3·m·n·μl,Ic2·∑i=-m/2m/2∑j=-n/2n/2‖Tc(i,j)‖2-3·m·n·μl,Tc2whereîandĵare the shift position in coordinate (x,y) after rotationφand scalingS; and the color average of the compared window imageμl,Icis modified by(6)μl,Ic=13·m·n∑i=-m/2m/2∑j=-n/2n/2[IR(x+î,x+ĵ)+IG(x+î,x+ĵ)+IB(x+î,x+ĵ)]Therefore, the objects with rotation, translation and scaling are first selected and located by the NCC metric, either Eq. (3) or Eq. (5). Unfortunately, the computation burden dramatically increases when the objects contain rotation, translation and scaling in an image. Hence, the image pyramid technique [19], the SIMD unit and the image pyramid search strategy are applied to reduce the computational burden in the objects selection process, and the other basic idea of these techniques quickly find the object and rapidly eliminate the dissimilarity object on each image pyramid level.The image pyramid search framework allows such a target search. The image pyramid hierarchically splits the objects involving information of different image pyramid levels into disjoint subsets. By this strategy, large numbers of the objects can be discarded early during the search process by noticing that their similarity coefficient are lower than the pre-defined threshold of similarityThmin. Here, the similarity coefficient between the template and the compared window image is computed by Eq. (3) in monochrome image or Eq. (5) in color image.Fig. 3shows a case with three image pyramid levels study of search process. The search process starts at the objects on the highest pyramid level. The object with the rotationθand scalingSat location (x,y), on the highest pyramid level is searched by computing the similarity coefficient between the template and the compared window image. The object,obj2,1={θ21,S21,(x21,y21)}, on level 2, which is indicated by a green circle, is searched in Fig. 3. Any object with similarity score that exceeds the threshold of similarityThminis stored in the object list, and theThminshould be set to the minimum expected object visibility. On the next lower pyramid levels, the search process is based on the objects that have been found on the previous pyramid level, such as the objectsobj1,{1,2,3,4}that are denoted by a yellow circle are inherited from their parent objectobj2,1with a2×2search area on image pyramid level 1 and the search rotation angle and scaling ratio are also inherited fromobj2,1={θ21,S21,(x21,y21)}with small range of rotation and scaling. Only one candidate,obj1,2={θ12,S12,(x12,y12)}, denoted by a red square, with similarity score exceeds the threshold of similarityThmin. This process is repeated until all match candidates have been tracked down to the lowest pyramid level. The final match result,obj0,6, is denoted by a red solid square on the lowest level, as shown in Fig. 3. According to this search strategy, theThminis set to skip unnecessary computations. This search strategy, it not only provides efficient matching for a single object, but also is easily extended to multi-object matching.The correlation-based image alignment method, NCC, executes repetitive operations based on summation, multiplication and multiplication and accumulation (MAC) involved in the calculation of NCC metric. In particular, the calculation bottleneck of the NCC metric is the computation of the summations, multiplications and dot product terms. But the speed can be significantly increased with the SSE2 (Streaming SIMD Extensions 2) instructions that parallel the computation of the NCC metric by performing several multiplications and summations through a single SSE2 instruction. With the advantages of SSE2, the SSE2 instructions are utilized to optimize evaluation of the NCC coefficient when an object has rotation, translation and scaling. The SSE2 is an SIMD extension on the Intel SIMD processor supplementary instruction sets, where SIMD means that the one instruction is applied to multiple data. SSE2 contains 8 registers, called XMM0 to XMM7, each with a width of 16bytes. That implies that those registers contain 2, 4, 8 or 16 variables with a corresponding width of 8, 4, 2, or 1byte. Therefore, it can perform the operation on several variables in parallel. Next, Fig. 4illustrates an example for computation of the arithmetic operation of summation, multiplication and MAC. Here, __m128i is a data type for integer arithmetical operations. In Fig. 4(a), _T and _I are variables of __m128i that operate in XMM registers, and they represent the data involved in unpacked in word data type loaded from the two different data sets. The summation of _T and _I in SSE2 instruction is expressed as:(7)_sum=_mm_add_epi16(_T,_I)After executing the eight summations between corresponding items, the summation result is stored into the destination register _sum. In Fig. 4(b), _T and _I also represent the data involved in unpacked in word data type as loaded the two different data sets. The multiplication of _T and _I in SSE2 instruction is expressed as:(8)_mul=_mm_mullo_epi16(_T,_I)After executing the eight multiplication between corresponding items, the result is stored into the destination register _mul. In Fig. 4(c), _T and _I also represent the data involved in unpacked word data type as loaded from the two different data sets. The dot product of _T and _I in SSE2 instruction is expressed as:(9)_madd=_mm_madd_epi16(_T,_I)After executing the eight multiplications and four summations between corresponding items, the MAC result is stored in the destination register _madd.This example shows that the summation and the MAC values of eight values are only executed as a single SSE2 instruction, and it can be computed very quickly by using SSE2 instructions.As mentioned above, the calculation bottleneck of the NCC metric is the computation of the summations and dot product terms. In NCC metrics for monochrome and color images, Eqs. (3) and (5), the numerator and the denominatoreffectively use the summation, multiplication and MAC arithmetical operations, such as mean values, and square values and the dot-product terms. For instance, the size of template and compared windows images are8×8pixel, the computation burden of the summation, multiplication and MAC requires only 26, 58 and 58 instruction cycles by using Eqs. (7)–(9), respectively. On the other hand, it requires 64, 640 and 703 instruction cycles without SSE2 instruction. Then for the similarity calculation in this example, the computation instruction cycles are 469 and 3736, with or without SSE2. This shows the computation advantage is about 8 times and also means the NCC metric can be computed very quickly using SSE2 instructions.In order to obtain higher accuracy with rotationθand translationT→estimation, higher accuracy can be achieved by the idea of parabolic surface fitting. For this the initial transformed parameters are estimated in the final match on image pyramid level 0. To refine the position and rotation, the facet model principle is applied to fit the second order polynomial to the NCC similarity coefficientδ(x,y,θ)in a3×3×3neighborhood around the initial parameters. The neighborhood parameter of the translation and rotation are±1and±Δα. Here, theδ(x,y,θ)represents the NCC similarity coefficientδg(x,y,θ)if the template image is monochrome; similarly, theδ(x,y,θ)represents the NCC similarity coefficientδc(x,y,θ)if the template image is in color. The model of the surface that fits the set of the data points is considered as the following general equation:(10)δ(x,y,θ)=k0x2+k1y2+k2θ2+k3xy+k4xθ+k5yθ+k6x+k7y+k8θ+k9The solution for the coefficientsk0,…,k9can be determined by solving the equationAz=s,whereA=xδmax-12yδmax-12θδmax-Δα2(xδmax-1)·(yδmax-1)(xδmax-1)·(θδmax-Δα)(yδmax-1)·(θδmax-Δα)(xδmax-1)(yδmax-1)(θδmax-Δα)1xδmax2yδmax-12θδmax-Δα2(xδmax)·(yδmax-1)(xδmax)·(θδmax-Δα)(yδmax-1)·(θδmax-Δα)(xδmax)(yδmax-1)(θδmax-Δα)1xδmax+12yδmax-12θδmax-Δα2(xδmax+1)·(yδmax-1)(xδmax+1)·(θδmax-Δα)(yδmax-1)·(θδmax-Δα)(xδmax+1)(yδmax-1)(θδmax-Δα)1⋮⋮⋮⋮⋮⋮⋮⋮⋮⋮xδmax+12yδmax+12θδmax+Δα2(xδmax+1)·(yδmax+1)(xδmax+1)·(θδmax+Δα)(yδmax+1)·(θδmax+Δα)(xδmax+1)(yδmax+1)(θδmax+Δα)1z=k0k1k2k3k4k5k6k7k8k91Tands=s(xδmax-1,yδmax-1,θδmax-Δα)s(xδmax,yδmax-1,θδmax-Δα)s(xδmax+1,yδmax-1,θδmax-Δα)⋮s(xδmax+1,yδmax+1,θδmax+Δα)wherexδmax,yδmaxandθδmaxare obtained from the final match on image pyramid level 0, respectively, as described in Section 4.2. There are 10 unknown values in the coefficient matrixz, and 27 correlation data points are supplied by the 27 neighbors surrounding the initial parameters(xδmax,yδmax,θδmax), as given ins. SinceAis a rectangular matrix, a least-squares regression must be used to solvez, and it can be re-written as:(11)ATAz=ATsSinceATAis a square matrix, the following equation yields the ten unknown parameters as:(12)z=ATA-1ATsThe refinement can finally be obtained by analytically computing the maximum of Eq. (10). For calculating, Eq. (10) can be re-written as:(13)δ(x,y,θ)=xyθk012k312k412k3k112k512k412k5k2xyθ+xyθk6k7k8+k9Finally, the sub-pixel precise position and rotation is obtained by solving Eq. (14). The values for(x,y,θ)can be solved(14)∇δ(x,y,θ)=2k0k3k4k32k1k5k4k52k2xyθ+k6k7k8=0Finally, the sub-pixel precise position and rotation of the maximum is obtained by solving Eq. (14) with respect to(x,y,θ). The values for(x,y,θ)can be solved by a3×3linear equation system:(15)x∗y∗θ∗=2k0k3k4k32k1k5k4k52k2-1-k6-k7-k8The solution of Eq. (15), noted as(x∗,y∗,θ∗), are the results of(x,y,θ)with sub-pixel accuracy in the estimation. It should be noted that since this is a closed form solution for the maximum value, the computation is deterministic. The accuracy of this refinement will be further discussed in the following section.In this section, a series of experimental results show the performance of the proposed method. Various images are used to verify our algorithm in different experiments. As mentioned in Section 3, several parameters are designed for our proposed method. The similarity thresholdThminis a critical value, since it impacts the performance directly. There are two real world AOI cases are shown in Fig. 11. Here, the certain objects should be recognized and the corresponding poses of the objects are further estimated. In those cases, the 1000 real captured images are obtained for setting the bestThmin. As a thumb of rule, we set theThminas 0.75 to guarantee a recognition rate higher than 99.9%. In this case, the recognition rates are 99.9% and 100% for Fig. 11(a) and (b), respectively. Only one of the 36 objects in a specified capture image is failed to be recognized in Fig. 11(a). The setting of other matching parameters, the rotation range,RmaxandRmin, the scaling range,SminandSmaxare dependent on the requirement of applications. In our experiment, the setting of all parameters is defined before the test. The results are shown and discussed in the following subsections. To verify the proposed method, the results are compared with Ref. [13] and the basic NCC template matching with rotation, called NCC_R. All of the experiments were performed in Visual Studio 2008 on a personal computer (PC) with an Intel Core i7 3.4GHz CPU and 8GB of memory.Fig. 5shows the rotation test images, which are used for evaluating the rotation accuracy of the proposed rotation estimation technique. For rotation accuracy estimation, the synthetic 72 rotated images are generated by rotating the original image with rotation angles from0°to359°by5°step increments, and the parameters of user defined,Thmin=0.75,Rmin=-180,Rmax=180,Smin=1,Smax=1. The proposed method outperforms the others two methods in terms of rotation accuracy under various rotation images. Detailed comparison results are shown in Table 1. To provide an overall accuracy evaluation, two performance indices, the mean and the standard deviation, are used to quantitatively show the performance that computed by 72 synthetic rotated images.As seen in Table 1, the errors of mean, standard deviation in all test cases that are conducted by our proposed technique are less than 0.218° and 0.307°. Even though the error of standard deviation of NCC_R in Fig. 5(b) and the errors of mean and standard deviation of Ref. [13] in Fig. 5(c) are close to our proposed method. In other cases, our method is superior to other competing methods. This indicates that proposed method is robust and can provide more reliable and accurate results, without being restricted to any special cases.In this experiment, the test images involved translation, rotation and scaling used to verify the proposed method. Here, only the proposed method was implemented. The test images are shown in Fig. 6and the test parameters of this experiment are listed in Table 2. In the TRS test, the rotation angle of test parameters are 30° and 45°, the scaling range of test parameters is from 80% to 120%, increasing by steps of 10%. Those 20 test images are used to evaluate the performance. To evaluate the performance for translation, the Euclidean distance between the original translation and the estimated translation is employed to evaluate the result in translation, and the overall errors of the proposed method are shown in Table 5. And the user-defined parameters areThmin=0.75,Rmin=-180,Rmax=180,Smin=0.7,Smax=1.3. Detailed results are shown in Tables 3 and 4.It can be seen in Table 5that the mean rotation errors of color and monochrome cases are 0.2° and 0.11°, and the mean scaling errors of color and monochrome cases are 0.55% and 0.37%, and then the errors of mean translation in each cases are 2.07pixel and 1.93pixel, respectively. For these results, the proposed method can provide precisely matching results when the object simultaneously involved rotation, translation and scaling.The images are captured when an object with arbitrary moving and rotating for further experiments. To compare the computational burden of the proposed method and the other comparative methods, Ref. [13] and NCC_R, the execution time is used as a performance index. There are six kinds of captured images, color images and monochrome images that are employed to evaluate the computation burden, and these are shown in Figs. 7–11and corresponding case 1 to case 6. In this experiment, the user-defined parameters areThmin=0.75,Rmin=-180,Rmax=180,Smin=1,Smax=1. The computation results are shown in Table 6.From Table 6 it can be noted that proposed method is notably more efficiency than other compared methods. The computation advantage range comparing to Ref. [13] and NCC_R is between 277 and 20,841 times. In median size of template cases, Case 1, Case 2, Case 4 and Case 5, the computation advantages comparing to NCC_R are 227, 20,841, 910 and 1561 times. On the other hand, the computational advantages are 998, 6054, 400 and 549 times in monochrome cases, respectively. For further comparison with small template cases, Case 3 and Case 6, with color and monochrome images, are used to evaluate the performance. In small cases, the speed-up range is between 340 and 1067 times. Moreover, the proposed method is always faster than the Ref. [13] and the NCC_R least over 340 times despite on small template size.This experiment is used to verify the proposed method is still efficient when more objects are located in test image. Fig. 11 shows the multi-object test images. There are totally 36 and 18 objects located in Fig. 11(a) and (b), respectively. Here, the parameters of user defined areThmin=0.75,Rmin=−180,Rmax=180,Smin=0.9 andSmax=1.1. In this experiment, the top 1, 5, 10 and 15 similarity ranking of targets in Fig. 11(a) and (b) are selected for evaluation by the efficient process in multi-object detection. The results of multi-object detection are shown in Table 7.As shown in Table 7, it can be seen that the computational burden is not increased when detection number of object is increased. In order to achieve multi-object detection, the all of possible candidates are tracked down to lowest image pyramid and the possible candidates are sorted in each image pyramid levels. By using the proposed search strategy, the results show that our method is efficient for multi-object detection.After the analysis of various experiments, the proposed method is verified for use in the real word AOI applications because of the convincing results.

@&#CONCLUSIONS@&#
