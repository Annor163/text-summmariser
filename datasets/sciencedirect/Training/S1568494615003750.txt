@&#MAIN-TITLE@&#
A surrogate-assisted evolutionary algorithm based on the genetic diversity objective

@&#HIGHLIGHTS@&#
A novel surrogate-assisted memetic algorithm is proposed.Multi-objective optimization problems featuring expensive fitness are dealt with.Evolutionary, gradient-based algorithms and a back-propagation neural network are used.Detailed performance analysis has been conducted.Proposed algorithm outperformed the other state-of-the-art evolutionary algorithms.

@&#KEYPHRASES@&#
Surrogate models,Metamodels,Memetic algorithms,Genetic diversity,Evolutionary algorithm design,

@&#ABSTRACT@&#
In this work, a novel surrogate-assisted memetic algorithm is proposed which is based on the preservation of genetic diversity within the population. The aim of the algorithm is to solve multi-objective optimization problems featuring computationally expensive fitness functions in an efficient manner. The main novelty is the use of an evolutionary algorithm as global searcher that treats the genetic diversity as an objective during the evolution and uses it, together with a non-dominated sorting approach, to assign the ranks. This algorithm, coupled with a gradient-based algorithm as local searcher and a back-propagation neural network as global surrogate model, demonstrates to provide a reliable and effective balance between exploration and exploitation. A detailed performance analysis has been conducted on five commonly used multi-objective problems, each one involving distinct features that can make the convergence difficult toward the Pareto-optimal front. In most cases, the proposed algorithm outperformed the other state-of-the-art evolutionary algorithms considered in the comparison, assuring higher repeatability on the final non-dominated set, deeper convergence level and higher convergence rate. It also demonstrates a clear ability to widely cover the Pareto-optimal front with larger percentage of non-dominated solutions if compared to the total number of function evaluations.

@&#INTRODUCTION@&#
In recent years, the research toward building advanced multi-objective evolutionary algorithms (MOEAs) for solving complex problems involving multiple conflicting objectives have been increasing enormously [1]. In particular, the need for effective optimization tools dealing with computationally expensive objective functions and constraints has become widespread in almost all the engineering disciplines. One of these is computational fluid dynamics (CFD), a set of numerical techniques which makes it possible to solve for the Reynolds-averaged Navier–Stokes (RANS) equations in complex domains, where a single run can require a huge computational effort.In those cases, the modeling and design optimization cycle time is roughly proportional to the number of calls to the computationally expensive solver, so that many evolutionary frameworks have been implemented around the idea of alleviating the computational cost by introducing an approximate, or surrogate, model of the real objective functions [2,3]. The surrogate model (SM) is actually a model of the mathematical model itself upon which the solver is built, and therefore con be referred to as a “metamodel”.Using a SM has become a very popular approach since the efforts required to build the surrogates and to use them along with MOEAs are much lower than those in the standard, direct-call methodologies [4]. Popular SM are built using several methodologies, e.g. parametric statistical methods such as the response surface methodology (RSM) [5–7] as well as nonparametric techniques [8–10], like multivariate adaptive regression splines [11], artificial neural networks (ANNs) [12] and radial basis functions (RBFs) [13–15]. Other, more recently developed, nonparametric techniques for surrogate modeling encompass support vector regression (SVR) [16–18], regression Kriging (RK) [19–22] and moving least squares or local polynomial regression (LPR) [23–27].In the framework of MOEAs, several approaches for dealing with computationally expensive problems using surrogate models have been documented in the open literature. First approaches were based on the concept of fitness inheritance [28,29], where the fitness of an individual is evaluated indirectly by interpolating the fitness of its parents. Evolution control techniques, e.g. clustering methods [30,31], were also proposed as a way to estimate when exact function evaluations are to be performed.Alternative approaches utilize a progressive refinement of a SM as the search evolves [32]. Within those methodologies, memetic algorithms (MAs) seem to be one of the most promising techniques [33].MAs are population-based metaheuristic search methods that follow the basis of Dawkins notion of “meme”, defined as a unit of cultural evolution that is capable of local refinements [34–36]. MAs have also been used under the name of hybrid evolutionary algorithms, Baldwinian evolutionary algorithms, Lamarkian evolutionary algorithms, or genetic local search [33]. The main advantage of MAs over concurrent strategies lies in creating a synergy between global and local search of a set of optimal solutions of the objective function. A SM can be used in lieu of the original, computationally expensive, objective function during the local refinement thus leading to the so-called surrogate-assisted memetic algorithm (SAMA) concept. To this purpose, the prediction accuracy of fitness predictions based on SMs can be significantly enhanced with the inclusion of gradient information in SM building [37,38].It has been extensively demonstrated in literature [39–41] that diversity preservation during evolution is a crucial aspect for evolutionary algorithms (EAs). If the lack of population diversity occurs too early the algorithm is trapped in confined regions and is not able to explore the whole search space. The diversity preservation becomes particularly important in the case of MAs [42,43] and SAMAs because of their marked tendency to exploitation. In this paper, a novel multi-objective SAMA is introduced and described which is based on the coupling between the genetic diversity evolutionary algorithm (GeDEA) [39] as global searcher and a local search framework built around a gradient-based algorithm working with a SM of the original optimization function. Because of its peculiar features we named it the genetic diversity memetic algorithm (GDMA).In the following chapters the rationale behind the development of the proposed algorithm is presented, starting with a brief recall about the importance of preserving diversity during evolution for genetic and memetic algorithms. The main methodologies used in literature to face such a problem are listed and compared with the selected diversity preservation method [39]. Second the GDMA is presented together with the metamodeling technique used to build the local search framework, and the framework itself. In the final stage GDMA is applied to five, commonly used, test functions in order to compare its performance against several well-known evolutionary algorithms by means of four different performance metrics. GDMA is then used on a real-world application where a multi-objective/multi-point optimization of a 2D aerodynamic airfoil is carried out.The genetic diversity of individuals for population-based algorithms has been recognized as a crucial property since the beginning of the subject [44] and many works were conducted with the aim of improving the performance of an algorithm by introducing specific diversity preservation strategies. As already mentioned the selective pressure driving toward optimal solutions can lead to a rapid impoverishment of the genetic material within the population, with consequent premature convergence of the algorithm. The strategies used in literature to prevent such a circumstance in GAs are many, in the following we briefly recall and describe the most relevant ones:•Crowding: it consists on the replacement of existing individuals on the parent population that present similarities on a genotypic viewpoint. The strategies differ for the way they select and replace the most similar elements inside the population. Deterministic [40,45] and probabilistic [46] crowding methods have been used. Other crowding algorithms are Metropolis algorithm [47], restricted tournament selection [48] and simulated annealing [49], which can be all considered local tournament algorithms [50].Niching: these techniques consist on promoting the formation within the population of stable sub-populations (niches) [51] which contain different genetic information. They are traditionally used in domains when the finding of multiple solutions is desired, such as in optimization of multi-modal functions. The most frequently used niching technique is the fitness sharing [52].Diversity as an objective: the idea of using the diversity as an objective during the evolution was first in [39]. The diversity, measured in terms of the Euclidean distance from the other individuals, is used in the ranking procedure thanks to a non-dominated sorting of (i) the diversity and (ii) the ranks scored with respect to the objectives of the original MOOP. In [53,54] instead the diversity is used as an additional objective, increasing the dimensionality of the original MOOP.Other methods: additional methods are available in literature like multiploidy [55], DCGA (diversity control oriented GA) [56], CSGA (complementary surrogate GA) [57], TMPGA (Tabu multi parent GA) [58], FUSS (fitness uniform selection scheme) [59] and other strategies not cited here.The importance of diversity has been highlighted also in some works where basic MAs (without the use of SMs) were used. In [60,61] the diversity has been used during the evolution of a parallel memetic algorithm (PMA) in order to dynamically control the local search frequency with the aim of reducing the number of function evaluations. On a similar fashion the individuals’ diversity has been considered in [62] with the only difference that here it is used to decide between three different local searchers of an adaptive MA. The works considered so far treat the diversity as an indirect parameter to adapt the local search strategy, which is not strictly a diversity preservation technique. On the contrary in [63] a MA is improved by direct population management removing solutions that are below a certain threshold in terms of distance from the other individuals. Again in [42] the population diversity is preserved in a bacterial MA by means of hibernation of individuals. The idea is to hibernate some bacteria for a while. Then the hibernated bacteria can help in directing back the evolutionary process to right way if it convergences to local optima. No works are known at the moment of writing about diversity preservation techniques implemented in SAMAs.In previous paragraph we stated the significance of preserving diversity within the evolutionary process in EAs and this is confirmed by the great importance that this subject has had in literature and the large number of works done on it. Diversity is considered as an important factor for classic GAs but it becomes crucial for MAs and SAMAs. These types of algorithms are specifically designed to emphasize the selective pressure mechanism toward the Pareto optimal set, thanks to the local search framework embedded. For this reason they are much more subject to premature convergence and impoverishment of the genotypic diversity within the population and a diversity preservation strategy is mandatory.The proposed optimization strategy consists in a SAMA strategy which is based on the genetic diversity preservation method GeDEA [39]. The GeDEA is here used as global searcher while additional tools are selected in order to set up a SAMA methodology. An ANN is selected as objective function's surrogate model and a Local Search Framework is built to perform local refinements of the individuals using the aforementioned metamodel. The main tools and their implementation are described in detail below.GeDEA is a multi-objective evolutionary algorithm based on the preservation of the diversity among the evolution of the individuals. The basic idea behind the methodology, the Genetic Diversity Evaluation Method (GeDEM), consists in treating the diversity as a driving objective during the evaluation phase, emphasizing the non-dominated solutions as well as the most genetically different. This results in a selection pressure driving the search simultaneously toward the exploitation of the current non-dominated solutions and to the exploration of the search space. Fig. 1shows the GeDEM's way of ranking the solutions.The ranks of the individuals are determined maximizing the original ranks scored with respect to the objectives of the original MOOP (useful for exploitation) and the values assigned to each individual as a measure of its genetic diversity (useful for exploration), calculated according to the chosen distance metric. In order to assign fitness to the individuals the GeDEM performs non-dominated sorting procedure among the two aforementioned objectives.GeDEA is here used as global searcher within the SAMA framework because of its ability in preserving the diversity as the population evolves, thus counterbalancing the peculiar tendency to exploitation of MAs.A surrogate-assisted strategy (for MAs) uses one or more surrogate models to perform a local refinement of the individuals that the global searcher (GeDEA in this case) provides. Several mathematical metamodels [65] are available to represent a n-dimensional function. In present work an Artificial Neural Network (ANN) is selected [66], since the following reasons:–It yields better approximations compared to the classical response surface methods when the nature of the problem is unknown, even for discontinuous functions;It can easily manage a large number of design parameters;It shows good responses if the boundaries of the design space are not well-defined;It has nominally unlimited representation capabilities of the function's complexity.A generic ANN is composed of basic elements, called neurons. Fig. 2shows a neuron structure: a vector input p, with a number Q of elements, is multiplied by the weights w and the solution is added to the bias b. The result is used as an input for the transfer function f, that provides the neuron's output a. The transfer function f can be of different nature, the most commonly used for multi-layer networks are the Tan-Sigmoid and the Linear Transfer Function.The neurons are connected together by a structure of links, as in a real biological nervous system. In an ANN the neurons are organized by layers with a finite number of them within each layer and the connections (weights), together with the biases, can vary their values to modify the response of the network. To reproduce a generic objective function the ANN requires a training, in which a set of starting individuals (stored in a database) are considered in relation with their scores. An iterative procedure changes the weights and the biases of the network until a sufficient approximation accuracy of the output data is reached.The original function is here represented as a global model, therefore the ANN uses all the available individuals in the database in order to be trained and tries to follow the function along the whole variability range of the parameters. In this case the model is unique and represents the entire function. The specific ANN used here is a feed-forward neural network which is composed of an input, two hidden layers with respectively 10 and 16 Tan-Sigmoid neurons, and an output layer with 10 linear neurons.Overfitting is one of the most important problems in ANN training, so to achieve a good generalization of the solution two techniques are employed. The first action to improve generalization is to use a network with the minimum number of neurons, just enough to adequate approximation. Larger networks can learn more complex problems but they have enough power to overfit the data. The second action is to use the available data to validate the network. The global database, containing individuals and scores from the true objective function evaluation, is divided into two subsets. The first one is the training subset, used to train the network. It contains the 75% of the data, that are chosen randomly. The remaining individuals composes the evaluation subset. The network is trained making use of the Levenberg–Marquardt algorithm (LM), a quasi-Newton method faster than the traditional back-propagation one. We use the mean square error (MSE) to measure the ANN performance, as defined in (1):(1)MSE=1MTDB∑i=1MTDB(Fi−FNNi)2where MTDBis the number of available individuals in the training subset. The iterative training process is stopped when the algorithm reaches 100 epochs or when the training performance index MSEtrainis lower than 5×10−7. Finally, the evaluation performance index MSEevalis calculated simulating the trained network on the evaluation subset. The training process is repeated more than once, in this case 3 times, because each training can evolve into a different model (since the random initialization of weights/biases and the random selection of the training subset) and only the surrogate model with the best (minimum) MSEevalis accepted. The ANN model used here has been developed and adapted from [67].The local refinement of the GeDEA individuals is performed using the Local Search Framework (LSF). The proposed method for local search makes use of a single metamodel (the ANN in Section 3.2) and a single local searcher. The latter is based on an active-set algorithm [68,69] that is a sequential quadratic programming (SQP) method in which an estimate of the Hessian of the Lagrangian is updated at every iteration using the BFGS formula.When the global searcher (GeDEA) is used for a multi-objective problem optimization the use of the local searcher (gradient-based algorithm) requires a specific treatment of the problem itself. Different strategies can be followed to handle this issue, like the conversion of the multi-objective problem into a weighted sum of objectives. This method lacks in generality and the selection of the weights by the user can be difficult, affecting the final results. A different technique is actually used here, which consists in the transformation of the multi-objective problem into a sequence of constrained single-objective problems. Each problem objective is minimized using the local searcher and its ANN approximation following a random sequence, while the remaining objectives are modified into inequality constraints. The new individual coming from the local refinement is accepted if, and only if, the selected objective has been improved respect to its original value and, contemporarily, the remaining objectives are lower than or equal to the original values.The GDMA algorithm can be described by the following main steps:Step 1: An initial number of generations using the GeDEA (GA) algorithm only, GGeDEA, are performed, with a population size of μ individuals;Step 2: First training of the ANN surrogate model, one for each objective, using the global set of individuals coming from the Step 1;Step 3: The population from the GeDEA crossover (and mutation) is passed to the LSF framework. During the first SAMA generation the individuals are locally improved by the gradient-based algorithm. If the original optimization problem is a multi-objective one, the objectives are improved one at a time, converting the others to inequality constraints;Step 4: The modified and improved population is now evaluated by the original fitness function to find the true score values. The scores return to the GeDEA algorithm, together with the locally improved population, replacing the old one;Step 5: If the maximum number of generations is reached then stop, otherwise the GDMA algorithm performs the ranking through the GeDEA operands, creates the mating pool for the crossover and repeats the operations from the Step 3. The algorithm periodically retrain the global surrogate model, every GNNtraingenerations, to improve and update the original fitness function approximation using the new available data.A systematic comparison of various MOEAs was originally provided in [70]. They compared the performance of eight algorithms (VEGA, HLGA, FFGA, NPGA, NSGA, SPEA, a random search algorithm and a single-objective EA using weighted-sum aggregation) on six test problems featuring the characteristics that may cause difficulties in converging to the Pareto optimal front and in maintaining diversity within the population [71]: convexity, non-convexity, discrete Pareto fronts, multimodality, deception and biased search spaces. The best overall results were obtained respectively with SPEA and NSGA. Further investigations demonstrated that an elitist variation of NSGA (the so-called NSGA-II) equals the performance of SPEA.The original version of GeDEA algorithm [39] was compared following a similar procedure, showing that peculiar feature of preserving the diversity within the population can highly help to reach better results respect to both SPEA and NSGA. GeDEA demonstrated improved performance in terms of convergence and Pareto front coverage for all the functions considered.In this section GDMA is tested on the same problems according to the same methodology. The results are compared using the same metric with the results coming from the original GeDEA and other state of the art evolutionary algorithms, SPEA2 [72] and NSGA-II [73], available for public use as part of the PISA framework at http://www.tik.ee.ethz.ch/sop/pisa. The test functions and the complete methodology will be briefly recall in the following chapters for clarity.The original methodology introduced by [70] involves six different functions, Tau1, …, Tau6, each one with a distinct feature, as identified by [71]. Each of those functions is a two-objective minimization problem constructed in the same way, according to the guidelines in [71]:(2)Minimizeτ(x)=(f1(x1),f2(x))subjecttof2(x)=g(x2,...,xn)h(f1(x1),g(x2,...,xn))wherex=(x1,...,xn)The function g controls the search space lateral to the Pareto-optimal front, while the function f1controls the search space along the Pareto-optimal front. The function h determines the shape of the front itself. This method makes it possible to investigate the problem features separately in order to assess whether an MOEA has the ability to converge to the true Pareto-optimal set and to find diverse Pareto-optimal solutions under particular conditions. Since the deceptive problems are not supported by the GDMA algorithm (due to the presence of a gradient-based algorithm inside the optimization loop), the Tau5 function in [70] is not considered and quoted here.•Test function Tau1 has a convex Pareto-optimal front:(3)f1(x1)=x1g(x2,...,xn)=1+9∑i=2nxi(n−1)h(f1,g)=1−f1gwhere n=30 and xi∈[0,1]. The Pareto-optimal front corresponds to g(x)=1.Test function Tau2 has a non-convex Pareto-optimal front:(4)f1(x1)=x1g(x2,...,xn)=1+9∑i=2nxi(n−1)h(f1,g)=1−f1g2where n=30 and xi∈[0,1]. The Pareto-optimal front corresponds to g(x)=1.Test function Tau3 has a discontinuous Pareto-optimal front consisting of several convex parts:(5)f1(x1)=x1g(x2,...,xn)=1+9∑i=2nxi(n−1)h(f1,g)=1−f1g−f1gsin(10πf1)where n=30 and xi∈[0,1]. The Pareto-optimal front corresponds to g(x)=1.Test function Tau4 involves a multi-modal problem:(6)f1(x1)=x1g(x2,...,xn)=1+10(n−1)+∑i=2n(xi2−10cos(4πxi))h(f1,g)=1−f1gwhere n=10, x1∈[0,1] and x2, …, xn∈[–5,5]. The Pareto-optimal front is convex and corresponds to g(x)=1.Test function Tau6 features non-uniformity of the search space:(7)f1(x1)=1−exp(−4x1)sin6(6πx1)g(x2,...,xn)=1+9∑i=2nxi(n−1)h(f1,g)=1−f1g2where n=10 and xi∈[0,1]. The Pareto-optimal front is non-convex and corresponds to g(x)=1.Several metrics can be considered to assess the performance of different EAs with respect to the different goals of optimization itself (refer to [70]): how far is the resulting non-dominated set from the true Pareto front, how uniform is the distribution of the solutions along the Pareto front, how wide is the Pareto front. Several metrics are used here. The first metric is a Pareto dominance-based technique, which is an assessment method based on pairwise comparisons of Pareto front sets, also known as Coverage of two sets or CTS metric [39,70]. The others are three Quality indicators[74] known as Generational Distance [75], Spacing Metric [73] and Maximum Pareto Front Error [75]. Following a brief explanation of the metrics used to assess the GDMA method.Coverage of two sets (CTS): Let X′, X″⊆N be two sets of decision vectors. The function CTS maps the ordered pair (X′, X″) to the interval [0,1]:(8)CTS(X′,X″)=|{x″∈X″;∃x′∈X′:F(x′)≺F(x″)}||X″|When CTS(X′, X″)=1 the set X″ is entirely covered by the solutions in X′. On the other hand, if CTS(X′, X″)=0, none of the solutions in X″ is covered by the set X′. The value of CTS(X′, X″) is not necessarily equal to 1−CTS(X′, X″) and both the possible ordered pairs are considered during the comparison. The selected metric, when applied to two non-dominated sets from different EAs, gives the percentage of solutions of the set X″ that are covered by solutions of the set X′, although superiority is not estimated.Generational Distance (GD): measures the root-mean-square of the distance of points in the approximation set from their nearest point in the true Pareto front. It is to be minimized.Spacing Metric (SP): measures the average discrepancy between the spacing of consecutive points and the mean spacing of consecutive points. It is to be minimized.Maximum Pareto Front Error (MPFE): measures the largest distance between a vector in the current Pareto front and the corresponding closest point in the true Pareto front. It is to be minimized.The three quality indicators are calculated for all the 30 runs of each algorithm (see §4.3 for the methodology description) and statistical representations of them are plotted to compare the algorithms’ performance.

@&#CONCLUSIONS@&#
