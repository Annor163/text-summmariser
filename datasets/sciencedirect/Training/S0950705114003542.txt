@&#MAIN-TITLE@&#
Joint sparse regularization based Sparse Semi-Supervised Extreme Learning Machine (S3ELM) for classification

@&#HIGHLIGHTS@&#
A joint sparse regularizer is employed to prune ELM.A semi-supervised strategy is used to exploit the information of unlabeled samples.S3ELM is proposed to solve the pruning ELM model.The proof of the convergence of S3ELM is shown.

@&#KEYPHRASES@&#
Sparse semi-supervised learning,Extreme learning machine,ℓ,2,,,1,-Norm,Joint sparse regularization,Laplacian,

@&#ABSTRACT@&#
Extreme Learning Machine (ELM) has received increasing attention for its simple principle, low computational cost and excellent performance. However, a large number of labeled instances are often required, and the number of hidden nodes should be manually tuned, for better learning and generalization of ELM. In this paper, we propose a Sparse Semi-Supervised Extreme Learning Machine (S3ELM) via joint sparse regularization for classification, which can automatically prune the model structure via joint sparse regularization technology, to achieve more accurate, efficient and robust classification, when only a small number of labeled training samples are available. Different with most of greedy-algorithms based model selection approaches, by usingℓ2,1-norm, S3ELM casts a joint sparse constraints on the training model of ELM and formulate a convex programming. Moreover, with a Laplacian, S3ELM can make full use of the information from both the labeled and unlabeled samples. Some experiments are taken on several benchmark datasets, and the results show that S3ELM is computationally attractive and outperforms its counterparts.

@&#INTRODUCTION@&#
Extreme Learning Machine (ELM) [18,19] is a special type of Single-hidden Layer Feedforward Neural network (SLFN) that has received increasing interests in the machine learning community. Compared to traditional SLFNs, ELM randomly generates the connected weights between input nodes and hidden nodes. Therefore, only the linearly connected weights in the output layer should be tuned in ELM, which makes the learning of ELM simple and efficient. A large number of numerical experiments show that ELM can obtain similar performance to (even better performance than) SVM in both classification and regression tasks [8].Nowadays ELM has been extensively studied in theory and application. For example, the universal approximation theorem of ELM is researched in [15,13,14]; [17,4,26] focus on the enhancing robustness and generalization of ELM. Kernel based ELM is developed in [17,9] by defining the inner product of hidden layer output vectors of two samples as a kernel function. Ensemble ELM is studies by [21,11,27,35], and [25,36] extended ELM to semi-supervised learning. In [21,24,23] the authors proposed some online sequential ELM algorithms for ELM. Additionally, ELM has found many successful applications in various engineering fields, including the classification in P2P networks [35], indoor location estimation based on Wi-Fi [25], sales forecasting [34], protein sequence classification [37], and human action recognition [28].According to learning theory [2], model selection is an important issue because too simple and too complex models will result in degraded generalization. Although ELM is characteristic of simple principle, low computational cost and excellent performance, in practical applications the number of hidden nodes should be manually tuned for better learning and generalization capability. Some researchers have taken efforts on the constructive model selection of ELM automatically [16], which can be split into three categories:(1)Nodes Growing Method (NGM). For NGM, model selection is realized by increasing the random hidden nodes step by step [15,13,7,22]. In [15], when a new hidden node is added into ELM network, all the existed weight connecting coefficients remain unchanged. The weight connecting coefficient of the added node is obtained by minimizing the approximation error and the algorithm stops when training error is less than a threshold. Similar with [15,13] also increases the random hidden nodes step by step. The difference between them is that [13] calculates the weight connecting coefficient of the added node by minimizing the approximation error in a special line segment. The characteristic in [13] is that the obtained weight connecting coefficients are also updated when calculating the weight connecting coefficient of the added node. In [14], multiple hidden nodes are generated randomly and the best hidden node among them is added into ELM network. Therefore, this method can obtain the better approximate performance for the same scale of the network. The method in [7] adds the multiple hidden nodes at the same time, and updates the weight connecting coefficients by using generalized inverse which makes ELM a stronger approximation performance. Meanwhile, this method becomes an online learning method by efficient matrix operations. The method in [22] is similar to [14], which generates the multiple hidden nodes as the set of the candidate nodes. When the ELM network is updated, the node in the candidate set which has maximum correlation with the current residual is selected and added the network. Then the weight connecting coefficients are optimized. Similarly, when training error is less than a threshold, this algorithm stops.Nodes Pruning Method (NPM). NPMs generate the sufficiently large ELM network randomly, and then adjust the structure by pruning strategies [26,32,29]. In [32], Chi-squared (χ2) method and information gain criterion are used to measure the degree of relevance between the hidden nodes and the class labels. Meanwhile, threshold method is used for pruning the less relevant hidden nodes. In [29], hidden nodes are sorted by Multi-Response Sparse Regression (MRSR) method. Then the least relevant hidden node is deleted one by one. The algorithm will not stop until the classification/regression accuracy reduces which is measured by leave-one-out (LOO) criterion. However, the method in [29] often encounters potential numerical risks. To fix this problem, Tikhonov regularization is applied by the pruning algorithm in [26], which has improved stability greatly.Hybrid Method (HM). HMs combine the above two strategies [20]. In [20], multiple hidden nodes are randomly generated at first by the same method in [14]. Then the redundant hidden nodes are deleted using the following methods: each hidden node is deleted from the network once in turn, and the remaining hidden nodes are used to construct the new ELM network and the new weight connecting coefficients are trained. The LOO error corresponding to each deleted hidden node is calculated and the hidden node with minimum error is considered as alternative deleting item. Compared to the current LOO error, if the new LOO error does not increase, this hidden node will be deleted. Otherwise, it will be kept and the algorithm is terminated.All the above model selection methods are based on greedy algorithms, which will inevitably trapped into local minimum in determining the number of hidden nodes. Although some methods, such as the methods in [14,22,20], try to reduce the negative effect from greedy strategies by enlarging the size of candidate sets of hidden nodes, they may still fall into local optimal. In order to fix this problem, we impose a joint sparse constraints on linearly connected weights and formulate a convexℓ2,1-norm regularizer in training an ELM. Therefore, the pruned nodes can be jointly selected by minimizing thisℓ2,1-norm. (The joint sparse property ofℓ2,1-norm has been described in Section 2.2.) Moreover, a semi-supervised regularizer is introduced to utilize both the labeled and unlabeled samples. Then, we propose a Sparse Semi-Supervised Extreme Learning Machine (S3ELM) that can automatically prune the model structure via joint sparse regularization technology. To be clearer, we highlight the contributions of this paper as follows:•A joint sparse regularizer is employed to prune ELM.A Semi-supervised strategy is used to exploit the information of unlabeled samples.S3ELM is proposed to solve the pruning ELM model.The proof of the convergence of S3ELM is shown.Some experiments are taken on several benchmark datasets, and the results show that S3ELM is computationally attractive and outperforms its counterparts.The rest of the paper is organized as follows. Section 2 describes the proposed S3ELM in detail. Theoretical analyses on the convergence and computational complexity of S3ELM provides in Section 3. The experimental results and analyses are provided in Section 4. Finally, the conclusions are presented in Section 5.In this section, three related ELMs methods are briefly described, including the original ELM [17,4,7], Optimally Pruned ELM (OP-ELM) [29], and Semi-supervised ELM (SELM) [25]. They are closely related with S3ELM. So, we briefly review them to facilitate introducing S3ELM and comparison of S3ELM with them.Assume there are n labeled samples(1)L={(xi,yi):i=1,2,…,n},wherexi=xi1,xi2,…,xiDT∈RD,(i=1,2,…,n)is the i-th training sample, D is the dimensionality of samples;yi=yi1,yi2,…,yiCT∈RC,(i=1,2,…,n)is the label ofxi, which satisfiesyij∈{0,1}(j=1,2,…,C),∑j=1Cyij=1; and C is the number of the classes. The outputs of the hidden layer are(2)h(x)=h1(x),h2(x),hL(x),with eachhi(x)=G(ai,bi,x),(i=1,2,…,L)is the output of i-th hidden nodes,aiis the D-dimensional column vector which is randomly generated according to some continuous distribution, andbiis a scale by the same method asai. G can have various forms, such as Sigmoid function:(3)G(a,b,x)=11+exp-(aTx+b)or Gaussian function:(4)G(a,b,x)=exp-b‖a-x‖2.More choice of G can be found in [17]. Usingh(x), the hidden layer output matrix corresponding to labeled training samples is defined as follows:(5)H^=h(x1)⋮h(xn)=h1(x1)⋯hL(x1)⋮⋮⋮h1(xn)⋯hL(xn),and the connecting weights matrix is defined as:(6)W=w1⋮wL=w11⋯w1C⋮⋮⋮wL1⋯wLC.To learnW, the labeled matrix is also needed to be defined, that is:(7)Y^=y1⋮yn=y11⋯y1C⋮⋮⋮yn1⋯ynC.ELM optimizes weight matrix by minimizing both the training error‖H^W-Y^‖F2and the norm of the weights‖W‖F2[7], where‖·‖F2denotes Frobenius-norm. The two terms are combined as a form of weighted sum and minimized to deriveWby [17,4], i.e.,(8)minWγ2‖H^W-Y^‖F2+12‖W‖F2,whereγis the parameter which balances the training error and the complexity of model. The analytical solution of formula (8) is:(9)W=1γIL+H^TH^-1H^TY^,whereILisL×Lidentity matrix. For a testing samplex∈RD,y=h(x)Wis the predicting label vector, whereh(x)is defined by Eq. (2). If thec∗-th element ofyis larger than others elements, the testing samplexis determined to belong toc∗-th class. Eq. (9) is only efficient whenn⩾L. Whenn<L, an equivalence formula to solveWcan be obtained by Woodbury formula:(10)W=H^T1γIn+H^H^T-1Y^,whereInisn×nidentity matrix. It is clear that calculating the inverse matrix ofn×nmatrix is faster than calculating the inverse matrix ofL×Lmatrix.A large number of numerical experimental results in [17] show that ELM is an effective classification/regression method. For many UCI data sets [8], ELM obtains similar performance to (or even better performance than) SVM. The authors of [17] also illustrate that ELM can receive very stable classification/regression accuracy when the number of hidden nodes increases to 1000.A large scale ELM network is chosen at first by OP-ELM [29], and then the importance of hidden nodes are ranked according to MRSR [33]. Finally, the most unimportant hidden nodes are deleted one by one until LOO error does not decrease any more.In detail, assumet-1nodes have been chosen in(t-1)-th iteration, and approximate matrix ofW(denoted asWt-1) are obtained. Ifj∗satisfy the following constraint,(11)j∗=argmaxj‖hjT(Y^-H^t-1Wt-1)‖1:hjisnotthecolumnofH^t-1,hj∗=hj∗(x1),hj∗(x2),…,hj∗(xn)Tis the best hidden node according to MRSR criterion in current iteration. Thenhj∗is added into the matrixH^t-1. The new matrix is denoted byH^t. Then the weight connecting coefficient matrix is updated byWt=(H^t)+Y^.After all the hidden nodes are added into ELM network, the ranking operation is finished. In the pruning stage, the least important nodes are deleted in turn. When LOO error increases, the algorithm terminates. When the weight connecting coefficient matrix is obtained, OP-ELM uses the same method to classify the testing sample with original ELM.Assume that(12)U={xi∈RD:i=n+1,n+2,…,N}is unlabeled training samples and labeled training sample is defined in Eq. (1). The hidden layer output matrix which consists of labeled and unlabeled training samples is:(13)H=h(x1)⋮h(xN)=h1(x1)⋯hL(x1)⋮⋮⋮h1(xN)⋯hL(xN).The correspondingN×Clabel matrix is,(14)Y∼=y1⋮yn0⋮0=y11⋯y1C⋮⋮⋮yn1⋯ynC0⋯0⋮⋮⋮0⋯0=Y^0.In addition, aN×Nmatrix is defined as follows:(15)J=In000,Jis used for indicate labeled training samples.In [25], the following programming is used for learning weight connecting coefficient matrix,(16)minβ12‖JHW-Y∼‖F2+λ(HW)TL(HW),whereLN×Nis the graph Laplacian, which is defined by(17)L=D-A,whereAis affinity matrix whose elements at i-th row and j-th column isaij=exp(-σ‖xi-xj‖2),(i≠j,i,j=1,2,…,N). All the main diagonal elements ofAare 0. The main diagonal elements of diagonal matrixDsatisfydii=∑i=1Naij,(i=1,2,…,N). More details about the graph Laplacian and semi-supervised learning can be seen in [25,1].The following approximate formula in [25] is used to solve the optimal formula (16):W=(J+λLT)H+JY∼.It may be adopted by [25] for saving the computational cost. Nevertheless, the above the formulation can often obtain outstanding classification/regression performance. Meanwhile, the accurate formula should be:W=HT(J+λLT)H+HTJY∼.After calculatingW, SELM uses the same method to determine the class of the testing sample with original ELM and OP-ELM.The original ELM adopts Frobenius-norm as the regularization, which controls the model complexity and ensures ELM has good generalization ability. However, Frobenius-norm is not a sparse regularization norm with which ELM networks are pruned difficultly. In order to introduce some joint sparse property, in our work we use a newℓ2,1-norm [30] ofW=(wij)L×Cto prune the redundancy nodes of ELMs, which is defined as,‖W‖2,1=∑i=1L∑j=1Cwij2=∑i=1L‖wi‖2=‖v‖1,wherev=(‖w1‖2,…,‖wL‖2)T. In comparison withℓ2-norm,ℓ1-norm encourages small elements ofvto be zeros. Because the elements ofvare theℓ2-norm of row vectors ofW, if some element ofvbecomes zero, the corresponding row ofWbecomes zero vector. So,ℓ2,1-norm has the row-sparsity, namely joint sparse property.Therefore, the minimization ofℓ2,1-norm ofWcan also improve the generalization ability and prune hidden nodes of ELM. Ifℓ2,1-norm is used instead of Frobenius-norm, formula (8) is converted into:(18)minW12‖H^W-Y^‖F2+λ‖W‖2,12,whereλis the parameter for balancing the training error and complexity of the model.H^,W,Y^are defined according to formulae (5)–(7). The pruning method used by formula (18) is similar with OP-ELM [29], both of which utilize the joint properties of some certain rows (or columns) in a matrix to prune. Their differences are that OP-ELM prunes the hidden nodes according to the correlation of the current residual vector and the columns of hidden layer output matrixH^, while formula (18) depends on the row-sparsity ofℓ2,1-norm. In comparison to OP-ELM, the formula (18) is convex which can avoid local optimal solution.In the ELM-based classification, a large number of labeled instances are often required to achieve accurate classification. However, the labeled samples may hardly obtain for some practical application. It is a real issue how to guarantee the classification accuracy under the condition of the small size of the labeled samples. Similar to [25], we extend our method to semi-supervised algorithm which can make full use of unlabeled training samples to improve the recognition rate. Our model is formulated as:(19)minW,Y‖HW-Y‖F2+λ‖W‖2,1+τ(1-μ)Tr(YTLY)+μ‖J(Y-Y∼)‖F2,whereTr(·)represents the trace of matrix,λ>0,τ>0,μ∈[0,1]are the parameters andH,Y∼,J,Lare defined by formulae (13)–(15), (17), respectively.Yis the approximation matrix of the real label matrix which is represented as follows:Y=y¯11⋯y¯1C⋮⋮⋮y¯N1⋯y¯NC=y¯(1),…,y¯(C).In comparison to formula (18), the third termTr(YTLY)=∑i=1C[y¯(i)]TLy¯(i)in formula (19) makes the matrixYas consistent with the manifold structure of data as possible and the fourth term‖J(Y-Y∼)‖F2makes the first n rows of the matrixYas similar to the first n rows of the matrixY∼as possible. The matrixJcan eliminate errors from the rest rows ofYandY∼.As there are two matrixesYandWwhich need to be optimized for formula (19), we adopt Alternating Direction Method (ADM) to do it. In detail, we minimize the following two objective function alternately:(20)minY‖HW-Y‖F2+τ(1-μ)Tr(YTLY)+μ‖J(Y-Y∼)‖F2,and(21)minW‖HW-Y‖F2+λ‖W‖2,1.WhenWis fixed, the second term in formula (19) are constants aboutY. Therefore, formula (19) is converted to formula (20). LetG1(Y)=‖HW-Y‖F2+τ(1-μ)Tr(YTLY)+μ‖J(Y-Y∼)‖F2.G1(Y)is converted into the following formula:G1(Y)=Tr(HW-Y)T(HW-Y)+τ(1-μ)Tr(YTLY)+τμTr(Y-Y∼)TJTJ(Y-Y∼)=Tr(WTHTHW)+Tr(YTY)-2Tr(YTHW)+τ(1-μ)Tr(YTLY)+τμTr(YTJTJY)+τμTr(Y∼TJTJY∼)-2τμTr(YTJTJY∼).We calculate the following matrix derivative and set it to be equal to zero matrix: (please refer to the Appendix C in [2] for more details about the matrix derivative)∂G1(Y)∂Y=2Y-2HW+2τ(1-μ)LY+2τμJTJY-2τμJTJY∼=0.We have(22)I+τ(1-μ)L+τμJTJY=HW+τμJTJY∼.Then the analytical solution of formula (20) is(23)Y=I+τ(1-μ)L+τμJTJ-1(HW+τμJTJY∼).It should be noted that bothLandJTJare symmetric positive semi-definite matrix. Hence,I+τ(1-μ)L+τμJTJis positive definite matrix and its inverse in formula (23) exists.WhenYis fixed, the third and forth terms in formula (19) are constants aboutW. Therefore, formula (19) can be converted into formula (21). The objective is minimizing formula (21). Asℓ2,1-norm is not differentiable in all the spaceRL×C, it is difficult to minimize formula (21). However, Nie et al. proposed an effective optimal method in [30] which has been also applied to various feature selection problems [30,10,12,40]. In this paper, this method is utilized for optimizing programming (21). Assume thatG2(W)=‖HW-Y‖F2+λ‖W‖2,1which has the following deformation:G2(W)=Tr(HW-Y)T(HW-Y)+λ‖W‖2,1=Tr(WTHTHW)+Tr(YTY)-2Tr(YTHW)+λ‖W‖2,1.Whenwi≠0(∀i=1,2,…,L)(please refer to formula (6) for the definition ofwi), We calculate the following matrix derivative and set it to be equal to zero matrix:(24)∂G2(W)∂W=2HTHW-2HTY+2λUW=0,whereU=diag12‖w1‖2-1,…,12‖wL‖2-1is a diagonal matrix. Solving the deformation equation of Eq. (24)(HTH+λU)W=HTY,we have(25)W=(HTH+λU)-1HTY.As the elements inUis the variables aboutwi(∀i=1,2,…,L)inW, Eq. (25) is not a real solution of formula (21). However, it can used to form an iterative algorithm and solve the formula (21). That is:Wt+1=(HTH+λUt)-1HTY,whereWt+1is the weight connecting coefficient matrix which is obtained at the(t+1)-th iteration,Ut=diag12‖w1t‖2-1,…,12‖wLt‖2-1,w1t,…,wLtare the row vectors ofWt. To sum up, the proposed S3ELM is described in detail in Algorithm 1.Algorithm 1S3ELM algorithm.1: Input the labeled training setLand unlabeled training setUaccording to the formulae (1) and (12). ThenY^,H,Y∼,JandLare calculated according to the formulae (7), (13)–(15), (17). Set the parametersλ>0,τ>0,μ∈[0,1], terminal toleranceε>0and maximum number of iterationstmax>0. Initialize weight connecting coefficient matrix byW0=H+Y^, and sett=0.2: Update the label approximation matrix by the formula introduced in the second paragraph of this subsection,(26)Yt+1=I+τ(1-μ)L+τμJTJ-1(HWt+τμJTJY∼).3: Update the weight connecting coefficient matrix using the method which is described in the third paragraph of this subsection:(27)Wt+1=(HTH+λUt)-1HTYt+1,where(28)Ut=diag12‖w1t‖2-1,…,12‖wLt‖2-1,w1t,…,wLtare the row vectors ofWt.4: When‖Wt+1-Wt‖F2⩽εor the iterations exceed the maximum number of iterationstmax, the algorithm terminates; otherwise, go to step 2.Various methods can be used to delete the unnecessary hidden nodes by S3ELM. For example, the method of sorting can be used to select features, which is introduced by [10,12]. For the problem in this paper, the method of sorting can be used according to the following steps:Firstly, the weight connecting coefficient matrixWt∗is calculated (we assume that the S3ELM terminates at thet∗-th iteration). Secondly,‖w1t∗‖2,…,‖wLt∗‖2is calculated. Thirdly, the columns ofHcorresponding to the largest severalℓ2-norms among‖w1t∗‖2,…,‖wLt∗‖2are kept and others are pruned. We use the symbolH‾to represent the new hidden layer output matrix. Finally, the final weight connecting coefficient matrix is calculated byW=H‾+Yt∗.However, another method is applied to prune the hidden nodes instead of the method of sorting by S3ELM. The new method is described as follows:Let the current hidden layer output matrix isHt-1. Firstly,‖w1t‖2,…,‖wLt‖2are calculated at the current iteration. If anyℓ2-norm among‖w1t‖2,…,‖wLt‖2is lower than a given thresholdε̃, it is considered as a zero vector approximately. So, the column ofHt-1corresponding to thisℓ2-norm is considered to have insignificant effect to the final classification. Therefore, these columns ofHt-1and corresponding rows ofWtare deleted. The new hidden layer output matrix and weight connecting coefficient matrix are denoted byHtandWt, respectively. Thus, the pruning operation has done at current iteration. In all this paper, the threshold is set byε̃=10-5.The new pruning method has three advantages: (1) With the descending size ofHtandWt, the computational cost in Steps 2 and 3 is greatly reduced. (2) When some of‖w1t‖2,…,‖wLt‖2tend to 0, there may be some numerical difficulties whenUtis calculated. However, deleting the row vector with smallℓ2-norm can overcome the numerical problem. (3) Using the new pruning method, S3ELM can adaptively select pruned hidden nodes. So, the new pruning method can reduce the number of parameters. The new pruning method still has some shortage, i.e. it may clear some useful hidden nodes. However, the probability that useful hidden nodes are cleared incorrectly is very low. The numerical experiments also illustrate that S3ELM obtains perfect recognition rate with the new pruning method.Assume that after pruning operation,WS×Ct∗(S⩽L)represents the weight connecting coefficient matrix, hidden layer output matrix isH‾N×S, and the corresponding hidden layer output ishi1(xj),hi2(xj),…,hiS(xj),(j=1,2,…,N). The unlabeled training samples can be classified by the lastN-ncolumns ofY∗=H‾Wt∗. For a testing samplex, the hidden layer output vectorh(x)=hi1(x),hi2(x),…,hiS(x)is calculated, and then the final label is determined by the vectory=h(x)Wt∗.As there are many parameters in formula (19), for example,λ,τ,μwhich make a trade off among the training error, the error from the manifold structure of the training samples and complexity of model, initial number of hidden nodes L, the terminal parametersεandtmax, andσwhich is used to build the graph LaplacianL(please refer to Eq. (17) and immediate statement for more details aboutσ). It is obvious that we cannot adjust all of them by hand in numerical experiments because of the huge computational cost. Hence, we fix some parameters as follows:λ=10-4,τ=200,ε=10-5,tmax=200. In numerical experiments,μandσare adjusted by hand, which will be represented in detail in Section 4.Note that if we imposeHW=Ywhich is used by SELM [25], the model of formula (19) can be converted into the follow form:(29)minW‖W‖2,1+τλ(1-μ)Tr(HTWTLHW)+μ‖J(HW-Y∼)‖F2.Using the similar derivation method in the third paragraph of Section 2.4, formula (29) can be optimize by the following iterative formula:Wt+1=τμτ(1-μ)HTLH+τμHTJTJH+λU-1HTJTJY,whereUtis defined according to Eq. (28). Although formula (29) has many advantages, such as less parameters and simpler solving algorithm, we still using programming (19) in this paper. There are two reasons for it. (1) S3ELM aims at pruning the hidden nodes of ELM network. IfYmust not be equal toHW, it is more flexible to optimizeW, which restricts more rows ofWto be equal to zero vectors by using‖W‖2,1. So, smaller size of ELM network can be obtained by using formula (19). (2) We can convert formula (19) into an equivalent formation:minW,Y1λ‖HW-Y‖F2+‖W‖2,1+τλ(1-μ)Tr(YTLY)+μ‖J(Y-Y∼)‖F2.If letλ→0andτ→0while the valueτλis kept unchanged,1λ‖HW-Y‖F2will imposeHW=Y. So, it can be considered that the formula (29) is a special case of the formula (19). We think that a more general model is more meaningful in comparison to formula (29).Compared to the original ELM, although the computational cost of S3ELM in training stage is expensive, its computational cost in testing stage reduces greatly. In the following part, the computational cost of S3ELM in training and testing stage is analyzed.Eqs. (26) and (27) have to be calculated at the current iteration. For Eq. (26),τ(1-μ)L,τμJTJ,HWt,τμJTJY∼and inverse matrixI+τ(1-μ)L+τμJTJ-1need be calculated. The computational cost forτ(1-μ)LisO(N2). SinceJis a diagonal matrix, the computational costs forτμJTJandτμJTJY∼areO(N)andO(NC), respectively. Meanwhile, it needsO(NLC)floating-point operation to calculateHWt. The computational cost for finding the matrix inverseI+τ(1-μ)L+τμJTJ-1isO(N3). At last, the computational cost of the operation thatI+τ(1-μ)L+τμJTJ-1is multiplied byHWt+τμJTJY∼isO(N2C). As the number of training samples N is much larger than class C usually, the computational cost for updatingYisO(N3)+O(NLC).If we use partially connected graph to compute Laplace matrixL,Lbecomes a very sparse matrix. Moreover, as bothIandJTJare diagonal matrix,I+τ(1-μ)L+τμJTJis still very sparse. If we updateYby solving the following linear system of equationsI+τ(1-μ)L+τμJTJY=HWt+τμJTJY∼,some available methods (e.g. Gaussian elimination) can be used to reduce its computational cost greatly.When calculating Eq. (27), we need to computeHTH,Ut,HTYt+1and matrix inverse(HTH+λUt)-1. The computational cost forHTHisO(N2L), while it needsO(NC)floating-point operation to computeUt. The computational cost forHTYt+1and(HTH+λUt)-1areO(NLC)andO(L3), respectively. Finally, the computational cost of the operation that(HTH+λUt)-1is multiplied byHTYt+1needsO(L2C). As the number of hidden layer nodes L is far greater than C usually, the computational cost to updateWneedsO(L3)+O(N2L).IfL>Noccurs in practical problem, we can utilize equation Woodbury formula to generate a new updating equation forWas follows:(30)Wt+1=1λ(Ut)-1-(Ut)-1HTλI+H(Ut)-1HT-1H(Ut)-1HTYt+1.Similar to the above analysis, the computational cost of Eq. (30) isO(L2N)which is far small thanO(L3)+O(N2L). Another benefit of Eq. (30) is that it uses(Ut)-1=diag(2‖w1t‖2,…,2‖wLt‖2)instead ofUt=diag12‖w1t‖2-1,…,12‖wLt‖2-1in Eq. (27). So, if some rows ofWtbecome zero vectors, Eq. (30) is not affected. However, we select Eq. (27) updateWin the numerical experiments because the performance of S3ELM degenerates on some data sets when Eq. (30) is applied. We still do not know the reason for it which will be researched as one of our future works.We have analyzed the main computational cost of S3ELM in training stage. In addition, because the pruning strategy in Section 2.5 is applied to S3ELM, the size ofHtandWtare decreasing with the increasing t. So, the total computational cost of S3ELM in training stage is lower usually. With the fact thatN>C, the overall computational cost of S3ELM in training stage is concluded as following theorem:Theorem 1Assume that S3ELM stops att∗-th iteration, the computational cost of S3ELM in training stage is not more thant∗[O(N3)+O(L3)+O(N2L)].□Now, the computational cost of S3ELM in testing stage is analyzed. Assume that there are S hidden nodes in ELM network after using S3ELM, i.e. for a testing samplex, the hidden layer output vector ish(x)=hi1(x),hi2(x),…,hiS(x). According to Eqs. (2)–(4), the computational cost forh(x)isO(SD), where D is the dimension of data. Moreover, the computational cost fory=h(x)Wt∗isO(SC). Therefore, we conclude the analysis about the computational cost of S3ELM in testing stage as following theorem:Theorem 2Assume that there are S hidden nodes in ELM network pruned by S3ELM, the computational cost in testing stage isO(SC)+O(SD).□Compared with original ELM, the computational cost of S3ELM in testing stage can be reduced greatly ifS≪L. In addition, the storage cost can also be saved greatly.In this subsection, the convergence analysis of S3ELM is presented. The adopted analysis method is similar to the method in [30,10,12,40]. At first, a lemma should be given as follows:Lemma 1[40]Assume thatwit,wit+1(i=1,2,…,L)are two arbitrary nonzero vectors, then(31)∑i=1L‖wit+1‖2-∑i=1L‖wit+1‖222‖wit‖2⩽∑i=1L‖wit‖2-∑i=1L‖wit‖222‖wit‖2.□Assume thatG(W,Y)is the objective function of formula (19), that isG(W,Y)=‖HW-Y‖F2+λ‖W‖2,1+τ(1-μ)Tr(YTLY)+μ‖J(Y-Y∼)‖F2.Then we will use three theorems to show thatG(W,Y)decreases monotonically.Theorem 3Assume thatWt,Yt(t=1,2,…)are two matrix sequences which are generated by S3ELM, then(32)G(Wt,Yt+1)⩽G(Wt,Yt).According to the analysis of Section 2.4, it is known thatYt+1obtained by formula (26) is the optimal solution of the following programming:minY‖HWt-Y‖F2+τ(1-μ)Tr(YTLY)+μ‖J(Y-Y∼)‖F2.Therefore,‖HWt-Yt+1‖F2+τ(1-μ)Tr(Yt+1)TLYt+1+μ‖J(Yt+1-Y∼)‖F2⩽‖HWt-Yt‖F2+τ(1-μ)Tr(Yt)TLYt+μ‖J(Yt-Y∼)‖F2.Ifλ‖Wt‖2,1is added to the both sides of inequality, we haveG(Wt,Yt+1)⩽G(Wt,Yt).□Assume thatWt,Yt(t=1,2,…)are two matrix sequences which are generated by S3ELM, then(33)G(Wt+1,Yt+1)⩽G(Wt,Yt+1).It is obvious thatWt+1calculated by Eq. (27) is the optimal solution of the following programming:minW‖HW-Yt+1‖F2+λTr(WTUtW).Therefore,‖HWt+1-Yt+1‖F2+λTr(Wt+1)TUtWt+1⩽‖HWt-Yt+1‖F2+λTr(Wt)TUtWt.That is:‖HWt+1-Yt+1‖F2+λ∑i=1L‖wit+1‖222‖wit‖2⩽‖HWt-Yt+1‖F2+λ∑i=1L‖wit‖222‖wit‖2.The above inequality is deformed equivalently as:‖HWt+1-Yt+1‖F2+∑i=1L‖wit+1‖2-∑i=1L‖wit+1‖2-λ∑i=1L‖wit+1‖222‖wit‖2⩽‖HWt-Yt+1‖F2+∑i=1L‖wit‖2-∑i=1L‖wit‖2-λ∑i=1L‖wit‖222‖wit‖2.Using inequality (31), we have:‖HWt+1-Yt+1‖F2+∑i=1L‖wit+1‖2⩽‖HWt-Yt+1‖F2+∑i=1L‖wit‖2.Ifτ(1-μ)Tr(Yt+1)TLYt+1+μ‖J(Yt+1-Y∼)‖F2is added to the both sides of inequality, inequality (33) can be obtained.□Theorem 5 can be shown easily by using Theorems 3 and 4.Theorem 5Assume thatWt,Yt(t=1,2,…)are two matrix sequences which are generated by S3ELM, thenG(Wt+1,Yt+1)⩽G(Wt,Yt).□Now, we have proved thatG(Wt,Yt)decreases monotonically as long as two matrix sequencesWt,Yt(t=1,2,…)are generated by S3ELM.In this section, the performance of S3ELM is validated by several numerical experiments. Several related methods are used to compare with our proposed method, including, Original ELM,OP-ELM and SELM, which have been described in Section 2.1.Nine data sets (or databases) are used to test the S3ELM. Firstly, three artificial data sets with nice manifold structures are employed to illustrate the function of graph Laplacian item. Secondly, S3ELM is test by four UCI data sets which shows the performance of S3ELM for classification problem from the real world. In the last, the two face databases are applied to test S3ELM because of the following three reasons: (I) it illustrates that S3ELM is effective for image data sets, (II) it shows that S3ELM is effective for multi-class classification problem since the face databases usually have more than ten classes, and (III) it illustrates that the good performance can be obtained by S3ELM only when the graph LaplacianLshould be calculated by different methods catering for the different data sets.There are three artificial data sets and four UCI data sets [8] which are test in the immediately following two subsections. Table 1shows the basic information of seven data sets, while Fig. 1illustrates the distribution of three artificial data sets. For the seven data sets, we normalize the values of each feature in[-1,1], while they has zero mean.In Section 4.4 two face databases, i.e. JAFFE face database [42] and CMU PIE face database [43], are used for testing the performance of S3ELM applying to the image data. JAFFE face database contains 213 images of ten Japanese female models. There are about 21 frontal face images shot under the different facial expressions and lighting conditions for each model. Please refer to [42] for more details of JAFFE face database. CMU PIE face database contains more than 40,000 images from 68 persons. These images show the 13 different poses, 41 different illuminations and 4 different expressions of models. As the number of images in CMU PIE face database is so large, we just select 1470 images corresponding to the first 30 persons which belong to c05 data sets of CMU PIE face database in this experiment. Please refer to [43] for more details of CMU PIE face database and c05 data sets. All face images in the experiment are cropped and resized to order32×32. Then they are converted to the column vectors. The Principal Component Analysis method (PCA) is used to reduce the dimensionality of these column vectors, which keeps the 99% energy. Please refer to the [38] (Section 4 in [38]) for more details about the PCA dimensionality reduction of face images.In Sections 4.2 and 4.3, the two parameters of S3ELM are searching in the gridμ∈{0.1,0.2,…,0.9},σ∈{4-5,4-4,…,45}. The normalized parameter of original ELM isγ∈{4-10,4-9,…,410}[17,4]. OP-ELM uses the default parameters (Matlab codes of OP-ELM are available in [41]). For SELM, the two parametersσandλare selected from the gridσ∈{4-5,4-4,…,45}andλ∈{0.1,0.2,…,0.9}(please refer to formula (16) and (17) for more details aboutλandσ). All of the three methods use Sigmoid function and RBF function as the activation function G (please refer to formulae (3) and (4) for more details), respectively.In Section 4.4, a new method to construct the graph LaplacianLis applied which is suitable for face data and need not any parameters. So, S3ELM and SELM need not the parameterσin Section 4.4. other parameters are adjusted according to the last paragraph. The new method to construct the graph LaplacianLwill be introduced in Section 4.4.In this test, three artificial data sets and four UCI data sets are used for showing the performance of S3ELM. According to the method used in [17], the b and the elements ofain G are generated from the uniform probability distribution on[0,1]independent and identically distributed. In this experiment, we randomly select 2, 4, 6, 8, and 10 labeled samples per class as labeled training samples. Then, the left samples are considered as unlabeled training samples for S3ELM (they are taken as the testing samples for original ELM and OP-ELM). For different parameters, 50 independent experiments are taken and the average results are calculated, which is shown in Tables 2–8.The bold data in Tables 2–8 are the best results among the different algorithms. Because codes of OP-ELM (which can be download from [41]) cannot output classification results for some experiments, the symbol ‘–’ is written in the corresponding position in the tables. The occurred errors of OP-ELM codes may be caused by too small training samples. The suffixes ‘RBF’ and ‘SIG’ represent that the algorithms (S3ELM, original ELM and OP-ELM) use RBF function and Sigmoid function as the activation function, respectively. For example, S3ELM-RBF represents the S3ELM algorithm with RBF function as the activation function and ELM-SIG represents the original ELM algorithm with Sigmoid function as the activation function. From the experimental results, we can see that the proposed method (S3ELM-RBF and S3ELM-SIG) can obtain better performance than the other two methods for the first five data sets. For the last two data sets, our method outperforms OP-ELM. Because of smaller scale of ELM network caused by pruning operation of S3ELM, it is reasonable that S3ELM is worse than ELM in few cases. However, in comparison to original ELM, S3ELM with the semi-supervised technology can also obtain the best results in most cases (especially for the case corresponding to less numbers of labeled training samples).Then, we report the consumed time of the different algorithms in both training and testing stages. Thinking of too many tables need to be list if all CPU time in the experiments are reported, only three artificial data sets are used for testing the CPU time in this subsection. We think that they can illustrate the computational cost of S3ELM in both training and testing stages well. All the experiments in this subsection are run on a HP-G2160cx PC, with Pentium (R) Dual-Core CPU (E5800 3.20GHz) and 4GB memory.Tables 9–11show the training time of three algorithms. S3ELM spends more time on training stage than another two methods because our method is dependent on the sparse regularization (i.e.ℓ2,1-norm). Expensive computational cost is an obstacle for all the methods based on sparse representation. It is appeared that there is no optimization algorithm to overcome it yet. However,ℓ2,1-norm decreases the size of ELM network and save the computational cost greatly in testing stage. The CPU time in testing stage is shown as follows.Note that S3ELM is a semi-supervised method. The labels of unlabeled training samples can be predicted in the training stage. So, the CPU time in Tables 12–14are obtained by using unlabeled training samples as testing samples. In another words, for unlabeled samples, the CPU time consumed by calculating hidden layer output matrixHand approximation matrix of the real label matrixY=HWare reported in Tables 12–14.Tables 12–14 shows the CPU time consumed by three methods in the testing stage. The data inside parentheses are the average number of hidden nodes corresponding to the best parameters. The average number of hidden nodes is 1000 for original ELM since it is not pruned. It is obvious that OP-ELM and S3ELM is far faster than original ELM. OP-ELM prunes more hidden nodes and faster than S3ELM. (The timing error of computer may cause that some of CPU time consumed by S3ELM is less than OP-ELM.) However, S3ELM has better recognition rates for the three data sets in comparison OP-ELM, which means OP-ELM prunes many useful hidden nodes.For four UCI data sets, the numbers of hidden nodes of OP-ELM and S3ELM are shown in Table 15.In this subsection, S3ELM is compared with SELM. All the data sets are divided into three parts: labeled training data, unlabeled training data and testing data. In detail, we choose 2, 4, 6, 8 and 10 samples respectively in each class as the labeled training samples, and the remaining data are divided into two parts: 90% of them are taken as unlabeled training samples and 10% as testing samples. Two methods are executed 50 times and the average accuracy rates are calculated. For all six data sets, the average accuracy rates corresponding to the best parameters are shown in the following tables. Each table is divided into two parts: part I shows the average accuracy rates about testing samples and part II shows the average accuracy rates about unlabeled training samples.From Tables 16–22one can observe that our method can obtain better performance than SELM for Artificial data set 1, Banknote-authentication data set, Ionosphere data set and Transfusion data set. The results are similar for other two artificial data sets. Only for Planning-relax data set, SELM presents better performance than S3ELM. Taking the pruning function into account, a smaller sizes of ELM networks can be obtained by S3ELM. Therefore, S3ELM is faster than SELM in the testing stage. The numbers of the hidden notes for S3ELM are not reported because they are similar to the results in Section 4.2 (Tables 12–15).In addition, to compare the numerical results from Tables 2–4 and Tables 16–18, the semi-supervised algorithms with graph Laplacian item (i.e. SELM and S3ELM) are much better than supervised algorithms (i.e. original ELM and OP-ELM) in the classification accuracy. Therefore, the numerical results of the three artificial data sets fully demonstrate the function of semi-supervised item for S3ELM.For face recognition, it has been proved that graph LaplacianLconstructed by sparse representation is more effective [3,6,31]. Therefore, in this experiment, both of the SELM and S3ELM use sparse representation method to construct graph LaplacianL. The method to construct graph LaplacianLis shown as follows:For a training samplexi(i=1,2,…,N), we construct a matrix which consists of the other training samples andD×Didentity matrix:B=[x1,…,xi-1,xi+1,…,xN,ID].Then the following programming is optimized: (the function of the following programming can be found in [39])minθ‖θ‖1s.t.Bθ=xi,whereθ=(ai1,…,ai,i-1,ai,i+1,…,aiN,e1,…,eD)T. There are two advantages for this programming. (1) It is very robust for partial occlusion of face image and partial sudden change caused by expression. (2) It does not have any parameter. For affinity matrixA∼, its element at i-th row and j-th column isaij(j=1,2,…,N,j≠i)inθand each element in the main diagonal ofA∼is zero. SinceA∼must be a symmetric and nonnegative matrix, we define a new affinity matrixAwhose element at i-th row and j-th column is|aij|+|aji|. The method to construct affinity matrixAhas been used by [5] and proved to be effective. Finally, the graph Laplacian is calculated by formulaL=D-A, where the elements in the main diagonal of diagonal matrixDsatisfy thatdii=∑i=1Naij(i=1,2,…,N).Similar with the settings in Section 4.2, some samples (face images) are chosen as labeled training samples and the left samples as unlabeled training samples for SELM and S3ELM (or as testing samples for original ELM). For different numbers of labeled training samples of each class and different face databases, all the three algorithms are executed 50 times and the average accuracy rates are calculated. Tables 23 and 24present the average accuracy rates of three algorithms corresponding to the best parameters. OP-ELM is not executed as a competitive algorithm because it cannot provide the final results in the part of 50 runs for all numbers of labeled training samples of each class and all face databases.The results in Tables 23 and 24 show that S3ELM-RBF can obtain best performance and need less hidden nodes for CMU PIE30 face databases which reduce the computational cost remarkably. S3ELM-SIG may not converge in 200 iterations and does not prune any hidden node of ELM network. For JAFFE face databases, SELM-RBG, SELM-SIG and S3ELM-SIG obtain the highest recognition rate. However, S3ELM-SIG cannot prune enough hidden nodes of ELM network. In comparison to S3ELM-SIG, S3ELM-RBF obtains better pruning performance. Although the recognition rates decrease slightly, S3ELM-RBF can obtain higher efficiency in the testing stages.In this paper, a new Sparse Semi-Supervised Extreme Learning Machine, named as S3ELM, is proposed for optimize the structure of ELM network. Compared to available ELMs, it has two characteristics: (1)ℓ2,1-norm based pruning method which causes a convex mathematical model; (2) a semi-supervised regularization term integrated into the mathematical model which is effective use of unlabeled samples. Meanwhile, the optimization algorithm of proposed mathematical model and the analyses of convergency and computational cost of the proposed algorithm are provided. Some benchmark data sets: three artificial data sets, four UCI data sets (Banknote-authentication data set, Ionosphere data sets, Planning-relax data sets, and Transfusion data sets) and two facial databases are used to investigate the performance of S3ELM, and the experimental results show that: (1) S3ELM can prune hidden nodes of ELM network effectively, and save testing time and storage memory resource and (2) S3ELM can provide similar or better classification accuracies.Although there are many advantages, S3ELM still needs to improve in many aspects. For example, the formula (19) has too many parameters. Although S3ELM is more flexible with them, it becomes problem how to select the value of the parameters. For another example, as all methods based on sparse representation, the computational cost consumed by S3ELM in training stage is very expensive. So, it is our tasks in the future to find the method to optimize parameters and develop fast algorithm for the programming withℓ2,1-norm.

@&#CONCLUSIONS@&#
