@&#MAIN-TITLE@&#
A novel hybrid system for feature selection based on an improved gravitational search algorithm and k-NN method

@&#HIGHLIGHTS@&#
We propose an improved gravitational search algorithm which makes the best of ergodicity of piecewise linear chaotic map to explore the global search while utilizing the sequential quadratic programming to accelerate the local search.Based on the binary improved gravitational search algorithm and the k-nearest neighbor (k-NN) method, a novel hybrid system is proposed to improve classification accuracy with an appropriate feature subset in binary problems.The proposed system is able to select the discriminating input features correctly and achieve high classification accuracy which is comparable to or better than well-known similar classifier systems.

@&#KEYPHRASES@&#
Feature selection,Binary classification,Gravitational search algorithm,k-nearest neighbor,Leave-one-out cross-validation,Function optimization,

@&#ABSTRACT@&#
Feature selection is an important pre-processing step for solving classification problems. This problem is often solved by applying evolutionary algorithms in order to decrease the dimensional number of features involved. In this paper, we propose a novel hybrid system to improve classification accuracy with an appropriate feature subset in binary problems based on an improved gravitational search algorithm. This algorithm makes the best of ergodicity of piecewise linear chaotic map to explore the global search and utilizes the sequential quadratic programming to accelerate the local search. We evaluate the proposed hybrid system on several UCI machine learning benchmark examples, comparing our approaches with feature selection techniques and obtained better predictions with consistently fewer relevant features. Furthermore, the improved gravitational search algorithm is tested on 23 nonlinear benchmark functions and compared with 5 other heuristic algorithms. The obtained results confirm the high performance of the improved gravitational search algorithm in solving function optimization problems.

@&#INTRODUCTION@&#
In many fields such as pattern recognition [31], data mining [7], gene selection from microarray data [12], text categorization [35] and multimedia information retrieval [22,26], datasets containing huge numbers of features are often involved. In such cases, feature selection will be necessary. Due to the abundance of noisy, irrelevant or misleading features, the ability to handle imprecise and inconsistent information in real world problems has become one of the most important requirements for feature selection [29].Feature selection is a process of choosing a subset of features from the original set of features forming patterns in a given dataset. The subset should be necessary and sufficient to describe target concepts, retaining a suitably high accuracy in representing the original features. The importance of feature selection is to reduce the problem size and search space for learning algorithms. In the design of pattern classifiers it can improve the quality and speed of classification. In other words, the objective of feature subset selection is to reduce the number of features used to characterize a dataset so as to improve a learning algorithm's performance on a given task. The maximization of the classification accuracy in a specific task for a certain learning algorithm is the ultimate objective.Most existing feature selection algorithms consist of four basic issues that determine the nature of the search process: a starting point in the search space, an organization of the search, an evaluation strategy of the feature subsets, and a criterion for halting the search. The search method is in charge of identifying promising candidate subsets, while the evaluator must assess the goodness of a given subset. Evaluators can be of two types: filter or wrapper [6,25]. Filter techniques evaluate the goodness of an attribute or set of attributes by using only intrinsic properties of the data. On the other hand, wrapper algorithms use a classifiers to assess the quality of the attribute subset proposed as candidate by the search algorithm. Wrappers usually perform better than filters but with the disadvantage of being more time-consuming [21].Basically, the problem of feature selection could be treated as a problem of optimization in a search space. Feature selection method based on stochastic search algorithms has attracted great attention. Several methods have been proposed to perform feature selection using evolutionary techniques. Raymer and Punch [40] suggested using Genetic Algorithms (GA) to tackle the problem. Authors in [4,45,48] proposed to use binary Particle Swarm Optimization (PSO) for feature selection. Zhang and Sun applied tabu search in this problem [53].Recently, a new population-based heuristic algorithm, namely gravitational search algorithm (GSA), based on the metaphor of gravitational interaction between masses was introduced mainly designed for problems in electrical engineering [38]. In this algorithm, the searcher agents are a collection of masses which interact with each other based on the Newtonian gravity and the laws of motion. A comprehensive comparison between GSA and some well-known heuristic algorithms such as GA and PSO was presented by Rashedi et al. Their results indicated that in optimization field, the GSA approach has some merit over other methods [38,39].However, there are two major issues associated with the search performance of GSA: one is premature convergence happening in existing GSA due to rapid reduction of diversity; the other is that GSA converges rapidly in the beginning of the search process while slows down quickly when the global best solution is near the optimum of the local search space. Consequently, to get a more accurate estimation of the local optima, more ineffective iterations are needed; in addition, it is hard to have a good balance between exploration and exploitation. In order to overcome these drawbacks, researchers have made much work to improve the performance of GSA. Sarafrazi et al. [41] introduced an operator called “Disruption” originating from astrophysics to improve the exploration and exploitation abilities of the original GSA. Li and Zhou [23] strengthened the algorithm by combination of the search strategy of particle swarm optimization and applied it to the solution of optimization problems in parameters identification of hydraulic turbine governing system. Binod Shaw et al. [43] proposed an opposition-based GSA (OGSA) which improves the convergence rate of the GSA by utilizing opposition-based learning for population initialization and also for generation jumping. Yin et al. [52] integrated two strategies, the maximum and minimum of the dth dimension and a new G(t), into the original GSA to add the objects’ diversity and make them explore more solution spaces. In this paper, we propose an improved gravitational search algorithm (IGSA) by making the best of ergodicity of PWL to help GSA explore the global search while employing the SQP to accelerate the local search.Since the IGSA is operated in continuous space and many optimization problems are set in binary space, we introduce a binary IGSA (BIGSA) referring to the original BGSA introduced by Rashedi et al. [39]. Based on the BIGSA, a new method for feature selection wrapped by k-nearest neighbor (k-NN) method is presented, in which the k-NN method based on Euclidean distance calculations serves as a classifier for evaluating classification accuracies [33]. We apply the proposed method to several UCI machine learning benchmark examples.The remainder of this paper is organized as follows: In Section 2 we provide a brief summary of GSA. Section 3 explains the IGSA. Section 4 presents the proposed hybrid system for feature selection. The results of the work are presented and discussed in Section 5. Finally, we provide conclusions in Section 6.GSA is a newly developed stochastic search algorithm based on the law of gravity and mass interactions [36–38]. This approach provides an iterative method that simulates mass interactions, and moves through a multi-dimensional search space under the influence of gravitation. In GSA, agents are considered as objects and their performances are measured by their masses; these objects attract each other by gravitational force which causes a global movement of all objects toward objects with heavier masses [36–38].Assumed there are k objects (masses), the position of the ith object is defined as Eq. (1):(1)Xi=(xi1,…,xid,…,xin),i=1,2,…,k,wherexiddenotes the position of ith object in the dth direction. The force exerting on the object i from the object j is defined as Eq. (2):(2)Fijd(t)=G(t)Mi(t)×Mj(t)Rij(t)+ε(xjd(t)−xid(t))where Mjis the mass related to object j, Miis the mass related to object i, G(t) is gravitational constant at time t, ɛ is a small constant, and Rij(t) is the Euclidian distance between two objects i and j. The total forceFid(t)that exerts on object i in the dth direction is a randomly weighted sum of dth components of the forces from other agents:(3)Fid(t)=∑j=1,j≠ikrandjFijd(t),where randjis a uniform random variable in the interval [0,1].The acceleration of the object i,aid(t), at time t and in the dth direction, is given as Eq. (4):(4)aid(t)=Fid(t)Mii(t)where Miiis the inertial mass of the object i. Its next velocityvid(t+1)and its next positionxid(t+1)are calculated as Eqs. (5) and (6):(5)vid(t+1)=randi×vid(t)+aid(t)(6)xid(t+1)=xid(t)+vid(t+1)where randiis a uniform random variable in the interval [0,1]. This random number is applied to give a randomized characteristic to the search,vid(t)andxid(t)are its current velocity and position, respectively.The masses of objects are evaluated by the fitness function. Assuming the equality of the gravitational and inertia mass, the mass Mi(t) is updated by Eqs. (8)–(10) and (11):(7)Mi=Mii,i=1,2,…,k,(8)mi(t)=fiti(t)−worst(t)best(t)−worst(t),(9)Mi(t)=mi(t)∑j=1kmj(t),(10)best(t)=minj∈{1,…,k}fitj(t),(11)worst(t)=maxj∈{1,…,k}fitj(t),where fiti(t) represents the fitness value of the object i at time t. The flow chart of GSA is shown in Fig. 1.The piecewise linear chaotic map (PWL) has been attached increasing attention in being applied to optimization areas due to its efficiency in implementation, simplicity in representation and good dynamical behavior [3,24,27,30,50]. In this paper, we use a piecewise linear chaotic map as denoted in (12):(12)F(xt+1,p)=xt/p,xt∈[0,p)(xt−p)/12−p,xt∈p,12F(1−xt,p),xt∈(12,1]where p is a control variable andp∈0,12. The PWL has the following properties: (1) the system behaves chaotically in (0, 1); the output signals satisfy ergodicity, mixing and determination certainty in the interval of definition; (2) the system subjects to uniform invariant distribution; (3) the self-correlation function of output trajectory is aδ−likefunction [3]. We abbreviate this chaotic map within range [0,1] as PWL (0, 1). Based on the fact that chaotic search is efficient in a small range of radius γ defined [24,27,30,50], chaotic local search for a solution can be executed in a small range near current location and global best position, respectively. In this paper, masses take chaotic steps within a dynamically reducing range near the global best of the whole space, as determined by Eqs. (13) and (14).(13)PWL(−1,1)=2PWL(0,1)−1(14)xidt=pgd[1+ω⋅PWL(−1,1)]In (14)ω is a scaling parameter and ω=1.1 is set according to experiences [46]. After PLM operation this mass will still be confined in the initial problem space of[xmin,xmax]according to (15).(15)Xit+1=max{min{Xit,xmax},xmin}Sequential quadratic programming (SQP) is one of the most successful methods for solving constrained nonlinear optimization problems. It was first proposed by Wilson in his doctoral dissertation in 1963 [49]. This method converts a nonlinear programming problem into a sequence of quadratic programming (QP) sub-problems. The application of SQP is to iteratively solve the quadratic programming sub-problem of the form as shown in (16)[9](16)minimizeS(d)=∇f(x)Td+12dTBkdsubjecttog(xk)+∇g(xk)Td≤0where∇xis the first order partial derivatives with respect to x, g is the vector of constraint function, d is the search direction of iterations, B is the Hessian matrix, which is updated using the Broyden–Fletcher–Goldfarb–Shanno (BFGS) formula given as follows:(17)B(k+1)=B(k)−B(k)δδkB(k)δTB(k)δ+γγTδTγwhereδ=x(k+1)−x(k), γ is the gradient difference of the Lagrange functions,γ=∇xL(x(k+1),λ(k))−∇xL(x(k),λ(k)). By using this formula, the Hessian approximate B remains positive definite. In this study sequential quadratic programming (SQP) is employed to speed up local search and improve precision of the GSA solutions.In this paper, we integrate piecewise linear chaotic map (PWL) and sequential quadratic programming (SQP) into gravitational search algorithm (GSA). First, PWL is combined with GSA to form a PWL_GSA, and then we use SQP to accelerate the local search for the global best found by PWL_GSA to do exploitation. This integration forms a new algorithm, called Improved Gravitational Search Algorithm (IGSA). In this algorithm, the masses are able to search the entire space while finding local optima rapidly, which increases the possibility of exploring a global optimum in problems with more local optima while speeding up the convergence rate of the algorithm. A brief flow chart of the proposed algorithm is shown in Fig. 2. In order to give a better description, two operations are presented as follows:GSA operation: Moving according to formulas (5) and (6) is called a gravitational search algorithm operation, namely GSA operation.PWL operation: Moving according to formulas (12)–(15) is called a piecewise linear chaotic operation, namely PWL operation.The search behavior of a mass i in PWL_GSA is illustrated in Fig. 3. After taking a step vt, mass i has reached its current positionXit. Then, there are two possible operations in the next step, GSA operation or PWL operation. GSA operation is the weighted sum ofvitand the accelerationait. PWL operation is a chaotic jump. Which operation will be taken depends on Ri,best, the Euclidean distance between mass i and the best solution. If Ri,best≥1, the PWL operation will be taken and mass i will move to the position P″. The aim of PWL operation is to carry on a chaotic jump so that the old masses become larger or smaller than what they were previously. Consequently, the new generated Xi(new) will be in a position far from Xi(old). Therefore, the algorithm is able to explore the search space thoroughly. On the other hand, when Ri,best<1 (i.e., mass i is close to best mass and the algorithm should exploit around the best solution in this case), the GSA operation will be taken and mass i will move to the position P′.Before the next iteration, the global best mass found by PWL_GSA is used as the starting point to begin a local search through SQP method. Pbest_SQPand Pbest_IGSArepresent the global best solution found by SQP and the whole algorithm (IGSA), respectively. Then, a comparison between Pbest_SQPand Pbest_IGSAis done; if a better solution is found in the local phase, the Pbest_IGSAwill be replaced by Pbest_SQP. The detailed steps of the proposed IGSA are presented as follows:Step 1 Population-based initializationStep 2 Fitness evaluation of agentsStep 3 Update G(t), Mi(t), Pbest(t), and Pworst(t) for i=1, 2,…, NStep 4 Calculation of the total forces in different directionsStep 5 Calculation of accelerations and velocitiesStep 6 PWL-based chaotic jumping and SQP-based local searchfor each particle Pidoif Ri,best≥1 thenUpdate the position Xiaccording to (12)–(15)//PWL operation.elseUpdate position Xiaccording to (5) and (6)//GSA operation.end ifif Fit (Pi)> Fit (Pbest) then//update the global best mass particleFit (Pbest)=Fit(Pi)SQPflag=1//arouse the SQP local searchif Fit (Pbest)>Fit (Pbest_IGSA) thenPbest_IGSA=Pbest//Pbest_IGSAis the global best solution derived by IGSAend ifend ifend forif SQPflag==1 then//use the SQP local search to update Pbest_SQPUpdate Pbest_SQPby solving (16)if Fit (Pbest_SQP)>Fit (Pbest_IGSA) thenFit (Pbest_IGSA)=Fit (Pbest_SQP)//update optimal function value derived by IGSAend ifSQPflag=0//sleep the SQP local searchend ifStep 7 Repeat steps 2–6 until the termination condition is metSince the IGSA is operated in continuous space and many optimization problems are set in binary space, based on the IGSA we introduce a binary IGSA(BIGSA) referring to the original BGSA introduced by Rashedi et al. [39].In binary format, agent string is a series of 1 and 0 values which indicates the presence or absence of features in the agent, respectively. In a system with k agents (objects), the position of the ith agent, Xi, in an n-dimensional space is represented by Eq. (1), wherexidrepresents the position of ith agent in the dth dimension and every dimension can only take the value of 0 or 1. Moving through a dimension means that the corresponding variable value changes from 0 to 1 and vice versa [38,39].In the BIGSA, a mass is attributed to each agent. The values of masses, M, are calculated by fitness evaluation criteria, i.e., fiti(t), worst(t) and best(t), according to Eqs. (8) and (9), where fiti(t) represents the fitness value of the agent i at time t, and, worst(t) and best(t) for a minimization problem are defined according to Eqs. (10) and (11). At a specific time t, the force of gravity acting on mass i from mass j is directly proportional to the product of their masses and inversely proportional to the square of distance between them [39], which is represented by Eq. (2), where Mjis the mass related to object j, Miis the mass related to object i, G(t) is gravitational constant at time t, ɛ is a small constant, and Rij(t) is the Hamming distance between two objects i and j.The value of gravitational constant depends on the actual age of the universe. At the beginning, G(t) is initiated with a predefined value and then it is reduced with time to control the search accuracy, which is denoted as Eq. (18),(18)G(t)=G01−tTwhere G0 is its value at the first iteration of the algorithm at time t0[39].The total forceFid(t)that exerts on object i in the dth direction is a randomly weighted sum of dth components of the forces from other agents, which is represented by Eq. (3), where randjis a random number in the interval 0 to 1.According to the Newton second law of motion, the acceleration of the agent i at time t, in direction d,aid(t), depends on the total force which caused the acceleration vector and its mass [38] as shown in Eq. (4). Agents can move in the search space due to the force between them. The next velocity of an agent,vid(t+1), is considered as a summation of a fraction of its current velocity,S(xid(t))and its acceleration,aid(t), as it is formulated in Eq. (5), where randiis a uniform random variable in the interval 0 to 1 [38].The new position in the original BGSA is considered to be either 1 or 0 at a given probability based on Eq. (5). And a probability function should be defined to transfervidinto a probability quantity [39]. This function is restricted within 0 to 1 interval such that asvidincreases, probability increases too.Based on the IGSA and BGSA, the position updating in the BIGSA include two possible operations: GSA operation and PWL operation.GSA operation is similar to the original BGSA, which is the weighted sum ofvitand the accelerationait. PWL operation is a chaotic jump. Which operation will be taken depends on Ri,best, the Hamming distance between mass i and the best solution. The detail description of the two operations is given as follows:If Ri,best≥1, the PWL operation will be taken, That is to say, the BIGSA first updates the position based on Eq. (12)–(15), and then considers the new position to be either 1 or 0 at a given probability. A probability function is restricted within 0 to 1 interval such that asxidincreases, probability increases too, which is defined as Eq. (19).(19)S(xid(t))=11+e−xid(t)OnceS(xid(t))is calculated, the agents will move according to the relationship explained below:(20)Ifrand<S(xid(t))thenxid(t+1)=1elsexid(t+1)=0On the other hand, when Ri,best<1, the GSA operation will be taken, which is similar to the original BGSA. That is to say, BIGSA updates the velocity based on Eq. (5) and considers the new position to be either 1 or 0 at a given probability. To get the given probability, a probability function is defined to transfervidinto a probability quantity. This function is restricted within 0 to 1 interval, which is defined as Eq. (21)[39].(21)S(vid(t))=|tanh(vid(t))|OnceS(vid(t))is calculated, the agents will move according to the relationship explained below:(22)Ifrand<S(vid(t+1))thenxid(t+1)=complement(xid(t))elsexid(t+1)=xid(t)wherexid(t+1)is the new position of agent i in dimension d, and can be obtained by:(23)xid(t+1)=xid(t)+vid(t+1)In this section, we will give a novel feature selection method based on the above IGSA. Since the IGSA is operated in continuous space and many optimization problems are set in binary space, we introduce a binary IGSA (BIGSA) referring to the original BGSA introduced by Rashedi et al. [39]. Based on the BIGSA, a new method for feature selection wrapped by k-nearest neighbor (k-NN) method is presented, in which the k-NN method based on Euclidean distance calculations serves as a classifier for evaluating classification accuracies.The k-nearest neighbor (k-NN) method is one of the most popular nonparametric methods introduced by Fix and Hodges in 1951 [5,8,44]. Since there is only one parameter K (the number of nearest neighbors) which needs to be determined, it is easy to implement the k-NN method. The number of nearest neighbors is key to the performance of the classification process. k-NN classifies a new object from the testing samples to the training samples based on the minimum distance which is calculated according to Euclidean distance. If an object is close to the K nearest neighbors, the object is categorized into the K-object category. For the purpose of enhancing the classification accuracies, the parameter K must be altered according to the different data set characteristics. In this paper, we use the leave-one-out cross-validation (LOOCV) method to choose the parameter K. When n samples need to be classified, they are divided into one testing sample and n−1 training samples at each iteration in the process of evaluating, and a classifier is constructed by training the n−1 training samples. The testing sample class can be evaluated by the classifier.In the fields of pattern recognition areas feature selection is one of the most active issue which is often formulated as an optimization problem. The task of feature selection is to find a transformed set of patterns in a lower dimensional space that satisfies the optimization criterion. In general, the transformed patterns are evaluated based on the classification criteria. There are some techniques to reduce the dimensionality of the feature space, such as principle component analysis, linear discriminated analysis, genetic algorithm (GA), binary particle swarm optimization (BPSO) and binary gravitational search algorithm (BGSA) [20].In this study, we use the above BIGSA to reduce the dimensionality of the feature space. In the IGSA trajectories are defined as changes in position on some number of dimensions, while in the BIGSA trajectories are changes in the probability that a coordinate will take on a 0 or 1 value, and every dimension has the value of 0 or 1. Moving in every dimension means that its value changes from 0 to 1 or vice versa.The encoding of a binary vector for feature selection can be seen in Fig. 4. Each vector is defined as a series of binary values representing a subset of features. Elements of vectors can take the value of 1 or 0 in order to represent including or excluding a feature in the agent, respectively. Being an individual in the search space a possible feature subset, a common notation is used to represent each individual: for a full dψ feature problem, there are dψ bits in each state, each bit indicating whether a feature is included (1) or excluded (0). The length of the vector is equal to the number of all features. The ith feature is included if the value of the ith bit equals 1; otherwise, this feature will be excluded [40].The fitness function is designed in terms of the two criteria of classification accuracy and the number of selected features. A good fitness value means having higher classification accuracy as well as lower dimensional numbers. We solve the problem of multiple objectives by creating a fitness function that integrates the two goals into one objective. The fitness function is defined as Eq. (24):(24)fiti=ω1×accui+ω2×1−∑j=1pfjpIn which there are two predefined weight factors ω1 and ω2; ω1 is the weight factor for the classification accuracy of the 1-nearest neighbor (1-NN) determined by the leave-one-out cross-validation (LOOCV) method; accuiis the 1-NN classification accuracy; ω2 is the weight factor for the number of selected features and fjis the value of feature mask. The weight factor of accuracy can be adjusted to a high value (such as 100%) if accuracy is the most important. The object with high fitness value has a high probability of affecting the other objects’ positions of the next iteration, so it should be set appropriately [14,15]. The accuiis obtained by Eq. (25), in which corr represents the number of correctly classified examples and incorr represents the number of incorrectly classified examples [14,15].(25)accui=corrcorr+incorr×100%As noted before, feature mask is chosen as the position of objects of the BIGSA. Initially, the position of each object is represented by a binary string in which “1” represents a selected feature and “0” represents a non-selected feature. We use the 1-NN method as a classifier for evaluating classification accuracies. We take an example to explain the 1-NN method for feature selection. Assume a datasets contains seven records, R1, R2, R3, R4, R5, R6 and R7, in which each has four features. A pth object, Sp, has a binary string which is randomly generated as 1010 (shown in Fig. 5). R1, R2, R3, R4, R5, R6 and R7 are in turn individually used as evaluating data and other six datasets are used as training data to evaluate it. As seen in Fig. 5(a), R7 is the evaluating data and the others six are the training data. Since Sp=1010, F1 and F3 are selected. When 1-NN (Fig. 5(b)) is used, because R4 is the nearest to R7, R7 is categorized as the shape square. However, since R7 is of the triangle shape in datasets, it is a wrongly classified sample. The fitness function, as mentioned above, considering the correct classification rate and the size of selected feature subset (Eq. (24)) is used and it must be maximized.Fig. 6shows the flowchart of the BIGSA-k-NN hybrid system and its pseudo-code can be seen in Table 1. Fig. 7presents a simplistic diagram of the three stages of feature selection employed in this study.

@&#CONCLUSIONS@&#
