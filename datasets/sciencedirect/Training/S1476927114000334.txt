@&#MAIN-TITLE@&#
An ensemble method for prediction of conformational B-cell epitopes from antigen sequences

@&#HIGHLIGHTS@&#
A support vector machine-based ensemble method is proposed to predict conformational B-cell epitopes.Epitopes are more accessible than non-epitopes, and preferred in beta-turn.The flexibility and polarity of epitopes are higher than non-epitopes.In bound dataset, Asn, Glu, Gly, Lys, Ser, and Thr are preferred in epitope regions.In unbound dataset, Glu and Lys are preferred in epitope sites.

@&#KEYPHRASES@&#
Bound dataset,Unbound dataset,Support vector machine,Beta-turn,Flexibility,

@&#ABSTRACT@&#
Epitopes are immunogenic regions in antigen protein. Prediction of B-cell epitopes is critical for immunological applications. B-cell epitopes are categorized into linear and conformational. The majority of B-cell epitopes are conformational. Several machine learning methods have been proposed to identify conformational B-cell epitopes. However, the quality of these methods is not ideal. One question is whether or not the prediction of conformational B-cell epitopes can be improved by using ensemble methods. In this paper, we propose an ensemble method, which combined 12 support vector machine-based predictors, to predict the conformational B-cell epitopes, using an unbound dataset. AdaBoost and resampling methods are used to deal with an imbalanced labeled dataset. The proposed method achieves AUC of 0.642–0.672 on training dataset with 5-fold cross validation and AUC of 0.579–0.604 on test dataset. We also find some interesting results with the bound and unbound datasets. Epitopes are more accessible than non-epitopes, in bound and unbound datasets. Epitopes are also preferred in beta-turn, in bound and unbound datasets. The flexibility and polarity of epitopes are higher than non-epitopes. In a bound dataset, Asn (N), Glu (E), Gly (G), Lys (K), Ser (S), and Thr (T) are preferred in epitope regions, while Ala (A), Leu (L) and Val (V) are preferred in non-epitope regions. In the unbound dataset, Glu (E) and Lys (K) are preferred in epitope sites, while Leu (L) and Val (V) are preferred in non-epitiopes sites.

@&#INTRODUCTION@&#
B-cell epitopes are the sites on antigen proteins which are recognized or bounded by B-cells. The identification of B-cell epitopes is useful for the design of peptide-based vaccines and drugs (Walter, 1986; Van Regenmortel, 2004). Predicted peptides can be synthesized and can be used as reagents for detecting anti-protein antibodies (Van Regenmortel, 2006). Predicted peptides can also used in synthetic peptide vaccines (Yadav et al., 2011). B-cell epitopes can be divided into two types: linear epitopes and conformation epitopes. Linear epitopes are continuous amino acids. Conformational epitopes are closed in the antigen protein sequence, but discontinuous in the protein sequences. Most B-cell epitopes are conformational epitopes (Pellequer et al., 1991).It is time consuming and expensive to identify B-cell epitopes experimentally. Some computational methods are proposed to identify the epitopes and non-epitopes. The first stage of using computational methods to predict B-cell epitopes is mainly concerned with linear B-cell epitopes. Simple models are proposed to predict these linear B-cell epitopes. These methods train on small datasets and are based on a single amino acid property (Hopp and Woods, 1981; Welling et al., 1985; Karplus and Schulz, 1985; Parker et al., 1986; Kolaskar and Tongaonkar, 1990; Pellequer et al., 1993; Pellequer and Westhof, 1993; Alix, 1999). Later, some researchers combined several properties, including hydrophilicity, solvent accessibility, flexibility, and secondary structure, in order to predict B-cell epitopes (Odorico and Pellequer, 2003).The main feature of the secondary stage of B-cell epitope prediction is the use of a more advanced model for linear epitopes. BepiPred is a method based on a hidden Markov model (Larsen et al., 2007). ABCPred is based on neural networks (Saha and Raghava, 2006). Chen et al. (2007) used 20mer peptides as the training dataset and built the model using support vector machine. BCPred (El-Manzalawy et al., 2008) was built by a kernel-based support vector machine (SVM). COBEpro (Sweredoski and Baldi, 2009) used a two-stage design with SVM. BayesB (Wee et al., 2010) predicted epitopes using the position specific scoring matrix (PSSM), with the Bayes feature selection method.With the development of linear epitopes, predictors of conformational epitopes are proposed. CBTOPE (Ansari and Raghava, 2010) is an SVM-based predictor for conformational epitopes. CEP (Kulkarni-Kale et al., 2005) is a structure-based method which uses solvent accessibility. DiscoTope (Haste Andersen et al., 2006) is a method which combines several properties, including solvent accessibility, contact numbers and amino acid propensity scores. SEPPA (Sun et al., 2009) combines propensity scores and the packing density of amino acids. PEPITO (Sweredoski and Baldi, 2008) determined the probabilities of epitopes by linear regression. EPSVR (Liang et al., 2010) assigned the probability via a support vector regression. Zhang et al. (2011) utilized a random forest model to predict epitopes. Epitopia (Rubinstein et al., 2009a, 2009b) utilized Naïve Bayes to improve the epitopes. BEST (B-cell Epitope prediction using Support vector machine Tool) (Gao et al., 2012) is a new method, using two-stage schemes to predict B-cell epitopes.More recently, some ensemble methods have been applied to identify the epitopes in antigen proteins. Ensemble method is a method that aggregates multiple machine learning methods by weighting individual classifiers to get a classifier which outperforms each one of them (Polikar, 2006; Rokach, 2010). Ensemble method has advantages in dealing with higher-dimensional and complicated data. EPMeta (Liang et al., 2010) is an ensemble method which combines six predictors to predict conformation epitopes. Zhang et al. (2012) built an ensemble method using random forests to predict epitopes.One motive of this paper is to examine whether an ensemble method can improve the prediction of B-cell epitopes. To this end, we have built an ensemble method which combined several support vector machines predictors to predict B-cell epitopes. We also noticed other ensemble method is proposed by Zhang et al. (2012). Different from Zhang's random forest-based ensemble method, multiple inputs including PSSM profile, amino acids pair indicators predicted disorder region and predicted secondary structure indicators are used. Here we compute the features for each residue, and analyze the selected features on a bound dataset and an unbound dataset. AdaBoostM1 and the re-sample method are used to deal with the imbalanced dataset. Some interesting results related on the bound dataset and the unbound dataset are investigated.Four datasets were used in this paper: (1) Rubinstein’ bound structure dataset was first introduced by Rubinstein et al. (2009a). There are 66 antibody and antigen complex structures. Antigen sequences were abstracted from the structure dataset. Since some proteins contain more than two chains, 83 antigen sequences are obtained, consisting of 1076 epitopes and 16,744 non-epitopes, with a ratio between epitopes and non-epitopes of 1:15.6 (named Bound83); (2) Liang's unbound structure dataset was first introduced by Liang et al. (2009, 2010). Antigen sequences from the structure dataset were abstracted. The final dataset contains 48 antigen sequences which consist of 885 epitopes and 10,145 non-epitopes. The ratio between epitopes and non-epitopes is 1:11.5 (named Unbound48). Both the Bound83 and Unbound48 datasets, were used as the training datasets. The proposed method is trained on the Unbound48 dataset in this paper; (3) 19 antigen sequences were abstracted from Liang's independent unbound structure dataset (Liang et al., 2010). This dataset is used as the independent dataset (named TEST19); (4) the other dataset, which was used in Rubinstein et al., 2009a, 2009b and Gao et al. (2012), is also used to evaluate the predictor. This dataset is named TEST194.We consider the following features to represent the residues.Physiochemical propensities of amino acids were used in this paper. Hydrophilicity, hydrophobicity, accessibility, flexibility, polarity, turns, antigenicity and beta-turn were all downloaded from the AAindex database (Kawashima et al., 2008). These features are chosen based on the work (Zhang et al., 2012; Su et al., 2012). We computed the minimum, maximum, and average of each of these properties within sliding windows, with window width from 5, 7, 9, …, 21. If the residue is in the head or terminus of sequence, we assigned zero for the special properties in the window.Each amino acid type was represented by a 20-bit binary string. For example, amino acid Ala (A) is denoted as (1, 0, 0, 0, …, 0), (19 zeroes, 1 one).Composition of the 20 amino acids (AA) in 21mer windows, i.e., the count residue number divided by the sequence length, where AAistands for one of 20 amino acids (20 features).If the residue is in ASN–Tyr (N–Y), His–Tyr (H–Y) and His–Met (H–M), we denoted this as 1, otherwise 0. This is motivated by the work from Sun et al. (2011).If the residue is within the top 10 or end 10 residues, we denoted this as 1, otherwise it was denoted as 0. These features are motivated by Chen et al. (2012).PSSMs were obtained by the PSI-BLAST program (Altschul et al., 1997), which searched the nr data set downloaded in 2012-09. The elements of PSSM for an amino acid were scaled by 1/(1+exp(x)). A residue is represented by its corresponding 20-dimensional row vector in the PSSM matrix. We computed the self-entropy score for each residue. We also used the sliding windows of PSSM profile with 21mer window width to represent the center residue. If the residue is in the head or terminus of sequence, we assigned zero for the PSSM profile in the 21mer window.We used PSIPRED (Jones, 1999) to predict the protein sequence secondary structure. Each type of secondary structure is encoded by {0,1,2}; 0 for helix, 1 for strand and 2 for coils. The binary profile of secondary structure gives, H (helix) for (1,0,0), E (strand) for (0,1,0), C (coil) for (0,0,1). The predicted probabilities for the three types of secondary structures were also used in this paper. For the secondary structure segment indicator, each 3mer secondary structure is mapped 1–27 numbers (3*3*3=27). For example CCC, is mapped 1, the secondary structure segment indicator of CCC is (1, 0, 0, 0, …, 0), CCH is mapped 2, the secondary structure segment indicator of CCH is (0, 1, 0, 0, …, 0).Relative solvent accessibility (RSA) was predicted by SpineX (Faraggi et al., 2012). We used sliding windows of size 5, 7, …, 21, and computed the minimal, maximal and average of the RSA values in each of the sliding windows. Buried/exposed residue notation was used. RSA values above 0.25, give an exposed residue (encoded as 1), otherwise we have buried residues (encoded as 0). The dihedral angles predicted by SpineX were also used here. If the residue is “X”, we assigned zero. We also computed the minimal, maximal and average of the Phi/Psi values in one-window using window sizes of 5, 7, …, 21.The disorder region was predicted by Disopred2 (Ward et al., 2004). Disorder residues were encoded as {1}, non-disordered residues are encoded as {0}.We noted that our training datasets (Bound83 and Unbound48) were imbalanced. We used two ways to deal with these imbalanced datasets: (a) AdaboostM1 (Freund and Schapire, 1996). If the sample training goes wrong, it will be picked out, and the sample will be trained again in the later classifier; (b) resample method. The ratio of non-epitopes to epitopes is 12:1 and 16:1 for Bound83 and Unbound48, respectively. If we used the original data set to build the model, the predictor would be biased, predicting all residues as non-epitopes. In this paper, we divided the non-epitopes (majority class) into N equal parts, and combined the epitopes (minority class), respectively, where N is the ratio of non-epitopes to epitopes. This scheme is shown in Fig. 1. N classifiers predict results with N probabilities. A vote algorithm is used to decide the results. If the number of votes for epitope exceeds the number of votes for non-epitope, we declare it to be an epitope with maximal probability, otherwise, it is declared a non-epitope, but with minimal probability.The predictor will output binary outputs, whether or not the residue is a part of an epitope. To evaluate the binary predictor, accuracy (ACC), sensitivity (Sn), specificity (Sp), and Matthews correlation coefficient (MCC) are used:Accuracy=TP+TNTP+FP+TN+FNSensitivity=TNTN+FPSpecificity=TNTN+FPMCC=TP*TN+FP*FN{(TP+FP)*(TP+FN)*(TN+FP)*(TN+FN)}where TP and TN are the number of correctly predicted epitope, and non-epitope residues, respectively, FP is the number of non-epitope residues that were predicted to be in epitopes, and FN is the number of epitope residues that were predicted not to be in epitopes. Higher values of MCC and ACC indicate better quality predictions.For some predictors, which output the probability that the residue is an epitope, we calculate the area under the ROC curve (AUC) to evaluate the real-value predictions. A perfect classifier will have an AUC equal to 1. A random predictor will have an AUC of 0.5.We optimized the parameters of the RBF kernel in support vector machine, C from 2−2, 2−1, …, 25, gamma from 2−3, 2−2, …, 22. We selected the parameters which achieved the highest average AUC, with 5-fold cross validation. Take the Bound83 for example, we divided the non-epitopes (majority class) into 12 equal parts, and combined the epitopes (minority class), respectively. There are 12 subsets. We calculate the AUC for 12 subsets with 5-fold cross validation, and the AUC for the training dataset Bound83 is computed by the average of 12 AUCs of subsets. The optimized parameters for the unbound dataset are C=8, gamma=0.000488. The SVM is implemented in libsvm (version 2.19) (Chang and Lin, 2011). The AdaBoostM1 is implemented in Weka (version 3-7-7) (Hall et al., 2009).

@&#CONCLUSIONS@&#
