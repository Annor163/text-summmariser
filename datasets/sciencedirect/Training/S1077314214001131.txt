@&#MAIN-TITLE@&#
Review of statistical shape spaces for 3D data with comparative analysis for human faces

@&#HIGHLIGHTS@&#
Extensive review of statistical shape models for 3D data fitting.Classification of existing models and methods based on type of data and type of model.Theoretical analysis of two distinctive examplars of statistical shape models.Extensive experimental comparison of two distinctive examplars of statistical shape models.Make learned statistical models available for research purposes.

@&#KEYPHRASES@&#
Statistical shape spaces,Statistical model fitting,3D data,Generative models,3D vision,

@&#ABSTRACT@&#
With systems for acquiring 3D surface data being evermore commonplace, it has become important to reliably extract specific shapes from the acquired data. In the presence of noise and occlusions, this can be done through the use of statistical shape models, which are learned from databases of clean examples of the shape in question. In this paper, we review, analyze and compare different statistical models: from those that analyze the variation in geometry globally to those that analyze the variation in geometry locally. We first review how different types of models have been used in the literature, then proceed to define the models and analyze them theoretically, in terms of both their statistical and computational aspects. We then perform extensive experimental comparison on the task of model fitting, and give intuition about which type of model is better for a few applications. Due to the wide availability of databases of high-quality data, we use the human face as the specific shape we wish to extract from corrupted data.

@&#INTRODUCTION@&#
Whether through laser range scanners, stereo reconstruction or structured light, methods and systems for 3D sensing and acquisition are now commonplace. However, these systems typically incur some amount of noise. Further, if we are interested in a particular object within data, it may be occluded by other objects. To extract the shape of an object of a particular class from such data it is often advantageous to use a statistical shape model to ensure that the extracted shape is valid for that object class. Another way of saying this, is that the shape is restricted to lie in a statistical shape space. In this paper, we review and describe, within a common mathematical framework, a wide variety of statistical shape spaces used for robust fitting to noisy and ambiguous data. We further perform a thorough theoretical analysis and experimental comparison of variants of such a model in which statistics are learned over global and local extents.Such a statistical model must be learned from a database of consistently parametrized (i.e. registered) instances from the object class in question. One class of shape for which there exist a number of available databases is the shape of the human face, and this is the shape which we use for evaluation in this paper. Furthermore, the 3D shape of human faces is important to a wide variety of applications, ranging from tele-presence to virtual avatar control to face recognition. However, we emphasize that the principles, models and algorithms discussed in this paper are applicable to any class of shapes for which a database containing parametrized data is available.The main reason to use a statistical shape model to fit to data, instead of a template fitting with a non-rigid iterative closest point (ICP) approach and some kind of regularization constraint, is that by learning a statistical model for a class of shapes, we can significantly reduce the search space, which results in the ability to reconstruct the underlying shape in the presence of severe noise or occlusions. These and other types of ambiguities are often present in real-world data captured in uncontrolled environments.The purpose of this paper is to provide researchers, engineers and end-application developers interested in employing existing statistical models or developing new ones with a targeted, focused review that includes a thorough analysis and comparison of the costs and benefits of two statistical models for the task of model fitting to noisy, corrupted or incomplete 3D data. The task of fitting a 3D shape model to ambiguous data is important for many applications, such as recognition tasks (identity and expression recognition in the case of faces, sex recognition and gait analysis in the case of human bodies), tele-presence, virtual avatar control, segmenting and extracting organ shapes from medical images for surgical planning or diagnosis, and so on. We specifically maintain a targeted scope to provide an in-depth analysis of the behaviors of various statistical models. Toward this end, we analyze different statistical models from those that model global geometry variation to those that model local geometry variation. We experimentally compare a purely global model to a purely local model, thereby providing a quantitative comparison of two models on either end of the spectrum of commonly used statistical models. Thus, the contribution of this paper is threefold:•An analysis of the theoretical properties, within a common mathematical framework, of a wide variety of seemingly very different statistical shape spaces.A quantitative and qualitative analysis, and extensive comparative evaluation of the practical performance of both global and local statistical shape models.We publish the learned statistical models and code to use them [1], thus allowing others to try them out and potentially fit them to other input modalities and for any application.We begin by reviewing the statistical shape spaces, or statistical shape models, that have been used for human face shapes, human body shapes, and medical data (Section 2). We give a common mathematical framework in which these models relate to one another and discuss the key differences between them. We then give a mathematical description of the process of learning or training a statistical shape space, and an in-depth focus on two specific models for human faces that represent extremes in the range of models (Section 3). This is followed by a mathematical description of the process of fitting a statistical model to ambiguous, partial or corrupted data via energy minimization (Section 4). Again, this is accompanied by an in-depth analysis of the process for the two shape spaces. We then provide an extensive experimental comparison of the two models (Section 5), followed by a discussion of the implications of the results (Sections 6 and 7).To extract an object from noisy input data, it helps to have a small basis in which the shape of the object can be represented. Traditionally, such bases have been generated for specific classes of models by artists. An example of such an artist-generated basis are blendshape models (as for instance used by Li et al. [2]), which can be used to encode a face performing different expressions. This way of generating a basis requires expertise about the possible deformations, and modeling the shapes is tedious. More recently, machine learning has been used to find a small basis from a set of training shapes using statistical analysis. This gives an easy and fully automatic way to find a small set of basis functions.In this vein, we formulate statistical shape analysis of a given object class as the task of finding a statistical shape space that efficiently and informatively represents the shape of objects of that class. We define a statistical shape space as a shape space equipped with a probability distribution, or prior, measuring how likely it is that an object of the given class would have a particular parametric representation in the shape space. The shape space itself is defined by the set of coefficients obtained by projecting the shapes onto the set of basis functions. (We use the terms basis functions and basis vectors somewhat interchangeably in the following; strictly speaking a basis function is only relevant for continuous surfaces, and in practice basis vectors are used for discrete data.)Thus, we focus on statistical shape analysis as a generative technique. A surface containing n vertices inR3is represented by d shape parameters or coefficients, which form a vectors∈Rd. A generator function(1)F(s):Rd→R3ngenerates from these shape parameters a surface representation (either mesh or point cloud) of n vertices. These shape parameters, and by extension the surface, can be fit to input data of varying modalities (3D point clouds, 3D voxel images, 2.5D depth, 2D images, sparse measurements, etc.), so long as there is a way to measure the distance between the surface and the data, or the quality of the fitting.As we see in the following review, by far the most common form of statistical analysis used for shapes is principle component analysis (PCA), which seeks a basis in which variance of the training data is maximized. The resulting basis vectors are the directions of greatest variation within the training data. Projecting the training samples onto this basis results in a diagonal sample covariance matrix. If the underlying distribution of the data is assumed to be multi-dimensional Gaussian, then this corresponds to the maximum likelihood estimate of the parameters of the density function. As a result, the resulting shape space is often equipped with a Gaussian prior. If this assumption does not hold, then a Gaussian prior may be arbitrarily far from the true prior, and choosing the correct prior may be challenging. A mathematical description of how to learn a statistical shape space using PCA is given in Section 3.2.The core aspect that varies between statistical shape spaces is the space in which PCA is performed. Careful selection of the space in which to perform PCA can lead to significantly better statistical properties. For many models, this amounts to a change of basis, followed by separate analyses in different sub-spaces of the transformed space. The resulting basis of the learned space is then formed by composing the decomposition basis with the sub-space PCA basis (directions of greatest variation within the subspace). For others, this amounts to a nonlinear transformation followed by a global analysis.This section reviews work on performing statistical shape analysis of 3D data for image processing applications. The categorization of the statistical models in different application domains is summarized in Table 1, where models and methods are grouped by the type of data they were applied to and by the extent of the basis functions used. The table further contains information on whether the models were designed to analyze articulation variations separately from shape variations (here, articulation can refer to facial expression, body posture, or the pose of bones before and after an operation).In this study, we focus on shape variations over a sample from a population, and hence in Sections 3.2 and 3.3 and we analyze and compare statistical models for faces without expression variations. In Section 5 we perform an extensive experimental comparison of the models. This allows us to better examine the differences between the statistical models themselves with respect to the model-fitting task.The first step to performing shape analysis is to acquire and register a set of training shapes that capture the shape variability that is of interest for a particular application. Subsequently, statistical shape analysis is performed on the registered training shapes: the shapes are projected onto a basis of choice and a probability distribution is fitted to the resulting coefficients to obtain a prior distribution for the shapes of interest. Without correspondence information, this statistical analysis is not possible. However, as indicated in the last column of Table 1, a few methods simultaneously compute a parametrization of a population of shapes while building a statistical model.Computing correspondences between a population of shapes is a challenging problem, and a detailed discussion about possible approaches is beyond the scope of this work. We refer the reader to recent surveys [3,4] for more information. However, we emphasize that the quality of the registration greatly affects the quality of the resulting statistical models, and by using a high-quality registration in this study, computed as discussed in Section 5.1, we are able to better analyze the properties of models themselves rather than the effects of gross mis-registration.In computer vision, statistical 3D shape models are commonly used to infer the three-dimensional shape of an object from images, mostly for the purpose of image manipulation. While recently, different classes of shapes have been considered [5,6], shape models of human faces and human bodies are of special interest due to their immense applicability in human–machine interaction. In medical image analysis, statistical shape models are commonly used to segment medical images and to find correspondences and abnormalities of anatomical shapes. In the following, we review statistical shape spaces used to analyze human faces (Section 2.1), human bodies (Section 2.2) and medical data (Section 2.3).We start our review by summarizing the use of statistical shape models of human faces. Blanz and Vetter [7] proposed the first statistical shape model for 3D models of human faces. The model, called morphable model, captures both 3D shape and texture information and can be used to predict a 3D face shape from a single input image. Texture is an important cue for human faces, and can greatly help with fitting directly to images. However, in this paper, we focus on statistical analysis of shape for clarity. A parametrized database of 3D face scans, mostly in neutral expression, is used to learn the statistical model using standard PCA. Given an input image in neutral expression, the learned shape space is searched to find the textured shape that best explains the input image using an optimization technique. This successful approach resulted in multiple follow-up works [8,9]. Amberg et al. [11] extended the morphable model to model expression variation by computing offsets from the neutral pose and performing PCA over these offset surfaces. With this method, they are able to include expression variation while maintaining a single linear model.Blanz and Vetter [7] experimented with manually segmenting the morphable model into four regions. A morphable model is then learned for each region and the regions are fitted to the data independently and merged in a post-processing step. This part-based model was shown to lead to a higher data accuracy than the global morphable model [7]. As this approach is suitable to obtain good fitting results in localized regions, it has been used in multiple follow-up works [16,17,19]. All of these methods proceed by manually segmenting the faces, by learning an independent morphable model for each part, and by fitting the parts to data independently. In this case, a basis arises out of the set of indicator functions on the segments, which are then composed with the union of PCA bases on the segments. As a result, the areas at the boundaries of the individual parts are not necessarily continuous, and a post-processing step is used to merge the fitted patches. Smet and Van Gool [18] proposed a similar method where the segmentation is found automatically by clustering the vertices based on features derived from their displacements over the training set. To address the potential discontinuities at the boundaries of the segments, they smoothly weight the segments to obtain regionalized basis functions for the training data. To estimate the optimal weights, a complex iterative algorithm is used. In each iteration, the training set is randomly partitioned into two disjoint subsets, where one set is used as test set to be reconstructed from the model learned on the other. This is followed by a two-step procedure. First, optimal reconstruction coefficients are estimated by weighted least-squares. Second, the optimal weights are estimated by solving many independent linear systems. Since for high-resolution training data this iterative algorithm converges slowly, they employ a coarse-to-fine approach to accelerate convergence.Brunton et al. [21] used a statistical analysis based on wavelet models to learn many localized independent prior distributions at multiple scales for the 3D shape of human faces. This allows to capture and combine localized shape variations in different areas of the face in a multi-resolution framework independently. They used this information to predict the 3D face shape from point clouds generated by stereo reconstruction. In this model, a basis is formed by composing the wavelet basis with the local PCA bases. As in the work by Blanz and Vetter, all faces are assumed to have a neutral facial expression. While PCA models such as the morphable model and part-based PCA models are commonly used in computer vision to model human faces, localized prior distributions based on wavelet models are only beginning to influence this field. In the following, we will refer to this method as local wavelet model.Golovinskiy et al. [20] proposed a statistical model based on hierarchical pyramids to synthesize geometric facial details. This statistical model, which allows for spatially varying geometric detail across the face, models the difference between a smooth face and a high-resolution face with geometric details, such as wrinkles.To allow for varying facial expressions, Vlasic et al. [12] used a statistical method based on tensor algebra called the multilinear model to analyze a set of faces captured of subjects performing a variety of facial expressions. This approach can be viewed as an extension of morphable models to higher dimensions. It is formed by taking the Cartesian product of two linear statistical shape spaces that capture variations according to two different attributes (i.e. identity and expression). The resulting basis is then the Cartesian product of the two bases associated with each attribute. This model allows for the prediction of a 3D face shape in multiple possible expressions from a single photograph.Yang et al. [10] exchanged the expression of a face in a single image based on a different input image of the same subject. For this application, they built multiple PCA spaces (one per expression) and combined these spaces for their application. A follow-up paper [14] used a multi-linear model to enhance or dampen expressions in videos.More recently, much work has focused on extracting a set of frames of three-dimensional face shapes from a video stream showing a face. The output of this type of algorithm is a four-dimensional sequence showing the three-dimensional face shape in motion. Dale et al. [13] extended the method by Vlasic et al. to compute such a four-dimensional sequence. This information is then used to exchange faces in video sequences; either keeping the sequences of expressions and replacing the identity of the subject, or keeping the identity and replacing the sequence of expressions. Bolkart and Wuhrer [15] learn a multi-linear model and fit it to sequences of 3D faces performing various expressions, producing a 4D parametrization, which can be used to animate a static scan with a specified expression.Another avenue of recent work is to track two-dimensional range images over time. These images can be captured using depth sensors, such as the Kinect sensor. Weise et al. [52] proposed a method to track a three-dimensional face model over time using prior information on the deformation model, and to use the tracked model to drive the animation of a virtual character in real-time. Unlike the other methods discussed in this section, this method learns a statistical prior that is subject-specific. In this paper, we do not consider subject-specific priors.Allen et al. [22] proposed a statistical model for human body shapes acquired in a similar posture that is similar to the morphable face model introduced by Blanz and Vetter. One main difference is that Allen et al. only learn information about the 3D shape and not about texture. We denote this model by global PCA model in the following. This model has been used to predict a 3D human body shape in a similar posture from one or more images [23–25] and from measurements [26]. Xi et al. [38] used a part-based PCA model to infer body shapes from silhouettes. Wuhrer et al. [37] performed PCA on a local shape representation based on the Laplace operator to analyze human body shape variations independently of posture changes. This is an example of a non-linear transform followed by a global PCA in the transformed space, where the transformed space has been chosen to be invariant to local rigid deformations, making the resulting statistical analysis posture invariant.To allow for posture variation, Anguelov et al. [27] proposed the SCAPE model. This model learns a PCA shape space for body shape variations using a database containing multiple subjects in a similar posture. Furthermore, the model learns a mapping from posture parameters (based on a skeleton) to shape changes using a database containing one subject in multiple poses. The model then combines the two variations using the assumption that body shape and posture are decorrelated. Since the SCAPE model successfully models human body shape and posture, it has been used to predict a 3D body shape in arbitrary posture from a single image [28]. Furthermore, this model can be used to predict a 3D human body shape in arbitrary posture based on a set of input images of a dressed person [29]. Such a 3D prediction can then be used to modify the input image [30]. Just like in the case of human faces, more recently, much work has focused on finding a four-dimensional sequence of three-dimensional human body shapes in motion from a video sequence. Jain et al. [31] used this to modify human body shapes in video sequences. Weiss et al. [32] proposed to use the SCAPE model to compute a 3D body scan from noisy Kinect data.A recent approach by Hirshberg et al. [33] treats the problems of learning a SCAPE model and computing point-to-point correspondences between a set of training data simultaneously by solving a single variational problem.A different avenue to allow for posture variation is to model shape and posture changes as correlated. This assumption is relevant, since the difference of the human body shape of the same subject in different postures depends on the body shape, e.g. on how muscular the subject is. Hasler et al. [34] proposed a shape space that jointly captures shape and posture variations by performing PCA on a rotation-invariant encoding of the shapes. This shape space is then used to predict the body shape of a dressed subject [35].An alternative for correlating human shape and posture variations is to use a multilinear model (as Vlasic et al. [12] do for face shapes). This avenue was explored by Hasler et al. [36] for the application of predicting 3D body shape and posture from an image. Chen et al. [39] proposed a part-based multilinear model using a manual segmentation into body parts.In medical imaging, one is especially interested in a body part, such as an organ or part thereof. Tasks of interest include finding the shape of interest in a medical image. This decomposition of the image is often called segmentation. To solve this task, Cootes et al. [40] propose the use of a statistical prior called active shape model, which is similar to the morphable model for faces. The active shape model learns the distribution of a set of registered and aligned training shapes using PCA, and uses this prior information to segment a given medical image. This model is commonly used for image segmentation, see Cootes and Taylor [41] and references therein.The quality of the registration of the training shapes directly influences the quality of the statistical shape model. Davies et al. [42] used this observation to derive an approach that jointly optimizes the registration and a shape model built using PCA. The main idea of this approach is that a good shape model should have a small information-theoretic description length. Hence, to simultaneously parameterize the shapes and build a statistical model, the approach by Davies et al. solves a variational problem that aims to minimize the description length of the PCA model.PCA assumes that the data can be approximated well using a linear model. However, in medical imaging, many data sets form a non-linear manifold in a high-dimensional space. To capture the structure of this high-dimensional manifold, Fletcher et al. [43] generalized PCA to this setting. This approach called principal geodesic analysis (PGA) considers geodesic distances between shapes measured along the high-dimensional manifold instead of Euclidean distances.One problem with active shape models is that they capture global shape variations. In medical imaging, one is often interested in detecting localized shape anomalies, as these can give insights in whether or not a specific organ is affected by a disease, for instance. In an active shape model, such local variations may be distributed over several principal components, and they may be controlled by principal components that capture a small percentage of the overall shape variability.To remedy this, part-based models have been proposed for medical imaging. Toews et al. [44] proposed a part-based model that captures the statistical variations of geometry and color. This model is used to analyze MRI volumes of the brain. Lecron et al. [45] used a different part-based model to analyze variations of the spine both within and across subjects. Each vertebra is considered one part and analyzed using a multilinear statistical model.Part-based models work well for applications where a segmentation into parts is meaningful. However, in many medical imaging tasks, a natural segmentation is difficult to define. To address this problem, Davatzikos et al. [46] used statistical analysis based on local wavelet models to learn a localized prior distribution of contours in images. Here, the localized regions with large variations are found automatically and do not need to be predefined. Nain et al. [47] extended this technique to use wavelets to perform a statistical analysis of three-dimensional shapes. Shape priors based on different types of wavelet models have been used to segment medical images [48–50]. Yu et al. [51] show that statistical wavelet models can be used to analyze cortical folding patterns, which is a challenging task.In the following, we provide an analytical comparative evaluation of a global PCA model and a local wavelet model. By doing so, we compare two methods on either end of the spectrum of the commonly used statistical shape models. It is already known [7] that manually segmented part-based decompositions improve fitting accuracy for human face shapes. A thorough numerical comparison has not been performed for automatically and object-class non-specific decompositions such as wavelets. We expect that, like part-based models, the local wavelet model will obtain locally more accurate results than the global model, since the basis functions have limited extent. However, since the part-decomposition for statistical models is to our knowledge always shape-specific, the local model can more readily be adapted to various application areas. In particular, when segmenting volumetric medical data for a particular organ or part of the brain, it may be impossible to identify distinct parts on which part-specific statistical models can be learned. However, one may still want localized fitting for these shapes.Applications of statistical model fitting are wide-ranging and require very different evaluation criteria. To evaluate them all would be impractical, and to evaluate one or two would unduly reduce the scope of this study. Our comparison (Section 5) therefore uses as a test meta-application the task of model fitting to point cloud data of human faces, and from the results we give an intuition of which model would be most appropriate for which end application (Section 6). All of the figures and tables in the following sections depend on the specific training data used, but they nonetheless illustrate how the geometric information captured in the statistical models can be evaluated and visualized. This comparison can provide a guide of how the locality of the basis functions of a model influence its performance for model fitting. Reconstruction of 3D shape from silhouettes or images, or segmentation of volumetric data, can also be viewed as a model fitting, with additional or different ambiguities, and therefore we infer that we can reasonably expect the statistical models to behave similarly in these scenarios.Furthermore, we allow those developing end applications to test the performance of both models in their specific scenario by publishing the statistical models and code to use them online [1]. The data and code provided will also allow others to derive fitting energies and code for different input modalities.For learning a statistical shape space, or training a statistical shape model, we assume we are given T training shapes in full correspondence. Fig. 1gives an overview of the model training. The key difference between the two models discussed in detail here is the basis in which a prior probability distribution is fit to the training data.We pre-align the data to remove rotation, translation, and uniform scale differences using generalized Procrustes analysis (GPA) [53]. Note that by removing uniform scale differences, we only consider shape differences and not size differences of the models. For data-fitting this is desirable due to different measurement units used by different acquisition systems, but in general this is application dependent [53]. GPA iteratively aligns each model to the mean shape and recomputes the mean. Removing transformations that are not of interest using GPA is an important pre-processing step that yields better statistical models.Learning a statistical shape space requires determining the basis functions or vectors, which define the shape space, and fitting a probability distribution to the resulting shape space coefficients from the training set. PCA-based methods perform these two steps simultaneously, selecting as a basis the directions of greatest variations in the data and computing a diagonal sample covariance matrix for the data projected onto this basis, which corresponds to a maximum likelihood estimate of a multi-dimensional Gaussian distribution. Part-based and wavelet-domain methods decompose the shapes into a localized basis before proceeding with PCA. The result is a basis consisting of the localized basis functions composed with the localized principal components. The learned prior is the product of the localized multi-dimensional Gaussian distributions. Note that the assumption of a Gaussian density function is only introduced when equipping the shape space with a Gaussian prior.With this framework, and by restricting ourselves to linear shape spaces for the purposes of this study, the generator function Eq. (1), can be written as a combination of the basis functions(2)F(s)=F‾+Φs=F‾+∑i=1dΦisiwhereF‾is the mean shape computed over the training set,Φ∈R3n×dis a matrix,Φi∈R3nare its columns, and as befores∈Rdis a vector of shape parameters. It is precisely the choice of Φ that determines the properties of the shape space, and determines the prior distribution learned from the training samples.If a statistical model with a small number of basis functions is fitted to a face, the result contains little shape detail, because the model only represents a small proportion of the variability of the training data. Keeping a large number of basis functions may cause the model to overfit to the training data. That is, the learned space may contain a bias towards shapes present in the training set. Overfitting can occur when the model is underconstrained by the training samples. This may occur if the model itself has too many degrees of freedom, or when it is learned in a very high-dimensional space, such as when the model is computed directly from very high-dimensional training samples.To pick a number of basis functions d that preserves a high amount of variability yet does not overfit the training data, we use the following three error measures similar to compactness, generalization, and specificity[54]. We use a slight modification of the original error measures to obtain results that are independent of the size of the training data.Compactness measures how much variability of the training data is explained by the learned statistical model. That is, we want to measure what fraction of the total variability of the training data is captured by d model parameters. This provides a measure of how well a given number of parameters explains the training data.Generalization measures the ability of the model to represent data, which are not part of the training set. To calculate this measure, we learn a PCA model on a subset of the training data, where one subject is excluded. The excluded subject is projected to the PCA space, reconstructed, and the distance between the source and the reconstruction is measured. To measure the distance between two faces, we use the average Euclidean vertex distance computed between all corresponding vertices. We perform this measurement for all subjects. The mean and standard deviation are then considered.Specificity measures the similarity between reconstructions from the statistical model and the training data. This estimates the plausibility of a random face represented using the learned shape space. To calculate specificity we choose a set of random points sampled from the probability distribution of the learned statistical shape space. For each of these points we reconstruct the shape using Eq. (2) and compute the distance to the closest face in the training data. The distance between two faces is computed as above. The mean and standard deviation for the random sample are then considered.Principal component analysis aims to reduce the complexity of a set of data. Due to its simplicity it is widely used for shape analysis. PCA is a linear transformation of a set of vectors fromR3ntoRdwithd<3n. A vectorf∈R3nis expressed by the scalar weightssiin a d-dimensional subspace, spanned by the orthogonal vectorsVi, by(3)F(s)=F‾+∑i=1dsiViFor each parameterized shape of the training set we have one vectorFi(train)∈R3nthat contains an ordered coordinate set of all points of the ith training shape. The vectorsViare the eigenvectors of the data covariance matrix(4)ΣF=1T∑i=1nFi(train)-F‾Fi(train)-F‾TwhereF‾is the mean of the training data. The eigenvectorsViare ordered with respect to the non-increasing corresponding eigenvaluesλi. The eigenvaluesλimeasure the variability captured by the ith principal component. More specifically,Vicaptures100λi∑i=1T-1λi%of the variability of the training data. The rank of the data covariance matrix is at mostmin(3n-1,T-1)and therefore the number of distinct non-zero eigenvalues and hence, the number or principal components, is at mostmin(3n-1,T-1).Thus, we get our basis directly from the data via the principal components: in matrix formΦGhas columnsΦGi=Vi, fori=1,…,d, whered⩽min(3n-1,T-1). Note that every basis vectorΦGihas global support in general: all3nelements are in general non-zero, and every vertex is influenced.The global shape space represents the high-dimensional differences of the training faces in a low-dimensional shape space that is spanned by the corresponding eigenvectors of the d largest eigenvalues of the data covariance matrix.Fig. 2visualizes the variations along two principal components in the range of-3σito+3σi, whereσidenotes the standard deviation of the ith principal component.The amount of details that can be expressed by the global statistical model is limited by the details present in the training data. It would be useful in some applications to increase the variability of the training data by increasing the mesh resolution of the training data by inserting new vertices as linear combinations of existing vertices. Unfortunately, this is not possible using this global approach. If a vertex expressed as a fixed linear combination of existing vertices is inserted to each training mesh, the corresponding additional vertex in the fitted surface is identical to the corresponding fixed linear combination. Therefore, if an additional vertex is chosen to be placed on a triangle, the corresponding additional vertex is located on the corresponding triangle of the fitted result. Hence, using fixed linear combinations to add points to the surface of the model and fitting this extended surface to a target face leads to the same result as fitting the original model to the target face and adding the points into the resulting surface using the fixed linear combinations. This is a key difference to the local model reviewed in Section 3.3.Computing the compactness statistical measure is straightforward for the global PCA model. For d principal components, compactness is defined as(5)C(d)=∑i=1dλi∑i=1T-1λiwhereλiis the ith eigenvalue of the data covariance matrix. Computation of generalization and specificity are also straightforward.The core principle behind wavelet transforms is to project sampled data onto a set of basis functions that are localized in space and frequency. In our context, we use the wavelet basis as a prefix, and extract data driven basis for individual wavelet coefficients using PCA. Second-generation or lifting wavelets [55] are computed in time linear in the number of samples in the original signal using local lifting operations and sub-sampling at each scale. The samples are partitioned into maximally correlated subsets, e.g. odd and even samples for signals on 1D domains. One subset is then used to predict the other, and the residual of this prediction is then called the detail, or wavelet, coefficients. The detail coefficients are then used to update the other subset, giving approximation, or scaling, coefficients. The process is repeated on the scaling coefficients. We refer the reader to Sweldens [55] for a more thorough explanation.While wavelets were originally defined regularly sampled Euclidean domains [56], spherical wavelets [57] are defined on subdivision surfaces, often topological spheres. A commonly used wavelet basis is a biorthogonal generalized B-spline basis [58] that uses the Catmull–Clark subdivision scheme and has been applied in multiple application domains [21,49]. The prediction and update operators are B-spline interpolations from the neighboring vertices. This scheme is stable for linear and cubic B-splines; our comparison uses the linear basis as we found that this produced satisfactory results.Aside from wavelet coefficients, one might use any localized basis as a prefix to PCA or other data-driven bases. However, wavelet bases have the important property that they decorrelate the data, meaning the resulting coefficients can be analyzed separately. Since individual coefficients are of much lower dimension than the whole surface, this greatly reduces the risk of overfitting, given the same number of training samples T.Performing PCA over the whole set of wavelet coefficients would result in the same principal components as the global model, because the wavelet transform is a linear transform, and PCA essentially just rotates the data so that the coordinate axes align with the directions of greatest variations. Instead, this method performs PCA locally on each coefficient, which is a 3D vector quantity, over the database.First, let us denote the mean of each wavelet coefficient over the database by(6)s¯k=1T∑i=1Tsikwhere k indexes the coefficients.While we can perform statistical analysis on eachskindependently of other values of k, we must consider their three components together. Eachskis a 3D vector representing either the scale (absolute value) or the detail (relative value) of the shape at a particular frequency and spatial location. However, the coordinate axes in general do not correspond to the directions of greatest variation in the database. Therefore, we perform PCA on each set of coefficient vectors, to obtain 3D vectorsrikthat represent the position along the directions of greatest variation, and3×3matricesUkthat transform these coordinates to our original world coordinate system, as in(7)sik=s¯k+Ukrikwhere we writesk=[xsk,ysk,zsk]Tandrk=[xrk,yrk,zrk]Tto denote the components of these vectors. Applying the transform(Uk)Tto the data diagonalizes the covariance matrix, thus making each component independent.The reconstruction of the face shape from the model is then given by the inverse wavelet transform(8)F(s)i=∑o∈V(0)ϕ0o(i)so+∑j=0J-1∑l∈W(j)ψjl(i)slwhere i is the vertex index in the reconstructed surface, j is the level of wavelet coefficient, J is the number of levels used,V(0)is the set of scaling functions at level zero,W(j)is the set of wavelet functions at level j, o and l are the coefficient indices,ϕ0ois the scaling function at the coarsest level centered on location o, andψjlis the wavelet function at level j and location l. While the transform is expressed here in terms of basis functions, it is computed using lifting operators, which amount to weighted averages of a vertex’s local neighborhood, and it can be expressed as a matrix multiplication. The basis functions themselves,ϕjandψj, are B-spline approximations to Gaussian and Mexican-hat functions. For more details, see Bertram et al. [58].We can now construct the combined basisΦWas follows. Observing that the inverse wavelet transform, Eq. (8), is a linear operator on the vector of concatenated wavelet coefficientss, we can write it as a3n×3nmatrixD-1. Thus, we have(9)F(s)=D-1sand from Eq. (7) we have(10)F(s)=D-1s¯+D-1Uswheres¯is the concatenation of the coefficient meanss¯k, and U is a block-diagonal3n×3nmatrix with the matricesUkon the diagonal. Therefore, becauseD-1s¯=F‾, we have(11)ΦW=D-1Uas our combined basis, and the dimensionality of our shape space isd=3n. Note thatΦWhas full rank.To use a spherical wavelet basis to represent shape, it must be a subdivision surface, in our case Catmull–Clark subdivision hierarchy. For training, the surfaces in the database are typically stored as triangle meshes without subdivision structure. Thus, we must resample the surfaces with the proper structure. The subdivision scheme uses quadrilateral elements, although it can handle extraordinary vertices. We resample the triangle meshes using the custom, yet straightforward, technique by Brunton et al. [21] tailored to the fact that we are dealing with faces, which are topologically like a disc. In principle, however, any quad-remeshing technique can be used.The local model has the benefit that it avoids overfitting, and as a consequence we can keep all variability present in the training set. Intuitively, the local surface properties of any given surface point are not likely to be specific to one set of faces or another. Whereas for the global model a bias in the training set, over-representation of one sex or a particular ethnicity or age range, can cause the lesser principal components to be highly specialized to that set, the geometry of a local surface patch is likely to be less dependent on the training data. The consequence is a somewhat unexpected behavior: by training and combining many low-dimensional models, which due to the limited flexibility of the training space (R3) have reduced sensitivity to bias in the training set, we get a final model with much greater flexibility, because truncation becomes unnecessary.Fig. 3visualizes the mean shape color-coded with the magnitude of the shape variability for four levels of the wavelet subdivision, which corresponds to the localized shape variations at different scales. At finer scales, the variation quickly localizes around major facial features and reduces in magnitude.The dimensionality of the local model is statistically more favorable. If, as is usually the case, the number of vertices is much greater than the number of training examples,n≫T, then the global model has problems of fitting to the particularities of the training set. In the local model, many independent statistical priors are learned, each with dimension 3. We have many more training examples than that. The independence of the local priors further allows an exhaustive search of the parameter space. Thus, we have no danger of getting trapped in local minima.The drawback of these properties, in particular of retaining all the variability of the training data, is that the local model is a much higher-dimensional representation than the global model. Thus, the dimensionality of the local model can be computationally much less favorable. There is, however, a trade-off that can be made by using the wavelet basis progressively, and working with the localized priors only up to a certain level.As the lifting operations of the wavelet transform amount to local weighted averages of vertex coordinates, the transform can be expressed as a matrix multiplication, if the surface is expressed as a vector containing the vertex coordinates. Because the transform is biorthogonal, this matrix is square and has full rank. In contrast, the global PCA basisΦG, has rankd⩽min(3n-1,T-1). As discussed in Section 3.2, resampling the surface at linear combinations of vertex coordinates (e.g., within a triangle), does not increase the rank of the transform. However, becauseΦWhas full rank, we can obtain more detail by linearly upsampling the training surfaces. This means that we can resample the training shapes at high resolution to obtain a statistical model that captures fine shape detail.The statistical measures generalization and specificity can be computed in the same manner as for the global PCA model. While there is no simple formula for the compactness of the wavelet model, if we retain all3nshape parameters, and hence all variation, the compactness measure is fixed at 100%. In this case, the generalization measure is 0.In this section we give an overview of the generic model fitting approach shown in Fig. 4. This assumes that a statistical shape model has been learned, for instance using one of the approaches given in Sections 3.2 and 3.3. As can be seen from Fig. 4, the process begins with an initial alignment using either landmarks or feature points, described in Section 4.1, regardless of the statistical shape space. Subsequently, we minimize an energy function with respect to the model parameters (Section 4.2), which are specific to the statistical shape space, and may affect the minimization strategy (Sections 4.3 and 4.4).To fit a statistical shape model to an input data set, we first need to align the input data and the statistical shape model to be in the same global coordinate system. Since we consider only shape differences in the training data, the initial alignment aims to find the rotation, translation, and uniform scaling that best aligns the statistical shape model with the input data.To compute such an initial alignment, corresponding landmarks are commonly used. These landmarks can be manually located on the mean shape of the aligned training database once. On the input data, the landmarks can be predicted in a fully automatic way [59,60]. However, since we use a test database that contains a set of landmarks, we choose to use a subset of these landmarks (the ones shown in red in Fig. 5a) to compute an initial alignment. This approach removes a potential source of fitting error due to landmark prediction inaccuracies.Another commonly used way to rigidly align two shapes is to use automatically detected features. We test a method of this flavor in our experiments. The method we use proceeds by finding corresponding features on the mean face and the input scan using spin images [61] and by performing random sample consensus [62]. This fully automatic method is expected to lead to less accurate alignments than the use of the given landmarks.Our goal is to fit the statistical shape model to the input data as closely as possible while staying in the learned shape space. To fit our model to data, we minimize an energy function that amounts to the sum of squared distances between each model vertex and its nearest neighbor in the input point cloud. For our experiments, we use the following commonly used basic energy to pull the model towards the data(12)Edata(s)=∑i=1nmin‖fi-pNN(i)‖22,τwherefiis vertex i ofF(s)(see Eq. (1)),p∈Pis a point in the input point cloud,NN(i)returns the index of the nearest neighbor in P offi, and τ is a truncation threshold to add robustness against outliers. We compute nearest neighbors with a k-d tree using the implementation in ANN [63].When fitting a statistical shape model to data, the space of possible solutions should only contain likely shapes, thus ensuring that only plausible results are possible. A common and intuitive approach is to use the (negative logarithm of the) learned prior distribution as an energy term. In the case of PCA, it is common to assume a multi-dimensional Gaussian centered on the mean shape. In terms of energy minimization, this amounts to placing a soft constraint that the solution should be close to the mean. By design, however, this introduces a bias into the optimization, and results using this technique tend to lose distinctiveness and look similar to the mean.Patel and Smith [64] proposed an alternative prior that is aimed at maintaining the distinctiveness of the models. They model the shape space as a manifold that is at a constant Mahalanobis distance from the mean. This is based on the observation that the squared Mahalanobis distances from the mean of a set of d-dimensional normally distributed vectors form aχd2distribution with expected value equal to d. Hence, Patel and Smith restrict the shapes to be on the hyper-ellipsoid at Mahalanobis distancedfrom the mean in order to preserve shape distinctiveness. While this approach models distinctiveness using the expected Mahalanobis distance from the mean, it does not consider the normal distributions along each dimension of the shape space. That is, the modeled shape space may contain highly unlikely shapes along the directions of the principal components, as can be seen for the shape at the intersection of the hyper-ellipsoid shown in red and the x-axis in Fig. 6.To simultaneously avoid mean-shape bias and highly unlikely shapes, in our experiments we constrain the shape to lie within a region of the shape space where the prior probability is sufficiently high, i.e. the shape is sufficiently likely. Ideally, in the case of PCA models, one would impose ellipsoidal constraints corresponding to a probability isosurface of a given value. However, from an optimization standpoint, it is much easier and more efficient to impose linear constraints. Hence, we constrain the shape to lie within the hyper-box of±cσiabout the mean shape, whereσiis the standard deviation of the training data along dimension i of the learned statistical space, and c is a parameter controlling the amount of deviation allowed. Fig. 6 shows a two-dimensional plot of this hyper-box. We demonstrate that in this shape space, by minimizing an energy function with only the data term given in Eq. (12), we can maintain distinctiveness, while avoiding unrealistic shapes. This is equivalent to a prior probability of the form(13)P(s)=∏i=1dPi(si)where(14)Pi(si)=1|si|⩽cσi0otherwiseif we assume the shape parameterssiare centered (mean subtracted). We call this a hyper-box prior.A common post-process to fitting the parameters of the statistical models, is to then leave the statistical shape space and perform a fine-fitting of the vertex positions directly, similar to a template fitting method. We deliberately do not do this for two reasons. First, such a step is most often necessary when the learned shape space is not sufficiently generalizable to express novel shapes. This can occur due to insufficient or poor training data. However, as detailed in Section 5, we train from clean data containing a good sampling of both sexes and different ethnicities with a high-quality registration. Thus, our learned models are of high-quality.Second, we wish to study the properties of the statistical shape spaces themselves, and leaving the space in a post-process would inevitably introduce additional uncertainty in analyzing the fitting results in Section 5. This is particularly true when we evaluate the fitting in the presence of occlusions.While the energy in Eq. (12) is not strictly differentiable at all points, it is continuous, and the number of points where it is not differentiable is small. Hence, we can minimize it using a bounded Quasi-Newton method [65]. The coordinate bounds on the parameters enforce the condition given in Eq. (14). This minimizer gives super-linear convergence rates without the need for an explicit inverse Hessian. Note that this optimization technique does not guarantee to find the global optimum of the energy function.LettGdenote the number of iterations required for the minimization to converge, and let m denote the number of data points in the target shape. The complexity of building a k-d tree of m points in3DisO(mlogm), and a single nearest neighbor search takesO(m2/3)time [66]. Given the nearest neighbor indices for all n points, a single evaluation of the energy given in Eq. (12) takesO(n)time, and a single evaluation of its gradient takesO(nd)time. Thus, the overall time complexity of fitting the global PCA model to a data set isO(mlogm+tGn(d+m2/3)). For this model, each training shape contains n=5996 vertices.We minimize the energy in Eq. (12) using a global search of each parameter. That is, we sample uniformly within the range given by the hyper-box prior given in Eq. (14) for each component ofrkfor each k sequentially, starting with the coarsest resolution coefficients and progressively increasing the resolution. For each sampled value, we reconstruct the surface using Eqs. (7) and (8), and evaluate the nearest neighbor energy (Eq. (12)). The parameter value that minimizes this energy is then taken as the estimate for this parameter.Since we use a sampling approach to minimize the energy for the local model, the complexity depends not on the number of iterations of a nonlinear optimization, but on the number of samplestLper parameter. The number of wavelet coefficients is equal to the number of vertices in the subdivision mesh, which we denote by n. Note, however, that n increases as the number of subdivision levels J increases. In our experiments, we resample all training shapes with n=24,897 vertices. The complexity of the nearest neighbor search remains unchanged, as does the cost of evaluating the energy. However, the energy must be evaluatedtLtimes for each of the n coefficients. Hence, the overall complexity isO(mlogm+n(m2/3+tLn)), which is dominated by theO(n2tL)part. Thus, assumingn≫tG, we expect the local model, with its higher-dimensional representation to take much longer to fit to the same point cloud. However, a trade-off between detail and running time can be made by fitting coefficients only up to some level less than the number of levels in the wavelet decomposition.In this section we evaluate both the fitting speed and quality of the global and local models. We evaluate both models for different initial alignment strategies and for different types of severe occlusion. Furthermore, we evaluate the models qualitatively for noisy scan data acquired using affordable stereo and range camera setups.For training, we use the neutral expressions of T=100 subjects from the BU-3DFE database [67]. This database contains relatively clean surfaces without occlusions, and a typical cropped face contains about 7500 vertices. Furthermore, each cropped face is equipped with 83 landmark points.We parametrize the database using the method of Salazar et al. [60] that deforms a template to each input face. This method is capable of predicting landmark points to aid in the template fitting. However, since we are given manually placed landmarks, our algorithm uses these instead of predicted ones. This removes a potential source of error during registration. The template we use contains 5996 vertices. We choose this low-resolution template for parametrization since the database has low resolution and does not contain small shape details. While the BU-3DFE database contains six additional expressions in four different levels, we consider only neutral expressions for our comparison. The resulting registration is of high-quality, which has been verified by manually inspecting each registered face.We use a subset of 20 subjects (10 female and 10 male) of the Bosphorus database [68] to test our algorithm. Each subject is present in five occlusion levels: without occlusion, with glasses, with an occlusion of one eye by a hand, with an occlusion of the mouth by a hand, and with an occlusion of parts of the face by hair. Examples for each occlusion class can be seen in the left column of Fig. 11. We chose this database as it allows the evaluation of different methods in the presence of severe occlusion. The resolution of this database is fairly high, and a typical face contains about 35,000 vertices.Each face is annotated with up to 22 landmarks. Fig. 5a shows a model of the Bosphorus database with 22 landmark positions. The landmarks shown in red are used to compute an initial alignment of the test face to the learned shape space and the landmarks shown in green are used for error evaluation. Not all of the landmarks may be present in the database for two reasons: first, landmarks may be missing due to occlusion, and second, some landmarks are placed erroneously and we manually removed these landmarks. The landmarks that are present are not perfectly located, as shown in Fig. 5b. Here, the location of the landmark at the nose tip is slightly shifted for two models of the same identity. We observed that in our test set the location of the landmarks usually varies by as much as 1cm across different scans.

@&#CONCLUSIONS@&#
