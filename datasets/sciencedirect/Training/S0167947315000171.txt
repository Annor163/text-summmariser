@&#MAIN-TITLE@&#
Identifying connected components in Gaussian finite mixture models for clustering

@&#HIGHLIGHTS@&#
Clusters are identified as connected components from high density regions.Gaussian finite mixture models are used for density estimation.Identified clusters are not constrained to have a Gaussian shape.Clusters need not be obtained by combining mixture components.A dimension reduction step is used in cases of higher data dimensionality.

@&#KEYPHRASES@&#
Finite mixture of Gaussian distributions,Cluster analysis,Connected components,High density regions,Cluster cores,

@&#ABSTRACT@&#
Model-based clustering associates each component of a finite mixture distribution to a group or cluster. Therefore, an underlying implicit assumption is that a one-to-one correspondence exists between mixture components and clusters. In applications with multivariate continuous data, finite mixtures of Gaussian distributions are typically used. Information criteria, such as BIC, are often employed to select the number of mixture components. However, a single Gaussian density may not be sufficient, and two or more mixture components could be needed to reasonably approximate the distribution within a homogeneous group of observations. A clustering method, based on the identification of high density regions of the underlying density function, is introduced. Starting with an estimated Gaussian finite mixture model, the corresponding density estimate is used to identify the cluster cores, i.e. those data points which form the core of the clusters. Then, the remaining observations are allocated to those cluster cores for which the probability of cluster membership is the highest. The method is illustrated using both simulated and real data examples, which show how the proposed approach improves the identification of non-Gaussian clusters compared to a fully parametric approach. Furthermore, it enables the identification of clusters which cannot be obtained by merging mixture components, and it can be straightforwardly extended to cases of higher dimensionality.

@&#INTRODUCTION@&#
Clustering methods aim to identify groups of similar observations that are relatively separate from each other. Since we ignore the “true” groupings, even though one does exist, these are also called unsupervised learning methods. In the model-based approach to clustering, each component of a finite mixture of density functions belonging to a given parametric class is associated with a group or cluster. Multivariate Gaussian distribution is often adopted with continuous data. However, a non-Gaussian cluster may require more than one single mixture component. As a result, there is no longer a one-to-one relationship between mixture components and clusters. In the same vein, it was noted that “it can be misleading to identify the number of Gaussian components with the number of clusters” (Hennig, 2010, p. 5).This problem was recently addressed by Baudry et al. (2010), who proposed a method based on an entropy criterion for hierarchically combining mixture components to form clusters. Hennig (2010) also discussed hierarchical merging methods based on unimodality and misclassification probabilities. The main limitation of these approaches is that clusters can only be obtained by merging two or more components. Therefore, data points assigned to a single Gaussian component cannot be subsequently allocated to different clusters.In the development of the method described in this paper, we adopted the definition introduced by Hartigan (1975, p. 205) from among several definitions of the concept of ‘cluster’: “Clusters may be thought of as regions of high density separated from other such regions by regions of low density”. The idea of using a density estimate as the basis for clustering methods has been considered in various recent papers, both in statistical and in machine learning literature. An earlier application in computer vision and image processing is the mean shift algorithm (Fukunaga and Hostetler, 1975), which is a mode-seeking algorithm for detecting the modes of a nonparametric density estimate. More recently, Stuetzle (2003) presented a method which exploits the connection between the minimum spanning tree of a sample and the nearest neighbour density estimation. An alternative to mode clustering is level set clustering, where the aim is to find the hierarchical structure of connected components of a density level set (Stuetzle and Nugent, 2010). This working definition of ‘cluster’ was also adopted recently by Azzalini and Torelli (2007) and Menardi and Azzalini (2014). In particular, Azzalini and Torelli (2007) employed nonparametric density estimation to identify the mode function, where each mode is associated with a subset of points with high density, formed by means of suitable manipulation of the associated Delaunay triangulation.Unlike traditional methods of cluster analysis based on heuristic or distance-based procedures, finite mixture modelling provides a formal statistical framework on which to base the clustering procedure. Unfortunately, the relationship between the number of modes and the number of components in the mixture is very complex. Carreira-Perpiñán and Williams (2003) showed that if the components of a mixture model have the same covariance matrix (up to a scaling factor), then the number of modes cannot exceed the number of components. However, the number of modes can be larger than the number of components, when the components are allowed to have different covariance matrices.In this paper, we propose a method which, assuming the working definition of clusters given by Hartigan (1975), adapt the methodology of Azzalini and Torelli (2007) to model-based clustering. Starting with an estimated Gaussian finite mixture model, the corresponding density estimate is used to identify the cluster cores, i.e. those data points which form the core of the clusters. Then, the remaining observations are allocated to those cluster cores for which the probability of cluster membership is the highest. This approach improves the identification of non-Gaussian clusters compared to a fully parametric approach. Furthermore, it enables the identification of clusters which cannot be obtained by combining mixture components, and, finally, it can be straightforwardly expanded to cases of higher dimensionality.A motivating example: Old Faithful data. Consider the data for the waiting time between eruptions (waiting) and the duration of the eruptions (eruptions) for the Old Faithful geyser in Yellowstone National Park, Wyoming, USA. Fig. 1(a) shows the clustering partition obtained by fitting the best Gaussian finite mixture model according to BIC. The selected model is a mixture of three components with common full covariance matrix. However, there appear to be two separate groups of points. The cluster with high values for both waiting and eruptions clearly cannot be fitted by a single Gaussian component. Nonetheless, the corresponding bivariate density estimate shown in Fig. 1(b) indicates the presence of two separate regions of high density. The method proposed in this paper aims to deal with similar situations.The outline of this article is as follows. Section  2 gives a brief review of background material on model-based clustering. Section  3 contains the proposal for identifying the cluster cores from connected components, and discusses the classification of the remaining unallocated points. Section  4 presents the extension of the proposed method to the case of high dimensional features. Sections  5 and 6 illustrate empirical results using synthetic and real data examples, respectively. The final section provides some concluding remarks.

@&#CONCLUSIONS@&#
