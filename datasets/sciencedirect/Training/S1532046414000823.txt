@&#MAIN-TITLE@&#
A framework to preserve the privacy of electronic health data streams

@&#HIGHLIGHTS@&#
The release of electronic health data in the form of data streams may lead to privacy violations.Existing techniques for offering privacy in data streams generate a significant delay in processing the data.We propose a delay-free anonymization framework to protect electronic health data streams.Input data streams are anonymized by using l-diverse counterfeit values.The effective management of counterfeits improves the utility of the anonymized data.

@&#KEYPHRASES@&#
Health data stream,Privacy,Anonymization,

@&#ABSTRACT@&#
The anonymization of health data streams is important to protect these data against potential privacy breaches. A large number of research studies aiming at offering privacy in the context of data streams has been recently conducted. However, the techniques that have been proposed in these studies generate a significant delay during the anonymization process, since they concentrate on applying existing privacy models (e.g., k-anonymity and l-diversity) to batches of data extracted from data streams in a period of time. In this paper, we present delay-free anonymization, a framework for preserving the privacy of electronic health data streams. Unlike existing works, our method does not generate an accumulation delay, since input streams are anonymized immediately with counterfeit values. We further devise late validation for increasing the data utility of the anonymization results and managing the counterfeit values. Through experiments, we show the efficiency and effectiveness of the proposed method for the real-time release of data streams.

@&#INTRODUCTION@&#
Recently, data streams of health records have been widely utilized in many applications, such as real-time diagnosis systems, biomedical data analysis, and health monitoring services [1–4]. Diagnosis systems, for example, diagnose patients’ diseases by monitoring the behavior of their real-time health data, and biomedical data analysts study the patterns of diseases by using the data streams from bio-signal sensors. In many cases, the data streams are entrusted to third parties who analyze the data in place of the data holders, because of the effective data utilization.Meanwhile, health data streams typically contain private attributes, such as age, gender, various bio-signals, and diagnoses. Privacy problems can arise in relation to these types of data during the transmission of the data streams to third parties, which can provide paths for a privacy attack. For example, consider a data stream with the schema (tupleID, time, age, sex, diagnosis) released from a hospital for the purpose of a biomedical study. The schema does not contain any direct identifiers, such as SSNs (social security numbers), that can distinguish individuals. However, using some background knowledge, i.e., time, age, and sex, an attacker can identify the diagnosis of the target individual [5,6].To protect data streams from background knowledge attacks, several privacy-preserving methods have been recently proposed [7–14]. These methods ensure that the released data streams meet the requirements of typical privacy models, such as k-anonymity [5] and l-diversity [6]. Although these methods differ in the way they transform the data streams to offer privacy protection, they are all based on the tuple accumulation strategy. That is, streaming tuples are postponed until they satisfy the given publication condition, i.e., reach a certain privacy level or satisfactory data utility. We call this type of method the accumulation-based method.Fig. 1shows an example of offering privacy protection in data streams by using the accumulation-based method. Assume an input stream of tuples, where a tuple consists of multiple attributes that include personal attributes, i.e., QI (quasi-identifiers) and SI (sensitive information). In the accumulation-based method, the anonymization system continues to accumulate and cluster the input tuples until the given privacy condition, such as k-anonymity and l-diversity, is satisfied. In Fig. 1, tuple 1, which has arrived at timeT1and has been accumulated in Cluster 1, waits for tuple 2. When tuple 2 arrives, it is assigned to Cluster 1, then the cluster becomes 2-diverse. When a certain cluster satisfies the privacy condition, the tuples in the cluster are generalized and released. Then, the QIs of the tuples in the cluster become indistinguishable from each other according to the privacy condition. Consequently, an adversary cannot identify an exact tuple from the anonymized data by using the QIs (i.e., background knowledge). Thus, an adversary cannot be certain about the target’s SI, although he or she can infer the list of possible SIs.In addition to employing accumulation to satisfy the required privacy level, most existing methods aim at accumulating more tuples than necessary, in order to reduce the information loss that is caused by the applied data generalization. For example, tuple 4 in Fig. 1 is accumulated in Cluster 3 instead of Cluster 2, because if it was accumulated in Cluster 2 this would lead to increased information loss.There are two inevitable delay problems in the accumulation-based methods: accumulation and computation delay. The former is directly relevant to the accumulation of the input tuples. For example, in Fig. 1, the release of tuple 3 is delayed until tuple 6 has arrived. The delay is due to the time taken to calculate the information loss. To assign an input tuple to an appropriate cluster that generates the minimum information loss, the information loss of every cluster must be calculated. Computing the information loss of all clusters takes a significant amount of time, which prohibits the release of large data streams in real time.To solve these problems, in this paper, we propose a DF (delay-free anonymization) framework to preserve the privacy of electronic health data streams in real time. We consider two types of attributes of an input tuple: the quasi-identifier (QI) and the sensitive information (SI). In the DF framework, QIs are released with artificially generated l-diverse SI values. For example, in Fig. 2, tuple 1 is anonymized withST1, which is the l-diverse set of the SI. We organize the set with the original SI from the input tuple and the artificially generated SI. Then,QI1has multiple SIs (SIaandSId), which satisfies 2-diversity. Thus, attackers cannot identify the exact SI of the target QI. In this manner, our anonymization process preserves the privacy of the health data streams.We would like to note that there is no accumulation delay in DF because it immediately anonymizes the input tuples and releases them. This is possible because there is no tuple accumulation in our anonymization process. Furthermore, in comparison with the accumulation-based methods, DF generates remarkably little computation delay, because DF executes only simple operations (i.e., referring to the sensitive table and managing it) instead of complicated operations (e.g., calculating the information loss).We also considered the data utility of the anonymization results. Our anonymization method, generating l-diverse SI values, generates counterfeit values. For example, in Fig. 2,SIdinST1is a counterfeit atT1. Counterfeit values function as a protection mechanism to prevent the disclosure of the correct SI values; however, they also decrease data utility. To ameliorate this issue, we propose late validation, which utilizes the SI values of the newly entered tuples to validate the existing counterfeits. In Fig. 2,SIdinST1is late validated atT4. After tuple 4, whose SI isSId, is anonymized usingST1,SIdinST1is considered a real value. To measure the data utility, including the utility loss from the counterfeits, we further devise sensitive attribute uncertainty as the quality metric for DF.The contributions of our delay-free anonymization framework are summarized as follows:•Immediate release: DF facilitates tuple-by-tuple release with the guarantee of l-diversity. DF is characterized by no accumulation delay and a low computation cost, since it immediately releases the data streams and requires only simple operations. The immediate release is a big advantage for applications that utilize real-time data streams.High-level data utility: To anonymize data streams, DF artificially generates l-diverse SI values instead of generalizing QIs. Thus, DF does not generate the typical information loss with respect to QIs. In addition, the counterfeits in the sensitive table are also minimized by late validation.The rest of this paper is organized as follows. In Section 2, we present the related work in this area. In Section 3, we define the data model and propose DF. In Section 4, we present a late validation scheme for effective anonymization, and the development of the DF algorithm. In Section 5, we discuss data utility issues in the context of our proposed framework. Section 6 presents our experimental evaluation and Section 7 concludes this work.

@&#CONCLUSIONS@&#
