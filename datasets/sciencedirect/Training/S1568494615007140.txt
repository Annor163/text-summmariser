@&#MAIN-TITLE@&#
Choosing among weight-estimation methods for multi-criterion analysis: A case study for the design of multi-purpose offshore platforms

@&#HIGHLIGHTS@&#
MCA results depend on methods applied to estimate relative weights.A logically consistent set of data and weight-estimation methods is used.Linear regression, factor analysis, revised Simos procedure, analytical hierarchy process.Pairs of weight-estimation methods are compared in terms of their similarity.Single weight-estimation methods are ordered in terms of their reliability.

@&#KEYPHRASES@&#
Multi-criterion analysis,Weighting,Linear regression,Factor analysis,Revised Simos procedure,Analytical hierarchy process,

@&#ABSTRACT@&#
Application of the sustainability concept to environmental projects implies that at least three feature categories (i.e., economic, social, and environmental) must be taken into account by applying a participative multi-criterion analysis (MCA). However, MCA results depend crucially on the methodology applied to estimate the relative criterion weights. By using a logically consistent set of data and methods (i.e., linear regression [LR], factor analysis [FA], the revised Simos procedure [RSP], and the analytical hierarchy process [AHP]), the present study revealed that mistakes from using one weight-estimation method rather than an alternative are non-significant in terms of satisfaction of specified acceptable standards (i.e., a risk of up to 1% of erroneously rejecting an option), but significant for comparisons between options (i.e., a risk of up to 11% of choosing a worse option by rejecting a better option). In particular, the risks of these mistakes are larger if both differences in statistical or computational algorithms and in data sets are involved (e.g., LR vs. AHP). In addition, the present study revealed that the choice of weight-estimation methods should depend on the estimated and normalised score differences for the economic, social, and environmental features. However, on average, some pairs of weight-estimation methods are more similar (e.g., AHP vs. RSP and LR vs. AHP are the most and the least similar, respectively), and some single weight-estimation methods are more reliable (i.e., FA>RSP>AHP>LR).

@&#INTRODUCTION@&#
Application of the sustainability concept to environmental management, in general, and environmental projects, in particular, implies that at least three feature categories (i.e., economic, social, and environmental) must be taken into account (e.g., for environmental projects, see [1–5]). In this paper, I will refer to the MERMAID multi-purpose offshore platform project (www.mermaidproject.eu) as an instance of development of offshore multi-use platforms (i.e., a possible combination of fish, wind and wave farms), that requires a comparison of alternative designs (e.g., for environmental designs, see [6–10]). However, each design option has different features that must be evaluated using dedicated indicators. Thus, as a starting point, the literature on the selection among alternatives will be considered to provide context for this analysis.To simplify the theoretical analysis, let us assume that time is not relevant, so that the sequence of project implementation or the movement of resources from t to t+1 can be disregarded [11]. Moreover, let us assume that there are no interdependencies between projects [12], neither through a monetary budget nor through spatial linkages. Finally, let us assume that there is no risk from the projects, so that the attitudes of decision-makers towards risk are irrelevant [13]. In other words, the problem consists of providing a list of n non-dominated designs. Alternatively, it is possible to search for an ordered list of n options [14]. However, the sustainability criterion that has become an increasingly important factor in modern project planning suggests that projects are characterised by a range of features that can be grouped based on some previously defined criteria and by using specified acceptable standards that should be met. This defines the context for a multi-criterion analysis (MCA), with relative weights attached to categories [15] and to acceptable standards, rather than a preference ratio approach in which projects are ordered according to percentage scores for some indicators [16] or a cost-benefit analysis [17] in which projects are ordered according to monetary achievements for all indicators. Alternatively, it is possible to use a Delphi method [18,19] or the analytical hierarchy process (AHP; [20–25]) or a group AHP [26,27], a PROMETHEE method [28–31], an ELimination and Choice Expressing REality (ELECTRE) method [32,33], a data envelopment analysis [34], an analytical network process [35,36], or an analogy-based estimation [37] to compare alternative design options. In the present study, MCA was chosen because it explicitly accounts for the possibility of multiple competing objectives that must be reconciled. As a result, it allowed me to compare the effects of choosing different methods of defining factor weights on the risk of decision errors.In this context, let us assume that there are no qualitative indicators [38], so that crisp rather than fuzzy analysis can be applied [39,40]. Moreover, let us assume that all indicators are continuous, so that normalised percentages [41] rather than binary values [42] can be used. Finally, let us assume that there are no weights attached to the acceptable standards (e.g., an internal rate of return greater than 2%; an overall score greater than 50%), so that linear optimisation can be applied [43]. Alternatively, it would be possible to use genetic algorithms [44]; however, they produce results that are difficult to explain to stakeholders, and are thus an unsuitable tool for helping stakeholders with competing objectives to reach consensus. Based on these assumptions, our problem consists of maximising the linear sum of weighted percentage scores subject to linear constraints [45] rather than identifying the best combination of options that satisfy a given budget [46]. However, in this approach, the best design depends crucially on the relative weights attached to the multiple criteria by stakeholders and, consequently, depends on the method chosen to estimate the weights, since monetary and time constraints often suggest the need to perform a single estimation of these relative weights.The purpose of this study is to show which methods should be chosen to estimate the relative weights attached to categories of criteria which affect the choice of the best design option. To do so, a consistent dataset will be constructed to allow the application of four methods: precisely, two statistical methods and two preference-elicitation methods. Next, an original methodology will be developed to improve the logical consistency of the two datasets used for the two different groups of methods. Specifically, I estimate the probability of erroneously rejecting a better design option based on the choice of a given method rather than an alternative or alternatives, within an intuitive overall framework. This approach considers both the satisfaction of predefined acceptable standards and a comparison between options.The MERMAID project aims at designing a sustainable offshore multi-use platform in alternative contexts (i.e., North Sea, Atlantic Sea, Mediterranean Sea, Baltic Sea), where experts and stakeholders are expected to discuss in order to identify technically feasible and socially preferable options: the development of a continuing participative process and the identification of final option designs are equally important within the project. This approach required the involvement of stakeholders in three round meetings (i.e., expressing initial preferences, moulding preliminary designs, judging final designs), where sociologists and economists submitted some questionnaires.In particular, for the Mediterranean case study, 15 Italian stakeholders were interviewed to determine their preferences related to the sustainability features of a multi-purpose offshore platform: S1=the Water Plan Office in Veneto Region, S2=the Harbour Office in Venice, S3=the National Environmental Agency in Veneto, S4=the Clam Producer Cooperative in Chioggia, S5=the Energy Agency in Venice Municipality, S6=the Naval League in Venice (a nongovernmental organisation), S7=the Environmental League in Venice (a nongovernmental organisation), S8=the Hotel Keeper Association in Venice, S9=SEABREATH (a producer of wave energy converters), S10=WEMPOWER (a producer of wave energy converters), S11=the National Alternative Energy Agency in Veneto, S12=Neural Engineering SpA (a technical consultant), S13=eAmbiente (an environmental consultant), S14=the National Research Centre in Venice, and S15=the Citizen Committee for the Preservation of the Venice Lagoon.My first goal was to determine the perceived importance of 39 features, split into three groups (i.e., goals, demands, and constraints; Appendix B): stakeholders are expected to specify an importance value from 1 (little importance) to 5 (major importance) for each item. Moreover, stakeholders are allowed to specify additional items, grouped into the three categories (i.e., economic, social, environmental), together with a non-answer (i.e., “I do not know”). Finally, stakeholders are informed about which category is attached to each item.My second goal was to determine the perceived importance of three categories (i.e., economic, social, environmental; Appendix C): stakeholders are expected to rank three cards which represent the main economic features (e.g., net annual and induce income), the main social features (e.g., net annual and induce employment), and the main environmental features (i.e., intermediate ecological services such as primary production, nutrient cycling, food chain dynamics; and final ecological services such as reduced alien species, preserved biogenetic habitats). For example, the stakeholder S1 ranked environmental more important than economic issues, and economic more important than social issues (i.e., EnvEcoSoc). Moreover, stakeholders are allowed to introduce blank cards in order to stress the greater importance attached to the feature which is ranked better. For example, stakeholder S7 introduced a blank card between environmental and economic issues and no blank cards between economic and social issues to express that environmental issues are much more important than economic ones, and economic issues are more important than social ones (i.e., EnvBEcoSoc). Finally, stakeholders are asked to express their preferences between all couples of features (i.e., Eco vs. Env, Eco vs. Soc, Env vs. Soc) in a double-side scale from 1 to 7 (i.e., 7–1 in favour of the first option, and 1–7 in favour of the second option), where if you like the first option better than the second one, you choose a mark between 7 and 1 on the left side; if you like the second option better than the first one, you choose a mark between 1 and 7 on the right side; if you are indifferent between the first and second option, you choose 1. For example, the stakeholder S1 said 2 in comparing Env and Eco issues, 4 in comparing Env and Soc issues, and 2 in comparing Eco and Soc isssues.Note that stakeholders were not prioritised, unlike Bendjenna et al. [47], who applied Mitchell et al.’s method to classify stakeholders and used the Croquet integral to integrate the criteria to allow for interactions. Next, I will disregard how and which a final design is achieved within the MERMAID project as irrelevant to this study.Many methods have been used to estimate relative criterion weights: the revised Simos Procedure (RSP; e.g., [48,49]), the analytical hierarchy process (AHP; e.g., [50–53]), conditional mean analysis or linear regression (LR; e.g., [54]). To these options, factor analysis (FA) was added. Table 1summarises the pros and cons of these methods, whereas Appendix A provides mathematical formulas and numerical examples for each method.Within the statistical methods, I chose to apply FA and LR to the same set of answers to questions designed to elicit stakeholder priorities (Appendix B). Before obtaining data, the questions were tested to increase their validity, and any unclear or misleading questions were revised. Within the preference-elicitation methods, I applied RSP and AHP to a consistent set of answers (Appendix C). Note that all four methods are based on expressed preferences, and the questions allowed indifference, whether directly stated or inferred through the absence of a reply in the questionnaire. Alternatively, El-Santawy and Ahmed [55] suggest that researchers should use the coefficient of variation to allocate weights when a respondent expresses no preferences among criteria.Next, I used an arithmetic mean for the AHP results instead of a geometric mean [56], and I used multiple-regression analysis rather than a genetic algorithm or artificial neural network [57] to find solutions in the conditional mean and LR analyses, since the latter tools produce numerical rather than mathematical results (i.e., from the same dataset, different outcomes could be obtained) and are thus unsuitable for analytical and replicable comparisons between weight-estimation methods. Note that conditional mean analysis is similar to the reversed swing method [58], with scores assigned to sub-objectives for the three main objectives (economic, social, and environmental), whereas RSP is similar to the even swaps method [59]. A multi-attribute utility function is implicitly assumed, in order to find overall acceptable solutions based on the significance of the criteria stated by stakeholders: the overall score for a given method of estimating the relative weights can then be depicted using intersecting 3D planes and 2D level curves. Alternatively, Makarouni et al. [60] suggested a non-compensatory approach, in which criteria can be simultaneously optimised and a small score in one dimension might not be compensated for by a large score in an alternative dimension; that approach emphasises the conflicting nature of the criteria.Note that LR and FA shared the same dataset, based on normalised scores for the importance of specific features, which were grouped into the three sustainability criteria. FA and AHP shared the same algorithm, which was based on the calculation of eigenvalues and eigenvectors. AHP and RSP shared the same dataset, which was based on normalised orderings of the three sustainability criteria. In other words, the difference between the relative weights estimated by LR and FA will depend on statistical algorithms; the difference between the relative weights estimated by FA and AHP will depend on stakeholder perceptions when facing alternative contexts; and the difference between the relative weights estimated by AHP and RSP will depend on computational algorithms.Table 2presents the results of a LR of each preference score vs. the dummy variables (one each for economic, social, and environmental values, with a value of 1 if the dummy variable is significant). The data were normalised by dividing each coefficient by the sum of the coefficients for the three categories (economic, social, and environmental). After normalisation, the following relative weights were obtained: for economic factors, Weco=31; for social factors, Wsoc=29; and for environmental factors, Wenv=40. Here and later, the units of all weights are %. Tables 3 and 4present the results from the FA. After normalisation, the following relative weights were obtained: Weco=29,Wsoc=27, and Wenv=44. After normalisation, the AHP produced the following relative weights, averaged over stakeholders: Weco=26,Wsoc=23, and Wenv=51. After normalisation, the RSP produced the following relative weights, averaged over stakeholders: Weco=27, Wsoc=25, and Wenv=48.Note that differences between LR and FA in the estimated weights for economic, social and environmental features are +2, +2 and −4, respectively; differences between FA and AHP are +3, +4 and −7, and differences between AHP and RSP in the estimated weights for economic, social and environmental features are −1, −2 and +3, respectively. These differences are reasonable, because LR and FA apply the same dataset, FA and AHP apply different datasets, and AHP and RSP apply the same dataset.The previous section calculated the relative weights to be attached to the three criteria by applying four alternative methods. This section will assess to what extent acceptance or rejection of a project as well as selection of a project over an alternative depend on the estimated relative weights.After normalisation, the score values for economic, social, and environmental factors (Xeco, Xsoc, and Xenv, respectively) areXecoa=1,Xsoca=1, andXenva=1if design option a provides the maximum value of these (possibly combined) indicators, andXecoa=0,Xsoca=0, andXenva=0if design option a provides the minimum value of these (possibly combined) indicators. Under the assumption that the indices for economic, social, and environmental issues are uniformly distributed throughout the feasible ranges (i.e., f(Xeco)=1 with Xeco in [0,1], f(Xsoc)=1 with Xsoc in [0,1], and f(Xenv)=1 with Xenv in [0,1]), the average probability of rejecting a design option (i.e., mistake 1=M1) because it does not satisfy a given threshold k (e.g., an overall score of k=0.50) if relative weights are estimated by methodology i rather than methodology j is given by:(1)M1=1003∫01∫01∫01|WecoiXeco+WsociXsoc+WenviXenv−(WecojXeco+WsocjXsoc+WenvjXenv)|f(Xeco)f(Xsoc)f(Xenv)dXecodXsocdXenv(2)M1=1003∫01∫01∫01|ΔWecoijXeco+ΔWsocijXsoc+ΔWenvijXenv|dXecodXsocdXenvwhere X represents the score and W represents the weight for economic (eco), social (soc), and environmental (env) factors, and k is irrelevant because the overall score is linear in relative weights. Table 5summarises the average probabilities for the present case study.Note that the assumption of a uniform distribution for each score [61] leads to overestimating the average probabilities; indeed, the largest differences are observed for the extreme values of the indicators, although these values are the least likely, since scores are expressed in percentages. For instance, if the normalised scores are assumed to be normally distributed with a mean of 0.5 and a variance of 1, the average probabilities depicted in would decrease by around 95%: the probability of erroneously rejecting a design option is 0.021 for the LR vs. FA, 0.059 for LR vs. AHP, 0.042 for LR vs. RSP, 0.037 for FA vs. AHP, 0.021 for FA vs. RSP, and 0.016 for AHP vs. RSP.Thus, the average probabilities for erroneously rejecting a design option turn out to be small (smaller than 1%), with larger average probabilities where both differences in statistical or computational algorithms and in data sets are involved.However, the average probabilities presented in Table 5 depict average risks of mistakes, where the 0 values obtained without the absolute value function in equations (1) and (2) highlight the fact that mistakes are symmetrical. Thus, the three-way interaction among Xeco, Xsoc, and Xenv could also be considered. Figs. 1–3fix k at 0.50 in order to intuitively show that there is a wrong rejection only where the Xenv score value is included between the two planes. Indeed, in these figures, one plane represents all Xenv score values such that the overall score is 0.50 with a given estimation method for relative weights; the overall score is larger than 0.50 for all Xenv score values above this plane, and the acceptable standard is met with one estimation method and unmet with the other estimation method if Xenv score values are above one plane and below the other plane. Note that Figs. 1 and 2 show that the plane depicting the overall score related to the methodology indicated in the columns of Table 5 lies below the plane depicting the overall score related to the methodology indicated in the rows of Table 5 for small values of Xeco, Xsoc, and Xenv, whereas the opposite occurs in Fig. 3. In other words, for each Xeco, Xsoc, and Xenv, the mistake caused by using AHP rather than RSP has the opposite sign from the sign for all other mistakes.Therefore, the linear relationships between the average probabilities confirm that errors due to algorithm and data differences are independent. For example, errors made by applying LR rather than RSP can be split into errors made by using LR rather than FA (i.e., differences in statistical algorithms) plus errors made by using FA rather than AHP (i.e., differences in data sets) minus errors made by using AHP rather than RSP (i.e., differences in computational algorithms).It is possible to calculate the differences in the score values for options a and b (i.e.,ΔXecoab,ΔXsocab, andΔXenvab) under the assumption that the differences in economic, social, and environmental scores are uniformly distributed throughout the feasible ranges (i.e., from −1 to 1). In that case, the probability of rejecting a better design option a while choosing a worse design option b (i.e., mistake 2=M2<0) and the probability of choosing a worse design option a while rejecting a better design option b (i.e., M2>0), with the relative weights estimated using method i rather than method j, is given by:(3)M2=ΔWecoijΔXecoab+ΔWsocijΔXsocab+ΔWenvijΔXenvabwhere:(4)M2=ΔWecoijΔXecoab+ΔWsocijΔXsocab+ΔWenvijΔXenvab=0ifΔXecoab=ΔXsocab=ΔXenvab=1(5)M2=ΔWecoijΔXecoab+ΔWsocijΔXsocab+ΔWenvijΔXenvab=0ifΔXecoab=ΔXsocab=ΔXenvab=0(6)M2=ΔWecoijΔXecoab+ΔWsocijΔXsocab+ΔWenvijΔXenvab=0ifΔXecoab=ΔXsocab=ΔXenvab=−1By applying the economic, social, and environmental weights estimated for the case study, the error probabilities were calculated and summarised in Table 6. Note that M2 forΔXecoab=ΔXsocab=1andΔXenvab=0equals −M2 ifΔXecoab=ΔXsocab=0andΔXenvab=1;M2 forΔXecoab=ΔXenvab=1andΔXsocab=0equals −M2 ifΔXecoab=ΔXenvab=0andΔXsocab=1; and M2 forΔXsocab=ΔXenvab=1andΔXecoab=0equals −M2 ifΔXsocab=ΔXenvab=0andΔXecoab=1.By relying on the linearity of M2 in the differences in score values (ΔXeco, ΔXsoc, and ΔXenv), the value of ΔXenv can be obtained for each ΔXeco and ΔXsoc such that option a has the same overall score as the score for option b, by setting the total differentiation of M2 at 0; smaller or larger values of ΔXenv imply mistakes with the probabilities depicted in Table 6.Thus, the average probabilities for erroneously choosing a worse option by rejecting a better option turn out to be significant (up to 11%).In addition, by relying on the results obtained in the previous section, which show that the probability of erroneously rejecting design option a because it does not meet a acceptable standard are small (under the uniform distribution assumption) or tiny (under the normal distribution assumption), one could use Figs. 4–6as a general framework to check for the probability of making a mistake in choosing design option a as opposed to design option b, after calculating the prevailing score differences. For example, there is no error caused by applying LR rather than FA if ΔXeco=1 and ΔXsoc=0, provided that ΔXenv is around 0.35 (Fig. 4), whereas the overall score for option a could be overestimated with a probability of up to 4% and underestimated with a probability of up to 4% with respect to option b (Table 6) if ΔXenv is smaller and larger than 0.35, respectively.Note that the same reasoning could be applied if one criterion (e.g., economic) is replaced by another one (e.g., social or environmental). Next, Table 6 depicts mistakes in case of extreme score differences, whereas Figs. 4–6 represent mistakes in case of interior score differences.

@&#CONCLUSIONS@&#
The application of the sustainability concept to environmental management in general, and to environmental projects in particular, suggests that at least three categories must be taken into account in choosing among alternative projects to be implemented: economic, social, and environmental features. In other words, MCA must be applied. However, MCA results will depend crucially on the relative weights attached to the criteria, and, consequently, on the method applied to estimate these relative weights. Ideally, all main weight-estimation methods should be used, and then check to what extent results depend on alternative estimations. Practically, due to monetary and time reasons, a single method is often applied.By introducing an original estimation method to allow reference to a logically consistent dataset and consequently, by obtaining general statements that do not depend on the particular dataset used for estimations, the methodology described in this paper shows that mistakes caused by using one method rather than an alternative are non-significant in terms of satisfaction of acceptable standards and are significant in terms of comparisons between options, and that these mistakes are larger if both differences in statistical or computational algorithms and in data sets are involved. In other words, if the stakeholders have achieved a full understanding of the issues involved, a method for estimating relative weights can be chosen based on practical reasons: LR and conditional mean analysis should be preferred because they are intuitive procedures that can be explained to stakeholders, without consistency problems; in contrast, AHP or RSP should be chosen to provide individual weights, although AHP might have consistency problems, and FA should be chosen to confirm categories, although many stakeholders and questions are required by FA.Note that these results could be developed further by performing a statistical analysis to explain the differences between LR and FA. Moreover, a mathematical analysis could be carried out to clarify the differences between AHP and RSP. Finally, these results could be developed further by performing a psychological analysis to explain the differences between FA and AHP.