@&#MAIN-TITLE@&#
A model-based evaluation of data quality activities in KDD

@&#HIGHLIGHTS@&#
MRDQA: a model-based approach for supporting the Data Quality task on KDD.Evaluation of quality requirements of weakly-structured data via model-checking.A fine-grained quality analysis of the cleansing procedures effectiveness.Automatic identification of error-patterns and interactive visualisation.Experiments done on a real scenario making data publicly available.

@&#KEYPHRASES@&#
Data quality,Data cleansing,Model checking,Real-life application,

@&#ABSTRACT@&#
We live in the Information Age, where most of the personal, business, and administrative data are collected and managed electronically. However, poor data quality may affect the effectiveness of knowledge discovery processes, thus making the development of the data improvement steps a significant concern.In this paper we propose the Multidimensional Robust Data Quality Analysis, a domain-independent technique aimed to improve data quality by evaluating the effectiveness of a black-box cleansing function. Here, the proposed approach has been realized through model checking techniques and then applied on a weakly structured dataset describing the working careers of millions of people. Our experimental outcomes show the effectiveness of our model-based approach for data quality as they provide a fine-grained analysis of both the source dataset and the cleansing procedures, enabling domain experts to identify the most relevant quality issues as well as the action points for improving the cleansing activities.Finally, an anonymized version of the dataset and the analysis results have been made publicly available to the community.

@&#INTRODUCTION@&#
Nowadays, huge masses of people’s data are available, thanks to the wide use of Information Systems, which represent the back-end of an increasing number of services and applications. Actually, public and private organizations recognise the value of data as a key asset to deeply understand social, economic, and business phenomena and to improve competitiveness in a dynamic business environment, as pointed out in several works (Batini, Cappiello, Francalanci, & Maurino, 2009; Fox, Levitin, & Redman, 1994; Madnick, Wang, Lee, & Zhu, 2009). Indeed, as Fayyad, Piatetsky-Shapiro, and Smyth (1996) remarks while introducing the KDD process, “the value of storing volumes of data depends on our ability to extract useful reports, events and trends, support decisions and policy based on statistical analysis and inference.” In the last years, the data quality improvement and analysis techniques have become an essential part of the KDD process as they contribute to guarantee the believability of the overall knowledge process,1Here the term believability is intended as “the extent to which data are accepted or regarded as true, real and credible” (Wang & Strong, 1996).1making the reasoning over data a very significant concern (Fisher, Lauría, Chengalur-Smith, & Wang, 2012; Herrera-Viedma & Peis, 2003; Holzinger, Yildirim, Geier, & Simonic, 2013; Pasi, Bordogna, & Jain, 2013a; Sadiq, 2013). In this paper we aim to draw the attention to data quality in the context of KDD.Indeed, most researchers agree that quality of data is frequently poor, and this represents a problem in practical applications of KDD since according to the “garbage in, garbage out” principle, dirty data can have unpredictable effects on the information derived from them, as noted by Fox et al. (1994), Levitin and Redman (1995), Ballou and Tayi (1999), Hipp, Güntzer, and Grimmer (2001), Haug, Zachariassen, and Van Liempd (2011), Dasu (2013).In recent years industrial and academic communities have spent a great effort to address data quality issues (e.g., by performing quality analysis and improvement, data visualisation and management, data cleansing, etc.) both from a practical and a theoretical point of view, as studied by Barateiro and Galhardas (2005), Pipino, Lee, and Wang (2002), Wang and Strong (1996). In this regard, Batini and Scannapieco (2006) reported that a gap between practice-oriented approaches and formal research contributions still exists in this field. Indeed, from an industry perspective, a lot of off-the-shelf tools are available, but often they lack of formality in addressing domain independent problems, as the case of the ETL tools.2The ETL (Extract, Transform and Load) is an approach supporting the data preprocessing and transformation tasks in the KDD process (Fayyad et al., 1996). The data extracted from a source system undergo a set of transformations that analyse, manipulate and then cleanse the data before loading them into a Datawarehouse.2In such tools a quite relevant amount of the data quality analysis and cleansing design has still to be done manually or by ad hoc developed routines, that may be difficult to write and maintain (Rahm & Do, 2000). On the other side, theoretical formalisms are sound and rigorous, but they often require a strong background from practitioners, reason that prevents their large-scale diffusion.Within this work we support the idea that model-based verification approaches (model checking for instance) can support the Data Quality task of the KDD process in real-life situations by:(i)modelling data evolution over time in a natural way (e.g, as path on a graph). This allows domain experts to concentrate on what quality constraints need to be modelled rather than how to verify them, thus supporting the definition and formalisation of domain related quality requirements;evaluating the effectiveness of cleansing activities performed through a practice-oriented approach (like the Extraction, Transformation, and Loading used in data warehousing).In this regard, here we present the Multidimensional Robust Data Quality Analysis, a novel technique we defined to formalise and automatically verify both the quality of the data and the robustness of an industrial cleansing process. The technique has been realised by using a model-checking based tool. Furthermore, we report our experience in the application of such technique to a public administration dataset composed by more than 21 million items framed in the context of the Italian Labour Market Domain, then providing a (smaller) database and the experimental results to the community.Huge amounts of data describing people behaviours are collected by the Information Systems of enterprises and organizations. Such data often have an unexpressed informative power, indeed the study of relations and correlations among them allows domain experts to understand the evolution of subtended behaviours or phenomena over time, as recently outlined by Holzinger (2012, 2011), Wong, Xu, and Holzinger (2011), Lovaglio and Mezzanzanica (2013). Among the time-related data, the longitudinal data (i.e., repeated observations of a given subject, object or phenomena at distinct time points, see, e.g., Bartolucci, Farcomeni, & Pennoni (2012)) have received much attention from several academic research communities as they are well-suited to model many real-world instances, including labour and healthcare domains, see, e.g. (Devaraj & Kohli, 2000; Hansen & Järvelin, 2005; Holzinger, 2012; Holzinger & Zupan, 2013; Lovaglio & Mezzanzanica, 2013; Prinzie & Van den Poel, 2011).In such a context graphs or tree formalisms, which are exploited to model weakly-structured data, are deemed also appropriate to model the expected data behaviour, that formalise how the data should evolve over time. In this regard, Holzinger (2012) has recently clarified that a relationship exists between weakly-structured data and time-related data. Namely, letY(t)be an ordered sequence of observed data, e.g., subject data sampled at different timet∈T, the observed dataY(t)are weakly structured if and only if the trajectory ofY(t)resembles a random walk (on a graph). The following example should help in clarifying the matter.Let us introduce the Mobile Phone Tracking Example. The dataset in Table 1shows the events recorded by a mobile telephone operator for lawful interception purposes.3Lawful Interception is a security process where a service provider or a network operator collects individuals intercepted data or communications on behalf of law enforcement officials, see (European Telecommunications Standards Institute ES 201 671, 2009) for more details.3The data describe mobile phones connecting to cells of a cellular network, performing calls, exchanging messages, and data packets. Such data represent a log of the activities that a law enforcement agency can request for investigation. Each record reports information about: the MS-ID (Mobile Station ID, i.e. an ID identifying the mobile phone involved); the BTS-ID (the ID of the base transceiver station to which the Mobile Phone is connected); and the Event-Type. For the sake of simplicity, we reduce the several existing event types to cell-in, cell-out, and traffic. The cell-in event happens when a mobile phone starts being served by a BTS (Base Transceiver Station), e.g. the mobile phone is switched on or it enters into the BTS coverage area. The cell-out event takes place when the mobile phone is no longer served by the BTS where it has previously performed a cell-in (this can be due to the mobile phone being switched-off, or to the exit from the BTS coverage area). The traffic event is recorded when a call is initiated, or a message is sent or received, or some data are exchanged by the phone. The Timestamp value reports the call start time or the message/data packet send time.Intuitively, one could model the longitudinal data evolution on a graph, then it could apply any graph-search to verify if the longitudinal data sequence (i.e., the trajectory) is “correct” or not (i.e., if it satisfies or not a set of quality requirements). To this aim, a mobile phone event sequence should evolve according to the automaton described in Fig. 1(a). Unfortunately, the real data do not fully comply with these criteria: several cell-in can be found in the same cell (with no cell-out in between), several traffic events have no previous cell-in on the BTS, etc. This is mainly due to signal drop issues affecting the radio connections. Let us suppose that the elapsed intervals should be computed for analysis purposes i.e., the intervals when a mobile phone is served by (and thus being into) a BTS. Unfortunately the data quality issue may prevent or affect such intervals computation. Note that, as we discuss in Section 3, modelling such quality requirements through functional dependencies (FDs) may be a hard task since they mainly work on attributes rather than tuples, even though their expressivity has been recently revisited and improved, see Bravo, Fan, Geerts, and Ma (2008).Actually, evaluating and improving the quality of a data source archive and, in turn, the effectiveness of a cleansing process is a challenging task while the comparison between archive contents against real data is often either unfeasible or very expensive (e.g. lack of alternative data sources, cost for collecting the real data, etc.). In such a case, cleansing procedures based on business rules still represent the most adopted solution by industry, as proved by the diffusion of several open source and commercial tools, see (Barateiro & Galhardas, 2005; Thomsen & Pedersen, 2005) for a survey. A reliable answer to questions like “How good are the adopted data cleansing processes?” becomes quite relevant, especially when formalising and measuring such “goodness” can strengthen the believability of the overall knowledge discovery process.Here we support the idea that a model-driven verification of data cleansing activities can strengthen the effectiveness of the KDD process, by providing to data mining algorithms a more reliable cleansed dataset. The contribution of this paper, which extends preliminary results from (Boselli, Cesarini, Mercorio, & Mezzanzanica, 2013), goes into three directions.•First, we present and formalise the Multidimensional Robust Data Quality Analysis (MRDQA for short), a domain-independent iterative technique aimed to evaluate the effectiveness of a black-box cleansing function over a dirty dataset. Then, a visualisation technique is used to facilitate the understanding and assessment of the MRDQA results, namely the parallel-coordinates;Second, we express the task of evaluating weakly structured data quality as a model checking problem, then we implemented the MRDQA using the UPMurphi tool (Della Penna, Intrigila, Magazzeni, & Mercorio, 2009);Third, we apply the MRDQA on a real-life government application in the field of Labour Market (The Italian Ministry of Labour & Welfare, 2012). Finally, a smaller version of the dataset we analysed and a demo are made available on line to the community.The outline of this paper is as follows. In the next section we provide an overview of the related work while in Section 4 we introduce some background notions about data quality, model checking and the interaction between them. Then, in Section 5 we present the Multidimensional Robust Data Quality Analysis while in Section 6 we introduce the labour market domain. Section 7 extensively draws the experimental results as well as the characteristics of the online database. Finally, in Section 8 we sketch some concluding remarks providing in Appendix A the code used to model the labour market domain.

@&#CONCLUSIONS@&#
