@&#MAIN-TITLE@&#
Shuffled frog leaping algorithm and its application to 0/1 knapsack problem

@&#HIGHLIGHTS@&#
Here, a modified discrete shuffled frog leaping algorithm (MDSFL) is proposed to solve 01 knapsack problems.Different types of knapsack problem instances are generated to test the convergence property of MDSFLA.The result shows that it is very effective in solving small to medium sized knapsack problems.Computational experiments with a set of large-scale instances show that the MDSFL can be an efficient alternative for solving tightly constrained 01 knapsack problems.

@&#KEYPHRASES@&#
Meta-heuristics,Discrete shuffled frog leaping algorithm,Knapsack problem,Genetic mutation,

@&#ABSTRACT@&#
This paper proposes a modified discrete shuffled frog leaping algorithm (MDSFL) to solve 01 knapsack problems. The proposed algorithm includes two important operations: the local search of the ‘particle swarm optimization’ technique; and the competitiveness mixing of information of the ‘shuffled complex evolution’ technique. Different types of knapsack problem instances are generated to test the convergence property of MDSFLA and the result shows that it is very effective in solving small to medium sized knapsack problems. Further, computational experiments with a set of large-scale instances show that MDSFL can be an efficient alternative for solving tightly constrained 01 knapsack problems.

@&#INTRODUCTION@&#
The knapsack problem is one of the classical NP-hard optimization problem and the decision problem belongs to the class of NP-complete. It is thoroughly studied in the literature for last few decades. It offers many practical applications in vast field of different areas, such as project selection [1], resource distribution [2], network interdiction problem [3], investment decision-making [4] and so on. 01 knapsack problem is defined by: given a set of items, each with a weight and a value, determine the number of each item to include in a collection so that the total weight is less than a given limit and the total value is as large as possible. The most common formulation of the problem is the 01 knapsack problem, which restricts the number xjof copies of each kind of item to zero or one.(1)Maximizef(x1,x2,…,xn)=∑j=1ncjxjSubjecttog(x1,x2,…,xn)=∑j=1najxj≤b,xj∈{0,1}j=1,2,…,n,cj>0,aj≥0,b>0.The binary decision variables xjare used to indicate whether item j is included in the knapsack or not. It may be assumed that all profits and weights are positive, and that all weights are smaller than the capacity b.In recent times, many heuristic and meta-heuristic algorithms have been employed to solve 01 knapsack problems: Zhao et al. [5] proposed genetic algorithm to solve 01 knapsack problem. Greedy strategy combining with traditional genetic algorithm proved to be much more effective to handle difficult instances. Lin [6] used genetic algorithm to solve knapsack problem with imprecise weight, and he investigated the possibility of using genetic algorithms in solving the fuzzy knapsack problem without defining membership functions for each imprecise weight coefficient. Liu and Liu [7] proposed a schema-guiding evolutionary algorithm (SGEA) to solve 01 knapsack problems. Wanga et al. [8] proposed quantum swarm evolutionary algorithm to solve 01 knapsack problems. Shi [9] modified the parameters of the ant colony optimization (ACO) model to adapt itself to 01 knapsack problems. The improved ACO has strong capability of escaping from the local optimum through artificial interference. Li and Li [10] proposed a binary particle swarm optimization based on multi-mutation strategy (MMBPSO) to solve knapsack problem. The MMBPSO can effectively escape from the local optima to avoid premature convergence due to the utilization of Multi-Mutation strategy. Zou et al. [11] proposed a novel global harmony search algorithm to solve 01 knapsack problems. They utilized position updating and discrete genetic mutation strategy to avoid the premature convergence.Although many 01 knapsack problems have been solved successfully by these algorithms, but some new and more difficult 01 knapsack problems hidden in the real world, so the research on this particular issue is still important. Many algorithms provide possible solutions for some 01 knapsack problems, but they may lose their efficiency on solving these difficult problems due to their own disadvantages and limitations. Most of these algorithm proposed recently are effective for solving 01 knapsack problem with very low dimension, but they may not be effective for 01 knapsack problems with high dimensional sizes.The shuffled frog leaping algorithm (SFLA) is a meta-heuristic optimization method which is based on observing, imitating, and modeling the behavior of a group of frogs when searching for the location that has the maximum amount of available food [12]. SFLA, originally developed by Eusuff and Lansey in 2003, can be used to solve many complex optimization problems, which are nonlinear, non-differentiable, and multi-modal [13]. SFLA has been successfully applied to several engineering optimization problems such as water resource distribution [14], bridge deck repairs [15], job-shop scheduling arrangement [16], multi-mode resource-constrained project scheduling problem [17], unit commitment problem [18] and traveling salesman problem (TSP) [19]. The most distinguished benefit of SFLA is its fast convergence speed [20]. The SFLA combines the benefits of both the genetic-based memetic algorithm (MA) and the social behavior-based PSO algorithm [21].In SFLA, the population consists of a set of frogs (solutions) that is partitioned into subsets referred to as memeplexes. The different memeplexes are considered as different cultures of frogs, each performing a local search. Within each memeplex, the individual frogs hold ideas, that can be influenced by the ideas of other frogs, and evolve through a process of memetic evolution. After a defined number of memetic evolution steps, ideas are passed among memeplexes in a shuffling process [22]. The local search and the shuffling processes continue until defined convergence criteria are satisfied [14].An initial population of P frogs is created randomly. For S-dimensional problems (S variables), a frog i is represented as Xi=(xi1, xi2, …, xiS). Afterwards, the frogs are sorted in a descending order according to their fitness. Then, the entire population is divided into m memeplexes, each containing n frogs (i.e. P=m×n). In this process, the first frog goes to the first memeplex, the second frog goes to the second memeplex, frog m goes to the mth memeplex, and frog m+1 goes back to the first memeplex, etc.Within each memeplex, the frogs with the best and the worst fitnesses are identified as XbandXw, respectively. Also, the frog with the global best fitness is identified as Xg. Then, a process similar to PSO is applied to improve only the frog with the worst fitness (not all frogs) in each cycle. Accordingly, the position of the frog with the worst fitness is adjusted as follows:(2)Di=Rand()×(Xb−Xw),where Diis the change in ith frog position and new position is given by:(3)Xw(new)=Xw+Di,−Dmax≤Di≤Dmax;where Rand() is a random number(Rand()∼U(0, 1)); and Dmaxis the maximum allowed change in a frog's position. If this process produces a better solution, it replaces the worst frog. Otherwise, the calculations in Eqs. (2) and (3) are repeated but with respect to the global best frog (i.e. Xgreplaces Xb). If no improvement possible in this case, then a new solution is randomly generated to replace that frog. The calculations then continue for a specific number of iterations [14].For handling integer programming problems the discrete version of the SFLA is used, called discrete shuffled frog leaping algorithm (DSFLA). The worst frog within each memeplex is updated [12] according to(4)Di=min{int[Rand×(Xb−Xw)],Dmax}forapositivestep,max{int[Rand×(Xb−Xw)],−Dmax}foranegativestep;Xw(new)=Xw+Di.Like SFLA, DSFLA also follows same steps to replace the worst frog. If Eq. (4) does not produce a better solution, then Xbis replaced by the global best frog i.e. Xg; and if in this case also we replace the worst frog by a new randomly generated solution, if Eq. (4) does not produce a better solution. Accordingly, the main parameters of DSFLA are: number of frogs P; number of memeplexes m; number of generation for each memeplex before shuffling n; number of shuffling iterations it; and maximum number of iterations iMax. The pseudocode for the DSFLA is given in Algorithm 1.Algorithm 1Pseudocode for a DSFLA procedureGenerate random population of P solutions (frogs)for each individual i∈Pdocalculate fitness(i)end forSort the population P in descending order of their fitnessDivide P into m memeplexesfor each memeplex doDetermine the best and worst frogsImprove the worst frog position using Eq. (4)Repeat for a specific number of iterationsend forCombine the evolved memeplexesSort the population P in descending order of their fitnessif termination = true thenReturn best solutionend if01 knapsack problem cannot be handled directly by SFLA or DSFLA because of its particular structure. For this reason original DSFLA is modified and applied to solve 01 knapsack problems as discussed below. Shuffled frog leaping algorithm has the most advantageous property of fast convergence speed. But at the same time it looses the searching capability of divergent field and sometimes trapped within a local optima. To make a balance between the convergent and divergent property we further modified DSFLA by hybridizing which include the genetic mutation property of divergent category. The modified discrete shuffled frog leaping algorithm (MDSFLA) is discussed in this section in full details.01 knapsack problem is an integer programming problem. There are two possible values for the decision variable xj, zero or one. The individual frog is represented by a n-bit binary string, where n is the dimension of the problem. The initial population is created randomly to achieve sufficient diversification.In this paper we have used three kinds of discretization to solve the 01 knapsack problems.(i)Method 1: the worst frogXwof each memeplex is replaced according to(5)t=Xw+D;Xw(new)=0ift≤0,round(t)if0<t<1,1ift≥1.Method 2: D is transformed to the interval [0, 1] by using sigmoid function. The worst frog is replaced according to(6)t=1/(1+exp(−D));u∼U(0,1)Xw(new)=0ift≤u,1ift>u.Method 3: the updating formula for the worst frog is given by(7)t=1/(1+exp(−D));Xw(new)=0ift≤α,Xwifα<t≤12(1+α),1ift≥12(1+α).The parameter α is called static probability.Constrained optimization problems are much more difficult to solve compared to the unconstrained part. Due to the presence of constraint, the global best solution of unconstrained problem is different from the constrained one. In the later case, we have to find the optimal balance between the constraints and objective function value. For this reason here two types of conventional algorithms are described and tested: algorithms based on penalty functions and algorithms based on repair methods [23]. Three types of penalty functions are used: logarithmic penalty, linear penalty, and quadratic penalty and they are represented as follows.(8)f1(x)=p×xT−log2(1+ρ(wxT−b)),(9)f2(x)=p×xT−ρ(w×xT−b),(10)f3(x)=p×xT−(ρ(w×xT−b))2,whereρ=maxi=1,…,n(pi/wi).In algorithms based on repair methods, the profit f(x) of each vector x is determined as(11)f(x)=p×x1T,where x1 is a repaired vector of the original solution x. Two types of repair algorithms considered here. The only difference is the selection procedure among them, which chooses an item for removal from the knapsack:•Rep1 (random repair): The selection procedure selects a random element from the knapsack.Rep2 (greedy repair): All items in the knapsack are sorted in the decreasing order of their profit to weight ratios. The selection procedure always chooses the last item for deletion.DSFLA sometimes trapped in a local optima. To avoid this situation we utilize the genetic mutation. After shuffling the memeplexes we use the genetic mutation operator. To avoid the premature convergence of DSFLA mutation operator with a small probability is applied to modify the original population, which serves as the minute diversification in original solution procedure. This will allow the modified discrete shuffled frog leaping algorithm (MDSFLA) to search for other optimal points with new basis.Maximum number of iterations (iMax) determines the termination condition for original SFLA. Here we choose another alternative criteria along with the iMax. If it is not possible to improve the best solution for a large number of steps (Δ), then the algorithm will terminate. And in the mean time total number of iterations must be less than the maximum number of iterations. The parameter Δ is problem dependent, and determined by hit and trial. Generally if we chooseiMax20≤Δ≤iMax10, then the performance of the algorithm is found to be optimal. For iMax>500, we use both these criteria to optimize the algorithmic performance. The flowchart of MDSFLA is given in Fig. 1.In this section, the performance of DSFLA and MDSFLA extensively investigated by a large number of experimental studies. Thirty five 01 knapsack programming problem instances are considered to testify the validity of the MDSFLA. All computational experiments are conducted with Matlab 7.6.0 in Intel(R) Core(TM)2 Duo CPU E7400 @2.80GHz with 4GB of RAM. Standard ten test problems are studied here and detailed information about these problems are given in Table 1. These problems are studied by many authors to test the performance of the different algorithms presented in the literature.Shi [9] introduced test problems 1 and 2 to test the performance of proposed improved ant colony algorithm. An and Fu [24] proposed a method called sequential combination tree algorithm to solve the test problem 3. You [25] used the test problem 4 for the performance analysis of greedy-policy-based algorithm. Yoshizawa and Hashimoto [26] used the information of search-space landscape to search the optimum of the test problem 5. Fayard and Plateau [27] employed a method to solve the test problem 6, and this method derives from the “shrinking boundary method”. Zhao [28] proposed a method called nonlinear dimensionality reduction to solve the test problem 7 and 8. Test problem 9 is from [29], in which the DNA algorithm is proposed to solve 01 knapsack problems. Test problem 10 is from literature [30], in which three algorithms are used to solve 01 knapsack problems. The detailed information of the ten test problems along with the best solution found by CPLEX V12.2.0 is given in the Table 2.According to the structure of the test problems, mainly two different parameter sets are determined. These parameters are set based on the general guidelines given in the literature [12].The effect of three methods of discretization discussed in earlier section, are given in Table 3.For ten test problems best solution and worst solution among 30 independent runs are reported in Table 3. Also average, median and standard deviation (std) for all the solutions are given here along with average total time (ATT) to solve the problem. Maximum number of iterations is considered as iMax=100, and population size is P=200 along with m=10 memeplexes. From Table 3, it is clear that, Method 3 is much more effective to find out best solution with respect to others. In most of the cases Method 3 performs better with respect to average, median, standard deviation and ATT (except for the function f10, Method 2 performs better with respect to median and standard deviation (std)).For handling the constrained part we used the penalty functions and repair operators. The effect of different penalty functions and repair operators is shown in Table 4. For performance analysis we use standard ten test problems given in Table 1.Here in this experimental setup, we find out the best solution and worst solution for 30 independent experiments for each case and average, standard deviation and ATT also reported in the Table 4. In case of the first penalty function only two functions (f2 and f10) are solved successfully. For other cases we get infeasible solutions. Also in the case of second penalty function we get infeasible solutions for three functions (f3, f4 and f8). Here maximum number of iterations is considered as iMax=100, and population size is P=200 along with m=10 memeplexes for all these experiments. Repair method 2 outperform others with respect to all the performance criteria as reported in Table 4.Among ten test problems f10 is much more difficult to solve. So we test the performance criteria for different static probabilities with respect to the test function f10. The effect of different alpha values for the function f10 is given in Fig. 2. Average profit for 30 individual runs corresponding to different α values are plotted. Corresponding to the results we may choose α=0.4.In this subsection, the effect of genetic mutation probability pmon the performance of the MDSFLA is investigated. For the standard ten test problems above, population size of MDSFLA is set to P=200 along with m=10 memeplexes, and the number of iterations in each memeplex is set to it=10. Total 30 independent experiments are carried out in each case, and Table 5gives the average of maximum number of iterations required to solve the particular problem with different values of pm. In all the cases we find the best solution for each test problem. So best, worst, average and standard deviation of objective function is same for each case. Range of pmis taken as [0.01, 0.1], as beyond this range the performance of MDSFLA degraded gradually.The best results were obtained when pm=0.06 for most of the cases (except for the function f5). From Table 5, it is clear that mutation probability with 0.01≤pm≤0.1 can be effective to solve all problems. But the complexity of the 01 knapsack problem increases with its size and the adaptivity of pmto problems with higher dimension sizes may decrease more or less within this region. So for finding dynamic balance between problem size and pmvalue, we fixed the value of pm=2/n, where n is the dimension of the 01 knapsack problem.We consider standard ten test problems to compare the performance of DSFLA and MDSFLA. In the first case, we present the comparison between these two with respect to objective function value, and in the second case we only consider the maximum number of iterations. The parameter setting for the two algorithms for the first case are as follows.For the DSFLA, population size P=200, number of memeplexes m=10, number of iterations within each memeplex it=10 and static probability α=0.4. For the MDSFLA, population size P=200, number of memeplexes m=10, number of iterations within each memeplex it=10, static probability α=0.4 and mutation probability pm=0.06. Three values of maximum number of iterations (iMax) 50, 100 and 150 respectively, are considered to test the performance analysis. Total 30 independent runs are made and corresponding results are given in Table 6.As we can see in Table 6, the MDSFLA performs better than the DSFLA, and it can easily find the optimal solution for all the cases. Within the small value iMax=50, MDSFLA is able to find the best solution in all cases. On the other hand, DSFLA is not able to find out the best solution within iMax=150 for all the test problems (for functions: f2, f8 and f10). When we consider iMax=150, MDSFLA finds best solutions for every independent runs for every test function (std=0 for each of the test functions).In the second case, we consider the maximum number of iterations (iMax) for each algorithm to find the best solution. The parameter settings of the two algorithms are as follows. For DSFLA, population size P=200, number of memeplexes m=10, number of iterations within each memeplex it=10, static probability α=0.4 and maximum number of iterations iMax=500. For the MDSFLA, population size P=200, number of memeplexes m=10, number of iterations within each memeplex it=10, static probability α=0.4, mutation probability pm=0.06 and maximum number of iterations iMax=500. Here, 30 independent runs are considered and the corresponding results for ten standard test problems are reported in Table 7.DSFLA fails to find best solutions for f2, f8 and f10, and it successfully solves function f1 only for 12 cases out of 30 and only 5 out of 30 cases for function f5. Whereas MDSFLA needs iMax on an average 5 (for function f1), 28 (for function f2), 1 (for function f3), 1 (for function f4), 3 (for function f5), 2 (for function f6), 1 (for function f7), 10 (for function f8), 1 (for function f9) and 29 (for function f10) respectively to solve the test functions. The fewer iterations shows that the MDSFLA has higher efficiency than DSFLA on finding best solutions of 01 knapsack problems. In general, suitable mutation operation can increase the diversity of candidate solutions and improve the capability of space exploration for the MDSFLA.The convergence process of DSFLA and MDSFLA for the standard test functions are shown in Fig. 3. Parameter setting is same as discussed above, only difference is iMax is set to 200. And objective function values are plotted against the iteration numbers for a single instance as long as both the algorithms reach their best solutions.For test functions f3, f4, f6, f7 and f9, both the algorithms find the best solutions with the first iteration itself. For test functions f1, f2, f8 and f10 DSFLA trapped in a local optimum point. For both the test functions f8 and f10, MDSFLA traps within a local optimum point for some time. But due to the divergence category introduced by genetic mutation in MDSFLA, it is able to get out from the local optimum point and easily find out the global optimum point. This eventually proves that the searching capability and algorithmic efficiency of MDSFLA is much more better than DSFLA. It is observed from Fig. 3, for other test functions also MDSFLA performs well better than DSFLA. For test function f1 as well as f10, MDSFLA starts with a lower basis than DSFLA though it finds the global optimum point really faster than DSFLA.In order to investigate effectiveness of the algorithm for different instance types we analyze the experimental behavior of algorithm on several sets of randomly generated test problems. Since the difficulty of such problems is greatly affected by the correlation between profits and weights [31], three randomly generated sets of data are considered [23]:•uncorrelated:wi∼U[1,2,…,v], andpi∼U[1,2,…,v].weakly correlated:wi∼U[1,2,…,v], ti∼U[−r, −r+1, …, r−1, r] andpi:=wi+ti(if, for some i, pi≤0, such profit value is ignored and the calculations are repeated until pi>0).strongly correlated:wi∼U[1,2,…,v], andpi:=wi+r.highly restrictive knapsack capacity: A knapsack with the capacity ofb1=v. In this case the optimal solution contains very few items.restrictive knapsack capacity: A knapsack with the capacity ofb2=2v. In this case the optimal solution contains few items. An area, for which conditions are not fulfilled, occupies almost the whole domain.average knapsack capacity: A knapsack with the capacityb3=0.5∑i=1nwi. In this case about half of the items are in the optimal solution.trimmed knapsack capacity: A knapsack with the capacityb4=R∑i=1nwi, where R∼U(0.25, 0.75). In this case E(R)=0.5 and Var(R)=0.25/12.In this case the parameter settings of the algorithms are as follows. population size P=200, number of memeplexes m=10, number of iterations within each memeplex it=10, static probability α=0.4, maximum number of iterations iMax=2000 and for MDSFLA mutation probability is pm=2/n. In the performance basis, DSFLA is only applicable for small item sized problem instances with highly restrictive knapsack capacity and restrictive type knapsack capacity. The average deviation from mean solution of 30 independent runs is 9.50% in case of uncorrelated problem type, 2.22% for weakly correlated problem type and 2.88% for strongly correlated problem type. So we may conclude that weakly correlated and strongly correlated problem instances are easier to solve than uncorrelated problem type in the case of DSFLA. The average deviation from best solution find by DSFLA is 5.35% in case of uncorrelated problem type, 0.99% for weakly correlated problem type and 1.33% for strongly correlated problem type which also supports our observation. From Table 9 it is observed that the standard deviation of the number of iterations to solve a particular problem instance within 30 independent runs is very high in most of the cases. So we prefer me.iter over a.iter to represents the average behavior of the algorithm. On an average number of iterations required to solve the instances are 171, 227 and 264 for uncorrelated, weakly correlated and strongly correlated medium size problems, respectively. Also results do not show any specific effect of correlation structure on the solving process by MDSFLA, though the solution process is much more dependent upon knapsack type.From Table 9, it is observed that type 3 and type 4 of knapsack capacity problem instances are difficult to solve, and depending upon the problem size number of iterations increases gradually. Depending upon the knapsack capacity and problem size, MDSFLA effectively solve all the problems within 1600 iterations and average number of iterations within 1000 range, which is much smaller.Nine 01 knapsack problems with large scales are devised to testify the performance of the modified shuffled frog leaping algorithm. The number of items n is set to 200, 500 and 1000, respectively. Only type 1 knapsack problems are generated and corresponding results are reported in Table 10. The uncorrelated, weakly correlated and strongly correlated knapsack instances of 200 items are graphically illustrated in Figs. 4–6, respectively.The parameter settings for both the algorithms are as follows: population size P=400, number of memeplexes m=20, number of iterations within each memeplex it=10, static probability α=0.4, mutation probability pm=2/n and maximum number of iterations iMax=2000. Total 30 independent runs are considered and the corresponding results for all test problems are given in Table 10.From Table 10 it is seen that, MDSFLA finds the best solutions in all the cases though only for two cases DSFLA able to find out the best solution. Also the number of successful runs to find out the optimal solution out of 30 independent experiments are also reported here. MDSFLA performs much better than DSFLA in all the cases, the average deviation from mean solution is 6.25% for DSFLA and 0.03% for MDSFLA. In worst case maximum number of iterations required to solve the problem is within 1500 range and for average case it is within 1000 range. Strongly correlated and weakly correlated problem instances are easier to solve than the uncorrelated problem type. Also depending upon the problem size number of iterations and average total time to solve the problem increase gradually. From the performance analysis it is clear that MDSFLA performs well to solve 01 knapsack problem in every cases of knapsack type and dimension. It finds best solution of knapsack problem within very small number iterations, which gives it preference over other algorithms.

@&#CONCLUSIONS@&#
In this work, the performance of the shuffled frog leaping algorithm has been extensively investigated by using a large number of experimental studies. We have shown, first, that each of the three discrete SFLAs obtained good quality of solutions by utilizing both the local search procedure based on swarm intelligence and the competitiveness mixing of information of the genetic based memetic algorithms. The experimental results show that the DSFLA has demonstrated strong convergence and stability for 01 knapsack problems due to the utilization of the discretization effect of static probability.However for solving unknown problems, DSFLA may trapped in some suboptimal point and modification of the searching space is required for such cases. On the other hand computational results reveal that the MDSFLA has strong capability of preventing premature convergence of the DSFLA throughout the whole iteration due to the utilization of the genetic mutation.Also different types of knapsack problem instances are generated to test the convergence property of MDSFLA. Results show that MDSFLA is found very effective in solving small to medium sized knapsack problems. Actually, the proposed algorithm easily found all of the best solutions for smaller size problems. Further in this study, we have investigated different correlated problems for the performance analysis of MDSFLA and corresponding results do not support any specific advantage over correlation structure of the problem. The proposed algorithm is also found very effective for solving larger size and tightly constrained 01 knapsack problem. These problems are very complex, therefore their solution can be considered as a very good indicator for the potential of the shuffled frog leaping algorithm.Based on our computational study we have observed that the proposed MDSFLA has a potential for solving any 01 knapsack problem. It may also be used for solving multiple knapsack problems or multi-objective knapsack problems and other combinatorial optimization problems like generalized assignment problems, set covering problems, etc. and is scheduled as a future work.