@&#MAIN-TITLE@&#
Feature selection with redundancy-complementariness dispersion

@&#HIGHLIGHTS@&#
We use inter-correlation of features to represent redundancy and complementariness.We add a modification item for feature complementariness in the evaluation function.Redundancy-complementariness dispersion is used to address the interference effect.

@&#KEYPHRASES@&#
Classification,Feature selection,Relevance,Redundancy,Pairwise approximation,Redundancy-complementariness dispersion,

@&#ABSTRACT@&#
Feature selection has attracted significant attention in data mining and machine learning in the past decades. Many existing feature selection methods eliminate redundancy by measuring pairwise inter-correlation of features, whereas the complementariness of features and higher inter-correlation among more than two features are ignored. In this study, a modification item concerning feature complementariness is introduced in the evaluation criterion of features. Additionally, in order to identify the interference effect of already-selected False Positives (FPs), the redundancy-complementariness dispersion is also taken into account to adjust the measurement of pairwise inter-correlation of features. To illustrate the effectiveness of proposed method, classification experiments are applied with four frequently used classifiers on ten datasets. Classification results verify the superiority of proposed method compared with seven representative feature selection methods.

@&#INTRODUCTION@&#
With the fast development of the world, the dimensional and size of data is fast-growing in most kinds of fields which challenge the data mining and machine learning techniques. Feature selection is an important and useful approach that can effectively reduce the dimensionality of feature space while retaining a relatively high accuracy in representing the original data. Thus, it plays a fundamental role in many data mining and machine learning tasks, particularly in pattern recognition, knowledge discovery, information retrieval, computer vision, bioinformatics, and so forth. The effects of feature selection have been widely recognized for its abilities in facilitating data interpretation, reducing acquisition and storage requirements, increasing learning speeds, improving generalization performance, etc. [1]. Therefore, feature selection has attracted significant attention of more and more researchers [2–8].Generally speaking, the feature selection methods can be divided into two types: Wrapper and filter. Wrapper methods depend on specific learning algorithms. Thus the performance of wrapper methods is affected by the selected learning methods. This may makes wrapper methods computationally expensive in learning, since they must train and test classifiers for each feature subset candidate. Conversely, filter methods do not rely on any learning schemes. Instead, it is only based on some classifier-irrelevant metrics, including Fisher score [9],χ2-test [10], mutual information [11–14], Symmetrical Uncertainty(SU) [15], etc., to estimate the discrimination power of features. Recently, new criteria and techniques such as sparse logistic regression attract increasing attention (e.g. [16]) since they have potential ability to handle very high-dimensional datasets. In this study, we only focus on filter methods.Filter methods can also divided into feature subset selection and feature ranking ones, with regard to their search strategy. The evaluation unit for subset selection methods is a set of features, thus the set with best discrimination power is trying to be discovered [17–19]. Nevertheless, to find the best feature subset, a total of2m-1candidate subsets (where m is # features in the original data) are possible to be traversed for feature selection task cannot be solved optimally in polynomial-time unlessP=NP[20]. Thus it is computationally intractable in nowadays practice, particularly in the context of big data. Unlike subset methods, feature ranking methods individually take features as the evaluation units and rank them according to their discrimination power [21,22]. These methods usually employ heuristic search strategies such as forward search, backward search, and sequential floating search.However, whatever feature ranking or feature subsets selection methods, there are two problems possibly leading to wrong rankings or lower capacity for classification. One is that neglecting feature interaction or dependence may lead to redundancy, as some feature selection methods like MIM [23] take the assumption of independence of features. For real-world datasets, particularly those high-dimensional ones, such strong assumption may produce results far from optimal. The other problem is that group capacity of features is usually ignored, since many methods only measure the relationship between two features [11,24,22]. For example, a feature that has low individual classification capacity but is highly dependent on other features may be overlooked and even misidentified as a redundant one by only measuring its pairwise relationship with other features. However, since it is highly dependent on other features, it is also possible that it contributes largely to the discrimination power of the subset consisting of such features. Thus, it should be evaluated as a salient feature and then selected. Since the dependence among features is related to both redundancy and complementariness, it is imperative to develop more precise correlation analysis in order to distinguish them effectively. To this end, we propose a novel feature selection algorithm which tries to modify the redundancy analysis applied in prior methods by introducing a modification item and a dynamic coefficient to effectively adjust redundancy-complementariness identification. The main contributions that distinguish our work form extant studies are listed as follows:•Complementary correlation of features is explicitly separated from redundancy.Redundancy-complementariness dispersion is taken into account to adjust the measurement of pairwise inter-correlation of features.The remainder of the paper is organized as follows: Section 2 reviews related work. Section 3 presents the Information theoretic metrics and evaluation criteria. A new feature selection method is included in Section 4. In Section 5, experimental study is conducted and the results are discussed. Finally, Section 6 concludes this study and proposes possible further work.

@&#CONCLUSIONS@&#
Relevance and redundancy are two important feature properties attracting much attention in the study of feature selection. Many algorithms eliminate redundancy by measuring pairwise inter-correlation between features, while they cannot identify the complementariness of features and the correlation among more than two features. Although the former problem can be effectively addressed by introducing a modification item, high inter-correlation of features still makes the result far from optimal. Specifically, pairwise approximation of high inter-correlation may misidentify and select FPs which will in turn impair the effectiveness of feature evaluation. In order to identify the interference effect of FPs, the redundancy-complementariness dispersion is taken into account in proposed method to adjust the measurement of pairwise inter-correlation of features. To illustrate the effectiveness of proposed method RCDFS, classification experiments are conducted with four frequently used classifiers on ten datasets.In the experiments, RCDFS is compared with seven representative feature selection methods namely CMIM, mRMR, FCBF, MIM, ReliefF, DEAFS and kASSI. Classification results have been proven to perform satisfactorily of RCDFS. To verify the stability of RCDFS, Wilcoxon test as well as Friedman test are adopted to assess the statistical significance of the differences among the results of the feature selection method. According to the test results, RCDFS performs better than the selected methods in most of the cases.Although the superiority of RCDFS has been verified in the experiments, there still remain challenges which are imperative to be solved in our future work. One is that how to properly set the weights of three objectives, i.e. coordinate relevance, redundancy-complementary, and dispersion of pairwise inter-correlation, is needed to be studied. Possible directions include multi-objective programming and multi-index evaluation techniques such as data envelopment analysis. Moreover, since there is no causal relationship between FPs and the dispersion of pairwise inter-correlation, only concerning such dispersion may not always be effective in feature evaluation. How to design more effective heuristics in the context of first-order approximation will be further studied. In addition, optimization techniques like sparse logistic regression [16] will be introduced to deal with high-dimensional data in our future work.