@&#MAIN-TITLE@&#
Voice conversion using General Regression Neural Network

@&#HIGHLIGHTS@&#
We model pitch residuals using wavelet packet decomposed coefficients.Thus problem of artifacts generated due to direct transformation by ANN is alleviated.GRNN is proposed to modify vocal tract and wavelet packet decomposed pitch residuals.Mapping using GRNN model perform slightly better than GMM and RBF mapping models.Fast convergence of GRNN reduces computation time and overtraining of conventional ANN.

@&#KEYPHRASES@&#
Gaussian Mixture Model,General Regression Neural Network,Pitch contour,Radial Basis Function,Voice conversion,Wavelet transform,

@&#ABSTRACT@&#
The objective of voice conversion system is to formulate the mapping function which can transform the source speaker characteristics to that of the target speaker. In this paper, we propose the General Regression Neural Network (GRNN) based model for voice conversion. It is a single pass learning network that makes the training procedure fast and comparatively less time consuming. The proposed system uses the shape of the vocal tract, the shape of the glottal pulse (excitation signal) and long term prosodic features to carry out the voice conversion task. In this paper, the shape of the vocal tract and the shape of source excitation of a particular speaker are represented using Line Spectral Frequencies (LSFs) and Linear Prediction (LP) residual respectively. GRNN is used to obtain the mapping function between the source and target speakers. The direct transformation of the time domain residual using Artificial Neural Network (ANN) causes phase change and generates artifacts in consecutive frames. In order to alleviate it, wavelet packet decomposed coefficients are used to characterize the excitation of the speech signal. The long term prosodic parameters namely, pitch contour (intonation) and the energy profile of the test signal are also modified in relation to that of the target (desired) speaker using the baseline method. The relative performances of the proposed model are compared to voice conversion system based on the state of the art RBF and GMM models using objective and subjective evaluation measures. The evaluation measures show that the proposed GRNN based voice conversion system performs slightly better than the state of the art models.

@&#INTRODUCTION@&#
During the past two decades various voice conversion systems have been proposed to modify the speaker dependent parameters of the source speaker utterance so that it emulates those of the target (desired) speaker utterance [1,2]. Voice conversion is an emerging technology in speech processing used for commercial applications like the personification of text to speech, design of multi speaker based speech synthesis, security related applications, broadcasting and multimedia applications. In the film industry, it is used for voice editing, voice dubbing and voice animation [3,4]. Voice conversion could prove to be a simple and efficient way to create the desired variety of speakers without the need to record different speakers. In the field of medicine, voice conversion may improve the quality and intelligibility of laryngectomees voice which might repair there past speaking capabilities to produce natural voice [5].Voice conversion is usually carried out in two different steps, namely training step followed by transformation step. In the training step, a set of speaker dependent features of the source and target speakers are derived and appropriate conversion function is established to map the source feature set onto that of the desired speaker's feature set. In the transformation step, the test speaker feature vector is modified using the conversion function derived in the training phase and the transformed speech signal holds the desired speaker characteristics [6]. In the training phase, the feature extraction is carried out by using speaker specific characteristics such as the shape of the vocal tract, the shape of source excitation and long term prosodic parameters [7]. Among these, the shape of the vocal tract and the shape of excitation parameters uniquely represents the speaker identity [7]. The vocal tract transfer function can be characterized using various speech features which can be classified into three categories: (1) features such as formant frequencies [8], which belong to acoustic phonetic model, (2) features derived, without speech production model such as, Cepstral Coefficients [9], Spectral Lines [3], and Mel-Cepstral Frequencies [9] and (3) Linear Predictive (LP) related features such as Linear Predictive Coefficients (LPC) [10], Line Spectral Frequencies (LSFs) [11–13]. Amongst these feature representations, LSF overcomes the limitations of the LPC and results in much improved speech quality than any other features [13].Various speaker specific models have been proposed in the literature to deal with the vocal tract mapping issue. Among these, the Vector Quantization (VQ) based codebook mapping [10] and Gaussian Mixture Model (GMM) are the most commonly used models [1,14]. In VQ, the source and target speakers utterances are clustered and a mapping rule for each cluster is formulated using minimum mean square error criteria. The main problem of VQ model is hard partitioning of the acoustic space produces discontinuities in the transition region. This affects the quality and naturalness of reconstructed speech signal [10,14]. An attempt [15] based on Fuzzy vector quantization and a Speaker Transformation Algorithm using Segmental Codebook (STASC) is further made to overcome the limitations of voice conversion system based on [2]. Dynamic Frequency Warping (DFW) [16] based voice conversion model is shown to yield the better quality of the converted speech signal. But, it transforms the formants to different frequencies without modifying the complete spectral shape leading to poor quality results. Therefore, DFW is further upgraded with weighted frequency warping technique [17].Another approach based on GMM model uses the joint distribution of speech signal features. It partitions the speaker spectral space into overlapping classes. Then, a continuous probabilistic linear transformation function is defined from these partitions for parametric vector representation of the envelope [1,14]. But the quality and naturalness of the converted speech are found to be inadequate due to two reasons: First is the use of a large number of parameters in speech synthesis technique, and the second is over smoothing these parameters [1,14,18]. Therefore, various methods have been proposed to deal with the reconstruction issue of the GMM based model. It includes the use of precise conversion of phase spectrum, the use of robust vocoding methods such as STRAIGHT and the use of appropriate source filter models [8]. The over-smoothing issue is resolved via maximum likelihood estimators and hybrid methods [16,19,20,17]. Partial Least Square (PLS) regression based kernel transformation technique is also proposed [21] to capture the nonlinearities in the data to overcome the over-smoothing issues of GMM.The speech synthesis technique based on Hidden Markov Model (HMM) produces the parameter vector when a new test input is given to a trained HMM model [22,23]. The resultant vector of the parameters is then used to synthesize the source speaker utterance by adopting HMM itself to the target speaker utterances [24,25]. However, the factors such as low quality of synthesized speech signal and over smoothing limits the usefulness of this approach. Followed by this nonlinear mapping capability of ANN is utilized to model dynamic pattern of vocal tract acoustic cues which itself is nonlinear in nature [11,12,9,13,26].Along with the vocal tract parameters, the source excitation signal is also an important speech parameter which contains speaker individuality [1,27,28] so the high quality voice conversion system needs to transform source excitation parameter with suitable mapping function. Some of the existing techniques in the literature are: linear transformation, unit selection [28], codebook copying [10], residual prediction [29] and time delay neural network [30]. However, one of the main issues of the ANN is that it can capture the speaker specific characteristics empirically at the epoch level. This problem is further compounded by the fact that both the vocal tract characteristics and residual signals are predictive in nature. Therefore the conventional approaches of residual modification fail to capture the correlation between the source and desired speaker. It produces artifacts and phase distortion in reconstructing speech [12]. In order to overcome these issues, a new wavelet based technique is proposed to characterize the pitch residuals. The pitch residual is decomposed using wavelet transform and GRNN based mapping function is developed to capture the relation between source and target speakers. The prosody modification such as speaking style and short time energy of the speaker is highly desired for procurement of the converted speech utterances which are perceptually closer to the desired speaker utterances [1,6]. In the literature, various techniques like segment and sentence contour codebook, scatter plots, GMM based and linear models are experimented for pitch contour mapping [31,32,26,33]. In our work, the pitch contour of test speaker is modified according to the desired speaker using PSOLA as a baseline method [34]. Fixed scale factor derived from test and target speaker utterances is used to modify the energy profile of the test speaker according to that of the target speaker.The physiological speech production structures for different speakers are highly nonlinear. The ANN and GMM based mapping functions are widely used to capture these nonlinearities in pattern. In spectral transformation approach using ANN and GMM models needs around 30–50 parallel utterances to formulate the voice conversion model. In addition both of these systems need to tune according to the amount of training data. An approach proposed in [9] has used Back propagation (BP) neural network to model the complex relations between the input–output feature vectors. The comparative analysis of spectral transformation between ANN and GMM based approaches using ARCTIC database is carried out in this approach. The results reveal the better performance of the ANN than that of the GMM based voice conversion model. Although this approach [9] is good in capturing the voice individuality and quality of the transformed speech, it also has some drawbacks. For instance, the performance of the network learning is strictly dependent on the shape of the error surface and connection weight initialization. The convergence to the global optimum is not guaranteed. Also the network architecture and parameters such as convergence rate and momentum needs to be determined by the user or by means of the optimized search. Besides the network parameters cannot be derived directly from the training examples. Therefore the selection of network parameters decides the success of network training for getting the required task done. Thus the time-consuming and iterative training procedure used in this approach limits its speed of convergence. Its another issue is only the convergence to local minima is guaranteed rather than global minima.In this paper, we present a powerful method for voice conversion based on GRNN. It is a kind of Radial Basis Function (RBF) networks, developed by Specht. Our approach can basically speed up the learning procedure by utilizing the well known characteristics of the GRNN such as parallel architecture and single pass learning. This approach also guarantees the network convergence to the optimal regression surface when the number of samples becomes very large without necessitating any type of iterative training [35–38].This paper mainly contributes to the following objectives: (1) Exploring GRNN based transformation model to capture the nonlinear mapping functions for modifying the LSF and wavelet decomposed LP-residual of a source speaker to that of a target speaker. (2) Evaluating the performance of GRNN based voice conversion system using subjective and objective methods. (3) Verifying that the proposed system performs better than that of the RBF and GMM based transformation models using evaluation measures.The paper is organized as follows: the brief introduction of the present work. Section 2 provides the overview of the voice conversion system. Section 3 explains the state of the arts GMM, RBF transformation models used for voice conversion system. The proposed GRNN based transformation model is explained in Section 4. Section 5 explains the prosody modification in detail. Experimental results and analysis are described in Section 6. The overall conclusions of the paper are derived in Section 7.A proposed model of voice conversion system is depicted in Fig. 1. The framework of the voice conversion system consists of two phases: training phase and testing phase. In the training phase, the phonetically balanced training utterances of the source and the target speaker are normalized and then the beginning and ending silence periods are removed using Voice Activity Detection algorithm (VAD). Then both the speaker's samples are analyzed (framing and windowing) to extract features to be transformed. The LPC based analysis is performed to extract the vocal tract parameters. The pitch residuals, derived from LPC parameters are used to characterize the source excitation of the speech signal. LPC parameters are then converted to LSF to overcome its poor interpolation, quantization and stability issues. In this paper, LSF parameters and first level wavelet decomposed pitch residual feature vectors of source and target speakers are time aligned using a DTW technique [9] and used to build the mapping rule for voice conversion system. The GRNN, GMM and RBF based transformation models are explored to form the mapping rules to modify the LSF and wavelet based pitch residuals.In the online voice conversion phase, the test speaker feature set is modified based on mapping rules obtained in the training phase. The modified LSF coefficients are de-normalized and reconverted into LPC parameters. In order to get the desired pitch residuals, the inverse wavelet transform is applied to the modified pitch residuals and modified short time frames are reconstructed from these modified features. The individual frames are combined using Overlap Add (OLA) method. The OLA methods generally consist of overlapping short-term modified frames of splitted speech signal at a certain analysis frame rate, and then overlap-adding them at a same frame rate. The desired speech waveform is obtained by concatenating the modified short time frames. Finally the pitch contour scaling factor between target speaker and modified speech is obtained and PSOLA is involved to change the transformed speech pitch contour according to that of target speaker pitch contour [34]. The energy of the converted speech is also scaled to that of the desired speaker speech. The post filtering is used to enhance the quality of the modified speech. Each part of proposed algorithm is explained in the subsequent sections.The RBF is the class of neural networks, which capture a globally nonlinear mapping function, whereas GMM model explores a locally linear and globally nonlinear transformation model in the context of voice conversion system [12,39,40]. The mathematical basis of the RBF and GMM are different, but they have common properties, hence expected to have the same performances [41–45]. This section presents a detailed theoretical analysis of the state of the art RBF and GMM based models for capturing the mapping rules to modify the vocal tract and the excitation parameters.The GMM is a soft decision classifier where each class has a Gaussian distribution, The GMM works with the assumption that the probability distribution of the sample observations takes the parametric form defined by Eq. (1)[1,14,46,47].(1)p(x)=∑i=1mαiN(x,μi,Σi)where m is the number of mixture models μiis the mean and Σicovariance matrix and N(x, μ, Σ) is the p dimensional distribution calculated as,(2)N(x,μ,Σ)=12πpΣexp12(x−μ)TΣ−1(x−μ)The weighting factor αiis the prior probability of class i with constraintsΣi=1mαi=1with αi≥1. The input vector xiis assumed to be independent. The conditional probability of cigiven the observation vector xis defined using Bayes rule as,(3)pcix=αiN(xi,μi,Σi)∑j=1mαjN(xj,μj,Σj)An unsupervised learning algorithm known as expectation maximization (EM) is used to find the parameter α, μ, Σ of Eq. (3) such that these estimated parameters give the maximum likelihood of observed data X. The aligned joint vectors of source, x and target, y are used to estimate GMM parameters. The total m number of GMM mixtures are used to implement the conversion function [1,16] defined as,(4)G(x)=∑i=1mpcix[μiyΣiyxΣixx−1(x−μi)x]where p(ci/x) is the conditional probability.(5)Σi=ΣixxΣixyΣiyxΣiyy(6)μi=ΣixΣiywhereΣixyis the cross covariance of the feature vector of the source and the target speaker, which is fully diagonal. Theμixandμiyare the mean vectors of the class i for the source and target feature vectors. We have made use of LSF as an acoustic parameter to characterize the vocal tract features and LP-residual are used to characterize the source excitation of the speech signal. The GMM model can effectively convert the acoustic cues of the speech signal. However the quality of the converted speech is over scaled due to extreme smoothing of the converted spectra. For GMM based mapping, the amount of training data decides the number of mixtures. It varies from 2 to 64. But in the experimental analysis, it has been observed that the 64 number of mixtures are enough to characterize the acoustic space of vocal tract and LP residual features. The transformation model developed during the training stage is then used to estimate the vocal tract and wavelet decomposed excitation parameters of the target speaker.Different network structures based on RBF are explored during training to capture non-linearity between the shape of the vocal tract and source excitation of the source and target speakers. The mapping functions are established to transform the nonlinearity between time aligned LSF and WP decomposed LP-residual. The RBF Neural Network is a special case of feed forward network which maps input space nonlinearly to hidden space followed by linear mapping from hidden space to output space. It is a 3-layer network with m input nodes, p hidden nodes and one or more output nodes. The output, φ(X, μj) of jth hidden node is radially symmetric. For the input X∈Rm, the output of the jth hidden node would be calculated as, φ(X−θj) where θj∈Rmis a parameter vector associated with jth hidden node.The output nodes compute a weighted sum of the outputs of the hidden nodes. Now, the output of the network with only one output node is calculated as,(7)y=∑j=1Pwjφ(||x−θj||)wherewjis the weight connecting the jthhidden node to the output node. For a Gaussian RBF network, the output is(8)y=∑j=1Pwjexp||X−θj||2σ2where μjand σ, are known as centre and the width of Gaussian function respectively. The different weights Wjessentially take care of different regions in the feature space. The mapping fromRm→Rm′), we have m′ nodes in the output layer. Then the outputs are computed as,(9)yq=∑j=1Pwjqexp||X−θj||2σ2Absorbing the bias parameter into the weights Eq. (9) becomes(10)yq=∑j=0Pwjqexp||X−θj||2σ2,q=1,2,…,m′where φ0 is an extra basis function with activation value fixed at 1. The training data isxn,tqn,n=1,2,N. The optimized weightswjqcan be obtained by minimizing the sum-of-squares error function(11)J=12∑n∑q[yk(xn)−tqn)]2where,tqnis desired value for qthoutput unit when input to the network is a vector xn. The weights are then determined by solving the linear equations(12)φTφWT=φTT(13)(φTφ)W=φTT(14)W=(φTφ)−1φTTwhere(φTφ)−1φTis pseudo inverse of matrix φ, T isdkn. The weight matrix W can be calculated by linear inverse matrix technique and used for mapping between the source and target acoustic feature vector [44,45]. The parameter matrix W obtained from Eq. (14) is used for mapping between the source and target acoustic feature vectors. Effective mapping between the source and target speakers needs to choose optimized kernel parameters which includes spread factor. LSF features are trained using spread factors lying in the range [0, 0.2] with a step of 0.05 and selected the value of sigma as 0.09 as shown in Fig. 2(a). Fig. 2(b) shows the LP residual training with the spread factor lies in range of [0, 1] and finalized value of sigma is 0.86. The sum squared error criteria (SSE) for each iteration were recorded as shown in Fig. 2. Optimum spread factor for each mapping function is chosen as the point of minimum SSE.The GMM and RBF based models are more efficient and robust than the various other models used for spectral mapping [12,42]. However, the GMM based models produce over-smoothed utterances and also, it needs some post processing to reduce the artifacts. RBF based models need to address over-training and over-smoothing issues. Also large amounts of data samples is required to train the GMM and RBF based voice conversion models [48]. In this paper GRNN based mapping function is explored, which has a special property that it does not need iterative training process as the kernel used in the network acting as a detector for computing the weight vectors simply adjusting its two parameters namely, width and centre.Let X be a vector of p independent variables which is used to predict a dependent scalar variable y. The conditional expectation of y for a given X is(15)EYX(X)=∫−∞∞yf(X,Y)dy∫−∞∞f(X,Y)dyIn practice, we usually never know the joint density f(X, y). Therefore the density has to be approximated by Parzen density estimator based on the set of training examples. The mathematics used in the estimator is simplest if a Gaussian kernel function is used. Parzen density estimator based on the Gaussian kernel based on j=1, 2, …, n training example is then given by Eq. (16).(16)fˆ(X,Y)=1(2π)(p−1)/2∑j=1n1σexp(X−Xj)T(X−Xj)2σ2exp(Y−Yj)T(Y−Yj)2σ2When the conditional mean of Eq. (16) is modified with the Parzen estimator of Eq. (15), we arrive at the basic equation of GRNN shown in Eq. (17).(17)Yˆ(X)=∑j=1nyjexp(−Dj2/2σ2)∑j=1nexp(−Dj2/2σ2)The Euclidean distance between the particular input sample and training data set pattern, Djis defined as:(18)Dj2=(X−Xj)T(X−Xj)ϑThe General Regression Neural Network topology consists in two static layers, namely pattern and summation layers as described in Fig. 3. The input layer nodes are fully connected to the pattern layer nodes. The pattern layer has one neuron for each of the input pattern. Letwijbe the target output corresponding to jth output and the input training vector xi. Each neuron in the pattern layer computes the Gaussian functions designated as:(19)θi=exp−Di22σ2Each unit in pattern layer thus stores and memorizes the relationship between the input xjand the desired response yj. The summation layer has two units P and Q corresponding to numerator and denominator terms of Eq. (19) respectively. The pattern layer outputs are applied to the summation layer units P and Q. Letwijbe the target output corresponding to jth output and the input training vector xiThe unit P computes the dot product of weightwijand pattern layer outputs θi. The second unit Q of summation layer has the weights equal to 1. Thus, it simply sums outputs from pattern units without applying any weights.Finally, the output unit just divides the output from unit P by the output from unit Q to produce the desired prediction result yjgiven as:(20)yj=∑i=1nwijθi∑i=1nθiThe spread factor also known as smoothing factor is the only free parameter in GRNN which is usually determined through iterative variations in the σ value. The optimum value of spread factor σ is selected after several trials in the training stage. The optimal value of smoothing factor is often identified with Sum Squared Error (SSE) criteria.In this work, the GRNN model is explored to map the vocal tract and the wavelet based residual parameters of the source speaker in accordance with the target speaker. General Regression Neural Network based vocal tract mapping is realized through the source and the target speaker LSF parameters xiand yirespectively. Fig. 4(a) shows that the minimum error is obtained when the (σ=0.06) which is in the range of 0.05–0.2. Fig. 4(a) illustrates that for smaller values of the spread factor (σ<0.06) the modified speech quality gets degraded as network cannot map large testing sets and for larger values (σ>0.05), modified speech quality becomes independent of new test inputs. In case of excitation mapping, the source and target speakers wavelet transformed residuals are used as input and output of the network, respectively. When GRNN model is used for residual mapping, the minimum error is found for the value of spread factor as 0.60.The prosodic parameters such as pitch contour, duration and energy of the test speech signal are modified according to the desired speaker. Out of these the pitch contour is modified using pitch synchronous baseline approach [49,34]. The pitch modification factor is the ratio of the average overall pitch of the target speaker to that of the desired target speaker. The modification factor, α can be formulated as [6],(21)α=psptwhere psand ptare the average pitch period of the source and target speaker respectively. The modified intonation pattern is obtained by scalar multiplication of the pitch contour of the test speech signal and pitch modification factor. For maintaining the naturalness of the converted speech signal modified intonation pattern is used during reconstruction. The pitch synchronous overlaps and adds method (PSOLA) provides appropriate integration of the modified pitch contour with the help of pitch markers (i.e. instants of significant excitation) of the test speaker [34]. The pitch of the particular segment can be changed by moving samples in the window. The sample movement is obtained using pitch markers further apart or close together to decrease or increase the pitch correspondingly, as shown in Fig. 5. Finally, the modified segments are combined using the overlap add technique.In order to design the effective voice conversion system, the energy of the test speaker also needs to be tuned according to the energy of the desired target speaker. The average energy for overall sentence in the database is calculated for both the speakers. The ratio of the average energy of the voiced segments is calculated for both the speakers. A scalar transformation factor is derived as the ratio of the average energy of the voiced segments of the source and target speakers. The mathematical donations for this scaling factor is,(22)β=esetwhere esand etare average energies of the source and target speakers respectively. The energy of the modified voice segments corresponding to the reconstructed speech signal is scaled with factor β in Eq. (22). The scaling factor is used to amplify or attenuate the energy of the reconstructed speech signal voice segments on frame by frame basis. Finally the scaled frames are merged by overlap-add methods resulting into prosody modified synthesized speech signal [12].The proposed voice conversion is carried out using publicly available CMU ARCTIC corpus to obtain the mapping rules. This corpus is recorded from two female and five male speakers at 16kHz sampling frequency. 1132 phonetically balanced parallel sentences are recorded by each speaker. The corpus includes sentences of JMK (Canadian Male), BDL (US Male), AWB (Scottish Male), RMS (US Male), KSP (Indian Male), CLB (US Female), and SLT (US Female) [50] and evaluated using objective and subjective tests.The objective evaluation involves computing the spectral variations between the desired and transformed speech signals. In this paper, different objective measures are considered to evaluate the comparative performance of the underlying voice conversion systems namely, LSF performance index, Mel Cepstrum Distortion (MCD) and Formant Distortion.For objective evaluation, we have used performance index (PLSF), which is formulated as follows,(23)PLSF=1−DLSF(d(n),d(n))ˆ(DLSF(d(n),s(n))where d(n) and s(n) represent the utterances of desired speaker and source speaker respectively,dˆ(n)is the transformed utterance. The numerator,DLSF(d(n),d(n)ˆ) is the spectral distortion between desired and transformed utterances and the denominator DLSF(d(n), s(n)) is the inter speaker spectral distortion, defined as,(24)DLSF(u,v)=1N∑i=1N1P∑j=1P(LSFui,j−LSFvi,j)2where N represents the number of frames, P refers to a LSF order andLSFui,jis the jth LSF component in the frame i. The performance index PLSF=1 indicates that the converted signal is identical to the desired one, whereas PLSF=0 specifies that the converted signal is not at all similar to the desired one.In the computation of this performance index, three different converted samples from male (M1) to female (F1) and female (F2) to male (M2) are considered and the results shown in Table 1. These results specify that the performance of the GRNN model is slightly better than that of RBF and GMM models.The performance of the transformation models is also evaluated using MCD scores as a function of different number of training pairs. The MCD is an objective evaluation measure, which correlates with subjective evaluation results. The MCD between the transformed and target speech is calculated as [9,21],(25)MCD[dB]=10ln10∑i=1Dmccita−mccitrwheremccitaandmccitrare the ith Mel-Cepstrum Coefficients (MCC) of the target and transformed speech respectively. The zeroth term is not considered in MCD computation as it describes the energy of the frame and it is usually copied from the source.For this evaluation, we have considered different trained models for AWB to CLB (AWB as a source speaker and CLB as a target speaker). Similarly, the transformation models for AWB to BDL, SLT to CLB, and SLT to AWB with different numbers of the source and target speaker parallel utterances are also developed. The MCD scores computed for different training pairs (5, 10, 17, 25 and 50) using different transformation models are shown in Table 2. From columns 2–4 of Table 2, it is clear that MCD score decreases with increase in the number of training pairs. It also illustrates that the MCD scores for GRNN mapping rules are much better than the state of the art RBF and GMM.The transformed LSF pattern for a specific source and target speech signal frame, obtained with GRNN, RBF and GMM are shown in Fig. 6(a), (b) and (c) respectively. Fig. 6 depicts the LSF patterns of particular target signal closely follows the corresponding transformed signals. It is clear from the formant mapping shown in Fig. 7that the predicted pattern closely follows the target pattern for first order LSF compared to higher order LSFs. This is the desired characteristics of the mapping as the speaker specific information is more dominant in the lower order LSFs.It can be seen from the above figures that the spectral peaks are located at similar positions for all the models. The magnitude of the power spectrum peaks in case of GRNN-based model is closer than that in the other state of the art models. The different objective measures for example, deviation Di, root mean square error (RMSE) and correlation coefficients σ(x, y) are also calculated for different speaker pairs [51–54]. Deviation parameter is defined as the percentage variation in the actual xkand predicted ykformant frequencies derived from the speech frames. It represents percentage of test frames within a specified deviation Dkis calculated as,(26)Dk=|xk−yk|xk×100The root mean square error is calculated as percentage of average of desired formant values obtained from the speech segments.(27)μRMSE=∑k|xk−yk|2x¯×100(28)σ=∑kdk2,dk=ek−μ,ek=xk−yk,μ=∑k|xk−yk|NThe error ekis the difference between the actual and predicted formant values. N is the number of observed formant values of speech frames. The parameter dkis the error in the deviation. The correlation coefficient (X,y) is the parameter which is to be determined using the covariance COV(X, Y) between the target (x) and the predicted (y) formant values and the standard deviations σX,σYof the target and the predicted formant values respectively. The parameters (X,y) and COV(X, Y) are calculated using Eq. (28),(29)γ(X,y)=COV(X,Y)σXσY,COV(X,Y)=∑k|(xk−x¯)(yk−y¯)|Nwherex¯andy¯are mean values of the target and predicted formants. Table 3shows different objective measures for male to female (M1–F1). The observation measures deviation (Di), root mean square error (RMSE) and correlation coefficients (γ(X,y)) are shown in columns 2–8, 9 and 10 of the Table 3. The scatter plots are used to evaluate the prediction performance of different voice conversion models. The scatter plots for first, second, third and fourth formant frequencies for different transformation models are shown in Figs. 7–9respectively.Figures show that GRNN and RBF based vocal tract envelope in term of predicted formants closely orient towards the desired speech frames formants as compared to GMM based predicted formants. Ideally, for zero prediction error, all the data points are expected to lie on the diagonal oriented in right side.The normalized frequency spectrogram is shown in Fig. 10; it consists of desired speech signal together with transformed speech signals using GRNN, RBF and GMM models. From the figures, it has been observed that the dynamics of the first three formant frequencies in case of the proposed algorithm are closely followed in the target and the transformed speech samples while there is non-conformity in fourth formant. However, the dynamics of formants in case of the state of the art techniques have some variations as compared to proposed method.The fundamental objective of the voice conversion system is to adopt the source speech signal in order to impersonate the desired speaker characteristics. Therefore the listening tests are conducted to assess the desired speaker characteristics present in transformed speech signal with speaker identity and speech quality. Distinct mapping rules are formed for transforming the speech signals from M1–M2, F1–F2, M2–F2 and F2–M2 and the corresponding mapping rules are used to synthesize 12 utterances for each of the case. Two subjective listening tests, such as ABX for speaker identity and Mean Opinion Score (MOS) for quality and naturalness of the transformed speech signal are conducted to evaluate the performance of voice conversion system. The recorded voices of the desired speaker corresponding to the synthesized speech signals are made available to thirteen listeners to assess the relative performance. Each of the listeners has to give opinion using mean opinion scores to assess the perceptual quality on a scale of 1–5. Rating 5 specifies an excellent match between the transformed and target utterances, the score of 1 indicates a poor match between the desired target utterance with the transformed utterance and the other scores specify different degrees of variation between 1 and 5. The ratings given to each set of utterances are used to calculate the MOS for intra-gender and inter gender voice conversion systems. MOS results for GRNN, RBF and GMM are compared in Fig. 11(a). The figure justifies that the GRNN based synthesized speech is more natural compared to the RBF which in turn is better compared to GMM based model. The proposed system is more efficient for inter-gender than the intra gender voice conversion. The larger divergence in the length of the vocal tract and the intonation patterns of different genders is the main reason for improved MOS results for the source and the target utterances of different genders.In addition to MOS test, the ABX (where A: Source, B: Target and X: Transformed speech signals) evaluations are also performed using the same set of utterances and speakers. In the ABX test, the same listeners are asked to evaluate whether the unknown utterance X sounds closer to the reference utterance A or B. The ABX is a degree of individuality transformation. The higher ABX percentage denotes that the transformed speech sounds more like the desired target utterance. The results of the different ABX tests corresponding to different voice conversion systems are shown in Fig. 11(b).In this work, we have also verified the mapping abilities of individual or/and combination of each parameter such as vocal tract, residual, pitch contour and energy of the source speaker on to that of target speaker. From a subjective analysis, it has been observed that the all speaker dependent parameters are essential to capture high quality voice conversion.

@&#CONCLUSIONS@&#
