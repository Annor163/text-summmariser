@&#MAIN-TITLE@&#
An exploration of ranking models and feedback method for related entity finding

@&#HIGHLIGHTS@&#
We derive six generative models for related entity finding.We conduct both analytical and empirical studies to compare these six models.We propose a novel entity relation-based feedback method to improve performance.

@&#KEYPHRASES@&#
Entity retrieval,Feedback model,

@&#ABSTRACT@&#
Most existing search engines focus on document retrieval. However, information needs are certainly not limited to finding relevant documents. Instead, a user may want to find relevant entities such as persons and organizations. In this paper, we study the problem of related entity finding. Our goal is to rank entities based on their relevance to a structured query, which specifies an input entity, the type of related entities and the relation between the input and related entities. We first discuss a general probabilistic framework, derive six possible retrieval models to rank the related entities, and then compare these models both analytically and empirically. To further improve performance, we study the problem of feedback in the context of related entity finding. Specifically, we propose a mixture model based feedback method that can utilize the pseudo feedback entities to estimate an enriched model for the relation between the input and related entities. Experimental results over two standard TREC collections show that the derived relation generation model combined with a relation feedback method performs better than other models.

@&#INTRODUCTION@&#
Traditional information retrieval models have mainly focused on finding documents relevant to an information need. However, information needs are certainly not limited to relevant documents. For example, a consumer may want to find cell phone carriers selling iPhones, or a conference PC chair may want to find potential reviewers who are knowledgeable in certain research areas. Although users could use existing IR systems to find relevant entities by going through every returned document and manually identifying the entities, the whole process would be labor-intensive and time-consuming. Thus, it is necessary to study how to automatically retrieve entities, such as persons and organizations, that are relevant to a given query.The problem of related entity finding is finding entities related to the information specified in a query. Unlike keyword queries used in document retrieval, an entity-related query needs to specify both the type and the relevance criteria of the target entities. Following the problem setup in the TREC Entity Track (Balog, de Vries, Serdyukov, Thomas, & Westerveld, 2010), a query for the related entity finding task is formulated as a structured query, which specifies an input entity, the type of the related entities to be found, and the relation between the input and the related entities. The goal is to find the related entities, which have the specified type and relation with the input entity, from a supporting document collection. For example, a user who wants to find ACM Athena award winners may formulate a query as shown in Fig. 1, and this query specifies that the input entity is “ACM Athena award”, the type of related entities (i.e., target entities) is “person” and the relation is “winners of” (note that the narrative field is a mixture of input entity and relation), in a supporting document collection. The results are expected to include a list of the award winners such as “Deborah Estrin”, “Shafi Goldwasser” and “Karen Spärck Jones”.The related entity finding problem can be naturally formulated as two subtasks: (1) candidate extraction, i.e., how to extract candidate entities based on the type specified in the query, and (2) candidate ranking, i.e., how to rank candidates according to their relevance to the query. While the first subtask can be solved using an off-the-shelf NER tagger or external resources such as DBPedia (McCreadie, Macdonald, Ounis, Peng, & Santos, 2010; Serdyukov & de Vries, 2010), the second subtask does not have any existing straightforward solution, and is thus more challenging.In this paper, we focus on the second subtask, i.e., to rank candidate entities based on their relevance to a structured query. The main challenge is to leverage the structured query to estimate the probability that a candidate entity is relevant. Motivated by the general probabilistic frameworks proposed for traditional ad hoc retrieval (Lafferty & Zhai, 2003) and expert finding (Fang & Zhai, 2007), we derive six generative models for candidate entity ranking and then analytically and empirically compare these six functions. To further improve the performance, we study the problem of feedback in the context of related entity finding. In particular, we propose a novel mixture model based method that can utilize the pseudo feedback entities to estimate an enriched model for relation between the input and related entities. We conducted experiments on two standard TREC collections to evaluate the derived generative ranking models and the proposed relation-based feedback method.We find that incorporating candidate priors is crucial to achieve reasonable retrieval performance. As a result, the retrieval models that can incorporate candidate priors are better choices than others since they are less sensitive to the quality of entity candidates. Furthermore, the proposed relation-based feedback methods are effective to further improve the retrieval performance.The rest of the paper is organized as follows. We formulate the problem of related entity finding and present a general approach in Section 2. We discuss the proposed generative models in Section 3 and propose relation-based feedback method in Section 4. We discuss experiment results in Section 5, and discuss the related work in Section 6. Finally, we conclude in Section 7.The problem of related entity finding is to find the entities that are related to the information described in a query based on a supporting document collection. Unlike keyword queries used in the traditional ad hoc retrieval task, queries used in the related entity finding task are structured as shown in Fig. 1. In particular, a query specifies the type of related entities, an input entity and the relation between the input and related entities.Formally, let D={d1,d2,…,dn} denote a supporting document collection, Et={et1,et2,…,etm}denote a set of entities with entity type t, and r(ei,ej)∈{0,1} denote whether there exists a relation r between two entities eiand ej. We define q= {t,ein,r} as a query with three components for the related entity finding task, where t is the type of related entities, ein is the input entity, and r is the relation between the input and related entities. Note that if a query is formulated in the same format as the one discussed in Fig. 1, ein corresponds to the value of the “entity_name” field, t corresponds to the value of the “target_entity” field, and r corresponds to the value of the “narrative” field excluding the value of the “entity_name” field. Given query q, the goal is to find a set of entities E(q) that have type t and have relation r with ein:(1)E(q)=⋃et∈Et∧r(ein,et)=1{et}.It often requires a two-step approach to solve the problem of related entity finding. The first step is candidate extraction, which is to find all the candidate entities with the specified type, i.e., et∈Et. A simple solution is to directly apply an off-the-shelf NER tagger to extract candidate entities based on the specified type. The second step is candidate ranking, which is to further refine the candidate entities extracted in the first step and find those having the specified relation with the input entity, i.e,. et∈Et, where r(ein,et)=1.In this paper, we focus on the candidate ranking step. Following the spirit of the probabilistic ranking principle (Robertson, 1977), we rank the candidate entities according to the probabilities that they are relevant to a query. Thus, the key challenge is how to estimate the probability of a candidate entity being relevant, i.e., how likely it has the specified relations r with the input entity ein. Unlike keyword queries used in traditional retrieval tasks, we focus on deriving generative retrieval models and feedback method for the structured queries. The details are given in Section 3 and Section 4.With a list of extracted candidate entities and the corresponding structured query, we now discuss how to assign a relevance score to each candidate based on the probability that the candidate has the specified relations with the input entity.Given a query q={t,ein,r} and a related entity candidate et∈Et, we need to estimate the probability that etis relevant to q, i.e.,P(R=1|q,et), whereRis a binary random variable denoting the relevance. Following the previous studies (Fang & Zhai, 2007; Lafferty & Zhai, 2003) we may use the odds ratio to rank candidates and then apply Bayes’ rule for the estimation:(2)P(R=1|q,et)=rankP(R=1|q,et)P(R=0|q,et)=P(q,et|R=1)·P(R=1)P(q,et|R=0)·P(R=0)=P(ein,r,et|R=1)·P(R=1)P(ein,r,et|R=0)·P(R=0),where=rankmeans the two values are equivalent for ranking entity candidates et.Now the challenge is how to factor the conditional probabilityP(ein,r,et|R). Since there are three variables involved, we may apply the chain rule in different ways based on their dependency relations shown in Fig. 2. Each dependency relation corresponds to a generative process among these variables. As an example, RG (relation generation) model assumes that relations are generated from the input and target entities.By comparing these models, we can make the following observations:•The top three models (i.e. RG, IEG and QG) share a similar property that etgenerates the other components, which enables us to incorporate a candidate-related prior for et, while the other models cannot utilize the candidate priors.Every pair of the models in the same column (i.e. RG and EG, IEG and RERG, QG and REG) would share some commonalities in the ranking functions because one component either generates the other two or is generated by the other two.We now discuss the derivation of the six generative models. Since the derivations are similar, we provide the details only for RG model and briefly explain the derivations for the other models.The RG model assumes that the relation between two entities is generated by a probabilistic model based on the entities (as shown in Fig. 2a). Thus, the conditional probability in Eq. (2) can be factored in the following ways:P(R=1|q,et)=rankP(ein,r,et|R=1)·P(R=1)P(ein,r,et|R=0)·P(R=0)=P(r|ein,et,R=1)·P(ein,et|R=1)·P(R=1)P(r|ein,et,R=0)·P(ein,et|R=0)·P(R=0)=P(r|ein,et,R=1)·P(R=1|ein,et)P(r|ein,et,R=0)·P(R=0|ein,et)It is reasonable to assume that conditioned on the eventR=0, entities and their relations (i.e., ein, etand r) are independent. Under this assumption, we get(3)P(R=1|q,et)=rankP(r|ein,et,R=1)·P(R=1|ein,et)P(r|R=0)·P(R=0|ein,et)=rankP(r|ein,et,R=1)·P(R=1|ein,et)P(R=0|ein,et)Note thatP(r|R=0)can be safely ignored for the purpose of ranking candidate entities since it is independent of et.We now discuss how to estimateP(r|ein,et,R=1)in Eq. (3), which is the likelihood of ein and ethaving the relation r given we know that two entities ein and etare related, One possible approach is to directly estimate the probability based on an existing knowledge base about entities and their relations. However, one limitation is that the coverage of the knowledge base is not complete, which could lead to inaccurate estimation. In this paper, we instead propose to leverage supporting documents from the collection as a bridge to estimate the probability by exploiting the co-occurrences of these three variables (i.e., ein, etand r) as follows:P(r|ein,et,R=1)=∑d∈DP(r|d,ein,et,R=1)P(d|ein,et,R=1)=∑d∈DP(r|d,R=1)P(d|ein,et,R=1)=rank∑d∈DP(r|d,R=1)×P(ein,et|d,R=1)∑d′∈DP(ein,et|d′,R=1)One assumption we have made in this estimation process is that r is independent of ein and etgiven supporting document d. Fig. 3shows a graphical representation of the relation generation model under the assumption.There is another component in Eq. (3) that we need to estimate:P(R=1|ein,et)P(R=0|ein,et)=P(R=1|ein,et)1-P(R=1|ein,et), in whichP(R=1|ein,et)serves as the prior probability that etand ein are related. We consider two methods: (1) UniPrior, which assumes that the prior probabilities are uniformly distributed; and (2) OccPrior, which estimates the prior probability of the two entities’ being related based on their co-occurrence in the supporting documents, i.e.,P(R=1|ein,et)P(R=0|ein,et)∝c(ein,et,D)∑e∈Etc(ein,e,D), where c(ein,et,D) is the number of documents that mention both entities in the supporting document collection D and Etis a set of to-be-ranked entity candidates with the type t. The supporting document collection contains top ranked documents for the query.The EG model assumes that the input and candidate entities are generated by a probabilistic model based on their relation (as shown in Fig. 2b). Thus, the conditional probability in Eq. (2) can be factored as follows:(4)P(R=1|q,et)=rankP(ein,r,et|R=1)·P(R=1)P(ein,r,et|R=0)·P(R=0)=P(ein,et|r,R=1)·P(r|R=1)·P(R=1)P(ein,et|r,R=0)·P(r|R=0)·P(R=0)=P(ein,et|r,R=1)·P(R=1|r)P(ein,et|r,R=0)·P(R=0|r)=rankP(ein,et|r,R=1)Here we make the similar assumption as in the derivation of the RG model (i.e., etand ein are independent of r conditioned onR=0). AndP(R|r)is safely ignored since it does not affect the ranking of the candidate entities. The component in Eq. (4) is estimated as:P(ein,et|r,R=1)=∑d∈DP(ein,et|d,r,R=1)P(d|r,R=1)=∑d∈DP(ein,et|d,R=1)P(d|r,R=1)=rank∑d∈DP(ein,et|d,R=1)×P(r|d,R=1)∑d′∈DP(r|d′,R=1)=rank∑d∈DP(r|d,R=1)P(ein,et|d,R=1)Here we drop∑d′∈DP(r|d′,R=1)as it does not affect the ranking of et.The IEG model assumes that an input entity is generated by a probabilistic model based on the candidate and their relation (as shown in Fig. 2c). Following the similar assumptions used for the derivation of other models, we can rank the candidates using:(5)P(R=1|q,et)=rankP(ein,r,et|R=1)·P(R=1)P(ein,r,et|R=0)·P(R=0)=rankP(ein|et,r,R=1)·P(R=1|et,r)P(R=0|et,r)The two component functions can be estimated as:P(ein|et,r,R=1)=rank∑d∈DP(ein|d,R=1)×P(et,r|d,R=1)∑d′∈DP(et,r|d′,R=1)P(R=1|et,r)P(R=0|et,r)∝c(et,r,D)∑e∈Etc(e,r,D)The RERG model assumes that both relation and candidate entities are generated by a probabilistic model based on an input entity (as shown in Fig. 2d). Following the similar assumptions used for the derivation of other models, we can rank the candidates using:(6)P(R=1|q,et)=rankP(ein,r,et|R=1)·P(R=1)P(ein,r,et|R=0)·P(R=0)=rankP(et,r|ein,R=1),whereP(et,r|ein,R=1)can be estimated as:P(et,r|ein,R=1)=rank∑d∈DP(ein|d,R=1)P(et,r|d,R=1)The QG model assumes that a query with an input entity and a relation is generated by a probabilistic model based on a candidate entity (as shown in Fig. 2e). Applying similar assumptions, the model is shown as follows:(7)P(R=1|q,et)=rankP(ein,r,et|R=1)·P(R=1)P(ein,r,et|R=0)·P(R=0)=rankP(ein,r|et,R=1)·P(R=1|et)P(R=0|et),whereP(ein,r|et,R=1)can be estimated as:P(ein,r|et,R=1)=rank∑d∈DP(ein,r|d,R=1)×P(et|d,R=1)∑d′∈DP(et|d′,R=1)andP(R=1|et)P(R=0|et)∝c(et,D)∑e∈Etc(e,D).The REG model assumes that a related entity candidate is generated by a probabilistic model based on the input entity and their relation (as shown in Fig. 2f). Applying similar assumptions made for other models, the model is shown as follows:(8)P(R=1|q,et)=rankP(ein,r,et|R=1)·P(R=1)P(ein,r,et|R=0)·P(R=0)=rankP(et|ein,r,R=1),whereP(et|ein,r,R=1)can be estimated as:P(et|ein,r,R=1)=rank∑d∈DP(ein,r|d,R=1)P(et|d,R=1)

@&#CONCLUSIONS@&#
We study the related entity finding problem in this paper. Specifically, we focus on ranking candidate entities based on their relevance to a structured query. To tackle this problem, we first derive six generative models to rank candidates. As far as we know, this is the first systematic study of the formal models for related entity finding. Experimental results indicate that the retrieval performance is closely related to the quality of extracted candidate sets. It is more effective to incorporate candidate prior when the quality of candidates is higher. Moreover, we propose a novel relation based feedback method that can utilize the pseudo relevant entities to better capture the relation between entities. Experimental results show that the proposed feedback method is effective to improve the performance.There are many interesting future research directions. First, we plan to explore alternative ways of estimating component functions in the models. Second, we could study how to leverage the NER results in the ranking procedure. Finally, it would be interesting to study how to combine our proposed ranking strategies with existing QA methods to improve the performance.