@&#MAIN-TITLE@&#
An investigation on the serendipity problem in recommender systems

@&#HIGHLIGHTS@&#
We design a Knowledge Infusion (KI) process for providing systems with background knowledge.We design a KI-based recommendation algorithm for providing serendipitous recommendations.An in vitro evaluation shows the effectiveness of the proposed approach.We collected implicit emotional feedback on serendipitous recommendations.Results show that serendipity is moderately correlated with surprise and happiness.

@&#KEYPHRASES@&#
Recommender systems,Serendipity problem,Knowledge representation,Spreading activation,Affective feedback,Facial expressions,

@&#ABSTRACT@&#
Recommender systems are filters which suggest items or information that might be interesting to users. These systems analyze the past behavior of a user, build her profile that stores information about her interests, and exploit that profile to find potentially interesting items. The main limitation of this approach is that it may provide accurate but likely obvious suggestions, since recommended items are similar to those the user already knows. In this paper we investigate this issue, known as overspecialization or serendipity problem, by proposing a strategy that fosters the suggestion of surprisingly interesting items the user might not have otherwise discovered.The proposed strategy enriches a graph-based recommendation algorithm with background knowledge that allows the system to deeply understand the items it deals with. The hypothesis is that the infused knowledge could help to discover hidden correlations among items that go beyond simple feature similarity and therefore promote non-obvious suggestions. Two evaluations are performed to validate this hypothesis: an in vitro experiment on a subset of the hetrec2011-movielens-2k dataset, and a preliminary user study. Those evaluations show that the proposed strategy actually promotes non-obvious suggestions, by narrowing the accuracy loss.In the book “The Filter Bubble: What the Internet Is Hiding from You”, Eli Pariser argues that Internet is limiting our horizons (Parisier, 2011). He worries that personalized filters, such as Google search or Facebook delivery of news from our friends, create individual universes of information for each of us, in which we are fed only with information we are familiar with and that confirms our beliefs. These filters are opaque, that is to say, we do not know what is being hidden from us, and may be dangerous because they threaten to deprive us from serendipitous encounters that spark creativity, innovation, and the democratic exchange of ideas. Similar observations have been previously made by Gori and Witten (2005) and extensively developed in their book “Web Dragons, Inside the Myths of Search Engine Technology” (Witten, Gori, & Numerico, 2006), where the metaphor of search engines as modern dragons or gatekeepers of a treasure is justified by the fact that “the immense treasure they guard is society’s repository of knowledge” and all of us accept dragons as mediators when having access to that treasure. But most of us do not know how those dragons work, and all of us (probably the search engines’ creators, either) are not able to explain the reason why a specific web page ranked first when we issued a query. This gives rise to the so called bubble of Web visibility, where people who want to promote visibility of a Web site fight against heuristics adopted by most popular search engines, whose details and biases are closely guarded trade secrets.Also recommender systems, which suggest to users items or information they might be interested in (Ricci et al., 2011), give their contribution to the filter bubble (Kamishima, Akaho, Asoh, & Sakuma, 2012; Resnick, Konstan, & Jameson, 2011). These systems analyze a user’s past behavior, maybe find others who have a similar history, and use that information to provide suggestions. For example, if you tell the Internet Movie Database (IMDb)1www.imdb.com.1that you like the movie Star Trek into Darkness, it will suggest movies liked by other people who liked that movie (“People who liked this also liked…” in Fig. 1), most of whom are probably science-fiction fans. Furthermore, one of those recommendations is a movie of the same saga, which is likely to be already known to the user. The user will be provided with items within her existing range of interests and her tendency towards a certain behavior is reinforced by creating a self-referential loop. This drawback is known as overspecialization or serendipity problem (McNee, Riedl, & Konstan, 2006), and stems from the fact that the goal of the system is to find items that best match the model of user preferences in order to improve accuracy, regardless of the actual usefulness of the suggestions. The importance of taking into account factors, other than accuracy, which contribute to the perceived quality of recommendations is emphasized in recent research (Castells, Wang, Lara, & Zhang, 2011; Hurley & Zhang, 2011; Zhang, Séaghdha, Quercia, & Jambor, 2012). One of these factors is serendipity, that can be seen as the experience of receiving unexpected suggestions helping the user to find surprisingly interesting items she might not have otherwise discovered, or that would have been really hard to discover (Herlocker, Konstan, Terveen, & Riedl, 2004). Serendipity has been recognized as a goal that often conflicts with accuracy (Fleder & Hosanagar, 2009), therefore it is important that systems were designed and evaluated by taking into account the need of properly balancing these two factors. As an extreme case, let us consider random recommendations, which improve serendipity but cause a drastic loss in accuracy, making the system actually ineffective.In this paper, we investigate the following two issues related to the serendipity problem:1.does the inclusion of knowledge into the recommendation process (which aims at providing the system with deeper understanding of the items it deals with) help to find serendipitous, non-obvious and at the same time effective suggestions?how to measure the perception of serendipity, i.e. how to assess the acceptance of suggestions, given that providing non-obvious recommendation can hurt the accuracy of the system?The main contributions of the paper with respect to the above mentioned issues are:1.the definition of a strategy based on a knowledge intensive process, called Knowledge Infusion (KI) (Semeraro, Lops, Basile, & de Gemmis, 2009a), that automatically builds a machine-readable background knowledge, the memory of the recommender system, exploited by a reasoning algorithm to find meaningful hidden correlations among items. The hypothesis is that, if the recommendation process exploits the discovered associations rather than classical feature similarities or co-rating statistics, more serendipitous suggestions can be provided to the user;the in-depth analysis of the results of both an in vitro experimental evaluation on a benchmark dataset and a preliminary user study carried out in order to validate the proposed hypothesis. In particular, the user study assessed the serendipity of suggestions by means of a tool which allows to gather implicit user feedback through the analysis of their facial expressions.The analysis of the items in the recommendation lists produced by the proposed strategy leads us to conclude that they show an acceptable balance of serendipity and accuracy.Several definitions of serendipity have been proposed in recommender systems literature. A commonly agreed one, proposed by Herlocker et al. (2004), describes serendipitous recommendations as the ones helping the user to find surprisingly interesting items she might not have discovered by herself. McNee et al. (2006) identify serendipity as the experience of receiving an unexpected and fortuitous item recommendation, while Shani and Gunawardana (2011) state that serendipity involves a positive emotional response of the user about novel items and measures how surprising these recommendations are.According to these definitions, serendipity in recommender systems is characterized by interestingness of items and the surprise for users who get unexpected suggestions. Therefore, in our work we define serendipitous suggestions those which are both attractive and unexpected. While attractiveness is usually determined in terms of closeness to the user profile (Lops, de Gemmis, & Semeraro, 2011), the assessment of unexpectedness of recommendations is not immediate. Previous studies agreed on defining unexpectedness as the deviation from a benchmark model or primitive prediction method that generates expected recommendations (Ge, Delgado-Battenfeld, & Jannach, 2010; Murakami, Mori, & Orihara, 2008). For example, in case of a movie recommender system, expected recommendations could be blockbusters seen by many people, or movies related to those already seen by the user, such as sequels, or those with same genre and director.For the evaluation of the proposed strategy, unexpectedness will be measured with respect to benchmark models based on popularity and average rating of items (Section 5.1.2).In order to make clearer the adopted definition of serendipity, it is useful to point out the differences with related notions of novelty and diversity.The novelty of a piece of information generally refers to how different it is with respect to “what has been previously seen” by a user or a community. Novelty occurs when a recommender system suggests to the active user an unknown item that she might have autonomously discovered (Herlocker et al., 2004; Vargas & Castells, 2011). Let us consider a recommender system that simply suggests movies directed by the user’s favorite director. If the system recommends a movie the user was not aware of, the movie will be novel, but not serendipitous. On the other hand, a movie by a young, not very popular director is more likely to be serendipitous (and also novel).Diversity represents the variety present in a list of recommendations (Adomavicius & Kwon, 2012; Fleder & Hosanagar, 2009; Zhang et al., 2012). Methods for the diversification of suggestions are generally used to avoid homogeneous lists, in which all the items suggested are very similar to each other. This may reduce the overall quality of the recommendation list because none of the alternative suggestions will be liked, in case the user wants something different from the usual. Although diversity is very different from serendipity, a relationship between the two notions exists, in the sense that providing the user with a diverse list can facilitate unexpectedness (Adamopoulos & Tuzhilin, 2011). Continuing with our example, we can reasonably assume that users could be surprised to some extent when seeing a romantic movie within the list of science-fiction movies shown in Fig. 1. However, the diversification of recommendations does not necessarly imply serendipity since diverse items could all fall into the range of user preferences.The acquisition of information in an accidental or serendipitous manner is a recognized information seeking metaphor investigated in literature (Erdelez, 2004; Foster & Ford, 2003; Toms, 2000). In particular, Toms suggests four strategies to induce serendipity in the search process (Toms, 2000):•Blind luck or role of chance, implemented via a random information node generator. In the context of recommender systems, that strategy might be implemented by providing random suggestions.Pasteur principle, i.e. “chance favors only the prepared mind”, meaning that sudden flashes of insight do not just happen, but they are the product of preparation. Recommender systems could implement the “prepared mind” paradigm by applying information about user preferences in different contexts. For example, if the system knows that a user is interested in science-fiction movies, it might exploit that information when the user is looking for a hotel as well, and suggest the Hilton in Las Vegas because it hosts a Star Trek flight simulator.Anomalies and exceptions, that might be implemented using distance measures able to identify items dissimilar to those the user liked in the past.Reasoning by analogy, which implies an abstraction mechanism allowing the system to discover the applicability of an existing schema to a new situation.In this paper we propose an approach related to the Pasteur principle. It is grounded on the idea that the capability of an algorithm to produce serendipitous suggestions could be improved by the knowledge infusion process described in Section 3, which provides the system with a memory of world facts and linguistic competencies, and therefore contributes to build the prepared mind.The recommendation algorithm adopted is Random Walk with Restarts (Lovasz, 1996), augmented with the infused knowledge to build an advanced item correlation matrix where more significant associations are inferred by the prepared mind, compared to the standard item similarity computation. Section 4 describes the details of the whole recommendation process.The Knowledge Infusion (KI) process builds a computer-understandable knowledge repository which constitutes the cultural and linguistic background of the system. The repository is automatically fed by information obtained from several knowledge sources freely available, such as Wikipedia. The main motivation for this choice, compared to the adoption of specific handcrafted ontologies, is the willingness to design a general strategy which allows to update the knowledge repository easily, as well as to plug in additional sources, without changing the overall organization and implementation of the process.KI consists of two steps:1.Knowledge Extraction and Harmonization: Linguistic knowledge is extracted from WordNet (Fellbaum, 1998), while encyclopedic knowledge is obtained from Wikipedia. Due to the different organization of the sources (articles in Wikipedia, synsets in WordNet), an harmonization phase turns the extracted concepts in a homogeneous format. Linguistic knowledge is useful to recognize general concepts into item descriptions, while encyclopedic knowledge is useful to recognize specific concepts or named entities, usually not included in a dictionary. More details are provided in Section 3.1.Reasoning: It allows to make inference on the background knowledge and item descriptions, in order to discover information potentially useful for the recommendation step. More details are provided in Section 3.2.The heterogeneity of the knowledge sources involved in the process requires:•the identification of the basic unit representing a concept in each specific source;the adoption of a unique representation model for all the basic units in the sources.As for the first issue, the idea is that the basic unit of a knowledge source corresponds to a primary concept it represents. We consider an article as basic unit for Wikipedia, since it provides details about an entity (“Alan Turing”), a world fact (“Normandy landings”), or a generic concept (“Computer”). For WordNet, the basic unit is the synset, which provides the short description of a generic concept (gloss) and lists all the synonyms expressing that concept.As for the representation model, being each basic unit a fragment of text, we adopt the standard bag-of-words (BOW) model and tf-idf as term weighting scheme. We call the BOW representation of a basic unit a Cognitive Unit (CU), because it provides the machine-readable format of the concepts on which the reasoning mechanism works. The name stems from the Adaptive Control of Thought (ACT) theory by J. R. Anderson, according to which information in the long term memory of human beings is encoded as cognitive units that form an interconnected network (Anderson, 1983). The reasoning algorithm of the system is inspired by that theory.Some Natural Language Processing operations are applied on basic units to obtain the corresponding CUs:•Wikipedia: Title and full text of an article are processed by tokenization, stopword elimination, lemmatization, named entity recognition. Simple heuristics (not described for brevity) are adopted for boosting tf-idf scores of emphasized words (Semeraro, Lops, Basile, & de Gemmis, 2009b), while feature selection by using tf-idf thresholding is applied to filter out less significant words.WordNet: The lemma and its synonyms, the keywords in the gloss, as well as the keywords in the example phrases, are processed by tokenization, stopword elimination, lemmatization, named entity recognition. Polysemous words originate one CU for each possible meaning.The resulting CUs are stored in separate repositories. The main advantage of having CUs in the form of BOWs is that CU repositories can be represented by using the Vector Space Model. This provides an easy and immediate way to find relevant CUs associated with any keywords, by simply querying the CU repositories and computing relevance as cosine similarity. The whole KI process is described in Fig. 2: the Knowledge Extraction and Harmonization phase is performed by the Knowledge Extractor; then, once the background memory is available, the reasoning step is triggered by a set of keywords, which we call clues, exploited to query the CU repositories in order to retrieve the most appropriate pieces of knowledge. Both clues and retrieved CUs are then passed to the Reasoning Module, which produces a new list of related keywords, which can be exploited in the recommendation process to suggest or search for related items. The advantage is that search is performed using keywords that are not necessarily included in the description of preferred items, which might allow for the selection of non-obvious and potentially serendipitous items.As introduced in previous section, the reasoning mechanism is inspired by the ACT theory (Anderson, 1983), according to which words and their meanings are stored in the mind in a network-like structure. The algorithm implemented in the Reasoning Module is based on a Spreading Activation model (Collins & Loftus, 1975), consisting of a network of nodes called Spreading Activation Network (SAN) on which a search process is performed. The model has been successfully adopted in Information Retrieval (Crestani, 1997; Crestani & Lee, 2000).Nodes in the SAN built for the KI process represent words or CUs, and links between them are usually weighted according to the strength of their relationship, obtained from CU repositories.The method that builds the SAN and activates the spreading process is described by means of the following illustrative example in a movie recommendation scenario. We start from the “seed movie” Star Trek into Darkness (Fig. 1), and we are looking for related items. We choose the two plot keywords (among those provided by the IMDb web site) alien and battle as clues to query the Knowledge Retrieval module. For the sake of simplicity, we assume that, for each clue, two CUs are retrieved from each knowledge source:•2 CUs from the WordNet dictionary (Dic1andDic2) corresponding to different meanings of the keyword alien, i.e. a stranger or an extraterrestrial being;2 CUs from Wikipedia (Wiki1andWiki2) corresponding to different meanings of the keyword alien, i.e. the movie by Ridley Scott or the extraterrestrial life;2 CUs from the WordNet dictionary (Dic3andDic4) corresponding to different meanings of the keyword battle, i.e. war or the battleship game;2 CUs from Wikipedia (Wiki3andWiki4) corresponding to different meanings of the keyword battle, i.e. war or the battleship game. In this case, battle has the same meanings as for the dictionary, but the keywords in the CUs are different due to different content of the Wikipedia article and the WordNet synset.The construction of the SAN is described in the following paragraphs and the result is shown in Fig. 3. Initially, two source nodes labeled with the two clues are included into the SAN. Then, retrieved CUs are included in the SAN. Each CU is linked to the corresponding source node; the edge is oriented from the clue to the CU and is labeled with the cosine similarity value between the clue and the CU. At this stage of the process, edges represent associations between clues and CUs, while similarity values measure the strength of those relationships. Finally, for each CU node, word nodes labeled with terms in the BOW of the CU are included in the SAN. Links are created from the CU node towards its word nodes and labeled with tf-idf scores of words.Once the SAN is built, the reasoning process starts from the clues, which trigger the search process over the network. Each nodenihas an associated activation levelali, which is a real number in the range [0.0 …1.0], and represents the level of stimulus of the node. At timetm=1the SAN is initialized by setting all activation levels to 0, with exception of the clues, whose activation level is set to 1. A threshold F, a real number in the range [0.0 …1.0], determines if a node is fired, that is to say whether it can spread its activation level over the SAN. Every fired node propagates its own activation value to its neighbors as a function of both its current activation level and the weights of the edges that connect it with its neighbors, and a decay factor D that limits the propagation of the activation value through the network. The activation level of neighbors is updated accordingly. At timetm=2, all clues are fired and the amount of activation levels spreading from them updates the activation level of CU nodes. At timetm=3, only CU nodes whose activation levels exceed F are fired and propagate their activation values to their neighbors, i.e. word nodes. Word nodes are ranked in descending order according to their activation values and the top-k nodes are selected as the most relevant words related to the clues. In the illustrative SAN depicted in Fig. 3, future, ufo, space, war, army, navy, sea, boat, stalingrad are the most relevant keywords related to the clues alien and battle. These new keywords could be exploited to lead the graph-based recommendation algorithm to produce unexpected suggestions, as described in the following section. The spreading activation algorithm is thoroughly described in (Semeraro, de Gemmis, Lops, & Basile, 2012), where it was successfully used as the “brain” of an artificial player for a language game demanding the linguistic and cultural background knowledge typically owned by human beings.Graph-based techniques are becoming popular since they allow to capture transitive associations between nodes (items), thus promoting the discovery of correlations between them (Desrosiers & Karypis, 2011). We adopted a Random Walk model, called Random Walk with Restarts (RWR) (Lovasz, 1996) as a recommendation technique to be enhanced by KI for discovering serendipitous items. We called the resulting algorithm Random Walk with Restarts enhanced by Knowledge Infusion (RWR-KI).Random Walk models exploit a correlation graph between items to predict user preferences. Nodes in the correlation graph correspond to items, while edges indicate the degree of correlation between items. A correlation matrix is built by filling in each entry with the correlation index between item pairs. In (Gori & Pucci, 2007) the correlation index is the number of users who co-rated the item pair, while in (Yildirim & Krishnamoorthy, 2008) the correlation index denotes the content similarity between movies.Given the correlation graph and a starting point, e.g. an item preferred by the user, in the random walk model a neighbor of the starting point is randomly selected for a transition; then, a neighbor of this point is recursively selected at random for a new transition. At each step, there is some probability to return to the starting node. The sequence of randomly selected points is a random walk on the graph.We have enhanced that model in a way that the correlation index between items can actually reflect some hidden associations discovered by the KI process, rather than using a classical similarity score based on a statistical correlation.The algorithm simulates a random walk by moving from an item i to a similar item j in the next step of the walk. The relevance score of an item j with respect to an item i is defined as the steady-state probabilityrijto finally stay at item j, and the correlation matrix is interpreted as a transition probability matrix. Formally, given:•a weighted graphGdenoting the degree of correlation between items;the corresponding column normalized correlation matrixSof the graphG, in which the elementSijrepresents the probability of j being the next state given that the current state is i;a starting node x;the column vectorpτ, wherepiτdenotes the probability that the random walk at stepτis at node i;the starting vector q, having zeros for all elements except the starting node x set to 1;the probabilityαto restart from the initial nodex,0⩽α⩽1;then, Random Walk with Restarts is defined as follows:(1)pτ+1=(1-α)Spτ+αqThe steady-state or stationary probabilities provide the long term visit rate of each node, given a bias toward the particular starting node. This can be obtained by iterating Eq. (1) until convergence, that is, until the difference betweenL2norm of two successive estimates is below a certain threshold, or a maximum number of iterations is reached.Letσbe the state after convergence,piσcan be considered a measure of relatedness between the starting node x and the node i. The final result is a list of items ranked according to the stationary probability of each node after convergence. The complexity of the method isO(mt), where m is the number of edges in the graph and t is the number of iteration steps (Fujiwara, Nakatsuji, Onizuka, & Kitsuregawa, 2012).Given an item I, the idea is to exploit the keywords associated with I by KI to compute the correlation index between I and other items in the collection. We adopt a content-based model in which each item I is represented as a vector in a n-dimensional space of features (Lops et al., 2011):(2)I→=w1,w2,…,wnFeatures are keywords extracted from item descriptions, therefore the feature space is the vocabulary of the item collection, whilewiis the score of featurekiin the item I, which measures the importance of that feature for the item.Given a query q, the ranking function adopted for searching in the item collection is based on the BM25 probabilistic retrieval framework (Robertson & Zaragoza, 2009; Sparck-Jones, Walker, & Robertson, 2000):(3)R(q,I)=∑t∈qf(t,I)·(α1+1)f(t,I)+α1·1-b+b|I|avgdl·idf(t)wheref(t,I)is frequency of the term t in the itemI,α1and b are parameters usually set to 2 and0.75respectively, avgdl is the average item length andidf(t)is the standard inverse document frequency of term t in the item collection.The procedure (Algorithm 1) for building the correlation matrix follows three main steps:1.selection of the most representative features (keywords) for itemIj(step 5);running KI by providing those keywords as clues in order to get new keywords related toIj(step 6);retrieval of items correlated toIjby using new keywords provided by KI as input for the ranking function (steps 7–10). The scores computed by the ranking function are used to fill in the correlation matrix.Algorithm for building the correlation matrix.1:C←{I1,…,IN}▷I1,…,INitems in the collection2:S←NULL▷Initialization of the correlation matrix3:procedure BuildCorrelationMatrix(S,C)▷Fills in the correlation matrix S for items in the collection C. Each elementSjiis the correlation index between itemIjand itemIi4:for allIj∈Cdo5:Featuresj←{k1,…,kn}▷k1,…,knset of features forIjgiven as clues to KI6:NewFeaturesj←KI(Featuresj,m)▷List of m related keywords associated with clues by KI7:q←NewFeaturesj▷Query for retrieving items correlated with new keywords provided by KI8:for allIi∈C∧Ii≠Ijdo▷Fill in rowIjof correlation matrix9:Sji←R(q,Ii)▷Correlation index betweenIjandIicomputed by the ranking function10:end for11:end for12:end procedureFig. 4depicts a fragment of the row of the correlation matrix for the movie Star Trek into Darkness.Starting from the most representative keywords for that movie (alien, battle, starship, captain, mission), KI produces new keywords which are exploited to compute the correlation index with the other movies in the collection. New keywords may be roughly subdivided in two main topics: science-fiction (space, future, ufo) and conflicts/fights (war, army, navy, boat, sea and stalingrad). While science-fiction keywords are quite understandable as clearly related to the movie, conflicts/fights keywords are probably obtained due to less obvious correlation with the input keywords captain and battle.Our hypothesis is that this kind of correlations can lead the recommendation algorithm towards serendipitous suggestions.The main goal of the experimental evaluation is to validate the hypothesis that top-N recommendations produced by the Random Walk with Restarts algorithm enhanced with the KI process are serendipitous. Measuring the degree of serendipity of a recommendation list is a complex task since it involves multiple dimensions upon which items are evaluated (Adomavicius, Manouselis, & Kwon, 2011). Furthermore, it is not only an issue of metrics, but it also depends on the difficulty of detecting and providing an objective assessment of the emotional response – the pleasant surprise – which serendipitous suggestions should convey (McNee et al., 2006). In order to clearly define the evaluation task, we consider serendipitous suggestions those relevant, i.e. close to the user profile, and unexpected at the same time. While computing relevance is a well established issue, the problem of assessing unexpectedness could be approached in different ways. To this purpose, we designed two experiments:1.an in vitro experiment on a benchmark dataset, in which unexpectedness is measured as the deviation from a standard prediction criterion which is more likely to produce expected recommendations, as suggested by Murakami et al. (2008). For example, if the standard prediction criterion is based on a non-personalized recommender algorithm based on popularity, the most popular items will be the most expected recommendations, while the items in the long tail will be the most unexpected ones. The investigation is described in Section 5.1;a study with real users aiming at assessing the actual perception of serendipity of recommendations and their acceptance in terms of both relevance and unexpectedness. The analysis is performed by using Noldus FaceReader™, a tool which allows to gather implicit feedback about users’ reactions to recommendations through the analysis of their facial expressions. The study is described in Section 5.2.The main aim of this experiment is to study the trade-off between relevance and unexpectedness of recommendation lists computed by RWR-KI, in order to understand whether the suggestions satisfy the personal interests of users on the one hand, and encourage the exploration of new areas of potential interests on the other hand. The results are compared to those reported by other state-of-art algorithms described in Section 5.1.4 in order to evaluate to which extent they provide serendipitous suggestions. Furthermore, we analyze the distribution of relevant, unexpected and serendipitous items within lists of different sizes. The aim of the analysis is to assess whether size is a significant factor when the goal is to provide the user with a balanced recommendation set.The evaluation is performed on a subset of the hetrec2011-movielens-2k dataset, made available at the 2nd International Workshop on Information Heterogeneity and Fusion in Recommender Systems, HetRec 2011 (Cantador, Brusilovsky, & Kuflik, 2011), and freely downloadable at grouplens.org/datasets/hetrec-2011. The original dataset contains 855,598 rating assignments on a 10-point Likert scale from0.5to5.0(step0.5), provided by 2113 users on 10,197 movies (sparsity96.03%). Due to the adoption of a content-based approach, we were forced to crawl the plot keywords and the summary of each movie from the IMDb web site. We removed those movies for which content was not available, and obtained a subset of hetrec2011-movielens-2k containing 2642 movies, rated by 2113 users, who provided 593,903 ratings (sparsity:89.4%).As regards the content associated with items, we analyzed the vocabulary of plot keywords and discovered that98%of terms occurred in less than 60 items. The other terms, due to their lower discriminatory power, were removed. The resulting vocabulary contained 36,075 terms.The same analysis on the vocabulary of the summaries led us to conclude that it was mostly made up of common terms, less distinctive than plot keywords, which are not very useful to discover hidden correlation among items. Therefore, only plot keywords were used in the experiments. The average number of keywords per item was13.65, which represents the average number of clues given as input to KI for building the SAN corresponding to a movie (step 5 of Algorithm 1).Relevance in the context of recommendation is a user-specific notion which can be equated to the interest of users for items (Vargas & Castells, 2011) and can be modeled as a binary concept: either an item is liked by a user or not. According to this idea, we define an item i as relevant to user u if the rating given by u on i is greater than the average value of all ratings provided by u. Given a recommendation list L of size N, the following metric defines the relevance of L as the ratio between the size of the subset of L that contains relevant items and the size of L:(4)Relevance@N=∑i∈LR(i)NwhereR(i)=1ifiisrelevant0otherwiseOn the other hand, unexpectedness can be defined independently of the user, based on some standard prediction criteria (Murakami et al., 2008). We adopt two criteria: popularity and item average rating.Popularity of the item i is defined as the ratio between the number of users who rated i and the total number of users in the dataset. According to this criterion, the item i is unexpected if its popularity score is below the average popularity computed across all the items in the dataset. This means that the average value of popularity allows to split items in the dataset in two parts: the short head, containing the most popular (and expected) items, and the long tail, containing the less popular (and unexpected) items. This criterion does not take into account whether ratings assigned to the items are positive or negative. This means that an item may be popular even though it is disliked by most of the users.The other criterion takes into account the ratings assigned to each item. The item average rating of the item i is the average rating provided by the users in the dataset on item i (the value is normalized using the maximum of those values in the dataset).2each item in the dataset has at least 50 ratings.2According to this criterion, the short head includes the items whose average rating is above the average rating computed across all the items in the dataset, while the long tail contains items below the average, i.e. those less liked by the users, and more likely unexpected.By adopting popularity,69%of items in the hetrec2011-movielens-2k dataset are unexpected, while by using average rating the percentage of unexpected items decreases to44%. This means that recommending unexpected items may be more difficult when the criterion based on ratings is adopted.TheUnexpectedness@Nmetric defines the unexpectedness of L as the ratio between the size of the subset of L that contains just unexpected items and the size of L (Eq. (5)):(5)Unexpectedness@N=∑i∈LU(i)Nwhere:U(i)=1ifiisunexpected;0otherwise.Serendipity@Ndefines the serendipity of L as the ratio between the size of the subset of L that contains serendipitous items, i.e. those relevant and unexpected at the same time, and the size of L (Eq. (6)):(6)Serendipity@N=∑i∈LS(i)Nwhere:S(i)=1ifiisserendipitous;0otherwise.Experiments were carried out using a per user evaluation, scheduled as follows:1.Correlation matrix S is built using Algorithm 1.Ratings of the active userua(for which recommendations must be provided) are split into a training set Tr and a test set Ts.Tr is used to set the starting vector q of the RWR algorithm described in Section 4.1. As proposed in (Cantador, Konstas, & Jose, 2011), the RWR algorithm can be generalized by setting more than one single starting node. Thus, we set the value of nodes corresponding to all relevant items foruato 1, i.e. those whose ratings are greater than the average rating value ofua. Next, we normalize q so thatq=1.Random Walk on S is performed, which returns the stationary probability vector corresponding touaof all the items in the dataset. The probabilityαto return to the initial node is set to0.8, as suggested in (Konstas, Stathopoulos, & Jose, 2009), in order to reduce random walks in the neighboring elements ofua.From this vector, all items in Ts (i.e. those for which the ground truth is known) are selected and ranked in descending order, with the top-ranked items having the highest probability scores that correspond to the most preferred ones.Performance measures are computed on top-N items and averaged for all users.The dataset partitioning technique was 5-fold cross validation (Kohavi & Li, 1995). The dataset is divided into 5 disjoint partitions, and at each step 4 partitions were used to set q, whereas the remaining partition was used as the test set. These steps were repeated until each one of the 5 disjoint partitions was used as Ts. Results were averaged over the 5 runs.We compared RWR-KI to the following algorithms:•RWR based on a correlation matrix built using similarity between plot keywords (RWR-KWD): this algorithm exploits only the content associated with items (endogenous knowledge) to build the correlation matrix. The comparison with RWR-KWD gives us the possibility to evaluate whether the exogenous knowledge introduced by KI allows the discovery of non-obvious correlations between items, which cannot be caught by exploiting endogenous knowledge exclusively. Top-N recommendations are computed as for RWR-KI;Item to Item Collaborative Filtering (I2ICF): even though collaborative filtering algorithms do not explicitly support the notion of unexpectedness, they constitute a fairly reasonable baseline because they perform reasonably well in terms of other performance measures besides classical accuracy measures (Adamopoulos & Tuzhilin, 2011; Burke, 2002). Top-N recommendations are computed by performing rating predictions on Ts, using the algorithm described in (Sarwar, Karypis, Konstan, & Riedl, 2001). Adjusted Cosine measure is adopted to assess similarity between items, while the neighborhood size has been set to 20, according to experiments in (Herlocker, Konstan, & Riedl, 2002; Sarwar et al., 2001);Random: this simple baseline randomly suggests N items from Ts. The rationale for including this algorithm in the evaluation is to compare our approach to a strategy inspired by the blind luck principle (Section 2).Fig. 5shows the relevance-unexpectedness tradeoff for recommendation lists of different sizes. For readability, the graph reports the metrics defined in Section 5.1.2 as percentages. For each algorithm we report the results of four runs, each corresponding to a different sizeN=5,10,15,20of the recommendation list.All algorithms are biased toward relevance, but RWR-KI and RANDOM seem better balanced than RWR-KWD and I2ICF, regardless of the criteria adopted for the definition of unexpectedness. In particular, RWR-KI significantly dominates all the algorithms (p<0.001using the Wilcoxon test) as regards unexpectedness. The fact that RWR-KI outperforms RWR-KWD is certainly due to knowledge infusion since both the approaches use the same recommendation algorithm, thus confirming our intuition that exogenous knowledge might help to produce non-obvious suggestions. The worst performance of I2ICF in terms of unexpectedness confirms its bias towards mostly liked items (Fleder & Hosanagar, 2009).These results suggest that in a multi-objective decision-making problem, RWR-KI would be the best approach, if unexpectedness is given higher weight than relevance, while I2ICF and RWR-KWD are more appropriate for recommendation scenarios where accuracy is more important that unexpectedness. In general, all the observed trends do not depend on the size of the recommendation lists, in the sense that the results of each algorithm forN=5,10,15,20are very similar.Results of the evaluation of serendipity, presented in Fig. 6, show that RWR-KI achieves the best performance compared to the other approaches (p<0.001), regardless of the criterion for defining unexpectedness. In general, algorithms with higher unexpectedness (RWR-KI and RANDOM) reach higher serendipity than those more biased towards relevance. It seems that there is more chance that the algorithms designed for finding unexpected items suggest relevant items as well, than algorithms designed for finding relevant items suggest unexpected items as well. As for the previous analysis, the size of the recommendation list does not affect the performance of the algorithms: the percentage of serendipitous items in lists of different sizes is quite uniform for all the tested algorithms.To sum up, the main outcomes of the experiment are:•RWR-KI dominates all the tested algorithms in the task of finding serendipitous recommendations, showing better balancing of relevance and unexpectedness;the size of the recommendation list is not a significant factor for providing users with a higher ratio of serendipitous items, meaning that event short lists (size 5 or 10) contain serendipitous items.Although this comparative evaluation allowed us to assess the ability of suggesting serendipitous items, a deeper analysis of the recommendation lists is required to decide whether they can be provided to users as final recommendation sets. For example, a list containing serendipitous items, as well as a high percentage of not relevant ones, is not suitable as a recommendation set. Therefore, in the next section we present a study of the distribution of relevant, unexpected and serendipitous items within the recommendation lists.The analysis focuses on recommendations lists of size 5 or 10, which are mostly used as final recommendation sets shown to the user. We define a list as serendipitous if at least 20% of its items are serendipitous. Fig. 7presents the number of serendipitous lists having size=5 provided by each tested algorithm, together with the corresponding distribution of serendipitous items.Regardless of the criteria for unexpectedness, RWR-KI produces the highest number of serendipitous lists. By looking at the popularity criterion, 48% of users (1025 out of 2113) received at least one serendipitous recommendation, while this percentage decreases to 36%, when average rating is adopted. It is worth to note the good performance of RANDOM, which provided 44% of users with serendipitous lists (32% when using average rating). Once again results confirm that I2ICF is biased towards most popular or most liked items: only 28% of the lists are serendipitous (5% with average rating). As for the distribution of serendipitous items in the lists, most of the top-5 recommendations produced by all the algorithms contains only 1 serendipitous item, but RWR-KI is the algorithm that produces the highest number of lists having more than 1 serendipitous suggestion.Figs. 8 and 9show the composition of serendipitous lists in terms of percentage of relevant or unexpected items.Results for both unexpectedness criteria confirm the primacy of RWR-KI and RANDOM. Over40%of the lists produced by RWR-KI have at least 3 unexpected items, while over60%contain at least 3 relevant items. The RANDOM approach has similar performance. The same analysis performed on top-10 recommendations revealed similar trends (results are not reported for brevity).Finally, we present some results about not serendipitous lists. Indeed, those recommendation lists may be worthless if they do not contain any items the users may like, and there is the risk that the advantage of surprising some users is obtained at the price of disappointing most of them.Fig. 10shows the box plot of the number of relevant and unexpected items in lists produced by each algorithm, using popularity as unexpectedness criterion. Upper and lower ends of boxes represent the3rdand1stquartile, respectively. Whiskers extend to the most extreme data point which is no more than1.5times the interquartile range. Median is depicted with a solid line. Empty circles are outliers.The main outcome is that for all the algorithms the median of the number of relevant items is greater than or equal to 2, which is acceptable for list of size 5. The number of unexpected items is low for all the algorithms, with a slightly better performance by RWR-KI. Similar results (not reported for brevity) were observed for the other unexpectedness criterion.Even if the results clearly show that RWR-KI overcomes the other algorithms, a surprisingly good performance is observed for the random strategy. This raises the following issues: is our strategy actually different from making random suggestions? Does the difference in performance justify the difference in complexity of the strategies? As a consequence, we conducted a study with real users, in which these two strategies are compared, aiming at assessing the actual perception of serendipity of recommendations and their acceptance by users.The aim of the study is twofold:•to assess the acceptance of recommendations produced by RWR-KI and RANDOM, the algorithms that excelled on the in vitro evaluation. This is achieved by gathering explicit feedback from users through a questionnaire. Results are presented in Section 5.2.3;to measure the perception of serendipity of recommendations. This is achieved by gathering implicit feedback from users through a tool able to detect their emotions when exposed to recommendations. Results are presented in Section 5.2.4.The experimental units were 40 master students in engineering, architecture, economy, computer science and humanities; 26 male (65%) and 14 female (35%), with an age distribution ranging from 20 to 35. None of them had been previously exposed to the system used in our study.We collected from IMDb.com some details (poster, keywords, cast, director, etc.) of 2135 movies released between 2006 and 2011. The size of the vocabulary of plot keywords was 32,583 and the average number of keywords per item was12.33, which is comparable to that of the in vitro experiment.

@&#INTRODUCTION@&#


@&#CONCLUSIONS@&#
