@&#MAIN-TITLE@&#
Forecasting with a noncausal VAR model

@&#HIGHLIGHTS@&#
Forecasting methods for a noncausal VAR model are proposed.Due to the nonlinearity of the prediction problem, simulation methods are required.Monte Carlo simulations are used to illustrate the methods.An empirical application to inflation forecasting is presented.

@&#KEYPHRASES@&#
Noncausal vector autoregression,Forecasting,Simulation,Importance sampling,Inflation,

@&#ABSTRACT@&#
Simulation-based forecasting methods for a non-Gaussian noncausal vector autoregressive (VAR) model are proposed. In noncausal autoregressions the assumption of non-Gaussianity is needed for reasons of identifiability. Unlike in conventional causal autoregressions the prediction problem in noncausal autoregressions is generally nonlinear, implying that its analytical solution is unfeasible and, therefore, simulation or numerical methods are required in computing forecasts. It turns out that different special cases of the model call for different simulation procedures. Monte Carlo simulations demonstrate that gains in forecasting accuracy are achieved by using the correct noncausal VAR model instead of its conventional causal counterpart. In an empirical application, a noncausal VAR model comprised of U.S. inflation and marginal cost turns out superior to the best-fitting conventional causal VAR model in forecasting inflation.

@&#INTRODUCTION@&#
The conventional vector autoregressive (VAR) model has become a standard tool in various fields of applications. In economics and finance the VAR model is typically used in structural analysis to study the dynamics and interrelationships between variables of interest. Another application of the VAR model is forecasting. For instance, economic decision makers, such as central banks and investors in financial markets, aim to forecast key macroeconomic and financial time series to assess the future state of the economy and investment opportunities.The conventional causal VAR model has a moving average representation in terms of its present and past error terms. A characteristic feature of this model is that its error terms are not predictable by past values of the involved time series. In contrast, the moving average representation of the non-Gaussian noncausal VAR model recently considered by Davis and Song (2010) and Lanne and Saikkonen (2013) also involve future error terms that are predictable by past values of the considered time series. In addition to theoretical advancements these authors demonstrate the practical usefulness of the noncausal VAR model in economic and financial applications. As discussed by Lanne and Saikkonen (2013), an important economic application of the noncausal VAR model is checking the validity of widely used test procedures based on the causal VAR model in testing economic hypotheses, especially in models involving expectations.As yet, the development of the noncausal VAR model is at its early stages and even the literature of univariate noncausal autoregressive models is scant (see Breidt et al., 1991, Rosenblatt, 2000, Davis and Song, 2010, Lanne and Saikkonen, 2011, 2013 and the references therein). As demonstrated in this previous literature, noncausal autoregressions can be distinguished from their causal counterparts only when the data generation process is non-Gaussian. In noncausal autoregressions non-Gaussianity can therefore be seen as a necessary identification condition. The object of this paper is to devise forecasting techniques for the non-Gaussian noncausal VAR model of Lanne and Saikkonen (2013). In addition to computing forecasts these techniques are also needed in computing impulse response functions, and hence in conducting structural analysis within the noncausal VAR model. Thus, our contribution should widen the applicability of the noncausal VAR model in empirical research.In the causal VAR model, forecasting is simple in that explicit formulas are available. In the noncausal VAR model the situation is different because the prediction problem is, in general, nonlinear and, consequently, forecasts cannot be obtained without resorting to numerical methods. Further discussion on this point is provided by Lanne et al. (2012b) who develop a simulation-based forecasting method for the univariate noncausal AR model proposed by Lanne and Saikkonen (2011). It turns out that forecasts of the noncausal VAR model considered in this paper can be computed analogously only when a suitable condition on the structure of the model holds. One case where the required condition always holds is the purely noncausal VAR model whose moving average representation only involves present and future error terms. In general, the required condition states that a certain parameter matrix involving the autoregressive coefficients of the model is nonsingular. Due to estimation errors this nonsingularity always holds in practice but, to avoid potential problems with nearly singular cases, we develop a forecasting technique which does not depend on the structure of the model. To achieve this robustness, more demanding computations based on importance sampling are needed. A somewhat similar technique has recently been used by Breidt and Hsu (2005) in forecasting non-Gaussian and potentially noninvertible (univariate) moving average processes (for a general discussion of importance sampling, see, e.g., Geweke, 1996).We examine the properties of our forecasting techniques by means of Monte Carlo simulations which also provide guidance for some user-chosen quantities needed in the application of these techniques. The simulations conducted demonstrate that our forecasting techniques perform well and that the correct noncausal VAR model outperforms its causal counterpart in forecast accuracy.Although empirical experience of noncausal VAR models is still very limited, the findings of Lanne et al. (2012c) based on applying univariate autoregressions to a large economic data set suggest that noncausality is quite prevalent among economic time series (see also Lof, 2013). The related work of Lanne et al. (2012a) and Lanne et al. (2012b) complement these findings by demonstrating that the univariate noncausal AR model outperforms its conventional causal counterpart in forecasting US inflation. Our empirical application to inflation forecasting is partly motivated by the work of these previous authors. We consider a bivariate system consisting of inflation and the real marginal cost that has often been employed in monetary economics, especially in studies related to the New Keynesian Phillips Curve (see, e.g., Gali and Gertler, 1999, Nason and Smith, 2008, and the references therein). Our results are similar to those obtained by Lanne et al. (2012a,b). We find that a noncausal VAR model provides the best in-sample fit and outperforms the best-fitting causal VAR model in out-of-sample forecasting.The rest of the paper is structured as follows. Section  2 describes the noncausal VAR model of Lanne and Saikkonen (2013) and briefly discusses statistical inference. Section  3 develops the forecasting techniques of the paper, while Section  4 illustrates their performance by Monte Carlo simulations. Section  5 presents the empirical application. Section  6 concludes. Finally, some technical details are collected in four appendices.In this section, we first describe the noncausal VAR model of Lanne and Saikkonen (2013) and then discuss briefly parameter estimation and statistical inference. Unless otherwise indicated, all vectors will be treated as column vectors and, for notational convenience, we shall writex=(x1,…,xn)for the (column) vectorxwhere the componentsximay be either scalars or vectors (or both).Following Lanne and Saikkonen (2013) we consider then-dimensional stochastic processyt(t=0,±1,±2,…)generated by(1)Π(B)Φ(B−1)yt=ϵt,whereϵt(n×1)is a sequence of independent, identically distributed random vectors with zero mean and finite positive definite covariance matrix, andΠ(B)=In−Π1B−⋯−ΠrBrandΦ(B−1)=In−Φ1B−1−⋯−ΦsB−saren×nmatrix operators withBthe usual backward shift operator, that is,Bkyt=yt−k(k=0,±1,…). Moreover, the determinants of the matrix polynomialsΠ(z)andΦ(z)(z∈C)have their zeros outside the unit disc, so that(2)detΠ(z)≠0,|z|≤1,anddetΦ(z)≠0,|z|≤1.These conditions guarantee the validity of various moving average representations to be used in our subsequent developments.IfΦj≠0for somej∈{1,…,s}, Eq. (1) defines a noncausal vector autoregression referred to as purely noncausal whenΠ1=⋯=Πr=0(orr=0). WhenΦ1=⋯=Φs=0(ors=0) the conventional causal model is obtained. Then the former condition in (2) guarantees the stationarity of the model. In the general set-up of model (1) the same is true for the process(3)ut=Φ(B−1)yt.Specifically, there exists aδ1>0such thatΠ(z)−1has a well defined power series representationΠ(z)−1=∑j=0∞Mjzj=M(z)for|z|<1+δ1. Consequently, the processuthas the causal moving average representation(4)ut=M(B)ϵt=∑j=0∞Mjϵt−j,whereM0=Inand the coefficient matricesMjdecay to zero at a geometric rate asj→∞.WriteΠ(z)−1=det(Π(z))−1Ξ(z)=M(z), whereΞ(z)is the adjoint polynomial matrix ofΠ(z). Then,det(Π(B))ut=Ξ(B)ϵtand, by the definition ofutin (3),Φ(B−1)wt=Ξ(B)ϵt,where, settingdet(Π(z))=a(z)=1−a1z−⋯−anrznr,(5)wt=det(Π(B))yt=a(B)yt.Note thatΞ(z)is a matrix polynomial of degree at most(n−1)rand, becauseΠ(0)=In, we also haveΞ(0)=In. By the latter condition in (2) one can find a0<δ2<1such thatΦ(z−1)−1Ξ(z)has a well defined power series representation(6)Φ(z−1)−1Ξ(z)=∑j=−(n−1)r∞Njz−j=N(z−1)for|z|>1−δ2.Thus, the processwthas the moving average representation(7)wt=∑j=−(n−1)r∞Njϵt+j,where the coefficient matricesNjdecay to zero at a geometric rate asj→∞. Using the equalities in (6) one can solve these matrices recursively as functions of the parametersΠj(j=1,…,r)andΦj(j=1,…,s)(see Appendix A.1). Finally, from (2) one obtains the moving average representation(8)yt=∑j=−∞∞Ψjϵt−j,whereΨj(n×n)is the coefficient matrix ofzjin the Laurent series expansion ofΨ(z)=defΦ(z−1)−1Π(z)−1which exists for1−δ2<|z|<1+δ1withΨjdecaying to zero at a geometric rate as|j|→∞. The representation (8) implies thatytis a stationary and ergodic process with finite second moments.Model (1) is referred to as the VAR(r,s) model. In the conventional causal case the abbreviation VAR(r) is also used. In the next section, we present the joint distribution of an observed time series generated by the VAR(r,s) process. This joint distribution is needed in the development of our forecasting methods and it also facilitates our discussion on parameter estimation and statistical inference.As discussed in the Introduction, causal and noncausal autoregressions cannot be distinguished by second-order properties or the Gaussian likelihood. Therefore, it is necessary to assume that the error termϵtis non-Gaussian. The theoretical results of Lanne and Saikkonen (2013) assume that the distribution ofϵtis of a fairly general elliptical form. However, an inspection of the arguments used in Section  3.1 of that paper reveals that this assumption is not needed to derive the distribution of the observed data and, therefore, it is not necessary for our forecasting methods. Thus, unless otherwise indicated we only assume that the (non-Gaussian) distribution ofϵtis continuous with density functionf(⋅), whose possible dependence on (unknown) parameters is not made explicit.A detailed derivation of the joint distribution of the observed data can be found in Lanne and Saikkonen (2013), so here we only describe the final result. To this end, define then×1vectors(9)vk,T−s+k=wT−s+k−∑j=−(n−1)r−kNjϵT−s+k+j,k=1,…,s,where the sum is interpreted as zero whenk>(n−1)r, that is, when the lower bound exceeds the upper bound (this convention will also be used elsewhere). Note also that, by (1) and (5),vk,T−s+kcan be expressed as a function of the observed datay1,…,yTand that, by (7), the representationvk,T−s+k=∑j=−k+1∞NjϵT−s+k+jholds, showing thatvk,T−s+k,k=1,…,s, are independent ofϵt,t≤T−s. We also introduce the vectorz=(z1,z2,z3)wherez1=(u1,…,ur),z2=(ϵr+1,…,ϵT−s), andz3=(v1,T−s+1,…,vs,T)are independent in view of the preceding discussion and (4). These vectors can be expressed as functions of the observed data (and parameters), and in what follows we use a tilde to make this functional dependence explicit. Thus, the components of the vectorsz̃1andz̃2areũt=Φ(B−1)yt,t=1,…,r, (see (3)) andϵ̃t=Π(B)Φ(B−1)yt,t=r+1,…,T−s, (see (1)), respectively, whereas the components ofz̃3,ṽk,T−s+k, are defined by replacingwT−s+kandϵT−s+k+jon the right hand side of (9) bya(B)yT−s+k(see (5)) andϵ̃T−s+k+j,j=−(n−1)r,…,−k,k=1,…,s, respectively.It is shown in Section 3.1 of Lanne and Saikkonen (2013) that the random vectorzis related to the data vectory=(y1,…,yT)according toz=H3H2H1y, whereH1,H2, andH3(T×T) are nonsingular transformation matrices that depend on the parametersΠj(j=1,…,r)andΦj(j=1,…,s)withH2andH3having unit determinant. Thus, it follows that the joint density function of the data vectoryis given by (assumingTlarge enough)(10)p(y)=hz1(z̃1)⋅∏t=r+1T−sf(ϵ̃t)⋅hz3(z̃3)⋅|det(H1)|,wherehz1(⋅)andhz3(⋅)signify the density functions of the random vectorsz1andz3, respectively. For our subsequent developments the explicit expression of the matrixH1is not relevant because the determinant term|det(H1)|will vanish from our forecasting formulas. In the purely noncausal case the joint density functionp(y)can be simplified by replacing the first factorhz1(z̃1)by unity, settingr=0andϵ̃t=Φ(B−1)ytin the second factor, andz̃3=(yT−s+1,…,yT)in the third factor.We shall now briefly discuss parameter estimation and statistical inference in the VAR(r,s) model (1). Following Lanne and Saikkonen (2013) we here assume that the error termϵthas an elliptical distribution and use the second factor of the right hand side of (10) to obtain a computationally feasible approximation for the likelihood function. Maximizing this function over the permissible parameter space yields an (approximate) maximum likelihood (ML) estimator. Lanne and Saikkonen (2013) show that, under appropriate regularity conditions, the resulting (local) ML estimator is consistent and asymptotically normally distributed and that conventional methods to compute standard errors for estimators and to construct likelihood-based tests apply.The preceding discussion assumes that the ordersrandsof the VAR(r,s) model (1) are known. As in Lanne and Saikkonen (2013) we specify these orders as follows. First, using least squares or Gaussian ML we find a causal VAR(p) model that adequately describes the autocorrelation structure of the data with the orderpdetermined by using conventional procedures such as model selection criteria and diagnostic checks. Then we check the residuals of this causal VAR(p) model for Gaussianity and, only when we detect deviations from Gaussianity, we consider noncausal VAR models. Next we choose a non-Gaussian error distribution, such as the multivariatet-distribution used in Lanne and Saikkonen (2013), and estimate all causal and noncausal VAR(r,s) models with the ordersrandssumming to the selected orderp. Finally, of these alternative models we choose the one that maximizes the likelihood function and evaluate its adequacy with conventional diagnostic tools.In this section, we consider forecasting future observationsyT+h(h≥1)and, unless otherwise stated, we shall assume that the model is not causal and not univariate, so thats>0andn>1. We letET(⋅)signify the conditional expectation operator given the observed datay=(y1,…,yT).Our starting point is Eq. (7) which we make operational by approximating the infinite sum therein by a finite sum. Specifically, from Eqs. (5) and (7) we obtain the approximation(11)ET(yT+h)≈a1ET(yT+h−1)+⋯+anrET(yT+h−nr)+ET(∑j=−(n−1)rM−hNjϵT+h+j),whereM>0is supposed to be “large”. AsET(yT+h−j)=yT+h−jforj≥h, (approximate) forecasts can be computed recursively starting fromh=1if the last conditional expectation on the right hand side of (11) can be computed for everyh≥1. In the univariate case (n=1) considered by Lanne et al. (2012b) this conditional expectation depends on the error termsϵT+1,…,ϵT+Monly. However, except for the purely noncausal case (r=0) this does not happen in our multivariate case, where the error termsϵT+1−(n−1)r,…,ϵTare also involved and the fact thatϵT−s+1,…,ϵT(s>0) cannot be expressed as functions of the observed data (see (1)) causes complications. In the purely noncausal case these error terms vanish from the right hand side of (11), simplifying the situation and allowing a straightforward extension of the forecasting method of Lanne et al. (2012b). Therefore, and also to help understand the difficulties in the general case (r>0,s>0), we shall first consider forecasting in the purely noncausal case. The general case requires a more delicate treatment provided in Section  3.2.In the purely noncausal case (r=0) the approximation (11) reduces to(12)ET(yT+h)≈ET(∑j=0M−hNjϵT+h+j),N0=In.To compute the conditional expectation on the right hand side we follow Lanne et al. (2012b) and derive the conditional density ofϵ+=(ϵT+1,…,ϵT+M)given the data vectory. Recall that nowϵ̃t=Φ(B−1)ytandz̃3=(yT−s+1,…,yT). Using the expression of the density functionp(y)in (10) and the preceding discussion one can check that the joint density function of(y,ϵ+)can be written as(13)p(y,ϵ+)=∏t=1T−sf(ϵ̃t)⋅hz3,ϵ+(y3,ϵ+)⋅|det(H1)|,wherehz3,ϵ+(y3,ϵ+)is the joint density function of(z3,ϵ+)andy3=(yT−s+1,…,yT)(in this section we replacez̃3by the more typical notationy3). From (10) (specialized to the present case) and (13) we find that the conditional density function ofϵ+givenyisp(ϵ+|y)=hz3,ϵ+(y3,ϵ+)hz3(y3)=hz3,ϵ+(y3,ϵ+)∫hz3,ϵ+(y3,ϵ+)dϵ+.The right hand side of (12) can thus be written as(14)ET(∑j=0M−hNjϵT+h+j)=∫∑j=0M−hNjϵT+h+j⋅hz3,ϵ+(y3,ϵ+)dϵ+∫hz3,ϵ+(y3,ϵ+)dϵ+.As in Lanne et al. (2012b), we now derive a feasible approximation for the density functionhz3,ϵ+(y3,ϵ+). Asyt=∑j=0∞Njϵt+jandN0=In, we have the approximate relation[InN1⋯⋯⋯⋯NM+s−10⋱⋱⋮⋮⋱InN1⋯⋯NM⋮⋱In00⋮⋱⋱⋱⋮⋮⋱⋱00⋯⋯⋯⋯0In][ϵT−s+1⋮ϵTϵT+1⋮ϵT+M]≈[yT−s+1⋮yTϵT+1⋮ϵT+M],or brieflyBϵ++≈υ. As the matrixBis nonsingular with unit determinant this yieldsϵ++≈B−1υor(ϵT−s+1,…,ϵT,ϵT+1,…,ϵT+M)≈(ϵ̃T−s+1(ϵ+),…,ϵ̃T(ϵ+),ϵT+1,…,ϵT+M),whereϵ̃T−s+1(ϵ+),…,ϵ̃T(ϵ+)(n×1)are the firsts(vector) components of the vectorB−1υ, and hence dependent onyT−s+1,…,yT. Thus, it follows that the density functionhz3,ϵ+(y3,ϵ+)can be approximated as(15)hz3,ϵ+(y3,ϵ+)≈∏j=1sf(ϵ̃T−s+j(ϵ+))⋅∏t=T+1T+Mf(ϵt).As in Lanne et al. (2012b), we can use this approximation to compute approximations for the two integrals on the right hand side of (14). More generally, for any function ofϵ+, sayq(ϵ+), we can use (15) to obtainET(q(ϵ+))≈∫q(ϵ+)⋅∏j=1sf(ϵ̃T−s+j(ϵ+))⋅∏t=T+1T+Mf(ϵt)dϵ+∫∏j=1sf(ϵ̃T−s+j(ϵ+))⋅∏t=T+1T+Mf(ϵt)dϵ+.(Here as well as in similar subsequent instances existence and finiteness of the stated expectations are assumed.) The numerator on the right hand side can be interpreted as the expectation of the product of the first two factors in the integrand with respect to the distribution ofϵ+=(ϵT+1,…,ϵT+M), whereas the denominator can be interpreted as the expectation of∏j=1sf(ϵ̃T−s+j(ϵ+))with respect to the same distribution. Using Monte Carlo simulation, we can therefore approximateET(q(ϵ+))by(16)EˆT(q(ϵ+))=1m∑i=1mq(ϵ+(i))⋅∏j=1sf(ϵ̃T−s+j(ϵ+(i)))1m∑i=1m∏j=1sf(ϵ̃T−s+j(ϵ+(i))),whereϵ+(i)=(ϵT+1(i),…,ϵT+M(i)),i=1,…,m, are mutually independent simulated realizations from the distribution ofϵ+so thatϵT+1(i),…,ϵT+M(i)are independent random vectors for everyi. Asm→∞, the right hand side of (16)  converges almost surely and provides an approximation forET(q(ϵ+))that can be made arbitrarily accurate by choosingmandMlarge enough.To obtain forecasts foryT+h(h≥1)one needs to compute values of the right hand side of (16) withq(ϵ+)=∑j=0M−hNjϵT+h+j(see (14)). Specifically, we have the following forecasting procedure.Step  1. Generateϵ+(i)=(ϵT+1(i),…,ϵT+M(i)),i=1,…,m, as described below (16).Step  2. Compute the forecastsEˆT(yT+h),h=1,2,…, by choosingq(ϵ+)=∑j=0M−hNjϵT+h+jin (16).FormandMlarge enough, the resulting forecasts approximate the true forecastET(yT+h)arbitrarily closely. Appendix A.1 shows how to compute the coefficient matricesNjrecursively as functions of the parametersΠj(j=1,…,r)andΦj(j=1,…,s). Choosing the values of the integersmandMwill be discussed in Section  4.Proceeding as in Lanne et al. (2012b) we can also obtain interval forecasts and forecasts for the conditional distribution of the components ofyT+h(h≥1). Let1(⋅)stand for the indicator function andιa=(0,…,0,1,0,…,0)(n×1)theath unit vector. Then a forecast for the conditional cumulative distribution function ofya,T+h=ιa′yT+h, theath component ofyT+h, at pointx∈Ris obtained as (see (5) and (7))ET(1(ya,T+h≤x))≈ET(1(∑j=0M−hιa′NjϵT+h+j≤x)),where the right hand side can be approximated by using (16) withq(ϵ+)=1(∑j=0M−hιa′NjϵT+h+j≤x). Thus, choosing a gridx1,…,xKwith a large enough value ofK, one can obtain a forecast of the whole conditional cumulative distribution function ofya,T+h, and using appropriate quantiles from the lower and upper tails of this forecast an interval forecast forya,T+hcan be constructed for anyh≥1.As already indicated, the general noncausal case seems to require techniques more burdensome than those in the purely noncausal case (or in the general univariate noncausal case). To demonstrate this, consider the joint density of the augmented data vector(y,ϵ+)and conclude from the discussion leading to the density functionp(y)in (10) that the joint density of(y,ϵ+), and hence the conditional density ofϵ+giveny, involves the joint density of(z3,ϵ+). For simplicity, suppose thats=1so thatz3=v1,T=∑j=0∞NjϵT+jandz3≈∑j=0MNjϵT+jforMlarge (see (9) and the subsequent discussion). In the purely noncausal case we haveN0=In, but this does not hold in the general case and it is even possible that the matrixN0is singular. This happens, for example, whenr=s=1andΠ1=[00−3/43/4]andΦ1=[2/32/300].When the matrixN0is singular the random vectorsz3andϵ+=(ϵT+1,…,ϵT+M)are approximately linearly dependent so that, apart from the approximation error, the joint distribution ofz3andϵ+is singular. This makes the conventional use of the joint density ofz3andϵ+, employed in the purely noncausal case, inappropriate.To overcome the difficulty described above we first develop a procedure that is generally applicable but requires the use of importance sampling not needed in the purely noncausal case considered in the preceding section. In Section  3.2.2, we show how a simpler technique, similar to that derived in the purely noncausal case, can be obtained when a suitable condition about the structure of the model holds. Whens=1this condition requires that the matrixN0is nonsingular.For our subsequent discussion it appears convenient to write the approximate forecasting formula (11)  as(17)ET(yT+h)≈a1ET(yT+h−1)+⋯+anrET(yT+h−nr)+∑j=−(n−1)r−s−hNjϵ̃T+h+j+ET(∑j=−s−h+1−s−h+snNjϵT+h+j)+ET(∑j=−s−h+sn+1M−hNjϵT+h+j),where we have divided the sum involving the error terms into three components of which the first one depends on the data and the second one contains the error terms that will be treated in a special manner.Our subsequent developments make use of thesn×nmatricesNj=[Nj⋮Nj−s+1],j=0,1,….It is demonstrated in Appendix A.1 that the matrix[N0⋯Nsn−1](sn×sn2)is of full row rank, implying that we can find a matrix[K0⋯Ksn−1](sn(n−1)×sn2)such that the matrix(18)Q=[N0⋯Nsn−1K0⋯Ksn−1],sn2×sn2,is nonsingular. One possibility that always applies is to choose the rows of[K0⋯Ksn−1]as basis vectors of the orthogonal complement of the space spanned by the rows of[N0⋯Nsn−1]. A simpler choice that applies when the matrix[N0⋯Ns−1](sn×sn) is nonsingular will be discussed in the next section. Using the matrixQintroduced in (18) we define the vector(19)[ζ1ζ2]=[N0⋯Nsn−1K0⋯Ksn−1][ϵT−s+1⋮ϵT−s+sn],sn2×1,whereζ1issn×1,ζ2issn(n−1)×1, and the error terms on the right hand side are the ones in the penultimate term on the right hand side of (17). Furthermore, asz3=(v1,T−s+1,…,vs,T)withvk,T−s+k=∑j=−k+1∞NjϵT−s+k+j(see the discussion following (9)) the definition ofNjshows thatz3=∑j=0∞NjϵT−s+1+j. Hence, we haveζ1=z3−∑j=sn∞NjϵT−s+j+1, which will be used below.Now, use Eqs. (18) and (19) to write the sum in the penultimate term on the right hand side of (17) as∑j=−s−h+1−s−h+snNjϵT+h+j=Ph[ζ1ζ2],wherePh=[N−s−h+1⋯N−s−h+sn]Q−1. Thus, using the approximationζ1≈z3−∑j=snM+s−1NjϵT−s+j+1we obtain(20)ET(∑j=−s−h+1−s−h+snNjϵT+h+j)≈Ph[ET(ζ̃1(e+))ET(ζ2)],whereζ̃1(e+)=z̃3−∑j=snM+s−1NjϵT−s+j+1withe+=(ϵT−s+sn+1,…,ϵT+M)andζ2=∑j=0sn−1KjϵT−s+j+1(see (19)). From this and the approximation (17) it follows that to obtain forecasts foryT+h(h≥1) we should be able to obtain forecasts for (functions of)e+andζ2. To this end, we consider the extended data vector(y,ζ2,e+)and derive the conditional density of(ζ2,e+)giveny.It is shown in Appendix A.2 that, forMlarge, the conditional density of(ζ2,e+)givenycan be approximated by using the joint density of the independent random vectors(ζ1,ζ2)ande+or, specifically,(21)p((ζ2,e+)|y)≈hζ1,ζ2(ζ̃1(e+),ζ2)⋅∏t=T−s+sn+1T+Mf(ϵt)∬hζ1,ζ2(ζ̃1(e+),ζ2)⋅∏t=T−s+sn+1T+Mf(ϵt)dζ2de+.Here the notation is as follows. First,ζ̃1(e+)is as in (20) with the (n×1vector) componentsζ̃1,k(e+)=ṽk,T−s+k−∑j=snM+s−1Nj−k+1ϵT−s+j+1(k=1,…,s). Second,hζ1,ζ2(ζ1,ζ2)is the joint density function ofζ1andζ2, and defined as(22)hζ1,ζ2(ζ1,ζ2)=∏j=1snf(∑k=1sRj,kζ1,k+∑k=s+1snRj,kζ2,k)⋅|det(R)|,whereζ1,kandζ2,ksignify thekth (n×1vector) components ofζ1andζ2, andR=[Rj,k]=Q−1(j,k=1,…,n)with the partitionsRj,kof ordern×n.Now, as discussed below (20), to obtain forecasts foryT+h(h≥1) we should be able to compute (an approximation for) the conditional expectationET(q(ζ2,e+))withq(ζ2,e+)a function of(ζ2,e+). From (21) we find that(23)ET(q(ζ2,e+))≈∬q(ζ2,e+)⋅hζ1,ζ2(ζ̃1(e+),ζ2)⋅∏t=T−s+sn+1T+Mf(ϵt)dζ2de+∬hζ1,ζ2(ζ̃1(e+),ζ2)⋅∏t=T−s+sn+1T+Mf(ϵt)dζ2de+,wherehζ1,ζ2(ζ̃1(e+))is obtained from (22) by replacingζ1,kbyζ̃1,k(e+).Numerical solutions for the integrals on the right hand side of (23) can be obtained but techniques more complicated than in the preceding section or in Lanne et al. (2012b) seem to be required. As in Breidt and Hsu (2005), where an analogous forecasting procedure for (univariate) noninvertible moving average models is developed, one can employ an importance sampling technique (see, e.g., Section 4.3 of Geweke, 1996). To this end, letφ(⋅)be ansn(n−1)-dimensional density function whose support contains the support of the distribution ofζ2, and define(24)W(ζ̃1(e+),ζ2)=hζ1,ζ2(ζ̃1(e+),ζ2)φ(ζ2).Then, the numerator in (23) can be written as(25)∬q(ζ2,e+)⋅W(ζ̃1(e+),ζ2)⋅φ(ζ2)⋅∏t=T−s+sn+1T+Mf(ϵt)dζ2de+and the denominator in (23) can similarly be written as(26)∬W(ζ̃1(e+),ζ2)⋅φ(ζ2)⋅∏t=T−s+sn+1T+Mf(ϵt)dζ2de+.Clearly, the integral in (25) is the expectation ofq(ζ2,e+)⋅W(ζ̃1(e+),ζ2)with respect to a distribution with densityφ×f×⋯×f(M−sn+scopies off) and the integral in (26) is the expectation ofW(ζ̃1(e+),ζ2)with respect to the same distribution. Thus, the conditional expectation in (23) can be approximated via Monte Carlo simulation as(27)EˆT(q(ζ2,e+))=1m∑i=1mq(ζ2(i),e+(i))⋅W(ζ̃1(e+(i)),ζ2(i))1m∑i=1mW(ζ̃1(e+(i)),ζ2(i)),where(ζ2(i),e+(i))=(ζ2(i),ϵT−s+sn+1(i),…,ϵT+M(i)),i=1,…,m, are mutually independent simulated realizations from a distribution with densityφ×f×⋯×f(regardingζ̃1(e+(i)), see Eq. (20)). Thus,ζ2(i)(sn(n−1)×1)is drawn from a distribution with densityφandϵT−s+sn+1(i),…,ϵT+M(i)(n×1)are drawn independently ofζ2(i)from a distribution with densityfand, similarly toϵT−s+sn+1,…,ϵT+M, the random vectorsϵT−s+sn+1(i),…,ϵT+M(i)are independent for everyi. Finally,W(ζ̃1(e+(i)),ζ2(i))is computed by using (22) and (24).Approximate forecasts, which can be made arbitrarily accurate by choosingmandMlarge enough, can be obtained recursively as follows.Step  1. Generate(ζ2(i),e+(i))=(ζ2(i),ϵT−s+sn+1(i),…,ϵT+M(i)),i=1,…,m, as described below (27).Step  2. Forh=1,2,…, apply (27) recursively with (see (17) and (20))q(ζ2,e+)=Ph[ζ̃1(e+)ζ2]+∑j=−s−h+sn+1M−hNjϵT+h+j=defqh(ζ2,e+),and computeEˆT(yT+h)=a1EˆT(yT+h−1)+⋯+anrEˆT(yT+h−nr)+∑j=−(n−1)r−s−hNjϵ̃T+h+j+EˆT(qh(ζ2,e+)),whereEˆT(yT+h−k)=yT+h−kfork≥handNj=0forj<−(n−1)r. Thus,∑j=−(n−1)r−s−hNjϵ̃T+h+j=0fors+h>(n−1)rand the first term in the definition ofqh(ζ2,e+)vanishes whenh+s−sn>(n−1)r.In addition to choosing values for the integersmandM(to be discussed in Section  4) the application of the preceding procedure requires two choices. First, one has to choose the matrix[K0⋯Ksn−1](sn(n−1)×sn2)whose rows we here assume to be formed of the basis vectors of the orthogonal complement of the space spanned by the rows of[N0⋯Nsn−1]. Second, one has to choose thesn(n−1)-dimensional auxiliary density functionφ(ζ2). Asζ2=∑j=0sn−1KjϵT−s+1+j, a potentially reasonable choice might be based on the chosen error distribution. In the bivariate special case withs=1the random vectorζ2is also bivariate, and one could chooseφ(ζ2)as the density function of the error termϵt. In general, as the dimension ofζ2iss(n−1)times the dimension ofϵt, one could similarly chooseφ(ζ2)as the density function of(ϵT−s+1,…,ϵT−s+sn), that is,f×⋯×f(sn(n−1)copies). This choice is probably not optimal but, due to its simplicity, will be used in our subsequent numerical illustrations where the error term is assumed to have a multivariatet-distribution. Breidt and Hsu (2005) use a somewhat similar importance sampler in their forecasting procedure.As in the purely noncausal case, it is also possible to obtain interval forecasts and forecasts for the conditional distribution of the components ofyT+h(h≥1). We illustrate this below in the case of one-step-ahead forecasts (h=1) and provide details of the more complex general case (h≥1) in Appendix A.4. Using the notation introduced at the end of Section  3.1 the optimal forecast for the conditional cumulative distribution function of theath component ofyT+1, at pointx∈Ris (see (5) and (7))ET(1(ya,T+1≤x))≈ET(1(∑j=1nrajιa′yT+1−j+∑j=−(n−1)rM−1ιa′NjϵT+1+j≤x)).Decomposing the latter sum inside the indicator function as in (17) we haveET(1(ya,T+1≤x))≈ET(1(∑j=−s−s−1+snιa′NjϵT+1+j+∑j=−s+snM−1ιa′NjϵT+1+j≤x−ιa′κ̃T,1))≈ET(1(ιa′P1[ζ̃1(e+)ζ2]+∑j=−s+snM−1ιa′NjϵT+1+j≤x−ιa′κ̃T,1)),where the latter approximation is based on the discussion leading to (20) and, for brevity,κ̃T,1=∑j=1nrajyT+1−j+∑j=−(n−1)r−s−1Njϵ̃T+1+j.Note thatκ̃T,1depends on the observed data and is treated as fixed, and the same applies toz̃3which appears in the vectorζ̃1(e+)(see (20)). Thus, to obtain (an approximation for)ET(1(ya,T+1≤x))we need to compute the conditional expectation ofET(q(ζ2,e+))with(28)q(ζ2,e+)=1(ιa′P1[ζ̃1(e+)ζ2]+∑j=−s+snM−1ιa′NjϵT+1+j≤x−ιa′κ̃T,1).Using this choice ofq(ζ2,e+)in (27) and the subsequent Steps 1 and 2 yields a forecast for the conditional cumulative distribution function ofya,T+1at pointx. A forecast of the whole conditional cumulative distribution function and interval forecast forya,T+1can be obtained as described at the end of the preceding section.It is possible to simplify the preceding simulation method if suitable knowledge of the structure of the matrix[N0⋯Nsn−1]is available. In particular, as will be seen below, it is possible to avoid the use of importance sampling if the matrix[N0⋯Ns−1](sn×sn)is nonsingular, for then we can chooseQ=[N0⋯Nsn−1K0⋯Ksn−1]=def[Q11Q120Isn(n−1)],whereQ11=[N0⋯Ns−1],Q12=[Ns⋯Nsn−1]and[K0⋯Ksn−1]=[0:Isn(n−1)]. In the purely noncausal case considered in Section  3.1, this choice is always possible because thenNj=0,j<0, andN0=In, implying that the matrixQ11is upper triangular with unit diagonal elements. However, in general the preceding choice of[K0⋯Ksn−1]may be inappropriate because the nonsingularity of the matrixQ11may fail (see the example at the beginning of Section  3.2 wheres=1andQ11=N0=N0is singular). On the other hand, in practice the matrixQ11is unknown and has to replaced by an estimate which, due to estimation errors, is necessarily nonsingular (with probability one). Moreover, a simulated example provided in the next section suggests that, even when the assumed nonsingularity does not hold, the forecasting procedure to be derived in this section performs well compared to its robust but computationally more demanding alternative developed in the previous section. Note also that in practice one can assess the possible singularity ofQ11by examining, for example, the eigenvalues or determinant of its estimate.When the matrixQis as defined above, we haveζ2=(ϵT+1,…,ϵT−s+sn)(see (19)) andR=Q−1is of the same form asQor, specifically,R=[R11R120Isn(n−1)]=[Q11−1−Q11−1Q120Isn(n−1)].Thus, in this case the joint density function ofζ1andζ2becomes (see (22))hζ1,ζ2(ζ1,ζ2)=∏j=1sf(∑k=1sRj,kζ1,k+∑k=s+1snRj,kϵT−s+k)⋅∏j=s+1snf(ϵT−s+j)⋅|det(R)|,so that the approximate relation (23) can be written asET(q(ζ2,e+))≈∬q(ζ2,e+)⋅∏j=1sf(∑k=1sRj,kζ̃1,k(e+)+∑k=s+1snRj,kϵT−s+k)⋅∏t=T+1T+Mf(ϵt)dζ2de+∬∏j=1sf(∑k=1sRj,kζ̃1,k(e+)+∑k=s+1snRj,kϵT−s+k)⋅∏t=T+1T+Mf(ϵt)dζ2de+,whereζ̃1,k(e+)is defined below (21). Thus, as now(ζ2,e+)=(ϵT+1,…,ϵT+M), the integral in the numerator is the expectation ofq(ζ2,e+)⋅∏j=1sf(∑k=1sRj,kζ̃1,k(e+)+∑k=s+1snRj,kϵT−s+k)with respect to a distribution with densityf×⋯×f(Mcopies) whereas the integral in the denominator is the expectation of∏j=1sf(∑k=1sRj,kζ̃1,k(e+)+∑k=s+1snRj,kϵT−s+k)with respect to the same distribution.The preceding discussion shows that, instead of (27), we can approximate the conditional expectationET(q(ζ2,e+))via Monte Carlo simulation as(29)EˆT(q(ζ2,e+))=1m∑i=1mq(ζ2(i),e+(i))⋅∏j=1sf(∑k=1sRj,kζ̃1,k(e+(i))+∑k=s+1snRj,kϵT−s+k(i))1m∑i=1m∏j=1sf(∑k=1sRj,kζ̃1,k(e+(i))+∑k=s+1snRj,kϵT−s+k(i)),where(ζ2(i),e+(i))=(ϵT+1(i),…,ϵT+M(i))=ϵ+(i),i=1,…,m, are independent draws from a distribution with densityf×⋯×f(Mcopies). Forecasts can be obtained by modifying the two steps in the forecasting procedure of the previous section as followsStep  1. Generate(ζ2(i),e+(i))=(ϵT+1(i),…,ϵT+M(i))=ϵ+(i),i=1,…,m, as described below (29).Step  2. Forh=1,2,…, apply (29) recursively withq(ζ2,e+)andEˆT(yT+h)as defined in Step 2 of the previous section.This simulation procedure is similar to that derived in the purely noncausal case in Section  3.1 to which it, in fact, reduces in that special case (for a detailed discussion of this issue, see Appendix A.3).The simulation procedure described above can also be used to obtain interval forecasts and forecasts for the conditional distribution of components ofyT+h(h≥1). In the case of one-step-ahead forecasts the approximation derived forET(1(ya,T+1≤x))at the end of the preceding section still applies and implies that a forecast for the conditional cumulative distribution function ofya,T+1at pointxcan be obtained by choosingq(ζ2,e+)as in (28) and applying the preceding Steps 1 and 2. A forecast of the whole conditional cumulative distribution function and, furthermore, interval forecast forya,T+1can be obtained as described at the end of Section  3.1. The general case of obtaining interval forecasts and forecasts for the conditional distribution of the components ofyT+hwithh≥1is discussed in Appendix A.4.In this section, we examine the performance of our forecasting techniques by using Monte Carlo simulations and data generation processes (DGPs) based on bivariate models estimated for real data. The same data set, comprised of quarterly US inflation and the real marginal cost, is also used in the next section to provide an illustration of our forecasting techniques. As mentioned in the Introduction, inflation and the real marginal cost are variables extensively studied in the previous literature, although instead of the real marginal cost other variables have also been considered along with inflation (see, e.g., Gali and Gertler, 1999, Canova, 2007, Nason and Smith, 2008, Gefang et al., 2012, and the references in therein).Our quarterly data set, from the Federal Reserve Bank of St. Louis FRED databank, covers the period from 1955:1 to 2010:3. Inflation is computed as the log-difference of the seasonally adjusted GDP implicit price deflator and the real marginal cost is approximated by the real unit labor cost (for details, see Lanne and Luoto, 2013). We use the period from 1955:1 to 1989:4 to estimate VAR(r,s) models that will serve as DGPs in the subsequent Monte Carlo simulations. Throughout this paper, GAUSS 10 and its BHHH optimization routine in the CMLMT package are employed in estimation, simulation, and forecasting.To specify a potentially noncausal VAR model we proceed along the lines discussed in Section  2.2 and first consider a Gaussian VAR(p) model. The conventional model selection criteria AIC and BIC as well as autocorrelation functions of the residuals suggested the orderp=2. However, the assumption of Gaussian errors could be rejected by theQ–Qplots of the residuals and, given uncorrelated residuals, by the clear autocorrelation in the squared residuals of the inflation equation. Thus, we consider second-order models, that is, VAR(r,s) models withr+s=2and, to capture the fat tails of the residual distribution, we choose the (bivariate)t-distribution for the errors.In the previous literature, the typical reaction to autocorrelation in squared residuals of a conventional causal VAR model has been to augment the model with GARCH errors. However, as theoretically demonstrated by Lanne and Saikkonen (2013, Section 2.3), one can expect to find autocorrelation in squared residuals when a causal VAR model is fitted to data generated by a (non-Gaussian and) noncausal VAR process (the same applies to a noncausal VAR model whose orders are misspecified). Thus, a potentially viable alternative to augmenting a causal VAR model with GARCH errors is to consider a noncausal VAR model.Of the second-order models the VAR(0,2) model maximizes the likelihood function but only marginally compared to the VAR(1,1) model, whereas in terms of residual diagnostics the VAR(1,1) model performs slightly better, as the residuals of the VAR(0,2) model appear conditionally heteroskedastic. In the VAR(1,1) model, the estimates of the parametersΠ1,12andΦ1,12appear small compared to their standard errors and the same applies to the estimates of the parametersΦ1,12andΦ2,12in the VAR(0,2) model (we useΦk,ijto signify the (i,j) element of the matrixΦkwith a similar notation used forΠk). Restricting these parameters to zero also seems reasonable according to the likelihood ratio test (p-values 0.271 and 0.083 in the VAR(1,1) and VAR(0,2) models, respectively) and, in the case of the VAR(0,2) model, their imposition considerably improves the rather imprecise estimation of the degrees-of-freedom parameter of thet-distribution. The restrictions has no marked effect on the residual diagnostics of the two models but, interestingly, the maximum value of the likelihood function of the restricted VAR(1,1) model turns out to be slightly greater than that of the VAR(0,2) model. All in all, both of these restricted models perform reasonably well and will be used as DGPs in our simulation experiments and in the forecasting exercise of Section  5. The estimation results are presented in Table 1.  Below, we shall also consider the conventional causal VAR(2) model for comparison and, to see how our forecasting procedures work in a higher order case, a fourth-order model will be briefly discussed.It may be worth noting that the restrictions employed in the noncausal models in Table 1 are imposed on purely statistical grounds. As they imply that neither leads nor lags of the marginal cost (y2t) appear in the equation of inflation (y1t), one might think that the marginal cost has no effect on inflation forecasts. However, one should be cautious about making such an interpretation. To see the reason for this, consider the VAR(0,2) model whose moving average representation is such thaty1t(inflation) depends onϵ1,t+j, whereasy2t(marginal cost) depends on bothϵ1,t+jandϵ2,t+j(j≥0). Thus, asϵ1,t+jaffects both inflation and the marginal cost, one cannot rule out the possibility that the marginal cost can help forecastϵ1,t+jand thereby inflation (see the (approximate) forecasting formula (12)). A similar argument applies to the VAR(1,1) model.We simulate 10000 realizations of lengthT+8from DGPs corresponding to the two estimated models in Table 1 (100 observations are discarded from the beginning and end of the simulated series to eliminate the impact of initialization effects). We estimate a causal VAR(2) model as well as the correct noncausal VAR(1,1) or VAR(0,2) model from the firstTobservations in each realization. Note that the estimated models are unrestricted, i.e., the restrictionsΦ1,12=Φ2,12=0andΠ1,12=Φ1,12=0discussed above are not taken into account. The sample sizeTis set to 300, and the number of simulated realizationsmemployed in the noncausal forecasting procedures ranges fromm=10000tom=500000. Results of some robustness checks with the sample sizeT=100will also be reported. Based on the findings of Lanne et al. (2012b), the value of the truncation parameterMis set at 50 (essentially the same results are obtained withM=30andM=100).Point forecasts 1–8 periods ahead are constructed as described in Section  3. When the forecasts are based on the noncausal VAR(1,1) model and importance sampling is used we have to choose the auxiliary density functionφ(ζ2). Following the discussion at the end of Section  3.2.1, our choice is the density function of(ϵT,ϵT+1)with the independentϵTandϵT+1having the bivariatet-distribution shown in Table 1 (Panel B). In the case of the forecasting procedure derived in Section  3.2.2 the assumed nonsingularity boils down to the nonsingularity of the matrixN0(see the beginning of Section  3.2.2 and note that nown=2ands=1). Using the estimates in Table 1 and formulas in Appendix A.1 we find that the determinant ofN0is 0.173, showing that the required nonsingularity holds.

@&#CONCLUSIONS@&#
