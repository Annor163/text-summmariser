@&#MAIN-TITLE@&#
On the solution of multidimensional convex separable continuous knapsack problem with bounded variables

@&#HIGHLIGHTS@&#
Multidimensional convex separable continuous knapsack problem is analyzed.Necessary and sufficient optimality condition is formulated and proved.Primal-dual analysis of the proposed approach is included.Examples of convex objective functions for the considered problem are presented.

@&#KEYPHRASES@&#
Convex programming,Separable programming,Necessary and sufficient optimality conditions,

@&#ABSTRACT@&#
A minimization problem with a convex separable objective function subject to linear equality constraints and box constraints (bounds on the variables) is considered. Necessary and sufficient optimality condition is proved for a feasible solution to be an optimal solution to this problem. Primal-dual analysis is also included. Examples of some convex separable objective functions for the considered problem are presented.

@&#INTRODUCTION@&#
Consider the following convex separable program(Cm=)(1)min{c(x)=∑j∈Jcj(xj)}subject to(2)∑j∈Jdijxj=αi,i=1,…,m(3)aj≤xj≤bj,j∈J,where cj(xj), j ∈ J, are differentiable convex functions, defined on the open convex sets Xj, j ∈ J, respectively, dij> 0 for everyi=1,…,mand j ∈ J;αi∈R,i=1,…,m;aj,bj∈R,j∈J;x=(xj)j∈J,where J ≡ {1, ..., n}.The feasible region X, defined by constraints (2) and (3), is a convex set because it is an intersection of the convex set (2), defined by m hyperplanes, and the box (3) of dimension|J|=n.Feasible region of the form (2)–(3) is a polytope and it is known as the (multiple) knapsack polytope.Problem(Cm=)and related problems arise in many cases, for example, in production planning and scheduling (Bitran & Hax, 1981), in allocation of resources (Bitran & Hax, 1981; Zipkin, 1980), in allocation of effort resources among competing activities (Luss & Gupta, 1975), in the theory of search (Charnes & Cooper, 1958), in subgradient optimization (Held, Wolfe, & Crowder, 1974), in facility location problems (Stefanov, 2000), in the implementation of various projection methods when the feasible region is of the form (2)–(3) (Stefanov, 2000; 2001; 2004), etc. That is why, characterization of the optimal solution of problem(Cm=)as well as efficient algorithms for solving such problems will be very useful.Related problems and methods for solving them are considered in the papers listed in References. The solution of knapsack problems with arbitrary convex or concave objective functions is studied in Bitran and Hax (1981), Luss and Gupta (1975), Moré and Vavasis (1991), Zipkin (1980), etc. Quadratic knapsack problems and related to them are studied in Brucker (1984), Pardalos, Ye, and Han (1991), Robinson, Jiang, and Lerme (1992), etc. A branch and bound algorithm for separable concave programming is proposed in Hong-gang Xue, Cheng-xian Xu, and Feng-min Xu (2004). Algorithms for the case of convex quadratic objective function are proposed in Brucker (1984), Dussault, Ferland, and Lemaire (1986), Helgason, Kennington, and Lall (1980), etc. Algorithms for bound constrained quadratic programming problems are proposed in Dembo and Tulowitzki (1983), Moré and Toraldo (1989), Pardalos and Kovoor (1990). A polynomial time algorithm for the resource allocation problem with a convex objective function and nonnegative integer variables is suggested in Katoh, Ibaraki, and Mine (1979). Surrogate upper bound sets for bi-objective bi-dimensional binary knapsack problems are studied in Cerqueus, Przybylski, and Gandibleux (2015). Valid inequalities, cutting planes and integrality of the knapsack polytope, which is the feasible region of the problem under consideration, are considered, for example, in Stefanov (1998; 2011), etc. Well-posedness and primal-dual analysis of convex separable optimization problems of the considered form are studied in Stefanov (2013b). Methods for solving variational inequalities over box constrained feasible regions, defined by (3), are proposed, for example, in Stefanov (2002; 2007; 2013a), etc. Iterative methods of nondifferentiable optimization are applied to some separable problems of approximation theory in Stefanov (2014), etc.This paper is devoted to formulation and proof of a necessary and sufficient optimality condition (characterization) of optimal solution to problem(Cm=). Rest of the paper is organized as follows. In Section 2, a necessary and sufficient condition (characterization theorem) for a feasible solution to be an optimal solution to problem(Cm=)is proved (Theorem 1). In Section 3, primal-dual analysis of the proposed approach is included. In Section 4, some strictly convex separable functions cj(xj), j ∈ J, important for practical problems like(Cm=),are presented. In Section 5, open problem for future work is formulated.Suppose that the following assumptions are satisfied.(A1)aj≤ bjfor all j ∈ J. Ifak=bkfor some k ∈ J, then the valuexk:=ak=bkis determined a priori.∑j∈Jdijaj≤αi≤∑j∈Jdijbj,i=1,…,m. Otherwise the constraints (2) and (3) are inconsistent and the feasible regionX=∅.The Lagrangian for problem(Cm=)is(4)L(x,u,v,λ)=∑j∈Jcj(xj)+∑i=1mλi(∑j∈Jdijxj−αi)+∑j∈Juj(aj−xj)+∑j∈Jvj(xj−bj),whereλ∈Rm;u,v∈R+n,whereR+nconsists of all vectors with n real nonnegative components.The Karush–Kuhn–Tucker (KKT) necessary and sufficient optimality conditions for the minimum solutionx*=(xj*)j∈Jto problem(Cm=)are(5)cj′(xj*)+∑i=1mλidij−uj+vj=0,j∈J,(6)uj(aj−xj*)=0,j∈J,(7)vj(xj*−bj)=0,j∈J,(8)∑j∈Jdijxj*=αi,i=1,…,m,(9)aj≤xj*≤bj,j∈J,(10)λi∈R1,i=1,…,m,uj∈R+1,vj∈R+1,j∈J,whereλi,i=1,…,m;uj,vj,j∈J,are the Lagrange multipliers, associated with the constraints (2), aj≤ xj, xj≤ bj, j ∈ J, respectively. Ifaj=−∞orbj=+∞for some j ∈ J, we do not consider the corresponding KKT condition (6) (KKT condition (7), respectively) and multiplier uj(multiplier vj, respectively).The following Theorem 1 gives necessary and sufficient optimality condition (characterization) of the optimal solution to problem(Cm=).Theorem 1(Characterization of the optimal solution to problem(Cm=)) Let assumptions (A1)and(A2) be satisfied. A feasible solutionx*=(xj*)j∈J∈Xis an optimal solution to problem(Cm=)if and only if there existλi∈R,i=1,…,m,such that(11)xj*=aj,j∈Jaλ=def{j∈J:−cj′(aj)≤∑i=1mλidij}(12)xj*=bj,j∈Jbλ=def{j∈J:−cj′(bj)≥∑i=1mλidij}(13)xj*:−cj′(xj*)=∑i=1mλidij,j∈Jλ=def{j∈J:−cj′(bj)≤∑i=1mλidij≤−cj′(aj)}.When convex function cj(xj), j ∈ J, are strictly convex, then inequalities, defining Jλ in (13), are strict.Necessity. Letx*=(xj*)j∈Jbe an optimal solution to problem(Cm=). According to KKT theorem, there exist constantsλi,i=1,…,m;uj,vj,j∈J,such that KKT conditions (5)–(10) are satisfied.(a) Ifxj*=aj,thenuj≥0,vj=0according to KKT conditions (10) and (7), respectively. Therefore, (5) impliescj′(xj*)≡cj′(aj)=uj−∑i=1mλidij≥−∑i=1mλidij.Multiplying this inequality by(−1),we obtain−cj′(aj)≤∑i=1mλidij.(b) Ifxj*=bj,thenuj=0,vj≥0according to KKT conditions (6) and (10), respectively. Therefore, (5) impliescj′(xj*)≡cj′(bj)=−vj−∑i=1mλidij≤−∑i=1mλidij.Multiplying this inequality by(−1),we get−cj′(bj)≥∑i=1mλidij.(c) Ifaj<xj*<bj,thenuj=vj=0according to KKT conditions (6) and (7), respectively. Therefore, (5) implies−cj′(xj*)=∑i=1mλidij.Using thataj<xj*<bjand cj(xj), j ∈ J, are convex functions, it follows thatcj′(aj)≤cj′(xj*)≤cj′(bj),that is, in case (c) we have−cj′(bj)≤∑i=1mλidij≤−cj′(aj).When convex function cj(xj), j ∈ J, are strictly convex, these inequalities are strict.In order to describe cases (a), (b), (c), we introduce the index setsJaλ,Jbλ,Jλ,defined by (11), (12), and (13), respectively. It is obvious thatJaλ∪Jbλ∪Jλ=J. The “necessity” part of Theorem 1 is proved.Sufficiency. Conversely, let x* ∈ X and components of x* satisfy (11), (12), (13).Set:∑i=1mλidij=−cj′(xj*),uj=vj=0forj∈Jλ;uj=cj′(aj)+∑i=1mλidij(≥0accordingtodefinitionofJaλ),vj=0forj∈Jaλ;uj=0,vj=−cj′(bj)−∑i=1mλidij(≥0accordingtodefinitionofJbλ)forj∈Jbλ.By the use of these expressions, it is easy to check that KKT conditions (5), (6), (7), (10) are satisfied. KKT conditions (8), (9) are also satisfied according to the assumption that x* ∈ X.Thus, in all possible cases, considered above,λi,i=1,…,m;xj*,uj,vj,j∈J,satisfy KKT conditions (5)–(10) for problem(Cm=),which are necessary and sufficient conditions for a feasible solution to be an optimal solution to a convex minimization problem. Therefore x* is an optimal solution to problem(Cm=). When cj(xj), j ∈ J, are strictly convex functions, the optimal solution is unique.□Since the optimal solution x* to problem(Cm=)depends onλ=(λ1,…,λm)(expressions (11)–(13)), we consider components of x* as functions ofλfor differentλ∈Rm:(14)xj(λ)={aj,j∈Jaλbj,j∈Jbλxj*:−cj′(xj*)=∑i=1mλidij,j∈Jλ.Functions xj(λ), j ∈ J, are piecewise linear, monotone, piecewise differentiable functions ofλ, with a pair of break hyperplanesAj={λ∈Rm:−cj′(aj)=∑i=1mλidij}andBj={λ∈Rm:−cj′(bj)=∑i=1mλidij},j∈J.Since uj≥ 0, vj≥ 0, j ∈ J, and since the complementary KKT conditions (6), (7) must be satisfied, in order to findxj*,j∈J,from the KKT system (5)–(10), we have to consider all possible cases for uj, vj: all uj, vjequal to 0; all uj, vjdifferent from 0; some of them equal to 0 and some of them different from 0. The number of these cases is22|J|=22n,where 2n is the number of all Lagrange multipliers uj, vj, j ∈ J, where|J|=n. Obviously this is an enormous number of cases, especially for large-scale problems. For example, whenn=1500,we have to consider 23000 ≈ 10900 cases. (Only for comparison, recall that astrophysicists have determined that the age of the Universe is approximately 15 billion years, that is, approximately “only” 1018 seconds.) Furthermore, in each case, we have to solve a large-scale system of nonlinear equations inλi,i=1,…,m;xj*,uj,vj,j∈J. Therefore, the direct application of the KKT theorem, using explicit enumeration of all possible cases, for solving large-scale problems of the considered form would not give a result in real time, and efficient characterization theorems and methods for solving the considered problem will be very useful.Importance of Theorem 1 consists in the fact that it describes components of the optimal solution to problem(Cm=)only through the Lagrange multipliersλi,i=1,…,m,associated with the equality constraints (2).When the (optimal) Lagrange multipliersλi*,i=1,…,m,associated with equality constraints (2), are known, then problem(Cm=),defined by (1)–(3), can be replaced by the following convex separable optimization problemmin{∑j∈Jcj(xj)+∑i=1mλi*(∑j∈Jdijxj)−∑i=1mλi*αi}subject tox≡(x1,…,xn)∈A=def{x∈Rn:aj≤xj≤bj,j∈J}.The problem, dual to problem(Cm=),is(15)maxΨ(λ)subject to(16)λ≡(λ1,…,λm)∈Rm,whereΨ(λ)=minx∈A{∑j∈Jcj(xj)+∑i=1mλi(∑j∈Jdijxj)−∑i=1mλiαi}.Thus, using the Lagrangian duality and Theorem 1, we can replace the multivariate problem(Cm=)ofx∈Rnby another multivariate problem (15)–(16) ofλ∈Rm. Since usually the number m of “general” linear equality constraints (2) is less than the number n of variables (of box constraints (3)) of the original problem(Cm=),it turns out that in many cases the dual problem (15)–(16) is easier to be solved than the original problem(Cm=).In this section, we present some convex separable objective functions cj(xj), j ∈ J, for problem(Cm=),which are important for many practical problems of this form, described in bibliographical notes of Section 1Introduction (Stefanov 2003; 2006a; 2006b; 2009; 2010).1.cj(xj)=sjxj,xj≠0,cj(xj)=−sjlnmjxj,sj>0,mj>0,xj>0,cj(xj)=cjxjq,cj>0,q>1,xj>0,cj(xj)=−sj(xj+cj)(xj+mj),mj>cj,sj>0,cj(xj)=12cj.(xj−x^j)2,cj>0,cj(xj)=−sjxj+mjxj2,mj>0,cj(xj)=12(hS−xjsj)2,sj>0,S≠0,cj(xj)=−sjln(1+mjxj),mj>0,sj>0,xj>−1mj,cj(xj)=sj(e−mjxj−1),mj>0,sj>0,cj(xj)=ekjxj,kj>0.Functions cj(xj) of Examples 1 and 4 are linear-fractional (hyperbolic) functions.Problem(Cm=)with objective functionc(x)=∑j∈Jcj(xj)of Examples 5–7 is a quadratic multiple knapsack problem.Problem(Cm=)with objective functionc(x)=∑j∈Jcj(xj)of Example 5 is the problem of projecting a given pointx^=(x^1,…,x^n)∈Rnonto the convex set X defined by (2)–(3). This problem must be solved at each iteration of algorithm performance of any projection method (e.g. projected gradient methods, stochastic quasigradient methods, etc.). Components of the optimal solution of this projection problem, according to Theorem 1, arexj*=aj,j∈Jaλ=def{j∈J:cj.(x^j−aj)≤∑i=1mλidij}xj*=bj,j∈Jbλ=def{j∈J:cj.(x^j−bj)≥∑i=1mλidij}xj*=x^j−1cj∑i=1mλidij,j∈Jλ=def{j∈J:cj.(x^j−bj)≤∑i=1mλidij≤cj.(x^j−aj)},where cj> 0, j ∈ J are the constants of the definition of objective function of Example 5, aj, bj, j ∈ J anddij,i=1,…,m;j∈Jare the coefficients of problem (1)–(3),λi,i=1,…,mare the corresponding Lagrange multipliers, andx^j,j∈Jare components of the given pointx^=(x^1,…,x^n)∈Rnthat must be projected.

@&#CONCLUSIONS@&#
In this paper, we have formulated and proved a necessary and sufficient optimality condition (characterization) for a feasible solution to be an optimal solution to problem(Cm=).An open question for future work is developing a convergent algorithm of polynomial complexity for solving problem(Cm=).