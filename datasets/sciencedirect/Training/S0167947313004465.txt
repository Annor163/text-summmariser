@&#MAIN-TITLE@&#
Automated learning of factor analysis with complete and incomplete data

@&#HIGHLIGHTS@&#
We propose a novel algorithm for learning factor analysis with complete and incomplete data.The algorithm is able to determine the number of factors in an automated manner.The algorithm is as effective as the previous two-stage procedure.However, the algorithm is much more computationally efficient.

@&#KEYPHRASES@&#
Factor analysis,Model selection,Maximum likelihood,Incomplete data,CM,EM,

@&#ABSTRACT@&#
In the application of the popular maximum likelihood method to factor analysis, the number of factors is commonly determined through a two-stage procedure, in which stage 1 performs parameter estimation for a set of candidate models and then stage 2 chooses the best according to certain model selection criterion. Usually, to obtain satisfactory performance, a large set of candidates is used and this procedure suffers a heavy computational burden. To overcome this problem, a novel one-stage algorithm is proposed in which parameter estimation and model selection are integrated in a single algorithm. This is obtained by maximizing the criterion with respect to model parameters and the number of factors jointly, rather than separately. The proposed algorithm is then extended to accommodate incomplete data. Experiments on a number of complete/incomplete synthetic and real data reveal that the proposed algorithm is as effective as the existing two-stage procedure while being much more computationally efficient, particularly for incomplete data.

@&#INTRODUCTION@&#
Factor analysis (FA) is a commonly used multivariate analysis technique that identifies the common characteristics among a set of variables. The parameter estimation can be easily performed by means of the maximum likelihood (ML) method via the popular expectation maximization (EM)-like algorithm (Dempster et al., 1977; Meng and Rubin, 1993; Meng and van Dyk, 1997). In addition, in density modeling for high-dimensional data, the covariance structure of FA offers significant advantages over full/diagonal/scalar covariance, because of its capability of providing an appropriate trade-off between overfitting full covariance and underfitting diagonal/scalar covariance (Tipping and Bishop, 1999).To find the trade-off, namely to determine the number of factorsq, a two-stage procedure is commonly adopted, where stage 1 performs parameter estimation for a set of candidate models and stage 2 chooses the best according to a model selection criterion. The most popular criteria for this purpose are Akaike’s information criterion (AIC) (Akaike, 1987) and the Bayesian information criterion (BIC) (Schwarz, 1978). Obviously, this two-stage procedure has to enumerate all candidate models over the set of{q}. Usually, to achieve satisfactory performance, a large set of values ofqis used and thus this procedure can be very time consuming.To overcome this problem, we propose in this paper a novel one-stage algorithm, in which we do not use a criterion to choose one from a set of candidate models; rather, we integrate the determination of the number of factors into parameter estimation and thus this one-stage algorithm is able to significantly alleviate the computational burden suffered by the two-stage procedure. We call our algorithm the automated learning algorithm of FA (AFA, in short). Specifically, the AFA consists of two conditional maximization (CM)-steps: CM-step 1 maximizes the model selection criterion with respect to (w.r.t.) the number of factors and factor loadings jointly while keeping uniqueness fixed; CM-step 2 maximizes the criterion w.r.t. uniqueness while keeping factor loadings fixed.In the presence of missing data, Song and Belin (2008) have shown that the AIC and BIC can still be used to determine the number of factors reliably. However, the implementation has to be drawn on the two-stage procedure and, as will be seen in Section  5, its computational burden is typically much heavier than that in the complete data case, due to the inclusion of more missing information. Therefore, we further extend the AFA to accommodate incomplete data by means of the standard methodology in Little and Rubin (1987) and obtain an automated learning algorithm of FA with incomplete data.Note that the likelihood ratio test (LRT) is also another popular choice for determining the number of factors (Lawley and Maxwell, 1971). However, to the best of our knowledge, there does not exist a corresponding ‘automated’ version of the LRT just like the AFA for the criterion-based FA in this paper. Thus, the LRT must be run many times for different values ofqwhile the criterion-based FA now can be run only once with the AFA.The remainder of this paper is organized as follows. In Section  2, we review the conditional maximization (CM) algorithm for fitting FA (Zhao et al., 2008; Yu and Zhao, 2013). Based on this, we propose the AFA for complete data in Section  3 and extend it to accommodate incomplete data in Section  4. We conduct an empirical study to compare the AFA with the existing two-stage procedure in Section  5. We end the paper with some concluding remarks in Section  6.Suppose that thed-dimensional data vectorxfollows aq-factor model:(1){x=Ay+μ+ϵ,y∼N(0,I),ϵ∼N(0,Ψ),whereμis ad-dimensional mean vector,Ais ad×qfactor loading matrix,yis aq-dimensional latent factor vector,Ψ=diag{ψ1,ψ2,…,ψd}is a positive diagonal matrix,ψiis the so-called uniqueness of variableiinxandIdenotes an identity matrix of suitable dimension.IfAin (1) is replaced byARandybyR′y, whereRis an orthogonal matrix, the FA model is invariant and hence the estimate ofAcan only be determined up to a rotation, from which we have that the number of free parameters in the FA model isD(q)=d(q+2)−q(q−1)/2(Lawley and Maxwell, 1971).Under model (1),(2)x∼N(μ,Σ),whereΣ=AA′+Ψ. To avoid over-parameterization, the number of degrees of freedom inΣ,d(q+1)−q(q−1)/2, should not exceed that of a fulld×dcovariance matrix,d(d+1)/2(Beal, 2003), which yields(3)qmax≤d+12(1−1+8d).Given a set of i.i.d. observationsX={xn}n=1N, let(4)x̄=1N∑i=1Nxi,andS=1N∑i=1N,(xi−x̄)(xi−x̄)′be the sample mean vector and sample covariance matrix ofx, respectively. The global maximum likelihood estimator (MLE) ofμis trivially the sample meanx̄and thus the MLE ofθ=(A,Ψ)can be obtained by maximizing the log likelihood(5)L(θ)=−N2{log|Σ|+tr(Σ−1S)}.There exist many algorithms to maximizeL. For example, the expectation maximization (EM) (Rubin and Thayer, 1982), the quasi Newton–Raphson algorithm (Jöreskog, 1967), the expectation CM either (ECME) algorithm (Liu and Rubin, 1998), the parameter-expanded EM (PX-EM) (Liu, 1994), etc. Since these algorithms have to either include latent factors as missing data that yield slow convergence or resort to numerical optimization methods that lack the simplicity and stability of EM, Zhao et al. (2008) propose a conditional maximization (CM) algorithm that does not involve numerical optimization and latent factors and their empirical results show that CM enjoys substantially faster convergence than EM and ECME. More importantly, it is a building block of our proposed automated learning algorithm for FA. A brief review of the CM algorithm is given in Section  2.2. For completeness, the EM and PX-EM are also briefly reviewed in Sections  2.3 and 2.4, respectively.LetΨ=diag(ψ1,ψ2,…,ψd),Ψi≜diag(ψ̃1,…,ψ̃i−1,ψi,ψi+1,…,ψd). Given an initialΨ, the CM algorithm alternates the following two steps until convergence ofLis met.•CM-step  1: GivenΨ, maximizingL(A)in (5) w.r.t.AyieldsÃ.CM-step  2: GivenÃandΨi, maximizingL(ψi)in (5) w.r.t.ψiyieldsψ̃i, sequentially fori=1,2,…,d.Let the normalized sample covariance matrix be(6)S̄=Ψ−1/2SΨ−1/2,whereSis given in (4), and(λi,ui)be its eigenvalue–eigenvector pairs ofS̄sorted in the orderλ1≥λ2≥⋯≥λd. GivenΨ,Ãis obtained by(7)Ã=Ψ1/2Uq′(Λq′−I)1/2R,where, ifλq>1,q′=q; otherwise,q′is the unique integer satisfyingλq′>1≥λq′+1,Λq′=diag(λ1,λ2,…,λq′),Uq′=(u1,u2,…,uq′)andRis an orthogonal matrix satisfyingRR′=I.By the FA model assumption thatΨis positive, we can pick an arbitrary very small numberη>0and assumeψ̃i≥η. LetĀ=Ψ−1/2A,eibe theith column of thed×didentity matrix,(8)Bi=∑k=1i−1ω̃kekek′+I+ĀĀ′,bkbe thek-th column vector ofBi−1andbkkstand for thekk-th element ofBi−1. Thenψ̃iis obtained by(9)ψ̃i=max{[bii−2(bi′S̄bi−bii)+1]ψi,η},and the requiredω̃iin (8) is given by(10)ω̃i=ψ̃i/ψi−1.By (9),ψ̃k≥η, and by (10),ω̃k>−1,k=1,…,i−1, thusBiin (8) is invertible andψ̃iin (9) can always be computed. The implementation using (8) and (9) directly as provided in Zhao et al. (2008) costsd2q+3d3. Recently, Yu and Zhao (2013) proposed a novel implementation that enjoys the lower costd2q+2d3/3. We simply give its detailed pseudocode in Algorithm 1 and interested readers can refer to Yu and Zhao (2013) for details.The expectation maximization (EM) algorithm (Dempster et al., 1977) is a well known iterative procedure for finding the ML estimate. Each iteration of EM consists of an E-step to obtain the expected complete data log likelihood, i.e., the so-calledQfunction, and a M-step to maximizeQw.r.t.θ.From (1), the probability density ofyngivenxnis given by(11)yn|xn∼N(M1−1A′Ψ−1(xn−μ),M1−1),whereM1=I+A′Ψ−1A. By (11), we obtain(12)E[yn|xn]=M1−1A′Ψ−1(xn−μ),and(13)E[ynyn′|xn]=M1−1+E[yn|xn]E[yn|xn]′.In the EM for FA, the set of latent factorsY={yn}n=1Nis treated as missing data, the augmented complete data is(X,Y)and hence the complete data log likelihood isL1c(X,Y|θ)=∑n=1Nlog{p(xn|yn,θ)p(yn)},whereθ=(A,Ψ)and the parameterμhas been replaced by its MLEx̄.•E-step: Givenθ, compute the expectedL1cw.r.t. the posterior distributionp(yn|xn,θ)(14)Q1(θ)=−N2log|Ψ|−12∑n=1N{E[yn′yn|xn]+E[(xn−Ayn−x̄)′Ψ−1(xn−Ayn−x̄)|xn]},where the required expectations are given by (12) and (13).M-step: MaximizingQ1w.r.t.θyieldsÃ=∑n=1N(xn−x̄)E[yn|xn]′(∑n=1NE[ynyn′|xn])−1,=SΨ−1AM1−1(M1−1A′Ψ−1SΨ−1AM1−1+M1−1)−1,andΨ̃=1Ndiag{∑n=1N(xn−x̄)(xn−x̄)′−ÃE[yn|xn](xn−x̄)′},=diag{S−ÃM1−1A′Ψ−1S},whereSis given by (4).The parameter-expanded EM (PX-EM) algorithm (Liu et al., 1998) is a covariance adjustment technique for accelerating the standard EM by means of parameter expansion. In the PX-EM for FA, an auxiliary covariance matrixVis introduced and it is assumed thatyn∼N(0,V). Thus the parameter is expanded toθ=(A,Ψ,V). The probability density ofyngivenxnis now given by(15)yn|xn∼N(M2−1A′Ψ−1(xn−μ),M2−1),whereM2=V−1+A′Ψ−1A. By (15), we obtain(16)E[yn|xn]=M2−1A′Ψ−1(xn−μ),and(17)E[ynyn′|xn]=M2−1+E[yn|xn]E[yn|xn]′.Similarly to that in the EM for FA, the complete data log likelihood of(X,Y)isL2c(X,Y|θ)=∑n=1Nlog{p(xn|yn,θ)p(yn)},whereθ=(A,Ψ,V). Each iteration of PX-EM consists of a PXE step and a PXM-step.•PXE-step: Givenθ, compute the expectedL2cw.r.t. the posterior distributionp(yn|xn,θ)Q2(θ)=−N2(log|Ψ|+log|V|)−12∑n=1N{E[yn′V−1yn|xn]+E[(xn−Ayn−x̄)′Ψ−1(xn−Ayn−x̄)|xn]},where the required expectations are given by (16) and (17).PXM-step: MaximizingQ2w.r.t.θyieldsV˜=1NE[ynyn′|xn]=M2−1A′Ψ−1SΨ−1AM2−1+M2−1,Ã=1N∑n=1N(xn−x̄)E[yn|xn]′V˜−1=SΨ−1AM2−1V˜−1,andΨ̃=1Ndiag{∑n=1N(xn−x̄)(xn−x̄)′−ÃE[yn|xn](xn−x̄)′},=diag{S−ÃM2−1A′Ψ−1S},whereSis given by (4). After convergence of the PX-EM is met, the expanded parameters can be reduced by settingΨ̃=Ψ̃andÃ=Ãchol(V˜), wherechol(⋅)denotes the Cholesky decomposition.Table 1lists the computation costs of EM and CM in each iteration given in Zhao et al. (2008). For comparison, the cost of PX-EM is also included. Since PX-EM and EM have similar updating formulae, they have similar per-iteration complexities. It can be observed from Table 1 that CM has the heaviest per-iteration complexity. However, when using the two-stage procedure to choose the number of factors, the candidate value ofqcould be large and the per-iteration cost with PX-EM and EM could also be heavy.In Section  3.1, we first review the previous two-stage procedure. In Section  3.2, we propose a novel one-stage algorithm for FA with complete data.Due to the reason that the log likelihoodLunder the FA model is a nondecreasing function of the number of factorsq, it cannot be adopted as a model complexity criterion. Several model selection criteria have been proposed to deal with this problem via a two-stage procedure, as described below.Given a range of values ofqfromqmintoqmax, which is assumed to include the optimal one, the two-stage procedure typically first obtains the ML estimatesθˆ(q)for each modelqand then employs a model selection criterionL∗(q,θˆ(q))to choose the valueqˆ=arg maxq{L∗(q,θˆ(q))}.The criterion often takes the form(18)L∗(q,θˆ(q))=L(X|θˆ(q))−D(q)2C(N),whereD(q)=d(q+2)−q(q−1)/2is the number of free parameters in aq-factor model as detailed in Section  2.1, andC(N)D(q)/2is a penalty term that penalizes the higher values ofq. Many criteria differ inC(N)only. For example, Akaike’s information criterion (AIC) (Akaike, 1987) takesC(N)=2while the Bayesian information criterion (BIC) (Schwarz, 1978) usesC(N)=logN.Such a two-stage procedure using the AIC/BIC is widely used for the FA model with complete data to determine the number of factors. However, this procedure has to enumerate all candidate models over the set of{q}. Usually, to achieve satisfactory performance, a large set of candidate values ofqis used and the computational burden of this procedure can be heavy. For example, for the 50-dimensional data used in Section  5, by (3) we obtain the maximumqmax=40, which means that, in principle, we have to enumerate a total of 40 FA models with different numbers of factors.To overcome the problem suffered by the two-stage procedure, we develop a one-stage algorithm that integrates parameter estimation and model selection in a single algorithm, which we call the automated learning algorithm of FA (AFA). Specifically, the AFA consists of the following two CM-steps.•CM-step  1: GivenΨ, maximizingL∗(A,q)in (18) w.r.t.AandqyieldsÃandq̃.CM-step  2: GivenÃandΨi, maximizingL∗(ψi)in (18) w.r.t.ψiyieldsψ̃i, sequentially fori=1,2,…,d.Substituting (7) into (5), we obtain (Zhao et al., 2008)(19)−2N⋅L=∑i=1q′(logλi−λi+1)+log|Ψ|+∑i=1dλi,whereλ1≥λ2≥⋯≥λdare the eigenvalues of matrixS̄defined in Section  2.2. GivenΨ,log|Ψ|+∑j=1dλjis obviously a constant. Substituting (19) into (18) to computeL∗, dropping the terms irrelevant toq, we obtain(20)q̃=arg minq{∑i=1q(logλi−λi+1)+D(q)C(N)N}.Onceq̃has been obtained by (20), we can easily obtainÃby (7) using the existing eigendecomposition ofS̄. For clarity, the detailed pseudocode of the whole automated leaning algorithm is given in Algorithm 2.Clearly, unlike the two-stage procedure, the AFA is free from exhaustive enumeration. The computational cost of the AFA looks roughly equivalent to that of a single run of CM for a fixedq. Thus, the more candidate values ofqare used in the two-stage procedure, the more advantageous the AFA is. As will be seen in Section  5.1, the AFA is much more efficient than the two-stage procedure by EM/PX-EM.In this section, we shall extend the AFA to accommodate incomplete data. Similarly to  Song and Belin (2008), we assume that incomplete data satisfy the missing at random (MAR) mechanism defined in Little and Rubin (1987), such that we can make use of the standard methodology detailed there to deal with incomplete data.Letxoandxmbe the observed and missing parts ofx=(xo;xm)andIoandImdenote their corresponding sets of indices inx, respectively. Alternatively, we could also introduce two additional permutation matrices to extractxoandxmfromxas done in Lin et al. (2006, 2009). From (2), we have the following well known normal results:xo∼N(μo,Σoo),andxm|xo∼N(μm⋅o,Σmm⋅o),where, according to the indices inIoandIm,μoandμmdenote the corresponding subvectors ofμ,Σoo,Σmo=Σom′andΣmmdenote the corresponding submatrices ofΣ,(21)μm⋅o=E(xm|xo)=μm−ΣmoΣoo−1(xo−μo),and(22)Σmm⋅o=cov(xm|xo)=Σmm−ΣmoΣoo−1Σom.μm⋅oandΣmm⋅ocan be calculated directly by (21) and (22), respectively. Actually, they can be obtained in a more efficient manner via the Sweep operator (SWP) or reverse SWP (RSWP) (see,e.g.,  Little and Rubin, 1987; Lange, 1999) as follows:SWP[Io]Σ=RSWP[Im]Σ−1=(−Σoo−1Σoo−1ΣomΣmoΣoo−1Σmm⋅o).For the FA model, we have the following relations (Zhao et al., 2008):(23)Σ−1=Ψ−1−Ψ−1A(I+A′Ψ−1A)−1A′Ψ−1,and(24)|Σ|=|Ψ||I+A′Ψ−1A|.When the missing portion ofxis not large (say, less than 50%), we prefer RSWP since the inverse ofΣcan be efficiently calculated by (23) and RSWP requires a smaller number of sweeps than SWP. In addition, dividing|Σ|from (24) by the product of the diagonal elements just after each is swept yields|Σoo|as a by-product, which is useful for evaluating the observed data log likelihood (25). When the missing portion ofxis large (say, more than 50%), we choose SWP. In this case,|Σoo|is the product of the diagonal elements just before each is swept.Up to a constant term, the observed log likelihood ofθ=(μ,A,Ψ)for incomplete dataXobs={xno}n=1Nis(25)Lo(θ∣Xobs)=−∑n=1N{log|Σnoo|+(xno−μno)′Σnoo−1(xno−μno)}.LetX=(Xobs,Xmis)be the complete data, whereXmis={xnm}n=1N. Apart from a constant term, the complete data log likelihood ofθfor complete dataXisLc(θ|X)=−∑n=1N{log|Σ|+(xn−μ)′Σ−1(xn−μ)}.To chooseqfor incomplete data, Song and Belin (2008) suggest using the penalized observed data log likelihood(26)Lo∗(q,θˆ(q))=L(Xo|θˆ(q))−D(q)2C(N).Correspondingly, the penalized complete data log likelihood isLc∗=Lc−D(q)C(N)/2. Maximization of (26) can be achieved though the following three CM-steps.•CML-step  1: Given(A,Ψ), maximizingLo∗in (26) w.r.t.μyieldsμ̃.E-step: GivenXoand(μ̃,A,Ψ), compute the expectedLc∗to obtain theQ∗function.CMQ-step  2: Given(μ̃,Ψ), maximizingQ∗w.r.t.AandqyieldsÃandq̃.CMQ-step  3: Given(μ̃,Ã), maximizingQ∗w.r.t.ΨyieldsΨ̃.Like the complete data case, the AFA with incomplete data does not require exhaustive enumeration. As will be seen in Section  5.2, the advantage of the AFA with incomplete data over the two-stage procedure by EM/PX-EM is much more substantial than that in the complete data case.

@&#CONCLUSIONS@&#
