@&#MAIN-TITLE@&#
Post-evolution of variable-length class prototypes to unlock decision making within support vector machines

@&#HIGHLIGHTS@&#
Support vector machines are used as a noise removal tool for a white-box classifier.The cooperative coevolution algorithm classifies the data re-labeled by SVMs.An evolutionary approach extracts prototypes for each class of the problem.Thresholds are generated for each attribute of the classification problem.The unimportant attributes are removed from the resulting prototypes.

@&#KEYPHRASES@&#
Support vector machines,Opaque prediction,Prototype discovery,Feature selection,Evolutionary computation,Cooperative coevolution,

@&#ABSTRACT@&#
Although neural networks and support vector machines (SVMs) are the traditional predictors for the classification of complex problems, these opaque paradigms cannot explain the logic behind the discrimination process. Therefore, within the quite unexplored area of evolutionary algorithms opening the SVM decision black box, the paper appoints a cooperative coevolutionary (CC) technique to extract discriminative and compact class prototypes following a SVM model. Various interactions between the SVM and CC are considered, while many experiments test three decisive hypotheses: fidelity to the SVM prediction, superior accuracy to the CC classifier alone and a compact and comprehensive resulting output, achieved through a class-oriented form of feature selection. Results support the hybridization by statistically and visually demonstrating its advantages.

@&#INTRODUCTION@&#
Within both the theoretical and practical research environments, when faced with having to select a methodology that is able to most accurately mine and model learning from existing knowledge, the choice is almost always made between the kernel-based methodologies of neural networks (NNs) and support vector machines (SVMs). These two superiorly performing techniques suffer nevertheless from the curse of an opaque engine [2], which is undesirable for both theoreticians, who are keen to control the modeling, and the practitioners, who are more than often suspicious in using the prediction results as a reliable assistant in decision making.A compromise between accurate prediction and comprehensibility of the underlying decision factors has been reached through the emergence of the field concerned with the development of white box extraction algorithms. These techniques start from the trained black box model at hand and use the derived data as a basis for various methods for informative knowledge generation. Two behavioral choices are established for such algorithms [1]:-Pedagogical: extract information following the input–output mapping established by the SVM, i.e., view the SVM as an oracle to label existing data which are subsequently used by the extractor.Decompositional: derive structured explanations following the internal structure of the SVM, i.e., the support vectors, which represent the data points that lie closest to the decision boundary.The goal in the creation of such algorithms is multicriterial [2]: the resulting explanatory system must predict the target for previously unseen data (accuracy) better than the information extractor alone, while the behavior of the extraction engine must approximate that of the black box model (fidelity). Finally, the output of the extractor must be more comprehensible than that of the initial learner, whichever form this information may take (comprehensibility).In these circumstances and after consideration of the existing literature entries on the subject, the aim of the present paper is to employ a new cooperative coevolutionary (CC) algorithm able to extract comprehensible class representatives from a SVM pre-modeled data set, in a manner both accurate to new data and faithful to the opaque model. Reference to attribute edge values is designed to be as simple and compact as possible, such that the comprehensibility goal is also attained. The envisaged algorithm will have to perform better in accuracy than CC alone and closer to the results of the SVM, while also providing a set of decision guidelines to conduct data to the given output in a short and understandable format.Having stated some formal description of implicated concepts (in Section 2), the current study starts from the state of the art of white-box extraction techniques and a preliminary testing version of the approach (Section 3). It subsequently arguments the reasons that support the further development of the framework (Section 4) and presents the SVM-CC methodology in Section 9. It is attempted to investigate the impact of both possible interactions between SVM and CC on the success on the task:1.CC (described in Section 7) evolves class prototypes based on data pre-classified by the SVM (outlined in Section 5). This pedagogical consideration is presented in 9.Support vectors are extracted from the SVM learnt model and are given to the CC. This decompositional approach is also depicted in Section 9.Comprehensibility of the generated decision factors, as the reason behind the development of white-box extraction techniques, is revealed through the encoding fashion, which is illustrated in Section 6. A supplementary mechanism for the elimination of redundancies in each of the resulting guidelines is described in Section 8. This form of posterior feature selection increases the understandability of explanations by making reference exclusively to the data attributes significant for each class of the problem in turn.Success is measured in terms of the two most widely acknowledged evaluation criteria to assess the performance of knowledge extraction techniques that have been proposed over the years [6]. The fidelity of the outcome of the SVM-CC hybridized technique to that of the SVM model is calculated on several benchmark data sets. On the other hand, the equally important requirement of a prediction accuracy higher than that of the CC classifier alone is also examined during experimentation. Comprehensibility of the resulting explanatory system is investigated both visually and by a case-study example. The potential and limitations of the method are tested through different comparisons to alternative (combination of) techniques. All these experimental results, observations and discussions are gathered in Section 10. Section 11 contains the overall conclusions.There are several key concepts related to a classification task [8,7] that must be formally explained before pursuing the targeted goal.Definition 1A sample (or example, feature, record, instance, object, point, vector, tuple, case) x is a line of the data set, which is described by n independent variables (or attributes, characteristics, predictors, indicators) and a dependent, categorical variable representing the class (or label, outcome, output).Remark 1Without loss of generality, please note that, since the proposed methodology involves evolution of attribute thresholds, we assume classification problems with continuous attribute domains. We thus further presume that every data sample is defined over a domain X=[a1, b1]×[a2, b2]×, ⋯, ×[an, bn], where aj, bjdenote the bounds of definition for feature j of a sample, j=1, 2, …, n, and each corroborated outcomeyi,yi′is discrete, taking values from a set {1, 2,…,k}.Definition 2A threshold for an attribute is the boundary real value which in combination to those of other features discriminate the samples into classes.Definition 3A prototype is a representative for a class of the problem and contains the attribute thresholds that place a sample into that class and prevent it from being labeled otherwise.Definition 4A classification problem may be defined by a training set of m pairs of the form (xi, yi), where each couple holds the information related to a data sample xiand its confirmed target yi, and a test set of p pairs of the type(xi′,yi′), where the responseyi′is hidden to the learning machine and must be predicted.Definition 5A classifier is a technique that learns the associations between each training sample and the acknowledged class. Either in a black-box manner or explicitly, it then takes each test sample and makes a forecast on its probable response, according to what has been learnt.Remark 2If the size of the data collection permits, an extra validation middle step can be performed. This checks the error of prediction of the classifier and potentially initiates training again for further tuning. Once this is finished, the prediction accuracy (generalization error) is computed through the testing phase and is the final output of the classifier.Definition 6The prediction accuracy of a classifier is the percent of correctly classified test cases over the total available number and is the common measure of comparison to other classification techniques.Existing attempts (most of them of very recent date, i.e., after 2008) have addressed white box extraction from NNs and SVMs through various information generators.As generally concerns knowledge extraction following SVMs, there are several state of the art methodologies reported in [1,9,10]. The approaches range from clustering [11], linear programming [12], decision trees [13], the area under the receiver operation characteristic curve [14] to active learning [15] or fuzzy rules [10].As more specifically regards EA extraction methods from NNs or SVMs, there are some reported attempts that are detailed below:-GEX [17] follows NNs and uses a special encoding for evolving rules and an island model [16] to allow the existence of multiple subpopulations, each connected to a class of the problem to be solved. A more elaborate representation for individuals and a multiobjective optimization engine in the Pareto sense is further on developed in Ref. [18].G-REX [6] is a better and more general technique that uses genetic programming [4,19] to extract rules of various representations from different opaque (or not) models (NNs included).A combination between NNs and ant colony optimization [20,21] for the same task is realized in [22].Specifically as regards white-box extraction from a trained SVM by means of EAs exclusively, it is only the independent entry from Ref. [1] that was found, where the G-REX method [6] is now applied to SVMs instead of NNs. A first version of the SVM-CC is however tested on a medical problem in Ref. [28]. A HC is additionally embedded for feature selection in synchronization with the evolution of the decision set. A pyramid of the importance of each attribute for the triggered outcome is also created.The proposed SVM-CC white-box classification framework consists of a SVM classification model, an indicative and minimal threshold representation for class prototypes and a CC engine to determine these attribute edge values against the resulting SVM labeled data.Among kernel-based methodologies, the SVM is preferred as it has shown top performance for a wide, diverse range of real-world applications [1,23,26,27,24,25]. On the other hand, the CC inherently performs the maintenance of several distinct concurrent subpopulations, where each of them contains candidate prototypes for a class of the problem. Its type of diversity conservation had previously proved useful for classification problems both for benchmark studies [5] as well as for practical tasks [3].Henceforth, in opposition to the few existing SVM-EA models (see Section 3), the SVM-CC brings a couple of benefits:-The encoding of the algorithm behind the decision process is simpler than genetic programming rules [6] and the CC implicit separation into multiple distinct sets of potential solutions for each outcome is more direct than ant colony optimization [22] and easier to control than island models [17].The possibility of easily allowing only the presence of the informative indicators for each of the final decision prototypes can only but additionally facilitate a deeper and more pointing understanding of the problem and relevant features, all centered on their discriminative thresholds.More importantly, as compared to the first version of the SVM-CC in Ref. [28], the current form renounces feature selection performed along with the generation of prototypes. Instead, it considers brushing up only the final class prototypes, in singular search of the attributes that are significant for each class in turn. Different from standard attribute preprocessing, this addresses the particularities of each problem and outcome at hand, with the aim of an increased comprehensibility.SVMs attempt learning through the establishment of a hyperplane of parameters w and b that must simultaneously discriminate between classes with a minimal training error and be able to generalize on new instances.The optimization scheme for classification is therefore mathematically translated into Eq. (1), where generalization is maintained by minimizing ∥w∥2 and a good training accuracy is expressed by a compact formulation where digressions from the hyperplane are permitted only by a minimum ξi, for each of the m samples.(1)findwandbastominimize∥w∥22+C∑i=1mξi,C>0subjecttoyi(w·xi−b)≥1−ξi,ξi≥0,foralli=1,2,…,mDefinition 7A training sample i, i=1, 2, …, m, with ξi=0 and for which the second line of equation (1) holds with an equality sign is called a support vector.The support vectors are the most important points for the determination of the decision surface. Their removal from the data set would change the found solution.If the samples cannot be modeled in a linear fashion, then a nonlinear surface is obtained by the kernel trick [29], where data are mapped into a higher dimensional space where a linear surface is able to perform the learning. Kernels are commonly taken in either a polynomial expression or Gaussian (radial) formulation.The optimization statement that is reached is resolved by relying on an extension of the Lagrange multipliers technique [29]. A dual formulation is derived and the optimal Lagrange multipliers are considered as the solutions of the system resulting by setting the gradient of the new objective function to 0. Once they are found, the output for every test case is derived, while the model remains hidden, with no information on the logic behind the decision-making.In order to mine for the underlying decision scheme, it can be profited from some nevertheless available insights. Such information includes how each training data point can be relabeled by the SVM or which are the support vectors. These will constitute the SVM output that the presented evolutionary white-box extraction framework will use as input for class prototypes determination.The prototype for each class consists of a conjunction of pairs of the type (attribute, threshold) for each indicator of the classification problem and the targeted label.Hence, the formal expression of a prototype for this approach can be written as Eq. (2), where there exists such a prototype for every class yi, i=1,2,…,k, of the problem. Therefore, the extracted knowledge indicates the thresholds that place a certain sample in a given class.(2)IFthreshold1=val1…ANDthresholdn=valnTHENclass=yi.The aim is to perform an evolutionary generation of prototypes for each class.Definition 8An individual (or candidate solution) is a single point (vector) in the population-based search of an EA.The EA determines the attribute thresholds by encoding them through the individuals that make up the population. The threshold value for each attribute is initially randomly generated following a uniform distribution within the definition bounds for that specific feature. Subsequently, individuals are tested against the training set and are continually adjusted in order to increase the similarity to samples whose class they represent. The CC further appoints a means through which individuals of different labels communicate, in order to obtain a functional interconnected prototype set.Mimicking the Darwinian principles of evolution, an EA individual advances through the interaction with the environment. As an important part of its environment is represented by other individuals, evolution can be also viewed as coevolution. On the one hand, the individuals could collaborate for the same purpose and thus construct the solution together within a cooperative establishment or, on the contrary, they could compete against each other for the same resources as part of a competitive framework.Generally speaking, CC [30] assumes that any candidate solution of the problem at hand is decomposed into several sub-solutions and each of these separate components is evolved in a distinct population (species). The only interaction between the different populations takes place when an individual is evaluated. Its performance cannot be measured separately, since it represents only a part of a potential solution. Individuals from all the other populations have thus to be selected and brought together in order to construct a complete candidate. The result of this overall evaluation is assigned as the fitness of the initial individual and, naturally, its value directly depends on the quality of its collaborators.For the white-box extraction task, the CC classification-designed framework [5] is considered. Since the solution to the classification task is represented by a set of decision prototypes, it follows that each population evolves representatives of a certain class and cannot vanish due to loss of diversity during the evolutionary process. However, a CC individual does not specifically encode the decision class as in the representation discussed in Section 6, as this is implicit from the population to which the prototype belongs.To evaluate an individual from a certain population, a complete set of prototypes has to be formed, in the sense that one representative for each of the other classes has to be selected. The entire potential decision set is then applied to the training data: for every sample, similarities to each collected individual are computed and the found class is concluded to be the one of the prototype that contains the closest matching thresholds. A prediction accuracy over all training samples is obtained and assigned as the fitness of the current individual to be evaluated under the established collaboration setup.To calculate how close the current prototype is to a sample, a distance measure has to be employed. In the experiments conducted within the current paper, it is the common Euclidean distance (3) that is considered in this respect:(3)d(c,s)=∑j=1n(cj−sj)2,where c and s represent an individual and a sample, respectively, and cjis the jth component of the potential solution.In the end of the evolutionary run, CC provides several populations of distinct individuals, each species containing the prototypes that define a certain outcome of the problem. In order to apply these class decision thresholds for samples in the test set, individuals are selected once more from each population, the former are labeled once more according to closest distance and the final prediction accuracy is achieved.When a set of prototypes is selected to be tested against samples from the test set (i.e., a collaborator is chosen from each population denoting representatives of a class), it is natural that the thresholds for certain attributes are closer to one another than others. We assumed that these attributes have little or no influence as concerns the classification of a new sample. In this respect, Algorithm 9 proposes a form of feature selection to eliminate from each prototype the attributes whose thresholds are very close to a mean over all prototypes for the corresponding values.The algorithm receives a number of prototypes equal to the number of labels k of the classification problem and a positive integer significance threshold s. The values for s start from 0, the case when no attribute is removed, and can be incremented until a situation when a prototype remains without any attribute. This threshold is applied to each attribute with respect to the size of its interval, so actually the value of parameter s represents a percent of the definition span of the current feature (lines 7 and 9). The algorithm starts by creating a vector of means for the attribute thresholds over all k prototypes. The value from a position i represents the average over all thresholds found at i in the considered prototypes (line 2). Then, for each prototype, the attribute with the farthest distance to the corresponding mean value is found (lines 3–5). The distance between a threshold and the mean is divided to the size of the interval (normalization) for each attribute in order to have a relative comparison. Such a value is important in order to quantify how much can the significance threshold be increased until a prototype is completely eliminated because all its attributes are marked as non-important. Line 6 of the algorithm opens a subsection (lines 6–14) where the fact that all prototypes have at least one attribute with a significant value is assured (line 7). Each attribute is considered and a difference in absolute value is computed against the rates from the vector of means. If the obtained positive number is lower than the significance threshold, then this attribute is ignored (marked as don’t care) for the current prototype. What is most important about this feature selection strategy is that, when an attribute is removed from a prototype, it is not automatically discarded from another one, as it can be important for one class, but insignificant for others.Algorithm 9Elimination process of the attributes with low significance.Require: The set of k prototypes, where k is the number of classes, and a significance threshold sEnsure: The k prototypes, each holding only the relevant attributes1:begin2:Compute vector mean of length n by averaging the values for each attribute threshold over all the prototypes {n is the number of attributes}3:for each prototype l do4:Findthresholddistlamong all thresholdi, where i ∈ {1, 2,…,n}, that is the remotest to meani, i.e., corresponds tomaxi=1n∣thresholdi−meani∣bi−ai5:end for6:for each prototype l do7:if(bi−ai)·s/100<thresholddistlthen8:for each attributeido9:if∣ thresholdi- meani∣ < (bi−ai)·s/100then10:Mark i as a don’t care attribute for prototype l11:end if12:end for13:end if14:end for15:endWhen a prototype that includes don’t care attributes is applied to the test set, the method described in the previous section (the paragraph before the last one) suffers the following modifications: distance (3) is applied only to the attributes that matter from the current prototype, divided by the total number of these contributing attributes.The hybridization is constructed in the following sequential steps:1.A SVM model pursues classification the given training data. The following outcomes of the resulting SVM model [1] are used and tested in turn:a.The pedagogical approach The proposed CC framework considers the samples with their consequential SVM relabeling [6]. In this scenario, the SVM is seen as an oracle that rethinks the outputs of the data, according to the model it has learnt. In other words, the SVM is actually a noise remover, which enables the subsequent decision information extractor to concentrate learning only on presumably more correctly labeled data.The decompositional approach Alternatively, the resulting output of the SVM model is taken in the form of the support vectors [10]. This may prove to be further beneficial since they give the decision boundary and hence represent a more concise and reliable data collection to generate class prototypes from.The pedagogical+decompositional mixed approachAs compared to the initial version in Ref. [28], the option of combining the two SVM output possibilities is also considered. The resulting support vectors together with their new labels as given by the SVM classifier serve as training data for the CC classifier. This mixture may yield better results by controlling the noise and achieving a gain in runtime by shrinking the data collection.The CC threshold generator is subsequently applied to the SVM modeled training data taken in each of the forms described earlier one by one.Comprehensibility is treated first through the EA individual representation and, more importantly, by discarding non discriminative feature thresholds in each class prototype. Thus, the end-user eventually gets a picture of only those indicators that are relevant for a given label, with their threshold values that achieve class separation. The requirements concerning fidelity to SVMs and good prediction accuracy are embedded into the CC fitness function in the following manner:a.Within the pedagogical approach: when a prototype is evaluated and the prediction accuracy over all training samples is computed (as explained in Section 7), the known labels for the data points are taken faithful to the SVM. Fidelity is thus addressed as in Eq. (4) and expresses the percentage of identically labeled samples [2]:(4)fidelitySVM−CC=Prob(yiSVM=yiSVM−CC|xi∈X),where xiis the current observation,yiSVMis its outcome as obtained by the SVM andyiSVM−CCthat which is reached by SVM-CC.Within the decompositional approach: when the same accuracy of prediction of a formed set of prototypes (starting from a current individual) is calculated, the known labels are taken as the real ones. Accuracy [2] is therefore also obeyed (5):(5)accuracySVM−CC=Prob(yireal=yiSVM−CC|xi∈X)Finally, when the mixed approach is applied, the known labels of the support vectors are those derived by the SVM.The generated class prototypes are applied to the test set. The resulting prediction accuracy is measured against the stand alone SVM and CC classification techniques. These assess the fidelity to the SVM prediction and the presence or absence of an enhanced accuracy to the standard CC classifier.The set of restricted prototypes, each containing solely the important attributes for that outcome of the problem, together with their thresholds, are tested on the unseen samples, with the aim of keeping a good accuracy, while allowing for less complexity and consequently more understandability.

@&#CONCLUSIONS@&#
