@&#MAIN-TITLE@&#
Solving difficult mixed integer and disjunctive non-linear problems on single and parallel processors

@&#HIGHLIGHTS@&#
Difficult NLP-, MINLP- and GDP-problems are solved scalability is achieved by maximizing the intelligence of the processors and minimizing the communication between processors.Parallel processing where Gray coding is applied, improves the performance of the algorithm in large-scale problems.Large-scale NLP-problems are solved using interior point methodology employing a new exponential barrier function.A large-scale disjunctive programming problem is solved on a huge mesh.

@&#KEYPHRASES@&#
Mixed-integer non-linear optimization,General disjunctive programming,Parallel processing,

@&#ABSTRACT@&#
Difficult non-linear, mixed integer non-linear and general disjunctive programming (NLP/MINLP/GDP) problems are considered in the present study. We show using random relaxation procedures that sampling over a subset of the integer variables and parallel processing have potential to simplify large scale MINLP/GDP problems and hence to solve them faster and more reliably than conventional approaches on single processors. Gray coding can be utilized to assign problems to the local processors. Some efficient non-linear solvers have been connected to the Genetic Hybrid Algorithm (GHA) through a switch board minlp_machine(), monitoring a vector of caller functions. The switch board is tested on single processor computers and massively parallel supercomputers in a sample of optimization problems. A new line search algorithm based on multi-sectioning, i.e. repeated bi-sectioning of parallel threads is applied to a set of irregular NLP-problems. Simulations indicate that step search based on multi-sectioning and re-centering is robust. Time recording indicates that the step search procedure is fast also in large optimization problems. Parallel processing with Gray coding and random sampling over a subset of the integer variables improves the performance of the sequential quadratic programming algorithm with multi-sectioning in large-scale problems. An early mesh interrupt guarantees load balancing in regular problems. General disjunctive programming (GDP) problems can be simplified through parallel processing. Certain problems are solved in larger scale than reported previously. Function pointer logic and intelligent switching procedures provide a fruitful basis for solving high-order GDP problems, where the computational challenge in direct MINLP-formulations would be insurmountable.

@&#INTRODUCTION@&#
To date, several efficient solvers for mixed-integer non-linear optimization problems have been proposed and compared in the literature (cf., e.g. [1–8]). The underlying algorithms are based on sequential quadratic programming (SQP), sequential linear programming (SLP) or interior point techniques. Active-set SQP methods and interior point methods are currently considered the most powerful algorithms for large-scale non-linear programming [9]. In non-convex or irregular problems, the algorithms cannot guarantee the global solution. However, the established algorithms mostly yield at least a feasible MINLP-solution also in difficult problems (cf. [10–13]). Certain non-smooth problems can be reformulated as smooth optimization problems, but in general a methodology for non-differentiable functions is required for non-smooth optimization (cf. [14,15]). Semi-definite programming algorithms have been quite successful in solving quadratic assignment (QAP) problems [16]. Integrated systems, where artificial or swarm intelligence is connected to mathematical programming methodology provide a powerful basis for difficult irregular optimization problems (cf. [17–20]). We will subsequently study the performance of some high-performance mathematical programming algorithms connected as support libraries to the Genetic Hybrid Algorithm (GHA). The principal working of the programming platform GHA is summarized in Appendix A.We consider the general mixed-integer non-linear constrained optimization problem (MINLP):(1.1)P:minf(x,y)|gi(x,y)≥0,i∈I,gi(x,y)=0,i∈E,{x,y}∈[LB,UB],g(x,y)∈ℜm,x∈ℜnx,y∈YnyE and I denote the sets of equality and inequality constraints and {ℜnx, Yny} the real- and discrete-valued sub-spaces respectively (the Lagrange equation for the continuous relaxation of (1.1) is presented in Appendix B). Depending on {nx, ny} and the functional form of the system, problem P is solved as a continuous or mixed-integer linear (MI)LP or non-linear (MI)NLP problem. If some of the functions in (1.1) are non-differentiable at a local solution, the derivatives can be approximated as one- or two-way differences in the corresponding interface functions as discussed subsequently.If ny>0 and if the problem is regular and not huge, P can be solved by a branch-and-bound strategy: a currently non-integer variable j is selected from Y using a specified criterion and two separate sub-problems are generated with upper bound floor(yj) and lower bound ceil(yj) respectively. The sub-problems corresponding to these branches are solved as continuous-valued non-linear optimization problems using efficient NLP-solvers. An advantage of a systematic branch-and-bound search is that sub-trees can be eliminated or fathomed at an early stage in certain situations:•A sub-problem is infeasible. In this case all sub-problems obtained by branching from this node are infeasible as well.The sub-problem yields a feasible mixed-integer solution. Further branching is interrupted and the current upper bound for the MINLP-problem updated if necessary.The solution of the sub-problem is higher than the current upper bound; hence further branching from the node is interrupted.In MINLP-problems where the discrete search space is huge, the emerging branch-and-bound tree may become computationally prohibitive. In such problems, random sampling over a subset of the integer variables can be applied as discussed below (cf. the random relaxation procedure 6.1 and its extension 6.2).In block-separable MINLP-problems, each processor or cluster of processors solves the local sub-problem and delivers the solution directly or through the cluster root to the main root for assembly based on the node id (cf. [21]). Sampling over a subset of the integer variables and parallel processing have potential to simplify large scale MINLPs. If the mesh is divided into clusters, then each cluster can distribute the local block problem to the processors, for example using the Gray code procedure [22] presented below.If the problem is regular, i.e. the variation in solution time for the different NLP-relaxations derived by Gray coding is relatively low, load balancing is achieved through an early mesh interrupt. In this case, the processor that has detected the global solution will broad cast an early mesh interrupt signal asynchronously. The incoming signal is checked by the receiving nodes in critical places of the MINLP-algorithm. If the complexity of the individual NLP-relaxations is highly variable, the job list can be adjusted according to the solution time accumulating for each problem solved by the processors. In practice, idle processors can be assigned jobs as available from the job lists of busy processors. GHA contains the communication resources for reassignment and fast communication between individual processors, to be applied if load balancing requires adjustments of the job list ([23,24]. cf. [25,26]). In an irregular problem, where the solution time for individual NLP-realizations varies considerably and all jobs are being processed, load imbalance accumulates for each new processor becoming idle. However, the computational complexity of the MINLP-problem is always higher than that of the relaxed NLP-problems determined and assigned through Gray coding. Solving a concurrent set of independent NLPs that guarantee the global MINLP-solution is therefore preferable to tackling the MINLP-problem using branch-and-bound. If the number of NLP- or reduced MINLP-relaxations exceeds the number of parallel processors, the emerging job list can be assigned to the processors and adjusted as explained above.Some efficient non-linear solvers have been connected to the Genetic Hybrid Algorithm (GHA) through a switch board (vector of caller functions) and thoroughly tested on single processor computers and massively parallel supercomputers: DNCONG [27], FSQP [28], NLPQLP [29] and SNOPT [30]. The source code for these support libraries has been provided by the corresponding authors. A Linux version of the ILOG Cplex optimization studio downloaded from the Academic Initiative web site of IBM was installed on the Sisu supercomputer at the Center of Scientific Computing (CSC) in Helsinki. This is the first Cray XC30 server in use in Europe. Cplex is connected to GHA through an interface function included in the switch board. GHA can be connected to Matlab applications through the MCC compiler of Mathworks.The choice between alternative algorithms can be automated with GHA using a parallel setting [18,19]. An SQP-algorithm applying a new multi-section step search procedure with re-centering is tested. The local quadratic problems are solved by the QL-solver of Schittkowski [31], applying sparse matrix algebra and an active set strategy.Some quadratic, cubic [32], heuristic and extended bi-section [33] methods for step search are connected to the switch board. Other efficient solvers e.g. CONOPT [34], IPOPT ([35], or KNITRO-AMPL/MATLAB®) can be connected to GHA through the switch board. For preliminary assessment of the code, a modest number of difficult NLP- and MINLP-problems have been solved. Limited comparisons to results obtained by the algorithms FILTER [36], KNITRO, LOQO [37], MINOS [38], NLPQLP [29] and SNOPT [30] are provided. Initial testing has been conducted both on single processor main frame computers and on massively parallel supercomputers. The results are encouraging for extensive comparisons in future research.GHA has been developed since the late nineties (cf. e.g. [39–41]). The algorithm has been subjected to rigorous scalability tests and is one of the few algorithms allowed to use all available cores on the massively parallel Cray XT and Cray XC30 supercomputers at the Center of Scientific Computing (CSC) in Helsinki for production runs(http://www.csc.fi/english/research/Computing_services/computing/servers/louhi_scalability). These computers belong to the group of the top 500 parallel supercomputers in the world (http://www.top500.org/). During the time period 09-10/2011, GHA was tested in a project arranged within the Partnership for Advanced Computing in Europe (PRACE) on the Jugene supercomputer at Jülich in Germany. The calculating power of Jugene is one Petaflop/s or one trillion calculation operations per second, the currently fastest supercomputer in Europe. Scalability was demonstrated with 65536 parallel processors as made available for the test. The algorithm does not restrict the maximum number of processors. This is a topic determined by the computational problem at issue. The intrinsic features of the algorithm allow suppression of processor level output and minimizing the inter-nodal communication when using huge mesh sizes. The support of flexible clustering, node level independence and problem dependent communication allow the design of systems utilizing swarm intelligence [42,43]. An electronic User's Guide for GHA and its support libraries can be downloaded from http://www.abo.fi/fak/esf/gha/research/geno_mathematics/presentation/download_gha_guide.php.The generic algorithm is presented along with concrete code and examples in the electronic User's Guide. The guide contains numerous cases and examples helping the researcher to conduct comparative studies on the computers where the platform is installed. A great number of hyperlinks in the guide gives access to test problems, their idea and exact C code. GHA and its solvers can be accessed and tested on the CrayXC30 supercomputer at CSC using numerical problems of the researcher. The guide illustrates how special-purpose algorithms can be connected to the system as found necessary.We have demonstrated scalability of GHA and its MINLP-switchboard on the Cray XC30 server at CSC – the first in production in Europe – in a set of non-convex MINLP-problems up to the maximum number of processors available for testing during 2013. The system has 1472 2.6GHz E5-2670 processors, 11,776 cores in total that produce a computing performance of 244TFlop/s.In the development work, some difficult hidden bugs have been detected and removed from GHA and its support libraries. In the FSQP-package, some critical scalar division statements have been replaced by a robust safe division procedure. In the SNOPT-package, some BLAS-functions have been renamed using snopt-specific naming conventions in order to avoid multiple definitions on different platforms. An illegal updating of the loop limit in an LU-decomposition routine in the SNOPT-package has been corrected and reported to the authors. GHA and the switch board are written in strict object-oriented ANSI C. The heap memory usage of GHA and the MINLP-switchboard has tested extensively by the author ind different NLP and MINLP-problems using the powerful Valgrind-debugger (cf. valgrind.org). The test result obtained on the MINLP-problem batch of Kocis and Grossmann [44] with the Linux main frame computer of Åbo Akademi University is summarized in Appendix D. The results show zero memory leakage.Four LP-solvers have been connected to GHA as separate callable support libraries:•a sparse bound flipping dual (BFD) algorithm lpr() based on the ideas in Taha [45], Bradley et al. [46] and Maros [47],the LP-solver of the IBM ILOG Cplex optimization studio,the DDLPRS()-routine of IMSL STAT/Library [27],the LP_SOLVE-package.The performance of lpr() and Cplex in MILP-search is compared in Östermark [23].The interface to external algorithms is implemented through objects monitored by GHA. For simplicity, the switch board is called minlp_machine(). The algorithms are linked as separate libraries, depending on the computational problem. For example, the selected – or specially designed – non-linear solver can be invoked in the interface function accelerator(). The optimization problem can be parameterized such that certain structural decisions (which non-linear solver to select, which penalty function to use, which disjunctive conditions to specify, etc.) are determined genetically [48,49].The switch board monitors a set of interface functions that can be connected to the interface functions of GHA because of its object-oriented setting. These functions return the address of the object containing relevant information, such as the function value and derivatives of the non-linear problem atzk. The derivatives can be calculated numerically (forward, centered) or analytically. In many sparse large-scale problems the analytical gradient and Hessian become a practical necessity, whereas automatic improvement of the condition of the Hessian (for example through eigen value analysis) turns into a prohibitive endeavor. We illustrate such problems below. In non-differentiable or irregular problems or at the bounds of integer variables, the derivatives can be approximated by one- or two-sided differences. A special interface function (add_minlp) can be used to adjust the optimization formulation dynamically, for example in disjunctive programming problems. This function can be used to construct new linear/non-linear algorithms monitored by the switch board.The mathematical framework is written as a parametric extension of (1.1) for processor p:(2.1)Pp:minf(x,y)|gi(x,y)≥0,i∈I,gi(x,y)=0,i∈E,{x,y}∈[LB,UB],g(x,y)∈ℜm,x∈ℜnx,x∈Zny{I,E,m,nx,ny,θ}∈ΩpΩpdenotes the parametric space of processor p monitored by GHA and θ parameters not explicitly presented in the mathematical problem formulation (e.g. convergence limits, penalty function specifications, the line search method, etc.). Disjunctive programming problems may require distinct mathematical problems to be specified for different processors or processor clusters. The root or cluster roots assemble the results and extract the overall best solution from the mesh. Two small disjunctive problems taken from literature are considered in some detail below.The mathematical support libraries of the platform are supplemented by a sequential programming algorithm (KKT_QL). The principal steps of the algorithm are presented in Appendix C. The directional information (gradients, Hessian) is computed analytically or numerically using sparse matrix algebra. The local quadratic problems are solved using the QL-solver of Schittkowski [31], hence the directional search of KKT_QL is similar to that in NLPQLP of Schittkowski [29]. The algorithm applies the multi-sectioning line search procedure introduced subsequently (‘Line search by recursive multi-sectioning and re-centering’). The convergence of the algorithm is tested using the well-known Karush–Kuhn–Tucker optimality conditions [81]. KKT_QL has passed extensive heap memory testing with the Valgrind debugger on constrained optimization problems of different size and complexity.In the subsequent tests, the interface functions of the programming platform (GHA) are loaded with the specifics of the respective problem. The programming code for representative test problems can be obtained from the author on request.MINLP-problems where the discrete search space is small can be reduced to a complete set of non-overlapping ordinary local NLP-problems, given a sufficient number M of concurrent processors. Coupled with asynchronous message passing for a mesh interrupt, such problems are solved without a branch-and-bound tree [50]. Concurrent reduction of the search space can be achieved through binary or Gray coding as defined below. If the discrete subspace of MINLP-problem contains only binaries (0/1), then we can use binary coding or Gray coding without much difference. But if the integer section contains, say integers within [0,5], it would be difficult to split the search space using binary coding assuming only 0:s and 1:s. Here, Gray coding can be used as a tool for uniquely splitting up the search space between the processors.Each processor solves the local MINLP-problem as a relaxed NLP, for example with the integer-valued variables locked at the unique binary translation of its processor-ID, or using different solution algorithms on the level of individual processors/clusters of processors. When a processor finds the global optimum, an asynchronous interrupt signal is broadcasted to the mesh to effect an early termination. In convex MINLP-problems where the set of binary-valued variables is small – yet the optimization problem may be huge – this procedure is sufficient. Parallel processing through binary or Gray coding is useful in many important problems in economics and engineering – for example in portfolio selection and production scheduling–involving a limited number of discrete variables. To accommodate load balancing requirements in problems where new tasks are fed continuously, processors can communicate their (average) work load, for example to the root or cluster roots. The job lists can then be reallocated, utilizing the built-in features of GHA [23]. Gray coding does not solve the load balancing problem in general, however: if the global optimum is obtained by a processor after the other processors have computed their local solutions and if no continuous stream of new tasks can be assigned to the idle processors, then perfect load balancing is not achieved. However, in many practical problems, the idle time possibly arising in the mesh is by far outweighed by the gains in reliability and speed. Solving the MINLP – when possible – through a concurrent set of reduced NLPs, is preferable to directly solving a set of (huge) MINLPs. Early mesh interrupts – embedded in the non-linear solvers – improve the speedup of the algorithm.When solving MINLP-problems containing only binaries, thenRemark 1Assume that a binary-valued MINLP-problem P in (1.1) has the global solutionz*=(x*,y*),x∈ℜnx,y∈Zny. If M=2ny, P is solved in the reduced spacex∈ℜnxby at least one parallel processor p under the binary transformationy=b(p), without branching.Proof >The binary sub-space of the MINLP-problem has 2nyCartesian combinations. M equals the cardinality of the Cartesian search space, hence the set of ny-bit binary translationsB={b(j), j=1, 2, …, M} of the decimal processor ID:s covers the whole sub-space and nothing but the sub-space. Thus, the (nx+ny)-dimensional MINLP-problem (1.1) reduces to an ordinary nx-dimensional NLP for each processor j={1, …, M}:(3.1)Pj=minf(x|y=b(j))|gi(x|b(j))≥0,i∈I,gi(x|b(j))=0,i∈E,x∈[LB,UB],g(x|b(j))∈ℜm,x∈ℜnx,b(j)∈Zny{I,E,m,nx,ny,θ}∈ΩjSince the binary translation is complete, the ny-bit codeb(p) of at least one processor p∈{1, ..., M} coincides with the global solution in the discrete sub-space,b(p)=y*. Hence, the NLP-solution of processor p in the reduced spacexsolves P:(3.2)f(x*,y*)=min{f(x|y*=b(p))|gi(x|y*)≥0,i∈I,gi(x|y*)=0,i∈E,x∈LB,UB,g(x|y*)∈ℜm,x∈ℜnx,(y*∈Zny){I,E,m,nx,ny,θ}∈Ωp}.If M<2ny, then the binary translation is incomplete but can still reduce the branch-and-bound tree of the local processor significantly [50].For the general MINLP-problem, where the box constraints for the integer-valued variables may vary, the n-ary Gray code with base k, also called the non-Boolean Gray code G(n,k) is useful. This type of Gray code encodes non-Boolean values. Several algorithms for (n,k)-Gray codes are presented in the literature. In this paper we use an iterative cyclical algorithm [22] with the property that only one position of the code changes at a time. The calculated Gray code is reported inversely to recognize the binary reflection of the algorithm. If the box constraints of the discrete sub-space are non-uniform, a complete mapping of the Cartesian search space is produced by unique concatenated Gray codes generated through all combinations, as long as the cycles of the box intervals do not coincide. Consider for example the following box constraints with two intervals [0,2] and [0,5]]:LB=[000111]; UB=[222555];The base for the first box interval is k1=3 and for the second k2=5. The length of both intervals is the same, n=3. Thus, the cardinality of the Cartesian search space isk1n×k2n=33×53=3375.By using the cyclical algorithm separately for both box intervals and by concatenating the Gray codes, all Cartesian combinations are reached. Since the cycles do not coincide, the mapping is complete. Hence, the MINLP-problem can be reduced to concurrent NLP's and solved without branching with a mesh size of 3375 parallel processors. Nonzero lower bounds are recognized by shifting the Gray code correspondingly. In GHA, the processors compute their Gray code directly from their decimal ID. The concatenated Gray codes for a subset of the processors are presented in Table 3.1based on the box constraints above.Only one position of the code changes at a time for each sub-interval, hence the concatenated code changes in two positions from one processor to the next. Gray-coding is sufficient for large-scale convex MINLP-problems where the number of binary variables is small.Remark 2Assume that a MINLP-problem P in (1.1) has the global solutionz*=(x*,y*),x∈ℜnx,y∈Zny. Assume that the box constraints in P have L sub-intervals {n1, n2, …, nL},∑l=1Lnl=nywith base {k1, k2, …, kL} and that their Gray cycles do not coincide. IfM=∏l=1Lklnl,P is solved in the reduced spacex∈ℜnxby at least one parallel processor p under the concatenated Gray transformationy=dp(ny,k), without branching.Proof >The discrete sub-space of the MINLP-problem has∏l=1LklnlCartesian combinations. M equals the cardinality of the Cartesian search space. Since the Gray cycles of the sub-intervals do not coincide, the set of ny-bit concatenated Gray codesG={dj(ny,k), j=1, 2, …, M} of the decimal processor ID:s is complete. Thus, the (nx+ny)-dimensional MINLP-problem (1.1) reduces to an ordinary nx-dimensional NLP for each processor j={1, …, M}:(3.3)Pj:minf(x|y=dj(ny,k))|gi(x|dj(ny,k))≥0,i∈I,gi(x|dj(ny,k))=0,i∈E,x∈[LB,UB],g(x|dj(ny,k))∈ℜm,x∈ℜnx,dj(ny,k)∈Zny{I,E,m,nx,ny,θ}∈ΩjSince the binary translation is complete, the ny-bit codedp(ny,k) of at least one processor p∈{1, ..., M} coincides with the global solution in the discrete sub-space,dp(ny,k)=y*. Hence, the NLP-solution of processor p in the reduced spacexsolves P:(3.4)f(x*,y*)=min{f(x|y*=dp(ny,k))|gi(x|y*)≥0,i∈I,gi(x|y*)=0,i∈E,x∈[LB,UB],g(x|y*)∈ℜm,x∈ℜnx,(y*∈Zny){I,E,m,nx,ny,θ}∈Ωp}.Analogously to binary-valued MINLP-problems, the branch-and-bound tree of the local processor can be reduced significantly even thoughM<∏l=1Lklnl.In this case, the local box constraints can be locked at the Gray code. Alternatively, the latter can be used as a promising starting point for the local branch-and-bound tree within the original box constraints. Problem reduction through Gray coding can be used also in block separable problems, where each processor or cluster of processors solves the local sub-problem and delivers the solution to the main root directly or through the cluster root. The Gray code is useful also in disjunctive programming problems, for which the traditional MINLP formulation involves an exponentially increasing number of discrete variables. The Gray code is illustrated in parallel tests subsequently.The quadratic approximation of the continuous relaxation of problem P atx=xk(both continuous and discrete variables are included inx) can be written in soft form as follows [51]:(4.1)PQ:min∇f(x)TΔx+12ΔxTHΔx+12pkδ2s.t.∇gi(x)TΔx+(1−δ)gi(x)=≥0,i∈Jk∇gi(x)TΔx+gi(x)≥0,j≥Kkx+Δx∈[LB,UB]0≤δ≤1,Jk≡{1,...,me}∪{me<j≤m,gi(x)T≤ε,orvj(k)>0}Kk≡{1,...,m}/JkH=∇xx2L(x,λ)is the Hessian of the Lagrangian (cf Appendix B) and vjis an approximation of the optimal Lagrange coefficient for the j:th constraint (the first-order conditions for problem PQare presented in Appendix C). Jkdenotes the set of active constraints and Kkthe set of inactive constraints atxk. pkis initialized at a minimum level pminand updated in each iteration by the chosen penalty function. In the subsequent tests pmin=1. The active set SQP-algorithm can be made very efficient in medium-sized problems, since the Jacobian of the constraints in Kkneed not be updated continuously.The sparse gradient, Jacobian and Hessian are provided as input to the quadratic solver. In difficult MINLP-problems (1.1) can be augmented by the constraint(4.2)||y−yl||22≥1,1≤l<k,k=iteration count of the SQP algorithmThrough this constraint revisiting previously obtained integer assignments is avoided and the branch and bound tree correspondingly reduced [13].The initial solutionx0 is loaded in the parent node of the branch and bound tree. The parameters of the algorithm are accessed through a specific solver object containing the solution history {xk−2,xk−1,xk}, {λk−2,λk−1,λk}, {fk−r, …, fk−1, fk}, the history of deviations from the feasible space {Dk−r, …, Dk−1, Dk}, pointers to the active constraints, the objective gradient and the Jacobian, etc. (cf. [13]).In convex problems, increased stability of the solution process is frequently achieved through outer approximation (cf. [52,53]). Linear approximation (SLP) can be invoked as a complement to SQP. If the profile of the active constraints stabilizes and stays unchanged during the last N iterations and ifxkis feasible, it is reasonable to assume that the current solution is close to a local optimum and that a switch to SQP will be successful. The process then continues with pure SQP to achieve quadratic or super-linear convergence ([9], pp. 556–560). This procedure is tested in the numerical section.The line search module monitors a switch board similar to that for direction finding, through which the preferred line search method and merit function are invoked. A second-order correction of the search direction is performed ([9,54], pp. 443–444). The corrected direction is used if the penalized objective is superior to fk+pDk, where p is calculated by the penalty function {linear, quadratic, concave, convex, sigmoid, constant} as selected for the test. Exler et al. [13] presented encouraging evidence in MINLP-problems when using second order correction to prevent an undesired increase in the merit function value along Δxk(the Maratos effect).Constraint redundancy and – especially in box constrained MINLP problems – dimension reduction possibilities need to be addressed for algorithmic efficiency. Sophisticated algorithms have been designed for detecting redundant constraints in linear and quadratic programming problems (cf. [55]). When traversing the branch and bound tree, some variable dimensions will ultimately collapse to fixed values as long as the nodes are feasible and unfathomed. Through these fixations the local optimization problem can be simplified, with the effect of increased accuracy and speedup of the algorithm. Variable locking is checked at each node before formulating the local problem.The bisection method is a simple and robust root-finding method that bisects an interval repeatedly and then selects a subinterval in which a root or minimum must lie for further processing. A drawback of the method is its linear or relatively slow convergence. In this section we introduce a recursive procedure where the bisection method is applied to M multiple non-overlapping line segments. While the local bisection procedures still have a linear convergence rate, the linear convergence will be improved by a factor of (1/M) through the use of parallel processors. This possibility is of importance in cases with computationally intensive merit functions.The multi_thread() line search procedure based on multi-sectioning (recursive bisections of parallel line segments) and re-centering is presented below. Some frequently used line search algorithms have been included as support libraries to the system and compared with the multi-sectioning procedure: quadratic and cubic line search [32] and Armijo search [29].The following merit functions and their gradients have been included in the algorithm: the linear, the standard augmented Lagrangian [9] and the augmented Lagrangian as specified in [51]. The linear (L1) and augmented Lagrangian (LA) merit functions are among the most frequently used (cf. Appendix B, Section B.2). The merit functions usually augment the objective function with some form of penalty for deviations from the feasible space. In the subsequent tests we have used L1. A generic merit function is presented in (5.1).(5.1)ϕkC(α)=ψC((xk,λk)+α(Δxk,Δλk)),α∈[0,1]where C identifies the chosen merit function and α is the optimal step size in the direction (Δxk, Δλk).Recursive bi-sectioning is a well-known approximation method. The method consists of two phases. In the first, an appropriate starting interval is determined using the gradient of the merit function. In this process, the search interval may be set inside or outside the [0,1]-interval. In the second phase, the initial interval is repeated recursively by bi-sectioning, where the lower/upper limits of the new interval are selected based on the gradient of the merit function. The process continues until a convergence criterion is met, i.e. the search interval is sufficiently small, i.e.αupper−αlower≤ε,the directional derivative of the merit function is sufficiently small or the number of iterations reaches a preset upper limit [56]. In its basic form, the bisection procedure is applicable to convex problems.Consider problem PQdefined in (4.1). Since Δxis feasible, the equality constraints are satisfied for any step length: ∇gj(x)TΔx=0⇒α∇gj(x)TΔx=0|j∈E, α∈ℜ. For the inequality constraints, we obtain the limit(5.2)αmin∇gi(x)TΔx+gi(x)≥0⇒αmin≥max−gi(x)∇gi(x)TΔx∇gi(x)TΔx≠0,i=0,...,m−1.In convex/regular cases where negative (correction) steps are excluded, the RHS of (5.2) can be augmented by the sign condition −gi(x)∇gi(x)TΔx>0.The box constraints imply the below step limits (feasibility of the current solution and the direction with respect to the box constraints follows from (4.1)):(5.3)x+αΔx∈[LB,UB]⇒LBj−xjΔxjΔxj<0≤α≤UBj−xjΔxjΔxj>0,j=0,1,...,n−1.(5.4)LetLBmax=maxLBj−xjΔxjΔxj<0,UBmin=minUBj−xjΔxjΔxj>0,j=0,1,...,n−1.From (5.2)–(5.4) we obtain:(5.5)max(αmin,LBmax)≤α≤UBminWe introduce the following notation: M, number of sub-intervals within S0=[max(αmin, LBmax), UBmin]; T, number of parallel threads βi, i=0, …, M are the end points in S0,(5.6)β0=max(αmin,LBmax),βM=UBminβi=M−iMβ0+iMβM,i=1,…,M−1.The midpoints of the M sub-intervals represent the parallel threads. If M=2, there is only one thread and the procedure reduces to ordinary bisection. We wish to probe the T threads for the most promising or dominating segment within which we expect to find the optimal step length. This is equivalent to bisecting M consecutive non-overlapping line segments within S0 and evaluating the merit function f at the corresponding midpoint.The minimum f over the midpoints determines the dominating segment Sk. The current best step size is determined from the end points of the dominating segment:(5.7)Sk=argminifβi−βi−22,i=2,4,…,M(5.8)αk=argminβi(f(βi),i∈Sk)In the next iteration, the dominating segment Skis once again split into M sub-intervals and the new dominating segment is secured from the corresponding midpoints. The initial search interval S0 is reduced by 1/M in each iteration k. When no a priori information is available, each search interval Skis split into M sub-intervals of equal length. The dominating segment is recursively split into sub-intervals until a convergence criterion is met. The multi-section search will approximate the optimal step length. However, if the current best step αkis close to an end point, the numerical precision of the approximation can be significantly improved by re-centering Sk+1 at αksuch that Sk+1∈S0.The multi-sectioning algorithm with re-centering is summarized in the pseudo code below.Algorithm multi_thread():Step 1determine the step limits ((5.3)–(5.5)). Split the search interval S0 into M sub-intervals and evaluate the chosen merit function f at the end points of the sub-intervals. Set k=0.Step 2split the leading sub-interval Skinto M sub-sections and secure αkfrom the point set defined in (5.7)Step 3ifmin(αk*−LBmax,UBmax−αk*)<ε,re-center the leading sub-interval at the superior step within S0.Step 4Let k=k+1 and repeat from step 2 until a convergence criterion is met, e.g. using the Armijo-Goldstein condition [9].When using parallel threads, gradient information of the merit function is not necessary, hence the procedure is applicable to non-differentiable/non-convex problems. In the numerical tests, we use two parallel threads, i.e. M=4 and T=M/2=2.A modest set of NLP- and MINLP-problems is considered below. The NLP-problems are taken from Hock and Schittkowski [10] and the COPS collection of [57]. The Hock-Schittkowski problems are classified as irregular with an unknown optimal solution, whereas the COPS-problems are large-scale and difficult. For the COPS-problems, a comparison to results obtained by FILTER [36], KNITRO, LOQO [37], MINOS [38], and SNOPT [58] is provided. The MINLP-problems are taken from the collection of [12] and from the literature. The obtained results are compared to those reported in the sources. Binary and Gray coding are illustrated in small test problems on parallel processors. A problem containing a huge number of disjunctions is solved on a large set of parallel processors. The approach is directly applicable to MINLP-/GDP-problems where the discrete sub-space is limited and useful especially if the continuous search space is huge (cf. [59]). Considerable testing is needed in future research for further corroboration and development.The Hock-Schittkowski collection contains some irregular problems [11], out of which four problems are studied below. The structure of the problems, the best known result (fEXACT) and the optimal solution obtained by SQP+multi-sectioning are presented in Table 6.1. Simulations where the starting point is generated randomly within the box constraints are summarized in Table 6.2for three algorithms. In Table 6.3we present the results of invoking the search from a feasible initial point determined with GHA. The pseudo random number generator was initialized with a fixed seed to allow replication of the simulations. In all tests 100 runs were conducted using the Monte Carlo routines of GHA. Because of the low variation in the results, a larger number of iterations was not considered necessary.The test problems are presented briefly below.TP67: irregular problem with a non-linear objective and non-linear constraints.(6.1)minf(x)=−.063y2(x)y5(x)−5.04x1−3.36y3(x)−.035x2−10x3s.t.yi+1(x)−ai≥0,i=1,...,7ai−yi−6(x)≥0,i=8,...,141.0e−5≤x1≤2.0e3,1.0e−5≤x2≤1.6e4,1.0e−5≤x3≤1.2e2The functions yi(x) and the constants aiare defined by elaborate subprograms presented in Appendix A of Hock and Schittkowski [10].TP73: irregular problem with a linear objective and linear/non-linear constraints(6.2)minf(x)=24.55x1+26.75x2+39x3−40.50x4s.t.2.3x1+5.6x2+11.1x3+1.3x4−5≥012x1+11.9x2+41.8x3+52.1x4−21−1.645(.28x12+.19x22+20.5x32+.62x42)1/2≥0x1+x2+x3+x4−1=00≤xi,i=1,...,4TP85: irregular problem with a non-linear objective and linear/nonlinear constraints(6.3)minf(x)=−5.843e−7y17(x)+1.17e−4y14(x)+2.358e−5y13(x)+1.502e−6y16(x)+.0321y12(x)+.00423y5(x)+1.0e−4c15(x)c16(x)+37.48y2(x)c12(x)−.1365s.t1.5x2−x3≥0y1(x)−213.1≥0405.23−y1(x)≥0yj−2(x)−aj−2≥0,j=4,...,19bj−18−yj−18(x)≥0,j=20,...,35y4(x)−.28/.72y5(x)≥021−3946y2(x)/c12(x)≥062212/c17(x)−110.6−y1(x)≥0704.4148≤x1≤906.3855,68.6≤x2≤288.88,0≤x3≤134.75193≤x4≤287.0966,25≤x5≤84.1988{yj(x),cj(x),aj,bj}are defined in Appendix A of Hock-Schittkowski(1981)..TP87: irregular problem with a non-linear objective and non-linear constraints(6.4)minf(x)=f1(x)+f2(x)f1(x)=30x1,0≤x1<30031x1,300≤x1≤400f2(x)=28x2,0≤x2<10029x2,100≤x2<20030x2,200≤x2<1000s.t.300−x1−1ax3x4cos(b−x6)+cadx32=0−x2−1ax3x4cos(b+x6)+cadx42=0−x5−1ax3x4sin(b+x6)+caex42=0200−1ax3x4sin(b−x6)+caex42=00≤x1≤400,0≤x2≤1000,340≤x3≤420,340≤x4≤420,−1000≤x5≤1000,0≤x6≤.5236,a=131.078,b=1.48577,c=.90798,d=cos(1.47588),e=sin(1.47588)The multi-sectioning line search procedure is robust in solving these problems. The deviation from the feasible space is smaller than 1.0e−9 in all tests with multi-sectioning.Test problems 67 and 87 are solved in two stages. In the first, a feasible point is extracted, with the objective gradient replaced by the negative unit vector −e[58]. In the next stage, the NLP is solved using the original objective and gradient.[11] specifies the following box constraints for test problem 87:LB=[0.0, 0.0, 340.0, 340.0, −1000.0, 0.0];UB=[400.0, 1000.0, 420.0, 420.0, 10000.0, 0.5236].The algorithm SNOPT requires significantly tighter box constraints for this problem. Therefore, we do not include SNOPT in the simulations with random starting points for the problem. Multi-section step search generates at least as good solutions as the other node solvers in three out of four cases and is stable in all tests. In test problems 67 and 85 NLPQLP and SNOPT are unstable, i.e. the convergence of these algorithms is significantly dependent on the starting point. The tests with irregular NLP-problems indicate that step search with multi-sectioning is relatively stable and still generates as least as good solutions as the established codes.The COPS-collection contains numerous challenging test problems. Below we present two difficult box-constrained problems from this collection using a single processor. The objective is to demonstrate the robustness of the algorithms connected as support libraries to GHA in large-scale problems. The characteristics of the test problems are presented in Tables 6.4 and 6.6 and the solutions in Tables 6.5 and 6.7. The results indicate that the mathematical programming support libraries of GHA are robust in large-scale industrial optimization problems, encouraging extensive testing in future research to corroborate the evidence.(i)The largest small polygonThe finite element approximation of the largest small polygon problem is written as follows:maximize polygon_area:(6.5)f(x)=max0.5∑i=1nvri+1risin(θi+1−θi)subject to0≤ri≤1,i=1,...,nv−1,rnv=00≤θi≤π,i=1,...,nv−1,θnv=πθi≤θi+1,i=1,...,nv−1ri2+rj2−2rirjcos(θj−θi)≤1,i=1,...,nv−1,j=i+1,...,nv.The largest small polygon problem is known to be irregular for even n[60]. The maximal area is expected to converge to the area of a unit-diameter circle π/4∼0.7854 as nv→∞. The test results are summarized in Tables 6.5 and 6.6.The sources of time absorbance for the polygon problem are specified in Table 6.6. When the problem size increases, the computation of numerical Hessian and the QP-solver dominate and absorb roughly the same CPU-time.The minimal surface with a plate obstacleThe finite element approximation of the surface problem is written as follows:(6.6)f(x)=min∑i=0nu∑j=0nwc1+(xi+1,j−xi,j)hu2+(xi,j+1−xi,j)hw21/2+∑i=1nu+1∑jnw+1c1+(xi−1,j−xi,j)hu2+(xi,j−1−xi,j)hw21/2subject tox0,j=xnu+1,j=0,j=0,...,nw+1,xi,0=xi,nw+1=1−(2ihu−1)2,i=0,...,nu+1,xi,j≥0,i=0,...,nu+1,j=0,...,nw+1,xi,j≥1,floor0.25hu≤i≤ceil0.75hu,floor0.25hw≤j≤ceil0.75hw,hu=1(nu+1),hw=1(nw+1),c=0.5huhw.For this problem, the analytical gradient and Hessian become necessary for increasing nuand nw. The elements of the analytical gradient and Hessian involve terms in the neighborhood of xij(cf. [74]). The theoretical expressions were checked by comparing the analytical gradient and Hessian to the numerical estimates. Analytical, centered and forward differences were computed in one iteration of KKT_QL in each case of Table 6.6. The maximum difference over all elements between the analytical and numerical derivatives was recorded. For the gradient, the maximum difference between analytical and forward derivatives was <2.0e−7. The centered derivatives differed from the analytical by less than 5.0e−10. For the Hessian the maximum differences were 0.0 (analytical-forward) and <3.0e−5 (analytical-centered) respectively. We can therefore safely assume that the analytical derivatives of the first and second order are programmed correctly for the test problem. The analytical form is analogous in many box-constrained problems presented in the COPS-collection of [57] and hence useful in future testing. The heap memory was checked in the case nu=nw=14 using Valgrind that indicated perfect memory usage.In numerical estimation, the function (6.6) requires summation over all (nu+1)×(nw+1) terms respectively for each xij, while the analytical expressions involve only 8 terms for the first and second derivatives at xijand its neighborhood. The optimization problem is dense in the gradient but increasingly sparse in the Hessian when the problem size increases. For example, in the cases nu=nw=50 and 150, the fraction of non-zeros is 0.27% and 0.03% respectively.In this problem, many SQP-algorithms based on solving the local Karush–Kuhn–Tucker (KKT) conditions encounter time absorbance issues when the problem size increases. Because of time absorbance of the ql-solver in larger problems, we confined the test of KKT_QL to the case nu=nw=50. Many algorithms apply interior point methods in large-scale problems. We included a variant of the conjugate projected gradient algorithm of Carpenter and Shanno [61] as a callable library (CPG) to minlp_machine() for large-scale testing. Alternative interior point methods can be included in the platform conveniently in future research. Since the problem has only box constraints, the projection is the identity matrix. Hence the projection calculation is automatically deactivated in CPG. The authors suggest a pre-conditioner C=diag(H)1/2 to be applied to H for numerical stability. In this study we use the exponential barrier function(6.7)μLBi=aexp(m(LBi−xi)),dμLBidxi=−m(LBi−xi)μLBiμUBi=aexp(−m|xi−UBi|),dμUBidxi=−m|xi−UBi|sign(xi−UBi)μUBiwherei=1, …, n. The tuple{μLB,μUB} is illustrated in Fig. 6.1in the case{μCPG=0.1,a=1,LB=−1.0e+3,UB=−LB}.We also included a procedure based on the exact solution of the KKT-conditions using symmetric indefinite factorization (LDL). The factorization applies the SAXPY-operation with bounded Bunch and Kaufman [62] pivoting. The distinctive feature of the procedure is pre-conditioning the Hessian of (6.6) and fast backward solution of the optimal direction atxk. The optimal value of the tuple {a,μCPG} can be determined using geno-mathematical programming on parallel processors (cf. [18]). The test results are summarized in Table 6.7. The methods in problems 1–3 have converged to slightly higher solutions than the other methods.In this section we consider eight MINLP-problems. Four of them are used in tests with parallel processors. In seven problems step search with multi-sectioning is used. In one problem genetic search is used. All reported solutions are feasible. All results presented in this section were solved on a Linux Intel® Xeon® CPU E5420 2.50GHz computer. Other MINLP-tests using GHA and non-linear solvers through the switch board on single and parallel processors are reported in Östermark [24], Östermark [50]. The characteristics of the test problems are presented in Table 6.8and the solutions in Table 6.9.Test problems 1–7 were chosen to provide preliminary comparisons to the MISQP and BONMIN algorithms [12,13]. Problems 6–7 are considered very difficult and require additional scaling of the objective function [12].The test problems are presented briefly below.TLN6 is an instance of a trim loss problem containing bivariate terms in the non-linear constraints [63]. This MINLP-problem has 6 binary variables, 6 integer variables with [LB,UB]=[0,16] and 36 integer variables with [LB,UB]=[0,5]:(6.8)minf(n,m,y)=∑j=1Jcjmj+Cjyjs.t.∑i=1Ibinij−Bmax≤0−∑i=1Ibinij+Bmax−Δ≤0∑i=1Inij−Nmax≤0yj−mj≤0mj−Myj≤0ni,o−∑i=1Imjni,j−Nmaxnij,mj∈Y+,y∈j[0,1],I=6,J=6.The search space of the integer variables encompasses 26×176×636=1.5934e+37 Cartesian combinations in total. Known feasible integer assignments have been avoided in the branching process by adding the constraint (4.2) to the DFP (4.1).Alan is a portfolio optimization problem with a non-linear objective and linear equality/inequality constraints. The discrete variables are binary-valued. The objective function to be minimized contains bivariate terms in the continuous variables:(6.9)f(x)=x1(4x1+3x2−x3)+x2(3x1+6x2+x3)+x3(3x2−x1+10x3)Synthesis3 is a process synthesis problem with a non-linear objective, 19 linear and 4 non-linear constraints. The discrete variables are binary-valued. The objective function contains logarithmic and exponential terms in the continuous variables:(6.10)f(x,y)=∑i=1nyaiyi+∑i=1nxbixi+ex1+e0.833333x2−65log(x3+x4+1)−90log(x5+1)−80log(x6+1)+120The coefficients in (6.10) are defined in Duran and Grossmann [64]. The four non-linear constraints contain logarithmic and exponential terms:(6.11)−1.5log(x5+1)−log(x6+1)−x8≤0−log(x3+x4+1)≤0exp(x1)−10y1−1≤0exp(0.833333x2)−10y2−1≤0Batch is a production scheduling problem. Originally, the optimal design of multiproduct batch plants is a non-convex problem [65]. A convex transformation involving binary-valued discrete variables is stated in Kocis and Grossmann [44] as the following MINLP:(6.12)minf(n,v,tL,b,Y)=∑j=1Mαjexp(nj+βjvj)s.t.vj≥ln(Sij)+binj+tLi≥ln(tij)∑i=1NQiexp(tLi−bi)≤Hnj=∑k=1NjUln(k)Ykj∑k=1NjUYkj=1,Ykj∈{0,1}0≤nj≤ln(NjU)ln(VjL)≤vj≤ln(VjU)ln(TLiL)≤tLi≤ln(TLiU)ln(BiL)≤bi≤ln(BiU)i=1,...,N;j=1,...,MIn the below test, the same problem parameters have been used for this problem as in Leyffer [66].Braak3 is a difficult and badly scaled MINLP-problem. All constraints are linear inequalities, but the objective function is highly non-linear in both the continuous and discrete variables:(6.13)f(x)=100(y1(2y1+y2)+y2(y1+2y2)+y32)+a(|y1|+|y2|+|y3|)+12|y1y2|+12|y1y3|−12|y2y3|−exp(0.01(x1−y1)2)+(1.25x2−y3)4+100x32+100x42−50≤xi≤50,−50≤yj≤50,i=1,...,4,j=1,...,3Dirty is difficult and badly scaled problem, where the non-linear objective and linear inequalities are defined using a large number of constants defined in [12]. The objective function involves both binary-valued and continuous variables in summation terms of the form:(6.14)f(x)=∑j=1nxxj∑i=1nxcixi+∑i=1nyhiyi+∑j=1nyyj∑i=1nxcixi+∑i=1nyhiyiCrop100 is a redundancy optimization problem stated as an INLP-problem with non-linearities in the objective to be minimized and the three constraints [67]:(6.15)f(y)=−log1−1−∑i=1ny0.8+0.18riyig1(y)=∑j=1nya1jyj2≤b1g2(y)=∑j=1nya2j(yj+exp(δjyj))≤b2g3(y)=∑j=1nya3j(yjexp(τjyj))≤b3yi∈[0,5],{δiτi}⊆[0,0.01]∀i.The parameters are specified for the test instance in Schittkowski [12].QAP256 is a convex version of the quadratic assignment problem [68]. It has a quadratic objective function, a single linear equality constraint and 256 binary-valued variables:(6.16)minf(y)=yTHys.t.eTy=92His the Hessian matrix defined for the QAP256 problem instance.The best known solution for this problem is f(y*)=1.0278e+2. The integer gap of the search space is 2.03%. The problem requires hundreds of CPU-hours to solve with a conventional branch-and-bound strategy on a single processor. When invoking SQP+multi-sectioning on this problem from a pseudo random starting point with a fixed seed, we obtained the feasible MINLP-solution f(y)=1.4645242e+2 at node 327 of a depth-first branch and bound tree. Node 627 returned the feasible MINLP-solution f(y)=1.4567726e+2, hence the processing of 300 nodes improved the objective value with only 0.53%. Corresponding results were obtained with the node solvers NLPQLP and SNOPT.Exler et al. [13] recommend the L1-norm for binary variables in order to cut off previously visited points in the Cartesian hypercube. Our experiments with test problems TLN6, Alan and Synthesis3 do not support this specification unequivocally. We have therefore used the L2-norm for all integer variables in TLN6. More theoretical and experimental evidence is needed to indicate precisely when one norm is preferable over another.To solve TNL6, we apply sequential linear programming (SLP). When the system stabilizes, SQP with multi-sectioning is applied (cf. Table 4.1). The optimal solution was reached by node 1754. When using SNOPT as the node solver, the optimal solution was obtained in node 3456. In comparison, Exler et al. [13] report the result f(x)=16.3 with constraint violation 0.65e−1 using only 1615 function evaluations in the MISQP algorithm. The BONMIN algorithm of [7] required 3679605 function evaluations to reach the solution f(x)=16.4 (the branch and bound tree is not reported by Exler et al. [13]). Exler et al. [13] used the unit vector as the starting point for this problem. For Crop100 the MISQP-solution is f(x)=1.70283, with constraint violation 0.86e−1. The sensitivity of BONMIN and MISQP to the initial point is not reported.The large MINLP-problems Crop100 and QAP256 are solved using random relaxations of the box constraints around the current best MINLP-solution. For Crop100 an initial feasible MINLP-solution is extracted using a branch and bound search with at most 200 nodes in the tree. For QAP256, the initial solution is generated as a random binary vector satisfying the single constraint∑i=1nyi=92.In the next stage, a five-step procedure is applied until convergence [69]:Random relaxation procedure 6.1:1.Lock the box constraints at the feasible MINLP-solutionzk=(xk,yk).Relax the box constraints for ni≤nyrandomly selected variables to the original bounds.Compute the MINLP-solution with the other box constraints fixed atzk.Update the current best solution.Repeat from step 1 until convergence.Procedure 6.1 is repeated in 10 and 24 cycles respectively in Crop100 and QAP256. QAP256 was tested with different sizes of niin {8, 10, 11, 16}. The optimal solution varies between [103.5, 103.9]. The optimal results are summarized in Table 6.10, with ni=10 and 11 respectively in Crop100 and QAP256. The number of cycles through procedure 6.1 is 10 and 32 respectively. Schittkowski [12] reports f(x)=1.70283e+0, Dev=0.83 for Crop100 obtained with MISQP.(i)Test problems {Alan, Synthesis3, Batch}The test problems were solved on Cray XC30 using different mesh sizes to study the impact of binary and Gray coding on the branch and bound process in small and medium sized MINLP-problems. These test problems are included for illustrative purposes. The problems are solved very fast already with one processor using conventional branch and bound. The optimal results are presented in Table 6.11.The portfolio problem Alan is solved to optimum with 16 parallel processors, when using binary transformation of the processor ID (cf. [50]). Since this is an MINLP with 4 binary variables, one of 24=16 processors will solve the problem, with the binary variables fixed at the encoded node ID. The binary code of processor 11 (1011) matches the optimal solutiony*, hence this processor solves the MINLP-problem as an NLP with nx=4 (cf. Remark 1). The inverted Gray-code G(4,2) of processor 13 matchesy*, hence this processor solves the problem under a Gray(n,k) transformation of the processor ID (cf. Remark 2), exactly as processor 11 under a binary transformation. When the optimal solution is found, the processor broadcasts an early interrupt signal asynchronically to the mesh and waits for further instructions from the root (or cluster root). The current best solution is delivered from each node to the root for rank ordering and mesh output. If the mesh size is huge, then reduced summary output will be produced, encompassing the node ID of the processor having found the solution, the solution itself, the CPU-time and related parametric information.Synthesis3 is solved as a reduced NLP with nx=9, using 256 parallel processors. The binary code of processor 85 and the Gray-code G(8,2) of processor 102 match the integer solution exactly (note that the numbering starts from 0 in C programming and that the Gray code is inverted as explained in ‘Binary and Gray coding’):y=y85BIN=y204G(8,2)=[01010101]We also solved this problem with the 2 last bounds ofyincreased from [0,1] to [0,2]. In this case, the Gray-code consists of two concatenated codes for the corresponding sub vectors: G(6,2)|G(2,3). The cycle length of the first sub vector is 26=64 and for the second 32=9: the first sub vector is repeated cyclically 4 times through the sequence {0, …, 255} and the second almost 29 times. The cycles do not coincide, hence a complete mapping of all 256 Cartesian combinations is obtained by running the cyclic Gray-algorithm through all decimal values [0, …, 255]. The concatenated Gray code of processor 217 matchesy*, hence this processor solves the MINLP as a reduced NLP without a branch and bound tree.The search space of the binary-valued MINLP-problem of [44] has 224=1.6777216e+7 Cartesian combinations, hence more than one combination needs to be allocated to each processor to solve it concurrently on 2000–4000 processors. However, the structure of the Batch-problem reveals that the first four binaries affect four equalities only and can be zeroed without cutting off the global solution from the search space. This reduces the search space by 93.75% to 220=1.0485760e+6 Cartesian combinations. We encode the processor ID up to the available mesh size (2048) for this job on Cray XC30 at CSC and apply it to the bounds onystarting from position 5. Thus, the first four positions of [LB,UB] fory(starting from the left) are zeroed for all processors and the next 11 positions are fixed at the corresponding Gray-code Gi(11,2) for each node i=0, …, M-1. The bounds for the remaining 9 binary variables are left untouched.With 16 parallel processors, the shifted Gray-code enables processor 10 to solve the problem with only 7 nodes in the branch and bound tree, a dramatic reduction by 86.27% from 51 nodes required by a single processor. With the mesh size 2048, the shifted Gray-code of processor 1342 matchesy* exactly, as indicated by the stems below (the first four positions are zeroed for all processors)y*=[0y1342G(11,2)0]=[0000|11110100001|000000000]∈Y24.Thus, processor 1342 solves the Batch-problem as a reduced NLP with nx=22.Test problem QAP256The QAP256-problem is solved on one processor by branch and bound using the random relaxation procedure 6.1. We generalize procedure 6.1 for parallel processors using the Gray code:Random relaxation procedure 6.2:1.Set M=log(MESH_SIZE)/log(2). Lock the M first integer variables at the Gray code of the processor ID.Lock the box constraints from the first M integers onward at the feasible MINLP-solution.zk=(xk,yk).Relax the box constraints for ni≤ny-M randomly selected variables i ∈ {M+1, …, ny} to the original bounds.Compute the MINLP-solution with the other box constraints fixed atzk.Update the current best solution.Repeat from step 2 until convergence.The results are reported in Table 6.12for the case ni=11, with 24 cycles through procedure (6.2). With 16 parallel processors, M=4 integers are locked at the Gray code G(2,4). The reduction has a significant impact on the local search trees. The average tree size per cycle of procedure 6.1 is 885 nodes for the single machine. A comparison between single and parallel processor runs is presented in Fig. 6.2. f1 denotes the evolution of the objective function for the single processor on Cray XC30 and f14 correspondingly for node 14 of 16 parallel processors. The solutions for all processors vary within [1.03168e+2, 1.04104e+02]. The algorithm converges to f=1.04617e+2 after 5 cycles through procedure 6.1 with a single machine, as indicated by the dashed vertical line in Fig. 6.2. With parallel processors, the Gray code applied in procedure 6.2 reduces the average tree size per processor to 207 nodes, i.e. by 76.7%. Without Gray code reduction, the computational task for each parallel processor is approximately the same as for the single machine. The results imply that the difficult QAP-problem is solved close to the best known solution efficiently using parallel processors and random relaxation. The distance between the parallel solution and the best known solution is roughly 20% of that between the single processor solution and the best known solution. Fig. 6.2 indicates that the improvement of the objective function terminates very quickly in single processor search compared to parallel search. In parallel search a significantly better solution is obtained almost in the same time as the best solution obtained by a single processor.We also solved QAP256 with GHA using genetic search on single and parallel processors. The results are summarized in Table 6.13. The best known result is approached when increasing the mesh size, yet the improvement is slow: the objective function improves by only 0.19% when increasing the mesh size from 1 to 1024. The number of function calls varies between [3.14e+3, 8.96e+5] per processor. Scalability has not been considered when parameterizing the tests.We conclude that by applying branch and bound or genetic search on concurrent processors, this very difficult assignment problem can be solved to near-optimum in reasonable time.Propositional logic provides a framework for solving discrete optimization problems. Such problems can be modeled mathematically by representing logical relations as linear inequalities. Grossmann and Biegler [3] discuss disjunctive programming and constraint programming as recent trends in logic based methods. Generalized disjunctive programming (GDP) was proposed by Raman and Grossmann [70] to cope with logic based programming. In disjunctive programming subsets of the constraints are activated or deactivated in the optimization problem depending on the values of Boolean variables. If a Boolean variable Diis true in a disjunctive sentence, then the i:th set of constraints is active in the optimization problem. The disjunctions are mutually exclusive: one and only one set of constraints in a disjunctive sentence can be activated. The relationships between the disjunctive sets are controlled by logical conditions Ω(Di) (AND, OR, IMPLICATION, etc.). For example, the logical statement {a OR b} is represented mathematically as ya+(1−y)b=1, y∈{0,1}. Analogously, the disjunction {x1−x2≤4 OR x1−x2≤0} is represented as: x1−x2≤(1−y)4, y∈{0,1}. Even if y=1 represents a more constrained space included in the half space x1−x2≤4, one condition does not imply the other. The logical conditions can often be utilized to simplify the problem formulation, as shown below.Every GDP can be represented as an MINLP and vice versa. However, MINLP-representations of disjunctive problems entail an increase in dimensionality and computational complexity through the binary variables (e.g. [71]). Hence, there is a need for geno-mathematical models that can simplify the GDP-formulation on parallel processors. We consider two GDP examples below. The discussion illustrates the possibilities offered through a parallel setting, large-scale GDP-problems need to be tested and compared in future research.The first problem is example 1 in Grossmann and Lee [72]:(6.17)minf(x)=c+x12+x22s.t.(x1−2)2−x2≤0Y1x1−2≥0x1−x2≤4c=1∨Y2x1−x2≤0x1−1≥0x2−1≥0c=1.5∨Y3x1−x2≤4x1−1≥3x2−1≥0c=1.50≤x1,x2≤4,c∈ℜ1,Yj∈{true,false},j=1,3.The disjunctions are represented by binary variables. The mathematical program can be written as a big-M formulation, for example. The resulting MINLP-problem has a non-linear objective, one non-linear constraint and 6 linear constraints in 5 variables (nx, ny)=(2, 3), where nyare binary:(6.18)minf(x)=y1+1.5y2+0.5y3+x12+x22s.t.(x1−2)2−x2≤0x1−2y1≥0x1−x2−4(1−y2)≤0x1−(1−y1)≥0x1−y2≥0x1+x2≥3y3y1+y2+y3=10≤x1,x2≤4,yj∈{0,1},j=1,2,3.The problem is solved to optimum using SQP with multi-sectioning. The branch and bound tree consists of 10 nodes on a single processor. With 2ny=23=8 parallel processors, the problem is solved as a reduced NLP with nx=2 andyfixed at the corresponding Gray code. The optimum is obtained by processor 3, without a branch and bound tree:x*|y*=[x*|y3G(3,2)]=[11|y3G(3,2)=(010)],f(z*)=3.5The second problem is example 4 in Grossmann and Lee [72] (adapted from [64]), having five sets of disjunctions. This problem is considered in more detail below.GDP-problem 2(6.19)minf=∑i=15ci−10x3−15x5−15x9+15x11+5x13−20x16+exp(x3)+expx51.260ln(x11+x13+1)+140s.t.−ln(x11+x13+1)≤0,−x3−x5−2x9+x11+2x16≤0,−x3−x5−0.75x9+x11+2x16)≤0,x9−x16≤0,2x9−x11−2x16≤0,−0.5x11+x13≤0,0.2x11−x13≤0Y1exp(x3)−11≤0c1=5∨¬Y1x3=0c1=0,Y2exp((x5/1.2)−11)≤0c2=8∨¬Y2x5=0c2=0,Y31.25x9−10≤0c3=6∨¬Y3x9=0c3=0Y4x11+x13−10≤0c4=10∨¬Y4x11=x13=0c4=0,Y5−2x9+2x16−10≤0c5=6∨¬Y5x9≥x16c5=0Ω1:[Y1∧¬Y2]∨[¬Y1∧Y2];Ω2:¬Y4∨¬Y5,x∈ℜ6,a≤x≤b,a=(0,0,0,0,0,0),b=(2,2,2,∞,∞,3)Yj∈{true,false},0≤cj,j=1,2,...,5.Solution reported by Grossmann and Lee [72]:f*=73.04, Y*={false, true, true, true, false},x*=(0, 2, 1.078, 0.652, 0.326, 1.078).We note an imprecision in the GDP formulation of [72]. The third linear constraint x9−x16≤0 and the constraint x9≥x16 in ¬Y5 imply that x9=x16, if the last disjunction is false and supplementary. We assume, however, that if the last disjunction was intended to be supplementary, the constraint would have been written out directly as equality. We therefore interpret this condition as complementary under ¬Y5.Consider the logical condition Ω2. Since the disjunctions are mutually exclusive, ¬Y4⇔Y5 and ¬Y5⇔Y4. Thus, Ω2 forms ties that can be used to reduce the disjunctions from five down to three:(6.20)P0=Ω1:[Y1∧¬Y2]∨[¬Y1∧Y2]P1:Y3∨¬Y3P2:[Y4∧¬Y5]∨[¬Y4∧Y5]⇔Ω2:¬Y4∨¬Y5.By utilizing the logical conditions, the possible combinations emerging from the disjunctions reduce from 25=32 down to 23=8, i.e. by 75%. The size of the MINLP-formulation naturally simplifies correspondingly. The mapping from Pi∈{0,1}, i=0,1,2 to Yj, j=1,2, …, 5 follows from (6.20):(6.21)P0=0→[Y1∧¬Y2]P1=0→Y3P2=0→[Y4∧¬Y5]P0=1→[¬Y1∧Y2]P1=1→¬Y3P2=1→∨[¬Y4∧Y5].Instead of solving this small problem as an MINLP as Grossmann and Lee [72], we code the GDP using function pointer logic and Gray coding in the interface function add_minlp() of minlp_machine(). An array pointing to three separate functions is constructed. Each function activates the corresponding disjunction. For example, disjunction P0 is coded as follows:void disjunction_0(MINLP_ptr *MINLP_PROB,DVECTOR LB,DVECTOR UB,int G) {switch(G) {case 0:/* Y1(TRUE),Y2(FALSE) */c[0]=5.0;/* Y1 */c[1]=0.0;/* ∼Y2 */UB[0]=log(11.0);/* Y1 */LB[1]=UB[1]=0.0;/* ∼Y2 */break;case 1:/* Y1(FALSE),Y2(TRUE) */c[0]=0.0;/* ∼Y1 */c[1]=8.0;/* Y2 */LB[0]=UB[0]=0.0;/* ∼Y1 */UB[1]=1.2*log(11.0);/* Y2 */break;}/* end of switch() */}/* end of disjunction_0() */This construct is flexible and can be applied to disjunctions in general. The code can be used to solve the GDP geno-mathematically or by NLP using parallel processors. By specifying the GDP through function pointer logic, many large or huge problems with prohibitive MINLP-formulations can be simplified to manageable concurrent MINLP's or NLP's (cf. [20]). This cannot be achieved through direct conversion of a large GDP to an MINLP. GDP 2 is solved geno-mathematically using GHA as follows:Geno-mathematical GDP-procedure 6.3:Step 1specify a population of three-digit binary vectorsStep 2formulate the NLP using (6.3) and the binary vector for each individual in the populationStep 3solve the NLP using minlp_machine() through a call from the interface function evaluator() of GHA (cf. Table 4.1).Step 4repeat from step 1 until convergence.To illustrate, the population size is set to 16, the inner GA-iterations to 7 and the outer iterations (GA_runs) to 3. SQP with multi-sectioning solves GDP in 8 GA-iterations (0.1s) using one-point crossover and non-uniform mutation from a random vector of disjunctionsP=(Pj∈{0,1}, j=1,2,3) and a random vectora≤x0≤bon the Cray XC30 at CSC: fMULTI_THREAD(x*|P*=(1,0,0))=2.2797851e+01, Dev=0.0. For comparison, GDP is solved with 23=8 parallel processors, without genetic search. In this case, the algorithm is invoked only once from the interface function accelerator() of GHA. The results are presented in Table 6.14. This small problem is solved very fast (0.33s) with 8 parallel processors.The optimal disjunctionsP*={1, 0, 0}→{Y1, Y2, Y3, Y4, Y5}={false, true, true, true, false} coincide with those reported by Grossmann and Lee [72], yet their MINLP-solution is sub-optimal. Direct MINLP-formulations of large scale GDP problems involve an increased risk of sub-optimality, compared to geno-mathematical solutions extracted on parallel processors.If the inequality under Y4 is replaced with x11+x13−5≤0,Pis fixed at {1, 0, 0} andx0=0 is used as the starting point, then the SNOPT-algorithm converges at the solution reported by [72]:fSNOPT(x*|P=(1,0,0), x11+x13−5≤0)=7.303531605474e+1, Dev=8.8817842e−16x=(0, 2, 1.0783883e+0, 6.5201465e−1, 3.2600733e−1, 1.0783883e+0)Y*={false, true, true, true, false}.In this case, SQP+multi-sectioning, FSQP and NLPQP all generate the solution f (x*|P=(1,0,0), x11+x13−5≤0)=5.482373677767e+1, whether starting fromx0=0 or a random point.To check the stability of multi-sectioning with GHA search, we tested the algorithm simulations on a single processor using 100 random starting points (P0,x0) and the same parameterizations as in test 1 of Table 6.10. fMULTI_THREADconverges at 2.2797851e+01 throughout the runs (standard deviation 2.48690e−14), indicating stability of the system.To shed some light on the scalability of the system, 275 replica of the batch-problem were solved on each one of 1-4048 Cray XC30-processors, retrieving the best solution in each case. The overall best solution is determined by the root processor. Thus, with 4048 parallel processors, in total 275*4048 independent MINLP-problems are solved concurrently. The solution time with one processor solving 275 MINLPs is 58.4s and with 128/1024/2048/4048 processors each solving 275 problems 59.7/66.1/73.6/88.7s respectively. The time increase is due to the root assembling the results from the mesh and determining the overall best solution for the increased mesh. This time increase is approximately 7.5s for an increase of the mesh size by 1024 processors. The time absorbance for one processor doing the job of 4048 would be 58.4*4048/3600∼65.7 CPU hours.Let k, the number of MINLPs to be solved by each processor; tb, the solution time per independent MINLP-problem (∼0.212s); Δts, the time increase needed by the root to assemble the results when increasing the mesh size by 1024 (∼7.5s).The time requirement for a mesh of size m*1024, m>1 to solve k independent MINLPs per processors is tm=ktb+mΔts. For a mesh of size m/2, the time requirement for solving the same total amount of independent MINLPs is tm/2=2ktb+mΔts/2. An estimate of speedup is obtained from the ratio of these time requirements:(6.22)tm/2tm=2ktb+mΔts/2ktb+mΔtsThe computation time (ktb) of (6.22) becomes dominating when the work load per processor (k) increases, i.e.limk→∞tm/2tm=2.Thus, in computationally demanding MINLP and GDP-problems, scalability can be obtained even for a huge mesh size.The scalability of GHA was tested on the massively parallel Jugene supercomputer in Jülich, Germany in a preparatory PRACE-project. The maximum number of processors allowed for the project was 65536. A small non-convex MINLP-problem was extended through a huge number of disjunctions in the parameters and solved by GHA using parallel processors. The non-convex MINLP-problem includes a continuous variable x and a discrete variable y:(6.23)minf(x,y)=c1y−c2xs.ta11y2−a12y0.5−a13x0.5y2+a14y+a15x−b1≤0a21y+a22x−b2≤0a31y+a32x−b3≤0(1,1)≤(x,y)≤(10,6)The problem is non-linear and non-convex due to the bilinear inequalities. The original problem parameters are presented in Pörn et al. [73]. The authors consider a convex transformation of (6.23), expanding the problem in terms of both variables and constraints. Since the problem is small, it is solved directly in the non-convex form below using GHA.The problem is extended by using a 20–50% zone around the original parameter values. This zone is divided into an equally spaced grid of non-overlapping instances of problem (6.23). Each problem instance is solved using parallel processors. The results corresponding to the number disjunctions per processors (MESH_RUNS*GA-siblings*GA-iterations*GA-runs) are presented in Table 6.15:For this problem, the output was minimized to the overall best solution produced in the mesh, with early mesh interrupt deactivated. Inter-nodal communication occurs at each mesh run, when the current best solution is delivered to the root for ranking. In this test, the current best solution is not returned to the nodes from the root. We find that, even with minor inter-nodal communication allowed in a huge mesh, the scalability requirement defined for the Jugene supercomputer is achieved. By setting MESH_RUNS to one and adjusting the genetic parameters correspondingly in Table 6.15, the inter-nodal communication would be eliminated, spare from the final solution delivered to the root from the mesh. This would improve the scalability of the system correspondingly.The key contributions of the paper can be summarized into the following three: (i) a new parallelizable step search procedure (ii) Gray coding for breaking down medium sized MINLP-problems and (iii) random relaxation+parallel programming for tackling large scale disjunctive problems. The test results indicate how large scale mathematical programming problems can be simplified and solved as ordinary non-linear problems in many important cases through parallel programming.We present a new line search algorithm multi_thread() operating as a support library to GHA. We tested the procedure on a small set of irregular NLP-problems where the chosen nonlinear solver was activated through the switch board minlp_machine(). Some of the problems are declared very difficult in the literature. Simulations indicate that step search based on multi-sectioning and re-centering is stable.A modest set of difficult NLP-problems was solved on a single processor. By way of illustration, some MINLP-problems were solved with single and parallel processors. Binary or Gray coding can be used to simplify the problems significantly. The approach can be important in many practical problems in economics and engineering, where the problem size is huge, but the number of binaries limited.Two difficult large-scale MINLP-problems were solved by fixing the box constraints at the current best MINLP-solution and relaxing a subset randomly. Parallel processing with Gray coding improves the performance of the algorithm in large-scale problems. One of these problems (Crop100) was solved to a significantly better solution than the currently best known. The other problem (QAP256) was solved close to the best known solution (departure=0.38% with parallel branch and bound, departure=0.14% with parallel genetic search) in reasonable time (150 and 205 CPU minutes respectively). More testing is needed in future research, however.General disjunctive programming (GDP) problems can be simplified by Gray coding and parallel processing, sometimes considerably. Function pointer logic and intelligent switching procedures seem to provide a fruitful basis for solving high-order GDP problems, where the computational challenge in direct MINLP-formulations would be insurmountable. The scalability of the system was demonstrated by solving a problem with a huge number of disjunctions on a large number of parallel processors.Most of the test problems were solved to the best known solution or a better one. Further testing with large scale and difficult NLP/MINLP/GDP-problems is needed in future research. Preliminary comparisons to alternative powerful non-linear solvers are presented, yet more elaborate comparative studies are needed. New efficient algorithms could be connected to the platform and compared over a large set of test problems in future research. GHA allows communication between processors in different levels of the solution process. In problems where load balancing is critical, the list of jobs can be dynamically adjusted in order to employ idle processors. Early mesh interrupt enables load balancing in regular problems where the solution time for individual NLPs does not vary extremely and in situations where new tasks are fed to the processors continuously. We have shown that, as the problem sizes continue to grow, applying regularization schemes for example on the gradient and Hessian will soon become numerically intractable. For huge problems, the problem formulation itself has to be numerically stable and the corresponding analytical gradient and Hessian information provided.It is difficult to pinpoint the exact reasons for why different algorithms work better than others in different situations. The tested algorithms apply sequential quadratic programming in the directional search using identical derivative information in small or medium-sized problems: the same gradient and Hessian information is submitted to the tested algorithms in the SQP-iterations (Appendix B). Since the local QP-solver of KKT_QL is the same as in NLPQLP [29], the differences between these two algorithms are mainly attributable to the different line search procedures applied. Also, sparse matrix algebra techniques and the robust programming conventions applied in KKT_QL may yield more exact solutions especially in numerically challenging problems. The results are encouraging for future testing and extensions.

@&#CONCLUSIONS@&#
