@&#MAIN-TITLE@&#
A hybrid particle swarm optimization for feature subset selection by integrating a novel local search strategy

@&#HIGHLIGHTS@&#
A hybrid feature selection method based on particle swarm optimization is proposed.Our method uses a novel local search to enhance the search process near global optima.The method efficiently finds the discriminative features with reduced correlations.The size of final feature set is determined using a subset size detection scheme.Our method is compared with well-known and state-of-the-art feature selection methods.

@&#KEYPHRASES@&#
Feature selection,Local search,Correlation information,Particle swarm optimization,

@&#ABSTRACT@&#
Feature selection has been widely used in data mining and machine learning tasks to make a model with a small number of features which improves the classifier's accuracy. In this paper, a novel hybrid feature selection algorithm based on particle swarm optimization is proposed. The proposed method called HPSO-LS uses a local search strategy which is embedded in the particle swarm optimization to select the less correlated and salient feature subset. The goal of the local search technique is to guide the search process of the particle swarm optimization to select distinct features by considering their correlation information. Moreover, the proposed method utilizes a subset size determination scheme to select a subset of features with reduced size. The performance of the proposed method has been evaluated on 13 benchmark classification problems and compared with five state-of-the-art feature selection methods. Moreover, HPSO-LS has been compared with four well-known filter-based methods including information gain, term variance, fisher score and mRMR and five well-known wrapper-based methods including genetic algorithm, particle swarm optimization, simulated annealing and ant colony optimization. The results demonstrated that the proposed method improves the classification accuracy compared with those of the filter based and wrapper-based feature selection methods. Furthermore, several performed statistical tests show that the proposed method's superiority over the other methods is statistically significant.

@&#INTRODUCTION@&#
In recent years, with the advance of science and technology, datasets with large numbers of features and relatively few patterns were produced. A large number of irrelevant and redundant features may significantly degrade the accuracy of learned models as well as reduce the learning speed of the models. This problem is known as curse of dimensionality in data mining methods and increases the computational complexity of building the model. Feature selection is one of the most feasible solutions to reduce the dimensionality of the datasets by selecting the most informative features and still retains sufficient information for the classification task. The main idea behind the feature selection is to choose a subset of salient features, by eliminating irrelevant features with little or no predictive information, as well as redundant features that are strongly correlated. On the other hand, this reduction helps to speed up the learning process, leads to a simple and understandable predictor model and avoids overfitting [1–3]. Feature selection has been successfully applied to many fields such as text categorization [4,5], face recognition [6,7], cancer classification [8,9], gene classification[10], finance [11,12], recommender systems[13] and customer relationship management [14].To find the optimal feature subset one needs to enumerate and evaluate all the possible subsets of features. The entire search space contains all the possible subsets of features, which means that the search space size is 2nwhere n is the number of the original features. Therefore, the problem of finding the optimal feature subset is a NP-hard problem [15,16]. Thus, evaluating the entire feature subset is computationally expensive and also impractical even for a moderate-sized feature set. Therefore, many feature selection algorithms involve heuristic or random search strategies to find the optimal or near optimal subset of features in order to reduce the computational time. These types of feature selection methods can be categorized into filter [17–26], wrapper [27–37], hybrid [33,38–42] and embedded [43–45] approaches. The filter approach looks for features that maximize a criterion which does not rely on any specific learning model. The wrapper approach utilizes a given learning algorithm to evaluate a candidate feature subset and thus the feature selection process is wrapped around the learning model. Although the wrapper approach uses learning algorithms to evaluate a feature subset, it requires a high computational cost for high-dimensional datasets. On the other hand, the filter-based methods are typically faster than the wrapper ones, but due to the lack of a learning model in their search process, the quality of the final result will be low compared with those of the wrapper methods. Therefore, the goal of the hybrid-based methods is to use the computational efficiency of the filter model and the proper performance of the wrapper model. However, the hybrid model may suffer in terms of accuracy because the filter and wrapper models are considered as two separate steps. Finally, the embedded approach seeks to subsume feature selection as part of the model building process and is thus associated with a specific learning model.The wrapper approach uses a predetermined learning model to evaluate features subsets [3]. Although this approach achieves a good result compared with that of the filter approach, it requires more computational resources. The wrapper-based methods can be classified into sequential and global search methods [15,45,46]. The sequential search methods in turn can be categorized into forward (SFS) and backward (SBS) search methods. The forward search method starts from an empty set of features and at each step a feature is added to the feature set to increase the classifier performance, while the backward search method starts from the full set of features and greedily removes a feature at each step according to the classifier performance. In other words, the salient (redundant) features are added (deleted) sequentially during the training of the classifier using the forward (backward) search strategy. A number of algorithms have been proposed in the sequential search strategy, the search processes of which are guided by specific learning models [28,30,47,48]. The sequential search strategies involve the local search rather than the global search; thus, these algorithms try to find solutions that range between sub-optimal and near optimal regions. Therefore, the sequential-based feature selection methods still suffer from a variety of problems, such as stagnation in local optima and high computational cost.On the other hand, the global search methods apply randomness into their search strategy to explore a large portion of the solution space in order to better address the feature selection problems. Recently, metaheuristic algorithms have attracted a lot of attention due to their good performance in solving the feature selection problem. These algorithms include genetic algorithm (GA) [36,49,50], particle swarm optimization (PSO) [21,51–63], ant colony optimization (ACO) [10,18,24,26,33,64], simulated annealing (SA) [65–67] and bacterial foraging optimization (BFO) [68]. The attention of researchers upon GA is due to its simplicity while PSO and ACO have higher accuracy in similar tasks [69]. It has been shown that PSO is computationally less expensive and can converge more quickly compared with the other metaheurestic algorithms such as GA and genetic programming (GP). Moreover, the PSO is an easy-to-implement algorithm, has less adjustable parameters and is also computationally inexpensive in both speed and memory requirements. Therefore, the PSO has been used as an effective technique in many fields, including feature selection.Up to now, several PSO-based feature selection methods have been proposed in the literature.[21,51,56,70–72]. Although the particle swarm optimization has been shown as an effective approach for finding optimal (or near optimal) feature subsets, it suffers from several shortcomings. One of the problems with existing PSO-based feature selection methods is that they ignore the number of features in their search processes while they only emphasize minimizing the classification error rate. Another limitation is selecting similar features in the final feature subset. Moreover, the existing PSO-based approaches do not use correlation information of the features to guide the search process. Therefore, similar features have a high probability to be selected in the final subset, which reduces the classifier performance. To overcome these problems, in this paper, a novel hybrid particle swarm optimization is proposed for feature selection. The proposed method tries to hybridize the PSO by integrating new local search operations. Such operations embedded in the PSO fine-tune the search process for feature selection in an organized fashion. The proposed method called HPSO-LS (i.e. hybridized PSO with local search operations) uses correlation information to guide the search process in PSO in such a way that relatively less correlated (dissimilar) features are selected with a high probability than more correlated (similar) features. In the proposed method, the correlation-based search strategy is used as a local search operation in the PSO. Moreover, HPSO-LS selects the reduced number of salient features by using a specific subset size determination scheme. This scheme works upon a bounded region and tries to provide a subset with a smaller number of features. Such utilizations ultimately lead to a significant performance gain for the feature selection in HPSO-LS.The rest of this paper is organized as follows; in Section 2, related works on feature selection are reviewed, and the basic PSO method and the feature selection task are also discussed. Moreover, details about the proposed method are introduced in Section 3. In Section 4, the proposed algorithm is compared with the other existing feature selection methods. Finally, Section 5 summarizes the present study.

@&#CONCLUSIONS@&#
