@&#MAIN-TITLE@&#
Statistical image reconstruction for low-dose CT using nonlocal means-based regularization. Part II: An adaptive approach

@&#HIGHLIGHTS@&#
Introduce spatial adaptivity in the NLM-based regularization.Demonstrate necessity and efficacy of introducing the spatial adaptivity.Achieve superior reconstruction for low-contrast objects and subtle structures.Systematic validation of the strategy with phantoms and clinical patient data.

@&#KEYPHRASES@&#
X-ray CT,Low-dose,Adaptive nonlocal means,Statistical image reconstruction,

@&#ABSTRACT@&#
To reduce radiation dose in X-ray computed tomography (CT) imaging, one common strategy is to lower the tube current and exposure time settings during projection data acquisition. However, this strategy would inevitably increase the projection data noise, and the resulting image by the conventional filtered back-projection (FBP) method may suffer from excessive noise and streak artifacts. The well-known edge-preserving nonlocal means (NLM) filtering can reduce the noise-induced artifacts in the FBP reconstructed image, but it sometimes cannot completely eliminate the artifacts, especially under the very low-dose circumstance when the image is severely degraded. Instead of taking NLM filtering, we proposed a NLM-regularized statistical image reconstruction scheme, which can effectively suppress the noise-induced artifacts and significantly improve the reconstructed image quality. From our previous investigation on NLM-based strategy, we noted that using a spatially invariant filtering parameter in the regularization was rarely optimal for the entire field of view (FOV). Therefore, in this study we developed a novel strategy for designing spatially variant filtering parameters which are adaptive to the local characteristics of the image to be reconstructed. This adaptive NLM-regularized statistical image reconstruction method was evaluated with low-contrast phantoms and clinical patient data to show (1) the necessity in introducing the spatial adaptivity and (2) the efficacy of the adaptivity in achieving superiority in reconstructing CT images from low-dose acquisitions.

@&#INTRODUCTION@&#
The usage of X-ray computed tomography (CT) has increased dramatically since its introduction in the 1970s. It was estimated that 76 million CT scans were performed across the hospitals and clinics of the United States in 2013 [1]. The consequential radiation dose is significant and the potential radiation risks are receiving growing concerns [2]. Since the radiation risks typically decrease with the reduced radiation dose, many techniques and strategies have been proposed for dose reduction on the CT examinations [3–5]. One cost-effective and commonly used way is to acquire CT projection data with a lower milliampere-second (mAs) protocol [6]. However, the use of the standard filtered backprojection (FBP) method (equipped on most of commercial CT scanners) to reconstruct the low-dose acquisitions frequently produces inferior results with excessive noise and streak artifacts. Many projection or image domain denoising methods were proposed to improve the quality of the FBP-reconstructed low-dose CT images. The low-pass filters have the drawback that while removing the noise, they may also blur other high-frequency components including edges and fine structures, which could be critical in clinical assessment. Some more sophisticated edge-preserving filters can mitigate this drawback to some extent. For instance, the nonlocal means (NLM) filter was successfully applied to FBP-reconstructed low-dose CT images for noise reduction [7]. Based on the success, several strategies were proposed to achieve further improvement, such as using large-scale neighborhood [8], considering local noise level [9], and exploiting a previous normal-dose CT image [10]. Despite all these efforts, it is still observed that the NLM filtering strategies sometimes cannot completely remove the noise and streak artifacts, especially under the desired circumstance for as low as possible on the radiation dose.On the other hand, many statistical image reconstruction (SIR) methods [11], which take into account the statistical properties of the low-dose projection data and accommodate the imaging geometry, have been shown to be superior in suppressing the noise and streak artifacts as compared to the NLM filtering strategies. The SIR strategy has recently become an endeavor for almost all major vendors of clinical CT systems [12–14]. The SIR approach is typically formulated by an objective function consisting of a data-fidelity term and a regularization (or equivalently, penalty) term where the penalized weighted least-squares (PWLS) is one of the commonly used objective function [11]. The penalty (or regularization) in the PWLS criterion plays a critical role for successful image reconstruction [15–28]. Among these studies, several explored the NLM-based regularization for PWLS image reconstruction of low-dose CT [24–28]. For instance, Ma et al. [24] proposed a previous normal-dose scan induced NLM regularization to improve the follow-up low-dose CT scans reconstruction. A temporal NLM regularization [25,26] was also investigated for four-dimensional CT and cone-beam CT reconstruction, where the reconstruction of current frame image utilizing two neighboring frame images. However, the previous normal-dose CT image or neighboring frame images may not be readily available for some applications. Therefore, in our previous study [27,28], a NLM-based generic regularization was explored using the currently available low-dose scan, wherein the regularization exploits a one-step-late (OSL) strategy to estimate the associated weighting coefficients. The NLM-regularized statistical image reconstruction scheme demonstrated promising reconstructions from low-dose data of relatively high-contrast phantoms [27,28]. For clinical applications where the CT images contain not only high-contrast objects but also low-contrast objects and subtle structures, the reconstruction scheme could be problematic due to the use of a spatially invariant filtering parameter h in the regularization. A spatially invariant denoising may be too strong for some regions (blurring much) while too weak for other regions (filtering little) across the entire field of view (FOV) [8]. To address this issue, in this study we developed a novel strategy in designing adaptive filtering parameters for the NLM-based regularization by considering local characteristics of the to-be-reconstructed image, and the resulting new name is called adaptive NLM-based regularization.The remainder of this paper is presented as follows. Section 2 explicitly illustrates the framework of the proposed adaptive NLM-regularized statistical image reconstruction algorithm, and further describes the associated issues about the algorithm implementation and performance evaluation. Section 3 reports the experimental results using both phantom and patient datasets. Finally, discussions on and conclusions from the experimental results are presented in Section 4.The NLM method was proposed as a non-iterative and edge-preserving filter for image de-noising [29,30]. It reduces image noise by replacing each pixel's intensity with a weighted average of its neighbors according to similarity. Although the similarity comparison could be performed between any two pixels within the entire image, it is typically limited to a fixed neighboring window area (called search-window (SW), e.g., 17×17, in 2D case) of target pixel in practice for computation efficiency. Mathematically, the NLM method can be describes as [29,30]:(1)NLM(μˆj)=∑k∈SWjwjk(μˆ)μˆkwhereμˆ=(μˆ1,…,μˆJ)Trepresents the noisy image to be smoothed, andNLM(μˆj)is the intensity value of pixel j after the NLM filtering.However, different from the previous neighborhood filters, the NLM calculates the similarity based on patch instead of a single pixel. A patch of a pixel can be defined as a squared region centered at that pixel (called patch-window (PW), e.g., 5×5, in 2D case). LetP(μˆj)denote the patch centered at pixel j andP(μˆk)denote the patch centered at pixel k. The similarity between pixels j and k depends on the weighted Euclidean distance of their patchesP(μˆj)−P(μˆk)2,a2. The exponential function converts the similarity to weighting coefficient which indicates the interaction degree between two pixels. Specifically, the weighting coefficient is given as:(2)wjk(μˆ)=exp(−||P(μˆj)−P(μˆk)||2,a2/h2)∑k∈SWjexp(−||P(μˆj)−P(μˆk)||2,a2/h2)The parameter h in Eq. (2) controls the decay of the exponential function as well as the weighting coefficient. When h is small, the image tends to be weakly smoothed, and vice versa. For simplicity, h is called filtering parameter hereafter. According to [29,30], the filtering parameter h is a function of the standard deviation of the image noise. And if we further consider the size of the patch-window, the parameter h can be given as [31]:(3)h2=τσ2=2ησ2|PWj|where τ and η are free scalar parameters, σ is the standard deviation of the image noise, and |PWj| denotes the size of the patch-window. However, it is well known that the noise distribution of low-dose CT images is non-stationary, so determining the standard deviation σ is not a trivial task. In the past, the parameter h has been simply set to be a global constant for the entire FOV, e.g., in [7,8,10], although such a practice may result in suboptimal filtering result. Besides, the NLM filter usually cannot effectively suppress the streak artifacts of low-dose CT images. The experimental results in Section 3 illustrate the limitations of the traditional NLM filtering with a spatial-invariant constant filtering parameter h.Mathematically, low-dose CT image reconstruction is an ill-posed problem due to the presence of significant noise and other inconsistencies in the projection data. Therefore, it is usually formulated as the maximum a posterior (MAP) estimation by posing a regularization (or equivalently, penalty) term to regularize the solution. One common choice for low-dose CT image reconstruction is to minimize a PWLS objective function, and a non-negative constraint is usually added considering the physical meaning of the attenuation coefficients [11,15]:(4)μ*=argmin{μ≥0(y−Aμ)TΣ−1(y−Aμ)+βU(μ)}wherey∈ℝI×1is the vector of measured line integrals, and I is the total number of line integral measurements;μ∈ℝJ×1is the vector of attenuation coefficients of the object to be reconstructed, and J is the number of image pixels;A∈ℝI×Jis the system or projection matrix and its element Aijis typically calculated as the intersection length of projection ray i with voxel j;Σ∈ℝI×Iis the covariance matrix, and since the measurement among different detector bins are assumed to be independent, it is diagonal andΣ=diag{σyi2}; U(μ) denotes the penalty term; β>0 is a scalar control parameter that balances the data fidelity and the penalty; the symbols T and −1 herein are transpose and inverse operators, respectively.The variance of the noise in line integral measurements can be determined by the following mean-variance relationship in consideration of X-ray quanta noise and system electronic noise [11,32,33]:(5)var(yi)=σyi2=N¯i+σe2N¯i2=1N¯0iexp(y¯i)1+σe2N¯0iexp(y¯i)whereN¯0irepresents the mean number of X-ray photons just before entering the patient and going toward the detector bin i, and can be measured by system calibration, e.g., by air scans;y¯iandσyi2are the mean and variance of the line integral measurements in detector bin i for repeated acquisitions; andσe2is the variance of the electronic noise.The NLM-based generic regularization can be described as [27,28]:(6)U(μ)=∑jϕμj−∑k∈SWjwjk(μ)μk,andwjk(μ)=exp(−||P(μj)−P(μk)||2,a2/h2)∑k∈SWjexp(−||P(μj)−P(μk)||2,a2/h2)where ϕ denotes a positive potential function and can be chosen asϕ(Δ)=Δp(1≤p≤2). It should be noted that the weighting coefficientswjk(μ)in Eq. (6) are computed on the image μ, rather than a reference image such as in [24,34]. Thus, we call it as NLM-based generic regularization to differentiate it from those needing a reference image. However, using the generic regularization in Eq. (6) may cause difficulty in optimizing the objective function in Eq. (4), because the weighting coefficients are actually computed on the unknown image μ. To address this issue, an empirical one-step-late (OSL) implementation is employed for the optimization task, where the weighting coefficients are computed on current image estimate and then are assumed to be constants when updating the image. Such an OSL iteration scheme has been demonstrated to be feasible and effective in the previous studies [28,35].The filtering parameter h determines the smoothness of the resulting image, where larger h results in more smoothing and smaller h results in less smoothing. In the previous studies, the h in the NLM-based regularizations was often set to be a constant across the entire FOV [24–28,34,35]. However, when the local characteristics of the image differ significantly across the entire FOV, a constant h may result in inferior/suboptimal reconstruction result, since it may be too large for some regions (blurring edges and subtle structures) while too small (filtering little) for other regions within the image domain. To mitigate this issue, in this study, we propose a novel locally adaptive estimation of the filtering parameter h at pixel j as:(7)hj2=s⋅meanP(μj)−P(μk)2,a,k∈SWj+twhere s and t are two constants, and can be determined through experiments.The rationale behind the mathematical expression of Eq. (7) is that the value of h should depend on the similarity between the patch of target pixel and the patches within the corresponding SW. That is, when SW contains many similar patches to P(μj), h needs to be decreased to reduce the influence of the other patches. On the contrary, when very few similar patches exist in SW for P(μj), h needs to be increased to relax the selection [36]. To achieve robust implementation of the similarity dependence in Eq. (7), the constant t is introduced to ensure numerical stability and adequate filtering for uniform regions, and the constant s is introduced to control the relative filtering strength for non-uniform regions in the image domain.There are five parameters related to the proposed adaptive NLM-based regularization (size of SW, size of PW, standard deviation a of the Gaussian kernel, and constants t and s). Determining the optimal values for these five parameters for the adaptive NLM-based regularization is not a trivial task. Extensive experiments revealed that the sizes of SW and PW as well as the standard deviation a do not have noticeable effects on the reconstructed image when they are set in a reasonable range. Therefore, a typical selection of SW=17×17, PW=5×5 and a=5 was used for all the cases in this study. As for the constants t and s in Eq. (7), it was found that t=4×10−6 was good for all the datasets, and the optimal value for s differed slightly within a small range for different datasets, e.g., from 4×10−4 to 1×10−3.The hyper-parameter β controls the tradeoff between the data fidelity term and the regularization term in the PWLS objective function. The selection of optimal β value for the SIR methods remains an open question. Generally, a larger β value produces a more smoothed reconstruction with lower noise level but also lower spatial resolution, and vice versa.According to our experiments, too small s value may introduce artifacts and too small β value cannot effectively suppress the noise, while too large value of either parameter may blur the image edges. In this study, we set the two parameters in a reasonable range and performed a grid search for different combinations of them, and then determined the optimal combination through visual inspection and quantitative measures of corresponding reconstruction result.A computer simulated clock phantom was utilized in this study, which is modified from the reported one in [37]. The clock phantom consists of a water background and eight circular inserts with different contrasts (C1: −100%, C2: +150%, C3: +7%, C4: −50%, C5: +85%, C6: −15%, C7: −7%, C8: +30%). The low-dose sinogram data of the clock phantom was acquired using the simulation method in [38]. After calculating the noise-free line integral, the noisy measurement Niat detector bin i was generated according to the statistical model:(8)Ni∼Poisson(N¯0iexp(−y¯i))+Gaussian(0,σe2)whereN¯0iwas set to be 3×104 andσe2was set to be 10 in this study [38]. Then the corresponding noisy line integrals were calculated by the logarithm transform. The scanning geometry was the same as the Siemens Somatom Sensation 16 CT scanner (Siemens Healthcare, Forchheim, Germany), which is illustrated in Table 1.To evaluate the above presented reconstruction algorithm in a more realistic situation, an anthropomorphic torso phantom (Radiology Support Devices, Inc., Long Beach, CA) was used for experimental projection data acquisition. The phantom was scanned by the same clinical Siemens scanner (as described in Table 1) in a cine mode at a fixed bed position. The X-ray tube voltage was set to be 120kVp and the mAs level was set to be 40mAs. The CT scanner was rotated 150 times around the torso phantom.The central slice sinogram data of one scan was extracted and regarded as the low-dose scan. The averaged sinogram data of that slice from 150 repeated scans was reconstructed by the FBP method to serve as the ground truth image for evaluation purpose.The projection data of a patient were acquired using the same Siemens scanner after obtaining informed consent from the patient, and this clinical data serve as a pilot clinical study. The patient was scheduled for CT scan for medical reasons. The X-ray tube voltage was 120kVp, and the mAs level was 20mAs, which was considered as ultra low-dose scan in clinic.

@&#CONCLUSIONS@&#
