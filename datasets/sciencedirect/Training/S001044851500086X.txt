@&#MAIN-TITLE@&#
A gesture-free geometric approach for mid-air expression of design intent in 3D virtual pottery

@&#HIGHLIGHTS@&#
We introduce a geometric approach for mid-air virtual pottery design.A user can design virtual pots without the need to remember gestures.The shape of a pot gradually converges to the point-cloud of the user’s hands.Applications are shown with two depth sensors, Leap Motion and SoftKinetic DepthSense.User evaluation demonstrates strengths and weaknesses of our approach.

@&#KEYPHRASES@&#
Hand-based shape modeling,Mid-air interactions,Virtual pottery,Gestures,Natural user interfaces,Mesh deformation,

@&#ABSTRACT@&#
Graphical abstract

@&#INTRODUCTION@&#
Mid-air interactions have received significant attention in the context of 3D shape conceptualization. These interaction approaches offer intuitive and direct manipulation of virtual shapes by using free hand movements. The core challenge in mid-air interactions for computer-aided design (CAD) is to map user input to virtual operations, such that the user can directly focus on the design task rather than spending time in learning the tool itself. Three-dimensional user input has been extensively studied, evaluated, and reviewed  [1–4] in the context of 3D selection, manipulation  [5], control, and navigation, in virtual environments. Mid-air interactions have found significant use in gaming  [6] and art  [7–9]. Within the context of 3D conceptual shape design, we find two broad classes of mechanisms that enable mid-air user input.The first class comprises of instrumented controllers such as gloves  [10–12], hand-held trackers  [13,14], and haptics devices  [15–17]. Special devices and setups  [18–23] have also been demonstrated for 3D interactions. These hardware systems offer great control, feedback, and unambiguity to the user while interacting in mid-air. In these approaches, the user provides explicit commands via the hand-worn or hand-held controller to indicate design intent such as starting, stopping, or selecting a modeling operation when desired. However, such systems are not accessible to the common user outside a lab environment. Further, wearing or holding can be intrusive to the user during a focused modeling task.The second class of mechanisms for mid-air user input are the so-called bare-hand gesture-based interactions  [24–31]. With the recent commercialization of depth cameras, gesture-based interactions have become accessible to the common user. Creative applications for free-form shape modeling  [32] in mid-air have gained significant popularity. The user input in these applications is represented as a combination of a hand posture (such as pointing with a finger) and the motion of a representative point (such as the palm or fingertip) on the hand. We call this the symbolic gesture approach. Here, converting user input into a meaningful shape modeling task involves (a) acquisition, segmentation, and processing of hand data, (b) extracting a virtual representation of the hand, (c) mapping the gestures to a shape modeling task, and (d) generating an appropriate response of the shape as intended by the user’s input. Such approaches involve estimation of the skeletal structure of the hand  [33–35] or classification of the hand’s image as a pre-defined posture  [36]. In this work, we aim to utilize the non-intrusiveness and accessibility of camera-based mid-air interactions for 3D shape deformation.Both classes of mid-air interactions (instrumented controllers and bare-hand gestures) have one common characteristic, namely, there is a clear distinction between interaction and geometric modeling. In this work, we take a different approach; we pose mid-air interaction itself as a geometric problem. Our focus is the geometric investigation of spatial interactions in the specific context of shape deformation. Particularly, we address the problem of determining how the shape and motion of a user’s hand and fingers geometrically relates to the user’s intent of deforming a shape.

@&#CONCLUSIONS@&#
In this paper, we demonstrated that it is possible to tailor a geometric modeling method to suit the needs of controllable spatial interactions that use hands and finger motions for 3D shape modeling. This idea creates new pathways for further research, exploring hand-based shape modeling. It enables the augmentation of users’ existing knowledge of real-world manipulations with new virtual modeling contexts in a “what you do is what you get” framework. At its core, proximal persistence is a general notion which can be instantiated in a variety of ways by combining different hand representations with different shape representations and modeling metaphors. It will be interesting to investigate methods which could automatically deduce appropriate hand representations for different modeling metaphors. With upcoming mid-air geometric design applications, achieving simultaneous efficiency, robustness, and controllability is a challenging problem towards enhancing the user’s creative process and outcome. We believe that proximal persistence takes a fundamental step towards problem.