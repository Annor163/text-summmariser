@&#MAIN-TITLE@&#
Controlling blood glucose variability under uncertainty using reinforcement learning and Gaussian processes

@&#HIGHLIGHTS@&#
Simulation-based learning of an optimal control policy for an artificial pancreas.Integration of reinforcement learning with Gaussian processes for policy iteration.Responding promptly to the varying activity levels seen in outpatients.Stochastic modeling of diabetic patients for controlling variability.

@&#KEYPHRASES@&#
Artificial pancreas,Diabetes,Gaussian processes,Policy iteration,Reinforcement learning,Stochastic optimal control,

@&#ABSTRACT@&#
Automated control of blood glucose (BG) concentration with a fully automated artificial pancreas will certainly improve the quality of life for insulin-dependent patients. Closed-loop insulin delivery is challenging due to inter- and intra-patient variability, errors in glucose sensors and delays in insulin absorption. Responding to the varying activity levels seen in outpatients, with unpredictable and unreported food intake, and providing the necessary personalized control for individuals is a challenging task for existing control algorithms. A novel approach for controlling glycemic variability using simulation-based learning is presented. A policy iteration algorithm that combines reinforcement learning with Gaussian process approximation is proposed. To account for multiple sources of uncertainty, a control policy is learned off-line using an Ito's stochastic model of the glucose-insulin dynamics. For safety and performance, only relevant data are sampled through Bayesian active learning. Results obtained demonstrate that a generic policy is both safe and efficient for controlling subject-specific variability due to a patient's lifestyle and its distinctive metabolic response.

@&#INTRODUCTION@&#
Insulin-Dependent Diabetes Mellitus (IDDM) is a chronic disease characterized by the inability of the pancreas to produce sufficient amounts of insulin. A high level of BG concentration is known to cause serious health problems, including heart disease and stroke, hypertension, retinopathy, nephropathy, and neuropathy [1,2]. Poorly controlled diabetes mellitus is associated with multiple long-term complications that contribute to increased morbidity and mortality. Also, abnormal glycemic variability contributes to oxidative stress, which has been linked to the pathogenesis of diabetes [3,4]. Compensating for this deficiency in endogenous insulin production requires 4–6 insulin injections to be taken daily; the aim of this diabetes therapy is to maintain normoglycemia – i.e., a blood glucose level between 4 and 7mmol/L. In defining the amount and timing of these injections, poor predictability of BG dynamics is a key issue that both patients and doctors must deal with [5]. Manual control of BG often results in high glycemic variability and the risk of a life-threatening hypoglycemic event is at stake. Hypoglycemia – i.e., low blood glucose levels – may lead to brain damage, coma and eventually death.Closing the glucose control loop with a fully automated artificial pancreas will certainly improve the quality of life for insulin-dependent patients. Such a device is made up of a glucose sensor, an automated insulin infusion pump, and a feedback control strategy or control algorithm that calculates the insulin delivery based on a glucose signal. The major challenge for the development of a closed-loop control system is the glycemic variability between subjects and for the same subject over time. A reliable closed-loop system for blood glucose regulation should be able to adapt “on the fly” to each patient response in order to cope with daily variations in glucose metabolism. Another challenge in controlling variability of BG is sensor errors. These errors depend nonlinearly on the BG rate of change and are subjected to a delay due to its subcutaneous nature. Additionally, the sensor noise is non-white (non-Gaussian) and consecutive sensor errors are highly interdependent [6]. Despite significant advances, the available technology for continuous glucose monitoring is still ineffective to deal with many issues related to sensitivity, stability, calibration, and the physiological time lag between BG and interstitial glucose (IG) concentration. Significant delays in delivering insulin to the blood stream give rise to delayed effects of control actions which increase the risk of hypoglycaemia episodes. Hypoglycaemia has been identified as the primary concern for a safe implementation of the artificial pancreas [7].Pioneering works with the proportional-derivative (PD) and the proportional-integral-derivative (PID) types of controllers demonstrated the advantages of closed-loop control [8]. The resulting control strategies, however, generally do not prove sufficiently effective in maintaining euglycaemia after meals when the less invasive choice of the subcutaneous route is used. PID algorithms can be considered reactive, as they respond to observed glucose levels and are less equipped to take advantage of announced meals and patient-directed insulin boluses. In a recent experimental study with a PID controller using the subcutaneous–subcutaneous (SC–SC) route [9], the observed mean glucose levels are not sensibly different to the ones corresponding to the uncontrolled glucose dynamics. Also, the two-hour postprandial glucose levels were significantly higher than those observed in healthy subjects under similar conditions. Glucose variability has a highly detrimental effect on the performance of a PID controller since even for subjects with a strict lifestyle, gain scheduling [10] and adding a feed-forward element based on manual entry of a meal disturbance (time and content) are required for satisfactory blood glucose control [41]. To prevent a hypoglycemic condition, PID controllers can be enhanced using a supervisory module that constrains insulin delivery by limiting the maximum infusion rate or by suspending altogether close-loop control when glucose levels are approaching a lower threshold or are decreasing too rapidly. However, the inability of PID controllers to accommodate system constraints in the computation of control actions further limits their potential for success with patients that have active life styles. Moreover, due to the myopic nature of its feedback law, a PID controller is unable to cope with delayed effects of control actions. Typically, in systems where the effects of control actions slowly unfold over time, a PID overreacts, which increases glycemic variability. The lag time associated with subcutaneous (SC) insulin infusion is an obstacle for any reactive control algorithm.A more suitable control framework for systems with large lag times and constraints is model predictive control (MPC) which has been proposed as a promising architecture for insulin delivery in the artificial pancreas [11–14]. The predictive framework is a powerful tool not only to deal with time delays in the system response, but also to evaluate the future effects of a meal and thus achieving disturbance rejection. Constraint handling and penalizing input actions (which will avoid too aggressive control actions that may lead to hypoglycemia) are also advantages of model-based control methods. However, one serious drawback of model-based control systems is that the controller performance is strongly dependent on the accuracy of the model used to represent the glucose-insulin dynamics. Most of the glucose-insulin models proposed in the literature are physiological compartmental models that are generally representative of only an average subject under specific conditions [15–18]. The metabolic processes underlying insulin action involve complex interactions of hormones, which lead to significant variation in insulin sensitivity [19]. Another disadvantage is that to implement a MPC requires repeatedly obtaining an on-line solution of a mathematical program. This on-line optimization can be avoided with a single set of a priori optimizations via multi-parametric programming; the on-line problem is thus reduced to the evaluation of an affine function obtained from a lookup table [14].More recently, new approaches combining an Iterative Learning Control (ILC) scheme with MPC strategies have been proposed. These hybrid methods try to benefit from the repetitive nature of insulin therapy to improve iteratively the efficacy of insulin doses by using run-to-run control algorithms [20,21,42,43]. In a situation with frequent data sampling, iterative learning control (ILC) is the alternative of choice. ILC attempts to mimic human learning in order to take advantage of subject-specific variation patterns in the glucose-insulin dynamics. A key issue for a successful ILC implementation is the design of a feedback control law that can handle inter- and intra-patient glycemic variability. To address the latter, adaptive control of blood glucose is considered a worth exploring alternative [22,23]. Recently, an adaptive model-based control strategy has been proposed by Oruklu et al. [24] which can dynamically detect blood glucose variations, and on that basis reject glycemic disturbances. The adaptability of the controller is based on subject-specific recursive linear models developed using data from a continuous glucose monitoring (CGM) sensor along with a change detection algorithm. Metabolic variations in a subject's body are addressed by online model identification. At each step, model parameters are updated by using new glucose data, and the future time course of BG concentration is estimated. These parameters are then used in a model-based control algorithm for calculating the appropriate insulin infusion rate.In recent years, clinical evaluations of different strategies for close-loop artificial pancreas systems have been reported. In Dassau et al. [25], a fully automated multi-parametric model predictive control algorithm with insulin on-board was experimentally tested with encouraging results. The first wearable AP outpatient study using a meal-informed MPC strategy was reported by Del Favero et al. [26] aiming to investigate the ability to control postprandial glucose. Despite promising results were obtained in short-term studies for a single meal (dinner), long-term randomized studies with numerous meals are needed to prove superiority of MPC over the commonly used bolus calculator. A bi-hormonal closed-loop artificial pancreas was experimentally assessed by El-Khatib et al. [27]. Even though results demonstrate the feasibility of safe BG control by a bi-hormonal artificial endocrine pancreas, inter- and intra-subject variability in metabolic responses to insulin and glucagon could hamper the effectiveness of the control algorithm. Reactive control algorithms need to integrate learning capabilities upon which they can promptly respond to the varying activity levels seen in outpatients, with unpredictable and unreported food intake and stress conditions, and may also provide the necessary personalized glucose control for individuals [28]. Controlling insulin delivery in a closed-loop using reinforcement learning algorithms revolves around obtaining robust, yet optimal control policies that are reactive to the immediate needs of the patient.In this work, a novel approach for controlling variability in blood glucose concentration using simulation-based learning of a robust control policy is presented. To account for inter-patient variability, a policy iteration algorithm that combines reinforcement learning with Gaussian process approximation is proposed. A generic control policy is learned off-line using an Ito's stochastic process model of the glucose-insulin dynamics that simulates glycemic variability comprehensively. For safety and performance, only relevant data are sampled through Bayesian active learning.Glucose-insulin dynamics exhibits significant variability from patient to patient. Due to this uncertain behavior even the same insulin dose with the same meal routine and the same amount of physical exercise may result in different blood glucose responses to insulin injections on consecutive days. Furthermore, blood glucose levels vary among different patients according to carbohydrate contents, exercise levels, age and stress [29]. Natural inter- and intra-patient variability needs to be addressed in developing an optimal glucose control profile. Since the interactions between insulin, meals, exercise and other factors and their effect on blood glucose is an on-going phenomenon, there exists significant uncertainty in the actual response of a patient to control actions.A reliable model for predicting future glucose concentrations should address intra- and intra-patient variability and should be able to adapt to unexpected glycemic perturbations. If the uncertainties are omitted and/or the model cannot accurately represent the glucose and insulin dynamics, significant performance degradation may occur in model-based control strategies whereas hypoglycaemic episodes cannot be ruled out. Significant variability of relevant parameters among patients and within a given patient during the course of the day or week has been reported in the literature [5,30]. Meals and exercise along with age and weight require different values of model parameters to describe in quantitative terms the variability observed in the insulin-glucose dynamics. For instance, exercise affects significantly blood glucose concentration, mainly due to the increase of glucose uptake in target peripheral tissues such as muscles. Metabolic changes that are caused by stress, illness or changes in insulin sensitivity may also lead to variation in glucose-insulin dynamics within the same subject. Furthermore, patients with diabetes are constantly exposed to external disturbances that cause large blood glucose perturbations such as meal consumption or physical activity on a daily basis.To simulate the glucose-insulin dynamics in a diabetic patient many mathematical models have been developed to describe the mechanisms of the glucose-insulin regulatory system. Most of the models proposed are based on either the Bergman's minimal model [15,31] or the Sorensen's physiological model [17]. These models offer a rather qualitative prediction tool for blood glucose dynamics to account for exogenous insulin infusions and carbohydrate intakes. The most common model is the minimal model of Bergman [15,31], which contains a small number of parameters. Based on the Bergman's model, Lehmann and Deutsch [32] developed a well-known physiological model to provide a suitable framework to characterize individual diabetic patients quantitatively in order to predict the BG profile for a given treatment. This model is still widely used in several works. This physiological deterministic model is called the automated insulin dosage advisor (AIDA) model and more information about this model can be found in the website http://www.2aida.net/. Also, there are well-known diabetes simulator such as the UVA/PADOVA Type 1 diabetes simulator and The Type 1 Diabetes Metabolic Simulator (T1DMS) that are often used to model the deterministic behavior of glucose dynamics. The AIDA model considers two subsystems (compartments) to represent the glucose-insulin dynamics based on in the following differential equations system:(1)dGdt=Gin(t)+NHGB(p3,I)VG−G(p2Ia+p5)(KM+GX)GX(KM+G)VG−GrenVG(2)dIadt=p3I−p4Ia(3)dIdt=−ηI+DtVIIn the above differential equations system, Eqs. (1), (2) and (3) describe the dynamics of the plasma glucose concentration G, the insulin in remote compartment Iaand the plasma insulin concentration I, respectively; their corresponding initial conditions are: G(0), Ia(0) and I(0). In Eq. (1), Ginis the systemic appearance of glucose via glucose absorption from the gut, NHGB is the net hepatic glucose balance, VGis the volume of distribution of glucose, GXis a reference glucose level, KMis the Michaelis-Menten constant between glucose utilization and plasma glucose concentration and Grenis the renal excretion of glucose. Eq. (3) gives the rate of insulin absorption after a subcutaneous insulin injection, where η is the fractional disappearance rate of insulin, VIis the insulin distribution volume and Dtis the total exogenous infused insulin; Dt=Dt−1+utwhere utis the control action, whereas [P=p1p2p3p4p5] is a vector of estimated patient-dependent parameters. In this work, the values estimated by Lehmann and Deutsch [32] are used. These parameters are given in Table 1, assuming a body weight of 70kg. The parameter Sh that appears in Table 1 is the hepatic sensitivity needed to determine the value of NHGB(t). For further details the reader is referred to the work of Lehmann and Deutsch [32] and the references therein.Eqs. (1)–(3) provide the deterministic physiological dynamic model of blood glucose [32]. However, there exists uncertainty in the estimation of model parameters in Table 1 which prevents describing variability among daily values of glucose in patients, whereas other sources of structural errors give rise to model-patient mismatch. Also, there exist inaccuracies in the glucose sensor and therefore the available measurements do not uniquely determine the true glycemic state of a diabetic patient. Thus, there is uncertainty in both estimating system state and predicting the outcome of control actions. Furthermore, noise is present in subcutaneous glucose sensors which is another important cause of variability [33]. Based on the deterministic physiological dynamic model developed by Lehmann and Deutsch [32] (Eqs. (1)–(3)), Acikgoz and Diwekar [34] proposed that different sources of time-dependent uncertainties could be represented by introducing stochastic processes in this deterministic model. By superimposing an Ito's stochastic process to represent the variability in plasma glucose concentration Eq. (1) as can be modified as follows:(4)dGdt=Gin(t)+NHGB(p3,I)VG−G(p2Ia+p5)(KM+GX)GX(KM+G)VG−GrenVG+σitoεdt;G(0)=G0where σito is the variance parameter and ɛ is a random number generated by a normal distribution with a mean equal to zero and a standard deviation of one(ε∼N(0,1)). Thus, the stochastic model is defined by Eqs. (2)–(4). In this way, for off-line learning of a robust, yet optimal control policy, glycemic variability can be taken in account using a stochastic process superimposed on a deterministic model of the glucose-insulin dynamics.In Fig. 1a, the key concept of a control policy (see below for details on how it can be learned using simulations) with optimality guaranteed in a stochastic sense is highlighted in a reinforcement learning control architecture. The inter- and intra-variability in diabetic patients are modeled by setting the parameter σito=0.25, whereas 125 independent simulations of the controlled Ito's process were made using Eqs. (2)–(4) to model the optimally controlled glucose-insulin dynamics (see Fig. 1b). It is worth noting, that despite the effects of control actions are quite uncertain, the optimal policy is able to achieve tight glycemic control by maintaining BG within soft constrains (green lines). The next section proposed a novel simulation-based policy iteration algorithm, GPRL, for off-line learning of an approximation to the optimal control policy by integrating reinforcement learning with Gaussian processes.The control policy is a function π:x→u that maps states (x) to a (scalar) control action (u). In this sense, a control policy defines how a controller chooses control actions to be applied depending on the state the system is in. More specifically, in BG regulation the control policy defines changes to the (exogenous) insulin infusion rate which is being delivered to the diabetic patient. Through formulating and solving a RL problem for glucose control in diabetic patients, it is possible to learn a generic control policy suitable for controlling blood glucose under uncertain conditions. Simulating uncertain conditions in a patient metabolic response to insulin infusions makes feasible to comprehensively account for inter- and intra-patient glycemic variability. To this aim, for learning a control policy, glycemic variability in a generic patient is described using the stochastic model presented in Section 2.The reinforcement learning (RL) problem [35] consists in learning iteratively to achieve a goal or to accomplish a control task from ongoing interactions with a real or simulated system. During learning, an agent (or controller) interacts with the target system by taking an actionut∈ℝnuand, after that, the system evolves from the statext∈ℝnxto xt+1 and in so doing the agent receives a numerical signal rtcalled reward (or cost) which provides a measure of how good (or bad) is the action taken at xtin terms of the observed state transition. Rewards are given as hints regarding goal achievement or optimal behavior. In applying RL to BG regulation, the main objective of the agent is learning the optimal policy, π*, which defines the optimal insulin infusion rate for any state the patient may be in bearing in mind both short- and long-term rewards. To this aim, the learning agent executes a sequence of actions to maximize the amount of reward received from any initial state (x0) until a certain goal state is reached. Under a given policy π, let's assume the expected cumulative reward Vπ(x0) or value function over a certain time interval is a function of xπ, wherexπ=xtt=1t=Nhas as its entries the observed sequence of states anduπ=utt=1t=Ndefines the policy-specific sequence of control actions. The sequence xπof state transitions gives rise to rewardsrtt=1t=Nwhich are used to define a discounted value function(5)Vπ(x0)≔γN.rN+∑t=1N−1γt.rtwhereγ∈0,1is the discount factor which weights future rewards. An optimal policy π*for the N-stage control problem maximizes Eq. (5) for any initial state x0. The associated state-value function must satisfy the Bellman's equation:(6)V*(xt)=argmaxu{rt+γ.Ext+1[(V*(xt+1)|xt,ut)]}The control value function Q* is defined by(7)Q*(xt,ut)=rt+γ.Ext+1[(V*(xt+1)|xt,ut)]such that V*(x)=argmaxuQ*(x, u) for all x. Once Q* is known through interactions, then the optimal policy is obtained directly through:(8)π*(x)=argmaxuQ*(x,u)The state-value function V*(xt) is the maximum cumulative reward over an N-step optimization horizon starting from state xtat time t.In Fig. 2, a policy iteration algorithm is proposed which resorts to simulated state transitions and it is referred to as Algorithm 1 hereafter. For a simulation model f of the state transition dynamics, a finite set of actionsUand a finite set of statesX, policy iteration increasingly approximates the optimal controlsπ*(X)∈Ubased on an arbitrary initialization of the value functionV*(X). It is worth noting that the learning algorithm in Fig. 2 can only be applied when the resulting state x from an actionu∈Uis a state in the setX. The first iteration is based on an initial policyπ0(X)(e.g., a random policy) and the estimation of optimal state valuesV1*(X)are based on the reward function r(x,u). In the kth iteration, the optimal policy is re-estimated throughπk(X)using an improved value functionVk*(X)with rewards generated using the control policyπk−1*(X)from the previous iteration. Policy convergence is achieved when given two successive policiesπk−1*(X)andπk*(X)both select the same optimal action u*(x) for state x, considering all states in the finite setX.To generalize the policy iteration algorithm in Fig. 2 to unseen statesx∉X, a function approximation technique is required. Also, value function generalization and inductive modeling of the optimal policy is mandatory in simulation-based RL to allow selecting optimal actions which are not necessarily part of the action setU. Gaussian Process (GP) models are a powerful alternative for generalization and on-line learning in reinforcement learning algorithms [36]. A Gaussian Process is a generalization of a Gaussian probability distribution where the distribution is over functions instead of assuming a model with a given structure before training data are considered.A brief introduction to GPs will be given based on the book by Rasmussen and Williams [37]. In RL problems, the experience is collected in data sets {X, Y}, whereX:{xi∈ℝd/i=1,2…n}is the set of input vectors andY:{yi∈ℝ/i=1,2…n}is the set of the corresponding observations; n is the number of samples. The objective of model learning is to infer an inductive model h of the (unknown) value functionVk*(⋅)that gives rise to the observed data. Assuming that the observations are generated by yi=h(xi)+ɛ,ε∼N(0,σε2), within a Bayesian framework, the inference of the underlying function h is described by the posterior probability:(9)p(h|X,Y)=p(Y|h,X)p(h)p(Y|X)where p(Y|h, X) is the likelihood, and p(h) is a prior distribution on plausible value functions assumed by the GP model. The term p(Y|X) is called the evidence or the marginal likelihood. When modeling any function using GPs, a prior p(h) is placed directly on the space of functions without the necessity to consider an explicit parameterization of the approximating function h. This prior GP typically reflects assumptions on the, at least locally, smoothness of h.Similar to a Gaussian distribution, which is fully specified by a mean vector and a covariance matrix, a GP is specified by a mean function m(·) and a covariance function Cov(·,·), also known as a kernel. A GP can be considered a distribution over functions. However, considering a function as an infinitely long vector, all necessary computations for inference and prediction of value functions can be broken down to manipulate well-known Gaussian distributions. The fact that the function h(·) is GP distributed is indicated byh(⋅)∼GPh(m,Cov)hereafter.Given a GP model of the function h, we are interested in predicting the value function for an arbitrary inputxt*. The predictive (marginal) distribution of the function valueh*=h(xt*)∼GPh(xt*)for a test inputxt*is thus Gaussian distributed with mean and variance given by:(10)E[h(xt*)]=Cov(xt*,X)+(K+σε2I)−1Y(11)Var[h(xt*)]=Cov(xt*,xt*)+Cov(xt*,X)(K+σε2I)−1Cov(X,xt*)where K is the kernel matrix withKij=Cov(xti,xtj)∀xt∈X. A common covariance function is the squared exponential (SE):(12)CovSE(xti,xtj)≔ζ2exp−12(xti−xtj)TΛ(xti−xtj)where the matrixΛ=diag([ℓ12,ℓ22,…,ℓnx2])and ℓr, r=1, …, nx, being the characteristic length scales. The parameter ζ2 describes the variability of the inductive model h. The parameters of the covariance function are the hyperparameters of the GPhand are collected in the vector ψ. To fit hyperparameters to value function data the evidence maximization or marginal likelihood optimization approach is recommended (see[37] for details). The log-evidence is given by:(13)logp(Y|X,ψ)=∫(Y|h(X),X,ψ))p(h(X)|X,ψ)dh=−12YT(Kψ+σε2I)−1Y−12log|(Kψ+σε2I)|−d2log(2π)In Eq. (13), h(X)=[h(x1), …, h(xn)], where n is the number of training points. The dependency of K on the hyper-parameters ψ is made explicit by writing Kψin Eq. (13). Evidence maximization yields an inductive model of the value function that: (i) rewards data fitting, and (ii) rewards simplicity of the fitted model. Hence, it automatically implements Occam's razor, i.e. preferring the simplest model. Further details on GP regression can found in the excellent books of Rasmussen and Williams [37] and MacKay [38], and references therein.As it is widely recognized, model-based RL methods often make better use of available information since they capture the underlying pattern (latent function) for state transitions. Gaussian process models appropriately quantify uncertainty about state transitions and, assuming the transition dynamics is smooth, they can lead to very data-efficient solutions. GP models of both transition dynamics and the value functionsVk*(⋅)andQk*(⋅,⋅). will be built simultaneously as data gathering bias are introduced through sampling using an increasingly improved control policy, which in turn is represented in a compact form by a GP model, the policy GP.For controlling glycemic variability, an inductive model of the state transition dynamics based on interactions between a learning algorithm and a diabetic patient, or a simulator of it, is used here. The key assumption made is that the glucose dynamics evolves smoothly while a given control policy is being applied. Also, it is assumed that glucose variability is due to uncertainty about both inter- and intra-patient variability, inaccuracies in glucose sensors and delays in insulin absorption from the subcutaneous tissue. A GP (inductive) model, the dynamics GP, is learned to describe the state transition dynamics through expectations on changes caused by control actions. For the plasma glucose concentration G, a GP model is trained in such a way that the effect of uncertainty about its change following a control action is modeled statistically as:(14)Gt+1−Gt∼GPf(mf,covf)where mfis the mean function and covfis the covariance function. The training inputs to the transition dynamics GPfare tuples (xt, ut), whereas the targets are the state differences shown in Eq. (14). The posterior distribution for the dynamics GP reveals the remaining uncertainty about the underlying latent function for any BG change ΔG=Gt+1−Gtcaused by a control action utwhen it is implemented at xt. GP models of the transition dynamics for observable states are built on the fly using data gathered from interactions with a patient simulator or the real patient.For generalization in RL algorithms with continuous state and action spaces, GP models are useful to approximate the value functions Vk(·) and Qk(·, ·) directly in function space by representing them by fully probabilistic GP models. These inductive models make intuitive sense as they use the data (coming from real or simulated interactions) to determine the underlying structures of the value functions, which are a priori unknown. Moreover, GPs provide information about confidence intervals for the value function predictions. Thus, the state-value function Vk(·) and the action-state value function Qk(·, ·) are approximated asVk(⋅)∼GPυ(mv,Covv)andQk(⋅,⋅)∼GPq(mq,Covq), respectively. The training targets (observations) are iteratively estimated by the RL algorithm. Thus, the training inputs to the mode GPυare the states xtwhereas the targets are the corresponding state values Vk(xt). Similarly, the training inputs for GPqare tuples (xt, ut) and the targets are the corresponding state-action values Qk(xt, ut). The advantage of modeling the value functions by GP models is that GPs provide a predictive distribution of Vk(xt) or Qk(xt, ut) for any xtor any pair (xt, ut) through Eqs. (10) and (11). Due to the generalization property of GPυand GPq, we can work in continuous domains of states and actions to determine a function value Vk(x*) or Qk(x*, u*) for any state x* or state-action pair (x*, u*).It is worth noting that using a probabilistic model for the value function allows addressing uncertainty in state transitions following a control action and the variability in corresponding rewards (costs) in a natural way. Also, by resorting to GPs the RL algorithm formulation can readily handle uncertainty in state transitions due to stochastic disturbances along with noisy measurements and rewards.The optimal policy π*is regarded as a probabilistic map from states to actions. In order to generalize the control policy all over the state space, a regression problem to obtain an optimal policy π*based on the observed data has to be solved. These data are obtained from the interactions (real or simulated) between the artificial pancreas and the patient. Although any function approximation technique can be used to model the control policy, here it is also approximated with a GP model, the policy GPπ, which readily allows assessing the remaining uncertainty when estimating optimal controls for unseen states. The training inputs of the model GPπare states xt whereas the training targets are controls ut. When the targets are the optimal controlsut=π*(v)obtained from Eq. (8), the GPπmodel generalizes the optimal control policy to unseen states during learning.In Fig. 3, the GPRL algorithm (referred to as Algorithm 2) is proposed, which is an integration of RL with GPs in a novel strategy for simulation-based policy iteration. GPRL is a generalization of the algorithm in Fig. 2 to handle continuous state and action spaces using GP regression to describe both value functions and the control policy. For policy iteration, GPRL describes the value functionsVk*(⋅)andQk*(⋅,⋅)directly in function space by representing them by fully probabilistic GP models. These inductive models (GPs) make intuitive sense as they use simulation data to determine the underlying structures of these value functions, which are a priori unknown. Moreover, GPs provide information about confidence intervals for value function predictions and optimal actions. The setsXkandUinstead of being a discrete representation for the state and action spaces, are now considered the support points (training data) for approximating the value functionsVk*(⋅)andQk*(⋅,⋅)using GP models. The training targets (observations) are iteratively determined by the GPRL algorithm itself.The advantage of modeling the state-value functionVk*(⋅)by GPυis that it provides a predictive distribution ofVk*(xt)for any state xtthrough Eqs. (10) and (11). In GPRL, this property is exploited in the computation of theQk*-value(line 12); due to the generalization property of GPυ, we are not restricted to a finite setXof successor states for any actionu∈Uwhen determining E[GPυ(xt+1)]. It is worth noting that using a probabilistic model for the value function allows addressing uncertainty in state transitions following a control action and the variability in corresponding rewards (costs) in a natural way. Since the expectation is with respect to the latent functionVk+1*(xt+1), which is probabilistically modeled using GPυ, E[GPυ(xt+1)], is simply given by mυ(GPυ(xt+1)), i.e. the predictive mean of theVk+1*(xt+1). Note that for anyxt∈X,the corresponding GPυmodels a function of over a continuum of control actions not only the finite number of actions in the action setU. Therefore, the optimal control for xtin line 15 is the maximum of the mean function of GPυwhich is obtained by solving the optimization problem:(15)argmaxuQk*(xt,u)≅argmaxumq(u),subjectto:umin(xt)≤u≤umax(xt)Since Eq. (15) defines a line-search optimization problem, well-known methods such as the Fibonacci or the Golden section can be used to advantage.A distinctive feature of simulation-based policy iteration using GPRL is that for allxt∈Xk, independent GP models forQk*(xt,⋅)are used rather than modelingQk*(⋅,⋅)in joint state–actions space [36]. The main reason for this choice is that by means of a simulation model for state transitions, the GPRL algorithm in Fig. 3 only needs the value functionVk*(xt), the maximum expected cumulative reward at any next state xt+1 which is estimated using theGPυ. Furthermore, a good model ofQk*in joint state–action space would require substantially more training points which makes the joint GP model too expensive in computational terms. When compared with RL algorithm in Fig. 3, the setsXandUcontain now support points of the GP models for value functions rather than a discrete representation of the state and action spaces. Also, by resorting to GPs, the algorithm GPRL can readily handle uncertainty in state transitions due to stochastic disturbance dynamics along with noisy measurements and rewards.In Algorithm 2, once the whole recursion for obtaining the support setXkand updated optimal controlsπk*(Xk)have been completed, a new version of the control policy is obtained in the form of a Gaussian process, the policy GP:GPπ*Xin line 23. Policy iteration converges when distributions for control policies are “close” enough. As control policies in successive iterations are also modeled using GPs, policy iteration can be stopped when the sum of the Kullback-Leibler (KL) divergences over the support setXk, with cardinalityXk, between two successive policy GPs is lower than a small tolerance δ(16)Kprevious−KcurrentKprevious⋅100%≤δwhere:(17a)Kprevious=∑x∈XkKL(GPπk−2*(Xk)||GPπp−1*(X))/||Xk||=∑x∈XkGPπk−1*(x).logGPπk−1*(x)GPπk−2*(x)/||Xk||(17b)Kcurrent=∑x∈XkKL(GPπk−1*Xk||GPπp*X)/||Xk||=∑x∈XGPπk*x⋅logGPπk*xGPπk−1*x/||Xk||Typically, only a few iterations are necessary for the control policy distributions to converge according to the criterion in Eq. (16). The optimal controlsπ*(X)(line 23 of the Algorithm 2 in Fig. 3) returned by GPRL are understood here as noisy estimations of optimal actions under uncertainty. To generalize an optimal, continuous-valued control policy we have to model it based on a finite number of evaluations in the support setX.Active learning can be considered as an organized and constrained self-exploration processes to minimize the sample complexity, i.e. reaching a given performance level with a minimal number of examples. The BAL procedure includes two main stages [36]. Firstly, a set of potential promising data must be determined, i.e. determining a set of candidate states(X∘). Secondly, based on this set the most relevant data (collected in the setsX¯pandU¯p) must be determined and then added to the support sets of the GP models for the value and policy functions.The candidate states can be determined as the predicted means for the dynamics GP model (GPf) of the successor states when starting from eachxi∈Xk−1and applying all actions from the finite and invariant setU. In this way, the set of candidate states is defined byX∘=E[GPf(Xp−1,U)]∀(xi,ui)∈(Xp−1×U).For efficiency, only ϑ states are chosen to augment the support setXpused for training the GPs at the pth iteration. In reinforcement learning control, a “good” statexi°∈X∘should provide both information gain about the latent value function and be able to reduce uncertainty about the optimal controls. Hence, similarly to Deisenroth et al. [36], a utility functionUthat captures both objectives to rate the quality of candidate states is used here to infer GPs. We aim to find the ϑ most promising statexi°∈X∘that maximize the expected utility function based on Bayesian averaging using(18)U(xi°)=ρE(V(xi°)X∘)+β2log(varv[V(xi°)X∘])where the predictive mean and variance of the value function are(19)Ev[Vk(xi°)X∘)]=Covv(xi°,X∘)Covv(X∘,xi°)−1;varv[Vk(xi°)X∘)]=Covv(xi°,xi°)−Covv(xi°,X∘)Covv(X∘,xi°)−1Covv(X∘,xi°)The parameters ρ and β are used to trade off optimality in action selection with information gain. A large (positive) value of ρ introduces data bias toward optimal controls, whereas a large value (positive) for β favors gaining information based on the predicted variance of the value function corresponding to unseen states at different decision stages. Then, the utility of a candidate statexi°expresses how much total reward is expected from it when acting optimally (first term) and how surprisingV*(xi°)the candidate state is given the current training inputsXkof the GP model for V(·) (second term). The second term in Eq. (18) is somewhat related to the expected Shannon information (entropy) of the predictive distributionV*(xi°)or the Kullback–Leibler divergence [36,39] between the predictive Gaussian distribution ofV*(xi°)XkandV*(Xk).The most ϑ promising statesxi°∈X∘,i=1…ϑ, based on the utility function (Eq. (18)), are then identified. Accordingly, state transitions must be simulated by applying each actionui∈Uat the corresponding statexi∈Xp−1. The setX¯pof the ϑ observed state transitions are added toXp−1to obtain the augmented support setXpwhereas the applied actionsui∈U¯p⊂U,i=1…ϑ,are used to augment Up–1 such that Upis obtained. Using the augmented support setsXpand Upfor states and actions, the dynamics GPfis then updated.To find an optimal control policy, Bayesian active learning is incorporated into the algorithm described in Fig. 3. Trough adding a Bayesian active learning (BAL) procedure to the formulation of Algorithm 2, we seek to learn probabilistic meta-models of the glucose-insulin dynamics and the value functions on the fly. Also, the exploration can be concurrently driven in various sub-spaces with an expected increase in informational gain and are thus relevant for finding an optimal policy. In this way, instead of a globally, sufficiently accurate value function model, the proposed approach aims to find a locally valid value function model in the neighborhood of promising state trajectories using an increasingly improved sequence of control actions such that only a relevant part of the state space is explored.A priori, it is unclear which parts of the state space are relevant for approximating the policyπ*(Xp). Relevance is rated by the utility function within a Bayesian active learning framework (Eq. (18)) in which the posterior distribution of the value function model GPvplays a central role. This algorithm largely exploits information, which is already computed within the GPRL algorithm. Instead of a globally, sufficiently accurate value function model, the policy iteration algorithm aims to find a locally valid value function model in the neighborhood of promising state trajectories using an increasingly improved sequence of control actions.In Fig. 4, the main steps of the GPRL algorithm for policy iteration using Bayesian active learning to define time variant support sets of statesXp,p=1,…,nare shown. First a small number of state transitions are simulated by applying, from different initial statesx∈X0, sequences of up to N control actions using the initial control policy π0. The resulting tuples (xt, ut, xt+1) observed along the simulated trajectories define the training data for the dynamics GP in the first iteration. The observed state transitions are necessarily noisy due to the variability in the system dynamics. Simulated data are used to define the setsX¯0,U¯0that are used to approximate GPfand GPv(lines 7 and 9). For policy iteration, the inner loop increasingly augments the size of the state support setXp,p=1,…,nusing Bayesian active learning as was described in Section 3.4.In the search for an improved control policy, policy iteration exploits current knowledge in the probabilistic value function model GPv. Gaining information means that the Algorithm 3 in Fig. 4 should explore selectively sparsely sampled subspaces with few training points (states). Thus, by adding statesx¯*to the support setXkwhich are informative for optimal control of an stochastic dynamic system, only the most promising sequence of control actions are actually tried. Using the augmented support setsXpand Upfor states and actions, the dynamics GPfis updated and consequently improved. To the best of our knowledge, Algorithm 3 is the first policy iteration algorithm that successfully integrates reinforcement learning with Gaussian processes to cope with a continuum of states and a (scalar) control action.The control task is about maintaining normal glycemic variability for insulin dependent diabetic patients. For this purpose, the reward function is conveniently designed in such a way that allows guaranteeing an acceptable behavior of blood glucose variability within a target band. Accordingly, the control task aims to maximize the expected cumulative reward of tight glycemic control over time. To this aim, the immediate reward function is defined as:(20)r=−1+e−12(G(t)−Gx)2a2;r∈[−1,0]The reward function is thus represented by a square exponential form which saturates for great deviations from a chosen reference value, Gx, of the glucose concentration. In this way, the artificial pancreas will aim to maintain the patient (real or simulated) glucose level as close as possible to the reference (Gx). But how close to the reference (Gx) the actual glucose level is allowed to vary? This is going to depend on the reward function parameters. More specifically, the controlled glucose dynamics will depend on the value for a which is the parameter that specifies the width of the desired glucose band for normoglycemia. By modifying a it is possible to control the values of BG concentration for which the reward function saturates. Hence, the difficulty of the control task for controlling glycemic variability can be stated by choosing appropriately the reward function parameters. Therefore, by fixing the parameter a, different control performances will be achieved. Choosing a relative low value for the parameter a demands a tighter control of glycemic variability which makes the control task more challenging. Accordingly, more iterations of the GPRL algorithm in Fig. 4 will be required for policy convergence, and a more elaborated exploitation/exploration strategy of the state space is needed. In this section, these issues are addressed aiming at demonstrating the superior capability of the control policy obtained using Algorithm 3. To guarantee a physiological state of normoglycemia, glycemic variability must be confined within an acceptable band of glucose values between 4 and 7mmol/L, so that a is set equal to 1.To illustrate the effect of changing the parameter a in the reward function, Fig. 5depicts the variability of blood glucose (BG) for the same (simulated) patient (diet, exercise, etc.) when the optimal policy found using Algorithm 3 in Fig. 4 is applied. For a=1.5, glycemic variability is acceptable although rather loose when compared to a much tighter BG control arising when a=1 is used in the reward function. As can be expected, the corresponding control policy is a bit more aggressive in the latter case to guarantee optimal performance in the controlling variability of BG. It is worth noting that green lines in Fig. 5 correspond to soft thresholds for BG concentrations in the range from 4 to 7mmol/L, whereas red lines correspond to hard thresholds imposed to rule out hypo- or hyper-glycemic events. Hereafter, a=1 will be chosen in all simulations runs.The control task aims to maintain safe glucose levels in diabetic patients which requires continuously changing the insulin infusion rate. In this case is assumed that control actions are taken every 6min. Hence, with the algorithm proposed in Fig. 4 an optimal control policy is sought, which means that the expected cumulative reward will be optimized over many decision steps to account for slowly unfolding effects of control actions and the parsimonious dynamics of glucose metabolism. The exploitation/exploration parameters in the utility function (18) are set to: ρ=1 and β=2. The observable state variables used to define the markovian state of the system at time t are: the measured BG concentration at time t (Gt) and the insulin flow rate in the previous time step Dt–1. Thus, a perception of the glycemic state of a patient is conveniently defined through xt=(Gt, Dt−1)T. The control action, ut, is the change to the insulin infusion rate. An advantage of perceiving the system state xtin this way is the fact that it only involves readily known variables, yet they are informative enough to describe the glycemic state for successfully controlling BG variability.During the simulation study (to be detailed in the next sub-section), the algorithm in Fig. 4 is applied to obtain a generic control policy by assuming a certain value for the Ito's parameter σito and a meal routine. Two meal routines are considered (referred as I and II) which are indicated in Table 2in terms of the carbohydrate intake profile. Accordingly, the corresponding control policies will be referred asπIσito*andπIIσito*. Furthermore, these policies will be combined in an off-line way to define a grand policyπGσito∼GPπσitoGsuch that(21a)πGσito∼GPπσitoG(mπGσito,CovπGσito)←{XGσito;UGσito}where its support sets for states and actions are the union sets:(21b)XGσito=XπIσito*∪XπIIσito*;UG=πIσito*(XπIσito*)∪πIIσito*(XπIIσito*)It is worth noting that the procedure indicated in Eq. (21) may be extended to use more than two policies to obtain an improved grand policy.One model commonly used to simulate a type 1 diabetic patient dynamics is the one proposed by Lehmann and Deutsch [32] given in Section 2 through the Eqs. (1)–(3). In the first evaluation, the Algorithm 3 performance will be assessed using the aforementioned deterministic simulation model. Subsequently, the uncertain glycemic behavior will be considered. So, to assess the performance of Algorithm 3 under uncertainty the stochastic model presented in Section 2 will be used to simulate the glycemic behavior taking into account the inter/intra patient variability. In Table 3, relevant information regarding different simulation studies is given including the policy obtained, the level of glycemic variability (featured by σitoin the simulation model) and the meal routine used. As can be seen in Table 3, σitotakes the values of 0.15, 0.25 and 0.50, which highlights that the level of variability is increased substantially by almost 350%. The remaining parameters for all simulations done using the algorithm in Fig. 4 are as follows:x0=(5.5,10)T, l=250, s=6,n=25, γ=0.98, ρ=1, β=2, δ=10%. For the simulation studies with σ=0.15 and σ=0.25, in each one of them, support sets forπI*andπII*will have 400 states. For σito=0.50, the input which defines the size of the state supports is set to 10, hence using GPs to model the generic control policiesπI*andπII*, are each based on 500 data points. In every case, the support set U is finite and made up of 20 values of control actions measured in mU/min which are evenly distributed over the interval [−10, 10]. The flow rate of the insulin infused will be changed every 6min, i.e. the sampling time is chosen equal to six.Fig. 6shows the results obtained in the simulation study #1 (in Table 3) for a diabetic patient simulated by the deterministic dynamic model proposed by Lehmann and Deustch (Section 2) under feeding routines I and II, respectively. In this case, by means of the Algorithm 3 the optimal generic policiesπI*andπII*were found for each meal routine. In the left part of Fig. 6, the corresponding glucose absorption rates from the gut Gin(t) are shown to highlight the disturbance associated with meal digestion over time. The insulin infusion rate required to control the BG concentration are depicted in the right part of Fig. 6. Routine II is definitively more demanding in terms of the exogenous insulin required for normoglycemia, but in both cases the time profile is vividly correlated with the glucose absorption rate, mainly in postprandial periods.For this case study, the Algorithm 3 is applied in a simulated patient with σito=0.15 under feeding routines I and II, respectively. Fig. 7shows the results obtained when testing the optimal generic policiesπI0.15*andπII0.15*for both meal routines. As can be seen, variability in glycemic behavior is taken in account when learning the optimal policy and that is why the stochastic model described in Section 2 is key for effective glycemic control. Glycemic variability for both meal routines is significantly low and both control policies,πI*andπII*, exhibit outstanding performance. It is worth noting that for different carbohydrate intakes, the BG evolution is always confined within the green lines as can be expected for an optimal control policy facing a rather small level of intra/inter-variability in glucose dynamics with σito=0.15.Knowledge about control policiesπI*andπII*is now used to define a grand policyπG0.15as described in Eq. (21). To test its robustness, the policyπG0.15was repeatedly applied in 125 independent simulations of a diabetic patient under meal routines I and II. Results obtained are shown in Fig. 8. As can be seen, BG levels are always confined within green lines which highlights that glycemic variability is optimally controlled. Fig. 9depicts the state support setXGfor the grand policyπG0.15. It is rather clear the effect of Bayesian active learning in biasing data gathering toward the most relevant subspace of states. Even though data points are mostly confined within the BG soft band (green lines), there are some points just below the minimum soft threshold to guarantee an appropriate response in states where a hypo-glycemic event is at stake.In order to assess the effect of increasing the variability in the glucose dynamics, let's consider now setting σito=0.25. This value has been recommended in the literature [34] to account for different source of intra- and inter-patient variability in stochastic optimal control. Results obtained using the learned optimal policiesπI0.25*andπII0.25*are shown in Fig. 10. Despite the significant variability in glucose dynamics both control policies are very effective in controlling BG. Similarly as in the previous case, now a grand policyπG0.25is defined using the support sets of policiesπI0.25*andπII0.25*. Then, the grand policyπG0.25is applied in 125 independent simulation runs providing the results which are summarized in Fig. 11. Even though a higher variability level makes BG regulation more challenging, the policyπG0.25is still able to guarantee that glycemic variability is under tight control. The 800 states in the support setXGfor the grand policy are shown in Fig. 12. As can be seen, an increased level of variability in BG dynamics requires that the relevant subspace of states for defining an optimal control policy has a larger coverage for safety and performance.This simulation scenario has been designed to test the performance of Algorithm 3 in an unusual situation for a hypothetical case where there is an extreme degree of glycemic variability. For this reason, it is assumed that σito=0.50, which means that the variability was increased 233% and 100% with respect to the simulation studies #2 and #3, respectively. Fig. 13highlights that even an unrealistic level of variability resulting from setting σito=0.50 can be handled quite well by the generic policiesπI0.50*andπII0.50*which are able to cope with such a level of uncertainty. Despite graceful performance degradation, it is noteworthy that a further increase in the Ito's parameter can be deal with by the proposed policy learning algorithm. Moreover, the corresponding grand policyπG0.5has been determined based on the support data ofπI0.50*andπII0.50*. Similarly to the previous cases,πG0.5has been evaluated in 125 independent simulation runs. Fig. 14reveals the capability of the grand policy for good glycemic control for both meal routines. The 1000 states in the support setXGfor the grand policyπG0.5are shown in Fig. 15. Note that the coverage of the support set has been enlarged so as to deal with a dynamic behavior exhibiting high variability in responding to control actions.Table 4summarizes the percentage of time spent in each glycemic zone for different combinations of variability levels and diets. The ratio is simply calculated as the total time BG readings remains within each zone with respect to the total time horizon.Grand control policies(πG)determined in the previous sections have the distinction of having a good generalization ability. In this section, the performance or safety robustness of a generic control policyπGobtained using a given value of σito but applied to a glucose dynamics having different level of variability is addressed.Firstly, the grand policyπG0.15is used for controlling a simulated patient with a higher level of variability. In Fig. 16,πG0.15has been used to control a patient with variability modeled using an Ito's parameter σito=0.25 with meal routines I and II. This means that the variability is nearly a 66.6% higher with respect to the variability taken used for learningπG0.15. Results obtained reveal that an increase in dynamic variability has little or no negative impact in the capability of the control policy to carry out the control task in a diabetic person. However, when setting σito=0.50 the control policyπG0.15somewhat degrades its performance as it is shown in Fig. 17. Nevertheless, this is an unlikely situation corresponding to a very unstable diabetic patient.When the control policy is trained using simulated patients with higher levels of variability, e.g. using a value σito=0.25, the resulting policies are more robust in terms of both performance and safety. For example, in Fig. 18the control policyπG0.25has been applied to simulated patients under the same feeding routine (e.g. II) and the glucose dynamics variability is due to Ito's stochastic processes with σito=0.15 and σito=0.50. As can be seen in Fig. 18a when dealing with a dynamics with lower variability compared with the one used to learn the policyπG0.25, the latter has outstanding performance. When dealing with a glucose dynamics having higher variability, the control policy exhibits good performance but as can be expected the capability for controlling BG variability gracefully degrades.Moreover, to test the robustness ofπG0.25to deal with changes in a person diet, two meal routines sensibly different to ones in Table 2 are used. In Fig. 19a, results obtained are shown for a meal routine (Test I) characterized by high intakes of carbohydrates which are evenly distributed from early morning to late evening. As can be seen glycemic variability is effectively controlled using insulin infusion rates that appropriately compensate for the rate of glucose absorption from the gut. It is worth considering now a meal routine with low carbohydrate intakes and long fasting periods to test the capability ofπG0.25to cope with hypo-glycemic conditions. In Fig. 19b, results obtained for this type of diet are given to highlight that despite variability in glucose dynamics and changes in patient diet, the policyπG0.25is capable of achieving near-optimal performance in controlling BG.To highlight this latter viewpoint, in Fig. 20πG0.50has been applied to three different simulation scenarios. In all of them the meal routine II was assumed and only the patient glycemic variability was varied. Hence, low glycemic variability (σito=0.15), average glycemic variability (σito=0.25) and finally almost chaotic glucose dynamics (σito=0.75) were considered. For the latter (Fig. 20c), despite some loss of performance, glycemic variability is still successfully controlled. For the other cases of glucose dynamics,πG0.50behaves with nearly optimal performance (Fig. 20a and b).Table 5summarizes the percentages of time in each glycemic zone for each testing scenario of a certain policy. Just as in Table 4, the relevant information in Table 5 is the percentage of overall time in each Glycemic zone. As a priori might be predicted, the most robust policy isπG0.50which has an optimal performance to face daily changes in glycemic behavior. As can be expected, when it is tested under a significantly larger variability than the one used for training, its performance is somewhat degraded (Fig. 20c). However, glucose levels do not significantly overshot hard thresholds (red solid lines in Fig. 20c).In Section 4.2.2, the robustness of the learned policy against different glycemic variability degrees was tested. However, the effect of calibration errors in glucose sensors is not negligible and an explicit analysis of the induced effects in controlling blood glucose variability using the optimal control policy follows. As continuous glucose sensors are placed in the subcutaneous tissue, they determine interstitial fluid (IG) rather than plasma concentration (G). Hence, under dynamic conditions, IG and G values are necessarily different because of a time-lag between both magnitudes. The magnitude of the time lag may be no more than 5min in normal conditions [40]. To simulate this effect, the model proposed by Facchinetti et al. [6] can be used, where each IG value is obtained through integrating a G-IG model (Eq. (22)); ρ is the static gain of the IG dynamics (considered equal to 1) and τ is the time-lag constant.(22)dIG(k)=−1τIG(k)+ρτG(k)In order to simulate noisy sensory data, each IG value is multiplied by a random time-varying calibration error ξ(k) and then corrupted by an additive noise sequence sampled from a zero mean white Gaussian noise process v(k) such that the reading from a continuous glucose monitor (CGM) is given as:(23)CGM(k)=(1+ξ(k))IG(k)+v(k)As continuous glucose sensors are typically placed in the subcutaneous tissue, they determine BG concentration in the interstitial fluid rather than plasma concentration. Clearly, setting ξ=0 corresponds to an optimally calibrated sensor. As described in Eq. (24), the calibration error ξ(k) can be modeled using a triple integrator of a zero mean white noise w(k). For implementation details refer to the work of Facchinetti et al. [6].(24)ξ(k+1)=3ξ(k)−3ξ(k−1)+ξ(k−2)+w(k)In Fig. 21, plasma glucose levels (blue line) and sensor responses (magenta line) are depicted for different sensor calibration errors and time-lag values. The BG dynamics is simulated with a value of σito=0.25 and it is controlled by the optimal policyπG0.50. As can been seen, as the time-lag (τ) and/or calibration error (ξ) increases, the gap between plasma glucose levels and sensor response levels also increase.Next, the setting of simulation study #4 in Section 4.2.1 is repeated, but now including sensor errors. Thus, a default time-lag of τ=5min and a calibration error of ξ=2% are used for learning the optimal policy. Thus, the policy obtained isπGs0.5. In Fig. 22, results obtained when testing the policyπGs0.5under meal routine II (Table 2) using different time-lags (τ) and calibration errors (ξ). Data obtained are summarized in Table 6.As can been seen in Fig. 22, the Algorithm 3 proposed is able to find a control policy which is capable of dealing successfully with the sensor calibration errors while maintaining the glucose variability within a safe range. Due to the capability of the Algorithm 3 (Fig. 4), which learns from discounted rewards, as expected, Fig. 22 reveals that the effect of the time lag is less harmful than the calibration error for the learned control policy. Moreover, the performance of the optimal control policy gracefully degrades as sensor errors increase significantly.To end this section, a comparison of the proposed approach is made against an optimal insulin profile obtained in the work of Acikgoz and Diwekar [34]. A worthy comparison is thus made between the control policyπII0.25*and a stochastic optimal solution which is implemented without the need of a glucose sensor feedback. The optimal insulin infusion profile that is applied “open-loop” was obtained by solving a stochastic optimal control problem using the model described in Section 2 with σito=0.25. To make a fair comparison, three different meal routines were considered such that carbohydrate intakes are significantly varied. Testing meal routines are thus different to the ones used to learnπII0.25*and the optimal insulin infusion profile, namely routine II. Results obtained for each meal routine are shown in Figs. 23–25, respectively. It is worth noting that the optimal insulin profile was obtained using the same meal routine and the same stochastic patient model used for learning the policyπII0.25*.The policyπII0.25*exhibits exceedingly better performance in controlling glycemic variability when compared to the optimal insulin profile for the three meal routines considered. For low carbohydrate intakes, the insulin profile developed by Acikgoz and Diwekar [34] is yet able to maintain BG concentration within the hard thresholds (red lines) but the amplitude of glycemic excursions are quite high. As the carbohydrate content in the diet increases (see Figs. 24 and 25), the optimal insulin profile found by Acikgoz and Diwekar [34] is not capable of controlling glycemic variability whereasπII0.25*is always able to maintain BG levels within soft thresholds (green lines). It can be argue that better results can be obtained for the optimal insulin profile if it is specifically derived for each meal routine. This is not only impractical, but also against the main objective of controlling glycemic variability under uncertainty: devising a BG control strategy that is effective despite changes in a patient lifestyle, including its diet.To guarantee low glycemic variability in Type 1 diabetic patients under uncertainty, a novel approach based on simulation-based learning of a generic control policy has been presented. Off-line policy learning is based on a stochastic dynamic model of glucose metabolism that accounts for both intra- and inter-patient variability. The integration of reinforcement learning with Gaussian processes in a policy iteration algorithm has been proposed. To speed up policy convergence, the use of Bayesian active learning was instrumental in biasing data gathering in such a way the resulting state support set makes the policy both robust to uncertainty and capable of achieving near-optimal behavior.Important conclusions can be drawn from results obtained in Section 4. Firstly, a generic control which has been learned using a significant level of simulated variability through the chosen Ito's parameter (say σito=0.50) will enjoy excellent performance for lower levels of variability in the glucose-insulin dynamics and will exhibit acceptable behavior for even highly unstable patients (σito=0.75). Secondly, changes in patient diets can be handled with little performance degradation. At last but not least, the proposed optimal policy can be easily programmed in wireless devices to facilitate a normal lifestyle in diabetic patients without any need for anticipating meals or other disturbances that affect glucose metabolism. All in all, integration of off-line policy learning using the algorithm GPRL is a promising alternative to realize an artificial pancreas in diabetic patients.Although the effects of glucose sensor errors in controlling blood glucose variability were assessed, other error sources (e.g., infusion pump and wireless signals) in an artificial pancreas should be accounted for so as to make the optimal control policy more robust. The main issue in this regard is addressing the trade-off between robustness and optimality. Furthermore, for safe implementation, is of paramount importance to integrate a control-loop monitoring function in the design of the artificial pancreas. Finally, clinical evaluation of the optimal control policy is mandatory to promote wider acceptance of the proposed approach.

@&#CONCLUSIONS@&#
