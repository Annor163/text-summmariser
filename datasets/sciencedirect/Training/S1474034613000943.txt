@&#MAIN-TITLE@&#
Vision-based material recognition for automated monitoring of construction progress and generating building information modeling from unordered site image collections

@&#HIGHLIGHTS@&#
We propose a robust vision-based method for material detection from single images.A new Construction Materials Library containing 20 typical construction materials.We achieve accuracies of 97.1% for high quality 200 by 200 pixel color images.We maintain accuracies above 90% for images as small as 30 by 30 pixels.We maintain accuracies above 92% for highly compressed, low quality images.

@&#KEYPHRASES@&#
Material recognition,Building information models,Texton,Support vector machine,

@&#ABSTRACT@&#
Automatically monitoring construction progress or generating Building Information Models using site images collections – beyond point cloud data – requires semantic information such as construction materials and inter-connectivity to be recognized for building elements. In the case of materials such information can only be derived from appearance-based data contained in 2D imagery. Currently, the state-of-the-art texture recognition algorithms which are often used for recognizing materials are very promising (reaching over 95% average accuracy), yet they have mainly been tested in strictly controlled conditions and often do not perform well with images collected from construction sites (dropping to 70% accuracy and lower). In addition, there is no benchmark that validates their performance under real-world construction site conditions. To overcome these limitations, we propose a new vision-based method for material classification from single images taken under unknown viewpoint and site illumination conditions. In the proposed algorithm, material appearance is modeled by a joint probability distribution of responses from a filter bank and principal Hue-Saturation-Value color values and classified using a multiple one-vs.-allχ2kernel Support Vector Machine classifier. Classification performance is compared with the state-of-the-art algorithms both in computer vision and AEC communities. For experimental studies, a new database containing 20 typical construction materials with more than 150 images per category is assembled and used for validation. Overall, for material classification an average accuracy of 97.1% for200×200pixel image patches are reported. In cases where image patches are smaller, our method can synthetically generate additional pixels and maintain a competitive accuracy to those reported above (90.8% for30×30pixel patches). The results show the promise of the applicability of the proposed method and expose the limitations of the state-of-the-art classification algorithms under real world conditions. It further defines a new benchmark that could be used to measure the performance of future algorithms.

@&#INTRODUCTION@&#
Material classification is an important part of any vision-based system for automated construction progress monitoring or generation of semantically-rich as-built 3D models. Beyond 3D geometrical information in form of point cloud models, such tasks require additional semantic information such as construction materials and interconnectivity to be recognized for building elements. In the case of materials such information can mainly be derived from appearance-based data contained in 2D imagery.Current state-of-the-art methods for automatically monitoring construction progress or generating building information models (BIM) primarily focus on using laser scanners or image-based 3D reconstruction methods to generate 3D point cloud models [1–13]. These methods can generate the required geometrical information for producing BIM [13]. Nonetheless, for generating semantically-rich models, beyond geometrical information, material and spatial relationship/interconnectivity among elements (e.g., a beam is supported by a column) needs to be extracted from the collected data. In this paper, we focus primarily on the problem of automated material classification. Automated material classification (term used interchangeably with recognition in computer vision literature) not only helps with deriving appearance-based data required for construction progress monitoring purposes, but it also helps with segmentation of elements for automated modeling of the semantically rich as-built 3D models. In the following, the overall process of automatically generating BIM, the process of construction progress monitoring, and the need for material recognition are discussed in detail. Next, the state-of-the-art techniques for material classification both in the Architecture/Engineering/Construction (AEC) and computer vision communities are reviewed and their limitations are presented. Building on the state-of-the-art texture recognition methods, a vision-based method for AEC industry is presented and is also accompanied with exhaustive validation experiments to benchmark its performance on all aspects of feature extraction, clustering, and learning. A comprehensive dataset and a set of validation methods that can be used in the field for development and benchmarking of future algorithms are also introduced. The perceived benefits and limitations of the proposed method in the form of open research challenges are presented. Detailed performance data, the experimental and validation codes, the benchmarking dataset, along with additional supplementary material for the proposed vision-based method can be found at http://raamac.cee.illinois.edu/materialclassification.3D modeling of the as-built environment is used by the AEC industry in a variety of engineering analysis scenarios. Significant applications include progress monitoring of construction sites, quality control of fabrication and on-site assembly, energy performance assessment, and structural integrity evaluation. The modeling process mainly consists of three sequential steps [11,13]: data collection, modeling, and analysis. In current practice, these steps are performed manually by surveyors, designers, and engineers. Such manual tasks can be time-consuming, expensive, and prone to errors. While the analysis stage is fairly quick, taking several hours to complete, data collection and modeling can be the bottlenecks of the process. The data collection can spread over a few days, nonetheless the modeling stage can span over multiple weeks or even months. Additionally, modeling often tends to be specific to certain analysis, making the application of one model to multiple analyses very challenging. Thus, the applicability of as-built modeling has been traditionally restricted to high latency analysis, where results need not be updated frequently. In fast changing environments such as construction sites, due to the difficulty in rapidly updating 3D models, model-based assessment methods for purposes such as progress or quality monitoring have had very limited applications. In consequence, there is a need for a fast, low-cost, reliable, and automated method for as-built modeling. This method should quickly generate and update accurate and complete semantically-rich models in a master format that is translatable to any engineering scenario and can be widely applied across all construction projects [11,13]. The development of such a 3D modeling method also needs to be accompanied with a low-cost and fast data collection process that is able to generate accurate and complete data sets for modeling purposes.Over the past decade, research on as-built modeling has primarily focused on data collection and modeling techniques that can generate 3D geometrical information. Cheok et al. [14] is among the early works that demonstrated how LADAR (Laser Distance and Ranging) can be used as an as-built 3D modeling tool for construction monitoring purposes. This led to more systematic studies of structured light data collection techniques [13,15,16]. Several research studies have focused on showing that Terrestrial Laser Scanning (TLS) techniques provide sufficient accuracy for use in construction dimensional surveying and Quality Assurance/Quality Control (QA/QC) [8,7,16–18]. Another line of research focuses on automated registration of 3D laser scanning point clouds with BIM (e.g., [6,13,19]) for the purpose of controlling the quality of the data collection. Turkan et al. [4,5] and Bosche et al. [7–9] have also focused on automated recognition of 3D CAD objects in site laser scans for project performance control purposes.While collecting data for as-built modeling using laser scanners can generate very accurate and complete geometrical models, in most cases the approach suffers from the lack of a semantic understanding of the scene; in particular the ability to account for construction materials. Indeed, there is a need for distinguishing different materials (e.g., formwork vs. finished concrete or concrete masonry units vs. facade bricks) during the 3D modeling process. Nonetheless, the laser scanning point clouds in their XYZ form do not provide the data necessary for such analysis. The recent addition of mounted cameras to laser scanners has allowed RGB color information to be associated with each 3D point. Recent works by Kim et al. [20] and Son et al. [21] proposed using color to identify concrete elements and construction equipment. However, this approach has not yet been shown to be applicable for the simultaneous classification of multiple classes of construction materials.Over the past few years, cheap and high-resolution digital cameras, extensive data storage capacities, and the availability of Internet connections on construction sites have enabled capturing and transmitting information about construction performance on a truly massive scale. Photographic data has become the favored documentation medium, as it contains rich information (geometry and appearance), and data is collected quickly and inexpensively. In the meantime, exciting research progress has been made on techniques that can assist with 3D point cloud modeling of construction sites or individual building elements using digital imagery. For example, Golparvar-Fard et al. [1,22] presented a new algorithm based on Structure-from-Motion (SfM) coupled with Multi-View Stereo (MVS) [23–25] and Voxel Coloring [26] for generating dense 3D point cloud models from unordered imagery. Fathi et al. [27–29] also presented a method for generating 3D point cloud models of infrastructure using video streams. Several research projects such as [30–35] have also focused on evaluating the accuracy of as-built 3D modeling from photos and laser scanners. In majority of the experiments conducted using laser scanners, compared to the image-based reconstruction methods, a higher density and completeness of the point cloud models has been observed. Despite the recent advancements, the generation of semantically-rich Solid Geometric Models (SGM) compatible with the Industry Foundation Classes (IFC) format still poses three main open research challenges:1.Segmentation: robust techniques in recognition are required to segment a 3D point cloud dataset based on geometric and appearance information into distinct subsets;Object placement: techniques in geometrical modeling and recognition are needed to populate the scene with distinct BIM objects based on the segmented subsets;Inter-object relationships: techniques in recognition are required to identify the physical relationships between BIM objects in the scene.Ongoing research on image-based 3D modeling techniques can benefit from appearance information during the production of semantically-rich SGMs and progress monitoring; in particular:1.For producing surface and solid geometry: by segmenting point clouds based on appearance information so each subset can be separately used for extracting geometric information (see Fig. 1),For associating semantic information to geometry: by recognizing construction materials to semantically label each generated 3D element (see Fig. 2),For producing semantically-rich SGMs: by assembling adjacent 3D elements of the same material into complete objects, andFor progress monitoring: by recognizing construction materials to detect the correct stage of the construction operation (see Fig. 3).The need for extracting semantic information from a scene has yielded two main computer vision research thrusts: object recognition and material recognition. Although object recognition methods have made significant advances in recent years [36], they tend to rely on material-invariant features and overlook material specificity. Although the visual characteristics of an object is to some degree a function of its material category, different classes of objects can be made of the same material and a given class of objects can be made from different materials [36]. Particularly, in the context of construction projects, shape and material of objects tend to have a weak correlation. Similar shapes can be made up of different construction materials; e.g., a foundation element as well as formwork section can both be shaped are rectangular boxes. Also objects of a same material can have high shape variability; e.g., (1) a concrete column can be rectangular as well as circular; (2) a column, a beam, or a slab can all be made of concrete.Material classification for automation in construction is closely related to, but different from material and texture classification in computer vision in interesting ways:(1)In computer vision, most methods assume there is no strong prior available for material recognition, and thus these methods should be able to handle classification on surfaces with certain randomness in orientation and periodicity in both their texture and geometry. In contrast, for the purpose of progress monitoring in construction, model-based methods such as [1,4,5] can take advantage of an expectation of materials present in the scene;Several works in computer vision such as [37] define texture in terms of dimensions such as periodicity, orientedness, and randomness. As addressed by Liu et al. [36], surfaces made of different materials can have similar texture patterns and thus existing computer vision methods for texture recognition such as [38,39] may not be ideal for material classification.In the context of material classification, a variety of feature based strategies have emerged and proven applicable to more loosely defined scenes. They are usually coupled with machine learning techniques for training and testing purposes. Popular features include filter responses [38,40,41] and image patches [42], and have been coupled with a Nearest Neighbor (NN) learning [39,43]. Although they have shown promising results reaching over 95% accuracy, yet they do not perform well with images collected from construction sites. The authors conducted exhaustive experiments (see Section 6.1) and in most cases accuracy dropped to 70% and lower using real-world construction imagery. Brostow et al. [44] have proposed to incorporate SfM point cloud information to appearance features for recognition and segmentation, however only accuracies of below 70% have been reported from real-world data. Caputo et al. [45] have shown that Support Vector Machine (SVM) learning can improve on NN, yet its performance has not been tested under real world construction conditions with high degree of variability. Classification in computer vision has been tested mostly on controlled and/or limited photographic data sets like CUReT (Columbia-Utrecht Reflectance and Texture Database), KTH-TIPS2 [45], and Brodatz textures (http://www.ux.uis.no/tranden/brodatz.html) and University of Illinois at Urbana-Champaigns textures (http://www-cvr.ai.uiuc.edu/poncegrp/data/). Dissatisfaction with these benchmarks has driven the emergence of real world data sets, like Flicker Materials Database [46], to take into account inconsistencies in data quality and context. In this respect, there is still no comprehensive construction materials library that could be used for training and testing of material classification algorithms for the purpose of semantically-rich as-built 3D modeling.Automated recognition of materials in a construction setting has been of particular importance to construction progress monitoring and quality control. Brilakis et al. [47–49] are the first to use texture recognition techniques to facilitate image retrieval as well as identify building elements from 2D images. Their ground-breaking work validated applicability of texture recognition for material classification. It also showed that a material-based construction site image retrieval method based on texture recognition can successfully support image queries on construction sites by pre-identifying the materials in each image and comparing signatures of materials instead of image signatures [47,48]. In [49], Brilakis and Soibelman showed that shape features increase the flexibility of multi-modal image retrieval methods and can further improve detection of elements by differentiating them into nonlinear and linear materials of certain directionality (e.g., steel columns, concrete beams). Zhu and Brilakis [50] further improved the method for automated concrete detection in image data. These methods clearly established the applicability of image-based texture recognition for material classifications. Kim et al. [51] developed a scanning technique that utilizes laser scanner point cloud data to automatically identify the sizes of stone aggregates.Nonetheless, to support automated generation of BIM and/or generate models for progress and quality monitoring purposes, the state-of-the-art approaches for material classification both in computer vision and AEC communities still need to be improved in the following ways:1.There is a need for testing various types of features and machine learning classification methods to form a robust automated material classification technique that maintains a high level of accuracy in uncontrolled construction environments. In Section 6, the performance of the state of the art algorithms are compared for uncontrolled construction environments.There is a need for systematic data collections and comprehensive datasets of material classification in uncontrolled construction environments that can be used for benchmarking material recognition algorithms.In the following sections, a new method and a comprehensive dataset for material classification are introduced. The performance of our proposed method is also compared against the state-of-the-art methods in both computer vision and AEC communities.Our proposed image-based material recognition algorithm is based on a statistical distribution of filter responses over the images in form of primitives such as edges, spots, waves, and Hue-Saturation-Value (HSV) color values, and is divided into two stages: learning and classification. The statistical distribution of filter responses has been shown to be a good descriptor for texture recognition [39,45]. The main reason for choosing color values similar to others such as [45,47,48] as it is validated in Section 6.3 is to improve robustness of the proposed method for recognition of materials when images are taken far from objects and texture information is less apparent. In addition, the HSV color values are chosen to improve the robustness to changes in brightness which can be dominant in construction imagery. Fig. 4shows how the brightness and color gradients of a concrete foundation wall can change when images are taken far from the elements.The main technical contributions of this work are (1) a Bag of Words (BoW) pipeline for forming statistical distributions of both filter responses and HSV color values; (2) a multiple binary SVM classifier that can robustly learn and infer construction material categories; and (3) a construction material library and validation metrics for future algorithmic developments. Fig. 5shows an overview of the learning and classification components of our proposed algorithm. Each step is discussed in the following subsections.In the learning stage of our algorithm, training images are initially converted to gray-scale and their color intensities are normalized to have zero mean and unit standard deviations. This will make the distribution of the color values less dependent on the illumination which varies significantly on a construction site. Next, these images are convolved with a comprehensive filter bank [38] to generate filter responses using following equation:(1)I[x,y]∗f[x,y]=∑k=n1n2∑l=m1m2I[k,l].f[x-k,y-l]whereI[x,y]is the normalized gray-scale intensity at(x,y),f[x,y]represents intensity of filterfat(x,y), and(n1,n2)and(m1,m2)represent the range of the filter bank. The filter bank consists of 48 filters similar to [39], first and second derivatives of Gaussian at 6 orientation and 3 scales (total of 36), 8 Laplacian of Gaussian filters, and 4 Gaussians. The scales of filters in this filter bank range between σ=1 to 10. These filters are shown in Fig. 6.Next, following a bag-of-words model, we cluster the filter responses to generate codebooks of material appearances using the k-means clustering method [52]. The cluster centers (codewords) will be Textons which are visual atoms of the visual perception of materials [38,53]. Thus, the texture of the materials will be defined by modeling the texton frequencies learned from the training images. For all images captured under various illumination and viewpoints for all different materials in the training dataset, the filter responses are concatenated. Next, a fixed number of texton cluster centers are computed using the k-means clustering algorithm where k is the number of visual words (the size is discussed in Section 6.1). After quantizing individual features into k visual words, where each feature is assigned a membership to the closest cluster, the distribution of visual words per image is calculated for all of the different features by assigning each pixel in the image to the nearest visual word index and forming the histogram over the frequency of the visual words.The histogram of texton frequencies is then used to form the codebooks and the material models that are corresponding to our training images. In our work, HSV colors are also incorporated to leverage the color information for a robust material classification. It was hypothesized that since HSV instead of standard R, G, and B color channels can separately model the brightness, it can provide a higher degree of invariance to varying illumination conditions on the sites or the time images are captured. Similar to the texture recognition, for all images in the training datasets, the normalized HSV color values are aggregated and clustered into HSV textons using the (k-means clustering algorithm). Next, a histogram of color textons, i.e., the frequency with which each color texton occurs in the images, forms the codebook corresponding to each training image. The texture and color histograms are then concatenated to generate a unique codebook for each image. Fig. 6 shows the details of the training process.Our visual representation based on codebooks for construction materials is leveraged by training discriminative machine learning models of material categories. We train binary C-Support Vector Machine (SVM) classifiers [54] with linear,χ2, and radial basis function (RBF) kernels for each material category independently. SVM classifiers are discriminative binary classifiers that optimize a decision boundary between two classes. The SVM classification solves the following optimization problem:(2)τ(w,ξ)=min12w2+C∑izξi(3)subjecttoyi(wTxi+b)⩾1-ξi,i=1,2,…,m(4)ξi⩾0,i=1,2,…,mHere,wis the normal vector to the hyperplane that separates the images of a material class from others,bis the offset of the hyperplane from the origin,ξiis the slack variables for measuring the degree of misclassification of the m-dimensional histogram datax,C>0is the tradeoff between regularization and constraint violation, and finallyyiis the outcome of the classification. After solving, the SVM classifier predicts 1 ifwTx+b⩾0and −1 otherwise. The decision boundary is given by the linewTx+b=0that separates the images from a material class from others in a m-dimensional space (m: the size of Texton and HSV cluster centers).Note that the above definition applies to a linear classifier. We also use non-linear kernels due to their suitability for classification of histograms. As validated in Section 6.1, the histograms cannot be separated linearly in a reasonable way, and thusχ2(5) and RBF (6) kernels are used to map the data into a high dimensional feature space.(5)k(xi,xj)=1-∑i=1n(xi-xj)212(xi+xj)(6)k(xi,xj)=e-γxi-xj2wherek(xi,xj)=〈φ(xi),φ(xj)〉denotes the inner product in the Hilbert space and corresponds to applying the algorithm to the mapped material histogram data pointφ(xi). In this dual formulation, we maximize the following:(7)W(α)=∑i=1mαi-12∑ijαiαjyiyjk(xi,xj)(8)subjectto0⩽αi⩽Cand∑αiyi=0In this formulation, the decision function will besign(h(x)), where:(9)h(x)=∑i=1qαlylk(x,xl)+bwhereqrefers to the number of kernel computations needed to classify a joint texton and color material histogram with the kernelized SVM. For solving these equations, we use LibSVM [55] andχ2kernel module of [56].In order to extend the binary classification decision of each SVM classifier to multiple classes, we adopt the one-versus-all multi-class classification scheme where a classifier for each material is defined. When training the SVM classifier that corresponds to each material class, we set all the examples from that class as positive and the examples from all other classes as negatives. The result of the training process is one binary SVM classifier per material of interest. Given a novel testing image for which we need to identify the material, we apply all binary classifiers and select the material class corresponding to the classifier with the highest classification score.Due to the lack of existing datasets for benchmarking performance of material recognition algorithms in real-world conditions, it was necessary to create a new comprehensive Construction Materials Library (CML) that enables recognition for a large quantity of material types recorded in a variety of as-built contexts. This dataset is for both training and testing purposes so that it can be released to the community for further development and validation of new algorithms. For this purpose, we collected a library of 3000 material samples, each being an uncompressed color image of resolution200×200pixels. These images were recorded from seven different scenes (i.e., 5 construction sites and 2 existing buildings). In order to create a comprehensive dataset, varying degrees of viewpoint, scale, and illumination where accounted for during the collection period span of seven months. The following describes the dataset:1.Material categories: The CML comprises 20 major construction material types: Asphalt, Brick, Cement-Granular, Cement-Smooth, Concrete-Cast, Concrete-Precast, Foliage, Form Work, Grass, Gravel, Marble, Metal-Grills, Paving, Soil-Compact, Soil-Vegetation, Soil-Loose, Soil-Mulch, Stone-Granular, Stone-Limestone, Wood.Sub-categories: Each course material definition is broken down to several representative sub-types, depending of the material. For example, concrete can be present in a scene as smooth finish, course finish, good condition, or deteriorated. Additionally, each material exhibits a specific appearance under different lighting conditions. Two to four lighting conditions per material where documented.Poses: Scale and orientation variability is documented for each sub-category. For a digital resolution of200×200pixels, images representing an approximate physical length of12″,24″and48″were gathered. Three to five view orientations in respect to the surface normal were also considered. Thus, a total of 9–15 poses make up each material sub-category. One sample for the24″and48″scales and two non-overlapping samples for the12″scale were extracted from each pose. The entire dataset is made public at: http://raamac.cee.illinois.edu/materialclassification. Fig. 7shows examples of the materials in the CML.To quantify and benchmark the performance of the material classification algorithm, we plot the Precision–Recall curve and use a confusion matrix to represent average classification accuracies. These metrics are both set-based measures; i.e., they evaluate the quality of an unordered set of data entries. Here, we define precision and recall as follows:(10)precision=TPTP+FP(11)recall=TPTP+FNwhere in TP is the number of True Positives, FN is the number of False Negatives and FP is the number of False Positives. For instance, if a brick image is correctly recognized under the brick class, it will be a TP; if a concrete image is incorrectly recognized as brick, it will be a FP for the brick class. When a brick image is not recognized under the brick class, then the instance is a FN. The particular rule used to interpolate precision at recall level i is to use the maximum precision obtained from the material class for any recall level great than or equal to i. For each recall level, the precision is calculated, and then the values are connected and plotted in form of a curve.The performance of the one-vs.-all multi-class material classifiers is analyzed using a confusion matrix. The confusion matrix returns the average accuracy per material class. The average accuracy of the material classification is calculated using the following formula:(12)accuracy=TP+TNTP+TN+FP+FNA confusion matrix shows for each pair of material classes〈C1,C2〉, how many material images fromC1were assigned toC2. Each column of the confusion matrix represents the predicted material class and each row represents the actual material class. The detected TP and FP are compared and the percentage of the correctly predicted classes with respect to the actual class is created and represented in each row.

@&#CONCLUSIONS@&#
