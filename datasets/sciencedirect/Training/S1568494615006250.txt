@&#MAIN-TITLE@&#
Gray Wolf Optimizer for hyperspectral band selection

@&#HIGHLIGHTS@&#
We propose a new approach for feature selection in hyperspectral image classification.The problem of band selection is reformulated as a combinatorial problem.We design a new objective function which takes into account two term, the classification error rate and the class separability distance.To optimize the objective function, we propose to use a new meta-heuristic called Gray Wolf Optimizer.The experiments have been conducted in three widely used hyperspectral images and compared with other approaches.

@&#KEYPHRASES@&#
Band selection,Hyperspectral image classification,Gray Wolf Optimizer,Class separability,Hausdorff distance,Jeffries-Matusita distance,

@&#ABSTRACT@&#
In this paper, we propose a new optimization-based framework to reduce the dimensionality of hyperspectral images. One of the most problems in hyperspectral image classification is the Hughes phenomenon caused by the irrelevant spectral bands and the high correlation between the adjacent bands. The problematic is how to find the relevant bands to classify the pixels of hyperspectral image without reducing the classification accuracy rate. We propose to reformulate the problem of band selection as a combinatorial problem by modeling an objective function based on class separability measures and the accuracy rate. We use the Gray Wolf Optimizer, which is a new meta-heuristic algorithm more efficient than Practical Swarm Optimization, Gravitational Search Algorithm, Differential Evolution, Evolutionary Programming and Evolution Strategy. The experimentations are performed on three widely used benchmark hyperspectral datasets. Comparisons with the state-of-the-art approaches are also conducted. The analysis of the results proves that the proposed approach can effectively investigate the spectral band selection problem and provides a high classification accuracy rate by using a few samples for training.

@&#INTRODUCTION@&#
Recently, hyperspectral image classification has been a very active research field in many applications [1–5]. It consists of affecting for each pixel a specific label. The hyperspectral images are composed of hundreds of bands with a very high spectral resolution from the visible to the infrared region [6]. The classification of pixels in hyperspectral image represents a difficult task because the few labeled pixels and the large number of spectral bands increase the capability to detect and distinguish various classes. This is why, the Hughes phenomenon is encountered. In hyperspectral image, the band can be not only redundant, but, do not all contain the relevant information, some bands are irrelevant and others are highly correlated which decrease the classification accuracy rate [7]. Feature selection (band selection) is the process of selecting the smallest subset of relevant features which will be used to construct the classification model. The feature selection algorithms can be subdivided into two categories: wrapper and filter methods. The wrapper methods use a classification model to compute the score of selected features. They are more computationally, but provide good results [8]. Filter methods are based on heuristic scoring to measure the quality of features and they do not use any classifiers. The filter methods are less computationally, but they provide a feature subset independently of classifiers [9,10].Several studies have been conducted in supervised and unsupervised band selection problems. In [11], the authors proposed a new band selection approach called Minimum Noise Band Selection. This approach is based on the quality of each band which is determined by high SNR and low correlation. The sequential backward selection is used to improve the search efficiency. A new band selection framework has been proposed in [12]. This approach uses an evolutionary strategy to handle the high computational burden associated with groupwise-selection-based methods. In [13], the authors developed a new band selection method called Progressive Band Selection (PBS) which is a very different from the traditional band selection approach in the sense that the former adapts the number of selected bands, p to various endmembers used for spectral unmixing, while the latter fixes the value of p at a constant for all endmembers. In [14], a criterion based on mutual information called Trivariate Mutual Information (TMI) is used to compute the redundancy of informations for hyperspectral image classification. In [15], the problem of band selection has been challenged by using the Particle Swarm Optimization. The authors proposed to optimizing the minimum estimated abundance covariance and Jeffries-Matusita distances.In this paper, a new procedure of band selection is proposed. The problem of band selection is reformulated as a combinatorial optimization problem. We propose a new fitness function based on class separability measures and the classification accuracy rate. To optimize the fitness function, we use a new meta-heuristic called Gray Wolf Optimizer (GWO) inspired by gray wolves [16]. Recent studies have shown that the GWO algorithm is able to provide very competitive results compared to Particle Swarm Optimization (PSO) [17], Gravitational Search Algorithm (GSA) [18], Differential Evolution (DE) [19], Evolutionary Programming (EP) [20], and Evolution Strategy (ES) [16,21]. The performance evaluations of the proposed approach are conducted on three hyperspectral images: Pavia University, Indian Pines and Salinas scenes. The proposed approach is compared with several feature selection methods.The novelty of our study lies on one side in the fact that the objective function take into account two important terms: the classification accuracy rate and the class separability measure. Therefore, the aim is to select the smallest subset of spectral band that maximize the classification accuracy and the class separability. In other side, the second novelty is the use of Gray Wolves Optimizer algorithm to find the optimal subset. This algorithm is very powerful compared to various optimization methods [16]. We note that the GWO algorithm has never been used in the context of spectral band selection.The rest of paper is organized as follows: In Section 2, we present and discuss the proposed approach. Section 3 describes the experimental results and the performance of the proposed method. Finally, the conclusion is given in Section 4.In each band selection approach, the definition of the objective function and the search strategy are the important and crucial criterions. In this study, the objective function is modeled by taking into account two important terms: the accuracy rate and the class separability distance.Generally, the problem of band selection can be regarded as a binary problem and the problem formulation is defined as follows [22]:We consider a hyperspectral image D with m×n×k where m×n is the pixels number and k is the number of spectral bands. Let b={b1, …, bk} the binary vector with bi=1, the band i is selected, and if bi=0, the band is not selected. We note Y={c1, …, cp} the classes vector.The objective function evaluates candidate subsets and returns the measure of their goodness. It can be divided in two classes: filter function which evaluates the subets by their information independently of the system and wrapper function which uses the classifier to measure the subsets of feature. The purpose of this study is to design an efficient objective function that combines the classification accuracy rate and the class separability measure, i.e, combine the advantages of filter and wrapper measures.The general form of the proposed objective function is as follows:(1)J(b)=Classificationaccuracy+measureclassseparabilityThe objective function is composed of two terms: the classification accuracy rate and the measure class separability.We use the equation defined in [23] for support vector machine and we use it for K nearest neighbor. This equation proposes that the classification accuracy rate is measuring according to:(2)accuracy=∑i=1|N|assess(px)|N|,ni∈Nassess(px)=1ifclassify(px)=c0otherwisewith N represents the set of pixels items to be classified (the test set). classify(px) is the function that provides the class of px. If the class is a true label for the pixel px, the function assess(px)=1, and 0 otherwise. The accuracy function returns the classification accuracy rate. Note that the classification accuracy rate will be computed with the selected band (bi=1), the band which has bi=0 are not considered. We denote the accuracy function as follows:(3)Ja(b)={accuracy,withtheselectedbands,bi=1}To get the classification accuracy rate, we must train and test the dataset according to the selected bands and by using a classifier system [23].In feature selection, the basic idea of class separability is to select the features that better separate the classes by computing the distance between classes. There is a several distance measures, the most spread out in hyperspectral band selection are:The Hausdorff distance is a measure of similarity defined by Nadler in 1978 [24]. It measures the extent to which each point in a set is located relative to those in another set [25]. The Hausdorff distance assumes that two sets are close if each point of either set is close to some point of the other set. In this study, the Hausdorff distance is used to measure the separability between classes for each feature (band). Consider the binary problem with two classes c1 and c2. The Hausdorff distance DH(c1, c2) between c1 and c2 is defined by:(4)DH(c1,c2)=max{h(c1,c2),h(c2,c1)}(5)h(c1,c2)=maxxi∈c1minxj∈c2‖xi−xj‖where xiis the pixel set affected to the class c1 and xjis the set of pixels affected to c2. The function h(c1, c2) is called the direct Hausdorff distance from c1 to c2[26].This Hausdorff distance is defined for a binary problem. For multiclass problems, we defined the Hausdorff distance for band bias follows:(6)Dbi=1p×(p−1)∑i=1p−1∑j=i+1pDH(ci,cj)The Hausdorff distance for selected features is given by:(7)JH(B)=∑i=1kbi×Dbi∑i=1kDbiwhere∑i=1kbi×Dbiis the function for computing the Hausdorff measure for the selected bands, and,∑i=1kDbiis the Hausdorff measure for all the bands. Note that biis a binary value, bi=1 the band is selected, and bi=0 the band is not selected and it not used.The Jeffries-Matusita (JM) distance is very used in feature selection probleme [27,28]. For binary problem, the JM distance between c1 and c2 is defined as follows:(8)JMbi=2(1−eBbi)(9)Bbi=18(μc1−μc2)TΣc1+Σc22(μc1−μc2)+12lnΣc1+Σc22Σc112Σc212where μ is the class mean vector and Σ the class covariance [27].For multiclass problem, the JM distance is given by the following equation:(10)Dbi=∑i=1p∑j=1pp(ϖi)p(ϖj)JMbiwhere p(ϖ) is the class prior probability.The JM distance for selected features is given by:(11)JJM(B)=∑i=1kbi×Dbi∑i=1kDbiThe JM distance assumes that the forms of the features are Gaussian distribution in each of the classes [29].We propose to use five objective functions. The first objective function uses only the classification accuracy term:(12)J1(b)=Ja(b)The second objective function uses only the Hausdorff distance:(13)J2(b)=JH(b)The third objective function uses only the JM distance:(14)J3(b)=JJM(b)The fourth objective function is the weighted sum of the accuracy and Hausdorff terms:(15)J4(b)=ωa×Ja(b)+ωH×JH(b)The fifth objective function is the weighted sum of the accuracy and JM terms:(16)J5(b)=ωa×Ja(b)+ωJM×JJM(b)The ωais the weight of the classification accuracy rate, ωHand ωJMare the weights of Hausdorff and JM distance respectively. We have the possibility to balance between the functions or to give more importance on one term with respect to the other.The aim for the mix both terms into one search is to select the smallest subset of bands which increases the classification accuracy and the class separability distances. Therefore, the noisy bands will be eliminated. Another way to solve the problem of noisy bands is to include spatial information of the neighboring pixels in the feature space through texture information.The choice of search strategy is a very important for a feature selection technique. To optimize the objective function we must choose a very efficient optimization algorithm to converge to the global optimum and do not trap in local optimum.In this study, we use the Gray Wolf Optimizer which is a new optimization algorithm recently developed. We slightly modified the algorithm to adapt it for band selection problem.In the last decades, meta-heuristic optimization algorithms have become very popular and widely used to solve many problems from different fields. The meta-heuristics are inspired from nature, typically related to physical phenomena, animal's behaviors, or evolutionary concepts [16].Gray Wolf Optimizer (GWO) is very recent meta-heuristic algorithm developed by Mirjalili et al. [16] from Griffith University, Australia in 2014. The GWO approach is inspired by gray wolves (Canis Lupus) belongs to the Canidae family. Gray wolves live in a pack and the size of group is between 5 and 12. The leader is called alpha and is responsible for making the decision about: hunting, sleeping place, etc.The second is called beta and helps the alpha in decision making. The beta wolf should respect the alpha.The lowest ranking gray wolf is omega and it submits the information to all the others dominant wolves. The rest of gray wolves are called delta [16] and dominate the omega.The hunting process of gray wolves is as follows:•Tracking, chasing and approaching the prey.Pursuing, encircling and harassing the prey.Attack toward the prey.The mathematical model of the social hierarchy of the gray wolves is defined as follows:We consider the hunting process as the optimization procedure,•The prey is the optimum that we must find.α: the fitness solution.β: the second best solution.δ: the third best solution.The first step is the case that the gray wolves encircle prey. The mathematical model is:(17)D→=C→·Xp→(t)−X→(t)(18)X→(t+1)=Xp→(t)−A→·D→where t indicates the current iteration,A→andC→are the coefficient vectors.Xp→is the position vector of the prey andX→is the position vector of a gray wolf [16].The vectorsA→andC→are given as follows:(19)A→=2a→·r→1−a→(20)C→=2·r→2wherea→is linearly decreased from 2 to 0 during the iterations.r→1,r→2∈[0,1]are random vectors [16].(21)Dα→=C→1·Xα→−X→,Dβ→=C→2·Xβ→−X→,Dδ→=C→3·Xδ→−X→(22)X→1=Xα→−A→1·(D→α),X→2=Xβ→−A→2·(D→β),X→3=Xδ→−A→3·(D→δ)(23)X→(t+1)=X→1+X→2+X→33Gray wolves search the prey according to the position of the leader alpha, beta and delta and they diverge from each other to search the prey [16].Actually, no binary version of GWO algorithm is defined. This is why; we slightly modified the algorithm for band selection problem. The pseudo code of the GWO algorithm is described as follows [16]:Algorithm 1Gray Wolf Optimizer for feature selection1:Initialization2:Initialize randomly: a, A, C and the search agents position X(i, j) (i=1, …, number of search agent, j=1, …, dimension)3:Calculate the fitness of each search agent4:Main Loop5:Initialize the position of Xα, Xβand Xδ6:fort=1 to max number of iterationdo7:foreach search agent ido8:foreach position of the current search agent jdo9:Update the position X(i, j) by Eq. (23)10:ifX(i, j)≥0.5 then11:b(i, j)=112:else13:b(i, j)=014:end if15:end for16:end for17:Update a, A and C18:Calculate the objective function J(b) of all search agents19:Update Xα, Xβand Xδ20:end for21:Return bThe final form of the band selection problem is summarized as follows:•b is the variable to optimize. It is a binary vector with a size identical to band number. If bi=1 the band number i is selected else the band i is not selected.J(b) is the objective function. We have defined four objective functions (J1(b), J2(b), J3(b) and J4(b)).The GWO algorithm is used to optimize the objective functions. To take into consideration binray problem, we propose to use a threshold σ=0.5:(24)IfX(t+1)≥σthenX(t+1)=1elseX(t+1)=0The performance assessments of the proposed methods are demonstrated over three real hyperspectral image data sets widely used in the literature [1–6].The first experimentation is done using the Pavia University scene. This data was collected by the ROSIS over the urban area of Pavia University. This image size is 610×340 pixels. The number of spectral bands is 103 in the wavelength from 0.4 to 0.86μm. This scene contains 9 classes: Asphalt, Meadows, Gravel, Trees, Painted Metal Sheets, Bare Soil, Bitumen, Self-Blocking Bricks, and Shadows. Fig. 1shows the Pavia University hyperspectral image.The second hyperspectral image used in this study is the Indian Pine scene. This data is a small segment of AVIRIS (Airborne Visible InfraRed Imaging Spectrometer). The data is taken over the agricultural area of Northwestern Indiana, USA. The Indian Pine hyperspectral image contains 220 bands in the spectral range from 0.5 to 2.5μm. The size of the image is 145×145 pixels. The ground truth differentiates 16 classes, namely: Alfalfa, Corn-notill, Corn-mintill, Corn, Grass-pasture, Grass-trees, Grass-pasture-mowed, Hay-windrowed, Oats, Soybean-notill, Soybean-mintill, Soybean-clean, Wheat, Woods, Buildings-Grass-Trees-Drives, and Stone-Steel-Towers. Fig. 2shows the Indian Pines hyperspectral image.The third hyperspectral image is the Salinas which is taken over the Salinas valley in Southern California, USA. It was acquired by the 224-band AVIRIS. Each band is made up of 512×217 pixels in the spectral range from 0.4 to 2.5μm. The Salinas scene contains 16 ground truth classes: Broccoli-green-weeds-1, Broccoli-green-weeds-2, Fallow, Fallow-rough-plow, Fallow-smooth, Stubble, Celery, Grapes-untrained, Soil-vinyard-develop, Corn-senesced-green-weeds, Lettuce-romaine-4wk, Lettuce-romaine-5wk, Lettuce-romaine-6wk, Lettuce-romaine-7wk, Vineyard-untrained and Vineyard-vertical-trellis. Fig. 3shows the Salinas hyperspectral image.The parameters of the Gray Wolf Optimizer are setting as follows: the number of search agents is set to 30 and the number of iteration is equal to 100 iterations.The weight coefficients ωa, ωH, and ωJMof the objective function are set to 1.To computing of Ja(b) is performed by a classifier system. We propose to use the K nearest neighbors algorithm with the Euclidean distance and K=7 nearest neighbors. The classification capability of the K nearest neighbors has been demonstrated in spectral data classification [30,31]. The most significant advantage of the K nearest neighbors lies in the fact that no training model is required.These parameters have proven their performance and were chosen through experimentation for the three hyperspectral images.In each classification process, the number of instances used for training and testing phases must be determined. We propose to use 10% of pixels is the learning set, 40% pixels are considered for the test and the 50% remaining pixels are used for the validation.To overcome the overfitting problem, we use several training and test sets. In each iteration of the algorithm, we generate randomly training and test sets by using the holdout method. A validation set is used to compute the classification accuracy rate by using the optimal subset of selected features.Table 1shows the information about the number of pixels used for training and test.

@&#CONCLUSIONS@&#
