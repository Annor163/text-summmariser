@&#MAIN-TITLE@&#
Coarse-to-fine skeleton extraction for high resolution 3D meshes

@&#HIGHLIGHTS@&#
A hierarchical 3D skeletonization algorithm is proposed.Our method is based on a density-corrected Hamiltonian analysis.The discretization errors from the coarser levels are corrected using dilation.The skeleton is further refined by aligning it to the true underlying medial surface.We show that our method is robust against noise and mesh resolution.

@&#KEYPHRASES@&#
Medial surface,Hamilton–Jacobi equations,3D shape descriptor,Hierarchical approach,

@&#ABSTRACT@&#
This paper presents a novel algorithm for medial surfaces extraction that is based on the density-corrected Hamiltonian analysis of Torsello and Hancock [1]. In order to cope with the exponential growth of the number of voxels, we compute a first coarse discretization of the mesh which is iteratively refined until a desired resolution is achieved. The refinement criterion relies on the analysis of the momentum field, where only the voxels with a suitable value of the divergence are exploded to a lower level of the hierarchy. In order to compensate for the discretization errors incurred at the coarser levels, a dilation procedure is added at the end of each iteration. Finally we design a simple alignment procedure to correct the displacement of the extracted skeleton with respect to the true underlying medial surface. We evaluate the proposed approach with an extensive series of qualitative and quantitative experiments.

@&#INTRODUCTION@&#
The skeleton has proven to be a valuable and widely used shape descriptor for a number of tasks such as 2-D and 3-D shape recognition [2,3], volumetric models deformation [4,5], segmentation [6] and protein structure identification [7]. The interest in this descriptor stems from its being a concise representation of the original shape, which is topologically equivalent to it, and invariant to several shape deformations.When working in two dimensions, the skeleton, or medial axis transform, is defined as the locus of the centers of the maximal inscribed circles bitangent to the shape boundary. Alternatively, it can be defined as the set of singularity points created by the inward evolution of the shape boundary with constant velocity according to the eikonal equationB→(t)dt=vN→(t), whereB→(t)is the equation of the boundary at time t, v is the constant velocity andN→(t)is the normal to the boundary. Finally the skeleton can be seen as the set of ridge points of the distance map [8,9], where the distance map is the function D(x,y) that assigns to every point in the interior of a shape its distance to the closest point on the boundary.Over the years several methods have been proposed to compute the 2D skeleton of a shape, but all of them can be basically divided into four main categories.The first class of methods are the thinning ones, which simulate Blum’s grassfire transform by iteratively eroding layers from the shape [10,11]. During the thinning procedure care must be given not to change the object topology and to ensure the correct geometrical position of the skeleton with respect to the original shape, since the result is clearly dependent on the order in which the erosion is performed. Unfortunately, while fast and simple to implement, these algorithms are quite sensitive to Euclidean transformations, so they typically fail to locating accurately the skeleton of the object.The second class of methods exploits the fact that the skeleton coincides with the local extrema of the Euclidean distance transform [9,12,13]. This in turn relies on the computation of the Euclidean distance between each point in the interior of the object and the boundary of the shape, which can be done in linear time O(n), where n is the number of pixels of the image [14]. These approaches then attempt to detect the ridges of the distance map either directly or by evolving a series of curves, such as snakes, under a potential energy field defined by the distance map. Although these methods fulfill the geometrical constraint, ensuring the topological correctness is not trivial.A third class of methods is based on the Voronoi diagram of a subset of the boundary points [15]. The idea of these approaches is that, under appropriate smoothness conditions, the Voronoi diagram of a subset of the boundary points converges to the skeleton as the number of the sampled boundary points increases. These methods ensure topology preservation and invariance under Euclidean transformations, in addition to locate the skeleton with great accuracy, provided that the boundary of the shape is sampled densely enough. However, if the object being skeletonized is not a polygon, they obviously suffer from limitations due to the computational complexity of finding the Voronoi diagram of the shape (or alternatively the Delaunay triangulation). Moreover, approximating a smooth shape with many straight line segments introduces a lot of spurious branches, which then need to be pruned with techniques typically based on heuristics.The fourth, and final, class of methods is based on the analysis of the differential structure of the boundary. In [16], the boundary is segmented at points of maximal curvature and the authors show that the skeleton is a subset of the Voronoi diagram of these segments. Despite its accuracy, the main drawback of this approach is the need to estimate the boundary curvature by fitting a curve to it, which is a computationally demanding and quite delicate task. A somehow similar approach is that of Leymarie and Levine [13], which is based on the concept of active contours introduced in [17]. Kass, Witkin and Terzopoulos cast the problem of boundary location into a curve evolution framework, where the curve is evolved in a potential energy field under certain smoothness constraints. By using the distance map as the energy function, Leymarie and Levine are able to estimate the shape skeleton by simulating the grassfire transform and identifying the points where the wavefront collapses as the skeletal points. Unfortunately, as in [16] this requires an initial segmentation of the boundary at curvature extrema, which is itself a challenging problem.Another important method that belongs to this class stems from the Hamiltonian analysis of the boundary flow dynamics [18]. Siddiqi et al. state that the singular points where the system ceases to be Hamiltonian (i.e., an energy conservation principle is violated) are responsible for the formation of skeletal points. Unfortunately, their analysis fails to take into account the effects of the boundary curvature, a problem which they only partially solve in [19]. Subsequently, however, Torsello and Hancock [1] show how to completely overcome the problem by performing a Hamilton–Jacobi analysis of the flow under conditions where the flow density varies due to curvature.Although there exist a considerable number of algorithms for the extraction of skeletons from 2D shapes which yield reasonably good results, the problem of medial surface extraction is still an open one. This is because the addition of a third dimension makes the task of medial surfaces extraction particularly challenging. At the same time, the wide availability of cheap 3D scanning devices demands for a robust representation which provides a simple venue to perform shape analysis and representation under deformation and articulation. For this reason, the design of efficient algorithms for 3D skeleton extraction is of key importance.Luckily, while in 2D the skeleton extraction needs to be preceded by a segmentation of the image, in 3D it is common to model objects as distinct meshes, and thus the skeletonization can be much more practical. However, when a third dimension is added the task of medial surfaces extraction turns out to be much more challenging than in 2D. The reason is threefold. First, we observe an exponential growth of the number of voxels, which may render the computation impracticable, especially if a high resolution is needed. Further, while in 2D it is common to work with raster images, and thus there is no need to discretize the shape, volumetric objects are usually represented as triangle meshes, that may eventually need to be voxelized before any further computation is done. Clearly, the result of this discretization depends on the resolution chosen. Moreover, the topology itself may change as the resolution changes. Finally, tasks that are almost trivial in two dimensions, such as ensuring the topological correctness of the skeleton, i.e., the equivalence between the object and its skeleton, require particular attention when a third dimension is added.According to the analysis that we need to perform on the shape, in the literature there are two competing 3D generalizations of the skeleton: the curve (or line) skeleton [20,21] and the medial surfaces. The curve skeleton provides a minimal yet efficient representation for shape analysis and recognition. The medial surfaces, on the other hand, carry enough information to accurately reconstruct the original shape from the skeleton. In fact, while the line skeleton is a lossy simplification of the shape, the medial surface is topologically equivalent to the original shape, i.e., it is possible to map its segments, considered as two oriented surfaces, to the original mesh through a homotety. In some degenerate cases, moreover, the curve skeleton turns out to be ill-defined. Consider for example the shape of a cup, which clearly cannot be abstracted in terms of a medial axis. For these reasons, in this paper we decide to concentrate on the extraction of medial surfaces from triangulated meshes.Recently, Arcelli et al. [22] proposed a distance-driven algorithm for medial surfaces extraction. Although the algorithm proves to be effective and it is shown to preserve the topology of the original shape, it works only on voxelized objects, and as a consequence cannot cope with high resolution inputs. The work of Siddiqi and Pizer [3] bears some similarities with the present paper, as it generalizes to three dimensions the Hamilton–Jacobi skeleton. However, it suffers from the same limitations of its two-dimensional counterpart, since it does not take into account the effects of boundary curvature. A more robust algorithm is that of Reniers et al. [23], where both the curve and the surface skeletons are located by means of an advection-based importance measure. Unfortunately this measure turns out to be well defined only for genus 0 shapes, and both [3,23] share again the problem of requiring a complete voxelization of the space, which makes the use of these algorithms limited to low resolution objects.In order to cope with increased spatial and time complexity, Bai et al. [24] and Quadros et al. [25] propose to use adaptive octrees, which allow some parts to be discretized more densely while the rest is analyzed at a coarser scale. However, both these approaches work on a precomputed octree, where the grid refinement criterion is based on simple heuristics. In [24] the authors propose to increase the grid resolution on those voxels that are roughly at the center of the shape, where the medial surface is more likely to be located. Anyway, they clearly state that the design of an optimal grid adaptation criterion for skeleton computation is beyond the scope of their paper, and a more efficient heuristic should be used instead. In [25] the octree nodes are generated according to the vertices and centroids of the facets of an input CAD model, therefore the density of the nodes is higher in the presence of small features or regions of high curvature. The resulting skeleton, however, is disconnected, and it is composed of sets of nodes at different levels of resolution.Finally, Yoshizawa et al. [5] and Hisada et al. [26] propose a generalization of the Voronoi-based approach to three dimensions. These approaches work directly on the original mesh by approximating the medial surface with a skeletal mesh which has the same number of vertices and connectivity as the original mesh. More precisely, the QuickHull algorithm [27] is used to extract the Voronoi diagram of the mesh vertices, then for each mesh vertex v they define a skeletal point p at a distance d along v’s normal, where the displacement d is computed as the distance from v to the arithmetic mean of the Voronoi vertices of the Voronoi region containing v. The connectivity between skeletal vertices is then defined according to the connectivity between the corresponding mesh vertices. These approaches are fast and do not require an initial voxelization, but extract only an approximation of the skeleton and are extremely sensitive to small perturbations of the boundary.Recently we [28] proposed a hierarchical skeletonization algorithm where the refinement criterion is based on the density-corrected Hamiltonian analysis [1]. In order to deal with the discretization errors incurred at the coarser levels, we proposed to dilate the skeleton at each step of the hierarchical refinement. Although this procedure clearly increases the quality of the extracted skeleton, some discretization artifacts remain unsolved. In particular, due to the discrete nature of the voxelization procedure, the center of the final skeletal voxels tend to be displaced with respect to the true underlying medial surface. This in turn will affect the quality of the extracted skeletal mesh.Our purpose in this paper is to extend the work by Rossi and Torsello [28]. We propose a novel algorithm for medial surfaces extraction that is based on a generalization to three dimensions of the density-corrected analysis of Torsello and Hancock [1], while taking an adaptive octree-based approach for the discretization of the initial mesh in a manner that is similar to that proposed by Bai et al. [24] and Quadros et al. [25]. Contrary to these approaches, we decide not to precompute the whole octree in advance, but instead we keep the original mesh, that is used for distance computations, and we iteratively decide whether to refine a voxel or not based on the local value of the divergence of the momentum field, i.e., the confidence we have in that point being skeletal. Finally we design a simple alignment procedure to correct the displacement of the extracted skeleton with respect to the true underlying medial surface. We evaluate the proposed approach with an extensive series of qualitative and quantitative experiments, comparing our method against other approaches in the literature under varying mesh conditions.In this section we review the two-dimensional continuous formulation of the Hamilton–Jacobi skeleton [18] and its density corrected counterpart [1], where the latter will form the basis for our medial surface extraction algorithm.Let the distance map D be a function that assigns to each point in the interior of the shape its distance to the closest point on the object boundaryB→, and letF→=∇Dbe the corresponding velocity field, where ∇=(∂/∂x,∂/∂y)Tis the gradient operator. We define the outward flux ofF→through the boundary ∂A of an arbitrary area A asϕA(F→)=∫∂AF→·n→dl, wheren→denotes the normal to ∂A and dl is the length differential on ∂A. Under the assumption that the vector fieldF→is conservative everywhere except on the skeleton, the skeletal points can be identified by looking for those points where the system ceases to be conservative. Since the net flux ofF→through the boundary of the shape is positive, by virtue of the divergence theorem the interior of the shapes contains a set of sink points, i.e., the skeletal points. Hence, in their original formulation, Siddiqi et al. propose to label as skeletal those points in which the divergence ofF→is non-zero [18]. However, under a compressing front, the divergence can be negative also at non-skeletal locations. More precisely, the density of the compressing front changes during its inward evolution in a way which is proportional to the boundary curvature, and as a result the velocity field is no longer conservative. Initially, Siddiqi et al. tried to overcome this problem with the introduction of the concept of normalized flux. They show that by normalizing the flux of the velocity field by the perimeter of a circular integration area, as the radius of the circle approaches zero so does the value of the divergence, if the point is not skeletal. Due to the discrete structure of the lattice, however, the integration radius has a lower bound of one pixel. Since the divergence of the velocity field inp→depends on the local boundary curvature, assuming an integration radius of one pixel, the value of the normalized flux atp→will beNϕA(F→)(p)=-12k(p→), wherek(p→)is the curvature of the evolving boundary atp→andNϕA(F→)denotes the normalized flux ofF→. The problem is that near the endpoints of the skeleton the value of the curvature will tend to infinity, thus the discrete normalized flux diverges in their proximity.Based on the observation that when the front is curved the average linear density is not constant over time, Torsello and Hancock [1] propose to change the problem into a mass conservation one. More precisely, they state that, rather than the velocity field, it is the momentum fieldM→=ρF→that is conservative, where ρ is a scalar field that assigns to each point along the inward-evolving boundary front its linear density. As a result, the divergence of the momentum field is zero at any non-skeletal point, i.e.,∇·(ρF→)=0, and thus alsoϕA(ρF→)=0for any region A not containing a skeletal point.The density of the inward-evolving boundary can then be determined by applying the rule of product differentiation to the conservation equation and setting σ=log (ρ), thus yielding(1)∇σ·F→=-∇·F→.Finally, this can be further reduced to the system of ordinary differential equations along the path of boundary points(2)∂∂tσ(s(t))=-∇·F→(s(t))∂∂ts(t)=F→(s(t))where s(t) is the trajectory of a boundary point under the eikonal equation.Our algorithm works as follows. We are given a triangulated mesh, a starting resolution resminand a desired resolution resmax. Initially we compute a complete voxelization of the shape at resolution resmin. Given this initial coarse discretization, we compute the distance transform D, its gradientF→=∇Dand the divergence∇·F→, then we integrate the density σ=log(ρ) and finally we compute the divergence of the momentum field∇·(ρF→). With this information to hand, we are able to extract a first approximation of the medial surface. Assuming that a very low starting resolution resminis given as input, we now wish to further refine the extracted skeleton up to a resmaxresolution.To this end, we iteratively increase the resolution by subdividing the leaves of the octree with a large value of∇·(ρF→), i.e., those voxels that are most likely to contain skeletal points. The Hamiltonian analysis is then carried over the newly created octree level and the refinement process is iterated until the required resolution resmaxand octree level log8(resmax) is reached.In order to carry over the Hamiltonian analysis at a lower octree level the following steps must be undertaken (see Fig. 1):1.Velocity field computation. For each voxelv→at the current resolution level we compute its distance to the shape boundary. Given the distance map, we first compute its gradient inv→by fitting a hyperplane in a least squares sense on the voxel neighbors, then we determine its Laplacian by computing the flux ofF→through the surface of the convex-hull bounded by the neighbors ofv→, divided by its volume.Integration of the front-density. For each voxel at the current resolution level we compute the density of the evolving front by evaluating Eq. 2. We integrate the density starting from the current level boundary inward, under the assumption that the initial boundary has a complete 26-neighborhood where the value of the density is inherited from the parent voxels.Thinning and dilation. With the divergence information to hand, we iteratively remove the current level boundary voxels in distance order when the value of the divergence is under a certain threshold. In order to guarantee the preservation of the object topology, we remove a voxel only if it is simple, i.e., if its removal does not alter the object topology by disconnecting the shape or introducing a hole [34]. Once the thinning procedure is completed, we dilate the skeleton to partially compensate for discretization errors incurred at the coarser levels. We alternate the thinning-dilation process until no voxels can be added to the thinned skeleton. Finally a last dilation is performed to guarantee that the exploded points have a complete neighborhood around each skeletal point.With this high-level overview in mind, we will now present all the computational ingredients needed by the proposed approach.The distance transform computation is certainly one of the most expensive operations that we need to perform. We decide not to compute the distance map with respect to a discretized boundary, instead we keep the original mesh and we make distance queries with respect to it. In particular, the input mesh is saved on an Axis Aligned Bounding Box (AABB) tree [29], a common data structure that is used to make distance queries faster. A voxel is assigned either to the interior or exterior of the shape by casting a ray from the center of the voxel to a random direction and computing the number of intersections with the mesh. If the number of intersections is odd, the point is classified as interior, otherwise it is classified as exterior. We acknowledge that better algorithms for computing the signed distance transform have been proposed in the literature (e.g., [30]), but we also want to stress that the distance map issue is completely incidental to the main problem of skeletonization, which is the one we are addressing in this paper.Once the distance map is to hand, its gradient and divergence can be determined. Note, however, that while in the beginning all the leaves of the octree are at the same level and thus the gradient and the Laplacian can be approximated using the finite difference method, as the skeleton is refined there will be several voxels at different levels of resolution. For this reason we need to resort to a different approximation method that is able to cope with a non-uniform grid setting.Note that in the remainder of the paper we will operate on different neighborhoods of a voxel, according to the type of operation that we intend to perform. This includes the 6-, 18- and 26-neighborhoods, where n- refers to the adjacency relation between the voxels. Recall that two voxels are 6-adjacent if they share a face, 18-adjacent if they share a face or an edge and 26-adjacent if they share a face, an edge or a vertex. In particular, we will always assume that a 26-neighborhood is used, with the exception of a few cases. As explained later in the text, when computing the Laplacian of the distance map we only use local information and thus we restrict ourselves to a 6-neighborhood. On the other hand, during the integration of the density, we will use the subset of the 26-neighbors that have already been visited by the inward-evolving boundary. Finally, when ensuring the topology preservation, we will refer to the work of Malandain et al. [34], where the 6-, 18- and 26-neighborhoods are used to characterize the voxels.Following [31], we compute the gradient by performing a 4D linear regression over all the neighbors ofx→. More formally, given a set of points{(xi,yi,zi,di)}i=1m, where (xi,yi,zi)Tis a neighbor ofx→and diits distance to the boundary, we look for the coefficients A, B, C, D so that the hyperplane d=Ax+By+Cz+D best fits the samples in a weighted least squares sense. Minimizing(3)E(A,B,C,D)=∑iwi(Axi+Byi+Czi+D-di)2.The gradient is thenF→(x→)=(A,B,C)T∥(A,B,C)T∥, where as a weight wiwe used the inverse of the distance of the point (xi,yi,zi)T.Note that this approach has a problem whenever the skeleton crosses the convex hull of the neighborhood, as we integrate across a singularity resulting in erroneous computation of the gradient. A common solution to this problems is to perform one-sided computations to avoid crossing the singularity, however one-sided computations usually exhibit larger bias. Here we chose to perform a two-sided computation of the gradient as we are not interested in its value close to the singularity as we are adopting a one-sided process for the computation of the momentum field. The experiments will show, that even with this possible instability due to the possibility of crossing a singularity in the computation of the gradient, the momentum field is well conserved outside the skeletal branches resulting in a well localized skeleton.As for the Laplacian of the distance map, i.e., the divergence of the velocity field, we compute it using a discretization of the divergence theorem around the convex hull of the 6-neighborhood of each point. Note that even if the leaves are not guaranteed to be at the same level, and thus we cannot guarantee to have a complete 26- or 18-neighborhood, due to the octree construct we always have at least a 6-neighborhood. Doing a linear approximation ofF→(x→)over the faces of the convex hull, we can approximate the flux(4)ΦU(x→)=∫δUF→(s)·n→(s)ds≈∑t=1813Atn→t·∑p→∈VtF→(p→),where U is the convex hull of the 6-neighbors ofx→andAt,n→t, and Vtare respectively the area, the normal, and the set of vertices of the (triangular) faces of U. Due to the divergence theorem, we have∫U∇·F→(x→)dx=ΦU(x→), from which we obtain the following discretization for the divergence:(5)∇·F→(x→)≈ΦU(x→)|U|≈∑t=1813Atn→t·∑p→∈VtF→(p→)|U|.Once the distance, gradient and Laplacian have been computed, we can integrate the density in the newly subdivided skeletal points.It is of key importance that the density integration is carried out only on those points that have a complete 26-neighborhood, i.e., those with a homogeneous neighborhood. The voxels with a non-homogeneous neighborhood, on the other hand, will simply inherit the value of the density and divergence fields of their parent node. The reason for this is that an inhomogeneous neighborhood induces a higher discretization error to the direction of the gradient which will severely affect the accuracy of the integration step. Thus, before refining the skeleton to a higher resolution level, we perform a dilation of the skeletal voxels in order to guarantee that all their children will indeed have a complete neighborhood. Then, after the refinement, there will be a 1-voxel thick boundary of voxels with non-homogeneous neighborhood that will be children of the dilation voxels, rather than of the skeletal voxels. Note that this dilation can simply be considered a part of the last thinning/dilation step of the refinement of the previous level, which will be described later.In order to compute the momentum field over the interior of the shape we need to solve Eq. 2. A common approach in this case is that of solving the linear system obtained by rewriting Eq. 2 as a system of difference equation. The problem here is that the skeleton is a set of singularities of momentum field, i.e., we expect the density field to have different values at opposite sides of a medial surface. Consequently, the linear system has no solution. Even looking for an approximate solution using a gradient descent method would result in oscillations near the skeleton, so a different approach is needed.As proposed by Torsello and Hancock [1], we decide to integrate the equation in the time domain. The critical point is to ensure that when we compute the log-density σ of boundary points at time t we reference only the values of σ calculated at points already crossed by the inward-evolving boundary. In order to do so, we opt to find a numerical solution of Eq. 2 using a Crank–Nicolson approximation [32].Assume that there exists a family of surfacesB→trepresenting the inward evolution of the boundaryB→, that can be locally parametrized asB→t(u,v)around any pointx→. Then, we have(6)σ(B→t(u,v))=σ(B→t-1(u,v))+12[∇·F→(B→t(u,v))+∇·F→(B→t-1(u,v))].In the spatial domain, ifx→=B→t(u,v)we haveB→t-1(u,v)≈x→-F→(x→), which, substituted into Eq. 6, yields(7)σ(x→)=σ(x→-F→(x→))+12[∇·F→(x→)+∇·F→(x→-F→(x→))].Unfortunately the pointx→-F→(x→)is not guaranteed to belong to the cubic lattice, so we actually need to interpolate it using the values at the eight vertices of the cube containing it. Once again we should ensure that the interpolation does not cross the medial surfaces. Luckily,x→is the last of the eight vertices visited by the evolving boundary, so this requirement is met. Thus we can safely use the trilinear interpolation which yields(8)σ(x→)=σ(x→-F→(x→))-(1-∣F1∣)(1-∣F2∣)(1-∣F3∣)σ(x→)+12[∇·F→(x→)+∇·F→(x→-F→(x→))](1-(1-∣F1∣)(1-∣F2∣)(1-∣F3∣))where F1, F2, and F3, are the three components ofF→(x→)and, due to the fact that we use trilinear interpolation,σ(x→-F→(x→))-(1-∣F1∣)(1-∣F2∣)(1-∣F3∣)σ(x→)does not depend on the value ofσ(x→). As Fig. 2shows, the pointx→-F→(x→)does not belong to the cubic lattice. We then interpolate it using the values of the log-density on the eight corners of the cube containing the point. Note thatx→is the last of the eight vertices which is visited during the boundary evolution, and thus we are guaranteed that all the points that we use for the interpolation are on the same side of the medial surface.Given this formulation, we can integrate the value of the log-density over the interior of the shape, starting from the most external voxels inwards. At the first level the most external voxels will be the boundary boxes, which have a unit density, and thus a null log-density. At all other steps, the external voxels will be the voxels with irregular neighborhood that inherit the log-density from their parents. Once the log-density has been integrated, we can proceed to compute the divergence of the momentum field in each point of the interior of the shape. The value of∇·(ρF→)(x→)is given by approximating Eq. 1 as follows(9)∇·(ρF→)(x→)=Δσeσ(x→)-12Δσ+12∇·F→(x→-F→(x→))eσ(x→-F→(x→))+∇·F→(x→)eσ(x→)whereΔσ=σ(x→)-σ(x→-F→(x→)). Note that, since the equations introduced in this section are to be evaluated at different levels of resolution, the integration step is actually dependent on the corresponding voxel size.With the divergence information to hand, we can select the voxels that are likely to contain skeletal points and that will be further subdivided to form the next level in the octree. The skeleton extraction is based on a thinning process guided by the value of the divergence of the momentum field at each voxel.In [33] Torsello and Hancock show that the fieldρF→is conservative outside skeletal branches, while its flux through a 1-voxel circle centered on a skeletal point is proportional to dl/ds, i.e., the ratio between the boundary length dl and the skeletal segment length ds. This means that theoretically, skeletal branches can be detected by checking voxels with negative divergence of the momentum field. However, adopting any spatial discretization to compute the flux results in a spread-out of the divergence-based signal.Following Torsello and Hancock, we thin the shape by iteratively removing boundary points in decreasing order of divergence. That is to say that without any further control on the thinning process we might actually end up introducing holes in the skeleton or even splitting it into disjoint parts.Recall that one of the key properties of the skeleton is that of having the same topology of the original shape. While for some approaches like the Voronoi-based ones this comes at no cost, the voxel-based methods should always take into account whether if the removal of a voxel would disconnect the shape, introduce a hole or erode it by deleting the endpoints. Unfortunately, when dealing with volumetric objects, ensuring that this property holds is not always an easy task. Hence, in this paper we resort to the voxel classification of Malandain et al. [34], which allows us to efficiently identify removable voxels by exploring the connectivity of their neighborhood. More precisely, Malandain et al. show how to classify a 3D pointx→in a cubic lattice by computing two features. LetNn(x→)denote the n-adjacent neighbors ofx→. ThenC∗(x→)andC‾(x→), are defined as follows.Definition 1C∗(x→)is the number of the 26-connected components 26-adjacent tox→inB∩N26∗(x→), where B is the set of object points.C‾(x→)is the number of the 6-connected components 6-adjacent tox→inW∩N18(x→), where W is the set of background points.With this result to hand, we can easily identify the simple points of the medial surface [34], i.e., those points whose removal does not alter the topology of the object. We can then proceed with the thinning process by iteratively removing all simple points in decreasing order of divergence. More precisely, the conditions for a point to be removed are that (1) it is simple, (2) it is not an endpoint and (3) it is characterized by a negative divergence of the momentum field. Note, however, that due to the errors introduced by the discretization of the shape, after the first thinning process the medial surface can be two-voxel thick in certain regions. To ensure thinness at the highest resolution level we further thin the shape by removing all those points that are simple but not endpoints of the surface, regardless of their divergence. Following [3], we decide to restrict our definition of an endpoint to a 6-neighborhood. In this case, it can be shown that a necessary condition for a point to be an endpoint is to have three 6-adjacent background voxels [3].With the proposed hierarchical approach, once a voxel is flagged as non-skeletal at any level, all its descendants will inherit the property. A problem with this is that fine details might be lost at coarser level, resulting in parts of the skeleton that will be missing at all levels (see Fig. 3). Further, note that the skeletal voxels detected at the coarsest level are not even guaranteed to be connected and, since all further processing is topology preserving, a disconnected skeleton will remain disconnected at all levels.We address the latter problem by keeping only the largest component, while the missing detail is addressed by dilating the skeleton after it has been computed at each new level. This way, once the voxels are small enough to capture the detail, the skeleton will regrow into the missing parts. Note that since the dilation adds new voxels to the current medial surface, we need to ensure that the topology is preserved, thus we dilate only into voxels that would become simple after the dilation (see Fig. 3).Let V denote the set of voxels before we start thinning the current level of the tree, and let U be the subset of V formed by the boundary voxels of V. We then thin V to reveal the skeletal voxels as previously described. After the thinning step, we check if some voxel v∈U has been selected as skeletal. If that is the case, we dilate it and we computeD,F→,∇·F→,ρ,∇·(ρF→)on the dilated set. Then, we apply the thinning process again. The dilation-thinning process is iterated until the thinned skeleton contains no boundary voxels. This process gives us an adaptive dilation which adds only new candidate skeletal voxels with a large value of∇·(ρF→)and thus can be skeletal. Fig. 4shows the special case of a box shape, together with the extracted medial surface. Initially, the whole set of voxels in the interior of the cube belongs to V, while the boundary voxels on the faces, edges and vertices of the cube belong also to U. Because of the negative value of the divergence, the voxels on the edges of the cube will survive the first thinning step, and thus will be selected as skeletal. Since these voxels belong to U, they will be dilated, as explained above. Note that U will also be updated in order to include the new dilated boundary of V. However, the following thinning iteration will remove all the voxels in U, and the dilation-thinning process will finally converge. Note that during all these steps we always ensure that the topology of the object is not altered by adding or removing only simple points.With this improvement, we are able to recover small details that might have been lost during the first discretizations, as well as longer skeletal segments. Fig. 5shows how critical this procedure is. The eagle model in the figure clearly needs a very dense voxelization in order to capture details such as the claws, or even entire parts such as the wings. With the proposed approach, one can simply start from a lower and less computationally intensive resolution and then refine the extracted skeleton to a certain desired resolution.Finally, once the iterated dilation-thinning process gives us the final skeleton, we perform one final dilation step to ensure the presence of a complete 26-neighborhood around the new set of voxels on which we need to compute ρ and∇·(ρF→). At the last resolution level, the final dilation process is substituted with the endpoint-driven thinning that gives us a 1-voxel thick medial surface.At the end of the thinning process, we obtain the set of voxels most likely to contain the medial axis, thus placing vertices at the center of the voxels, and deriving the mesh connectivity from the adjacency information of the voxels, will result in a fine approximation of the medial surface in the form of a triangulated mesh. There are, however two sources of noise that limit the quality of the extracted surface, but that can effectively be addressed with a post-processing step.The first is an artifact due to the limited control over the order in which the thinning process eliminates the voxels. The final iteration of the thinning procedure removes all the simple points which are not endpoints, however, thinning order, and the topology and endpoints preservation rules might prevent us from choosing the correct skeletal voxels as candidate for elimination, while preferring some adjacent voxel which are not endpoints and whose removal does not alter the object topology (see Fig. 6). As a consequence, depending on the spatial order of the thinning, we might introduce little bumps on the surface. Due to their formation process, these bumps can be detected easily by comparing their distance to the surface to that of a nearby voxels. Let d(v) be the distance of candidate point v from the shape’s surface, letF→(v)be the gradient of the distance map in v, and let w be the neighbor of v in the direction ofF→(v), i.e., closest to the linev+tF→(v). If d(w)>d(v) then we v is a bump and we simply remove v from the set of skeletal voxels and mark w as skeletal.The second limit is a result of the discrete nature of the grid: the centers of the skeletal voxels will be actually slightly displaced with respect to the true underlying medial surface. We address this issue by allowing the final vertices to move within the voxel from the central position to one that is most likely to lie in the skeletal surface, resulting in a higher precision skeletal mesh even at low voxel resolution (see Fig. 7).Hence, given a voxel v, we compare the orientation of its velocity field (gradient of the distance transform) with that of its 26-neighbors, in order to determine which voxels lie on the other side of the medial surface. We call this set Ov. Note that thanks to the previous refinement step, we are sure that at least one of v’s neighbors will indeed lie on the other side of the medial surface. With the set of voxels to hand, we proceed by computing for each voxel w∈Ovbelonging to this set the intersection between the true medial surface and the line connecting w and v. Let svand swbe the surface points closest to v and w respectively, we look for the point pw=αv+(1−α)w along the line connecting v to w, for which ∥pw−sv∥=∥pw−sw∥, i.e., is equidistant from the closest surface points. This point pwis likely to be very close to the medial surface, but it displacement from the original position is not limited to the direction of inward motion of the surface and has also a tangential component. We eliminate this by interpolating the position over all the neighbors in Ov.Fig. 8illustrates the interpolation process. Let Ov={w1,…,wk} and let p1,…,pkbe the corresponding estimated points on the medial surface, we interpolate between their position using Shepard’s inverse distance weighting method [35]. Shepard’s interpolation method is a generalized barycentric interpolation approach designed for sparse data. It reconstruct the position of a point as a linear combination of the samples pi(10)p∗=∑i=1kwipi∑i=1kwiwhere the weights wiare a function of the inverse distance diof the interpolant p∗ to the samples pi, usuallywi=1di2.In order to apply Shepard formula we need to estimate the (squared) distances of the points pito the interpolant p∗. To this end we make the simplifying assumption that the gradient of the distance mapF→is approximately orthogonal to the medial surface at p∗. Under this assumption we note that di=∥pi−v∥ sinθi, where θiis the angle betweenF→(v)andvpi→, and thus(11)wi=1di2=1‖pi-v‖2sin2θi=1‖pi-v‖2(1-cos2θi)=1‖pi-v‖2-(pi-v)TF→(v)2.Fig. 7 shows the result of the alignment procedure on the voxels of a medial surface segment. Perhaps the major advantage of the proposed procedure is that it yields a faster convergence speed for the medial surface extraction algorithm. Fig. 9clearly shows that when we skip the alignment step we need to increase the depth of the hierarchical refinement considerably in order to get a decent approximation of the underlying medial surface. On the other hand, if we align the skeletal voxels as described in this Section we can stop the hierarchical refinement earlier and still get a good result.

@&#CONCLUSIONS@&#
