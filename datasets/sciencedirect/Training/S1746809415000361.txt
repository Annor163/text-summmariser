@&#MAIN-TITLE@&#
Neural signal compression using a minimum Euclidean or Manhattan distance cluster-based deterministic compressed sensing matrix

@&#HIGHLIGHTS@&#
We build the MDC matrix that has a simple structure to compress neural signals.MDC matrix largely compresses neural signals and has a small reconstruction error.MDC matrix obeys the RIP for the related vector under two prerequisites.Basic pursuit and lasso algorithms are best for the reconstruction of the signal.MDC matrix can be used over a large range of the sampling rate for compression.

@&#KEYPHRASES@&#


@&#ABSTRACT@&#
Multichannel wireless neural signal recording systems are a prominent topic in biomedical research, but because of several limitations, such as power consumption, the device size, and enormous quantities of data, it is necessary to compress the recorded data. Compressed sensing theory can be employed to compress neural signals. However, a neural signal is usually not sparse in the time domain and contains a large number of similar non-zero points. In this article, we propose a new method for compressing not only a sparse signal but also a non-sparse signal that has identical points. First, several concepts about the identical items of the signal are introduced; thus, a method for constructing the Minimum Euclidean or Manhattan Distance Cluster-based (MDC) deterministic compressed sensing matrix is given. Moreover, the Restricted Isometry Property of the MDC matrix is supported. Third, three groups of real neural signals are used for validation. Six different random or deterministic sensing matrices under diverse reconstruction algorithms are used for the simulation. From the simulation results, it can be demonstrated that the MDC matrix can largely compress neural signals and also have a small reconstruction error. For a six-thousand-point signal, the compression rate can be up to 98%, whereas the reconstruction error is less than 0.1. In addition, from the simulation results, the MDC matrix is optimal for a signal that has an extended length. Finally, the MDC matrix can be constructed by zeros and ones; additionally, it has a simple construction structure that is highly practicable for the design of an implantable neural recording device.

@&#INTRODUCTION@&#
Over the past several years, neural recording and stimulation systems have contributed substantial benefit to patients who suffer from Parkinson's disease, major depressive disorder, and epilepsy [1,2]. However, research and applications demand an increasing number of requirements, which implies more requirements for the neural recording system. These requirements include having high-density integration of the recording electrodes [3,4] (now, to our knowledge, a neural recording system can integrate more than a thousand electrodes [5]), low temperature (an increase in the temperature of the cortex must be smaller than one centigrade, which means that the maximum power density should be 0.8mW/mm2 for the exposed tissue area [6]), long device lifetime, and small device size. Among all of these requirements, the power consumption is one of the most challenging issues. In a patient who requires an implantable medical device, there must be limit to the frequency of replacing the batteries to both reduce the cost of the surgeries and improve the quality of life. For example, if there is a portable battery that has an energy density in the range of 1W-h/cm3, a battery volume on the order of 10μW average power per cubic centimeter is required for a 10-year device life span [7]. Moreover, many of the implantable devices integrate a wireless transmission part, which aggravates the situation of having stringent energy constraints, because large amounts of recorded data required a very high carrier frequency, which substantially increases the power consumption of the device [8–10]. A common ultra-wideband (UWB) radio exhibits energy-efficiencies in the nJ/bit range, whereas the power consumption of the other components is 103 times less than that of the UWB radio [7]. Therefore, a data reduction strategy for an implantable device should be employed to minimize the power consumption of the system.Most of existing methods for implementing integrated data compression under these constraints involves detecting neural spikes [11,12] or extracting the data features of the signal [13,14]. However, both of these methods cause distortion or loss of the data information. For example, in a neural spike-detection recorder, the data are obtained only in a time series or as an impulse signal but not as the signal itself [15]. If the thresholds of the detection are not properly set, then the spikes cannot be detected. At the same time, the feature extraction requires a period of time to train. Based on this method, the precision usually cannot be guaranteed, and the hardware design is also complicated [16]. Therefore, we must find a new method that does not lose the details of the signal to accomplish the goal of recording the signal.Compressed sensing (CS) technology gives us a new choice for signal compression. In recent years, this approach has attracted considerable attention in the areas of computer science, applied mathematics and electrical engineering [17,18]. CS constitutes a revolution over the traditional Nyquist sampling frequency (Shannon theory). CS technology can be divided into three main parts: sparse signal, signal reconstruction and sensing matrix.CS theory is based on the sparsity of the signal. If a signal Y, which can be found in a basis such asV=[v1,v2,v3,…,vn]has a sparse representation, then the signal is called a sparse signal. Specifically, suppose Y can be described as in the following equation:(1)Y=VXorY=∑i=1nxiviwhere xiis the coefficient vector for Y under the basis V. If Y is sparse, then the coefficient ximust be almost zero or negligible, and as a result, they can be omitted without any loss.If a signal is sparse under some basis, then it can be regarded as a compressible signal. Usually, a signal is not sparse, but if the basis can be changed, then the sparse representation under the new basis can be obtained. For example, a sine wave is not sparse in the time domain, but it is sparse in the Fourier domain.There are many reconstruction methods; an example is the ℓ1(or ℓ2) norm-based reconstruction method, which searches for the minimum ℓ1(or ℓ2) value to construct the signal [19,20]. This type of algorithm includes the basic pursuit algorithm (BP), match pursuit algorithm (MP), orthogonal matching pursuit algorithm (OMP) [21,22], and threshold-based method (such as the iterative hard or soft thresholding algorithm [22,23]). Probability-based reconstruction methods constitute another type; for example, the sparse Bayesian method uses the maximum likelihood to reconstruct the signal [24,25]. As of now, it has been proven that for a k-sparse signal, if the order of the measurement is 2k, the original signal can be recovered exactly [26].Not all of the signals are sparse, and the “sparse” basis is usually difficult to find. Although the “sparse” basis of a signal can be found, how to implement it into a device is still difficult [27]. To compress the non-sparse signal, we introduce a new concept for the compressed sensing, which is that not only the zero points in a signal can be compressed but also the identical non-zero points in the signal can be compressed. Therefore, in this article, we construct a deterministic sensing matrix that is based on this idea to compress the neural signals.The sensing matrix can be divided into two types: random and deterministic matrices. Currently, most of the designers use a type of random matrix as a sensing matrix in the system, such as the sub-Gaussian sensing matrix [7,15] or the random discrete Fourier transmission matrix [28]. However, the random matrix has disadvantages. First, storing the random matrix requires a large amount of space, and the effectively proven random sensing matrices require items with superior randomness, which causes there to be stringent requirements for the design of a random number generator. Moreover, a random number generator aggravates the complexity of the hardware design, especially for an implantable device, because the generator usually has large power consumption and a large silicon area. Therefore, the current random sensing matrices are not the best choice for an implantable hardware design.In addition, a deterministic sensing matrix is discussed as an optional type of sensing matrix. The advantage of the deterministic matrix is that it can generate the items of the sensing matrix on the fly without storing the data, and it is also easy to reconstruct the original signal. However, current deterministic sensing matrices, such as the Discrete Chirp sensing matrix [29], the Reed Muller sensing matrix [30], and the BCH sensing matrix [31], are also complicated with respect to the hardware implementation, and they cannot be used for a non-sparse or low-sparse signal; although a low-density parity-check (LDPC) matrix contains only 0's and 1's, the compression of a non-sparse or low-sparse signal requires a very high-girth sensing matrix that is very difficult to generate [32,33]. Therefore, a novel deterministic sensing matrix must be constructed.Moreover, there are two important contributions in this article. First, we use the similarity that is in a signal to construct the compression. In fact, a specific neural signal may contain many identical (or similar) points, and traditional compressed sensing concerns only the zero items in a signal; it does not concern two identical (or similar) non-zero points in the signal. Therefore, we research these identical or highly similar non-zero points, i.e., the similarity of the points in a signal, from the perspective of compressed sensing theory. Additionally, we use the advantages of the deterministic sensing matrix to construct a sensing matrix that is based on the clustering of the neural signal itself. In brief, the primary contribution of this article is that we design a deterministic compressed sensing matrix to compress non-sparse or low-sparse signals that have identical non-zero points, and the compressed signals can be largely recovered.To illustrate our work, we give definitions and proof for the MDC sensing matrix in Section 2. We introduce the dataset of the simulation in Section 3. The simulation results and a discussion based on the MDC sensing matrix are given in Section 4. Finally, in Section 5, we provide a conclusion.First, we provide the definitions of several basic concepts and the method of MDC matrix construction. (Some important variables or symbols are illustrated in Table 1).The most important concept in compressed sensing theory is the Restricted Isometry Property (RIP), which is shown as follows.Restricted Isometry Property An M×N sensing matrix Φ is said to satisfy the Restricted Isometry Property of order k if it satisfies the following equation:(2)(1−εk)||X||22≤||ΦX||22≤(1+εk)||X||22for all of the k-sparse vectors X. The restricted isometry constant ɛkof matrix Φ lies between 0 and 1. The restricted isometry constant ɛk, k∈(1,n) of sensing matrix Φ is defined as below:(3)εk(Φ)=max|T|≤k||ΦT*ΦT−IℝT||=max|T|=k||ΦT*ΦT−IℝT||where the maximum is over all of the subsets T⊆[n] with |T|≤[k] or |T| = [k], and ΦTmeans all M×k sub-matrices of Φ.After the presentation of the RIP, we give some basic concepts to construct the MDC matrix.Definition 1(Equal Index Permutation) Given a vector X(x1, x2,…, xn), there exists a permutation A1(a1, a2,…, at) of the index vector (1,2, …, n), and there is a vector that is based on this index permutationXA1(xa1,xa2,…,xai,…,xat),xai∈X. If every two items fromXA1are identical under some measures, in other words,xai=xaj,xai,xaj∈XA1, A1 is called an equal index permutation. If this measure is based on Euclidean (or Manhattan) distance, then A1 is called an equal index permutation under the Euclidean (or Manhattan) distance.A vector that has identical items can be clustered into several clusters according to the minimum Euclidean or Manhattan distance; thus, some other concepts are given.Definition 2Given a vector X(x1, x2, …, xn), there exists an index set that contains M equal index permutations under the Euclidean (or Manhattan) distance, i.e., AM(A1, A2,…, Am), whereAi=(ai1,ai2,…,aij,…,ait),aij∈(1,2,…,n). X can be clustered into M clusters according to the index AM, in other words,XAM(XA1,XA2,…,XAm). Ifaii∈(1,2,…,n)∀xi∈X,xi∈XAiandxi∉XAj, Ai, Aj∈AM; Ai≠Aj∀xi, xj∈X,xi∈xAi,xj∈xAjand xi≠xi, Ai, Aj∈AM; Ai≠AjThus, AMis called an exclusive equal index permutation set, and vector X is called an M-cluster exclusive vector under permutation set AM.Definition 3(p-dissimilar vector) Assume that a vector X(x1, x2, …, xn) is an M-cluster exclusive vector under the permutation set AM(A1, A2,…, Am), whereAi=(ai1,ai2,…,aij,…,ait),aij∈(1,2,…,n). According to Definition 2, X can be clustered into M clusters based on the index AM, in other word,XAM(XA1,XA2,…,XAm). Let p=M; then vector X is called a p-dissimilar vector. The size of each clusterCxAiisICxAi=t,XAi∈XAM. SoCxAiis called a t-large cluster. If t=1, thenCxAiis called a unit-large cluster. If Ai, ∀Ai∈AM, is an equal index permutation under the Euclidean (or Manhattan) distance, then X is called a minimum Euclidean (or Manhattan) distance p-dissimilar vector.From the definition above, t≠0, because a cluster contains at least one point.Lemma 1Assume that there is a p-dissimilar vector X under permutation setXAM(xA1,xA2,…,xAm)and that its length is L(X)=n; thus it can be clustered into M clusters, i.e.,Set(C)={C(xA1),C(xA2),…,C(xAm)}. Thus, Set(C) satisfies (4) and (5).(Equivalent Index Subset Vector) Assume that there are two vectors,Xx1,x2,...,xnandYy1,y2,…,yn, that have the same length L(X)=L(Y)=n. X is an M-cluster exclusive vector under permutation set AM(A1, A2, …, Am), whereAi=(ai1,ai2,…,aij,…,ait),aij∈(1,2,…,n). For a determined subset Ai, Ai∈AM, if{yai=r|ai∈Ai}and{yai=0|ai∉Ai}, then Y is called an equivalent index subset vector of the vector X.Assume that X is an p-dissimilar vector under permutation set AM(A1, A2, …, Am) and that its length is L(X)=n; then, it can be clustered into M clusters under an exclusive equal index permutation set, and the equivalent index subset vector of every cluster is YM{Y1, Y2, …, Ym}, which implies that it satisfies (6).IfYi×Y′j≠0, thenC(xAi)∩C(xAj)≠0, but the opposite holds under the assumption in Lemma 1, soYi×Y′j=0.With the definitions above, we can construct the sensing matrix for a minimum Euclidean (or Manhattan) distance p-dissimilar vector.Definition 5(Minimum Euclidean or Manhattan distance cluster-based deterministic sensing matrix (MDC matrix))If a vector X is a minimum Euclidean (or Manhattan) distance p-dissimilar vector, then we can construct a deterministic sensing matrix through the following three steps.(St1) Divide X into M dissimilar clusters{C(xA1),C(xA2),…,C(xAm)}based on the exclusive equal index permutation setXAM(xA1,xA2,…,xAm).(St2) The equivalent subset index vector of these clusters{C(xA1),C(xA2),…,C(xAm)}is {ϕ1, ϕ2, …, ϕm},m∈ℕ.(St3) Composing the matrix with {ϕ1, ϕ2, …, ϕm},m∈ℕ, which is Φ=[ϕ1;ϕ2;⋯;ϕm].Thus, Φ is called a minimum Euclidean or Manhattan distance cluster-based deterministic sensing matrix (MDC) matrix. If all of the ϕi, i∈[1, m] in the Φ are the normalized equivalent index subset vectors, so Φ is called a normalized MDC (NMDC) matrix. If all the ϕi, i∈[1, m] in the Φ are the unit equivalent index subset vectors, so Φ is called a unit MDC (UMDC) matrix. An example of the construction is shown as follows.Given a vector X(x1, x2, ⋯, x6) and that X can be clustered into three clusters based on the minimum Euclidean (or Manhattan) distance (these clusters are {{x1, x2, x5}, {x3, x6}, {x4}}); therefore, the NMDC matrix for the vector X is Φ, which is shown in the following equation:(7)Φ=1/31/30001/200001/30001/2100To research the property of the MDC matrix, we give a similar definition based on the Restricted Isometry Property.Definition 6(Cluster Restricted Isometry Property (CRIP)) An M×N sensing matrix Φ is said to satisfy the Cluster Restricted Isometry Property of order k if it satisfies the following equation:There are two important points that must be explained. First, every vector X has its own MDC matrix, and the MDC matrix is a collection of all of the matrices that are built through the algorithm in Definition 5. In the following sections, we show that the MDC matrix obeys the RIP for its related vector under some prerequisites. Moreover, the vector X is random, but if the clustering method and the measure are determined, then the MDC matrix of a certain vector X is determined; thus, the MDC can be regarded as a deterministic matrix.Lemma 3The NMDC matrix is a unit tight (or Parseval) frame.Given a random vectorX∈ℂN, and that the NMDC matrix Φ satisfies (9).Vector X satisfies the following:(10)∑i=1N|〈X,ϕi〉|2=X*Φ*ΦT*XT=||X||22Therefore, the NMDC matrix is a unit tight frame.Lemma 4The NMDC matrix satisfies the Cluster Restricted Isometry Property definitely.From Ref. [34], for signal X, if the restricted isometry constant of Φ is ɛ2k andε2k<2−1, the solution to the ℓ1 problem is a unique k-sparse solution.Given a k-sparse p-dissimilar signal, the index set of the non-zero items is T. A new signal X′ is constructed by the non-zero itemsX′2T∼{XT,XT}. The NMDC matrix of X′ is Φ′. From Lemma 3, the NMDC matrix is a unit tight frame, in other words, ||Φ′X||2 = ||X′||2, which means that ∃λ>0, |ε2k|<λ; therefore,X′2Tcan be recovered through the compression. Moreover, a subset T of the set 2T can be found to construct Φ, in other words, ||ΦTXT||2 = ||XT||2, which means that||Φx||22=||x||22; thus, from measurement Y, it can exactly reconstruct X.The NMDC matrix can compress any signal that contains identical items without considering the signal to be a sparse signal or not, because the NMDC matrix is a unit tight frame for its corresponding compressed signal; in other words, the restricted isometry constant ɛkof the NMDC matrix is 0. In the simulation, we can find that the UMDC (the items of the UMDC matrix are 0's and 1's) can reconstruct the original signal correctly, which indicates that UMDC also satisfies the Cluster Restricted Isometry Property.Although the MDC matrix obeys CRIP, we still want to know whether the MDC matrix satisfies RIP, and we also hope to use the UMDC matrix that contains only zeros and ones. As a result, we prove that the UMDC matrix obeys RIP when (k−M)/N→0 and Imax(Set(C))≤N/M.Theorem 1[35]Let ΦMNbe a sensing matrix, and let a vectorXx1,x2,…,xnbe a random vector. Given the following inequality (11):From Ref. [35], it can be learned that if we assume a sensing matrix ΦMNand a k-sparse signal and if Φ satisfies two conditions:E(||Φx||22)=||x||22and||Φx||22converges to||x||22, then Φ obeys the RIP with a probability of at least 1−2exp(−c0(ε/2)M)(12/ε)k, ε∈(0, 1). In Lemmas 5 and 6, we prove both conditions.Lemma 5We are given a k-sparse n-length p-dissimilar random vector X(x1, x2, …, xn) and that every item of X is uniformly distributed in the vector X, and it can be clustered into M t-clusters. Every two items in the vector have the same probability of being identical. The number of non-zero items in all of the clusters are {l1, l2, …, lm}, and their sum is∑i=1Mli=k. Φ{ϕ1, ϕ2, …, ϕm} is an M×N UMDC matrix, and Y=ΦX. The ℓ1 norm of each column |ϕi|1=ni, i∈(1, 2, …, n). If (k−M)/N→0, thenE(||Φx||22)=||x||22.With the notation presented above, we are given a random vector X(x1, x2, ⋯, xn) and assume that its measurement can change to be the following equation:If the sensing matrix is a UMDC matrix, in other words, ni=1, i∈(1, n), thus, (13) can change to be (14):(14)EY22=EΦX22=x22+1/N∑i=1M(ni2li−ni)lixli2If we want to obtainEΦx22=x22, the second item1/N∑i=1M(ni2li−ni)lixli2in (14) should be 0. Applying the Cauchy–Schwarz inequality, we can obtain the following equation:(15)∑i=1M(ni2li−1)lixli2≤∑i=1M(ni2li−ni)∑i=1Mlixli2=∑i=1Mni2li−niM||x||22(15) can change to be the following equation:(16)1/N∑i=1M(ni2li−ni)lixli2≤1/N∑i=1Mni2li−niM||x||22=((k−M)/N)||x||22Because k≥M, if (k−M)/N→0, then(1/N∑i=1M(ni2li−ni)lixli2)→0, which meansE(||Φx||22)=||x||22.In the next step, we prove that||Φx||22converges to its expectationE(||Φx||22).Theorem 2[35,36](Self-Avoiding McDiarmid inequality) Let X1, X2, …, Xmbe the probability space, and define X as the probability space of all distinct m-tuples, which is the subset of the product set χ=X1×X2⋯×Xmgiven by the following equation:Let h(t1, …, tm) be a function from the set X toℝsuch that for any coordinate i, given t1, t2…, ti−1,(18)|supu∈Xi;u≠tn,n=1→iE[h(t1,…,ti−1,u,Ti+1,…,Tm)]−infl∈Xi;l≠tn,n=1→iE[h(t1,…,ti−1,l,Ti+1,…,Tm)]|≤ciwhere the expectations are determined by the random variables Ti+1, …, Tm. (For more information, see in Ref. [36]). If (18) is satisfied, then for any positive γ, (19) can be obtained.(19)Pr[|h(T1…,Tm)−E[(T1,…,Tm)]|≥γ]≤2exp−2γ2/∑ci2We use this theorem to prove the concentration inequality.Lemma 6Assume that a random vector X(x1, x2, …, xk) is a p-dissimilar vector and that xi≠0. Every item of X is uniformly distributed in X, and its MDC sensing matrix is ΦMN. Assume thatf(X)=∑i=1kxiϕpi. The cluster set of this vector is Set(C)={C1, C2, …, Cm} and Imax(Set(C))≤N/M. Therefore, ΦMN satisfies (20).Assume that Ωkis the set of all k-tuple permutations (t1, t2, …, tk), which follows the definition that all entries of k-tuples of Ωkare distinct. The set Ωkis finite, has a counting measure and also can be renormalized to have a total mass of 1. Ωkis the probability space of the random vector X(x1, x2, …, xk) with k non-zero entries. Let setTk∼t1,t2,…,tkdenote a permutation of1,2,…,N. Because X is random and every item distributes uniformly in X, (t1, t2, …, tk) can be regarded as being uniformly distributed in Ωk.Let f:tk→CMbe defined byf(t1,t2,…,tk)=∑i=1kxiϕpi, and leth:tM→ℝbyh(t1,t2,…,tk)=||f(t1,t2,…,tk)||22, in other words (21).(21)h(t1,t2,…,tk)=∑i=1k|xi|2+∑i,j=1i≠jkxixj¯(ϕti)Tϕtj¯Then, (22) can be obtained:(22)h(t1,⋯,tℓ,…,tk)−h(t1,…,tp,…,tk)=∑iwithi≠ℓxℓxi¯(ϕtℓ−ϕtp)Tϕtj¯+∑iwithi≠ℓxℓxi¯ϕtjT(ϕtℓ−ϕtp)¯=∑xℓxi¯ϕg(tℓ,tj)−ϕg(tp,tj)+∑xℓxi¯ϕg(tj,tℓ)−ϕg(tj,tp)whereϕg(i,j)(x)=ϕiTϕj.Assume that t1, …, tℓ, …, tNand tpare all different and|ϕi(x)|i=1,2⋯,N2≤M−2η, η≥0.From the definition of the MDC matrix, it can be obtained thatmaxϕg(i,j)x=Imax(Set(C))andminϕg(i,j)x=0; therefore, (23) can be obtained.(23)|h(t1,…,tℓ,…,tk)−h(t1,…,tp,…,tk)|≤2|xℓ|∑|xj||ϕg(ℓ,j)(x)−ϕg(p,j)(x)|≤2|xℓ|∑|xj|Imax(Set(C))M−η≤2NM−(η+1)|xℓ|∑jandj≠ℓ|xj|Assume thatxtℓis the largest item and has the most identical points in X; at the same time,xtphas no identical items in X, which implies the sufficient condition of the Self-Avoiding McDiarmid inequality. Therefore, we can use this inequality to obtain the following equation:(24)Pr[|h−E(h)|≥β||x||22]≤2exp−β2||x||24M2(η+1)2N2∑ki=1|xℓ|2∑jandj≠ℓ|xj|2Because(25)∑ℓ=1kxℓ2∑jandj≠ℓxj2≤∑ℓ=1kxℓ2∑j=1kxj2≤||x||22×k×||x||22=k||x||24Can be combined with (25), (24) can change to be the following equation:(26)Pr[|||f||22−E(||f||22)|≥β||x||22]≤2exp−β2M2(η+1)2N2kNow, the statement that||Φx||22converges toE(||Φx||22)is proved.From Lemma 5, we can obtain the following equation:(27)Pr[|||f||22−||x||22|≥β||x||22]=Pr[|h−E(h)|≥β||x||22]≤2exp−β2M2(η+1)2N2k≤2exp−β2M2η+12N2kWhen η=0, Φ is the UMDC matrix.Given the determined values for N and k, let ε(x)=x/2N2k, then (27) can change to be the following equation:(28)Pr[|||f||22−||x||22|≥β||x||22]≤2e−(Mε(β2))Now, we prove that the UMDC matrix obeys the concentration inequality.As mentioned above, when a matrix obeys Lemmas 5 and 6, it will satisfy the RIP, in other words, (29):(29)(1−ε)‖x‖22≤‖ΦUMDCx‖22≤(1+ε)‖x‖22with a probability of at least 1−2exp(−c0(ε/2)M)(12/ε)k, ε∈(0, 1).After proving the RIP of the UMDC matrix, a small point that we need to mention here is that the MDC matrix is based on the signal having identical points, but not all of the signals are sparse, and not all of the signals have the identical points; thus, the signal approximation is needed. In the following sections, it can be observed that for a neural signal, its approximation vector contains large numbers of identical points that can be compressed, which makes the neural signals largely compressed.All of the algorithms, methods and data analysis procedures were implemented in MATLAB (Mathworks, Natick, MA).The first dataset is obtained from an adult male rhesus macaque monkey in the Cognitive Neurophysiology Laboratory of McGill University. The data are from a recording system that contains 32 extracellular channels with a Utath 10×10 microelectrode array implemented in the prefrontal cortex. The data comprise three different recordings over three trials. The duration of each trial is 300s. First, data were filtered with a third-order bandpass Butterworth analog filter that utilized cutoff frequencies of 300Hz and 7kHz. Then, the filtered data were amplified with a gain of 80db amplification, sampled at 30kHz and digitized (10 bits per sample).The second set of data was recorded from the visual cortex of a rat at the Center for Studies in Behavioral Neurobiology of Concordia University. The researchers used a stainless-steel-tipped microelectrode that had a shank diameter of 75μm to record the data. The data were filtered with a fourth-order bandpass Butterworth analog filter, and the cutoff frequencies were between 150Hz and 10kHz. After the filtration, the data were amplified with a gain of 100db, sampled at 32kHz and digitized (10 bits). The duration of the recording was 60s.The third dataset comes from the NeuroEngineering Lab, University of Leicester [37]. The dataset comprises the simulated extracellular signals that were recorded from a human medial temporal lobe using intracranial electrodes. The duration of the signal is ten seconds long. The data were sampled at 32kHz, filtered between 300Hz and 3000Hz and digitized (12 bits).First, to imitate similar recording conditions, all of the datasets were refiltered with a fourth-order non-causal Butterworth high-pass digital filter with a cutoff frequency of 300Hz and were resampled at 24kHz.Then, we randomly selected ten (or five) groups of test data from three datasets and ensured that the data of every set were used. To construct the MDC matrix, two different clustering methods were used for the test: one is the core data clustering that we designed, and the other one is the agglomerative hierarchical clustering. The algorithm of the core data clustering is described in Table 2. Because of the comparison with the sparse signal, we also used an approximation method that was based on the Manhattan distance to construct the sparse signal. The MDC matrices in all of the simulations (except for the special explanation) are all UMDC matrices. The core data clustering uses the Manhattan distance, and the agglomerative hierarchical clustering uses the Euclidean distance.Finally, all of the algorithms used in this article are BSBL (Block sparse Bayesian Learning algorithm), BP (Basic Pursuit algorithm) and OMP (Orthogonal Matching Pursuit algorithm), MP (Matching Pursuit algorithm), IRLS (Iterative Reweighted Least Square algorithm), StOMP (Stagewise Orthogonal Matching Pursuit algorithm) and Lasso (Least Absolute Shrinkage and Selection Operator). BSBL is BSBL_BO (groupStatLoc, learnlambda is 0, prune_gamma is −1, max_iters is 20, see [38]). BP, OMP, MP, IRLS and StOMP are from [39] using the default values. Lasso is from [40] using the default values.

@&#CONCLUSIONS@&#
In this article, first, several concepts regarding the construction of the MDC sensing matrix in a signal are presented. In addition, the construction method of the MDC matrix is given. To prove the RIP of the UMDC matrix, two prerequisites must be satisfied: the first prerequisite is that (k−M)/N→0, and the second prerequisite is that the clustering must be more even and Imax(Set(C))≤N/M. When both prerequisites are met, we prove that given a p-dissimilar vector, the expectation of the measurement equals its ℓ2 norm. Then the concentration inequality and the Self-Avoiding McDiarmid inequality are applied to prove the convergence of the expectation of its measurement.Moreover, five different random or deterministic sensing matrices under different reconstruction algorithms are given to prove the performance of the compression of the neural signals. To construct the MDC matrix, we use two clustering methods to construct the MDC matrix: the core data clustering and the agglomerative hierarchical clustering methods. Throughout the simulation, the MDC matrix can largely compress a neural signal, and with the BP or Lasso algorithm, the results of the reconstruction are satisfactory. Additionally, the agglomerative hierarchical clustering method is more complicated than the core data clustering method; thus, the core data clustering method is more suitable for hardware design. Second, for an MDC matrix, the longer the signal is, the larger the compression rate that can be employed under the same reconstruction error. In addition, it is proven that the UMDC matrix has reconstruction errors that are very similar to those of the NMDC matrix when using the BP reconstruction algorithm; thus, the UMDC matrix is suitable for the hardware design of a neural recording system. Finally, the sampling rate has a slight influence on the reconstruction error.In the end, the MDC matrix is compared with some sensing matrices from the other researchers’ work. From the comparison, it can be observed that the MDC matrix has an advantage in cases that involve non-sparse or low-sparse neural signal compression. From the simulation results, we found that the RIP is too strict for the MDC matrix and that it still has some “loose” limitations for the MDC matrix; as a result, in future work, we will perform more research on these limitations. Moreover, the neural signal compression device based on the MDC matrix will be considered for implementation.