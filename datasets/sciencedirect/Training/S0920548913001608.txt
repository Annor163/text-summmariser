@&#MAIN-TITLE@&#
A regular expression matching engine with hybrid memories

@&#HIGHLIGHTS@&#
The access probability of states in DFA is investigated through Markov theory.A matching engine with two-level memories is proposed for RE matching.Order instead of exact probability value is gained through matrix multiplication.L7-filter's patterns are used to evaluate the throughput of the proposed method.

@&#KEYPHRASES@&#
Deep Packet Inspection,Regular expression,Matching engine,Markov chain,

@&#ABSTRACT@&#
A key technique of network security inspection is by using the regular expression matching to locate the specific fingerprints of networking applications or attacks in the packet flows, and accordingly identify the underlying applications or attacks. However, due to the surge of various networking applications and attacks in recent years, even more fingerprints need to be investigated in this process, which leads to a high demand on a large memory space for regular expression matching. In addition, with the frequent upgrading of the network links nowadays, the network flow rate also increases dramatically. As a result, it demands the fast operation of regular expression matching accordingly with the enhanced throughput for network inspection. However, due to the limited space of the fast memory, the requirements on fast operations and large memory space are conflicting. On addressing this challenge, in this paper, we propose to use hybrid memory for regular expression matching. In specific, by investigating on the transition table state access probability through the Markov theory, it can be observed that there exist a number of states which are much more frequently accessed than others. Therefore, we devise a matching engine which is suitable for FPGA implementation with two-level memories, where the first-level memory uses the on-chip memory of FPGA to cache the frequently accessed state transitions, and the second-level memory, composed of slow and cheap DRAM, stores the whole state transitions. Furthermore, the L7-filter's regular expression patterns have been applied to obtain the state access probability, and different quantities of memory assignment approaches have also been investigated to evaluate the throughput.

@&#INTRODUCTION@&#
Deep Packet Inspection (DPI) is a key technique for Network Intrusion Detection System (NIDS) and Network Forensic System (NFS) [1,2]. In these network security devices, the payload of packets is matched against certain pre-defined patterns, e.g., fingerprints, to identify specific classes of applications, viruses, attacks and criminal evidences, etc. One approach in DPI is by using the string matching. However, it is not quite flexible and powerful to describe complicated fingerprints. As a remedy to that, regular expressions become a main approach rapidly in place of explicit string patterns as the pattern matching language in packet scanning applications. Due to its expressive power and flexibility for describing useful patterns, regular expression has been widely used. For example, in the Linux Application Protocol Classifier (L7-filter) [3], all protocol identifiers are expressed as regular expressions. Similarly, the Snort [4] intrusion detection system has evolved from no regular expressions in its rule set in April 2003 to several thousand rules using regular expressions currently. Another intrusion detection system, Bro [5], also adopts regular expressions as its pattern language.To conduct the match, regular expressions are compiled into Finite State Machine (FSM), and then the packet payload is scanned with the FSM. In the case of a light-weight network, with low flow rate and a small number of patterns, pattern separated approach (in which every regular expression is compiled into several FSMs, and the packet payload is matched those FSMs one by one sequently) can satisfy the throughput requirement. However, with the dramatic increase of bandwidth, multi-Gbps links are nowadays widely applied in campus networks, and the scale of the patterns of the typical DPI system is over one hundred, as thus the traditional pattern separated approach no longer meets the critical performance requirement. If a pattern integrated FSM is utilized, the inflation of the states makes it impossible to be filled into high speed memory like Static Random Access Memory (SRAM) or on-chip memory of Field Programmable Gate Array (FPGA).In regular expression matching, the storage capacity and the memory speed become an irreconcilable conflict. In order to decrease the storage requirement, previous researches (a detailed survey of the previously reported literatures will be provided later) were exerted to reduce the transition table memory. In this paper, we do not consider how to deduce the memory of the state transition table, instead, we will take hold of how to conjointly use high speed memory with small space and slow speed memory with large space to make matching engine at almost the throughput of the high speed memory.Every byte of packet payload needs at least one memory access in DPI systems; as a result, the throughput constraint of the network security devices is memory access time. Generally, the immanent fluidness of network packets results in a very low hit ratio of the cache. Since the number of the memory access is pre-defined and cannot be changed subsequently, the potential to enhance system performance is to improve the efficiency of each memory access.Traditional network security devices like NIDS and NFS adopt high performance CPU platforms such as ×86 and MIPS, to improve the system performance. These typical CPUs are developed for the improvements on the calculation performance, so they are made as perfect or effective as possible on cache coherence, branch prediction, out-of-order execution, multi-core parallelism, etc. The improvements on memory access concentrate on how to increase the hit rate of the cache, but the fluidness of network packets cannot exploit these advantages; therefore, customizing hardware based approach such as FPGA is introduced as its better pipelining and parallelism. In addition, FPGA has specially designed on-chip memory for high bandwidth linkage with logic units, which could accomplish high speed communication between matching engine and memory. Nevertheless, the space requirement of the state transition table compiled from real-world patterns is far more than the capacity of the FPGA's on-chip memory.To address the contradiction between the performance of the matching engine and the capacity of the memory, in this paper, we present a new hybrid memory matching engine, which enables the high throughput and a large scale of state transition table storage by utilizing two levels of memories to accommodate the state transition table. Specifically, the contributions of this paper are three-fold.•First, based on the transition table, a state transition possibility matrix has been constructed, and the state accessing probability using Markov chain with stable state vector is investigated. To the best of our knowledge, it is the first work researching on formed states of regular expressions.Second, a two-level-memory hierarchy storage mechanism has been introduced, in which the Markov theory is used to obtain the frequently accessed states and their transition table entries are stored in the first-level memory while the second-level memory is used to hold the whole transition table entries.Finally, real-world regular expression patterns are used to produce the state transition table and state probability table, and a simulation analysis is employed to show that our hybrid-memory architecture can obtain the throughput almost as the first-level memory while the memory cost is nearly the same as the second memory.The remainder of this paper is organized as follows. The related work is provided in Section 2, and the motivations on introduction of Hybrid Regular Matching Engine are presented in Section 3. Then, Section 4 depicts the system architecture and framework. The process of obtaining the stable vector is introduced in Section 5. The experiments and result analysis are in Section 6. Finally, we conclude our work in Section 7.Finite State Automata (FSM) is a natural formalism for regular expressions. Although Deterministic Finite Automaton (DFA) and Nondeterministic Finite Automaton (NFA) are two kinds of FSMs that can be used to conduct DPI, the more preferred approach is DFA as it supports non-backtracking search. In DFA-based systems, a number of researches based on DFA systems compile m regular expressions into a composite DFA, which provide guaranteed performance benefit over running m individual DFAs. Specifically, a composite DFA reduces processing cost from O(m) (O(1) for each automaton) to O(1), i.e., a single lookup obtains the next state for any given character. However, the number of states in the composite automaton grows to O(∑nm) in the theoretical worst case.Even though the approach compiling m regular expressions into a single FSM is relatively fast, its O(1) scanning complexity still cannot meet the current links' bandwidth requirement, as the aggregated Internet traffic has been experiencing an annual bandwidth growth of 40%–50% in recent years [6]. To break the performance bottleneck of regular expression matching engine, a number of researches have been studied to improve the overall throughput by achieving efficient content-matching. Previously reported researches mainly focus on improving the throughput of the rule matching algorithms, and/or employing FPGA [7–9], GPU [10–13] or TCAM [14] for efficient content-matching.Another kind of researches exerted to reduce the memory requirements can be clustered into two classes:(1)The first FPGA-based solutions implement NFA [15–17]. Although NFAs are always smaller than DFAs, they need more memory bandwidth as an NFA may be in several states simultaneously whereas a DFA is always only in one state. Thus each byte that is processed might need to access the transition table for ∣Q∣ times. Prior works have looked for different ways to find good NFA representations of transition table that limits the number of states that need to be processed simultaneously. These approaches combine the matching engine with the state transition table, which may lead to inconvenient pattern update.The second class of researches exerted to reduce the memory is based on DFA. Although the approach with m regular expressions compiled together into a DFA can gain O(1) computing complexity, its storage cost is extremely higher than that of the separated DFA. The number of states in a DFA scales poorly with the size and number of wildcards in the regular expressions. For a naive DFA with k states, its memory requirement is ∣∑∣×k×⌈lnk⌉, which is unsustainable as the k increases so fast with the increasing of the regular expression number m and n (average length of the expressions). In 2007, Becchi and Cadambi [18] proposed a redundancy eliminating method. Its basic idea is merging “nonequivalent” states in a DFA by introducing labels on their input and output transitions. Its evaluation shows that it can drastically reduce the DFA memory requirement, but its performance is influenced as it needs to access several sub-tables.In [19], the authors analyzed the size of DFAs for typical payload scanning patterns, and developed a grouping scheme which can strategically compile a set of regular expressions into several engines, resulting in a remarkable improvement on regular expression matching speed without much increase in memory usage.Default transition and D2FA (Delayed Input DFA) were introduced and employed in [20,21]. D2FA is a special FSM based on DFA but with default transitions where each state can have at most one default transition to one other state. The directed graph named as deferment forest consisting of only default transitions in a D2FA must be acyclic. D2FA representation reduces transitions by more than 95% but with a speed penalty for long default transition paths in the D2FA matching.In [14], the authors introduced a hardware-based RE matching approach that uses Ternary Content Addressable Memory (TCAM) and its associated Static Random Addressable Memory (SRAM) to hold state transition table. They proposed three novel techniques: transition sharing, table consolidation, and variable striding, to reduce TCAM space and improve regular expression matching speed. The experiments based on 8 real-world regular expression sets show that small TCAMs can be used to store large DFAs and achieve potentially high regular expression matching throughput.We find the above-mentioned DFA based space compressing methods except [14] sacrifice performance to some extent to occupy less memory.Our research is orthogonal to these researches as the second-level memory is not so sensitive to the access performance, which signifies that using these research achievements can reduce the second-level memory space in our proposed approach.The proposed matching architecture is based on the Deterministic Finite Automata (DFA), which is because DFA is faster than NFA (Nondeterministic Finite Automata), as depicted in Table 1. A DFA is a 5-tuple (Q, ∑, δ, q0, A), where Q is a set of states, ∑ is an alphabet (containing the 28 symbols from the extended ASCII code), δ: ∑×Q→Q is the transition function, q0 is the initial state, and A⊆Q is a set of accepting states.A set of regular expressions can be converted into an equivalent DFA with the minimum number of states by transforming the regular expressions into NFA, converting NFA to DFA using subset construction algorithm, and state minimization algorithm [22]. The fundamental issue in regular expression matching is the throughput and the large amount of memory required to store transition table δ, as we have to store δ(q, a)=p for each a of every q.To make the matching progress more clear, a simple example on the regular expression “ab.c” is discussed next. The DFA after compilation is depicted in Fig. 1. If a string (packet body) “ababac” needs to be scanned, the matching engine begins from initial state 0; byte ‘a’ makes it go to state 1; byte ‘b’ makes it go to state 2. Afterwards, matching engine goes through state 4, state 2, state 4, and finally state 5. As state 5 is an accepting state, the input is accepted.From the aforementioned example, it can be observed that every byte of packet payload needs one memory access, and every two successive memory accesses are interrelated. Pipeline for memory access cannot be achieved due to this type of interrelation. Therefore, the matching throughput performance is almost totally dependent on the number of states that needs to be accessed and the state access delay. As the state number is not easy to be decreased because it is the same as the length for every packet payload, we consider how to reduce the state access delay. It seems that the best way is by using faster memory to reduce the delay.Different memories have different properties. For example, SRAM has a relatively high access speed (166MHz when this paper is being written), but its price is high and its size could not be very large (about several megabytes according to the current technology); while the Dynamic Random Addressable Memory (DRAM) has a relatively large size and high throughput for its burst, but one memory access may take a number of cycles to obtain a result as DRAM has a long startup time. In the regular expression matching, the successive accesses are associative as the following access is dependent upon the result of the previous access, so pipeline cannot take effect and the high throughput of the DRAM cannot be fully utilized. FPGA always has its own embedded (on-chip) memory with high throughput and fast response time (about 1 to 2ns), thus it is suitable to the continuous dependant access, but its size is often less than megabytes, which is too small to accommodate the whole transition table in practical security systems.Although embedded memory in FPGA is small, its maximum frequency can reach more than 500MHz. Even through the frequency will be reduced to about 400MHz (depending on the complexity of logic and the model of the FPGA) after being synthesized, it is still much faster than DRAM. As an example, if Altera Stratix II EP2S180 (with 9 M-RAM blocks and all the blocks can be configured as real dual-port mode) is utilized and the time frequency after being synthesized is 400MHz, matching engine with single scanning module can reach a throughput of 400MHz∗2∗8=6.4Gbps. In fact, it is very easy to parallelize the memory block access in one matching engine to reach more than 20Gbps throughput. The biggest challenge is to solve the memory space problem of the transition table.In brief, there is not a single memory that can satisfy both the size and throughput requirements. As a popular technology used in computer architecture, cache instead of the main memory is often accessed by the CPU of a computer to reduce the average time to access the data and instructions. A cache is a smaller, faster memory which stores copies of the data from the most frequently used main memory locations. As long as most memory accesses are cached, the average latency of memory accesses will be closer to the cache latency than to the latency of main memory.The challenges of most multi-memory computer systems are that:(1)How to make the first-level memory fast enough?How to make the last-level memory large enough to accommodate the frequently accessed program instructions and data?How to make the data being accessed with higher hit ratio at the first-level memory?We borrow the idea from the computer architecture. In our design, we find that the fastest memory in optional is the on-chip memory, which has a feature that almost every mainstream FPGA has several banks of on-chip memory that can be concurrently accessed. If the same entries are stored in different banks of memory, access performance can be increased several-fold for the access operations that can be distributed to different bank, but the constraint is that its size gives an upper bound of the DFA states stored on-chip.The whole number of states in a security system is often larger than one hundred thousand, which makes the whole memory requirement more than a couple of gigabytes. Although there are some kinds of compression methods that can decrease the storage space, a conservative estimate is that more than one dozen megabyte space is still required to implement a practical security detection system [18–21]. DRAM is chosen to be our second-level memory as its capacity could be very large.Now, the arisen problem is that how to select the most frequently accessed states with the limited space of the on-chip memory in FPGA. From Fig. 1, the intuitive feeling is that state 0 is the most frequently accessed state, as there are 1528 transitions that go to this state. It is observed that state 3 has 254 incoming transitions while state 1 has only 5 incoming transitions. Does it mean that state 3 is more frequently accessed than state 1? In fact, the answer is “no” since state 3 can only be transferred from state 2 with only two outgoing transitions, and the accessing frequency of state 3 may be less than that of state 2. Now that state 2 has only two incoming transitions, the accessing frequency of state 3 may be less than that of state 1. It is a probability and statistics issue to determine which states are more frequently accessed than the others. The second issue is that the memory delay needs to be 8bits/10Gbps=0.8ns in the mainstream 10G POS or Ethernet links, and on-chip memory cannot meet the performance requirement, so concurrently accessing multi-banks may be a choice.Fig. 2shows the proposed system architecture. The design mainly contains a Regex Compiler, a Storage Assigner, a matching engine and two levels of memories.As Regex Compiler and Storage Assigner are components with very complex computation processes, they are implemented by software in embedded CPU connected to FPGA. Regex Compiler's input is multiple regular expression patterns while its output is the state transition table. Storage Assigner obtains the state transition table and determines how to assign the entries to the two different memories. In the proposed architecture, Storage Assigner is very significant as the perfect state assignment can lead to the best performance/cost ratio, and the detailed analysis of it will be shown in Section 5.The whole state transition table is divided into two sub-tables: the first-level sub-table is a duplicate of some entries stored in different on-chip memory banks while the second-level sub-table is stored in the off-chip memory. The matching engine consists of several scanning modules each of which processes particular streams (Algorithm 1). As the packets from the same stream (the packets defined by a five tuple consisting the same source IP address, source port number, destination IP address, destination port number and protocol number, which presents the bidirectional communication comes from the same peer) must be scanned by the same modules, a Stream Assigner is employed to make sure the fairness of the load among scanning modules. There are a number of researches [23,24] that have been conducted on solving the balance of the stream and packet, which are beyond the range of this paper. In the performance analysis in Section 6, it is assumed that all the scanning modules are fully utilized.During the matching process, every scanning module utilizes the current state identification left shifting 8bits with the addition of the current byte to obtain the entry that consists of the next state and the hit tag.Every state has 256 entries each of which consists of a special hit bit and the next state indexed by the corresponding character (as depicted in Fig. 3). The hit bit (tag) is introduced to avoid one more memory access, and although most states do not hit any rule, one memory access is still necessary to check whether it is an accepting state. The entry including the introduced hit bit can indicate not only that one memory access gains the next state but also whether the next state is hit, while the trade off is only a little more memory. The hit bit is 1 if the next state is hit, otherwise it is 0. Accepting table is a map of state identification and its correspondent hitting rule identification. As this table is not so large and is only accessed when hit happens, either SRAM or DRAM can be used according to the remaining resource of the chosen FPGA model as SRAM is easier to control than DRAM while DRAM is cheaper than SRAM.The scan algorithm of the matching engine is described in Algorithm 1. From the scan algorithm, we find that the matching engine is still very simple compared with the naive matching engine. The only difference is that our matching engine needs one operation to decide which memory to access.Algorithm 1Scan(pkt).The above design is suitable for implementing high-speed regular expression matching engine, as almost every FPGA has several bands of on-chip memories that are connected with logic units closely, which makes the matching throughput higher than the usage of any kind of off-chip memory. The only requirement to achieve cost-effective matching of such a system is to ensure that the scan process can access the on-chip memory in overwhelming major occasions. The coming section will discuss the state probability problem and the key issues on how to ensure such a locality.The storage arrangement of the state transition table consists of (1) obtaining the state transition probability matrix, (2) gaining the steady probability vector and (3) state renumbering. We depict them in the following sub-sections.Assume that the matching engine is being in a state qi, what about the probability that the next state will be qj(0≤j<|Q|)? It is obvious that if there is no a that meets δ(qi,a)=qj, then the probability from qito qjnamed as Pi,jis zero.In addition, how about we have some a that meets δ(qi,a)=qj? Here, as DFA instead of NFA is used, for an arbitrary input a, δ(qi,a) is not a set consisting of several states but only one state, and it is postulated that the packet content is stochastic, thus the probability is proportional to the number of a that meets δ(qi,a)=qj. As an example, if the next states of the one step reachable transitions from qiare only qjand qk, and the set of inputs from qito qjis A and the set of inputs from qito qkis B, then the probability from qito qjis |A|/|A|+B, while the probability from qito qkis |B|/|A+B.The state transition table is a |Q|×|∑| matrix with the values in the matrix being the next state numbers or zeros if the next states are unreachable directly. From the state transition table, the transition probability |Q|×|Q| matrix P is constructed as follows:(1)pi,j=|a|δia=j,a∈Σ,i,j∈Q|/|Σ|.Since we postulate that the packet content is stochastic, P is a matrix whose entries are all between 0 and 1 and whose row-sums are all 1. P is a Markov chain, as if the chain is currently in state qi, then it moves to state qjat the next step with a probability denoted by Pi,jand this probability does not depend upon which states the chain was in before the current state.The probability Pi,jis called transition probability from state i to state j. During the scanning process, the next state can be the same as the current state, and this occurs with the probability Pi,i. As the example in Fig. 1, when current state is 1, and the input character is ‘a’, the next state is still 1, which means P1,1=1/256 as there is only one character that makes the transition from state 1 to itself.To make the transition probability issue more clear, Fig. 1 is taken as an example again to construct the transition probability matrix as follows:(2)P=255256125600002542561256125600000025525612560254256125600012562532561256125600125625525612560000.As it can be seen from the probability matrix, state 0 has a probability of 255/256 to move to itself and a probability of 1/256 to move to state 1, which is due to that only one character ‘a’ leads the state 0 to transfer to state 1 while the other 255characters lead the state 0 to transfer to itself.As patterns in DPI systems are always very complicated, some of them are with starting anchor (^), the constructed DFA transition matrix is very huge and complex, and some states induced from the pattern with starting anchor are caused to be transient states. Take patterns “ab” and “^a.*c” as an example, the DFA diagram is depicted in Fig. 4.Consider the transition diagram in Fig. 1. Starting at any state, we are able to reach any other state in the diagram directly in one step or indirectly through one or more intermediate states. Such a Markov chain is termed as irreducible Markov chain. On the other hand, starting from any state, we might not be able to reach other states in Fig. 4, directly or indirectly. For example, if we begin at state 2, we cannot reach states 0, 1, 3 and 8. Such a Markov chain is called reducible Markov chain.An irreducible Markov chain is defined as one in which each state is reachable from every other state either directly or indirectly; whereas a reducible Markov chain is one in which some states cannot reach other states. Furthermore, the states of a reducible Markov chain can be divided into two sets: closed states (C) and transient states (T). In Fig. 4, states 0 and 2 are transient states, whereas the two groups {1,3,8} and {4,5,6,7} are both closed states.We study the steady-state behavior of reducible composite Markov chains in this sub-section. In our reducible Markov chain, it is composed of some sets of closed states. At the beginning of the scan, it is always in the initial state 0, which is a transient state (as in Fig. 4); therefore, it can move to either state of closed sets. In the view of the scanning, hitting states of unanchored patterns are always in closed sets, while those of anchored patterns may be in closed or transient sets. From the initial state, the matching engine can reach every closed sets to match different patterns.The transition matrix is given by the canonic form:(3)P=TA1A2…Al0C10…000C2…0……………000…ClwhereCisquare stochastic matrices (i=1, …, l).rectangular nonnegative matrices (i=1, …, l).square substochastic matrix.Therefore, the states of the original Markov chain are now separated into l+1 mutually exclusive subsets, where states in Ci(i=1, 2, …, l) are closed states, each of which can communicate with others within the same subset and they have no edge outgoing to the states out of the set, and states in T are transient. The probability transition probability matrix after n multiplications [25] is(4)Pn=TnY1nY2n…Yln0C1n0…000C2n…0……………000…Clnwhere T(n)→0 as n→∞ and(5)Yin=∑k=0n−1Cin−k−1⋅Ai⋅Ti=Cin⋅Ai⋅I−T−1.As scan begins at initial state 0, we only need to gain the first row vector of Yk(n) to obtain the distribution probabilities after n bytes (steps). We observe that each row of matrix Yi(n) is a scaled copy of the rows oflimn→∞Cinfrom Eq. (5). Moreover, the sum of each row of Yi(n) is less than one, and the total sum of the row from every Yi(n) is exactly one. From Eq. (5), we know that the convergence speed of Yi(n) is strictly dependent upon the convergence speed of the Ci(n).For a reducible Markov chain, during the state transition process, there is stationary distributions ofW=limn→∞Pn, but unlike the irreducible Markov chain, rows in W are not the same for a reducible Markov chain. The steady probabilities of a reducible Markov chain are dependent upon the initial state.There are two kinds of approaches to obtain the stationary distributions of a Markov chain, the first is iteration (like Jacobi, GaussCSeidel and Successive Over-relaxation), and the other one is direct computation. Traditional iteration approaches are not the preferred methods as we do not know how to define the threshold. In other words, we do not know when is the right time to stop the iterations. Thus, the direct computation may be our selection. Even though the example in Fig. 1 has only 6 states, the states of a FSM consisting of more patterns may inflate to several millions. The computation complexity of gaining the stationary distributions must be acceptable as the update of the pattern occurs sometimes though it is not very frequent.To gain the steady matrix W from WP=W is likely to solve the m-linear equations. The first row of W is the steady vector beginning at initial state 0. For this reason, the issue is converted to obtaining w1, w2, … and wmfromw1w2…wmP−E=0wherew1+w2+…+wm=1.As P is a large-scale square matrix, it is computation-intensive to use general methods of linear algebra on such an issue, because most of the algorithms need O(m3) arithmetic operations devoted to solving the set of equations. Even though we can obtain a set of solutions of w from the equations, we still need some computation to know which w is located at the first row of W. Special sparse matrixes, such as tridiagonal, and band diagonal with bandwidth M block diagonal have some special approaches to solve the problem. However, our probability transition matrix does not have such features that can be used to accelerate the solution.We turn back to the method of iteration and find that the convergence speed is seriously dependent upon the scale of the matrix. From Eq. (5), if the state number of every Ciis not so big, multiplication of Ciwill converge rapidly, so every Yi(n) will converge rapidly; as a result, the first row of the iterative multiplication of P will converge rapidly. Fortunately, our Markov chain has such a feature, even though the whole chain is in large scale, which will be discussed in Section 6. The repeated multiplication technique is prone to numerical roundoff errors and one has to use repeated trials until the first row stop changing for increasing values of n, which is not a problem as our approach does not need to obtain the exact result of the probability distribution.After the order of the first row vector in P(n) is already steady, the state transition entries with the highest probabilities in the first vector of the matrix are selected to be stored in the first-level memory. In Algorithm 1, in order to accelerate the memory access speed, only the state identification number and the input character are utilized to obtain the access address of the next state; as a result, these states stored in the first-level memory (M1) must have relatively lower identification number to avoid more complicated addressing mechanism.Rearranging the state identification numbers is not a hard work as we only need the most frequently accessed state transitions to be stored in M1, instead of sorting the states according to their probabilities. If M1 is arranged to hold mL1 states (every state has 256 entries), the original preceding mL1 states can be copied to another buffer, and then scan the whole transition table to change the state identification number and conduct movement, whose computation complexity is only O(m·mL1).The second-level memory (M2) holds the whole transition entries instead of the remaining ones. The first reason is that we do not prefer to use a complex addressing mechanism as if we store the whole entries in the second-level memory, and simple state shifting with the addition of the current byte can gain the access address. The second reason is that the entries stored in the first-level memory are very small relative to the whole memory requirements, so even repeated storage is a cost that can be sustained relative to the performance it brings to.The selection of how many states are stored is dependent on what throughput is required and the FPGA types, which will be discussed in the next section.

@&#CONCLUSIONS@&#
