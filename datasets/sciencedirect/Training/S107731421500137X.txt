@&#MAIN-TITLE@&#
Location recognition over large time lags

@&#HIGHLIGHTS@&#
We introduce a new dataset where the objective is to recognize a location of an old photograph using modern digital images.We evaluate a large number of existing features and encoding methods for this task.We show that existing domain adaptation methods can help to improve results for this specific task.We further show that there is the need for further research in cross-domain image retrieval task.

@&#KEYPHRASES@&#
Location recognition,Cross-domain image retrieval,Domain adaptation,

@&#ABSTRACT@&#
Would it be possible to automatically associate ancient pictures to modern ones and create fancy cultural heritage city maps? We introduce here the task of recognizing the location depicted in an old photo given modern annotated images collected from the Internet. We present an extensive analysis on different features, looking for the most discriminative and most robust to the image variability induced by large time lags. Moreover, we show that the described task benefits from domain adaptation.

@&#INTRODUCTION@&#
A hundred year old photograph or a postcard can reveal a lot about our culture and history. Following this idea, many cultural heritage campaigns recently started to promote the digitization of large amounts of visual data. Several cities and towns all over the world, as well as institutions such as universities or museums, are bringing archives with their images and footage online, providing public access and calling for methods to efficiently open up and exploit these resources [1,2].At the time when photography was not affordable for private and everyday use, most of the pictures were taken in public places and depict buildings, monuments, statues, or more in general, common locations of interest. Some of those are landmarks and tourist attractions. Others are locations with historical value. Popular landmarks often appear in modern digital images which are shared online through applications such as Flickr. Other historical locations can be associated to their geographic coordinates through Google Maps and visualized by means of applications like Google Street-View. Despite the place correspondence, the visual appearance of old and new images is dramatically different. As shown in Fig. 1, ancient photographs have different colors, texture, and contrast characteristics compared to modern digital images [3]. Moreover it is not possible to control the acquisition perspective: changes in the urban planning along the years may have made some viewpoints not accessible.Numerous efforts have been dedicated to recognizing landmarks in image databases containing photographs of the same era [4–7], but to our knowledge, no previous work focused on tackling location recognition over large time lags. Here we define this task: annotate an ancient photograph with the correct location label, given a set of labeled modern photos. In particular, we propose several useful tools to cope with this problem, making three main contributions:•we introduce a collection of images spanning over 25 locations and more than one century, with the eldest photographs dating back to the 1850s;we present a detailed analysis of existing feature representations, looking for the most robust features, suitable to handle the variability induced by different imaging processes adopted over time;old and new images can be considered as belonging to two different domains. We use existing domain adaptation methods and we show promising results in both location recognition and interactive location retrieval.The rest of the paper is organized as follows. Section 2 revises the related work on location recognition and domain adaptation. Section 3 introduces our large time lags locations dataset and indicates the challenges of location recognition on this testbed. Section 4 briefly reviews the domain adaptation methods used in our study. In section 5 we present and discuss the obtained experimental results. Finally, section 6 concludes the paper and points out possible directions for future research.

@&#CONCLUSIONS@&#
