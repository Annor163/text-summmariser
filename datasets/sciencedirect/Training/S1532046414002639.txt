@&#MAIN-TITLE@&#
Stable feature selection for clinical prediction: Exploiting ICD tree structure using Tree-Lasso

@&#HIGHLIGHTS@&#
We model new application of Tree-Lasso for stable feature selection in healthcare.Tree-Lasso finds more stable features compared to other feature selection methods.Tree-Lasso results in better prediction accuracy compared to other methods.The features selected by Tree-Lasso are consistent with those used by clinicians.

@&#KEYPHRASES@&#
Feature selection,Lasso,Tree-Lasso,Feature stability,Classification,

@&#ABSTRACT@&#
Modern healthcare is getting reshaped by growing Electronic Medical Records (EMR). Recently, these records have been shown of great value towards building clinical prediction models. In EMR data, patients’ diseases and hospital interventions are captured through a set of diagnoses and procedures codes. These codes are usually represented in a tree form (e.g. ICD-10 tree) and the codes within a tree branch may be highly correlated. These codes can be used as features to build a prediction model and an appropriate feature selection can inform a clinician about important risk factors for a disease. Traditional feature selection methods (e.g. Information Gain, T-test, etc.) consider each variable independently and usually end up having a long feature list. Recently, Lasso and relatedl1-penalty based feature selection methods have become popular due to their joint feature selection property. However, Lasso is known to have problems of selecting one feature of many correlated features randomly. This hinders the clinicians to arrive at a stable feature set, which is crucial for clinical decision making process. In this paper, we solve this problem by using a recently proposed Tree-Lasso model. Since, the stability behavior of Tree-Lasso is not well understood, we study the stability behavior of Tree-Lasso and compare it with other feature selection methods. Using a synthetic and two real-world datasets (Cancer and Acute Myocardial Infarction), we show that Tree-Lasso based feature selection is significantly more stable than Lasso and comparable to other methods e.g. Information Gain, ReliefF and T-test. We further show that, using different types of classifiers such as logistic regression, naive Bayes, support vector machines, decision trees and Random Forest, the classification performance of Tree-Lasso is comparable to Lasso and better than other methods. Our result has implications in identifying stable risk factors for many healthcare problems and therefore can potentially assist clinical decision making for accurate medical prognosis.

@&#INTRODUCTION@&#
Recent advances in information technology has changed the way health care is carried out and documented [37]. Nowadays, not only traditional clinical narrative but also other types of data related to healthcare such as laboratory test results, medications and radiological images are automatically captured by databases in modern health centers. Clinical data describing the phenotypes and treatment of patients represents an underused data source that has a great research potential. Mining of Electronic Medical Records (EMR) can yield useful patterns that support clinical research and decision making [21]. The EMR contains rich information about a patient, including demographics, history of hospital visits, diagnoses, physiological measurements and interventions. As these data come with considerable amount of irrelevant and redundant features where only a subset of these features are useful for prediction, feature selection plays an important role to identify important features for building predictive models. For example, it is crucial to identify risk factors of cancer mortality for designing care plan and prognosis of a cancer patient. Similarly, it may be useful to find risk factors that are responsible for avoidable hospital re-admissions to reduce the cost of healthcare.Feature selection methods can be broadly classified into three categories: (1) Filter methods such as T-test, Information Gain [7], ReliefF [52], and Chi Square [31] that assess the relevance of features by looking only at the intrinsic properties of the data. These methods consider each feature separately and ignore dependencies between features. Hence, comparing to other types of feature selection methods, they may cause long feature lists. (2) Wrapper methods such as Beam search [46], Sequential forward selection (SFS) [25], and Sequential backward elimination (SBE) [25] that utilize a supervised learning algorithm in the process of selecting feature subsets. The downside of these techniques is their high cost of computation and the risk of an over-fitted model [23]. (3) Embedded methods such as Weighted naive Bayes [10] and Lasso [49] search for an optimal subset of features, which is built within the classifier construction, and can be seen as a search in the combined space of feature subsets (joint feature selection property) and hypotheses. In this contextℓ1-norm methods such as Lasso has received an increasing attention. Usingℓ1-norm penalty, Lasso regularizes linear models and achieves automatic feature selection by driving some coefficients toward zero.Beyond classifier performance, the other main objective of feature selection is to obtain a stable list of features[23,44]. The stability of feature selection methods has been used to examine their sensitivity to changes in input data and is defined as the degree of agreement of classification models produced by an algorithm when trained on different training sets. The stability is an important property in applications where the features carry intuitive meanings and actions are taken based on these features, e.g. risk factors for cancer survival or hospital readmission prediction must be stable to help a practitioner towards making clinical decisions. The need for stable feature sets has also been strongly felt in pattern recognition community [22,27].Despite obtaining great success in many applications with high dimensional data [43,45,54], Lasso is known to be unstable when features exhibit strong correlations [56,59]. In these situations, Lasso shows acceptable predictive performance but selected predictors are quite sensitive to small changes in data and vary drastically. This variability is due to the property that among several correlated variables, Lasso penalty term (ℓ1-norm) tends to select one of them randomly [49,60]. Different methods have been proposed to solve this problem. Elastic net is one of these remedies that reduce this randomness by adding a convex penalty term (squaredℓ2-norm) [60]. However, it is not capable of exploiting the correlation structure of the data. Other methods use sub-sampling techniques [2,35], where only those features are taken that are stable across several sub-samples. Group Lasso offers another solution when the features form different groups and the variables within a group are correlated. Feature selection is performed at group level by penalizing the sum of theℓ2-norm of these groups, so that if a group is selected then all the features in that group are selected [20,56]. In many problems, features can naturally be represented using certain tree structures e.g. ICD-10 codes used in healthcare data have an intrinsic tree structure, (Fig. 1shows an example of ICD-10 codes for diseases of musculoskeletal system and connective tissue). For such problems, application of group-Lasso is not straight forward.Addressing these problems, we propose to use Tree-Lasso algorithm – a technique which has been proposed as a prediction model for classifying images where its pixels are considered to be the features lying on a tree [32]. However, the stability behavior of Tree-Lasso is not well understood. In this paper we take up this problem and study the stability behavior of Tree-Lasso. To do so, we use two different stability measures Spearman’s rank correlation coefficient and Jaccard similarity measure. We compare its stability with stability of other feature selection methods such as T-test, Information Gain (IG), ReliefF, and Lasso using a synthetic and two real-world datasets: Cancer cohort and Acute Myocardial Infarction cohort. Furthermore, we evaluate predictive performance of Tree-Lasso with other feature selection methods using several classifiers namely, logistic regression, naive Bayes, SVM, decision trees and Random Forest.In summary, Our main contributions are:•Introducing novel application of Tree-Lasso algorithm to obtain stable feature sets for developing healthcare predictive models from ICD codes.An extensive experimental study that shows stability behavior of Tree-Lasso based feature selection is significantly better than Lasso and comparable with other feature selection algorithms.Assessing thepredictive performance of models with the corresponding feature sets using several classifiers, e.g. logistic regression, naive Bayes, SVM, decision trees and Random Forest and find that under the constraint of stable feature selection, Tree-Lasso prediction performance is always better than that of many feature selection algorithms, namely T-test, IG, ReliefF, and Lasso.Comparing the risk-factors obtained using Tree-Lasso with the list of features used by clinical experts and find that many risk factors found by Tree-Lasso are consistent with those used by domain experts.Our findings have implications in identifying stable risk factors for many healthcare problems and therefore assist clinicians and patients to arrive at a better care plan and prognosis. Although, we have applied our model for healthcare data, it is applicable to other real-world problems where features are hierarchical in nature and stability of features is important.The stability of a feature selection algorithm is the robustness of algorithm in selecting features in different training sets which are drawn from same distribution [29,33]. Different methods have been proposed to assess the stability of feature selection algorithms [22]. These methods can be categorized into three groups based on the representation of the selected features used by a specific feature selection algorithm. First group, known as stability by index, considers the indices of the selected features. In this category, the selected features have no particular order or corresponding relevance weight. In the second group, known as stability by weight degree of relevance of each feature is considered by a weight that is assigned to the feature. In the third group, which is called stability by rank, the features order is important in evaluation of stability. In this group, each feature is assigned by a rank that shows the feature importance.In order to measure similarity between subsets of features we use Jaccard index. Jaccard index is a metric that measures the similarity between two sets. Given two setsSqandSq′, the Jaccard indexJSq,Sq′is defined as(1)J(Sq,Sq′)=Sq⋂Sq′Sq⋃Sq′To define a stability measure using Jaccard index, we generate Q sub-samples of the training data, indexed asq=1,…,Q. For each sub-sample, we run feature selection model and obtain a feature set, denoted bySq. Given feature setsS1,…,SQ, the Jaccard stability measure (JSM) is defined as the average of Jaccard indices over each pair of feature sets, i.e.JSq,Sq′. Formally, we have(2)JSM=2Q(Q-1)∑q=1Q-1∑q′=q+1QJSq,Sq′.If we assume that a feature selection algorithm assigns a weight to each feature, to evaluate the similarity between two weight vectorsβ,β′, we use Pearson’s correlation coefficient:(3)PCC(β,β′)=∑j(βj-μβ)(βj′-μβ′)∑j(βj-μβ)2∑i(βj′-μβ′)2.PCC(β,β′)takes values in[-1,1], wherePCC(β,β′)=1implies that the feature weights are completely correlated,PCC(β,β′)=0implies that feature weights are uncorrelated andPCC(β,β′)=-1implies that feature weights are anti correlated.Given weight vectorsβ1,…,βQ, we define PCC as the average of Pearson’s correlation coefficient over each pair of weights of feature sets i.e.PCC(βq,βq′)as follows:(4)PCC=2Q(Q-1)∑q=1Q-1∑q′=q+1QPCC(β,β′).We construct a ranking (denoted by r) over features by sorting the weight vector β. To measure rank based similarity between any two rankings r andr′, we use Spearman’s rank correlation coefficient:(5)SRCC(r,r′)=1-6∑j(rj-rj′)m(m2-1),whererjandrj′are the ranks of jth feature in rankings r andr′and m is the size of the whole feature set. Similar to Pearson’s correlation, the possible range of values are[-1,1], where 1 means that the two rankings are identical, 0 means that there is no correlation between two ranks, and a value of −1 means that rankings are in reverse order.Given rankingsr1,…,rQ, we define SRCC as the average of Spearman’s rank correlation over each pair of ranks of feature sets, so we have:(6)SRCC=2Q(Q-1)∑q=1Q-1∑q′=q+1QSRCC(r,r′).Based on the fact that PCC works directly on the weight vectors that are obtained using each feature selection method, which may use different scales to assign weights and so its results may not be directly comparable across different methods. Therefore, in this paper we only use SRCC and JSM to assess stability of each feature selection method.We study the stability behavior of Tree-Lasso and compare it with various feature selection methods, namely T-test, Information gain, ReliefF and Lasso. In the following, we briefly describe Tree-Lasso and the other feature selection methods. Furthermore, we evaluate the predictive performance of obtained features using each feature selection method by using different types of classifiers such as logistic regression (LR), naive Bayes (NB), support vector machines (SVM), decision trees (DT) and Random Forest (RF). In Section 3.1 we briefly introduce feature selection methods used in this paper and in Section 3.2 we introduce classifiers used for evaluating predictive performance of each feature selection method.In large datasets in order to determine which input features are more important, feature ranking is often used. One of the most important feature ranking measures is T-test. It calculates a ratio between the difference of two class means and the variability of the two classes. Using this ratio we can assess whether the means of two classes are statistically different from each other. In a binary classification problem, for T-test we compute the following test statistic for each featurefj(7)t(fj)=f¯j0-f¯j1sj02N0+sj12N1,wheref¯j0andfj1¯are the feature means for class 0 and class 1,sj0andsj1are the standard deviation of featurefjfrom class 0 and class 1 andN0andN1are size of class 0 and class 1, respectively. In this method after calculating t for each feature, best features (those who have p-value≤0.05) are selected as final feature set.Information Gain (IG) [7] is one of the most important feature ranking methods, which measures dependency between a feature and a class label. IG of jth featurefjand class y is calculated as(8)IG(y|fj)=H(y)-H(y|fj),whereH(·)is the entropy and is a measure of the uncertainty of a random variable. If we assume that we have a two-class classification problem,H(y)andH(y|fj)are defined as follows:(9)H(y)=-P(y=0)logP(y=0)+P(y=1)logP(y=1)(10)H(y|fj)=P(y=0|fj)logP(y=0|fj)+P(y=1|fj)logP(y=1|fj).In this method, for each feature we evaluate IG independently and top K features are selected as the final feature set.Relief [24] is a supervised feature selection algorithm for binary classification problems. It randomly samples instances from the training data and for each sample computes the nearest instance of the same class called “near-hit” and the nearest instance of the different class called “near-miss”. The scoreS(j)of the jth feature is updated in each iteration of algorithm as follows:(11)St(j)=St-1(j)-d(xt-nearHitt)n+d(xt-nearMisst)n,wherextis the random instance at iterationt,nis the number of randomly sampled examples, andd(·)is the Euclidean distance measure. Kononeko et al. [26] proposed ReliefF by using Manhattan (l1) norm instead of Euclidean (l2) norm for finding near-hit and near-miss. For selecting final feature set using ReliefF, we compute S score for each feature and select top K features with best S score as the final selected features.Lasso is a regularization method that is used to learn a regularized regression/classification model that is sparse in the feature space [49]. Consider a supervised learning problem consisting of N training instances denoted as(x(i),y(i),i=1,…N, where eachx(i)∈RPis a P-dimensional feature vector andy(i)∈{0,1}is a class label. For classification problems, Lasso is used with logistic regression [19], which models the probability distribution of the class labely(i)given a feature vectorx(i)as(12)p(y(i)=1|x(i);β)=σ(βTx(i))=11+exp(-βTx(i)),whereβ∈RPis a parameter of the logistic regression model andσ(·)is the sigmoid function. The parameter β is also known as classification weight vector. The Lasso regularization acts by penalizing the sum of absolute value of weights, i.e.ℓ1-norm of β, denoted as||β||1. The combined optimization function can be written as(13)minβ-Σi=1Nlogpy(i)=1|x(i);β+λΣj=1pβjwhere λ is a non-negative regularization term. The solution of the above optimization does not have a closed form and is usually found iteratively by minimizing the cost function using pathwise co-ordinate optimization [13]. By increasing the regularization parameter λ, Lasso increasingly shrinks the coefficients toward 0. A large enough λ, makes some of the weights to become exactly zero.In many applications, the features can be naturally represented as a tree structure, e.g. ICD-10 features in healthcare data form a tree. ICD-10 is “standard diagnostic tool for epidemiology, health management and clinical purposes”.1http://www.who.int/classifications/icd/en/.1Fig. 2shows a part of ICD-10 tree relevant to Cancer dataset used in this paper. The set of diseases shown here relates to the musculoskeletal system (ICD-10 codes:M00up toM99). According to ICD-10 hierarchy, these diseases are classified into 6 groups and each of these groups are further classified into several subgroups, giving rise to a tree-structure. We note that the grouping of codes is mostly based on disease similarity and co-occurrences causing correlations among features. Due to using a flatl1-penalty on features, Lasso randomly selects only one feature from every such correlated set. Although Lasso mechanism for feature selection results in selecting less features, it causes that this method to be unstable in selecting important features. This drawback of Lasso is undesirable in many real-world applications such as clinical prediction.For classification and regression problems having hierarchical features, a more suitable model is the Tree-Lasso [32] as it can exploit the feature correlations in the form of a tree-structure. In this context, the definition of a tree is as follows. For a tree T of depth d, all the nodes corresponding to depth i are inTi={G1i,G2i,…,Gnii}, whereGjidenotes the jth node at depthi,n0=1,G10={1,2,…,p}andni⩾1,i=1,2,…,d. The nodes must satisfy the following two conditions:1.The nodes at the same depth should not have overlapping indices.The index set of a child node is a subset of its parent node.Given the above definition of feature tree, Tree-Lasso learns the classification weight vector β by minimizing the following cost function(14)minβ-Σi=1Nlogpy(i)=1|x(i);β+λϕ(β)where λ is a non-negative regularization parameter. The regularization termϕ(β)is given by(15)ϕ(β)=Σi=1dΣj=1niwjiβGjiwhereβ∈RPis the weight for nodeGji, andβGjiis a vector composed of the entries of β with the indices inGji. The other parameter in the regularization term iswji(i=0,1,…,d;j=1,2,…,ni), which is a predefined weight for the nodeGji. As mentioned in [32], this parameter can be set according to importance of feature groups. In our application, since we do not have any prior knowledge about importance of feature groups, we useωji=1for all the groups.To solve the problem efficiently, the termϕ(β)is re-formulated through Moreau–Yosida regularization asϕλ(v)=minβ12β-v2+λΣiΣjwjiβGjifor someλ>0. It has been shown that the above problem admits an analytical solution. For details of the minimization, we refer the reader to [32].Logistic regression is a linear classifier that models the posterior probabilities of the K classes via linear function in an examplex. In logistic regression, the parameters of the model can be interpreted as changes in log odds and also the results can be interpreted in terms of probabilities [19]. Hence, logistic regression is a widely used classifier in medical domain [1,18,47]. Lasso and Tree-Lasso has built-in logistic regression algorithms. Therefore, these methods can perform feature selection and prediction, simultaneously.Naive Bayes (NB) is a probabilistic classifier based on Bayes theorem [48]. It assumes that given a class label, all the features are independent and posterior probability that an examplex∈RPis classified to class c is obtained as(16)Pr(C=c|x)∝Pr(C=c)∏j=1PPr(xj|C=c).Despite its unrealistic independence assumption, research shows that naive Bayes often works well in practice [42]. furthermore, due to its independence assumption, in naive Bayes the number of parameters (which is equal to the number of features) do not depend on the number of examples. This property helps naive Bayes to scale well for large problems.Support vector machines (SVM) are a type of classifiers that work based on the principle of structural risk minimization (SRM) [6,50,51]. They have some advantages such as ability to handle large feature spaces, and avoidance of overfitting [55]. SVM uses inner product to measure the similarity or distance between patterns, which is known as kernel function. In our experiments, we use Gaussian RBF kernels, where the kernel width σ is of values{0.001,0.005,0.01,0.05,0.1,0.5,1,2}and the value of box constraint C is varied between10-9to105by factors of ten. The best parameters of σ and C are obtained using 5-fold cross validation.Decision trees (DT) are well-known classification methods in the field of machine learning. Popular decision tree algorithms include ID3, C4.5, C5, and CART [4,38,39]. Decision trees recursively partition the data based on its features to construct a tree for the purpose of improving prediction accuracy. To achieve this, they use mathematical algorithms such as information gain (used in ID3, C4.5, C5), Gini index (used in CART), and Chi-squared test (used in CHAID) to specify the variable and its threshold that splits the data into two or more subgroups. The splitting of the data is repeated until the complete tree is constructed. Based on the favorable predictive performance, obtained from preliminary runs, in this study we chose CART as our decision tree method.Random Forest (RF) is an ensemble classifier that generates multiple decision trees and aggregates their results [3]. Each tree is trained on a bootstrap sample of the training data. In addition, a subset of features is randomly selected to consider at each node of each decision tree. To classify an example, decisions (votes) of all trees in the forest are aggregated and the majority voting of the trees is considered as the output of the classifier.In this paper we grow 100 trees in the forest and the number of features at each split is chosen as the square root of the number of features.

@&#CONCLUSIONS@&#
