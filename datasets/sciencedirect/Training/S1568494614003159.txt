@&#MAIN-TITLE@&#
Evolutionary optimization of sparsely connected and time-lagged neural networks for time series forecasting

@&#HIGHLIGHTS@&#
We propose two novel evolutionary neural network approaches (SEANN and TEANN) for time series forecasting (TSF).As the search engine, we adopt the estimation distribution algorithm (EDA).We conducted experiments with six time series, comparing the proposed methods with an four other methods (ARIMA, RF, ESN and SVM).Overall, SEANN and TEANN provided the best forecasting results.

@&#KEYPHRASES@&#
Estimation distribution algorithm,Multilayer perceptron,Time series,Regression,

@&#ABSTRACT@&#
Time series forecasting (TSF) is an important tool to support decision making (e.g., planning production resources). Artificial neural networks (ANNs) are innate candidates for TSF due to advantages such as nonlinear learning and noise tolerance. However, the search for the best model is a complex task that highly affects the forecasting performance. In this work, we propose two novel evolutionary artificial neural networks (EANNs) approaches for TSF based on an estimation distribution algorithm (EDA) search engine. The first new approach consist of sparsely connected evolutionary ANN (SEANN), which evolves more flexible ANN structures to perform multi-step ahead forecasts. The second one, consists of an automatic Time lag feature selection EANN (TEANN) approach that evolves not only ANN parameters (e.g., input and hidden nodes, training parameters) but also which set of time lags are fed into the forecasting model. Several experiments were held, using a set of six time series, from different real-world domains. Also, two error metrics (i.e., mean squared error and symmetric mean absolute percentage error) were analyzed. The two EANN approaches were compared against a base EANN (with no ANN structure or time lag optimization) and four other methods (autoregressive integrated moving average method, random forest, echo state network and support vector machine). Overall, the proposed SEANN and TEANN methods obtained the best forecasting results. Moreover, they favor simpler neural network models, thus requiring less computational effort when compared with the base EANN.

@&#INTRODUCTION@&#
Nowadays, forecasting the future using past data is an important tool to reduce uncertainty and support both individual and organization decision making. For example, multi-step predictions (e.g., issued several months in advance) are useful to aid tactical decisions, such as planning production resources. In particular, the field of time series forecasting (TSF) deals with the prediction of a given phenomenon (e.g., ice cream sales) based on the past patterns of the same event. TSF has become increasingly used in areas such as agriculture, finance, management, production or sales.Several Operational Research TSF methods have been proposed, such as Holt-Winters (in the sixties) or the Autoregressive Integrated Moving Average (ARIMA) methodology [30] (in the seventies). More recently, several Soft Computing methods have been applied to TSF, such as Artificial neural networks (ANN) [13], evolutionary computation (EC) [10] and fuzzy techniques [27] Also, several hybrid systems that combine two or more soft computing and/or forecasting techniques have been proposed for TSF, such as proposed in [2,23,24,35].This paper is focused on the use of ANN [20], which are a natural solution for TSF due to advantages such as flexibility (i.e., no a priori knowledge is required), nonlinear learning and robustness to noisy data. ANN were initially applied to TSF in 1987 [25] and such research has been consistently growing since [13,33,44]. Some examples of successful ANN forecasting applications are Internet traffic [9], air pollution [34] and financial markets [24].While several types of ANN have been proposed for TSF (e.g., radial-basis functions, recurrent networks), the majority of the studies adopt the multilayer perceptron architecture [13,25,44]. In particular, the time-lagged feedforward neural network (TLFN) is a popular approach [9,20,33]. The TLFN adopts a multilayer perceptron ANN as the learning base model and uses a sliding time window method to create supervised training examples. The sliding time window defines a set of time lags that are used as inputs by the ANN.When adopting multilayer perceptrons for TSF (i.e., TLFN), a crucial issue is the design of the best forecasting model, which involves both feature and model selection [13,40]. The former is required since a small set of time lags will provide insufficient information to the ANN, while using a high number of time lags will increase noise and probability of having irrelevant inputs. Indeed, time lag selection is a core step of the ARIMA methodology, which often selects the 1, 12 and 13 time lags for monthly seasonal and trended series [30]. The latter selection is needed to get a good generalization capacity, since a too complex ANN model will overfit the data, while a model that is too simple will present limited learning capabilities. However, most ANN works for TSF adopt a manual design for this feature and model selection that is ad hoc (e.g., [9,12,22,25,33,44]), based either in domain knowledge or in trial and error experimentation. An alternative is use Evolutionary Computation to search for the best ANN, in what is known as Evolutionary ANNs (EANNs) [15,38,42]. Often, EANNs require more computation when compared with manual ANN design, since more ANNs are tested. Yet, EANNs are much more appealing to non specialized users, given that few parameters need to be selected, the search is fully automatic and more exhaustive, thus tending to provide better performances when compared with the manual design.EANN systems have been treated mainly using three different optimization points of view [6,42]: topology (e.g., number of hidden layers, number of nodes in each layer); connection weights (e.g., values for each ANN connection); and learning rules (e.g., learning factor). Within the TSF domain, the majority of EANN works make use of rather rigid ANN structures that are fully connected, evolving only ANN hyperparameters, such as number of input and hidden nodes [6,34]. For instance, once the number of inputs is set, all time lags are adopted by the TLFN. Working with fully connected structures also means that ANNs can be more complex than needed. As a consequence, these EANNs tend to require an heavy computational effort. Moreover, most EANN works for TSF use the standard Genetic Algorithm (GA) as the search engine, which requires setting several parameters to (e.g., mutation rate, population size). The Estimation Distribution Algorithm (EDA) is a more recent Evolutionary Computation variant, proposed in 2001 [26], and that makes use of exploitation and exploration properties to find good solutions. When compared with other search methods (e.g., GA), EDA has the advantage of requiring just one parameter (i.e., population size), since crossover and mutation processes do not exist in EDA. Also, EDA has a fast convergence and in recent previous work [35] it has outperformed the standard GA and differential evolution methods when selecting the best ANN TSF models.In this paper, we propose two novel EANN variants for TSF that are fully automatic and can be used by non specialized users to perform multi-step ahead time series forecasts, since no a priori knowledge is assumed from the analyzed time series. In contrast with the majority of EANN works for TSF, the proposed EANN variants make use of EDA as the search engine under two design strategies: Sparsely connected EANN (SEANN) and Time lag selection EANN (TEANN). Both strategies perform a simultaneous feature and model selection for TSF, although with a different emphasis. SEANN puts more effort in model selection by explicitly defining if a connection exists and time lag deletion only occurs when an input has no connections. TEANN enforces feature selection, explicitly defining which time lags are used in the chromosome, while ANN structure selection is made only in terms of number of input and hidden nodes. These strategies are addressed separately in order to measure the contribution of each other when compared with the fully connected EDA EANN [35]. Moreover, we also compare all EANN methods with the popular ARIMA methodology and three recently proposed machine learning methods: Random Forest (RF), Echo State Network (ESN) and Support Vector Machine (SVM). The experiments were performed using several real-world time series from distinct domains and the distinct forecasting approaches were compared under both forecasting and computational performance measurements. The paper is organized as follows. First, section “Evolutionary design of artificial neural networks” described the EANN approaches. Next, in section “Experimental setup and results” we present the experimental setup and analyze the obtained results. Finally, we conclude the paper in section “Conclusions”.The problem of forecasting time series with ANN [35] is considered as obtaining the relationship from the value at period yt(in this system the resulting ANN will have only one output neuron) and the values from previous elements of the time series, using several time lags {t−1, t−2, …, t−I}, to obtain a function:(1)ytˆ=f(yt−1,yt−2,…,yt−I)whereytˆdenotes the estimated forecast, as given by the ANN (f), and I the number of ANN input nodes.In order to obtain a single ANN to forecast time series values, an initial step has to be done with the original values of the time series, i.e., normalizing the data. The original values (yt) are normalized into the range [0, 1] (leading to the Ntvalues). Once the ANN outputs the resulting values, the inverse process is carried out, rescaling them back to the original scale. Only one neuron was chosen at the output layer and multi-step ahead forecasts are built by iteratively using 1-ahead predictions as inputs [9]. Therefore, the time series is transformed into a patterns set depending on the k inputs nodes of a particular ANN, each pattern consisting of:•I inputs values, that correspond to I normalized previous values: Nt−1, Nt−2, …, Nt−I.One output value: Nt(the desired target).This patterns set will be used to train and validate (i.e., compute fitness value) each ANN generated during the evolutionary execution. Thus, the patterns set is split into two subsets, using a timely ordered holdout scheme with 70% of the elements for training and the most recent 30% elements for validation. We note that the 70/30 split is very common (e.g., [22,27]) and in [12] this split provided better TSF results for ANN when compared with other divisions (e.g., 60/40 and 80/20). As an example, Fig. 1shows how such training and validation sets are created with I = 3. Finally, after evolving the ANN, the best model is evaluated on a test set, which includes the most recent ytelements that were not used during the EANN procedure.The problem of designing ANN could be seen as a search problem into the space of all possible ANN. While several EC methods could be used for this search, we adopt in this paper the EDA algorithm, since it has outperformed the standard GA in our previous work [35]. As a base ANN structure, we adopt the TLFN with all time lags, from 1 to i, as the forecasting model. We use fully connected multilayer perceptrons with only one hidden layer and one output node [33,35].The Resilient Propagation (RPROP) learning algorithm is an enhanced version of the backpropagation algorithm, performing a local adaptation of the weight-updates based on behavior of the error function, given by the local gradient information. When compared with other algorithms (e.g., backpropagation), the RPROP presents a faster training, requiring less computational effort [31,37].In this paper, an evolving hybrid system that uses EDA and RPROP learning algorithm, is adopted. This approach uses a digit number representation (i.e., ∈{0, …, 9}) to encode the ANN topology hyperparameters and RPROP learning parameters, with multiple initializations. Under this EANN, the hyperparameters that we optimize are the number of inputs nodes (i) and number of hidden neurons (h) of the hidden layer. For the ANN learning, we use the RPROP algorithm [37], which presents a faster training convergence when compared with other algorithms (e.g., Backpropagation), requiring less computational effort. The performance of RPROP may depend on the correct adjustment of two numeric parameters, known asΔmax∈R(although the default value is 50.0) and α∈[0, 1] (although the value is typically closer to 0), also known as Δ0. Hence, we adopt a direct encoding schema, which places into the chromosome (Fig. 2): two decimal digits (i.e., from 0 to 9), to codify the number of inputs nodes (i); another two digits for the number of hidden nodes (h); two more Δmax∈0, 1, …, 99 and finally one gene for α={1, 0.01, 0.001, …, 10−9}.As explained in Section “Introduction”, EDA contains just one parameter, the population size (P). Since the EDA works as a second order optimization procedure, the tuning of this internal parameter is not a critical issue. In this work, a fixed value of P=50 was adopted. In preliminary experiments, we tested a sensitivity analysis with other values (e.g., P=48 and P=52) and achieved similar results.The EANN search process consists of the following steps (Fig. 3):1A randomly generated population, i.e., a set of randomly generated chromosomes, is obtained.The phenotypes (ANN architectures) and fitness value of each individual of the actual generation is obtained. To obtain the phenotype associated to a chromosome and its fitness value:(a)The phenotype of an individual of the actual generation is first obtained (using the Stuttgart Neural Network Simulator (SNNS) tool [43]).Then, for each ANN, training and validation pattern subsets are obtained from time series data depending on the number of inputs nodes, as it was explained in Section “Time series”The ANN is trained with RPROP (using SNNS). The architecture (topology and weights) of the ANN when the validation error (i.e., error for validation patterns subset) is minimum during the training process is stored (i.e., we adopt early stopping). Thus, this architecture is the final phenotype of the individual. The fitness for each individual is the minimum mean squared error (MSE) validation error (Eq. (2)), during the learning process.Once the fitness values for whole population have been already obtained, Univariate Marginal Distribution Algorithm (UMDA)-EDA (with no dependencies between variables) [35] operators are applied in order to generate the next population. The UMDA operators work as follows. First, a truncation selection is adopted, which selects half of the best solutions from the current population. This subset population is included in the next population. Then, a distribution probability for the subset population is automatically estimated. For instance, if there are 25 individuals in the subset and only 5 of these individuals contain the first gene with a 1 digit, then the probability for setting this gene as 1 is set to 5/25=0.2. New individuals are then sampled using the probabilities previously calculated and included in the next population.Steps 2 and 3 are iteratively executed until a maximum number of generations is reached.The fitness function uses the MSE and not the mean absolute error (MAE) due to three main reasons. First, squared error metrics are more popular to validate forecasting methods [1,12,30]. Second, MSE is more sensitive to extreme errors (i.e., outliers) than MAE. Given that we use feedback of 1-ahead predictions to generate ANN multi-step forecasts, such outliers would dramatically propagate and affect the performance of long term predictions. Third, preliminary experiments with two series (Passengers and Temperature) have revealed better results for the MSE fitness approach when compared with the MAE one. In these preliminary experiments, the training data (e.g., 125 samples for Passengers) was further split into training (e.g., 74 samples for Passengers, used to train the ANN), validation (e.g., 32 samples for Passengers, used to compute the fitness) and test (e.g., last 19 of the 125 Passengers samples, used to compare the forecasting performance of MSE and MAE fitness approaches).It should be noted that this EANN is competitive. This method was ranked at 6th position, when comparing the other Soft Computing methods, at the NN5 competition [11]. EANN presented an average symmetric mean absolute percentage error (SMAPE) error of 21.9% and also outperformed the ARIMA forecasts performed by the commercial Autobox tool (www.autobox.com, average SMAPE of 23.9%).The topic of ANN topology selection was first suggested by Miller et al. [32], which proposed GA as a very good candidate for the search. Miller et al. identified two approaches to code the topology in a string: the strong specification scheme (or direct encoding scheme), where each connection of the network is specified by its binary representation, and a weak specification scheme (or indirect encoding scheme), where the exact connectivity pattern is not explicitly represented but it is computed on the basis of the information encoded in the string by a suitable developmental rule. Several authors have followed Miller et al. suggestion, including Whitley et al. [41] and Schaffer et al. [39], which adopted a direct encoding scheme. The main advantage of this direct approach is that it is easy to evolve networks with special connectivity properties, either by constraining the representations allowed or by including some specific penalty term in the fitness function. On the other hand, the disadvantage is that these representations do not scale well to large networks, with hundreds of nodes and thousands of connections. However, when dealing with networks of smaller size, the direct encoding scheme induces a less-fuzzy fitness function, i.e., the fitness value associated with each chromosome is more coherent when it is computed several times using different training sets.The proposed Sparsely EANN (SEANN) works as the previous EANN (section “Evolutionary neural network design”), except that now we can evolve more flexible structures. To achieve this, and following the work of [41] and [39], we adopted a direct binary encoding scheme that defines which connections are used by the ANN. The proposed chromosome includes three components. The first two components work exactly as described in section “Evolutionary neural network design”. The maximum number of input and hidden nodes are needed to set the connection matrix size and the two RPROP parameters are used to train the ANN. The third component includes the direct binary encoding of the ANN connections. Fig. 4shows the sparsely connected chromosome, where the last binary digits (i.e., connection matrix) set the active connections of the model.In Fig. 5, it can be observed an example of how the direct binary codification works, in order to obtain the ANN connection matrix from the chromosome. In general, each matrix cell represents a valid connection between an incoming node (at the row) with the outgoing node (at the column). In the example, the third digit (b3=1) sets a connection between the first input node and the third hidden node. The exception is the last row, which represents the connections between the hidden nodes (at the columns) and the output one (the last row). By default, the largest possible connection matrix is always set, with the dimensions Row×Col, although the real dimensions are limited by the i and h values. We have opted for this solution to set the same fixed length of the chromosome, for all the individuals. Every time a new individual (genotype) is used to generate an ANN (phenotype) first the i and h parameters are read, in order to discard the unwanted extra columns and rows of the default matrix, leading to a matrix with the dimensions (i′+1)×h′, where i′ and h′ denote the real number of input and hidden nodes. In most cases i=i′ and h=h′. Yet, in some rare cases it may occur that i′<i or h′<h if the last binary rows (or columns) are all set to zero.This section presents the proposed Time lag selection EANN (TEANN), which is similar to the EANN of section “Evolutionary neural network design” accept that we now also search for the best set of time lags for the TLFN. As it was explained in section “Time series and ANN”, every time a new individual (i.e., ANN) is generated, training and validation patterns subset have to be obtained. In previous example (Fig. 1), if the ANN had k input nodes, all k previous values from the time series (t−1,t−2,...,t−k) were used to generate the patterns set. A new level of specialization is considered here, where the patterns are obtained by time lag filtering; i.e., selecting the relevant previous time lags of the series to generate the patterns to feed the ANN, which defines how the sliding window is set.To carry out this new approach, more genes were added into the base chromosome. In particular, we adopted a binary codification, where each new gene defines if the time lag is (or not) used by the model. Fig. 6shows the new codification scheme that includes time lag selection.We set k to the maximum number of inputs (i.e., 100). Yet, it should be noted that i now sets the maximum number of input nodes, i.e., only the up to bitime lags are considered by the model. Lets considering the same example of Fig. 1, where i=3, but now with the time lag selection where b1=1, b2=0 and b3=1. As shown in Fig. 7, the number of input nodes of the ANN are set not only by i=3, but also depends on the binary encoding, which only activates two lags, thus I=2. Thus, the first pattern is Nt0, Nt2⇒Nt3.In this work, we selected a total of six time series, with different characteristics and from distinct domains. Five series were selected from the well-known Hyndman's time series data library repository [21]. These are named Passengers, Temperature, Dow-Jones, Quebec and Abraham12. Passengers have the information about the number of passengers of an international airline in thousands, measured monthly from January of 1949 till December of 1960. Temperature is about the mean monthly of air temperature measured at Nottingham Castle from 1920 till 1939. Dow-Jones contains the monthly closings of the Dow-Jones industrial index from August of 1968 till August of 1981. Abraham12 represents gasoline demand at Ontario, in millions of gallons, from 1960 to 1975. Quebec includes the number of births, as daily measured in Quebec, from 1st of January of 1977 till 31 of December of 1978. We also adopt the Mackey–Glass series [16], which is a common benchmark for comparing the generalization ability of different forecasting methods. This series is a chaotic time series generated from a time-delay ordinary differential equation.It should be noted that these six times series were also adopted by the NN3 and NN5 forecasting competitions [11]. Except for Mackey–Glass, all datasets are from real-world domains and such data can be affected by external issues (e.g., floods, strikes, technological advances), which make them interesting datasets and more difficult to predict.

@&#CONCLUSIONS@&#
Time series forecasting (TSF) uses past patterns from a given event to forecast its future values. TSF is useful to support decision making in several domains (e.g., finance or production). In this paper, we approach multi-step ahead TSF using Artificial Neural Networks (ANNs). In particular, we adopt fully automatic Evolutionary ANN (EANN) search methods, which do not require prior knowledge from the user and that are based on evolutionary computation. As the evolutionary engine, we use the Estimation Distribution Algorithm (EDA), which has outperformed the standard genetic algorithm and differential evolution in our previous work [35]. Furthermore, we proposed two novel evolutionary design strategies: sparsely connected evolutionary ANN (SEANN) and Feature selection EANN (TEANN).The two novel forecasting strategies (SEANN and TEANN) were compared over six distinct time series. As three baseline benchmarks, we also performed forecasts using the base EANN strategy, the popular ARIMA methodology and three recently proposed machine learning methods: Random Forest (RF), Echo State Network (ESN) and Support Vector Machine (SVM). Also, the obtained multi-step forecasts were analyzed under two error criteria: MSE and SMAPE. The experiments held reveal the proposed SEANN and TEANN as the best forecasting methods. Moreover, when compared with the base EANN, both SEANN and TEANN tend to favor simpler structures and require less computational effort. Globally, when comparing SEANN and TEANN, similar predictive capabilities are achieved, although the former method tends to give better overall results, while requiring less computation effort and optimizing simpler strategies. Thus, we recommend SEANN as the best option for TSF with ANN.In the future, we intend to explore a mixed TEANN-SEANN approach, by using the EANN to evolve both the time lags and the ANN connection weights. Such mixed approach is expected to optimize simpler ANN structures. In addition, the EDA search algorithm can be improved by using dependencies between its variables like Mutual Information Maximization for Input Clustering (MIMIC) [26] (i.e., variables with order one dependencies) or even ”tree” EDA [26], with no restriction on the numbers of dependencies.