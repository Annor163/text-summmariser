@&#MAIN-TITLE@&#
Combined VMD-SVM based feature selection method for classification of power quality events

@&#HIGHLIGHTS@&#
Different types of power quality disturbances are discriminated from each other.VMD and ST are used for extraction of dominant features.SVM with simple structure and few adjustable parameters is used as the classifier core.The generalization capability and detection accuracy of the proposed method are increased by elimination of redundant features by using different feature selection methods.The start and end points of PQ events can be detected accurately.

@&#KEYPHRASES@&#
Feature selection,Pattern recognition,Support vector machines,Signal analysis,S-transform,Variational mode decomposition,

@&#ABSTRACT@&#
Power quality (PQ) issues have become more important than before due to increased use of sensitive electrical loads. In this paper, a new hybrid algorithm is presented for PQ disturbances detection in electrical power systems. The proposed method is constructed based on four main steps: simulation of PQ events, extraction of features, selection of dominant features, and classification of selected features. By using two powerful signal processing tools, i.e. variational mode decomposition (VMD) and S-transform (ST), some potential features are extracted from different PQ events. VMD as a new tool decomposes signals into different modes and ST also analyzes signals in both time and frequency domains. In order to avoid large dimension of feature vector and obtain a detection scheme with optimum structure, sequential forward selection (SFS) and sequential backward selection (SBS) as wrapper based methods and Gram–Schmidt orthogonalization (GSO) based feature selection method as filter based method are used for elimination of redundant features. In the next step, PQ events are discriminated by support vector machines (SVMs) as classifier core. Obtained results of the extensive tests prove the satisfactory performance of the proposed method in terms of speed and accuracy even in noisy conditions. Moreover, the start and end points of PQ events can be detected with high precision.

@&#INTRODUCTION@&#
The power quality (PQ) problem can be interpreted as voltage quality. In nowadays-electrical networks, the voltage waveforms are more distorted due to augmented use of nonlinear loads like solid-state switching devices, power electronically switched loads, computers and data processing equipment, industrial plant rectifiers and inverters. Distorted voltage signals can affect the proper operation of these sensitive electronic devices. Therefore, PQ monitoring is an essential task to evaluate the performance of power systems. Before any corrective solution, the type of PQ disturbances should be determined by an accurate detection scheme [1].Different solutions based on pattern recognition methods have been proposed for detection of PQ events [2]. Most of them are realized through two main steps: feature extraction and feature classification. Different signal analysis tools have been used for extraction of dominant features. Some of them analyze signals in time domain like empirical mode decomposition (EMD) [3,4] and some analyze signals in frequency domain such as Fourier transform (FT) [5,6] and some like wavelet transform (WT) [7–11], S-transform (ST) [12–16] and Hilbert transform [17,18] analyze wave shapes in both time and frequency domains, simultaneously.The main drawback of EMD is that the number of intrinsic mode functions (IMFs) changes according to waveform of PQ events. Moreover, it provides no information about frequency contents of signals. WT decomposes signals into details and approximation levels in consecutive steps using special filter called mother wavelet. Extracted features from approximation and detail levels can be used for classification [19,20]. WT based methods suffer from two disadvantages: firstly the different types of mother wavelet and different decomposition levels should be scrutinized to obtain the best performance and secondly extracted features from details levels are sensitive to noise. ST as another signal processing tool provides a complete visualization of signal in both time and frequency domains, simultaneously. The output of ST is a matrix whose rows and columns pertain to frequency and time, respectively. Each element of the matrix is a complex number, thus the magnitude and phase of each frequency component is determined for each time instant.In order to classify extracted features, different classifiers have been implemented such as: feed forward neural network (FFNN) [3,5,15,17], decision tree (DT) [14], support vector machines (SVM) [7,8,11] and probabilistic neural network (PNN) [3]. Neural networks (NNs) are well known for their learning ability but its learning process is a very time consuming task and there is no definite rule for optimum setting of NNs’ structures. PNN needs a large number of exemplar patterns to yield acceptable classification accuracy. This obstacle leads to slow execution speed and large memory requirements.In spite of large number of research works in the field of PQ events detection, there is still a lack of analysis of extracted features effects on classifiers detection accuracy. On the other hand large dimension of extracted features may mislead the classifier which results in the reduction of detection accuracy. So, redundant features should be eliminated from extracted features. Moreover, most of the proposed method are not able to detect the interval time in which the signal is distorted [3–17].In this study, a relatively large dimension feature vectors are extracted using ST [21] and VMD [22], and more useful features are selected applying several feature selection methods based on filter and wrapper. Wrapper based method are very time consuming due to their huge computational burden while filters are faster since they rank features based on intrinsic attributes. Sequential forward selection (SFS) [23] and sequential backward selection (SBS) [24] as wrapper based methods and Gram–Schmidt orthogonalization (GSO) based feature selection [25] as a filter based method are used to eliminate redundant features. In the last step, dominant selected features are discriminated by support vector machines (SVMs) classifier [26,27]. The proposed detection scheme has advantages from the following aspects:•VMD and ST have few tuning parameters as compared to WT which has many mother wavelet filters and decomposition levels.SVM classifier with simple structure and only few adjustable parameters has better performance as compared to FFNN.Elimination of redundant features by using feature selection methods, augments the generalization capability and detection accuracy of the proposed method.Results have been presented for both filter and wrapper based feature selection methods.The start and end points of events can be detected by using mode 2 of VMD analysis.The proposed algorithm is robust in noisy conditions.The VMD is a signal processing technique that decomposes a real valued signal into different levels named modes uk, which have specific sparsity properties while producing main signal. It is assumed that each mode k to be concentrated around a center pulsation ωkdetermined during the decomposition process. Thus, the sparsity of each mode is chosen to be its bandwidth in spectral domain. In order to obtain the mode bandwidth, the following steps should be implemented: (1) applying Hilbert transform to each mode ukin order to obtain unilateral frequency spectrum. (2) Shifting the mode's frequency spectrum to “baseband”, by using an exponential tuned to the respective estimated center frequency. (3) Estimation of the bandwidth through the H1 Gaussian smoothness of the demodulated signal, i.e. the squared L2 – norm of the gradient. Thus, the decomposition process is realized by solving the following optimization problem [22]:(1)min∑k∂t[δ(t)+jπt*uk(t)]e−iωkt22s.t.∑kuk=f(t)where f(t) is the main signal to be decomposed, {uk}={u1, …, uK} and {ωk}={ω1, …, ωK} implicates the set of all modes and their center frequencies, respectively. δ(t) is the Dirac distribution and * denotes convolution. In order to address the constraint, both penalty term and Lagrangian multipliers λ are considered. The combination of the two terms benefits both from the nice convergence properties of the quadratic penalty at finite weight, and the strict enforcement of the constraint by the Lagrangian multiplier. So, the above optimization problem is changed to unconstraint one as below [22]:(2)L({uk},{ωk},λ)=α∑k∂tδ(t)+jπt*uk(t)e−iωkt22+f(t)−∑ku(t)22+λ(t),f(t)−∑ku(t)Then the alternate direction method of multipliers (ADMM) is used for solving the original minimization problem (2) by finding the saddle point of the augmented LagrangianLin a sequence of iterative sub-optimizations. Plugging the solutions of the sub-optimizations into the ADMM, and directly optimizing in Fourier domain, the complete algorithm for variational mode decomposition is summarized in the following algorithm [22].Algorithm: Complete optimization of VMDInitializeuˆk1,ωk1,n←0repeatn←n+1fork=1: KdoUpdateuk1for all ω≥0:uˆkn+1←fˆ(ω)−∑i<kuˆin+1(ω)−∑i>kuˆin(ω)+(λˆn(ω)/2)1+2α(ω−ωkn)2Update ωk:ωkn+1←∫0∞ωuˆkn+1(ω)2dω∫0∞uˆkn+1(ω)2dωend forDual ascent for all ω≥0:λˆn+1(ω)←λˆn(ω)+τfˆ(ω)−∑kuˆkn+1(ω)until convergence:∑kuˆkn+1−uˆkn22/uˆkn22<εAccording to above algorithm, ukand ωkshould be updated to realize the VMD analysis process. To update the modes, the optimization problem of relation (2) is solved with respect to uk. This sub optimal problem is represented as follows [22]:(3)ukn+1=argminuk∈Xα∂tδ(t)+jπt*uk(t)e−iωkt22+f(t)−∑iui(t)+λ(t)222The solution of this quadratic optimization problem is readily found by letting the first variation vanish for the positive frequencies [22]:(4)uˆkn+1(ω)=fˆ(ω)−∑i≠kuˆi(ω)+(λˆ(ω)/2)1+2α(ω−ωk)2which is clearly identified as a Wiener filtering of the current residual, with signal prior 1/(ω−ωk)2. The full spectrum of the real mode is then simply obtained by Hermitian symmetric completion. Conversely, the real part of the inverse Fourier transform of this filtered analytic signal yield the mode in time domain.In the second subproblem, the optimization problem of relation (2) is solved with respect to ωk. The center frequencies do not appear in the reconstruction fidelity term, but only in the bandwidth prior. The relevant problem thus reads [22]:(5)ωkn+1=argminωkα∂tδ(t)+jπt*uk(t)e−iωkt22The solution of above suboptimization problem in frequency domain is obtained as follows [22]:(6)ωkn+1=∫0∞ωuˆk(ω)2dω∫0∞uˆk(ω)2dωwhich puts the new ωkat the gravity center of the corresponding mode's power spectrum. This means carrier frequency is the frequency of a least squares linear regression to the instantaneous phase observed in the mode [22].The purpose of SVM is to find an optimal separating hyperplane by maximizing the margin between the separating hyperplane and the data set [26]. Given a set of dataT=xi,yii=1mwhere xi∈Rndenotes the input vectors,yi∈+1,−1stands for two classes, and m is the sample number. It is supposed that the hyperplane f(x)=0 separates the given data which are linearly separable.(7)f(x)=w⋅x+b=∑k=1mwk⋅xk+b=0where w and b denotes the weight and bias terms. These variables adjust the position of the separating hyperplane. The separating hyperplane should satisfy the following constraints:(8)yif(xi)=yiw⋅xi+b≥1,i=1,2,…,mPositive slack variables ξiare defined to measure the distance between the margin and the vectors xithat lie on the wrong side of the margin. Then, the optimal hyperplane separating the data can be obtained by the following optimization problem:(9)Minimize12w2+C∑i=1mξi,i=1,2,…,msubject toyi(w⋅x+b)≥1−ξiξi≥0where C is the error penalty.By introducing the Lagrangian multipliers αi, the above-mentioned optimization problem is transformed into the dual quadratic optimization problem, that is:(10)MaximizeL(α)=∑i=1mαi−12∑j=1mαiαjyiyj(xi,xj)(11)subject to∑i=1mαiyi=0,αi≥0,i=1,2,…,mThus, the linear decision function is created by solving the dual optimization problem, which is defined as:(12)f(x)=sign∑ij=1mαiyi(xi,xj)+bSVM can also be used in nonlinear classification using kernel function. The nonlinear mapping function φ is applied to map the original data x into a high-dimensional feature space, where the linear classification is possible. Then, the nonlinear decision function is:(13)f(x)=sign∑ij=1mαiyiK(xi,xj)+bwhere K(xi, xj) is called the kernel function, K(xi, xj)=ϕ(xi)ϕ(xj). The most commonly used kernel functions are as follows [26,27]:(1)Linear:K(xi,xj)=xiTxjPolynomial:K(xi,xj)=(βxiTxj+r)d,β>0Gaussian radial basis function:K(xi,xj)=exp(−ρxi−xj2),ρ>0Sigmoid:K(xi,xj)=tanh(βxiTxj+r)Here β, r, ρ and d are kernel parameters. Currently in the literature, there is no method available for deciding the value of C, for choosing the best kernel function and for setting the kernel functions. As the proper setting of the SVM parameters has direct effect on the detection accuracy, the appropriate kernel function and other parameters are obtained using heuristic optimization techniques such as continuous ant colony optimization algorithms [28].The SVMs find an optimal hyperplane on the feature space. Therefore, to classify more than two classes, two straightforward approaches could be used. The first approach is to compare class by class with several machines and combine the outputs using some decision rule. The number of machines Nmneeded for the m classes, separation problem is given by [26,27]:(14)Nm=m!2(m−2)!In this approach, each class is associated with m−1 outputs. The advantage of such method is that it gives information about class-by-class separation, which could be used in another system to solve the problem of simultaneous events. In this work, a simple decision rule is used: the winner class is the one which all related outputs m−1 are greater than zero.The second approach to solve the multiple class (m>2) separation problem using SVMs is to compare one class against others. Therefore, the required number of machines Nmis the same number of classes m. The decision rule applied to the m outputs is the winner takes all. In this case, the decision rule is simplified because each output is related to one class. An advantage of this solution is the expected lower computational cost in comparison to the first approach as fewer machines are needed.Sequential forward selection was first proposed in [23]. It operates in the bottom-to-top manner. The selection procedure starts with an empty set initially. Then, at each step, the feature maximizing the criterion function is added to the current set. This operation continues until the desired number of features is selected. The nesting effect is present such that a feature added into the set in a step cannot be removed in the subsequent steps [11]. As a consequence, SFS method can offer only suboptimal result.Sequential backward selection [24] method proposed in works in a top-to-bottom manner. It is the reverse case of SFS method. Initially, complete feature set is considered. At each step, a single feature is deleted from the present set so that the criterion function is maximized for the remaining features within the set. Removal operation continues until the desired number of features is obtained. Once a feature is eliminated from the set, it cannot enter into the set in the subsequent steps [11].GSO process is a simple forward selection method which can be effectively used for features ranking. Suppose the kth feature of M patterns is denoted by vectorXk=xk1,xk2,…,xkMTandY=y1,y2,…,yMTrepresents the vector of output target. In order to select the best correlated feature with output, the cosine of angle between each input feature Xkand target Y is calculated as an evaluation criterion [25]:(15)cos(φk)=Xk⋅YXkYwhere φkis the angle between input kth feature vector Xkand output target Y, N is the number of all features andXk⋅Ydenotes the inner product between Xkand Y. If the output is fully proportional to input, the φkis zero, and inversely if the output is fully uncorrelated to input, the φkis π/2 [25]. So, in an iterative procedure the feature that maximizes the above mentioned evaluation criterion, is selected as the most correlated feature to target. For selection of the next feature, the output vector and all other candidate features are mapped to null space of the selected feature and then input features and output vectors are updated with new data. The ranking procedure is repeated until all candidate features are ranked, or when a predetermined stopping condition is met [25].

@&#CONCLUSIONS@&#
Despite an extensive number of research works in the area of PQ events detection, there is still a need of precise analysis of combination of extracted features. At first large dimension of extracted feature vectors are constructed using well-known analysis tools VMD and ST, and then more meaningful and important feature subsets are selected using several feature selection methods on the basis of filter and wrapper methods. By elimination of redundant features, required memory and computational burden of detection process decrease while its generalization capability is improved, considerably. Results show that extracted features using only a single signal analysis tool cannot yield the best detection accuracy. On the other hand, the extracted features using both signal analyzers, i.e. VMD and ST together with SVM classifier has the best detection accuracy. Among the applied feature selection methods, SBS gives the best detection accuracy of 99.66%. Besides, mode 2 which is the output of VMD analysis can be used for detection of start and end of PQ events. Thus, this new presented algorithm can be effectively used for monitoring of PQ disturbances in real time application.