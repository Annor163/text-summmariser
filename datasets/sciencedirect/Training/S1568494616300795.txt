@&#MAIN-TITLE@&#
Adaptive modulation recognition based on the evolutionary algorithms

@&#HIGHLIGHTS@&#
Presentation a novel modulation recognition algorithm based on clustering approach.Distinguishing multicarrier modulations of OFDM from single-carrier modulations.Extraction of two statistics of the amplitude of received signal as key features.Considering four famous optimization algorithms named: TLBO, PSO), ACO, and GSA.Simulation results demonstrate the efficiency of proposed algorithm at low SNR.

@&#KEYPHRASES@&#
Automatic Modulation Recognition (AMR),Multicarrier (MC) modulations,Single-carrier (SC) modulations,Orthogonal Frequency-Division Multiplexing (OFDM),Feature extraction,

@&#ABSTRACT@&#
Nowadays, the development of classification algorithms gives the ability to improve Automatic Modulation Recognition (AMR) effectively. This paper presents a novel modulation recognition algorithm based on clustering approach. Generally, we aim to distinguish multicarrier modulation OFDM from single-carrier modulations. In this regard, two statistics of the amplitude of the received signal are calculated at the output of a quadrature mixer as key features. The extracted features of training data points are submitted to the clustering algorithm, then, centroids for single-carrier and multicarrier modulations are assessed. Afterwards, each point of testing dataset is dedicated to its nearest centroid based on Euclidian distance and the recognition is accomplished. Simulation results demonstrate that the algorithm is beneficial in a wide range from low to high SNRs.

@&#INTRODUCTION@&#
For any communication signal analysis, knowing the modulation type is an important subject. Thus, automatic modulation recognition has gotten a major importance in different communication systems such as software defined radios, cognitive radios and military communication systems for various purposes including spectrum monitoring and adaptive communication. Basically, available Automatic Modulation Recognition (AMR) algorithms can be categorized into two main approaches [1]: (1) Feature-Based method (FB) and (2) Likelihood-Based algorithm (LB). FB methods classify the received signals based on extracted parameters called key features. These features can be derived from instantaneous information, higher order statistics, spectrum features, and etc. Although this technique leads to a suboptimum solution, it decreases the computational complexity. With regard to two distinct part of FB algorithms i.e., feature extraction and classifier, several investigations have been conducted in this area [2]. In [3], a genetic algorithm selects a suitable combination of higher order cumulants and higher order moments as key features and a hierarchical support vector machine is proposed as classifier. Ebrahimzadeh et al. proposed higher order statistics as features and hybrid radial basis function neural network (classifier) and particle swarm optimization in [4]. In [5], authors proposed an investigation on two classifiers, i.e., neural network and support vector machine in AMR. These algorithms have high percentage of correct classification to distinguish different types of digital signal even at low SNRs. As it is evident, in LB algorithms, probabilistic and hypothesis arguments are utilized to solve the modulation recognition problem [6–9]. The LB algorithms attempt to minimize the probability of misclassification and attain optimal solution. In despite of giving the optimal solution, the LB approaches suffer from several disadvantages. Owing to analytical difficulties, computational complexity is imposed on the implementation of such algorithms. In contrast to LB method, easy implementation and lower complexity of FB techniques made these methods more popular.The recent trends have shown a growth of multicarrier modulation utilization in various applications including DVB, WLAN, WMAN and etc. This issue necessitates researchers to introduce novel algorithms for distinguishing multicarrier (MC) modulations from single-carrier (SC) ones. For instance, in [10], a detector based on fourth-order cumulants is proposed which employs the Gaussian characteristic of Orthogonal Frequency-Division Multiplexing (OFDM) in the time domain. In this method, the statistical test of Giannakis and Tsatsanis is adapted to the modulation recognition algorithm. However, this technique faces problem when the number of subcarriers is low and also lacks robustness to the model mismatch such as channel effect. Then, Li et al. applied a Gaussianity test based on empirical distribution function (EDF) to identify OFDM from SC modulations in [11]. These two algorithms are too complex and suffer from computational complexity. In [12], the authors have presented a moments-based algorithm in High Frequency (HF) “poor path” channel which uses the ratio of higher order moments to eliminate the multipath effect. Although the method is easy to implement and is able to take the effects of channel into consideration, the probability of detection decreases in low Signal-to-Noise Ratio (SNR). Wavelet transform is another approach in OFDM recognition which is implemented in [13,14]. Despite the fact that the correct identification rate of OFDM versus SC reaches 100% in multipath Rayleigh fading channel and when SNR=0dB, there is a limitation on symbol rate [14]. Zhu et al. have introduced energy distribution parameters as spectrum-based features for OFDM discrimination [15]. However, this method is not efficient and reliable at low SNRs. In [16], the distinction of ambiguity function (AF) images of different modulation signals leads to presentation of a novel algorithm. In this algorithm, low dimensional vector based on the principal component analysis (PCA) and invariant moment of AF images are submitted to Support Vector Machine (SVM) classifier as key features. This method suffers from computational complexity, although it achieved the desired results in negative SNRs. Ulovec utilizes the statistics of the signal at the output of the intermediate-frequency stage for recognizer in [17,18].Despite the fact that many algorithms have been suggested to distinguish OFDM from SC modulations but no reliable solution which can guarantee the optimal solution has been found. Consequently, this paper aims to introduce a new modulation recognition algorithm which benefits from supervised learning technique. Actually, this method makes use of labeled dataset for training the system. The training data points tend to form discrete clusters such that members in one cluster share high degree of likeness. On the other hand, the data in one cluster should be dissimilar to the data from other cluster. In this regard, centroids for each cluster are found based on our proposed modified K-means algorithm. The modification consists of four famous optimization algorithms named: Teaching-Learning-Based Optimization (TLBO), Particle Swarm Optimization (PSO), Ant Colony Optimization (ACO), and Gravitational Search Algorithm (GSA). It should be note that TLBO comprises two phases and consequently the system has five sub-modification methods. In the iterations, one of the sub-modification methods is chosen based on a calculated probability and the roulette wheel mechanism. Finally, any unlabeled data can be dedicated to its nearest cluster center. The key features are calculated based on the signal amplitude in presence of additive white Gaussian noise. In this paper, we introduce two different procedures to recognize input data modulation. In the first technique, the algorithm needs no prior information and estimation about the incoming radio signal except the training procedure. Instead, the second method utilizes a SNR estimator to complete the classification.The remaining of the paper can be categorized as follows. Section 2 describes extraction procedure of the key features. In Section 3, the K-means clustering approach and modified algorithm are introduced, respectively. Section 4 gives a step-wise description on the application of modified clustering method in modulation recognition. Section 5 depicts the adaptive K-means algorithm for adjustment of centers by input dataset. There would be a discussion on the complexity of our proposed method in Section 6. In Section 7, the simulation results illustrate the feasibility of the proposed method and some comparisons are performed. A summary and conclusion on suggested algorithm is presented in Section 8.The proposed algorithm can be separated into two distinct parts: (1) feature extraction part and (2) recognition subsystem. The first subsystem has an obligation to extract key features from received signal. In this section, it is desired to introduce the system model and feature extraction procedure. If we assume Sr(t) and Sm(t) as received and modulated signals, a detailed description of sampled radio-frequency signal can be expressed as below [17]:(1)Sr[k]=Sm[k]1Nf∑k=0Nf(Sm[k])2+n[k]k=0,…,Nf−1where Nfis the number of samples with sampling frequency fs, and Sm[k] stands for sampled modulated signal with central frequency fc. Due to the normalization of the Sm[k] to its power, the standard deviation of the useful signal is equal to 1. Other than that, an additive white Gaussian noise is considered to declare channel effect. If B displays the system bandwidth, n[k] can be expressed as a normal distributed vector with zero-mean and varianceσn2given by:(2)σn2=fs2B110SNR10where SNR is Signal-to-Noise Ratio. Two channels of quadrature and in-phase are considered and two signals are produced at the output of inter-mediate (IM) quadrature mixer with frequency of fIF, where IF refers to Inter-mediate Frequency.(3)rQ'[k]=cos2πfofskSr[k]k=0,…,Nf−1(4)rI'[k]=sin2πfofskSr[k]k=0,…,Nf−1where forepresents the local oscillator frequency (fo=fIF+fc). In order to prevent the presence of useless products of mixing, a Finite Impulse Response (FIR) filter is implemented with bandwidth equal to 2fIF[18]. After FIR filtering,rQ'kandrI'kare decimated by factor two and formedrQk'andrIk'. The amplitude of the received signal is calculated as follows:(5)SA[k']=(rQ[k'])2+(rI[k'])2k'=1,…,Nf/2Without loss of generality, we can assume Nfis an even integer number. Afterwards, the amplitude signal is normalized to its power.(6)SAn[k']=SA[k']stdSA[k']k'=1,…,Nf/2In the above expression, index n corresponds to normalization and std stands for standard deviation of the signal. The first key feature is assessed based on following formulation:(7)μ4=2Nf∑k'=1Nf/2(SAn[k'])4It is evident that μ4 shows the Kurtosis of the normalized amplitude signal. Actually, Kurtosis, as a high-order statistic, measures the deviation of a distribution from the normal distribution. As a consequence of normal distribution of the amplitude of OFDM, it can be beneficial to use Kurtosis as a key feature to distinguish MC modulations. Another key is the amplitude that can identify modulations from each other. So the amplitude feature is calculated as below:(8)A=2Nf∑k'=1Nf/2|SAn[k']|The second subsystem is responsible for recognizing the MC modulation from extracted features. This section gives a detail expression on the proposed clustering algorithm as recognizer subsystem. Actually, it is divided into two subsections. At first, we would have a short glance at K-means clustering approach. Then, the suggested modified algorithm is being reviewed. This modification is derived from four famous heuristic algorithms such as TLBO, ACO, PSO, and GSA which are described particularly. As mentioned before, TLBO has two phases and so the algorithm benefits from five different sub-modification methods.The problem of finding groups in inharmonious datasets by maximizing similarity criterion in groups is one of the vital tools in data mining, pattern recognition and machine learning [19]. K-means algorithm attempts to partition data points {x1,x2, …,xn} into K clusters by finding cluster centers {c1,c2, …,cK} such that minimizes an objective function. The data points and centers are vectors with the dimension of d. This paper utilizes Euclidean distance metric to specify the distance of any point from its cluster centroid within a cluster. The Euclidean distance between two vectorsx=(x1, x2, …, xd) andy=(y1, y2, …, yd) is calculated as below:(9)d(x,y)=∑i=1d(xi−yi)2For the sake of finding K best centers, the problem can be considered as an optimization problem such that it minimizes the Sum of Squared Euclidean (SSE) distance. This fitness function is assessed based on both the data points and the center points as follows:(10)f=∑i=1nMinxi−cl2|l=1,…,KIn the above description, ||.|| is Euclidian distance operator. The K-means algorithm can be described in the following steps:Step 1: Select randomly the K centers from the data points.Step 2: Allocate each point of dataset {xi| i=1,…,n} to its nearest centroid {cj| j=1,…,K} such that ||xi−cj||<||xi−cp||, p=1, …, K, andj≠pStep 3: Update the allocation of K centers based on a given algorithm.Step 4: Repeat steps 2 and 3 until the convergence criterion is satisfied.It is worth noting that, the given algorithm mentioned in step 3 is performed based on our proposed algorithm which is described in the next subsections.In this paper, we propose a new modification to mainly improve the global search effectiveness of the K-means. The proposed modified method benefits from five sub-modification methods. Note that each member of population is able to choose only one of the following sub-modification methods to update its position for the next iteration based on roulette wheel mechanism. In order to introduce our modified method, consider the population matrixYwith N rows and each row consists of K centers with random initial position. The population matrix can be described as follows:(11)Y=[y1,y2,…,yN]Tyi=[c1i,c2i,…,cKi]=yi1,yi2,…,yiKdi=1,2,…,Nwhereyildisplays the position of the ith member in the lth dimension. The sub-modification methods are described in the next subsections.Gravitational Search Algorithm (GSA) mimics the law of gravitational force between two bodies in the nature [20,21]. Although, the gravitational force is the weakest among four main interactions in the nature, it is inescapably in all over universe. As we know, the gravity is widespread and omnipresent and every particle attracts other particles. Thus, the GSA simulates an artificial world of masses which obey the Newton laws of motion and gravitation. Since we are talking about a minimization problem, the best b(k) and the worst w(k) solution in k-th iteration are defined as below:(12)b(k)=min{fj(k)}j=1,2,…,N,k=1,…,NI(13)w(k)=max{fj(k)}j=1,2,…,Nandk=1,…,NIwhere fj(k) denotes the fitness function for j-th point at k-th iteration based on (10) and NIis maximum number of iterations. The following formulations depict the amount of force between two members and the total force that acts on member i.(14)Fijl=GkMi(k)Mj(k)Rij+ε(yjl(k)−yil(k))(15)Fil=∑j∈kbest≠iαjFijlIn the above equations, Rijis Euclidian distance between i-th and j-th masses based on (9) and ɛ is a given small number. Besides, αjis a uniform random variable in interval [0,1]. Gkis gravitational constant in k-th iteration which is given by:(16)Gk=G0eτkNIwhere G0 and τ are considered to be 100 and 20, respectively.MikandMjkare inertia masses in k-th iteration. In this algorithm, the fitness evaluation determines the gravitational and inertia masses such that the better members are heavier and have higher attractions and slower movement. In iteration, the gravitational and inertia masses are updated in the following form:(17)mi(k)=fi(k)−w(k)b(k)−w(k)k=1,2,…,NI(18)Mi(k)=mi∑Ni=1mik=1,2,…,NIAt the end, the underneath explanations demonstrate i-th member's acceleration in k-th iteration, the velocity and position at iteration k+1, respectively.(19)ail(k)=FilMi(k)(20)vil(k+1)=ail(k)+βivil(k)(21)ym1,il(k+1)=yil(k)+vil(k+1)where βiis a uniform random variable in interval [0,1]. It is worth noting that the velocity for each member is initialized randomly in the first step and is calculated in following iterations based on above formulation.The Teaching-Learning-Based Optimization (TLBO) algorithm is a new evolutionary algorithm based on the influence of a teacher on students and trade-off information between students. It is expected that the teacher augments the mean of his or her class information level according to his or her experience. Moreover, pupils in a class enjoy the ability of exchanging their knowledge in order to progress efficiently. Thus, TLBO algorithm considers two teacher and learner phase to take the advantage of these two trends practically [22].a)Teacher is the most knowledgeable member (best solution) in the population. As mentioned before, in the teacher phase, the goal is to move the population mean vectoretoward the global best solution vectorg. The teacher phase can greatly help the algorithm to reach the global optimum much faster and much easier. The teacher phase is based on following expression:(22)ym2,il(k+1)=yil(k)+ρ1(gl−TFel(k))where TFis 1 or 2 randomly. ρ1 is a uniform random variable in interval [0,1]. gland elare global best position and population mean in dimension l, respectively.On the other hand, the learner phase desires to exchange the information of two members. In this regard, two new members are chosen from population randomly such that n1≠n2. In other words, two distinct members of population share their knowledge and contribute to the selected member in order generate new one. A new member is generated as below:(23)ym3,il(k+1)=yil(k)+ρ2(yn1l(k)−yn2l(k))In the above formulation, ρ2 is a uniform random variable in interval [0,1].Particle Swarm Optimization (PSO) algorithm is an evolutionary optimization algorithm which its concept is inspired by social behavior of birds flocking and fish schooling. In fact, it strives to simulate the group communication in the nature when such swarms share individual knowledge during flocking, migrating or hunting. The foundation of PSO is based on best position of the member and the best position in total neighboring. In other word, members of the swarm remember their best position and communicate good positions to each other [23]. Therefore, the searching procedure based on PSO can be described as follows:(24)ym4,il(k+1)=yil(k)+ωbα1pil−yil(k)+α2ωegl−yil(k)In the mentioned formulation,piis the best position that member i has experienced andgis the best position in the neighboring. ωeand ωbare fixed on 0.5 and 1.5, respectively. α1andα2 are uniform random variables in interval [0,1].Ant Colony Optimization (ACO) is taken from the idea of ant's behavior for finding shortest path from its nest to food source. In this method, one member of population gets help from another one to make a movement [23]. This expression can be formulated as below:(25)ym5,il(k+1)=yil(k)+βynl(k)−yil(k)where vectorynis chosen from population in a random manner and β is a uniform random variable in interval [0,1].It should be noted that some limitations are considered for position of members and their velocity in GSA based on minimum and maximum values of input data. After the generation of new members in each method, if the values exceed the boundaries, they would be replaced by the exact values on boundaries. Then, the effectiveness of new member should be assessed by fitness function and the old member should be replaced by the new one, if it showed a lower fitness value.The algorithm follows by initial definition of a probability variable (Prm=0.2 & m=1,…, 5) and an accumulator (Acm=0 & m=1,…,5) for each sub-modification method. By sorting the population according to the objective function, the best and the worst solutions would be distinguished asy1 andyN, respectively (N is the number of members in the population). Now, a weighting value will be calculated for each solution in a manner that better solution would gain higher weighting value.(26)ωi=Log(N−i+1)∑k=1NLog(k)i=1,…,NHereby, the accumulator and probability variables are updated as follows:(27)Acm=Acm+∑j=1NmωjNmm=1,…,5(28)Prm=(1−ε)Prm+εAcmkm=1,…,5where Nm;m=1, …, 5 is the number of masses which have chosen the m-th sub-modification strategy and variable ɛ is a balance constant which is preferred to be 0.25 empirically. The subsequent normalization of the values of Prmis performed to prevent the large values:(29)Prm=Prm∑m=15PrmIt is obvious that better fitness function value in the each iteration has certain outcome on higher probability (Prm) for each sub-modification method. Consequently, it would have more chance to be chosen in the next iteration. Moreover, the roulette wheel mechanism is employed here so that the random characteristics of the algorithm have been kept.This section presents the application of the novel hybrid evolutionary clustering algorithm on modulation recognition. Accordingly, following steps should be taken:Step 1: At first, the key features are extracted for all training signals (xn(t), n=1,…,Ns) and matrixXis organized as input data for clustering algorithm. In this matrix, each column depicts the related feature and each row denotes one of Nstraining data points.Step 2: The population (Y) and velocity (V) are initialized from input data matrix randomly.(30)Y=[y1,y2,…,yN]T(31)V=[v1,v2,…,vN]Twhereyi=[c1i,c2i,…,cKi]cji=[c'1j,c'2j,…,c'dj],cmin<c′i<cmaxvi=[vc1i,vc2i,…,vcki]vcji=[v1j,v2j,…,vdj],vmin<vi<vmaxAs it can be seen, each row of the population (yi) consists of K random clusters each of which its dimension is equal to d.cjidenotes the j-th cluster center for the i-th member or i-th row of the population. Since each member needs an initial velocity for GSA,viis considered to be the velocity of i-th member andvcjis the velocity of the j-th cluster center for the i-th member. As mentioned before, d is the dimension of each cluster center;vmaxandvminare the maximum and minimum values for velocity of each point belonging to the j-th cluster center, respectively. cmaxandcmin (each center feature) are the maximum and minimum values for each point belonging to the j-th cluster center, respectively. These boundaries are assessed based on minimum and maximum values of input data. This limitation can greatly help to curb the search process in the scope of input data.Step 3: The fitness function value given in (10) is calculated for each member of population. Minimum and maximum value of objective function is selected as best (b) and worst (w) solution.Step 4: Initial population is sorted based on objective function and the member which has minimum objective function is selected as global best position (g). Moreover, best local position of each member is opted forpi.Step 5: i-th member is chosen and next position for this member is assessed based on algorithm described in Section 3.2. As it is discussed before, each member chooses one of five techniques for updating its position in a random manner and by getting help from roulette wheel method. This process continues until all members are selected.Step 6: In this step termination criterion is checked. If the current iteration number reaches NI, the algorithm follows by the next step. Otherwise, the algorithm goes to step 3.Step 7: The lastgis determined as solution of the algorithm and centers for MC and SC modulations are obtained based on training data points.Step 8: We assume a series of testing signals (zn(t), n=1,…,Nt), extract the key features and form the testing dataset.Step 9: The Euclidian distance of each member in testing dataset from determined centers are calculated. Each testing data point is dedicated to the cluster with minimum distance from its center and modulation recognition is performed.The pseudo code of the suggested modulation recognition algorithm is shown in Fig. 1.In the discussed algorithm, fixed cluster centers have been calculated for all value of SNR. In other word, the cluster centers are calculated and set based on a predefined SNR value for training data points. While an adjustable algorithm is seemed to be needed in the cases that statistics of the problem slowly varies with time. Adaptive K-means clustering algorithm has been utilized in many artificial neural network structures to partition the input domain [24]. In order to improve the performance of the classification for time varying cases, this section has introduced an adaptive K-means technique which profits from adjusting of cluster centers based on the incoming data. In other word, an updating mechanism has been employed for centers. This method biases the clustering process toward an optimal partition by adjusting the learning rate efficiently.For input data points {x1,x2,…,xn}, cluster centers {c1,c2, …,cK}, and an underlying density probability p(x), the within-cluster variation can be expressed as:(32)Wj=∫Im(x)||xi−cj||2p(x)dxj=1,…,Kwhere m(x)=1 ifxbelongs to cluster j, and m(x)=0 otherwise. In [25], it is illustrated that for an optimal Voronoi partition, all regions have the same within-cluster variation. In order to achieve the optimality in clustering procedure, we should bias the winner indicator toward centers with smaller within-cluster variations. The biased winner indicator ωj,bias[t] is defined as bellow:(33)ωj,bias[t]=1ifWj[t](||x[t]−cj[t]||2)≤Wi[t](||x[t]−ci[t]||2)i≠jootherwisewherex[t] andcj[t] are t-th pattern presented to algorithm and j-th cluster center at that time, respectively. In fact, t is used to show the number of pattern presented to algorithm and to differ from the number of iteration in last section. The following description indicates the within-cluster center for each cluster.(34)Wj[t+1]=αWj[t]+(1−α)(ωj,bias[t])(||x[t]−cj[t]||2)In the above formulation, α is a constant less than 1. An efficient learning rate would be adapted based on the difference of the quality of the partition at that moment and that of a target partition. In this paper, the entropy of the normalized within-cluster variations is considered as quality of a partition as below:(35)H(W1,W2,…,WK)=∑j=1K−Wj,normln(Wj,norm)(36)Wj,norm=Wj∑i=1KWiAs it is mentioned before, the within-cluster variation of each region in a target partition must be equal. Thus, the quality of a final partition is equal to ln(K) when W1,norm=W2,norm=…=WK,norm=1/K. Consequently, the adaptive learning rate at time k is defined as follows:(37)η[t]=ln(K)−H(W1[t],W2[t],…,WK[t])ln(K)It is evident that the learning rate would be large when the partition is far from its target and it would reduce when the partition get closer to its destination. By making use of aforesaid definitions and incoming data points, the cluster centers are updated as below:(38)cj[t+1]=cj[t]−εm(x[t])η[t](cj[t]−x[t])As it is said before, m(x)=1 ifxbelongs to cluster j, and m(x)=0 otherwise. ɛ is constant.This section would catch a glimpse of the complexity of our proposed algorithm. It is desired to investigate the complexity of the approach from different points of view. It is clear that the recognition algorithm can be divided into two sections: (1) training section and (2) testing part.In the training part, the problem faces a cluster analysis. Since, sum of squared Euclidian distance, as the objective function, is a nonlinear function of the decision variables, the problem is nonlinear. In addition, nonlinear functions may be convex or non-convex. Convexity is a key issue for the problem function. A non-convex problem may have multiple feasible regions and multiple locally optimal points within each region. Due to nonlinear and non-convex characteristics of clustering problem, it is much harder to find a global optima solution and necessitates us to introduce a more complex algorithm which benefits from five different sub-modification methods. Each of these sub-modification techniques explores the solution space in a different manner to improve the search ability of algorithm effectively. For example, as it is discussed before, teacher phase in TLBO strives to move the population mean toward the global best solution or PSO and ACO methods give the members the opportunity to communicate their knowledge to reach a better solution. Moreover, the proposed method is a multimodal optimization which deals with finding all or most of the optimal solutions to the problem. In other word, the presented hybrid evolutionary algorithm is a population based approach which provides a population of possible solutions and they are processed in the iterations. Therefore, at the termination of the algorithm, we will obtain multiple good solutions.In spite the fact that the cluster analysis and finding centers of each cluster increase the complexity of algorithm but this process can be accomplished offline. It is better to say that computational complexity of this part is negligible. Actually, the main procedure of algorithm occurs in testing section which is an online process. As it is discussed before, this section includes parameter extraction, Euclidian distance computation, and a comparison for the first technique. In addition, for the second technique, an updating procedure is added. We observe in reference [18] that the algorithm calculates the value of parameters and then they are compared with a set of thresholds in order to make decision about the type of modulation. Obviously, in comparison with reference [18], only the calculation of Euclidian distances from two centers is added to the algorithm computation. It is evident from the previous explanations that the calculation of Euclidian distance has six mathematical operations. As a result, if we ignore the parameters extraction, our proposed algorithm has thirteen extra arithmetic operations which consist of two distance calculations and one comparison in order to determine the label (MC or SC) of the input data. Moreover, the updating procedure consists of 25 mathematical operations. Thus, it can be seen that the complexity of first part has no impact on the online recognition procedure and the suggested algorithm demonstrates superior performance. Table 1depicts the number of mathematical operations for different sections of our suggested algorithm.The proposed algorithm has been simulated in MATLAB R2015a environment. The probability of detection and probability of false alarm are considered as metrics to illustrate the effectiveness of the algorithm. In this paper, the probability of detection is defined as the percentage of realizations with correct recognition of MC modulation type. In addition, the probability of false alarm is the percentage of realizations with wrong recognition as MC modulation. The set of signals is {4FSK, 4PSK, 16QAM, OFDM}. For convenience, the carrier frequency fc, the frequency of IM signal fIF, and the number of samples Nfare fixed at 80kHz, 60kHz, and 20000, respectively. Also, the sampling frequency and the system bandwidth are considered to be 524288Hz and 120kHz individually as in [18]. 50 random realizations are generated for each class of modulations to train the system and the achieved centers are tested by 100 randomly produced realizations per class of modulations. In the following Table 2, the simulation parameters for algorithms and their values are summarized.The SNR value of training data is set at −10dB in the first technique. Fig. 2illustrates the reasons behind choosing this value for training SNR. In this regard, the SNR of testing dataset has got the values of −5, 0, and 5dB. The figure demonstrates the probability of detection Pdand the probability of false alarm Pfaversus SNR of training data. Dotted lines refer to Pfa, while the solid lines show the Pd. It is clear from the figure that although the probability of false alarm would decline by decreasing the training SNR, the detection probability would decrease as well. Among all, we observe that for training SNR=−10dB the Pdis higher than 0.8 and Pfais lower than 0.1 at worst. Note that the test SNR is −5dB, here. Therefore, −10dB is selected as the value of training SNR which leads to proper amount of Pdand Pfasimultaneously in comparison with other values of SNR. That is to say, if the system is trained with SNR=−10dB, it would have appropriate performance for higher SNRs in the test phase. Consequently, this issue can help the system to avoid updating centers by incoming testing data.A comparison between paper [18] and our proposed algorithm is performed in Fig. 2. As it is obvious, the suggested algorithm has better performance even at low SNRs. According to Fig. 3(a), by increasing the SNR, there is a significant decline in probability of detection for the proposed algorithm of reference [18] and this algorithm is unable to detect OFDM when SNR is lower than −1dB. However, our proposed method has indicated better ability to recognize OFDM modulation from single carrier ones with low SNR and the rate of detection is approximately 70% even when SNR=−10dB. As depicted in Fig. 3(b), similar results can be obtained for probability of false alarm. The Pfais null when SNR=0dB, while for reference [18], Pfais about 0.2 at this stage. Moreover, the gap between two methods is widened as SNR falls.The confusion matrix showed in Table 3, illustrates the ability of recognizer in distinguishing multicarrier modulation from single-carrier ones. As it can be seen, QAM modulation is more probable to be recognized as multicarrier modulation in comparison with two other single-carrier modulations, specifically at low SNR. As an example, for SNR=−10dB, 16QAM is mostly recognized as a multicarrier modulation (93 of 100 trials) which undesirably results in higher rate of Pfa. This issue demonstrates the demand for other techniques in order to diagnose QAM from OFDM at low SNR. However, introducing a method for distinguishing QAM from OFDM at low SNR is beyond the purpose of this paper and should be examined in another work.In order to show the tracing effect of adaptive K-means algorithm utilized in AMR, α is considered to be 0.9999,vj[0]is equal to 10−10 and ɛ=0.001. Although this consideration for α leads to slower movement ofvj, the estimation is more accurate. Besides, smaller amount ofvj[0]disappears the effects of initialization. For initialization of cluster centers, we have divided the SNR interval [−10,20] into six intervals with length of 5dB and calculated the cluster center of each interval by proposed algorithm. In this regard, the least value of each interval is considered as training SNR. Therefore, a SNR estimation is needed for the first block of input data andcj[0] is selected based on the estimated SNR. Then, the cluster centers are updated by testing dataset and based on the proposed algorithm. In this section, it is assumed that the SNR of input data varies with time slowly and linearly. Fig. 4compares the results of adaptive K-means with situation that no updating is made. It is clear that despite the fact that SNR varies, the presented adaptive K-means can trace the SNR variation of the channel efficiently. In other words, the probability of detection reaches about 0.75 and the probability of false alarm is approximately 0.25 when SNR=−10dB. In addition, this technique approaches the best performance i.e., Pfa=0 and Pd=1, for SNR=−2dB. Thus, the superiority of adaptive K-means is depicted and proved perfectly.

@&#CONCLUSIONS@&#
