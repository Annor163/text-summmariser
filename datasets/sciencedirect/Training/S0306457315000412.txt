@&#MAIN-TITLE@&#
An empirical evaluation of the User Engagement Scale (UES) in online news environments

@&#HIGHLIGHTS@&#
We examined the robustness of the User Engagement Scale (UES).Three studies were conducted in Canada and the United Kingdom.The UES sub-scales were reliable across three samples of online news browser.A four-factor structure, rather than six, may be more appropriate.The UES differentiated between online news sources and experimental conditions.

@&#KEYPHRASES@&#
Measurement,Experiential scales,User engagement,Information interaction,Online news,

@&#ABSTRACT@&#
Questionnaires are commonly used to measure attitudes toward systems and perceptions of search experiences. Whilst the face validity of such measures has been established through repeated use in information retrieval research, their reliability and wider validity are not typically examined; this threatens internal validity. The evaluation of self-report questionnaires is important not only for the internal validity of studies and, by extension, increased confidence in the results, but also for examining constructs of interest over time and across different domains and systems.In this paper, we look at a specific questionnaire, the User Engagement Scale (UES), for its robustness as a measure. We describe three empirical studies conducted in the online news domain and investigate the reliability and validity of the UES. Our results demonstrate good reliability of the UES sub-scales; however, we argue that a four-factor structure may be more appropriate than the original six-factor structure proposed in earlier work. In addition, we found evidence to suggest that the UES can differentiate between systems (in this case, online news sources) and experimental conditions (i.e., the type of media used to present online content).

@&#INTRODUCTION@&#
In information interaction research, there is growing recognition for information systems to satisfy both utilitarian and hedonic needs, particularly in the area of exploratory search where users’ motivations go beyond merely completing an information task (White & Roth, 2009). This has created a call to action for researchers to design systems that are fun and engaging and that foster learning and discovery (White & Roth, 2009), but also to systematically consider the measurement of such experiences. Kelly (2009) underscores the need to capture affective responses during “particular IR [information retrieval] interactions and states” (p. 200), while O’Brien and Lebow (2013) advocate developing robust measures of experience. There is an increasing impetus to support searchers to not only retrieve and evaluate information, but to engage with it. Further, we need “good measures” that capture searchers’ affective experiences and allow a degree of standardization to facilitate comparisons between different search systems, user groups, etc. (Käki & Aula, 2008, p. 86).Questionnaires are commonly used to measure attitudes toward systems and perceptions of search experiences. Kelly (2009) notes that such measures have good face validity, but are often developed “ad hoc” and their reliability and wider validity are not well established. This is problematic because, without reliable and valid measures, how can we be confident of our findings (Cairns, 2013)? Thus it is essential to evaluate the robustness of self-report measures to bolster internal validity.This paper looks to a particular questionnaire, the User Engagement Scale (UES) to evaluate its potential as a “good measure” for adequately assessing human information experiences. The UES is a 31-item self-report questionnaire developed in the e-shopping domain (O’Brien & Toms, 2010a). It has been subject to some evaluation in search systems (Arguello, Wu, Kelly, & Edwards, 2012; O’Brien & Toms, 2013), multimedia presentation software (O’Brien & Toms, 2010b), social networking applications (Banhawi & Mohamed Ali, 2011), and games (Wiebe, Lamb, Hardy, & Sharek, 2014) with mixed results regarding its generalizability in experimental settings, and the number of user engagement dimensions (UE) is captures. Given these mixed results, our motivating question was: “Is the UES a reliable and valid experiential scale to use in information interaction research, specifically online news browsing?” Here, we investigate its viability across three studies.User engagement (UE) encompasses users’ initial reactions to technologies (Sutcliffe, 2010), as well as sustained use of and re-engagement with information systems over time (Jacques, 1996; O’Brien & Toms, 2008). Engaging experiences involve system feedback and challenge, novelty, aesthetic and sensory appeal, interactivity, interest, choice, control, motivation, and positive affect (Jacques, 1996; Jacques, Preece, & Carey, 1995; O’Brien, 2008; O’Brien & Toms, 2008; Webster & Ho, 1997). UE has become increasingly important in recent years for evaluating search, social networking, and entertainment applications (see for example, Hong & Yang, 2013), though it has been the subject of some research for nearly two decades (Jacques, 1996; Jacques et al., 1995; Webster & Ho, 1997).Currently, behaviour-based metrics are being proposed as indicators of engagement (Lehmann, Lalmas, Yom-Tov, & Dupret, 2012). For example, search trail length and dwell time may signal continuous use or sustained engagement with a website (Singla & White, 2010). However, objective measures do not capture the subjective experiential aspects of information interaction, such as people’s motivations to use a system, or their emotional response to that usage. Indeed, the relationship between subjective and objective measures is not always clear. While Kelly, Fu, and Shah (2010) found that the number of documents retrieved was directly related to how favourable participants rated the IR system in three experiments; other studies have found weak or conflicting relationship between user perceptions and performance measures (Al-Maskari & Sanderson, 2011; O’Brien & Lebow, 2013).Self-report measures, which include interviews, focus groups, think aloud or think after protocols, questionnaires and experiential scales offer a means to capture the subjective aspects of information experiences, though are also not without their disadvantages. Kelly, Harper, and Landau (2008) provide an excellent summary of these (e.g. demand effects, acquiensence, etc.) based on their review of relevant psychology literature. In general, however, self-report measures allow people to express their knowledge, emotions or attitudes about an experience, are a staple of user-centered research (Kelly, 2009), and an important element of multiple method approaches (O’Brien & Lebow, 2013). Further, well-designed and empirically validated questionnaires and scales allow constructs of interest to be explored, defined and standardized (Fulmer & Frijters, 2009).In the case of a construct such as UE, a robust self-report measure would be useful for researchers, information purveyors and systems developers. It could be used in prototyping situations to provide designers with quick user feedback about an interface or search task, or in information retrieval studies to complement and corroborate behavioural or physiological measures, such as eye tracking. The ability to measure user engagement will provide a more holistic picture of people’s experiences with information systems. However, the measure must be broad enough to encompass the many dimensions of UE and capable of functioning in different interactive settings. In other words, it must demonstrate reliability, validity and generalizability.The User Engagement Scale (UES) is a self-report measure that builds upon earlier work in the area of educational multimedia (Jacques, 1996; Webster & Ho, 1997). Jacques’ (1996) questionnaire consisted of 14 items related to achievement, difficulty, attention, control, perceived time, motivation, boredom, patience, curiosity, and the desire to use the software again. Though rigorously developed, this questionnaire was never published, and as a result, its longitudinal reliability and validity were never addressed. Webster and Ho (1997) developed a questionnaire to measure “influences on engagement” (challenge, feedback, control, and variety) and “engagement” (attention focus, curiosity, and intrinsic interest) with multimedia presentations. They compared audience engagement with two types of presentation software and reported high internal consistency amongst influences on engagement and engagement items, and a unidimensional factor structure. Subsequent work confirmed the internal consistency or reliability of the items with a multimedia training systems (Chapman, Selvarajah, & Webster, 1999), but the make-up of the questionnaire (one or two items per attribute) is problematic for reliability (Webster & Ho, 1997).The more recent UES was motivated by the conclusion that engagement was a multi-dimensional construct, and that there was a need to re-examine its potential attributes and how these related to each other (O’Brien & Toms, 2008, 2010a). Through a systematic scale development process, over 400 items were compiled, evaluated, and pre-tested. An in-depth review process with an independent researcher reduced the number of items to over 100, and these items were administered to 440 online shoppers. Six distinct factors emerged through exploratory factor analysis (EFA): aesthetic appeal, perceived usability, felt involvement, novelty, focused attention and endurability. Results from this study informed the second large-scale evaluation of the instrument with 800 shoppers of a specific online retailer. Structural Equation Modelling (SEM) was used to confirm the factor structure and examine relationships amongst them (for more information about the process of developing and evaluating the UES, see O’Brien & Toms, 2010a). These two studies resulted in a 31-item instrument that encompassed system variables (aesthetic appeal, perceived usability), and users’ response to the online shopping interaction (felt involvement, focused attention, novelty) and its outcome (endurability).Since its development, the UES has been administered to users of multimedia presentation software (O’Brien & Toms, 2010b), search systems (Arguello et al., 2012; O’Brien & Toms, 2013), social networking applications (Banhawi & Mohamed Ali, 2011), and games (Wiebe et al., 2014). Collectively, these studies offer mixed results regarding the number of items and factor structure of the UES. The original UES consisted of 31 items and six distinct factors. However, subsequent studies have eliminated items in their initial analysis because they demonstrated poor contextual or system fit (Arguello et al., 2012). In addition, some researchers have reported a four- or five-factor structure (Banhawi & Mohamed Ali, 2011; O’Brien & Toms, 2010b, 2013; Wiebe et al., 2014). In the majority of these studies, perceived usability, aesthetics, and focused attention remain distinct factors, while items from the novelty, felt involvement, and endurability subscales merge to form one factor. Wiebe et al. (2014) labeled this fourth factor Satisfaction, but we do not agree with this terminology because “user satisfaction” has, as a construct, its own history in the information systems and IR literature. We feel this fourth factor taps into the more hedonic and pleasurable aspects of the experience, and could be labeled “hedonic engagement”. An exception to the four-factor structure was O’Brien and Toms’ (2010b) examination of the UES in an informational webcasts experiment; felt involvement items were eliminated in the preliminary analysis, but the remaining five factors were present and distinct.Despite issues with the factor structure, all of the studies confirm the multidimensional nature of UE and the internal consistency of the subscales (pre-factor analysis). Other work supports the concurrent validity of the UES with behavioural measures (Hyder, 2010) and other scales, including the Cognitive Absorption Scale and System Usability Scale (O’Brien & Lebow, 2013), and Flow State Scale (FSS) (Wiebe et al., 2014). Wiebe et al. (2014) found that the UES, in combination with the FSS, predicted video game performance as measured by the highest level of a game that players attained. However, the amount of predicted variance was low (11%), indicating that other variables (not accounted for by either scale) contributed to performance. Arguello et al. (2012) did not discern any differences in users’ interface preferences as measured by the UES in their investigation of aggregated search and task complexity; however, Levesque, Oram, MacLean, et al. (2011) successfully used items from the UES to compare haptic UE with widgets that provided different degrees of tactile feedback.In summary, the UES has had positive results regarding its reliability and certain types of validity; some types of validity remain to be explored. Research has cast doubt on the original six-factor structure. Wiebe et al. (2014) advocate using a modified four-factor scale to evaluate engagement with video games, a finding concurrent with O’Brien’s more recent work. We sought to make sense of previous findings by conducting further investigations in the online news domain to examine the reliability and validity of the UES in this context. Here we ask, “How generalizable is the UES to online news?”Online news was the domain selected for these studies because it is interactive and multimodal (Chung, 2008), is widely used by a cross-section of the population for information seeking and browsing (Marshall, 2007), and satisfies both purposeful and serendipitous information needs. Online news, with its relevance to everyday life, ubiquity, and the interactive and multimodal nature of news delivery – is a fitting environment for studying UE and the utilitarian and hedonic aspects of information interaction.We drew upon North American and United Kingdom news sources in the design of three quasi-experimental studies. Study 1 acted as a pilot to examine the meaningfulness of transferring the UES to the news domain with particular emphasis on scale reliability. Study 2 involved browsing online news websites in the context of a socially situated information scenario. Study 3 manipulated the mode in which participants interacted with news (text, audio, or video). Questions guiding this research were:1.To what extent does the UES yield internally consistent subscales and a six-factor structure (all studies)?Is the UES sensitive to differences between user engagement with different news websites based on people’s familiarity with the news sources (Study 2)?Does the UES detect differences between experimental conditions, such as the modality for interacting with online news (i.e., text, audio, video) (Study 3)?Research question #1 is focused on establishing the reliability of the UES for use in the online news environment. This is important, as we need to determine how the UES functions outside of the original e-commerce environment in which it was developed. Questions #2 and 3 looked specifically at construct validity. In IIR studies, we need to ascertain that self-report instruments can differentiate experiences with different interfaces and experimental conditions, should differences indeed exist.To reduce redundancy in other parts of the paper, we present our approach to data analysis used in all studies. First, the wording of the UES items was modified to fit the online news context. This was necessary because the original items were developed and tested in the online shopping domain.For each data set the following procedures and tests were used to examine the data. Descriptives and missing value analysis was first done on each item though, as this produced no concerns, it is not reported in detail here. Cronbach alpha (α) was used to examine the reliability of the subscales of the UES. Values between 0.7 and 0.9 were considered optimal (DeVellis, 2003). Where scales were considered to lack reliability, the Measure of Sampling Adequacy (MSA) of each item was used to indicate potentially problematic items. MSA below 0.6 is a commonly used threshold to indicate problematic items, though this is not wholly dependable for small samples (Tabachnick & Fidell, 2013). Pearson’s correlation coefficients were used to describe inter-scale correlates and identify possible overlap between the scales; this would suggest a factor structure requiring fewer than the original six factors.Where factor structure was considered, the Kaiser–Meyer–Olkin MSA and Bartlett’s test of sphericity were used to indicate that factor analysis would be suitable, though we note the issues of using this with small samples. The component structure of the UES was validated through replication of the original six-factor analysis on the new sample (Kline, 2000) using Principal Components Analysis (PCA) with oblique rotation (Direct Oblimin). The structure matrix was used for the analysis in each case. This decision was based on the significant relationships observed in the inter-item correlation matrix. The structure matrix is more difficult to interpret than the pattern matrix because it does not “partial out” the overlapping variance among factors but allows items to load significantly on multiple factors (Tabachnick & Fidell, 2013, p. 654). At the same time it is conceptually clearer as it reveals the correlations seen between the factors. The cut-off value for loadings was 0.35 based on our modest sample sizes (Kline, 2000).

@&#CONCLUSIONS@&#
