@&#MAIN-TITLE@&#
Multiple Empirical Kernel Learning with dynamic pairwise constraints

@&#HIGHLIGHTS@&#
Existing pairwise constraints (PC) seldomly consider PC dynamically.It might lead to a poor robustness in some cases.A dynamic PC (DPC) selection method is proposed.We firstly introduce (DPC) into the Multiple Empirical Kernel Learning.With the DPC, a superior classification performance is achieved.

@&#KEYPHRASES@&#
Multiple Kernel Learning,Dynamic pairwise constraints,Empirical Kernel Mapping,Prior knowledge,Pattern recognition,

@&#ABSTRACT@&#
Unlike the traditional Multiple Kernel Learning (MKL) with the implicit kernels, Multiple Empirical Kernel Learning (MEKL) explicitly maps the original data space into multiple feature spaces via different empirical kernels. MEKL has been demonstrated to bring good classification performance and to be much easier in processing and analyzing the adaptability of kernels for the input space. In this paper, we incorporate the dynamic pairwise constraints into MEKL to propose a novel Multiple Empirical Kernel Learning with dynamic Pairwise Constraints method (MEKLPC). It is known that the pairwise constraint provides the relationship between two samples, which tells whether these samples belong to the same class or not. In the present work, we boost the original pairwise constraints and design the dynamic pairwise constraints which can pay more attention onto the boundary samples and thus to make the decision hyperplane more reasonable and accurate. Thus, the proposed MEKLPC not only inherits the advantages of the MEKL, but also owns multiple folds of prior information. Firstly, MEKLPC gets the side-information and boosts the classification performance significantly in each feature space. Here, the side-information is the dynamic pairwise constraints which are constructed by the samples near the decision boundary, i.e. the boundary samples. Secondly, in each mapped feature space, MEKLPC still measures the empirical risk and generalization risk. Lastly, different feature spaces mapped by multiple empirical kernels can agree to their outputs for the same input sample as much as possible. To the best of our knowledge, it is the first time to introduce the dynamic pairwise constraints into the MEKL framework in the present work. The experiments on a number of real-world data sets demonstrate the feasibility and effectiveness of MEKLPC.

@&#INTRODUCTION@&#
Kernel-based learning method has been successfully applied [23,26,32,33]. It maps the input space into a feature space, i.e.Φ(x):x→F. There are two kinds of Φ(x) including implicit and explicit forms represented by Φi(x) and Φe(x), respectively. The implicit mapping Φi(x) called Implicit Kernel Mapping (IKM) [23] is achieved by a kernel function k(xi, xj)=Φi(xi)·Φi(xj) by which the explicit form of Φi(x) is not necessary to be given. In contrast, the Φe(x) called Empirical Kernel Mapping (EKM) [35] has to give the explicit form of Φe(x) to get the exact features of x in feature space. On the other hand, according to the number of kernels used in the learning process, kernel-based learning can be divided into Single Kernel Learning (SKL) [7] and Multiple Kernel Learning (MKL) [39]. SKL maps the input space into one feature space, while MKL maps it into multiple feature spaces through the corresponding mapping functions. Most of the existing MKL using the IKM is called the Multiple Implicit Kernel Learning (MIKL). In contrast, the MKL employing the EKM is called the Multiple Empirical Kernel Learning (MEKL).Although MIKL has got much attention [1,18,27,28] in recent decades, it is the necessity of inner-product in IKM that restricts other methods unsatisfying this formulation to be kernelized. For instance, it is pretty difficult to formulate the Kernel Direct Discriminant Analysis [22]. Moreover, for some linear discriminant analysis algorithms, such as the Uncorrelated Linear Discriminant Analysis [38], and the Orthogonal Linear Discriminant Analysis [37], to directly kernelize them via the kernel trick is impossible, since these algorithms need to compute the singular value decomposition [34]. Fortunately, most methods can be directly implemented in EKM due to the explicit representation of the corresponding feature vectors, which results in an easy way to extend the application of the kernel-based method. Wang et al. [32] have pointed out that the EKM is exactly equal to the IKM, and the mapped spaces generated by them have the same geometrical structure. In [25,35], it is shown that the EKM is much easier in processing and analyzing the adaptability of kernels for the input space than the IKM. Moreover, the MKL is more efficient in depicting heterogeneous data sources than the SKL. To a certain extent, MKL also relaxes the model selection about kernels. Thus in this paper, we focus on the MKL with EKM, i.e. MEKL. MEKL can be viewed as the data-dependent kernel learning model since Φe(x) is directly generated based on the input data. The existing MEKL is treated as an alternative way of kernel learning. It mainly introduces the existing techniques into EKM, and gives the illustration on the differences between EKM and IKM. However, few researches concentrate on the inherent characteristic of EKM. This paper gives an investigation onto the structure of the empirically generated feature spaces. The traditional MEKL problem is to optimize the learning framework by minimizing the empirical risk, and the regularization risk, as well as the loss term of the multiple feature spaces [32]. We can find that it ignores some information among the training samples, which may provide great contribution to the classification performance.This paper explores the relationship between samples in each feature space through the pairwise constraints. It is known that pairwise constraint provides the relationship between two samples, which tells whether they belong to the same class or not. In this paper, we boost the original pairwise constraints, which are considered to be static during the training process, by designing the dynamic pairwise constraints, which are dynamically constructed by the boundary samples, to result in the decision hyperplane more reasonable and accurate. Furthermore, we introduce the dynamic pairwise constraints into MEKL framework to propose a Multiple Empirical Kernel Learning with dynamic Pairwise Constraints method (MEKLPC). The proposed MEKLPC not only inherits the advantages of MEKL, but also owns multiple folds of prior information. Firstly, in each mapped feature space, MEKLPC gets the side-information to promote the classification performance. Here, the side-information is the dynamic pairwise constraints which are constructed by the samples near the decision boundary, i.e. the boundary samples. Secondly, in each mapped feature space, MEKLPC still measures the empirical risk and generalization risk. Lastly, different feature spaces can agree to their outputs for the same input sample as much as possible through a loss term of these multiple feature spaces. To the best of our knowledge, the present work is the first time to introduce the dynamic pairwise constraints into MEKL framework.In order to generate an instance of the proposed MEKLPC, we adopt our previous MEKL work named MultiK-MHKS [32] as the incorporated paradigm. In practice, MEKLPC firstly maps the input data into multiple feature spaces by the corresponding EKMs. Then, it introduces the dynamic pairwise constraints into each feature space to obtain the corresponding decision function. Finally, the final decision function is obtained by combining the decision functions of all feature spaces. To validate the feasibility and effectiveness of MEKLPC, the experiments on a number of real-world data sets are implemented, and demonstrate that MEKLPC provides a superior performance.The rest of this paper is organized as follows. Section 2 presents a brief review on the related work of the existing pairwise constraints. Section 3 demonstrates the designed dynamic pairwise constraints. Section 4 gives the detailed illustration on the proposed MEKLPC. The experimental results of MEKLPC on real-world data sets are reported in Section 5. Finally, the conclusions are represented in Section 6.

@&#CONCLUSIONS@&#
This paper introduces the pairwise constraints into the traditional MultiK-MHKS so as to consider the local information of training samples to result in a novel Multiple Empirical Kernel Learning with dynamic Pairwise Constraints method (MEKLPC) which can provide superior classification performance. Unlike the traditional methods which consider all the training samples as the pairwise constraint samples or pre-specify some pairwise constraints before training, we design a sample selection method to dynamically determine the pairwise constraints during the training process. With the dynamic pairwise constraints, we can pay more attention to the samples which have greater impact on the decision hyperplane. The experimental results indicate that the MEKLPC can result in superior performance on most of the used data sets. We also investigate the convergence of the MEKLPC and the impact of the parameter β and α on the performance of MEKLPC. The following conclusions are obtained: (i) the MEKLPC can converge in a limited iterations. (ii) β should not be too small or too large in case of the performance degeneration. (iii) Too many pairwise constraints might lead to poor classification performance.