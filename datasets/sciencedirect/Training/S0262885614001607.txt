@&#MAIN-TITLE@&#
Robust tracking with interest points: A sparse representation approach

@&#HIGHLIGHTS@&#
The proposed tracker combines the flexibility of interest points and robustness of sparse representation.Proposed a robust matching criteria for reliable tracking via L1 minimizationThe proposed tracker is computationally efficient and provides real-time performance.The tracker is benchmarked with many publicly available complex video sequences.Performance is compared with many recent state of the art trackers using 50 benchmark video dataset.

@&#KEYPHRASES@&#
Visual tracking,l,1 minimization,Interest points,Harris corner,Sparse representation,

@&#ABSTRACT@&#
Visual tracking is an important task in various computer vision applications including visual surveillance, human computer interaction, event detection, video indexing and retrieval. Recent state of the art sparse representation (SR) based trackers show better robustness than many of the other existing trackers. One of the issues with these SR trackers is low execution speed. The particle filter framework is one of the major aspects responsible for slow execution, and is common to most of the existing SR trackers. In this paper,11An earlier brief version of the paper has appeared in ICIP'13 (R. Venkatesh Babu and P. Priti, “Interest points based object tracking via sparse representation”, in proceedings of International Conference on Image Processing (ICIP), Melbourne, Australia, 2013).we propose a robust interest point based tracker in l1 minimization framework that runs at real-time with performance comparable to the state of the art trackers. In the proposed tracker, the target dictionary is obtained from the patches around target interest points. Next, the interest points from the candidate window of the current frame are obtained. The correspondence between target and candidate points is obtained via solving the proposed l1 minimization problem.In order to prune the noisy matches, a robust matching criterion is proposed, where only the reliable candidate points that mutually match with target and candidate dictionary elements are considered for tracking. The object is localized by measuring the displacement of these interest points. The reliable candidate patches are used for updating the target dictionary. The performance and accuracy of the proposed tracker is benchmarked with several complex video sequences. The tracker is found to be considerably fast as compared to the reported state of the art trackers. The proposed tracker is further evaluated for various local patch sizes, number of interest points and regularization parameters. The performance of the tracker for various challenges including illumination change, occlusion, and background clutter has been quantified with a benchmark dataset containing 50 videos.

@&#INTRODUCTION@&#
Visual tracking has been one of the key research areas in computer vision community for the past few decades. Tracking is a crucial module for video analysis, surveillance and monitoring, human behavior analysis, human computer interaction and video indexing/retrieval etc. Major challenges for tracking algorithms that arise in real life scenarios are due to both intrinsic and extrinsic factors. Intrinsic factors include pose, appearance and scale changes, and common extrinsic factors are illumination variation, occlusion and clutter.There have been several proposals for object tracking algorithms in the literature. Based on object modeling, a majority of the trackers can be brought under the following two themes: i) Global object model and ii) Local object model. In the global approach, the object is typically modeled using all the pixels corresponding to the object region or some global property of the object. The simple template based SSD (sum of the squared distance) tracker, color histogram based meanshift tracker [1] and probabilistic tracker [2] are examples of global trackers. Traditional Lucas–Kanade tracker [3], fragment based approaches [4,5] and many bag-of-words model based trackers are some examples of local object trackers.In global modeling, the features representing the global properties of the object are utilized for modeling. They could be simple template based models or histogram based models or shape based models. The template models carry appearance information of the object from a single view. These models are good for tracking objects whose appearances do not change much over time and not suitable for tracking objects undergoing significant appearance changes, which need frequent model updates. Since image intensity based template models are sensitive to illumination changes, image gradients have been used as a feature [6]. Template matching approach is computationally very expensive due to the brute force search. Efficient template matching methods have been proposed in the literature [7,8]. On the other hand, Comaniciu et al. [1] use the kernel weighted color histogram for modeling the object. Though the spatial information is lost in this model, it is suitable for applying iterative meanshift procedure. This meanshift tracker maximizes the similarity between the target and candidate models by iteratively seeking the mode of the underlying similarity space. Since the meanshift tracker performs gradient ascent over similarity space, it quickly converges to the mode in a couple of iterations and delivers real-time tracking performance. Unlike the template based object model, the histogram based object model provides better robustness to appearance change, since it captures the color configuration of the object rather than the spatial structure. These global object modeling approaches are sensitive to partial occlusion, illumination and scale changes since the model depends on the attributes of entire object region.On the other hand, local object model uses the information from object parts for tracking. Most of these trackers use bag-of-words model based approaches [9–12,5]. The proposed interest point based tracker falls under the local object model based tracking. Shi and Tomasi showed [13] that the corner-like points are more suitable for reliable tracking due to its stability and robustness to various distortions like rotation, scaling, and illumination. Though the interest-point based trackers show more robustness to various factors like rotation, scale and partial occlusion, the major issues surface from description and matching of interest points between the successive frames. For example, Kloihofer and Kampel [14] use SURF [15] descriptors of interest points as feature descriptors. The object is tracked by matching the object points of the previous frame with candidate points in the current frame. The displacement vectors of these points are utilized for localizing the object in the current frame [16]. A detailed survey on various object tracking methods can be found in [17–19]. Matching the features of interest points between two frames is a crucial step in estimating the correct motion of the object and typically Euclidean distance is used for matching. The recently proposed vision algorithms in sparse representation framework clearly illustrate its superior discriminative ability even with very low dimensional data [20,21]. This motivated us to examine the matching ability of sparse representation approach for interest points.In this paper, we have proposed a robust interest point based tracker in sparse representation framework. The interest points of the object are obtained from the initial frame by Harris corner detector [22] and a dictionary is constructed from the small image patch surrounding these corner points. The candidate corner points obtained from the search window of the current frame are matched with object points (dictionary) by sparsely representing the candidate corner patches in terms of dictionary patches. The correspondence between the target and candidate interest points is established via the maximum value of the sparse coefficients. A ‘robust matching’ criterion has been proposed for pruning the noisy matches by checking the mutual match between candidate and target patches. The displacement of these matched candidate points indicates the location of the object. Since the dictionary elements are obtained from a very small patch surrounding these corner points, the proposed approach is robust and computationally very efficient compared to the particle filter based l1 trackers [23–25].The rest of the paper is organized as follows: Section 2 briefly reviews related works. Section 3 explains the proposed tracker in sparse representation framework. Section 4 discusses the results and concluding remarks are given in Section 5.The concept of sparse representation recently attracted the computer vision community due to its discriminative nature [20]. Sparse representation has been applied to various computer vision tasks including face recognition [20], image video restoration [26], motion segmentation [27], image denoising [28], image compression [29], action recognition [30], super resolution [31], tracking [21] and background modeling [32].Wright et al. [20] exploited the discriminative nature of sparse representation for face recognition. In place of generic dictionaries, they have used the overcomplete dictionary with the training samples as its base elements. Given sufficient number of training samples for each class, the test sample can be sparsely represented using the training elements of the same class via l1 minimization. The same concept can be effectively used for object tracking, since the correct candidate can be sparsely represented using target dictionary.The l1 minimization tracker proposed by Mei and Ling [33] uses the low resolution target image along with trivial templates as dictionary elements. The candidate patches can be represented as a sparse linear combination of the dictionary elements. To localize the object in the future frames, the authors use the particle filter framework. Here, each particle is an image patch obtained from the spatial neighborhood of previous object center. The particle that minimizes the projection error indicates the location of object in the current frame. Typically, hundreds of particles are used for localizing the object. The performance of the tracker relies on the number of particles used. This tracker is computationally expensive and not suitable for real-time tracking. Faster version of the above work was proposed by Bao et al. [24], here the l2 norm regularization over trivial templates is added to the l1 minimization problem. However, our experiments show that the speed of the above algorithm is achieved at the cost of its tracking performance. Another faster version was proposed by Li and Shen [34]. Jia et al. [12] used structural local sparse appearance model with an alignment-pooling method for tracking the object. All the above approaches are again implemented in the particle filter framework and run at low frame rate, hence not suitable for real-time tracking. A real-time tracker proposed by Zhang et al. [35] uses a sparse measurement matrix to reduce the dimension of foreground and background samples and uses naive Bayes classifier for classifying object and background.Feature detection and matching has been an essential component of computer vision for various applications such as image stitching [36] and 3D model construction. The features used for such applications are typically obtained from specific locations of image known as interest points. The interest points (often called as corner points) indicate the salient locations of the image, which are least affected by various deformations and noise. The features extracted from these interest points are often used in various computer vision applications including visual tracking.Most of the recently developed sparse trackers model the object as templates [23] or local patches [12] without considering the importance of salient locations of the target. Since many densely sampled object patches contain least information, using them for tracking will potentially drift the tracker over time. Further, these non-salient patches will increase the computational cost many times without much benefit.In this paper, we propose a tracker that utilizes the flexibility of the interest points and robustness of sparse representation for matching these interest points across frames. The overview of the proposed tracker is illustrated in Fig. 1. The proposed tracker models the object as a set of interest points. The interest points are represented by a small image patch surrounding it. The vectorized image patches are l2 normalized to form the columns of the target dictionary (T). In order to localize the object in the current frame, the interest points are obtained from the predefined search window in the current frame. The vectorized image patches of candidate interest points form the candidate dictionary (Y). Each target interest point (object dictionary element) is represented as a sparse linear combination of the candidate dictionary atoms. The discriminative property of sparse representation enables matching between target and candidate patches. For each target patch, the candidate patch whose corresponding coefficient is maximum, is chosen as the match. The green patches shown in Fig. 1 indicate the matched candidate points with the target points (called as ‘1-way’ matched points). The noisy matches are pruned via the ‘robust matching’ criterion by checking the mutual match between candidate and target patches. The mutual correspondence is confirmed by sparsely representing the candidate interest points in terms of target interest points. Only the points that mutually agree with each other were considered for object localization. The red patches shown in Fig. 1 indicate the matched target points with the candidate points selected from the previous step (called as ‘2-way’ matched points). The object location in the current frame is estimated as the median displacement of the candidate interest points. Since the proposed tracker is a point tracker, it is robust to global appearance, scale and illumination changes. Further, the proposed tracker uses only a small patch around the interest points as dictionary elements, making it computationally very efficient.Let T={ti} be the set of target patches corresponding to the target interest points, represented as l2 normalized vectors. The candidate interest points, represented as the vectorized image patches yi, obtained within a search window of the current frame could be represented by a sparse linear combination of the target dictionary elements ti.(1)yi≈Tai=ai1t1+ai2t2+,…,+aiktkwhere, {t1,t2,…,tk} are the target bases and {ai1,ai2,…,aik} are the corresponding target-coefficient vectors. The coefficient aijindicates the affinity between the ith candidate and the jth target interest point based on the discriminative property of sparse representation.The coefficient vector ai={ai1,ai2,…aik} is obtained as a solution of(2)minai∈Rk∥ai∥1s.t.∥yi−Tai∥22≤λwhere, λ is the reconstruction error tolerance in mean square sense. Considering the effect of occlusion and noise, Eq. (1) can be written as,(3)yi=Tai+ϵ.The nonzero entries of the error vector ϵ indicate noisy or occluded pixels in the candidate patch yi. Following the strategy in [20], we use trivial templates I={[i1,…,in]∈Rn×n} to capture the noise and occlusion.(4)yi=Tai+Ici=ai1t1+,…,+aiktk+ci1i1,…,+cininwhere, each of the trivial templates {i1,…,in}, is a vector having nonzero entry only at one location and {ci1,…,cin} is the corresponding coefficient vector. A nonzero trivial coefficient indicates the reconstruction error at the corresponding pixel location, possibly due to noise or occlusion.The object is tracked by finding the correspondence between the target and candidate interest points in successive frames. Only the reliable candidate interest points that mutually match with each other are considered for estimating the object location. Let l be the number of interest points in the current frame within the search window (all points shown in Fig. 2(b)). Only k interest points out of l points that match with the dictionary atoms will be considered for object localization (all green+red points shown in Fig. 2(b)).In order to find the candidate points that match with the target points, each target interest point tjis represented as a sparse linear combination of candidate points (yi) and trivial bases.(5)tj=Y·bj+ϵ=bj1y1+,…,+bjlyl+cj1i1,…,+cjnin.Utilizing the discriminative property of sparse representation, the matching candidate point for the given target point tjis determined by the maximum value of the candidate dictionary coefficient (bj). Let the coefficient bjicorresponding to the candidate dictionary element yibe the maximum value. Then the matching candidate point for the target point tjis declared as yi.Mathematically,bji=maxrbjr,r∈12…l, indicates the match between jth target point and ith candidate point. All the matched candidate points corresponding to each target dictionary element are shown by green and red patches in Fig. 2(b). If two or more target points match with same candidate point, only the candidate patch with higher coefficient value will be considered. Hence, the number of candidate matching points is less than or equal to the number of target dictionary elements.Let k1 (k1≤k) be the number of matched candidate interest points (all red and green points shown in Fig. 2(b)). In order to prune the noisy matches, now each of these candidate interest points (yi) is represented as a sparse linear combination of target points (tj) and trivial bases as in Eq. (4). Letaij=maxrair,r∈1…kindicate the match between ith candidate point and jth target point. The ith candidate point is selected if:(6)maxrbjr=bji,r∈1…lmaxrajr=aji,r∈1…k.All the candidate points that agree with the above robust matching criterion were considered for object localization (all red points shown in Fig. 2(b)). Let k2 (k2≤k1) be the candidate points located at [xic,yic],i∈{1,…,k2} with respect to object window center (x,y). Let [xit,yit],i∈{1,…,k2} be the corresponding locations of target points with respect to the same object window center (xo,yo). Now the displacement of object is measured as median displacement of candidate points with respect to its matching target points. Let dxiand dyibe the displacement of ith candidate point measured as:(7)dxi=xic−xit,i∈12…k2dyi=yic−yit,i∈12…k2.The object location in the current frame is obtained using the median displacement of each coordinate.(8)xo=xo+mediandxi,i∈12…k2yo=yo+mediandyi,i∈12…k2Since the object appearance and illumination change over time, it is necessary to update the target dictionary for reliable tracking. We deploy a simple update strategy based on the confidence of matching. The top 10% of matched candidate points are added to the dictionary if the matched sparse coefficient is above certain threshold. The same number of unmatched points from the target dictionary are removed to accommodate these new candidate points.Algorithm 1Proposed Tracking

@&#CONCLUSIONS@&#
