@&#MAIN-TITLE@&#
Calibrating wavelet neural networks by distance orientation similarity fuzzy C-means for approximation problems

@&#HIGHLIGHTS@&#
We propose a novel fuzzy C-means algorithm.Its effectiveness is tested in optimizing wavelet neural networks.Performance comparison in function approximation is made.The proposed model shows higher generalization capability.

@&#KEYPHRASES@&#
Clustering,Distance similarity,Function approximation,Orientation similarity,Time series,Wavelet neural networks,

@&#ABSTRACT@&#
Improperly tuned wavelet neural network (WNN) has been shown to exhibit unsatisfactory generalization performance. In this study, the tuning is done by an improved fuzzy C-means algorithm, that utilizes a novel similarity measure. This similarity measure takes the orientation as well as the distance into account. The modified WNN was first applied to a benchmark problem. Performance assessments with other approaches were made subsequently. Next, the feasibility of the proposed WNN in forecasting the chaotic Mackey–Glass time series and a real world application problem, i.e., blood glucose level prediction, were studied. An assessment analysis demonstrated that this presented WNN was superior in terms of prediction accuracy.

@&#INTRODUCTION@&#
Artificial neural networks (ANNs) – computing technology inspired by biological neural networks, have been implemented vigorously in function approximation problems. The popularity of ANNs is due to their attention-grabbing characteristics of self-learning capability, massively parallel architecture, fault tolerance characteristics, and the ability to approximate complex nonlinear mapping directly from the input samples [1]. The combination of the wavelet decomposition with single hidden layer ANNs paved the way for an even more appealing network model, namely, the wavelet neural networks (WNNs). With sufficient training, WNNs can approximate any continuous function to any desired degree of accuracy [2]. Moreover, the constitution of the localized wavelet activation function in the hidden layer facilitates a more compact network topology. The utilization of the wavelet activation function also leads to faster learning ability, as compared to conventional multilayer perceptrons [3]. Even though the WNNs are continuously gaining prevalence [4,5], properly designing a WNN is a critical issue that needs to be addressed cautiously. Improperly modeling a WNN may jeopardize its generalization capability and predicting competence.Typically, construction of WNNs involves two phases: (i) structure initialization, i.e., specifying the number of hidden nodes, the initial parameters of wavelet activation functions and the initial weight vectors of the WNNs, and (ii) parameter optimization, i.e., solving the connection weight vectors between the hidden layer and the output layer of the WNNs. The focus of this study lies in the determination of the appropriate initial values of the translation parameters. This is crucial in order to achieve a better configuration with fewer hidden nodes, simpler structure and better accuracy. A proper initialization of the translation parameter will do a good job of reflecting the essential attributes of the input samples, which is vital to generate optimal interpolation between the input–output spaces.Various efforts have been directed to solve this concern, in which the initialization of the translation vectors using an explicit expression is one of the widely used approaches. A recursive mathematical formulation which considered the domain of the function, in conjunction with the stochastic gradient method, was applied by Zhang and Benveniste [2] and Oussar and Dreyfus [6] in WNNs initialization. These developed techniques have been highlighted in several WNNs studies [7–10]. However, using an explicit expression to define translation vectors is merely a rule of thumb. The explicit expression relies on the characteristics of the dataset, which are usually unknown beforehand and vary in different situations. Thus, a tailor-made equation for all cases is rare. Determining the WNNs translation vectors based on the clustering algorithm was another alternative that has been proposed. Wei et al. [11] used hierarchical clustering to determine a set of the best initial translation vectors based on the obtained tree structure. A Euclidean distance-based connectionist clustering method was presented by Lin [12], in which the cluster means of the input data were positioned as the translation vectors. Hwang et al. [13] employed the K-means clustering algorithm to find the optimal locations of the translation vectors. The obtained solutions were updated subsequently by using a dyadic selection scheme in order to eliminate unnecessary hidden nodes. However, unresolved issues remain with the aforementioned clustering algorithms. For instance, the Euclidean distance-based clustering approaches do not consider the non-spherical occupancy in the feature space [14]. For hierarchical clustering, intricacy exists in defining the initial values from the branches of the obtained tree structure [15]. Support vector machines (SVMs) [16] and genetic algorithm (GA) [17] were among the other attempts used in defining the appropriate initial values of the translation vectors. Although such improvements were noteworthy, drawbacks such as algorithmic complexity and time-consuming programming execution occurred during the implementation of the SVMs and GA [18,19].In this paper, a distance orientation similarity-based fuzzy C-means algorithm (DOS-FCM) – an alternative to existing translation parameter approaches, was developed. The theoretical rationality of using the DOS-FCM in WNNs initialization could be attributed to reasons such as: (i) The DOS-FCM is insensitive to the input data domain, as compared to the commonly used recursive mathematical formulation which takes the input data domain into consideration; (ii) the DOS-FCM does not require prior knowledge of the essential attributes residing in the input space; (iii) the Euclidean-based clustering approaches may fail in reflecting the essential attributes if the data are in non-spherical distribution, but the DOS-FCM works in such cases; (iv) the DOS-FCM involves both coarse-tuning and fine-tuning processes. In this regard, it is insensitive to the initial values of the cluster centers compared to the hierarchical clustering approach; and (v) the control parameters of the mutation rate, crossover rate, mutation percentage in population and the number of maximum generation in the GA need to be assigned judiciously as it may be prone to premature convergence. The DOS-FCM, however, has fewer control parameters to be addressed. The potential benefit of the DOS-FCM has been validated quantitatively and qualitatively for a wide range of artificial and real world data with various geometrical structures and overlapping characteristics. The details are omitted here for conciseness [20].The performance of the proposed WNNs with DOS-FCM was compared to WNNs with random and fuzzy C-mean (FCM) initialization methods. Simulations were carried out on a few artificial benchmark approximation functions and a real-world application problem.The paper is organized as follows. Section 2 presents a brief introduction of WNNs, followed by a description of the proposed DOS-FCM algorithm in Section 3. The details on how the DOS-FCM was integrated into the WNNs are described in Section 4. Section 5 presents the experimental simulations and comparisons of the proposed modified WNNs in approximating the nonlinear functions and real-world data followed by a discussion of the results obtained. Finally, Section 6 contains concluding remarks of this study.WNNs are three-layered feedforward neural networks with single input layer, hidden layer and output layer. A schematic diagram of a WNN architecture is illustrated in Fig. 1. The nomenclature and operations involved in each layer are presented as follows:Input Layer:The input layer receives input data x=[x1, x2, …, xn], where n is the number of input dimensions, i.e., the number of input nodes. Subsequently, the accepted input will be transmitted to the hidden layer.Hidden Layer:In this present work, Mexican Hat wavelet function is integrated as the activation function in the hidden nodes of this layer. The relationship between the wavelet function and the received input data is given as follows:(1)Φaij,bij(xi)=n−2xi−bijaij2exp−(xi−bij)aij22,fori=1,…,nandj=1,…,mwhere aijand bijare the dilation and translation parameters of the jth activation function of the ith input variable xi, respectively. The output of each node in this layer is expressed as the product of the jth multi-dimensional wavelet with n input dimensions of xi, which is represented as:(2)ψj(x)=∏i=1nΦaij,bij(xi)=∏i=1nn−2xi−bijaij2exp−(xi−bij)aij22.The products of the hidden layer are then propagated to the output layer.Output Layer:The kth output of the WNN will be the linear combination of the weighted sum of the hidden layer, which is defined in the form of:(3)yˆk=∑j=1mwjkψj=∑j=1mwjk∏i=1nn−2(xi−bijaij)2exp−(xi−bij)aij22,fork=1,2,…,Lwhere wjkrepresent the connecting weights between the jth hidden node and kth output node.As mentioned in the introduction, the typical learning process of a WNN involves two stages, i.e., structure initialization and parameter optimization. In this present work, the initial values of bijwill be chosen systematically with the proposed DOS-FCM approach, discussed in the next section.The basic idea of parameter optimization in a WNN is to adjust the weight vectors linking the hidden layer and output layer to map the underlying relationship between the input and output space. The network output weights are derived analytically by applying the pseudo-inverse of the hidden layer output matrix. This solves the weakness of the gradient-based learning algorithms where the networks parameters are adjusted step by step, which are usually slow in learning and get trapped in local minima easily [21].For brevity, Eq. (3) is written as Y=ΨW, where(4)Ψ=ψ(x1,a1,b1)ψ(x1,a2,b2)…ψ(x1,am,bm)ψ(x2,a2,b2)ψ(x2,a1,b1)…ψ(x2,am,bm)⋮⋮…⋮ψ(xn,a1,b1)ψ(xn,a2,b2)…ψ(xn,am,bm)n×m,denotes the hidden layer output matrix. In most cases, the number of input dimension n is less than the number of training samples m. This indicates that Ψ is a non-square matrix and the least-squares solution of output weight vector W such that Y=ΨW might not exist. However, assuming that Ψ has full rank, it has been shown that the smallest norm least-squares solution of output weight vector W can be obtained from [21]:(5)W=Ψ+Y,where Ψ+=ΨT(ΨΨT)−1 is the right pseudo-inverse of the hidden layer output matrix Ψ. It is worth mentioning that the right pseudo-inverse is used in this case since n<m.Clustering is the act of partitioning a set of observations into dissimilar clusters. The observations belonging to the same cluster will have a stronger degree of similarity than the observations in the others. FCM which integrates the concept of fuzzy partitioning is one of the most widely used clustering algorithms. Compared to crisp clustering, FCM assigns any given observation into more than one cluster. The degree of belongingness of each observation with respect to the particular cluster is graded by the membership function which is quantified by the Euclidean distance between the observation and the cluster centers. Nonetheless, utilization of the Euclidean distance as a measure of similarity in the FCM is unfavorable due to its drawback of not being able to detect the non-spherical shaped clusters.Instead of assigning each observation to a cluster based on the shortest distance measure, a novel similarity metric that considers both the distance similarity and orientation similarity of the observations with respect to the cluster centers is proposed. For convenience, the proposed novel similarity measure is denoted as DOS-Index, as it measures the distance similarity and orientation similarity of the observations in the feature space.Given a set of observations, pi, i=1,2,…,N, in order to determine the most similar observation pjfor observation piwith respect to a cluster center ckin terms of distance and orientation aspects, the proposed DOS-Index is defined as:(6)DOS-Index(pi,ck,pj)=max1≤j≤N,pj∈CkDSL(pi,ck,pj)+OSL(pi,ck,pj)2.The observation pjis said to be the most similar observation for observation pirelative to cluster center ck, if its DOS-Index has the highest value among all considered observations. As observed from this equation, the DOS-Index characterizes the sense of similarity between observation piand observation pjwith respect to a cluster center ckfrom two aspects, i.e., the distance similarity level (DSL) which takes the distance between pito ck, and distance between pjto ckinto account, and the orientation similarity level (OSL) which considers the orientation angle between the line segmentspick→andckpj→based on the scalar projection concept. The DSL and OSL are expressed as:(7)DSL(pi,ck,pj)=1.5−21+exp(−|di−dj|),wheredi=pick¯anddj=pjck¯, and(8)OSL(pi,ck,pj)=vi.vj2||v|i|||vj||wherevi=pick→andvj=ckpj→.The developed DOS-Index is subsequently integrated as the similarity measure in the conventional FCM algorithm. Presenting a set of observations pi, where i=1, …, N, the proposed DOS-FCM algorithm works as follows:Step 1:InitializationInitialize the membership function uijrandomly, where uij∈[0,1],∑i=1Kuij=1,∀j=1,…,Nand K is the number of cluster centers.Step 2:Coarse-tuningUse the conventional FCM algorithm to update the membership function.Step 3:Fine-tuning 1Find out the set Sbikof all possible similar observations pjfor each pi, such that OSL(pi, ck, pj)≥β(=0.47) and forDSL(pi, ck, pj), the interval 0≤|di−dj|<2 is held. The Sbikdenotes the collection of all the observations pjwhich satisfy the conditions OSL(pi, ck, pj)≥β and 0≤|di−dj|<2 for each pi, while ckis the kth cluster center.Step 4:Fine-tuning 2Compute Eq. (6) for each observation pi.If the value ofDOS-Index(pi,ck*)is the largest and the most similar observation pj, relative tock*belongs to Sbik, assign observation pito the cluster centerck*.Otherwise, assign observation pito the cluster centerck**with the shortest distance.Next, update the membership matrix uijbased on the criterion that ifDOS-Index(pi,ck*)>θ∈[0,1],uij=uij=1,ifj=kuij=0,ifj≠k.Otherwise, update the membership matrix uijbased on the criterion thatuij=1∑k=1cdjidki2/(m−1)where c is the number of cluster centers.Step 5:UpdatingUpdate the new cluster center for K clusters bycknew=∑i=1nujimpi∑i=1nujim,where n is the number of observations in cluster ckand pi∈ck.Step 6:ContinuationIf there are no pattern changes or when the iterations reach a predefined maximum value, then stop. Otherwise, go to Step 3.In Step 3, the threshold value β=0.47 is imposed on the OSL(pi, ck, pj) based on the assumption that the tolerance rate of the angle of orientation, θ cannot be greater than 20°, as suggested in [20]. As shown in Fig. 2, if this constraint is violated, the observation pjis considered as not having a sense of similarity in terms of orientation angle with the observation pi, with respect to the cluster center ck. Thus, from Eq. (8),β=vi.vj2||vi||||vj||=||vi||||vj||cosθ2||vi||||vj||=cos20°2=0.47Similarly, it is assumed that observation pjis no longer considered similar to observation piif their difference in distance is more than 2. Hence, the value of |di−dj| in DSL operator as shown in Eq. (7) is bounded by 0 and k (k=2 in this case) as used in Ref. [20], in order to reduce the total time complexity.In Step 6, pattern change is defined as the absolute difference between the objective functions of two consecutive iteration steps. In other words, minimizing the objective function is the concern. In the DOS-FCM algorithm, the objective function is specified as:(9)Objective function=∑i=1N∑j=1Kuijmdij2where uij∈[0, 1],∑j=1Kuij=1, m>1 whiledij2is the squared Euclidean distance between the ith fuzzy observation and the jth fuzzy cluster center [22].The efficiency of the DOS-Index as opposed to the Euclidean distance used in the conventional FCM is illustrated in Fig. 3. As shown in this figure, there are three observations p1=(8, 7.2), p2=(2, 9.1), p3=(12, 9.3) and two cluster centers c1=(5, 8) and c2=(9, 8). Based on the idea of the shortest Euclidean distance, the FCM assigns the observation p1 to cluster C2, sincep1c1¯=3.1048andp1c2¯=1.2806, which violate the visual inspection. On the other hand, by applying Eqs. (6)–(8):DSL(p1,c1,p2)=1.5−21+exp(−|d1−d2|)=0.4548OSL(p1,c1,p2)=v1.v22||v|1|||v2||=0.4979DOS-Index(p1,c1,p2)=DSL(p1,c1,p2)+OSL(p1,c1,p2)2=0.4764whileDSL(p1,c2,p3)=1.5−21+exp(−|d1−d3|)=−0.2593OSL(p1,c2,p3)=v1.v32||v|1|||v3||=0.4827DOS-Index(p1,c2,p3)=DSL(p1,c2,p3)+OSL(p1,c2,p3)2=0.1116From the calculation of DOS-Index, DOS-Index(p1, c1, p2)>DOS-Index(p1, c2, p3), hence, the DOS-FCM assigns observation p1 to cluster C1, which is in agreement with the visual examination.The effectiveness of the DOS-FCM in comparison to the FCM will be evaluated quantitatively and qualitatively in the following two examples. The performances are assessed using two criteria.In order to assess the effectiveness of the DOS-FCM with respect to accurate partitioning, the Minkowski (MS) score [23] is adopted for quantitative evaluation purpose. The MS score is defined as:(10)MS(T,C)=||T−C||||T||where||T||=∑i∑jTij. Tn×ndefines the reference solution, whereas Cn×nis the resulting solution that we wish to compare to T (n is the number of observations). The element of Cijor Tijis 1, if i and j belong to the same cluster, and 0 if otherwise. A zero MS score indicates that a perfect clustering result is achieved. In addition to the MS score, the computation time taken by the algorithm in order to meet the stopping criteria is recorded to assess the efficiency of the clustering model. For each artificial dataset, the simulations of the FCM and DOS-FCM are executed for 10 trials. The obtained results are averaged and summarized in Table 1.In the first simulated artificial dataset (Fig. 4(a)), there are six spherical-shaped clusters with 50 observations each. Fig. 4(b) and (c) presents the partitioning results achieved by the FCM and the DOS-FCM, respectively. As shown in Fig. 4(b), it is not surprising that the FCM is able to provide accurate clustering result since the dataset under consideration consists of mainly hyper-spherical shaped clusters. A perfect partition is attained by the DOS-FCM as well, which can be observed in Fig. 4(c) and indicated by the MS score in Table 1. It can be concluded that the proposed DOS-FCM accounts for the existence of spherical-shaped clusters as the standard FCM.One compact circle and two overlapping stripes with a total number of 340 observations are generated in the second artificial dataset (Fig. 5(a)). As shown in Fig. 5(b), the FCM algorithm is not able to give satisfactory partitioning results, albeit the compact circle is internally hyper-spherical in nature. This suggests the infeasibility of the FCM when the existing clusters are different in shape and intertwined, which are common in practice. Apparently, the DOS-FCM delivers a better partitioning result, which can be observed in Fig. 5(c). Moreover, Table 1 demonstrates that a lower MS score is attained by the proposed DOS-FCM. Despite such improvement, however, the DOS-FCM takes longer computation time. The time complexity of the FCM is O(ifcmNdK), where i is the number of required iterations, N is the number of observations, d is the dimension of the observations and K is the number of cluster centers. However, the time complexity of the DOS-FCM is O(ifcmNdK)+O(idos-fcmN2d2K), where the latter is attributed to the calculation of the DOS-Index for each observation piwith respect to each cluster center ckat each iteration during the fine-tuning process. An acceleration strategy is would be required.From the simulation of the artificial dataset, it can be summarized that:(i)Both FCM and DOS-FCM provide accurate partition for the first dataset, as indicated from the MS score of 0 in all trials.The DOS-FCM performs better than the FCM, when the existing clusters possess different characteristics.The DOS-FCM remedies the deficiency of the FCM that does not consider the non-spherical occupancy in the feature space.The DOS-FCM has a longer computation time than the FCM.Presenting the training samples {(xi, yi), i=1, …, N}, the DOS-FCM performs the partitioning, in which the obtained cluster centers are then positioned as the initial translation vectors of the WNNs.The implementation of the DOS-FCM in selecting the optimal translation vectors is summarized as follows:Step 1: A set of training samples {(xi, yi), i=1, …, N}is presented to the WNNs.Step 2: Apply the DOS-FCM as explained in Section 3 to form a set of optimal cluster centers of the training samples, in which the cluster centers are then assigned as the initial translation vectors for WNNs.Step 3: Construct the WNNs as shown in Equation (3).Step 4: Update the weight vectors adaptively by using the pseudo-inverse method in order to minimize the criterion function. Stop the learning process when the stopping criteria are met.Step 5: Validate the forecasting abilities of the obtained WNNs network structure by using a set of testing samples.The workflow of the integration of the DOS-FCM with the learning process of the WNNs is further summarized in Fig. 6.In the previous section, the superiority of the DOS-FCM algorithm has been evaluated qualitatively and quantitatively by clustering the artificial dataset. The competence of the DOS-FCM as an efficient approach in the parameter initialization of WNNs will be explored in this section, where simulations on a few benchmark approximations and a real-world problem were carried out. All the simulations were performed using Matlab R2010a. WNNs were trained using the pseudo-inverse learning algorithm. The number of input nodes was determined by the input dimension, and the number of hidden nodes was determined by the number of training samples (for random initialization approach) or number of cluster centers (for FCM and DOS-FCM initialization approaches). The weight vectors were initialized randomly, and the error goal was set to 0.01 as an early stopping criterion. In addition, in order to validate the statistical significance of the differences in the performance indexes obtained by the WNNs across various initialization methods, Wilcoxon signed rank test was utilized.A benchmark piecewise nonlinear function which was referred to by Zhang and Benveniste was described as [2]:(11)f(x)=−2.186x−12.864,−10≤x<−24.264x,−2≤x<010exp(−0.05x−0.5)sin[(0.03x+0.7)x],0≤x≤10was used to study the predictive competence of the WNNs with the proposed DOS-FCM as initialization approach. 300 training data and 150 testing data were randomly generated by a uniform distribution on the definition domain. The performance index, ERR, was utilized to assess all models. It is given as:(12)ERR=∑j=1n[f(xj)−yj]2∑j=1n[yj−y¯]2where f(xj) and yjwere the actual and predicted value at point j, respectively.y¯was defined as:(13)y¯=1n∑j=1nyjwhere n was the number of testing data.All WNNs were able to approximate the piecewise function almost perfectly since the obtained ERR values were relatively small, i.e., 3.1596e−005 for WNNs with random initialization approach, 1.3625e−005 for WNNs with FCM initialization approach, and 3.2005e−007 for WNNs with DOS-FCM initialization approach. A net improvement was noticeable when clustering algorithms were adopted, as compared to the conventional WNNs that selected the initial parameters randomly. In addition, it was apparent that generalization capability of the WNNs increased the most when the WNNs were trained in conjunction with the proposed DOS-FCM algorithm. Remarkable prediction accuracy improvements of 98.99% and 97.65% were attained, respectively, as opposed to the WNNs with random initialization and FCM approach. It can be concluded that the interpolation capability of the WNNs could benefit from the proposed DOS-FCM algorithm since more effective hidden nodes were used to construct the WNNs model. In addition, statistical analysis based on the Wilcoxon test validated the statistical significance of the differences in the performance scores (p=8.3898e−07 and p=3.1518e−22, for random and FCM initialization approaches, respectively) at the confidence level of 5%. The approximation of the benchmark piecewise function based on the WNNs with DOS-FCM method was depicted in Fig. 7. It is clear that the obtained model was able to capture the manners of the chosen function well, even though the behavior of linearity, nonlinearity and an abrupt sharp spike were coexisting in this function.Table 2presents a comparison of performances of the proposed WNNs with DOS-FCM and other wavelet-based models in literature. It can be deduced that the proposed WNNs offered more satisfactory results than the rest. The lowest prediction error achieved by others was 0.0006 (in terms of ERR), where the result of 3.2005e−07 (in terms of ERR) obtained by the proposed WNNs surpassed that respective performance significantly. It is of interest to note that most of the listed models were focused on employing an enhanced learning algorithm or integrating the fuzzy rules with WNNs while using the recursive mathematical formulation [2] or the concept of multiresolution analysis [24–29] for networks initialization. Tzeng assigned the initial translation vectors based on GA [17], whilst Yao et al. integrated a self-tuning process with evolutionary programming to select the translation vectors [30]. It can thus be inferred that the quality of approximation for WNNs was much improved by integrating the DOS-FCM initialization approach into WNNs learning strategy, though neither enhancement of learning algorithm nor the incorporation of fuzzy rules was taken into account in this present study.The Mackey–Glass equation was originally developed by Mackey and Glass [34] to simulate blood cell regulation, and was subsequently used extensively as a benchmark time series to evaluate the performance of ANNs. The Mackey–Glass equation was given by:(14)dx(t)d(t)=ax(t−τ)1+xr(t−τ)−bx(t),t>0.The delay parameter, τ, characterizes the behavior of time series generated by this first-order differential-delay equation. It exhibits chaotic characteristics when τ≥17 [35].In this study, the parameters in Eq. (13) are specified as a=0.2, b=0.1, r=10, τ=17, and x(0)=1.2, respectively, according to the published works [36–38]. The fourth-order Runge Kutta method was applied to obtain the numerical solution of this differential equation. The four input variables, x(t–18), x(t–12), x(t–6) and x(t) were used to forecast the single output variable, x(t+6). A set of input–output data pairs with a length of 1000 were extracted, where the first 500 values served as training data and the remaining as testing data. To evaluate the competence of the proposed WNNs in forecasting the future value for the Mackey–Glass time series, mean squared error (MSE), root mean squared error (RMSE) and normalized mean squared error (NMSE) were utilized, which were defined as:(15)MSE=1n∑j=1n(f(xj)−yj)2(16)RMSE=1n∑j=1n(f(xj)−yj)2(17)NMSE=∑j=1n(f(xj)−yj)2∑j=1n(f(xj)−y¯)2where f (xj), yjandy¯represent the actual value at point j, predicted value at point j, and average of the actual values respectively, and n was the number of testing data.The approximation results for WNNs with different initialization approaches were summarized in Table 3. It can be observed from this table that the network mapping constructed by all WNNs fitted the targeted output adequately, since the approximation errors were within the range of 8.2786e−005 to 7.8794e−009 (in terms of MSE). The conventional WNNs delivered poorer results which were mainly attributed to the initial randomness of the parameters configuration. The initial random selection of translation vectors might overlook the contributable features in the input space that play an important role in refining an optimal architecture for WNNs.As predicted, the integration of the clustering algorithms in the learning phase of WNNs was accompanied by an improvement in the network generalization capability, which was reflected by a noticeable drop in the performance index. Almost 100% in reduction was attained compared to the conventional WNNs. As presented in Table 3, the employment of the proposed DOS-FCM in WNNs reached an extremely small value of MSE, i.e., a significant improvement of 99.99% from 8.2786e−005 (random initialization) and 97.62% from 3.3161e−007 (FCM) to 7.8794e−009 (DOS-FCM) were measured. The predictive performance of the WNNs with DOS-FCM was statistically different at the confidence level of 5%, as opposed to the random initialization (p=1.3431e−83) and FCM initialization approaches (p=3.9841e−77). Fig. 8(a) displays the forecasted Mackey–Glass time series of WNNs with DOS-FCM. It was evident that the chaotic and the oscillating behavior of the time series under investigation was successfully captured, which was further manifested by the small prediction errors in Fig. 8(b). The goodness-of-fit between the forecasted values and the generated time series values was portrayed in Fig. 8(c), from which it can be concluded that a practically precise forecasting was achieved, since all the data points were located on the diagonal line. The performance assessment of the proposed WNNs and other methods reported in literature related to the Mackey–Glass time series under the same criteria can be found in Table 4, from which the superiority and feasibility of the proposed method can be validated.Abrupt elevation in blood glucose level for diabetics is life-threatening as it may lead to dehydration, nausea, fatigue and eventually coma. Hence, monitoring of the blood glucose concentration needs to be done judiciously so that patients with diabetes can adjust the dose of insulin injection accordingly. In this section, the practicability of the proposed WNNs in extracting the complex interaction of blood glucose metabolism will be explored.The data used in this study was provided by Kok [50], in which it covered a 77-day study period. Eight fixed measurement points were assigned to night, before breakfast, after breakfast, before lunch, after lunch, before dinner, after dinner, and before sleeping. Based on these measurement points, four different intervals (morning, afternoon, evening and night) with three fixed points in each were formed. For example, the morning interval consisted of before breakfast, after breakfast and before lunch. At each data recording, the patient was required to fill up information such as time of blood glucose test, blood glucose concentration, dosage of short acting insulin injection, dosage of long acting insulin injection, food intake, stress level and exercise. The 19 input variables as listed in Table 5were selected. Principal component analysis was applied to these variables subsequently in order to extract the characteristic features that signify the predominant trend in the data. By using the condition of percentage of variance, 3 principal components were chosen as the input features for WNNs in predicting the blood glucose level for morning interval. A RMSE criterion was used as performance evaluation. Further details of the material and methodology were omitted for brevity [38].The experimental results for the WNNs in predicting the blood glucose level for diabetics at the end of the morning interval were presented in Table 6. These RMSEs were rather small (within the range of 0.6465–0.0360), which indicated a promising performance of all WNNs. In terms of the initialization approach of WNNs, the obtained results demonstrated that employing clustering algorithms to determine the number and location of translation vectors would lead to a decrease in RMSE. Occurring in a similar trend as previous sections, when classical FCM was compared against the random initialization approach, the former reduced the prediction error considerably, i.e., an improvement of 55.30% was attained. Using the proposed DOS-FCM algorithm in conjunction with WNNs led to the smallest value of RMSE wherein, it surpassed the WNNs with random initialization and FCM by a percentage improvement of 94.43% and 87.54%, respectively. The results were statistically significant at a confidence level of 5% (p=3.6061e−14 and p=3.9410e−09, for random and FCM initialization approaches, respectively). The forecasted blood glucose level at the end of the morning interval by WNNs with DOS-FCM algorithm was plotted in Fig. 9. It was observed that a relatively good prediction was obtained. WNNs with the proposed DOS-FCM algorithm were able to detect the chaotic characteristic in the blood glucose level time series, even though it was highly irregular and had a number of big leaps throughout the time series.Considering the same input variables as listed in Table 5, the forecasting ability of the proposed WNNs was evaluated and compared with the obtained prediction accuracies by Kok [50], radial basis function networks (RBFNs) [51], probabilistic neural networks (PNNs) and recurrent neural networks (RNNs), as shown in Table 7. Instead of applying the principal component analysis and WNNs, a combination of input variables selection by heuristic approach and predictor based on multilayer perceptrons was developed by Kok [50]. For the blood glucose predictor based on RBFNs used in [51], instead of using random input selection, a pruning method was applied. The input variables of (1, 2, 5, 6, 7, 10, 15, 17, 18) (Table 5) were selected after the elimination process. The blood glucose prediction using the PNNs and RNNs were performed too, in which the same principal components chosen for WNNs were used as the input features.The dismal performance of Kok's approach can be observed in Table 7, where RMSE of 1.8 was achieved. The unsatisfactory result was probably attributed to the input selection by trial and error, since random input selection might miss the complex interactions between the governing factors for blood glucose metabolism. Moreover, the nature of multilayer perceptrons with gradient-based learning algorithm which tends to get trapped in local minima may jeopardize its performance in a number of ways. On the other hand, WNNs were able to deal with this difficulty by adopting the pseudo-inverse algorithm in its learning strategy. The performance of the proposed WNNs was found to be more superior when its predicting accuracy was compared with the performance of RBFNs, PNNs and RNNs, as shown in Table 7. Its superiority could be due to essential attributes in the input space reflected through the selected optimal translation vectors by the DOS-FCM. The WNNs began the learning process from good initial points hence improving the generalization capability of the WNNs.

@&#CONCLUSIONS@&#
Initialization of the WNNs parameters needs to be handled precisely as the approximation solution may be drastically affected if improper initialization occurs. The pursuit of this goal was continued in this paper, where a novel algorithm, i.e., the DOS-FCM was proposed and adopted in the learning phase of WNNs. The usefulness of the DOS-FCM in searching for the appropriate features was first shown using two artificial datasets. Simulation demonstrated that the DOS-FCM outperformed the FCM in partitioning the observations into appropriate clusters, regardless of the geometric structures and overlapping characteristics.Subsequently, its feasibility in the initialization of the WNNs parameters was evaluated and validated in the context of function approximation: (i) approximation of a benchmark piecewise function; (ii) approximation of a benchmark Mackey–Glass chaotic time series; and (iii) prediction of blood glucose concentration of diabetics in a real-world application problem. Simulation studies showed that the DOS-FCM possessed good selective ability. The improvements of 87% to 100% in terms of the performance index were observed in the considered cases compared to the WNNs with random and FCM initialization approaches. The DOS-FCM initialization method was able to select a set of best initial translation vectors from the input space, which eventually enhanced the validity of modeling of the WNNs. The WNNs with the DOS-FCM initialization approach also consistently outperformed the models reported in the literature.Possible future work will be the utilization of the DOS-FCM in selecting the appropriate weight vectors during the learning process of the WNNs. Apart from that, it could also be interesting to investigate the application of the DOS-FCM in clustering the stationary and non-stationary time series data [52–55] and anomalous data [56].