@&#MAIN-TITLE@&#
A semantic approach to data translation: A case study of environmental observations data

@&#HIGHLIGHTS@&#
We propose an ontology-mediated approach for environmental data translation.We outline the principles underlying the design of a mediating ontology, and show the development of such an ontology.We propose a declarative formalism for representing spreadsheet-to-ontology mappings, and give an algorithm for the mapping evaluation.We propose a declarative formalism for representing ontology-to-XML mappings, and give an algorithm for the mapping evaluation.We have developed an ontology-mediated spreadsheet-to-XML translation tool, and showed its effectiveness with real environmental observations data.

@&#KEYPHRASES@&#
Data translation,Declarative mapping,Ontology,Spreadsheet,XML-based exchange language,

@&#ABSTRACT@&#
To facilitate the exchange of environmental observations, efforts have been made to develop standardised markup languages for describing and transmitting data from multiple sources. Along with this is often a need to translate data from different formats or vocabularies to these languages. In this paper, we focus on the problem of translating data encoded in spreadsheets to an XML-based standardised exchange language. We describe the issues with data that have to be resolved. We present a solution that relies on an ontology capturing semantic gaps between data and the target language. We show how to develop such an ontology and use it to mediate translation through a real scenario where water resources data have to be translated to a standard data transfer format. In particular, we provide declarative mapping formalisms for representing relationships between spreadsheets, ontologies, and XML schemas, and give algorithms for processing mappings. We have implemented our approach in AdHoc, an ontology-mediated spreadsheet-to-XML translation tool, and showed its effectiveness with real environmental observations data.

@&#INTRODUCTION@&#
Recent years have seen a proliferation of availability of environmental observations data as a result of the improvement of sensor technologies, the growing number, size, and complexity of environmental monitoring programs, and the realisation of the importance of using observations to characterise the environment as well as to describe it with models and simulations [1]. The successful use of these data to achieve new scientific breakthroughs, as well as in making well-informed resource management decisions, depends to a large extent on the ability to access, integrate and analyse these data [2].Recognising this, there have been efforts to develop standardised markup languages for the exchange of environmental observations data. Examples include Water Data Transfer Format (WDTF) [3], Water Markup Language (WaterML) [4], Ecological Metadata Language (EML) [5], and Earth Science Markup Language (ESML) [6]. These languages provide a structured syntax for communicating data from multiple sources as eXtensible Markup Language (XML) documents. With each language, a set of specifications may be provided,1In this paper, we consider a language and its specifications as a whole unless explicitly differentiated.1which describes additional requirements on data. Together, they define the information required, e.g. the location at which the observation was made, or the property that was observed; the constraints to be satisfied, e.g. “each observation has exactly one observed property”, or “a water level must be measured in metres”; or the vocabulary terms to be used, e.g. “streamflow” instead of “flow” or “discharge”.On the other hand, there is no standardisation in the methods of data storage or management, and each data source can have its own methods for storing and managing its data. This gives rise to data with different formats or vocabularies. To access and use such data, there is often a need to translate them to a standardised exchange language (e.g. one of those mentioned above). For example, in Australia, in response to the increasing demand for improving the efficiency of water management practices, the Bureau of Meteorology (BoM) has been given a mandate to build and maintain an integrated national water information system, which involves collecting water resources data from over 200 organisations [7,8]. As the organisations involved use various software systems with many different data formats, WDTF was developed for transfer and ingestion of data into the national system. As a result, data from organisations have to be translated to WDTF; also, the translated data have to conform to a set of constraints and vocabularies [9].The effort involved in data translation such as this2There are similar cases in other domains as well. As XML becomes a common standard for data exchange, legacy data are often required to be placed into a predefined XML schema (defined, e.g. by a standards committee to permit meaningful exchange within a specific domain) [10].2could be considerable. For each data source, it requires writing and managing complex data transformation programs or queries, including being familiar with the source data formats and vocabularies, and the target syntax and semantics (e.g. the constraints to be satisfied, and the vocabularies to be used). Although there are tools available to facilitate translation (by generating transformation queries), e.g. [11–14], these tools are designed for general use, focusing mainly on data with well-defined structure, and having little support for capturing the inherent meaning or semantics of data or the target language, and thus are insufficient for handling the type of translation discussed here, i.e. translation of environmental data from different formats or vocabularies to an XML-based standardised exchange language.In this paper, we investigate ways to facilitate such translation. We focus on data in spreadsheets, as spreadsheets are commonly used to store environmental data. Starting by looking at some real data examples, we identify the issues with data that have to be resolved for data translation, including data being provided at various levels of information detail, having various structures, and using various terminologies and value representations. We then propose an ontology-mediated approach for data translation. We define an ontology for capturing semantic gaps between data and the target language; based on this, we use the ontology to mediate across different structures, terminologies and value representations of data, to check data against the constraints captured by the ontology and ensure that data be provided by the information required, and finally to produce data satisfying the requirements of the target language (in both syntax and semantics). The way the ontology is used makes it necessary to translate data into ontology instances first. We describe the approach in detail through the aforementioned water data translation scenario, including ontology development, spreadsheet to ontology mapping and translation, and ontology to XML mapping and translation. In particular, we provide declarative formalisms for mapping representation (to facilitate mapping customisation and reuse), and give algorithms for processing mappings.To demonstrate the value of our approach, we have developed a tool (named AdHoc) for mapping construction and data translation. The tool provides a graphical interface for users to specify correspondences between spreadsheet data and the ontology. Based on correspondences, the system generates spreadsheet-to-ontology mappings, checks data constraints and translates data to the target XML format (all these are done in the back-end). Ontology-to-XML mappings are constructed manually, but only once, due to a single target language assumed in our work. We have applied AdHoc to the water data translation scenario, and showed its effectiveness with real water resources data. We note that because of the ontology-driven nature of AdHoc, it can be generally applied to other data translation scenarios that are not related to water, but have spreadsheets as data sources and XML as common exchange formats. In summary, we make the following contributions:•We propose an ontology-mediated approach for environmental data translation, based on an analysis of the issues with data that have to be resolved.We outline the principles underlying the design of a mediating ontology for data translation, and show the development of such an ontology through a real environmental data translation scenario.We propose a declarative mapping formalism for representing the relationship between spreadsheets and ontologies, and give an algorithm for the evaluation of spreadsheet-to-ontology mappings.We propose a declarative mapping formalism for representing the relationship between ontologies and XML schemas, and give an algorithm for the evaluation of ontology-to-XML mappings.We have developed an ontology-mediated spreadsheet-to-XML translation tool, and showed its effectiveness with real environmental observations data.The rest of the paper is organised as follows. In Section 2, we identify the issues with environmental data and discuss their implications on data translation. In Section 3, we present the proposed approach, and describe it in detail through the water data translation scenario. In Section 4, we report on the tool implementation and evaluation. Finally, we summarise related work and conclude the paper in Section 5 and Section 6, respectively.According to Beran and Piasecki [15], the biggest challenge in seamlessly integrating multiple data sources is resolving heterogeneity issues. This is also true when exchanging data from multiple sources, and translating data from different formats or vocabularies to standardised languages. Horsburgh et al. [2] classify heterogeneity in environmental observations data into two general types: syntactic and semantic heterogeneity. Syntactic heterogeneity refers to a difference in how data and metadata are organised (e.g. rows vs. columns) and encoded (e.g. text files vs. Excel spreadsheets). For this type of heterogeneity, we are mainly concerned about differences in data organisation or structure, as we assume in this paper that data are all encoded in spreadsheets.Semantic heterogeneity, on the other hand, refers to the variety in language and terminology used to describe observations, including different languages used to describe the names of observation attributes, or to encode observation attribute values [2]. Semantic heterogeneity occurs when there is disagreement in the meaning, interpretation or intended use of the same or related data [16]. In the following, we illustrate both syntactic and semantic heterogeneity, and elaborate the issues involved that have to be resolved in data translation.Fig. 1shows 12 real data examples. Among these examples, (A) is about water usage information, (B) about ground water level information, (C) and (D) about watercourse level information, and (E)–(L) about water storage level or volume information. Although data in these examples are all encoded in spreadsheets, there is no fixed structure for data description; even for the same type of data, data structure could be different. For example, both (C) and (D) record watercourse level information; however, (C) stores water levels of one watercourse per spreadsheet, while (D) stores water levels of several watercourses per spreadsheet. As another example of structural differences, water storage names in (K), (L) and (E) are listed as column names, while in (I) they are stored as column values.Besides differences in data structure, the examples in Fig. 1 also expose several semantic heterogeneity issues. One is that contextual information or metadata is provided at various levels of detail. Contextual information is the descriptive information about data that explains the measurement attributes, their names, units, precision, accuracy and data layout, as well as the data lineage describing how the data was measured, acquired, or computed [17,1,18]. Such information is typically required to enable unambiguous interpretation of data. For the data in Fig. 1, examples of important contextual information include the method and unit in which the measurements were taken, and the datum or reference location from which water levels were measured. However, none of the data examples provide the measurement method information; not all examples (C, D, and F) provide the unit information; and only two (F, G) provide the datum information (AHD) for water level measurements.Another issue is various terminologies being used for data description. This includes different names for the same thing, or the same name for different things. For example, “Volume” is called “Usable” in (E) and “Content” in (J); “Level” is “Height” in (G) and “Gauge” in (J) and (K); “m” is “Metres” in (H) and “mtrs” in (L). Although “Level” and “Volume” are used in several data examples, they can have different meanings in different contexts. For example, “Level” in (B) denotes the ground water level, in (C) the watercourse level, and in (E–F, H, L) the water storage level; “Volume” in (A) denotes the water usage volume, and in (G–I, K–L), the water storage volume. This, from another perspective, illustrates the importance of contextual information in unambiguous interpretation of data.The final issue is different representations or encodings of data values. For example, water levels can be measured in feet or metres (L). Also there are several expressions of date, e.g. 20/08/2008 (A), 31-Dec-87 (B), 1-January-1984 (E) and 20090113 (H). In some literature (e.g. [19]), these are considered as syntactic heterogeneity. Here, we follow the classification by Horsburgh et al. [2] and regard them as a type of semantic heterogeneity.The implications of the above issues on data translation are twofold. First, translation should be flexible enough to handle data with potentially different structures, terminologies and value representations. Second, translation should be able to ensure that sufficient information be provided so that data can be unambiguously interpreted. To meet these two requirements, we employ an ontology-based approach, as detailed in the next section.Ontologies are representations of the knowledge within a domain of interest, defined via the terminology (concepts) used within the domain and the properties and relationships among domain objects [20]. Formal (logic-based) ontology languages, such as the Web Ontology Language (OWL) [21], enable precise expressions of knowledge and have support for automated reasoning. In our approach, we define an ontology for capturing semantic gaps between data from different sources and the target language by including the concepts and relationships describing the semantics of data and the target language. Details of the ontology is given in Section 3.1.We then use the ontology for data translation. Fig. 2shows the whole translation process, in which the ontology plays a mediating role (as such, we call the ontology the mediating ontology). There are two main phases: Mapping Construction and Data Translation. During Mapping Construction, data from a spreadsheet file are mapped to the ontology, where the correlation relationship between data and the ontology is captured using spreadsheet-to-ontology mappings (i.e. S–O mappings); also, the ontology is mapped to the target language, where the correlation relationship between the ontology and the target XML schema is captured using ontology-to-XML mappings (i.e. O–X mappings). Note that with the target schema fixed, we only need to map the ontology to the target schema once and leverage the obtained mappings for all translations. During Data Translation, data are first transformed into an ontology instance by processing S–O mappings; the instance is then checked to ensure its consistency and integrity (through inferencing and constraint checking), and finally transformed into an XML document by processing O–X mappings. We describe spreadsheet-to-ontology mapping and translation in Section 3.2, ontology-to-XML mapping and translation in Section 3.3, and ontology inferencing and constraint checking in Section 3.4.By mapping data from different sources to the ontology and translating them into ontology instances, we are moving from different structures, terminologies and value representations of data to structurally flat, precise and consistent representations of data. In this way, we are freed from worrying about syntactic and semantic differences of data when translating ontology instances to the target language, and are able to focus on producing data satisfying the requirements of the target language. In addition, during the translation process, we can use the ontology to check data against the constraints captured by the ontology, thus ensuring that data be provided with the information required. The key to all this lies in the design of the ontology. Before going into details of ontology design, we would like to illustrate first, through a few examples, what the ontology can do, and how data translation can be helped by the ontology:•Suppose data are continuous water storage level observations, and the target language contains time series concept only (due to, e.g. coarse classification of observations). To bridge semantic gaps between data and the target language, we can use the ontology to capture the relationship between water storage level observations and time series, and specify that continuous water storage level observations are a type of time series. This enables us to use ontology inferencing during the translation of water storage level observations into a time series.Water storage levels may be measured in feet or centimetres, while the target language may specify metres as the unit of water storage level measurements. In such a case, we can use the ontology to capture relationships between feet, centimetres and metres, including how feet and centimetres can be converted into metres. As such, automatic unit conversion is supported during data translation.Water storage level observations may come without datum information. On the other hand, the target language may specify that datum information must be provided with each water storage level observation. By capturing such constraints in the ontology, data can be checked and validated during translation. For those without datum information provided, constraint violations will be triggered during translation.To facilitate data translation like the above, there are three basic principles underlying the design of the ontology. First, the concepts and relationships to be included should be able to facilitate the mapping between data and the ontology, as well as the mapping between the ontology and the target language. This requires the ontology to include the concepts and relationships that describe data and the target language. In general, there are two groups of such concepts and relationships: those describing generic environmental observations, and those specific to a particular domain. Take the water data translation scenario as an example. Data in the scenario are mainly water resources observations such as water storage levels and volumes, and the target language is WDTF, built on the International Standard Organisation’s General Feature Model, ISO 19109, and the Open Geospatial Consortium (OGC)’s Observations and Measurements (O&M) model [22,23]. Therefore, we should consider to include in the ontology generic observation and measurement concepts such as Feature, Observation, Property, and Unit, and water domain specific concepts such as WaterStorage, WaterStorageObservation and WaterLevel. In addition, it is desirable to have concepts and relationships describing unit conversions, to accommodate different units used in data on the one hand, and specific unit requirements of the target language on the other hand. For example, although water storage levels can be measured in metres or feet, WDTF requires that they be represented in metres. By defining the relationship between feet and metres in the ontology, we can facilitate automatic unit conversion.Second, definitions of concepts should be aligned with the vocabularies specified by the target language. For the water data translation scenario, there are 17 vocabularies defined based on the set of Water Regulations under the Commonwealth Water Act 2007 [9]. To describe the processing type of an observation value in a time series, for example, only the following terms are allowed: “Normalised” (as observed with normalised time window), “Derived” (derived from other observations), “Interpolated” (interpolated from other observations), and “None” (as observed) [3].Third, semantic constraints that are placed on the meanings of data should be captured. Constraints can come from several sources. Some have been encoded by the target language through XML schema restrictions or occurrence indicators. For example, WDTF by itself defines that each time series has exactly one observed property, or each water feature has at least a name. Some are provided additionally as specifications by the authority who defines the target language. For example, along with WDTF, a set of specifications are provided by BoM, defining the constraints that data should conform to, e.g. “water storage levels must be represented in metres”. Finally, some constraints, though not explicitly encoded by or provided with the target language, are considered in a domain as general rules that data should follow, e.g. “length must be measured in a unit of length”. Such constraints should be captured by the ontology as well.Following these principles, we develop the ontology for translation. We can develop the ontology either from scratch by examining data, the target language and associated specifications, and identifying the concepts, relationships and constraints to be captured, or by extending the conceptual model of the target language, if available, with additional necessary concepts, relationships and constraints. We follow the former approach for the water data translation scenario. We express the ontology using OWL for its expressivity and reasoning support (the logical underpinning of OWL is provided by Description Logics [20]). We define concepts and relationships through OWL classes and properties, vocabularies through OWL classes and individuals, and constraints through OWL restrictions respectively.We employ a modular approach to the ontology design. The ontology includes three modules: the units of measure (uom) module, the observations and measurements (om) module, and the water information (water) module. Among these modules, the first two are not specific to the water domain and can be used for data translation in other domains. The uom module3It was originally developed for our other projects. See Section 5 for similar modelling efforts.3describes physical quantities, units, and unit conversions. It is designed according to the Energistics POSC Units of Measure dictionary [24]. Fig. 3shows the main concepts and relationships. Each property or quantity is measured in certain units, and has a certain unit as representative unit, or as base for conversion. Relationships between different unit representations are captured by UnitConversionSpecification, which has three subclasses, Factor, Fraction and Formula, each describing a type of conversion specification (with increasing complexity). For example, given that 1cm is 0.01m (the base unit of centimetre), the unit conversion for centimetre has factor of 0.01 (an instance of Factor).The om module describes generic observations and measurements (Fig. 4). It is designed based on existing modelling efforts (e.g. OGC O&M [22,23], which WDTF is built on). One key concept in the module is Observation. Related to Observation are concepts such as Property, Procedure, Time, and SamplingFeature. For each observation, there is exactly one observed property, one measure (unit and value) for the property, one procedure to make the measurement, one observation time and one sampling feature. A SamplingFeature is associated with one or more sampled features (usually domain features). We define two types of SamplingFeature: GeoSamplingFeature and Sample. A GeoSamplingFeature can be a SamplingPoint or SamplingInterval, which typically refers to a CoordinateReferenceSystem for locating the feature. A CoordinateReferen- ceSystem is either predefined or user-defined. For a user-defined Coordinate- ReferenceSystem, its Datum and CoordinateSystem need to be specified. A set of observations constitute an ObservationCollection. We define TimeSeries as a type of ObservationCollection, which consists of observations sharing the same property, the same procedure, the same sampling feature but having different observation times.The water information module is a domain extension of the om module, which covers the concepts and relationships specified in WDTF. Fig. 5shows part of the module. As shown, a WaterObservation (a subclass of Observation) is further associated with a Quality code (e.g. quality-A), a ProcessingType (e.g. Interpolated), an InterpolationType (e.g. maximum in the preceding interval), and an ObservationCondition (e.g. reading taken at bore while dry). Examples of water observations include GroundWaterObservation and WaterStorageObservation. A WaterStorageObservation can be WaterStorageLevelObservation or WaterStorageVolumeObservation, which have WaterLevel (a subclass of Length) and WaterVolume (a subclass of Volume) as observed properties respectively. A Property (e.g. WaterLevel) can have a required unit (e.g. “metre”), which is specified according to the unit representation requirements of WDTF. A WaterLevelMeasure is associated with a Datum and a LengthUnit, through which, we define the dead storage level and fully supply level of a WaterStorge (a type of SampledFeature). Furthermore, we constrain the sampled feature of an Observation (e.g. WaterStorageObservation) to a particular type of SampledFeature (e.g. WaterStorage).We have developed a declarative language to describe mappings between spreadsheets and OWL ontologies. Based on the language, we can construct mappings either manually, or semi-automatically. In this part, we give details of the mapping language, mapping generation, and mapping processing.A mapping in our language (Fig. 6) specifies data from spreadsheets in terms of instances of OWL classes and properties. Fig. 7gives an example mapping for the data and ontology in Fig. 8. A class in a mapping specifies an OWL class, with instances to be mapped to, or to be related to instances of another class through an object property, or to values through a data property. A constraint4Note that constraint information of a property is derived from the axioms (e.g. cardinality and existential restrictions) defined in the ontology (see Section 3.2.2 for details). Strictly speaking, it is not necessary to include it in a mapping, as the ontology covers all information. We keep it mainly because of performance considerations – to save us from inspecting the restrictions defined against a class at runtime when a mapping is processed and instances for the class are to be generated. The same applies to the hasKey clause.4indicates whether a property is functional (indicated by ‘F’ in the language), or non-functional (indicated by ‘N’); a functional property (or each instance of a functional property) relates each instance of a class to at most one individual or value, while a non-functional property can relate each instance to more than one individuals or values. For example, in the mapping (i.e. mapping1) of Fig. 7 (Fig. 9will show an instantiation of the mapped data), since water:hasProvider (where water is the namespace prefix of hasProvider) is functional, each instance of water:hasProvider links each instance of om:TimeSeries to at most one instance of water:Agency; also, since water:hasID is functional, each instance of water:Agency has at most one ID. In addition, in the language, an optional hasKey indicates whether each instance of a class is uniquely identified by a set of values. An owlPath specifies a path to reach a value, taking the form “C1/P1/C2/P2/…”, whereCiis a class andPiis a property (note that a datatype property appears only at the end of the path). In the above example, no two instances of water:Agency have the same ID, since each instance of water:Agency is uniquely identified by its ID, i.e. “water:Agency/water:hasID”.Values for datatype properties are either user-provided or exacted from spreadsheets. In the latter case, a matching specifies data from spreadsheets, which are referenced by ranges. A range is identified by sheet identifier and range expression. In the simplest case, a range expression refers to a single cell in a sheet (a cell is referenced by its column letter and row number), e.g.A1:A1. A range expression can also refer to a block of cells, e.g.A1:B2, which consists of cellsA1,B1,A2, andB2. Cells in a block can be partitioned by row (i.e. ‘split_by=R’) or column (i.e. ‘split_by=C’). For example, the row-wise partitioning ofA1:B2generatesA1:B1andA2:B2, and the column-wise partitioning generatesA1:A2andB1:B2. Furthermore, functions can be used in or applied to a range expression, to allow for flexibility in cell operations. For example,end_of_row()can be used to get the last non-blank cell in the current row. Data from spreadsheets may be correlated in some way. For example, in the spreadsheet of (J) of Fig. 1,A4:A5are two observation times, andB4:B5are two water storage levels, while each row ofA4:B5(i.e.A4:B4andA5:B5) belongs to a single observation. To capture such data correlation, a relativeto is used. A matching relates to another matching, if the data items covered by the first matching correlate to those covered by the second matching, or to each partition of their row-wise or column-wise partitioning, in the order the items appear. In the example of Fig. 7, there are two matchingsM1(coveringA4:A5of sheet 0) andM2(coveringB4:B5of sheet 0), andM2is specified to relate toM1, as the cells ofM2correlate to those ofM1as described earlier.Mappings can be generated manually (based on the proposed mapping language), or semi-automatically. In Adhoc, a graphical interface is provided to facilitate mapping generation (Fig. 17 in Section 4). Through the interface, users create correspondences between data and the ontology, i.e. selecting the instances of a class that are related to the data, specifying data cells that correspond to a datatype property, or providing values for a data property. The ontology is displayed in a tree structure for easy exploration. Internally, it is represented as a graph. For each class C, there is a corresponding class node in the graph labelled with C. For each object property op, an edge labeled with op is created from class nodeC1to class nodeC2, if op is defined with domainC1and rangeC2, or there is a restriction stating that each instance ofC1is related to one or more instances ofC2by op. Similarly, for each datatype property dp, if dp is defined with domain C and data range D, or there is a restriction stating that the dp value of each instance of C is limited in data range D, we create a data range node labeled withDdp,C, and an edge labeled with dp from node C toDdp,C. Also, for a classC1which is a subclass of classC2, we create an edge labeled withis_afrom nodeC1toC2. For the ontology in Fig. 8, the corresponding ontology graph will have 7 class nodes, 3 data range nodes, and 10 edges.An exploration of the ontology graph starts from a class nodeC1and ends at a class nodeC2(could be same asC1) with its instances selected, or a data range node D to which data from spreadsheets or user-provided values are corresponded. Once all correspondences are built and graph explorations are done, the minimum spanning trees or Steiner trees that connect those ending class or data range nodes are found with existing Steiner tree algorithms (e.g. [25]). To ensure that correct edges are selected by the algorithm, we record the paths which users select to reach class instances or data range nodes during explorations.We then replace each data ranges nodes in a Steiner tree with corresponding matchings or user-provided values, which results in a mapping tree. Suppose, for the data and ontology in Fig. 8,A4:A5are indicated to relate to datatype property inDateTime, andB4:B5to relate to hasMeasuredValue. In addition, 001 is provided as Agency ID, metre (the instance of LengthUnit) selected as the unit of Measure and the required unit of WaterLevel. Then we can get the mapping tree in Fig. 9. Finally, from a mapping tree, we generate the corresponding mapping by traversing the tree in a top-down fashion. Here, we give the general idea of the process only. Suppose N is the root of a mapping tree. If N is a class node, we check whether it is a leaf node, i.e. a class node with instances specified. If it is, we output the specification of class instances and terminate the process. Otherwise, we check whether the corresponding class has keys and output the hasKey specification if it has; meanwhile, for each edgepifrom N toNi, we determine whetherpiis functional or not (we considerpifunctional ifpiis defined as a functional property in the ontology, or there is an exact or a Max 1 cardinality restriction defined against N bypi) and output the constraint specification, and then processNirecursively ifpiis an object property, or output the corresponding value or matching specification and terminate the process ifpiis a datatype property. From a mapping, we can also generate the corresponding mapping tree, with the nodes corresponding to the classes specified, values provided or data extracted from spreadsheets, and the edges to the properties specified.The evaluation of a mapping is a process of generating class and property instances that satisfy the mapping, and populating the knowledge base with those instances. The process involves the evaluation of the classes and properties specified. The evaluation of a class requires evaluating its properties (if any); in the case of object properties, the classes being linked to need to be evaluated first. This is in fact performed by traversing the corresponding mapping tree in a bottom-up manner. Fig. 10illustrates the evaluation process of the example mapping in Fig. 7 and the instances generated and asserted.Fig. 11gives the pseudo code for the evaluation of a class, which may call the algorithms in Fig. 12for the evaluation of an object or a datatype property. During the evaluation, we maintain intermediate states of the property instances generated through RDF node chains. An RDF node chain contains at most three elements; with three elements, an RDF node chain corresponds to an RDF triple [26], which will be asserted as an instance of a property. The evaluation of a class depends on whether there are instances specified. If there are instances specified, we create an RDF node chain for each specified instance and add the instance to the chain (lines 2–5 in Fig. 11); otherwise, we evaluate the properties of the class (Fig. 12): for an object property, we add the property to each RDF node chain resulted from the evaluation of the class that the property links to; for a datatype property, we create an RDF node chain for each user-provided value or each cell covered by a matching, and add the value or cell content, and the property, to the chain. Note that during the evaluation of a datatype property, a value or the content of a cell, before being inserted into an RDF node chain, is checked against the data type of the property value defined in the ontology, and an error will be issued if there is a type mismatch and type conversion is not supposed to be done (e.g. a string value is provided for a datatype property whose range is xsd:double); in some cases, data need to be converted into the required representations (e.g. a value in feet is converted into that in metres). Once an RDF triple is formed, it is committed to the knowledge base, and the RDF node chain is updated with only one element included, the individual just inserted.For example, for the mapping tree in Fig. 9, the leaf nodes, i.e.V1,C5,M1andM2, are first processed (the processing ofV1,M1andM2is done during the evaluation of datatype properties hasID, inDateTime and hasMeasuredValue respectively), as shown in Fig. 10. Next,C2,C3,C6andC7are processed, followed by the processing ofC4. Finally, the root nodeC1is processed. During the process, new RDF node chains are created, and new instances are generated. For example, the evaluation ofV1results in one RDF node chain, which includes 001 only; whenC2(i.e. Agency) is processed, an RDF triple, i.e.(agency_001,hasID,001), is formed, whereagency_001is an instance generated for Agency, and after the triple is committed, and the RDF node chain is updated to includeagency_001only; similarly, whenC1(i.e. TimeSeries) is processed,(ts1,hasProvider,agency_001)is formed and committed, andts1(an instance of TimeSeries) becomes the only element left in the node chain.A class in a mapping may have several properties specified. As the evaluation of each property results in at least one RDF node chain, a question arises during the evaluation of the class regarding how many instances of the class need to be generated, and how to relate them to the RDF node chains generated from the evaluation of properties. We address this question by taking advantage of the cardinality constraint of functional properties, and correlations between RDF node chains. For each RDF node chain resulting from the evaluation of a functional property, there should exist at least one unique instance of the class; also, correlated RDF node chains should be related to a single instance of the class. We consider two RDF node chains correlated, if their generation histories involve correlated spreadsheet data. To generate the instances of a class, we first group the RDF node chains generated from the evaluation of its properties through instance groups. Initially, for each property, an instance group is created, including all the RDF node chains generated from the evaluation of the property, and is set to be functional if the property is functional and non-functional otherwise (lines 7–12 in Fig. 11). Two groups are merged (lines 13–16) if they have correlated RDF node chains. During the merging, correlated RDF node chains are put into the same bucket, and the merged group is set to be functional if one group being merged from is functional. The merging process is continued until no pairs of groups can be merged. At this point, we generate class instances by differentiating the following cases (lines 20–30):1.there are no functional groups: we generate a single instance of the class and add it to all RDF node chains.there is only one functional group (e.g. when instances forC4in Fig. 9 are to be generated): we generate an instance for each bucket of the functional group, and add it to each RDF node chain in the bucket, and each RDF node chain of non-functional groups (if any).there are more than one functional groups (e.g. when instances forC7in Fig. 9 are to be generated): we employ Cartesian product between buckets of functional groups: each bucket of one functional group is combined with each bucket of another functional group. For each combination, we generate an instance of the class, and add it to each RDF node chain in the combination and each RDF node chain of non-functional groups (if any).We acknowledge that these cases present a particular interpretation of the semantics of a given mapping. However, it works in practice. In implementation, a random, 128-bit UUID (universally unique identifier) is generated for an instance of a class. In the case where an hasKey is specified for a class, i.e. each instance of the class is uniquely identified by a set of owlPaths, each owlPath is evaluated, and an instance is generated with a unique identifier created by concatenating the name of the class with string representations of the values resulted from the evaluation of the owlPaths.We now use the mapping tree in Fig. 9 to illustrate how to generate instances of a class with multiple properties. TakeC7(i.e. class Measure) as an example. There are two functional properties, hasMeasuredValue and hasUnitOfMeasure. The evaluation of the former property results in two RDF node chains,(_,hasMeasuredValue,(B4))and(_,hasMeasuredValue,(B5))while the evaluation of the latter results in(_,hasUnitOfMeasure,metre). Since these two properties cannot be merged, we follow case 3 for the generation of class instances (i.e. there are more than one functional groups), and generate two instances of Measure, i.e.m1andm2in Fig. 10. We then addm1to(_,hasMeasuredValue,(B4))and(_,hasUnitOfMeasure,metre), andm2to(_,hasMeasuredValue,(B5))and(_,hasUnitOfMeasure,metre), resulting in 4 RDF triples, i.e.T5,T6,T7andT8in Fig. 10. As another example,C4(i.e. class WaterStorageLevelObservation) also has two functional properties. Different from the case withC7, these two properties can be merged asA4:A5andB4:B5are correlated. After the merging, there is only one functional group with two buckets. Thus, we follow case 2 for the generation of class instances and generate two instances of WaterStorageLevelObservation. After adding the two instances to the two buckets respectively, we get 4 triples, i.e.T9,T10,T11andT12in Fig. 10.Similarly, we develop a declarative language to describe mappings between ontologies and XML schemas. For data translation, we construct ontology-to-XML mappings manually and only once (due to a single target schema assumed).Fig. 13shows the abstract syntax of the language. A mapping in the language describes XML instance data in terms of OWL individuals and literals. An owlPath in a mapping specifies the OWL individuals or literals to be mapped, while an xmlPath specifies the XML instance data to be mapped to. A root mapping (with its identifier indicated by the keyword root) is the mapping from which a transformation starts. All other mappings refer to the root mapping either directly or indirectly through a relativeTo. In a mapping without a relativeTo, the owlPath (or xmlPath) is absolute. An absolute owlPath takes the same form as described in Section 3.2, i.e. “C1/P1/C2/P2/…”. For example, “C1” represents the instances ofC1, and “C1/P1” the individuals or literals reached viaP1from the instances ofC1. In addition, an absolute owlPath of “” represents all the individuals and literals in the ontology to be mapped. An xmlPath uses a simplified XPath expression. For example, “E1” represents all the instance data of elementE1, and ‘‘E1/E2/@a” values of attribute a ofE2that is the child ofE1.In the case where a relativeTo is specified in a mapping, the owlPath and xmlPath should be interpreted with respect to those in the mapping being referred to. Suppose that there are two mappings,m1andm2, andm2is specified in reference tom1:m1: mapping{id=1 owlPath=“C” xmlPath=“E”}m2: mapping{id=2 owlPath=“P” xmlPath=“@a” relativeTo=1}Thenm2needs to be interpreted in the context ofm1. That is, when mapping the instances of property “P” to the values of attribute “@a”, they should be reached from the instances of “C” and element “E” (specified inm1) respectively. Equivalently,m2can be rewritten as:mapping{id=2 owlPath=“C/P” xmlPath=“E/@a”}To indicate the owlPath (resp. xmlPath) in a mapping to be same as that in the mapping being referred to, “” (resp. “.”) is used.Fig. 14gives an example, which maps the ontology used in the example mapping of Section 3.2, to an XML schema wdtf-mini (an instance of the schema is shown in Fig. 16). The example includes 12 mappings, and mapping 1 is the root mapping that relates all the individuals and literals of the ontology to all the instance data of element “wdtf-mini:HydroCollection” (where wdtf-mini is the namespace prefix of HydroCollection). All other mappings refer to mapping 1 directly or indirectly, and should be interpreted in the context of mapping 1. For example, mapping 6 directly refers to mapping 1, mapping 7 directly refers to mapping 6, and mapping 7 indirectly refers to mapping 1; also, the owlPath and xmlPath in mapping 6 should be interpreted as “om:TimeSeries” and “wdtf-mini:HydroCollection/TimeSeries’’5For easy of presentation, we may omit namespace prefixes when the context is clear.5respectively, and the owlpath and xmlpath in mapping 7 as “om.TimeSeries/hasObservedProperty/Property” and “wdtf-mini:HydroCollection/Time-Series/ObservedProperty/data()” respectively.Furthermore, an optional xmlTargetFragments allows a mapping to operate on ontology individuals or literals, or to include data not present in the ontology (e.g. those that need to be generated dynamically). It can be a number of attributes, or a cdata section. Values for attributes or cdata can be obtained through function applications. Fig. 14 gives two example uses of xmlTargetFragments: in mapping 4,getCurrentDate()is called to get the current date time for element wdtf-mini:generateDate; in mapping 5, a constant value ‘AdHoc’ is provided for element wdtf-mini:generateSystem.Note that the mapping language described here assumes a close relationship between the ontology and the XML schema – the ontology is built based on the XML schema, or derived from the conceptual model of the XML schema. It may need to be extended to describe general cases of ontology and XML mapping.Given an ontology, an XML schema and the mappings describing the relationship between the ontology and the XML schema, mapping processing is to generate an XML instance document satisfying the mappings. An XML document is internally represented as a tree, which starts at the root element and branches to the lowest level of elements. To construct an XML tree, the root mapping is first processed, and then the mappings that refer to the root mapping are processed; if a mapping is not being referred to by any mappings, the processing is terminated; otherwise, the processing is continued until all the mappings that refer to the mapping are processed. During the processing, new XML nodes are generated and appended to the XML tree. Fig. 15shows the algorithm.Processing a mapping involves evaluating its owlPath by ontology inferencing and updating the context for the processing of other mappings (lines 2–5 in procedure transformToXML). If an owlPath ends in a class or an object property, the evaluation result will be a set of ontology individuals; otherwise, the result will be a set of literals. In the former case, the individuals will be provided as context for the mappings that directly refers to the mapping being processed. Note that the evaluation of an owlPath should be done in the context of the mapping being referred to. Processing a mapping also involves evaluating its xmlPath (line 11). The evaluation of an xmlPath results in an XML object node chain, which provides the basis for subsequent generation of new XML nodes (lines 13–17). For each individual or literal in the evaluation result of an owlPath, a new XML subtree which corresponds to the node chain will be generated, with the individual or literal bound to the subtree. The same applies for the evaluation result of an xmlTargetFragments (if any). Once the subtree is appended to the current XML node, the mappings that directly refer to the current mapping will be processed with the updated context (lines 19–22). If the current xmlPath is “.”, no new XML nodes will be generated, and the mappings that directly refer to the current mapping will be processed instead (lines 7–10). Fig. 16shows the XML document tree generated from the evaluation of the mappings in Fig. 14.During translation, instances of classes and properties are created based on data from spreadsheets, and the consistency of instances needs to be checked; also, ontology instance data need to be retrieved so as to generate target XML instance data. All this can be handled using a standard OWL reasoner such as Pellet [27], which supports the formal semantics of OWL provided by Description Logics. On the other hand, extensions to OWL are necessary for the task of constraint checking, as the open nature of OWL (i.e. the Open World Assumption) makes it hard to use OWL for the task [28,29].To use OWL for both inferencing and constraint checking, we follow an approach similar to the one proposed in [28]. Basically, among all the axioms in the ontology, we identify those that should be interpreted as integrity constraints (ICs) – checks that verify whether the information explicitly present in the ontology satisfies certain conditions [29]. We then translate each constraint into a SPARQL ASK query [30] such that when the constraint is violated, the query is entailed (i.e. a non-empty result is returned). We focus on OWL existential and cardinality restrictions and treat them as ICs. For example, given a constraint of “each time series contains at least one observation” (i.e.TimeSeries⊑>=1hasMember.Observation), we translate it into the following SPARQL query:PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>PREFIX om: <http://www.csiro.au/om.owl#>ASK WHERE {?x rdf:type om:TimeSeries.FILTER NOT EXISTS {?x om:hasMember ?y.?y rdf:type om:Observation.}}When the query is executed, a non-empty result indicates that the constraint is violated, i.e. there exists a time series without any observations. In this way, we turn constraint checking into SPARQL query answering. For the axioms not ICs, we interpret them with standard OWL semantics. These axioms are the only ones involved in inferencing.We have implemented AdHoc, an ontology-mediated spreadsheet-to-XML translation tool, and applied it to the water data translation scenario with WDTF as the target language of translation. The tool provides a graphical interface for users to load spreadsheets and ontologies, and to construct spreadsheet-to-ontology mappings (Fig. 17). The construction of mappings is performed by selecting the spreadsheet data to be mapped and the part of the ontology to be mapped to. A set of predefined cell operations is provided (on top of the left pane of the interface) to facilitate the selection of spreadsheet data, and the ontology is displayed in a tree structure for easy exploration (in the right pane of the interface). During the mapping construction, a matching is decided by relating a range of cells to a data property of the ontology, and can be indicated to be relative to another matching. Also, values can be provided on the fly for the information not included in spreadsheets.Once spreadsheet-to-ontology mappings are constructed, they can be saved, edited, loaded and processed. Processing of spreadsheet-to-ontology mappings is done in the back-end, followed by ontology constraint checking, consistency checking, and processing of ontology-to-WDTF mappings. Constraint checking is supported by Jena ARQ [31], and Pellet is used for consistency checking. We construct ontology-to-WDTF mappings beforehand, and leverage them for all translations. After processing of ontology-to-WDTF mappings, a WDTF instance document is generated. Fig. 18shows the screenshot of a generated WDTF instance document.We evaluated AdHoc with real water resources data provided by BoM. Data are distributed in 55 MS Excel files, covering ground water, watercourse, water storage and water usage observations. Among the files, more than half have different data structures. For example, among the 11 files covering water storage observations, 2 include only water levels or volumes; 9 include both, 10 include observations of a single site, and 1 includes observations of several sites. Semantic heterogeneity is also present in data. For example, to denote water level, terms such as “level”, “height”, or “gauge” are typically used, and variations of these terms (e.g. “gauge reading” instead of “gauge”) are very common. AdHoc can handle such data well, due to its ontology-driven nature. Through AdHoc, we mapped data in each file to the ontology, which resulted in 55 WDTF instance documents; we then validated each instance document through the WDTF online validation service (http://www.bom.gov.au/jsp/wdtf/wdtf-validation/). The validation service is produced by BoM which designed WDTF, and provides two types of validation: structure validation for validating documents against the WDTF schema, and content validation for validating documents against hydrological semantic rules and vocabulary service. The result is quite satisfactory: all 55 instance documents passed validation – both structure and content validation. This shows that AdHoc is effective in handling spreadsheet data with structural and semantic differences, and capturing the constraints required to generate valid target instance documents. To use AdHoc for translation, users are mainly required to find the data to be mapped and then relate them to appropriate ontology classes and properties. This can be done with a few mouse clicks through the interface of AdHoc.We also evaluated AdHoc in terms of the time needed for translation after spreadsheet-to-ontology mappings are constructed. The evaluation was done on a Dell laptop with a 2.90GHz Intel Core i5-3380M and 4GB RAM. We focused on one spreadsheet file being provided, which covers daily watercourse level observations of 5 sites for 6months. We varied the coverage of the data to be mapped and tested the time used for translation of observations of 1day, 1week, 1month, 3months and 6months. Table 1shows the results, including the number of the instances generated, the time used for spreadsheet-to-ontology translation (S–O time), and the time for ontology-to-WDTF translation (O–X time). As expected, the time used for translation increases with the amount of data being translated. For 1month’s observations, the translation took less than 2s (with 23K instances generated), while for 6months’ observations, the translation took about 10s (with 141K instances generated). Fortunately, most spreadsheet files being provided are of small size (less than 100KB), and translation can be finished within a couple of minutes. We also tested the time needed for consistency checking and constraint checking. The time needed for consistency checking also increases with the amount of data being translated (or the size of the instances being generated). For the instance data covering 1month’s observations, consistency checking took on average 3s, while for the instance data covering 6months’ observations, consistency checking took on average 20s. The time needed for constraint checking increases with the number of ICs. There are 102 ICs identified, and validation of each IC for 1month’s observations and 6months’ observations took on average 3 and 4ms respectively.

@&#CONCLUSIONS@&#
With availability of more environmental observations data, how to access, integrate and analyse these data becomes a challenge due to heterogeneous nature of data across sources. This drives a lot of efforts in developing standardised markup languages for exchanging information describing the collection, analysis and reporting of environmental observations. On the other hand, there is often a need to translate data from different formats or vocabularies to these languages. In this paper, we focused on the problem of translating environmental data encoded in spreadsheets to an XML-based standardised exchange language, and proposed an ontology-based approach for this. The basic idea of our approach is to define an ontology for capturing semantic gaps between data and the target language, and then to use the ontology to mediate across different structures, terminologies and value representations of data, to check data against the constraints captured by the ontology, and finally to generate data satisfying the requirements of the target language.We have implemented our approach in a tool named AdHoc which provides a graphical interface for users to specify correspondences between spreadsheet data and the ontology (the only major task for users to do when new data come in for translation). We have applied Adhoc to a water data translation scenario and showed its effectiveness with real water resources data. We can also apply Adhoc to a different scenario, e.g. with a different XML-based target language. In doing so, however, some changes to the mediating ontology may be necessary, or the whole ontology may need to be re-developed; also, the mappings between the ontology and the target language need to be re-constructed.Nevertheless, the main parts of AdHoc would stay the same. This includes the graphical interface for specifying spreadsheet-to-ontology correspondences, the generation of spreadsheet-to-ontology mappings, and the processing of spreadsheet-to-ontology and ontology-to-XML mappings. Currently, in Adhoc, we assume a single XML output format. An interesting question that arises is that whether it would be possible for Adhoc to support translation of data to multiple output formats. Our answer is yes; however, we may need to re-develop the ontology or re-construct ontology-to-XML mappings for each format, load them into Adhoc, and perform translation sequentially. Two cases are differentiated: if different output formats share the same conceptual model, then we only need to re-construct ontology-to-XML mappings for each format, since the mappings need to capture the structure information of the format; otherwise, we need to re-develop the ontology and re-construct the mappings for each format. For the first case, we can reuse existing spreadsheet-to-ontology mappings as the ontology would be the same. In general, we believe that the proposed approach which underlies AdHoc can be employed in generic data translation cases where structural and semantic differences of data are present.