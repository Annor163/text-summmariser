@&#MAIN-TITLE@&#
A tree does not make a well-formed sentence: Improving syntactic string-to-tree statistical machine translation with more linguistic knowledge

@&#HIGHLIGHTS@&#
String-to-tree SMT systems aim to produce grammatically well-formed translation.Modelling assumptions of synchronous grammars can result in ungrammatical output.Grammar overgeneralisations can be reduced by parser choice and modifications.Overgeneralisation errors can be prevented with rule-based lingustic constraints.

@&#KEYPHRASES@&#
Statistical machine translation,Syntactic translation models,String-to-tree models,Morphology,

@&#ABSTRACT@&#
Synchronous context-free grammars (SCFGs) can be learned from parallel texts that are annotated with target-side syntax, and can produce translations by building target-side syntactic trees from source strings. Ideally, producing syntactic trees would entail that the translation is grammatically well-formed, but in reality, this is often not the case. Focusing on translation into German, we discuss various ways in which string-to-tree translation models over- or undergeneralise. We show how these problems can be addressed by choosing a suitable parser and modifying its output, by introducing linguistic constraints that enforce morphological agreement and constrain subcategorisation, and by modelling the productive generation of German compounds.

@&#INTRODUCTION@&#
The modelling limitations of phrase-based statistical machine translation (SMT) are well known, for instance its inability to model discontiguous phenomena such as verb complexes in German, and the limitation to local fluency modelling. Hierarchical models and synchronous context-free grammars (SCFGs) are an attractive alternative because they do not suffer from these theoretical limitations. We can learn an SCFG from a parallel corpus that is syntactically annotated. For a string-to-tree system, annotation of the target side is sufficient. During decoding, such an SCFG is used to build a target-side syntactic tree from a source string. A common expectation might be that building a syntactic tree ensures that the sentence that is produced is grammatically well-formed, but in reality, this is often not the case.In this work, we discuss why string-to-tree SMT systems can produce ungrammatical output, and we examine the reasons in detail. Specifically, we investigate the following crucial aspects:•Data sparseness issues, specifically unknown words.Overgeneralisation phenomena of SCFG models.The relevance of the syntactic annotation scheme for specific linguistic phenomena.The impact of morphosyntactic ambiguities.Problems related to productive compositional morphology.We describe the inclusion of linguistic features into the translation process to promote grammatical translation output, including a unification-based morphological agreement checks for noun phrases, subcategorisation constraints for verbs, and a target-side compound splitting and merging approach that makes use of a finite-state morphology.In most modern syntax-based SMT models, the translation units are either SCFG rules or synchronous tree-substitution grammar (STSG) rules.11STSG is a variant of synchronous tree-adjoining grammar (Shieber and Schabes, 1990) that includes the substitution operation but not the adjunction operation.For the purposes of string-to-tree decoding, the two formalisms are equivalent unless the decoding model uses the internal structure of the STSG rules to define scoring features. Since SCFG is more widely used in the literature, we will use SCFG throughout this paper, but note that all points apply equally to STSG.In its most general form, a SCFG rule is a rewrite rule:〈A,B〉→〈α,β,∼〉where the head is a pair of source and target non-terminals, A and B, and the body comprises a string, α, of source terminals and non-terminals; a string, β, of target terminals and non-terminals; and a one-to-one correspondence ∼ between source and target non-terminals. As in context-free grammar, the terminals and non-terminals are atomic symbols.As the name SCFG implies, derivation using an SCFG involves the same assumption of context-freeness as in CFG: a pair of linked source and target non-terminals, A and B, in a sentential form can be rewritten using the body of some rule, r, provided that that r's non-terminal head symbols match. For the purposes of bottom-up parsing, a synchronous subderivation with head non-terminals A and B is equivalent to any other with the same head symbols.In string-to-tree models, only one non-terminal symbol, X, is used on the source side of the grammar. As in hierarchical phrase-based SMT (Chiang, 2005, 2007), the X non-terminal is used generically to represent a gap in a discontiguous phrase (here we use “phrase” in the same sense as in phrase-based SMT: a sequence of words). In contrast, the vocabulary of target non-terminal symbols may be arbitrarily large. Depending on the grammar learning approach, it may comprise tens, hundreds, or even thousands of distinct symbols. Typically, these are derived from the constituent labels of phrase-structure parse trees. In the following rules:〈X,NP〉→〈thedog,derHund〉〈X,SENT〉→〈ThenX1barked,DannbellteNP1〉the head non-terminals are used to label the string der Hund as an NP and the string Dann bellte NP1 as a SENT. (In the rule body, the subscripts are used to indicate the non-terminal correspondence.) One or more additional non-terminal symbols are used for the “glue” rules, which concatenate partial derivations.There are two main approaches to rule extraction for string-to-tree models: the first extends Chiang (2005)'s SCFG extraction method to incorporate target-side annotation derived from the labels of phrase-structure parse trees. This is the approach first described in the syntax-augmented MT (SAMT) model (Venugopal and Zollmann, 2006). The second is GHKM (Galley et al., 2004, 2006), which derives STSG rules from training data annotated in the same way. The two approaches are closely related (Hanneman et al., 2011; Hopkins et al., 2011) and differ in details such as the restrictions they place on extracted rule size, the handling of unaligned words, and the requirement that the target side of the extraction site is covered by a parse-tree constituent.The SAMT and GHKM rule extraction algorithms are dependent on automatic word alignments. Fig. 1shows a word-aligned sentence pair, annotated with a phrase-structure parse tree on the target side. Both algorithms employ phrase-based style heuristics that require a rule extraction site to contain consistent word alignments. For instance, an extraction site that covers the source span also show few must include the aligned words zeigen ebenfalls wenig. Given the input sentence pair of Fig. 1, it is not possible to extract, for example, the rule〈X,AP−HD〉→〈alsoshowfew,ebenfallswenig〉because it is not consistent with word alignment, nor is it possible to extract a rule spanning zeigen ebenfalls wenig because the phrase is not covered by a constituent.Decoding in an SCFG-based model involves searching the space of synchronous derivations for the highest-scoring translation, according to some scoring model. A single translation can have many possible derivations and for decoding to be computationally tractable, the search criterion is typically approximated by a search for the highest-scoring derivation; the search itself is typically an approximate beam search.As in phrase-based SMT, derivations are usually scored according to a log-linear model (Och and Ney, 2002) that allows for the incorporation of arbitrary feature functions defined over the source string and target derivation. To facilitate efficient dynamic programming, the feature functions should be defined such that they are local to SCFG rules in order that the score is decomposable along subderivation boundaries. In practice, the n-gram language model, which violates this desideratum, is sufficiently important for translation quality that it is integrated at the expense of search efficiency. n-gram language model integration is usually achieved using cube pruning (Chiang, 2007).Typically, the feature functions of a string-to-tree model include scores for the individual rules such as the bidirectional translation probabilities p(α∣β, B) and p(β, B∣α); bidirectional lexical translation probabilities (Koehn et al., 2003); the number of terminals in β; a constant rule penalty; and some measure of rule frequency. Marcu et al. (2006) and Williams and Koehn (2012) both use an unlexicalised PCFG grammar to score the tree fragment from which a rule is extracted. This feature is intended to encourage the production of syntactically well-formed derivations, and essentially serves as a syntactic language model.We will focus our discussion and experiments on the translation direction English→German. As background to our linguistically motivated extensions of a string-to-tree SMT system, we will first discuss the linguistic annotation of the German target text, and the role of glue rules and non-terminal labels for unknown words.The syntactic annotation of the target text in a parallel training corpus has various effects for string-to-tree translation modelling. Non-terminal symbols constrain which rule rewrites are allowed during parsing, and the size of its vocabulary is a trade-off between sparseness (if the vocabulary is large) and overgenerality (if it is small). The degree of branching of syntactic trees affects rule extraction, where only aligned phrases that cover a constituent are extracted as rules, unless we relax this constraint as in the SAMT model. Parsing errors, in particular systematic ones, result in ungrammatical patterns being learned by our SCFG. While annotation schemes of German treebanks and their impact on parsers has been discussed in the parsing literature (Kübler, 2005), our aims in this work is to adapt the output of a syntactic parser to fit the need of the downstream application, which is machine translation.We focus on the annotation scheme used by ParZu (Sennrich et al., 2013). ParZu is a syntactic parser which implements the dependency grammar described in (Foth, 2005) and is trained on the dependency representation of the TüBa-D/Z treebank (Telljohann et al., 2004; Versley, 2005). Part-of-speech tags from the Stuttgart-Tübingen tagset for German (STTS) (Schiller et al., 1999) are used as pre-terminal labels. Parse trees may be non-projective, but since the SCFG model requires a context-free annotation we use the projective representation which is optionally provided by ParZu. We convert the dependency trees into a constituency representation by considering each token to be the head of a constituent, using its dependency label as non-terminal symbol, and adding a virtual root node SENT, to which all words without a head (this typically includes the finite verb of main clauses, sentence-final punctuation, and sentence fragments that were left unattached) are attached. Fig. 2shows a dependency tree produced by ParZu, and Fig. 3shows the same tree in a constituency representation.The most obvious reason for ill-formed translations is when parsing fails to produce a full tree. SCFG systems typically resort to a concatenation of partial trees in this case, implemented through a set of glue rules.The root cause for an inability to form complete trees is often data sparseness, in particular words that are unknown to the SCFG. However, when discriminatively optimising the cost of the glue rules on a development set, the system may learn to use them more frequently. It is also common to limit the maximum span of CFG trees for efficiency reasons. With Scope-3 pruning (Hopkins and Langmead, 2010), the complexity of parsing is cubic to the input length, albeit with a high constant due to grammar size. Nadejde et al. (2013) use a maximum span of 25 for the string-to-tree grammar.To allow the production of syntactic trees even if words in the source string are unknown to the SCFG, Nadejde et al. (2013) use statistical evidence from the training data, specifically the label distribution of singletons, to assign probabilities to different non-terminal labels for unknown words. Assigning a suitable non-terminal label to unknown words is important because its label constrains the possible derivations of the sentence.As an alternative strategy, we propose to instead use sparse features to discriminatively learn which labels to use for unknown words during decoding. We initially label unknown words with UNK, and relax the matching constraint during rule application. Instead of requiring each non-terminal symbol in the body of a rule to exactly match the head of the rule that is substituted into it, we also allow a number of soft matches. Specifically, we allow soft matches from UNK to all other non-terminal symbols, and trigger a sparse feature for every soft (and exact) match that identifies the two non-terminal symbols of the rule expansion. Also, since our syntactic constraints that we discuss later rely on the internal tree structure, we want to avoid the use of glue rules, and thus fix their cost at a sufficiently high value so that they are only used if no other derivation can be found, and set the maximum span of rules to 50.It is easily apparent why translations that are a product of incomplete derivations fail to be syntactically correct. However, even full trees that are produced by the SCFG translation model, and deemed acceptable by both the n-gram language model and the target-side PCFG, may be ill-formed. This is the result of the independence assumptions of the model, which scores rules independently and treats rules with the same head symbol as interchangeable, and overgeneralisations in the linguistic annotation. Overgeneralisations in the set of syntactic labels are typically unproblematic for parsing because modern parsers use a rich feature set and do not make such strong independence assumptions as the SCFG and target-side PCFG that we use for decoding. Also, the ability to discriminate between grammatically correct and incorrect sentences is not a central goal for most probabilistic parsers, with the main evaluation criterion being performance on natural text (e.g. Nivre et al., 2007; Kübler, 2008). While it would be desirable to use syntactic parsers to distinguish between well-formed and ill-formed translation hypotheses, in past research on using parsers as language models, parser scores failed to improve translation, partially because the parsers used gave high scores to ungrammatical hypotheses (Och et al., 2004; Post and Gildea, 2008).An example of a grammatical error that is produced by our baseline SCFG is shown in Fig. 4. In this example, subject–verb agreement is violated in the relative clause, the correct inflectional form being the 3rd person singular form einführte (Engl: introduced), instead of the plural form einführen. Since person and number are not encoded in the SUBJ symbol, nor in the pre-terminal symbols of either the pronoun or the verb, the SCFG learns various rules which allow wrong subject–verb combinations, for instance the following:REL→,SUBJOBJAeinfu´hrenREL→,PRELSOBJAeinfu´hrenREL→,derOBJAVVFINREL→,SUBJOBJAVVFINIn other words, the grammar incorrectly assumes independence between the subject and the verb, unless we use a rule in which both are lexicalised. It is also apparent that an n-gram language model is unlikely to promote agreement due to the distance between the subject and the verb.Morphological agreement, either between the subject and the verb or within a noun phrase, is a frequent problem in a morphologically rich language such as German. However, there are other overgeneralisations that our grammar makes. For instance, consider Fig. 5, in which the word order of the second clause is wrong because it is analysed as a relative clause, which has verb-last word order in German, rather than a coordination of two main clauses with verb-second word order. The analysis as a relative clause is obviously wrong because the clause does not start with a relative pronoun, but with the nominal subject die Tragödie (Engl: the tragedy). However, the label SUBJ does not specify whether the subject is relative or not, and a rule of the form REL→, SUBJAUXVAFIN is perfectly consistent with the training data.Even though the errors in Figs. 4 and 5 affect different linguistic phenomena, namely morphological agreement and word order, they share the same root cause: the SCFG assumption that subderivations with the same head symbol are equivalent, and that decoding can be decomposed into rule-local feature functions. Depending on the language and syntactic annotation, other structures will be affected by this assumption.Our aim is to reduce the number of errors stemming from the independence assumption made during SCFG decoding. One potential solution is to increase the granularity of the non-terminal symbol set to introduce new rule derivation constraints. However, increasing the granularity of the non-terminal set, e.g. by enriching the labels with morphological information, can impose too many restrictions on decoding, and prevent valid generalisations, especially in a language such as German which is syncretic, i.e. where multiple morphological analyses share the same word form. For example, we typically want to enforce case, number and gender agreement within noun phrases, but because adjective inflection does not depend on gender in the plural, naively enriching the label set of noun and noun attribute non-terminals with the full set of morphological information would also prevent correct derivations.We only perform minimal modifications to the original ParZu label set. Firstly, its dependency grammar does not analyse brackets and punctuation marks, and gives them the label ROOT. The same label is used for the verbal root of a sentence, and any unattached structures that could not be fully parsed. We split the ROOT label in the treebank into five categories: brackets, commas, sentence-final punctuation marks (all easily identified by the pre-terminal labels), verbal roots of main clauses (VROOT) and other tree fragments (ROOT).A second enrichment that we perform is concerned with coordinated elements, which are all given the label KON, or CJ for the last element, by ParZu. This is problematic because KON and CJ are overgeneral, being used for noun phrases, verb phrases, prepositional phrases, adverbs, and others. Instead, we copy the label of the coordination head to each conjoined element, and make the conjoined elements dependent on the preceding coordinating conjunction or comma, if they are not already. The label of subtrees headed by coordinating conjunctions is concatenated with the label of their head. This allows the model to learn generalisations such as:OBJA→NNKON_OBJAKON_OBJA→undOBJAThe original and the modified annotation of a coordination are illustrated in Fig. 6.Thirdly, we distinguish between prenominal and postnominal genitive modifiers. Since prenominal genitive modifiers are typically named entities, as in Peters Vorschlag (Engl: Peter's proposal), and postnominal ones noun phrases with an article, as in der Vorschlag des Präsidenten (Engl: the proposal of the president), separating the two types of genitive modifiers puts more constraints on word order during decoding.German has a rich inflectional morphology, which marks grammatical features, like gender and case, on determiners, adjectives, noun, and verbs. Much of German's morphosyntax can be understood in terms of feature agreement and case government. For instance, the number and person features of a finite verb should agree with those of the subject; the case of a prepositional phrase is governed by the choice of preposition.The production of inflection that is coordinated over multiple target words poses a problem since the words that bear the inflectional markers may be produced by the application of independent translation rules. Typically, the n-gram language model is the only means of enforcing consistency and this may be inadequate for longer-range agreement or for n-grams that were not seen during training.Whilst morphological features could in principle be encoded in the non-terminal labels (for example, using SUBJ-SG-3-F to indicate a singular, third person, feminine subject), the use of highly specific labels runs the risk of exacerbating problems of training data sparsity. We therefore follow Williams and Koehn (2011) and use a unification-based approach to enforcing agreement. During training, we pass the target-side terminal vocabulary through the Zmorge morphological analyser (Sennrich and Kunz, 2014) and use the analyses to extract a lexicon of feature structures. The lexicon associates each target surface form with a set of feature structures. For example, two entries for the definite article das and the noun Kätzchen (Engl: kitten) areSyncretism is common in German and many target words (including das and Kätzchen) have multiple morphological analyses and therefore multiple entries in the lexicon with different feature values.Our SCFG grammar rules are augmented with constraints: identities that require feature compatibility between feature structures. For example, in the following rule:SUBJ→dieADJANN〈SUBJinfl〉=〈dieinfl〉〈SUBJinfl〉=〈ADJAinfl〉〈SUBJinfl〉=〈NNinfl〉〈SUBJinfl case〉=nom〈SUBJinfl declension〉=weak〈diecat〉=ARTthe first three constraints ensure that the article die and the target words for the ADJA and NN subderivations have lexicon entries with inflection values that are compatible under unification. The fourth constraint requires nominative case. This constraint is based on the noun phrase label: subjects in German are indicated by the use of nominative case. The fifth constraint ensures that the inflection value of the ADJA is consistent with the weak declension paradigm (which is required for a noun phrase containing a definite article). The final constraint ensures that the lexicon entries considered for the terminal die are for the ART word class.During decoding, a hypothesis's constraints are evaluated after it is popped from the cube pruning queue. If the constraints succeed then the hypothesis is added to the beam; otherwise the hypothesis is discarded.We include constraints for the following phenomena:1.Agreement of determiners and adjectives with the noun they modify.Agreement of finite verbs with their subjects.Choice of adjectival declension paradigm based on presence and definiteness of determiner.Prepositional case government.Selection of noun phrase case according to grammatical function.Our constraint extraction algorithm is similar to that of Williams and Koehn (2011), but adapted for ParZu's parse tree style. It involves two steps: (i) the nodes of each training parse tree are grouped into sets according to their common membership of agreement and government relations; (ii) the GHKM rule extraction algorithm is extended to generate identities between terminals and non-terminals that belong to a common set.Additionally to noun phrase agreement, we model a number of subcategorisation phenomena relating to verbs and clauses. We do this through a feature function in the decoder that has access to the internal tree structure of each hypothesis, and hand-written rules that check if any of the following subcategorisation constraints are violated when hypotheses are combined into a new tree. These constraints need only be checked if a potentially overgeneral non-terminal is being expanded, so we do not need to check the full hypothesis tree for constraint violations. For instance, only if a non-terminal that is being expanded is the first constituent of a relative clause do we check if it contains a relative pronoun. If a constraint violation is found, we add a cost that is sufficiently high to push the hypothesis to the bottom of the hypothesis stack. We define the following constraints:1.Relative clauses must contain a relative (or interrogative) pronoun in their first constituent (ignoring commas).Modal verbs subcategorise for an infinitive, the auxiliary verbs haben (Engl: have) and sein (Engl: be) subcategorise for a past participle, or an infinitive clause (zu + infinitive).The past participle of some verbs, mostly intransitive verbs that describe a movement or change of state, must be formed with sein (Engl: be) instead of haben, and cannot be passivised (except with the impersonal subject es). An example is gestorben, the past participle of English die.Passive clauses, identified by the auxiliary verb werden or sein dominating a past participle, cannot subcategorise for an accusative object.22This rule may misidentify the past perfect of the above-mentioned verbs of movement or change of state as passive forms. Since these verbs are all intransitive, disallowing accusative objects is still valid.Most subordinating conjunctions subcategorise for a finite verb, while um and ohne subcategorise for an infinitive clause. Since the treebank label for conjunctions (KONJ) is ambiguous, this constraint enforces the correct subcategorisation.For verbs whose past participle is formed with sein, we use a list extracted from Wiktionary, manually corrected, with 260 past participle forms. All other rules are on the level of non-terminal labels, with the exception of full form lists to disambiguate the auxiliary verbs haben, werden and sein, which share the pre-terminal label VAFIN, but which we need to distinguish for the rules.Compositional morphology in German is productive, and a translation process that treats word forms as atomic elements is unable to account for this productive generation of new word forms. As a result, the translation of compounds may be ill-formed if they have not been observed during training. In contrast to the problems discussed previously, which were problems of overgeneralisation, this is an example where our baseline string-to-tree system shows a lack of generalisation.At best, the failure to productively produce new compounds results in translations that remain comprehensible, e.g. producing kulturelle Experten instead of the compound Kulturexperten for the English phrase cultural experts. At worst, the meaning becomes distorted, as in the example sentence in Table 1, where city State, which corresponds to the German compound Stadtstaat, is instead translated as Stadt des Staates (Engl: city of the state). The translation is grammatically correct, but inaccurate. This example illustrates that enforcing grammaticality is a necessary, but not a sufficient condition for a successful translation. The model also needs to be powerful enough to generate the correct translation.We perform compound splitting for nouns, based on a hybrid approach described by Fritzinger and Fraser (2010), using a finite-state morphology to identify compound boundaries, and frequency statistics from the corpus to choose the most probable split (Koehn and Knight, 2003). Since we perform compound splitting on the target side, we need to represent the split compounds in a way that facilitates compound merging in post-processing. For this reason, we explicitly add the junctures that join compound segments to the split representations, or a special null token if there is no juncture. We use the Zmorge morphology for German (Sennrich and Kunz, 2014), which is a variant of SMOR (Schmid et al., 2004) with an open lexicon. One advantage of Zmorge over SMOR is that Zmorge's analysis retains and marks junctures that join compound segments. We mark all junctures, including null junctures, with special characters to identify them during compound merging.Additionally, we represent compounds as a syntactic tree with the new pre-terminal symbols SEGMENT for noun stems and JUNC for junctures, and the non-terminal symbol COMP for compound modifiers. Minimally, compounding can be modelled with two non-terminal rules:NN→COMPSEGMENTCOMP→(COMP)SEGMENTJUNCThe second rule is recursive to allow the production of compounds with more than two segments.While target-side identification and merging of compounds is challenging with phrase-based SMT due to its distortion model (see Stymne and Cancedda, 2011), this tree representation ensures that a (possibly empty) juncture will always be surrounded by two segments that can be merged, and that a reordering of elements is only possible if licensed by the grammar. Thus, compound merging simply consists of removing the whitespace (and special characters) around junctures. The representation of German Sonnensystem (Engl: solar system) before and after compound splitting is shown in Fig. 7.A complicating factor with compound splitting is that the language model is unsuited to choose the right inflection for articles and adjectives, since the gender of the compound is determined by its last segment. We thus extend the morphological constraints discussed in Section 5.2 to split compounds by projecting the morphological feature structure of the final segment of the compound to the full compound, ignoring the morphology of compound modifiers.

@&#CONCLUSIONS@&#
Out-of-the box parsers and treebanks do not necessarily provide suitable syntactic annotation for string-to-tree statistical machine translation. Focusing on German as a target language, we have discussed various ways in which string-to-tree translation models overgeneralise. Earlier research has focused more on how the syntactic representation overconstrains rule extraction and decoding in syntax-based translation. This paper highlights another important aspect, namely the fact that grammars learned from treebanks may impose too few constraints on tree derivation, and thus fail to avoid ill-formed output.Linguistically, most of the grammatical errors that we identified are morphological, as is typical for translation into morphologically rich languages, but we have also identified overgeneralisations that cause word order errors.We have discussed how the syntactic representation underlying a string-to-tree model affects translation quality, and have shown novel techniques to introduce additional linguistic knowledge into a string-to-tree translation system, including morphological agreement and subcategorisation constraints for noun phrases and verbs, a syntactic representation for target-side compound splitting and merging, and discriminative learning of labels for unknown words. We submitted the system described in this article to the ACL 2014 Ninth Workshop on Statistical Machine Translation, where it was ranked 1–2 (out of 18) and significantly outperformed state-of-the-art phrase-based systems.As future work, we plan to further refine our syntactic constraints for phenomena such as coordinations. Furthermore, we could extend our work by adding more subcategorisation constraints, for instance specifically modelling possible subcategorisation frames of full verbs. Grammatical well-formedness is a necessary, but not a sufficient condition of a good translation. Thus, we also plan to increase the expressive power of our models by learning to produce inflectional forms that do not occur in the training corpus.