@&#MAIN-TITLE@&#
Bayesian sparse solutions to linear inverse problems with non-stationary noise with Student-t priors

@&#HIGHLIGHTS@&#
Bayesian sparsity enforcing inference for linear inverse problems.Non-stationary noise modelled via Gaussian with unknown varying variances.A generalized Student-t prior model for enforcing sparsity.Application in sparse signal deconvolution.Application in periodic components estimation in biological time series.

@&#KEYPHRASES@&#
Bayesian sparsity enforcing,Variational Bayesian Approximation (VBA),Student-t,Deconvolution,Periodic components estimation,Biological time series processing,

@&#ABSTRACT@&#
Graphical abstract

@&#INTRODUCTION@&#
Many linear inverse problems such as signal deconvolution, image restoration, Computed Tomography (CT) image reconstruction, Fourier Synthesis (FS) inversion can be modelled aswhererepresents the unknown quantity: input signal(t)or original imagef(x,y);represents the measured data: output signal(t), blurred imageg(x,y)or projectionsgϕ(r)in CT;Hrepresents the forward matrix operator obtained from: the Impulse Response Function (IRF) of the measurement systemh(t), the Point Spread Function (PSF) of the imaging systemh(x,y)or the geometry of detector-object-detectors in CT; and finally,ϵrepresents the errors [1–6].When the systemHis known and we know, computing or predictingis the forward problem. When the inputand the outputare known, determiningHis called identification. WhenHis known andis given, estimatingis called inversion and the problem is called inverse problem. WhenHis partially known, for example when the IRF in signal deconvolution or the PSF in image restoration depend on some unknown parameters, we have myopic or blind deconvolution problems.Between the classical inverse problems arising in signal and image processing, we mention here a few examples:– Deconvolution:where, when discretized, we obtain the equation (1) and where: the vectorcontains the samples of(t); the vectorsandϵcontain the samples of(t)andϵ(t)and theHis equivalent to the convolution operationh(t)⁎(t). In this case,His a Toeplitz matrix which is entirely defined from the samples of the impulse responseh(t)denoted by the vectorh.– Image restoration:whereh(x,y)represents the Point Spread Function (PSF) of the imaging system [1–4,7,5,6]. Here too, when discretized, we obtain the equation (1) with:is a vector containing all the pixels of the image(x,y)scanned column by column;andϵcontain the pixels of(x,y)andϵ(x,y)andHis a Toeplitz-Bloc-Toeplitz (TBT) matrix which is entirely defined from the pixels of the PSFh(x,y).– X ray Computed Tomography (CT) image reconstruction:where(r,ϕ)is the projection at angle ϕ andf(x,y)represents the image to reconstruct [8–14]. Here too, when discretized, we obtain the equation (1) with:representing the pixels of the object(x,y)scanned column by column as in the previous example;contains the samples of projections(r,ϕ)organized row by row for different angles and called sinogram andHis a very sparse matrix which is entirely defined from the geometry of the tomographic system. In a simple straight line model, the elementsHijis the length of the intersection of the ray i in the pixel j.– Inverse Fourier series signal modelling problem:where(t)represents a time series with periodic components(ν). Here again, in the discretized version of equation (1):represents values off(ν)for different frequencies,represents(t)andHis the Discrete Fourier Transform (DFT) matrix.– Fourier Synthesis inverse problem:where(u,v)is the 2D Fourier Transform (FT) of the objectionf(x,y). Here again, in the discretized version of this equation,representsf(x,y),represents(u,v)andHis the 2D DFT matrix [15].Many other examples can be given in Microwave imaging [16,17], Ultrasound echography, Seismic imaging, Radio astronomy [18] Fluorescence imaging [100], Inverse scattering [19–22], Eddy current non-destructive testing [23], SAR imaging [24] etc.In all these examples, the common inverse problem is to estimatefrom the observations of. In general, the inverse problems are ill-posed [25]. This means that, in practice, the dataalone is not sufficient to define an unique and satisfactory solution. Regularization theory and the Bayesian inversion have been successfully used for this task. See for example [33,35,36,48] for quadratic and Tikhonov regularization, [27,42,47] for Total variation, [28–30,34] for different entropy based regularization, [32,40] forLpand sparsity enforcing, [37–39,43] for blind deconvolution and applications, [31,41] for Cross Validation (CV) and generalized CV methods for determining the regularization parameter, and [26,54] for Bernoulli–Gaussian models, [44] for Compress Sensing approach, [45,46] for multichannel blind deconvolution, [49,50] for nonlinear and space variant PSF, [51,52] for document image restoration, [53] for joint restoration and segmentation.In this paper, first we consider the linear inverse problem=H+ϵwhere we know that the noiseϵ=[ϵ1,⋯,ϵM]′is non-stationary and that the input is sparse. Accounting for sparsity has been considered in many ways. The first one is byL0orL1regularization methods [55,57,56,58,59]. One of these methods which has become now the standard is LASSO [60]. The second way is via Bayesian inference using strict sparsity or sparsity enforcing priors. For the strict sparsity requirement, very often Bernoulli distribution is used: For example, Bernoulli–Gaussian [26,54,72], Bernoulli–Laplace [95], Bernoulli–Gamma, etc. For the sparsity enforcing, mainly three categories of priors have been considered and used very often: Generalized Gaussian (GG), Mixture models and heavy tailed probability laws such as Student-t. See [60,61] and their references for a review of these priors.To account for the non-stationarity of the noise, a zero mean Gaussian model with unknown varying variance has been considered in [62] and a Cauchy–Gaussian model in [63]. To account for both of these two prior information, we propose to model the noise as a zero mean non-stationary Gaussian with unknown variances{,i=1,⋯,M}, on which we assign Inverse Gamma priors and to enforce the sparsity, we propose to use the Student-t prior which is a heavy tailed probability law. The main advantage of Student-t is that, thanks to its Infinite Gaussian Scaled Mixture Model (IGSM) property, it can be used in a hierarchical Gaussian–Gamma model. In this way, in fact both for the non-stationarity of the noise and for sparsity enforcing we have the same prior model structure: Gaussian with unknown variances on which we assign Inverse Gamma priors. In our knowledge, this combination is new and first communicated in a conference by the first author [101,102]. However, the Bayesian framework with different priors both on the noise and on the solution goes back to 1950 with Gaussian [64] or Poisson [65] for the noise and Gaussian for the solution. But more specific priors and in particular the Markovian model [66,67], Non Gaussian priors [16] the hierarchical models [68] are more recent. The main difficulties in these methods have been more on the computational aspects. Beside the classical Gaussian approximation [69] and the MCMC methods [70–72], we may mention the more recent ones: the Approximate Bayesian Computation (ABC) [73–76], Variational Bayesian Approximation (VBA) [77,78] and Message Passing (MP) [79–82] methods.The rest of this paper is organized as follows: In the next section, the details of the above mentioned prior laws are given and the expression of the joint posterior law of all the unknowns is obtained. Then, successively are presented the Joint Maximum A posteriori (JMAP) and the Variational Bayesian Approximation (VBA) methods and algorithms. A comparison of their computational costs is also given. Some discussions on their theoretical and practical implementations are presented. In the simulation section, we show some results in deconvolution of sparse signals and another application of the proposed method in a study related to biological dynamic circadian cycle studies where the observed time series are modelled by an expansion of very limited number of periodic components. Finally, some conclusions are presented in the last section.Starting from=H+ϵ, whereis a vector of the length M andis a vector of length N, to account for possible non-stationarity of the noise, we propose to use:where=[ϵ1,⋯,ϵM]′contain the unknown variances of the non-stationary noise. To be able to estimate them, we assign an Inverse Gamma conjugate prior on:From this, we can define the expression of the likelihood:To account for the sparsity, as mentioned in the previous section, we use the Student-t model:whereThanks to the Infinite Gaussian Scaled Mixture (IGSM) property of this probability law:we propose to use the following hierarchical modelandwhere=[f1,⋯,fN]′.RemarkWith this hierarchical model, we have a kind of generalization of the Student-t, called since after IGSM, which is now defined with two parameters:The mean value of this probability distribution is evidently zero and its variance is given by(16)Var{X}=∫x2S(x|α,β)dx=2β[Γ(α+1/2)Γ(α−1)Γ(α)Γ(α−1/2)−1].Forα=β=ν/2we obtain the variance of classical Student-t which isνν−2. This remark takes its importance when we need to fix the two parameters(α,β)for the initialization of the algorithms. In particular, to ensure the sparsity of the solution, we need to have a probability distribution which is concentrated enough around the zero and has enough heavy tails for the range of the variation of the quantity of interest. In general, we want to have small values for the variancesvj=1/ujwhenfjis zero and whenfjhas high value, we need to reach high values ofvjwhich means that the parameters of the Inverse Gamma model ofvjhave to be fixed in such a way that its mean is close to zero and its variance in accordance to the dynamic of the variation of desired solutionfj. So, to fix a priori the parameters, we propose to choose(17){α=2+ζ2β=v0ζ(1+ζ2)which gives(18){E{vj}=βα−1=v0ζVar{vj}=β2(α−1)2(α−2)=v0where ζ is a small positive value to insureα>2and also insures small value forE{vj}and the desired variancev0forvj.The Fig. 1presents the shape of this model for different values of the two parameters(α,β)compared to the normal distribution.The global generative model described via the equations (1), (7), (8), (9), (13) and (14) is illustrated graphically in the Fig. 2.Using these prior laws, the joint posterior law of all the unknowns becomesFrom this point, at least two directions can be followed: first one is the JMAP solution:whereThe second is the Variational Bayesian Approximation (VBA) which mainly consists in first approximatingp(,,|)by a separable probability law, for exampleq(,,)=q1()q2()q3(), and then, to use this for defining any estimators, for,forandfor. In recent years, there was extensive works on VBA in Machine Learning community [83–87] and in general [88–92]. However, very few works have been done for inverse problems [93–97,103].In the following sections, we give details of these methods.The criterion to be optimized is:which, using the above mentioned priors becomes:which can also be written as:One of the basic optimization algorithm for this optimization problem is an alternate optimization with respect to each of the arguments which is detailed in the following. In fact, whenandare fixed, the criterion as a function ofis a quadratic one:which has an analytical solution:where=diag[]and=diag[].Whenis fixed, the criterion is separable inand infjand we obtain easily the expressions of the minimizers by putting equal to zero its derivative with respect to eachorfj:andThese relations are summarized in the following algorithm:We may note that, in fact, the implementation of this algorithm does not need any matrix inversion because the computation of=(H′−1H+−1)−1H′−1in Step 1 can be done via the optimization of the following quadratic criterion:which can be done by any appropriate gradient based algorithm. This step is however very important in particular for high dimensional data. In this paper we do not focus much more on this point. In practice, we used either a steepest gradient descent or conjugate gradient algorithms.As we will see later, the main advantage of this approach is its low computational cost. The main drawback is in the fact that, at each iteration, the uncertainties associated to the output of each step are not accounted for. Also, theoretically, the JMAP estimation may not have all the necessary good characteristics of a Bayesian approach because it corresponds to the mode of the posterior. Theoretically, a better estimator is the Posterior Mean (PM). Its exact computation needs huge dimensional integration which has only an analytical solution in the Gaussian case. In general, its approximate estimate can be done by MCMC methods which are very intensive in cost. Variational Bayesian Approximation (VBA) methods are alternatives to MCMC which can theoretically give posterior mean estimates with lower computational costs than MCMC methods. The main steps of this approach are presented in the next subsection.VBA mainly consists in first approximatingp(,,|)by a separable probability law, for exampleq(,,)=q1()×q2()q3(), and then, to use this for defining any estimators, for,forandfor. The main steps to find q is to use the Kullback-Leibler divergenceKl(q:p)and to optimize it to find the expressions ofq1(),q2()andq3(). Using an alternate optimization technique, we obtain:To obtain the expressions ofq1(),q2()andq3(), we need to compute<ln⁡p(,,,)>with respect toq1,q2andq3. Looking at the expression ofJ(,,)=ln⁡p(,,,)in our case, we find very easily that if we chooseq1()to be Gaussian,q2()andq3()to be products of Inverse Gamma, during the iterations these families are conserved due to the conjugate properties of Gamma and Gaussian:We then need to find appropriate update relations between the parameters(μf,Σf),(αfj,βfj)and(αϵi,βϵi).The details of these steps are given in the Appendices A and B. The resulting algorithm is the following:We may note that, here, a costly step is the computation ofΣf=(H′−1H+−1)−1. However, we only need its diagonal elements for the computation ofin Step 3 and the computation ofin Step 2.If we can decomposeΣf=D′D, then(36)Tr{Σf}=‖D1‖2andTr{HΣfH′}=‖HD1‖2,where 1 is a vector of ones entries. This hint gives the possibility to compute quantities needed in different steps in this algorithm.Looking in the details on the two methods and as illustrated in the Fig. 3we see that:In JMAP, during the iterations only the values are transmitted between the different steps without accounting for the uncertainties.In VBA, during the iterations the probability laws are transmitted between the different steps thus accounting for uncertainties. In particular, we see that in steps 2 and 3, not only the value of (which is the expected value ofq()) is transmitted but also its covariance matrixΣf. This process has many similarities with message passing methods. However, the computation of this covariance matrix is the main extra cost of VBA with respect to JMAP.From the theoretically point of view, we cannot say too much about the convergence properties of these algorithms. The proposed JMAP is an alternate optimisation algorithm and so its convergence depends mainly on the convexity of the JMAP criterion with respect to all of its arguments. The proposed VBA is also an alternate optimisation algorithm in the space of probability density functions. However, in the space of parameters, as mentioned before, the fact that the covariance matrix ofis used in steps 2 and 3 makes a better theoretical property. A rigorous mathematical proof is not easy. However, in practice, both algorithms converge to a local minimum. The initialization is then important.For initialization, we need to choose appropriately the hyper-parameters(αϵ0,βϵ0)in such a way thatbe fixed according to a reasonable prior knowledge of the variance of the noise and its variability along the observation process and(αf0,βf0)in such a way that the corresponding prior (Student-t) be well concentrated around the zero with heavy tailed scaled to the dynamic of the seeking solution. These points are discussed more extensively in the second application of the proposed method for periodic components estimation.We implemented these two algorithms for many linear inverse problems. Here, we give two examples in signal processing which are the deconvolution and periodic components estimation of biological time series problem. In both cases, the main idea is first to simulate sparse inputsand then generate data=H+ϵwith different SNR and different realization of the noiseϵ. Then, applying different reconstruction algorithms, compute the estimateand compare with, for example using the following quantitative relative distances:δ2fis the Normalized Mean Square Error (NMSE) andδ1fis the Normalized Mean Absolute Error (NMAE). This simulation protocol is illustrated in Fig. 4.Also, when the original signalis sparse, we can try to obtain a sparse solution by thresholdingto be able, for example, to give the Missing Values (MV):and the False Alarms (FA):as a measure of performances for the algorithm.These performance measures can be computed in simulation, but for real applications these are not possible. One can then, compute=Hand compare withusing the following relative distances:δ2gis the Normalized Mean Square Residual Error (NMSRE) andδ1gis the Normalized Mean Absolute Residual Error (NMARE).The whole protocol of the forward simulation and inversion and possibilities of comparison are illustrated in the Fig. 4.In simulation, methods with which reach lower values for NMSE, NMAE as well as MV and FA are preferred. In real case, we cannot have access to these quantities. We may then reject the methods which give high values for NMSRE and NMARE. However, having very low values for these quantities (over fitting) does not forcibly mean that the method is good. We may use Cross validation methods in this case.In all these simulations, there are other quantities which are in general important to monitor:•The evolution of the different quantities such as,fjandduring the iterations;final values ofandf;final values ofμfanddiag[Σf]which can be interpreted as the posterior means and variances of the solution.In the following, we present two examples, one for a deconvolution and another for periodic components estimation of time series.For the deconvolution problem, we simulated an input(t), an Impulse Response Function (IRF)h(t)and computed the ideal output0(t)=h(t)⁎(t)on which we added a non-stationary noiseϵ(t)with slowly varying variancesto obtain the simulated data(t)=0(t)+ϵ(t). Fig. 5shows these signals.From these data, we applied the JMAP and VBA algorithms and compared the results. As a reference, the results obtained by Lasso is also given. In Fig. 6we can see one of such results.To show some other relative performances of the proposed methods compared to Lasso, which is the most concurrent in deterministic methods, we present the Normalized Mean Absolute Errors (NMAE) and Normalized Mean Square Errors (NMSE) betweenandand the Normalized Mean Absolute Residual Errors (NMARE) and Normalized Mean Square Residual Errors (NMSRE) betweenandas a function of SNR. These results are presented in Fig. 7.As we can see, the performances depend on the criterion. To show the main advantage of the proposed method, we simulated the case where the noise variance is changing during the measurement process. As the proposed method is designed to estimate the noise variance, the performances are surely better than the methods which cannot do that. This is shown in Fig. 8.For this non-stationary case, also, the Fig. 9shows the NMSE, NMAE, NMSRE, NMARE, MFA and MV as in Fig. 7 for the stationary case.We did many other extensive simulations comparing the performances of these two proposed algorithms compared to more classical regularization based methods and in particular withL2orL1regularization criteria. In general, the results withL2regularization are not sparse, but those withL1are as good as the proposed method. However, in regularization methods, the results depend on the regularization parameter. Even if there are methods based on cross validation which can give a good value to use, but there is no easy way to measure the remaining uncertainty of the computed solution.In the proposed Bayesian method, not only we have an unsupervised method, but also we have the possibilities to quantify the uncertainties, for example to put error bar on the solution. A typical result obtained by VBA is given in Fig. 10. In this figure, the results are shown with error bars using the estimated diagonal elements of the posterior covariance matrixΣˆ. First row shows the results for the stationary noise case and the second row for the non-stationary noise case.As a final conclusion for these simulations, we see the superiority of the Bayesian approach, mainly in three main points:i)the possibility to easily account for the non-stationarity of the noise;the possibility of estimating noise variances;the possibility of putting error bars on the solution.For the periodic components estimation of biological time series, here we show some simulation results modelling time series issued in cancer treatment experiments in the chronobiological context. The time series considered represents the photon absorption of mice inoculated with cancer and due to the tumour growth the time series presents some particularities: short length (10 days) and increasing trend. The challenge is to be able to decide the stability or instability of the periodic components, having as a prior information the sparse structure of the periodic component vector, i.e. the reduced number of clocks expressed. Since the circadian period is defined at 24 h, in order to analyse the stability or instability of the periodic components, we need a method that can analyse very short signals relative to the circadian period (4 days signals) and offer informations of the periods in a specific range with a precision of one hour. More precisely, for such signals we want informations concerning the periods inside the interval [8–32] (which represents the circadian domain plus the corresponding harmonics.)whereg(tm)represents the observed value at timetm,pnrepresents the n-th periodic component andϵmaccounts for errors, uncertainties as well as the measurement noise. With the notationg(tm)=gmandf(pn)=fn, defining the vectors=[f1,f2,…,fN]T,=[g1,g2,…,gM]Tandϵ=[ϵ1,ϵ2,…,ϵM]Twe obtain the following model:where the elements of the matrixHmn=ej2π1pntm. We may note that, as the data are real valued,ncan be considered as a complex numberj=nR+jnIand we can also write the forward model asBefore going to the numerical experimentation, a complementary point for applying the proposed method for this case is that we assume the variances of the real partsRand imaginary partIare the same:and that they are only conditionally independentNote however that this does not mean that a priori they are independent, because they are conditionally independent.For validating the proposed method, in a first step, we generated synthetic data. In the real case the theoreticalis unknown, so the only possible comparison is between the available(representing the real data) and the estimated=H(obtained via the reconstruction done over the estimated). An important step for validating the method is to consider signals with known corresponding periodic components vector, which gives the possibility to compareand the estimated. We consider the following protocol:(a) Generate a sparse amplitude periodic components vector, viaRandI, Figs. 11(a) and (b). For the simulations used in this article, we analysed a periodic components vector for the interval associated with the circadian domain and the most important corresponding harmonics, i.e. the interval[8,32], with one hour precision [99,98].(b) Generate the corresponding signal0=Hcalled the theoretical output (representing 4 days = 96 hours length, simulating a body temperature measurement or a gene expression measurement or a rest-activity pattern measurement. In the analysis of the real data, we will present simulations corresponding to the last two, i.e. gene expression and rest-activity pattern measurements), Fig. 11 (c).(c) Add some noise=0+ϵto generate the data for the inversion step. For the first simulation presented, we added a Gaussian noise, corresponding toSNR=05dB, Fig. 11 (d). We include during this simulation section the detailed cases of levels of noise corresponding toSNR=10dB,SNR=15dB,SNR=20dB, comparing the proposed method with the FFT method and also with LASSO. Also, the comparative study between the proposed method and LASSO depending on the SNR is presented, considering levels of noise between 05 dB and 40 dB.(d) We then applied the proposed method and compared the estimatedR,Iandwith the originalR,Iand, Figs. 12(a), (c) and (e). The proposed method also indicates the variances corresponding to each estimated period from the PC vectorsRandI. We note that via the proposed algorithm, after we applied VBA we concluded thatis modelled by a multivariate Normal distribution, and the corresponding covariance matrix was estimated,Σˆ. We also compare the original signaland the theoretical output0with the reconstructed one, Figs. 12 (b) and (d). A comparison between the estimated noise the original one is presented in Fig. 12 (f).As a comparison, we decided to compare the performances of the proposed method with two classical methods: the classical FFT which is today the standard technique used in the chronobiological community and the Lasso which is now the standard L1 regularization, so a good candidate for a method enforcing the sparsity.Fig. 13presents a comparison between the proposed method (d), Lasso (c) and FFT (b). The FFT is not offering the wanted precision, while the estimation corresponding to Lasso is far for being precise. In particular, the number of false alarms is 7, and the most important peak in the PC vector, corresponding to 23 hours is estimated as zero. For the proposed method, the estimated PC vector is a sparse one, and the only peaks estimated are the three ones that appear in the theoretical PC. In terms of the error, the normalized MSE corresponding to the proposed method isNMSE=0.03, while for LASSO isNMSE=0.69.Fig. 14presents a comparison between the proposed method (d), Lasso (c) and FFT (b), corresponding to a noise levelSNR=10dB. We note that for the LASSO estimation, the number of false alarms is 6, while the dominant period of the PC vector, i.e. 23 hours, is inaccurate.Fig. 15presents a comparison between the proposed method (d), Lasso (c) and FFT (b), corresponding to a noise levelSNR=15dB. We note that for the LASSO estimation, the number of false alarms is 3, while the dominant period of the PC vector, i.e. 23 hours, is inaccurate. This is also the case of the simulations corresponding toSNR=20dB, Fig. 16.The behaviour of the proposed method compared to LASSO for different levels of noise is presented in Fig. 17. We have considered levels of noise corresponding toSNR={05,10,15,20,30,40}dB. We compare the estimations corresponding to the two methods forI,Randusing as measurements of the errors the NMSE Figs. 17 (b), (d), (f) and MAE Figs. 17 (a), (c), (e).The same analysis is presented forand0in Fig. 18.We may note that, for typical data we have, i.e.;tn=nΔt,n=1,⋯,NwithΔt=1hour,N=96,pm=8:32hours, the matrixHwhich is of size96×25is very ill-conditioned. This is due to the fact that, the successive rows of the matrix are very close to each other. In DFT matrix, this is not the case, because the spacing are linear in frequencies but not in periods. Here, we are looking for a precision of 1 hour in period estimation between 8 and 32 hours.The final step is applying the proposed method on real data. As mentioned, since the standard method in chronobiology today is FFT, we include the corresponding FFT results for comparison. The detailed biological explanation of the experiments are given in [98,99]. From the signal processing point of view, the main objective is the estimation of a few periodic components of a signal observed for a very short period of time relative to the prior knowledge of the circadian period (∼24 hours) to study their evolution during the days with a precision of one hour. Knowing that this precision cannot be obtained with FFT based methods when observing the signal on the intervals of 4 days, we developed the proposed method.For the results corresponding to real data, we consider data obtained in experiments in chronobiology for cancer treatment. The particular experiment presented is realized over mice, investigating the locomotor activity (rest-activity patterns) of KI/KI Per2::luc mouse, aged 10 weeks, singly housed in RT-BIO and synchronized with LD-12:12 (i.e. 12 hours of light, followed by 12 hours of dark, Light–Dark, LD). The particular signal considered represent the locomotor activity of the mouse, which is known to be rhythmic. After the LD part of the signal, the mouse is kept in total darkness (Dark–Dark, DD) for 3 days, corresponding to the before treatment part of the signal and then D-luciferin is loaded in subcutaneous implanted Alzet pump [90 mg/ml], recording for 5 days the Activity signal corresponding to the during treatment part of the signal. The last two days represents the during treatment part of the signal. During the DD segment, the locomotor activity might be perturbed, due to the absence of the Light-Day regime and due to the treatment effects. Fig. 20 (a) presents the raw data corresponding to the activity signal. The figure indicates the four segments of interest: the LD segment, the DD-before treatment segment, the DD-during segment and the DD-after treatment segment. Each segment is analysed using the proposed method and the FFT method, the standard method in chronobiology today. The raw data signal is sampled every minute. For the four segments studied, we consider mean-zero signals, normalized between [−10:10] and sampled every hour. Fig. 19presents the raw data corresponding to the rest-activity pattern, Activity signal considered. The four segments of interest are presented in Figs. 20(a), (b), (c) and (d). As mentioned, the context is the analysis of short signals relative to the prior knowledge of the ∼24 hours. We note that for the experiment discussed here we need to analyse signals having a length of 3 days (Fig. 20 (c), corresponding to the before treatment segment) or 2 days (Fig. 20 (e), corresponding to the after treatment segment)For the LD segment, 8 days are available. The PC vector corresponding to the LD segment is estimated using the Proposed Method and FFT, Fig. 21:The dominant period is estimated at 24 hours, via FFT. Moreover, the PC vector is difficult to be interpreted by the biologist, since the vector is not sparse and the peaks corresponding to the biological phenomena cannot be distinguished from the peaks due to the noise. Via the Proposed method, the PC vector is sparse, estimating only two non-zero peaks, and setting the dominant period at 25 hours. For analysing the stability of the dominant period we consider 4-days length signals (windows) from the available signal, with a shift of one day and compute the PC via FFT and the Proposed method. The comparison is presented in Fig. 22:Via FFT, all four windows considered present a 24 hours dominant periodicity, Figs. 22 (c), (f), (i) and (l), leading to the conclusion that the rest-activity patterns are stable during the LD segment. Via the proposed method a variability of the dominant period is detected: for the first and fourth windows the dominant period is 25 hours while for the second and third windows the dominant period is 24 hours. The representation of the results corresponding to the two methods is presented in Fig. 23.The lines of matrices represents the PC vector corresponding to different windows. The colourbar indicates the numerical values of the amplitudes. The figure shows the stability and the variability detected by the FFT method, respectively the Proposed Method. We also note the sparse structure of PC vectors estimated via the Proposed Method. For the DD-before treatment segment, only 3 days of data are available. The PC vector is estimated using FFT and Proposed Method, Fig. 24. In particular, the only two non-zero peaks that are present in the PC vector estimated via the Proposed Method correspond to two highest peaks from the PC vector estimated via FFT. For the DD-during treatment segment 5 days are available, Fig. 25. For analysing the stability of the dominant period we consider 4-days length signals (windows) from the available signal, with a shift of one day and compute the PC via FFT and the Proposed method. The comparison is presented in Fig. 26.Via the Proposed Method, a variability is detected for the dominant period, while via the FFT method this variability is not detected. The representation of the results corresponding to the two methods is presented in Fig. 27.For the DD-after treatment segment, 2-days length data are available. We compare the Proposed Method with the FFT method in Fig. 28. The circadian rhythm is perturbed during this phase. The result corresponding to the Proposed Method is consistent with the result obtained via FFT, the only three peaks associated with biological phenomena being the 3 highest peaks present in the PC vector estimated via FFT. All the other non-zero peaks that are present in the FFT estimated PC vector are associated with the noise in the Proposed Method estimation.

@&#CONCLUSIONS@&#
In this paper, we considered linear inverse problems and proposed appropriate prior models to account for non-stationarity of the errors (forward modelling and measurement noise) and for sparsity enforcing of the input. A generalized Student-t probability density function and its IGSM equivalence are used for both. Even if Student-t has been used previously for sparsity enforcing, its use for non-stationarity is new. Using these prior laws, we obtained the expression of the joint posterior law of all the unknowns of the problem and proposed two main estimators: JMAP using an alternate optimization algorithm and the Posterior Mean computed via appropriate VBA method. Even if the MCMC methods are the standard methods for Bayesian computation, we prefer to use VBA which is more effective for practical applications. The advantages of VBA compared to MCMC methods have been noticed by many authors [79,93,94,96,91,21,103,101,102]. Indeed, it is crucial to consider the implementation issues, in particular, for great dimensional problems in imaging science such as image deconvolution or 3D computed tomography. However, in this paper, to show some of the performances of the proposed methods, we considered two signal processing inverse problems: Impulse input deconvolution and periodic components estimation in biological time series.In the presented chrono-biological application, we have showed the importance of the precision in the estimation of the periodic component around 24 hours. For the FFT based method which is the standard one in the chrono-biology community, the result has not enough resolution to see small changes (a few hours) in the period during the days. The results obtained by the proposed method could show these variations which are very important to biological studies.To summarize, here we give the highlights of this paper:•Bayesian inference for linear inverse problemsNon-stationary noise is modelled via Gaussian with unknown varying variancesA generalized Student-t prior model is proposed for enforcing sparsityDetails of two Bayesian algorithms: JMAP and VBA are presented and their performances are compared to the most concurrent method which is Lasso in two applications: parse signal deconvolution and in periodic components estimation in biological time series.In the deconvolution, we only compared the performances in simulation, but in the second application, not only we showed the performances in simulation but also for real data obtained in a chrono-biological experiments for cancer research.•With respect to:With respect to,i∈{1,2,…,N}:With respect to,j∈{1,2,…,M}:The analytical expression of the logarithm:•Expression ofq1():_The proportionality relation concerningq1()refers to, so in the expression ofln⁡p(,,|)all the terms free ofcan be regarded as constants:leading to:Considering the notations corresponding toand denoting the i-th line of the matrixwith,i∈{1,2,…,N}, we write:so the norm is written:Introducing the notations:we can write:Introducing the notationwe can write:Finally from (B.2), (B.6) and (B.8), for the expression of the logarithm〈ln⁡p(,,|)〉q2()q3()we have:and via the first proportionality and the notation:the probabilityq1()can be expressed by the following proportionality:The criterionJ()introduced in equation (B.10) is quadratic in. Equation (B.11) establish a proportionality relation betweenq1()and an exponential function having as argument a quadratic criterion. This leads to the followingThe probability distribution functionq1()is a multivariate Normal Distribution.Of course, the mean is given by the solution that minimize the criterionJ()i.e. the solution of the equation∂J()∂=0(and in particular, this is the same criterion that arrived in the MAP estimation technique for, with some the formal differences):The corresponding covariance matrix is computed by identification. On one hand we have the following relation:One the other hand, we have the following proportionality, given by equation (B.11):So, the covariance matrixΣˆmust respect the following relation:where the sign ≡ represents a equality between the two terms until a free-term. If we consider the covariance matrixwe have the following equalities:where we have used the equalityTT=T, as a consequence of the fact that one term is the transpose of the other and the term is a scalar. We usedΣˆ=ΣˆTandT(T+)−1Twas viewed as a constant C. We also have the following equalities:Equations (B.17) and (B.18) shows that equality imposed in (B.15) is verified with the covariance matrix defined as in (B.16). So, for the Normal distributionN(|,Σˆ)proportional toq1()we have the following parameters:•Expression ofq2i()_The proportionality relation concerningq2i()refers toso in the expression ofln⁡p(,,|)all the terms free ofcan be regarded as constants:For the first integral, it is trivial to verify:For the second integral, we have the following development:where we have introduced the following notations:Again, using the fact thatq1()is a multivariate Normal Distribution we have:and considering as constants all terms free ofwe have:andwhereis the line i of the matrix, so we can conclude:From (B.20) via (B.21) and (B.27) we get:from which we can establish the proportionality corresponding toq2i():Equation (B.28) leads to followingThe probability distribution functionq3i()is an Inverse Gamma Distribution, with the parametersαϵiandβϵi:We can write:•Expression ofq3j()_The proportionality relation concerningq3j()refers to, so in the expression ofln⁡p(,,,|)all the terms free ofcan be regarded as constants. Considering allfree terms as constants it is easy to verify:For the second integral:where we have introduced the notations:Considering the fact thatq1()was established a multivariate Normal Distribution, we have:Via (B.30) and (B.33) we get:from which we can establish the proportionality corresponding toq4():Equation (B.35) leads to the followingThe probability distribution functionq4()is an Inverse Gamma Distribution, with the parametersαfjandβfj:Expressions (B.19), (B.29) and (B.36) resumes the distributions families and the corresponding parameters forq1(),q2i(),i∈{1,2,…,N}andq3j(),j∈{1,2,…,M}. However, the parameters corresponding to the multivariate Normal distribution are expressed viaand(and by extension all elements forming the three matrices,i∈{1,2,…,N}and,j∈{1,2,…,M}).•Computation of,:_For a Inverse Gamma Distribution with parameters α and β,IG(x|α,β), the following relation holds:〈x−1〉IG(x|α,β)=αβThe prove of the above relation is done by direct computation, using the analytical expression of the Inverse Gamma Distribution:〈x−1〉IG(x|α,β)=∫x−1βαΓ(α)x−α−1exp⁡{−βx}dx=βαΓ(α)Γ(α+1)βα+1∫βα+1Γ(α+1)x−(α+1)−1×exp⁡{−βx}dx=αβ∫IG(x|α+1,β)︸1dx=αβSinceq2i(),i∈{1,2,…,N}andq3j(),j∈{1,2,…,M}are Inverse Gamma Distributions, with parametersαϵiandβϵi,i∈{1,2,…,N}respectivelyαfjandβfj,j∈{1,2,…,M}we can express the expectanciesandvia the parameters of the two Inverse Gamma Distributions using the result above:Using the notation introduced in (B.5) and (B.7) we obtain:In equation (B.38) we have introduced other notations forand. All three values were expressed during the model via unknown expectancies, but in this point we arrive to expression that don't contain any more integrals to be computed. Therefore, the new notations represents the final expressions for the density functions q that depends only on numerical hyperparameters, set in the prior modelling.