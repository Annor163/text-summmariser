@&#MAIN-TITLE@&#
An efficient regularized K-nearest neighbor based weighted twin support vector regression

@&#HIGHLIGHTS@&#
Our RKNNWTSVR implements structural risk minimization principle by introducing extra regularization terms in each objective function.Our RKNNWTSVR cannot only help to alleviate overfitting issue and improve the generalization performance but also introduce invertibility in the dual formulation.The square of the 2-norm of the vector of slack variables is used in RKNNWTSVR to make the objective functions strongly convex.Four algorithms are designed to solve the proposed RKNNWTSVR.The solution reduces to solving just two systems of linear equations which makes our RKNNWTSVR extremely simple and efficient.No external optimizer is necessary for solving the RKNNWTSVR formulation.

@&#KEYPHRASES@&#
Machine learning,Support vector machines,Twin support vector machines,K,-nearest neighbor,Newton method,Smoothing techniques,

@&#ABSTRACT@&#
In general, pattern classification and regression tasks do not take into consideration the variation in the importance of the training samples. For twin support vector regression (TSVR), this implies that all the training samples play the same role on the bound functions. However, the number of close neighboring samples near to each training sample has an effect on the bound functions. In this paper, we formulate a regularized version of the KNN-based weighted twin support vector regression (KNNWTSVR) called RKNNWTSVR which is both efficient and effective. By introducing the regularization term and replacing 2-norm of slack variables instead of 1-norm, our RKNNWTSVR only needs to solve a simple system of linear equations with low computational cost, and at the same time, it improves the generalization performance. Particularly, we compare four implementations of RKNNWTSVR with existing approaches. Experimental results on several synthetic and benchmark datasets indicate that, comparing to SVR, WSVR, TSVR and KNNWTSVR, our proposed RKNNWTSVR has better generalization ability and requires less computational time.

@&#INTRODUCTION@&#
Based on the Vapnik–Chervonenkis (VC) dimensional theory and statistical learning theory [11,44], the support vector machine (SVM) is an extremely powerful kernel-based machine learning tool for pattern classification and regression problems. SVM has been successfully applied to a wide variety of applications [8,16,28,29,31]. To reduce the computational complexity of SVM, various algorithms have been proposed [5,9,34,35,40]. Specifically, weighted support vector regression (WSVR) [17] has been proposed recently by combining the weights to the slack variables in the objective function. This procedure directly shrinks the coefficient of each observation in the estimated functions, and thus it is widely used for minimizing the influence of outliers.Unlike the standard SVM, which uses a single hyperplane, some recently proposed approaches, such as generalized eigenvalue proximal support vector machine (GEPSVM) [27] and twin support vector machine (TWSVM) [20], use two hyperplanes. TWSVM seeks two non-parallel proximal hyperplanes such that each hyperplane is closest to one of the classes and as far as possible from the other class. The main difference between TWSVM and SVM is that TWSVM solves two smaller size quadratic programming problems (QPPs), whereas SVM solves one large QPP. Experimental results show the effectiveness of TWSVM over SVM and GEPSVM [20]. Inspired by the study of TWSVM, Peng [32] proposed an efficient regressor called twin support vector regression (TSVR), determined by a pair of ϵ-insensitive down- and up- bounds functions by solving two related SVM-type problems. This strategy solves two smaller size QPPs in TSVR instead of a single large QPP that makes learning faster over SVR. To reduce the computational complexity of TSVR, various algorithms have been proposed [2,3,7,10,38,43,45,46,48].TSVR assumes that all samples are equally weighted. This implies that whether they are important or not, they play the same role on the bound functions. However, different samples should have different effects on the bound functions. Inspired by this observation, Xu and Wang [46] proposed a novel weighted twin support vector regression (WTSVR), where different penalty coefficients are given to the samples that do not satisfy constraints depending on their positions. A point x is important if it owns a great number of nearest neighbors, whereas it is not important if it is an outlier. Motivated by this observation, Xu and Wang [45] proposed a novel K-nearest neighbor weighted twin support vector regression (KNNWTSVR) [45] to make full use of sample knowledge. Specifically, higher weights are given to the samples with more nearest neighbors. On the other hand, lesser weights are given to the samples with a smaller number of nearest neighbors. Successive overrelaxation (SOR) technique is applied to further enhance the computational speed.The main advantage of SVR is the implementation of the structural risk minimization principle. However, only empirical risk is considered in the primal problems of TSVR, WTSVR and KNNWTSVR. In addition, we note that the dual problems of TSVR, WTSVR and KNNWTSVR require the inverse of positive semi-definite matrices and thus may not be well-conditioned in some situations. To overcome this difficulty, a small regularization term is added, so that the matrix becomes positive-definite. Recently, Shao et al. [39] proposed ϵ-twin support vector regression (ϵ-TSVR) based on TSVR. It also constructs a pair of nonparallel ϵ-insensitive down- and up-bound functions for the unknown regressor. But the difference is that the structural risk minimization principle is implemented by adding the regularization term into the primal problems of TSVR. Again, SOR technique is applied to further enhance the computational speed. In fact, empirical results show that ϵ-TSVR is not only faster but also shows better generalization.In this paper, we propose reformulating the primal problems of KNNWTSVR by adding regularization terms in the objective functions. By doing this, our new formulation, called RKNNWTSVR, is not only singularity free but also is theoretically supported by the statistical learning theory [37,39,42]. In particular, the 1-norm of the vector of slack variable is replaced with 2-norm to make the objective functions strongly convex. These straightforward, but important changes, lead to a considerably simpler positive definite dual problems (see Eqs. (32) and (34)) with nonnegativity constraints only. Although the changes as indicated above are nonstandard, they have been used effectively in previous work [3,23,24,26,36,41,42]. In theory, the new formulation leads to a robust solution but loses the sparsity characteristic. However, the new formulation does not seem to have a detrimental effect, as can be seen in the experimental results of Section 4. By introducing the regularization term and replacing 1-norm of slack variables with 2-norm, our RKNNWTSVR only needs to solve simple systems of linear equations with low computational cost. At the same time, it improves the generalization performance.Motivated by [3,14,26], four algorithms are designed to solve the proposed RKNNWTSVR. First, a simple iterative algorithm is presented and its global convergence is established. Second, Newton’s method for an implicit Lagrangian formulation for RKNNWTSVR is presented. However, the objective functions contain non-smooth plus functions. Hence, based on Lee et al. [22] and Peng [33], two methods, using smooth function in place of the plus function followed by the Newton–Armijo algorithm, are proposed. Empirical results on several artificial and benchmark datasets show the effectiveness of our proposed formulation compared to SVR, WSVR, TSVR, and KNNWTSVR using multiple performance measures.In this work, all vectors are taken as column vectors. The inner product of two vectors x, y in the n-dimensional real space Rn, will be denoted by: xty, where xtis the transpose of x. Whenever x is orthogonal to y, we write x⊥y. Forx=(x1,x2,…,xn)t∈Rn,the plus functionx+is defined as:(x+)i=max{0,xi},wherei=1,2,…,n. Further, we define the step function x⋆ as:(x☆)i=1forxi>0,(x☆)i=0if xi< 0 and(x☆)i=0.5whenxi=0.The 2-norm of a vector x and a matrix Q will be denoted by ‖x‖ and ‖Q‖ respectively. For simplicity, we drop the 2 from ‖x‖2. The identity matrix of appropriate size is denoted by I and the column vector of ones of dimension m by e.The paper is organized as follows. Section 2 dwells briefly SVR, TSVR and KNNWTSVR. In Section 3, the proposed formulation and the four algorithms used for solving it are described. Numerical experiments are performed and their results are compared with other baseline methods in Section 4. Finally, we conclude our work in Section 5.

@&#CONCLUSIONS@&#
