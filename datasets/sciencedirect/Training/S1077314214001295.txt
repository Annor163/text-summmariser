@&#MAIN-TITLE@&#
SDART: An algorithm for discrete tomography from noisy projections

@&#HIGHLIGHTS@&#
We present the algorithm SDART for discrete tomography from noisy projection data.A set of soft constraints is introduced to replace hard constraints in DART.A constrained optimization approach is used to improve the segmented reconstruction.The results show that SDART is more accurate, compared to DART, for high noise levels.

@&#KEYPHRASES@&#
DART,Discrete tomography,Noise,Prior knowledge,Relaxation,

@&#ABSTRACT@&#
Computed tomography is a noninvasive technique for reconstructing an object from projection data. If the object consists of only a few materials, discrete tomography allows us to use prior knowledge of the gray values corresponding to these materials to improve the accuracy of the reconstruction. The Discrete Algebraic Reconstruction Technique (DART) is a reconstruction algorithm for discrete tomography. DART can result in accurate reconstructions, computed by iteratively refining the boundary of the object. However, this boundary update is not robust against noise and DART does not work well when confronted with high noise levels.In this paper we propose a modified DART algorithm, which imposes a set of soft constraints on the pixel values. The soft constraints allow noise to be spread across the whole image domain, proportional to these constraints, rather than across boundaries. The results of our numerical experiments show that SDART yields more accurate reconstructions, compared to DART, if the signal-to-noise ratio is low.

@&#INTRODUCTION@&#
In tomographic imaging, a three dimensional object is reconstructed from a series of projection images that have been acquired over a range of angles. Tomography has a wide variety of applications, ranging from medical imaging to materials science [1–5]. In many of these applications, the object under investigation consists of only a few different materials, each corresponding to a particular gray level in the reconstructed image. Therefore, the set of gray values in a reconstruction should be small and discrete. Most common reconstruction algorithms, such as the Filtered Back Projection or SART, produce a continuous range of gray values [6]. However, it has been shown that incorporating this set of admissible gray values as prior knowledge can lead to superior image quality in the reconstruction, especially if the set of projection images is small [7–10]. This type of tomography is known as discrete tomography.The Discrete Algebraic Reconstruction Technique (DART) is one such algorithm that exploits the discrete nature of the object. It assumes that the gray values corresponding to the different compositions of the object are known a priori [8]. If only the number of different gray values is known, and not their actual values, these gray values can be adaptively estimated during reconstruction by using PDM-DART [11]. DART is an iterative method, which aims to solve a system of linear equations that models the tomographic projection process. In each iteration, a reconstructed image is segmented, i.e. the gray values are thresholded to the nearest a priori known gray value. It is assumed that the interior regions of this segmentation are segmented with high accuracy and that most errors are located on the boundaries. The key idea behind DART is to reduce the system of equations in each iteration by fixing or removing these interior pixels/voxels (i.e. unknowns) from the equations. The governing equations in tomography are ill-conditioned and rank deficient. Due to this dimension reduction, the equation system becomes increasingly better determined. Nevertheless, removing unknowns assumes that the gray values of the corresponding pixels are correct. Only the remaining free pixels are iteratively refined. Therefore, the operation of fixing a pixel imposes a hard constraint on the solution of the equation system and can only be effective if the selection criterion for a pixel being fixed or free is sufficiently accurate.In practice, we see that the interior regions of the segmentation initially contain many errors. However, since the boundaries evolve due to the update steps, these pixels will be corrected eventually and the algorithm can converge to the correct solution. A problem occurs when the projection data contain noise. Imposing hard constraints on non-boundary pixels leads to noise being spread mainly over boundary pixels. This leads to major errors in the update steps applied to the boundary pixels. As a result, edges will be less resolved in the reconstruction and convergence problems can arise.In this paper, we propose an alternative to the hard constraints imposed in DART. We introduce a set of relaxation parameters that imposes soft constraints on the pixel values. The parameters penalize deviation from the current segmented value of a pixel. Subsequently, the relaxed system with soft constraints is solved and the parameters are updated based on the intermediate reconstruction and segmentation. The proposed method is called Soft DART, to indicate the use of soft constraints. By using a penalty matrix, flexibility is increased in comparison to DART. It enables us to impose confidence levels on individual pixels instead of indicating if a pixel is correct or not. The results of our simulation experiments suggest that for a suitably chosen set of relaxation parameters and for datasets with low signal-to-noise ratios (SNR), SDART produces a reconstruction closer to the ground truth when compared to DART.The outline of this paper is as follows: first we will briefly discuss the DART algorithm in Section 2 and show some of its limitations. In Section 3 we will introduce SDART: a new variant of DART that includes a soft constraint. Possible choices for selecting the soft constraints are discussed. In Section 4 we compare the behavior of DART and SDART to see the effect of the soft constraints. We also discuss how to select an important regularization parameter that is used in SDART. In Section 5 an overview is given of the experiments and the results are discussed. We conclude the paper in Section 6.In this section we will briefly introduce the notation and concepts of DART and summarize the main details of the algorithm.The governing equations in tomography can be posed as a linear system, which models the tomographic projection process. The linear model is generic, but for simplicity we will focus on the reconstruction of a 2D slice of the object from 1D detector measurements. The generalization to three dimensions is straightforward. Throughout this paper we consider a parallel beam geometry, which is illustrated in Fig. 1. In our implementation the geometry can be easily changed to fan or cone beam geometry. In fact, one of the experiments from Section 5 is based on a cone beam dataset.Letx∈RNdenote a vector containing the gray value of each pixel in the unknown object. The vectorp∈RMcontains the detector measurements. The 1D detector has D elements and K projections are available. The total number of line projections is therefore M=KD. We now introduce the projection operatorW∈RM×N, which relates the object to its projections:(1)Wx=p.Reconstruction methods aimed at solving Eq. (1) are referred to as algebraic reconstruction methods. Examples of such methods based on Kaczmarz’ method are ART, SIRT or SART [6]. Since the system of equations is usually underdetermined, and in practice no solution exists due to noise, it is typically solved in a least square sense:(2)minimizex∈RN‖Wx-p‖2,such that an object is found that matches with the observed data optimally. In discrete tomography, the small, discrete set of admissible gray valuesR={ρ1,…,ρl}is known a priori. We can include this prior knowledge as constraints in the optimization problem:(3)minimizex∈{ρ1,…,ρl}N‖Wx-p‖2.DART combines a continuous algebraic reconstruction method (ARM) with a segmentation step and uses heuristics to improve on this segmentation. The ARM that is typically used is SIRT or SART [6]. In principle, any linear least squares solver is suitable. Also the Krylov subspace methods such as CGLS or LSQR [12].The flowchart in Fig. 2a illustrates the algorithm and its computational steps. We will briefly summarize these:1.An initial continuous reconstructionxcis computed using an algebraic reconstruction method.The reconstruction is segmented by applying thresholding. All pixel values are rounded to the nearest gray value in the set R.Those pixels that have at least one (out of eight) neighbor with a different gray value (called boundary pixels) are free. In addition, a random subset of image pixels is also selected to be free. The corresponding columns are removed from Eq. (1) and their projections are subtracted from the right-hand side.The solution of the reduced system is refined by applying an ARM to the free pixels.If a stop criterion is not met, the free (boundary) pixels are smoothed. The smoothing step is performed by means of a discrete convolution of a 3×3 kernel with the image. The middle pixel of the kernel is weighted by a smoothing factor b, the other pixels in the kernel are weighted by (1−b)/8. Although the smoothing is not used in every implementation of DART [13], it is used in the original paper [8]. The process repeats from step 2.The thresholding step rounds the pixel values of the reconstruction to the nearest a priori known gray value. We use the same notation for this operation as presented in [8]:(4)T(x):RN↦R.Note that the discrete nature of the gray values is not exploited in the ARM iterations. Instead, DART relies on the segmentation to produce solutions with discrete gray values. Nevertheless, the strength of DART is based on the observation that pixels in the interior of a homogeneous region are likely to be thresholded correctly [8]. This result can be found empirically. A possible explanation is that least squares methods in general reconstruct low-frequent components of the solution prior to the high frequencies. Since large homogeneous regions (including the background) are part of the low-frequent components of the image, the non-boundary pixels are better resolved compared to the boundary pixels. This idea is used to classify the segmented imagexsinto the sets of fixed pixels F and free pixels U.Formally, the sets F and U are then defined as index sets:(5)F={i|xi=xi+rn+q,forallq,r∈{-1,0,1}},(6)U=Fc,whereFcdenotes the mathematical complement of F. In addition, the set of free pixels U is combined with a random subset of pixels. Each pixel has a probability(1-p)to be included in the set of free pixels. The probability0<p⩽1is referred to as fix probability. This random set of free pixels improves the reconstruction of “holes” in the object, which are typically not found easily.Since pixels in the interior regions are likely to be correct, they are removed from the equation system Eq. (1) and subtracted from the right-hand side. To fix a pixeli∈F, we apply the following operation on the linear system:(7)||||w1…wi-1wi+1…wN||||x1⋮xi-1xi+1⋮xN=p-viwi,whereviis the segmented gray value of pixel i. Subsequently, other pixels in F are treated in an analogous way. This leads to a reduced system that has fewer unknowns. The pixels in the free set U are refined by iterating an ARM on the reduced system.This process is repeated in each DART iteration. The boundary pixels are determined from the complete image, not only from the free pixels corresponding to the reduced system. Therefore, the elimination of pixels in the fixed set always starts from the full system in Eq. (1). As a consequence, pixels that were previously fixed can be free in a consecutive DART iteration. Therefore, errors in the interior regions can be corrected in a later stage due to evolution of the boundaries.Batenburg and Sijbers show that with increasing noise levels, the DART reconstructions have a large pixel error (i.e. the number of pixels that have a wrong gray value in the reconstruction compared to the ground truth) [8]. Since only boundary pixels are free, the noise has a large effect on the boundary update. To remedy this problem, the fix probability can be decreased such that noise is also spread over a large random subset of pixels. While this improves the accuracy of DART with noisy projection data to some extent, it does introduce heavy salt and pepper noise, as was also observed in [13].The main contribution of this paper is to introduce a different approach to classify and improve incorrectly segmented pixels. Instead of fixing pixels and updating free pixels, we propose to solve a relaxed system (i.e. under soft constraints) as an alternative to the ARM update step. In this section we will discuss the main details of the new approach.A flowchart of the new method is shown in Fig. 2b. SDART follows the same steps as DART, but it does not eliminate unknowns (pixel values) from Eq. (1).We propose to introduce a soft constrained optimization problem. Letvbe the segmentation of the intermediate reconstruction and letD∈R+N×Nbe a diagonal matrix with nonnegative real entries (dii⩾0, it is referred to as penalty matrix). We then introduce the relaxed reconstruction problem:(8)minimizex∈RNWλDx-pλDv22≡minimizex∈RN‖Wx-p‖22+λ2‖D(x-v)‖22.In this setting, the diagonal matrix elementdiigives a penalty to pixel i for deviating from its segmented valuevi. Ifdiiis large, only small deviations are allowed, while the pixel can be considered “free” ifdii=0.The system in Eq. (8) is solved by a linear least squares solver, e.g. a reconstruction method such as SIRT or a Krylov subspace method such as CGLS. The algorithm is started with initial guessx0=xc, the reconstructed image (with continuous gray levels) that resulted from the previous SDART iteration. The parameter λ is introduced both for regularization, as well as to compensate the difference in scale of the two terms in the cost function in Eq. (8). Note that the term‖Wx-p‖22depends on the number of angles, whereas‖D(x-v)‖22does not. As a result, the scaling between the two terms is important and the value of λ needs to be adjusted accordingly.The entries ofDdepend on the current reconstruction, so the matrixDneeds to be updated at each SDART iteration. The main advantage of this approach, compared to using hard constraints, is that no pixel will be truly fixed. Therefore, noise in the projection data will be distributed over the entire image (proportional toD). Moreover, the relaxation parametersdiican express a confidence level for the accuracy of pixel’s i gray value. The confidence level can be based on any error measure for the reconstruction we have. Due to the generality of Eq. (8) we can even choose a different reference imagevinstead of the segmented reconstruction.Naturally, the increased flexibility comes with a price. The system has gained N unknowns as well as N equations, making it more costly to solve Eq. (8). In addition, we lose the efficiency resulting from the removal of columns from Eq. (1). Instead, each SDART iteration will be as costly as the first. As will be explained in Section 3.2, an efficient implementation can still lead to satisfactory performance. The full algorithm is presented in pseudo code in Algorithm 1. Note that a stopping criterion is not included in the algorithm description. In general, the question when to stop an algorithm to obtain the best solution is a very difficult one. Therefore, a reasonable choice is to terminate the algorithm when the relative change in the solution is small. This can be achieved by using a fixed number of iterations. In the experiments section we use 30 to 50 iterations, which is enough for all the datasets we considered.In our simulation experiments in Section 5, we consider two different penalty matrices, defined as follows:For validation purposes, we introduce a penalty matrix that should result in SDART mimicking the original DART algorithm. SDART does not allow us to fix pixels, but instead we can give non-boundary pixels a very large weight. By giving a weight of zero to boundary pixels, we do not put any restrictions on those pixels. The resulting penalty matrix is given by(9)dii≔106,i∈F0,i∈U.These weights are found to be effective from preliminary simulation experiments. We refer to SDART using this penalty matrix as SDART-ORIG, the first variant of SDART.In DART, a pixel is fixed when all 8 neighbors have the same gray value. If at least one neighbor has a different gray value, the pixel is considered free. This leads to a relatively fat boundary. Therefore, a logical choice forDwould be to give a penalty that is proportional to the number of neighborsbithat have a different gray value, i.e.,(10)bi≔∑r=-11∑q=-111{xi≠xi+rn+q},where1{}is an indicator function that is 1 if the condition is true and 0 otherwise. In this way, boundary pixels have different weights that reflect their position in the boundary. As a result, the boundary can be considered to be narrower. The penalty matrix is then defined as(11)dii≔1003bi.Note thatdiiis an exponential, monotonically decreasing function inbi. The factor 3 in the denominator was chosen based on early simulation experiments. If the factor is too small or too large, the reconstruction will either not change much in each iteration or noise is distributed more on the boundary, respectively. This version of SDART is referred to as SDART-NB.In this section we will go into more detail how the soft constrained optimization problem in Eq. (8) is solved.This optimization problem involves solving the system(12)WλDx=pλDvin a least squares sense.The matrixWhas M rows and N columns and is typically very large, especially in the three dimensional case, where the number of voxels/pixels in the reconstruction grid and in the projection data, is large. In Eq. (12) we added another N rows to the system matrix and right-hand side. However, in our implementation using the ASTRA toolbox [14,15], the full matrix is never formed explicitly. If a matrix–vector product is computed, we split this operation in two parts:WxandλDx. The first matrix–vector product is computed by generatingWon the fly to avoid high memory usage. This can be done efficiently due to our GPU implementation. The second partλDxis simply an inner product, becauseDis diagonal and is stored as a vector. Therefore, the computational overhead compared to solving Eq. (2) is small.Algorithm 1SDARTInput: Projection datapOutput: Segmented reconstructionxs1. Letxcbe the initial CGLS reconstruction from projection datap.Compute the initial segmentationxs=T(xc).repeat/* Setting up the soft constraints */2. Compute the matrixDbased onxs.3. Setv=xs.4. Setx0=xc./* Soft constrained reconstruction */5. Solve:minimizexc‖Wxc-p‖22+λ2‖D(xc-v)‖22using CGLS with initial solutionx0/* segmentation */6. Computexs=T(xc).until convergenceFor solving Eq. (12) we can apply any linear least squares solver. However, we noticed during preliminary experiments that methods based on Kaczmarz’ method such as SIRT [6], have slow convergence and do not yield very accurate results. Krylov subspace methods perform better in this case. We found that the method CGLS performs very well and methods such as LSQR and LSMR are suitable too [12,16], but they all have slightly different results. This is why we have chosen to combine CGLS with SDART in our numerical experiments in Section 5.In this section we highlight differences in behavior of DART and SDART using numerical experiments. We also introduce an experimental way to compute the regularization parameter λ that has an important role in the convergence of SDART. We want to point out that this section serves the reader to illustrate the different behavior between DART and SDART. Therefore, the choice of our phantom shown in Fig. 3is somewhat arbitrary.The effect of the hard versus soft constraints of DART and SDART can be visualized by looking at the evolution of boundary pixels (i.e. free pixels in case of DART). Since SDART has no concept of free pixels, we show the penalty matrixDafter rescaling its values such that the elements are in the range [0,1], i.e. we show the image{xi}such that(13)xi=1-diimaxj(djj).In this example we consider the cylinder block phantom, shown in Fig. 3. The projection data were computed by forward projecting the image, using a parallel beam geometry. In total 25 projections were computed at equidistant angles in the domain [0,π). The projection data were perturbed by Poisson noise. Noise due to a limited photon count, which is encountered in many types of tomography, follows a Poisson distribution. The intensity of the noise is quantified by the photon count of the incident X-ray beam, when no object is between the source and detector. In other words, this represents the total dose that is emitted during the full scan of the object. In this case, noise was simulated corresponding to a photon count of 103. In DART, the fix probability was set to 0.99. With very low signal-to-noise ratios, a lower fix probability is preferred, but this would make it difficult to show the boundary evolution.In Fig. 4the boundary evolution of both DART and SDART-NB (using the neighbor criterion) for the first three full iterations is shown. Note that for SDART-NB, we show the weights represented in Eq. (13). A pixel is black if the corresponding penalty of the pixel is maximum,maxidii. This is comparable to a fixed pixel in DART. A white pixel corresponds to a minimum penalty. The pixel attains any other gray value if it is in between these extrema. This representation of the “amount of fixedness” of pixels is not directly comparable to DART’s free and fixed pixels. However, we think that these images give insight in the different ways that DART and SDART update the reconstruction.The initial boundary of DART in Fig. 4a is only slightly refined in the next iterations. Although the boundary becomes thinner, many of the background pixels are indicated as free pixels. In SDART-NB, we see a similar thinning of the boundaries. Moreover, the contours of the ground truth image are approximated more accurately. Background pixels have a large weight (indicated by black pixels). This shows that the background is more homogeneous rather than that noise is producing clusters, as is the case in DART.Images of the boundary evolution, do not give a clear insight in the quality of the reconstructions. Therefore, we also show the segmented intermediate reconstructions in Fig. 5. DART is distributing a significant part of the noise throughout the background, where many pixels are free. Another consequence of noise is visible in the jagged boundaries of the cylinder block. The reconstructions of SDART-NB have finer and more distinct boundaries. In addition, background noise is reduced within consecutive iterations.The behavior shown in this example depends largely on the set of soft constraints imposed by the matrixD. For example, a strategy of fixing pixels similar to DART (e.g. SDART-ORIG) is very effective for projection data without noise [8], while it fails in cases with heavy noise. Therefore, finding a single penalty matrixDthat is accurate in all possible datasets is unlikely. An adaptive approach might be more successful. For example, the order of magnitude of the weights can be changed according to noise levels. In case of low noise levels, high weights steer the solution more to the segmentation, while smaller weights prevent over-fitting to noise. We see an important role here for the parameter λ in Eq. (8). It can be used to assign a larger weight to the data fidelity term or to correspondence to the segmented solution.In this section we will discuss how to select a value for the regularization parameter λ that is close to optimal, where we use the term optimal reconstruction to refer to the reconstruction with smallest pixel error over all possible choices for λ. Recall that the pixel error indicates the number of pixels that do not have the right gray value compared to the ground truth. We have chosen this error norm over, e.g., a chi-2 or jaccard distance, since our images are inherently discrete. A chi-2 measure is more suitable when comparing two images that are continuous with respect to their pixel values. Moreover, we expect that our findings will not be changed significantly when another error norm is used.Consider the second formulation of the cost function in Eq. (8). It consists of two terms: a data fidelity term‖Wx-p‖22and a discrete tomography prior‖D(x-v)‖22. The order of magnitude of these terms is in general not directly comparable. Therefore, a regularization parameter λ is added to properly adjust the bias to the discrete tomography prior.Note that the magnitude of the data fidelity term depends strongly on the current solutionx(and thus on the ground truth) as well as the number of projection angles. Adding more projection angles results in more rows inWas well as more elements inp. By making the assumption that a projection image does not change in a small angular range, we can assume that the sum of squared residuals‖Wx-p‖22behaves linearly in the number of angles. (Provided that the additional angles are close to the original angles.)To verify this assumption, we perform a small numerical experiment. Considering the Shepp–Logan phantom, the bottom image in Fig. 10a, we simulate projection data for a fixed number of angles (e.g. 50). For these projection data we reconstruct the image by using 4 iterations of LSQR (as explained in the previous section). This yields an approximate reconstruction. Consequently we construct the projection operatorWand corresponding right-hand sidep(by forward projecting the ground truth image) for a varying number of equidistant projection angles. In Fig. 6a the squared residual norm is plotted for a varying number of angles, indeed showing a linear curve.The last term in Eq. (8) does not depend on the number of projection angles. Instead it depends linearly on the number of pixels in the reconstruction. Since this is also true, to some extent, for the data fidelity term, no adjustments should be necessary if the number of reconstruction pixels is changed (e.g. a value λ at low resolution can be found that is also suitable for high resolution reconstructions).Due to the linearity of the data fidelity term, we can extrapolate λ if a value is known for a dataset with few projection angles.We still lack a way of determining a good value for λ for a given dataset. For this goal we can use the residual 2-norm. If the projection data are consistent with the ground truth (no noise), there is usually a good correspondence between the true (pixel) error and the residual of Eq. (1). We can exploit this to find a value for λ for which the pixel error of the SDART reconstruction is minimal. In Fig. 6b we have plotted the pixel errors of the SDART-NB reconstructions and the value for λ that was used. The result suggests that there exists an optimal λ at which the pixel error takes its minimum value. Moreover, the residual 2-norm agrees with this minimum. Therefore, if an initial guess for λ is known, we can compute several SDART reconstructions for different λ in a small range. If a minimum of the residual is found, we also found the optimal λ.Of course this method will fail if noise is present in the projection data. Then the correspondence between the residual 2-norm and the true error is in general very poor. Nevertheless, we will show that a value found for λ for a phantom dataset (or a dataset with very low noise levels) may also give near-optimal results when high noise levels are considered.In Fig. 7a, a mesh plot is shown of the pixel error for a range of λ as well as several photon counts. For these computations the cylinder phantom in Fig. 3 was used and projections at 20 angles were computed. We see that the minimum pixel error is attained at λ≈0.5 for data with high signal-to-noise ratio (SNR). If the SNR is decreased, the optimal value for λ does not seem to change much.The optimal λ for each SNR still varies slightly. The corresponding pixel errors are minimum at that specific SNR. We also selected a constantλ∗≈1that attains pixel errors closest to these minimum pixel errors in a least square sense. This is the optimal choice if we fix λ. The pixel errors are shown in Fig. 7b. We also plotted these curves for λ=1.38 and λ=0.24 (which was optimal for the noiseless case as shown in Fig. 6b). From this result we can see that whileλ∗is the better choice overall, choosing λ=0.24, the same as in the noiseless case, produces near-optimal results. The key conclusion is that the value for λ as in the noiseless case also works well in low-dose datasets.In Fig. 8, the pixel error and residual are plotted for a dataset with limited noise (a photon count of 106), for varying λ. From these data we see that there still is a good correspondence between the minimum pixel error and the minimum of the residual MSE. This implies that we can effectively estimate λ for a dataset with high SNR. Consequently, datasets collected from the same object with low SNR can use the same λ.

@&#CONCLUSIONS@&#
We proposed a new variant of DART that introduces a set of soft constraints to replace the hard constraints. We have seen that the hard constraints in DART lead to problems if the projection data contain a high level of noise. Our new method, named SDART, was introduced to enhance the robustness of DART for noisy projection data. The soft constraints allow noise to be spread across the whole image domain. As a result, boundaries of the object are less influenced by the noise, leading to sharper edges.Two sets of soft constraints, or penalty matrices, were introduced. The first variant, SDART-ORIG, mimics the original DART algorithm. The other variant, called SDART-NB, discriminates boundary pixels by the number of surrounding pixels with a different gray value.We performed several simulation experiments that compare the accuracy of SDART with DART, BART and SIRT. The results from noiseless data show that SDART has similar, but slightly less accuracy compared to DART. The accuracy of SDART-NB compared to BART is slightly improved. On datasets with very low signal-to-noise ratios, SDART-NB outperforms DART and BART by large. The results of SDART-ORIG were not accurate in this case and SDART-NB is the preferred method for noisy projection data. The qualitative results show that SDART-NB is less prone to salt and pepper noise. Results from experimental data, containing a large amount of noise, further support that SDART-NB is more accurate compared to DART. For this particular dataset, the pixel error of the SDART-NB reconstruction was approximately 35% smaller than that of DART. In this case, the difference in quality between the DART and SDART-NB reconstructions was less obvious visually. The SDART-NB reconstruction has sharper edges, distorted by some salt and pepper noise. DART produces clusters of a gray value on the edges that is different from the metal interior. This might lead to the false conclusion that water adhered to these edges. From the SDART reconstruction it is clear that this is not the case. This shows that the SDART reconstruction can give additional insight, when conclusions about the DART reconstruction are not decisive.So far, we have investigated only two possible choices for the penalty matrixD. Compared to DART, SDART can encode a more specific representation of the prior by using continuous weights. We expect that SDART can be further improved by using more sophisticated choices for this matrix, which will be investigated in future research.