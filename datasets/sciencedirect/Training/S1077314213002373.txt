@&#MAIN-TITLE@&#
Visual estimation of pointed targets for robot guidance via fusion of face pose and hand orientation

@&#HIGHLIGHTS@&#
Problem formulation: given a number of possible pointed targets, compute the target that the user points to.Estimate head pose by visually tracking the off-plane rotations of the face.Recognize two different hand pointing gestures (point left and point right).Model the problem using the Dempster–Shafer theory of evidence.Use Demspster’s rule of combination to fuse information and derive the pointed target.

@&#KEYPHRASES@&#
Human–robot interaction,Computer vision,Gesture recognition,Pointing gestures,Head pose estimation,

@&#ABSTRACT@&#
In this paper we address an important issue in human–robot interaction, that of accurately deriving pointing information from a corresponding gesture. Based on the fact that in most applications it is the pointed object rather than the actual pointing direction which is important, we formulate a novel approach which takes into account prior information about the location of possible pointed targets. To decide about the pointed object, the proposed approach uses the Dempster–Shafer theory of evidence to fuse information from two different input streams: head pose, estimated by visually tracking the off-plane rotations of the face, and hand pointing orientation. Detailed experimental results are presented that validate the effectiveness of the method in realistic application setups.

@&#INTRODUCTION@&#
In the emergent field of social robotics, human–robot interaction via gestures is an important research topic. Among the communicative gestures performed by humans, pointing gestures are especially interesting for interaction with robots. They open up the possibility of intuitively indicating objects and locations and are particularly useful as commands to the robot. Pointing gestures can also be used in combination with speech recognition to specify parameters of location in verbal statements and also provide a clear input when speech recognition ambiguities occur. An example scenario of this type is the use of pointing gestures to direct the robot to a specific object or location. The robot must be able to detect the pointing gesture and estimate the pointed target location, thus major issues arise that have concerned recent vision research in this field [1–4,8]. Some of the most important challenges are related to the requirement for real time computations, the obtained accuracy and the operation in difficult cluttered environments with possible occlusions, variable illumination and varying background. Another common requirement is that pointing gestures must be recognized regardless of scale, referring to large pointing gestures performed with full arm extend and small pointing gestures reduced to forearm and hand movement only [3,4].Based on the fact that, for most applications, it is the pointed target rather than the actual pointing direction which is important, we formulate a novel approach which, in contrast to existing pointing gesture recognition approaches, also takes into account prior information about the location of possible pointed targets. Assuming the most common type of deictic gesture, i.e., the one that involves the index finger pointing at the object of interest and the user’s gaze directed at the same target [5,6], we formulate our approach using a monocular setup to track off-plane head rotations with high accuracy and at the same time recognize hand pointing gestures. These two input streams are combined together to derive the pointed target using a formulation which is based on the Dempster–Shafer theory of evidence [7]. Employment of a single camera differentiates substantially our approach from the majority of existing methods that use stereo or multi-camera setups. However, the main contribution of the paper relies in the combination of the input streams based on the Dempster–Shafer theory of evidence, allowing the approach to elegantly handle situations where one or both of the input streams are missing (e.g. the hand pointing direction is not visible due to self-occlusions); namely, use the lack of input as evidence, achieving impressive results which could not have been derived with contemporary probabilistic fusion approaches [1,8]. Moreover, with the proposed formulation beliefs are assigned to set of pointed targets rather than individual pointed targets. Demster’s rule of combination facilitates the combination of these beliefs without the need to “split” them to individual pointed targets, if the observations do not explicitly suggest so.In the following sections the related work (Section 2) and proposed methodology (Section 3) are discussed. The algorithm for hand pointing gesture recognition is briefly discussed in Section 4 and the face pose estimation is explained in Section 5. The fusion of face pose and hand pointing, being a main focus of the paper, is analyzed in Section 6. Experimental results in a simulated environment as well as results using ground truth data are provided in Section 7. The paper concludes with a discussion in Section 8.

@&#CONCLUSIONS@&#
In this paper we have presented a novel method for estimating a pointed target by fusing information regarding hand pointing gestures and the pose of the user’s head. The proposed method is able to achieve surprisingly good performance by considering prior knowledge about the location of possible pointing targets which reduces the problem to deciding which is the pointed target rather than calculating the actual pointing direction.Unlike most other contemporary methods, our approach operates with a monocular vision system and we have demonstrated its ability to achieve significant recognition rates even in cases that either of the two or both input streams are missing. For the reviewed monocular methods [25,23,24] a direct comparative evaluation is not justified since they address different variants of the pointing problem, under different experimental setups and assumptions.The proposed method is readily applicable in a large variety of human–robot interaction scenarios. Future work will involve its enhancement by fusing additional sources of information, such as arm pose, body orientation and a priori probabilities of POI selection. Moreover, apart from extending the interaction scenario to simultaneously handle multiple users, it is of interest to extract additional information on vertical pointing directions.