@&#MAIN-TITLE@&#
New upper bounds for tight and fast approximation of Fisher’s exact test in dependency rule mining

@&#HIGHLIGHTS@&#
A family of new tight upper bounds to approximate Fisher’spis introduced.The new approximations suit for data mining purposes, because they are much faster to evaluate than the exactp-value.Theoretical analysis and empirical evaluation show that the new approximations are very accurate for all practical purposes.The approximations are not sensitive to the data size, distribution or small expected counts.

@&#KEYPHRASES@&#
Fisher’s exact test,p,-value,Hypergeometric distribution,Upper bound,Approximation,Dependency rule,Data mining,

@&#ABSTRACT@&#
In the dependency rule mining, the goal is to discover the most significant statistical dependencies among all possible collapsed2×2contingency tables. Fisher’s exact test is a robust method to estimate the significance and it enables efficient pruning of the search space. The problem is that evaluating the requiredp-value can be very laborious and the worst case time complexity isO(n), wherenis the data size. The traditional solution is to approximate the significance with theχ2-measure, which can be estimated in a constant time. However, theχ2-measure can produce unreliable results (discover spurious dependencies but miss the most significant dependencies). Furthermore, it does not support efficient pruning of the search space. As a solution, a family of tight upper bounds for Fisher’spis introduced. The new upper bounds are fast to calculate and approximate Fisher’sp-value accurately. In addition, the new approximations are not sensitive to the data size, distribution, or smallest expected counts like theχ2-based approximation. In practice, the execution time depends on the desired accuracy level. According to experimental evaluation, the simplest upper bounds are already sufficiently accurate for dependency rule mining purposes and they can be estimated in 0.004–0.1% of the time needed for exact calculation. For other purposes (testing very weak dependencies), one may need more accurate approximations, but even they can be calculated in less than 1% of the exact calculation time.

@&#INTRODUCTION@&#
Dependency rules are simple data mining patterns which can be used for a thorough analysis of statistical dependencies in categorical data sets. A dependency ruleX→Aexpresses a positive dependency between a set of attributes,X, and a single binary attribute,A. It can be read as “If factorsXoccur, thenAis more likely to occur than otherwise”. Similarly, a negative dependency betweenXandAcan be expressed by ruleX→¬A. In the dependency rule mining, the goal is to search for the best or all sufficiently good dependency rules with the selected goodness measure. Because the patterns are very simple, the search can be done quite efficiently, and dependency rule algorithms can handle even millions of rows of data containing thousands or tens of thousands binary attributes without any suboptimal heuristics (see e.g. Hämäläinen, 2012).A classical application of dependency rules (and related association rules (Agrawal et al., 1993) which, however, do not necessarily express statistical dependencies) is market basket analysis, which aims to find dependencies describing shopping habits. A rule may reveal, for example, that if a market basket contains coffee and cream, then it is also more likely to contain sugar than other baskets. This kind of information has several applications in marketing, from optimal product arrangements to individual recommendations in web-stores. Medical science is another application field which contains huge binary or easily binarized data sets. In this context, dependency rules can be used to analyze which gene alleles, habits, environmental and phenotypic factors predispose or prevent diseases. This is an important application for dependency rule mining, because disease mechanisms are often very complex, involving dozens of factors. An extra difficulty is that statistical dependence (as well as statistical significance) is not a monotonic property. This means that ruleA,B→Ccan express a strong and highly significant dependency, even ifAandCas well asBandCwere statistically independent. Therefore, it is not possible to find the most important dependencies without efficient search algorithms.Example 1Let us consider a medical database which contains information on patients, their diseases (Alzheimer disease, stroke, heart attack, and coronary heart disease (CAD)), medical measurements (blood pressure and HDL and LDL cholesterol), and occurrence of certain gene alleles (ABCA1-R219K, ACE-D, ApoE-e2, -e3 and -e4). Here are some examples of discovered dependency rules:ApoE-e4→high LDLABCA1-R219K,age<57→high HDLACE-D, male→hypertensionACE-D, age≥62, no diabetes→heart attackApoE-e4, ACE-D, smoking→CADhypertension→strokeApoE-e4→strokeApoE-e3, male, exercise→low LDLACE-D,age<62, overweight→CADABCA1-R219K→strokeACE-D,age<62→CADApoE-e4, modest smoking→no AlzheimerApoeE-e4→AlzheimerABCA1-R219K, female→AlzheimerApo-e3, ACE-DD→strokeACE-D,age≥62→heart attackApoE-e2→low LDLApoE-e2, male, exercise→low LDLHere, simple rules tend to be frequent but weaker, while more specific rules are rarer but stronger. For example, carriers of the ApoE-e2 allele tend to have low LDL cholesterol, in general, but the dependence is not strong. However, if the carrier belongs to some special group, like exercising men, the dependence is much stronger. These rules also demonstrate the non-monotonic nature of statistical dependence. For example, allele ABCA1-R219K has no effect on Alzheimer disease, when considered alone, but among women, it surprisingly increases the risk. Similarly, ApoE-e4 is a risk factor for Alzheimer disease, but with modest smoking, it seems to protect from the same disease.The applications of dependency rules are not restricted to categorical (easily binarized) data, although numerical variables require some kind of discretization. This can still be beneficial as a preliminary data analysis, because dependency rules can reveal even complex dependency structures without any assumptions on their form. This information on dependencies is anyway needed before one can select suitable methods for a more detailed analysis.The main dilemma in dependency rule analysis (like most data mining) is how to perform the search efficiently without trading off quality of results. As one can expect, the problem is computationally very demanding, because the number of possible patterns is exponential,O(k2k), wherekis the number of binary attributes. Even a simpler problem, searching for the optimal classification rules (dependency rules with a fixed consequence attribute) is known to be NP-hard with common statistical measures like theχ2-measure (Morishita and Sese, 2000), and no polynomial time solutions are known. The search algorithms try to prune the search space as much as possible without explicit testing, but still it is necessary to test millions of potentially promising patterns. For this reason, all algorithm steps and implementation details have to be polished to as fast as possible.Concerning quality, the main concern is that the discovered patterns should be genuine dependencies which are likely to hold also in future data. In practice, this means statistical significance testing—the algorithm should find those dependencies which are least likely to have occurred by chance. In dependency rule mining and related pattern discovery, the most commonly used statistical significance measure is theχ2-measure (Morishita and Sese, 2000; Nijssen et al., 2009; Hämäläinen, 2011), but also other measures like Pearson’s correlation coefficient (Antonie and Zaïane, 2004),z-score (Hämäläinen, 2010), mutual information (practically, log likelihood ratio) (Nijssen et al., 2009), and odds ratio (Li et al., 2013) have been used. Fisher’s exact test–which is the focus of this paper–has been used only rarely. In Kingfisher (Hämäläinen, 2012), Fisher’sp-value,pF, is the main search measure. In MagnumOpus (Webb and Zhang, 2005), the rules are searched for with other measures, but Fisher’s exact test is used to test the improvement of a rule against its simplifications. In addition, there is a new graph mining algorithm (Sugiyama et al., 2015) which usespFto evaluate dependencies between subgraphs and class values. It seems that the interest in Fisher’s exact test is rising, especially among bioinformaticians, but efficient tools are still lacking.Asymptotic measures have been preferred in data mining, because they are fast to evaluate and therefore suitable to exhaustive search. It has been implicitly assumed that since the data sets are large, asymptotic measures can be safely used. However, the most significant (non-trivial) dependency rules may be relatively infrequent and the corresponding distributions too skewed to meet the requirements of asymptotic tests. Especially, the popularχ2-measure can produce very unreliable results, where the discovered dependency rules do not hold in future data. This was shown in extensive cross-validation studies, where the accuracy of the best rules discovered by theχ2-measure and Fisher’s exact test (pF) were compared (Hämäläinen, 2012). In these experiments, theχ2-measure often selected rules which expressed much weaker dependence or even independence in the test set, while the rules found bypFheld always well in test sets. This was not surprising, because Fisher’s exact test is known for its robustness (Lydersen et al., 2009). A more surprising result was the inefficiency of the search with theχ2-measure, due to its weaker pruning ability. Thus,pFturned out to be a superior search measure in terms of both accuracy and efficiency of pruning.The only problem with Fisher’s exact test is that it is computationally laborious. ThepF-value is the sum of the probabilities of the observed and all more extreme contingency tables. In the worst case, the sum may containn4terms, which means that the worst case time complexity isO(n), wherenis the data size. For example, ifXandAhave frequenciesfr(X)=fr(A)=500000and the frequency of combinationXAisfr(XA)=300001, then one should evaluate 200 000 terms. In addition, each term involves binomial factors, but they can be evaluated in a constant time, if all factorials have been tabulated.In this paper, we introduce a family of tight approximations for the exactpF-value, which can still be calculated in a constant time. The approximations are actually upper bounds forpF, but when the dependency is sufficiently strong, they give tight approximations to the exact values. In practice, they give identical results with the exactpF-values, when used for rule ranking.The main idea of the new approximations is to calculate only the first terms frompFexactly and estimate an upper bound for the remaining terms. The simplest upper bound evaluates only the first term exactly. It is also intuitively appealing as a goodness measure, because it is reminiscent to the existing dependency measures like the odds ratio. When the dependencies are sufficiently strong (a typical data mining application), the results are also highly accurate. However, if the data set contains only weak and relatively insignificant dependencies, the simplest upper bound may produce too inaccurate results. In this case, one can use tighter upper bounds, which can be adjusted arbitrarily accurate. Nevertheless, there is always a trade-off between speed and accuracy. The more accuratep-values are wanted, the more terms have to be calculated exactly. Fortunately, the largest terms ofpFare always the first ones, and in practice it is sufficient to calculate only a small number (say, 10) of them exactly.As far as we know, there does not exist any other computationally fast but accurate approximations topF. The likely reason is that statisticians have totally different speed requirements than data miners. If a measure is evaluated just a couple of times, then a second or two is not too long. However, if one has to evaluate the measure a million times, then a millisecond is already too much. In addition, the users of data mining software have got used to short executions times and are reluctant to change into slower but more accurate methods.The related research has mostly concentrated on developing efficient network algorithms for Fisher’s exact test in the2×c(Mehta and Patel, 1980; Requena and Ciudad, 2006) andr×c(Mehta and Patel, 1983) cases. However, these algorithms do not offer any solution to the summation problem. There is also an interesting method (Wu, 1993) for improving the accuracy of calculation, by simplifying the binomial coefficients, but it is computationally far too demanding for data mining applications. In practice, the old technique of tabulating factorials (Verbeek and Kroonenberg, 1985) is still the fastest technique to evaluatepF.The rest of the paper is organized as follows. In Section  2, the basic concepts and notations are defined. In Section  3, we introduce the new upper bounds and give theoretical error bounds for the resulting approximations. In Section  4, we evaluate the accuracy and computational efficiency of the new upper bounds experimentally. The final conclusions are drawn in Section  5.Dependency rules are data mining patterns which describe statistical dependencies between sets of attribute–value combinations in data. Given the set of all binary attributes in data,R, a dependency rule can be expressed asX=x→A=v, whereX⊆Ris a set of attributes,A∈R∖Xis a single attribute,v∈{0,1}is a single truth value, andX=xis a short-hand notation for the truth value assignment ofX’s attributes. So, in terms of propositional logic, the rule antecedent (condition part) is a conjunction of atomic propositions and their negations (Ais and¬Ais,Ai∈X) and the consequence is a single atomic proposition or its negation. In addition, it is required that a genuine dependency rule expresses statistical dependence between the antecedent and the consequence. Conventionally, the rule is represented in the form which shows positive dependence between the condition and the consequence. So, ruleX=x→A=vtells that there is a positive dependency between eventsX=xandA=v, i.e.,P(X=x,A=v)>P(X=x)P(A=v), wherePdenotes relative frequency (data-based estimate for the real probability).For clarity, we will assume here a simpler form of rules, where all attributesAi∈Xare true-valued and the condition can be represented by listing the attributes(A1,…,Am), wherem=|X|is the number of attributes in setX(i.e.,|⋅|denotes the cardinality of a set). In addition, we assume (without loss of generality) that the consequence is also true-valued, i.e.,A=1, which can be represented by mereA. All results can be easily generalized toA=0by changingA=1andA=0. Moreover, we will use a shorthand notation¬AforA=0and¬Xfor the negation of(A1,…,Am), which is¬A1∨⋯∨¬Am.Each dependency ruleX→Acorresponds to a contingency table (Table 1). This contingency table can be considered as a collapsed version from anm×2table,m=|X|, listing all possible attribute–value combinationsX=x. Now the task of dependency rule mining is the same as searching for the best collapsed contingency tables defined by the set of all attributes in data,R. Because any attribute setY∈P(R),|X|≥2, can form a dependency rule and any attributeAi∈Ycan be a consequence, the number of all possible collapsed contingency tables is∑i=2k(ki)i=k(2k−1−1), wherek=|R|.Let us now use notationsfrfor absolute frequency,Pfor relative frequency (“probability”), andnfor data size, such thatP(X)=fr(X)n. The simplest measures for the strength of a statistical dependency ruleX→Aare leverageδ(X,A)=P(XA)−P(X)P(A),which measures the absolute deviation from the expected frequency, ifXandAwere actually independent, and liftγ(X,A)=P(XA)P(X)P(A),which measures the relative deviation from the expected frequency. We note that leverage measures dependency between binary variablesXandAand has the same absolute value,|δ(X,A)|, for all attribute–value combinationsXA,X¬A,¬XAand¬X¬A. In the case of positive dependency,δ(X,A)=δ(¬X,¬A)>0andδ(X,¬A)=δ(¬X,A)<0. On the other hand, lift measures dependency between the given attribute values,X=1andA=1. Therefore, the lift valuesγ(X,A),γ(X,¬A),γ(¬X,A)andγ(¬X,¬A)are generally unequal, unless all of them are 1 (perfect independence). In the following sections, we will use both leverage and lift frequently in upper bounds and proofs, because they enable intuitive, simple expressions. For example, theχ2-measure can be expressed asχ2(X,A)=nδ(X,A)2P(X)P(¬X)P(A)P(¬A)=n(γ(X,A)−1)(γ(¬X¬A)−1).Theχ2-measure can be used as such as a goodness measure for rule discovery, but one can also determine the correspondingp-value from theχ2-distribution with one degree of freedom.In Fisher’s exact test, thep-value is estimated directly from a hypergeometric distribution with fixed marginsfr(X),fr(¬X),fr(A), andfr(¬A). In this model, the probability of the observed or a stronger dependency, ifXandAwere actually independent, is defined by equationpF(X→A)=∑i=0J(fr(X)fr(XA)+i)(fr(¬X)fr(¬X¬A)+i)(nfr(A)),whereJ=min{fr(X¬A),fr(¬XA)}. ThisJ, the number of terms (in addition top0, the probability of the observed contingency table) is computationally the main problem ofpF. In the worst case, whenP(X)=P(A)=0.5andP(XA)is near its expected value, 0.25, the number of terms isn4. This means that the computational complexity of estimating the sum isO(n), while the asymptotic measures can be estimated in constant timeO(1). In practice, the computational burden depends on the data. If the data contains many strong dependencies, the algorithm does not have to estimate weak dependencies which usually involve most terms. Similarly, if the data is sparse (allP(Ai)s small), the number of terms is also restricted. However, the algorithm should be able to handle also dense data and sets which contain only weak dependencies.Before we present solutions to these problems, we should make two important notes onp-values. First, we recall thatpFand otherp-values are used here only as goodness measures, for ranking patterns. We do not have any good solution how to interpret the extremely smallp-values (likep=10−2000) or when a discovery could be considered “significant”, when millions or billions of patterns are tested either implicitly or explicitly. Second, we note that the hypergeometric model with fixed margins is not the only possible model for estimating the significance of dependency rules. However, the exact tests for unconditional models are computationally even more demanding than Fisher’s exact test. In addition, it is known that Fisher’s exact test approximates the alternative unconditional tests pretty well, when the data is not too small (at leastn=100) (Andrés et al., 2004).In this section, we will first introduce two simple upper bounds and analyze their error bounds. After that we generalize the idea and introduce a family of adjustable upper bounds.The following theorem gives two simple upper bounds which can be used to approximate Fisher’spFwith just one term. The first upper bound is more accurate, but it contains an exponent, which makes it more difficult to evaluate. The latter upper bound is always easy to evaluate and also intuitively appealing.Theorem 1Let us notatepF=p0+p1+⋯+pJandqi=pipi−1,i≥1. For a positive dependency ruleX→Awith liftγ(X,A)holds(1)pF≤p0(1−q1J+11−q1)(2)≤p0(1+1−P(A)γ(X,A)−P(X)γ(X,A)+P(X)P(A)γ(X,A)2γ(X,A)−1).ProofEach termpi(i=0,…,J)frompFcan be expressed aspi=pabsti, whereti=(fr(X)fr(XA)+i)(fr(¬X)fr(¬X¬A)+i)andpabs=fr(A)!fr(¬A)!n!is constant. Therefore, it is enough to show the result forpX=pFpabs:pX=t0+t1+⋯+tJ=t0+q1t0+q1q2t0+⋯+q1q2⋯qJt0, whereqi=titi−1=(fr(XA)+i−1)!(m(¬X¬A)+i−1)!(m(X¬A)−i+1)!(fr(¬XA)−i+1)!(fr(XA)+i)!(m(¬X¬A)+i)!(m(X¬A)−i)!(fr(¬XA)−i)!=(m(X¬A)−i+1)(fr(¬XA)−i+1)(fr(XA)+i)(m(¬X¬A)+i).It is easy to see thatqiis decreasing byi(its numerator decreases and denominator increases) and, therefore, the largest value ofqi(i=1,…,J)isq1. Because we have a positive dependency,qi<1. This is easy to verify by using equalityP(XA)P(¬X¬A)−P(X¬A)P(¬XA)=δ(X,A). Nowq1=fr(X¬A)fr(¬XA)(fr(XA)+1)(fr(¬X¬A)+1)<fr(X¬A)fr(¬XA)fr(XA)fr(¬X¬A)=fr(XA)fr(¬X¬A)−n2δ(X,A)fr(XA)fr(¬X¬A),which is<1, whenδ(X,A)>0(positive dependency). We get an upper boundpX=t0+q1t0+q1q2t0+⋯+q1q2⋯qJt0≤t0(1+q1+q12+q13+⋯+q1J).Becauseq1<1, the upper bound is a converging geometric series. Its sum ist01−q1J+11−q1, which is the first upper bound. On the other hand,t01−q1J+11−q1≤t01−q1=t0(1+q11−q1). Let us insertq1=fr(X¬A)fr(¬XA)(fr(XA)+1)(fr(¬X¬A)+1)and express the frequencies using liftγ(X,A). For simplicity, we use notationsγ=γ(X,A),x=P(X)anda=P(A). Nowfr(XA)=nxaγ,fr(X¬A)=nx−nxaγ,fr(¬XA)=na−nxaγandfr(¬X¬A)=n(1−x−a+xaγ). We getq11−q1=fr(X¬A)fr(¬XA)(fr(XA)+1)(m(¬X¬A)+1)−fr(X¬A)fr(¬XA)=n2xa−n2xa2γ−n2x2aγ+n2x2a2γ2n2xaγ+2nxaγ+n−nx−na+1−n2xa=nxa−nxa2γ−nx2aγ+nx2a2γ2nxaγ+2xaγ+1−x−a+1/n−nxa.The denominator is>nxaγ−nxa, because2xaγ+1−x−a+1/n=P(XA)+P(¬X¬A)+1/n>0. Thereforeq11−q1≤nxa(1−aγ−xγ+xaγ2)nxa(γ−1)=1−aγ−xγ+xaγ2γ−1, where the first equality holds only ifq1=0(i.e.,P(XA)=P(X)orP(XA)=P(A)). Now we have provedt01−q1J+11−q1≤t0(1+1−aγ−xγ+xaγ2γ−1), which gives the second upper bound.□In the following, we will denote the tighter upper bound (which uses the sum of a geometric series) byub1and the looser (simpler) upper bound byub2. Inub1, the first two terms ofpFare exact and the rest are approximated, while inub2, the first term is exact and the rest are approximated. (Note that the sum of a geometric series produces another exact term,p1, in addition top0.)We note thatub2can be expressed equivalently asub2=p0(P(XA)P(¬X¬A)δ(X,A))=p0(1+P(X¬A)P(¬XA)δ(X,A)), whereδ(X,A)is the leverage. This is expression is closely related to the odds ratio,odds(X,A)=P(XA)P(¬X¬A)P(X¬A)P(¬XA)=1+δ(X,A)P(X¬A)P(¬XA),which is often used to measure the strength of a dependency. When we substitute the odds ratio intoub2, we get an intuitively appealing expression(3)ub2=odds(X,A)p0odds(X,A)−1.The odds ratio is not defined, when eitherP(X¬A)=0orP(¬XA)=0, but then the cumulative sum contains just one term,p0, and there is no need for upper bounds.Eq. (3) reveals clearly that the upper bound decreases, when the dependency becomes stronger (odds ratio increases). The following theoretical analysis and empirical results show that in the same time the accuracy of both upper bounds also increases.In practice, bothub1andub2give tight approximations to Fisher’spF, when the dependency is sufficiently strong. The error is difficult to bind tightly, but here we present loose upper bounds which already guarantee sufficient accuracy in dependency rule mining.The following theorem gives a loose upper bound for the absolute error ofub1for any positive dependency, irrespective of its strength:Theorem 2WhenpFof a positive dependency is approximated byub1, the absolute error is bounded byerr=ub1−pF<p0(q121−q1).ProofUpper boundub1can cause error only, ifJ>1. IfJ=0,ub1=p0=pFand ifJ=1,ub1=p0(1−q121−q1)=p0(1+q1)=p0+p1=pF. WhenJ>1, the absolute error iserr=ub1−pF=p0(1+q1+⋯+q1J−1−q1−q1q2−⋯−q1q2⋯qJ)=p0(q12+q13+⋯q1J−q1q2−⋯−q1q2⋯qJ). It has an upper bounderr<p0q12(1+q1+⋯+q1J−2)=p0q12(1−q1J−11−q1)<p0(q121−q1).□This leads to the following corollary, which gives good guarantees for the safe use ofub1:Corollary 1Ifγ(X,A)≥1+52≈1.62(the golden ratio), thenerr=ub1−pF<p0.ProofIn the proof, we use a shorthand notationγ=γ(X,A). According to Theorem 2,err<p0, ifq12≤1−q1. This is true, whenq1≤5−12. On the other hand,q1<1γ, whenγ>1(see Lemma 1 in the Appendix). Therefore, a sufficient condition forq1≤5−12is that1γ≤5−12⇔γ≥25−1=5+12.□Interestingly, an identical result holds for the odds ratio, assuming that it has been defined:Corollary 2IfP(X¬A)>0,P(¬XA)>0andodds(X,A)≥1+52≈1.62(the golden ratio), thenerr=ub1−pF<p0.ProofThe proof is similar to the previous one, except now we haveodds(X,A)instead ofγ(X,A). Therefore, it suffices to show thatq1<1odds(X,A), whenP(X¬A)>0andP(¬XA)>0(i.e.,odds(X,A)has been defined). This is easy, becauseq1<P(X¬A)P(¬XA)P(XA)P(¬X¬A)=1odds(X,A).□Because the odds ratio is always larger than the lift (see Lemma 2 in the Appendix), we can conclude that the requirement for the lift in Corollary 1 is exaggerated. In fact, by substitutingγ(X,A)=P(X¬A)P(¬XA)P(X)P(A)P(¬X¬A)odds(X,A)into Corollary 2, we see that forerr<p0, it would suffice thatγ(X,A)≥P(X¬A)P(¬XA)P(X)P(A)P(¬X¬A)(1+5)2, assuming that the odds ratio has been defined.The simpler upper bound,ub2, can cause a somewhat larger error thanub1, but it is even harder to analyze. However, we note thatub2=pFonly, whenJ=0. WhenJ=1, there is already some error, but in practice the difference is marginal. The following theorem gives guarantees for the accuracy ofub2, whenγ≥2.Theorem 3WhenpFof a positive dependency is approximated withub2andγ(X,A)≥2, the absolute error is bounded byerr=ub2−pF<p0.ProofWe use the same shorthand notations as before:x=P(X),a=P(A),γ=γ(X,A). The error iserr=ub2−pF=ub2−ub1+ub1−pF, whereub1−pF<p0(q121−q1)by Theorem 2. Whenγ≥2,ub2(being a decreasing function ofγ) is bounded byub2=p0(γ(1−a−x+xaγ)γ−1)≤p02(1−a−x+2xa). Therefore, the error is bounded byerr<p0(2(1−a−x+2xa)−1−q1J+11−q1+q121−q1)=p0(2−2a−2x+4xa−(1−q12−q1J+1)(1−q1))=p0(2(1−q1)−2a(1−q1)−2x(1−q1)+4xa(1−q1)−1+q12+q1J+11−q1)=p0(1−2q1+q12+q1J+1−2a(1−q1)−2x(1−q1)+4xa(1−q1)1−q1).Whenγ≥2,q1≤12, and thus1−2q1+q12+q1J+1=1−q1(2−q1−q1J)<1−q1(2−12−12)=1−q1. Therefore,err<p0((1−q1)(1−2a−2x+4xa)1−q1)=p0(1−2a−2x+4xa). The latter factor is always≤1, because2a+2x−4xa=2a(1−x)+2x(1−a)≥0. Thereforeerr≤p0.□The previous results (Corollary 1 and Theorem 3) show that even for relatively weak dependencies, the absolute error of bothub1andub2is less thanp0. This also means that the approximatedpFis at most twice the realpF, becauseub<pF+p0≤2pF. A 200% upper bound for the relative error may sound large, in comparison to the traditional significance levels likeα=0.05orα=0.01. However, whenpFis used as a goodness measure in data mining (like dependency rule analysis), the values are of a totally different magnitude. For example, when the 100 best dependency rules were searched from seven classical benchmark data sets in Hämäläinen (2012), the largestln(pF)value varied from −350 to −73000 depending on the data set. These values correspond topF=10−152…10−31703.Our experimental results support the theoretical analysis, according to which both upper bounds,ub1andub2, give tight approximations to Fisher’spF, when the dependency is sufficiently strong. However, if the dependency is weak, we may need a more accurate approximation. A simple solution is to use exact values of themfirst (largest) termsp0+p1+⋯pm−1and estimate an upper bound only for the smallest termspm+⋯pJusing the sum of a geometric series. (Note that, once again, the sum of the geometric series produces one more exact term,pm, so that we end up withm+1exact terms.) The resulting approximation and the corresponding error bound are given in the following theorem. We omit the proofs, because they are essentially identical with the previous proofs for Theorems 1 and 2.Theorem 4For a positive dependency ruleX→AholdspF≤p0+⋯pm−1+pm(1−qm+1J−m+11−qm+1),whereqm+1=(fr(X¬A)−m)(fr(¬XA)−m)(fr(XA)+m+1)(fr(¬X¬A)+m+1)andm+1≤J. The absolute error of the approximation iserr<pl(qm+121−qm+1).The tail probability,pm+⋯pJ, is the same as thepF-value of a hypothetical ruleY→A, which hasfr(Y)=fr(X)andfr(YA)=fr(XA)+m. If we use the lift of this hypothetical rule, we can give another upper bound analogous toub2:Corollary 3For a positive dependency ruleX→AholdspF≤p0+⋯pm−1+pm(1+1−P(A)g(X,A,m)−P(X)g(X,A,m)+P(X)P(A)g(X,A,m)2g(X,A,m)−1),whereg(X,A,m)=n(fr(X,A)+m)fr(X)fr(A)=γ(X,A)+nmfr(X)fr(A)andm≤J−1. Thisg(X,A,m)is the lift of hypothetical (not necessarily existent) ruleY→Awhich hasfr(Y)=fr(X)andfr(YA)=fr(XA)+m.These kinds of generic upper bounds can be adjusted to as accurate as one wishes, but in the same time the computational cost increases. However, in practice, 10 or 15 terms from hundreds or thousands of terms give already very tight approximations even for weak dependencies.The goal of the experimental evaluation was to evaluate both the accuracy and computational efficiency of the new upper bounds. For this purpose, several experiments were done simulating the requirements of typical pattern mining algorithms. In addition, we evaluated the performance of mining dependency rules from real world data sets using the simplest upper bound,ub2, instead of exactpF.All experiments were run on 2.7 GHz Intel i7-2620M processor having 8 GB RAM and using Linux operating system (Kubuntu 11.10, kernel 3.0). The source code for the C-language implementations of all evaluated functions is available on http://www.cs.uef.fi/~whamalai/fishersourcecode.html.In the accuracy comparison, we compared the exactpF-value to four upper bounds,ub1,ub2,ub3,ub4, and theχ2-based approximation,pχ2. Upper boundsub1andub3were based on the sum of a geometric series andub2andub4were their simpler counterparts. Upper boundsub1andub2contained one exact term (as defined in Theorem 1) andub3andub410 exact terms (as defined in Theorem 4 and Corollary 3). Theχ2-based approximation was determined by equationpχ2=erfc(χ2(X,A)2)2, whereerfc=1−erf(x)is the complementary error function. This is half of the tail probability (the cumulative distribution function of theχ2-distribution with 1 d.f. is simplyerf(x2)). Because theχ2-test is non-directional, the tail probability estimatesP(δ≤−δ(X,A)∨δ≥δ(X,A)), i.e. the probability of observing a positive or a negative dependency whose squared leverage is at leastδ(X,A)2, ifXandAwere actually independent. Since we wanted to approximateP(δ≥δ(X,A)), we used the common convention and halved the probability. This is fully correct, if eitherP(X)orP(A)is 0.5 (and thusP(δ≤−δ(X,A))=P(δ≥δ(X,A))), but otherwise it causes some error.The comparisons were done for both balanced and unbalanced data distributions and for different data sizes. In the balanced case, we usedP(X)=P(A)=0.5and in the unbalanced caseP(X)=0.25andP(A)=0.20. The data sizes weren=100,n=1000,n=10000andn=100000. In practice, approximations are not needed with small data sizes (n=100andn=1000), but these cases were included to show the behavior of new upper bounds with smalln. For the same reason, we tested also very weak dependencies (nearly independence), which are not be tested in practice.Fig. 1shows the behavior ofpF, four upper bounds and theχ2-based approximation as a function offr(XA), whenn=100. Graphs for largernare not shown, because then all measures have too wide range to be pictured. However, the behavior is similar: for weak dependencies, thepF-value is relatively large and the errors ofub1,ub2andpχ2are also largest. The more accurate upper bounds,ub3andub4, coincide nicely with the exactpF. When the dependency becomes stronger, the measure values drop very fast and errors diminish.Absolute errors ofub1,…,ub4andpχ2are given in Table 2(forn=100andn=1000) and Table 3(forn=10000andn=100000) for selectedfr(XA)values. The corresponding leverage and lift values are also given to characterize the strength of the dependency. The weakest dependencies have leverageδ=0.01and strongestδ=0.20(balanced distribution, whereδ≤0.25) orδ=0.13(unbalanced distribution, whereδ≤0.15). The corresponding lift values vary from 1.04 to 1.80 (balanced distribution, whereγ≤2.00) and from 1.20 to 3.60 (unbalanced distribution, whereγ≤4.00), respectively. In addition, the tables give exactpF-values as a reference.All measures were calculated with aC-program using type long double for all numeric representations. With the gcc compiler, this corresponds to the 80-bit precision. Still, this precision was not sufficient to determine the exactpFor any of its approximations for the strongest dependencies withn=100000(an underflow occurred and the values were rounded to zero). Calculating logarithms instead of probabilities would prevent underflows, but there is noClibrary function forlog(erfc).The results were very good. For all practically occurring cases, i.e.,n≥10000(or at leastn≥1000) andδ≥0.05, even the weakest upper bound,ub2, gave accurate results. Forδ=0.05, the relative error ofub2was always less than 1%, and decreased when the dependency became stronger. Here, we noticed an interesting regularity: the relative error decreased approximately to110whennbecame 10-fold. Forδ=0.05, the relative errors were 0.7% (n=1000), 0.07% (n=10000) and 0.007% (n=100000) for the balanced distribution and 0.13% (n=1000), 0.014% (n=10000) and 0.0014% (n=100000) for the unbalanced distribution. Upper boundub1performed always better thanub2, but the difference was surprisingly small, especially for stronger dependencies. Upper boundsub3andub4were clearly superior and gave excellent approximations even for smallnand the weakest dependencies. This is a noteworthy result, because both upper bounds estimated only 10 terms exactly from over 10000 terms in the case ofn=100000.Theχ2-based approximationpχ2could compete with the simplest upper bounds,ub1andub2, only for the weakest dependencies (δ=0.01). With these, the absolute errors were of the same magnitude forn=100,n=1000andn=10000. Withn=100and the balanced distribution,pχ2was even more accurate thanub2. However, this performance has little practical meaning, because one would hardly test so weak dependencies. In all other cases (stronger dependencies and even the weakest dependencies forn=100000), even the simplest upper bounds clearly outperformedpχ2.The inaccuracy of thepχ2-values was rather surprising. It has been traditionally assumed that theχ2-approximation works well, when the data distribution is balanced,nis large, and all expected cell counts are sufficiently large (at least 5, according to a classical rule of thumb) (Agresti, 1992). In our experiments, the balanced distribution produced slightly better approximations than the unbalanced one, in comparison toub1, but only for weaker dependencies. For stronger dependencies, allpχ2-values were inaccurate. For the balanced distribution, thep-value was seriously overestimated and for the unbalanced distribution, seriously underestimated (near zero, resulting an error of sizepF). Small expected cell counts occurred only in one case (the unbalanced distribution withn=100), but even there, the smallest expected cell count was 5. Increasingndid not improve, but, rather, detracted the accuracy.Cumulating rounding errors could not explain so large discrepancy (orders of magnitude) betweenpχ2andpF. However, we checked the correctness of programs carefully and compared the results to other tools. Whenn=100, allpχ2-values could be verified using an onlineχ2-calculator (Online probability distributions, 2014) and forn=1000, a similar validation could be done using an on-line erfc-calculator (erfc, 2014). (A similar comparison had been done forpF-values earlier, when the Kingfisher program (Hämäläinen, 2012) was developed.) For largernwe did not find any correctly working alternative calculator with a sufficient precision. However, since the inaccuratepχ2-values were correct forn=100andn=1000, it is unlikely that the inaccuracies in Table 3 could result from computation errors.After all these considerations, we can only conclude that theχ2-based approximation to Fisher’ exact test is even more unreliable than previously thought. This is in line with earlier observations made with small data sets, for both 2×2 and generalr×ctables (Verbeek and Kroonenberg, 1985; Mehta and Patel, 1980, 1983).In the speed comparison, we compared the execution times for calculating either the exactpF, its upper bound with 1 exact term (ub1andub2), 10 exact terms (ub3andub4) or 100 exact terms (ub5andub6). Upper boundsub1,ub3andub5were based on the sum of a geometric series andub2,ub4andub6were their simplified versions.For parameter values, we usedn=100000,fr(X)=50000andfr(A)=50000. Thefr(XA)value varied withJ=fr(X)−fr(XA), whenJwent from 1000 to 25000 (andfr(XA)from 26000 to 50000). We recall thatJis the number of terms inpFwhich, together withp0, determines the computational cost. We tried also other parameter values, smaller and largern, balanced and unbalanced distributions, but these had quite small effect on the execution time of upper bounds orp0. Only upper boundsub1,ub3andub5(which use the sum of a geometric series) were somewhat sensitive ton(faster with smallern). Increasingnto 10-fold increased the execution of 100000ub1(ub3,ub5) commands by about 10 ms. When computing the exactpF,Jis the most important factor, andnhas only an indirect effect, by allowing largerJ.For a better precision, the execution times were measured per 100000 repeated calculations ofpFor its upper bounds. In addition, the test program itself was repeated three times and the results were averaged. It is quite common that the processor times can vary 5%–10% between consecutive executions.For all compared measures, we estimated only the sumQ=1+q1+q1q2+⋯+q1q2⋯qJ, wherepF=p0Q(for notations, see the proof of Theorem 1). The reason is that all upper bounds assume thatp0is given (as it is in typical mining algorithms). On the other hand, the most effective way to calculate the exactpFis to calculate the heavy binomial coefficients just once (forp0) and then calculate theQ-sum recursively. This approach has also other benefits. It allows the use of logarithms instead ofp-values which tend to underflow or factorials and binomial coefficients which tend to overflow. In addition, the logarithms of factorials can be tabulated, which allows fast evaluation of binomial coefficients. TheQ-sum itself does not underflow or overflow, if the calculation is done recursively. These are all well-known tricks which have been used also in the previous algorithms (Verbeek and Kroonenberg, 1985).The results of the speed comparison are shown in Fig. 2. The execution time for the exactQ-sum grows linearly as a function ofJ, from 0.74 to 19.15 s. All upper bounds took only a constant time:ub118 ms,ub20.8 ms,ub326 ms,ub47.5 ms,ub594 ms andub676 ms. This means that even the most accurate upper bound,ub5(using 100 exact terms, which is more than sufficient for any practical purposes), saved 87%–99.5% of the execution time, in comparison to the exact calculation. The fastest and least accurate upper bound,ub2, took only 0.00004–0.001 of the exact calculation time and the 10-term approximationub4(which is practically as accurate asub3) took 0.04%–1.0% of the exact calculation time. These are remarkable savings and certainly needed in exhaustive data mining algorithms, where the number of terms may be even larger (oftennis millions) and the estimation ofpFhas to be done millions of times.In these experiments, the objective was to test how the new approximations affect the accuracy and speed when dependency rules are mined from real world data.The experiments were done using the Kingfisher program (Hämäläinen, 2012) which searches for the best, non-redundant dependency rules with the selected goodness measure. The requirement of non-redundancy means that ruleX→A=vcan be selected only if it is better than all more general rules,Y→A=v,Y⊊X, with the selected goodness measure. In practice, this means preference for simpler and stronger (more significant) rules. The default goodness measures is the exactpF(orln(pF), to avoid underflow) which has been implemented using tabulated factorials (i.e., the current state-of-the-art technique). In addition, we implemented the simplest (and least accurate) of the new approximations,ub2(ln(ub2)), as an alternative search measure.The data sets are described in Table 4. Heart contains features extracted from cardiac computer tomography (SPECT) images; Mushroom contains descriptions of hypothetical samples corresponding to 23 species of gilled mushrooms; Plants lists all plant species and genera from the USDA plants database and the states of USA and Canada where they occur; and Pumsb contains census data for population and housing from USA. Binarized versions (in the transactional form) are available either in the UCI Machine Learning Repository (UCI Machine Learning Repository, 2013) (Heart and Plants) or the FIMI repository (FIMI, 2010) (Mushroom and Pumsb).From each data set, we searched for the 100 and 1000 best rules with both exactpFandub2. The accuracy ofub2with respect to the exactpFwas evaluated by comparing the discovered rules and their measure values with the given precision (Kingfisher uses float type for storing measure values and prints four significant figures forln(p)andln(ub2)). The speedup byub2was more difficult to evaluate, because the total execution time depends on many factors. Therefore, we analyzed the distribution of execution times among program functions using the GNU profiler (gprof) program (Gnu, 2009).The results of the accuracy evaluation are given in Table 5. In three from four data sets,ub2produced identical results withpF. Only with the Heart data set, the approximation produced somewhat different results, although the same rules were discovered. When only the 100 best rules were searched for, the rule order was correct, but six rules had a slightly different measure value. However, the inaccuracy was very small, and only the least significant digits differed by one. This is probably due to truncation instead of rounding the measure values in the printing phase. When the 1000 best rules were searched for, the number of such inaccurate measure values was 220. In addition, two rules with equal measure values occurred in a different order in the two result sets.The results of the speed comparison are given in Table 6. When only the 100 best rules were mined, there were no big differences in the total execution times between the exact and approximated measures. The differences became more apparent with the 1000 best rules. In Plants, the speedup byub2was 12 s and in Pumsb, nearly two minutes. The analysis of the distribution of the execution times with the GNU profiler program revealed more dramatic results. Calculation of the exactpFvalue could take nearly 40% of the execution time and sometimes it was the heaviest subprocess instead of popcount (a routine for efficient frequency counting). The time consumption was smallest with the smallest data set (Heart) and when only the best 100 rules were searched for. This is quite natural because the number of evaluated terms is smallest, when the data set is smallest or the dependencies are strongest (ideally, just one term). Withub2, the time consumption was always substantially smaller. The savings were 3.8–13.4 percentage points with the search of 100 rules and 4.8–33.0 percentage points with 1000 rules. Sometimes, the measure evaluation withub2took so little time that it was rounded to zero (Mushroom, 100).Overall, the results were very good, especially concerning the accuracy of the new approximation. However, the results should be interpreted cautiously, because Kingfisher searches for only the best, non-redundant rules. Therefore, it has a strong preference for strong rules, even during the search (areas of the search space containing too weak or insignificant rules are pruned out). This means that the evaluatedpFvalues tend to contain only a small number of terms. Consequently, the approximation produces accurate results but the time saving is only modest. It is possible that the situation is different in a general enumeration task, where all sufficiently significant dependency rules are listed. This means a substantially larger number of measure evaluations, also on weaker dependencies with a large number of terms inpF. It is likely that the time savings are much more substantial but in the same time the approximated values are less accurate. As a solution, one should consider using the other introduced upper bounds or select a suitable accuracy level on-line.

@&#CONCLUSIONS@&#
We have introduced a family of upper bounds, which can be used to estimate thepF-value of Fisher’s exact test fast but accurately. Unlike theχ2-based approximation, these upper bounds are not sensitive to the data size, distribution, or small expected counts.All new approximations can be evaluated in a constant time (asymptotic complexityO(1)), while the exact calculation ofpFdepends on the data size (complexityO(n)). In practice, the execution time of the new approximations depends on the desired accuracy level. According to our experimental evaluation, even the simplest upper bound is sufficiently accurate for dependency rule mining purposes and it can be estimated in 0.004%–0.1% of the time needed for exact calculation. For other purposes (testing very weak dependencies), one may need more accurate approximations, which evaluate more terms frompFexactly. According to our experiments, 10 exact terms produced already highly accurate estimates (more than sufficient for any purposes), but still the computation time was always less than 1% of the exact calculation time. These are remarkable savings in data mining, where millions of patterns can be tested.The applications of the new approximations are not restricted to dependency rule mining, but there are other mining tasks where fast evaluation of Fisher’s exact test is crucial. For example, in bioinformatics, the interesting patterns are often graph-formed and the search space is even larger than in rule mining. This means an urgent need for fast measure evaluation. In the same time there is a growing interest in the exact tests, because the biological verification of patterns is expensive and should be used only for reliable discoveries. Fisher’s exact test is also used to test the significance of improvement in a more complex pattern against its simplifications. In the set-formed dependency patterns, the number of such tests can be quite prohibitive for exact calculation ofp-values.So far, we have concerned only the traditional Fisher’s exact test (2×2 case), but it is possible that the new upper bounds (or their generalizations) can be applied in the calculation of the general Fisher’s exact test (r×ccase), as well. In addition, the new upper bounds, especially their simpler versions, as well as the derived error bounds, can be useful in theoretical proofs.