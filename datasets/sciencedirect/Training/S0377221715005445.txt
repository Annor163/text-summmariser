@&#MAIN-TITLE@&#
The multi-objective generalized consistent vehicle routing problem

@&#HIGHLIGHTS@&#
The multi-period vehicle routing problem is extended by considering service consistency goals.We analyze the trade-off between arrival time consistency, driver consistency, and routing cost.Our results support companies in finding adequate consistency goals to aim for.A 70 percent better arrival time consistency is achieved by increasing travel time by not more than 3.84 percent, on average.Arrival time consistency improves as a side effect of visiting each customer with a single driver.

@&#KEYPHRASES@&#
Vehicle routing,Periodic distribution problems,Service consistency,Multi-objective optimization,

@&#ABSTRACT@&#
More and more companies in the routing industry are providing consistent service to gain competitive advantage. However, improved service consistency comes at the price of higher routing cost, i.e., routing cost and service consistency are conflicting objectives. In this paper, we extend the generalized consistent vehicle routing problem (GenConVRP) by considering several objective functions: improving driver consistency and arrival time consistency, and minimizing routing cost are independent objectives of the problem. We refer to the problem as the multi-objective generalized consistent vehicle routing problem (MOGenConVRP). A multi-objective optimization approach enables a thorough trade-off analysis between the conflicting objective functions. The results of this paper should help companies in finding adequate consistency goals to aim for. Results are generated for several test instances by two exact solution approaches and one heuristic. The exact approaches are based on theϵ-constraint framework and are used to solve small test instances to optimality. Large instances with up to 199 customers and a planning horizon of 5 days are solved by multi directional large neighborhood search (MDLNS) that combines the multi directional local search framework and the LNS for the GenConVRP. The solution quality of the heuristic is evaluated by examining five multi-objective quality indicators. We find that MDLNS is an eligible solution approach for performing a meaningful trade-off analysis.Our analysis shows that a 70 percent better arrival time consistency is achieved by increasing travel cost by not more than 3.84 percent, on average; visiting each customer by the same driver each time is significantly more expensive than allowing at least two different drivers per customer; in many cases, arrival time consistency and driver consistency can be improved simultaneously.

@&#INTRODUCTION@&#
Vehicle routing with consistency considerations is a multi-objective process: Companies put large efforts in optimizing vehicle routes in order to decrease cost. At the same time, many companies are willing to increase routing cost to visit each customer at similar times of the day with drivers familiar to them. This is because service consistency increases customer satisfaction and, thus, the lifetime value of customers. As proposed in Kovacs, Golden, Hartl, and Parragh (2014b), we improve driver consistency by reducing the maximum number of different drivers that visit a customer. Arrival time consistency is achieved by minimizing the maximum difference between the earliest and the latest arrival time at each customer within the planning horizon. Decision makers are faced with the task of choosing between a minimal-cost routing plan that is poorly consistent, a high-cost routing plan with perfect consistency (i.e., visiting each customer at exactly the same time with his favorite driver), or any routing plan in between these extremes.Several papers examine the trade-off between service consistency and routing cost, but the multi-objective nature of the problem is often oversimplified. For example, Coelho and Laporte (2013b), Francis, Smilowitz, and Tzur (2007), Groër, Golden, and Wasil (2009), Kovacs, Parragh, and Hartl (2014c), and Spliet (2013) compare the routing cost of solutions in which consistency is ignored to solutions in which consistency is enforced by hard constraints. In Coelho, Cordeau, and Laporte (2012), Coelho and Laporte (2013a), Kovacs, Golden, Hartl, and Parragh (2014a), and Smilowitz, Nowak, and Jiang (2013), the multi-objective problem is transformed into a single-objective problem by aggregating routing cost and consistency measurements into a single objective function. This approach involves a priori decisions about the importance of each objective, i.e., service consistency needs to be expressed in monetary gain. Feillet, Garaix, Lehuédé, Péton, and Quadri (2014) introduce the time-consistent vehicle routing problem with two objectives: minimizing routing cost and improving arrival time consistency. A general survey on consistency considerations in vehicle routing is given in Kovacs et al. (2014b).Multi-objective optimization is a flexible approach for analyzing the trade-off between conflicting objective functions. With conflicting objectives, there may exist a large number of relevant solutions. These solutions are not optimal in the sense of single-objective optimization; rather, they are trade-off solutions, i.e., solutions that cannot be improved in one objective without deteriorating another. Among all solutions found, the one that maximizes the utility of the current decision maker will be implemented in the field.The active interest by researchers and practitioners in multi-objective combinatorial optimization (MOCO) has generated a vast amount of literature. Survey papers are presented, e.g., by Ehrgott and Gandibleux (2002). Prominent examples of mathematical programming-based optimizers for multi-objective problems are theϵ-constraint method (e.g., Srinivasan & Thompson, 1976) and the two-phase method (e.g., Przybylski, Gandibleux, & Ehrgott, 2008; Vise, Teghem, Pirlot, & Ulungu, 1998). Both approaches provide trade-off solutions by repeatedly solving a single-objective problem with changing parameters (e.g., tightness of constraints and coefficients of the objective function). Modified branch-and-bound and branch-and-cut algorithms solve the multi-objective problem in a single run (e.g., Jozefowiez, Laporte, & Semet, 2012; Parragh & Tricoire, 2014; Vincent, Seipp, Ruzika, Przybylski, & Gandibleux, 2013).Heuristic multi-objective optimizers are often based on evolutionary algorithms (EAs). EAs maintain a pool of solutions (called population) during the search process. Therefore, EAs are particularly suited for finding many trade-off solutions in a single run. An overview of evolutionary solution approaches is presented, e.g., in Fonseca (1995), Konak, Coit, and Smith (2006) and Zitzler and Thiele (1998). Multi-objective extensions of single-objective heuristics such as variable neighborhood search, ant colony optimization, and simulated annealing are presented, e.g., in Jozefowiez, Semet, and Talbi (2002), Parragh, Doerner, Hartl, and Gandibleux (2009), Doerner, Gutjahr, Hartl, Strauss, and Stummer (2004), and Czyżak and Jaszkiewicz (1998). Paquete, Chiarandini, and Stützle (2004) and Tricoire (2012) present heuristic frameworks for MOCO problems that integrate various local search strategies.In this article, we present the multi-objective generalized consistent vehicle routing problem (MOGenConVRP). The problem is based on the generalized consistent vehicle routing problem (GenConVRP, Kovacs et al., 2014a) that aggregates routing cost and arrival time consistency into a single objective function; the number of different drivers per customer is bounded. In the MOGenConVRP, routing cost, arrival time consistency, and driver consistency are independent objectives of the problem. Solving this multi-objective problem in practice is unrealistic: typically, the long computation time caused by the large search space and the high number of relevant solutions is prohibitive. The aim of our work is to find all trade-off solutions for different test instances; these solution sets enable a thorough analysis of the trade-off between arrival time consistency, driver consistency, and routing cost. Our results should help companies in the routing industry in finding adequate consistency goals to aim for. We devise two exact solution approaches and one heuristic. The exact approaches are based on theϵ-constraint method (Haimes, Lasdon, & Wismer, 1971; Laumanns, Thiele, & Zitzler, 2006; Srinivasan & Thompson, 1976). For both exact approaches, the computation time is reduced by introducing new valid inequalities and by exploiting special properties of the problem. Optimal solutions for the MOGenConVRP are important for benchmarking tests that measure the quality of approximation methods. Our heuristic is a combination of the large neighborhood search algorithm (LNS) for the GenConVRP (Kovacs et al., 2014a) and the multi directional local search framework (MDLS, Tricoire, 2012). We refer to the heuristic algorithm as multi directional large neighborhood search (MDLNS). The quality of the MDLNS is validated by computing several unary quality measurements for multi-objective solution sets. The computation time of the algorithms is secondary for our analysis. For practical applications, our results provide a guideline for choosing proper bounds (in hard-constraint models) or weights (in soft-constraint models) on consistency measurements. The resulting single-objective problems can be solved quickly and provide a routing plan for a long period of time.Our paper provides several contributions. In Section 2, we introduce and formally define the multi-objective generalized consistent vehicle routing problem with three objective functions: arrival time consistency, driver consistency, and travel cost. We propose two exact solution approaches with new valid inequalities and one heuristic; the algorithms are described in Section 3 and Appendix A. For analyzing the trade-off between conflicting objectives, we introduce a transformation scheme that allows aggregating results across several instances in a meaningful way. Computational experiments are presented in Section 4 and numerical results are summarized in Section 4.4.The MOGenConVRP is based on the GenConVRP (Kovacs et al., 2014a) and is modeled on a complete directed graphG=(N0,A).N0={0,1,…,n}is the set of nodes representing customer locations and the depot 0, N is the set of customers (N0\{0}). Customers are divided into AM customers who can only be visited in the morning (Nam⊆ N) and PM customers who require a visit in the afternoon (Npm⊆N,Nam∩Npm={},Nam∪Npm=N). This constraint is important for customers that are not available all-day, require service at specific times of the day (e.g., medical treatment in home-care services), or hire part-time workers to handle package deliveries (Wong, 2008). Splitting the day into two halves also models applications in which drivers perform pick-up operations in the morning and delivery operations in the afternoon. Our algorithms are applicable to problems with and without time windows. Yet, the problem can be solved faster if time windows are incorporated.The planning horizon involves |D| days where D is the set of days. Set Nf⊆ N contains all customers that require at least two visits during the planning period; consistency goals are only relevant for customers in Nf.A={(i,j):i,j∈N0,i≠j}is the set of arcs. Arcs from customers ∈ Npmto customers ∈ Namare omitted from the graph. Each arc (i, j) ∈ A is associated with travel time tij. The travel time matrix satisfies the triangle inequality. Customers are visited by a homogeneous fleet of vehicles in the setK={0,1,…,v}. The number of vehicles is not restrictive (i.e.,v=n). Each vehicle has a capacity of Q. Drivers and vehicles share a one-to-one relationship. We use drivers and vehicles interchangeably, depending on the context. Each route starts and ends at the depot. The departure time is flexible, but vehicles must return to the depot before time T. On each day d ∈ D, each customer i ∈ N has demand qidand service time sid. Auxiliary parameters widare set to 1 if customer i requires service on day d (qid> 0) and set to 0, otherwise. Each set of nodes is specified by index d to denote customers that require service on day d (e.g.,Nd0contains customers that require service on day d and the depot). Additionally, we define set Ad⊂ A that contains arcs between customers in Nd;Ad0also contains the arcs from and to the depot. The model uses the following binary variables:xijkd={1,ifarc(i,j)∈Ad0istraversedbyvehiclekondayd,0,otherwise;yikd={1,ifcustomeriisassignedtovehiclekondayd,0,otherwise;zik={1,ifcustomeriisassignedtovehiclekatleastonceintheplanningperiod,0,otherwise.Variables aiddenote the arrival time at customer i ∈ Ndon day d; zmaxis the maximum number of different drivers that any customer encounters; lmaxgives the maximum arrival time difference, i.e., largest difference between the latest and the earliest arrival time per customer among all customers. Vehicle idling to reduce the arrival time difference is not allowed. There are several reasons for adding this constraint: first, idling is unproductive working time that has to be paid for by the service provider; second, planned idle times are often ignored by the staff; third, Kovacs et al. (2014c) show that a high level of arrival time consistency can be achieved without idle times if vehicle departure times are flexible.The objective (1) is to minimize a vector of conflicting objective functions. The vector f is composed of the total travel time (2), the maximum number of different drivers per customer (3), and the maximum arrival time difference (4).(1)minf=(f1,f2,f3)(2)f1=∑d∈D∑k∈K∑(i,j)∈Ad0tijxijkd(3)f2=zmax(4)f3=lmaxsubject to(5)y0kd=1∀k∈K,d∈D(6)∑k∈Kyikd=1∀i∈Nd,d∈D(7)∑i∈Ndqidyikd≤Q∀k∈K,d∈D(8)∑i∈Nd0xijkd=∑i∈Nd0xjikd=yjkd∀j∈Nd0,k∈K,d∈D(9)aid+xijkd(sid+tij)−(1−xijkd)T≤ajd∀(i,j)∈Ad,k∈K,d∈D(10)aid+xijkd(sid+tij)+(1−xijkd)T≥ajd∀(i,j)∈Ad,k∈K,d∈D(11)zik≥yikd∀i∈Ndf,k∈K,d∈D(12)∑k∈Kzik≤zmax∀i∈Nf(13)(aiα−aiβ)wiαwiβ≤lmax∀i∈Nf,α,β∈D(14)[t0i,min(T2,T−ti0−sid)]∋aid∀i∈Ndam,d∈D(15)[max(t0i,T2),T−ti0−sid)]∋aid∀i∈Ndpm,d∈D(16)xijkd∈{0,1}∀(i,j)∈Ad0,k∈K,d∈D(17)yikd∈{0,1}∀i∈Nd0,k∈K,d∈D(18)zik∈{0,1}∀i∈Nf,k∈KEqualities (5) ensure that each route starts from the depot. Constraints (6) guarantee that each customer is serviced on each day he requires service. Inequalities (7) limit the vehicle capacity to Q. Equalities (8) are flow conservation constraints. Inequalities (9) and (10) set the arrival times at the customers. Vehicle idling to improve time consistency is prohibited by (10). Inequalities (9) also prevent sub-tours. Driver consistency is defined in (11) and (12). Constraints (11) set the z variables and constraints (12) set the maximum number of drivers per customer. Constraints (13) define the maximum arrival time difference. Finally, constraints (14)–(18) define the domains of the decision variables. Time window feasibility is ensured by restricting the domains of the arrival time variables; this approach reduces the computation time of the applied integer linear programming optimizer.With multiple objectives of equal rank, we cannot decide whether or not a solution is better than another for any pair of solutions, i.e., the multi-objective search space is partially ordered. Comparable solutions are ordered by a dominance relation defined as follows: solution s1 dominates solution s2 (s1≻s2) if and only if ∀ j ∈ {1, 2, 3}: fj(s1) ≤ fj(s2) and ∃ j ∈ {1, 2, 3}: fj(s1) < fj(s2).A solution s is efficient if and only if there is no other solution s′ that dominates s. Set E contains all efficient solutions,E={s:∄s′such that s′≻s}. (Note: solutions in E are incomparable and equally good for a neutral decision maker.) The Pareto-front is defined asP={f(s):s∈E}. Elements of P are said to be non-dominated points in the objective space. Depending on the context, we use solution either to denote an element of E or an element of P.Our goal is to generate P in order to study the trade-off between travel cost and service consistency. There are at least as many efficient solutions as non-dominated points, i.e., several solutions might give the same objective vector. For us, it suffices to find only one solution for each point in P.We propose two exact solution approaches (i.e., approaches that find all points in P) based on theϵ-constraint method. In Section 4, we show that the exact algorithms are able to solve MOGenConVRP instances with up to 10 customers that require service over 5 days. In order to study realistic problem instances, we also devise a metaheuristic algorithm that provides an approximation of P, denoted by Papprox. MDLNS integrates the large neighborhood search algorithm for the GenConVRP (Kovacs et al., 2014a) into the multi directional local search framework (Tricoire, 2012). The LNS is an advanced approach for single-objective consistent vehicle routing problems; the performance of the MDLS is comparable to the best known solution approaches for three different combinatorial optimization problems: the multi-objective multi-dimensional knapsack problem, the bi-objective set packing problem, and the bi-objective orienteering problem (Tricoire, 2012). In this section and in Appendix A, we describe the algorithms in detail and present problem-specific enhancements.In theϵ-constraint method, the multi-objective problem is transformed into a single-objective problem by optimizing one objective function and restricting the objective value of the remaining functions. By modifying the tightness of the constraints, we can identify different elements of the Pareto-front. The single-objective problem is solved by a procedure that we refer to asopt(f,ϵ,ϵ′); it is defined as follows (Laumanns et al., 2006):(19)lexminf(s)=(f1,f2,f3)subject to(20)ϵj≤fj(s)<ϵj′∀j∈{2,3}(21)s∈SThe objective is to minimize the three objective functions in lexicographic order (19): f1 first, f2 second, and f3 third. Constraints (20) areϵ-constraints that bound the objective value of f2 and f3. Set S contains all feasible solutions defined by constraints (5)–(18). Expression (21) restricts the search space to solutions from S.Without lexicographic optimization, the procedure might return solutions that are not efficient. For example, a solution s1 with minimal travel cost and objective vectorf(s1)=(100,3,21)is dominated by solution s2 withf(s2)=(100,2,20). The computation time for finding s1 will be wasted, if s1 and s2 are both within the same search space. By performing a lexicographic optimization, we can guarantee that the provided solution is always non-dominated for the given search space. The output of the procedure is either an optimal solution for the constrained problem (if a feasible solution exists) or null (if the search space is empty).In our implementation,opt(f,ϵ,ϵ′)is based on a branch-and-bound algorithm. The lexicographic optimization is performed in two phases: The single-objective problem with f1 as the only objective is solved in the first phase; let s1 be the provided solution. In the second phase, we restrict the f1 value of the second solution s2 to the previously obtained value (f1(s2) ≤ f1(s1)) and solve the problem with the following objective function:(22)minf2(s2)+f3(s2)UB3.Parameter UB3 is an upper bound for f3 such that UB3 > f3. (A trivial upper bound for f3 is half the closure time at the depot T/2.) This approach optimizes f2 and f3 in lexicographic order sincef2∈Nandf3(s)UB3<1. Furthermore, the second phase can be solved with minimal effort: once the first phase solution is found, the second phase can be solved by exploring the unprocessed nodes of the branch-and-bound tree; all other nodes have already been pruned either by infeasibility or by the f1-bound. The resulting solution is the correct output ofopt(f,ϵ,ϵ′).We propose two algorithms that repeatedly applyopt(f,ϵ,ϵ′)in order to obtain P. The algorithms differ in the scheme theϵ-constraints (20) are modified. The first approach, denoted by three dimensional adaptiveϵ-constraint method, is described in the next section. The second approach is called two dimensional adaptiveϵ-constraint method; it performs slightly worse than the three dimensional method and is described in Appendix A. The total computation time of both algorithms depends mainly on the computation time ofopt(f,ϵ,ϵ′). The optimization process can be speeded up by aborting the procedure when a certain optimality gap (i.e., difference between the best found solution and the best lower bound) is reached. Yet, this might prevent us from generating the optimal Pareto-front.Our first exact algorithm is referred to as three dimensional (3D) adaptiveϵ-constraint method. It is based on the framework proposed by Laumanns et al. (2006). The authors present a general algorithm for solving multi-objective problems with an arbitrary number of objectives, h. The worst case time complexity of the algorithm, measured by the calls ofopt(f,ϵ,ϵ′),depends on the number of points in P (|P|) and the number of objectives:O(|P|h−1).The basic concept is anh−1dimensional hypergrid that divides the objective space into rectangular subspaces that are parallel to the axes, e.g., into stripes in bi-objective problems (Laumanns et al., 2006). Each cell in the grid represents one constrained search space for whichopt(f,ϵ,ϵ′)is applied. The coordinates of the grid are given by solutions that have been found earlier; so, the number of cells in the grid grows to the power ofh−1each time a new solution is found. In our case, the objective space is divided with respect to f2 and f3. For each efficient solution found during the search, we store the objective values f2 and f3 in vectorsef2andef3,respectively. Matrixe=(ef2,ef3)defines the coordinates of the grid.Fig. 1illustrates the f2–f3 projection of the objective space for three iterations of the 3D method, respectively. For each iteration, the figure on the left shows the initial grid and the figure on the right shows the grid after an efficient solution has been found. In the first iteration, the grid consists of a single cell that contains an efficient solution denoted by a point. The new solution divides the grid into four cells. The numbering of the cells starts with 0 in the lower left corner and increases successively column by column. In the second iteration, we continue the search in cell 0. Cell 3 is hatched because it represents a subspace that is dominated by the solution found in the first iteration. We find a new efficient solution in cell 0. The grid is partitioned and the cells are renumbered. In iteration 3, we again start the search with cell 0. Cell 4 is hatched because it is dominated by the solution found in iteration 2. Cells 5 and 7 are not dominated because they might contain solutions with a lower f1 value. The search in cell 0 is without result, i.e.,opt(f,ϵ,ϵ′)returns null. So, cell 0 is marked as searched and we continue with cell 1. Here, we find a new solution, partition the grid, and mark cell 6 as searched.It is essential to search the cells in ascending order of indices starting with cell 0: Let us assume that in iteration 2 there is a solution s2 with objective vector (100, 2, 12) in cell 2 and a solution s1 with objective vector (100, 2, 10) in cell 0. If we started with cell 2, we would partition the grid based on s2 even though s1 dominates s2.The pseudo-code of the algorithm is given in Algorithm 1. The procedure is initialized with an empty set of efficient solutions E, an empty set of already searched subspaces F, and the initial coordinates of the grid e. Variable m is an iteration counter that indicates the index of the cell that is to be searched and variable c is the current number of cells in the grid. Initially, the grid contains one cell that is defined by the lower and upper bounds on f2 and f3, respectively. In each iteration of the outer loop (lines 3–24), the algorithm provides one new solution. The inner loop (lines 4–18) examines each cell for new efficient solutions; in each iteration, the subspaces are explored in increasing number of cell index starting with index 0. The current subspace is provided by function getConstraints(m, e, |E|). The function takes the current number of iterations, the coordinates of the grid, and the cardinality of the set of efficient solutions as arguments and translates the current cell (i.e., iteration m) intoϵ-constraints (line 8) (the implementation of this function is given in Laumanns et al. (2006)). If the current search space has not yet been examined (line 9), we applyopt(f,ϵ,ϵ′). Depending on the outcome, we distinguish two paths: First,opt(f,ϵ,ϵ′)provides a solution that is dominated by a solution in E or there is no feasible solution in the respective cell. Then, we mark the current subspace defined byϵandϵ′as searched (line 12) and move on to the next cell (line 17). Second,opt(f,ϵ,ϵ′)provides a new efficient solution. In this case, we leave the inner loop (line 14), add the solution to E (line 19), and mark the subspace that is dominated by the new solution ([f(s),ϵ′]) as searched (line 20). In line 21, we call the function updateConstraints(f(s), e) that divides the objective space into smaller subspaces. This is achieved by inserting the new objective values into matrix e and updating the coordinates of the grid (for details on this function see Laumanns et al. (2006)). The number of cells grows with each solution found; c is updated in line 22. The algorithm generates one solution for each point in P, i.e.,|E|=|P|. This is guaranteed by the definition of the subspaces (inequalities (20)) and the lexicographic optimization within each subspace. Finally, we reset the iteration counter to zero in order to start the search in the inner loop with cell 0. The algorithm stops when all cells have been examined (lines 5–7); the output is a set of efficient solutions E.The computation time of our exact approaches is mainly affected by the lexicographic optimizeropt(f,ϵ,ϵ′). Therefore, we put emphasis on strengthening the model in a branch-and-cut fashion. In the following, we describe and evaluate the efficiency of five types of valid inequalities: Capacity cuts and subtour cuts are general inequalities for routing problems (see, e.g., Laporte & Nobert, 1987; Naddef & Rinaldi, 2001; Toth & Vigo, 2001). Symmetry breaking constraints have been investigated in Coelho and Laporte (2013b) and Fischetti, Salazar González, and Toth (1995); we present a modification of these constraints that is valid for the MOGenConVRP. Finally, two time consistency related inequalities are introduced.The model defined by inequalities (5)–(18) is complete. Nevertheless, adding a limited number of rounded capacity constraints (inequalities (23)) and generalized subtour elimination constraints (inequalities (24)) can speed up the optimization process.(23)∑i∉S∑j∈Sxijkd≥⌈∑i∈SqidQ⌉∀S⊆Nd,|S|≥2,d∈D,k∈K(24)∑i∈S∑j∈Sxijkd≤|S|−1∀S⊆Nd,|S|≥2,d∈D,k∈KDrivers are homogeneous in the MOGenConVRP; therefore, the number of feasible driver-customer assignments grows exponentially with the number of customers. If zmax≤ 1, there are vnfeasible assignments; v is the number of available drivers and n is the number of customers. Motivated by Coelho and Laporte (2013b) and Fischetti et al. (1995), we use symmetry breaking inequalities that reduce the number of feasible assignments significantly:(25)zik≤∑j<izj,k−1∀k∈K∖{0},i∈Nf(26)zik≤∑d∈Dyikd∀k∈K,i∈NfInequalities (26) restrict the zikvariables; inequalities (25) allow a customer–driver assignment (i, k) only if driverk−1visits a customer with a smaller index than i.Inequalities (25) are invalid if zmax≥ 2. In this case, the number of feasible driver–customer assignments is∏i=0zmax−1(v−i)nbecause each customer can be assigned to different drivers on different days. A slight modification of inequalities (25) is valid regardless of the number of different drivers per customer:(27)zik≤∑j≤izj,k−1∀k∈K∖{0},i∈NfProposition. Inequalities (27) are valid for the MOGenConVRP.Proof: LetP(N)be the power set of all customers.Nk∈P(N)is the set of customers assigned to driver k ∈ K and ikis the customer with smallest index in set Nk. By sorting the ik’s in non-descending order, we achieve eitherik−1<ikorik−1=ik∀k∈K∖{0}. □For a given bound on lmax(that is imposed by theϵ-constraint method), L, we can add inequalities to the model that exclude solutions with poor arrival time consistency. Time consistency-related inequalities TC1 (28) state that if customer i is visited before customer j on the same route on one day, then it is infeasible to assign customer j before customer i on the same route on another day if the arrival time consistency constraint would be violated. The reasoning behind these inequalities is illustrated in Fig. 2. The arrival time difference at customer i (li) issjβ+tjiminus the departure time from the depot a0α11We consider only two routes on two different days α and β. Therefore, we can assume that the vehicle departure time is fixed on day β and flexible on day α.; the arrival time difference at customer j (lj) issiα+tij+a0α. In an optimal solution liis equal to lj; otherwise, the maximum arrival time difference could be reduced by leveling the arrival time differences at the two customers. So,sjβ+tji−a0α=siα+tij+a0α. This leads toa0α=(siβ−sjα−tij+tji)/2and, therefore, toli=lj=(siα+sjβ+tij+tji)/2.(28)xijδα+xjiγβ≤1∀(i,j)∈Ad0:(siα+sjβ+tij+tji)/2>L,α,β∈D,δ,γ∈KTime consistency-related inequalities TC2 (29) prevent the assignment of a customer l between two customers i and j if i and j are scheduled one after the other on another day and if the assignment of l would violate the constraint on the maximum arrival time difference. Fig. 3illustrates the reasoning: The arrival time difference at customer i isli=a0αthe arrival time difference at customer j islj=til+tlj+slβ−tij−a0α. Again, we setli=lj,i.e.,2a0α=til+tlj+slβ−tij. Finally, we obtainli=lj=(til+tlj+slβ−tij)/2.(29)xijδα+xilγβ+xljγβ≤2∀(i,j),(i,l),(l,j)∈Ad0:(til+slβ+tlj−tij)/2>L,α,β∈D,δ,γ∈KOur heuristic approach for approximating the Pareto-front is based on the MDLS framework for general multi-objective problems (Tricoire, 2012). For a given solution, MDLS applies different local search strategies in order to identify new efficient solutions. Each objective value in the objective vector is improved by a specialized search strategy. As mentioned above, for the MOGenConVRP, we combine the MDLS framework with the LNS algorithm proposed in Kovacs et al. (2014a). The resulting algorithm is referred to as MDLNS.Algorithm 2gives the outline of the MDLNS. Starting with an initial set of efficient solutions E, we iteratively select an unprocessed solution (line 2) and perform a local search for each objective function (lines 3–5). New efficient solutions are added to E; processed solutions are marked in line 6. The algorithm stops as soon as all solutions have been processed.Set E is initialized by using the construction heuristics proposed in Kovacs et al. (2014a): two solutions are generated by a travel-time oriented heuristic; one by ignoring driver consistency and one by restricting driver consistency to one driver per customer. One solution is generated by a time-consistency oriented heuristic. Among the three solutions, we keep only efficient ones. Function applyLNS(s, fj, E), applies the LNS algorithm for a given number of iterations on solution s with the goal of improving objective function fj. The LNS was devised for the GenConVRP; it integrates arrival time consistency into the objective function and achieves driver consistency by restricting the feasible driver-to-customer assignments. In the MOGenConVRP, for each j ∈ {1, 2, 3}, the objective function fjis a weighted average of the total travel time and the maximum arrival time difference; the number of different drivers per customer is bounded because this objective is not suitable as an evaluation function. The f1 and f3 values are weighted by αjand(1−αj),respectively:(30)fj=αjf1+(1−αj)f3.We setα1=11+δUB3when we optimize f1 andα3=11+UB1δwhen we optimize f3; parameter δ is set to10−3and UBiis an upper bound for the respective objective function. By using α1 and α3, we perform a lexicographic optimization, respectively. In both cases, f2 is bounded byf2(s)+1in order to exclude solutions with large f2 values. Parameter α2 is set to(α1+α3)/2when optimizing f2 (α1 and α3 are calculated as defined above); additionally, f2 is bounded byf2(s)−1.Each solution found during the search is checked whether or not it is dominated by another solution in E; if not, it is added to E. Solutions that are dominated by the new solution are removed from E. This approach is time consuming; yet, we aim for finding the best possible approximation of the Pareto-front. Adding solutions to E is performed efficiently by using the data structure illustrated in Table 1. For each f2 value, we maintain a sorted list that contains all solutions with the respective f2 value. Sorting solutions in ascending order of f1 will automatically sort the solutions in descending order of f3. This results from the definition of an efficient solution: a solution can be improved in one objective only by deteriorating another. In the worst case, the computation time for finding the potential insertion position of a new solution s increases logarithmically with the number of solutions in E with the same f2 value. Solutions to the left of s and solutions with a smaller f2 value than s have a chance of dominating s; solutions to the right of s and solutions with a larger f2 value than s might be dominated by s.The data structure relies on the property thatf2∈Nand on the assumption that the domain of f2 is small. A similar approach for managing E is proposed in Tricoire (2012) for the bi-objective case; a general approach for maintaining efficient solutions for an arbitrary number of objectives is presented in Habenicht (1983) and further examined in Mostaghim and Teich (2005) and Sun and Steuer (1996).The computation time of the MDLNS can be reduced by applying applyLNS(s, f, E) only to a subset of E, by checking only selected solutions for dominance (e.g., best found solutions or current incumbent solutions) and ignoring other solutions that are generated during the search, and by reducing the number of LNS iterations.Solutions for different test instances are obtained by applying the described algorithms. In this section, we examine the results and analyze the trade-off between routing cost and service consistency. All algorithms are implemented in C++ and run on Intel Xeon X5550 computers with 2.67 GHz. Mixed integer linear programs (i.e., calls toopt(f,ϵ,ϵ′)) are solved by IBM’s CPLEX 12.5. Experiments are run on a single CPU except stated otherwise. Approximate solution sets are obtained by merging the solutions of three independent MDLNS runs, i.e., we perform three independent runs and aggregate the results to a single approximation set that contains only efficient solutions. The reported computation times are average computation times for a single run. The LNS algorithm embedded in the MDLNS performs robustly with regard to solution quality; for a sample of 10 runs, the average variation coefficient (standard deviation/mean) is 0.0077 (Kovacs et al., 2014a). The stochastic variations are further mitigated in the multi-objective problem: MDLNS explores the solution space extensively and we check for each generated solution whether or not it is efficient. The parameters of the LNS are set as proposed by Kovacs et al. (2014a).In the following, we introduce the data sets and examine the performance of the proposed solvers. In Section 4.3, we quantify the conflict between the objective functions. Additionally, we investigate the effect of lexicographically optimizing driver consistency first and travel time second on the arrival time consistency. This approach is similar to assigning customers to districts and serving each district with a separate driver on minimal cost vehicle routes. In Section 4.4, we summarize our findings.The test instances for the MOGenConVRP are taken from the GenConVRP (Kovacs et al., 2014a). The instances are extensions of the benchmark data set for the ConVRP by Groër et al. (2009). ConVRP instances are turned into GenConVRP instances by associating customers with AM/PM time windows. The new data sets are named C0.5, C0.7, and C0.9 (C={C0.5,C0.7,C0.9}), with respect to the service frequencies that are set to 50 percent, 70 percent, and 90 percent, respectively. Service frequency is a measure for the variation in the demand of the customers. A service frequency of 100 percent means that each customer is visited each day. In this case, we could achieve perfect service consistency by executing the same routing plan on each day of the planning horizon. With decreasing service frequency, there is more variation in the demand. Each data set consists of 12 instances with 50–199 customers and a planning horizon of 5 days. Additionally, we consider data set Csmalland CsmallEwith 10 and 5 instances, respectively. Set Csmallcontains instances with 10–12 customers that are visited over a planning horizon of 3 days. In set CsmallE, we extend the instances with 10 customers to a planning horizon of 5 days. Data set Csmalland CsmallEare derived from the small ConVRP instances presented in Groër et al. (2009).Experiments on data set Csmallare mainly performed for benchmarking tests. In this section, we examine the results to evaluate the efficiency of the single objective optimizer. Furthermore, we experiment with the 3D method and evaluate the performance (i.e., solution quality and computation time) of the MDLNS.In Section 3.1.2, we proposed several valid inequalities for the single-objective model. The efficiency of these inequalities is tested by optimizing only the total travel time (f1) of the instances in Csmalland by bounding the maximum number of different drivers per customer (f2) and the maximum arrival time difference (f3). Each instance is run with three different driver consistency configurations (zmax≤ Z, Z ∈ {1, 2, 3}) and four different arrival time consistency configurations (lmax≤lmin(i,j)∈A0{tij},l ∈ {0, 1, 2, 3}). We have a separate set of bounds on lmaxfor each instance, but for a specific instance the bounds are the same for all solver configurations. The total number of test instances is 120, i.e., 10 instances times (3 × 4) configurations.In Table 2, we present the results for different combinations of valid inequalities. The first column gives the configuration of the optimizer. Configuration “Plain” refers to the model specified by inequalities (5)–(18). The remaining rows refer to the plain model extended by the respective inequalities: SB are symmetry breaking inequalities, ST2 and ST3 are subtour elimination inequalities with|S|=2and|S|=3,respectively, TC1 and TC2 are time consistency-related inequalities, CC are capacity cuts, and “All” means that all inequalities are added to the model. Capacity cuts are added only if there are not more than 100 combinations of choosing |S| customers out of Nd(i.e.,(Nd|S|)≤100) and only for subsets S that require at least two vehicles (i.e.,∑i∈SqidQ>1). The second column gives the number of instances that were solved to proven optimality within 24 hours per instance. The remaining columns show the best, average, and worst speed up that is obtained compared to the plain model. The speed up values are averaged over the 83 instances that were solved by each configuration.Each configuration is helpful in increasing the number of instances solved. However, the computation time might increase by 12753 percent (from 82.75 seconds to 10636.6 seconds) in the worst case (see configuration TC2 + SB). Even though configuration TC2 + SB performs poorly, configurations TC2 and SB achieve good results separately. This indicates that combining different inequalities might deteriorate the performance significantly. The most efficient configuration is SB + ST3: we can solve 104 out of 120 instances with an average speed up of more than 18 percent; the average computation time for solving the 104 instances is 56 minutes. This configuration is used in the lexicographic optimizeropt(f,ϵ,ϵ′).In the MOGenConVRP, we assume that the number of vehicles is not restrictive. This assumption is necessary for achieving the highest consistency level for any problem instance: in the extreme case, we could assign each customer to one driver exclusively that visits the customer at exactly the same time of the day. However, in most solutions only a small fraction of the fleet is active; several customers are consolidated on each route in order to reduce travel cost. We define a reasonable fleet size by generating a solution withlmax=0,zmax=1,and a total travel time that is not worse than 1.2 times the optimal travel time. The fleet size is set to the number of vehicles used in the obtained solution. The computation time ofopt(f,ϵ,ϵ′)is significantly reduced by this preprocessing approach; however, it might prevent us from finding optimal solutions.In Table 3, we demonstrate results of experiments with the 3D method on data set Csmall. In the first column, we list the instances: in “convrp_x_test_y.vrp” x is a wildcard for the number of customers and y is a wildcard for the instance ID. The second column gives the computation time in minutes for the preprocessing phase, CPUpp(min). The third column shows the resulting decrease in the fleet size |K|. The fourth column is the number of points on the Pareto-front |P|. For the 3D method, we report the number of calls to the lexicographic optimizeropt(f,ϵ,ϵ′)and the total computation time in minutes, CPU(min), for generating the entire Pareto-front.On average, the preprocessing phase described above reduces the fleet size by 73.5 percent in 1 minute. The size of the model is significantly influenced by the number of vehicles; yet, our approach might generate solutions that are not optimal with regard to travel cost. The lexicographic optimizer is called 24.7 times (the average number of points on the Pareto-front is 20). The average computation time is more than four hours. Compared to the other instances, the computation time for solving convrp_12_test_3.vrp is significantly longer when the bound on the number of different drivers is loose. In this instance, several customers are arranged almost on a ray originating from the depot presumably causing a high level of symmetry.The performance of multi-objective heuristics is defined by the quality of the generated set of solutions and the required computation time. With a single objective, the quality of an optimizer is proportional to the achieved objective value: the lower the objective value (in minimization problems), the better the algorithm. However, in multi-objective optimization, we need to evaluate sets of objective vectors. Zitzler, Deb, and Thiele (2000) list three features of high quality multi-objective optimizers: the distance between Papproxand P is small, the points in Papproxare distributed evenly, and the range of each objective value (i.e., for each objective function, the difference between the best and worst objective value in Papprox, respectively) is large. Unary quality measurements assign each approximation set a single value that reflects a certain quality feature (Zitzler, Thiele, Laumanns, Fonseca, & da Fonseca, 2003). Typically, heuristics are evaluated by several quality measurements. Binary quality measurements assign a value to each pair of approximation sets and are used for comparing heuristics by pairs. Papers that examine quantitative approaches for evaluating multi-objective optimizers are presented, e.g., by Sarker and Coello Coello (2002), Van Veldhuizen and Lamont (2000), Knowles and Corne (2002), and Zitzler et al. (2003).We evaluate the quality of the MDLNS by applying five unary quality measurements. Each measurement has drawbacks that prevent us from using it as the only quality measurement. A meaningful evaluation of the algorithm is possible only if all measurements are considered at the same time.Providing more efficient solutions means giving the decision maker more choices (Schott, 1995; Van Veldhuizen, 1999). The cardinality of the Pareto-front (|Papprox|) gives the number of alternative solutions generated by the algorithm. However, this measurement ignores the quality of the approximation: a single solution might dominate the entire approximation set. Additionally, decision makers might be overwhelmed by an (unnecessarily) large number of solutions.The error ratio (ER) (Van Veldhuizen, 1999) gives the share of points in Papproxthat are not in P:(31)ER=∑p∈Papproxep|Papprox|.For each point p ∈ Papprox, epis 0 if p ∈ P and 1, otherwise. Lower ER values indicate better approximations; yet, the value may be misleading. Consider, for example, two approximation sets: one with a single solution that is ∈ P and one that contains many alternative solutions ∈ P and one solution ∉P. The first set has an ER value of zero and would, therefore, be preferred to the second set with ER > 0.The generational distance (GD) gives the distance between Papproxand P (Rudolph, 1998; Van Veldhuizen & Lamont, 1998, 1999):(32)GD=∑p∈Papproxdp2|Papprox|;dpis the Euclidean distance from a point p to the closest point q ∈ P (dp=minq∈P(p−q)(p−q)). A generational distance of zero indicates that all points in Papproxare also in P; the larger GD, the larger the distance between the approximation set and the optimal Pareto-front. Similar to the error ratio, GD may favor algorithms that generate one solution ∈ P over algorithms that find many solutions ∈ P but also solutions ∉P. Additionally, the results will be biased if the objective values are of different orders of magnitude.The unaryϵ-indicator (Iϵ) defines by how much an approximation set is worse than the optimal set with regard to all objective functions (Zitzler et al., 2003):(33)Iϵ=maxq∈Pminp∈Papproxmaxj∈{1,2,3}pjqj.For each point p ∈ Papproxand each point q ∈ P, we havepj≤Iϵqj∀j={1,2,3};Iϵ=1indicates that all points in Papproxare also in P. Theϵ-indicator is a worst case quality measurement; so, the quality evaluation might be based on a single poor solution in Papprox.Hypervolume (HV) is the size of the bounded objective space that is dominated by a set of solutions P:(34)HV={⋃pcp|p∈P}.Each point p ∈ P covers a space cpwhere cpis the volume of the cuboid that is defined by p and a reference point r (rj≥pj∀p∈P,j={1,2,3}). An example of a hypervolume in the two-dimensional objective space is given in Fig. 4: p and q are elements of P, r is a reference point, and cpand cqare areas dominated by p and q, respectively. The union of cpand cqis the hyperarea. The HV measurement is sensitive to the choice of the reference point as it may affect the quality evaluation (Knowles & Corne, 2002). For easier comparability, we report the gap between the HV of the optimal set P (HVP) and the HV value of the approximation set Papprox(HVPapprox):(35)GapHV=100(HVP−HVPapprox)HVPComputing HV in a three-dimensional objective space is nontrivial. We apply version 1.3 of the recursive, dimension-sweep algorithm for computing the hypervolume proposed and implemented by Fonseca, Paquete, and López-Ibáñez (2006) and Fonseca, López-Ibáñez, Paquete, and Guerreiro (2010).In Table 4, we evaluate the quality of the MDLNS on data sets Csmalland CsmallE. Solution sets are aggregated over three runs and results are averaged over all instances. The second column gives the number of points in P. The remaining columns show the five unary quality indicators for the MDLNS with 30, 40, and 50 thousand iterations per LNS run. The number of iterations seems to have a low influence on the solution quality. Starting from different solutions, the MDLNS explores the solution space by optimizing several objective functions. Therefore, the algorithm performs robustly regardless of the chosen parameters. Running the LNS with 20 thousand iterations does not provide the required performance; we have (ER, GD,Iϵ) = (0.411, 0.204, 1.096), on average, on data set CsmallE. In both data sets, each MDLNS configuration provides at least as many points on average as there are points on the Pareto-front. In Csmall, on average, 24.1 percent to 25.7 percent of the solutions are not element of P; in CsmallEthe average ER value is between 35.1 percent and 39.8 percent. The average generational distance is between 0.120 and 0.128 in Csmalland between 0.167 and 0.235 in CsmallE. The average unaryϵ-indicator shows that the approximation set is between 2.2 percent and 2.5 percent worse than the optimal set in Csmalland less than 9.2 percent worse in CsmallE. The MDLNS provides approximation sets that, on average, have only 0.424–0.477 percent smaller hypervolumes than P (GapHV) in Csmalland between 0.720 percent and 0.796 percent smaller hypervolumes in CsmallE. For each instance, the reference point for computing the hypervolume is (r1, r2, r3) where r1 is the longest total travel time, r2 is the maximum number of drivers per customer, and r3 is the maximum arrival time difference among all solutions in the examined approximation sets.The approximation sets generated by a single run with 40 thousand iterations are, on average, 0.64 percent worse in terms of theϵ-indicator in Csmalland 1.34 percent worse in CsmallE; the average GapHV increases from 0.477 percent to 0.562 percent in Csmalland from 0.796 percent to 1.134 percent in CsmallE. The results indicate that we cannot generate better solutions by increasing the number of iterations. However, aggregating approximation sets from independent runs can improve the solution quality. We conclude that the MDLNS approach is appropriate for performing a meaningful trade-off analysis.The computation time of the MDLNS is examined in Table 5for data set Csmalland in Table 6for data set CsmallE. As a reference point, we report the computation time of the 3D method in the second column; the remaining columns show the average computation time for the MDLNS with 30, 40, and 50 thousand iterations per LNS run, respectively. On data set Csmall(Table 5), the MDLNS requires not more than 2.01 minutes on average for a single run. This result is noticeable when compared to the 3D method that requires more than 4 hours. For solving the instances in CsmallE(Table 6), we run the 3D method in parallel on eight CPUs; the reported time is the wall-clock time. MDLNS runs on a single CPU. The 3D method requires more than 30 hours while MDLNS provides results between 3.34 and 5.66 minutes on average per run. This result indicates that applying the proposed exact solution approaches in practice is unrealistic.The trade-off analysis between total travel time, driver consistency, and arrival time consistency is performed on data sets C0.5, C0.7, and C0.9. The results are obtained by applying the MDLNS with 40 thousand iterations per LNS run. Solution sets are aggregated over three independent runs and dominated solutions are removed. An example of the output of the algorithm is shown at the left of Fig. 5. The horizontal axis of the diagram shows the maximum arrival time difference and the vertical axis shows the total travel time. The maximum number of drivers per customer is distinguished by colors; each f2 value is associated witha different color.Each point on the Pareto-front P represents a solution in the set of efficient solutions E, i.e., for each pointp=(p1,p2,p3),there exists a solution s ∈ E such thatp=f(s). Points p with a lower p3 value than0.01maxp∈P{p3}are excluded from the approximation set. In instance Christofides_4_5_0.7 with a planning horizon of 5 days and a service frequency of 70 percent, for example, the travel time increases by 332 percent in the solution withf3=0compared to the solution with the second best f3 value. Solutions with very small f3 values and very large f1 values would distort the analysis and are, therefore, treated as outliers. We think this approach would also be justified from a managerial perspective, because aiming for perfect arrival time consistency is unreasonable.The magnitude of the objective values varies instance-by-instance. In order to make general statements, we transform the Pareto-front P into an improvement front T that shows the percentage improvement in one objective as a function of the percentage improvement in another objective:(36)T={(a,b,c):a=p1N−p1p1N,b=p2,c=p3N−p3p3N∀p∈P}.Each point on the Pareto-front is transformed into a triple (a, b, c): a and c are the improvements in f1 and f3, respectively, compared to the Nadir-point(p1N,p3N)(p1N=maxp∈P{p1}andp3N=maxp∈P{p3}); b is the number of drivers per customer in the respective solution. The right figure in Fig. 5 shows the improvement front associated with the Pareto-front at the left. By putting the absolute values into perspective, we can aggregate several results in order to conduct a meaningful analysis.The correlation between total travel time and maximum arrival time difference is unclear when examining single results. For example, in the figure on the left of Fig. 6, we see a partly convex relationship, i.e., we have to give up much travel time in order to improve arrival time consistency a little. In the figure on the right, the maximum arrival time difference seems to improve gradually with increasing travel time. We examine the trade-off that is to be expected by aggregating the results of several instances as follows: The hull of the f1–f3 projection of the improvement front is defined by a set of line segments that are parallel to the f3-axis and bound the improvement in f1 from above. LetTf1−f3denote the subset of T that contains only points that are non-dominated in the f1–f3 plane. The points inTf1−f3are sorted in ascending order of the improvements in f3; the line segment between each pair of neighboring solutions(pi,pi+1)is defined by the improvement in f1 of pointpi+1. By quantizing the improvement in f3 and averaging the hull over several instances, we can aggregate the results of several instances and observe the correlation between the maximum arrival time difference and the total travel time that is to be expected. An example of a quantized hull, illustrated by horizontal dotted lines, is given at the left of Fig. 7; the average improvement curve for data sets with different service frequency is given at the right.By ignoring driver consistency, we obtain a relaxed improvement curve between cost and arrival time consistency. For examining a strict scenario, we use the same principle as above but the hull is defined only for points in{T|f2=1}. An example of this hull, again illustrated by horizontal dotted lines, is given at the right of Fig. 8; the left figure shows the average improvement curves for different service frequencies whenf2=1. The findings are summarized in Tables 7and 8. The tables show by how much the total travel time increases compared to the solutions with minimal travel time when the maximum arrival time difference is decreased in the relaxed scenario and in the strict scenario, respectively. The first column is the improvement in lmax; columns 2–4 are the percentage increase in the total travel time (TT) for data sets C0.5, C0.7, and C0.9, respectively. The last column gives the average increase in the travel time. The results are similar in both tables: we can improve arrival time consistency by 50 percent at the cost of increasing travel time by less than 1.58 percent, on average. Decreasing lmaxby 70 percent is achieved at 3.84 percent higher cost when driver consistency is ignored and at 2.43 percent higher cost when driver consistency is optimal, i.e., there is one driver per customer. Decreasing lmaxfurther significantly increases travel time: the travel time increases by 17.07 percent in the relaxed scenario and by 14.81 percent in the strict scenario.We can use the hull of the improvement front for examining the cost of driver consistency. In Table 9, we compare the relaxed scenario to the strict scenario. For each data set and for different levels of arrival time consistency, we give the average increase in travel time in the strict scenario (zmax=1) and in the relaxed scenario (zmax≤ ∞), respectively, compared to the solution with minimal travel time (i.e., the solution that ignores arrival time and driver consistency). Column Diff gives the difference between the strict scenario and the relaxed scenario. The comparison shows the cost of perfect driver consistency for different levels of arrival time consistency.The larger the maximum arrival time difference, the higher the cost of driver consistency: the average increase in travel time caused by servicing each customer by a single driver is 4.61 percent, 3.88 percent, and 2.08 percent for data sets C0.5, C0.7, and C0.9, respectively, when lmaxis improved by 10 percent. At the highest level of arrival time consistency (improving lmaxby 90 percent), the increase in travel time decreases to 2.49 percent, 0.81 percent, and 0.77 percent in C0.5, C0.7, and C0.9, respectively. This result suggests that arrival time consistency and driver consistency can be optimized simultaneously.The cost of driver consistency is further examined by defining a variant of the unaryϵ-indicator (see Section 4.2.3):(37)IϵxQ=x-quantileq∈PZ+1minp∈PZmaxi∈{1,3}piqi.Sets PZandPZ+1are subsets of the Pareto-front that contain only points withp2=Zandp2=Z+1,respectively. The modifiedϵ-indicator gives the value for which x percent of the solutions get less thanIϵxQtimes worse if zmaxis reduced fromZ+1to Z.Figs. 9, 10, and 11show the averageIϵxQvalues forx=25percent, 50 percent, and 75 percent, respectively. In each figure, we show by how much the solutions deteriorate for different Z values and for different service frequencies (i.e., for data sets C0.5, C0.7, and C0.9).The pattern in all figures is similar: the solutions deteriorate most if the number of drivers per customer is reduced from two drivers to one driver. The difference between solutions is minor when Z > 1; in this case, the solutions do not deteriorate by more than 0.9 percent. The effect of driver consistency decreases with increasing service frequency, i.e., decreasing fluctuations in the demand. A quarter of the solutions withzmax=1in data sets C0.5, C0.7, and C0.9 deteriorates by less than 3.5 percent, 2.5 percent, and 0.9 percent, respectively, compared to solutions withzmax=2(Fig. 9); half of the solutions deteriorate by less than 5.1 percent, 3.7 percent, and 1.5 percent in data sets C0.5, C0.7, and C0.9, respectively (Fig. 10). In three quarters of the solutions, the cost of achieving perfect driver consistency increases by less than 6.7 percent, 5 percent, and 2.3 percent in in data sets C0.5, C0.7, and C0.9, respectively (Fig. 11).In this section, we examine the effect of enforcing perfect driver consistency on arrival time consistency when the focus is on travel cost. Many companies, e.g., in the small package shipping industry, perform a districting strategy in order to reduce the operational complexity. Here, the service territory is partitioned into smaller areas called districts and each driver is assigned to one district. Districts are designed in a tactical phase and routes are planned in an operational phase. Typically, the focus in both phases is on travel cost.Visiting each customer with only one driver is equivalent to assigning customers to a district that is served by the same driver each day. Bounding the number of different drivers by one results in disjoint districts but, in contrast to districting approaches, the districts might be non-convex and dispersed over a large area. Each customer experiences perfect driver consistency but the effect of districting on arrival time consistency is unclear. Our intuition is that it is easier to achieve arrival time consistency for each district separately than for the entire service territory. With districting there is less room for reducing travel cost by visiting customers outside a district and, therefore, daily routes become more similar. An example is illustrated in the left figure of Fig. 12. By restricting the maximum number of different drivers to one, we implicitly improve arrival time consistency by 81 percent compared to the solution without districting when travel cost is the primary objective. However, in the figure on the right, we see the opposite: with districting, we have a 32 percent worse arrival time consistency (and longer travel time), i.e., improving arrival time consistency and reducing the number of different drivers per customer are conflicting objectives.In Table 10, we present aggregated results that show the effect of strict driver consistency on arrival time consistency. We report the number of instances in which the arrival time difference is larger with districting (i.e., with emphasis on cost andzmax=1) than without (# Worst lmax) and the average improvement in lmax. (Each lmaxvalue refers to the solution in Papproxwith minimal total travel time.) Additionally, we use another variant of the unaryϵ-indicator to measure by how much the districting solution, denoted by p, is worse compared to solutions in which each customer may be visited with an arbitrary number of different drivers (denoted by PZ ≤ ∞):(38)Iϵ′=minq∈PZ≤∞maxi∈{1,3}piqi.The effect of districting depends primarily on the number of days in the planning horizon. The planning horizon involves 3 days in data set Csmalland 5 days in the remaining data sets (i.e., CsmallE, C0.5, C0.7, and C0.9). In Csmall, the arrival time difference increases in 6 out of 10 instances; nevertheless, lmaxdecreases by 13.34 percent on average; in terms of theIϵ′indicator, the closest efficient solution without districting is 1.145 times better, on average, than the solution with districting. In data set CsmallE, lmaximproves in each of the five instances due to strict driver consistency; the average improvement is 26 percent. The solutions are, on average, 4.8 percent worse compared to solutions without districting, i.e.,Iϵ′=1.048. A positive impact of districting on arrival time consistency is observed in the large instances. The arrival time difference increases in only one out of 36 instances and the average improvement is between 35.33 percent and 54.25 percent. The cost of districting, i.e.,Iϵ′,decreases with larger service frequency from 1.052 in data set C0.5 to 1.014 in data sets C0.9.We remark that different strategies for partitioning the service territory might influence the effect of districting on arrival time consistency. For example, the results might be significantly different when customers are assigned to drivers not only with regard to cost but also with regard to workload balancing.Based on different unary quality indicators, we can summarize that the proposed MDLNS is an eligible algorithm for solving the MOGenConVRP. Compared to the 3D method, the heuristic provides good approximation sets in reasonable amount of time. MDLNS is specialized in finding the best possible approximation of the optimal Pareto-front: we check each solution generated during the search process whether or not it is dominated and we apply the LNS for a large number of iterations. The average computation time on the large instances (data set C) is 23.2 hours (the average number of elements in P is 88.5). Nevertheless, the MDLNS is applicable in the field subject to slight modifications, e.g., by checking only selected solutions for efficiency and by reducing the number of LNS and MDLNS iterations, respectively.The trade-off between travel time, driver consistency, and arrival time difference depends significantly on the fluctuations in the demand: the lower the service frequency, the more costly it is to achieve service consistency. Adequate levels of arrival time consistency can be provided with modest increase in travel time. Compared to the minimal cost routing plan, the maximum arrival time difference can be reduced by 50 percent at the cost of 1.58 percent longer travel time, on average; improving arrival time consistency by 70 percent costs 3.84 percent. Feillet et al. (2014) report 5.9 percent higher routing cost when arrival time consistency is considered; the increase in Groër et al. (2009) is between 6.6 percent and 15 percent (the results in Groër et al. (2009) also include the cost of perfect driver consistency.) The vehicle departure time from the depot is fixed in Feillet et al. (2014) and Groër et al. (2009); with this restriction, a 60 percent tighter constraint on lmaxincreases cost by up to 186.15 percent if departure times from the depot are fixed (Kovacs et al. (2014c)). High levels of time consistency can be provided with small increases in travel cost when departure times are flexible (Kovacs et al., 2014a, 2014c). Improving arrival time consistency by 90 percent can still be achieved at 17.07 percent higher travel cost, on average. However, perfect arrival time consistency is not manageable at a reasonable cost.In many cases, the level of arrival time consistency affects the cost of driver consistency: Visiting each customer with the same driver when arrival time consistency is improved by 10 percent increases travel time between 2.08 percent and 4.61 percent compared to the solution with minimal travel time. Achieving perfect driver consistency when lmaxis decreased by 90 percent increases travel time between 0.77 percent and 2.49 percent. Perfect driver consistency is significantly more expensive than visiting each customer with two different drivers: the routing cost increases by up to 5.1 percent in 50 percent of the solutions; visiting each customer by two drivers instead of three drivers increases cost by less than 0.9 percent. Similar results have been presented in the literature: Feillet et al. (2014) report cost savings of up to 7.5 percent if driver consistency is ignored. Enforcing perfect driver consistency increases cost by up to 9.85 percent in the inventory routing problem (Coelho et al., 2012). In the driver assignment vehicle routing problem, the increase in cost decreases from 12 percent to 2.9 percent if a perfect driver consistency is enforced only for 75 percent of the customers (Spliet, 2013). Francis et al. (2007) and Smilowitz et al. (2013) report that improving driver consistency increases routing cost by up to 6.1 percent and 5.2 percent, respectively. Allowing more than one driver per customer reduces the routing cost by up to 6.5 percent in the GenConVRP (Kovacs et al., 2014a).A strict driver consistency, as it is enforced, e.g., in districting applications, improves arrival time consistency in most cases as a side benefit. The average maximum arrival time difference decreases between 35.33 percent and 54.25 percent as a consequence of allowing only one driver per customer when travel cost is the primary objective.

@&#CONCLUSIONS@&#
