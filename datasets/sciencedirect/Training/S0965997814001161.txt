@&#MAIN-TITLE@&#
Easy gesture recognition for Kinect

@&#HIGHLIGHTS@&#
We created a tool that facilitates the development of gesture-controlled applications.We implemented two recognition techniques: Dynamic Time Warping and Hidden Markov Models.We compared the efforts required to develop gesture recognizers, with and without the support of EasyGR.We obtained correct-recognition rates of over 99%.The results have shown that our approach reduces the effort involved in implementing gesture-controlled applications.

@&#KEYPHRASES@&#
Natural user interfaces,Gesture recognition,Machine learning,Kinect,Human-computer interaction,Gesture-recognition framework,

@&#ABSTRACT@&#
Recent progress in entertainment and gaming systems has brought more natural and intuitive human–computer interfaces to our lives. Innovative technologies, such as Xbox Kinect, enable the recognition of body gestures, which are a direct and expressive way of human communication. Although current development toolkits provide support to identify the position of several joints of the human body and to process the movements of the body parts, they actually lack a flexible and robust mechanism to perform high-level gesture recognition. In consequence, developers are still left with the time-consuming and tedious task of recognizing gestures by explicitly defining a set of conditions on the joint positions and movements of the body parts. This paper presents EasyGR (Easy Gesture Recognition), a tool based on machine learning algorithms that help to reduce the effort involved in gesture recognition. We evaluated EasyGR in the development of 7 gestures, involving 10 developers. We compared time consumed, code size, and the achieved quality of the developed gesture recognizers, with and without the support of EasyGR. The results have shown that our approach is practical and reduces the effort involved in implementing gesture recognizers with Kinect.

@&#INTRODUCTION@&#
Over the last years, controlling systems by gestures through Natural User Interfaces (NUI) has become a common practice in our daily lives [1,2]. In 2006, WiiMote changed the concept of remote control by allowing players to control games through hand movements in the 3D space [3]. Then in 2010, Microsoft Kinect for Xbox became the most popular device not only for its low cost but also for its simplicity. Kinect allows players to control games through full-body movement without using a remote control [4]. Indeed, it has promoted the development of new natural interaction applications.Although Kinect is able to recognize the position of users’ joints, developers are still left with the time-consuming and tedious task of recognizing gestures. More precisely, early Software Development Kits (SDKs) only provided interfaces and code samples that enabled developers to access sensor data in real time, such as RGB-D camera, microphones, and the 3D position of 20 body joints. Later, some tools proposed to augment the interfaces with a rule-based approach that relied on a set of parameters and thresholds on joint location to track movements [5,6]. This approach supports the creation of gestures by allowing developers to adjust gesture sensibility by means of threshold values, and then to link these gestures to specific actions. However, this has become an error-prone process that requires domain knowledge, experience, and effort to ad hoc define a set of rules or heuristics so as to recognize human body gestures, in particular when defining complex gestures. Therefore, techniques that easily adapt to these complex needs would be necessary.In order to provide a more flexible and robust approach, we can see gesture recognition as a classification problem [7]. In this context, a classification problem consists in assigning one label or class to a gesture in such a way that it is consistent with the available data about the problem. For dealing with a classification problem, machine learning techniques can be applied. These techniques use a gesture training set, in which each gesture is labeled to generate a classifier. In turn, this classifier evaluates the similarity between a new gesture and each of the trained gestures, resulting in the label of the most similar gesture. Although these methods offer high correct-classification rates [8–10], developers still have to implement complex algorithms for including gesture recognition in their own applications.In this context, we propose EasyGR (Easy Gesture Recognition),1EasyGR Homepage http://www.isistan.unicen.edu.ar/?page_id=501.1a gesture-recognition tool that allows developers to define and recognize gestures without demanding from them specific knowledge of machine learning algorithms. The user interface of EasyGR allows non-specialist users to record, edit, and store gestures, enabling them to easily create a new training set. Then, by using this training set, these users may train different machine learning techniques for gesture recognition. Particularly, EasyGR supports two techniques: Dynamic Time Warping (DTW) [11] and Hidden Markov Models (HMM) [12]. Therefore, the approach was validated by analyzing two factors: the techniques’ accuracy and the efforts required to develop gesture-controlled applications using and not using EasyGR. We tested the techniques’ accuracy, by using 7 different gestures with 80 samples for each gesture. The results showed correct-recognition rates of over 99%. Concerning the amount of effort involved, we asked 10 developers to implement a gesture controlled application using a rule-based approach, and then to re-implement the application using EasyGR. The comparison between the solutions of each approach showed that EasyGR can certainly reduce the efforts to develop gesture recognizers.The remainder of this article is organized into four sections. Section 2 covers related works. Section 3 describes the kind of assistance provided by EasyGR through a motivating example. Section 4 discusses the experiments and results, along with the benefits of using EasyGR. Finally, Section 5 presents the conclusions of this work.

@&#CONCLUSIONS@&#
In this article we have presented a gesture-recognition tool to help developers include gesture recognition in NUI applications. In particular, EasyGR enables developers to create, train and recognize gestures. For the first two activities, EasyGR brings a graphic interface that allows non-specialist users to capture gestures and train the recognition techniques. EasyGR supports Dynamic Time Warping and Hidden Markov Models as recognition techniques. For the third, the developers have a template code for gesture recognition that they can include in their application, thus decreasing the development effort. Therefore, the main contribution of EasyGR is that it assists developers in the implementation of NUI applications. By encapsulating the gesture recognition, EasyGR reduces the complexity of managing the gesture data and the algorithms needed to build a NUI application.The results of applying EasyGR in the case studies reported in this article are encouraging. We have evidence that EasyGR contributes to reducing the code size and complexity of gesture controlled applications. Furthermore, we have obtained correct-recognition rates of over 99% with each techniques. In particular, we observed that increasing the number of clusters used in HMM improves the gesture fidelity but makes the gesture harder to imitate; thus the developer should find a balance between the gesture fidelity and the usability of the application.Nonetheless, EasyGR still has some limitations and improvements that we expect to address in future versions of the tool. First, the gesture recognizer is unable to automatically adapt itself to the skills of the user. For instance, a developer creating a multilevel game would need a more restrictive recognizer every time the user completes each level. Although EasyGR does not have this feature, the developer can create different gestures to suit the user’s skills. Second, we will focus on including new gesture-recognition techniques like Naïve-Bayes-Nearest-Neighbor [24] and String Matching [25].