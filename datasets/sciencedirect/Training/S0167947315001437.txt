@&#MAIN-TITLE@&#
Covariance matrix estimation for left-censored data

@&#HIGHLIGHTS@&#
ML based covariance matrix estimator for left-censored data is introduced.Computation times are decreased considerably with parallelized pairwise estimation.The proposed estimators produce unbiased estimates with reasonable variation.

@&#KEYPHRASES@&#
Maximum likelihood estimation,Covariance matrix,Left-censoring,Non-detects,

@&#ABSTRACT@&#
Multivariate methods often rely on a sample covariance matrix. The conventional estimators of a covariance matrix require complete data vectors on all subjects—an assumption that can frequently not be met. For example, in many fields of life sciences that are utilizing modern measuring technology, such as mass spectrometry, left-censored values caused by denoising the data are a commonplace phenomena. Left-censored values are low-level concentrations that are considered too imprecise to be reported as a single number but known to exist somewhere between zero and the laboratory’s lower limit of detection. Maximum likelihood-based covariance matrix estimators that allow the presence of the left-censored values without substituting them with a constant or ignoring them completely are considered. The presented estimators efficiently use all the information available and thus, based on simulation studies, produce the least biased estimates compared to often used competing estimators. As the genuine maximum likelihood estimate can be solved fast only in low dimensions, it is suggested to estimate the covariance matrix element-wise and then adjust the resulting covariance matrix to achieve positive semi-definiteness. It is shown that the new approach succeeds in decreasing the computation times substantially and still produces accurate estimates. Finally, as an example, a left-censored data set of toxic chemicals is explored.

@&#INTRODUCTION@&#
Multivariate methods often rely on the sample covariance matrix. For example, principal component analysis, which is used to describe high dimensional data in lower dimensions, uses the eigenvalue decomposition of the covariance matrix. In canonical correlation analysis, blocks of the covariance matrix are used to find the maximal correlation between linear combinations of variables belonging to two different data sets. The conventional estimators of the covariance matrix require complete data vectors on all subjects, which is an assumption that can frequently not be met. Many fields of life sciences are constrained by the measurement accuracy of modern measurement technology. Often, it is impossible to quantify the exact concentration or the complete absence of a compound, especially at the low end of the detectable concentration range. The lowest concentration that can be reliably detected with the given analytical method is referred to as the lower limit of detection (LLOD) (Browne and Whitcomb, 2010). Measurements falling below the LLOD are referred to as left-censored values or, in some contexts, non-detects.For data that contain left-censored values, if the analysis is carried out using only the completely observed data, the means of the concentrations would be overestimated and the standard deviations would be underestimated. Consequently, any related test statistic or estimate would be biased. Thus, neglecting these informative censored values can lead to a severe bias and a loss of precision due to the decrease of the effective sample size.In the presence of left-censored values, a common and simple way to deal with the estimation of the covariance matrix is to delete any subject containing at least one censored value. To avoid completely ignoring these subjects, Little and Rubin (2002) propose to build the estimate of the covariance matrix one element at a time by using all observations for which both values are present. However, the resulting covariance matrix estimate is not necessarily positive semi-definite. According to Mehrotra (1995), the efficient use of all observed data is more important than the possible lack of positive semi-definiteness. If the positive semi-definiteness is lost, he recommends the element-wise estimation of the variances and covariances combined with a possible adjustment.Another commonly used approach is to substitute the left-censored values with a suitable constant and then to compute the sample covariance from the resulting complete data. The potential substitution value can be the sample mean of the uncensored values for the corresponding variable, zero,LLOD/2, or the minimum of the observed values. These alternative approaches have been investigated in previous studies (Farnham et al., 2002; El-Shaarawi and Esterby, 1992; Succop et al., 2004). All of them are more or less biased, but they are still used despite the criticism (Helsel, 2005, 2006).In addition to the mentioned quick fixes, there exists more eloquent methods to overcome the challenges set by censored data. Instead of single and simple substitutions, one approach is to multiply impute the censored values. The key idea is to use the conditional distribution of the observed data to generate a set of plausible imputations for the censored data (Rubin, 2004, 1996; Carpenter and Kenward, 2013). Imputations are repeated several times, creating multiple data sets, which are then analyzed individually as if they were complete. Thus, if the main interest lies in estimating the covariance matrix for the left-censored data, the multiple imputation would be followed by computing the standard sample covariances for each of the imputed data sets. Finally, the results are combined across all multiply imputed data sets by so-called “Rubin’s rules”, which incorporate the imputation-related uncertainty into the analysis (Rubin, 2004). The multiple imputation methods for left-censored data may be appealing due to their relatively simple computational algorithms. The literature includes applications in univariate (Baccarelli et al., 2005; Huybrechts et al., 2002), bivariate (Chen et al., 2011) and multivariate settings (Hopke et al., 2001; Chen et al., 2013).Hewett and Ganser (2007) divide censored data analysis methods into four categories: substitution methods, log-probit regression, maximum likelihood (ML) estimation methods, and non-parametric methods. None of the methods has been recommended for all different scenarios. The recommendation depends on the sample size, the divergence from log-normal distribution or the degree of censoring. However, due to its many desirable statistical properties, ML estimation is often considered the gold standard provided the data is well-described by some parametric probability distribution (Helsel, 1990; Koo et al., 2002; Zhao and Frey, 2006).Extensive literature exists regarding univariate and bivariate ML-based methods for estimating the measures of centrality and variability in the presence of the left-censored values, such as Lyles et al. (2001), Lynn (2001) and Williams and Ebel (2014). The majority of these works are based on the normality assumption. Song et al. (2004) propose an alternative, more robust approach based on generalized estimating equations to estimate the correlation between two continuous variables with left-censored values. Their method also has an advantage of not requiring time consuming optimization routines.The extensions of ML-based estimation techniques to the multivariate setting are fewer. If it could be assumed that each variable is identically distributed (normal, gamma, or Weibull), then the results proposed by Gupta (1952) and Harter and Moore (1967) could be applied. In practice, however, this is not often a realistic assumption. Chung (1993) proposes a covariance matrix estimation method that uses marginal ML estimation. Despite its good practical properties, theoretically it is not a ML estimator but rather a good-enough approximation.Building on the work of Lyles et al. (2001), Perkins et al. (2013) propose a ML estimator for the mean vector and the covariance matrix based on the multivariate normality assumption in the presence of left-censored values. They formulate the likelihood function as a product of the marginal multivariate distribution for the observed variables and the conditional multivariate distribution for the left-censored variables. They also develop an approximation for the Fisher information and covariance matrix for the estimated parameters and give an example based on a three-parameter multivariate normal distribution.To overcome the complexity of estimating the parameters of multivariate normal and log-normal models by maximizing the logarithm of the likelihood equations, Hoffmann and Johnson (2014) proposes a pseudo-likelihood approach. The pseudo-likelihood estimate maximizes a computationally simpler approximation of the log-likelihood function but is not equal to it.Against this background, we address the challenge of dealing with the left-censoring in the analysis of multivariate data and introduce an efficient way to use the values that are below the set LLOD. We formulate the likelihood maximization problem similar to Perkins et al. (2013) by assuming a multivariate normal distribution but in addition investigate both theoretical and practical properties of the proposed estimators. In practice, it is difficult to obtain the ML estimates for the covariance matrix from a data set including left-censored values due to the need for complex iterative procedures. As we show in Section  3.3, the numerical optimization of the log-likelihood function involving multivariate distributions is extremely slow, especially in higher dimensions. Thus we propose pairwise maximum likelihood estimation of the covariance matrix with an adjustment to achieve positive semi-definiteness of the resulting estimate.In Section  2 we describe the likelihood functions for partially observed data. In Section  3 we discuss the computational issues and find solutions to them. In Section  4 we present results of simulations to assess the performance of our proposed estimators. In Section  5 we present an application using toxic chemical data. We conclude with the discussion in Section  6.Many biological variables follow a log-normal distribution meaning that the log-transformed variables are normally distributed. Thus, in general, we observe the (possibly log-transformed) biological variablesY=(y1,…,yn)Tonnindependent subjects from ap-variate distributionfwith the expectationμand the covariance matrixΣ. Letyijdenote the value of thejth variable,j=(1,…,p), for subjecti,i=(1,…,n). Potentially, a substantial number ofyijare not detected. Letyoandymdenote the observed and the censored elements in the1×pvectory, respectively.Let LLODsc=(c1,…,cp)be a vector of lengthp. Generally, the measuring instrument determines the LLODS, i.e., how small concentrations can be reliably measured. Consequently, the left-censored values arise when the concentrations fall below the LLODs. In practical settings, the LLODs are often variable specific (as presented here), but our method is also applicable in settings where the LLODs are subject- or sample-specific, and for the most general case, where the LLOD for each variable varies from one subject to another. (For generality, we also investigate a setting where the exact LLODs are not known, but they are estimated as the minimum observed concentrations for each variable or for each subject or sample. In this case, the minimum observed concentration value for each subject or sample is the ML estimate for the LLOD, if an assumption on multivariate normal distribution holds, and the LLODs are the same across all subjects.)Without censored values, the likelihood function contribution of subjectiis(1)Li(μ,Σ)=f(yi;μ,Σ).Ifkvariables are observed andp−kvariables are left-censored, the contribution of one subject to the likelihood can be defined as(2)Li(μ,Σ)=fo(yi,o)∫Ωifm(yi,m∣yi,o)dyi,m,whereΩiis the part of the observation space that we are unable to observe and that is defined by the LLODs. (When the LLODs are estimated, thenΩiis an approximation, as is the resulting likelihood.) The integral represents the probability of having thep−kvariables below the LLOD given that thekvariables are observed. Finally, the full likelihood is often presented in the mathematically convenient log-scale as a sum of the logarithms of likelihood contributions of each subject,(3)ℓ(μ,Σ)=logL(μ,Σ)=log∏i=1nLi(μ,Σ)=∑i=1nlogLi(μ,Σ).In the special case of the multivariate normal distribution, the subjects’ contributions to the likelihood are built in parts as follows. First, for each subject, the number of the left-censored values is counted. Next, we partition the expected values and the covariance matrices for observed and censored variables. These are the blocks of the joint mean and the joint covariance of the observed and left-censored variables,μ=(μo,μm)T,andΣ=(ΣooΣomΣmoΣmm).Depending on the number of the left-censored variables, the log-likelihood for subjectiis combined from one or two of the following parts:(1)Observed variables inyiThe log-likelihood function for the observed variables,ℓi,o, is based on the density function of the multivariate normal distributionNk(μo,Σo)evaluated atyo. If there are no observed values, thenℓi,o=0.Left-censored variables in partially observedyiIf some variables are observed and some left-censored, then the log-likelihood function for the left-censored variables,ℓi,m, is based on the probability function of the multivariate conditional normal distribution,N(p−k)(μm+ΣmoΣo−1(yo−μo),Σm−ΣmoΣo−1ΣmoT),evaluated atci. If all variables are observed or left-censored, thenℓi,m=0.Left-censored variables in fully unobservedyiIf all variables are censored, the likelihood functionℓi,mais based on the cumulative distribution function of the multivariate normal distributionNp(μm,Σm)evaluated at LLODs. If there are one or more observed values, thenℓi,ma=0.For the case of ap-variate normal distribution to estimate the mean vector and covariance matrix in the presence of left-censored data using ML estimation, we solved the maximization problem by using the limited memory algorithm for bound constrained optimization (Byrd et al., 1995).The estimate of the covariance matrix has to be positive semi-definite and symmetric. Just by maximizing the log-likelihood function we cannot ensure that this basic property holds. To overcome this problem, the optimization task was re-parametrized via estimating the elements ofLin the Cholesky decompositionΣ=LLTinstead of estimatingΣ. The initial values of the parameters were computed by substituting all left-censored values withLLOD/2and then computing the sample covariance matrix and its Cholesky decomposition as well as the component-wise sample means from the resulting complete data set.In the iterative optimization, the estimate of the covariance matrix (and its sub-matrices such as the covariance matrix of the observed valuesΣoof subjecti) has to be positive semi-definite. If the estimate was not positive semi-definitive (assessed by its reciprocal condition number) at a particular step of the iteration, a small constant of 10−6 was added to its diagonal elements to force the covariance matrix to be invertible (Golub and Loan, 1989).For complete data with no censored values, the covariance matrix can be constructed element-wise by computing the variances and the pairwise covariances separately. When discussing the robust estimation of the covariance matrix estimation in the presence of any kind of missing data, Mehrotra (1995) proposes the element-wise estimation of the covariance matrix using all the observations for which both variables have valid values. We adopt only the idea of element-wise estimation and use the ML estimator with known LLODs to allow for the presence of the left-censored values.As the ML estimation problem can be solved fast only in low dimensions, a practical approximation is to use the ML estimates of 2×2 covariance submatrices for thejth andkth variables,∀j,k∈{1,…,p}s.t.j<kand combine these into the full covariance matrix. This results inp−1estimates for each variance parameter and one estimate for each covariance parameter. One possible solution to combine thep−1variance estimates into one is to take their mean, and this is what we propose to do. However, the resulting estimated covariance matrix may not be acceptable as it is not guaranteed to be positive semi-definite.If needed, the nearest positive-definite matrix can be found using a method suggested by Higham (2002). For the pairwise estimated symmetric covariance matrixSPair∈Rp×p, the problem is to find a symmetric positive definite matrixSˆPairsuch that the distance betweenSPairandSˆPairis minimized, i.e.,(6)SˆPair=argmin{‖SPair−SˆPair‖:SˆPairis a symmetric positive definite matrix}.The norm used is the Frobenius norm,‖SPair‖2=ΣiΣjsij2, wheresijare the elements ofSPair. The method is implemented, for example, in the R-package Matrix (Bates and Maechler, 2014).Even though solving the optimization problem is fast atp=2, the number of pairs increases rapidly as the number of variables increases. For example, forp=100, the number of pairs is 4950. Thus, the computation times for pairwise estimation can be substantial in higher dimensions. In order to decrease the computation time, the pairwise estimation problems can be divided for parallel cores and solved simultaneously. The resulting estimates are then combined into the full estimates of the covariance matrix and the mean vector.We conducted an experiment to compare the computation times between the genuine ML estimate with known LLODsSMLE, adjusted pairwise estimateSˆPair, andSˆPairachieved via parallel computation. All computations were performed using R 3.0.2 on a Windows server with two Intel(R) Xeon(R) CPU R5 2440 with 2.40 GHz and 64 GB. We generated observations forn=100subjects from ap-dimensional normal distributionN(μ,Σ)whereμ=(0,…,0). Three different dimensions were selected,p=3, 10, and 100. Forp=3, the covariance structure was defined asΣ=(10.5−0.50.510−0.501),and forp=10asΣ=(10000.5000000100000.500000100000.50000010000−0.500.500010−0.50000000010.500000.500−0.50.51000000.5000010.50000−0.50000.510.5000000000.51).For simplicity, the covariance matrix forp=100was defined asΣ=0.5⋅(I+11T), whereIis the identity matrix and1is a vector of ones.The percentage of left-censored values was controlled with the choice of LLOD in two different settings. The LLOD was chosen as the 10% or 30% quantile from the standard normal distribution. For simplicity, we set the LLOD to be the same over all subjects and variables. Left-censored values were generated by comparing each concentration value to the LLOD. If the concentration was below the LLOD, it was set as a left-censored value.Forp=3and 10, the covariance matrices were estimated from 1000 replications, and forp=100, the covariance matrices were estimated from 20 replications. The computation times were averaged over the replications. The results are presented in Table 1.For the ML estimateSMLE, the number of estimated parameters for the covariance matrix and the expected values in dimensions:p=3, 10, and 100 are 9, 65, and 5150, respectively. For the pairwise estimateSˆPair, the number of 2×2-covariance matrices to be estimated is: 3 inp=3, 45 inp=10, and 4950 inp=100, and in each small estimation problem, the total number of parameters is 5. As seen in Table 1, the computation times of the ML estimate increase substantially as a function of the dimension and the proportion of censored values. As described in Section  2, likelihoodsℓiinclude the cumulative distribution function of the multivariate normal distribution if subjectihas left-censored values. Thus, as the dimension and the proportion of the left-censored values increase, the evaluation of the cumulative distribution function of the multivariate normal distribution becomes slower and slower to evaluate. The proportion of left-censored values does not have a significant effect on the pairwise estimateSˆPairas the maximum number of left-censored values in each small estimation problem is always the same.For parallelSˆPair, we divided the pairwise computations across 10 cores using the R-package snowfall (Knaus, 2013). This speeded up the computation considerably and allowed us to estimate bothμandΣforp=100in approximately 47 min with 10% of censored values and in 57 min with 30% of censored values. The computation times in different dimensions are approximately proportional to the number of pairs (if all 10 cores are used). This is expected, as each of the pairwise estimation has an equal number of parameters to estimate.In this section we describe the simulation studies performed to compare the ML covariance matrix estimator with selected three competing estimators and also study the performance of the pairwise estimator.Data were generated from ap-dimensional normal distributionN(μ,Σ)(as they would have been originally following multivariate log-normal distribution and then log-transformed) whereμ=(0,…,0)andΣwas comprised of homogeneous variances of 1 and three varying covariance elements:0,0.5, and −0.5. The covariance matrices forp=3andp=10are shown in Section  3.3.Two dimensions were used,p=3andp=10, resulting in six unknown parameters forp=3and 55 forp=10. The number of subjects,n, was 100. Also, the percentage of left-censored values was controlled with the choice of LLOD as described in Section  3.3.We have two different versions of the ML covariance estimator. The first one,SMLE(c), uses the known LLODs, and the second one,SMLE(cˆ), uses the estimated LLOD.The first competing estimator isSC, the traditional sample covariance matrix computed from the data including only the subjects having no left-censored values. The second estimator,S(0), is the sample covariance matrix computed from the data where all censored values are substituted withlog(10−6). How biased theS(0)is, depends on the distance between zero and LLOD. In our simulations, the LLOD was 0.278 (on log-normal scale) for 10% of left-censored values and 0.595 for 30% of left-censored values. The third competing estimator isS(c/2), which is the sample covariance matrix from the data where censored values are substituted withLLOD/2.The covariance matrices were estimated from 1000 replications. We compared the differences between the real underlying covariance matrix and each of the covariance matrix estimates from the 1000 replications. The smaller the differences are, the less biased the estimator is. The bias is calculated as an element-wise difference between the real covariance matrix and the estimated covariance matrix,Σ−Σˆ, and the results are presented separately for the variance and covariance elements of the covariance matrices.

@&#CONCLUSIONS@&#
