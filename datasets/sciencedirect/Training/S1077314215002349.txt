@&#MAIN-TITLE@&#
A scalable and flexible framework for smart video surveillance

@&#HIGHLIGHTS@&#
We developed a scalable and flexible framework to video surveillance analysis.By using the framework, the researcher can focus only on the design of his/her specific task.The framework allows the creation of a high-level semantic representation of the scene and scalable feature extraction.We report results demonstrating the framework performance and the viability of its usage.

@&#KEYPHRASES@&#
Smart surveillance framework,Surveillance systems,Computer vision,Video analysis,Video surveillance,

@&#ABSTRACT@&#
In the last years, the number of surveillance cameras placed in public locations has increase vastly and as consequence, a huge amount of visual data is generated every minute. In general, this data is analyzed manually, a challenging task which is labor intensive and prone to errors. Therefore, automatic approaches must be employed to enable the processing of the data, so that human operators only need to reason about selected portions. Computer vision problems focused on solving problems in the domain of visual surveillance have been developed aiming at finding accurate and efficient solutions. The main goal of such systems is to analyze the scene focusing on the detection and recognition of suspicious activities performed by humans in the scene, so that the security staff can pay closer attention to these preselected activities. However, these systems are rarely tackled in a scalable manner. Before developing a full surveillance system, several problems have to be solved, which are usually solved individually. However, in a real surveillance scenario, these problems have to be solved in sequence considering only videos as the input. With that in mind, this work proposes a framework for scalable video analysis called Smart Surveillance Framework (SSF) to allow researchers to implement their solutions to the surveillance problems as a sequence of processing modules that communicates through a shared memory.

@&#INTRODUCTION@&#
Due to the reduction in prices of cameras and the increase in network connectivity, the number of surveillance cameras placed in several locations increased significantly in the past few years. If on one hand, a distributed camera network provides visual information in real time covering large areas, on the other hand, the number of images acquired in a single day can be easily in the order of billions, preventing their manual processing and posing an intricate problem for monitoring such areas [1].While the ubiquity of video surveillance provides safer environments, the monitoring of large amount of visual data is a challenging task when performed manually by human operators since most of the visual data do not present interesting events from the surveillance standpoint, turning it into a repetitive and monotonous task for humans [2,3]. Hence, automatic understanding and interpretation of activities performed by humans in videos are of great interest since such information can assist the decision making process of security agents [2].The addition of automatic understanding and interpretation to surveillance systems does not entail the replacement of human operators as foreseen on several Sci-Fi movies, on the contrary, it aims at supplying information to the operator. For instance, instead of a security agent monitoring continually up to 50 screens with live security video feed (task which humans do not present high performance due to the lack of important events during most of the time [4]), an automated system might perform a filtering in the videos and indicate only those video segments more likely to contain interesting activities, such as suspicious activities that might lead to a crime.In the last two decades, professionals of industry and researchers have dedicated their studies to improve surveillance systems. To understand the increase of works related to video surveillance and inspired by Huang [5] study, we searched the keywords video and surveillance in IEEE Xplore Digital Library11http://ieeexplore.ieee.org/(within metadata only) and the IEEE Computer Society Digital Library22http://www.computer.org/csdl(by exact phrase). The findings are shown in Fig. 1as a function of the publication year. The large number of publications in the past ten years indicates that research on surveillance video has been very active.Smart visual surveillance systems deal with the real-time monitoring of objects within an environment. The main goal of these systems is to provide automatic interpretation of scenes and understand activities and interactions of the observed agents based on the visual information being acquired. Current research regarding these automated visual surveillance systems tend to combine multiple disciplines, such as computer vision, signal processing, telecommunications, management and socio-ethical studies.One of the great challenges of automatic surveillance systems is that to interpret what is happening in the scene, a sequence of problems need to be solved, which is highly prone to noise generated during the process. Among the problems are the background subtraction [6], pedestrian detection [7], face recognition [8], gesture recognition [9], pose estimation [10], person tracking [11] and re-identification [12], action recognition [13] and activity recognition [14]. Even thought each one of these problems present a vast literature, they are usually considered independently such as in currently available evaluation data sets, e.g., the evaluation of face recognition methods is performed using already detected, cropped and aligned faces [15], which cannot be accomplished in real surveillance scenarios where the only inputs are video feeds without annotations. Therefore, dealing with the problems individually does not allow one to identify what are the effects of the results obtained by solving one problem on the following steps in the processing sequence.Although visual surveillance has been subject to a huge growth, there is still a lack of contributions from the field of system engineering to the area [16]. The small number of frameworks that are open and focus on visual surveillance usually require a steep learning curve. In addition, with the contemporary advances in video sensors and increasing availability of network cameras allowing the deployment of large-scale surveillance systems, distributed in a wide coverage area, the design of smart and scalable surveillance system remains a research problem: how to design scalable video surveillance systems considering aspects related to processing power, memory consumption and network bandwidth?Motivated by the presented issues, this work proposes a framework for a scalable video analysis able to readily integrate different computer vision algorithms into a functional surveillance system. This framework, called Smart Surveillance Framework (SSF), aims at bringing several improvements providing scalability and flexibility, allowing the users (researchers) to focus only on their application by treating the sequence of problems as a set of processing modules that communicates through data streams, stored in a shared memory. This framework was presented in a preliminary and reduced version focusing only on its main concepts in [17] and its feature extraction server was described in [18]. However, this work presents the SSF in details and focuses on the evaluation of its several components33The Smart Surveillance Framework is available for download at http://www.ssig.dcc.ufmg.br/ssf/.In addition, this work also presents new features, such as the data stream and the image and feature managers.More specifically, the Smart Surveillance Framework is a development environment in which the researcher can implement and evaluate his/her algorithms related to surveillance in an integrated manner, as illustrated in Fig. 2. It is based on execution modules that communicate to each other using data streams controlled by a shared memory. The framework provides the following features to aid the researcher: memory management to allow handling large amounts of data in regular computers; communication control among execution modules; predefined data structures specifically designed for surveillance environment; management of multiple data input, such as cameras or stored videos; feature extraction server to maximize the usage of the processing power available to compute local descriptors; query server to allow high level reasoning and scene understanding; and a configuration interface to help setting up sequences of execution. These features and the framework architecture will be discussed in details in Section 4.The main contributions provided by the development of the SSF are the following: (i) A novel framework to allow the processing of large amounts of data provided by multiple surveillance network cameras; (ii) A platform to compare and exchange research results in which researchers can contribute with modules to solve specific problems;(iii) A framework to allow fast development of new video analysis techniques since one can focus only on his/her specific task; (iv) Creation of a high-level semantic representation of the scene using data extracted by low-level modules to allow the execution of video event analysis based on individual or group activities; (v) A testbed to allow further development on activity understanding since one can focus directly on using real data, instead of annotated data that may prevent the method from working on real environments; (vi) A platform to allow scalable feature extraction that uses the full power of multi-core architectures;This work is organized as follows. Section 2 discusses briefly the most common problems tackled in visual surveillance. Section 3 presents a review of published papers in recent years that discuss the issues and challenges involved in the deployment of modern visual surveillance systems and discusses works similar to the proposed framework. Then, Section 4 describes the proposed Smart Surveillance Framework (SSF). While Section 5 presents our experimental evaluation, Section 6 describes how a surveillance application can be designed in the SSF. Finally, Section 7 points our final remarks.According to the taxonomy proposed by Nazare et al. [17], problems considered in the surveillance domain might be divided into four groups: visual information representation, regions of interest location, tracking and identification, and knowledge extraction. Fig. 3 shows these groups and the relationship among the problems within each. While modules located at the top of the diagram define low-level problems, in the sense that they present low dependency to solutions obtained by other problems, e.g., background subtraction and pedestrian detection, modules at the bottom comprise high level problems since they depend on the results of other problems, e.g., action and activity recognition.The arrow in the right-hand side of Fig. 3 represents the dependencies among the problems. For example, to solve the action recognition, one first needs to correctly detect and track the person who is executing an action. Tasks composing this process might be affected by errors propagated along the task chain (e.g., detection errors will affect the tracking of a person, which will prevent the recognition of the action executed by this person). Therefore, it is necessary to solve the tasks in an accurate manner be able to solve problems presenting several dependencies, such as the activity recognition, responsible for making inferences regarding the activities being executed in a scene (e.g., loitering, identification of suspicious collaborations or carjacking).In the diagram shown in Fig. 3, Visual Information Representation comprehends tasks aiming at representing the information contained in the visual data, e.g., converting pixel information to a feature space which is more robust to noise and transformations taking place in the video. The goal of the Regions of Interest Location is to narrow down efficiently the locations of the scene where information regarding activities taking place can be extracted. Then, once the tasks in the previous category have located the relevant regions in the scene for each frame, the problems in the Tracking and Identification category will estimate their trajectories and identify the agents based on information including their appearance or their faces. Finally, after the objects and agents have been located, identified and their trajectories have been estimated, their actions and activities can be recognized, these problems refer to the Knowledge Extraction category. All the information collected by executing the tasks will be used to generate a knowledge representation regarding the scene, so that one can use such information to make inferences and perform scene analysis.The framework developed in this work has been designed to allow researchers to tackle the problems shown in Fig. 3 in such a way that the results achieved by solving these problems might feed an inference system used to understand the scene and the activities performed by the agents (persons in the scene).Nowadays, there is an increasing interest in surveillance applications because of the availability of low-cost sensors and processors. There is also an emerging need from the public for improving safety and security in urban environments and the significant utilization of resources in public infrastructure. These two factors associated with the growing maturity of algorithms and techniques, enable the application of technology in public, military and commercial sectors [19].Security surveillance systems are becoming crucial in situations in which personal safety could be compromised resulting from criminal activity. For this, video cameras are constantly being installed for security reasons in prisons, parks, banks, automatic teller machines, gas stations, and elevators, which are the most susceptible for criminal activities [20].In general, images acquired by a set of cameras may be monitored in real time from a command center, where exists many display screens from which security personnel constantly monitor suspicious activities. However, the burden of watching the entire video, detecting threats, and locating suspects is assigned to the human operator. This process of manually watching video is known to be tedious, ineffective, and expensive [2], because the attention span of human observers is inevitably limited [4]. Therefore, the addition of computational intelligence to alert the observers to the infrequent image feed which contained events of possible importance was thus a natural development as computing resources became both cheaper and more powerful.In the following sections, we overview the state-of-the-art in smart visual surveillance systems and introduce the evolution of these systems, as well as, their challenges.According to Valera [16] and Räty [20], the technological evolution of surveillance systems can be divided into three generations, summarized in Table 1.The first generation of surveillance systems started with analogue Closed-Circuit Television (CCTV). These systems consist of a number of cameras placed in multiple locations and connected to a set of monitors, usually placed in a single control room via switches (a video matrix). The main disadvantages of these systems concern the reasonably small attention span of operators that may result in a significant miss rate of the events of interest. The advantage is that the technology is mature. To perform computational processing on this type of system conversion from analog to digital video is required which may cause quality degradation.The advent of digital CCTV and high performance computers have led to the development of semi-automatic systems, known as second generation of surveillance systems. This generation benefited from the early progress in digital video communications, e.g., digital compression, robust transmission and bandwidth reduction. The advances of the second generation are that the surveillance efficiency of CCTV is enhanced. The difficulties lie within the robust detection and tracking algorithms needed for behavioral analysis.Most of the research on the second generation of surveillance systems is based on the creation of computer vision algorithms aiming at improving results for identification, tracking of multiple objects in complex scenes, human behavior comprehension, and multi-sensor data fusion. The second generation also improved intelligent human-machine interfaces, performance evaluation of video processing algorithms, signal processing for video compression and multimedia transmission for video-based surveillance systems [20].In the third generation, the technology revolves around wide-area surveillance systems, dealing with a large number of cameras, geographically distributed resources and several monitoring points. Such factors allowed the acquisition of more accurate information by combining different types of sensors and the distribution of the information. The difficulties are in achieving efficient information integration and communication, the establishment of design methodologies, and the task of designing and deploying multi-sensor platforms.The current research on the third generation concentrates on distributed and centralized intelligence, data fusion, probabilistic reasoning frameworks, and multi-camera surveillance techniques [16]. According to Räty [20], the main objective of the fully third generation system is to provide efficient data communication, management, and extraction of events in real-time video from a large collection of sensors. To achieve this goal, improvements in automatic recognition functionalities and digital multi-user communications are required.Several surveillance systems of the third generation have been designed and developed both in the industry and in the academia. These systems can be classified into two groups: general purpose and specialized in a certain function. The framework proposed in this work can be classified as general-purpose system because the user (researcher) has the freedom to develop his/her modules (as described in Section 4.6) and use them for any purpose involving surveillance.Several technologies for video-based surveillance have been developed under a United States government funded program called Video Surveillance and Monitoring (VSAM) [21]. This project, which can be considered one of the pioneers among the third-generation systems, designed a testbed system to demonstrate how automated video understanding technology can be combined into a coherent surveillance system that enables a single human operator to monitor a wide area. It looked at several fundamental issues in detection, tracking, auto-calibration, and multi-camera systems. The goal of VSAM testbed system was to develop efficient wide-area video surveillance systems using a distributed network of cameras. Similar to other newer systems, the SSF incorporates several concepts based on VSAM project, such as scalability, modularization and code reuse.Knight [22] is a fully automated system with multiple surveillance cameras that detects, categorizes and tracks moving objects in the scene using computer vision techniques. Although it can be used in various types of surveillance environments, the Knight is a closed framework that does not allow the implementation of new methods to replace or extend to the existing ones.Another system is the IBM Smart Surveillance System (S3) [23], which is among the most advanced surveillance systems nowadays. It provides the following capabilities: automatic monitoring of a scene, management of surveillance data, perform event based retrieval and receive real-time event alerts. In S3, computer vision routines are not implemented directly into the system, but as plugins. One of its disadvantages is that it requires the use of technologies from IBM, such as IBM DB2 and IBM WebSphere, which reduces its applicability for research purposes.San Miguel et al. [24] and Suvonvorn [25] proposed two general-purpose frameworks for processing and analyzing surveillance videos. Similarly to the SSF, they enable the development of modules for processing images and videos. However, they have adopted a different approach for data communication between the modules. In [24], the communication between modules is mapped through a database system, while in [25], the modules communicate directly, where a buffer is used as an exchange zone. In contrast, modules in the SSF do not communicate directly, but through a shared memory, which allows modules to be launched in an asynchronous way and the dependency among them can be defined as parameters, making the SSF flexible.The work proposed by Afrah et al. [26] addresses two aspects in the development of vision-based systems that are not fully exploited in many current frameworks: abstraction above low-level details and high-level module reusability. They proposed a systematic classification of subtasks in vision-based system development. However, this framework is inflexible in according to the exchange of modules, preventing researchers from comparing results obtained by different methods, which would be an important feature for the academic community.With a proposal similar to the SSF, the work proposed by Wang et al. [27] presents a vision system architecture that can readily integrate computer vision processing and make application modules share services and exchange messages transparently. The model of computation assumed by the authors is the same used in the SSF. In this model, modules communicate with each other through a shared memory and are executed independently and in parallel.Despite their similarities, there are some key difference between the two approaches: (i) In Wang et al. [27] the processing is centralized for some tasks, such as capturing sensor data, encoding and decoding video streams, and transforming different types of data, but on the SSF all processing is performed in parallel on modules, which allows a better use of the processing power; (ii) the shared memory on the SSF stores the scene information in a hierarchy based on the necessary structures for surveillance environment to avoid data redundancy, allowing low memory consumption (for more details, see Section 4.2); (iii) the SSF allows one to perform complex queries on data in shared memory through the Complex Query Server (CQS) (Section 4.4).Another aspect that differentiates SSF from other systems is that SSF implements the Feature Extraction Server (FES), described in Section 4.3, which allows the feature extraction to be performed using the entire computational power available in the system with the objective of maximizing the performance (one can use all available CPU cores). Even though feature extraction plays a central role on the surveillance algorithms, it does not receives special treatment in the other systems reviewed, being under the user‘s responsibility.To design efficient systems, it is necessary that researchers understand the nature of the environments in which the systems will be used. Another issue is to be able to interpret the requirements of the end user. Several authors [16,19,28,29] classified real-world applications into the following monitoring categories: public areas, interior and exterior of buildings, transport, military, entertainment and efficiency improvement.There are several papers published on specific-purpose surveillance systems. The work of Xia et al. [30] that focuses on wide-area traffic monitoring for highway roads. Odobez et al. [31], in turn, designed a metro station monitoring system that aims at automatically detecting dangerous situations which may lead to accidents or violence. The system proposed by Thornton et al. [32] allows an operator to search through large volumes of airport surveillance video data to find persons that match a particular attribute profile. Siebel et al. [33] especially deal with the problem of multi-camera tracking and person handover, on metro stations. A framework for people searching, where the user can specify personal attributes through queries such as “Show me the bald people who entered a given building last Saturday wearing a red shirt”, was proposed by Vaquero et al. [34]. It is important to note that, many surveillance applications are of commercial license, and thus, there are no scientific sources that describe them.As mentioned earlier, surveillance systems of the third generation contribute significantly to the design of various types of secure environments. Meanwhile, along with improvements, several challenges have emerged, causing many researchers devote their studies to do so. The work published by Liu et al. [35] discusses some challenging issues faced by researchers. Other papers addressing the challenges of smart surveillance systems have also been published recently [19,20,29,36].The next paragraphs present an overview on these challenges.Images are not always perfect in such systems. For instance, objects of interest can be partially occluded, camera lenses maybe covered or damaged, the person being identified may have covered him self/herself by purpose. Even when these problems do not exist, there are other aspects causing decreasing the image quality, such as, poor illumination, sensor noise, particularly in poor lighting conditions and low resolution of the cameras.A large-scale video surveillance system comprises many video sources distributed over a large area, transmitting live video streams to a central location for monitoring and processing. Contemporary advances in video sensors and the increasing availability of networked digital cameras have allowed the deployment of large-scale surveillance systems over existing network infrastructure. However, designing a smart and scalable surveillance system remains a research problem: how to design scalable video surveillance systems according to aspects related to processing power, memory consumption and network bandwidth?According to Fleck and Strasser [37], the privacy is a fundamental and very personal property to be respected so that each individual can maintain control of the flow of information about himself/herself. According to Gilbert [38], privacy comprises confidentiality, anonymity, self-determination, freedom of expression, and control of personal data. In the surveillance environment, it is important to guarantee privacy, as persons within a perimeter covered by cameras have very little choice of being filmed or not, whereas e.g., in the case of cell phone tracking the user still has the choice to turn his phone off. An example of functionality able to maintain the privacy of individuals is mask out some portions of the image [39].According to Haering et al. [36], one of the major challenges of developing a smart surveillance system is that it has to operate robustly during the entire time in a in wide range of scenarios. The only way to ensure robust and reliable performance is to perform extensive testing.The following questions are relevant for system evaluation. Is it possible to establish a repository containing some common surveillance scenarios? Who are the people providing these scenarios, and what are the evaluations criteria? To answer these questions, Venetianer and Deng [40] discuss some of the major challenges involved and provides a case study for addressing the evaluation problem.For algorithms in other areas, such as machine learning, there are standard data sets to validate, evaluate and compare the algorithms. However, for visual surveillance systems, each security concern is different, the objects being recognized and events being detected are more specific according to the application. Therefore, it is a very difficult task to evaluate a complete surveillance system from a case awareness viewpoint [35].The performance evaluation of video analysis systems requires significant amount of annotated data. Typically, annotation is a very expensive and tedious process. Additionally, there can be significant errors in annotations and part of the evaluation of the surveillance systems depends on what the system operator considers as relevant action since they are not objective. All of these issues make performance evaluation a significant challenge [29].Written in C/C++, using Open Source Computer Vision Library (OpenCV) and C++ Standard Template Library (STL), the Smart Surveillance Framework (SSF) is a tool built to provide a set of functionalities to aid researchers not only on the development of surveillance systems, but also on the creation of novel algorithms for problems related to video surveillance, such as those presented in Section 2.The proposed framework allows research works to focus only on their problems of interest without the necessity of creating an infrastructure for every problem that will be tackled, as it is done in the majority of cases nowadays. By using the SSF, the researcher can concentrate only on the problem at hand without concerning with the design of data representation, storage, communication and parallelism.The SSF has been designed to allow the development of third generation surveillance systems, providing features as tools to perform scene understanding, scalability, real-time operation, distributed multi-sensor environment and communication control, as discussed in Section 3.1. The next sections describe the design choices of the SSF to provide such desirable features.The architecture of framework can be divided into two main parts: user modules and SSF kernel, as illustrated in Fig. 4. While the former is where the user implements his/her surveillance and computer vision algorithms, the latter, responsible form controlling data communication, parallelism and data structures, lies outside of the user domain, being accessible only through configuration parameters.The SSF kernel is composed of the following components. (i) shared memory: the backbone of the SSF, it allows the communication among all other components and stores the data generated by user; (ii) Feature Extraction Server (FES): it processes feature extraction requests and return feature vectors to user modules to maximize the occupancy of the processing units available; (iii) Complex Query Server (CQS): this component allows user modules to search for specific data in the shared memory by using Prolog or queries in Structured Query Language (SQL) databases, for instance; (iv) execution control: this component controls the execution of the others SSF components and is responsible for the SSF initialization. In addition, this component has a graphical interface to aid the user to configure the run-time environment.The user modules are components written by the researchers to solve surveillance and computer vision problems (in fact, any algorithm can be implemented in the user modules). These modules use a well-defined interface to communicate with the the kernel components and the concept of data stream to communicate to other modules through specific data types (SMData).To the user’s perspective, the communication between modules does not exist directly because when implementing his/her module, the user requests data types as input without specifying which module will provide it and provides data types as output also without specifying target modules. The actual communication, controlled by the shared memory, is only set in the execution time when the user specifies the input and output modules. This communication scheme, known as publish-subscribe messaging pattern [41], allows the reuse of modules as components of applications with different goals and increases the flexibility of the framework once the modules with the same purpose are interchangeable.To achieve a flexible and modular software architecture, it is necessary that the modules be designed independently without knowing each other interfaces, which would reduce the flexibility when integrating a set of modules to solve a given task. Therefore, to address this constraint, the SSF provides a resource to store data and control of the data communication between the user modules. Such feature is referred to as shared memory and is responsible for the data communication control. This way, the modules only need to know the interfaces provided by the shared memory and not each other specific interfaces.The shared memory was designed to enable the development of different types of applications, including applications that are not in the visual surveillance purpose (outside of the scope of this work). To accomplish that goal, the shared memory is composed of four components as illustrated in Fig. 5and described as follows (the last component, the image and feature managers will be described in details in Section 4.2.3).The first component, called Specialized Shared Memory, is a specialization of the shared memory, with surveillance purposes. This component provides methods and specific data types for the surveillance domain (the data types are described in Section 4.2.1) and is available when the researcher is developing user modules.The second component is the Basic Shared Memory, responsible for the functions to access the data. This component does not depend on the context of the application, that is, their interface functions are general (i.e., functions to read and write data items) and have no knowledge of the data type being manipulated.The third component, called Memory Management, is responsible for the storage and management of the handled data. In the SSF, the data items are created by user modules and their references are passed on to the shared memory and the Memory Manager becomes responsible for the management of these references and their contents, releasing the user from this tedious and intricate task.To focus on the surveillance domain and to avoid data redundancy, the shared memory stores the data scene information in a hierarchy manner, as illustrated in Fig. 6. All data information are store in lists and only their references on the lists are stored in the hierarchy elements, which not only reduces the data redundancy, but also avoid the need for updating the information when data structures are changed.The hierarchical data structures available on SSF to represent the scene under surveillance are described in Table 2. All these data structures inherit from the class called SMData. This class contains the creation timestamp and a unique and unchangeable identifier, the latter is used as reference between data entries.For the information that cannot be represented as one of the aforementioned data structures, the SSF allows the creation of new data structures by inheriting from SMData, referred as User data. This special type enables specific data definition such as sensors output (audio, temperature, multi-spectral images) or exchange of specific data types between modules.Fig. 5 also illustrates the references between the data stored in shared memory. For instance, the samples of a frame are not stored directly within the frame data structure but only references between the frame data structure and their samples are used. One might also note the presence of links in frame data structure, that is because the content of images and feature vectors is passed to managers that provide a better memory management, as it will be discussed in Section 4.2.3.As described in Section 4.1, the SSF implements the publish-subscribe messaging pattern for communication. In this pattern, the senders (publishers) do not send messages (data items) to specific receivers, but instead, their messages are characterized into classes defined by the sender identification and the data type being transmitted, e.g., the tuple (Input, Frame) defines uniquely the output of module Input, showed in Fig. 7. Then, the receivers (subscribers) define the class of messages that they are interested in receiving, e.g., in Fig. 7, module Display expresses its interest in receiving messages defined by tuples (Input, Frame) and (Detector, Sample), this way it can shows in the screen the input frames overlaid with the bounding boxes provided by the Detector module.Since the publish-subscribe messaging pattern does not perform direct communication between the modules, i.e., the subscriber has to express its interest for a piece of data, the data provided by the publishers are stored in the shared memory and the actual communication between modules is performed by using the concept of data stream. First, when a module is designed, the researcher must define the input and output streams of module. A data stream is simply the definition of a class of messages, e.g., Stream(Input, Frame), in Fig. 7, indicates that module Input will provide frames. In a later moment, when the framework will be executed, the user must configure the connection between the inputs and outputs of the modules, according to data type compatibility.The great advantage of using data streams defined by the publisher and the data type is the flexibility brought to the framework. For instance, an user module that performs face recognition receives samples (regions of the image containing a face) as input. Using the SSF, this module only has to define Stream(publisher, Sample), in which the variable publisher will be instantiated only on the execution time with the identification of the face detector module that will be used. Therefore, multiple face detection approaches could be easily evaluated without any change on the source-code.Despite the communication between modules using data streams might appear direct from the user’s point of view, it is in fact performed through the shared memory. First, the data is written in the shared memory by the publishers and then, the subscribers request specific data. Therefore, all communication control and synchronization of the streams is performed by the shared memory.The use of a shared memory avoids the data replication. But no only that, another benefit of this design decision is that it is incremental in the sense that when a new data item is stored, it receives a new and unique identifier together with a creation time stamp. This allows one to trace back the entire lifecycle of stored data. For instance, one could verify when tracklets were merged and when new objects were created, which might be useful in the development of novel object tracking and recognition approaches.Given that user modules are executed asynchronously, it is the responsibility of shared memory to perform data synchronization because a module might consume information faster than another module can provide. Fig. 8illustrates an example of synchronization between two modules (M1 and M2) and the shared memory (SM). In the first instant of time (t1), M1 writes a new data item to the SM while M2 reads and processes the current data item in the memory. At time t2, the module M1 is processing, while M2 reads and processes the only available data item. Since there is no more data to read in t3, M2 is locked until a new information is made available, which occurs in t4. Finally, at t5, M2 is unlocked and performs a new reading. Therefore, by using locking mechanisms in the data reading, the SM is able to synchronize dependencies among modules without compromising the performance of independent modules.Since surveillance systems must handle large volumes of data, the memory on the SSF host machine can be easily filled. The data types with higher memory consumption in a surveillance system are the images generated by cameras and the extracted features. For instance, a video feed being recorded for hours can easily use the entire RAM memory on a common desktop. In addition, another significant factor is that depending on the problem to be solved, the number of extracted features can be large, which also consumes large amounts of computer memory. The other data types usually only have metadata or small integers such as the location of a bounding box or the identifier of the object. Therefore, their impact in the memory consumption is low.To deal with the high memory consumption by images, the shared memory has a mechanism, called Image Manager, capable of storing the captured images into the disk when necessary. To improve the computation performance, the Image Manager has a cache (with size set by the user), that from the principle of temporal locality44Once a location is referenced, there is a high probability that it will be referenced again in the near future.[42], stores the last manipulated images.To control the amount of allocated memory, each frame data has a reference to the manager, instead of having the actual data, as illustrated by the links in Fig. 5. The Image Manager, basically has two operations. The first is responsible for storing the image on the disk every time a new frame data is inserted into the shared memory. While the second operation, retrieves the disk image corresponding to a frame, when requested by the user module. Even though simple, these operations are performed in an intelligent way. For instance, if the image is already stored somewhere in the disk, instead of saving a copy, it just point to that original image.Since feature descriptors also consume large amounts of memory, the shared memory has a similar mechanism to Image Manager, called Feature Manager to handle the extracted feature vectors. It is important to note that both managers work transparently for the user, which manipulates images and feature descriptors as usual.The proposed managers not only dramatically reduce memory consumption, as described in Section 5.3, but they also allow the user to control the amount of memory consumed by the framework. The lack of such mechanisms would prevent the tracing of the entire lifecycle of stored data, which might be important when developing surveillance algorithms and applications as mentioned earlier.Feature extraction is critical for surveillance systems since several algorithms require feature descriptors as input. However, most feature extraction algorithms are highly time consuming and not suitable for real time applications. Researchers have also devoted their studies to optimize the feature extraction methods. One of the early works was proposed by Viola and Jones [43], the integral image, an intermediate representation that allows faster computation of rectangle features. Dollar et al. [44] proposed linear and non-linear transformations to compute multiple registered image channels, called Integral Channel Feature. Authors employed these descriptors into their ChnFtrs detector achieving state-of-the-art results in pedestrian detection. Another approach is the use of parallel architectures, as multi-core processors and Purpose Graphics Processing Unit (GPGPU), for feature extraction. For instance, Prisacariu and Reid [45] showed in their work efficient ways to extract Histogram of Oriented Gradients (HOG) descriptors using GPGPU, achieved speedups of over 67 × from the standard sequential code.To address the feature extraction problem, the SSF provides a powerful tool: the Feature Extraction Server (FES). It allows the feature extraction to be performed using the entire computational power available in the system to maximize the performance (one can use all available CPU cores). More specifically, researchers implement their feature extraction methods based on a template class and the feature extraction server will be responsible for splitting the task among the available processing units.The feature extraction server relies on an asynchronous approach to receive requests, process them and return feature vectors to the user modules with the objective of maximizing the occupancy of the processing units available. Once a request has been sent to the FES, it does not block the processing being executed in the module, which can continue working while the request is been processed by the FES. For instance, the module might be processing the feature vectors already extracted while others are being extracted. Therefore, all features vectors do not need to be stored in memory before processing, preventing from high memory consumption. In fact, the maximum amount of allocated memory can be set to avoid the process from using the virtual memory.Fig. 9illustrates the main components of the feature extraction server: request control, extraction method and feature extraction memory. Using FES, a feature extraction request is performed as follows. First, a module sends extraction requests by passing image regions from which the features will be extracted by a given extraction method. Such requests are sent to a queue in the request control, which allows the module to make all requests for an image and continue its processing while the features are extracted. Then, the request control selects the extraction method chosen by the module and forwards the requests to the extraction method, which process them using N instances (N is defined by the user). First, it checks the memory availability in the feature extraction memory, if no memory is available, the extraction method waits until some memory has been released. Finally, once the feature extraction is completed, the feature vector is pushed to the output queue and it is ready to be retrieved by the requesting module.The request control is responsible for screening the requests made by the modules. It is composed of an input queue a data structure for storing information regarding the feature extraction methods available. Once a request enters the queue, the request control forwards it to the correct feature extraction method. The request control is useful in the sense that the feature extraction becomes centralized, such that two modules requiring the same feature extraction method will use the same instance of the extraction method, which will allow the usage of cached features if two modules request feature extraction for the same image region.The extraction method manages the feature extraction for a specific feature descriptor, such as HOG, Gray-Level Co-occurrence Matrix (GLCM) and others [46]. When the extraction method receives a request, it first verifies in the cache if the same request had been made before and the feature descriptors are already available, if so, return them, otherwise it checks in the feature extraction memory whether there is memory available in the feature extraction memory (experiments show that the usage of cache reduces greatly the computational cost for feature extraction, see Section 5.4.2.).The feature extraction memory allows the FES to set a limit of memory that can be used for the feature extraction process, otherwise the entire memory available in the machine could be consumed quickly compromising the execution. If there is no memory available, the extraction method is blocked until some memory is released (some module retrieves an extracted feature vector from the output queue, process it, and sets it as released), otherwise, it sends the request to one of its instances to perform the actual feature extraction for an image region.The advantages provided by the feature extraction server include the following. Besides of using methods already implemented, the user can implement his/her own feature extraction methods which will have their processing distributed according to the computational power at hand or according to the parameter setting chosen by the user. In addition, it allows users to develop novel feature descriptors and evaluate them easily on problems related to surveillance, such as detection and recognition. Finally, this centralization approach based on a server to extract features allows the caching of features vectors so that several modules might share the same vectors for different purposes.To search for specific data, such as actions being performed in a given time interval or tracklets intersection of two given subjects, one may retrieve data from the shared memory by implementing the query in a module. However, such approach may be inefficient since the architecture of the shared memory is optimized for simple write and read requests. To allow user modules to search efficiently for specific data in the shared memory, the SSF provides the Complex Query Server (CQS).CQS is independent of the underlying query/inference solution, for instance Relational or Big Data Databases and logic programming such as Prolog. Therefore, the user modules are not required to know how to write a query in a specific solution. To achieve this independence, the CQS defines a common interface with modules so that each complex query solution underlying must implement this CQS common interface which either may be simplified to allow easily integration with as many underlying solutions as possible or may also be complete enough to easily allow complex queries. Any implementation of an underlying query/inference solution in the CQS common interface can be performed by implementing initialization, storing and querying methods. The following paragraphs describe how these methods are used in the SSF.The initialization method requires that the user informs which fields for instance, time-stamp of image, location of sample and time interval of a tracklet, will be stored in the CQS for future search. This information is given at the definition of each data type and allows the framework to grow in a scalable way, i.e., without modifying CQS structure when new data types are incorporated to the framework.At execution time, the CQS initializes by iterating over each data type and registering the searchable fields. This initialization is required in some solutions to create underlying structures such as tables in SQL Databases. Then, the CQS retrieves data items from the shared memory and passes them to storing methods so, they can be registered in the underlying structure, a row in SQL Database or a fact in Prolog, for instance.For the querying methods, a user module retrieves a copy of a CQS instance with access only to query methods. Query methods are subdivided into filter and retrieve methods: filter methods are simple operations (“equal to”, “less than”, “or”, among others) that receive field and data types and change the internal state of the CQS instance by building a partial filter of the field and integrating it with the previously state; retrieve methods return data to the user considering the filtered state of the CQS instance.As an example, suppose that one is interested in recognizing a fighting activity between two subjects by analyzing the output of an identity recognition module and an action recognition module. The fighting activity is characterized by two subjects, close together, facing each other, and at least one of them is performing punching actions. Examples of queries to identify this fighting activity are given in Prolog, SQL and in CQS query format in Fig. 10. In this example, tracklets are represented by horizontal lines and the action being performed is shown inside a rectangle. A fighting activity may be characterized by any two subjects that are close together facing each other and at least one subject is performing punching actions. The query result R is the reference to the video segment containing the action.The SSF components that will be used for an execution are chosen by parameter settings, which increases the customization of the framework. The parameters might be supplied via a configuration file or assigned through the Graphical User Interface (GUI). Once the configuration file is provided, the Execution Control is responsible for initializing the remaining components and for assigning values to the parameters.The SSF first initializes the internal components (i.e., FES and CQS), by assigning values to its parameters. Then, the instantiation and configuration of the parameters of the user modules is performed. It is worth noting that only the modules that are listed on the configuration are initialized. The execution control also defines data flows referred to as data streams, between modules and shared memory. These streams are declared in the configuration file (or in the GUI), in which the user defines how the modules will communicate with shared memory, stating which types of data will be transmitted, according to those that are implemented in the user module.Due to the large number of parameters, the configuration file becomes complex and difficult to maintain. Thus, to deal with this problem, the SSF provides a Graphical User Interface (GUI) component. Its goal is help the user to configure the runtime environment for the SSF. Through it, we can perform the following tasks: (a) configure modules defining the parameters values; (b) create and setup pipelines; (c) define the data flow between the modules and/or pipelines; (d) configure the SSF internal components, such as the shared memory and CQS.The user modules are the framework mechanism where the researcher implements his/her algorithms of typical routines of a surveillance system, such as person detection, background subtraction, face recognition, person tracking and re-identification, and action and activity recognition.Every module follows the same standard interface, in which the user (researcher) defines its input and output data types and its parameters without specifying which module will provide or receive them. This is done later, in execution time by reading the dependencies from a parameter file (or the GUI), which makes the framework highly flexible and versatile. Once the module is launched, an execution routine (where the user implement his/her method), is called and executed.To illustrate the design of an user module, Fig. 11shows the source-code for a background subtraction module. The user must create a new class, in this example called ModBGS, inheriting from the UserModule class and implement a set of methods, described as follows.Class Constructor (lines 6–10): This is where the user declares which parameters are used and which data types are required and provided by the module. The example module uses an integer parameter (lines 3 and 7), requires a frame as input (line 8) and provides another frame as output (line 9). In execution time, after reading the parameters, the execution control (Section 4.5) will set the values of the variable inMod with the name of the module that will generate the input frame. Therefore, with such information and the type of the data (frame in this example), it is possible to define the data streams for this module, as discussed in Section 4.2.2.Setup (lines 12–15): This method performs consistency checks. The example shows a test where the only module parameter must be a positive integer, in case of failure, an error message will be showed to the user and the execution will be aborted until the parameter values be corrected.Execute (lines 17–27): This is the main method of the module and is where the user implements his/her algorithm. The example first instantiates the data streams for the input and output. Line 18 creates an input stream, whereas the line 19 creates an output stream, both of frames (SMFrame type) – note that the output stream does not specify the module name, meaning that any module can read the frames provided by this module. Then, specific smart pointers to manipulate SSF data types are declared. These pointers will make reference to the input image, as well as the image resulting from operation. Finally, the iteration loop of lines 22–26 performs the background subtraction.In the main loop of the source-code showed in Fig. 11, for each image received from the input stream (line 23), the background subtraction operation is applied (line 24) and then the resulting image, bgsMask, is sent to the output stream. The auxiliary function BGSAuxFunction should be implemented in advance by the user. The iteration loop is finished when there is no more input images (input module stop providing frames).It is important to note that when the execution control (Section 4.5) calls the Execute method, it first launches a new execution thread, which increases the performance of the system, and the lifetime of this thread is valid while its is within the Execute. After that, the thread will be terminated and the execution of the module will be finished. Therefore, the user is responsible for reading the data received in the input, for instance as performed in line 22 of the source-code in Fig. 11, in which the module keeps reading the input stream while there is data available.Another important feature related to modules, is the creation of execution pipelines – collections of user modules behaving as a single module. A pipeline allows one to group several modules of individual methods in a sequence. Once defined, multiple instances of the pipeline can be launched just by changing their inputs. For instance, one pipeline can be launched to process data from each surveillance camera attached to the system. Such a feature also makes the framework more scalable.A demand that the framework user might have during the development of surveillance algorithms and applications is the creation of cyclic pipelines, as illustrated in Fig. 12. An example is the on-line learning, where results of the execution of the algorithm are re-used by the learning method to improve results. However, such synchronization might not be easy in a system based on data streams because before updating, a module might need to know if some another module has finished processing some piece of data.To make the necessity for synchronization clear, let us consider part of the pipeline in Fig. 12, assuming that module M3 receives frames and performs object detection, module M4 receives samples and performs non-maximum suppression, and module M5 performs filtering to identify which samples present very high confidence as object and passes to M3 only those samples, which will be used to update the object model in module M3 after the detection be performed for each frame. However, the problem is that module M3 does not know when it should update the model because it outputs n samples and it will receive back m samples, where m ≤ n. To allow this type of synchronization, we developed a special mechanism called pulse.The pulse works as follows. When a module, referred to as Feedback Module, needs to wait for other modules in the pipeline to finish part of the processing, it will generate a pulse that will be propagated through the pipeline and returned back to the feedback module. When the pulse is propagated, the processing for the feedback module is locked until it receives the pulse back.Using the previous example, after all samples have been generated by M3 for a frame f1, M3 sends a pulse and stop processing waiting for the pulse to return. The shared memory will only deliver the pulse to M4 when all samples of f1 have been processed, i.e., when M4 tries to read a sample related to the next frame. At this point, the pulse will be propagated forward and will reach module M5 only after all samples related to frame f1 have been propagated to M5. Based on the same idea, the pulse will be propagated to M3 only after all samples related to f1 have been processed by M5. Therefore, when the pulse reaches M3, the samples for the frame f1 will already be processed and the processing of M3 can be resumed.The modules comprising the cyclic pipeline does not need to have knowledge of the pulse, because the pulse is automatically transmitted between modules through the shared memory. In addition, the pulse is propagated to other modules in the pipeline, e.g., M6 and M7, which are not in the cycle and will not interfere in the processing. The reason is that the feedback module is not aware of the existing pipeline (the data could come back to module M3 through module M7). Thus, this design choice maintains the flexibility of the framework to add and remove modules from cyclic pipelines.

@&#CONCLUSIONS@&#
This work proposed a novel framework to allow further development on computer vision methods and surveillance applications. The architecture of the Smart Surveillance Framework (SSF) allows the simultaneous execution of multiple user modules that can be developed independently since they have communication and synchronization through a shared memory, which contributes to the scalability and flexibility. The framework also provides two important components, the feature extraction server and the complex query server, these components maximize the computational resource usage and facilitate the scene understanding, respectively.The proposed framework will be made publicly available and besides of making surveillance research using real data and in real-time processing easier, it will also allow researchers to provide their methods (implemented as modules) to be used by other researchers to compare how results. Nowadays, it is difficult to compare results to previously published works since the code is not always available or it is necessary to adapt the code to work on new data sets. By using the SSF, one can provide the source-code (or just its compiled version) of the module to solve a computer vision problem and when another researcher proposes a novel solution, he/she can use that module to compare the results different data sets or to compare the computational cost in the same machine. Therefore, the SSF might also contribute to a more accurate validation of computer vision algorithms, mainly those related to surveillance.