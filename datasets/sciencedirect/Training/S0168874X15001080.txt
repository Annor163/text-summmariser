@&#MAIN-TITLE@&#
Analyzing the potential of GPGPUs for real-time explicit finite element analysis of soft tissue deformation using CUDA

@&#HIGHLIGHTS@&#
We present and examine an implementation Total Lagrangian FE algorithm.Implemented for many-core GPGPU devices to dramatically increase execution speed.Detailed performance analyses and error measures are put forth.Algorithm and devices׳ performance is compared to an industry-proven code.The paper examines and demonstrates the potential in real-time FE usage.

@&#KEYPHRASES@&#
Finite element method,Biomechanics,Parallelization,Soft tissue deformation,Real-time,Nonlinear solver,Time integration,Explicit,GPGPU,

@&#ABSTRACT@&#
As the presence of finite element implementations on General Purpose Graphics Processing Units (GPGPUs) is the literature increases, detailed and in-breadth testing of the hardware is somewhat lacking. We present an implementation and detailed analysis of an FE algorithm designed for real-time solution, particularly aimed at elasticity problems applied to soft tissue deformation.An efficient parallel implementation of Total Lagrangian Explicit Dynamics implementation is elucidated and the potential for real-time execution is examined. It is shown that in conjunction with modern computing architectures, solution times can be significantly reduced, depending on the solution strategy.The usability of the method is investigated by conducting a broad assay on ranging model sizes and different cards and comparing to an industry-proven FE code Abaqus. In doing so, we study the effect of using single/double precision computation, quantify and present error measurements as a function of the number of time-steps. We also examine the usage of a special texture memory space and its effect on computation for different devices. Adding material complexity in the form of a tissue damage model is presented and its computational impact elucidated. The aggregate results show that, for a particular set of problems, it is possible to compute a simple set of test cases 30–120 times faster than current commercial solutions.According to the speedups achieved, an indication is provided that the GPGPU technology shows promise in the undertaking of real-time FE computation.

@&#INTRODUCTION@&#
As the Finite Element method is becoming more pervasive in scientific and engineering endeavors, the application space of the method increases as well. Likewise, computational architectures are changing and computational power advances rapidly. It is therefore prudent to re-examine the performance of existing algorithms on novel hardware and implementations. We present an analysis of an efficient finite element implementation on modern General Purpose Graphics Processing Units (GPGPUs). The potential of real-time execution is examined, particularly aimed at elasticity problems of soft tissue deformation.Surgery is shifting towards less invasive techniques, which come at the cost of increased complexity, and in the case of telesurgery, a lack of haptic feedback. The accompanying problem of the lack of force-feedback can be partly compensated by intelligent information flow to the surgeon, in the form of mimicking the known elastic behavior of soft tissues. Likewise, virtual reality training simulators benefit from increased visual and haptic fidelity in the treatment of tissue deformation. It is clear that in both use cases execution time is critical.In essence, a biomechanical simulation of the surgical situation is required. As with virtual surgical simulators, real-time requirements solicit efficient computation schemes capable of dealing with large deformation of soft biological tissue, many of which are known to behave in a nonlinear hyperelastic fashion. Implementations of various algorithms on different novel many-core GPGPU devices have shown promise in fulfilling these speed constraints. However, the rapid pace of advancement in computational technology necessitates the reimplementation and reevaluation of existing solutions on novel hardware. With the evident rate of increase in their performance and complexity, this is especially true for many-core GPGPU devices.Even with the numerous publications on the topic of real-time FE on GPGPU devices, there exists an apparent lack of in-breadth testing and examination involving more, different, physical devices and solution strategies, a track followed here.In the remainder of the section we present a short exposition on the background of soft tissue deformation methods, GPGPU technology evolution and the employed Compute Unified Device Architecture (CUDA).There currently exist a multitude of research papers dealing with deformable models used for surgical simulation, varying in levels of accuracy and speed of execution. The general rule of thumb is that more accurate methods require more computation time, as expected. One can say that at one end of the spectrum there are FEM-based models – very accurate yet prohibitively slow; on the other end of the spectrum the spring–mass model is fast and often-used for surgical training. However, fast heuristic models like the spring mass method [1,2] generally lack the ability to use nonlinear material models or material models in general beyond simple springs and dampers. For an overview of methods seen in literature, the reader is referred to [3,4].In line with the aforementioned requirements imposed by the biological domain under consideration one is compelled to use a solution method that is nonlinear both in terms of geometry and material behavior. The method of choice satisfying the requirements is a nonlinear finite element formulation capable of processing large deformations.FE analysis of soft tissues for surgical simulation is an idea already pursued by several groups, most noticeably and in chronological order [5–10]. These works also represent a sample of the evolution of research on real-time usage of the finite element method. Several approaches have been explored, both for implicit and explicit integration methods, static and variable topology of the mesh, different element types, orders and solution strategies. Publications on implicit FE are numerous, but perhaps the most notable early work can be found at [11] and a more recent [12]. To the best of the authors׳ knowledge, Miller et al. [13,14] were the first to put forward work on an efficient nonlinear real-time FE solution to be used intraoperatively to provide information about tissue response on patient-specific anatomy during actual surgery. Their aim was to compute the final deformation field and track the position of critical elements within the mesh, e.g. a tumor. The solver was subsequently used by several groups, most recently [15].Regardless of the details of the method chosen, it is still evident that real-time FE analysis is computationally very demanding, and this is further exacerbated by the nonlinearity of the problem. Consequently, implementations of FE algorithms are subject to rigorous optimization strategies, most important of which being parallelization. Such a solution, using high performance computing devices and principles is widely adopted and most (if not all) commercial FE software packages support this feature, solving on multiple computing cores within a CPU, multiple CPUs within a single platform, or multiple platforms using a fast interconnect network. This type of parallelization can be considered coarse-grained, since generally a larger amount of work is done by individual units of the computing system.Real-time computation on parallel or distributed systems demands favorable computation to communication time ratios. It also necessitates execution of problems of sufficient sizes to offset the latencies emerging from data transfers. The problem at hand, for which total solution times are essential, is of such size and nature that execution on a loosely coupled system (e.g. CPU clusters) would exhibit high transfer latencies and stall computation. Finite element analysis is a non-arbitrarily divisible problem that imposes high communication requirements. Moreover, for our case of explicit analysis of limited size models, a high frequency of relatively small transactions is necessary. Such a problem benefits from having high memory throughput (in excess of 200GB/s on current GPU generations) and low latencies of a very tightly coupled system such as a GPGPU.It is well known that there exists a significant difference between GPU and CPU hardware architecture. For specific problems this fact potentially leads to large increases in computational effectiveness [16]. Even older generations of graphics cards have higher computational throughput and memory bandwidth internally than current CPUs [17]. Utilization of GPUs to perform general purpose computation, especially linear algebra, started a decade ago by Göddeke [18] in 2004. GPUs, being inherently parallel machines, now in a considerably enhanced form, constitute a platform for parallelization of their own. Graphics processing units with a multitude of computing cores on a single board and on-board memory have very high arithmetic and memory throughput. Certain limitations do exist: memory capacity, accuracy and others elucidated further in the text. They are considered for fine-grained parallelization and these principles and hardware are employed to speed up nonlinear FE computation. Their development and dissemination being driven by the gaming industry and making the hardware more accessible through frameworks such as OpenCL [19] and CUDA contributed decidedly to their increased use in scientific computing today. Harnessing the power of the GPU, however, was not straightforward. Prior to 2007 programming for the GPU was constrained to a more complex and indirect approach of inserting non-graphics code into the graphics pipeline of underlying purely graphical frameworks like OpenGL or Direct3D. Pixel shaders are an integral part of these graphics pipelines and are intended to execute in a highly parallel fashion, computing each pixel׳s RGBA color value. It is by writing shader programs that parallelization was exposed. It was, in effect, tricking the GPU to perform general computation instead of computing color values. For an excellent overview of pre-CUDA GPGPU computation see [20].The CUDA programming model uses kernels, essentially C language functions, that execute the same code on different data, analogously to a vector processor. Threads are organized in 1-, 2- or 3-dimensional blocks, themselves generally consisting of one or multiple thread warps (a group of 32 synchronously executing threads). The total number of possible warps executing concurrently depends primarily on the memory requirements per-thread and is governed by a complex memory model involving architecture specific limitations on maximum runnable blocks, cf. Table 1. The CUDA memory model consists of several types of memory with different physical location (on and off the chip) and therefore different sizes, latencies and usage rules. Cards generally feature a two-level cache memory system. This proves very useful with data of high access frequency, or data with random memory locality or data dependent memory locality. Generally speaking, a problem that exhibits internal parallelism is decomposed into small work units that have no sequential dependency and can therefore be processed concurrently. It is up to the programmer to determine the optimal granularity of this decomposition, taking into account memory types, their capacity, rules of use and the algorithm at hand.Other considerations while using CUDA involve synchronization, memory layouts, throughput, latency and memory bottlenecks, etc. Large speedups can be achieved, but at this point in time, some understanding of the underlying architecture is required. This holds true especially of the interplay between memory and instruction issue subsystems and the executing algorithm, since CUDA delegates much of the complexity back to the developer. The task is further hindered by the existence of different architecture generations that feature different amounts and even types of memory, different instruction issue mechanics, etc. Herewith we avoid further functional detail on the CUDA architecture as the work focuses more on testing than implementation details. Some technical detail is necessary, however, and will be mentioned alongside associated observations further in the text. Necessary information about the architecture of GPUs used is available in Table 1. For the interested reader a detailed overview of CUDA is available through [17], and [21] for advanced material.This paper presents an implementation of the Total Lagrangian Explicit Dynamic algorithm on modern GPGPUs using CUDA. We use a range of GPUs including the current (as of the time of the writing) Maxwell and previous Kepler and Fermi architectures. A single/double precision floating point computation study is performed to examine the accumulation and evolution of the round-off error and its impact on accuracy and execution time. Similarly, the tests have been performed on a range of model sizes to examine the scaling of performance. Furthermore, each simulation is run with and without the texture memory space and the presence or absence of additional material complexity (tissue damage model) and its impact is commented upon. An industry-proven finite element code Abaqus is used as the ground truth in terms of accuracy, as well as a baseline to measure speedup against. In Section 2 details are presented of the algorithm and implementation, hardware, example problems and solution regimes, followed by results (Section 3), discussion (Section 4) and conclusions (Section 5) on the topic.After finite element spatial discretization using the total Lagrangian (TL) formulation, we arrive at the familiarP(u)=Rsystem of equations, where P, u, and R are the internal nodal forces, displacements and external forces, respectively. The central difference scheme is used for temporal discretization and drives the simulation forward in the time domain. Since the aim is to solve a static problem using a dynamic (explicit) method, mass and damping contributions are added into the equation:(1)Mu¨+qMu̇+P(u)=R,where M is the diagonalized mass matrix. In the notation throughout, the left superscript denotes time-step and the right subscript denotes element scope. By using the familiar constant step central difference formulas, we define the following temporal derivatives:(2)u̇t=ut+Δt−ut−Δt2Δt;u¨t=ut+Δt−2ut+ut−Δt2Δt2By combining (1) and (2) and with some algebraic manipulations around the known forces and the sought displacements:(3)ut+Δt=a(R−P)+but−cut−Δt;where a, b, c are the final central difference coefficients, Cris the convergence rate of the central difference scheme set close to unity and q is the damping coefficient and its expression we opted for(4)a=2Δt2(2+qΔt)Me,b=1+(2−qΔt)2+qΔt(5)c=2−qΔt2+qΔt,q=2(1−Cr2)Δt(1+Cr2)Further detail can be found in [13,22]. A pseudo-code of the solution of the system in Eq. (1) is provided below.Pre-computation phase:1.Compute shape function derivatives in initial global coordinates (the reference configuration in total Lagrangian) from natural isoparametric coordinates∇NIsoand initial nodal positionspe, using the Jacobian matrixJe0:(6)Je0=∂(x,y,z)∂(ξ,η,ζ)=∇NIsope(7)∇NGlo,e=Je−10∇NIsoAssemble initial strain-displacement matrix:Be0Compute element volumes and diagonalized mass matrixM=MeICompute the central difference coefficients a, b, c and damping factor q from Eq. (4).Iteration phase. For every time point t:5.Compute the deformation gradient using the shape function derivatives and current displacementsuet:(8)Fet=I+∇NGlo,euetCompute the right Cauchy–Green deformation tensor, Jacobian determinant:(9)Cet=FeTtFet,Jet=det(Fet)Compute the Second Piola–Kirchhoff stress:(10)Set=2∂tΨe∂tCeCompute force contributions. Total Lagrangian expression for force computation, using 1 point Gaussian (under-)integration:(11)Pet=∫V0Be0FeTtSetdV0=8det(Je0)Be0FTtSetSummation of elemental contributions into the global force vector:(12)Pt=∑PetObtain new displacementsut+Δtusing Eq. (3)Impose boundary conditions for the next step:(13)uBCt+Δt=uBCimposedt+Δt(14)RBCt+Δt=RBCimposedt+ΔtAdvance to the next step:t←t+ΔtIn light of increasing the speed of the algorithm, pre-computing all possible values is essential. Due to the total Lagrangian spatial discretization scheme chosen, all derivatives with respect to the original (reference) configuration do not change. The shape function derivatives, therefore, do not change across time-steps and are computed in the first stage. The central difference method coefficients are also pre-computed as they too do not change throughout.Note that since the mass matrix is diagonal, the equation in step 10 is algebraic and each solution displacement vector may be computed independently. This is, in fact the originating point of much of the increased speed provided and is fundamental to explicit formulations. An optimal convergence rate, especially for dynamic relaxation has been discussed in [23], indicating an additional potential optimization in terms of reduced number of executed steps, an idea not presently pursued.The contributions to the global force vector are computed in step 8. The force contributions are summed in the following step. Single point Gaussian integration is employed within the 8-node under-integrated hexahedra to obtain a simple expression for nodal forces from element stress in step 8. Note that here we implicitly update the strain-displacement matrix with the current deformation gradient, at every time-step. The element stress was in turn computed from the right Cauchy–Green tensor and deformation gradient in steps 5 to 7. Depending on the material model used, the stress computation in step 7 can vary. In our implementation, Neo-Hookean material with and without damage are used, as explained in Section 2.2. The nodal displacements are calculated using the resultants and the central difference scheme. Finally, new displacement boundary conditions are assigned to the boundary nodes, to prepare for the next step. For a more detailed discussion on TLED, the reader is referred to [13] and subsequent work by the group.The critical time-step was computed by using linear theory [24] and honors the Courant–Friedrichs–Lewy (CFL) condition. A costly modal analysis of the domain is avoided this way, and it is assumed that the following equations with an appropriate weighting factor are sufficient. A weighting factor (Courant number) of 0.8 was used in the present case but generally depends highly on the problem and requires some engineering insight. E.g. presence of contact or highly nonlinear behavior of material or loading curve would necessitate a lower weighting factor:(15)Δt=Lec;Le=VeAe;c=λ+2μρwhere Leis the characteristic length, computed using the smallest element area in the model Aeand its volume Ve, ρ is the mass density of the element.The material model employed is a 3 dimensional hyperelastic Neo-Hookean model, often used for soft tissues. The Neo-Hookean is conceptually relatively simple and easy to implement, especially using the proposed computational framework. A strain energy density function (SEDF) can be defined by an additive split into a deviatoric and a volumetric term:(16)Ψ=μ2(I¯1−3)+κ2(J−1)2,where μ and κ are material constants – shear and bulk modulus, respectively, cf. Table 2.I¯1=J−2/3I1is the first invariant of the deviatoric part of the right Cauchy–Green deformation tensor C. The second Piola–Kirchhoff stress tensor S can be derived from the SEDF according to step 7 in Section 2.1.By using the chain and product rules, and the known:(17)∂I1∂C=Iand∂J∂C=12JC−1the final form of the Second Piola–Kirchhoff stress is obtained:(18)S=2∂Ψ∂C=μJ−2/3(I−tr(C)3C−1)+κJ(J−1)C−1(19)Ψdev=(1−d)Ψ^dev(20)d=γ[1−exp(−β/m)]whereΨ^is the undamaged deviatoric strain energy,γ∈]0,1]is a weighting factor and m is a parameter of the damage model. The actual damage variable is d, indicating the loss of integrity of the material on a scale from 0 to 1. β is the maximum value of energy in the interval0<t<τinitially set to zero:(21)β=max0<t<τ(Ψ^(t)−Ψ0)The value forΨ0is set to an experimentally obtained value above which damage is initiated. In the case of arterial tissue, for example, the strain energy at systolic pressure can be used. Using this adapted strain-energy function, the second Piola–Kirchhoff stress in the damaged materialSdambecomes simply(22)Sdam=(1−d)SThe pseudocode described in Section 2.1 is implemented using Nvidia CUDA. An overarching and somewhat obvious factor in speeding up code is replacing loops with parallel functions (kernels). However, this can only be done if iterations are independent from one another. For this reason parallelization is done over elements and nodes rather than time-steps.The solver has been ported to the GPU entirely. The host CPU serves only to iterate through time-steps invoking the respective kernels, swapping pointers and performing other trivial work. The GPU handles the entire volume of calculation described in steps 5–11. The parallelization of the algorithm is achieved by splitting the work into three functions: the computation of elemental forces, assembly of the global force matrix and computation of new displacements, and finally the imposition of new displacement boundary conditions. Costly communication between the CPU and the GPU depends on the higher latency and lower bandwidth of the PCI bus, and is kept to an absolute minimum, transferring data only at the very start and end of a simulation.The first kernel runs one element per thread and computes equations in steps 5–8. It receives input data on element connectivity, shape function derivatives, material model parameters, damage parameters, volumes, etc. The output of the routine is individual force contribution vectors of each node, for all elements. The damage computations are included in this function since the strain energy and stress are computed here. Computing force contributions is the most demanding part of the algorithm, taking about 70% of total solution time. With high internal memory requirements to store input data as well as the derived values (like the deformation gradient or stresses) the parallelization level is significantly reduced. Consequently the hardware will allow and maintain only a relatively small amount of threads in flight, e.g. 3584 elements are processed concurrently on the C2075 device in the code. Note that individual force contribution vectors from adjacent elements of each node are stored separately instead of summed immediately to avoid storage serialization or simply incorrect results due to the non-atomic nature of CUDA global memory writes as dedicated global atomic operations have not been used. If required, damage computation is performed in this kernel, between steps 7 and 8. In the case that damage-corrected behavior is requested, for relatively few memory fetches (Ψ0 and β) per element we can examine the computation time response. Note that the computation of the strain energy (Eq. (18)) is only performed in this case.The second kernel operates on a per-node basis and processes steps 9 and 10. Initially, the computed force contributions from the surrounding elements are accumulated and summed. The aggregate forces are used immediately in the central difference scheme without explicit storage. Here, the precomputed central difference coefficients a, b and c are used. The resulting displacement vectors are again per-node values and are stored to global memory for use in the following kernel. The imposition of force loading is applied in step 9 in the developed code, after summation.The last and in many ways the simplest kernel (step 11) performs the trivial task of imposing displacement or force boundary conditions by artificially modifying associated vectors of relevant nodes. Note that another approach is possible and was explored comprehensively in [25] where kernels 2 and 3 were merged into one due to the same granularity of the kernels. Due to the much lower number of state variables associated with nodes in the two node-granular kernels, they are generally never memory-bound. Since they do not take up much of the execution time they are much less interesting from an optimization point of view.A special type of cached global memory access called texture memory or texture fetching is provided to the programmer in part to facilitate the handling of data with irregular or random memory locality. In our algorithm, the nodal displacement data depend on element connectivity information and therefore has irregular spatial locality in memory. Consequently, our code includes toggling of texture memory functionality, applied only on nodal displacement data. Similarly, the code includes the toggling of damage subroutines for easy measurement of the computational impact of including damage calculation. These devices also have both single and double precision computing cores, which are physically distinct. The number of double precision cores is generally a fraction of their single precision counterparts, exact numbers depending on the device generation. Due to this fact, computation using double precision is considerably slower (cf. Table 1) at peak saturation but enables more accurate results, as examined and discussed in Section 4.The testing regimes employed in this study are all performed on the uniform compression of a simple cube model (Fig. 1). Cube meshes of 50×50×50mm with varying mesh density have been created for this purpose. The above-mentioned Neo-Hookean material model was applied to all the elements of the mesh and stable time-steps have been computed according to equations in Section 2.1.The simple boundary conditions applied prescribe no displacements in the axial direction (Z) for the bottom nodes and enforces displacements along the same axis for the top nodes, all other degrees of freedom are unconstrained. The load was applied gradually along a smooth loading curve up to 20% strain. This enabled us to minimize inertial effects and to study the phenomena under quasi-static conditions. The 5s loading time was distributed along around 6700 to more than 60400 steps in different models.The solution (Fig. 1 on the right) is homogenous in the stress distribution. For this simple problem, an even coarser mesh or an analytical solution would essentially provide the same resulting solution field – the accuracy does not increase with mesh refinement. This intentional decision is made for the purpose of easier mesh generation and error comparisons as well as the lack of complicating effects such as potential loss of stability due to subdivisions of complex meshes or a likely additional reduction of step size in case of a few poorly formed elements. These issues would somewhat occlude the effect of parallelization of TLED, our primary goal.Simulations were run multiple times with a different combination of settings. These solution regimes are designed to provide a multitude of results and test the algorithm and the hardware under different conditions.The round-off errors were evaluated by the difference between GPU solutions only.Extensive testing of the described algorithm has been conducted encompassing a variety of changing conditions. Since different GPUs exhibit different internal hardware and methods, the algorithm was executed and data collected on the Nvidia devices listed in Table 1. The list contains devices from the latest Maxwell [26], and preceding Kepler [27] and Fermi generation [28]. As opposed to older generation cards, all of the mentioned GPUs have double precision floating-point computation capability.The simulation code is extensively templated to toggle among different solution regimes, including those between single (fp32) and double precision (fp64) for comparison in terms of speed, accuracy and the evolution of the round-off error. The templated device and host code data structures and kernels also assure that the compiler optimizes away all unnecessary branch checks when running a particular combination of parameters in a solution regime, i.e. double precision with textures and damage code in a single kernel. An important reduction in thread divergence and consequent slow-down is avoided this way and presently there is no divergence anywhere in the code.The existence or absence of additional material complexity (damage in our case) is also included in the testing regimes to examine the effect of the additional arithmetic and memory requirements in the kernels, specifically for the described simple damage model.Similarly, the usage of texture memory fetching on displacement variables is also set as a templated boolean value so performance can be tested with or without this special memory space.In addition, to understand the performance scaling of the algorithm, tests were performed on meshes of varying density, ranging from 5 elements to 45 elements per edge and totaling from 125 to 91125 elements.As mentioned, respective Abaqus solutions were used to verify each of the solutions performed by the described custom solver. The execution parameters used for Abaqus simulations are single core, double precision with all other additional settings at default. Furthermore, in contrast to our solution, Abaqus does use a smaller time-step with a fixed ratio for all simulations of 2/3. All comparative results between the two solvers are corrected for this fact.Control of floating point precision, material complexity in terms of damage, and texture memory space usage are mutually inclusive and constitute eight employed execution combinations. These have been tested on all GPUs and models. All results except damage inclusion have been compared to respective Abaqus solutions. Identical boundary conditions and smooth loading curve were used by both solvers as well as the material parameters as explained in Section 2.2. Abaqus jobs were run on a system with an Intel Xeon E5-2630 processor and 128GB of RAM, using only one core, double precision and a built-in identical material model. Results for the GTX980 and GTX780 were obtained using CUDA nvcc v6.5 compiler and Visual Studio 2013 C++ compiler, and nvcc v5.5 and Visual Studio 2008 C++ for older cards. The duration of the Abaqus solutions was taken from its output files, while timing CUDA code was performed on a high-accuracy hardware clock accessed through the dedicated CUDA built-in functions. The precomputation phase of the algorithm is not considered significant in its duration with respect to the iteration phase and is not timed.

@&#CONCLUSIONS@&#
