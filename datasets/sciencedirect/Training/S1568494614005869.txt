@&#MAIN-TITLE@&#
Bayesian analysis of time series using granular computing approach

@&#HIGHLIGHTS@&#
We review applications of the soft computing techniques in the statistical time series analysis.We propose the Bayesian granular computing approach for time series forecasting.The employed data mining and classification methods provide useful information for forecasting.We build the prior model probability distributions taking advantage of the information granules.The proposed approach provides accurate forecasts and additional, human-consistent information.

@&#KEYPHRASES@&#
Time series forecasting,Granular computing,Soft computing,Data mining,Bayesian methods,Linguistic summaries,

@&#ABSTRACT@&#
The soft computing methods, especially data mining, usually enable to describe large datasets in a human-consistent way with the use of some generic and conceptually meaningful information entities like information granules. However, such information granules may be applied not only for the descriptive purposes, but also for prediction. We review the main developments and challenges of the application of the soft computing methods in the time series analysis and forecasting, and we provide a conceptual framework for the Bayesian time series forecasting using the granular computing approach. Within the proposed approach, the information granules are successfully incorporated into the Bayesian posterior simulation process. The approach is evaluated with a set of experiments on the artificial and benchmark real-life time series datasets.

@&#INTRODUCTION@&#
The Bayesian methods for the time series analysis are proven successful in many practical applications, e.g., [1–3]. Nonetheless, the Bayesian time series analysis and probability theory are not meant to process directly the information described in natural language, and according to Zadeh [4], people granulate information and operate mainly on words and propositions easy to express in natural language. Hopefully, the soft computing methods, especially data mining, enable to retrieve and process the conceptually meaningful human-consistent information from large datasets. Such information entities have given rise to the general framework of Granular Computing [5,6].Although, time series data mining with the use of the soft computing tools has gained a lot of attention in the literature, the interdisciplinary combination of soft computing and Bayesian approach does not seem to be extensively investigated. As presented in Table 1, according to the keyword and abstract search, there are over 2.7k articles on ‘Bayesian’ AND ‘time series’, and over 2.3k results about ‘granular computing’. However, only 1 result combing these keywords. Furthermore, the Scopus search engine returns only 6 positions for the keywords ‘soft computing’ AND ‘time series’ AND ‘Bayesian’.The objective of this paper is to review the related work on the application of the soft computing methods for the time series analysis, and to propose the conceptual Bayesian Granular Computing (B-GC) framework for Time Series Forecasting. The goal is to incorporate the human-consistent soft computing methods, especially data mining and classification techniques, in the construction of the prior model probability distributions.The proposed approach assumes employing techniques from the following research fields: the fuzzy sets theory, the knowledge discovery from sequential data, especially the time series abstraction and mining for linguistic summaries, Support Vector Machines for the classification problem and the Markov Chain Monte Carlo methods for the posterior simulation and the Bayesian inference. The approach is in line with the Generalized Theory of Uncertainty [7].We provide an application example as a demonstration of concept and the numerical results of forecasting real-life data. It is observed that the granular computing approach may successfully support the probabilistic time series analysis and forecasting. In the opinion of the authors of this paper, the intelligent combination of the granular computing methods and the Bayesian modeling is a promising direction for the future research on the time series analysis and forecasting.The structure of this paper is as follows. Next section summarizes state-of-the-art of the soft computing techniques supporting the time series analysis and forecasting. Section 3 recalls basic definitions related to the time series analysis and granular computing. Section 4 presents the proposed Bayesian Granular Computing (B-GC) method for Time Series Forecasting. The description of the experiments is gathered in Section 5. This paper concludes with discussion in Section 6.Within this review, the focus lays on the approaches processing information that can be easily interpreted in natural language. First, we review the main developments and objectives of the time series data mining. Secondly, methods for the human-consistent time series forecasting are reviewed. Finally, we shortly discuss the Bayesian approach to the time series analysis.The Data Mining and Knowledge Discovery adapt the soft computing techniques to provide the human-consistent description of large datasets, that can be easily understood by users. Following Zadeh [8], the Soft Computing is based on the Computational Intelligence, and together with the Hard Computing (that is based on the Artificial Intelligence) form the Machine Intelligence. The field of Computational Intelligence was formally initiated in 1994 during the IEEE World Congress on Computational Intelligence in Orlando. Computational Intelligence (CI) is defined as a methodology involving computing that exhibits an ability to learn and/or deal with new situations such that the system is perceived to possess one or more attributes of reason, such as generalization, discovery, association, and abstraction cf. [9].Data mining usually leads to some generic and conceptually meaningful information entities like information granules. The theory of information granulation [5] is inspired by the ways in which people granulate information and reason about it. The general framework of Granular Computing [6,10–12] studies the information granulation and the information granules. It endows the information processing with a facet of human-centricity.As defined by Zadeh [5], the information granule is a clump of points (objects) grouped together by indistinguishability, similarity, proximity or functionality. The information granules are formally described with the use of fuzzy sets, intervals, rough sets, shadowed sets, probabilistic sets, etc. Pedrycz and contributors [13,14] study the optimal granular representation of time series. In [15], the authors review the granular computing developments in general. In [16], the fast interval predictors for large-scale, nonlinear time series with noisy data using fuzzy granular support vector machines are presented.The knowledge discovery process from sequential data may be divided into segmentation, clustering, classification of identified meaningful intervals or patterns, detecting anomalies, frequent patterns and discovery of association rules. For the review of the time series data mining methods, see e.g., Fu [17]. The comparative analysis of the time series representations and the similarity measures is provided in [18]. Batyrshin et al. [19] introduce an interesting perception-based approach to time series data mining.Linguistic summaries in the sense of Yager [20] are an example of information granules. Linguistic summaries describe general facts about evolution of time series with (quasi) natural language e.g., Among all increasing segments, majority are long, and are intuitive and easily interpretable for people. Kacprzyk et al. [21–23] employ the classic calculus of linguistically quantified propositions for the development of the linguistic summaries.Another example of the information granules are linguistic descriptions based on the Computational Theory of Perceptions [4] and the Systemic Functional Linguistics by Halliday. The linguistic descriptions [24–28] are characterized by more complex semantic and lexico-grammar structures than linguistic summaries, e.g., Before the knee lesion, the gait quality is high because the gait symmetry is medium and the gait homogeneity is high cf. [26]. In [29], the authors further develop the concept of the Granular Linguistic Model of a Phenomenon and provide basic architecture of a computational system to generate the linguistic descriptions.Apart from the linguistic summaries and descriptions, another example of imprecise information entities mined from the time series datasets are temporal patterns. Höppner et al. [30] propose the approach for the identification of frequent patterns based on the Allen's temporal logic [31]. In [32], the authors focus on enhancing patterns with a context information and operating on block constraints instead of Allen's relations e.g., If A happens before B and in the meantime we do not observe C, then we have a failure of class X. Schockaert et al. [34] fuzzyfy Allen's temporal interval relations and propose a complete framework to represent, calculate and reason about the temporal relationships for fuzzy intervals.Temporal patterns are used to build the association rules, that may be exemplified by If exchange rate for currency X decreases slowly, the number of international travels will increase rapidly. In [33], Agrawal et al. propose an interesting algorithm for discovery of rules in sets of items. Methods for the frequent patterns and association rules recognition have been successfully applied to describe huge datasets in different contexts, e.g., [35–39].The primer goal of the data mining in most of the cited papers is the interpretation of huge datasets by generation of meaningful information granules. The retrieval of such information helps to increase the understanding of the dataset and may reduce the storage space. In the opinion of the authors of this paper, the generic methodology on how to benefit in statistical forecasting from the results of the time series data mining shall be further investigated in the literature.Traditional autoregressive and moving average (ARMA) processes [40] are the most popular simple probabilistic models for forecasting, and though, very successful in applications. In the recent application [41], D’urso et al. apply the autoregressive estimates of models for the time series classification and clustering. One of the attractive features of the Box–Jenkins approach to forecasting is that the class of its processes is rich, and it is usually possible to find a process or a combination of processes, which provide an adequate description to the dataset. Nonetheless, the identification of the mathematical model and the specification of its required parameters may be time consuming and difficult to interpret for practitioners (experts of the field) involved in the forecasting process.To alleviate this problem and increase the overall understanding of the probabilistic time series analysis, there have been proposed some hybrid systems supporting the model selection like e.g., [42,43]. In [44], the autoregressive and movinag average models are combined with the fuzzy rule based approach using Mamdani inference mixed with some techniques of counting in fuzzy sets. Chen et al. [45] propose an approach for the automatic generation of the fuzzy rule base. In [46], the imprecise linguistic information is incorporated into the forecasting process by means of the linear regression. In [47], the authors propose a hybridization of intelligent techniques such as ANNs, fuzzy systems and evolutionary algorithms and the use of fuzzy rules to identify the ARMA model for prediction.Alternatively, to reflect the imprecision about the dataset and the model itself, there have been studied fuzzy forecasting models. Tanaka [48] suggests the fuzzy regression. Tseng et al. [49,50] propose to use the fuzzy triangular numbers instead of crisp parameters in the ARIMA models. Helin et al. [51] suggest employing the fuzzy conditional distributions in the GARCH model. Song and Chissom introduce fuzzy time series [52], that are based on the concepts of the linguistic variables and fuzzy relations. The approach assumes the split of the universum into intervals, defining the linguistic variables and relations, fuzzification of the observations, forecasting and then, defuzzyfication of the outputs. Further modifications to the Song and Chissom's approach are proposed in e.g., [53–56]. Chen et al. [57] introduce the fuzzified variation and the fuzzy logical relationship groups. The fuzzy time series are linked to the linguistic concepts, and therefore, are usually easy for interpretation. However, fuzzy time series scheme of representation of the fuzzy logical relationships may not be relevant for the complex forms of the information granules like the linguistic summaries or linguistic descriptions.Hopefully, the intelligent combination of the soft computing methods with the statistical time series analysis is gaining attention in the literature. Other examples of the applications of the soft computing methods for the time series forecasting include e.g., the Fuzzy Transform by Perfilieva [58,59], the neural networks approaches [60–62], fuzzy cognitive maps [63], rough sets [64], genetic algorithms and support vector regression [65,66] or fuzzy clustering and fuzzy rule interpolation techniques [67].Within this paper, we introduce an innovative approach to identify the a priori probability distributions for the Bayesian forecasting with the use of the information granules mined from the time series datasets.The practitioners are very often posed to the dilemma of choice between the wealth of mathematic models for forecasting for the imprecise real-world data. Clemen [68] review the aggregation methods used for the predictive purposes concluding that the Bayesian approach in the risk analysis is a very powerful tool. One of the main advantages of the Bayesian approach is the possibility to describe the data imprecision, also about the model selection, in terms of the a priori probability distributions. However, the proper selection of a priori distributions is essential for the overall performance.Geweke [1] describes the Bayesian inference for the time series and provides a detailed review of the Bayesian forecasting elements with numerous references. Prior distributions are usually assumed subjectively. Doan, Litterman and Sims [69] introduce a method to calculate the priors based on the simple statistics. The Bayesian approach for inconsistent information is discussed also by Beer and contributors in [70,71].In this section we recall basic definitions related to both, the Bayesian time series analysis and the granular computing.The purpose of the time series analysis is to describe the model (stochastic process) that produced the time series.Discrete time series[40]Let O={o1, o2..., oq} denote a finite set of objects in a considered domain. The properties of objects are measured by observables P={p1, p2..., pr}. Discrete time seriesy={yt}t=1n∈Ψnis a sequence of observations of given object's property (o, p) such that o∈O and p∈P measured at successive t∈T={1, ..., n} moments and at uniform time intervals. For each t∈T the observation ytis a realization of the random variable Ytdefined on the probability space (Ω, A, P). A sequence of Ytformulates a stochastic process.The main principles for the Bayesian analysis are to express all assumptions using probability statements, and then, to design the distributions for the future events conditional on the observed values and a certain loss function. The reflect the uncertainty about identification of the model (process) that generated the time series, we average multiple models.Bayesian averaging[1]The posterior density of the vector of interest ω conditional on multiple competitive probabilistic models for forecasting M={M1, M2, ..., MJ} is defined as follows(1)p(ω|y,M)=∑j=1Jp(Mj|y,M)p(ω|y,Mj)This posterior density p(ω|y, M) is a weighted average of the posterior densities of models {M1, M2, ..., MJ} which are defined as follows:(2)p(Mj|y,M)=p(Mj)p(y|Mj)p(y|M)=p(Mj)p(y|Mj)∑j=1Jp(Mj)p(y|Mj)We propose the approach to construct the prior model distributions p(Mi), i∈{1, ..., J} with the support of the intelligent techniques.We now recall definitions related to the selected granular computing aspects.Information granule[5]Information granule is a clump of points (objects) grouped together by indistinguishability, similarity, proximity or functionality.A simple example of an information granule is a labeled interval. Labeled intervals are used to build e.g., linguistic summaries.Linguistic summary[20]Let A={a1, a2..., au} denote a finite set of attributes (eg. dynamics, duration, variability), S={s1, s2..., sl} denotes a finite set of imprecise labels for attributes (eg. low, increasing, short). The protoform-based linguistic summary(3)QRy′sarePconsists of quantifier Q (e.g., most, among all), summarizer P (attribute together with an imprecise label), qualifier R.For further details see also [21,22].Another constructs created from the labeled intervals are temporal patterns. The relations between labeled intervals in temporal patterns may be expressed according to Allen's interval temporal logic [31]. For any 2 intervals, at least one of the available 13 relationships, e.g., before, meets, during, overlaps is defined. The set of all possible relationships is denoted as I.Temporal pattern[30]A pair P:=(s, R) such that s:{1, ..., n}∈S and R∈Inxnis called a temporal pattern if there is a labeled interval sequence (A1, s1), (A2, s2), ..., (An, sn) such that for every i∈{1, ..., n}s(i)=siand R[i, j]=ir(Ai, Aj)Frequent temporal patterns are building elements for the association rules.Association rule[33]An association rule is an implication of the form P⇒C such that P, C are sets of items representing premise and conclusion, respectively.There have been proposed in the literature different measures to evaluate the quality of the information granules depending on their type and the considered evaluation purposes. Table 2presents exemplary definitions of quality measures.The information granules like linguistic summaries, frequent patterns or association rules are usually in line with the visual pattern recognition and perception capabilities of humans, and therefore, seem appealing to support not only the interpretation of time series, but also the time series forecasting. From the range of the statistical methods, we adapt the Bayesian approach, which has been proven successful in many real-life applications. Multiple models are included in the forecasting process through the Bayesian averaging and the focus of the approach is to establish the prior probabilities for the competitive models.The input data for the proposed Bayesian Granular Computing (B-GC) method for forecasting are multivariate time seriesYnk={{yt1}t=1n,...,{ytk}t=1n}being a k-vector of discrete time series. We assume that the preprocessing, that is the normalization of the time series and handling of the missing data, is completed beforehand.The output data are both, forecastsYhk={{yt1}t=n+1n+h,...,{ytk}t=n+1n+h}and the descriptions of the imprecise information used in the forecast generation procedure.The overview of the proposed Bayesian Granular Computing (B-GC) method for forecasting is presented in Fig. 1. The procedure starts with the definition of the linguistic concepts, then mining for the information granules, identification of the probabilistic model, and finally, the Bayesian posterior simulation with the prediction generation procedure.We show that the imprecise information processed with the selected soft computing methods is supportive for the construction of the a priori model probability distributions required for the Bayesian forecasting.We start from defining the linguistic variables related to the imprecise information. Then, we define the selected template probabilistic predictive models used to build the training database.Elements of the algorithm for the definition phase are as follows:1Define linguistic variables for imprecise labelsDefine probabilistic models and build training datasetThe first step of the definition phase requires providing the specification of trend attributes A and the fuzzy sets for the interpretation of imprecise labels S. Membership functions for all the linguistic variables are established. We assume that the interpretation of linguistic variable for a given property p∈P of an object o∈O in a given domain is known and does not change in time.The second step requires defining the template probabilistic predictive models M={M1, M2, ..., MJ}. Then, the training time series dataset is built based on models from M. The definitions of the a priori distributions for all parameters related to the models are saved asMθ={M1θ,M2θ,...,MJθ}.Tms={{yt1}t=1m,...,{yts}t=1m}is a resulting s-vector of the sample time series generated from the template models M, andCTJ={c1,...,cs}is the respective s-vector denoting the template models that produced the time series fromTms.The data mining phase aims at processing of the crisp time series datasets into meaningful information objects. Elements of the algorithm are as follows:1Mine for the information granulesDiscover linguistic informationCalculate the validity matrixData mining is performed successively on the training datasetTmsand then, for the time series to be predictedYnkto find meaningful information granules. As a result, e.g., labeled intervals are mined from the time series datasets.Secondly, human-consistent and more complex information granules are constructed like linguistic summaries, descriptions, frequent patterns or association rules. The selection of the considered types of information granules depends on the application context. As a result, the k x l matrix LIYof the information entities discovered forYnktime series is created and the s x l matrix LITforTms, respectively. The linguistic information may also be formulated with the support of experts of the field thanks to the human-computer interaction.Finally, the set of the considered quality measures is definedQM={q1,...,qw}and for each instance of the information granule in LIYand LIT, their quality is measured. The applied quality measures shall be chosen adequately depending on the considered types of linguistic information. The results form the validity matrices VYand VTof size k×z and s×z, respectively, where l≤z≤card(QM)l.The prior model distributions p(Mi), i∈{1, ..., J} are established with the use of the classification methods. The classification problem is defined and solved for the vectors of the information granules.The algorithm for the probabilistic model identification phase consists of the following elements:1Classify the probabilistic modelsIdentify prior distribution of the unobservablesThe rows of the validity matrices VTand VYare feature vectors in the classification tasks. The classifier is trained on VT, and then applied for VYto identify the probabilistic models.First, we assign categories (classes) related toTmkthat correspond to the template probabilistic models M indicated byCTJ. Support Vector Machines (SVM) [72] are adapted to analyze the training data (VT,CTJ) and learn the classifier. Cross validation is performed to estimate the classification accuracy.Having trained the SVM classifier, we apply it to classify the sets of the information granules describing the time seriesYnk. The rows of the validity matrix VYare the feature vectors. The classification assumes that a score is assigned to each of J possible categories (models).(8)Score(VY(i),j)=BjVY(i)where VY(i) is the feature vector being i-th row of VY, Bjis the vector of weights corresponding to category j, and Score(VY(i), j) is the score associated with assigning instance i to the category (model) j. The resulting scores for all time series to be predicted form the k-vectorCYJ={Score(VY(1))J,...,Score(VY(k))J}. The prior model distributions p(Mj) are estimated with the scores fromCYJ. Finally, the a priori distributions for the parametersp(θMi|Mi)for Mi∈M are adapted from Mθ.The last phase of the proposed approach assumes prediction basing on the identified prior distributions for the probabilistic models with the Bayesian inference. The elements of the algorithm for the prediction phase are as follows:1Simulate posterior distributionsGenerate forecast and describe its componentsIn line with [1], the posterior density of the vector of interest(9)p(ω|yo,M)=∑j=1Jp(Mj|y,M)p(ω|yo,Mj)(10)p(ω|yo,Mj)=∫ΘMjp(ω|yo,θMj,Mj)p(θMj|yo,Mj)dv(θMj)and the posterior density of the unobservablesθMj(11)p(θMj|yo,Mj)=p(θMj|Mj)p(yo|θMj,Mj)p(yo|Mj)in whichθMj∈ΘMjis akMj×1vector of unobservables. We adapt the Markov Chain Monte Carlo (MCMC) posterior simulator that yields pseudo-random sequence of the vector of interest to estimate the posterior moments.The estimates from the formula (9) are used to generate point forecasts and their errors according to the model definitions M. Once the prediction is calculated, additional information that supported the forecasting process is provided. This information consists of the objects with the information granules and the quality measures, and therefore, shall be easily understood and helpful for the experts of the field responsible of decision making based on the forecasting results.

@&#CONCLUSIONS@&#
