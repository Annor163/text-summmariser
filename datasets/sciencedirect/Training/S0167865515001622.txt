@&#MAIN-TITLE@&#
The Parzen Window method: In terms of two vectors and one matrix

@&#HIGHLIGHTS@&#
We revisit the Parzen Window approach widely employed in pattern recognition.The Parzen Window approach can suffer from a severe computational bottleneck.This manuscript introduces a new scheme to ameliorate this computational drawback.

@&#KEYPHRASES@&#
Probability density function,Kernel functions,Parzen Window,

@&#ABSTRACT@&#
Pattern classification methods assign an object to one of several predefined classes/categories based on features extracted from observed attributes of the object (pattern). When L discriminatory features for the pattern can be accurately determined, the pattern classification problem presents no difficulty. However, precise identification of the relevant features for a classification algorithm (classifier) to be able to categorize real world patterns without errors is generally infeasible. In this case, the pattern classification problem is often cast as devising a classifier that minimizes the misclassification rate. One way of doing this is to consider both the pattern attributes and its class label as random variables, estimate the posterior class probabilities for a given pattern and then assign the pattern to the class/category for which the posterior class probability value estimated is maximum. More often than not, the form of the posterior class probabilities is unknown.The so-called Parzen Window approach is widely employed to estimate class-conditional probability (class-specific probability) densities for a given pattern. These probability densities can then be utilized to estimate the appropriate posterior class probabilities for that pattern. However, the Parzen Window scheme can become computationally impractical when the size of the training dataset is in the tens of thousands and L is also large (a few hundred or more). Over the years, various schemes have been suggested to ameliorate the computational drawback of the Parzen Window approach, but the problem still remains outstanding and unresolved.In this paper, we revisit the Parzen Window technique and introduce a novel approach that may circumvent the aforementioned computational bottleneck. The current paper presents the mathematical aspect of our idea. Practical realizations of the proposed scheme will be given elsewhere.

@&#INTRODUCTION@&#
In mathematical pattern recognition, the problem of pattern classification entails assigning an object – based on a number of specific features of the object – to one of a finite set of predefined classes/categories ωj, where j =1, 2,…, J, with J being the number of classes/categories of interest. Typically the object (or simply the pattern) is represented by an L-dimensional vector x whose elements,(x1,x2,…,xL),are values assumed to contain the appropriate information about the specific pattern features utilized to accurately classify the pattern represented by x.When L discriminatory features for a pattern can be determined accurately, the pattern classification problem presents no difficulty: it reduces to a simple look-up table scheme. However, identifying the relevant features to classify realistic patterns without classification errors is generally impossible. Thus, the pattern classification problem is often cast as the task of finding a classifier that minimizes the misclassification rate [1]. One popular way of achieving this objective is to treat both the pattern vector x and the class label ωjas random variables. In this case, the posterior class probabilities p(ωj|x) for a given pattern x is computed; then pattern x is assigned to the class, for which the p(ωj|x) value is maximum [1–6]. (In the last step it is being assumed that all misclassification errors are equally bad [1,3,4].)However, in practice, the form of the function p(ωj|x) is unknown; instead, N patterns xiand their corresponding correct class labelsyi∈{ω1,ω2,…,ωJ}– i.e.,D={(xi,yi)}i=1N,assumed to constitute a representative dataset of the joint probability density function p(ωj, x) for ωjand x – are usually available. It is from these prototype patterns xiand their corresponding class labels yithat one tries to estimate p(ωj|x).According to basic probability rules [1–7],(1)p(ωj|x)p(x)=p(ωj,x)=p(x|ωj)p(ωj)These rules allow one to modularize the estimation problem and estimate p(ωj|x) (and, of course, p(x)) in terms of p(x|ωj) and p(ωj):(2)p(ωj|x)=p(x|ωj)p(ωj)p(x),whereby we may have a better chance of being able to estimate p(x|ωj) and p(ωj) fromDthan estimating p(ωj|x) directly fromD. In the denominator,p(x)=∑j=1Jp(x|ωj)p(ωj).In the Bayesian statistics framework, p(ωj) is referred to as the class prior probability, which is the probability that a member of class ωjwill occur. The function p(x|ωj) is called the class-conditional probability density function, i.e. the probability density of observing pattern x given that x is a member of class ωj. The denominator term on the right hand side of Eq. 2 is often called the “evidence” or “marginal likelihood”. For the purpose of this paper we can afford to simply view this term as a normalization factor.If there is evidence that the number of prototype patterns per class is an indication of the importance of that class, then a sensible approximation of p(ωj) can be(3)p(ωj)=∑i=1NνjiN,whereνji=1if the ith prototype xibelongs to class ωj, otherwiseνji=0; and N is as described before. Nonetheless, p(ωj) is typically assumed to be uniform, i.e.,p(ωj)=1J,where J is as defined before.Estimating p(x|ωj) fromDis not straightforward [1–5]. In the last half-century, a plethora of methods have been proposed for estimating p(x|ωj) based onD,the so-called training set. There are ample excellent reviews and text books on this topic; for example, the two books – one by Hand [4] and the other by Murphy [8] – give adequate and accessible descriptions of the bulk of these approaches devised in recent (and not so recent) years.In this paper we are concerned with one particular approach that is widely thought to be apropos to the task of estimating p(x|ωj) from a representative training dataset: the so-called Parzen Window method [1,2,4,9,10], also known as Parzen estimator, Potential function technique [10], to name but a few.In the preceding discussion and in the rest of the paper, the terms “class”, “label”, “class label” and “category” are employed interchangeably. For notational simplicity we use x, xiand ωjboth as random variables and their realizations. Furthermore we follow (in line with the current trend in machine learning and statistics) the convenient – but not necessarily correct – practice of using the term “density” for both a discrete random variable’s probability function and for the probability function of a continuous random variable. An implicit assumption being made throughout the paper is that all spaces, matrices, vectors, functions and variables (discrete or not), etc., are real.

@&#CONCLUSIONS@&#
