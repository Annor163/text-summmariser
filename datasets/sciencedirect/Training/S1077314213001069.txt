@&#MAIN-TITLE@&#
Flexible calibration of structured-light systems projecting point patterns

@&#HIGHLIGHTS@&#
We propose an automated calibration of structured-light systems projecting points.We obtained a flexible calibration since no dedicated positioning device is needed.We successfully tested the method for very different projector types.We applied the calibration to patterns with very different shapes and point number.The calibration algorithm can be used both for industrial and medical applications.

@&#KEYPHRASES@&#
Structured-light systems,Point patterns,3D point reconstruction,Flexible projector calibration,Projector parameter optimization,Directed Hausdorff distance,3D image mosaicing,

@&#ABSTRACT@&#
Structured-light systems (SLSs) are widely used in active stereo vision to perform 3D modelling of a surface of interest. We propose a flexible method to calibrate SLSs projecting point patterns. The method is flexible in two respects. First, the calibration is independent of the number of points and their spatial distribution inside the pattern. Second, no positioning device is required since the projector geometry is determined in the camera coordinate system based on unknown positions of the calibration board. The projector optical center is estimated together with the 3D rays originating from the projector using a numerical optimization procedure. We study the 3D point reconstruction accuracy for two SLSs involving a laser based projector and a pico-projector, respectively, and for three point patterns. We finally illustrate the potential of our active vision system for a medical endoscopy application where a 3D cartography of the inspected organ (a large field of view surface also including image textures) can be reconstructed from a video acquisition using the laser based SLS.

@&#INTRODUCTION@&#
Structured-light systems (SLSs) are used to perform three-dimensional (3D) measurement in various computer vision fields such as robotic guidance [1], medical endoscopy [2–4], virtual environment construction [5], and dimensional analysis of manufactured parts [6]. A basic SLS consists of a camera and a structured-light projector rigidly fixed with respect to the camera. A known pattern of light is projected onto the scene and the camera acquires images of the pattern modulated by the depth of objects in the field of view (FOV).The shape of the projected pattern particularly depends on the application needs. On the one hand, a pattern of stripes enables dense 3D reconstruction [7–9]. However, stripe patterns are mainly used for 3D object modeling, e.g. for profilometry. On the other hand, projecting a set of points [10,11] can be a solution for very different computer vision tasks. When projecting a dense point set, surfaces can be very accurately constructed with high 3D resolution. On the contrary, when projecting a sparse point set, both 3D information and 2D image texture are available. Although the 3D point resolution is lower, it can be sufficient for applications where textures have to be preserved. For instance, in medical 3D-endoscopy, it is of great interest to reconstruct a 3D textured wide FOV of organs. When a 3D-endoscope acquires, from different viewpoints, small FOV images and some 3D points are placed at the periphery of the FOV, it is possible to compute textured surfaces [12]. These textured surfaces cannot be obtained with patterns of stripes or with dense point patterns [2].Whatever the projected pattern, the SLS must first be fully calibrated before performing any 3D measurement. The SLS calibration usually consists of two successive steps, namely the camera and projector calibration. The former step is a standard problem in computer vision [13]. In this paper, we focus on the latter step. We pay special attention to the calibration of SLS projecting different kinds of point patterns. Usually, the SLS calibration is based on the mathematical modeling of the projection of a light pattern in the space, the model being expressed in the camera coordinate system. In the literature, there are mainly two approaches. The first is based on the pinhole model: a camera projects a set of 3D points in the scene into a set of 2D points located on the image plane. On the contrary, the principle of a projector is to project a set of 2D points (located on the projector focal plane) into the scene. A projector can therefore be considered as a projective reverse camera [14,15]. In the second approach, the trajectory of each pattern element is modeled in the 3D space. The projection of a set of stripes generates a set of planes, and the calibration amounts to determining the plane equations. To do so, many authors [7,9,16,17] have exploited the cross ratio geometric property to calibrate their SLSs equipped by a stripe projector. This property enables one to compute the 3D coordinates of the projected pattern onto a calibration piece.Unlike for patterns of stripes, few works are dedicated to the calibration of SLSs equipped by a point pattern projector [18,19]. The SLSs proposed in [10,20,11] project a grid of points onto a known surface using a laser beam and optics required for generating the grid. The projection angle between each laser point trajectory and the central projection axis is fixed and known. To calibrate the system, dedicated equipment is used to place a calibration board perfectly orthogonal to the main projection axis and to displace it very precisely to known positions. From the projector calibration methods presented so far, we notice that they usually involve expensive equipments such as three orthogonal planes [17] or dedicated and precise positioning devices [10,20,11]. In addition to their expensive cost and their lack of flexibility, the performance of these methods heavily depends on the precision of the required calibration equipment.We present an automated, flexible, point number independent and accurate SLS calibration method that does not involve any positioning device nor precise calibration pieces. Our method requires a planar calibration board only, which is a paper sheet with circular control disks attached to a planar PolyMethyl MethAcrylate (PMMA) surface. A preliminary version of this active vision system calibration method was sketched in the conference paper [21]. In this paper, we show how the calibration can be extended to other systems projecting different patterns, i.e., with a variable number of points and various spatial distributions of the points depending on the application. Our method is generic: it is able to calibrate projectors using different kinds of point patterns. The calibration method does not depend on the number, color and spatial distribution of the projected points.The rest of this paper is organized as follows. In Section 2, we detail our calibration method for a laser based SLS projecting sparse point sets. We first describe the experimental setup and then we focus on the projector calibration method itself consisting of a pipeline of several algorithms. Finally, we briefly present the 3D point reconstruction principle using the calibrated laser SLS. Section 3 shows that the calibration method described for sparse point sets can also be used to calibrate SLSs working with a larger set of points and with various spatial distributions of the points composing the pattern. In this section, the patterns are obtained with a pico-projector. Again, the experimental set-up is described. Then, the procedure of Section 2 is slightly adapted. This adaptation is related to 3D point reconstruction but the calibration method remains unchanged. In Section 4, the calibration method is validated from the metrological point of view for both the laser (sparse point set) and the pico-projector SLSs. For the latter SLS, two point patterns are utilized. The number of points is increased and the spatial distribution also differs with respect to the original laser based SLS. Although a simple test object is used to compare the reconstruction accuracy of both SLSs, the calibration method can be used for various applications like dimensional analysis of manufactured parts or medical applications. Section 5 illustrates the potential of the calibrated laser SLS in a medical application: 3D endoscopy of bladder organs. We show that our SLS prototype makes it possible to reconstruct a 3D large FOV surface of the internal bladder wall including textures from a video acquisition. Moreover, this SLS can be implemented on an endoscope [22], and the surface representation facilitates bladder cancer detection [23].Our sparse point SLS is composed of a CCD camera and a laser projector being all rigidly fixed on a metallic plate (see Fig. 1). The projector to camera angle is approximately equal to 30°. The camera is a Basler Scout model equipped with a wide angle lens (6mm focal length) and acquires images with 768×576pixels. The structured-light is generated by a green laser collimated source and a holographic binary phase lens (diffractive optics). The laser beam passes through the holographic lens and diffracts itself in eight new laser rays located on a cone in the 3D space. The eight points are lying on a circle, but the algorithm described in this section remains appropriate for other pattern geometries.Because the camera and projector constitute a complete active vision system, we start by describing the camera calibration method. Then we will move onto the projection calibration which is our main contribution. Although the camera calibration is a classical task, the calibration method is worth being summarized for two reasons. This will allow us (1) to specify the complete projective system geometry and (2) to introduce the projective transformations that are involved when a 2D image of a 3D scene is acquired by the camera. All these notations and equations will be used later in the projector calibration algorithm.The pinhole camera model involves two sets of parameters. The extrinsic parameters describe the 3D rigid transformation relating thewP3D=(wx3D,wy3D,wz3D)Tpoint in a world {w} coordinate systemOw,x→w,y→w,z→wto the same pointcP3D=(cx3D,cy3D,cz3D)Texpressed in the camera {c} coordinate system(OC,x→c,y→c,z→c)taking the camera optical center as origin. This transformation involves a 3×3 rotation matrixRand a 3D translation vectort:(1)cP3D=RwP3D+tThe intrinsic parameters are related to the 2D projection onto the camera image plane. This projection is modeled by three successive steps. First,cP3Dis projected onto the image plane yielding a distortion free pointc Pu=(xu,yu)Taccording to:(2)xu=fcx3Dcz3D,yu=fcy3Dcz3Dwhere f refers to the focal length. The second step models the radial lens distortion according to a polynomial model [24, p. 191] in r, where r is the distance fromc Puto the projection of the optical center on the image plane. Following [25], we use a fourth order polynomial obtained by removing the monomials of larger degrees:(3)xd=xu(1+k1r2+k2r4)yd=yu(1+k1r2+k2r4)r=xu2+yu2wherecPd=(xd,yd)Tstands for the observed (distorted) point, and k1 and k2 are radial distortion coefficients. This model is well suited to camera optics involving strong barrel distortion [25]. The last step bringscPdin the image coordinate system(Oim,x→im,y→im)taking the upper left corner of the image as origin (see Fig. 3). This step involves the (xC,yC) coordinates of the focal point projection on the image plane and the pixel side lengths along the x and y axes, called Sxand Sy, respectively:(4)xim=xdSx+xC,yim=ydSy+yC.To calibrate the camera in a complete manner, we use the planar calibration board of Fig. 2aincluding hundreds of small circular disks and we apply Zhang’s method [27]: the camera acquires several images of the calibration board, manually placed at different positions. Due to the perspective geometry, the circular disks become elliptically shaped in the images acquired by the camera. These ellipses are segmented in each image using the method described in [26] (see Fig. 2b). The ellipse centers being now available for all viewpoints, Zhang’s method allows us to recover both the extrinsic and intrinsic camera parameters (including the distortion coefficients) from the knowledge of the real positions of the disk centers on the calibration board [27]. In the following subsections, we will only exploit the intrinsic camera parameters for the projector calibration.Because our projector calibration method also involves a planar calibration board, we demonstrate how to automatically determine the calibration plane equation from the images acquired by the camera. This task is a prerequisite to the projector calibration algorithm which will be fully presented in Section 2.4.Our planar calibration board differs from the camera calibration board of Fig. 2 as it contains four control disks only, placed in the calibration board corners. We make this choice so that the calibration disks do not occupy the whole acquired image but only a limited part of it (for reasons which will become clear in Section 2.4). Our projector calibration method requires the acquisition of at least two images of the calibration board, manually set to different positions k∈{1,…,K} with K⩾2 while the camera is fixed (unlike the methods [10,20,11], the calibration board displacement is done manually and approximately with a rough translation along the cameraz→caxis). For each position k, we define a related “world coordinate system”{wk}=(Ok,x→k,y→k,z→k)using three control disk centers (CDC) among the four in such a way thatz→kis orthogonal to the calibration plane (see Fig. 3). We denote bycC3Di,k=cx3Di,k,cy3Di,k,cz3Di,k,i∈{1,…,4}the CDC positions in the camera coordinate system. When expressed in the world coordinate system, the CDC coordinates do not depend on k because {wk} is local to the calibration board. Therefore, we use the simplified notationwC3Di. For anyi,wC3Diis known and we havewz3Di=0. To determine the calibration board position in the (fixed) camera coordinate system {c}, we first remove the radial distortion from the observed images [24, pp. 189–191]. Then, we segment the CDC in the corrected images [26] yielding estimates of thecC2Di,kpositions. Finally, their corresponding world coordinate positionswC3Diare used together with the intrinsic camera parameters to define a system whose unknowns include the extrinsic parametersRkandtklinking the camera coordinate system to the world coordinate system representing the calibration plane. Specifically, combining Eqs. 1, 2 and 4 yields the projective model in the matrix form:(5)cz3Di,kcC2Di,k1=KcC3Di,k=KRkwC3Di+tkwhere the camera matrixKgathers the intrinsic parameters:(6)K=f/Sx0xC0f/SyyC001Following [28, Section 1.5], we rewrite (5) by remarking thatwz3Di=0:(7)cz3Di,kcx2Di,kcy2Di,k1=KR1:2,ktkwx3Diwy3Di1(8)=αkh1,kh2,kh3,kh4,kh5,kh6,kh7,kh8,k1︸Hkwx3Diwy3Di1where the 3×2 matrixR1:2,kgathers the first two columns ofRkandHkis a 3×3 matrix. At this point, we notice thatHkis a projective transformation relating the 2D points(wx3Di,wy3Di) located on the calibration plane to their projection on the camera image plane. The entry h9,kis set to 1 up to the introduction of the scale factor αkin (8).In the homogeneous system (8), theHkparameters are unknown together with αkandcz3Di,k. Advantageously, (8) can be recast [28, Section 1.5] in the inhomogeneous system (9), where the only unknown parameters are theHkparameters (αkandcz3Di,kdo not appear anymore). For each i∈{1,…,4}, (8) rereads as a system of two linear equations in the eightHkunknowns:(9)h1,kwx3Di+h2,kwy3Di+h3,k-h7,kwx3Di+h8,kwy3Dicx2Di,k=cx2Di,kh4,kwx3Di+h5,kwy3Di+h6,k-h7,kwx3Di+h8,kwy3Dicy2Di,k=cy2Di,kMoreover, (9) reads as a standard linear system of equations depending on the unknown parameters inHk. Since eachcC2Di,k,wC3Dicorrespondence provides two equations and the CDC are non-collinear, four correspondences i∈{1,…,4} are enough to solve for the eight degrees of freedom ofHk[28]. KnowingKandHk, we can deduceR1:2,k,tkand the scale factor αksince the columns ofR1:2,kare of unit norm. The third column ofRkis determined by computing the cross product of its first and second columns. Finally,cC3Di,kare obtained by applying (1) towC3DiusingRkandtk.As the fourcC3Di,kpoints are now perfectly known for acquisition k, we can easily deduce the calibration plane equation in the camera coordinate system {c} by solving an over-determined system of four equations in three unknowns.The projector can be seen as a projective reverse camera without distortion because a diffractive lens does not induce any image deformation. We use the same calibration board images as in Section 2.3 (see Fig. 3) positioned with manual displacements. The acquired images not only include the four calibration disks but also some green laser points originating from the projector. In Section 2.3, these calibration planes were numerically computed in the camera coordinate system. We therefore consider that they are known.On Fig. 3, the main laser ray is diffracted from a particular point called projector optical center OP, yielding N laser rays Djwith j∈{1,…,N}. The laser rays intersect each calibration plane yielding a set of 3D points calledcP3Di,kcP3Di,kis the ith laser point projected onto the kth plane, expressed in the camera coordinate system) whose projections onto the camera image plane are denoted bycP2Di,k. Physically, a laser spot is projected from the projector optical center onto the calibration board, and then onto the camera image plane while the image is recorded.The projector calibration is as follows. Starting from the K observed images, we estimate the projector parameters, namely the OPoptical center and the Djlaser ray equations in four steps:1.Segment the laser points (i.e., blobs of connected greenish pixels) in the images and compute their centerscP2Di,k=cx2Di,k,cy2Di,k.Calculate thecP3Di,kcoordinates.For all i∈{1,…,N} and k∈{1,…,K}, match the obtainedcP3Di,kpoints to their corresponding laser rays Dj. To calculate Dj, one needs to estimate the OPposition. However, the point-to-ray matching can be done based on the approximate knowledge of OP.Determine all Djlaser ray equations and refine the OPposition estimation in the camera coordinate system.We now elaborate each step.The acquired images are first preprocessed in order to remove the distortion [24, p. 191]. The distortion removal is based on the knowledge of the calibrated camera parameters (see Eqs. (3) and (4)). For our SLS prototype, the structured light is greenish and well contrasted with the background colors (gray and black). Therefore, we can automatically segment the green laser pointscP2Di,kby thresholding the images in the Hue-Saturation-Value (HSV) color domain. Specifically, the images are recorded in the RGB format, and they are converted to the HSV domain. Because the green color yields a hue value equal to 120°, the basic idea is to keep the pixels whose hue value is close to 120. However, the other gray (background) or dark (calibration control disks) pixels in the image do not have a significant hue value since they are gray level pixels. Thus, a single thresholding in the hue domain may result in false detection of some gray level pixels together with the green pixels. To overcome this problem, we perform another thresholding in the saturation domain, as the saturation of a pure color (green) pixel is maximum while a gray level pixel has a close to zero saturation. These two image thresholding operations afford us to correctly segment the green pixels. We refer the reader to the Pratt’s book [29] for further details about the HSV format.Once the green pixels have been detected, we apply an erosion procedure to compute the center of the areas where the neighboring green pixels have been detected. Fig. 4shows an example of the segmentation results where the control disks are segmented as well. As mentioned earlier, the latter are only used to determine the calibration plane equation in the camera coordinate system for each viewpoint.The resulting set ofcP2Di,klaser point centers will be used in the next step to compute the 3D position of each laser pointcP3Di,kin the camera coordinate system.The intrinsic camera parameters are used to project thecP2Di,kpoints in the 3D space, yielding a set of Ri,kcamera rays all passing through OC(see Fig. 3). Finally, thecP3Di,kcoordinates are determined by computing the intersection of each Ri,kcamera ray with the corresponding plane k.Although the 3D laser pointscP3Di,kare now available, they are not yet matched to their respective laser rays. For each calibration plane k, we need to associate eachcP3Di,kpoint (i=1,…,N) to some laser ray Djoriginating from the projector optical center OP. Moreover and importantly, OPalso needs to be estimated in the camera coordinate system.This matching step is based on the following idea. Assume that OPis perfectly known. Firstly, we remark that Djare all passing through OP, and the K points to be matched to a given Djray are the K closest points to Dj. Secondly, for a given calibration board pose k (e.g., for k=1), we remark that Djare the N rays joining OPandcP3Di,k,i=1,…,N. From this definition of the Djrays, the OPposition is optimized in such a way that N groups of K points are all simultaneously close to the N Djrays. In the following, we use thecP3Dj,1notation to indicate that the 3D points of the first calibration plane are already associated to their corresponding Djlines, and we keep thecP3Di,knotation for the other calibration planes k>1 because we must establish a one-to-one correspondence between eachcP3Di,kand some Djray.In computer vision, the directed Hausdorff distance (DHD) is an efficient measure to register points on 3D surfaces [30–32]. We estimate a first (coarse) OPposition, denoted byO^P, by minimizing the DHD between the laser rays and thecP3Di,klaser points:(10)O^P=argminOPDHDD,cP3Di,kwhereDgathers the set of laser rays Djpassing through OPandcP3Dj,1(j=1,…,N). The DHD is defined as [30]:(11)DHDD,cP3Di,k=maxi∈{1,…,N}k∈{2,…,K}minj∈{1,…,N}dcP3Di,k,Djwhere d stands for the Euclidean distance between a point and a 3D line. In words, using the DHD ensures that the KcP3Di,kpoints matched with a given ray Djare indeed the K closest points to Dj. We use the simplex optimization algorithm [33,34] since the cost function (11) is not analytically differentiable. At each iteration, the OPposition is updated and the Djlines are updated accordingly. Fig. 5a and b illustrates the initial simplex and the simplex evolution while converging to the optimal position of the projector optical center. Since (10) is a 3D optimization problem, the simplex is a tetrahedron region [34]. By construction of the active vision system, we approximately know the relative position of the diffractive lens (i.e., that of OP) with respect of the camera optical center OC. Thus, we can define the initial tetrahedron in such a way that OPis close to a tetrahedron edge. The tetrahedron of Fig. 5a is centered on the camera optical center and the edge lengths are set to the camera to projector distance (≈100mm). The iterations of the simplex algorithm are stopped when the tetrahedron volume is below a given threshold which is chosen small enough for the coarse estimation ofO^P.When the optimization stage is completed, eachcP3Di,khas been assigned to its respective laser ray Dj: Djis set to the closest ray tocP3Di,k. In the following, with a slight abuse of notations, we renamecP3Di,kbycP3Dj,kto express that for a given calibration board k, there is a one to one correspondence between i and j.In the previous step, the Djrays were constrained to pass through thecP3Dj,1laser points of the first calibration plane. Here, we improve the Djestimation by relaxing this constraint and taking allcP3Di,kpositions into account (for k⩾1). This improvement also yields a finer estimation of OP.To decrease the errordcP3Dj,k,Dj, we impose that Djpasses through OPand the center of mass (i.e., the mean position over k)cP¯3Djof allcP3Dj,kpoints matched to Dj. The center of mass is ideally located on DjwhencP3Dj,kare exactly known and there is no numerical error. Since thecP¯3Djpoints are known and independent of OP,OPis the only remaining unknown parameter. OPis estimated as the position minimizing the sum of Euclidean distances between allcP3Dj,kpoints and their related Djray:(12)OP=argminOP∑k=1K∑j=1NdcP3Dj,k,DjOP,cP¯3Dj.Again, we use the simplex algorithm. To initialize the algorithm, we use theO^Pposition already obtained in step 3. This starting position is close to the final (expected) position of the projector optical center.We consider a single acquisition of some unknown surface (which is not necessarily planar) using our calibrated projective system. This yields an image of the surface containing the N projected laser rayscP2Dias well. We show that our calibration method enables the reconstruction of the N laser points (namedcP3Di) that are being projected onto the imaged surface. The 3D reconstruction in the camera coordinate system is done as follows:1.Remove the radial distortion from the 2D acquired image [24, pp. 189–191].Segment the laser points in the image and compute their centerscP2Di.Use the intrinsic camera parameters to projectcP2Diinto the 3D space, yielding a set of camera rays Ri, i=1,…, N.Match each camera ray Rito its corresponding laser ray Dj. Towards this end, we exploit that the projection of some laser ray Djonto the camera image yields an epipolar line [28, Section 1.10] on whichcP2Dishould lay. We remark that the epipolar geometry between two cameras is unchanged when considering a camera and a projector seen as a reverse camera (see Fig. 6). As the number of epipolar lines is limited (we set N=8 in practice), we assign eachcP2Dipoint to the closest epipolar line, leading to the matching of each camera ray Riwith some projector ray Dj.Regarding the last step, both camera and laser rays are theoretically passing through the samecP3Dipoint located on the 3D surface, thuscP3Dimay be reconstructed by intersecting both lines Riand Dj. In practice, Riand Djare likely to not exactly intersect, thus we estimatecP3Dias the midpoint of the smallest segment joining both lines, as shown in Fig. 6.Our second SLS (see Fig. 7a) is composed of a JAI camera of resolution 1920×1080pixels. The same 6mm objective as for the sparse point laser SLS is mounted on the camera. A digital light processing (DLP) pico-projector (Optoma PK 301) is used to illuminate the scene with a larger number of points. The constant angle between the camera and the pico-projector remains equal to about 30°. Two different patterns are calibrated in separate procedures: a color coded pattern of 24×15 squares and a monochrome random pattern of 58 green squares (see Fig. 7b and c). It is worth noticing that no color or other code is used during the calibration step. This information may be required for the 3D-point reconstruction after SLS calibration depending on the density of points.To calibrate the SLS, the planar board of Figs. 2 and 7a is again manually placed into the scene. For a given calibration board position, two images are systematically acquired. The first is acquired with the room light switched on and without pattern projection. This image contains only the disk matrix as shown in Fig. 7a. For the second image, the room light is switched off and a pattern is projected onto the calibration board. Now, the disks of Fig. 7a are invisible while the structured light appears as in Figs. 7b and c.The camera is calibrated as described in Section 2.2 with the disk matrix images and using Zhang’s algorithm [27]. The four steps described for the sparse point projector calibration in Section 2.4 remain unchanged when calibrating the pico-projector SLS (projected point segmentation, 3D point reconstruction, matching of 3D-points to their projector rays, and refinement of the projector parameters). The only small difference with the sparse laser point set SLS occurs during the computation of calibration board planes. Instead of using the four control disks of Fig. 4, all disk centers of the camera calibration board of Fig. 7a are now used to estimate the plane poses. The plane computation method of Section 2.3 remains unchanged.We actually simplified the laser based SLS calibration described in Section 2 involving two calibration boards. In this procedure, different poses were used for the camera and the projector. Here, the pico-projector SLS is calibrated with a unique calibration board and two acquired images per board position. This facilitates the practical positioning of the board by reducing the number of board displacements. Because they are common to both the camera and projector calibrations, these displacements are done only once. The use of single calibration board can also be applied in the calibration procedure of the sparse point set SLS of Section 2. Conversely, the calibration board with four control disks (Fig. 4) could be used to calibrate the pico-projector.The reconstruction method of Section 2.5 has been slightly modified to cope with the color coded pattern because there is a larger number of points. The only step to be adapted is the matching of the points in the acquired images to their projector rays. For the sparse point SLS, this matching is possible without ambiguities using solely the epipolar geometry. For the pico-projector SLS, the epipolar geometry alone may not be systematically sufficient for the matching step. For the color coded pattern, the M-array (also called perfect map [35]) of 24×15 points was built with the method described in [36] maximizing the Hamming distance. A given color code can only appear once in the 3×3 point neighborhoods of the M-array.We evaluate the 3D point reconstruction accuracy of the laser (Section 4.1) and pico-projector (Section 4.2) SLSs by acquiring images of a simple 3D object whose geometry is precisely known. This test allows for comparison of the results obtained for both SLSs and a discussion on the adequacy of the calibration method to different active vision systems.The test object is a “3D step” composed of two parallel half-planes π1 and π2 laying at a distance h=21.95mm (see Fig. 8a). The step shape is inspired by the SLS prototype evaluation of Chan et al. [20]. Our complete data acquisition procedure includes two steps:1.Acquisition of P=120 images of the 3D object using the active vision system (see Fig. 8b). These images are split into two groups of 60 images. For the first 60 acquisitions, the cameraz→caxis is maintained orthogonal to the object surface and the camera to object distance varies in between 150 and 300mm. For the other 60 acquisitions, the camera to object distance is set to 250mm while the viewing angle varies from −60° to 60°.Acquisition of K⩾2 images of the calibration board of Fig. 4a leading to the active system calibration. For each acquisition, the calibration board is manually maintained in front of the camera without accurately controlling its distance and orientation with respect to the SLS.For each image p∈{1,…,P}, N=8 laser pointscP3Di,p(i∈{1,…,N}) are being reconstructed according to the procedure described in Section 2.5. The image acquisitions are done in such a way that four laser points are projected on each half-plane π1 and π2 (see Fig. 8b). Therefore, each half plane equation can be reconstructed from the four related points. The orientation angle θpbetween both half planes and their relative distance hpare being evaluated subsequently (ideally, we have hp=h and θp=0° since the planes are parallel). These two criteria allow us to assess the accuracy of our method to preserve the shape and dimension of a scene. In addition, we define the 3D reconstruction error as follows. For each acquisition p and each reconstructed point i, we compute the Euclidean distance (denoted by εi,p) betweencP3Di,pand its corresponding half plane. The mean error is defined by:(13)ε¯laser=1NP∑p=1P∑i=1Nεi,pWe also define the normalized error as the ratioε%i,p=εi,p/czi,pwhereczi,pis the depth of the reconstructed point with respect to the camera device. Again, we define the mean normalized errorε¯%laserby averagingε%i,pover both i and p.We define the calibration robustness as the 3D point reconstruction accuracy with respect to the number K of calibration images. To assess the robustness, we perform three tests by setting K to 2, 10 and 20 images, respectively. For each test, P evaluations of θpand hpare performed in addition to theε¯laserandε¯%laserevaluations. Moreover, for a given value of K, we study the precision of reconstruction by keeping the camera to object distance constant while varying the viewing angle, and conversely, varying the camera to object distance while maintaining the cameraz→caxis orthogonal to the object surface.The calibration robustness results are gathered in Table 1. We observe that the reconstruction errors are small whatever the number K of calibration images:ε¯laseris equal to 0.15mm whileε¯%laseramounts to 0.07%. The mean values of hpare very close to h while the θpvalues tend towards θ=0°. Again, these values do not depend on K. These results indicate that the robustness sensitivity is very low. We emphasize that the calibration is flexible, as the calibration plane is approximately positioned perpendicular to the cameraz→caxis. In our tests, no particular attention was paid to the calibration board position when holding it in front of the camera as long as it is approximately perpendicular to the camera axis. Because two image acquisitions are enough for accurate calibration, we will always set K=2 in the following precision evaluation tests.We first evaluate the point reconstruction accuracy with respect to the camera to object distance. Here, the cameraz→caxis is orthogonal to the 3D step half-planes and the camera to object distance lays in the interval [150,300] mm. The 60 acquisition distances are regularly spaced in this interval. The mean error on hpis sub-millimetric (0.22mm) and the related standard deviation is equal to 0.29mm. The average angle θpis equal to 0.26° and the related standard deviation is equal to 1.06°. We note that both errors on hpand θpslightly increase with the camera to object distance.We now assess the accuracy with respect to the viewing angle which is defined as follows. The viewing angle is set to 0° when the cameraz→caxis is orthogonal to the object surface, and to the projector to camera angle (≈+30°) when the laser rays are orthogonal to the object surface. Fig. 9gathers the hpand θpevaluations obtained for different viewing angles ranging from −60° to 60°. It is noticeable that the hpvalues are often slightly underestimated and that the least errors on θ are obtained when the camera axis is orthogonal to the half planes. The errors are gradually increasing with the viewing angle but they remain limited (lower than 1.5mm and 1.5°, respectively). Therefore, the shape (here keeping parallel planes) of the 3D step can be accurately restored even for large viewing angles. Let us stress that the maximum viewing angle configuration (60°) is extreme and unrealistic in many applications, as in medical endoscopy where the endoscope is often orthogonal to the surface being imaged; see Section 5. Similarly, large angles are usually avoided in dimensional analysis of manufactured parts for which the acquisition conditions can be controlled. In these applications, the 3D point reconstruction is very accurate. While being more flexible, the related reconstruction errors are of same magnitude as those reported in [10,20].In addition to the above results, we tested the sensibility of the simplex algorithm with respect to the initial solution for steps 3 and 4 of the calibration process. Regarding step 3, we experimentally verified that the same OPsolution is reached when initializing the simplex with a regular tetrahedron centered on the optical center of the camera while varying the tetrahedron edge lengths. When the edge length is large enough to approximately cover the camera to projector distance, the simplex algorithm systematically converges to the same solution. The optimization process of step 4 is always completed within a few iterations since theO^Pposition yielded by step 3, which is used as starting position, is very close to the expected position of the projector optical center.We repeat the same experiment as in Section 4.1.1 for (i) the monochrome random pattern and (ii) the color coded pattern. Let us briefly recall this procedure. For each pattern, the reconstruction accuracy of the calibrated pico-projector SLS is tested using the step edge of Fig. 8a. Again, the projected points are equally spread over the half planes π1 and π2. Three images are acquired for camera/object distances of about 20, 30 and 40cm. The step edge orientations are arbitrary (see Fig. 10).It is noticeable that our algorithm only takes the (radial) distortions due to the camera into account, but not the projector distortions. The point reconstruction accuracy confirms that the latter distortions can potentially be neglected (according to the application requirements). However, correcting the distortions due to the pico-projector may help to further improve the reconstruction accuracy when high precision is required.The edge height h=21.95mm and the angle θ=0° are again ground truth values used for the reconstruction result assessment. Moreover, for each pattern, the Euclidean distances εi,pbetween the ith reconstructed 3D point of acquisition p and its corresponding half-plane are used to compute the mean reconstruction error(14)ε¯pico=1NP∑p=1P∑i=1Nεi,pwhere p∈{1,2,3} (P=3) and the point index i belongs to {1,…,N}. The number of points is equal to N=58 and 360=24×15, respectively for the monochrome random pattern and the color coded pattern.

@&#CONCLUSIONS@&#
