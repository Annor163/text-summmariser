@&#MAIN-TITLE@&#
Detection and accommodation of outliers in Wireless Sensor Networks within a multi-agent framework

@&#HIGHLIGHTS@&#
Investigated three techniques for outliers detection in Wireless Sensor Networks.A Least Squares-Support Vector Machine-based technique with a sliding window-based learning.Principal Component Analysis method along with subspace tracking with rank-1 modification.Univariate statistics-based scheme within an oversampling environment.All methods are implemented within a hierarchical multi-agent framework.

@&#KEYPHRASES@&#
Outliers detection,Least Squares-Support Vector Machine,PCA,Univariate statistics,Multi-agent systems,

@&#ABSTRACT@&#
This paper studies three techniques for outliers detection in the context of Wireless Sensor Networks, including a machine learning technique, a Principal Component Analysis-based methodology and an univariate statistics-based approach. The first methodology is based on a Least Squares-Support Vector Machine technique, together with a sliding window learning. A modification to this approach is also considered in order to improve its performance in non-stationary time-series. The second methodology relies on Principal Component Analysis, along with the robust orthonormal projection approximation subspace tracking with rank-1 modification, while the last approach is based on univariate statistics within an oversampling mechanism. All methods are implemented under a hierarchical multi-agent framework and compared through experiments carried out on a test-bed.

@&#INTRODUCTION@&#
A Wireless Sensor Network (WSN) is a network comprising tiny, low-cost and low energy sensor nodes, connected to one or more sink devices. Each node is, usually, provided with a wireless radio transceiver, a small micro-controller, a power source and multi-type sensors, such as temperature, humidity or pressure. It can also include analogue-to-digital converter (ADC) and/or digital-to-analogue converter (DAC) ports, as well as a variety of network services, namely localisation, coverage, synchronization, data compression and aggregation, or even security mechanisms [25,27].This kind of infrastructure is becoming increasingly popular in a number of fields and applications, such as in environmental contexts, habitat or health monitoring, or in military surveillance activities, just to name out a few (see e.g. [22,18]). Because of their inherent constraints, in particular power autonomy, memory, computational power and communication bandwidth, raw data collected from WSNs are quite often unreliable and inaccurate [2,11]. These inaccuracies, generically referred to as outliers in the context of this work, can be regarded as measurements that significantly deviate from the normal pattern of sampled data [28]. For this reason it is recommended that raw data collected through wireless sensor nodes should be purged from outliers.Outliers detection techniques designed to be implemented on WSNs nodes should have a high detection rate and a low false alarm rate, while presenting a parsimonious consumption of resources. A number of detection methods have been proposed in the last few decades. They can be classified according to the underlying techniques, the network structure or even the type of outliers they can detect (see e.g. [26,25,21]). In what the state-of-the-art recursive methods are concerned, they have not been, to the best of the authors’ knowledge, assessed regarding their implementability on sensor nodes and the underlying performance in the context of monitoring systems over WSN, where collected data is quite often non-stationary.In order to shed some light on this issue, the present work evaluates three different approaches for online detection and accommodation of outliers in raw data over WSNs under a hierarchical multi-agent system based framework. The first approach is a Machine Learning technique relying on a Kernel-based methodology, namely the Least Squares (LS)-Support Vector Machine (SVM), along with an online sliding window scheme [17]. This choice is to some extent motivated by the fact that they do not demand the definition of a probability density function (p0) for a given hypothesis, they provide computationally efficient decision functions, and they can be applied in high dimensional data sets [7]. To improve the LS-SVM's performance in non-stationary conditions, the present work considers a modification to the standard method, characterised by redefining the Gaussian kernel. The second methodology relies on Principal Component Analysis (PCA), which usually has a high computational complexity due to the expensive eigendecomposition (ED). To reduce the underlying complexity, this work follows an approach based on a recursive subspace tracking scheme, namely the orthonormal projection approximation subspace tracking (PAST) (OPAST) algorithm [4], as the major subspace is only recursively tracked by using a rank-1 modification. This method is referred to as robust OPAST with rank-1 modification (OPASTr). The last methodology is based on Univariate Statistical Analysis, commonly used in Shewhart control charts. In order to improve its consistency, this technique is implemented under an oversampling framework.The remainder of this paper is organized as follows. Section 2 presents an introduction to the LS-SVM approach, describes the training algorithm used for online implementation, and presents the proposed modification so as to improve the detection performance in transient time-series. Section 3 provides a brief description with regard to the second technique based on PCA, while Section 4 presents the methodology based on Univariate Statistical Analysis. Section 5 gives a brief overview on the multi-agent framework deployed on the sensor nodes, while Section 6 presents some results and Section 7 concludes this work.This section provides a brief introduction to the machine learning technique and describes the proposed modification to improve its performance in transient time-series. The reader is referred to [7] and references therein for a comprehensive description of the standard approach.The Support Vector Novelty Detection (SVND) method deals with the problem of given a set of vectorsX=x1,…,xm∈Xm, such that the sequence xi, i=1, …, m∼p0 (with p0 unknown) and two hypotheses H0 and H1, categorising a new readingx∈X, with identical probability density function p0, under the underlying two hypotheses. This problem is addressed by defining a decision functionf(x)∈S⊂Xand a real number b, such that f(x)−b≥0 ifx∈S(x is “normal”), and f(x)−b<0 if x is an outlier. The decision function is designed taking into account the following two constraints:•Most of the training vectors are assumed to be normal (X∈S), except for a small subset of outliers;The bound that surrounds the uncorrupted data should be as small as possible, that isS⊂Xshould have minimum volume.(1)k(x1,x2)=exp−12σ2∥x1−x2∥2where ∥·∥ represents the canonical norm.It should be mentioned that a positive definite kernel k(·, ·) induces a RKHS, that is a linear space of functionsFrepresented by a dot product and denoted as〈·,·〉F, with the corresponding norm denoted as∥·∥F. In addition,Fis complete in this norm, and for anyf(·)∈Fthe reproducing property holds, namely〈k(x,·),f(·)〉F=f(·).For a positive definite kernel and the corresponding RKHSF, the SVND method provides the function f(x) as the solution to the following convex optimisation problem, with 0<υ<1 [7]:(2)maxf(·)∈F,ei,b−12∥f(.)∥2−1υm∑i=1mei2+bsubject tof(xi)−b=−ei,ei≥0In (2) the slack variables ei, along with the constraints, guarantee that the underlying decision function fx(·) fits the training data, which implies that almost all the training data are located inside the regionS. The samples xilying outside this region are assumed to be outliers. Further, the number of outliers is kept low by minimizing the term∑i=1mei2, while the term ∥f(.)∥2 ensures that the second constraint holds, which results in a minimum volume forS.The dual minimisation problem associated with (2) is obtained by appealing to a set of Lagrange multipliers α={α1, …, αm}, with the Lagrangian given as:(3)L=12∥f(.)∥2+1υm∑i=1mei2−b−∑i=1mαi[f(xi)−b+ei]By computing the Lagrangian's partial derivatives with respect to f(x), b, eiand αi, and setting them equal to zero, it follows that,(4)∂L∂f(.)=0⇒f(.)=∑i=1mαik(xi,.)(5)∂L∂b=0⇒∑i=1mαi=1(6)∂L∂ei=0⇒ei=υm2αi(7)∂L∂αi=0⇒f(xi)−b+ei=0The above four equations can be rewritten as:(8)∑j=1mαjk(xj,xi)−b+υm2αi=0∑j=1mαj=1In a compact form (8) can be described by the following matrix equation:(9)0I−ITHbα=01where I and α are vectors with length m, while H is a square matrix of size m×m, as follows:(10)I=[1⋯1](11)α=[α1⋯αm]T(12)H=k(x1,x1)+υm2⋯k(x1,xm)⋮⋱⋮k(xm,x1)⋯k(xm,xm)+υm2The optimal decision function fx(x) is given as the solution of (9), namely(13)fx(x)=∑i=1mαik(x,xi)−bwith fx(x)≥0 when x is a “normal” reading and fx(x)<0 if x is an outlier.Since readings collected from a given system are in most cases not clean, i.e. noisy raw data, the above discriminant is rather inefficient in what the sensitivity and specificity of the underlying decision (H0 or H1) is concerned. In order to get around this issue, an outlier index Itwas proposed in [7]. At a given time t, the detection algorithm is trained using the m most recent samples, yielding the vector αtand bt, and being the outlier index Itcomputed according to:(14)It=−log∑i=1mαi,tk(xt−(m+1)+i,xt)+log[bt]where btcan be regarded as a scaling factor for αt, while the subscript (t−(m+1)+i) corresponds to the online sliding window used in the training algorithm. By making use of It, a measurement is consider an outlier if It>0. In practice, however, in order make Itless sensitive to noise in raw data it is instead compared to a threshold η>0, with η≈−log(η′), η′<1, η′≈1, typically chosen as 0.99. Interestingly, it can be shown for m→∞ andx∈R:x∼N(μ,ς2)and k(·, ·) the Gaussian kernel (1)[8] that,(15)It≥η⇔∥xt−μ∥ς2≥ψσς,η,υwith ψ(·) a given threshold. In such conditions (see e.g. [7]), the proposed modified test is equivalent to comparing the distance to the distribution mean to the distribution spread.For online detection of outliers, the training set is updated at each sampling time with a new sample collected from the system, while the oldest one in X is discarded. At time t the training time-series consists of m samples, namely:(16)X=xt−mxt−m+1⋯xt−1TBy solving Eq. (9), the following equations hold:(17)bt=1I·Ht−1·IT(18)at=Ht−1·IT·btIn order to compute btand atthe inverse of matrix Hthas to be found.(19)Ht=ftFtTFtWtwith:(20)ft=k(xt−m,xt−m)+υm2(21)Ft=[k(xt−m+1,xt−m)⋯k(xt−1,xt−m)]T(22)Wt=k(xt−m+1,xt−m+1)+υm2⋯k(xt−m+1,xt−1)⋮⋱⋮k(xt−1,xt−m+1)⋯k(xt−1,xt−1)+υm2At time t+1, Ht+1 is given by:(23)Ht+1=WtVt+1Vt+1Tvt+1with(24)vt+1=k(xt,xt)+υm2(25)Vt+1=[k(xt−m+1,xt)⋯k(xt−1,xt)]TTo cope with the complexity of inverting high dimensional block matrices, the matricesHt−1andHt+1−1are computed by appealing to the Sherman–Woodbury theorem (see e.g. [1]).Let Z be a symmetrical n×n matrix described by:(26)Z=AuuTa=auTuAwhere A is a square matrix and a is a scalar. Then the inverse matrix of Z can be computed as:(27)Z−1=BqqTτwith:(28)B=A−1+τA−1uuTA−1(29)q=−τA−1u(30)τ=1a−uTA−1uTaking into account (27), matricesHt−1andHt+1−1can be calculated as follows:(31)Ht−1=τhthtTGtwith(32)τ=1ft−FtTWt−1Ft(33)ht=−τFtTWt−1(34)Gt=Wt−1+τWt−1FtFtTWt−1and(35)Ht+1−1=Gt+1ht+1ht+1Tτwhere(36)τ=1vt+1−Vt+1TWt−1Vt+1(37)ht+1=−τWt−1Vt+1(38)Gt+1=Wt−1+τWt−1Vt+1Vt+1TWt−1It follows from (31) and (35) thatWt−1is common to both equations. Finally, taking into account (31),(39)Gt=Wt−1+1τhtTht⇔Wt−1=Gt−1τhtThtTaking into account (39), the block matrixWt−1can be calculated fromHt−1, and by replacing in (35)Ht+1−1can be recursively updated. The outlier detection technique is outlined in Algorithm 1.Algorithm 1Outlier detection.Input:υ, mInitialiseX←x1⋯xmCompute H as in (12)Calculate H−1repeatxt← read_sampleCompute btand atas in (17) and (18)Obtain Itfrom (14)ifIt>ηthenxtis an outlierend ifObtainvt+1and Vt+1 from (24) and (25)ComputeWt−1as in (39)CalculateHt+1−1using (35)Update X by adding xtand removing the oldest sampleuntil End_DetectionOne drawback of the standard approach based on the RBF kernel (1) is associated with the fact that when the system from which the readings are taken is not in a steady state, the outliers detection performance is seriously impacted. This is due to transient response influence on the Euclidean norm ∥xj−xj+1∥, which results in an increase of the false positive samples. This fact points out to a modification on the original kernel.In the proposed formulation, the argument is replaced by the difference to the m-samples trend line. The rationale is propped up on the fact that, by taking the deviation to LS approximation, it makes the underlying discriminant less sensitive to the system's dynamics. The modified kernel takes the following form:(40)k(x˜1,x˜2)=exp−12σ2∥x˜1−x˜2∥2withx˜t=∥xt−xˆt∥the deviation from the actual sample xtto the Least Squares estimatexˆt.The new kernel leads to a change in the equations used for computing the matrix H, namely (12) and (19)–(25). For new kernel, they are written as:(41)H=k(x˜1,x˜1)+υm2⋯k(x˜1,x˜m)⋮⋱⋮k(x˜m,x˜1)⋯k(x˜m,x˜m)+υm2At time t, Htis given by:(42)Ht=ftFtTFtWtwith(43)ft=k(x˜t−m,x˜t−m)+υm2(44)Ft=[k(x˜t−m+1,x˜t−m)⋯k(x˜t−1,x˜t−m)]T(45)Wt=k(x˜t−m+1,x˜t−m+1)+υm2⋯k(x˜t−m+1,x˜t−1)⋮⋱⋮k(x˜t−1,x˜t−m+1)⋯k(x˜t−1,x˜t−1)+υm2while at time t+1, Ht+1 is computed as follows:(46)Ht+1=WtVt+1Vt+1Tvt+1with(47)vt+1=k(x˜t,x˜t)+υm2(48)Vt+1=[k(x˜t−m+1,x˜t)⋯k(x˜t−1,x˜t)]TUnder this new modified kernel-based approach, tagged outliers are replaced by the corresponding Least Squares prediction. Namely, when It>η (outlier detected)xt=xˆt. The overall approach is sketched in Algorithm 2.Algorithm 2Proposed outlier detection and accommodation.Input:υ, mInitialiseX←[x1⋯xm]ObtainXˆby fitting a curve to XX˜←∥X−Xˆ∥Compute H as in (41)Calculate H−1repeatxt← read_sampleObtain predictorxˆby fitting a curve to Xx˜t←∥xt−xˆt∥Compute btand atas in (17) and (18)Obtain Itfrom (14)ifIt>ηthenxtis an outlierxt←xˆt% sample accommodatedend ifObtainvt+1and Vt+1 from (47) and (48)ComputeWt−1as in (39)CalculateHt+1−1using (35)Update X by adding xtand removing the oldest sampleUpdateX˜by addingx˜tand removing the oldest sampleuntil End_DetectionThis section provides a brief introduction to the PCA-based approach. The reader is referred to [4] and references therein for a comprehensive description.In the PCA-based technique, the measurement vector x is usually centred. This implies estimating the mean at each sampling time. A recursive mean estimator of x(t) can take following form:(49)μˆ(t)=μˆ(t−1)+1t(x(t)−μˆ(t−1))withμˆ(t)the recursive mean and x(t) the raw measurement vector at time t. In the case of online implementation, a forgetting factor β, with β<1 and β≈1, is included in order to give less weight to older samples, so as to cope with changes in the system's dynamics. The resulting estimator is given by:(50)μˆ(t)=βμˆ(t−1)+(1−β)x(t)Given the estimator's sensitivity to outliers in the data, a better choice is to consider the median with regard to a L-samples window:(51)μˆ(t)=βμˆ(t−1)+(1−β)med(A(x(t)))where A(x(t))={x(t−L+1), ⋯, x(t)}, with L the length of the window, and med(·) the median operator. The length L of the estimation window is commonly kept small to reduce the underlying computational burden. For larger windows, the pseudomedian should be considered instead (see e.g. [20]). Finally, the measurement vector is centred by means of (52).(52)x¯(t)=x(t)−μˆ(t)Principal Component Analysis-based algorithms usually require the computation of the entire ED, which turns out to be computationally demanding, and it is not recommended for online implementation, particularly on sensor nodes. As such, this work takes advantage of subspace tracking, which recursively tracks the signal subspace spanned by the major Principal Component (PC)s UB(t), instead of computing the whole ED. The PAST algorithm (see e.g. [6]) estimates the signal subspace recursively by minimizing the following cost function:(53)J(W(t))=∑i=1tβt−1∥x¯(i)−W(t)y(i)∥22wherey(i)=WT(t)x¯(i)and J(W(t)) represents the energy inx¯(i)that is outside the subspace W(t). The subspace W(t) is equal to the major PCs UB(t) up to an orthogonal transformation or rotation and thereforeW(t)WT(t)=UB(t)UBT(t).In the PAST algorithm the approximationy¯(i)≈WT(i−1)x¯(i)is chosen so that the objective function (53) is relaxed to a quadratic function in W(t), for which a standard recursive LS algorithm can be used to obtain W(t), with low complexity. In order to guarantee the orthonormality of W(t), an additional orthonormalisation is added to the PAST algorithm, being this formulation denoted as OPAST.To apply the OPAST algorithm in the context of outliers detection, an initial ED has to be available, either taken from an initial data block or by offline estimation. The eigenvalues from the initial ED are then used to estimate the dimension B of the signal subspace, by appealing to a given stopping rule [15], such as the Kaiser's rule [16]. The OPAST algorithm, is outlined in Algorithm 3.Algorithm 3OPAST algorithm.Input: Initial data block X(0)Initialisation:Initialise β         % forgetting factorInitialiseμˆ(0)←mean(X(0))Estimate subspace dimension B with Kaiser's ruleN0←length(X(0))Cxx(0)←1N0XT(0)X(0)Obtain UB(0) and ΛB(0) from the ED of CxxW(0)←UB(0)Cyy(0)←WT(0)Cxx(0)W(0)Ω(0)←Cyy−1(0)Recursion:repeaty(t)←WT(t−1)x¯(t)g(t)←1βΩ(t−1)y(t)γ(t)←11+yH(t)g(t)p(t)←γ(t)(x¯(t)−W(t−1)y(t))Orthonormalisation Step:τ(t)←1∥g(t)∥2211+∥p(t)∥22∥g(t)∥22−1p′←τ(t)W(t−1)g(t)+(1+τ(t)∥g(t)∥22)p(t)Update:W(t)←W(t−1)+p′(t)gT(t)Ω(t)←1βΩ(t−1)−γ(t)g(t)gT(t)until End_Time_SeriesRecall that in the PAST algorithm the estimated subspace W(t) is equal to the major PCs UB(t), up to an orthogonal transformation or rotation Q(t), that is(54)UB(t)=W(t)Q(t)with Q(t) an orthogonal matrix of size B×B satisfying Q(t)QT(t)=I.To recursively estimate the eigenstructure associated with the underlying subspace, this work relies on the rank-1 modification.Let the centred measurementx¯(t)projection onto W(t) be given as:(55)y(t)=WT(t)x¯(t)with the corresponding correlation matrix given by:(56)Cyy(t)=E[y(t)yT(t)]If W(t) is slow time-varying, then the correlation matrix can take the following form:(57)Cyy(t)=WT(t)Cxx(t)W(t)Then, by projecting Cxx(t) onto the signal subspace W(t), and taking into account (54), the correlation matrix can be written as:(58)Cyy(t)=Q(t)ΛB(t)QT(t)Through this projection only the major eigenvectors, which span the signal subspace are retained. Therefore, the eigenvectors of the transformation Q(t) can be obtained using the ED of Cyy(t). Nevertheless, this method is still rather complex for being implemented on sensor nodes. As such, the ED of the rank-1 update is recursively computed.In the OPASTr algorithm, the correlation matrix Cyyin (58) can be recursively updated according to(59)Cyy(t)=βCyy(t−1)+(1−β)y(t)yT(t)with β a forgetting factor and Cyy(t−1)=Q(t−1)ΛB(t−1)QT(t−1), as in (58). Eq. (59) can be rewritten as a rank-1 modification as follows,(60)Cyy(t)=Q(t−1)[βΛB(t−1)+(1−β)z(t)zT(t)]QT(t−1)with z(t)=QT(t−1)y(t).Consider the corresponding ED described by:(61)βΛB(t−1)+(1−β)z(t)zT(t)=Q˜(t)ΛB(t)Q˜T(t)The term inside the square brackets of (60) is the rank-1 modification, and the eigenvectors of the correlation matrix Cyycan retrieved by means of the following relationship:(62)Q(t)=Q(t−1)Q˜T(t)Finally, taking into account (62), the major PCs UBcan be obtained through (54).The Squared Prediction Error (SPE) and the T2 score are two measures commonly used in comparing the distance between x(t) and the remaining time-series. For a centred measurement vectorx¯(t), the corresponding SPE is given as,(63)SPE(t)=∥x¯(t)−UB(t)UBT(t)x¯(t)∥22whereUB(t)UBT(t)is replaced with W(t)WT(t). Regarding the T2 score, its computation can be carried out through the following expression:(64)T2(t)=∥x¯T(t)UB(t)ΛB−1(t)UBT(t)x¯(t)∥22These measures are then compared to predefined thresholds, namely ΓSPEandΓT2, in order to detect possible outlying samples. This method is dependent on the selection of an appropriate threshold, which should be recursively updated to allow its adaptation to changes in the system dynamics. Since outliers can impact the threshold update, a robust version of SPE and T2 score will be considered. The detection of outliers is then based on the following discriminants:(65)|SPE(t)−μˆSPE(t)|≥ξσˆSPE(t)(66)|T2(t)−μˆT2(t)|≥ξσˆT2(t)whereμˆSPEandμˆT2are, respectively, robust location estimators of SPE(t) and T2(t),σˆSPEandσˆT2are robust scale estimators of SPE(t) and T2(t) and ξ is the threshold quartile parameter [4]. These robust estimates are computed according to:(67)μˆSPE(t)=βμˆSPEμˆSPE(t−1)+(1−βμˆSPE)med(A(SPE(t)))(68)σˆSPE2(t)=βσˆSPE2σˆSPE2(t−1)+c(1−βσˆSPE2)med(A(ΔSPE2(t)))(69)μˆT2(t)=βμˆT2μˆT2(t−1)+(1−βμˆT2)med(A(T2(t)))(70)σˆT22(t)=βσˆT22σˆT22(t−1)+c(1−βσˆT22)med(A(ΔT22(t)))withβμˆSPE,βσˆSPE2,βμˆT2andβσˆT22forgetting factors close to but smaller than one, c=2.13 is a correction factor for a Gaussian input, med(·) is the median operator, A(·) as defined in Section 3.1 with argument, respectively, SPE(t),ΔSPE2(t), T2(t) andΔT22(t). Finally,ΔSPE2(t)andΔT22(t)are innovations with regard to the underlying estimates, which are given as:(71)ΔSPE(t)=SPE(t)−μˆSPE(t)(72)ΔT2(t)=T2(t)−μˆT2(t)The previous approach is still sensitive to impulsive outliers. For this reason the robust OPASTr (R-OPASTr) makes use of a robust weight function ψ(t), which affects the updating procedure of the signal subspace, namely when an outlying sample is detected, the updating step is skipped. In this work ψ(t) is chosen as(73)ψ(t)=qSPE(t)·qT2(t)with(74)qSPE(t)=1if|ΔSPE(t)|<ΓSPE0otherwise(75)qT2(t)=1if|ΔT2(t)|<ΓT20otherwisewhere ΔSPE(t) andΔT2(t)are, respectively, given by (71) and (72). ΓSPEandΓT2are thresholds recursively updated according to:(76)ΓSPE=ξσˆSPE(t)(77)ΓT2=ξσˆT2(t)Taking into account ψ(t), the cost function (53) can be rewritten as follows:(78)J(W(t))=∑i=1tβt−1ψ(i)∥x¯(i)−W(t)y¯(i)∥22wherey¯(i)≈WT(i−1)x¯(t)is the projection approximation defined in Section 3.2. The R-OPASTr approach is sketched in Algorithm 4.Algorithm 4R-OPASTr algorithm.Input: Initial data block X(0)Initialisation:Initialise μ(0), UB(0), ΛB(0), W(0), Cyy(0), Ω(0) and β as in Algorithm 3InitialiseβμSPE,βσSPE2,βμT2andβσT22Initialise Q(0) as the eigenvectors of the ED of Cyy(0)Recursion:repeatμˆ(t)←Eq. (51)x¯(t)←Eq. (52)Update W(t) and Ω(t) using Algorithm 3         % OPASTRobust SPE:SPE(t)← Eq. (63)μˆSPE(t)←Eq. (67)ΔSPE(t)← Eq. (71)σˆSPE2(t)←Eq. (68)RobustT2score:z(t)←QT(t−1)y(t)ComputeQ˜(t)and ΛB(t) from the ED of (61)Q(t)← Eq. (62)UB(t)← Eq. (54)T2(t)← Eq. (64)μˆT2(t)←Eq. (69)ΔT2(t)←Eq. (72)σˆT22(t)←Eq. (70)Robust subspace update:ΓSPE← Eq. (76)qSPE(t)← Eq. (74)ΓT2←Eq. (77)qT2(t)←Eq. (75)ψ(t)← Eq. (73)ifψ(t)=0 thenW(t)←W(t−1)Ω(t)←Ω(t−1)ΛB(t)←ΛB(t−1)UB(t)←UB(t−1)end ifuntil End_Time_SeriesThis section introduces succinctly the univariate statistics-based method. The reader is referred to [10] and references therein for a comprehensive description regarding this technique.The univariate statistical-based approach to limit sensing assumes that a single variable is observed through sensor readings, and used to determine boundary thresholds for in-control operation [5]. The violation of these limits by observed data is indicative of possible outliers. This methodology is typically employed in Shewhart control charts, and commonly referred to as limit sensing or limit value checking. The upper (Δu) and lower (Δl) bounds on the Shewhart chart are critical to minimize the rates of false outliers and missed detection.Statistical hypothesis theory can be used to predict false outlier and missed detection rates on a given ensemble, given a set of thresholds. Consider for a monitoring variable z that any deviations from its mean,z¯, are due to additive errors, and the variability of z follows a Gaussian distributionN(z¯,σ2), with standard deviation σ. Then the probability P that z is within a given interval is given by:(79)P{z<(z¯−cα/2σ)}=P{z>(z¯+cα/2σ)}=α2(80)P{(z¯−cα/2σ)≤z≤(z¯+cα/2σ)}=1−αwhere cα/2 is the standard normal deviation corresponding to the (1−α/2) percentile, and α the level of significance, which specifies the degree of trade-off between false outliers and missed detection rate. Typical values for the standard normal deviation include cα/2=1.0;1.5;3.0.A drawback of monitoring schemes based on the Shewhart control chart is that they are quite sensitive to the thresholds, which ultimately compromises the detection performance. Several methods based on the incorporation of samples from multiple consecutive instantiations can improve the false outlier and missed detection rates, namely the cumulative sum control chart [14] and the exponentially weighted moving average control chart [19]. However, these methods are better suited for small persistent mean shifts.This work assumes that the deterministic-stochastic process is stationary and the ergodicity is held within each sampling interval [kTs, (k+1)Ts], with Tsthe sampling period and k the discrete time. Additionally, instead of relying on samples taken at each sampling time k·Ts, the methodology makes use of an oversampling mechanism (Tos≪Ts) within the sampling interval, and including (k+1)Tsin these readings. By means of this scheme, a statistically consistent dataset is collected, which is subsequently used within the univariate statistical approach based on the Shewhart control chart. If the sample taken at time (k+1)Tsis outside the threshold limits, then the sample is replaced by the mean of the oversampling data set.The standard approach discussed in the previous section assumes stationary conditions. When steady-state conditions do not hold, the observations can be assumed as taken from a non-stationary random process with deterministic transient behaviour, corrupted with an ergodic random variable (additive noise).If the sampling period Tsis adequately chosen, taking into account the bandwidth of the system, the deterministic-stochastic time series for τ∈[kTs, (k+1)Ts] can be approximated by means of a linear regression, as follows:(81)z(τ)=a+b·τThe computation of the deterministic parameters in (81) is carried out by minimizing the χ2 merit function, given as:(82)χ2(a,b)=∑i=1Nsz(τi)−a−b·τiσi2with Nsthe number of samples and σithe standard deviation.Since the uncertainty associated with each measurement included in the dataset is not a priori known, some considerations concerning the χ2 fitting have to be taken, in order to derive a plausible value for σi. If it is assumed that all samples have equal standard deviation (σi=σ) and the model fits sufficiently well, then it is possible to assign an arbitrary value to the standard deviation σ, namely σ=1. In this case, the χ2 merit function can be rewritten as the following residual sum of squares:(83)χ2(a,b)=∑i=1Ns(z(τi)−a−b.τi)2Considering the operatorS(·)≜∑i=1Ns(·)and(84)S(t)=∑j=1NstjS(z)=∑j=1NszjS(t2)=∑j=1Ns(tj)2S(t·z)=∑j=1Nstj·zjthe computation of θ=[a b]T can be carried out according to(85)θ=S(t2)−S(t)−S(t)1S(t2)+S2(t)S(z)S(t·z)If a sensor reading taken at t=(k+1)Tsis outside the threshold limits, then it is tagged as an outlier, and accordingly replaced with the trend of the oversampling data, which is computed taking into account (81), for τ=(k+1)Ts. This methodology is outlined in Algorithm 5.Algorithm 5Outlier detection and accommodation in transient time series.Input:fs, fos, snd// fs– Sampling Frequency// fos– Oversampling Frequency// snd – Standard Normal DeviateOutput Sample/Accommodated SampleTs←1/fs// sampling periodTos←1/fos// oversampling periodti←GetTime// initial timei←0// initial iterationrepeat// oversampling until next samplei←i+1// iterationz[i]←GetSample// oversampleSleep(Tos)// node sleep for TossecondsuntilGetTime≥ti+Tsy←GetSample// sample(a, b)←DataFitting(z)// a and b from Eq. (85)z[i+1]←a+b×(i+1)// next value from linear regressionΔu←z[i+1]+snd×STD(z)// upper thresholdΔl←z[i+1]−snd×STD(z)// lower thresholdify∈[Δl, Δu] then// the sample is not an outlierreturny// return sampleelse// the sample is an outlierreturnz[i+1]// return the accommodated sampleend ifThe detection architecture is composed of four main components (see Fig. 1), including a multi input and multi output (MIMO) plant under monitoring, a computer running the required middleware, along with a dispatcher, which feeds the data stemming from the communication infrastructure, based on a IPv6 WSN, to top-level applications, namely the monitoring framework.Each node collects information from the environment, through attached sensors or transmitters, and subsequently sends it along with generated reports to a router or sink. In addition, each sensor node is provided with a set of dedicated detection and accommodation functionalities, implemented by means of mobile agents.The communications between the router/sink and the applications are carried out in the middleware layer. The Tunslip (see e.g. [3]) creates aSerial Line Protocol (SLIP) tunnel between the “physical” serial port and the “virtual” network interface. The dispatcher software translates IPv6 packets into IPv4 format in order to allow the communication with IPv4 based services and servers. Each message comprises a header and a payload. The message payload (see Table 1) consists of Message Type: the message can be originated from the system's application or from a local agent; Node ID: denoting the node address; Control ID: the command flag for local agents; Data ID: data collected in the node ID; Agent ID: agent's identifier to be launched, stopped or resumed; Agent MSG: data provided by an agent.The hierarchical multi-agent based architecture is composed of two layers, namely a higher-level layer with coordination functionalities and a bottom layer comprising subordinate agents. These sort of agents are committed to specific tasks, such as monitoring ADCs readings, or detecting and accommodating outliers (see Fig. 2).Concerning the agents’ commands, the platform is provided with: (i) Start Agent: starts a monitoring agent by sending the command flag, the agent's ID and the node destination address to the underlying sensor node; (ii) Stop Agent: stops a particular monitoring agent; (iii) Start All Agents: starts all monitoring agents stacked at the sensor node memory; (iv) Stop All Agents: stops all monitoring agents running on a given sensor node.The main goal of the master agent is to carry out management routines related to subordinate local agents and to coordinate the communications between the node and the sink. When this agent is activated, it automatically launches dependent lower-level agents. This agent is also responsible for monitoring the status of all local dependent agents and, in case of an agent's crash, the master loads to the node's memory a copy of underlying agent.Monitoring agents are responsible for collecting data from the environment and for accommodating possible outliers in raw data. Each monitoring agent implements one of the approaches considered in this work. If a given sample taken at a given discrete time is tagged as an outlier, the corresponding value is replaced by the accommodated sample and an alarm sent to the sink.In this section the approaches under assessment are compared in terms detection sensitivity and specificity using a test-bed.The test-bed consists of a benchmark three-tank system, two remote desktop computers, one devoted to the high-level applications (PC2), such as the process monitoring, and the other for running the middleware, namely the Tunslip and the dispatcher (PC1). In addition four wireless sensor nodes are used for building a WSN.The AMIRA® DTS 200 benchmark three-tank system (Fig. 3) consists of three Plexiglas cylindrical tanks, with identical cross-section, supplied with water. The liquid levels h1, h2 and h3 are measured by piezoresistive transducers in the range of−10,+10V. The middle tank (T3) is connected to the other two adjacent tanks through circular cross-section pipes, provided with manually adjustable ball valves, while the main outlet of the system is located at tank T2. This tank is connected to the bottom reservoir by means of a circular cross-section pipe provided with an outflow ball valve.The WSN infrastructure relies on Zolertia Z1 sensor nodes, which leverage several industry standards, such as USB, IEEE 802.15.4 and Zigbee to interoperate seamlessly with other devices. The Z1 is a low power wireless device that comes with built in support for a number of popular open source operating systems, such as the TinyOS and Contiki, while the supported network stacks include 6LowPAN and Zigbee. Each node includes analogue and digital ports, to which sensors/transmitters can be attached.The operating system used in WSN programming is based on the Contiki. This operating system has been written in C language with support for dynamic loading and replacement of individual programs and services. Additionally, it was built around an event-driven kernel, but provides optional preemptive multi-threading, which can be applied to individual processes (see e.g. [9,24]).Regarding the configuration of the test-bed (Fig. 4), three nodes are configured as sensors, to which the level transmitters are connected in order to collect the tanks(tm) levels, h1, h2 and h3, while the fourth node is used as a sink. The sink node is attached through a USB interface to the remote desktop computer (PC1) where the Tunslip is running, while the monitoring system is located on the remote desktop computer PC2. Finally, it should be recalled that the detection and accommodation algorithms are implemented under a multi-agent framework.

@&#CONCLUSIONS@&#
The present paper evaluated three online outliers detection techniques with potential application in Wireless Sensor Networks. The first approach regards a machine learning technique based on a Least Squares-Support Vector Machine algorithm, under the form of a Reproducing Kernel Hilbert Space (RKHS) with Radial Basis Function (RBF) kernel, along with a sliding window-based learning technique. Aiming at improving the performance of this method in non-stationary conditions, a modification to the RBF kernel was suggested. It is characterised by replacing the Euclidean norm between adjacent readings with the norm of the corresponding differences to Least Squares estimates. The third methodology is a statistical PCA-based technique, with recursive subspace tracking and rank-1 modification, while the last approach relies on univariate statistics, along with an oversampling mechanism within each sampling interval. These methodologies were implemented on local sensor nodes within a hierarchical multi-agent framework, and assessed on a test-bed consisting of a WSN, a benchmark three-tank system and two desktop computers. Experiments have shown the feasibility of implementation of the tested methodologies on the WSN nodes and the superior performance, in terms of sensitivity and specificity, presented by the LS-SVM based approach with modified RBF kernel.