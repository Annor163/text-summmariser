@&#MAIN-TITLE@&#
Univariate subdivision schemes for noisy data with geometric applications

@&#HIGHLIGHTS@&#
We construct univariate subdivision schemes for noisy data.The constructed schemes are based on fitting local least squares polynomials.We study the convergence, smoothness, and basic limit functions of these schemes.A statistical model is analyzed and validated by several numerical examples.We present applications of the schemes for data sampled from curves and surfaces.

@&#KEYPHRASES@&#
Subdivision schemes,Least squares,Convergence analysis,Noisy data,

@&#ABSTRACT@&#
We introduce and analyze univariate, linear, and stationary subdivision schemes for refining noisy data by fitting local least squares polynomials. This is the first attempt to design subdivision schemes for noisy data. We present primal schemes, with refinement rules based on locally fitting linear polynomials to the data, and study their convergence, smoothness, and basic limit functions. Then, we provide several numerical experiments that demonstrate the limit functions generated by these schemes from initial noisy data. The application of an advanced local linear regression method to the same data shows that the methods are comparable. In addition, several extensions and variants are discussed and their performance is illustrated by examples. We conclude by applying the schemes to noisy geometric data.

@&#INTRODUCTION@&#
In recent years, subdivision schemes have become an important tool in many applications and research areas, including animation, computer graphics, and computer aided geometric design, just to name a few (Andersson and Stewart, 2010; Peters and Reif, 2008). A subdivision scheme generates values associated with the vertices of a sequence of nested meshes, with a dense union, by repeated application of a set of local refinement rules. These rules determine the values associated with a refined mesh from the values associated with the coarser mesh. The subdivision scheme is convergent if the generated values converge uniformly to the values of a continuous function, for any set of initial values.The particular class of interpolatory schemes consists of schemes with refinement rules that keep the values associated with the coarse mesh and only generate new values related to the additional vertices of the refined mesh. An important family of interpolatory schemes is the family of Dubuc–Deslauriers (DD) schemes (Deslauriers and Dubuc, 1989).Intensive studies have been carried out recently on the generalization of subdivision schemes to more complicated data such as manifold valued data (Wallner and Dyn, 2005; Wallner et al., 2007), matrices (Sharon and Itai, 2013), sets (Dyn and Farkhi, 2002), curves (Itai and Dyn, 2012), and nets of functions (Conti and Dyn, 2011). In Donoho and Yu (2000) subdivision schemes have been used in a multi-resolution fashion to remove heavy-tail noise. In this paper, we propose a way how to approximate a function from its noisy samples by subdivision schemes.The linear and symmetric refinement rules of the DD schemes and their dual counterparts (Dyn et al., 2005) are based on local polynomial interpolation. These schemes are stationary in the sense that the same rules are applied at all localities in all subdivision steps, and their approximation order is determined by the degree of the local interpolating polynomials.In this paper we generalize this approach and propose linear and symmetric refinement rules based on local polynomial approximation, where the polynomial is determined by a least squares fit to the data. We call these schemes least squares schemes. The least squares schemes are designed to fit noisy data. Indeed, our numerical experiments indicate that in some cases these schemes outperform an advanced linear regression method.A very recent paper (Mustafa et al., 2015) computes refined values by localℓ1optimization rather than by local least squares. The lack of explicit expressions for the refined values of theℓ1optimization enables experimental results only, which are compared with the performance of our schemes.The least squares schemes and their tensor-products can also deal with geometric data, consisting of contaminated samples of curves and of surfaces. The performance of such schemes is demonstrated in the last section on two examples of curves and two examples of surfaces.The paper is organized as follows. We start by introducing the simplest case of least squares schemes in Section 2. These schemes are based on primal refinement rules and on best fitting linear polynomials to symmetric data points. This is a one parameter family of schemes, with the number of data points as the parameter. We prove convergence and smoothness of these schemes and investigate properties of their basic limit functions. The construction of least squares schemes based on best fitting polynomials of higher degrees and on dual refinement rules is postponed to Section 4. In Section 3 we review a statistical model for fitting noisy data, analyze the suitability of the primal least squares schemes of degree 1 for dealing with this kind of data, and provide several numerical examples. Further numerical examples for primal schemes based on best fitting polynomials of higher degrees are presented in Section 4.4. Section 5 shows the application of the least squares schemes and their tensor-product to geometrical data. Throughout this paper we use several well-known properties of least squares polynomials. A short survey of these properties and a method for the efficient evaluation of our schemes are given in Appendix A.In this paper we consider the univariate setting. We denote byfk=(fik)i∈Zthe data at refinement levelk∈N0. We assume that the initial dataf0=(fi0)i∈Zis given at the integersZand thatfikis associated with the dyadic pointtik=2−ki. The main idea of least squares subdivision is to generate the data at levelk+1by evaluating a polynomial that locally fits the data at level k in a symmetric neighborhood.In particular, we use polynomials that best fit the data in the least squares sense. That is, for given datay1,…,ymat nodesx1,…,xm, we are interested in the polynomialpdof degree d that minimizes the sum of squared residuals,(1)∑i=1m(pd(xi)−yi)2.Ford<mthis problem has a unique solution and in Appendix A we provide a summary of the relevant theory, which also includes the cased≥m.We start by considering the simplest least squares subdivision schemes corresponding to the cased=1, which we denote bySnforn≥1. Such a scheme generates the data at levelk+1as follows. On the one hand, the valuef2ik+1, which replacesfik, is determined by fitting a linear polynomial to the2n−1data values in a symmetric neighborhood aroundtikand evaluating it at the associated dyadic pointtik=t2ik+1. On the other hand, the scheme computes the new valuef2i+1k+1betweenfikandfi+1kby evaluating att2i+1k+1=1/2(tik+ti+1k)the linear least squares polynomial with respect to the data at the nearest 2n nodes. In this construction the parameter n controls the locality of the scheme and we study its effect in Section 3.For the cased=1and equidistant nodesxi=a+ih, letp1⁎be the linear least squares polynomial which minimizes (1). The value ofp1⁎at the centrec=(x1+⋯+xm)/mof the nodes isp1⁎(c)=(y1+⋯+ym)/m.Thus, the refinement rules ofSnturn out to be(2)f2ik+1=12n−1∑j=−n+1n−1fi+jkandf2i+1k+1=12n∑j=−n+1nfi+jk.Consequently, the symbol (Dyn, 1992) of the scheme is(3)an(z)=12n∑j=−n+1nz2j−1+12n−1∑j=−n+1n−1z2j.It follows from the symmetry of the nodes determining the linear least squares polynomials, thatan(z)=an(1/z), hence the scheme is odd symmetric (Dyn et al., 2008). As the data at levelk+1depends on at most 2n values at level k, we conclude thatSnis a primal 2n-point scheme. The masks of the first three schemes area1=[1,2,1]/2,a2=[3,4,3,4,3,4,3]/12,a3=[5,6,5,6,5,6,5,6,5,6,5]/30.Note that the schemeS1is the interpolating 2-point scheme, which generates piecewise linear functions in the limit.Following the usual definition of convergence in Dyn (2002, Chapter 2), we denote the limit of a convergent subdivision scheme S for initial dataf0byS∞f0.Theorem 1The least squares subdivision schemeSnis convergent forn≥1.The explicit form of the symbol in (3) implies thatan(1)=2andan(−1)=0, which are necessary conditions forSnto be convergent (Dyn, 1992, Proposition 2.1). In addition, since the coefficients of the symbol in (3) are all positive, and there are at least three such coefficients, it follows from Cavaretta et al. (1991, Theorem 3.3) that the scheme is convergent.  □Following the analysis in Dyn (1992), we define(4)qn(z)=an(z)1+z=12n(2n−1)(∑j=−n+1n−1(n−j)z2j−1+∑j=−n+1n−1(n+j)z2j),which is the symbol of the difference scheme associated withSn. The norm of this scheme,‖S[qn]‖∞=max⁡{12n(2n−1)∑j=−n+1n−1|n−j|,12n(2n−1)∑j=−n+1n−1|n+j|}=12n(2n−1)∑j=12n−1j=12,is the least possible, as in the case of the uniform B-spline schemes, indicating “quickest” possible convergence. The structure ofqnfurther reveals that the limit functions generated bySnareC1.Theorem 2The least squares subdivision schemeSngeneratesC1limit functions forn≥2.ProofIt is known (Dyn, 1992, Theorems 3.2 and 3.4) that in order to prove the theorem, it is sufficient to show that the scheme with symbol2qnis convergent. By (4),2qn(1)=2and2qn(−1)=0,henceS[2qn]satisfies the necessary conditions for convergence. As in the proof of Theorem 1 we conclude that the schemeS[2qn],n≥2is convergent, and thereforeSn,n≥2generatesC1limit functions.  □The statement in Theorem 2 is confirmed by the numerical results presented in Table 1, which were obtained by using 16 iterations of the algorithm in Dyn and Levin (2002) to compute lower bounds on the Hölder regularity. In addition, it is easy to verify thatqn′(−1)<0and therefore(1+z)2is not a factor ofqn(z)or equivalently that(1+z)3is not a factor ofan(z). Thus, the schemeSndoes not generateC2limits from any initial data (Dyn et al., 2008).Let us denote byδthe sequence which is zero everywhere except at 0, where it is 1. The basic limit function of the convergent subdivision schemeSnis then defined as(5)ϕn=Sn∞δ.Some examples ofϕnfor small values of n are shown in Fig. 1.Many properties of a linear subdivision scheme can be derived from its basic limit function. In particular, due to linearity, the limit function generated from the initial dataf0=(fi0)i∈Zby the schemeSnhas the form(6)(Sn∞f0)(x)=∑j∈Zfj0ϕn(x−j).Our first observation is that the support ofϕnis[−2n+1,2n−1], becauseSnis a primal 2n-point scheme (Deslauriers and Dubuc, 1989). Moreover,ϕnis positive inside its support, because the coefficients of the maskanare positive in the mask's support, andϕnhas the partition of unity property(7)∑j∈Zϕn(x−j)=1,due to the reproduction of constant polynomials bySn.The simple structure ofanfurther allows us to derive several interesting properties regarding the values of the basic limit functionϕnat the integers. These values are of importance, because they constitute the filter which operates on the initial data and generates the final values at the integers. Taking into account thatϕnis continuous and therefore vanishes at the end points of its support, we conclude from (6) that the limit at the integersk∈Zis(8)(Sn∞f0)(k)=∑j=−2n+22n−2fk−j0ϕn(j).The non-zero values ofϕnat the integers constitute an eigenvectorv=(ϕn(−2n+2),…,ϕn(2n−2))corresponding to the eigenvalue 1 of the transposed subdivision matrix (Dyn, 1992), which in this case is the(4n−3)×(4n−3)column stochastic, two-slanted band matrixAn=(rs000000rsrs0⋯000rsrsr000⋮⋱⋮rsrsrrs0rsrsr⋯rsr0srsrrsr⋮⋱⋮00000⋯0sr)with entriesr=1/(2n−1)ands=1/(2n).The odd symmetry of the maskanguarantees thatϕnis a symmetric function. Thus, the eigenvectorvis also symmetric, as indicated by the structure ofAn. Taking these symmetries into account, we get that the vectorv˜=(ϕn(−2n+2),…,ϕn(0))is an eigenvector corresponding to the eigenvalue 1 of the(2n−1)×(2n−1)matrixA˜n=(rs00000000rsrs0⋯00000rsrsr00000⋮⋱⋮rsrsrrs000rsrsrrsrs0rsrsr⋯rsr2srrsrsrr2s2r2srrsrsr2r2s2r2sr⋮⋱⋮rsr2s2r2r2s2r2srr2s2r2s2r⋯2r2s2r2sr2r2s2r2s2r2r2s2r2sr).The particular structure ofA˜nallows us to derive the following observation.Proposition 3The values ofϕnat the non-positive integers in its support are strictly increasing,0<ϕn(−2n+2)<ϕn(−2n+3)<⋯<ϕn(−1)<ϕn(0).Moreover,ϕn(−n)=n−12n−1ϕn(0).ProofNote that each row ofA˜nis equal to the previous row plus at least one positive term. Sincev˜satisfiesA˜nv˜=v˜and its componentsv˜i=ϕn(i−2n+1),i=1,…,2n−1, are positive, the latter must be strictly increasing.To establish the second statement, consider the(n−1)-th and the last row ofA˜n,α˜n−1=(r,s,r,s,…,r,s,0)andα˜2n−1=(2r,2s,2r,2s,…,2r,2s,r),and note thatα˜2n−1=2α˜n−1+(0,0,…,0,r).Then, sincev˜n−1=α˜n−1v˜andv˜2n−1=α˜2n−1v˜=2α˜n−1v˜+rv˜2n−1=2v˜n−1+rv˜2n−1,the second statement follows directly from the definition ofv˜, becauser=12n−1.  □By the symmetry ofϕn, the statements of Proposition 3 hold analogously for the values ofϕnat the non-negative integers. As an immediate consequence we have(9)ϕn(j)<12ϕn(0),|j|≥n,as well as the following bounds onϕn(0).Corollary 4The value ofϕn(0)satisfies(10)13n−2<ϕn(0)<1n−1.The upper bound follows from (7) and Proposition 3, because1=∑|j|≤2n−2ϕn(j)>∑|j|≤nϕn(j)>(2n+1)ϕn(−n)=(2n+1)(n−1)2n−1ϕn(0).Using (9), we further have1=∑n≤|j|≤2n−2ϕn(j)+∑|j|<nϕn(j)<(2n−2)12ϕn(0)+(2n−1)ϕn(0),leading to the lower bound.  □Proposition 3 and its consequences clarify the properties ofϕnat the integers, which are confirmed by the examples in Fig. 1. A further analysis ofϕnreveals more details, in particular about the asymptotic behavior for large n as well as an improvement in the upper bound in (10).Theorem 5The basic limit functionϕnand its derivativeϕn′converge uniformly to the zero function as n grows. More specifically,‖ϕn‖∞∼1nand‖ϕn′‖∞∼1n2.ProofWe first observe that the masks corresponding to the refinement rules (2) are positive. Thus, for non-negative data such asδwe have‖Snk1(δ)‖∞≤‖Snk2(δ)‖∞for any integersk1>k2>0. We can therefore bound‖ϕn‖∞from above,(11)‖ϕn‖∞=‖Sn∞(δ)‖∞≤‖Sn1(δ)‖∞=12n−1∼1n.A similar behavior holds for the derivativeϕn′, which exists sinceSngeneratesC1limits by Theorem 2. To see this, first recall the definition ofqnin (4), which implies the relation (Dyn, 1992, Section 2.3)ϕn′(x)=S[2qn]∞(Δδ)(x),where Δ is the forward difference operator with(Δδ)0=−1,(Δδ)−1=1, and zero otherwise. This implies‖ϕn′‖∞≤‖S[2qn]Δδ‖∞. Further note thatS[2qn]has a positive mask(2qn)with coefficients(12)(2qn)2j−1=1n(2n−1)(n−j)and(2qn)2j=1n(2n−1)(n+j)forj=−n+1,…,n−1and(2qn)2j−1=(2qn)2j=0for|j|≥n.A direct calculation yields(13)|(S[2qn]Δδ)j|={1/n(2n−1),if−2n−1<j<2n−2,1/n,ifj=−2n−1orj=2n−2,0,otherwise.From (12) and (13) we then conclude that each summand in(14)(S[2qn]2Δδ)j=∑i∈Z(2qn)j−2i(S[2qn]Δδ)iis of order1/n3, except for at most one summand of order1/n2. Since there are at most2n−1non-zero terms in the sum (14), the order of the sum is1/n2. Thus, we have‖ϕn′‖∞≤‖S[2qn]2Δδ‖∞∼1/n2.  □Proposition 3 and Theorem 5 provide a good understanding of the basic limit functionϕn, which is supported by our numerical tests.The schemesSnforn>1are designed to deal with noisy data, which is confirmed by the following discussions and experiments. We first introduce a statistical model and then compare the performance of our schemes and an advanced local linear regression method.Letf:R→Rbe a continuous scalar function and suppose we are given a discrete set of noisy samplesyi=f(ih)+εi,i∈Z,where{εi}i∈Zare independent random variables, normally distributed with zero mean and varianceσ2. As an estimatorfˆof f we use the limit (6) ofSn, that is,(15)fˆ(x)=∑j∈Zyjϕn(x−j).Note thatfˆ(x)is a random variable and the estimation quality offˆis given by the expectation of the squared error.With E denoting the expectation operator, the “bias-variance decomposition” (see e.g., Hastie et al., 2009, Chapter 7) of the expected squared error forx∈Ris(16)E[(fˆ(x)−f(x))2]=σ2∑j∈Zϕn(x−j)2+(∑j∈Zf(jh)ϕn(x−j)−f(x))2.The first term in (16) is the product of the variance of the noiseσ2and the function(17)ψn(x)=∑j∈Zϕn(x−j)2.The second term is the square of the deterministic approximation error corresponding to data without noise. We first studyψnand come back to the second term later.It follows from (16) that the effect of the noise on the estimatorfˆis small ifψnis small, which motivates us to further analyzeψnand establish upper bounds. First note that by (7) and the positivity ofϕnwe have(18)ψn≤1,with strict inequality forn>1, namely for non-interpolatory schemes. For the interpolatory schemeS1, we haveψ1(x)=1atx∈Z, which matches the common knowledge that interpolation is not appropriate for noisy data. This behavior is confirmed by Fig. 2, which presents several numerical evaluations ofψnand indicates thatψnbecomes smaller and tends to be the constant zero function as n grows. This is indeed the case, as the following summary of properties ofψnshows.Theorem 6The functionψnin(17)is positive, symmetric, and periodic with period 1. Moreover,‖ψn‖∞∼1nand‖ψn′‖∞∼1n2.ProofBy definition,ψnis positive, periodic, and finite. The symmetry ofϕnimplies the symmetry ofψn. In addition, we have thatψnis symmetric about 1/2 due to the periodicity ofψn.The first asymptotic bound follows from the definition ofψnin (17) after noting that only4n−2terms in the sum are non-zero, and that each term is of order1/n2by Theorem 5. The second asymptotic bound follows by similar arguments using the chain rule, the explicit bound onϕnin (11), and the asymptotic bound onϕn′in Theorem 5,|ψn′(x)|≤2∑j∈Z|ϕn(x−j)ϕn′(x−j)|≤222n−1∑j∈Z|ϕn′(x−j)|∼1n2.□The second term of the expected squared error in (16) is the deterministic error or the approximation error. We use the approximation order as a standard measure for the quality of the approximation; see for example Dyn and Levin (2002, Chapter 7). For the case of schemes based on linear least squares polynomials, the approximation order ish2, where h is the distance between the sampled points of the initial data. This observation follows from the polynomial reproduction property of our schemes, that is, the reconstruction of any linear polynomial from its samples.In conclusion, there is a trade-off between the deterministic approximation error and the effect of the noise on the expected squared error. In particular, higher values of n decrease the effect of noise but increase the deterministic error due to averaging of the values{fi}i∈Zby weights with a large support.We illustrate the performance of some of the schemes by several numerical examples, starting from noisy data. We compare their performance with the algorithm of local linear regression (LLR) for local fitting of noisy data. This local estimator around a given data pointx⁎is obtained by including kernel weights into the least squares minimization problem in the neighborhood ofx⁎,minα,β⁡∑i=0n(yi−α−β(xi−x⁎))2Ker(xi−x⁎).This approach can be generalized to higher degree polynomials as well; see Härdle et al. (2004, Chapter 4) for more details. Although the concept of LLR is rather simple, it is one of the most important statistical approaches used.We take the LLR variant which is based on the normal kernel with the kernel parameters chosen dynamically, and we compare it with the limits of several subdivision schemes with different support sizes, for various types of functions and levels of noise. The noise we consider is normally distributed and measured using the signal-to-noise ratio (SNR). The SNR is defined as the ratio between theL2norm of the signal (true function and additional noise) and theL2norm of the noise. Thus, when this ratio tends to one, the noise becomes as significant as the signal itself. The standard unit is decibel (dB) which is calculated in a logarithmic scale. We examine the range of (roughly) 1–20 dB and consider 1–5 dB a very high level, 5–7 a high level, and 10–12 a low level of noise. Noise levels>12dBare considered negligible. In each example we plot the relative approximation error of LLR and the subdivision scheme, as a function of the noise levels. This relative error is defined as the ratio between the norm of the approximation error and the norm of the function.In the first examples we consider the slowly varying functionf1(x)=sin⁡x10+(x50)2and examine the three subdivision schemesS3,S5, andS7. Due to the dynamic implementation of LLR, we can use it as a benchmark for all cases. As discussed in Section 3.1, the subdivision schemeS3with smaller support is more sensitive to the variance of the noise thanS5andS7. We observe in Fig. 3(left) that for all levels of noise, LLR gives a smaller reconstruction error. The difference in the actual function reconstruction for a specific noise level is illustrated in Fig. 3 (right). The same presentation is repeated forS5andS7in Figs. 4 and 5, respectively. For the slowly varying functionf1, the subdivision schemeS5behaves almost identically to LLR, whileS7is even better. These trials match our theory which suggests that as the support gets larger, the corresponding functionψnbecomes smaller, resulting in a weaker response to noise.In our second example we sample the oscillatory functionf2(x)=cos⁡2x5+(x40−1)3and compare LLR withS3andS5. The results are presented in Figs. 6 and 7and show that the smaller support ofS3makes it more suitable for these type of functions (except for extremely high noise), whileS5provides inferior results for any reasonable level of noise.To conclude, we observe from the numerical examples that there is a range of parameters for which a subdivision scheme outperforms LLR. Also, the numerical examples support our understanding about the trade-off between the effect of noise and the deterministic approximation error, as discussed in Section 3.1.The family of primal least squares schemes of degree 1 can be extended in several ways. We first discuss the extension to dual schemes (Section 4.1), as well as minor variations of both primal and dual schemes (Section 4.2). A further extension relies on fitting least squares polynomials of higher degree (Section 4.3) and we provide a few numerical examples of such schemes (Section 4.4).The idea of the schemesSnin Section 2 is to fit linear least squares polynomials and to evaluate them in a primal way, that is, at the points and the midpoints of the current mesh. Another option is to design subdivision schemes based on dual evaluation (Dyn et al., 2008). The dual least squares schemeS¯nis obtained by fitting a linear polynomial to the 2n data values at the pointsti−n+1k,…,ti+nkat level k and evaluating this polynomial at 1/4 and 3/4 betweentikandti+1kto compute the new dataf2ik+1andf2i+1k+1.The refinement rules ofS¯nare slightly more complicated to derive than those of the primal schemes, but they still have a rather simple closed form,(19)f2ik+1=12n∑j=−n+1n(1−6j−38n2−2)fi+jkandf2i+1k+1=12n∑j=−n+1n(1+6j−38n2−2)fi+jk.The corresponding symbol is(20)a¯n(z)=12n∑j=−nn−1(1+z+6j+38n2−2(1−z))z2j,and it is easy to verify thata¯n(z)z=a¯n(1/z), which confirms thatS¯nis an even symmetric scheme (Dyn et al., 2008). Overall we conclude thatS¯nis a dual 2n-point scheme and the support of its basic limit functionϕ¯nis[−2n,2n−1]. The masks of the first three schemes area¯1=[1,3,3,1]/4,a¯2=[7,13,9,11,11,9,13,7]/40,a¯3=[55,85,61,79,67,73,73,67,79,61,85,55]/420,and we recognizeS¯1as Chaikin's corner cutting scheme (Chaikin, 1974).The proofs of Theorems 1 and 2 carry over to the dual schemes, and so the limit functions generated byS¯nare at leastC1forn≥1. But unlike the primal schemes, the symbols of the dual schemes are divisible by(1+z)3, and so they may potentially generateC2limits. However, there is no simple proof as forC1in Theorem 2, because the symbol4a¯n(z)/(1+z)2has negative coefficients. Table 2lists lower bounds on the Hölder regularity of the first few schemes, computed using 16 iterations of the algorithm in Dyn and Levin (2002), demonstrating that the limits ofS¯nare in factC2, at least for2≤n≤10.In addition to the dual 2n-point schemesS¯n, it is also possible to define dual(2n+1)-point schemes. These schemes fit a linear polynomial to the2n+1data values in a symmetric neighborhood aroundfikand evaluate it at 1/4 the distance to the left (right) neighbor to define the new dataf2i−1k+1(f2ik+1). The resulting refinement rules aref2i−1k+1=12n+1∑j=−nn(1−3j4n(n+1))fi+jkandf2ik+1=12n+1∑j=−nn(1+3j4n(n+1))fi+jk,and the support of the corresponding basic limit function is[−2n−1,2n]. The masks of the first three schemes of this kind aren=1:[5,11,8,8,11,5]/24,n=2:[6,10,7,9,8,8,9,7,10,6]/40,n=3:[13,19,14,18,15,17,16,16,17,15,18,14,19,13]/112.Similarly, we can define primal(2n+1)-point schemes as variants of the primal 2n-point schemesSn. We simply replace the refinement rule forf2ik+1in (2) byf2ik+1=12n+1∑j=−nnfi+jkand keep the rule forf2i+1k+1. For these schemes, the support of the basic limit function is[−2n,2n], and the masks of the first three schemes aren=1:[2,3,2,3,2]/6,n=2:[4,5,4,5,4,5,4,5,4]/20,n=3:[6,7,6,7,6,7,6,7,6,7,6,7,6]/42.Adapting the proofs of Theorems 1 and 2, one can show that both variants generateC1limit functions, and our numerical results demonstrate that the dual(2n+1)-point schemes are evenC2for1≤n≤10.The least squares schemes of degree 1 reproduce linear polynomials by construction, but they do not reproduce polynomials of any higher degree. So, their approximation order is onlyh2, unless the data is being pre-processed (Dyn et al., 2008). We can improve this by using least squares polynomials of higher degreesd>1.To derive the refinement rules at level k, letpn,idbe the least squares polynomial of degree d for the2n−1data(ti+jk,fi+jk),j=−n+1,…,n−1in a symmetric neighborhood oft2ik+1, and letp˜n,idbe the polynomial of degree d that fits the 2n data(ti+jk,fi+jk),j=−n+1,…,nin a symmetric neighborhood oft2i+1k+1. The polynomialspn,idandp˜n,idare well-defined ford<2n−1andd<2n, respectively (see Appendix A.1).The primal 2n-point least squares scheme of degree d is then characterized by the refinement rules(21)f2ik+1=pn,id(tik)andf2i+1k+1=p˜n,id((tik+ti+1k)/2),which simplifies to the rules in (2) ford=1. The resulting subdivision schemeSndreproduces polynomials of degree d by construction, and thus has approximation orderhd+1. It is well-defined ford<2n, even though ford=2n−1the rule forf2ik+1is based on an underdetermined problem. In that case we getf2ik+1=fik(see Remark 9 in Appendix A.1), henceSn2n−1is the interpolating Dubuc–Deslauriers 2n-point scheme.As shown in Remark 12 in Appendix A.3, it is sufficient to consider only primal 2n-point least squares schemes of even degree, becauseSn2dandSn2d+1are identical. This also means that the schemes of degree 2d reproduce polynomials of one degree more than expected by construction. This is in accordance to the observation in Dyn et al. (2008) that the reproduction of odd degree polynomials comes “for free” by the primal symmetry. In particular, this shows that the refinement rule of the interpolating 4-point scheme (Dubuc, 1986) forf2i+1k+1can be derived not only from fitting a cubic polynomial to the datafi−1k,…,fi+2k, but also by fitting a quadratic polynomial in the least squares sense to the same data.We can also generalize the construction in Section 4.1 and define the dual 2n-point least squares scheme of degree d by the refinement rules(22)f2ik+1=p˜n,id((3tik+ti+1k)/4)andf2i+1k+1=p˜n,id((tik+3ti+1k)/4),which simplify to the rules in (19) ford=1. LikeSnd, the schemeS¯ndreproduces polynomials of degree d by construction and its approximation order ishd+1. Moreover, the schemeS¯n2n−1is the dual 2n-point scheme (Dyn et al., 2005).Similar constructions lead to primal and dual(2n+1)-point least squares schemes of degree d, but we omit the details as they are straightforward. Apart from the increased approximation order, these schemes also tend to have a higher smoothness. For example, we verified numerically that the schemesS¯n3generateC3limit functions forn=4andn=5, but we do not recommend using them, because the rules become more complicated and the benefit of using them for reconstructing functions from noisy data is marginal, as shown in the next section.The statistical model presented in Section 3.1 is also valid for schemes based on higher degree least squares polynomials, due to the linearity of the schemes (see also Appendix A.2), but proving asymptotic bounds forψndbecomes difficult, because the mask ofSndis no longer positive and not given explicitly ford>1. However, our numerical tests, which are summarized in Table 3and Fig. 8, indicate that the bounds in Theorem 6 for the special cased=1also hold ford>1.Conjecture 7Letψndbe defined as in(17)for schemes based on least squares polynomials of degree d. Then,‖ψnd‖∞∼1nand‖(ψnd)′‖∞∼1n2.The deterministic error in (16) is strongly related to d. This can be seen by the polynomial reproduction property of our schemes, that is, the reconstruction of any polynomial of degree d from its values at the integers by the limit ofSnd. The latter property implies that the approximation order is at leasthd+1. Thus, for larger d the contribution of the deterministic error decreases, while we conjecture that the effect of the noise increases. This relates to the following predicted behavior ofψndwith respect to d and n and is supported by the results shown in Fig. 9.Conjecture 8For any fixed support size n and different degreesd1andd2withd1>d2,ψnd1(x)≥ψnd2(x),x∈[0,1].For any fixed degree d and different support sizen1andn2withn1<n2,ψn1d(x)≥ψn2d(x),x∈[0,1].To further back up this conjecture, let us consider some numerical experiments, similar to those in Section 3.2. We first compare the schemesS51andS53, applied to noisy data taken from the slowly varying function(23)f3(x)=cos⁡x10−(x50−1)3,for which the deterministic error is expected to be small. Fig. 10shows thatS51, which is based on locally fitting linear polynomials, gives better reconstructions, as long as the noise is significant. However, as the noise decays, the deterministic error becomes more relevant and the schemeS53, which is based on locally fitting cubic polynomials and therefore has approximation orderh4, manages to estimate the function more accurately thanS51, whose approximation order is onlyh2. This example emphasizes the trade-off between the deterministic approximation error and the effect of noise on the expected squared error, and this effect becomes even clearer if we consider the function(24)f4(x)=cos⁡2x5−(x50−45)3.Due to the oscillations of this function, the deterministic error is dominant and the results in Fig. 11confirm thatS53outperformsS51for all noise levels.Finally, we repeat the experiments with the test functionsf3andf4for the schemesS63andS93, which are both based on locally fitting cubic polynomials but have different support sizes. Fig. 12shows that the larger support helps to smooth out noise if the deterministic error is small. But if the deterministic error is more relevant than the noise, than the smaller support leads to smaller reconstruction errors for all noise levels, as illustrated in Fig. 13.We conclude the paper by presenting applications of our least squares subdivision schemes to noisy samples of curves and of surfaces. We measure the level of the noise by SNR, although this measure in the geometrical setting is less informative than in the functional setting, because the significance of the noise also depends highly on the geometry.The parametrization of a curve enables us to apply our univariate subdivision schemes to each of its components. By doing so, we can construct an approximation to the curve from its noisy samples. We introduce two such examples.The first example consists of an alpha-like curve, given by(25)x(t)=3t4+t2+1,y(t)=t5−2t,sampled equidistantly over[−1.4,1.4], that is, with samples taken atti=−7/5+ih, whereh=14/145andi=0,…,29. This curve and its sample points are shown in Figs. 14(a) and 14(b). The first set of noisy samples with a relatively low level of noise is shown in Fig. 14(c). We applyS3andS5(both based on linear fitting, see Section 2) to these samples, giving the limit curves in Figs. 15(a) and 15(b), respectively. The limits of both schemes retain the general shape of the curve, but a minor artifact appears on the limit curve generated byS3since it closely fits the noisy samples. Perturbing the samples with high level noise, as seen in Fig. 14(d), reveals an overfitting by the limit curve ofS3in Fig. 15(c), while the limit curve generated byS5in Fig. 15(d) preserves the topology of the original curve.In the second example we apply four different least squares schemes to noisy samples of a star-shaped curve, given by(26)x(t)=4cos⁡(t)+cos⁡(4t),y(t)=4sin⁡(t)−sin⁡(4t).We sample this curve atti=i/(100π)fori=0,…,49. This curve and its sample points are shown in Figs. 16(a) and 16(b). In this example we compare the performance of four schemes: two schemes based on linear fitting,S3andS5, and two schemes based on cubic fitting,S43andS63. As in the first example, we start by investigating the case of low level noise, with the samples shown in Fig. 16(c). The limits of all schemes are presented in the upper row of Fig. 17. They all have the shape of a star, except for the limit ofS5, which is more similar to a pentagon than to a star. By zooming in, it can be seen that the limit curve generated byS43suffers from a minor artifact next to its lowest vertex, this being consequence of trying to fit the noisy data. For the set of samples with high level noise in Fig. 16(d), the results confirm our previous observation. Namely, bothS3andS63generate reasonable results, while the limit curves generated byS5andS43suffer from geometrical artifacts caused by oversmoothing and overfitting, respectively. These limits are presented in the lower row of Fig. 17.Equipped with univariate least squares subdivision schemes, we use tensor-product bivariate schemes based on them. These bivariate schemes are applied to noisy samples of surfaces, given at vertices of quadrilateral grids. Two examples are provided to illustrate the application of these bivariate schemes to noisy data.The first surface we examine is a torus surface, given by(27)x(u,v)=cos⁡(u)(10+5cos⁡(v)),y(u,v)=sin⁡(u)(10+5cos⁡(v)),z(u,v)=5sin⁡(v),sampled every 15 degrees, that is, atui=iπ/12andvj=jπ/12fori,j=0,…,23. This surface and its sample points are shown in Figs. 18(a) and 18(b). We investigate the limits of the bivariate tensor-product schemesS3⊗S3andS5⊗S5. First, we study the application of these schemes to the samples with low level noise in Fig. 18(c). The limits of both schemes in Figs. 19(a) and 19(b)are fairly good. For the samples with high level noise in Fig. 18(d), the limit ofS3⊗S3in Fig. 19(c) keeps the general shape but is a poor approximation to the torus, while the limit ofS5⊗S5in Fig. 19(d) provides a better approximation.The surface of the second example is not a mathematical surface but a scan of a mechanical element, parameterized by a quadrilateral grid, and given in terms of49×81=3969vertices. Figs. 20(a) and 20(b)show the surface and its sample points, respectively. Similarly to the second example in the curve case, we investigate the limit surfaces generated by the four tensor product schemes:S3⊗S3,S5⊗S5,S43⊗S43, andS63⊗S63. We compare their limits from a set of samples with a low level of noise and a set of samples with a high level of noise. These sets of samples are given in Figs. 20(c) and 20(d), respectively. The limit surfaces for the samples with low level noise, shown in the upper row of Fig. 21, indicate thatS3⊗S3andS63⊗S63outperform the other two schemes. For the samples with high level noise, the performance ofS5⊗S5is superior to that of the other three, as can be seen in the lower row of Fig. 21.

@&#CONCLUSIONS@&#
