@&#MAIN-TITLE@&#
Speaker verification based on the fusion of speech acoustics and inverted articulatory signals

@&#HIGHLIGHTS@&#
A practical feature-level and score-level fusion approach for speaker verification with articulatory information.Concatenating real articulatory measurements with MFCCs improves the performance.Concatenating acoustic-to-articulatory inversion features with MFCCs also improves the result.

@&#KEYPHRASES@&#
Text independent speaker verification,Text dependent speaker verification,Speech production,Articulatory features,Acoustic-to-articulatory inversion,

@&#ABSTRACT@&#
We propose a practical, feature-level and score-level fusion approach by combining acoustic and estimated articulatory information for both text independent and text dependent speaker verification. From a practical point of view, we study how to improve speaker verification performance by combining dynamic articulatory information with the conventional acoustic features. On text independent speaker verification, we find that concatenating articulatory features obtained from measured speech production data with conventional Mel-frequency cepstral coefficients (MFCCs) improves the performance dramatically. However, since directly measuring articulatory data is not feasible in many real world applications, we also experiment with estimated articulatory features obtained through acoustic-to-articulatory inversion. We explore both feature level and score level fusion methods and find that the overall system performance is significantly enhanced even with estimated articulatory features. Such a performance boost could be due to the inter-speaker variation information embedded in the estimated articulatory features. Since the dynamics of articulation contain important information, we included inverted articulatory trajectories in text dependent speaker verification. We demonstrate that the articulatory constraints introduced by inverted articulatory features help to reject wrong password trials and improve the performance after score level fusion. We evaluate the proposed methods on the X-ray Microbeam database and the RSR 2015 database, respectively, for the aforementioned two tasks. Experimental results show that we achieve more than 15% relative equal error rate reduction for both speaker verification tasks.

@&#INTRODUCTION@&#
The goal of a speaker verification system is to determine automatically whether a given segment of speech is indeed spoken by the claimed speaker. It can be further divided into text independent speaker verification (TISV) and text dependent speaker verification (TDSV) depending on whether we constrain the speech content during verification.Total variability i-vector modeling has gained significant attention in speaker verification due to its excellent performance, compact representation and small model size (Dehak et al., 2011a). In this framework, first, zero-order and first-order Baum-Welch statistics are calculated by projecting the acoustic level Mel-frequency cepstral coefficients (MFCC) features onto universal background model (UBM) components using the occupancy posterior probability. Second, in order to reduce the high dimension of the concatenated statistics supervectors, a single factor analysis is adopted to generate a low dimensional total variability space which jointly models language, speaker and channel variabilities all together (Dehak et al., 2011). The factor analysis can also be extended to a simplified and supervised version to enhance the performance and reduce the computational cost (Li and Narayanan, 2014). Within this i-vector space, variability compensation methods, such as within-class covariance normalization (WCCN) (Hatch et al., 2006), linear discriminative analysis (LDA) and nuisance attribute projection (NAP) (Campbell et al., 2006) are performed to reduce the variability for subsequent scoring methods (e.g., cosine similarity (Dehak et al., 2011a), support vector machine (SVM) (Cumani et al., 2011), probabilistic linear discriminant analysis (PLDA) (Prince, 2007; Matejka et al., 2011), deep belief networks (Cumani et al., 2011), etc.). Several types of phonetics-aware generalized i-vectors have also been recently proposed for better performance (Lei et al., 2014; D’Haro et al., 2014; Li and Liu, 2014).In addition to the aforementioned state-of-the-art modeling methods, various features have also been proposed for speaker verification (e.g. short-term spectral features, voice source features, spectral-temporal features, prosodic features and high-level features) (Kinnunen and Li, 2010). Based on these multiple sets of features, both feature-level and score-level fusion approaches have been shown to enhance the overall system performance (Kinnunen and Li, 2010; Kim and Stern, 2012; Shao and Wang, 2008; Wang and Johnson, 2014). Specifically, by fusing the phonetic level tandem features and the acoustic level MFCC features together at the feature level, more than 40% relative error reduction is achieved (Li and Liu, 2014; D’Haro et al., 2014; Wang et al., 2013). In this work, our goal is to examine the use of speech production oriented features for the speaker verification task.The ability to understand sources of inter-speaker variability in speech production and to predict those sources of variability from the acoustic signal can afford a variety of advantages. Several studies have shown that an important source of inter-speaker variability in speech acoustics lies in the variability in the vocal tract morphology across various speakers. Morphological variability could result from the differences in the vocal tract length (Peterson and Barney, 1952; Fant, 1960; Lee et al., 1999; Stevens, 1998), or the morphology of the hard palate and the posterior pharyngeal wall (Lammert et al., 2011, 2013b,a). Fig. 1shows magnetic resonance images of the vocal apparatus of four different subjects from the USC-TIMIT corpus (Narayanan et al., 2014) illustrating this variability. Since vocal tract length is closely related to the formant frequency (Stevens, 1998; Fant, 1960), change in vocal tract length scales the spectral envelope for voiced sounds. This has been extensively used for vocal tract length normalization (VTLN) (Eide and Gish, 1996; Lee and Rose, 1996) in automatic speech recognition (ASR). Unlike normalization, we focus on exploiting morphological variations as the cue for capturing speaker characteristics in speaker verification applications. In order to obtain the vocal tract morphological shape information, direct measurements using magnetic resonance imaging or electromagnetic articulography are desirable. Our earlier work (Li et al., 2013) explored the possibility of automatically characterizing hard palate and pharyngeal wall morphological shape patterns directly from speech acoustics. However, indications are that those morphological differences may not be abundantly evident in the acoustics because speakers adjust their lingual articulation in compensation (Brunner et al., 2007; Lammert et al., 2013a), making estimation of these characteristics from acoustics a challenging task. But on the other hand, the details of the associated articulation can itself carry useful speaker-specific information.The present study is based on the working hypothesis that articulatory movements reflect the morphological variability of speakers. For example, it is known that speakers with flat palates exhibit less articulatory variability during vowel production than speakers with highly domed palates (Perkell, 1997; Mooshammer et al., 2004; Brunner et al., 2005, 2009). Articulation of coronal fricatives is also influenced by palate shape, including influencing apical vs. laminal articulation of sibilants (Dart, 1991), as well as jaw height and the positioning of the tongue body (Honda et al., 2002; Thibeault et al., 2011). Inter-speaker differences in vocal tract morphology characteristics hence can lead to differences in speech articulation patterns. In Fig. 2, we can see that there is a clear relation between the speaker's vocal tract morphology and articulatory behavior. Finally, both morphology and articulation jointly influence the generation of the acoustic signal. This motivates us to perform speaker verification by using both the articulatory and the acoustic features (light blue arrows in Fig. 2).We find that concatenating articulatory features obtained from measured speech production data with conventional Mel-frequency cepstral coefficients (MFCCs) improves the speaker verification performance dramatically. Fig. 3shows some examples of articulatory measurements and the corresponding speech waveform. However, since measuring articulatory movement during speech production is not practical for real world applications, experiments are performed where the measured articulatory features are replaced with estimated articulatory features obtained using acoustic-to-articulatory inversion techniques. In this way, the inter-speaker variations could be projected into the intra-speaker variabilities of the exemplar speaker assuming he/she is asked to mimic different speakers’ pronunciations. Since some components of the articulatory trajectories are highly correlated, we apply the principal component analysis (PCA) for dimension reduction and used multiple exemplar speakers in our experiments to enhance the speaker verification performance. Specifically, we show that augmenting MFCCs with features obtained from subject-independent acoustic-to-articulatory inversion techniques achieves promising results against the MFCC baseline and significantly improves the performance after score level fusion. In this work, we applied one exemplar-based speaker independent acoustic-to-articulatory inversion methods based on Ghosh and Narayanan (2011) and one deep neural network (DNN) based approach based on Uria et al. (2011) to generate the estimated articulatory signals. It is worth noting that other types of acoustic-to-articulatory mapping, such as CCA (Bharadwaj et al., 2012; Arora and Livescu, 2013), Kernel CCA (Rudzicz, 2010; Arora and Livescu, 2013), Gaussian Mixture Model (GMM) (Ghosh and Narayanan, 2013; Ozbek et al., 2011; Özbek et al., 2012), attributes classification (Leung et al., 2004; Zhang et al., 2007; Siniscalchi et al., 2013, 2012) and articulatory phonological code (Zhuang et al., 2009), etc., could also be applied here. The reason to choose the exemplar-based speaker independent acoustic-to-articulatory inversion methods is that we can directly compare the performance against the real articulatory trajectories measurement to find out the gap which shows the potential for better speaker aware inversion techniques. Our future work includes investigating the effects of different speaker independent acoustic-to-articulatory mapping methods in terms of speaker verification performance with training data from multiple exemplar speakers, especially in the direction of highlighting inter-speaker variations.Although the inverted articulatory features are also generated from speech signals, we can show that adding this new information (articulation-acoustics mapping learned from the exemplar data) on top of MFCCs can still enhance the speaker verification performance. Theoretical support from machine learning fields is provided in Pechyony and Vapnik (2010), Vinyals et al. (2012). Previously, this concatenation based speech-articulatory feature level fusion has been reported to increase the performance of ASR (Toutios and Margaritis, 2003; King et al., 2007; Ghosh and Narayanan, 2011b) significantly. In this work, we show that by utilizing information from both speech and inverted articulation, the equal error rate of speaker verification system is also reduced.Furthermore, we demonstrate that the articulatory level constraints introduced by the inverted articulatory features also help the TDSV system to reject wrong password trials and therefore improve the performance. In Hébert (2008), text dependent speaker verification is defined as a speaker verification task in which the lexicon used in the test phase is a subset of the lexicon pronounced by the speaker during the enrollment. By constraining the text of enrollment and testing utterances to be the same (verbal password), higher accuracy with shorter utterances can be achieved (Larcher et al., 2014b; Novoselov et al., 2014; Kenny et al., 2014; Variani et al., 2014). In Larcher et al. (2014b), the hierarchical multi-layer acoustic model (HiLAM) was shown to outperform the conventional i-vector approach, since the latter does not explicitly take advantage of the temporal structure of the text dependent speech utterances. However, the HiLAM approach requires composing a specific acoustic model for each known password text content which may not work well on accented speech, dialect or out-of-vocabulary words. In the present study, we enhance the robustness of the i-vector representation against variation in the lexicon contents by adding articulatory features into the generalized i-vector framework. The proposed feature level fusion approach reduces the error rate of those wrong lexicon trials. However, it can also make the system vulnerable to those trials where imposter speakers utter the same password as the target speaker. The solution in this work is to fuse the i-vector baseline and the proposed feature-level fusion system together at the score level to achieve performance improvement for all three types of trials.The remainder of the paper is organized as follows. The baseline and the proposed algorithms are explained in Section 2. The databases are described in Section 3. Experimental results and discussion are presented in Section 4. Conclusions and future work directions are provided in Section 5.

@&#CONCLUSIONS@&#
We propose a feature-level and score-level fusion approach by combining acoustic and (estimated) articulatory information for both text independent and text dependent speaker verification tasks. From a practical point of view, we study how to improve the speaker verification performance by combining the articulatory trajectories information characterizing the speech production. For the text independent speaker verification task, we find that concatenating articulatory features obtained from the measured speech production data with conventional MFCCs improves the performance dramatically. However, since access to the measured articulatory data is impractical for real world speaker verification applications, we also experimented with estimated articulatory features obtained using acoustic-to-articulatory inversion technique. We explore both the feature-level and score-level fusion methods and observe that the overall system performance is enhanced significantly. Since articulatory trajectories also contain speech content information, we also study the usage of inverted articulatory features in the text dependent speaker verification task. We demonstrate that the articulatory constraints introduced by the inverted articulatory features help to reject wrong password trials and improve the performance after score level fusion. Future work should include investigating the effects of different speaker independent acoustic-to-articulatory mapping methods in terms of speaker verification performance with training data from multiple exemplar speakers, especially in the direction of highlighting inter-speaker variations.