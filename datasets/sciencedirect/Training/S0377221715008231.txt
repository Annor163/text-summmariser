@&#MAIN-TITLE@&#
Customer-base analysis using repeated cross-sectional summary (RCSS) data

@&#HIGHLIGHTS@&#
We conduct customer base analysis using summaries of individual-level data.We use repeated cross-sectional summaries (RCSS), e.g., quarterly histograms.RCSS are easy to create, visualize, and distribute, and preserve privacyFour quarterly histograms are a good substitute for individual-level data.

@&#KEYPHRASES@&#
Customer-base analysis,Probability models,Data aggregation,Data privacy and security,Information loss,

@&#ABSTRACT@&#
We address a critical question that many firms are facing today: Can customer data be stored and analyzed in an easy-to-manage and scalable manner without significantly compromising the inferences that can be made about the customers’ transaction activity? We address this question in the context of customer-base analysis. A number of researchers have developed customer-base analysis models that perform very well given detailed individual-level data. We explore the possibility of estimating these models using aggregated data summaries alone, namely repeated cross-sectional summaries (RCSS) of the transaction data. Such summaries are easy to create, visualize, and distribute, irrespective of the size of the customer base. An added advantage of the RCSS data structure is that individual customers cannot be identified, which makes it desirable from a data privacy and security viewpoint as well. We focus on the widely used Pareto/NBD model and carry out a comprehensive simulation study covering a vast spectrum of market scenarios. We find that the RCSS format of four quarterly histograms serves as a suitable substitute for individual-level data. We confirm the results of the simulations on a real dataset of purchasing from an online fashion retailer.

@&#INTRODUCTION@&#
There is a rich literature in marketing on statistical models for customer-base analysis. These models use data on customers’ past transactions with the firm to make predictions about their future behavior, be it total purchasing by the whole customer base or individual-level predictions (conditional on the customer’s past behavior). These predictions can also be used to derive formal metrics of the expected future value to the firm of individual customers (such as customer lifetime value, or CLV) and the cohort (such as customer equity). Examples of this research include Abe (2009)Batislam, Denizel, and Filiztekin (2007)Buckinx and Poel (2005)Ekinci, Ülengin, Uray and Ülengin (2014)Fader and Hardie (2002)Fader, Hardie, and Lee (2005a, 2005b); Fader, Hardie, and Shang (2010)Jerath, Fader, and Hardie (2011)Schmittlein, Morrison, and Colombo (1987)Singh, Borle, and Jain (2009), and Wu and Chen (2000).These models have been developed assuming the analyst has ready access to the raw customer-level transaction data and has the resources to process it. However, reality is frequently different. In most organizations, model development and customer scoring activities are undertaken using relevant data extracted from the organization’s operational database(s), and there are a number of challenges associated with this process (Brobst, Collins, Kent, & Watzke, 2009).Such a replication of data is risky from a security perspective. Because of customers’ concerns regarding the privacy of their behavioral data — especially in the light of multiple data breaches at high-profile firms in recent years (eMarketer, 2014) — firms are wary of creating and distributing data files containing disaggregate individual-level customer activity data. In addition, policy makers all over the world (particularly in Europe) are progressively tightening data protection laws. Complying with these regulations complicates the process of transferring raw data to the analyst (Carey, 2009; Singleton, 2006), even to the extent of there being partial bans on transborder data flows.Furthermore, there can be a number of data access challenges. Within most firms, marketing neither owns nor controls customer data, relying instead on the IT department for access (Rackley, 2015; Teradata, 2015). Firms are typically organized into departmental silos leading to a wall between IT and the rest of the organization (Basu & Jarnagin, 2008), including marketing, which can make it very difficult for the marketing analyst to get access to the required customer-level transaction data. It is all-too-easy for IT to cite data security concerns or human resource costs as an excuse for turning down such requests.Finally there is the “too-much-data problem” (Burby, 2006) and its consequences. While the data they collect can run into petabytes, state-of-the-art technology allows firms to actively use only a small part of it (Kettenring, 2009). Davenport and Harris (2007) estimate that data availability is growing at a compounded rate of about 50% annually, but data utilization is growing at a compounded rate of only about 2% annually. As such, organizations are implementing data retention policies that dictate that “data should remain accessible up to a certain time, and afterwards be deleted permanently with no recoverable trace. Timely removal not only allows the enterprise to manage sensitive data in compliance with regulatory policies, but also reduces storage costs of ever-growing data” (Li, Singhal, Swaminathan, and Karp, 2012, p. 394). For example, Verizon maintains individual behavioral data on its customers (specifically, call records) for only one year (Nelson, 2015). Retaining customer transaction data for too short a period of time can mean that the analyst is unable to obtain the data required to develop the customer-base analysis models of interest.With these issues in mind, we ask the following question: Can customer data be stored and analyzed in a secure, easy-to-manage and scalable format that does not significantly compromise the inferences that can be made about customer activity? Working in a setting where we track the behavior of individual customers over time, we present a methodology that meets these objectives through a carefully designed data aggregation process.More specifically, with reference to Fig. 1a, it is typically assumed that we know the timing of each transaction for each customer (denoted by × on the individual time lines), along with its associated monetary value, etc. In some cases, the data may be stored in the firm’s databases in such a way that we do not know the timing of each transaction, only how many transactions occurred in each pre-specified time interval (as illustrated in Fig. 1b); such data are said to be interval-censored. Going further, individual-level interval-censored data can be summarized across individuals for each time interval, resulting in what we call repeated cross-sectional summary (RCSS) data (as illustrated in Fig. 1c). Each “summary” is a histogram and, for each histogram, the heights of the bars indicate the number of people in the database making0,1,2,⋯transactions in that period.The RCSS data structure has a number of attractive properties and provides an elegant solution to address the issues with disaggregate individual-level data that firms face (as discussed earlier). First, it is truly impossible to “reverse engineer” the behavior associated with any specific customer from the data summaries. This makes RCSS data an appealing option from a privacy-preservation and data security viewpoint. Second, RCSS data summaries are very easy to create and distribute, which make them amenable to being shared across departmental silos and even across company and country boundaries. This is especially true when compared to any data structure that involves sharing data for individual customers (even if it is summary data for each individual) — the size of the RCSS dataset is effectively independent of the number of customers. As we go from summarizing the behavior of one thousand customers to one million customers, the only thing that changes is the scaling of the y-axis (i.e., the heights of the bars of the summary histograms in Fig. 1c). Finally, the RCSS data structure naturally lends itself as part of a good data retention policy as its cost of storage is nearly zero, and it overcomes the analysis-related problems associated with a rolling data retention time frame.Of course, the downside of such a data summary is obvious: it is no longer possible to see the timing of the individual-level transactions. For instance, we have no way of knowing that the customer with ID=1 above made 2, 0, 3 and 1 transactions in periods 1, 2, 3 and 4, respectively. This potentially reduces the information content of the data as we seek to estimate the parameters of the model intended to characterize customer buying behavior.Some marketing managers and analysts already use RCSS data as they seek to describe the behavior of their customers and compute basic performance metrics. For example, the Tuscan Lifestyles case (Mason, 2003) features a marketing director using summaries of customer behavior in the form of five annual histograms (of the number of orders per year) for the purpose of computing the value of a new customer. There is some evidence that such data can be used as a basis for more sophisticated analyses, with Fader, Hardie, and Jerath (2007) illustrating how to estimate the parameters of the Pareto/NBD model using the RCSS data provided in the Tuscan Lifestyles case.While they provide an initial “proof of concept” for RCSS-based analyses, Fader et al. (2007) do not address three issues that are critical for any analyst contemplating the use of such a data structure. First, they do not demonstrate that it is an adequate substitute for individual-level data. For example, are we able to draw the same model-based inferences we would make if we had access to the individual-level data? Second, they do not give any guidance for the creation of the repeated cross-sectional summaries of the transaction data. (They simply used the five annual histograms reported in Mason (2003).) For example, if we have a year of transaction data, can we fit the model using just one histogram that summarizes the year’s transactions by the customer base, or is it better to use two six-monthly histograms, or four quarterly histograms? Such guidance is needed for any analyst considering the use of such a data structure. Third, the Tuscan Lifestyles case is purely a “backward-looking” analysis: it takes an historical dataset and “chops it up” into a RCSS format. Practitioners also need “forward-looking” advice; that is, how to create RCSS data “on the fly” as new transactions are being recorded. In this paper, we develop and explore this kind of analysis for simulated and real datasets.33We note that the term “transaction” used above is very general. To list a few examples from practical situations, it can refer to purchasing from the firm through online and/or offline channels, visits to the firm’s website, instances of content viewing on different media channels, advertisement exposures over time, and so on. We use the terms “transaction” and “purchase” interchangeably.With this background in mind, we have two specifc research questions: How much information is “lost” when RCSS data are used instead of the individual-level data, and how many cross-sections are required to minimize information loss? We will answer these questions using a simulation study. Any such study must be based on a model of buyer behavior and we will make use of the Pareto/NBD model of buyer behavior in noncontractual settings (Schmittlein et al., 1987).The rest of the paper is organized as follows. In the next section, we briefly review the Pareto/NBD model and present the key results relevant to our study. We then present a comprehensive simulation study that seeks to answer our research questions. This is followed by an analysis of a real dataset from the online fashion retailer Bonobos, which confirms the results of our simulations. We conclude with some reflections on the generalizability of our findings, possible directions for future research, and the managerial implications of using RCSS data.The Pareto/NBD (Schmittlein et al., 1987) is the seminal model for customer-base analysis in noncontractual settings. We know from the empirical validations presented in Schmittlein and Peterson (1994) and Fader, Hardie, and Lee (2005b), amongst others, that its predictive performance is impressive. Applications of this model include the work of Reinartz and Kumar (2000, 2003) on customer profitability, Hopmann and Thede (2005) on churn prediction, and Wübben and Wangenheim (2008) and Huang (2012) on managerial heuristics for customer-base analysis. It is the “gold standard” against which any alternative models are compared and is therefore an appropriate model on which to base our simulation study.The model is based on the following assumptions. First, a customer’s relationship with the firm has two phases: he is “alive” for an unobserved period of time, then becomes permanently inactive (i.e., “dies”). While alive, the customer’s transaction activity is characterized by the NBD model (i.e., times between a customer’s transactions are iid exponential with heterogeneity in the transaction rates across customers captured by a gamma distribution with shape parameter r and scale parameter α). The customer’s unobserved “lifetime” (after which he is viewed as being dead) is treated as-if random, characterized by another exponential distribution; heterogeneity in the underlying death rate across customers is assumed to follow a gamma distribution with shape parameter s and scale parameter β. Noting that a gamma mixture of exponentials is also known as the Pareto (of the second kind) distribution, the resulting model of buyer behavior is called the Pareto/NBD. This model falls in the category of latent-attrition/“buy till you die” models in which every customer is assumed to transact until they “die” at some unobserved (and unobservable) point in time.Given “full-information” data (as illustrated in Fig. 1a), it turns out that we do not need to know the timing of each transaction; the number of transactions made in the period(0,T]and the time of the last transaction (i.e., frequency and recency) are sufficient statistics. For a sample of I customers, where customer i had xitransactions in the period(0,Ti]with the last transaction occurring attxi,the four Pareto/NBD model parameters (r, α, s, β) are estimated by maximizing the sample log-likelihood function(1)LL(r,α,s,β|data)=∑i=1Iln[L(r,α,s,β|xi,txi,Ti)],where the expression for L(·) is given in the Appendix, Eq. (A.1).This standard approach to parameter estimation, which assumes that we have the recency and frequency data for each and every customer, is of no use given RCSS data. However, all we need to do is re-derive the model likelihood function for the data structure at hand. Fader et al. (2007) demonstrated how this can be done; we now extend their approach.Fader, Hardie, and Jerath (2006) derive an expression forP(X(Tj−1,Tj)=x|r,α,s,β),which is the probability of observing x transactions in the time interval(Tj−1,Tj],whereTj−1≥0. (The associated function is given in the Appendix, Eq. (A.2)). This quantity lies at the heart of any effort to estimate the parameters of the Pareto/NBD model using RCSS data.Suppose the calibration period(0,T]is split into J contiguous periods:(0,T1],(T1,T2],…,(TJ−1,TJ]. (These periods do not have to be of equal length, but in practice we would expect them to be.) For each period, we determine how many people made 0, 1, 2, 3, … transactions, giving us a histogram of transactions (as illustrated in Fig. 1c). Let us assume each histogram is right censored and has bins0,1,2,…,z−1,z+(i.e., the number of individuals who did not make a transaction, the number of individuals who transacted exactly once, exactly twice,…,exactlyz−1times, and z or more times). Let n(j, x) denote the number of people making x transactions in the jth time interval(Tj−1,Tj]andn(j,z+)denote the number of people making z or more transactions in this time interval.The four Pareto/NBD model parameters (r, α, s, β) are estimated by maximizing the following log-likelihood function for RCSS data spanning J consecutive periods, with the first period starting atT0=0:(2)LL(r,α,s,β|data)=∑j=1J{∑x=0z−1n(j,x)ln(P(X(Tj−1,Tj)=x|r,α,s,β))+n(j,z+)ln(P(X(Tj−1,Tj)≥z|r,α,s,β))}.Note that this is not a function of I; computation times are independent of the number of customers in the dataset. As such, this data structure is inherently scalable.Once the parameters of the Pareto/NBD model are estimated, we can compute a number of different measures of interest to management (Schmittlein et al., 1987). One such measure is customer lifetime value, which is the net present value of the future cashflows associated with a customer. The general explicit formula for the computation of customer lifetime value isE(CLV)=∫0∞E[v(t)]S(t)d(t)dt,where v(t) is the net cashflow associated with the customer at time t, S(t) is the survivor function (i.e., the probability that the customer has remained alive to at least time t), and d(t) is a discount factor that reflects the present value of money received at time t. Following Fader et al. (2005b), if we assume a constant net cashflow per transaction ofv¯,we havev(t)=v¯t(t),where t(t) is the transaction rate at time t, and we haveE(CLV)=v¯∫0∞E[t(t)]S(t)d(t)dt.The solution to the integral is called the Discounted Expected Transactions (DET) of the individual. For a just-acquired customer, DET measures the present value to the firm of all future transactions by the customer (accounting for the transactions while alive, and the death processes), with transactions at a future time point appropriately discounted to obtain their present values. Multiplying byv¯gives us an estimate of expected CLV.When the flow of transactions is characterized by the Pareto/NBD model, DET of a randomly chosen customer is given by(3)DET(r,α,s,β,δ)=rαβΨ(1,2−s;βδ),where δ is the continuous compound rate of interest and Ψ(·) is the confluent hypergeometric function of the second kind. (The derivation of this quantity is presented in the Appendix.) Given the central importance of CLV (and thus DET) as both a managerially interesting metric as well as a long-run output of the Pareto/NBD model, we will use it in the analyses that follow.We approach the question of how best to create the RCSS data from two directions. First, we undertake a “backward-looking” analysis in which we consider how finely we need to partition a fixed period of time into cross-sectional summaries before model performance and parameter recovery stabilizes (if at all). For example, suppose we have 52 weeks of purchasing data: should we use one 52-week histogram, or two 26-week histograms, or three 17.3-week histograms,…,or six 8.7-week histograms? Second, we undertake a “forward-looking” analysis in which we consider how many cross-sectional summaries (each of a fixed length) are required for a newly emerging dataset before model performance and parameter recovery stabilizes.Consider the case of a backward-looking analysis where the analyst already has 52 weeks of data and has to decide how many cross-sectional data summaries he should construct (one 52-week histogram, two 26-week histograms, etc.). In the case of one 52-week histogram, the recovery of the true underlying parameters (especially those associated with the death process) will be weak because we have grouped together all the data available for one year. With such aggregation we lose the ability to track any time trends across the 52-week period that help identify the customer death process. Many different combinations of the parameters may lead to very similar 52-week histograms. For example, a large number of zero buyers could be due to a low underlying purchase rate while alive (i.e., r/α is small) or the fact that a large number of existing customers died early on in the year before they got around to making any purchases (i.e., s/β is large). Now consider the case where we have two 26-week histograms. In this case, the ability to pin down the correct model parameters is better, since we can start to separate out patterns related to the death process through the growth in the number of zeroes from the first histogram to the second one. But it is still difficult to describe the nature of this growth pattern over time.As the number of histograms increases, we can track the underlying dynamics in purchasing patterns more closely and we have more information for recovering the true parameters. For instance, in the four histograms in Fig. 1c, we can see that the number of customers making zero purchases increases over time and the number of customers making one, two or three purchases decreases over time, which is a strong indication of customer death. If we used one histogram for this length of time, we would not be able to make any such inferences about customer death over time. If we used two histograms for this length of time, we would be able to make such inferences but they would be rather coarse.There may be limits to this logic if we take it too far. As we “chop” the data into a greater number of histograms (each covering a smaller number of weeks), we may lose the meaningful information content from each histogram. In the extreme case of, say, weekly histograms, we would have an enormous number of 0’s and a very limited number of 1’s in every histogram. Thus, we do not want to go too far in creating histograms to summarize the data.Similar arguments can be extended to a forward-looking analysis, except that the problem here is even tougher. In this case, the analyst is constructing, say, quarterly histograms “on the fly” and has to decide how many quarters of data are sufficient to yield stable parameter estimation. As before, one histogram for one quarter is not expected to capture the purchasing behavior particularly well — the data are for a very short time period and we have only one histogram which makes it difficult to track any underlying time trends. If we use two histograms for the first two quarters, we can begin to gain some insight into the (latent) attrition patterns in the cohort, but the total period of time covered is still short and it will be hard to discern trends in the death process as well as heterogeneity in the purchasing while alive process. Histograms for three or four quarters start to capture behavior over a considerable length of time and are expected to further improve model performance. At some point, the incremental value of waiting for additional histograms will be quite limited for the purposes of estimating the model parameters.For both the backward- and forward-looking analyses we need to summarize how much information is “lost” when RCSS data are used in place of the individual-level data. We do so by using a likelihood-based distortion metric, which is defined in the following manner. LetXdenote the individual-level data andYhthe associated RCSS data with h histograms. We first estimate the model parameters using the individual-level data and obtain the maximum value of the log-likelihood function in (1); for notational convenience, we denote the associated estimates of r, α, s and β by the parameter vectorPXand the corresponding value of the log-likelihood function byLLPX. We then estimate the model parameters using a particular configuration of the RCSS data with h histograms by maximizing (2) and denote the associated estimates of r, α, s and β by the parameter vectorPYh. UsingPYh,we calculate the value of (1) and denote this byLLPYh. We note that it will always be the case thatLLPYh≤LLPX. Interpreting the value of the likelihood function as a measure of the agreement of the model with the data, we note thatLLPX−LLPYhcaptures the reduction in agreement associated with parameter vectorPYhrelative to the parameter vectorPXfor dataX. We scale this and multiply by 100 to obtain the distortion metric(4)D(PX,PYh)=|LLPX−LLPYhLLPX|×100=|1−LLPYhLLPX|×100,which gives a percentage measure of the difference in the agreement between the model and the data for the parameter vectorPYhrelative to the parameter vectorPX. The distortion metric D( ·, ·) is similar in spirit to the metric of model comparison known as U2 or the “likelihood ratio index,” which is based on a proportional deviation between the log-likelihoods of two models (Hauser, 1978; McFadden, 1974).The closerD(PX,PYh)is to 0, the closer is the performance of the set of parametersPYhto the set of parametersPXin terms of explaining the individual-level dataX. Drawing on our earlier discussion, as h increases, we expectD(PX,PYh)to become progressively smaller (up to a point).We turn to an extensive simulation study that helps us answer the following questions: Can models built using RCSS data match the performance of those estimated using individual-level data? If yes, how many histograms are required, both for backward-looking and forward-looking analyses?As previously noted, the Pareto/NBD model has four parameters: r and α, which capture the heterogeneity in the latent purchase rates in the cohort, and s and β, which capture the heterogeneity in the latent death rates in the cohort. For the purchasing process, as α increases for a fixed value of r, the mean purchase rate for the cohort decreases, and as r decreases for a fixed value of α, the heterogeneity in purchase rates in the cohort increases. For the death process, as β increases for a fixed value of s, the median lifetime of customers in the cohort increases, and as s decreases for a fixed value of β, the heterogeneity in lifetimes increases. Hence, by manipulating these parameters we can construct a number of cohorts, each of which reflects a different pattern of customer purchasing and latent attrition behaviors.We vary each of the four parameters at three levels, thus generating34=81different “worlds.” For the purchase process, we assign to r the values 0.5, 1 and 1.5 and we assign to α the values 5, 10 and 15; this results in average weekly purchase rates (while alive) ranging from 0.03 to 0.30. For the death process, we assign to s the values 0.5, 1 and 1.5, and to β the values 5, 10 and 15; this results in median lifetimes ranging from 2.9 to 45 weeks. (These parameter values are consistent with the range of values seen in many prior papers.) In the least active of the 81 worlds, the median customer life is a mere 2.9 weeks and the average customer is expected to make only 1.7 purchases if he lives for 52 weeks. This lower bound in our simulation is indeed a very low-activity world. In the most active of the 81 worlds, the median lifetime is 45 weeks and the average customer is expected to make 15.6 purchases if he lives for 52 weeks. This upper bound represents a very high-activity world, with both the metrics (median lifetime and average number of purchases while alive) being a full order of magnitude larger than in the lower bound. In terms of observable quantities, annual customer-base penetration (P(X(0, 52) > 0) × 100%) ranges from 13.1% to 85.7%, with an average of 47.9%, while average annual sales per member of the customer base (E[(X(0, 52)]) ranges from 0.2 to 9.9, with an average of 2.4.For each of the 81 worlds, we simulate 104 weeks of individual-level data for 25 synthetic cohorts of 10,000 customers each. We use the first 52 weeks of data as the calibration sample and the last 52 weeks as the holdout sample.44We coded the simulations in the R programming language (Version 3.0.1). We ran the simulations on a standard Windows PC. The estimation time per synthetic cohort (of 10,000 customers) was under 5 seconds.As explained above, using a 52-week data window provides a wide variation across cohorts in the purchasing and death processes. We now proceed to the backward-looking and forward-looking analyses.In the backward-looking analysis, the analyst already has individual-level data for 52 weeks and wants to convert it into the RCSS format. The question is: How many data summaries should he construct? Is there a sufficient or optimal level of aggregation so that the resulting summaries recover the underlying story of customer purchasing dynamics (i.e., recover the model parameters)?To answer these questions, for each of our 81 worlds, for each of the 25 simulation runs, we consider one 52-week histogram, two 26-week histograms, three 17.3-week histograms, four 13-week histograms, five 10.4-week histograms and six 8.7-week histograms. Each histogram has eleven “number of purchases” bins:0,1,…,9,10+.55We also ran the complete simulation study with different numbers of bins, specifically, with the histograms ending at13+,16+,19+,22+and25+. In every case, we obtained virtually identical results for all metrics considered.For each of the configurations of the RCSS data (i.e., the number of histograms), we calculate the distortion metric defined in (4) and determine the RCSS configuration that is an acceptable substitute for individual-level data. (Note that the distortion metric used to determine the appropriate RCSS configuration is based on a measure of in-sample fit.) Following this, we show that out-of-sample performance, parameter recovery, and recovery of DET are also very good for the appropriate RCSS configuration.To illustrate the patterns in the results, we present in Table 1the values of the distortion metric,D(PX,PYh),for h ∈ {1, 2, 3, 4, 5, 6} averaged across 25 simulation runs, for five worlds. These worlds are chosen to represent simulated scenarios with a wide range of mean purchase rates and median lifetimes for the cohort. They are:•World MM: specified byr=0.5,α=5,s=0.5,β=5,with a medium mean purchase rate (0.10 purchases per week) and a medium median lifetime (15 weeks);World LL: specified byr=0.5,α=15,s=1.5,β=5,with the lowest mean purchase rate (0.03 purchases per week) and the lowest median lifetime (2.9 weeks);World LH: specified byr=0.5,α=15,s=0.5,β=15,with the lowest mean purchase rate (0.03 purchases per week) and the highest median lifetime (45 weeks);World HL: specified byr=1.5,α=5,s=1.5,β=5,with the highest mean purchase rate (0.30 purchases per week) and the lowest median lifetime (2.9 weeks);World HH: specified byr=1.5,α=5,s=0.5,β=15,with the highest mean purchase rate (0.30 purchases per week) and the highest median lifetime (45 weeks).The numbers in Table 1 show that, for all five worlds, there is a substantial improvement in the value of the distortion metric as the number of histograms increases up to four, but beyond that point the improvements are minuscule. The plot in Fig. 2a shows this pattern visually for World MM. Furthermore, for all five worlds, the value of the distortion metric when four histograms are used is very small. We present in Table 2statistics on the distribution of the distortion metric across all 81 worlds (where the value used for each world is the average across the 25 runs for that world). Interestingly, the patterns of Table 1 are repeated almost identically in Table 2. The graph in Fig. 2b, which plots the values of the distortion metric for different h averaged across the 81 worlds, confirms this. (Note that the scales on the y-axes are different in Fig. 2a and b.)Given these results, what is an appropriate value of h? We take a conservative approach to determine this value. The third column in Table 2 presents the maximum value of the distortion metric across all 81 worlds for different values of h; for notational simplicity, denote this by Dmax, hfor different values of h. Now consider a threshold D*. We choose the valueh=h*for whichDmax,h*−1≥D*≥Dmax,h*.Different values of D* will give different values of h*. For instance,D*=0.025%givesh*=4,whileD*=0.05%givesh*=3. The appropriate value of D* is at the discretion of the analyst. For the purposes of this paper, we make yet another conservative choice and chooseD*=0.025%and, thereforeh*=4; by any standards, this value of D* is very small and corresponds to excellent performance by the associated RCSS configuration.We also note that the values of all statistics of the distortion metrics stabilize beyond four histograms. This indicates that the RCSS configuration with four histograms is appropriate for all 81 worlds.The above analysis is essentially based on a comparison of in-sample performance of RCSS and individual-level data, and determines four histograms as an appropriate substitute for individual-level data. We now show that the four-histogram RCSS configuration gives performance comparable to individual-level data on other important metrics as well. We consider measures related to parameter recovery, out-of-sample predictions, and the recovery of a forward-looking managerially relevant metric related to future value (DET). Note that since we have already determined the four-histogram configuration as the appropriate one, we present the above metrics only for this configuration.A natural (and strong) test of the performance of the RCSS approach is to compare the associated parameter estimates with those obtained using the individual-level data (i.e., parameter recovery). For each world, we know the original data-generating parameters and, for each of the 25 runs, we know the parameters recovered using the individual-level data and the parameters recovered using the four-histogram RCSS configuration. For each parameter we compute the root mean square error (RMSE) across the 25 simulation runs for that world. A small value of RMSE indicates that the parameter recovery is good, with zero denoting perfect recovery. The RMSE values for parameter recovery in Worlds MM, LL, LH, HL and HH are provided in Table 3. These RMSE values for the individual-level analysis and the four-histogram RCSS analysis are comparable, and show that parameter recovery is good for both, though it is, understandably, less accurate for the RCSS case.To obtain an idea of accuracy in parameter recovery across the 81 worlds, we present the RMSE values for the four parameters averaged across all 81 worlds. For the individual-level analysis, these average RMSE values for r, α, s and β are 0.044, 0.451, 0.061 and 1.293, respectively, while for the four-histogram RCSS configuration, the average RMSE values are 0.069, 0.565, 0.100 and 2.332, respectively. As for the five worlds discussed above, average RMSE values across the 81 worlds are comparable for the individual-level analysis and the four-histogram RCSS analysis, and show that parameter recovery is good for both. However, as may be expected, it is less accurate for the RCSS case. Furthermore, recovery for the parameters of the death process (s and β) is less accurate than recovery for the parameters of the purchasing while alive process (r and α).A second, more practical, way to understand how close the RCSS performance is to that associated with the individual-level data is to examine the quality of the forecasts created by the model (i.e., out-of-sample performance). Using the data from each simulation run, we construct the true histogram of purchases for the weeks 53–104 holdout period (i.e., one histogram for this 52-week period). We then generate the expected histogram of purchases for the same time period using the individual-level parameters and the RCSS parameters. The out-of-sample performance is based on how close the predicted histograms are to the true histogram. The metric we use to evaluate how closely a predicted histogram matches the true histogram is the standard χ2 goodness-of-fit test statistic computed using these two histograms. A smaller value of this statistic corresponds to a better match between the true histogram and the predicted histogram, zero denoting a perfect match.66When assessing the in-sample “goodness of fit” of a model, a p-value is usually reported, which depends on the number of parameters used to estimate the model. In this case, however, we are comparing out-of-sample histograms: we report the χ2 statistic only as a measure of the match between the original and predicted out-of-sample histograms, and not as a measure of the “goodness of fit” of the model. No parameters (or “degrees of freedom”) are associated with the holdout period, so it does not make sense to compute p-values here.The χ2 statistics for Worlds MM, LL, LH, HL and HH are provided in Table 4. Across the 81 worlds, the average χ2 statistic from the individual-level-data parameters has the value 9.8, and the χ2 statistic from the four-histogram RCSS configuration parameters has the value 10.7. The values of the χ2 statistics from the individual-level estimation and the four-histogram RCSS estimation are close to each other; RCSS performance compares favorably to individual-level performance.77Other metrics can also be used for evaluating out-of-sample performance. For instance, using RMSE between the predicted and the original out-of-sample histograms yields the same conclusions regarding out-of-sample performance in all cases.Looking beyond one year, we now examine how well the RCSS method compares with individual-level data by comparing estimates of lifetime value, as captured by the Discounted Expected Transactions (DET) measure (defined in (3)) computed using the two sets of parameters. For our DET calculations, we use an annual discount rate of 15%, which corresponds to a continuously compounded rate ofδ=0.0027. The DET values for Worlds MM, LL, LH, HL and HH are presented in Table 5. (The DET number for any given world is obtained by averaging the DET numbers from each of the 25 simulation runs for that world.) We see that the DET values from both the individual-level analysis and the RCSS analysis with four histograms are very close. Across the 81 worlds, the average percentage deviation in the DET obtained from the individual-level parameters and the four-histogram RCSS parameters is 0.6%.This DET comparison is not only very favorable for the RCSS approach but we also see it as the most diagnostic of the various analyses conducted in this section. Inaccuracies in parameter recovery can counterbalance each other, e.g., a high value of the r parameter can be nullified by a similarly high value of the α parameter, if the overall mean of the transaction rate (r/α) is unaffected. Likewise, a year-long forecast period is useful, but it is dominated by the infinite-horizon nature of the DET calculation. Thus, the very close matches seen in Table 5 are a very good indication about the validity of the RCSS estimation approach.As we complete our detailed investigation of the backward-looking analysis, we offer a final comment on our assertion that four 13-week histograms are generally recommended for the RCSS method. This recommendation is a conservative one; looking across the 81 worlds, there are a number of scenarios in which the three-histogram configuration would be quite satisfactory as well. We used a variety of data-mining procedures to try to discover common characteristics of worlds that tend to support three versus four histograms, but did not come up with anything sufficiently systematic or robust. Thus for clarity and convenience, we stick with our global recommendation of four histograms, but we encourage future researchers to look more carefully for conditions under which alternative configurations may be preferable.In the forward-looking analysis, imagine that the analyst receives data in the form of quarterly histograms. The key question is: How many quarterly histograms are needed before one can confidently uncover the story behind the purchasing process of the cohort, if at all? To answer this question, for each of our 81 worlds, we consider one 13-week histogram, two 13-week histograms, three 13-week histograms and four 13-week histograms.88As in the backward-looking analysis, we have eleven “number of purchases” bins (0,1,…,9,10+) for each histogram.As noted earlier, this is quite a different test compared to the preceding backward-looking analysis because we are changing the amount of data we use instead of varying the summary period for a fixed dataset. We should be able to better uncover the latent buying and dropout processes with more histograms, but it is possible that we can do this without all four quarters of data — the marginal improvement in model fit after, say, three quarterly histograms might be small.In order to determine the number of RCSS histograms that adequately serve as a substitute for the individual-level data, we analyze the values of the distortion metricD(PX,PYh),for h ∈ {1, 2, 3, 4}. To illustrate the patterns in the results, we present in Table 6these values for Worlds MM, LL, LH, HL and HH, as defined earlier, averaged across the 25 simulation runs of each world. In Table 7, we present statistics on the distribution of the distortion metrics across all 81 worlds (where the value used for each world is the average across the 25 runs of that world). Note that in the forward-looking analysis, we cannot have more than four quarterly histograms for 52 weeks of customer data, while in the backward-looking analysis we could go beyond four histograms by reducing the time interval covered by each histogram.As in the backward-looking analysis, these numbers show that there is a significant drop in the value of the distortion metric as we increase the number of histograms, and the values stabilize for large-enough h. To determine the RCSS configuration that serves as an appropriate substitute for individual-level data, we use the same procedure as in the backward-looking case, i.e., we choose a value for the distortion threshold D*, and findh=h*for whichDmax,h*−1≥D*≥Dmax,h*(where Dmax, hare the values in the third column of Table 7).D*=0.025%givesh*=4; interestingly, this value for h* is the same as in the backward-looking case.We note, once again, that our choice of four histograms is a conservative one, and the three-histogram RCSS configuration would be satisfactory (note thatD*=0.05%givesh*=3). Using three quarterly histograms would imply that we can do without even using all four quarters of data. However, to be on the safe side when making any claims, we choose the four-histogram RCSS configuration.For the other metrics for the four-histogram RCSS configuration (i.e., out-of-sample predictive accuracy, parameter recovery and recovery of managerially relevant metrics), we refer the reader to the previous section on backward-looking analysis. This is because the four-histogram RCSS configuration for the forward-looking analysis is exactly the same as the four-histogram RCSS configuration for the backward-looking analysis, which implies that the values of interest for these metrics, and thus the conclusions that we draw from them, are the same as those discussed in Section 3.2.We now replicate our analysis on a dataset from Bonobos, a popular US online fashion retailer. This dataset has been used in Lee and Bell (2013). Beginning in October 2007, this dataset tracks the purchasing activity of 10,000 customers, starting with each customer’s first-ever purchase at Bonobos. To simplify the analysis process, and to maintain consistency with our simulation study, we use exactly 52 weeks of purchase data for each customer (starting from the time of each customer’s first-ever purchase with the company).First, we fit the Pareto/NBD model on 52 weeks of individual-level data. Next, we conduct a backward-looking analysis using RCSS data. For this, we construct one 52-week histogram, two 26-week histograms,…,and six 8.7-week histograms; Table 8a shows the values of the distortion metric,D(PX,PYh),for these RCSS configurations. We see that the pattern for the distortion metric found in the simulations is found in this dataset as well. (There is a slight hint of degradation as we get to six histograms, suggesting that we are starting to “chop” the data into too many histograms and are therefore losing the meaningful information content from each histogram.) Following this, we conduct a forward-looking analysis using RCSS data. For this, we construct one 13-week histogram, two 13-week histograms, three 13-week histograms, and four 13-week histograms; Table 8b shows the values of the distortion metric for these RCSS configurations. Once again, we observe the same pattern for the distortion metric as in the simulations.The recommendation from the simulation study is to use four RCSS histograms. Indeed, we see here from Tables 8a and b that the distortion metric has a very small value for the RCSS configuration with four histograms, and it is approximately the same as the threshold value that we had used in our simulation. Furthermore, we show in Table 8c that the parameter estimates associated with the individual-level data are close to those associated with the RCSS data with four 13-week histograms. For a randomly chosen customer from this cohort, the DET values obtained from the individual-level data and the RCSS data with four 13-week histograms are 3.976 and 3.875, respectively, which implies a difference of only 2.5%.We note that not all the parameter values obtained for the Bonobos dataset lie within the bounds of the simulation study in Section 3. Therefore, as a robustness check, we run a standalone simulation for the “world” represented by the parameter valuesr=0.887,α=28.784,s=0.241,β=2.241,exactly as per the procedure for each world described in Section 3. We find that all the results from the simulation study are valid for the world represented by the parameter values for the Bonobos dataset (specifically, the performance from four 13-week RCSS histograms is comparable to the performance from the individual-level data). More details of this simulation are available on request.This brief analysis shows that the patterns we find for in-sample fit and parameter recovery from the simulation apply quite well to the “real world” Bonobos dataset. This is strong indication that our simulation results are practical and robust.

@&#CONCLUSIONS@&#
