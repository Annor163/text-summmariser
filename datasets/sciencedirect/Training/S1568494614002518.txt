@&#MAIN-TITLE@&#
Uncertainty treatment in expert information systems for maintenance policy assessment

@&#HIGHLIGHTS@&#
We know the stochastic model of the degradation process affecting a component.Its parameters are poorly known, and only via expert judgment.We use possibility distributions to represent the information retrieved.We develop a method based on Dempster–Shafer Theory of Evidence and Fuzzy Random Variables to propagate the uncertainty.The method provides the plausibility and belief distributions of these indicators.

@&#KEYPHRASES@&#
Maintenance,Uncertainty,Fuzzy Random Variables (FRVs),Possibility Theory (PT),Dempster–Shafer Theory of Evidence (DSTE),

@&#ABSTRACT@&#
This paper proposes a framework based on the Dempster–Shafer Theory of Evidence (DSTE), Possibility Theory (PT) and Fuzzy Random Variables (FRVs) to represent expert knowledge and propagate uncertainty through models. An example of application is given with reference to a check valve of a turbo-pump lubricating system in a Nuclear Power Plant, which is degrading due to mechanical fatigue and undergoes condition-based maintenance interventions. The component degradation-failure model used to evaluate the performance of the maintenance policy contains parameters subject to epistemic uncertainty.vector of the O output variableslink between the vector of the input uncertain variablesY_and that of the output variablesZ_input uncertain variablesCDF of YjVector of the parameters of the CDFFYj(yj;θj)of variable YjGeneric interval provided by the expert for parameter θGeneric confidence level associated to Aii-th interval provided by the expert for the p-th parameter of the j-th input variable YjConfidence level associated toAij,pp-th parameter of the j-th random variableGeneric uncertain parameterGeneric value of θNecessity measure associated to the set APossibility measure associated to the set APossibility function of parameter θsample a vector for variable Yjmade of NTuniform random numbers in [0,1[Random interval of Yjcorresponding to the random number{ujω}, using theαi-cut[θ_j,θ¯j]αi={[θ_j,1,θ¯j,1]αi,...,[θ_j,Mj,θ¯j,Mj]αi}αi–cut of θjSmallest value of the Z0component of g, within the intervals[y_jω,y¯jω]αi,j=1,2,⋯,kLargest value of the Z0component of g, within the intervals[y_jω,y¯jω]αi,j=1,2,⋯,kPlausibility measure of set ABelief measure of set A

@&#INTRODUCTION@&#
Processing of uncertainty is crucial in industrial applications and consequently in decision making processes [1]. In practice, it is often convenient to distinguish uncertainty due to the inherent variability of the phenomena of interest from that due to lack of precise knowledge [2]. The former type is referred to as aleatory, irreducible, stochastic or random uncertainty and describes the inherent variation associated with the physical system or the environment, the latter is referred to as epistemic, subjective or reducible uncertainty, and relates to the lack of precise knowledge of quantities or processes of the system or the environment. Although probability theory is well suited to handle stochastic uncertainty due to variability, it has been argued that the probabilistic approach may have some limitations in the representation and treatment of epistemic uncertainty in situations of poor knowledge, since it tends to force assumptions which may not be justified by the available information [3]. For example, ignoring whether a value of a parameter is more or less probable than any other value within a given range does not justify assuming a uniform probability distribution, which is the less informative probability distribution according to both the Laplace principle of insufficient reason and the maximum entropy criterion [4].In this work, we consider alternative approaches to probability theory for the representation of epistemic uncertainty, such as Dempster–Shafer Theory of Evidence (DSTE) and Possibility Theory (PT). These approaches have been considered due to their ability in handling the uncertainty associated to the imprecise knowledge on the values of parameters used by expert information systems and for which reliable data are lacking. In this respect, it is worthy noticing that some research effort has been devoted to capture the relationships between DSTE, PT and probability theory, and a vivid research debate is still ongoing about the capability of probability theory in representing the epistemic uncertainty ([4–7]). For example, in Ref. [8], a new framework is proposed, which extends Bayesian Theory to perform probabilistic inference with uncertain evidence. The extension is based on an idealized view of inference in which observations are used to rule out possible valuations of the variables in a modeling space. On the contrary, in Ref. [9] probability is conceptualized at the ‘betting’ level where decisions are made, which is different from the ‘credal’ level, where we find the epistemic uncertainty we are dealing with in this work. A pignistic transformation is required to pass from the credal level to the betting level. In Ref. [9], the authors also provide a comparison between the Bayesian framework and the Transferable Belief Model (TBM), which highlights that they may lead to different results.The strength of DSTE and PT lies in their capability of representing the epistemic uncertainty in a way less committed than that offered by probability theory. PT has been embraced to tackle a number of interesting issues pertaining to different fields such as graph theory [10], database querying [11], diagnostics [12], data analysis [13] and classification [14], agricultural sciences [15], probabilistic risk assessment (e.g., [16,17]), etc. to cite a few. Analogously, applications of DSTE can be found in diverse domains such as signal and image processing [18], business decision making [19], pattern recognition [20], clustering [21], etc.In spite of the liveliness of the research in the field, it seems fair to say that the non-probabilistic treatment of uncertainty within soft computing methods has not yet been exhaustively investigated. After all, given the relative immaturity and small size of research community working on non-probabilistic approaches, it is hardly fair to expect that these are elaborated from soft methods to the same extent of that of probability theory [22]. In this respect, two main considerations can be done on the basis of the authors’ best knowledge:•There is no work in the literature which performs a comprehensive comparison of the main techniques to represent and propagate epistemic uncertainty together with aleatory uncertainty, from a practical, engineering point of view. For example, an interesting comparison of PT, DSTE and probability theory is provided in Ref. [23], where a simple case study is introduced as a workbench to highlight the differences among those approaches; however, also in that case the comparison is not complete, as neither (type 1 or 2) fuzzy theory nor Bayesian probability theory are considered. In conclusion, the issue of comparing the different frameworks is still open and future research effort will be spent by the authors in this direction. On the other side, while doing this, it is important to bear in mind that, quoting Smets [9]:“Uncertainty is a polymorphous phenomenon. There is a different mathematical model for each of its varieties. No single model fits all cases. The real problem when quantifying uncertainty is to recognize its nature and to select the appropriate model. The Bayesian model is only one of them. The Transferable Belief Model is also only one of them. Each has its own field of applicability. Neither is always better than the other”For example, in Ref. [24] a different technique has been proposed to cope with the maintenance assessment issue in the case in which a team of experts is available to provide the ill-defined parameters, whereas the method proposed in this work assumes that there is just one expert providing them.PT has never been applied in the context of maintenance modeling, which is the core of this paper.Maintenance is a key factor for safety, production, asset management and competitiveness. Establishing an optimal maintenance policy requires the availability of logic, mathematical and computational models for:(i)The evaluation of performance indicators characterizing a generic maintenance policy. Possible performance indicators are the production profit, the system mean availability, the maintenance costs, etc.The identification of the optimal maintenance intervention policy from the point of view of the identified performance indicators, while fulfilling constraints such as those regarding safety and regulatory requirements. In practice, this multi-objective optimization problem has to be faced in a situation in which some constraints and/or the objective functions are affected by uncertainty. To effectively tackle this problem, a number of approaches have been already propounded in the literature considering different framework for uncertainty representation: probability distributions in Refs. [25–27], fuzzy sets in Ref. [28] and [29], and plausibility and belief functions in Refs. [30,31].The present work aims at contributing to the above step (i) by developing a methodology for maintenance performance assessment that properly processes the involved uncertainties. More specifically, we consider a situation in which:•a stochastic model of the life of the component of interest, in terms of degradation process, failure behavior and maintenance interventions is known without any uncertainty. This is, for example, the case for the degradation process ‘fatigue’ which has been successfully modeled by means of gamma processes [32], Weibull distributions [33], Paris–Erdogan law [34], etc.The model of the component's behavior depends on a number of ill-defined parameters. With reference to the example of fatigue degradation, the gamma process, Weibull distribution and Paris–Erdogan law depend on parameters whose values are usually not precisely known. Moreover, knowledge of other model parameters such as those describing the maintenance effectiveness (e.g., the improvement of the component degradation), duration and cost may also be imprecise. This framework of analysis where the aleatory and epistemic components of the uncertainty are separated into two hierarchical levels is often referred to as ‘level 2’ approach or setting [41].Information about the ill-defined parameters is available only from experts; in particular, it is assumed that there is a single expert, who provides for every uncertain parameter a set of intervals, which contain its true value with different degrees of possibility.Although methods for a priori evaluating the performance of a maintenance policy while taking into account the aleatory uncertainty on the future behavior of the component of interest have been investigated in the literature (see Refs. [35–38] for surveys), only few works (e.g., [39]) tackle the maintenance policy performance assessment problem considering the epistemic uncertainty on the maintenance model parameters. In this work, the information elicited from the expert is described by means of possibility distributions and propagated through the model by resorting to a method that exploits the concept of FRVs [3,40] and DSTE.Notice that other techniques have been proposed in the literature to represent the imprecision in the expert qualitative statements. For example, type-2 fuzzy theory allows describing the uncertainty in the model parameters given by the expert by means of a fuzzy set, and also the uncertainty in the shape of its membership function, again by a fuzzy set [42]. This latter would introduce the ‘third-level’ uncertainty in our framework, which requires additional computational complexity to propagate the epistemic uncertainty described by the type-2 fuzzy set together with the aleatory uncertainty.The method proposed in this work is illustrated with reference to an exponential, non-repairable, binary component. A practical case study is shown with reference to the degradation model of a check valve of a turbo-pump lubricating system in a Nuclear Power Plant.The remainder of the paper is organized as follows. Section “Uncertainty setting” describes the method to represent and propagate the uncertainties. Section “Case Study” illustrates a case study, which is for reference firstly investigated assuming that there is no epistemic uncertainty affecting the parameters of the stochastic model. The FRV-based method is then applied in Section “Representation and propagation of the uncertainties in the considered case study” to this case study to treat epistemic uncertainty. Finally, a discussion on the pros and cons of the method, as emerged from its application to the case study, concludes the work. The main aspects of DSTE and PT are briefly recalled in Appendix 1 for completeness of the paper.Let us consider a modelZ_=g(Y_), whereZ_=(Z1,Z2,…,Z0)is the vector containing the O output variables of interest, and g(·) is a function that models howZ_depends on the k uncertain variablesYj,j=1,⋯,k, of vectorY_; the uncertainty on these variables is characterized by known probability distributionsFYj(yj;θj),j=1,2,⋯,k, whereθj={θj,1,...,θj,Mj}, are the vectors containing the hyper-parameters of the corresponding probability distributions. Also these parameters are uncertain and the information to characterize this uncertainty is drawn from an expert.As mentioned, information is elicited from an expert for estimating the parametersθj,j=1,...,k. The associated uncertainty is represented within the framework of PT, and propagated by means of the method based on the concept of FRVs. For the sake of clarity of the illustration, the treatment of uncertainty is described by ways of a simple case study concerning a non-repairable component whose state can only be either working or failed and whose Time To Failure (TTF) is exponentially distributed with uncertain failure rate λ. The mission time is T (taken equal to 105h in the numerical case study). Hence, in this reference example there is k=1 uncertain variable, i.e.,Y_=Y1=(TTF), described by the Cumulative Distribution Function (CDF)FTTF(ttf;λ)=1−e−λ⋅ttf, with M1=1 uncertain parameterθ1={λ}. The output vectorZ_contains only one variable: the portion D of the mission time in which the component is in a down state, i.e., unavailable. The function g that links TTF to D is given by:(1)D=g(TTF)=T−TTFTifTTF≤T0otherwiseThen, D is also a random variable, because it is a function of the random variable TTF. The range of variability of D is the interval [0,1], and its distribution, for a given value of the failure rate λ, is:(2)FD(d|λ)=P(D≤d|λ)=PT−TTFT≤d|λ=P(TTF≥T(1−d)|λ)=e−λT(1−d)where d represents the generic value taken by the variable D.Fig. 1shows the shape of this function for a value of the failure rateλ=10−5h−1. Notice that FD(0) (i.e., the probability that the component is always available during the mission time) is equal to e−λT, i.e., the probability that the component fails after T.Within the PT framework, for a generic uncertain parameter θ, an expert is asked to provide a set of n nested intervalsAi,i=1,…n,(A1⊆A2⊆...⊆An), which are believed to contain the true value of θ with a positive confidence level qi; this latter can be conveniently interpreted as the smallest (subjective) probability that the true value of the parameter θ lies within Ai(i.e.,P(ϑ∈Ai)≥qi). Alternatively, the interval Aican be seen as the smallest one whose probability of including the true value of θ is at least qi[44], for anyi=1,...,n. From the expert's point of view, qiis the portion of cases where θ∈Aifrom his/her experience [44]. To sum up, the expert provides a weighted family{(A1,q1),(A2,q2),....,(An,qn)}(for example, see Fig. 2(a)). Notice also that the value of the largest confidence level qnmay be smaller than 1, i.e., qn=1−ɛ, ɛ>0; this is equivalent to admitting that even the widest, safest interval contains some residual uncertainty (ɛ), i.e., it is assumed that the expert is not absolutely sure about his judgment [44].Finally, the inequalitiesq1≤q2≤...≤qnhold, due to the fact that qiof the interval Aiis necessarily smaller than qi+1 associated to Ai+1, ifAi⊆Ai+1, for any i=1,…, n−1.With reference to the simple case study of the exponential, non-repairable, binary component, let us suppose that the expert characterizes his/her knowledge about the value of the failure rate λ with the information summarized in Table 1.The Universe of Discourse (UoD), i.e., the interval of all the possible values of the failure rate is[0,∞[, where the lower bound (0) corresponds to an infallible component, whereas an infinite failure rate corresponds to a component that fails at t=0+. From Table 1, it appears that the expert provides the interval A1 that is believed to normally, unsurprisingly contain the true failure rate value, with confidence level q1=0.1, which represents the portion of cases where λ∈A1 from the expert's experience point of view. The interval A1=[9.9e−6h−1, 1.01e−5h−1] is ‘unsurprising’ in the sense that any intervalA1*of the same length of A1 would have been associated to an equal or smaller frequency of occurrence of the eventλ∈A1*. Obviously, the expert cannot be less confident that the true value of the failure rate belongs to intervals that include A1; thus, larger intervals are associated to larger confidence levels. In particular, A5=[8e−6h−1,1.2e−5h−1] is the interval which λ.Fig. 2(a) reports the set of intervals provided by the expert, and corresponding confidence levels (degrees of certainty). For visualization, both Fig. 2(a) and (b) report only the interval [1e−6h−1,1e−4h−1], instead of the entire UoD, and the abscissa axes are logarithmically scaled.From Fig. 2(b), it also emerges that the elicitation process should be checked against the overconfidence problem, which relates to the fact that experts may provide intervals of values such that they were X% sure that the correct values lay between them, but in fact the truth is that the real values fall inside the assigned intervals less than X% of the times. In this respect, the results of the study in Ref. [47] shows that the overconfidence problem is reduced, or even avoided, when experts assign both the interval and the corresponding level of confidence; on the contrary, when experts are asked to assign the interval associated to a pre-fixed level of confidence the correctness of the judgment worsens. Thus, the findings in Ref. [47] seem to support the elicitation method proposed in this work.With respect to the elicitation exercise of failure rates, a practical difficulty relates to the typically very low values (even lower than 10−6h−1), which make them difficult to assess by experts. Alternatively, the experts can assess the mean time between failures, based on their experience on the total time of observation and the number of failures observed in such time span [48].More generally, in the uncertainty setting with k variables, an expert is asked to provide for everyj=1,...,kandp=1,...,Mj, a set of nj,pnested intervalsAij,p,i=1,...,nj,p, (A1j,p⊆A2j,p⊆...⊆Anj,pj,p), which are believed to contain the true value of the pth parameter of the jth random variable, θj,p, with a positive confidence levelqij,p.In this work, the uncertainty on the information elicited from the experts is represented by resorting to the possibility theory (see Appendix 1). With respect to an uncertain generic parameter θ, the possibility theory defines, for a given set A, the possibility and necessity measures, Π(A) and N(A), which can be interpreted as the upper and lower limits of the probability that the true value of the parameter belongs to A. These two measures are related to the possibility distributionπθ(ϑ), which expresses the degree of possibility that the true value of the uncertain parameter be ϑ, by:(3)Π(A)=supϑ∈A{πθ(ϑ)}(4)N(A)=1−Π(not(A))=1−supϑ∉A{πθ(ϑ)}In our case, a possibility distribution is directly built from the weighted families{(A1,q1),(A2,q2),....,(An,qn)}provided by the expert, according to the procedure proposed in Ref. [44] and whose steps are here briefly recalled, for convenience.•First of all, it is postulated that the necessity measure, N(Ai), i.e., the lower probability that the true value of θ is in the interval Ai, is equal to the confidence level qidefined by the expert. Thus, the inequalityP(ϑ∈Ai)≥N(Ai)=qiholds, for any i=1, …, n.Then, since there are infinite possibility distributions πθ(ϑ) that obey the constraint qi=N(Ai), it has been decided to choose the one which maximizes the degree of possibility πθ(ϑ) for all the values ϑ. The solution is unique and is [4]:In particular, it is possible to show that this is the least specific possibility distribution with respect to the available data, i.e., any other possibility distributionπ′θobeying the constraints qi=N(Ai) is such thatπ′θ≤πθ[4].With reference to the case of the exponential, non-repairable, binary component, the possibility distribution πΛ of the failure rate λ associated to the weighted family of Table 1 and built according to the procedure depicted above, is reported in Fig. 2(b). To verify that this distribution obeys the constraints qi=N(Ai) for i=1, …, 5, let us consider, for example, the first interval A1; then,N(A1)=1−Π(notA1)=1−supϑ∉A1{πθ(ϑ)}=1−0.9=0.1=q1. Notice also the residual uncertainty ɛ=0.05 associated to the points of the UoD external to A5.The uncertainty in the parameters of the model needs to be propagated to assess the uncertainty on the outputs. To this aim, we exploit the concept of FRVs within the methodology proposed in Ref. [3]. FRVs can be intuitively conceptualized as random variables whose values are not real numbers, but fuzzy numbers, since there is a vague perception of their true values, which are crisp but unobservable [40]. In other words, a FRV is a generalization of a random variable or a fuzzy variable [45].The operative steps of the uncertainty propagation procedure are reported in the following with reference to the case of the exponential, non-repairable, binary component. Since this case is characterized by a single uncertain variable (k=1), we omit in the notations the subscript 1 referred to the uncertain variable. The general procedure is given with reference to the sample vector{ujω}, but in our example we indicate it by {uω}.1)For each uncertain variable Yj,j=1,2,⋯,k, sample a vector{ujω}ω=1,2,⋯,NT, made of NTuniform random numbers in [0,1]; for example in our case, since k=1, we need a vector of random numbers {uω},ω=1,2,⋯,NT. In particular, let us assume that the first sampled value u1=0.65.Select a value of αion [0,1] and take as intervals of possible values the cuts[θ_j,θ¯j]αi={[θ_j,1,θ¯j,1]αi,...,[θ_j,Mj,θ¯j,Mj]αi}corresponding to the possibility distributions of the parametersθj={θj,1,...,θj,Mj}, of the variables Yj,j=1,2,⋯,k; in our case, let us start from αi=1: the interval of possible values for the parameter λ is [9.9e−6h−1,1.01e−5h−1] (see Fig. 2(b)).Identify the set of random intervals[y_jω,y¯jω]αi, of the variables Yj,j=1,...,k, corresponding to the random vector{u1ω,...,ujω,...,ukω}, using theαi-cut[θ_j,θ¯j]αi={[θ_j,1,θ¯j,1]αi,...,[θ_j,Mj,θ¯j,Mj]αi}, found at step (2). In particular, the ω-th random interval of the jth variable,[y_jω,y¯jω]αi=[infθj∈[θ_j,θ¯j]αiFYj−1(ujω;θj),supθj∈[θ_j,θ¯j]αiFYj−1(ujω;θj)], whereFYj−1(ujω;θj)is the quasi-inverse function of the CDFFYj(yj;θj)of the random variable Yj, for any value of the vectorθj(i.e., if U is a random variable uniformly distributed on [0,1], thenFYj−1(U;θj)has CDFFYj(yj;θj)). This procedure can be regarded as an extension of the Monte Carlo (MC) sampling method Ref. [51], modified to take into account the fact that the parameters of the CDFs are fuzzy-uncertain in their UoDs: each sample from the uniform distribution is associated to an interval of values, instead of a single value (Fig. 3), so that different CDFs are obtained from the sampling, and lower and an upper bounding CDFs can be identified.In the reference case study, the interval associated to the sample u1=0.65 and αi=1 is[ttf_,ttf¯]1=[1.63e5h,1.66e5h](Fig. 3(a)). This is obtained by considering the two extremes of the interval of the uncertain parameter λ equal to[θ_,θ¯]1=[9.9e−6h−1,1.01e−5h−1], which define the upper and lower exponential distributions,1−e−θ¯tand1−e−θ_t, respectively. Then, these functions are inverted to find the interval[ttf_,ttf¯]1, which is given by:[ttf_,ttf¯]1=−ln(1−u1)θ¯,−ln(1−u1)θ_Notice that in this particular case, the interval[ttf_,ttf¯]1is trivially obtained, since the inverse function of the exponential distribution is known. In general, it may be difficult to find out the analytical expression of the minimum and maximum values of the inverse functionFYj−1(U;θj), especially if it depends on a large number of parameters (e.g., Mj>4, 5). In these cases, one has to devise efficient methods for identifying the minimum and maximum values of the random variable corresponding to the different combinations of the uncertain parameters[θ_j,θ¯j]αi={[θ_j,1,θ¯j,1]αi,...,[θ_j,Mj,θ¯j,Mj]αi}.For every output variable Zo, o=1, …, O, calculate the smallest and largest values of g (denoted byg_αiZo(ω)andg¯αiZo(ω), respectively), within the intervals[y_jω,y¯jω]αi,j=1,2,⋯,k, of the variables:(6)g_αiZo(ω)=infj,yj∈[y_ji,y¯ji]αig(y1,...,yj,...,yk)(7)g¯αiZo(ω)=supj,yj∈[y_ji,y¯ji]αig(y1,...,yj,...,yk)for o=1, …, O, and consider the interval:(8)Γαizo(ω)=[g_αiZo(ω),g¯αiZo(ω)]In the case of the exponential, non-repairable, binary component, the minimum and maximum values of the TTF found in the previous step (i.e., 1.63e5h and 1.66e5h, respectively) are both larger than the mission time T=105h, and thus the corresponding values of D are zero (Eq. (1)).Return to step (2) and repeat steps (3) and (4) for another α-cut. For the exponential, non-repairable, binary component, the intervals[ttf_11,ttf¯11]αicorresponding to different values of αiare reported in Fig. 3. For example in the case of αi=0.5,[ttf_11,ttf¯11]0.5=[9.54104h,1.16105h], whereas for αi=0.05 the interval[ttf_11,ttf¯11]0.05=[0,∞].The FRV corresponding to the ωth realization is computed as:(9)πZo(ω)(zo)=sup[αi∈[0,1]|zo∈ΓαiZo(ω)]The FRV that describes the portion of the component downtime associated to the first sample is shown in Fig. 4: sinceπD1(0)=1it is fully plausible that the component is available for the overall mission time, whereas, sinceπD1(1)=0.05it is not impossible that the component is unavailable for the entire mission time. Furthermore, according to the probabilistic interpretation of the possibility distribution, it is possible to observe that the probability that the component is fully available during its mission time is between 0.5 and 1. In fact, the necessity that the component is fully available isN(D(1)=1)=1−supD(1)≠1(πD(1)(d))=0.5<P(D(1)=1). In the same way, the probability that the portion of downtimes is larger than 0.07 is between 0 and 0.2. Notice that the FRV of Fig. 4 is consistent with the intervals represented in Fig. 3. In fact, only the intervals corresponding to αi≤0.5 contain the value T=105h: this means that only for these values of αithe component may experience a failure before T, which entails its unavailability for the remaining part of the mission time.Repeat steps (1)–(6) for a new realization of the random variables, until ω=NT.Compute the Plausibility and Belief distributions for Zo, o=1, …, O by:(10)Pl(Zo∈]−∞,zo])=∑ω=1NT1NT⋅supzo∈]−∞,zo]πZo(ω)(zo)Bel(Zo∈]−∞,zo])=∑ω=1NT1NT⋅infzo∉]−∞,zo](1−πZo(ω)(zo))where 1/NTis the probability assigned to the ωth FRV, for any ω. In particular, Eq. (10) is derived from the interpretation of the FRVs under the setting of random sets [46].The Plausibility and Belief distributions of D (i.e., the upper and lower bounds, respectively, of the probability distributions of the portion of the mission time T in which the exponential, non-repairable, binary component is in a fault state) are reported in Fig. 5; for comparison, the CDF (see Eq. (2)) is also provided, which lies between the Plausibility and Belief distributions.A comment seems in order about the requirement that the uncertainties on all the input parameters must be described by the same expert, which is mandatory for applying this procedure. This constraint comes from the application of the extension principle in Eq. (9), which introduces a strong dependence between the information sources supplying the input possibility distributions. Indeed, the same confidence level for all the input variables is chosen to build the α-cuts of the output variables; this suggests that if the expert source informing on one parameter is rather precise or gives the same mean values to the confidence levels, then the one informing on another parameter must also be precise, i.e., to ensure this, it must be the same source. Further research effort should be spent in order to verify whether the procedure here illustrated can be interpreted as a conservative counterpart to the calculus of probabilistic variables under stochastic independence, due to the dependence between the choice of confidence levels.The present case study is taken from Ref. [49] and regards the degradation and maintenance of a check valve of a turbo-pump lubricating system in a Nuclear Power Plant. The degradation modeling is based on information collected from dependability analyses (e.g., FMECA) or directly from experts. This leads to the identification of one principal degradation mechanism, i.e., fatigue, and only one failure mode, i.e., rupture. A Condition-Based Maintenance (CBM) policy is applied to this component on a time horizon T=104h (notice that the time horizon of this case study is different from that of the simplified case study discussed in Section “Uncertainty setting” to illustrate the methodology). The performance of the maintenance policy is assessed in terms of cost and component unavailability.Notice that the case study proposed in this work is derived from a real industrial case addressed by the authors. However, for confidentiality reasons, the values of the parameters of the degradation – failure model considered in this Section have been arbitrarily set, so that the focus of the paper could be put specifically on the method proposed to cope with the issue of assessing a maintenance policy in the presence of uncertainty in the model parameters (rather than on the findings of the specific case study).The fatigue phenomenon affecting the check valve is here modeled as a discrete-state, continuous-time stochastic process that evolves among the following three degradation levels (Fig. 6):1.‘Good’: a component in this state is new or almost new (no crack is detectable by maintenance operators).‘Medium’: if the component is in this degradation level, then it is convenient to replace it.‘Bad’: a component in this degradation state is very likely to experience a failure in few working hours.The choice of describing the degradation process by means of a small number of levels, or degradation ‘macro-states’, is driven by industrial practice: experts usually adopt a discrete and qualitative classification of the degradation states based on qualitative interpretations of symptoms.The probability density functions (pdfs) of the transition times are Weibull distributions, with scale parameters ηijand shape parameters βijfor the transitions from state i to state j (i, j ∈ {1, 2, 3}, and i<j). The Weibull distribution is commonly applied in fracture mechanics (e.g., [33]), especially under the weakest-link assumption [50].A further state, ‘Failed’, can be reached from every degradation state upon the occurrence of a shock event. The exponential distribution with constant failure rate λjdescribes the failure behavior of the component while it is in state j, for every j=1, 2, 3. The choice of assigning a constant failure rate to every degradation state is driven by industrial practice: experts are familiar with this setting and comfortable with providing information about the failure rates values.The CBM policy applied to the system is composed by the following tasks:•Inspections: these actions, which are the only scheduled actions, are aimed at detecting the degradation state of the component, and are considered to last 5h for a cost of 50€.CBM actions: Preventive Maintenance (PM) actions which are dependent on the result of an inspection action. More precisely, if the component is found to be in state ‘Good’, no action is performed, whereas if the degradation state is ‘Medium’ or ‘Bad’, then the component is replaced and, consequently, the degradation state is taken back to ‘Good’. Both these replacement actions are supposed to take 25h and cost 500€, each.Corrective Maintenance (CM) actions. The corrective action, performed after a component failure, is assumed to be the replacement of the component. Due to the fact that this event is unscheduled, this action brings an additional duration of 85h and an additional cost of 3500€, with respect to the replacement after an inspection, leading to a total duration of 100h and to a total cost of 4000€. In particular, the additional time may be caused by the supplementary time needed for performing the procedure of replacement after failure or to the time elapsed between the occurrence of the failure and the start of the replacement actions.The Inspection Interval (II), which is the time span between two successive planned inspections, is the only decision variable considered in this case study; optimization is then directed to the search for the value of the II that minimizes the costs and maximizes the availability of the component.Notice that to keep the case study presented in this work consistent with the practical industrial problem tackled by the authors, the values of duration and cost of the different maintenance actions are given proportional to the actual ones.The case study is firstly investigated in the unrealistic situation in which the values of the model parameters θj,p,j=1,...,kandp=1,...,Mjare assumed to be exactly known (i.e., there is no epistemic uncertainty). Table 2reports the values of these parameters, which have been taken from [49].Fig. 7shows the CDF of the portion of the mission time in which the component is unavailable. Two main steps can be observed, which can be explained by imagining to have a population of identical components. According to the MC simulation, almost 60% of the population experience one out of the following two behaviors.•The component never fails during the mission time, and thus is inspected 4 times (at t=2000h, 4000h, 6000h and 8000h); in 3 out of these 4 inspections the component is found in degradation states Medium or Bad (75h of downtime) and in the remaining one in degradation state Good (5h of downtime). Thus, the total downtime is 80h, which constitutes the 0.8% of T. Components experiencing this life explain the CDF step in correspondence of d=0.008.The component never fails during the mission time, and when inspected is always found in degradation states Medium or Bad (100h of downtime). This behavior explains the CDF step in correspondence of d=0.01.Notice that it is possible to lump together the information provided by the cumulative distribution of the portion of downtimes into the mean value of the distribution, i.e., the average unavailability over the mission time, which provides an useful and easy to be interpreted indicator of the component expected state in the mission time. The estimated average unavailability is 0.011, and the related 68.3% confidence intervals is [0.011−9.8e−8, 0.011+9.8e−8].Fig. 8shows the estimated average unavailability of the component over the mission time (i.e., the mean value of the component downtime over the entire mission time), with the related 68.3% confidence interval, for different values of the II. The narrowness of the confidence intervals (the error bars seem to reduce to horizontal segments, especially in the left hand part of the time axis) is due to the large number (5×104) of MC simulations performed in this case study; roughly speaking, the larger this number the smaller the (confidence) interval that with a given probability (confidence level) contains the true value of the estimated variable. Thus, in the present case study the actual value of the average unavailability over the mission time is affected by a small amount of error, which can be reduced by increasing the number of simulations.Initially, there is a decreasing behavior that reaches a minimum in correspondence of II=1000h; after this point, the unavailability starts rapidly increasing. This is the result of two conflicting trends: on one side, the more frequent are the inspections the larger is the probability to find the component in degradation states Medium and Bad: this prevents the component to fail and thus saves the corresponding large time to repair. On the other side, frequent replacements are ineffective, since the component life is not completely exploited in this case. The minimum at II=1000h represents the optimal balance between these two tendencies.Fig. 9shows that the maintenance costs associated to different choices of the II have a shape similar to that of the mean unavailability. Thus, one may conclude that under the considered maintenance policy, the best II is at II=1000h with respect to both availability and cost objectives. On the other hand, both the mean unavailability and the maintenance cost remain small, with little variations, when the value of the II ranges in the interval [1000h, 2000h]. This relative flatness of both performance indicators (unavailability and cost) allows a certain freedom to choose the II within such range: other criteria (e.g., opportunistic maintenance) not included in this analysis can be taken into account if the related advantages recover the small losses due to the increase of unavailability and cost, that would be incurred when moving away from the optimal value of 1000h.The aim of this Section is to apply the method illustrated in Section 2 to the case study described above, when the parameters of the distributions that model the transitions of the component among the four states of Fig. 6 are ill-defined and there is only one expert who estimates their values. To sum up, the uncertainty situation is the following:•there are k=5 uncertain variables, which define the 5 transition times reported in Table 3.The distributions associated to the variables are known, and depend on the set of the uncertain parametersθj, j=1, …, 5 reported in Table 3. In turn, there are Nu=7 uncertain parameters, which are the shape and scale parameters of the two Weibull distributions and the failure rates pertaining to the three degradation levels (see Table 3).Notice that the simulation of a single MC history (steps 1–5 of the procedure in Section “Uncertainty propagation”) requires that the model g encodes a number of random variables k>>5, since the history corresponding to a given sample of these 5 uncertain times in general do not cover the entire time horizon T. For example (Fig. 10(a)), if the first transition is from state 1 to state ‘Failed’ and occurs at t=2000h, then the interval time between t=2000h+100h (i.e., the time instant at the end of the repair action that starts after the failure) and T remains not investigated. This problem can be overcome by thinking of g as a function that depends on a number K of 5-ple (the 5 probabilistic variables), and not just on 5 variables. Obviously, the number K that allows to cover the entire mission time is also a random variable, since it depends on the sampled times, which produce histories of different lengths. However, this is not a problem in practice: the number K can be chosen such that it is reasonably sure that the sampled times simulate histories of duration larger than the time horizon T. Then, the analysis focuses only on the interval [0,T] (Fig. 10(b)). Finally, the output vectorZ_is made up of the portion of T in which the component is unavailable, and the cost associated to the maintenance policy to be assessed; thus O=2.In all generality, the difficulty in estimating the uncertain parameters of the model may heavily vary from one case to another; the weighted families{(A1j,p,q1j,p),(A2j,p,q2j,p),....,(Anj,pj,p,qnj,pj,p)}provided by the expert to represent his/her knowledge about the parameters are expected to reflect this difference.In regard to the considered case study, the weighted families elicited from the single expert are reported in Table 4. It is assumed that he/she is able to infer the information on the transition times between the different states, from the observations gathered during past component inspections. For example, let us suppose that a component is monitored every 100h, and that it was found in degradation state Medium a t=1800h; if the component is found in degradation state Bad upon the next observation at t=1900h, then the expert acquires the information that the transition from degradation state Medium to Bad occurred in the interval [1800h, 1900h]. This kind of information can be used to define the scale and shape parameters of the Weibull distributions representing these transitions. In relation to this, recall that the scale factor of a Weibull distribution is by definition the time before which almost 65% (i.e., 63.2%) of the components of an homogeneous population have experienced a transition, whereas the shape factor, roughly speaking, determines the uncertainty around the scale value. The uncertainty affecting the estimations of the scale and shape parameters is expected to be very different: the expert has a more confident knowledge on the scale parameter seen as the 65th percentile, than on the shape parameter which only relates to the slopes of the Weibull probability plots: these are difficult to construct from the observations of the components’ degradation states during the inspections. Anyway, it is the authors’ experience that in some cases (e.g., the degradation behavior of some components of the turbines used in the Oil&Gas industry), the experts of the maintenance engineering department have a relatively precise knowledge about the values of the shape factors, too.With regards to the scale parameters (rows 4 and 6 of Table 4), it has been assumed that the expert provides three nested intervals corresponding to the confidence levelsq1j,p=0.1,q2j,p=0.5andq3j,p=0.95, j={1,2} and p=1, and the UoDs within which these parameters range. With regards to the shape parameters (rows 5 and 7 of Table 4), given the difficulty in their estimation, it has been assumed that the expert provides just the UoDs and the intervals corresponding to the confidence levelq3j,p=0.95. In particular, the UoDs, which contain the true values of the parameters with probability 1, are very large: the expert tends to reduce the sets of values that surely do not contain the true values of the scale parameters.Finally, with reference to the failure rates, also the estimation of the Mean Time To Failure (MTTF, i.e., the inverse of the failure rate) of the components in a given degradation state may not be easy; in fact, failure from the first degradation state is usually a rare event, whose frequency is difficult to estimate even in a qualitative way, whereas the lack of precise knowledge of the time instants in which the components transit toward the other degradation states affects the evaluation of the mean times to failure associated to these states; that is, if the time instant since one has to start to count is unknown, then the resulting measure of the time to failure is biased, especially if the component is rarely inspected. Thus, a more vague description of the uncertainty is provided by the expert for these parameters of the model g. Namely, he/she gives just the intervals corresponding to the 0.95 confidence level and the UoD, as for the scale parameters of the Weibull distributions. The extreme points of these intervals are reported in rows 8–10, columns 7–10 of Table 4.On the other side, the larger the number of the uncertain parameters, the larger the space in which the maxima and minima of the function g in Eqs. (6) and (7) have to be searched for, and the larger the required computational time. In this regard, a sensitivity analysis can be preventively performed in order to identify which are the input parameters whose variations lead to smaller changes in the output value; this allows to neglect the uncertainty affecting these parameters while losing a small amount of information and considerably reducing the computational times.In the present case study, the sensitivity analysis is performed by a local approach [55], i.e., the uncertain parameters of the model are varied one by one within their UoD, while the other parameters take their nominal values. The results of the analysis are reported in Table 5: the portion of T in which the component is unavailable is estimated in correspondence of the extreme values of the UoD of every uncertain parameter. For example, the estimation of D is 0.0142 in correspondence of the lower bound of the UoD of the scale parameter of the first Weibull distribution (1700h), whereas it is 0.0082 in correspondence of the upper bound (2020h). In particular, these values are reported with the related 68.3% confidence interval. The last column of the Table reports Δ, i.e., the differences between the average unavailability corresponding to the two limiting situations. These quantities give an estimation of the amount of output uncertainty (i.e., the unavailability uncertainty) due to the variation of the model parameters. In practice, high values of Δ indicate the importance of properly considering the uncertainty in the parameters, whereas low values correspond to parameters whose uncertainty has no remarkable effect on the model output uncertainty.In this case, the failure rate associated to the degradation state ‘Bad’ turns out to be the parameter which the model is less sensitive to. Then, the uncertainty affecting this parameter is not considered and the nominal value (Table 2) is assigned to it.Notice that the application of a global approach to sensitivity analysis (e.g., variance decomposition, Sobol indexes, Fourier amplitude sensitivity test, etc. [54,55]) would have provide a ranking of the parameters based on the extent to which their variability contribute to the uncertainty in the output variable. Thus, the result of a sensitivity analysis (Columns 2 and 3 of Table 5) cannot substitute the uncertainty representation and propagation tasks carried out in this work, but can be used to identify the input parameters whose uncertainty is most relevant. To further clarify this issue, we can say that the difference between the results in Table 5 and those of the approach proposed in this work to represent and propagate the uncertainty is twofold: on one hand, the PT-FRV based method is a global method to propagate the uncertainty [55]; on the other hand, the possibility distributions used to represent the uncertainty in the model parameters allow having a smaller commitment of the information provided by the expert.Finally, notice that for every uncertain parameter and for any confidence level, the value considered in Ref. [49] is the middle point of the corresponding intervals provided by the expert.With regards to the representation of the epistemic uncertainties, Fig. 11reports the possibility distributions of the uncertain parameters of the case study, corresponding to the weighted families of Table 4. These are obtained by applying the procedure showed in Section “Uncertainty representation”. For example, Fig. 11(a) shows that the expert considers unsurprising that the value of the scale parameter η12 value of the distribution that describes the transitions from the first state to the second state belongs to the interval [1843,1880]. On the contrary, the probability that the value of η12 hits the interval [1720,2001] is 0.95.Fig. 12shows the results obtained by applying the FRVs-based method to the considered case study. The Plausibility and Belief distributions of the portion of the mission time in which the component is in a down state are quite distant; this shows that for some favorable combinations of the uncertain parameters the system results to be much more available than for other combinations of the uncertain parameters which lead to high portions of downtime. Notice also that the Plausibility and Belief distributions bracket the CDF found the case in which the uncertainty on the model parameters is not accounted for (Section “Analysis of the case study”).Notice that the results provided by the method discussed in this work are difficult to be interpreted. This is due to the fact that, differently from the case of no uncertainty, it is not possible to lump together the information provided by the method, i.e., the Plausibility and Belief function, into indicators such as their mean values which can be easily interpreted. This impossibility is due to the fact that the DSTE does not allow to define the mean value of an uncertain variable. However, in order to give an interpretation to the obtained results, one can focus on a given percentile of the belief and plausibility distributions. For example, the interval bounded by the values of the 95th percentile of the Plausibility and Belief distributions is [0.015, 0.026]; the extremes of this interval constitute the lower and upper bounds, respectively, of the 95th percentile of the portion of downtime in the mission time. In other words, this interval tells us that the 95% chance of the downtime of the component can be nor smaller than 1.5% nor larger than 2.6% of the mission time. Thus, if one is interested in the worst case, then he/she can assume that the 95th percentile of the downtime is 0.026, whereas in a more optimistic view, it can be valued at 0.015.Figs. 13 and 14report the Plausibility and Belief distributions of the portion of downtime over the mission time and the total cost, respectively, corresponding to three different values of the II, i.e., II=1000h, II=1500h and II=2000h. These results do not lend themselves to an easy interpretation and do not allow to make a decision in a simple way. Indeed, while it is easy to state that inspecting the component every 1000h is better than every 2000h, since these distributions are not overlapped, answering the question ‘which value of the II is best?’ is not trivial, as the distributions corresponding to II=1500h and II=1000h are overlapped. This calls for devising a method in support of maintenance decision makers, to help them get around these distributions. Notice also that the small amount of uncertainty on the values of both downtime and cost, when the component is inspected every 1000h, derives from the fact that the ‘crowd’ of the simulated MC component histories (i.e., the large number of components experiencing the same behavior) remains very compact in this case.On the contrary, when the uncertainties affecting the parameters are not accounted for, the identification of the best value of II is more straightforward, since it is usually accepted to consider the mean value of the portion of mission time in which the component is faulty or the mean value of the cost as good indicators of the performance of the maintenance policy.

@&#CONCLUSIONS@&#
Uncertainty affects the parameters of the models of the behavior of components subject to a given maintenance policy. An incorrect treatment of such uncertainty may lead to a serious bias in the outcomes of the analysis. In particular, such outcomes (e.g., occurrence probability of a failure scenario) may be biased in both conservative (e.g., estimated failure probability value larger than the actual one) and non-conservative (e.g., estimated failure probability value smaller than the actual one) directions. Often in practice the only information available on these parameters comes from experts, in an ambiguous and qualitative form. Most commonly, all that is known is that a certain value belongs to a certain interval [39]. The representation of the uncertainty of this information in terms of probability distributions would force a set of assumptions, with introduction of biases and loss of generality. In this work, a methodology has been proposed based on the following steps:(1)Elicitation of the expert knowledge on the model parameters.Representation of the uncertainty associated to the expert's judgment, avoiding introduction of unjustified, biasing assumptions. In this respect, notice that the choice of any probability distribution to represent the uncertainty in the expert's assignments would be absolutely arbitrary, if the expert is not able to assign such additional piece of information.The propagation of the uncertainty on the maintenance performance indicators.The methodology has been applied to a case study concerning the degradation model of a check valve of a turbo-pump lubricating system in a Nuclear Power Plant. The study has shown that neglecting uncertainty may drive the maintenance decision maker toward incorrect conclusions. In this case, if the unavailability computation were performed without taking into account the uncertainty on the input parameters, the decision maker would set the inspection intervals between maintenance actions to the value of II=1000h, whereas a proper consideration of the uncertainties through the use of FRV suggests that, on the basis of the available knowledge, this choice for the maintenance inspection interval is not better than other intervals such as II=1500h and II=2000h.The main current limitations of the methodology discussed in the present paper are:•it is required that a single expert is knowledgeable, at least qualitatively, on all uncertain parameters and, what is more, is able to provide intervals of values for the uncertain parameters with associated confidence levels: this may be very difficult in practice. However, the FRVs-based method can be also applied when the expert provides just one interval of possible values per parameter, thus avoiding the problem of the confidence intervals.The results provided are difficult to be interpreted and managed. In this respect, a novel method has been proposed by the authors to compare the pairs of Belief and Plausibility values corresponding to two different solutions [56]. On this basis, an advanced extension of the Genetic Algorithms technique has been concocted to optimize maintenance problems in the presence of imprecision [31].Very large memory demand and computational times are required. Table 6reports the computational times in case of 2000 samples and 8000 combinations of values of the uncertain parameters. However, being Matlab an interpretative language, a tool developed in other environments may be more efficient (i.e., less time-consuming). This issue will be tackled in future works.Further research effort needs to be spent in order to verify whether the procedure here illustrated can be interpreted as a conservative counterpart to the calculus of probabilistic variables under stochastic independence, due to the dependence between the choice of confidence levels.