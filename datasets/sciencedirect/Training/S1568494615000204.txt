@&#MAIN-TITLE@&#
High performance EEG signal classification using classifiability and the Twin SVM

@&#HIGHLIGHTS@&#
Use of the classifiability metric to select discriminative frequency bands.Use of the Twin SVM to learn unbalanced datasets with low error rates.Improvements of up to 20% over state-of-the-art.

@&#KEYPHRASES@&#
Brain Computer Interface,Motor imagery,Classifiability,Common Spatial Patterns,Frequency band selection,LIBSVM,

@&#ABSTRACT@&#
Classification of Electroencephalogram (EEG) data for imagined motor movements has been a challenge in the design and development of Brain Computer Interfaces (BCIs). There are two principle challenges. The first is the variability in the recorded EEG data, which manifests across trials as well as across individuals. Consequently, features that are more discriminative need to be identified before any pattern recognition technique can be applied. The second challenge is in the pattern recognition domain. The number of data samples in a class of interest, e.g. a specific action, is a small fraction of the total data, which is composed of samples corresponding to all actions of all users. Building a robust classifier when learning from a highly unbalanced dataset is very difficult; minimizing the classification error typically causes the larger class to overwhelm the smaller one. We show that the combination of ‘classifiability’ for selecting the optimal frequency band and the use of the Twin Support Vector Machine (Twin SVM) for classification, yields significantly improved generalization. On benchmark BCI Competition datasets, the proposed approach often yields up to 20% improvement over the state-of-the-art.

@&#INTRODUCTION@&#
A Brain Computer Interface (BCI) system allows users to operate devices such as computers using their brain activity as a control signal. BCI interfaces are invaluable for those suffering from locomotive disabilities such as locked-in syndrome because BCIs pave the way for them to communicate using their brain activity, which remains intact despite locomotive degeneration.BCI Systems typically use Electroencephalogram (EEG) data recorded over-the-scalp as input to represent brain activity. The acquired signals are then processed using feature extraction and machine learning techniques to identify intents from them. The intent is translated to an appropriate command or action to control an external device. A typical BCI processing pipeline is shown in Fig. 1.There are various types of BCI systems [1], which may be differentiated on the basis of their control paradigms, signal acquisition techniques and mode of operation. We primarily focus on motor-imagery based BCI systems [2–4], wherein the system tries to identify imagined motor movements from the user's EEG data and maps these to associated intents. In other words, these systems are trained using EEG data pertaining to motor-imagery tasks for a particular user, such as imagining the movement of the left/right limb or hand and identifying features to distinguish between them. Such BCI systems are calibrated offline using the subject's EEG data (recorded during the training phase) by training a classifier to recognize these imagined movements. During the online phase of operation, the trained classifier maps these intents into actions. A comprehensive review of trends in BCI systems have been presented in [5–8], whereas processing paradigms can be found in [9–12].In practice, the accuracies obtained by these systems need improvement in order to be widely usable. Two hurdles hinder accurate recognition of EEG data. The first is the high variability in EEG signals [13,14], which is seen not just across individuals, but also across different trials of the same individual [15,16]. The high variability indicates that some features may simply be noise; using the right feature set may allow better discrimination and lead to higher recognition accuracies. While the eventual proof of the pudding is in the recognition accuracy, a wrapper kind of feature selection scheme is computationally prohibitive and inefficient. By using the notion of classifiability, we show that a good set of features are chosen without using a classifier in the loop. Since optimal feature selection is a combinatorial task, this approach yields good solutions rapidly. Guyon [17] points out that such methods are attractive because of their simplicity, scalability, and good empirical success. The advantage of classifiability is that it is independent of the predictor that is used, and can be used in conjunction with several different classifiers.A second hurdle arises in the classification task. Consider the task of learning to recognize each of a long list of several tasks performed by a given user. This is a multi-class classification problem [18,19], and is usually solved by using a one-versus-rest approach [20–22], wherein data samples for the task under consideration fall into one class, and the other class comprises the rest of the data. The number of data samples for a given task is a small fraction of the whole data. Hence, the sizes of the two classes in each binary classification problem in a one-versus-rest approach are very unbalanced. Conventional learning techniques, such as the classical Support Vector Machine (SVM), which try to find a maximum margin classifier, while also trying to minimize the training error, often fail to generalize well on the smaller class in such cases. The larger class tends to overwhelm the smaller one. Methods such as weighting the error contributions of the two classes, often need careful tuning of hyperparameters in order to be effective [23].To give a specific example, consider a scenario with N users, t tasks, and m recordings per user per class. Each binary classification problem in the one-versus-rest approach will have samples corresponding to a specific task imagined by one user in one set, and all other samples in the other set of the binary classification problem. This means that there will be t×m samples in class (+1) and (t−1)×m×N samples in class (−1) of each binary classification task. For example, if there are 10 users, 10 tasks, and 10 samples per user per task, we would have 10 samples in class (+1) and 990 samples in class (−1). Now examine the optimization problem in a typical classifier. For instance, the SVM problem is(1)Minimize12∥w∥2+C∑i=1Mqi(2)subjecttotheconstraintsyi[wTxi+b]+qi≥1(3)qi≥0where the separating hyperplane is given bywTx+b=0; xiis the ith sample, and yi∈{−1, 1} is its class label, where i=1, 2, …, M. The hyperparameter C controls the tradeoff between the emphasis on the margin, and the mis-classification error on the training set.We note that from the second term of the objective function (1), the mis-classification error is summed up over all training points. As a consequence, when the sizes of the two classes in a binary classification setting are unbalanced, error on the larger class dominates, and the solution obtained usually ignores the error on the smaller class. In the multiclass setting described above, minimizing the error on 990 samples would tend to dominate any error on the 10 samples of the smaller class. Careful choice of hyperparameters may sometimes, but usually not often, alleviate the problem [23]. This is discussed in further detail in Section 3.2.3.In this paper, we propose an approach that addresses these two hurdles. We use a simple technique involving a combination of feature extraction using CSP [24], sub-band feature selection using classifiability, and classification using Twin SVM [25]. Experimental results on benchmark datasets show that the proposed approach can yield improvements of nearly 20% over state-of-the-art. We address the problem of optimal feature set selection by using a measure known as classifiability, proposed by Kothari and Dong [26]. The use of this measure enables us to identify the ideal frequency band in which the EEG signals should be filtered in order to obtain robust features. Elimination of noisy features can help improve the classifiability index of a dataset; thus, classifiability can be used to select more discriminative features without building a classifier or using computationally expensive methods such as cross-validation. This technique is discussed in greater detail in Section 3.2.2.The rest of this paper is organized as follows. Section 2 discusses the challenges faced in the design and operation of an efficient motor-imagery based BCI system, as well as a review of prior work related to the use of frequency band selection for motor imagery classification. Section 3 details the EEG processing pipeline for our approach and its components. Section 4 presents the results and analysis using our approach on benchmark datasets, followed by the future work in Section 5.There are various issues faced in the design of efficient BCI systems [27]. The primary challenge is due to the nature of EEG signals. As these signals are acquired from the scalp at a distance from their source of generation, EEG signals are susceptible to noise and artifacts, such as those induced due to eye and muscular movements, or due to supply lines interference [28]. These artifacts may be removed by manual inspection of EEG signals by clinicians, or to an extent by spatial filtering [29].A technique of spatial filtering, Common Spatial Patterns (CSP) [24], has conventionally been used on EEG data for BCI systems [30]. It is essentially a decomposition technique for separating a multivariate signal into principal components, and selecting a subset that retains at least a specified fraction of the total information content. This has been used as a feature extraction technique to select the most prominent distinguishing features between the classes of EEG data (e.g. left and right limb movement) to be identified.Further, there is also a lot of variability in the EEG data of multiple subjects as well as sessions for motor imagery [31]. The physiological basis for motor imagery is Event-Related Desynchronization (ERD), which manifests as a decrease in neuronal oscillations that occur contra-lateral to the direction of movement [32]. In other words, moving the right limb causes a decrease in oscillations in the left cortical region of the brain, in the mu and beta rhythm band (8–30Hz). Though this phenomenon is common across users, the frequency band of these oscillations varies between subjects. The identification of the appropriate frequency band for each action of each individual is a key factor in designing a better BCI system in terms of classification accuracy.Our work aims at addressing the issue of selection of the best feature set associated with the optimal frequency band for EEG signals, considering the above mentioned issues. Various approaches have been proposed to select the optimal frequency band for classification of motor imagery EEG datasets. The technique of Filter Bank CSP (FBCSP) by Ang et al. [33] suggests filtering of EEG signals into multiple frequency bands and selecting discriminative pairs of bands as features, which is an extension of Sub-Band CSP (SBCSP) by Novi et al. [34]. A bi-spectrum (BS) approach for feature extraction has been presented by Shahid et al. [35], whereas Suk and Lee propose a Data-Driven Frequency Band Selection (DDFBS) approach [36]. A review of EEG processing paradigms has been presented by Majumdar in [37], whereas the use of a modern classification technique, the Recurrent Quantum Neural Network (RQNN) has been presented by Gandhi et al. [38]. We compare our work with these approaches in Sections 4.2 and 4.3. We also compare our work with the results obtained in [39] which uses sub-band filtering approach followed by CSP on the same dataset, in order to introduce a Linear Dynamical System (LDS) approach to BCI.These works form the underlying premise of our approach of choosing the optimal frequency band for using CSP as a feature extraction technique. The key novelty of our approach is in the use of a method which is computationally simple and cost-effective, as well as provides significant improvement in terms of performance, specifically the classification accuracy. Approaches proposed in literature [33–36,38] use feature selection methods that involve using the classifier to determine the robustness of the feature set, which is computationally intensive. Our approach entails selecting the band based on classifiability of the corresponding feature set. The classifiability measure for separability of feature sets [26] is simple to implement and intuitive to realize, and does not require building a classifier as part of the process. It provides a measure to select the frequency band that enables selection of features for improved classification accuracy. We observe that the classifiability value of features of band-filtered EEG correspond to the classification accuracy, which validates the use of the classifiability measure. Further, our approach provides lower computational cost and improved classification by using the Twin SVM, which solves smaller optimization problems and generalizes better to the classification task at hand. The following section discusses the components of our approach in detail.This section details the sequence of steps that our approach uses for the selection of frequency band and classification of a given set of training EEG data. The idea is primarily to filter the EEG data in various frequency bands in the mu and beta rhythms for ERD [40], and then compare the classifiabilities of the feature sets obtained using CSP on the band-passed data. The process is illustrated in Fig. 2.We compare our approach with the techniques of SBCSP [34] and FBCSP [33] in Fig. 3to illustrate the lower computational cost of our proposed method. SBCSP, shown in Fig. 3(a), uses a classifier on the CSP features obtained in each frequency band, and the scores from the classifiers are combined to obtain the final result. Similarly, FBCSP (Fig. 3(b)) uses multiple frequency selection and classification methods to determine the optimal frequency band. Our approach, as shown in Fig. 3(c) shows that we do not use the classifier for determining the frequency band. We employ the use of classifiability, which is much simpler, and is faster to compute as compared to the other techniques which use a cross-validation strategy to select the frequency band. This ensures that our method provides feature selection with a lower computational cost.This section discusses the acquisition protocol for EEG data, and the pre-processing steps. The results reported in this paper are based on benchmark BCI datasets. These sets are collected by employing the following procedure. Motor Imagery experiments are conducted, in which a user is seated in front of a computer screen while wearing an EEG acquisition device. The user is then shown an arrow on the screen which may point either to the left or right, and the user is instructed to imagine the movement of the limb corresponding to the direction of the arrow. This constitutes a trial, and the user undergoes multiple trials during a session. The EEG signals of users are recorded with event markers indicating the timing and the type of event (left/right arrow). During these sessions, the sequence in which the arrows appear in a particular direction is randomized to prevent the human brain from adapting to the patterns, which is not desirable when acquiring EEG signals during the training phase [41].Our approach uses CSP for feature extraction, classifiability for feature selection, and the Twin SVM for classification, which are discussed in Sections 3.2.1, 3.2.2 and 3.2.3 respectively.The first step is to extract features and determine their classifiability in each frequency band. In order to determine the optimal frequency band for a subject, we filter the EEG signals using a band-pass filter in different frequency bands. For our case, we use frequency bands that are 4Hz wide each. These bands lie in the interval of 8–30Hz (mu rhythm band). Thus we obtain a set of bandpass signals.To extract features, the band-filtered EEG signals are grouped according to the classes (left/right imagined movement) for all trials. We then obtain individual epochs by extracting the EEG signals in a time window [tA, tB] around each event marker, corresponding to tAseconds prior to appearance of the stimulus to tBseconds after its appearance. From these epochs, we extract features for the filtered EEG data of each class using CSP.In principle, the CSP technique aims at maximizing the objective function shown in Eq. (4):(4)JCSP(w)=||wTX1||22||wTX2||22where X1 and X2 represent the EEG data corresponding to the classes to be identified, which have been filtered in a frequency band, andwis the spatial filter computed by the CSP method.The CSP technique, detailed in Algorithm 1, involves finding the Principal Components. A whitening matrix is determined as shown in Step 5, and used to de-correlate the EEG data by multiplying it with the EEG signal matrix during feature extraction. The eigenvectors corresponding to the k highest eigenvalues are chosen to form the spatial filter. The parameter k is chosen empirically and determines the reduced dimensionality of the feature set. In our case, we determine k based on the amount of energy contained in k highest eigenvalues, where energy is proportional to the square of the eigenvalue. Specifically, k is chosen such that the fraction of energy contained in k highest eigenvalues is above 75%.To obtain the feature set for a trial, we multiply the EEG data with the spatial filter and compute its log-variance. This provides the feature set for that trial, in the frequency band in which the EEG signal was filtered. This process is repeated to obtain features for EEG data corresponding to each filter band. Note that features for the test set are obtained by multiplying the EEG data with the spatial filter obtained from the training data in the corresponding band, after which the log-variance is computed.Algorithm 1CSP Algorithm for extracting EEG features0Input: EEG Data X of multiple trials of dimensions [channels×samples]Output: CSP FilterObtain covariance matrix C=XXT.Compute normalized transformation z over N trialsz=∑i=0NCi/trace(Ci), where trace is the sum of diagonal elements of the matrix.Sum all z to obtain composite normalized transformation Z.Decompose Z into eigen values λ and eigen vectors U, Z=UλUT.Obtain whitening matrix P=λ−0.5UTCompute S=PZPT.Decompose S into eigen values λ and eigen vectors V, Z=VλVT.Select k eigen vectors corresponding to k highest eigen values, Vk←V.Return spatial filterSF=VkTP.The next task in our approach is to determine the best frequency band using the CSP features. We compute the classifiabilities for each of the filtered EEG features corresponding to the various frequency bands.To motivate the use of classifiability as a feature selection technique, consider a classification problem in two-dimensions. Fig. 4(a) and (c) shows samples of binary classification datasets. In Fig. 4(a), there are 200 data points, of which 100 (Class +1) lie within a circle of known radius and the remaining (Class −1) lie outside it. In Fig. 4(c), 100 points are randomly assigned to each class. Fig. 4(b) and (d) shows plots of the samples in three dimensions, where the third dimension is the class label. We observe that the roughness of the surface provides an idea of the classifiability of the dataset. The classifiability measure quantifies this notion; as the measures for the two datasets are 0.99 and 0.33, respectively. If a given neighborhood contains points of both classes, the surface is rough and the classifiability measure is low. On the other hand, if the neighborhood contains points of the same class, the classifiability values are high. Thus, the classifiability of a feature set can be used as a measure to determine the performance of a classifier, without actually using the classifier. This would help reduce the computational cost of the feature selection stage significantly. We use the classifiability measure to select the optimal feature set in an EEG signal classification pipeline by determining the classifiability of Common Spatial Patterns (CSPs). To compute the classifiability, we choose a neighborhood distance, d, that signifies the locus of points in a circle of radius d within which we find the occurrence of other features.We now illustrate the computation of classifiability in the context of EEG data. The original data is a matrix of size [channels×samples×trials] and we have a corresponding label vector which indicates the class of the trial, which is of size [trials×1]. After computation of CSP features, each trial is represented by a vector of size [k×1], where k is a parameter chosen for the CSP technique as discussed previously. Thus, we now work with a feature set of size [k×trials] and the label vector of size [trials×1].The classifiability of this feature set is computed for a particular value of neighborhood distance d. We create a co-occurrence matrix W for each pattern of each class. The elements of W, say for a patternpi1of class (+1), are defined as follows:•W1,1: Number of patterns of class (+1) that lie within a sphere of radius d aroundpi1.W1,2: Number of patterns of class (−1) that lie within a sphere of radius d aroundpi1.W2,1: Number of patterns of class (+1) that lie within a sphere of radius d aroundpi−1, wherepi−1is the corresponding pattern for class (−1).W2,2: Set as zero.Specifically, for training sample x(i), the co-occurrence matrix W(x(i)) of size [c×c], where c is the number of classes, is computed. The elements of W(x(i)), denoted byw(x(i))jk, are the samples of classwkthat occur within a circular neighborhood of radius d of a samplewj. This is shown in Eq. (5), where x(l) denotes a sample of classwjand x(m) denotes a sample of classwk. f(.) is an indicator function which is 1 if ||x(l)−x(m)||≤d andNwkis the number of patterns from classwk.(5)w(x(i))jk=∑n=1Nwkf(x(l),x(m))This computation is illustrated in Fig. 5, where the co-occurrence matrix for pattern 1 of class (+1),p11, is denoted asW11. For the sake of brevity of notation, we denote the elements ofW11by Wi,j. We find that for the neighborhood distance d, there are four patterns of class (+1) within the circle of radius d, hence W1,1=4. Similarly, there are two patterns of class (−1) which makes W1,2=2. Considering the corresponding patternp1−1of class (−1), there is one pattern of class (+1) and one of class (−1) within the circle of radius d which makes W2,1=W2,2=1.The co-occurrence matricesWi1andWj2for i patterns of class (+1) and j patterns of class (−1) can be computed similarly. After obtaining Ws for all patterns of each class, these are added for all patterns and then for both classes, asW=∑i,jWi1+Wj2. This matrix is then normalized such that the sum of elements in the matrix is unity. The classifiability measure is computed as the difference between the diagonal and off-diagonal elements of this normalized matrix.The computation of the classifiability of a dataset involves the selection of the neighborhood distance d. To illustrate how this parameter affects the computed classifiability value, we compute the classifiability for two datasets shown in Fig. 6(a) and (c) for various values of d in the range [0, 1] at an interval of 0.1. The corresponding classifiability values are shown in Fig. 6(b) and (c) respectively. As the datasets are scaled in the range [0, 1], the choice of taking d in this range is justified.Dataset 1 shown in Fig. 6(a) consists of points randomly sampled from a uniform Gaussian distribution for both classes. This dataset is not very “separable”, hence the classifiability values are low, for all values of the neighborhood distance. On the other hand, the points of dataset 2 shown in Fig. 6(c) are “separable”, hence the computed classifiability values shown in Fig. 6(d) are higher and closer to 1 for initial values of the neighborhood distance d. As soon as the neighborhood distance exceeds 0.3, there is a sharp decrease in the classifiability value. This is explained by the fact that if we consider a circle of radius greater than 0.3 around any point of dataset 2, we start finding points of the other class within the circle. Further, the classifiability values saturate for higher values of d. On the basis of empirical results obtained from our experiments, it can be concluded that the classifiability of a dataset can be easily computed using values of the neighborhood distance d that are larger than R/2, where R is the radius of the dataset. It may be noted here that the radius R of a dataset is computed as the maximum of the Euclidean distances between the samples of the dataset and its mean.We also observe that the computation of classifiability is not prohibitive. In terms of complexity bounds, the computation of classifiability of a dataset is bounded by O(n2) for the worst case, which is attributed to the computation of the co-occurrence matrix. However, the use of data structures can reduce the cost to about O(NlogN), and sampling can be used for large datasets.The rationale behind using classifiability as a measure to select the optimal frequency band is the fact that it provides a measure of separability for the feature sets, effectively enabling the selection of the best feature set that should be used to train the classifier. Essentially, the computation of classifiability of a feature set involves the computation of a numeric value to represent the disjointness of the feature sets. Thus the features extracted from the EEG data filtered in the optimal frequency band will have the maximum separability for a particular subject/session, represented by the classifiability measure.It is important to mention here that classifiability is not directly used for feature extraction. It is used as a simple measure to determine which feature set (extracted using CSP in the multiple frequency bands) is most effective to train the classifier. The task at hand is to select the best feature set, and we require a measure of the separability of the feature sets of each class, which forms the premise for using classifiability.Classifiability is a simple measure to compute, in comparison to methods such as cross-validation; as the latter is computationally expensive and time consuming when working with large datasets involving numerous trials, which is usually the case when training BCI systems offline for users. In such cases, classifiability provides a simpler measure of the robustness of the feature set. The computation of classifiability is independent of the classification process itself, and hence it can be used to determine the suitability of a feature set prior to training a classifier. Hence, it can be used as an indicator in selecting the optimal frequency band. In the present case, features that are discriminative and less noisy get selected as a consequence. The next step is training a classifier using the selected features.The features corresponding to the frequency band having the highest value of classifiability indicate the optimal feature band and are used to train the classifier. We now introduce the soft-margin Support Vector Machine (SVM) [42] classifier which has the following formulation. Given training vectors xi∈Rn, i=1, …, l, in two classes, and a vector y∈Rlsuch that yi∈{1, −1}, C – Support Vector Classification (also known as the L2-norm SVM) solves the following primal problem:(6)minw,b,ξ12||w2||+C∑i=1lξisubjecttoy(i)(wT(x(i)+b))≥1−ξi,ξi≥0,i=1,…,l.where the separating hyperplane is given by the vector[w,b], C is the upper bound, ξirepresents the slack variables for the soft-margin formulation and αirepresents the Lagrange multipliers.From the SVM formulation, one can observe that it involves the solution of a single quadratic programming problem (QPP). This can be time-consuming for datasets with large number of features. Also, the SVM involves obtaining the predicted label using a single maximum-margin hyperplane, represented by the equationwTx(i)+b=0, which is mid-way between the hyperplanes given bywTx(i)+b=1andwTx(i)+b=−1. For a given test samplexˆ, the SVM prediction will be obtained by determining the sign ofwTxˆ+b. Our technique is based on the intuition that a better prediction can be obtained by using a formulation which allows for non-parallel, as well as more than one hyperplanes.To this end, we use the Twin SVM [25]. The Twin SVM generates two non-parallel hyper-planes by solving two smaller-sized QPPs such that each hyper-plane is closer to one class and as far as possible from the other. Several extensions of the Twin SVM have also been proposed such as Twin Bounded SVMs [43], Coordinate Descent Margin-based Twin SVM [44], Twin SVM with probabilistic outputs [45], Least-squares Twin SVM [46], Least-squares Recursive Projection Twin SVMs [47], Knowledge based Least Squares Twin SVM [48], Margin Based Twin SVM [49], ϵ-Twin SVM [50], Least squares twin parametric-margin [51] among others. The Twin SVM has also been used for regression [52,53], and has proven to be efficient even in the primal formulation [54,55]. A comprehensive review of the variants and applications of Twin SVM has been presented by Ding et al. [56].The Twin SVM formulation involves solving the following set of QPPs.(7)minw(1),b(1),q12∥Aw(1)+e1b(1)∥2+c1e2Tq,(8)s.t.−(Bw(1)+e2b(1))+q≥e2,q≥0(9)minw(2),b(2),q12∥Bw(2)+e2b(2)∥2+c2e1Tq,s.t.−(Aw(2)+e1b(2))+q≥e1,q≥0,Here, data belonging to classes (+1) and (-1) are represented by matrices A and B respectively, e1 and e2 are vectors of ones of appropriate dimensions, q represents the error variable associated with the data point and c1, c2(>0) are hyperparameters.The Twin SVM is insensitive to imbalance in the class sizes. This is because it solves two smaller sized QPPs in order to find two non-parallel hyperplanes that pass through the respective classes. The first QPP tries to find a hyperplane that passes through points of class (+1) and is at least at unit distance from the points of the other class. This hyperplane is determined by solving the optimization problem given by Eqs. (10) and (11).(10)Minimize∑xi∈class(+1),i.e.yi=(+1)∥wTxi+b∥2+c1∑qi(11)subjecttotheconstraintswTxi+b+qi≤−1,forxi∈class(−1),i.e.yi=(−1)Note that only samples of class (−1) contribute to the constraints of this problem. The second optimization problem tries to find a hyperplane that passes through samples of class (−1) and is at a distance of at least one from samples of class (+1). This problem is given by(12)Minimize∑xi∈class(−1),i.e.yi=(−1)∥wTxi+b∥2+c2∑qi(13)subjecttotheconstraintswTxi+b+qi≥1,forxi∈class(+1),i.e.yi=(+1)While details of the Twin SVM may be found in the original paper [25], one can observe that the relative sizes of the two datasets are immaterial in this formulation. One solves for the two hyperplanes and then, for a test sample, determines which is the closer hyperplane and assigns the point to that class. The use of the Twin SVM imparts a second advantage to our approach; it works very well on multiclass problems such as the one in the present context. This is also evident from the formulation, where the optimization problem is governed only by the constraints of class (+1) in the first case and independent of the number of samples of the class (−1), and vice-versa for the second QPP.In practice, it is more efficient to obtain the dual formulation. For the problem indicated in Eqs. (7) and (9), the Lagrangian is given by Eq. (14):(14)L(w(1),b(1),q,α,β)=12||(Aw(1)+e1b(1))||2+c1e2Tq−αT(−(Bw(1)+e2b(1))+q−e2))−βTqThe KKT conditions can then be obtained as in Eqs. (15)–(20), taking α and β as Lagrange multipliers.(15)AT(Aw(1)+e1b(1))+BTα=0(16)e1T(Aw(1)+e1b(1))+e2Tα=0(17)c1e2−α−β=0(18)−(Bw(1)+e2b(1))+q≥e2,q≥0(19)αT(−(Bw(1)+e2b(1))+q−e2)=0,βTq=0(20)α≥0,β≥0As β≥0, from Eq. (17) we have(21)0≤α≤C1Combining Eqs. (15) and (16), we have(22)[ATe1T][Ae1][w(1)b(1)]T+[BTe2T]α=0We introduce the notation H=P=[A, e1],G=Q=[B,e2],u=[w(1)b(1)]T,v=[w(2)b(2)]Tusing which Eq. (22) can be re-written as(23)HTHu+GTα=0,or,u=−(HTH)−1GTαAfter basic mathematical manipulations, the dual formulation can be obtained for the Twin SVM, as shown by Eqs. (24) and (25):(24)maxαe2Tα−12αTG(HTH)−1GTαs.t.0≤α≤c1(25)maxβe1Tβ−12βTP(QTQ)−1PTβs.t.0≤β≤c2The solution to the Twin SVM formulation provides two non-parallel hyperplanes given by Eqs. (26) and (27):(26)xTw(1)+b(1)=0(27)xTw(2)+b(2)=0For a new test pointx∈ℝn, its class r, r∈{1, −1} is computed using Eq. (28):(28)xTw(r)+b(r)=minl=1,2|xTw(l)+b(l)|We expect the Twin SVM formulation to perform better on the datasets as compared to the L2-norm SVM formulation. The Twin SVM formulation based on generating non-parallel hyper-planes enables it to provide better separability for the features of the training data. Also, there are two hyper-planes and the final classification result is determined by both of these. These factors would contribute to the Twin SVM providing a better classification accuracy.To illustrate this point further, we use the SVM and Twin SVM on a synthetic unbalanced dataset. Consider the dataset shown in Fig. 7, where there are 12 points in class (+1) and 7 points in class (−1). Fig. 7(a) shows the hyperplane obtained by the standard SVM formulation, whereas Fig. 7(b) shows the non-parallel hyperplanes obtained using the Twin SVM. As is evident, the Twin SVM formulation yields two non-parallel hyperplanes, one of which is closer to points of class (+1), whereas the other is closer to points of class (−1).Thus for a test point, the Twin SVM would be able to give a better prediction as compared to the standard SVM as the prediction in the former case would be based on computing the distance of the test point from two hyperplanes, and assigning the class corresponding to the hyperplane to which the test point is closer to.During the online session of the BCI system usage, the EEG data pertaining to user intents is filtered in the selected frequency band, as identified by the classifiability measure. Features are extracted from EEG data filtered in this frequency band, and the trained classifier is used to identify the user intent.This section provides the results of using our approach on benchmark BCI Competition datasets, followed by a comprehensive analysis of the performance of our approach. Section  4.1 provides an overview of the datasets used, recording paradigm and the performance measures used. Section 4.2 provides the results of using our approach in a binary classification scenario, whereas Section 4.3 presents the results for multi-class scenarios. An attempt has been made in Section 4.4 to provide a visual explanation to why our approach provides superior performance, which is followed by an analysis of the lower computational cost of our approach is Section 4.5. All experiments were carried out on a system running 64-bit Windows 8 OS, with Intel i3 CPU @ 2.53GHz and 4 GB RAM.We used the classifiability based approach to find the optimal frequency bands on motor imagery datasets of subjects from BCI Competition III-A [57] and IV (dataset 2(b)) [58]. The dataset provided for BCI Competition III-A consists of four-class cued motor imagery (left hand, right hand, foot, tongue) for three subjects, whereas the dataset 2(b) from BCI Competition IV consists of cued motor imagery for two classes (left and right hand movement) for 9 subjects, with 3 sessions of each subject. For initial experiments, we use only the events corresponding to left and right imagined movements from BCI Competition III-A for two subjects which have been rated for good performance in the dataset.The recording protocol for obtaining the datasets is described next. The subject sat in a chair with armrests, wearing an EEG acquisition device. The task was to perform imagery of left hand, right hand, foot or tongue movements according to a cue for multiple sessions. The order of appearance of the cues was randomized to prevent adaptation [41]. A session consisted of several trials. The first 2s of a trial were quiet, and the subject was at rest. At t=2s, an acoustic stimulus indicated the beginning of the trial, and a cross was displayed; then from t=3s, an arrow to the left, right, up or down was displayed, and at the same time the subject was asked to imagine a left hand, right hand, tongue or foot movement, respectively, until the cross disappeared at t=7s.The recordings were made with a 64-channel EEG amplifier from Neuroscan. The EEG was sampled with 250Hz, it was filtered between 1 and 50Hz with Notch filter on and 60 EEG channels were recorded according to the 10–20 system [59] for the dataset of BCI Competition III-A, whereas 3 bipolar channels were recorded for dataset 2(b) of BCI Competition IV. The data of all runs was concatenated and converted into the GDF format.EEG signal processing was done using EEGLAB [60], a MATLAB plugin for EEG signal processing. We selected only the cortical and occipital channels of the recording for analysis. EEG data was filtered into different frequency bands from 8 to 30Hz, with each frequency band being 4Hz wide. Epochs corresponding to imagined left and right hand movements were selected from the filtered data and CSP was applied to extract features for each class of data. The value of k for the CSP filter as mentioned in Section 3.2 is chosen as 6. We computed the classifiability values for each band by varying the neighborhood distance d between 0 and 1. The classifiability value for features of a particular frequency band was chosen as the highest value obtained when varying the neighborhood distance d. The feature sets were classified using SVM classifier (LIBSVM [61] and Twin SVM). We used Radial Basis Function (RBF) kernel with 5-fold cross validation to obtain results using LIBSVM and Twin SVM. The model parameters for the SVM, specifically C for SVM and σ for RBF Kernel were chosen using grid-search [62] in the range 0–1.A statistical measure to determine the classification accuracy of the BCI Competition datasets considered in this paper has been in terms of the Kappa coefficient(κ). The computation of κ is determined based on two factors, the overall agreement, p0, which is same as the classification accuracy and the chance agreement denoted by pe, which represents the case when the classification is purely by chance. The Kappa coefficient is estimated in terms of these parameters as given by Eq. (29):(29)κ=p0−pe1−peIn a two-class scenario, the chance agreement pe=0.5. Hence κ=0 would imply no or pure-chance agreement between the actual and predicted labels, whereas κ=1 would indicate perfect classification. In a multi-class scenario having M classes, pe=1/M and the classification accuracy (p0) and Kappa coefficient are related as shown in Eq. (30). Hence we can directly compute the Kappa coefficient from the classification accuracy p0 as shown in Eq. (29) for two-class classification, or Eq. (30) for a multi-class scenario:(30)p0=Mκ−κ+1MWe discuss the results for the dataset from BCI Competition III-A in Section 4.2.1, followed by a comparison with the state-of-art technique of Filter Bank CSP using Dataset 2(b) of BCI Competition IV in Section 4.2.2.The classification accuracies (5-fold cross validation) for the trials and the corresponding classifiability values for the various frequency bands of the dataset from BCI Competition III-A is shown in Table 1.The results indicate that the classifiability measure computed for the various frequency bands is consistent with the classification accuracy obtained for that frequency band. There is also an increase in classification accuracy from the 8–30Hz band, when compared to the optimal frequency band for the subjects determined using the classifiability measure. For subject k6b, the optimal frequency band as determined using the classifiability measure is the 20–24Hz band, for which the classification accuracy is 88.6%±8.274 (LIBSVM) and 100% (Twin SVM). This is higher than the maximum accuracy of 71.78% (obtained using LIBSVM) in the larger mu-rhythm band of 8–30Hz. For the subject l1b, though the accuracy in the 8–30Hz band is 81.6%±9.83 (LIBSVM), comparably higher accuracy is obtained in the 26–30Hz band, 79.66%±16.329 (LIBSVM) and 100% (Twin SVM). A significantly higher cross-validation accuracy is obtained for all bands using Twin SVM, indicating that the formulation generalizes better on the training data to provide higher accuracy.Also, as compared to the classification accuracy obtained by the work in [39], our results indicate substantial improvement. For the subject k6b and l1b, the average classification accuracy is reported as 60–70% and 65–78% across tasks respectively. However, we obtain higher accuracy for both subjects, as already indicated. We choose to compare our results with those in [39] as they provide results using similar metrics on the same datasets.Further, we compare the performance of our approach against the state-of-art FBCSP approach [33] on the dataset 2(b) from BCI Competition IV in Table 2, as well as with other approaches based on BS [35], RQNN [38] and DDFBS [36]. The paper on FBCSP compares the average κ values across subjects and sessions, whereas the papers on BS, RQNN and DDFBS compare the best case κ values. Consequently, we provide average and best case κ values obtained using our approach for comparison in Table 2.As is evident from the results, our approach of using CSP, classifiability and Twin SVM outperforms the FBCSP approach for almost all subjects. We compare the results with those presented in Table 3 of [63], and observe that on the average, the FBCSP approach obtains a κ value of 0.493, whereas our approach attains a κ value of 0.526, which is a 6.6% improvement on the average across sessions and subjects. Also, when compared with the techniques that cite best case κ values, our approach outperforms BS [35] for 6 out of 9 subjects, and RQNN [38] and DDFBS [36] for 7 out of 9 subjects across sessions. In terms of average accuracy across subjects and sessions, we obtain an improvement of 21.9%, 13.16% and 43.22% over BS, RQNN and DDFBS respectively. This demonstrates that not only does our technique perform better than state-of-art feature selection methods such as BS, DDFBS, but also over modern classification techniques such as RQNN, apart from the conventional SVM.Moreover, our approach is more robust in the sense that it allows selection of the optimal frequency band not only for each subject, but also for each session. This provides a better solution to the issue of inter-subject and inter-session variability for motor-imagery EEG data.We also present a summary of the comparative analysis of our approach against state-of-art methods in Table 3. The results clearly indicate that our approach outperforms the state-of-art approaches.We also validate our approach for a multi-class scenario, where we use the dataset from BCI Competition III-A. The results in case of multi-class classification are indicated in Table 4. From the results, we observe that our approach selects the optimal frequency band and provides superior results than using LIBSVM. This further illustrates the superiority of using the Twin SVM for multi-class problems having unbalanced datasets, where the non-parallel hyper-plane formulation allows the Twin SVM to generalize better.We now proceed to provide a more detailed explanation to augment our claim regarding the Twin SVM's superior performance. Fig. 8shows the plot of the distances of the feature points from the hyper-planes obtained using Twin SVM. The red points represent features of the “imagined left movement” class and the green represent those of “imagined right movements”. Essentially, for each feature, Twin SVM makes the class prediction made by comparing its distance from the two hyper-planes. Thus, for greater classification margin, the distance of points from one hyper-plane must be higher than the other, and the converse holds for features of the other class. Comparing Fig. 8(a) and (b) and 8(c) and (d), we observe that there is a higher separation between the points in the optimal sub-bands for the respective subjects, which is reflected in the corresponding classification accuracy.In the case of prediction using L2-norm SVM formulation implemented using LIBSVM, the plots of the prediction values, which represent the distance on the side of the hyper-plane on which the point lies, are shown in Fig. 9. The Fig. 9(a) and 9(b) (for subjects k6b and l1b respectively) show the decision values obtained on the test set of the 5 fold classification. The actual labels are indicated using the markers ‘O’ and ‘X’ for left and right classes respectively. It is observed that the prediction values lie close to zero, indicating that the separation margin obtained by the classifier is low, which explains the lower classification accuracy in comparison to Twin SVM.In conclusion, it is observed that the use of Twin SVM and classifiability gives far better classification results. Using our approach, up to 20% improvement has been obtained for binary classification using dataset from BCI Competition III-A, whereas 6.6% improvement has been attained over state-of-art technique using dataset 2(b) from BCI Competition IV, which involves multiple subjects and sessions. We have also obtained higher classification accuracy in the multi-class scenario over LIBSVM.We also present a comparison of the computational cost of using our approach against FBCSP [33]. For this experiment, we used EEG data of a single session of the dataset 2(b) from BCI Competition IV. The dataset has 60 trials from 3 bi-polar EEG channels, at a sampling rate of 250Hz. We implemented our approach as well as the FBCSP approach (using BCILAB toolbox). The comparison of the computational time has been obtained using the MATLAB profiler and is shown in Fig. 10.The time shown involves the time to read the EEG data, extract EEG around event markers, filter the EEG data into various frequency bands, compute the features, perform feature selection and compute the classification accuracy using ten-fold cross validation. As can be observed from Fig. 10(a), our approach of using classifiability and Twin SVM takes 37s, whereas the FBCSP approach takes 91s as in Fig. 10(b). Specifically, our approach takes 18.59s to compute classifiability, and 12.24s to compute features in various frequency bands. Classification using Twin SVM takes 1.809 and 1.014s to compute the two hyper-planes, whereas tuning the SVM parameters using grid search takes 3.380s. The lower computation cost of our approach is attributed to the fact that we do not use cross-validation for feature selection, and our classifier solves smaller sized QPPs.

@&#CONCLUSIONS@&#
