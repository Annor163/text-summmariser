@&#MAIN-TITLE@&#
Emotions ontology for collaborative modelling and learning of emotional responses

@&#HIGHLIGHTS@&#
Affective applications require a common way to represent emotion knowledge.Ontologies provide rich semantic models for emotion knowledge modelling.EmotionsOnto is a generic ontology for describing emotions.EmotionsOnto is used in EmoCS to collaboratively collect emotion common sense.Currently, emotion from user input but Brain–Computer Interfaces being tested.

@&#KEYPHRASES@&#
Emotion,Ontology,Collaborative learning,Social networks,Knowledge representation,Affective computing,

@&#ABSTRACT@&#
Emotions-aware applications are getting a lot of attention as a way to improve the user experience, and also thanks to increasingly affordable Brain–Computer Interfaces (BCI). Thus, projects collecting emotion-related data are proliferating, like social networks sentiment analysis or tracking students’ engagement to reduce Massive Online Open Courses (MOOCs) drop out rates. All them require a common way to represent emotions so it can be more easily integrated, shared and reused by applications improving user experience. Due to the complexity of this data, our proposal is to use rich semantic models based on ontology. EmotionsOnto is a generic ontology for describing emotions and their detection and expression systems taking contextual and multimodal elements into account. The ontology has been applied in the context of EmoCS, a project that collaboratively collects emotion common sense and models it using the EmotionsOnto and other ontologies. Currently, emotion input is provided manually by users. However, experiments are being conduced to automatically measure users’s emotional states using Brain–Computer Interfaces.

@&#INTRODUCTION@&#
The emotional dimension of the interaction of humans with computers was considered for a long time a marginal factor (Brave & Nass, 2002). However, human beings are eminently emotional, as their social interaction is based on the ability to communicate their emotions and to perceive the emotional states of others. In this sense, emotions must be taken into account when implementing computing systems as a mechanism to improve the user experience.Consequently, the study of emotion as a key faction in Human–Computer Interaction has gained a lot of traction resulting in well-established research and applications areas like affective computing. This paradigm deals with detecting, interpreting and responding to user’s emotions when developing systems and devices (Picard, 2000).Interest in this area is driven by a wide spectrum of promising applications, such as virtual reality, smart surveillance or perceptual interfaces (Tao & Tan, 2005). More recently, emotion-awareness has been highlighted as a key issue in online learning environments, especially in Massive Open Online Courses (MOOCs). In this case, massiveness makes it impossible to maintain a direct contact between instructors and learners that facilitates adapting the learning experience to learners needs while keeping their engagement.This, among others factors, seem to explain the big dropout rate of most MOOCs, usually above 90% (Yang, Sinha, Adamson, & Rosé, 2013). Therefore, techniques like sentiment analysis have been applied to try to anticipate students’ dropout (Wen, Yang, & Rosé, 2014). However, it is not enough to anticipate it, mechanisms should be provided to improve engagement and minimise it.Therefore, the objective, in the case of MOOCs but also other Human–Computer interactions, should be to model, track and influence emotion through the concept of “emotional affordance” (Cheng, 2014), defined as the characteristics of a situation that influence emotions, as perceived by the people in that situation.However, there are a great variety of theoretical models of emotions and there are different technologies that can be used for their implementation. Although there are many common properties, emotions are not universal: they are differently expressed in different cultures and languages, while many emotional properties are individual.There is rarely a one-size-fits-all solution for the growing variety of computer users and interactions (Obrenovic, Garay, López, Fajardo, & Cearreta, 2005). Therefore, emotion-aware applications should be designed in a flexible way so a wider class of users can use them. In this way, personalisation is necessary for more efficient interaction, better tuning and acceptation of developed systems.There is a broad terminology related to affective states in human beings. The term “emotion” tends to be used in a broad sense, especially in technological contexts. Scherer (Scherer, 2000) proposed a number of taxonomies for these affective states. The original list was later modified and redefined in Douglas-Cowie and et al. (2006). This new list includes: Attitudes, Established emotion, Emergent Emotion (full-blown), Emergent Emotion (suppressed), Moods, Partial emotion (topic shifting), Partial emotion (simmering), Stance towards person, Stance towards object/situation, Interpersonal bonds, Altered state of arousal, Altered state of control, Altered state of seriousness and Emotionless.This paper focuses on Emergent Emotion (full-blown), instead of a global taxonomy of affective states. This is made to reduce the complexity of proposed domain so it is easier to deal with. Besides, for the same reason, focus is mainly devoted to emotion detection and expression systems instead of modelling internal emotion processing in humans.In Douglas-Cowie et al. (2006), Emergent Emotion (full-blown) is defined as “states where the person’s whole system is caught up in the way they react to a particular person or situation”. It involves aspects such as:•Distinctive positive or negative feelings about the people or situations involved.Impulses to act or express yourself in particular ways and avoid others.Distinctive changes in your body, for instance in your heart rate or tendency to sweat.Emotion does not last very long – it comes on quite quickly, and dies down reasonably soon (unless there is something very unusual happening).Our contribution is to overcome the above mentioned limitations referring to the lack of flexibility when personalising affective computing applications by providing a generic ontology for describing detection and expression systems related with emotions, while taking contextual and multimodal elements into account. The ontology is proposed as a way to develop an easily implementable formal model, as it is based on the Web Ontology Language (OWL) standard (W3C OWL Working Group, 2012). However, the knowledge about emotions in the ontology can be used by affective computing applications with independence of underneath technology.The proposed ontology in this paper is based on a generic model geared towards capturing the entities that take part in the Emergent Emotion process (López, Gil, García, Cearreta, & Garay, 2008). The model is presented in Section 3 and it has been formalised as an ontology following a classical ontology engineering methodology (García, 2006). The ontology has been developed to be as agnostic as possible regarding existing emotion theories. This way, developers of affective resources are not tied to a given theoretical approach.In this sense, developers can use different theoretical approaches in the same ontological and technological framework. For instance, the categorical theory of emotions (Ekman, 1984), the dimensional one (Lang, 1979) or the one based on appraisal (Scherer, 1999).Consequently, the ontology can help implementing emotion-aware applications based on a wider range of theoretical approaches. This flexibility and wide applicability of EmotionsOnto is in great part due to the fact that it is capable of modelling contexts. In order to do that, the DOLCE upper ontology (Gangemi, Guarino, Masolo, Oltramari, & Schneider, 2002) is reused and extended, particularly the Description and Situation concepts.Descriptions correspond to the representations for the situations, which then trigger and are associated with emotions. Moreover, in order to cope with the enormous range of different situations that might need to be associated with emotions, they are modelled using the building blocks provided by FrameNet (Scheffczyk, Baker, & Narayanan, 2008). It is a big lexical database, with more than 10,000 word senses, structured following Frame Semantics (Fillmore, 2006). Frames fit really well with situations modelling as they try to explain words meaning by building a description of a type of event, relation, or entity and the participants in it.The resulting ontology, EmotionsOnto, is validated in a real-world application that manages affective information, the Emotions Common Sense (EmoCS) initiative. It is inspired by the Open Mind Common Sense1Open Mind Common Sense, http://commons.media.mit.edu.1initiative, but integrating emotional feedback together with the common sense sentences contributed by users. In order to help understanding and automatising users input to guide emotions-aware applications, EmoCS provides dynamic forms with autocomplete features that guide the user while building structured descriptions of the situations to be associated with emotions, as detailed in Section 4. Moreover, we have started to explore the use of Brain–Computer Interfaces to automate emotional input gathering, as also reported in Section 4.The rest of the paper is structured as follows. The next section presents several theories and concepts relevant for describing emotions, together with several topics related to ontologies and emotions. Then, the conceptual model underlying the proposed EmotionsOnto is introduced in Section 3. Then, the EmoCS application scenario is presented in Section 4. Finally, conclusions and future work conclude this paper in Section 5.

@&#CONCLUSIONS@&#
Affectivity in human beings is a very complex term where a lot of multidisciplinary research has been performed from different fields. Providing a computerised basis to perform a knowledge base that allows dealing with affective related concepts such as emotions requires a ground. Performed work tries to provide such ground. In this paper we present a generic model for describing emotions and their detection and expression systems taking contextual and multimodal elements into account. The model is formalised as an easily computerised ontology.The ontology has been developed to be as totally agnostic as possible regarding existing emotion theories. Although this fact can seem to be surprising, it is made due to allow independence from theoretical approaches, so developers of affective resources are not tied to a given theoretical approach. In this sense, different theoretical approaches can be used by the developers.Thus, it makes the ontology valid for a wide range of proposed theoretical approaches and applications domain, like students’ engagement in MOOCs. It is remarkable that context has received little attention regarding emotion-aware application development. This work takes this concept into consideration as a necessary component for modelling emotion. In this sense, proposed ontology is based on the definition of relevant contextual elements.Proposed ontology does also have implications regarding emotion-aware applications development. On the one hand, it allows integrating emotion-related context and multimedia elements. On the other hand, defining sensors in the way performed allows making use of them as user interface elements for inputs (such as text through the keyboard, utterances through the microphone) and outputs (such as emotional pictures in user interfaces, text output on the screen or embodied avatars).They can be linked with emotion expression and recognition systems. This approach has proven to be very useful for describing how emotion-aware applications work. Finally, it must be highlighted that being performed ontology developed using standards like OWL, it is also valid for a wide range of software development technologies and environments and thus can be used as a basis to engineer emotion-aware applications.The ontology has been applied in the context of the Emotions Common Sense project, inspired by MIT’s Open Mind Common Sense. It enriches that initiative by providing also emotional input and by providing more structured user input based on EmotionsOnto and other ontologies that can be more easily put into practice to build emotions-aware applications.Future work from the EmotionsOnto perspective focuses on extending the ontology beyond Emergent Emotion. The first extension considered is to model affective states in humans in order to make the ontology capable of modelling more complex aspects of human affectivity. This will make possible to model users bearing above-mentioned affective states in mind. These enriched user models enable including aspects related with user disabilities and developing applications even more adapted to their needs. Finally, the inclusion of social context in the ontology allows exploring emotion in computerised social environments such as social networks.On the other hand, from the evaluation of EmotionsOnto in the context of EmoCS, future work aims to provide automatic emotion annotations for situation descriptions based on the data collected by the Emotiv EEG neuroheadset. Currently, we are collecting raw data from EEG sensors and processing it. Training sets, associating situations to expected emotions and direct user feedback, are being user together with machine learning algorithms to train a system capable of producing Ekman emotions out of the EEG data.Moreover, neuroheadset measurements are going to be complemented with other kinds of sensor to increase robustness. A preliminary study of the state of the art shows that measuring skin conductance gives a quite reliable measure of stress that can complement EEG readings to better categorise the corresponding emotions (Ekman, 1992).