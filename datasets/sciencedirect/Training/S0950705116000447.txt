@&#MAIN-TITLE@&#
Data calibration for statistical-based assessment in constraint-based tutors

@&#HIGHLIGHTS@&#
Item Response Theory models for constraint-based intelligent tutoring systems.Data-driven assessment of problem solving tasks.Data filtering criteria for Item Response Theory parameters estimation.Best model fit selection criteria.

@&#KEYPHRASES@&#
Learning analytics,Assessment,Constraint-Based Modeling,Intelligent Tutoring Systems,Item Response Theory,

@&#ABSTRACT@&#
Intelligent Tutoring Systems (ITSs) are one of a wide range of learning environments, where the main activity is problem solving. One of the most successful approaches for implementing ITSs is Constraint-Based Modeling (CBM). Constraint-based tutors have been successfully used as drill-and-practice environments for learning. More recently CBM tutors have been complemented with a model derived from the field of Psychometrics. The goal of this synergy is to provide CBM tutors with a data-driven and sound mechanism of assessment, which mainly consists in applying the principles of Item Response Theory (IRT). The result of this synergy is, therefore, a formal approach that allows carrying out assessments of performance on problem solving tasks. Several previous studies were conducted proving the validity and utility of this combined approach with small groups of students, in short periods of time and using systems designed specifically for assessment purposes. In this paper, the approach has been extended and adapted to deal with a large set of students who used an ITS over a long period of time. The main research questions addressed in this paper are: (1) Which IRT models are more suitable to be used in a constrained-based tutor? (2) Can data collected from the ITS be used as a source for calibrating the constraints characteristic curves? (3) Which is the best strategy to assemble data for calibration? To answer these questions, we have analyzed three years of data from SQL-Tutor.Intelligent Tutoring SystemArtificial Intelligence in EducationConstraint-Based ModelingItem Response TheoryEvidence Centered DesignItem Characteristic CurveBayesian NetworkConstraint Characteristic Curve

@&#INTRODUCTION@&#
Intelligent Tutoring Systems (ITSs) are probably the most well-known product of the Artificial Intelligence in Education (AIED) research community. ITSs are environments that help student learn a subject matter. To do that, they use a knowledge base that is comprised of a student model and a domain model, modeling what the student knows and what to teach, respectively. The teaching process of an ITS consists of consulting the knowledge base and adapting the content and tutorial actions according to the student model. This behavior tries to mimic an expert human teacher who adapts the process to every individual student. Perhaps the most extended interaction pattern an ITS provides is an environment where students can solve problems belonging to certain domain matter. According to Jonassen [18], “most educators agree that problem solving is among the most meaningful and important kinds of learning and thinking”. A problem exists when a problem solver has a goal but does not know how to reach it. Problem solving is a mental activity aimed at finding a solution to a certain problem [3]. The challenge of solving a problem forces students to build models through a process of understanding, exploring and interacting with the world, developing several branches of science at all levels of education [30]. Thus, problem solving entails cognitive processing with the goal of transforming a given situation into a desired scenario when no obvious method of solution is available to the problem solver [21]. According to Mayer [22] problem solving expertise can be decomposed into four components:1Problem translation, where the student transforms the problem stem into an internal mental representation.Problem integration, a mental model of the situation described in the problem stem is constructed.Solution planning, where the strategy to solve the problem is determined, i.e. the steps to take in order to solve the problem. This component requires the student to apply his/her procedural knowledge.Solution execution, that is, the previous plan is applied to solve the problem.Constraint-Based Modeling (CBM) [39] is one of the most popular approaches for developing ITSs [8,43]. Its effectiveness as an instructional methodology has been proved in a range of tutors and studies performed over 15 years [33,35,37,38]. A characteristic that makes it a very attractive approach is its ability to be applied in a tutoring system easily since it does not require a complex architecture. Furthermore, it does not require identifying all possible steps a student could take to reach a solution to a problem. Instead, it only requires the identification of domain principles (represented as constraints) that no solution should violate.Educational assessment characterizes aspects of student knowledge, skill, abilities, or other attributes. For this characterization it makes inferences from the observation of what they say, do, or make in certain kinds of situations [5]. Furthermore, educational assessment provides at least three different uses in instructional improvement [3]: first, results obtained through assessment motivate students and educational staff to achieve the academic goals set by policy makers. In addition, it represents a way of helping teachers to plan or revise their pedagogical strategies. Finally, assessment can be used to help stimulate deep understanding. The use of computers in testing is extensive nowadays. In the area of problem solving, however, there is still an enormous range of opportunities to explore [3,52]. Problem solving activities require students to apply their knowledge in constructing a solution to a certain situation [23]. One of the most recognized assessment techniques is Item Response Theory (IRT), which gave rise to a set of different models with different assumptions (see next section).In our previous work [14,15] we made a first proposal of a model of assessment combining CBM with IRT. This proposal can also be seen as an implementation of the Evidence Centered Design (ECD) framework [1,29,41], which focuses on providing a generic methodology to perform assessments of problem solving. This synergy between the AIED and psychometric mechanisms opens the door to enhancing ITSs with new methods to perform automatic assessment of tasks that, if carried out by a human expert, would be highly difficult and prone to subjectivity. As will be explained later, the utilization of IRT makes it possible to apply new formal psychometric methods in CBM that were not possible before. In the same way, some of the fundamentals of CBM extend the typical use of IRT in testing environments, where theoretical concepts are assessed, to ITS, which requires applying practical knowledge to solve a problem.Initially, in order to explore the validity of the approach for assessment purposes, two educational systems were developed and tested with undergraduate students of the University of Malaga in Spain [13–15]. Although the knowledge base of these ITSs was developed in well-defined domains, according to the classification made by Mitrovic and Weerasinghe [36], the tasks involved were completely different. In the first system, focused on the Simplex algorithm for mathematical optimization, the number of constraints was small and the tasks were well-defined (i.e. those tasks for which the process of solving them is known). On the other hand, the second system, focused on teaching fundamentals of Object Oriented Programming, had a relatively large number of constraints and the tasks were ill defined with a complex solution procedure (having more than one solution or many ways to achieve it).Initial results obtained using CBM and IRT showed that the methodology was feasible and promising in these types of domains. Nevertheless, the experiments were carried out in systems constructed for assessment purposes, with a small group of students, using a particular IRT model and strictly following the restrictions imposed by the IRT to guarantee valid assessment results under this theory. To the contrary, the most successful CBM-based systems have been used mainly for learning purposes in drill-and-practice environments. That means that a student is allowed to solve the same problems several times which leads to the violation of the IRT models assumed hypotheses (i.e. student knowledge is constant during a session). This difference makes it necessary to explore the scalability and validity of the existing models based on the combination of IRT with CBM in tutoring systems used for learning purposes and with a large number of students.The research carried out in this paper tries to cover the aforementioned problems by extending the existing methodology (explained in detail in the following sections) and performing a study with a larger dataset obtained over three years of use of the SQL-Tutor [34]. The aims of the study are: (1) to define an appropriate methodology to accommodate IRT models to constraint-based tutors; (2) to determine the most appropriate IRT models in this case; and (3) to explore different strategies for grouping and filtering existing ITS data to be used for the IRT calibration process. The advantages of using this approach are that it provides a data-driven technique that does not require heuristic knowledge. The resulting ITS would be adjusted by standard statistical calibration procedures that are not biased with the designer subjectivity.The paper is structured as follows: Section 2 presents the theoretical background needed to understand both the model and the calibration strategies presented in this paper. Section 3 describes the related work in the field of AIED. Section 4 is devoted to a formalization of our assessment model and a generalization of that model to be used for ITS under the Evidence-Centered Design framework; it also outlines the drawbacks of the early approach. Section 5 proposes a new methodology to overcome the limitations of our proposal with several strategies that can be performed in the process of calibration. Section 6 describes the experiments and the methodological issues and Section 7 presents and discusses the results. Finally, conclusions are summarized in Section 8.The approach for assessment in ITSs is based on two main pillars, corresponding to the two methodologies already mentioned: CBM for modeling the ITS domain, and the IRT for assessing the student's knowledge in terms of the evidence provided by him/her while solving problems. Both techniques are summarized here. Moreover, the system used in this paper, i.e. SQL-Tutor, is also described briefly.The first element of the methodology is the CBM paradigm for building ITSs, which will be the instrument through which students’ evidence is gathered. CBM is based on Ohlsson's theory of learning from performance errors [39,40], according to which incomplete or incorrect student's knowledge can be used within an ITS to provide guidance. This faulty knowledge is detected using constraints, which are the key element of CBM. Constraints are principles that must be followed by all correct solutions in the given instructional domain. If the student's solution violates any constraints, it is incorrect and the system provides the student with the appropriate feedback for remediation. Each constraint consists therefore of an ordered pair (Cr, Cs), where Cris the relevance condition and Csis the satisfaction condition [33]:If < relevance condition Cr > is true,then < satisfaction condition Cs > had better also be true.The application of CBM is very simple, since only an inference engine and the appropriate representation of the solution are required [31]. Accordingly, once the student has finished solving a problem (or it can also be done before by student demand), constraints are checked against the student's solution using simple pattern matching. Constraints are only applied to solutions for which they are relevant (as determined by the relevance condition of each constraint). The satisfaction condition of a relevant constraint specifies properties that the solution must fulfill to be correct. The set of constraints and problems that can be presented to students form the domain model of a particular tutor. The performance of a student with respect to the constraints, i.e., the list of violated and satisfied constraints in each solution take part of his/her student model.In this paper, we have used data from one of the most popular and successful constraint-based tutors, SQL-Tutor [34]. Although its main source of users comes from the students enrolled in database courses at the University of Canterbury in New Zealand, it is available worldwide via the DatabasePlace portal established by Addison Wesley,11http://www.aw.com/databaseplacedemo/sqltutor.html.which uses SQL-Tutor and two other tutors developed in the databases domain [32].SQL-Tutor teaches SQL queries, which is the dominant relational database query language. It is designed to help undergraduate students with their difficulties mastering the subject. Although SQL is a simple and well-structured language, students find it difficult to learn due to the advanced concepts and cognitive overload [45] associated with this type of problem, which is a result of having to keep in mind many details involved in the problem that is being solved.The interface of SQL-Tutor reduces the working memory load by displaying the database schema and other information related to the problem (see Fig.1). Without this information, the student would have to keep in mind the structure of the database or handle it by other means. Besides, the system presents the parts of the solution, simplifying the problem in different subgoals, each one associated with the building of a particular component.The correctness of a student's solution can be verified by submitting it to the system. Incomplete solutions can be submitted too. The system compares the student's solution to the constraints. SQL-Tutor's domain model is comprised of a huge set of constraints, with more than 700 defined so far. This can give the reader an idea about the difference in magnitude between the data that can be obtained with this system, with respect to the systems used in existing studies, where the most complex domain was comprised of 87 constraints and the simplest had 18. Examples of constraints are shown in Fig. 2.The violations and satisfactions of the constraints are used to inform the students about their mistakes. The system provides feedback in increasing levels of detail, starting from one that gives little information to one that gives the complete solution [20]. The history of use of each constraint is stored in the student model, showing for each attempt whether the constraint was used correctly or whether it was violated.Simultaneously with the process described above, the system records all relevant activities of each student in a log file. This includes all the results that affect the student model and the scaffolding information. This log file containing qualitative information about the students has been the source of evidence used in the research presented in this paper.The second pillar of our assessment model a well-founded technique specifically developed for assessment, i.e. the IRT [49]. This theory assumes that a latent trait (i.e. the student knowledge level) can be inferred from the student's answers to independent questions or items, which provides evidence based on conditional probabilities named the Item Characteristic Curve (ICC) [17]. Its main advantage in comparison with other assessment techniques is the invariance of measurement. This means that the assessment score is independent of the instrument of measure being used and, thus, the same score would be obtained in any test taken [16].The ICC, which is probably the most important concept in the IRT, models the probability of answering a question correctly given the student knowledge. Fig. 3 illustrates the shape of the ICC. As can be seen, the greater the knowledge value (x axis), the higher the probability of giving a correct response (y axis). There are different IRT models based on different ICC functions. This figure contains what is probably the most popular function that implements the ICC, i.e. the 3 Parameters Logistic (3PL), which is also depicted in the equation below:(1)P(ui=1|θ)=ci+(1−ci)11+e−1.7ai(θ−bi)Here,P(ui=1|θ)represents the probability of correctly answering the item i, given a student's knowledge level θ within the interval (−∞,…,+∞). The correctness of the question is represented withui=1, otherwise 0 would be used to reflect a wrong state. The other elements in the equation are the three parameters characteristic of the 3PL function:•aiis called discrimination factor and is a value proportional to the slope of the curve. The greater this value, the higher the distinction between different student's knowledge levels.bi, also called difficulty, is the value of θ for which the probability of answering correctly is the same as answering wrongly.The last parameter, ci, is the guessing factor and represents the probability of a student without knowledge answering correctly.Only those models whose items can be assessed as correct or incorrect, i.e. the dichotomous models, are considered here, such as the Two Parameters Logistic (2PL) or the One Parameter Logistic (1PL). Both of them are simplifications of the 3PL function: the 2PL is equivalent to the 3PL but the guessing parameter would be 0, and the 1PL is equivalent to the 2PL but fixing the discrimination parameter to a given value, i.e. ai=1. However, there are other approaches, e.g., the polytomous models, where more than two answers are allowed and therefore partial credit to items can be given [17]. This initial decision is congruent because constraints are dichotomous.Using the ICCs, and assuming (1) item independence; and (2) constant knowledge throughout the session, the knowledge of the jth student θjcan be computed as shown is equation:(2)P(θj)=∏i=1nP(ui=1|θj)uij[1−P(ui=1|θj)]1−uijwhere P(θj) is the jth student knowledge distribution; n is the number of items administered to the student;uij=1indicates that the jth student's answer to item i was correct, otherwiseuij=0.The likelihood function of a given set of response patterns is therefore:(3)L(u|ai,bi,ci,θj)=∏j=1N∏i=1nP(ui=1|θj)uij[1−P(ui=1|θj)]1−uijwhere N is the total number of students.There are different techniques for estimating the model parameters ai, bi, ciand the students’ knowledge θjthat maximizes this function. One of them is the Marginal Maximum likelihood (MML). This process is known as calibration and is carried out with the help of the computer program Multilog [48].In order to compare the goodness of fit of two different models, with a different number of parameters, the ratio of the likelihood function can be used. The test statistic is twice the difference in these log-likelihoods:(4)D=−2ln(L1L2)=−2ln(L1)+2ln(L2)≈χ2(g)where g is the degree of freedom, which is computed as the difference in the number of parameters of the two models. Multilog output includes the negative-twice-log-likelihood value for each model calibration. A model with more parameters will always fit at least as well (have an equal or lower negative-twice-log-likelihood). Whether it fits significantly better and should thus be preferred is determined by deriving the probability or p-Value of the difference D.

@&#CONCLUSIONS@&#
Assessment is an important part of any learning process since it is used as a way to determine the starting knowledge state of the student, how this knowledge evolves during the instruction and, at the end of this process, to compute the level of achievement. In computer-based educational research, one of the challenges is the construction of problem-based environments. Automatic assessment of these kinds of tasks (i.e. the problems or complex exercises) is complicated due to the complexity of the knowledge required to be applied by the student. The combination of CBM and IRT can be used as a well-founded approach for this type of assessment.When the technique is applied to the data of a CBM system for learning purposes with a large number of students using the system and multiple sessions over long periods of time, some limitations have to be taken into account. The main limitation is related to the way in which characteristic curves are calibrated. Calibration is an important previous stage when assessment is accomplished with data-driven theories such as IRT. One of the requirements of IRT to accomplish calibration is to have available datasets of students’ performance where the knowledge of each individual had to be kept constant. This means that during the process of collecting this information, no learning could happen. This requirement is difficult to satisfy when data is taken from learning environments.To study the applicability of different calibration strategies in a real environment, we used log data from SQL-Tutor collected over three years. To guarantee that this principle is met we have introduced two concepts, i.e. the “CK-session” and the “virtual student”, and described three grouping strategies to construct performance matrices from the raw data obtained from the ITS to be used to calibrate the IRT models. Additionally, some data filtering was needed to reduce the “noise” of the data obtained from a ITS. The main conclusion is that better results are obtained by discarding constraints with low variability, and that the IRT models are better adjusted if we consider a “virtual student” for each resolution of a single problem in the ITS. Gathering evidence through problems would produce higher-quality CCCs during the calibration phase.In addition, we have explored the performance of the three most commonly used IRT models. The goodness of model fit has been measured using the output of the Multilog tool with different combinations of assembling criteria. The results suggest that the 2PL model is the most suitable to for use with CBM constraints in all cases, and that there is no reason to use the 3PL model, which requires more data to be calibrated and fails to provide any significant improvements.In order to implement any of these calibration approaches in future ITS the conclusions obtained in the study presented here could be taken into account as a guideline. The utilization of these techniques produces a more accurate calibration of the basic elements of the system knowledge base, the CCCs. Furthermore, we would like to explore the performance of this methodology in an ITS to study the improvement in terms of learning that this approach could provide.