@&#MAIN-TITLE@&#
Cautious label ranking with label-wise decomposition

@&#HIGHLIGHTS@&#
We propose to rank labels using label-wise information.We consider the use of probability sets over ranks to produce cautious predictions.We approximate the solution of the corresponding bilinear problem.We perform some first experiment showing the potential benefits of the approach.

@&#KEYPHRASES@&#
Label ranking,Label-wise decomposition,Assignment problem,Bilinear optimization,

@&#ABSTRACT@&#
In this paper, we are interested in the label ranking problem. We are more specifically interested in the recent trend consisting in predicting partial but more accurate (i.e., making less incorrect statements) orders rather than complete ones. To do so, we propose a ranking method based on label-wise decomposition. We estimate an imprecise probabilistic model over each label rank and we infer a partial order from it using optimization techniques. This leads to new results concerning a particular bilinear assignment problem. Finally, we provide some experiments showing the feasibility of our method.

@&#INTRODUCTION@&#
In recent years, machine learning problems with structured outputs received an increasing interest. These problems appear in a variety of fields, including biology (Weskamp, Hullermeier, Kuhn, & Klebe, 2007), natural language treatment (Bordes, Glorot, Weston, & Bengio, 2012), image analysis (Geng, 2014), etc.In this paper, we deal with the problem of label ranking, where one has to learn a mapping from instances to rankings (complete orders) defined over a finite number of labels. Various ways have been proposed to solve the problem, most of them intending to reduce the initial complexity of the problem. Some approaches propose to fit a probabilistic ranking model with few parameters (Mallows (Cheng, Hühn, & Hüllermeier, 2009), Plackett–Luce (Marden, 1996)) using different approaches (Cheng, Dembczynski, & Hüllermeier, 2010a; Meila & Chen, 2010). Other approaches tend to decompose the problem. Ranking by pairwise comparison (RPC) (Hüllermeier, Furnkranz, Cheng, & Brinker, 2008) transforms the problem of label ranking into binary problems, combining the results to obtain the final ranking. Constraint classification and log-linear models (Dekel, Manning, & Singer, 2003; Har-peled, Roth, & Zimak, 2002) learn, for each label, a (linear) utility function from which the ranking is deduced. More recently, some authors (Cheng & Hüllermeier, 2013) have proposed to solve the problem by performing a label-wise (rather than a more classical pairwise) decomposition.Additionally, some authors (Cheng, Rademaker, De Baets, & Hüllermeier, 2010b) have discussed the interest, in preference learning problems and in label ranking in particular, to predict partial orders rather than complete rankings. Such an approach can be seen as an extension of the reject option (Bartlett & Wegkamp, 2008) or of the fact of making partial predictions (Corani, Antonucci, & Zaffalon, 2012). Such cautious predictions can prevent harmful decisions based on incorrect predictions. In practice, current methods (Cheng et al., 2010b) exploit pairwise information and consist in thresholding a pairwise comparison matrix containing probabilistic estimates. It has been recently shown that such methods, when coupled with parametric probabilistic models (Plackett–Luce and Mallows), are particularly interesting, as they are guaranteed to produce semi-orders, thus avoiding the presence of cycles in predicted relations.In this paper, we retain the recent ideas of label-wise decomposition, and explore how partial predictions can be obtained from them. More precisely, we propose to learn for each label an imprecise probabilistic model of its rank, and use these models to infer a partial prediction, using robust optimization techniques. Note that imprecise probabilistic approaches are well tailored to make partial predictions (Corani et al., 2012), as well as to deal with incomplete data (Zaffalon, 2002a).Section 2 introduces the problem and our notations. Section 3 shows how rank can be predicted from imprecise probabilistic models. Section 4 presents the proposed inference method based on bilinear optimization techniques. Finally, Section 5 shows some experiments on synthetic data sets.The usual goal of classification problems is to associate an instance x coming from an instance spaceXto a single (preferred) label of the space Λ = {λ1,…,λk} of possible classes. Label ranking problems correspond to the case where an instance x is no longer associated to a single label of Λ but to a total order over the labels, that is a connected, transitive, and asymmetric relation ≻xover Λ × Λ, that amounts to give a rank to each label λ1,…,λk. Hence, the space of prediction is now the whole setL(Λ)of complete rankings of Λ that contains|L(Λ)|=k!elements (i.e., the set of all permutations). Fig. 1illustrates a label ranking data setDwith k = 3.We can identify a ranking ≻xwith a permutation σxon {1,…,k} such that σx(i) < σx(j) iff λi≻xλj, as they are in one-to-one correspondence. σx(i) is the rank of label λiin the ranking ≻x. In the label-wise decomposition method introduced later on, we will often refer to the assignment matrix yxof ranking, which is a k × k Boolean matrix (yx, ij∈ {0, 1}) of elements yx, ijsuch that(1)∑i=1kyx,ij=1,j=1,…,k,∑j=1kyx,ij=1,i=1,…,k.The value yx, ij= 1 means that label λihas rank j (σx(i) = j), and constraints (1) ensure that each label has a different rank (hence defining a proper order). We denote by Y the set of all possible assignment matrices. Note that there is a one-to-one correspondence between assignment matrices, permutations and complete rankings. In the following, we will use these terms (rankings, permutations, assignment matrices) interchangeably.Example 1Consider the set Λ = {λ1, λ2, λ3} and the observation λ3≻λ1≻λ2, then we haveσx(1)=2,σx(2)=3,σx(3)=1and the associated assignment matrix y is(010001100)The task in label ranking is the same as the task in usual classification: to use the training instancesD={(xi,yi)|i=1,…,n}to estimate the theoretical conditional probability measurePx:2L(Λ)→[0,1]associated to an instancex∈X. However, in label ranking problems the size ofL(Λ)quickly increases, even when k is small (for instance,|L(Λ)|≃1012for k = 15). This makes the estimation of Pxdifficult and potentially quite inaccurate if only little data is available, hence an increased interest in providing accurate yet possibly partial predictions. This rapid increase of|L(Λ)|also means that estimating directly Pxis in general not doable, except for very small problems. The most usual means to solve this issue is either to decompose the problem into many simpler ones or to assume that Pxfollows some parametric law. In this paper, we shall focus on a label-wise decomposition of the problem. To simplify notations, we will drop the subscriptxin the following when there is no possible ambiguity.This section explains how the original label-ranking problem can be reduced to ordinal regression problems in a label-wise manner. Indeed, since in when an observation is precise (i.e., corresponds to a complete ranking over all labels), each label can be associated to a unique rank, a natural idea is to learn a probabilistic model pi: K → [0, 1] with K = {1, 2,…,k} and where pij= pi(j) is interpreted as the probability P(σ(i) = j) that label λihas rank j.A first step is to decompose the original data setDinto k data setsDj={(xi,σxi(j))|i=1,…,n},j = 1,…,k. The decomposition procedure is illustrated by Fig. 2. Estimating the probabilities pijfor a given label λithen comes down to solving an ordinal regression problem. A natural way to estimate the expected cost cijof assigning label λito rank j is then to consider a distanceD:K×K→Rbetween ranks and to compute(2)cij=∑ℓ=1kD(j,ℓ)piℓ.Common choices for the distances are the L1 and L2 norms, corresponding to(3)D1(j,ℓ)=|j−ℓ|and(4)D2(j,ℓ)=(j−ℓ)2.These distances are often used in label-ranking, since D1 is connected to Spearman’s footrule and D2 to Spearman’s rank correlation (Spearman’s ρ). In the sequel, we focus on the D1 distance, yet all presented results extend to D2 in a straightforward way. Note however that other distances or losses such as Kendall’s tau that are not label-wise decomposable cannot fit the current framework.Precise estimates for piissued from the finite data setDkmay be unreliable, especially if these estimates rely on little, unreliable or incomplete data (e.g., if data are scarce around x). Rather than relying on precise estimates in all cases, we propose to consider an imprecise probabilistic model, that is, to consider for each label λia polytope (a convex set)Piof possible probabilities. We will denote byP=×i=1kPithe Cartesian product of these polytopes. Under a distance D, each setPithen induces through Eq. (2) a corresponding setCiof costs such that(5)Ci={cij=∑ℓ=1kD(j,ℓ)piℓ|pi∈Pi}.Note that, asPiis convex and as cijis a linear function of pik,Ciis also a convex set. A common and simple way to define the setPiis to provide bounds over the individual values pij, obtaining the set(6)Pi={p̲ij≤pij≤p¯ij,∑j∈Kpij=1}.This model is commonly called probability intervals (de Campos, Huete, & Moral, 1994), yet other learning techniques may produce more complex sets, such as binary decomposition approaches (Destercke & Quost, 2012) or the popular naive credal classifier (Zaffalon, 2002b).Example 2The following bounds give an example of modelsPion the set Λ = {λ1, λ2, λ3}P1123p¯1j0.30.50.2p̲1j0.30.50.2P2123p¯2j0.600.7p̲2j0.300.4P3123p¯3j0.70.60.4p̲3j0.30.20.These models indicate that we have precise information about λ1 and believe its rank is 2, while knowledge about the two other labels are imprecise. We nevertheless know that λ2 should not have rank 2, while there is much more imprecision about λ3.This approach requires to learn k different models, one for each label. This is to be compared to the RPC (Hüllermeier et al., 2008) approach, in which k(k − 1)/2 models (one for each pair of labels) have to be learnt. There is therefore a clear computational advantage for the current approach when k increases. It should also be noted that the two approaches rely on different models: while the label-wise decomposition uses learning methods issued from ordinal regression problems, the RPC approach usually uses learning methods issued from binary classification.However, as for the RPC approach (and its cautious versions (Cheng, Hüllermeier, Waegeman, & Welker, 2012; Cheng et al., 2010b; Destercke, 2013)), the label-wise decomposition requires to aggregate all decomposed models into a single (partial) prediction. Indeed, focusing only on decomposed modelsPi,nothing forbids to predict the same rank for multiple labels. In the next section, we focus on how to describe a set of potentially optimal solutions given the uncertain costsCi. To do this, we introduce a new way to handle the well-known assignment problem when costs are uncertain.When costs cijare precisely valued, i.e., whenPiis reduced to a single probability pifor any i = 1,…,k, then finding an optimal ranking comes down to finding the assignment matrix yijthat minimizes the overall cost c. One can easily see that this results in an assignment problem, which can be modelled with binary optimization variables yijequal to 1 iff label i is assigned to position j:(7)min∑i,j∈Kcijyij(AP)s.t.∑i∈Kyij=1,j∈K∑j∈Kyij=1,i∈Kyij∈{0,1},i,j∈K.Solving the above optimization problem can be done by using the well-known Hungarian algorithm (Kuhn, 1955) which has complexityO(k3). However, in our case the costs are uncertain and belong to polytopeC=×i=1kCi. A classical approach for such problems, such as the one used in robust optimization (Ben-Tal, Ghaoui, & Nemirovski, 2009), would seek minmax solutions of the problem (Deineko & Woeginger, 2006), yet this would not give us a partial prediction reflecting our lack of information about the costs. Also, recent works suggest that minmax solutions are likely to be sub-optimal in Machine learning problems (Hüllermeier, 2014).Let us denote by Y the set of binary matrices that are feasible for (AP) (the set of possible assignment matrices). Given two binary matrices y and y′ feasible for (AP), we say that y dominates y′, which is denoted y≻y′, if∑i,jcijyij<∑i,jcijyij′for allc∈C. This way of defining dominance is strongly connected to the so-called maximality criterion used in imprecise probabilities (Troffaes, 2007). Ideally, we would want to retrieve the full Pareto set of non-dominated solutionsY={y∈Y|∄y′∈Ywithy′≻y}induced by this dominance criterion. Yet Y has the same size asL(Λ),and computing or even approximatingYwith theoretical guarantee can be very difficult in practice. This is why we focus in what follows on finding outer approximations ofY. We provide a means to infer a partial order on Λ whose linear extensions form a superset ofY. The method comes down to assessing for a given pair of labelsλi1,λi2whether labelλi1is preferred to labelλi2for all elements ofY. By doing this for every pair of labels, we then obtain a partial order. Note that as all elements ofYare proper rankings, there is no risk of inferring cyclical relations.We first introduce some notations. Given i1, i2 ∈ K, let us define byYi1≻i2={y∈Y|yi1j1=yi2j2=1,j1<j2}the set of all solutions where labelλi1is preferred to (has a lower rank/position than) labelλi2. Then labelλi1is preferred over labelλi2,which is denotedλi1≻λi2,ifY⊆Yi1≻i2. This characterization is not practical, however, since we are unable to compute the full Pareto setY. Our objective below is to provide a sufficient condition to assert whether labelλi1is preferred over labelλi2. In other words, we compute a subsetI′of the setIthat contains the pairs of labels for which a preference can be established, i.e.(8)I={(i1,i2)∈K×K|Y⊆Yi1≻i2}.In this context, we introduce additional notation. For anyy∈Yi1≻i2,we defineyi1∥i2as the element inYi2≻i1such thatyiji1∥i2=yijfor i ≠ i1, i2, andyi1ji1∥i2=yi2jandyi2ji1∥i2=yi1jfor all j ∈ K. That is,yi1∥i2corresponds to the ranking y where only the positions of the labelsλi1andλi2have been swapped. The result below provides a sufficient condition forλi1≻λi2in the form of an optimization problem, whose objective function has the advantage to focus only on the ranks of labelsλi1andλi2. The proof is presented in Appendix A.Proposition 1Given i1, i2 ∈ K, a sufficient condition forλi1≻λi2is that the optimal solution cost of the optimization problem below is negative(9)z(i1,i2)=max∑j∈Kci1j(yi1j−yi2j)+ci2j(yi2j−yi1j)(WeakDomi2i1)s.t.c∈Cy∈Yi1≻i2Proposition 1 provides us with a first approach for finding relations between pairs of labels. However, the result suffers from two drawbacks. First, it amounts to solve a bilinear mixed-integer program, which isNP−hardto solve in general. Second, the sufficient condition is too restrictive. Indeed, when probabilities are precise (i.e., whenCis reduced to a singleton), we would like that our approach returns the full setIdefined in (8), determining a unique optimal solution (or a set of solutions with equal minimal costs, if the optimal solution is non-unique). Unfortunately, Proposition 1 fails to include all relations inI,even when probabilities are precise and when there is only one optimal solution. Namely, letIz={(i1,i2)∈K×K|z(i1,i2)<0}be the set returned by Proposition 1. The example below shows that Proposition 1 can lead to a setIzstrictly included inI.Example 3Consider again the space Λ = {λ1, λ2, λ3} with the following precise cost matrix c (possibly derived from precise probabilities)c=(234423342).Solving the problem (AP) gives y11 = 1, y22 = 1 and y33 = 1, hence the ranking λ1≻λ2≻λ3 and the setI={(1,2),(2,3),(1,3)}. Now, consider the ranking λ2≻λ3≻λ1, that is the matrix y with y13 = 1, y21 = 1 and y32 = 1, then the value∑j∈Kc2j(y2j−y3j)+c3j(y3j−y2j)=(c21−c31)+(c32−c22)=(4−3)+(4−2)=3is positive, indicating that the maximum of z(i1, i2) for i1 = 2 and i2 = 3 is positive as well (as it is higher than 3). Hence, according to Proposition 1, we have(2,3)∉Iz,while(2,3)∈I.The next proposition shows that in our setting, the general problem(WeakDomi2i1)of Proposition 1 can be solved efficiently. This result will be instrumental to solve a further optimization problem resulting from a more stringent sufficient condition.Proposition 2Assume thatC=×i∈KCiand let i1, i2 ∈ K be given. Then, the value of z(i1, i2) is equal to the maximum of(10)maxc∈Cci1j1+ci2j2−ci1j2−ci2j1taken over all pairs j1 < j2such that (j1, j2) ∈ K × KThe proof can be found in Appendix B. Interestingly, this result showing that(WeakDomi2i1)can be solved in polynomial time relies on the fact thatCis the Cartesian product ofCifor i ∈ K, and therefore can be used thanks to the label-wise decomposition we consider. More specifically, we show that z(i1, i2) can be computed by solving O(K2) linear programs.Example 4Let us pursue Example 3 considering i1 = 2 and i2 = 3, then we have:•for j1 = 1, j2 = 2, c21 + c32 − c22 − c31 = 3for j1 = 1, j2 = 3, c21 + c33 − c23 − c31 = 0for j1 = 2, j2 = 3, c22 + c33 − c23 − c32 = −3and the maximal value is 3, the one found in Example 3.We must now deal with the second issue, the one of conservatism. We can address this issue of Proposition 1 by restricting the number of elements considered in constraint (9) to a relevant subset ofYi1≻i2. In this aim, we put a filter on any y that is considered in (9), by imposing that either y oryi1∥i2be non-dominated by somey*∈Y. For instance, y* can be the minmax solution, computable by dualizing the linear program (Ben-Tal et al., 2009), or more simply the solution of (AP) corresponding to sampled values ofPi. We obtain the sufficient condition stated next, less conservative than Proposition 1, whose proof can be found in Appendix C.Proposition 3Lety*∈Ybe given and consider i1, i2 ∈ K. A sufficient condition forλi1≻λi2is that the optimal solution cost of the optimization problem below is negative(11)w(i1,i2)=max∑j∈Kci1j(yi1j−yi2j)+ci2j(yi2j−yi1j)(Domi2i1)s.t.c∈Cy∈Yi1≻i2y⊀y*ORyi1∥i2⊀y*One readily sees that w(i1, i2) ≤ z(i1, i2) for all i1, i2 ∈ K, as constraints of(Domi2i1)defines a set of feasible solutions strictly included in the one described by the constraints of(WeakDomi2i1). Therefore, the sufficient condition from Proposition 3 is less conservative than the condition from Proposition 1. In particular, the result below shows that Proposition 3 never leads to the situation witnessed in Example 3. Its proof is provided in Appendix D.Proposition 4LetCbe a singleton and letIw={(i1,i2)∈K×K|w(i1,i2)<0}. It holds thatIw=I.Proposition 3 indicates how to get an outer-approximation that is likely to not be too conservative, but does not solve the complexity issue. The next result shows that we can use Proposition 2 in a slightly modified way to integrate the constraint given by Eq. (11), therefore keeping the complexity of computing w(i1, i2) polynomial. Its proof can be found in Appendix E.Proposition 5Assume thatC=×i∈KCiand let i1, i2 ∈ K be given. Then, the value of w(i1, i2) is equal to the maximum of(12)maxc∈Cci1j1+ci2j2−ci1j2−ci2j1taken over all pairs j1 < j2such that v(j1, j2) ≤ 0 or v(j2, j1) ≤ 0, where v(j′, j′′) is the optimal solution cost ofmin∑i,j∈Kcij*yijs.t.y∈Yyi1j′=yi2j′′=1,andcij*=minci∈Cicij−ciji*for each i, j ∈ K, whereji*is such thatyiji**=1.Compared to Proposition 2, the only added step is to check which pairs (j1, j2) are such that v(j1, j2) ≤ 0 or v(j2, j1) ≤ 0, which can be done before solving (12). Let us denote by S(y*, i1, i2) the set that contains all pairs (j1, j2) such that j1 < j2 and v(j1, j2) ≤ 0 or v(j2, j1) ≤ 0 for the element y*. Algorithm 1summarizes the procedure to obtain a predicted partial ranking, given probability setsPi.Example 5Let us pursue Example 4 considering i1 = 2, i2 = 3, j1 = 1 and j2 = 2. Let us now compute the values v(1, 2) and v(2, 1). The only possible y* isy11*=1,y22*=1,y33*=1. We then have the matrixc*=(012201120).We then have, for example,v(1,2)=c13*+c21*+c32*=6which is indeed positive. Note that as matrix c* is positive, the only case for which we have v(j1, j2) = 0 is when j1 = 2 and j2 = 3, which does correspond to the unique optimal solution.Interestingly enough, we can notice that picking multiple elementsy*1,…,y*mand adding the constraintsy⊀y*jORyi1∥i2⊀y*jfor j ∈ {1,…,m} to(Domi2i1)simply comes down to considering the pairs (j1, j2) within the intersection∩j=1mS(y*j,i1,i2). We will denote byS*(i1,i2)=∩j=1mS(y*j,i1,i2)this intersection. ComputingS*(i1,i2)can then even be done iteratively, as the pairs eliminated forS(y*j,i1,i2)do not have to be checked forS(y*j+1,i1,i2),…,S(y*m,i1,i2). The slightly more complex procedure to obtainIwwith multiple filtering solutionsy*jis summarized in Algorithm 2.Note that when the costs(13)cij=∑ℓ∈K|ℓ−j|piℓ,are derived using D1 distance, problem (12) can be rewritten as a linear program on variables p rather than c:(14)maxp∈P∑ℓ∈K|ℓ−j1|pi1ℓ+∑ℓ∈K|ℓ−j2|pi2ℓ−∑ℓ∈K|ℓ−j2|pi1ℓ−∑ℓ∈K|ℓ−j1|pi2ℓ=maxp∈P∑ℓ∈K(|ℓ−j1|−|ℓ−j2|)pi1ℓ+(|ℓ−j2|−|ℓ−j1|)pi2ℓ.Alternatively, Eq. (14) can be rewrittenmaxpi1∈Pi1∑ℓ∈K(|ℓ−j1|−|ℓ−j2|)pi1ℓ+maxpi2∈Pi2(|ℓ−j2|−|ℓ−j1|)pi2ℓ=−minpi1∈Pi1∑ℓ∈K(|ℓ−j2|−|ℓ−j1|)pi1ℓ−minpi2∈Pi2(|ℓ−j1|−|ℓ−j2|)pi2ℓ=−E̲i1(D1(j2,·)−D1(j1,·))−E̲i2(D1(j1,·)−D1(j2,·))whereE̲i1(resp.E̲i2) is a lower expectation underPi1(resp.Pi2). Said in other words,λi1≻λi2if(15)E̲i1(D1(j2,·)−D1(j1,·))+E̲i2(D1(j1,·)−D1(j2,·))is positive for every(j1,j2)∈S*(i1,i2). Expression (15) has a nice interpretation, as it says thatλi1≻λi2if the expected cost of swapping i1 from rank j2 to j1 (D1(j2, ·) − D1(j1, ·)) and swapping i2 from rank j1 to j2 is positive. This, again, emphasises the strong links this approach has with the notion of maximality (Troffaes, 2007).Furthermore, when probability setsPicorrespond to probability intervals (see Eq. (6)) and when the costs are derived using the D1 distance (Eq. (3)), we can solve problem (14) by using two sorting algorithms. We denote ak= |k − j1| − |k − j2| and bk= |k − j2| − |k − j1| for each k ∈ K, reorder variablespi1k(resp.pi2k) according to the decreasing values of ak(resp. bk), and define a0 = 1 and b0 = 1. Then, the solution of problem (14) is obtained by fixingpi11toa*=min(p¯i11,a0)(resp.pi21tob*=min(p¯i21,b0)), subtracting a* from a0 (resp. b* from b0), and repeating the operation with the second element of the list until a0 and b0 are equal to 0. This corresponds to applying the so-called Choquet integral to obtain lower expectations.

@&#CONCLUSIONS@&#
