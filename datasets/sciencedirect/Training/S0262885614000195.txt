@&#MAIN-TITLE@&#
Adaptive on-line similarity measure for direct visual tracking

@&#HIGHLIGHTS@&#
We present an adaptive metric for measuring the similarity of a target for the purpose of visual tracking.This metric assigns a robust weight to each matching error based on the error type.A histogram-based classifier is learned on-line to determine the error type.The proposed robust metric dynamically adapts to the actual appearance changes by tuning its parameters.

@&#KEYPHRASES@&#
Adaptive metric,Similarity measure,Visual tracking,Template matching,

@&#ABSTRACT@&#
This paper presents an on-line adaptive metric to estimate the similarity between the target representation model and new image received at every time instant. The similarity measure, also known as observation likelihood, plays a crucial role in the accuracy and robustness of visual tracking. In this work, an L2-norm is adaptively weighted at every matching step to calculate the similarity between the target model and image descriptors. A histogram-based classifier is learned on-line to categorize the matching errors into three classes namely i) image noise, ii) significant appearance changes, and iii) outliers. A robust weight is assigned to each matching error based on the class label. Therefore, the proposed similarity measure is able to reject outliers and adapt to the target model by discriminating the appearance changes from the undesired outliers. The experimental results show the superiority of the proposed method with respect to accuracy and robustness in the presence of severe and long-term occlusion and image noise in comparison with commonly used robust regressors.

@&#INTRODUCTION@&#
Visual tracking is a fundamental and essential part of many computer vision, robotic, and video analytic applications including Automatic visual surveillance [10], Behavior analysis [23], Motion capture and animation [20], Vehicle navigation and tracking [1], Traffic monitoring [3], Intelligent preventive safety systems [9], and Industrial robotics. In its simplest form, visual tracking is defined as the problem of locating three-dimensional (3D) target objects (such as a human or car) in a two-dimensional (2D) image plane as they move around a scene [24]. Besides other main parts such as target representation model and localization algorithm, the efficiency and reliability of a tracker are also highly affected by the used similarity measure method. The main goal of a similarity measure is to estimate the distance from the target representation model and the received data or image. Usually a predefined metric such as Euclidean distance is employed to measure the distance. However, these static metrics cannot accurately and robustly estimate the similarity level over time under challenging situations such as long-term occlusion and significant appearance changes.A primary similarity measure used for the template matching problem is the Euclidean distance between the object template and the candidate sub-image. Assume that T is the object template, I is the received image frame, and W(X;P) is the warping function which maps every pixel X={x,y} in the image plane to a pixel X′=W(X;P) in the template based on the transformation parameters P={p1,…pk}. At every tracking time instant t, the goal of a template-based tracker is to find the best transformation parameter Ptin a way that the distance between the template Ttand the candidate sub-image Itis minimized. [16] used the sum of square difference (SSD) to measure this distance:(1)Pt=argminP∑XTtX−ItWXP2.As illustrated in Eq. (1), the SSD measure can be used in conjunction with a gradient based optimization to estimate the transformation parameter. A least square algorithm is proposed in Ref. [16] to optimize Eq. (1). In general, L2-norm of errors is not robust against outliers, severe appearance variations, illumination changes, and occlusion. As a remedy, a robust error function, ρ(e) is used to estimate the error e between the template and the candidate sub-image. Using a robust estimator instead of L2-norm, we obtain:(2)Pt=argminP∑XρTtX−ItWXP.Any function which satisfies the following criteria can be considered as a robust estimator [18]:1.∀e∈ℜ→ρe>0e1>e2>0→ρe1>ρe2e1<e2<0→ρe1<ρe2ρ(e) is piece-wise differentiable.A wide variety of robust error functions have been used in the literature. The Geman-McClure function is commonly used for the task of visual tracking [2,21].(3)ρe=e2e2+σ2Another robust estimator used for tracking [8] is the Huber function.(4)ρe={12e2if|e|≤σσ|e|−12σ2otherwisewhere in Eqs. (3) and (4), σ is a scale parameter.It has been shown that these functions can improve the robustness of a visual tracker against outliers and occlusion [2]. In general, a robust estimator assigns a weight to each error value based on the magnitude of the error. The weight is less when the error is large. Despite the theoretical benefits, there are two practical problems which may significantly damage the efficiency and robustness of these functions. First the robust estimator is application dependent and has to be picked by a designer for different cases. This can be an acceptable limitation for some application, but it is not feasible under general conditions. Also, depending on the distribution of the error a proper scale vector (σ) has to be selected. Moreover, robust regression methods cannot distinguish between outliers and actual significant target appearance changes.Besides the sum of square differences and robust estimators, other metrics such as cross cumulative residual entropy (CCRE) [22], mutual information (MI) [6], the Bhattacharyya coefficient[5], a convolution of spatial and feature space kernel functions [7], and sum of conditional variance (SCV) [19] have been proposed to measure the similarity of the target model and the received images. However, these methods are developed based on static and predefined measures which cannot sufficiently deal with challenging situations in a visual tracking scenario. One challenge is that the most similar candidate sub-image to the target model may not be the best match using a predefined similarity measure. The mentioned problem mainly rises when the target appearance changes over time or it is partially occluded by either itself or other background objects. Another phenomenon which can cause a tracker to fail is the existence of similar background objects known as distracters in a close proximity to the target object. Several works have been introduced to improve the accuracy of trackers in such situations. For instance, Li et al. [15] presented a pyramid-base scale adaptation method for mean-shift tracking. This tracker generates similarity functions at different scales and uses a coarse-to-fine search to avoid trapping in local minimum. Also, Karavasilis et al. [14] used the Gaussian Mixture Model (GMM) as the target representation model and the Differential Earth Mover's Distance (DEMD) as similarity measure for the task of tracking. This method combines DEMD-based tracker and Kalman filer algorithm to handle occlusion. Nevertheless, still the applicability of these predefined similarity measures is limited to specific cases.Adaptive similarity measures, on the other hand, can be used to find the best match of the target model over time robustly. Collins et al. [4] proposed a dynamic feature selection method for estimating the similarity of the target model and the candidate image. In this method, the total number of features is fixed and the goal is to adaptively rank these features and use a subset of high ranked ones for matching. Although the method proposed in Ref. [4] can select discriminative features properly in some cases, the color features used in this method are not suitable in various applications, and also it is not always feasible to employ a more discriminative feature vector instead of color features due to the used exhaustive search for ranking the features. Recently Jiang et al. [13] proposed a classifier which is learned on-line from the tracking information to find the best match of the target model over time. In this method, an adaptive Mahalanobis distance is used to weight each feature in the classification process. According to the experimental results, this adaptive metric performed well in the existence of distracters. However, this method may fail in case of occlusion because of several reasons. First, this method uses proximity based approach to generate positive and negative samples at every time instant. However, in case of occlusion (specifically long term occlusion which has been emphasized in our work) image regions in very close vicinity of the target may not be true positive samples. Therefore, learning from false positive samples may cause the tracker to drift from the target. In addition, there is no specific mechanism in this method for handling occlusion and outliers. Although the method proposed in Ref. [13] is adaptive against target appearance and illumination changes, there is not enough evidence from the experimental results to verify its robustness and accuracy in case of occlusion.Our proposed adaptive similarity measure differs from the works in the literature in several ways. First, unlike metrics presented in [4] where a subset of the feature vector is adaptively selected for matching, in our method the distance between the target and the image is modeled on-line by an adaptive hybrid model. Also, our method is more robust against severe and long-term occlusion than other relevant methods such as in [13]. Thus, our proposed adaptive metric is designed to reject outliers whereas it deals with appearance changes. Finally, our method requires less predefined parameters in comparison with other methods such as robust regression estimation [18] where a scale vector plays a crucial role in the robustness of the regressors.In Section 2, first the proposed similarity measure is defined, and then an on-line algorithm to train a histogram-based classifier is described in detail. Next in Section 3, the proposed adaptive metric is used in a template matching problem. The results obtained by our metric is compared with several robust regressors as well as manually labeled ground truth data in Section 4. Lastly, in Section 5 some conclusions and potential future works are discussed.From the definition, the goal of a similarity measure is to estimate the distance from a target model and an image. In the proposed adaptive similarity measure, the Euclidean distance of the target model and the image is considered as the matching error. However, unlike a typical SSD method, a histogram-based classifier is learned on-line using the matching error history. Later, this classifier is used to assign a robust weight to each matching error based on its type.Let A={a1,…,am} and B={b1,…,bn} be the features describing the target model and the image, respectively.11In this work, the image pixel values are considered as features. However, the proposed method can be suitably integrated with a feature-based tracker.Assuming that the feature space is metric, the number of features of the target and the image are the same (i.e.,m=n), and features have injective relation (i.e., aj=bk⇒j=k), we can find the Euclidean error E={e1,…,en} in the feature space as:(5)ej=aj−bj.Inspiring from the work proposed in Ref. [12], we categorize the matching error E based on their history into three classes:Eiimage noise and/or illumination variations,target appearance changes, andoutliers and occlusion.The first source of error, Ei, is mainly caused by either small illumination variations or some image noise which is inevitable in image capturing and computer vision. Usually the distribution of this type of error can be modeled by a zero-mean Gaussian function as Ei∼N(0,σi). In this work, instead of a Gaussian function a symmetrical range is learned from the previous matching errors. Other source of errors (i.e., Eaand Eo), on the other hand, cannot be easily discriminated from each other. The actual appearance changes may cause significant matching errors which are usually considered as outliers or occlusion by the conventional robust estimators [18]. A proper similarity measure has to reject outliers while it is adapting to the errors because of actual changes in target appearance and pose. Since in a tracking scenario, the target appearance usually changes smoothly22In visual tracking, the input images are captured with a high frame per second rate e.g., 15 and also the target is usually a real-world object such as a human face; therefore, it is very unlikely that the target appearance significantly changes between two consecutive images.over time, we model the distribution of Eaby two adaptive ranges which are learned on-line from previous errors, and the outliers are identified if a matching error does not occur because of either Eior Easource. We consider outliers as abnormal matching errors which cannot be easily modeled or predicted. In the following, the algorithm to model error types Eiand Eais presented in detail.Assume that each matching error ejis quantized into Q bins where b(ej)∈[1,Q] is the bin index in the quantized space. Using k previous matching errors of feature j, we can estimate the number of times that ejoccurred in the bin index q as hj,q=∑l=k−t+1tδ[b(ejl)−q] where δ is the Kronecker delta function. In this work, image features33In this work, gray-scale values are used as features. However, the proposed similarity measure can be integrated with different image features.are first normalized into the range of zero and one, i.e., ∀j;aj,bj∈[0,1], and accordingly, the matching errors are in the range of negative one and one i.e., ∀j;ej∈[−1,1]. As a result,q¯=Q/2is the bin index corresponding to the smallest errors i.e.,bej=q¯→ej<1/Q.We propose an iterative algorithm to estimate the ranges of error types Eiand Ea. In this algorithm, the center and radius of each range are estimated in the quantized feature space. For error type Ei, the center pointμEiis fixed and set toq¯, and the radiusϵEiis iteratively estimated based on the following algorithm. Note that the subscript j is eliminated from the equations in Algorithms 1 and 2 for clarity.Algorithm 1Error type Eirange estimationAs shown in Algorithm 1, small matching error (Ei) distribution is modeled by an adaptive error range where its center point is intuitively set to the histogram bin corresponding to the smallest matching errorq¯. The radiusϵEiis found by iteratively expanding the range starting from the center point. The expansion is terminated where the number of newly considered matching errors due to the recent range expansion is not significant in comparison with the previous expansions. In this work, a non-linear ratio of the number of matching errors and the range size (Algorithm 1 line 6) is experimentally selected to terminate the range expansion. Thus, using a cubic form of the number of errors, we can find a proper wide range which suitably models the aforementioned error type.Unlike Ei, error type Eais modeled by two adaptive ranges which are estimated using Algorithm 2. Also, the center of these ranges is set to the histogram bin corresponding to the first and second highest number of errors respectively. Similar to Ei, the range boundaries are found based on a non-linear ratio of the number of matching errors in the history and the range size.This algorithm is repeated two times to obtain both ranges Ea1 and Ea2. These ranges adaptively model a quantized multi-modal distribution of matching errors which is changing over time. Also shown in Algorithm 2 line 16, the number of errors that occurred in selected bins is set to zero after estimating the center and radius of each range, therefore, the error type ranges are not overlapped. In the next section, the error ranges (i.e., Eiand Ea) are used to calculate the weight of each matching error w(ej).Algorithm 2Error type Earange estimationAt every matching step, the matching error of each feature ejis compared with the error type ranges and accordingly a weight w(ej) is obtained.(6)wet={2ifej<ηQϵEi1ifej−μEa1<ηQϵEa11ifej−μEa2<ηQϵEa20otherwisewhere η=2/m×∑jδ(w(ej)) is two times of the previous outliers' percentage and m is the number of features.As illustrated in Eq. (6), the adaptive weight w(et) is robust against outliers and occlusion. Moreover, the errors caused by small illumination variations receive a higher weight in comparison with those of the appearance changes to improve the accuracy of the method.Obtaining the weights of matching errors, we can calculate the similarity distance S by taking the weighted squared error per-pixel and sum over them as:(7)SAB=∑jwej×ej2.In the following sections, the proposed similarity measure is formulated along with a typical template-based tracking.In this section, the proposed similarity measure is applied on a typical template-based tracker. In this method, the object is represented by a dynamic template which is updating every k frame using the new received images. As opposed to the conventional template trackers, in this method a condensation-like sampling algorithm [11] is used to locate and track the target at every image frame. In the following subsections, the tracking algorithm followed by the representation model is described in detail.In the conventional template-based tracking method, the target object is simply represented by its sub-image region obtained from the first image I1, i.e., T1(X)=I1(W(X;P));X∈R1 where W(X;P) is the warping function and R1 is the object region at time step t=1. The function W(X;P) maps the image pixel at location X={x,y} from the candidate sub-image into the reference model using an affine transformation consisting of six variables P={tx,ty,θ,s,α,ϕ} which are x and y translations, rotation angle, scale, aspect ratio, and skew direction, respectively. There are different methods to update the template over time. One option is to not change the template [16] i.e., Tt=T1 which performs poor in case of appearance and illumination changes, the second method is to update the template every frame i.e., Tt=Tt−1 called naive update [17]. This approach is also not stable because of drift problem.44It is the problem of updating the target model using unrelated information such as background pixels [17].In this work, the template is updated every k frames based on a forgetting factor λ. Therefore, the model can simply represent the target appearance changes while it is robust against the drift problem.(8)TtX=λt−kλt−k+kTt−k−1X+kλt−k+kI¯Xwhere λ and k are empirically set to 0.97 and 5 respectively for all experiments, andI¯is the average value of k most recent object image. Also, for the first k image frames, the first image is used as the template.(9)I¯X=1k∑j=t−ktIjWXPjVisual tracking can be viewed as a sequential inference task in a Markov model with hidden state variables Ptdescribing the object motion parameters at time step t. Given an image sequence I={I1,…,It} and reference models (in this case object templates) T={T1,…,Tt}, the hidden state variables can be estimated based on Bayes' theorem as follows:(10)pPt|It;Tt∝pIt|Pt;Tt∫pPt|Pt−1pPt−1|It−1;Tt−1dPt−1where p(Pt|It;Tt), p(It|Pt;Tt), p(Pt|Pt−1), and p(Pt−1|It−1;Tt−1) are the posterior probability, observation likelihood, dynamical or motion model between two states, and prior probability respectively.The proposed adaptive similarity measure is used to define the observation likelihood.(11)pIt|Pt;Tt=exp−STtI˜tσcwhere in this work, the condensation algorithm variance σcis set to 0.2, andI˜tX=ItWXPtis the transformed candidate image.Modeling the observation likelihood using the proposed similarity measure in the previous subsection, we aim to approximate the posterior distribution p(Pt|It;Tt) defined in Eq. (10) using a condensation-like sampling algorithm [11].Assume that the prior distribution p(Pt−1|It−1;Tt−1) is approximated by N samples (or particles) with corresponding weights ({Pnt−1,πnt−1}n=1N). The first step is to randomly choose N samples (with replacement) from the set {Pnt−1} based on the probability {πnt−1}. As a result, those samples with high weight may be selected several times. In the next step, known as diffusion, each sample undergoes a Brownian motion using a Gaussian distribution usually with a diagonal covariance matrix. The weights {πnt} of the new sample set {Pnt} are obtained as:(12)πnt=pIt|Pnt;Tt.As a result, the best transformation parameter Ptis thus the particle corresponding to the maximum sampling weight.(13)Pt=argmaxPntpPnt|It;Tt=argmaxPntπntIn the next section, the accuracy and robustness of the proposed method are evaluated using several challenging videos.

@&#CONCLUSIONS@&#
This paper presented a robust similarity measure which can adaptively learn the matching error type using on-line classification. The proposed method is capable of categorizing the error into three classes: 1) small variations of the target illumination and appearance, 2) significant changes in target appearance, and 3) abnormal errors because of outliers and occlusion. According to the error types an image mask is generated to assign a weight to each matching error. The normalized weighted errors are then used to find the best match to the target model. As an advantage to other comparable methods, our similarity measure is able to adapt its regression parameters over time.The accuracy and robustness of our method have been compared with several commonly used robust regression methods. The proposed method is able to find the best match to the target template in different challenging situations including partial illumination variations, significant appearance changes, and long-term occlusion. It is observed that our proposed method excels all the regressors including R5 (regression method with scaling factor 0.5) which has performed better than the others.A new direction of this work is to use the outliers masking by the proposed similarity measure to update the target representation model. Note that a matched sub-image with proportionally high percentage of occlusion may not be a proper information for updating the target model. In addition, using our method, we can approximate the start and end of an occlusion which is useful for generating a temporary representation model.