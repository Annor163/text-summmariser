@&#MAIN-TITLE@&#
Conception of a dominance-based multi-objective local search in the context of classification rule mining in large and imbalanced data sets

@&#HIGHLIGHTS@&#
Formulation of the classification rule mining problem as a multi-objective problem.Proposal of MOCA-I that deals both with uncertainty, class imbalance and volumetry.Comparison of different MOCA-I based DMLS versions and DMLS 1·* shows better results.Comparison with 13 state-of-the-art classification algorithms.MOCA-I gives shorter and statistically more effective rules than other algorithms.

@&#KEYPHRASES@&#
Partial classification,Imbalanced data,Multi-objective,Local search,

@&#ABSTRACT@&#
Classification on medical data raises several problems such as class imbalance, double meaning of missing data, volumetry or need of highly interpretable results. In this paper a new algorithm is proposed: MOCA-I (Multi-Objective Classification Algorithm for Imbalanced data), a multi-objective local search algorithm that is conceived to deal with these issues all together. It is based on a new modelization as a Pittsburgh multi-objective partial classification rule mining problem, which is described in the first part of this paper. An existing dominance-based multi-objective local search (DMLS) is modified to deal with this modelization. After experimentally tuning the parameters of MOCA-I and determining which version of DMLS algorithm is the most effective, the obtained MOCA-I version is compared to several state-of-the-art classification algorithms. This comparison is realized on 10 small and middle-sized data sets of literature and 2 real data sets; MOCA-I obtains the best results on the 10 data sets and is statistically better than other approaches on the real data sets.Classification on real data sets raises several challenges, especially when dealing with medical data sets. One common issue is class imbalance, where the class to predict is underrepresented among the observations of the data set. Not so uncommon repartitions are 100:1 or even 10,000:1. As an example in hospital data, stroke – a frequent disease – will concern at best 1% of the hospital stays. Most classification algorithms build their classifiers using Accuracy, which measures the percentage of well-classified observations. However, this is ineffective with imbalance data: in the stroke example, a dummy classifier labeling each stay as “no stroke” will have a 99% classification Accuracy, while being totally useless to predict stroke. Some approaches have been proposed to overcome this problem, as detailed in the review of He et al. [1] and will be more developed further in this paper.Another challenge comes from the absence of real negation in medical files, which brings uncertainty. The absence of some information in the patient medical file has a double meaning. In most cases when information about a disease is missing, it means the patient does not suffer from the disease. In other cases, the patient may have the disease but is not diagnosed yet, or this information has not been entered in the system. Moreover, in some medical coding, such as Anatomical Therapeutic Chemical (ATC) Classification System,11Available on the World Health Organisation's website: http://www.whocc.no/atc_ddd_index/a same information can have several encodings, depending on the context: a same procedure or diagnose may be coded differently depending on the healthcare professional. In the presence of these “yes”/“no∨unknown” binary values, it is not reliable to predict patients having class=“no∨unknown”. This kind of problem is particularly indicated to partial classification, which focuses only on predicting a subset of the population, for example only the patients having class=“yes”. The amount of hospital data available raises another challenge. More than 50,000 medical procedures and diseases can be entered in patient data through ICD-10 encoding, which is available in all French hospitals. Since a classifier is a combination of tests on patient information, classification can be seen as a combinatorial problem. Operational research and meta-heuristics are indicated to solve this kind of problems. In their review, Corne et al. explain how these techniques can be applied to data mining [2]. In this context, many multi-objective optimization algorithms have been proposed for rule mining, most of them are detailed in the review of Srinivasan and Ramkrishnan [3].Another issue is the simplicity to understand and use the algorithm. Indeed, the predictions will be a part of a medical decision aiding tool, the OPCYCLIN project – an industrial project dedicated to decision aid for clinical trials, involving Alicante Company, hospitals and academics as partners. Thus, the generated classifiers must provide a good interpretability, allowing the users assessing the validity of the predictions. Besides, the main features of the data sets depend on the hospital under study [4] or on the patient information to predict: depending on the disease, class imbalance represents from 1% to 20% of the stays or patients under study. This requires setting the parameters depending on the data set under study. However, users often do not have sufficient knowledge in data mining to parameter the algorithms [5]. Robust approaches able to give good results on most data sets will be preferred.Many recent contributions have been proposed to deal with some of these issues, for example [6–9]. As far as we know, an approach overcoming all these issues at the same time had not been proposed yet. In this paper a new algorithm is introduced: MOCA-I (Multi-Objective Classification Algorithm for Imbalanced data), which is an optimization algorithm able to generate partial classification rules in large and imbalanced data sets. This paper aims to find the better parameters to use with MOCA-I and then will compare it to algorithms of literature. Section 2 describes more deeply the partial classification rule mining problem. Then Section 3 presents MOCA-I – our implementation as a multi-objective problem – while first explaining notions about multi-objective algorithms. Section 4 contains a deep study of MOCA-I algorithm, assessing the best parameters to use, such as size of rules, archive size, etc. to ensure the best results over most data sets. Section 5 compares the results to those obtained by state-of-the-art algorithms, both on benchmark data sets from literature and real data sets. Finally, Section 6 gives the conclusion and perspectives.This section first describes the partial classification problem and how to evaluate the performance of a classifier. Then it describes the problems risen by our real data: class imbalance and high volume of data.The classification task aims to predict a fact – called a class, for example “flu?yes/no” – on unknown observations. Observations depend on the domain of application and can be of various forms like bills, patients, events, etc. For each observation several information are available, which are called attributes. In the flu example, each observation is a patient; attributes are a list of symptoms that were observed; and each patient may or may not have presented each symptom. The classification task will generate a classifier that describes how to determine the class – here “flu” – by using the attributes – here the symptoms. A classifier is a combination of attributes tests (AT). Each represents a test on an attribute, for example “age>25”. Partial classification is a subclass of classification, which interests only in predicting observations matching a subset of the class: observations not matching the subset class are not predicted. An example of partial classification task could be to predict flu=yes on unknown patients, while having no interest to find healthy patients. Several kinds of classifiers can be extracted to predict the class. The most common are trees and rules, which consist of conjunctions or disjunctions of attributes. As an example: cough=yes and fever=yes and musclepain=yes⇒flu. Less interpretable classifiers exist such as support vector machines or neural networks but they are not the object of this paper.More than 40 metrics have been proposed in the literature to assess the efficiency of these classifiers [10,11]. Most of them are based on the confusion matrix given in Table 1. Given a rule classifier of the form C⇒P, it counts the number of observations well classified – true positives (TP) and true negatives (TN) – as well as the wrongly classified ones – false negatives (FN) and false positives (FP). As a rule of thumb, classifiers are often evaluated on both known and unknown data, to assess the capacity of the classifier to deal with unknown data. In order to do so, the data are split as training and test data sets. The training data set is used to build the classifier, while the test data set is used to evaluate it.Previously it has been mentioned that medical data can bring several problems. One of them is uncertainty: the absence of information in the patient file can have several meanings. Thus,P¯observations cannot be completely relied upon: a small part of them (up to 15%) may be in fact P observations. The use of metrics adapted to partial classification – such as F-measure[1] – allows dealing with such data by not focusing too much onP¯.Another major problem is imbalance data. When dealing with imbalance data,|P|<<|P¯|: positive observations are less available than the other observations. Metrics based on counting the number of well classified observations (both P andP¯), such as Accuracy or the number of wrongly classified observations will tend to encourage the classification ofP¯observations. This is especially the case with high imbalance (class to predict is less than 1% of observations), where the cost on a “misclassified” observation will be minimal. Several solutions have been proposed to solve this problem. Most of them are detailed in the review of He et al. [1]. Cost sensitive methods set weights on P andP¯to force metrics to deal with the class to predict, while Boosting methods set weights directly on observations and work on several iterations. Each iteration, weights on the misclassified observations are increased, to force the classifier to deal better with them. These weight-based approaches are often used to enhance the results of some basic classifiers such as the famous C4.5 algorithm [12]. Hence, Ting et al. proposed a cost-sensitive version of C4.5 named C4.5-CS[13], while AdaC2 and DataBoost-IM use it within a boosting algorithm [14,15]. However, the weights used in these approaches can be hard to set. The object of this paper is to build a classifier system able to deal natively with class imbalance, to avoid dealing with more parameters due to weighting. Thus a metric well-adapted to deal with imbalance will be chosen instead of dealing with weights. However boosting or cost-sensitive methods can probably improve the results and may be the object of further works. Other methods focus on resampling the data to obtained well-balanced data, adding new observations (over-sampling), for example by generated new observations like the SMOTE method [16] or by removing observations (under-sampling) like the ACOSampling method [17]. Since our medical data contain partial information or errors, methods altering the data are avoided. Moreover, Jo et al. [18] showed it is not the class imbalance that directly causes problems, but the underlying small disjunct problem. Small disjuncts are rules or combinations of attributes covering a small number of observations. Thus, according to Jo et al. work, it is more effective to deal with the small disjunct problem and be able to find rules covering a small number of observations, rather than dealing with the class imbalance problem, that will not assure that the obtained rules match accurately the small number of observations under study.The last problem lies within the high number of attributes. Since the classification problem can be seen as a combinatorial problem, a high number of attributes – here 10,000 – quickly becomes an issue. Two main approaches have been proposed to deal with this combinatorial problem: greedy and optimization. Greedy approaches – such as C4.5 algorithm [12] or Ripper [19]– build a classifier step by step from an initial classifier, by choosing at each step the modification that will give the best performances. These approaches are quick but are subject to be stuck in a local optimum since they depend on the starting solution. Optimization approaches require more computational time but allow exploring the set of possible classifiers to find one of the most effective. A lot of modelizations and algorithms have been proposed, as referenced in the review of Srinivasan et al. [3]. Some modelizations consider the problem as a multi-objective one. It is shown that it is more effective than solving the same problem where all objectives are aggregated a single one, in the case of partial classification [20]. Most of the optimization approaches are based on genetic algorithms (GA) like OCEC [21], SIA [22], DT_GA [23] or DT-Oblique [24]. Others are based on hybridization with GA like in Learning classifier systems (LCS) that combine GA with reinforcement learning, as in the GAssist algorithm [25] or XCS [26]. This paper will be focused on multi-objective local search, which has been proven to be as effective as NSGA-II algorithm on a classical combinatorial problem while needing less parameters than a GA [27].In the next sections, a model able to deal with partial classification in the context of imbalance data is proposed. Then, it is compared to the results obtained by previously cited approaches (LCS, genetic algorithms, greedy algorithms, etc.).This section first describes how the partial classification rule mining problem can be modeled as a local search problem. It gives details about the encoding, neighborhood and objectives. Then it gives notions about multi-objective optimization and explains the DMLS meta-heuristic scheme and the different parameters that are available and will be studied further in this paper.A local search algorithm is a meta-heuristic that enhances an initial solution by visiting similar solutions, called neighbors. The most famous local search algorithm is the Hill Climbing algorithm, which naturally evolves the solution until no improving neighbor can be found. The definition of what is a solution depends on the problem under study. In our case a solution represents a candidate classifier. In order to be able to run a local search, three components must be defined: how to encode a solution, how to evaluate the quality of a solution and how to generate the neighbor solutions. Solution encoding. Several solution encodings exist. All interpretable encodings are based on combinations of attribute tests (AT). Each AT is composed of one attribute, one operator and one value. For example, fever=yes, or age>10 are AT. AT can be combined using disjunctions, conjunctions or both, allowing creating a lot of different forms: decision trees, rules, rule sets, etc. Hence, Reynolds et al. proposed a multi-objective algorithm using an encoding as a tree [28]; Bacardit et al. use an encoding where each solution is a rule set [25] called Pittsburgh encoding; Wilson et al. use a rule as encoding [26] called Michigan encoding. The length of the encoding can be fixed or variable. Variable-length encodings will be preferred because they are less sensitive to the number of attributes under study. However they are subject to bring bloat. When bloat happens, the rule sets tends to contain needlessly over-specific rules such as cough=yes and fever=yes and musclepain=yes and diabetes=yes and glycaemia=high and insulin=yes⇒flu where a simpler rule could perform better: cough=yes and fever=yes and musclepain=yes⇒flu. MOCA-I is based on a Pittsburgh encoding, adapted to partial classification. Thus, each solution is a set of rules predicting the same outcome. Since all rules predict the same outcome, there is no need to deal with inconsistencies brought by rule sets like rule overlapping with different outcomes [29]. An observation is considered having the outcome if it triggers at least one rule from the rule set. A variable-length encoding is chosen to allow dealing with a high number of attributes (in our real data sets from 10,000 to 50,000).Evaluation function. More than 40 metrics are available to assess the quality of a classifier, all of them are candidates to be part of the objective function. However increasing the number of objectives will increase the computational time and make the interpretation harder. Moreover, in a multi-objective approach it is not worth considering several objectives measuring a similar effect and hence it is important to choose conflictive objectives [30]. In a previous work, an analysis was performed to determine the best objective candidates [20]. First, a subset of 15 metrics adapted to partial classification and imbalance data were selected. Then a principal component analysis (PCA) was realized to group these metrics according to a correlation map. Finally, Confidence and Sensitivity revealed to be well-suited to partial classification, leading to the following objectives:Objective 1maximizeConfidence=TPTP+FPmaximizeSensitivity=TPTP+FNminimize Complexity (size of rule set)The last objective is introduced as an application of the Minimum Description Length (MDL) principle [31]. When two rules have the same performance, it gives priority to the simpler one. It allows avoiding bloat which is brought by variable-length encodings. Moreover it allows limiting the number of duplicated rules: a rule will not be added if it does not improve one of the objectives. These objectives will be treated independently, which has been proven to be more effective than aggregating them in a single objective [20].Neighborhood. The neighbor of one rule set RS is the set of all rule sets having one AT difference with RS: one AT more, one AT less or a difference on one AT's value or operator. Considering < and > operators, a simple neighborhood is used that generates only boundary values.Choice of a solution. Some multi-objective algorithms deal with a population of solutions; this is the case of MOCA-I, which obtains a population of trade-off solutions having different levels of Confidence, Sensitivity or Complexity. In order to compare MOCA-I to other data mining algorithms, a single solution is needed, that will be selected within this population of solutions. Thus, the rule set having the best F-measure on the training data is selected as the final solution. F-measure is a measure well adapted to the partial classification problem. It is the harmonic mean between Confidence and Sensitivity – two of the objectives of MOCA-I. A high F-measure indicates both a high Confidence and a high Sensitivity. This final solution will be then evaluated on training and test data.The problem has been modeled as multi-objective local search problem. As DMLS approaches have shown good performances on different types of multi-objective problems, we propose to adapt them to the problem under consideration. After an introduction to multi-objective optimization, the DMLS approaches used in MOCA-I are described and their different available components.Multi-objective combinatorial optimization is applied in various problems and domains, like scheduling, routing or data mining. An overview of these applications can be found in Coello et al.'s book [32]. Multi-objective combinatorial optimization deals with the identification of the best solution(s) regarding a set of optimization criteria (rule interestingness measures for example). Such a problem may be described as follows: given a solution x, several evaluation functions f1(x), f2(x), …, fn(x) characterizing x and objectives (maximizing f1, minimizing f2, …), multi-objective optimization tries to find values of x that fit well objectives. For example in the traveling salesman problem, x can be a possible route, f1(x), f2(x) the cost (energy) of the route, and the time needed, the objectives can be minimizing f1(x) and f2(x). The representation of x is dependent on the problem, in data mining x could be a classifier, or a rule, etc.Different approaches are available to deal with jointly optimizing several objectives. Scalar approaches combine all the objectives into a single function f, where f is a weighted combination of the objective functions:f(x)=w1×f1(x)+w2×f2(x)⋯+wn×fn(x). They produce a single optimal solution. Dominance-based approaches use dominance relations to handle all objectives and result with a set of compromise solutions. The widely used dominance relation is the Pareto one: solution S1 is said to dominate solution S2 using objective functions f1, ..., fn(to maximize) if:∀i∈1,…,n,fi(S1)≥fi(S2)and∃j:fj(S1)>fj(S2).The set of non-dominated solutions is called Pareto Front. It represents all solutions of best compromise. In Fig. 1, where f1 and f2 have to be maximized, point S1 dominates point S2 because f1(S1)>f1(S2) and f2(S1)>f2(S2). Point S4 is dominated by both S1 and S3 because f1(S1)>f1(S4) and f1(S3)>f1(S4). The Pareto front is composed of points S1 and S3 because no other point dominates them.As explained later, in this work a dominance-based approach will be adopted. Meta-heuristics working on a population of solutions are particularly well suited for this type of problems [32] and thus will be adopted for this classification rules problem. In the following, the algorithm used in MOCA-I is detailed.Different multi-objective local search approaches have been proposed, including Pareto Archived Evolution Strategy (PAES) [33] or Pareto Local Search[34]. Liefooghe et al. proposed a model that unifies them [27] under the denomination DMLS (Dominance-based Multi-objective Local Search). This unification leads to different algorithms variants, due to several operators opportunities (selection, neighborhood exploration,...). Some variants have been proven to be more effective as NSGA-II – a state-of-the-art genetic algorithm – for some multi-objective problems [27], while needing less parameters. DMLS approaches are based on Pareto Dominance. It allows obtaining a set of trade-off solutions that do not dominate each other, which is called Pareto Front. Consequently, DMLS approaches have to deal with a population of trade-off solutions, unlikely to single-objective local search which deals with one current solution.Algorithm 1DMLS algorithm used in MOCA-IAlgorithm 2Rule initializationThe DMLS algorithm is detailed in Algorithm 1. It deals with an archive, which contains the set of current trade-off solutions. This archive is initialized with a set of non-dominated solutions. In MOCA-I, this initial population is composed of 100 rule sets. The number of solutions available in the initial population is determined experimentally further in this paper. Ideally the initial population must be composed of random observations. However data sets having a large number of attributes may raise problems: two randomly chosen attributes seldom appears together on at least one observation; consequently a rule composed of randomly chosen attributes will be likely to match none observation. Thus a rule set generation method is used, which is based on generating rules from existing observations, which is described in Algorithm 2. This method allows generating rules matching at least one observation. Then, each rule set is composed of two of these rules. Once the initial population is generated, all non-dominated solutions are extracted and constitute the initial archive. All solutions within the archive are flagged as “unvisited”; when the archive contains no more “unvisited” solutions the algorithm will naturally stop: the current archive reached a local optima. While it remains at least one “unvisited” solution in the archive, DMLS first selects one or more solutions to explore. For each selected solution, the neighborhood will be generated and explored. All non-dominated neighbors are stored. When all its neighbors have been explored the current solution is marked as “visited”. In some DMLS variants, the neighborhood exploration can stop when a neighbor dominating the current solution is found. After the neighborhood exploration, the archive is updated with all non-dominated neighbors: first solutions from the archive that are dominated by non-dominated neighbors are removed; then neighbors not dominated by solutions of the archive are added into it. The updated archive can now be used in the next iterations.Different parts of the DMLS algorithm allow several strategies. Regarding the selection of the solutions to explore (current set selection), one strategy is to choose a single solution to explore (1-random solution strategy, which will be referred as 1) while another strategy is an exhaustive exploration (that will be referred as *): all solutions of the archive are explored. Moreover, as in the Hill Climbing algorithm, the neighborhood exploration can be stopped as soon as an improving neighbor is found (1st improving strategy, which will be referred as 1); another strategy is the exhaustive exploration of the neighbors (that will be referred as *). In the following of this paper, the impact of both current set selection and neighborhood exploration will be studied, comparing these different strategies. The following notation will be used to indicate a particular version of DMLS, using the abbreviations proposed in Table 2: DMLS <currentsetselection> · <neighborhoodexploration>. Thus, DMLS 1·* refers to DMLS with 1-random selection as current set selection strategy, and exhaustive neighborhood exploration. Table 2 refers all the DMLS implementations details under study. Also, some other factors may influence the results of DMLS: the initial population which is used to start the search, the size of the archive of current solutions, or regarding our Pittsburgh modelization, the maximum number of rules contained in a rule set. The influence of each of these parameters will be studied in the following section; they are presented in Table 3. The archive's size is the parameter which is the upper bound of the archive of current solutions: when the archive reaches this maximum number of solutions no more solution can be added. In this case, a new solution will only be added if it dominates at least one solution of the archive. Several upper bounds will be tested: 100, 300 and 500, to measure if the bounding of the archive has an impact. Another parameter is the initial population size: it represents the number of solutions used to initialize the archive of current solutions. The last parameter is the maximum number of rules contained in each solution. A higher number of rules will increase both the complexity of the solutions and the size of the search space, with probably an impact on the solutions performance.This section focuses on finding the most effective parameters of DMLS – size of archive, size of initial population or explorer type – as well as parameters dependent on the model – such as the maximum size of each rule set – or of the data – such as discretization to apply to prepare the data. All these parameters will be experimentally tested on 10 data sets of the literature. First these data sets will be presented. Then each parameter under study will be detailed, as well as the protocol used to assess its performance. Finally the results obtained for each parameter and the corresponding discussion are given. As a result reference parameters will be obtained, that will be used to compare MOCA-I to literature in further sections.In order to study the effects of the different DMLS strategies and parameters some data sets are needed to evaluate their performances. Two (private) real data sets generated from hospital data are available: tia-f and s06-f containing 10,000 patients, where the aim is to predict respectively transient ischemic attack or brain injury. These data sets suffer from a high imbalance since less than 1% of the observations have the class to predict. At the origin, these two data sets contain more than 10,000 binary attributes. In order to allow literature algorithms to deal with them in Section 5 only attributes available at least on one observation with prediction were kept, leading to respectively 699 and 1023 attributes. Statistical tests that will be used in the next of this paper need at least 10 data sets to be efficient [35]. Thus, 10 additional data sets were chosen from the literature presenting several degrees of imbalance, from 27.42% to 0.77%. Most of these data sets come from the UCI repository.22http://archive.ics.uci.edu/mlMulticlass data sets are modified into binary-class data sets, as proposed by Fernandez et al. [36], to allow obtaining imbalance data sets. Since our real data sets contain binary data, these 10 additional data sets will be discretized when necessary. Further in this paper the impact of discretization will be studied, in order to choose the most adapted discretization. The lucap0 data set is a generated data set coming from a Machine Learning Challenge [37]. The two data sets w1a and a1a are binarized UCI data sets [38], which have a higher number of attributes and are close to our real data sets. Table 4gives an overview of these data sets, detailing for each of them the number of observations, attributes (including numerical attributes), imbalance ratio (I.R.) (percentage of observations having the prediction) and the reference.This subsection details the protocol that will be used to compare different versions or parameters of the algorithms in the remaining of this paper.Measure of performance. One of the focuses of this paper is to assess the ability of each algorithm to deal with unknown data, rather than the multi-objective efficiency through the use of indicators like hypervolume[40]. The efficiency of each algorithm will be assessed using the F-measure on the test data, which is the recommended measure in partial classification [1]. The F-measure is the harmonic mean of Confidence and Sensitivity:F−measure=2×Confidence×SensitivityConfidence+Sensitivity.It has a high value when both Confidence and Sensitivity are high. Confidence reflects the ability of a rule (or a classifier) to detect correctly the prediction: if the rule is wrong most of the time, it will have a low Confidence. Sensitivity reflects the proportion of observations with prediction that are detected by the rule. Our objective is to find a configuration of MOCA-I which is robust enough to give good results on most data sets. Thus, the F-measure will be collected on each data set, for each algorithm and Friedman and Iman–Davenport statistical tests will be used to check if a configuration outperforms the others on most data sets, as recommended by Demsar [35]. These tests are similar to the parametric test ANOVA at the difference they are non-parametric; it allows using them more often since parametric test's conditions are seldom met with real data sets. Friedman and Iman–Davenport tests compute the ranks obtained by each algorithm under comparison over all the data sets. The H0 hypothesis is that all algorithms are equivalent over the given data sets, regarding their measured average ranks. It is considered that H0 is accepted when obtained values are under a critical value – which depends on the number of data sets, algorithms under study and α – the accepted probability of making a type I error. When H0 is rejected, post-hoc statistical tests can be carried on to determine which algorithm outperforms the others. Tests well-adapted to multiple testing are used, such as Holm and Bergman and Hommel's tests. Statistical tests are realized using the Java software provided by Garcia et al. [41].Tests are carried out on a computer with a Xeon 3500 quad core and 8GB ram, under Ubuntu 12, using gcc 4.6.1. All tests of MOCA-I use the DMLS implemented in ParadisEO framework which is a white-box object-oriented C++ framework dedicated to the reusable design of meta-heuristics [42].Validation on unknown data. The selected configuration of MOCA-I must be able to determine effectively the prediction on unknown observations. Thus, the F-measure must be evaluated on observations different from those used for building the rules. A common protocol to do so in data mining is to split the data set into several folds, one fold is used for validation – it will be referred as test data – while the others are used to generate the rules. In the next a 5 folds cross-validation is used: each data set is split in 5 folds, 4 are used for training while 1 is used as test data; the algorithms are run for each combination of folds: each fold will be used one time as test data. The F-measure on test data gives the ability of the tested algorithm to deal with unknown data, while the F-measure on training data only gives an indication of the ability of the algorithm to fit the training data. Since MOCA-I is a stochastic algorithm, 5 different seeds will be used for each fold, leading to a total number of runs of 25. In the results section the average F-measure obtained over the 25 runs will be provided, both on training and test data. Since each DMLS gives a set of trade-off solutions as a result, the solution having the best F-measure on training data is selected as the final solution.Stopping criterion. Each algorithm under comparison will be stopped after a given number of neighborhood evaluations. This number of evaluations is determined experimentally: each DMLS version and parameter are run using 5 × 5 folds until they reach a local optimum; the average of neighborhood evaluations needed for each combination of DMLS and parameter to reach a local optimum is computed; finally the highest average number of neighborhood evaluations is taken, which allows each combination DMLS and parameter to have a chance to reach a local optimum. This number depends on the data set under study. To remain fair, quick algorithms that reach local optimum before the stopping criterion is triggered are allowed starting from another initial population; at the end all the obtained solutions from the multiple-start are merged.

@&#INTRODUCTION@&#


@&#CONCLUSIONS@&#
