@&#MAIN-TITLE@&#
Evaluation of automatic feature detection algorithms in EEG: Application to interburst intervals

@&#HIGHLIGHTS@&#
A software to improve algorithms for feature detection in neonatal EEG is proposed.The clustering quality of feature detectors with EEG rating is quantified.The method is tested on a modular definition of interburst intervals detectors.The power supply filter can be removed with little effect on the detection quality.Detectors are robust compared with the standard deviation thresholding of the EEG.

@&#KEYPHRASES@&#
EEG,Interburst intervals,Signal analysis,Cluster Validity Index,Clustering,

@&#ABSTRACT@&#
In this paper, we present a new method to compare and improve algorithms for feature detection in neonatal EEG. The method is based on the algorithm׳s ability to compute accurate statistics to predict the results of EEG visual analysis. This method is implemented inside a Java software called EEGDiag, as part of an e-health Web portal dedicated to neonatal EEG.EEGDiag encapsulates a component-based implementation of the detection algorithms called analyzers. Each analyzer is defined by a list of modules executed sequentially. As the libraries of modules are intended to be enriched by its users, we developed a process to evaluate the performance of new modules and analyzers using a database of expertized and categorized EEGs. The evaluation is based on the Davies–Bouldin index (DBI) which measures the quality of cluster separation, so that it will ease the building of classifiers on risk categories. For the first application we tested this method on the detection of interburst intervals (IBI) using a database of 394 EEG acquired on premature newborns. We have defined a class of IBI detectors based on a threshold of the standard deviation on contiguous short time windows, inspired by previous work. Then we determine which detector and what threshold values are the best regarding DBI, as well as the robustness of this choice. This method allows us to make counter-intuitive choices, such as removing the 50Hz filter (power supply) to save time.

@&#INTRODUCTION@&#
Through electrodes placed on the scalp, the EEG sensors directly record the spontaneous electrical activities of the cerebral cortex. It gives information on the ongoing neurological status of a patient and remains a major diagnostic tool in neurology for many conditions such as epilepsy, sleep disorders, coma, etc. In some cases, long term recordings are needed, generating heavy files and requiring tedious analysis. Extracting the relevant information automatically may considerably help physicians to detect abnormal patterns such as seizures or other neurological dysfunctions. Various analysis methods can be employed [1] and the calculated data have to be compared to the visual analysis by an expert. Such computer aided diagnosis systems have been developed for seizure detection in epilepsy.The EEG is also used on newborns to detect seizures that are often clinically silent in infants and has specific applications to evaluate cerebral injury and cerebral maturation in premature infants. Neonatal EEG analysis is a highly specialized task and few experts are available to perform this time consuming activity. Therefore, an automated system for computer aided diagnosis would be significantly valuable for neonatal intensive care units.The goal of this work was to develop a new method to evaluate the performance of specifically developed algorithms for neonatal EEG, using a database of expertised and classified tracings. For the first application, we tested this framework for interburst intervals (IBIs) detection in preterm born infants׳ EEGs.In premature infants, the normal background EEG activity has specific features including spontaneously discontinuous signals with periods of electrical activity alternating with periods of quiescence – called IBIs. The diagnostic and prognostic value of neonatal EEG abnormalities in premature infants are well established [2,3]. IBI duration has been shown to be related to the maturational process of cortical folding [4]. Prolonged IBIs, as measured by the longest IBI, have been shown to be related to abnormal brain maturation in premature infants [5]. The presence of IBIs is also highly abnormal in full-term infants with birth asphyxia.However, systematic assessement of IBIs duration is not of current use as the manual measurement of IBIs is a problematic and time-consuming task. This difficulty might be alleviated by recourse to their automated detection, as several recent studies have shown [6–8]. The validation of the detectors is performed visually on several EEG sequences, comparing intervals marked by experts with the intervals automatically detected and then by calculating their specificity and sensitivity. We propose a complementary approach that takes into account the life cycle of the EEG record in the medical routine. The EEG is sent to a neurologist, who analyzes the signal (without scoring intervals one by one) and proposes a diagnostic opinion. Finally the EEG, the diagnosis and some patient data are registered in a database. Therefore, our aim is to compare statistically different algorithms to optimize their settings without recourse from a second detailed visual analysis process. For this purpose, we have developed a component-based approach implementation of several algorithms (see [22] for a review on component-based implementations). Indeed, signal analysis is rarely based on a unique and simple algorithm. It can be described by a set of sequential operations allowing the elimination of useless information, the extraction of information from different sources, their agregation and transformation in a concise form, that is easy for physicians to interpret. We have implemented this analysis pipeline, called an analyzer, as a sequence of modules, where each module corresponds to an operation on the EEG or derived data.The comparison between the different algorithms is based on two criteria: the quality and the speed of the signal. While the speed criterion is easy to define, the quality criterion is normally more difficult to express. Generally, the quality is evaluated by manually comparing the individual IBIs visually marked by the specialist to the automatically detected IBIs (see for example [9]). This is useful for rejecting bad algorithms and bad settings, but it is too time-consuming to be used on high number of EEGs. Moreover, our experiments have shown that the IBI detection is partly subjective as there may be discussions between specialists determining the beginning and end of an IBI. The overall impression based on the global appreciation of the IBIs is more reliable. And because feature detection is not an end in itself but a means to assess risks on brain development, it is not clear that the detail of each feature, taken one by one, allows for better classification of risk categories for patients.Ultimately, we propose an additional quality criterion to the specificity/sensitivity criteria. It is based on the ability of the feature׳s detection algorithms to produce discriminative information, predictive of a pathological EEG. For this, we use the Davies–Bouldin index (see Section 2.4 for the details) which measures the quality of cluster separation, so that it will ease the building of classifiers on risk categories.Our paper is structured as follows. We present our framework and software to define and run analyzers in Section 2, as well as the methodology and criteria quality that we use to evaluate different analyzers. In Section 3 we illustrate our methodology on the automated evaluation of several analyzers for IBIs detection based on the algorithm proposed by Jennekens in [6], and we provide our future work direction. We conclude this paper in Section 4.A database of 416 EEG and relative information on infants was created as part of the LIFT cohort. All surviving children born between January 2003 and December 2004 at less than 35 weeks of gestation in a systematic follow up (see [10]) were enrolled. Written consent was obtained from their parents before the children were included in the LIFT cohort, which was registered at the French CNIL (The National Commission for Information Technology and Civil Liberties, or Commission Nationale de l׳Informatique et des Libertés, No. 851117) ethics committee, allowing clinical data collection from the patients’ records.The EEG signals were recorded (Alliance, Nicolet Biomedical) with adapted electrodes (cups) with reduced neonatal montages using 8 or 11 scalp electrodes depending on head size, in accordance with the international 10–20 system for a duration of 30–45min. The EEG abnormalities were assessed according to the established classifications used routinely in France: each EEG was interpreted according to the guidelines of the French Neurophysiological Society. An EEG is considered as normal (normal background activity for the corrected age, no abnormal features), moderately abnormal or doubtful (increased discontinuity with longer IBI less than 50% more than the maximal value for age, abnormal delta brushes, dysmature patterns, positive temporal sharp waves, positive rolandic sharp waves of less than 1 per minute), severely abnormal or pathological (excessive discontinuity with maximal IBI duration above 50% of the maximal value for corrected age, seizures, positive rolandic sharp waves with more than 2 per minute). Clinical information on the status at birth and the outcome was registered. For analysis purposes, a visual inspection of each EEG was performed to reject low quality tracings with too many artifacts that may impair IBI detection. 22 EEGs were rejected (around 5%) to obtain the base of 394 EEGs used in this work. The EEGs were anonymised.Alongside proprietary software developed by EEG manufacturers where users have little control, few platforms exist for researchers to validate their algorithms. EEGLab and Brainstorm [11,12] on the MATLAB platform (from Mathworks Inc.) are examples of such platforms. With EEGDiag we developed a Java application embedded in the e-health Web portal “BBEEG portal” dedicated to neonatal EEG. EEGDiag represents an intermediary approach between proprietary softwares and research oriented platforms. EEGDiag is designed for researchers and physicians. EEGDiag is downloadable from the web browser (at http://bbeeg.chu-angers.fr/portal, users can register by clicking on “Vous pouvez créer un compte”) and functional on any workstation of NICUs subscribed to the BBEEG portal.But EEGDiag is not just an application, but also a framework based on the plug-in and observer design patterns. Thus, EEGDiag is used in three situations: as a stand-alone application during routines in NICUs, as a Web service in the BBEEG portal thanks to the underlying Java framework, and for clinical/biomedical engineering research. Finally, EEGDiag and its underlying framework allows:•implemention, testing and comparison of different algorithms on EEGs in a user-friendly way,establish the best default setting for each algorithm by testing different tunings on a large EEG database,build indices and risk categories from detected features,provide results in a short time and in an easily readable form for routine usage.Each algorithm is decomposed into elementary operations and can be modeled by a graph describing the sequence of these operations. This has enabled us to achieve a graphic modeling tool (seeFig. 3) that easily generates new algorithms. Such a tool proved particularly beneficial when teaching its use as it allows a better understanding of the underlying processes by looking at the effect of the adding or removing one of the elementary operations. In addition, this component-based approach allows a better reusability of algorithms and facilitates the integration of new modules. Each module-based algorithm is called an “analyzer”.EEGDiag works with different EEG formats. The analyzers’ results are displayed in a specific window (see Fig. 1) and can be exported in a csv file. A “tools” menu enriches analyzers with new features based on a plug-in mechanism: provided one follows a well-defined framework, the user can add new modules and tools in the form of Java (⁎. jar) libraries (see Fig. 2).EEGDiag is composed of the Java libraries built during the project. SeriesCommon.jar is the base library, since it contains all the data structures for handling the time and interval series (the read/write operations rely on a double-buffer technique to avoid too much disk access). AnalyzerFramework.jar and BBEEG-ToolFramework.jar are frameworks to enrich the application of new modules (jar libraries), new algorithms (XML files) and tools (jar libraries). Several free libraries available on the Internet were used, especially Java Simple Plugin Framework (JSPF, see [13]) for dynamic loading of modules and tools, and Better Swing Application Framework (BSAF, see [14]) to simplify the separation of concern (user interface and functional aspects).Preanalysis steps are necessary to improve the quality of the results. As we wanted to study the impact of these operations on the final results (mainly quality and speed of analysis), we choose a pipeline approach. We defined an EEG analyzer as an ordered list of EEG processing modules. Each module performs an operation (e.g. signal transformation like filters and smoothers or signal analysis like IBI detection or frequency decomposition) and completes its operation before sending its results to the next module. An analyzer can be described graphically by a flow chart where a box is a module or by an XML-file (like in Fig. 3) that provides the ordered list of modules to the analyzer thread. This thread also holds a data source and a state (running, stopped, paused). The data source is either an EEG (mainly in European Data Format) or a csv/Excel file that contains the location of the EEG files to analyze. In the latter case, the analyzer works in batch mode: it loops on all the EEG provided by the csv/Excel file. An analyzer algorithm simply consists of running each module in the list one after the other. It can be stated as follows:•Initialize text report.Current=the first module in the list.Input(Current)=the data source embedded in an EEG reader.Run Current module and append text report with Current report.While (analyzer is in running state) and (Current is not the last module in the list) do:Last=Current;Current=next module in the list;Input(Current)=Output(Last);Run Current module and append text report with Current report;Set the analyzer state in stopped state and return text report.Each time a module is run, it processes the input to produce the output, builds a graphical report with different statistics (inside a panel) and finally returns a text version of the report. In Section 3, we present and evaluate two analyzers used to detect IBIs.Each EEG in our database has been visually analyzed and categorized as “normal” (N), “doubtful” (D) or “pathological” (P). Therefore we have three clusters of visually inspected EEG (N, D and P).Given an analyzer A, features related to brain maturation of newborn in an EEG can be extracted and a vector of m variables computed from the features can be built. For example, IBI detection produces for each EEG an array of intervals (the IBIs) from which we compute IBIs statistics such as the mean duration, the max duration, the total duration, etc. Therefore, we can associate each EEG to a labeled point in IRm, where the label is its category (N, D or P). For a database with n EEG, we obtain a set of n points partitioned in three clusters N, D and P. The choice of the analyzer and its parameters can shift these points, distorting the three clusters.We propose an evaluation of an analyzer A overall quality based on its ability to produce the least amount of overlap in the clusters. The unsupervised evaluation of the partitioning obtained by chosen methods and the parameters when the categories are not known (unsupervised classification) is an important issue in automatic clustering. Many authors have then defined cluster quality or validity indices to optimize parameters and compare same class algorithms (like fuzzy clustering, agregation algorithms, etc., see for example [15–17]). Our problem is slightly different: we know which cluster each point belongs to and our goal is to optimize the feature detection pipeline so that the points inside the clusters will facilitate the building of classification models without knowing the models that will be used beforehand. A good analyzer will minimize the intra-class inertia (the points inside a cluster have the least possible dispersion) and maximize the inter-class inertia (the centers of each cluster are as distant as possible from each other). To measure this ability we choose the Davies–Bouldin index (DBI), defined by its authors in [18], because it was well suited to our purpose as we have a known number of clusters. DBI is calculated as(1)DBI=1M∑i=1Mmaxj=1,...,MRijwith(2)Rij=Ii+Ijd(ci,cj)and(3)Ii=1ni∑k=1nid(xi,k,ci)wherexi,kis the kth point of the ith cluster and ciis the center of the ith cluster. Given the generalized inner product〈x,y〉=xtMy, where M is a Hermitian positive-definite matrix, the distance is defined byd(x,y)=〈x,y〉. Different distances are commonly used to compute DBI and here we use the Euclidian distance, where M is the identity matrix. Another useful distance is the Mahalanobis distance, where M is the inverse of the covariance matrix computed on the whole set. If the Mahalanobis distance better reflects the distribution of the labeled points, it is not appropriate for our data. Indeed, each time we run a given analyzer for a given value with one of its parameters, we obtain points in IRmthat are different from those obtained with an other value of the same parameter. Therefore, the covariance matrix is different from one experiment to another, including the Malahanobis distance. Because we want to compare the analyzers between them, if all other variables are equal (ceteris paribus assumption), we use the Euclidean distance.DBI varies in [0,+∞[ and the lower the value of DBI, the better the clusters are regrouped (low intra-class inertia) and separated (high inter-class inertia). Therefore, DBI is an index for the quality of clustering with our analyzers: the lower the value of DBI, the better the analyzer will separate the clusters.Finally, in order to evaluate an analyzer and its settings, we apply the following procedure to the database of n categorized EEG. Denoted by A(p) an analyzer A with its vector of parameters p and P={p1,p2,p3,…,pq}, the set in which p can take its values, we have:For i=1,…,qCreate the table TiFor j=1,…,nBuild the feature’s list from EEG j with analyzer A(pi)Compute the statistics from detected featuresStore these statistics in the jth row of TiEnd ForCompute the associated Davies–Bouldin index DBIiEnd ForThereby, we have built q tables (one per parameter’s vector) each containing the results of the analysis of n EEG. Each row of Tiis a point in the variable’s space categorized N, D or P: therefore we have a total of n points in the 3 clusters N, D and P. Then we calculate for each analyzer A(pi) a quality index DBIiand we retain the parameter’s vector pisuch that DBIiis minimum. The default settings for analyzer A will be pifor its use with an automatic classifier, without a definitive choice for a given method of classification. DBI calculations and DBI curve building are done using free statistical software R with the clusterSim package from Marek Walesiak and Andrzej Dudek (Department of Econometrics and Computer Science, University of Economics, Wroclaw, Poland).

@&#CONCLUSIONS@&#
