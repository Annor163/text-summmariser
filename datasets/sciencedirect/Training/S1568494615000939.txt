@&#MAIN-TITLE@&#
A directional mutation operator for differential evolution algorithms

@&#HIGHLIGHTS@&#
We propose a directional mutation operator for differential evolution (DE).The directional mutation operator is parameterless and can be combined with any DE algorithm.The directional mutation operator is combined with eleven DE algorithms.The proposed method is evaluated experimentally on CEC 2005 test set and CEC 2008 test set.

@&#KEYPHRASES@&#
Differential evolution,Directional mutation,Generic mutation operator,Global numerical optimization,

@&#ABSTRACT@&#
Differential evolution (DE) is widely studied in the past decade. In its mutation operator, the random variations are derived from the difference of two randomly selected different individuals. Difference vector plays an important role in evolution. It is observed that the best fitness found so far by DE cannot be improved in every generation. In this article, a directional mutation operator is proposed. It attempts to recognize good variation directions and increase the number of generations having fitness improvement. The idea is to construct a pool of difference vectors calculated when fitness is improved at a generation. The difference vector pool will guide the mutation search in the next generation once only. The directional mutation operator can be applied into any DE mutation strategy. The purpose is to speed up the convergence of DE and improve its performance. The proposed method is evaluated experimentally on CEC 2005 test set with dimension 30 and on CEC 2008 test set with dimensions 100 and 1000. It is demonstrated that the proposed method can result in a larger number of generations having fitness improvement than classic DE. It is combined with eleven DE algorithms as examples of how to combine with other algorithms. After its incorporation, the performance of most of these DE algorithms is significantly improved. Moreover, simulation results show that the directional mutation operator is helpful for balancing the exploration and exploitation capacity of the tested DE algorithms. Furthermore, the directional mutation operator modifications can save computational time compared to the original algorithms. The proposed approach is compared with the proximity based mutation operator as both are claimed to be applicable to any DE mutation strategy. The directional mutation operator is shown to be better than the proximity based mutation operator on the five variants in the DE family. Finally, the applications of two real world engineering optimization problems verify the usefulness of the proposed method.

@&#INTRODUCTION@&#
Evolutionary algorithms (EAs)[1,2] are inspired from natural evolution of species. The procedures of EAs parallel those in the evolutionary process of species. Typically, EAs are population-based and depend on variation operators and survivor selection to realize the evolutionary process. They only assume that function values can be obtained given a feasible solution. There is no assumption about the explicit expression or differentiability of the function. In EAs, the function to be optimized is often called fitness functions; the domain of variables called search space; a feasible solution called individual and the function value of a feasible solution called fitness or fitness value. In practice, EAs have been applied to many fields such as engineering design[3], energy management[4], financial strategies [5] and computer vision [6] etc. These applications justify the usefulness of EAs.Generally, the study of EAs is targeted to propose an algorithm that is applicable to a class of problems, is computationally efficient and converges quickly to the global optimum. Several popular EAs are genetic algorithm (GA)[7], genetic programming (GP), evolution strategies (ES), evolutionary programming (EP) and differential evolution (DE). Note that some researchers consider DE as a swarm intelligence algorithm. The classic version of DE is simple to implement, ea sy to use and fast. Although some classic EAs are easy to be programmed and computationally efficient, yet the classic version of EA is often stuck in a local optimum of fitness functions. Hence, numerous researches are proposed to balance the exploitation and exploration search process of EAs. A good and robust EA should not only have a fast convergence rate, but can also reach the global optimum for complex fitness function with many local optima.DE, proposed in the mid-1990s, has been extensively studied. Many variants of DE are reported in the past decade. The paradigm of DE is shown to be very powerful. For example, it secures competitive rankings in all IEEE Congress on Evolutionary Computation (CEC) competitions, whereas no other search paradigm presents such a good performance [8]. The procedure that makes DE stand out is the mutation operator. Specifically, three mutually different individuals are randomly selected from the population. One of them is set as a base, and then it is added with the scaled difference of the other two individuals. To scale the difference is to control the search neighborhood and thus to increase the exploitation and exploration capacity. In this paper, without loss of generality, we assume that the optimization is a minimization problem; fitness improvement means the fitness of a generated individual is less than or equal to the minimal fitness found so far by the algorithm. Why a new individual having equal minimal fitness is seen as an improvement? The reason is to escape from a plateau.Through the above analysis, it is known that difference vectors are helpful for fitness improvement. It is important to point out that these differences only provide a possibility of improvement. It is not guaranteed that fitness would be improved at each mutation operation, or even after a number of mutation operations, say in one generation. Actually, in computer simulations, we observe that the number of generations that can obtain fitness improvement is less than half of the total number of generations (see Section 4.3). Moreover, not only is improving the fitness imperative, but the quantity of improvement is also crucial; otherwise, if only a slight improvement is obtained at every generation, the ultimate performance would not be good either, especially when the computational time or the total number of function evaluations is limited.On the one hand, the difference of two individuals is helpful for improving fitness; one the other hand, fitness improvement cannot happen at every generation. In the case that fitness improvement happens, the child individual should contain some better components than its parent individual. In this case, the difference vector of the child and parent individual can be seen as a good direction. Note that this direction may not be a descent direction. Because this direction leads to a child having better fitness than its parent, a further fitness improvement is possible if continuously searching along this direction. This difference vector should have a higher probability to be a descent direction resulting in better fitness value than the difference of two randomly selected individuals. Hence, we come up with the idea that uses the difference between the child and parent individuals in the case that fitness value is improved to guide the search in the next generation. It is expected to increase the number of generations that improve the fitness, and thus accelerate the convergence process and achieve a better performance. Note that our discussion is based on fitness improvement between two consecutive generations. It is on the population level. The difference information is not immediately fed back to the current generation. In other words, we discuss whether fitness value is improved after a new population of individuals is generated. Once fitness improvement happens between two consecutive generations, the differences will be used to guide the search of the next generation.The character of this idea is similar to the swarm behavior in a swarm system. In swarm intelligence, individuals are sent out to search for a good fitness and share their collected information. Then, more efforts are paid to search the neighborhood of the location where high fitness is obtained; whereas less effort is expended to search the neighborhood of the location with low fitness obtained. In our case, once fitness value is improved at a generation, the difference vectors are collected. Clearly, they contain direction information toward to high fitness. In the next generation, more effort is made to search along these difference vectors.The most important character of this idea is that the idea is generic; it can be combined with any variants of DE. Because it works on the mutation operator and mutation is an essential part of DE, hence, the proposed method can be incorporated into any variants of DE. It is expected that this idea is an intrinsic improvement to DE.The rest of the paper is organized as follows. Section 2 reports a summary of DE and related works. Section 3 presents the proposed approach, its pseudo-code and an analysis of this approach. Section 4 reports the experimental results compared with classic DE, and results after combining the proposed method with eleven DE algorithms. The proposed approach is also compared with the proximity-based mutation operator. It is tested on two real-world engineering optimization problems in Section 5. Section 6 gives the conclusion.This section describes the classic DE method and mutation variants in the DE family of Storn and Price [8,9]. Some related works since 2005 are given in three categories.Through the paper, suppose the real parameter optimization problem is to find the global minimization solution. The problem is represented as f(·). An individual x is represented as a D×1 column vector, e.g. x=[x1, x2, ⋯, xD]T, where D is the dimension of the optimization problem, xj(1≤j≤D), is the jth component (gene) of x. f(x) means the function value at x. In this paper, fitness value is used as a synonym of function value. The search space Ω is delimited by two vectorsxmin=[x1min,⋯,xjmin,⋯,xDmin]Tandxmax=[x1max,⋯,xjmax,⋯,xDmax]T, denoting the lower bound and upper bound of the search space.In general, DE is composed of four steps as first introduced by Storn and Price [10,11]. The four steps are initialization, mutation, crossover, and survivor selection. DE is a population based method. The initialization step is to randomly generate a population of NP individuals, where NP is the population size predefined by the user. Each component of an individual is randomly generated between the lower bound and upper bound. The initialized population is denoted as G1={x1,1, ⋯, xi,1, ⋯, xNP,1}, where the two subscripts of each individual stand for the order of the individual in the population and the number of the generations, respectively. The number of generations NG is predefined by the user. Mathematically, the ith individual xi,1=[x1,i,1, ⋯, xj,i,1, ⋯, xD,i,1]Tin the first generation is generated as follows:xj,i,1=xjmin+(xjmax−xjmin)·rand(0,1),j=1,⋯,Dwhere rand(0, 1) returns a uniformly distributed random number between 0 and 1 (0≤rand(0, 1)≤1).Example 1Suppose a minimization problem is given as follows:minf(x)=x12+x22s.t.−5≤xj≤5,j=1,2.The problem dimension D is 2. The population size NP is 5. Initially five solutions are randomly created. Suppose they are G1={x1,g, ⋯, xi,g, ⋯, x5,g}: x1,g=[−4, −4]T, x2,g=[−3, −3]T, x3,g=[−1, −1]T, x4,g=[2, 2]T, x5,g=[4, 4]T. Accordingly, their function values are f(x1,g)=32, f(x2,g)=18, f(x3,g)=2, f(x4,g)=8, f(x5,g)=32.Denote the current generation as Gg. In the mutation step, NP mutant vectors will be generated, denoted as V={v1,g, ⋯, vi,g, ⋯, vNP,g}. Now, take the generation of the ith mutant vector vi,gfor example. Select three mutually different individuals from the current population. Suppose the chosen individuals are denoted as xr1,g, xr2,g, xr3,g, where r1, r2 and r3 are three mutually different individual indices. They are also chosen to be different from the index i. Mathematically, the mutant vectorvi,g=[v1,i,g,⋯,vj,i,g,⋯,vD,i,g]Tis generated as follows:vj,i,g=xj,r1,g+F·(xj,r2,g−xj,r3,g),j=1,2,⋯,Dor in vector form:(1)vi,g=xr1,g+F·(xr2,g−xr3,g)where F is the so called scale factor. It is a parameter predefined by the user. A popular way to reset an out of bounds componentvj,i,gis to randomly reinitialize it. This is to make sure that each individual is a feasible solution of the optimization problem.Example 2Following Example 1, first generate mutant vector v1,g. Randomly choose r1, r2, and r3. Suppose r1=5, r2=3, r3=4, and F=0.5, then v1,g=x5,g+F(x3,g−x4,g)=[2.5, 2.5]T. Second generate v2,g. Suppose r1, r2, and r3 are randomly chosen as 4, 5, and 3, respectively. Thus v2,g=x4,g+F(x5,g−x3,g)=[4.5, 4.5]T. Similarly we can generate v3,g, v4,g, and v5,g. Suppose v3,g=[−2, −2]T, v4,g=[4.5, 4.5]T, and v5,g=[−1.5, −1.5]T.In the crossover step, NP trial vectors will be generated, denoted as U={u1,g, ⋯, ui,g, ⋯, uNP,g}. Take the generation of the ith trial vector ui,gfor example. Each component of ui,gis taken from the ith mutant vector vi,gwith probability Cr or from the ith individual xi,gwith probability 1−Cr. The probability Cr is the so called crossover rate and predefined by the user. In order to make sure that at least one component of ui,gis inherited from vi,g, randomly choose one component index between 1 and D, and the chosen component in ui,gwill be taken from the same position of vi,gwith probability 1. Mathematically, ui,g=[u1,i,g, ⋯, uj,i,g, ⋯, uD,i,g]Tis generated as follows:(2)uj,i,g=vj,i,gifrand(0,1)≤ Crorj=kxj,i,gotherwisewhere k(1≤k≤D) is a randomly generated integer.Example 3Following Example 2, first generate trial vector u1,g. We need to choose an integer k, and suppose k=1. The crossover operates on each dimension with crossover rate Cr=0.9. Let j=1, suppose rand(0, 1) = 0.31, then u1,1,g=v1,1,g=2.5 because 0.31<0.9 and also j=k=1. Next for j=2, suppose rand(0, 1) = 0.64, then u2,1,g=v2,1,gbecause 0.64 < 0.9. Thus trial vector u1,g= [2.5, 2.5]T. Similarly we can generate u2,g,u3,g,u4,g, and u5,g. Suppose u2,g=[4.5, 4.5]T, u3,g=[−2, −2]T, u4,g=[4.5, 4.5]T, u5,g=[−1.5, −1.5]T.In the survivor selection step, a new population containing NP individuals will be generated, denoted by Gg+1={x1,g+1, ⋯, xi,g+1, ⋯, xNP,g+1}. Take the generation of the ith individual xi,g+1 for example. xi,g+1 is set to the vector with the smaller fitness between ui,gand xi,g, which are respectively the ith trial vector and ith individual in generation Gg. Clearly, there are NP parallel competitions between the parent individual and the child individual in this step.Example 4Following Example 3, the function values of the trial vectors are f(u1,g)=12.5, f(u2,g)=40.5, f(u3,g)=8, f(u4,g)=40.5, f(u5,g)=4.5. First we have x1,g+1=u1,gbecause f(u1,g) = 12.5 < f(x1,g) = 32. Second we have x2,g+1=x2,gbecause f(x2,g) = 18 < f(u2,g) = 40.5. Similarly we have x3,g+1=x3,g, x4,g+1=x4,g, x5,g+1=u5,g. Then generation Ggends and a new population Gg+1 is obtained.After Gg+1 is generated, the above mutation, crossover and survivor selection steps are repeated until some termination condition is reached.Definition (Current best solution): Assume Ggis the gthgeneration with a population of individuals Gg={x1,g, ⋯, xi,g, ⋯, xNP,g}. For minimization, the best fitness in Ggis defined as:fgbest=min1≤i≤NPf(xi,g).fgbestis called current best fitness. The corresponding solution offgbestis called current best solution, denoted asxgbest.Definition (Best so far solution): Assume the current generation is Gg,fhbestis the best fitness in Gh, 1≤h≤g, andxhbestis the current best solution in Gh, 1≤h≤g. For minimization, the best so far fitness is defined as:fbest=min1≤h≤gfhbest.fbestis called best so far fitness. The corresponding solution of fbestis called best so far solution, denoted as xbest.The description above is the original version of DE. It is called classic DE (CDE) in this paper because it is the earliest published version and most popularly used. The source code in popular programming languages can be freely downloaded from the web page of Storn [12]. Later on, many different variants were proposed for improving the performance of CDE. The DE family can be written in the form of DE/x/y/z [2], where x stands for a string denoting the base vector to be mutated. For example, x is “rand” means a randomly chosen base vector; “best” means the individual with the best fitness in the current population; y is the number of differences of individuals used in the mutation operator; and z is the type of crossover operator. For example, z is “bin” means binomial crossover, while “exp” means exponential crossover. Using this expression, CDE can be represented as: DE/rand/1/bin. The variants in DE family differs only in how new solutions are generated. They are listed below:“DE/rand/1:”(3)vi,g=xr1,g+F·(xr2,g−xr3,g)“DE/best/1:”(4)vi,g=xgbest+F·(xr1,g−xr2,g)“DE/target-to-best/1:”(5)vi,g=xi,g+F·(xgbest−xi,g)+F·(xr1,g−xr2,g)“DE/best/2:”(6)vi,g=xgbest+F·(xr1,g−xr2,g)+F·(xr3,g−xr4,g)“DE/rand/2:”(7)vi,g=xr1,g+F·(xr2,g−xr3,g)+F·(xr4,g−xr5,g)Exponential crossover and either-or crossover [11] are less popular than binomial crossover. In this paper, only binomial crossover is considered. As said in [11,13], DE/rand/1/bin is more robust for solving multimodal functions but slower to converge when compared to DE/best/1/bin, DE/target-to-best/1/bin and DE/rand/best/2/bin, which are faster to converge but unreliable for highly multimodal functions. From our experiments, DE/rand/2/bin is similar to DE/rand/1/bin but slightly slower.

@&#CONCLUSIONS@&#
