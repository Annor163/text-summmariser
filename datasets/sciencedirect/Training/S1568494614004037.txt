@&#MAIN-TITLE@&#
A non-linear clustering method for fuzzy time series: Histogram damping partition under the optimized cluster paradox

@&#HIGHLIGHTS@&#
The clustering problem is an emergent issue in fuzzy time series.The existing clustering methods deal with the number of clusters or their size.Optimized cluster paradox refers to that trade-off between size and number.The histogram damping algorithm (HDP) is proposed to deal with this problem by estimating a proper cluster form with number and size characteristics.The proposed model is tested against the conventional methods and the case of random data is also presented. Results indicated superiority of the proposed method.

@&#KEYPHRASES@&#
Fuzzy time series,Length of intervals,Cluster optimization,

@&#ABSTRACT@&#
The aim of this paper is to investigate the problem of finding the efficient number of clusters in fuzzy time series. The clustering process has been discussed in the existing literature, and a number of methods have been suggested. These methods have several drawbacks, especially the lack of cluster shape and quantity optimization. There are two critical dimensions in a fuzzy time series clustering: the selection of a proper interval for fuzzy clusters and the optimization of the membership degrees among the fuzzy cluster set. The existing methods for the interval selection assume that the intended data has a short-tailed distribution, and the cluster intervals are established in identical lengths (e.g. Song and Chissom, 1994; Chen, 1996; Yolcu et al., 2009). However, the time series data (particularly in economic research) is rarely short-tailed and mostly converges to long-tail distribution because of the boom-bust market behavior. This paper proposes a novel clustering method named histogram damping partition (HDP) to define sub-clusters on the standard deviation intervals and truncate the histogram of the data by a constraint based on the coefficient of variation. The HDP approach can be used for many different kinds of fuzzy time series models at the clustering stage.

@&#INTRODUCTION@&#
Among the fuzzy related studies in the literature, fuzzy time series is one of the unique research fields, and it contributes to the conventional time series analysis by data clustering and rule-based extrapolation features. Traditional time series research has several weaknesses such as a difficult modeling procedure, emergence of data noise and the requirement of superior expert consultation for model selection and control. Under these circumstances, the fuzzy time series improves the conventional one by noise reduction or handling uncertainty (fuzzy clustering) and has the convenience and simplicity of modeling. In the literature, several assumptions such as normality, stationarity and minimum data constraint are thought to be needless while the theoretical background of forecasting requires dealing with these assumptions. For example, the stationary problem retains its importance since the impact of non-stationary also exists for fuzzy extended data in principle [4,5,33].The fuzzy time series can be expressed by a number of alternative terms such as rule-based forecasting method, rule of thumb solution, an educated guess method, pattern recognition, time series clustering or heuristic modeling. All of these indications denote the fuzzy time series and present its data bundling and approximate case-based reasoning characteristics. Zadeh [6] stated that the most important contribution of the fuzzy logic is the approximate reasoning for automatic control systems. The fuzzy time series is an outstanding method for approximate reasoning by using historical analogies and circumscription of data in a specific range.Song and Chissom [1,7] first defined the fuzzy time series forecasting (FTSF) method. It was improved by later studies which ensured the convenience and superiority of the method. For instance, Chen [2] improved Song and Chissom's [7] FTSF method by using simple arithmetic operations; the accuracy of Chen's method resulted in a method that is superior to Song and Chissom's approach. Even a bivariate FTSF model is employed for long-term forecasting of dry cargo freight rates based on 268 years of data [34]. In the last decade, fuzzy time series research dramatically increased and the term “fuzzy time series” is used in more than 2500 published papers (according to the Google Scholar results dated 20th March 2014). However, the majority of this research presents similar solutions for different samples; only a limited number of them contribute to the clustering problem in the fuzzy time series. The major problem in fuzzy time series clustering is the assignment of the number of clusters or, in other words, the assignment of the intervals of the clusters. A number of studies proposed an optimization algorithm for the length of clusters which was based on squared error or accuracy metric minimization [8–11]. The fuzzy C-means clustering also optimizes the distribution of membership degree of data and indirectly estimates the length of intervals while the number of clusters or their initial lengths is prerequisite. These studies assume that the number of clusters is already defined by an external procedure (usually anonymous). Therefore, it raises an arbitrary (subjective) selection problem for a parameter (number of clusters). In addition to that, the length of intervals is assumed to be uniform in most of the FTSF applications. The uniformity assumption is taken granted with an implicit short-tail distribution assumption. Short-tail distribution means that the intended data does not accumulate and form stack of data points in a particular range. Data spreads through the range, and the data frequency of each clusters does not differ much. Under these circumstances, uniform clusters may work well. On the other hand, most of the data used for forecasting have long-tail distributions. Therefore, the spread of data causes frequency oscillations or stacking on a particular cluster. For example, a cluster may have two to three times more data than another cluster. The explanatory power of former cluster will be limited (less degree of freedom) as a result of cluster generalization. Between every single data (every data is a cluster) and entire dataset (only one cluster covers all dataset), an optimum partitioning is needed for eliminating both noise and generalizations.The optimized cluster paradox is a phenomenon which is found particularly in fuzzy clustering methods. The existing clustering approaches cause a trade-off between the assignment of the number of clusters or the optimization of the membership degrees with an initial guess for the number of clusters. The proposed HDP method contributes to the optimized cluster paradox by ensuring the assignment of the number of clusters while defining the initial length of intervals. The HDP algorithm is a frequency sensitive method to define the proper length of intervals according to the data distribution. A further process can be performed for the final optimization of the membership degrees by utilizing the fuzzy C-means algorithm. The HDP algorithm is independent of the primary model of the FTS. Therefore, it is useful any kind of existing and future forms of the FTS models.Rather than a pure engineering approach, this paper follows an integrative perspective between the theoretical forecasting, business practice and the superiority of engineering as an instrument. Based on the principles of forecasting [32], post-sample control, stationarity control, benchmark selection and accuracy metric selection are performed accordingly. The practical users cannot easily deal with complexities, initial numbers/inputs and other kind of user settings. The proposed approach eliminates such task loads existing in the conventional FTS models since some parameters are initially set in most of the clustering techniques. One of the rationales behind the expert systems and algorithms like FTS models is subjectivity and user-dependence problems in traditional methods (e.g. econometrics). Therefore, the elimination of any user-dependent processes is raison d’être for the FTS based models. This paper broadly improves a paradoxical process of the traditional algorithms while ensuring both theoretical and practical soundness.This paper proposes a novel method named histogram damping partition (HDP) to convert long-tailed distributions to short-tailed ones by an iterative subdivision process. The HDP method is an algorithm that selects the highest frequency on standard deviation intervals and splits it into two subdivisions while the frequency variation constraint is ensured. Every highest frequency partition at the end of iterations is split into subdivisions, and the process is terminated when the coefficient of variation (CV) of the data frequencies reaches below 0.5. The selection of the constraint is dependent on two reasons: first, the CV is a conventional indicator for data variations and degree of volatility; and second, the value of 0.5 is the approximate boundary for short-tailed distributions. Once the CV is truncated below the 0.5, the histogram of the cluster frequencies proves the short-tail distribution. When the HDP process is terminated, the initial midpoints, and intervals of clusters are assigned according to the final results of the major and subdivisions.The accuracy of the HDP method is compared with benchmark methods found in existing literature due to the mean absolute scaled error (MASE) metric. In the existing literature, a number of studies utilize the root mean squared error (RMSE) and mean absolute percentage error (MAPE) for accuracy control. However, the reliability of RMSE and MAPE are found very questionable according to the empirical evidence of biased results [12]. Therefore, the use of RMSE and MAPE are not recommended for further studies while the drawbacks of MASE are taken into consideration. Although the optimization of membership degrees is not performed in this study, the remaining part is already superior to the benchmarks. A further fuzzy C-means process may inherently improve the remaining results.In existing literature, many researchers implement the student enrolment data of the University of Alabama for accuracy control using benchmark methods. The basis for using this data is motivated by the initial practices in the FTS and its convenience for comparison with the benchmarks. However, there is a theoretical shortcoming of using such data since its frequency is very low, and the existing data points are few. From the point of view of conventional forecasting science, it is very difficult to interpret the prediction outcome of such a limited dataset. Another theoretical drawback exists on the simplicity of this dataset. The practical applications of prediction are usually performed for data including many different characteristics such as seasonality, boom-bust cycles, inclusion of irregular noise and structural breaks. Unless the proposed method is tested for a traditional cyclic dataset, it is an incomplete interpretation to verify its superiority. The post-sample control requirement is another reason for preference. In the forecasting theory, the post-sample control (i.e. out of sample, test period) is essential among other principles of forecasting. For performing a post-sample control, one needs to divide entire data into two sub-samples (initialization period and test period). When the data range is short, sub-samples will be incapable of accuracy control.Therefore, an economic dataset, the time charter freight rate data of a dry bulk carrier, is preferred for the empirical tests. The proposed data is composed of more than 10 years monthly series (January 2000–June 2011) including a number of boom-bust fluctuations, seasonal impacts and a long term trend with short term irregular oscillations.The length of intervals (clustering) is one of the critical issues in improving forecasting accuracy in the FTSF. Song and Chissom [1] preferred to use “1,000” as the interval for clustering in the FTSF method. This is an arbitrary number that was used without theoretical or empirical evidence. Later research in the existing literature investigates the clustering problem from different perspectives. Huarng [13] proposes a partition method by using the absolute mean of the first differences. On the other hand, Yolcu et al. [3] introduced a new approach which uses a single variable constrained optimization to state the ratio for the length of intervals. They proposed to optimize the ratio by using the minimum (RMSE) constraint. The empirical control is performed for two datasets, and the optimized particulars (the ratio is 0.2 for both of them) are found or presented identically without an interpretation on possible impacts. Since there are no reported particulars for identical solutions, it appears to be an arbitrary selection rather than a parametric approach.The assignment of the number of initial lengths of intervals (ILI) is also an existing gap in cluster optimization methods such as K-means [8] and fuzzy C-means [14,15]. Both methods require the number of clusters as a prerequisite and do not contribute to the assignment of the number of clusters. An arbitrary selection of the number of clusters is noted in vast numbers of empirical works in the literature. Liao [16] discusses this phenomenon and expresses that many scholars propose no procedure to determine the optimal number of clusters or they use large numbers of clusters as an initial guess.Chen and Tanuwijaya [17,18] introduced an automatic clustering method by using one-half of standard deviation of the first differences (after sorted in numerical order) as the membership constraint. The problem with their method is based on the indifference value which is recommended to be a half of a standard deviation of the first differences. Although this method can be useful for a number of samples, it is very impractical for economic time series. For example, this method is applied to the intended dataset of this paper (time charter rates in monthly frequency). There are 132 data points, and the number of clusters is found to be 38. So, after cluster 14, every single data point generates a new cluster, and 26 clusters consist of just one or two data points (68% of whole sets) while cluster 2 consists of 31 data points. Under these foundations, the proposed method of Chen and Tanuwijaya [17,18] has practical anomalies which may deteriorate estimation and forecasting accuracy.The equal length clustering approach is frequently used in the literature (for an extended discussion please refer to Liao [16]) even though it causes a quantity prediction bias. It is very common that fuzzy C-means clustering dramatically changes the length of intervals, and some intervals might be very long while some others are very short. These outcomes indicate that the number of clusters based on the initial equal length design might be biased due to the equal length assumption. Therefore, the clustering process should be designed in two steps including an initial clustering algorithm which ensures scale-free and non-equal length of intervals for the posterior fuzzy C-means optimization.The clustering process of the FTS is very complicated under the optimized cluster paradox, and it is an emergent issue to establish a parametric and scale-free algorithm to deal with this problem. HDP algorithm is proposed to contribute to the assignment of the number of clusters and the initial length of intervals which can also be used for further optimizations such as fuzzy C-means.

@&#CONCLUSIONS@&#
