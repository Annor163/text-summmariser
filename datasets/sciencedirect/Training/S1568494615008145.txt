@&#MAIN-TITLE@&#
A robust hybrid approach based on particle swarm optimization and genetic algorithm to minimize the total machine load on unrelated parallel machines

@&#HIGHLIGHTS@&#
Unrelated parallel machines scheduling problem with past-sequence-dependent setup times, release dates, deteriorating jobs and learning effects is presented.The objective is to determine jointly the jobs assigned to each machine and the order of jobs such that the total machine load is minimized.An efficient hybrid meta-heuristic (PSO-GA) approach is presented.

@&#KEYPHRASES@&#
Unrelated parallel machines scheduling,Deteriorating jobs,Learning effect,Release dates,P-S-D setup times,Meta-heuristic algorithms,

@&#ABSTRACT@&#
This paper dealt with an unrelated parallel machines scheduling problem with past-sequence-dependent setup times, release dates, deteriorating jobs and learning effects, in which the actual processing time of a job on each machine is given as a function of its starting time, release time and position on the corresponding machine. In addition, the setup time of a job on each machine is proportional to the actual processing times of the already processed jobs on the corresponding machine, i.e., the setup times are past-sequence-dependent (p-s-d). The objective is to determine jointly the jobs assigned to each machine and the order of jobs such that the total machine load is minimized. Since the problem is NP-hard, optimal solution for the instances of realistic size cannot be obtained within a reasonable amount of computational time using exact solution approaches. Hence, an efficient method based on the hybrid particle swarm optimization (PSO) and genetic algorithm (GA), denoted by HPSOGA, is proposed to solve the given problem. In view of the fact that efficiency of the meta-heuristic algorithms is significantly depends on the appropriate design of parameters, the Taguchi method is employed to calibrate and select the optimal levels of parameters. The performance of the proposed method is appraised by comparing its results with GA and PSO with and without local search through computational experiments. The computational results for small sized problems show that the mentioned algorithms are fully effective and viable to generate optimal/near optimal solutions, but when the size of the problem is increased, the HPSOGA obtains better results in comparison with other algorithms.

@&#INTRODUCTION@&#
In classical deterministic scheduling, the processing times of jobs are generally assumed as given constants. However, this assumption may not be appropriate due to deterioration jobs and/or learning effect phenomena. In scheduling with deteriorating jobs, any delay in starting of processing a job may lead to increase of required time and effort to accomplish the job. For instance, Kunnathur and Gupta [1] gave a practical example in the production of steel rolling where the temperature of a molten ingot, while waiting to enter a rolling machine, drops below a certain level, the ingot requires to be reheated before rolling. Extensive surveys of scheduling problems with deteriorating jobs have been conducted by Alidaee and Womer [2], Cheng et al. [3], and Gawiejnowicz [4]. In addition, more recent papers which have considered scheduling problems with deteriorating jobs include [5–8]. In scheduling with learning effects, the time needed to perform a job may be decreased by the repetition of processing operation. For example, is pairman has to repair a number of similar products. The time required to repair a product depends on his knowledge, skills, tools and equipment. The repetition of similar operation will be lead to increase of repairmans’ skills and knowledge and more appropriate application of tools and equipment. As a result, the repair time of a product will be decreased. Biskup [9] and Cheng and Wang [10] were the pioneers that introduced the concept of learning in a scheduling setting. Moreover, in recent years, extensive surveys of research related to scheduling with learning effects were provided by [11–15].However, scheduling problems with deteriorating jobs and learning effects simultaneously have received considerable attention recently because the phenomena can be found in many real-life situations. For example, Cheng et al. [16] gave a practical example in the manual production of glass crafts in which silicon-based raw material is first heated up in an oven until it becomes a lump of malleable dough from which the craftsman cuts pieces and shapes them according to different designs into different glass craft products. On the other hand, the pieces that are shaped later require shorter shaping times because the craftsman's productivity improves as a result of learning. Lee [17] first considered single machine scheduling problems with deteriorating jobs and learning effect which the objectives are to minimize makespan and total completion time. He proposed two models that the actual processing time of job Jjscheduled in position r of a sequence is Pj[r]=αjtraor Pj[r]=(P0+αjt)ra, where αjis the deterioration rate, t is the starting time of job Jj, a is the learning index, and P0 is the common basic processing time. He showed that the both scheduling problems are polynomially solvable under simple linear deterioration. Wang [18] considered the scheduling problem that the processing time of job JjisPjr=(αj+βt)ra, where αjis the basic processing time of job Jjand β is the common deteriorating rate. He showed that several single machine problems and several flow shop problems are polynomially solvable. In addition, Wang [19] studied a linear combination model of Pinedo [20] and Biskup [9] in which the processing time of job Jjis Pj[r]=Pj(α(t)+βra), where Pjis the basic processing time, and α(t) is an increasing deterioration function. He showed that the single-machine problems are polynomially solvable when the objectives are the makespan, the total completion time, and the sum of squared completion times. Wang and Cheng [21] considered a model in which the processing time of job Jjis defined as: Pj[r]=(Pj+αjt)ra, where Pjis the basic processing time and αjis the deterioration rate of job Jj. They introduced polynomial-time solutions for some single machine problems and flow shop problems. Wang and Cheng [22] studied the single-machine problem to minimize makespan in which the actual processing time of job Jjis Pj[r]=(P0+αjt)ra, where P0 is the common basic processing time and αjis the growth rate of the processing time of job Jj. They showed that the schedule of jobs by the largest growth rate rule is unbounded for their model. Apart from these articles, other studies which have investigated scheduling problems with deteriorating jobs and learning effects simultaneously include [16,23–27].On the other hand, it is reasonable that in scheduling problems addition to effects of deterioration and learning, the setup times are also considered. Scheduling problems involving setup times are divided into two classes: sequence-independent and sequence-dependent. In the first case, the setup time is usually added to the job processing time while in the second case, the setup time depends not only on the job currently being scheduled but also on the last scheduled job. Koulamas and Kyparisis [28] first introduced the concept of “past-sequence-dependent” (p-s-d) setup times to scheduling problems whereby the setup time is dependent on all already scheduled jobs. They considered standard single machine scheduling with p-s-d setup times and proved that the problems of minimizing maximum completion time (makespan), the total completion time and the total absolute differences in completion times can be solvable in polynomial time, respectively. They also extended their results to nonlinear p-s-d setup times. Kuo and Yang [29] studied single-machine scheduling problem with past-sequence-dependent setup times and learning effects. They showed that the problems of minimizing makespan, the total completion time minimization, the total absolute differences in completion times and the sum of earliness, tardiness, and common due-date penalty could be solved in polynomial time, respectively. Biskup and Herrmann [30] considered single-machine scheduling problems with past-sequence-dependent setup times and due dates. Their results showed that the single-machine problems remain polynomially solvable when the objectives are total lateness, total tardiness (with agreeable processing times and due dates), maximum lateness (with agreeable processing times and due dates) and maximum tardiness (with agreeable processing times and due dates). However, for other problems, namely, the number of tardy jobs, maximum lateness and maximum tardiness, the well-known solution approaches that guarantee optimality are no longer valid. Wang et al. [31] considered single-machine scheduling with past-sequence-dependent setup times and exponential time-dependent learning effect. They showed that the makespan minimization problem, the total completion time minimization problem and the sum of the quadratic job completion times minimization problem can be solved by the SPT rule, respectively. Apart from these articles, there are other contributions to scheduling problems with p-s-d setup times, see for example [32–35].Within the aforesaid researches done on scheduling with deteriorating jobs, learning effects and p-s-d setup times neglected the release dates and almost all studies concentrated on single machine problems. Release dates help to the attractiveness of problems and may lead to decrease in deterioration of jobs and their actual processing time. Also, since the setup time is dependent to the actual processing times of the past processed jobs, it may lead to reduction of setup times and production cost. Thus, in this paper we study scheduling problem with past-sequence-dependent setup times, release dates and effects of deterioration and learning on unrelated parallel machines. Some of the applications of this type of scheduling problem can be pointed out for industries with high technology, such as manufacturing of integrated circuit (IC) boards. An IC board consists of a number of electronic components mounted on it. These electronic components must be processed one-by-one by a machine, where by each component prior to processing requires a setup operation which is proportional to the actual processing times of the already processed components [28]. On the other hand, release dates and effects of deterioration and learning arise from the unavailability of components at the beginning of the scheduling horizon, erosion of machine and equipment, and increase in workers’ proficiency by repeating the tasks, respectively.In recent years, many nature-inspired meta-heuristic algorithms such as genetic algorithm (GA), ant colony optimization algorithm (ACO), simulated annealing algorithm (SA), particle swarm optimization algorithm (PSO), artificial immune system algorithm (AIS), artificial bee colony algorithm (ABC), harmony search algorithm (HS), differential evolution algorithm (DE) and cuckoo search algorithm (CSA) have been widely used to solve optimization problems with complex nature in which a process for enhancing the obtained solution is repeated until a pre-defined terminating criterion is reached [36–39]. Ruiz-Torres et al. [36] considered the unrelated parallel machine scheduling problem with deteriorating effect. They designed a simulated annealing algorithm for the given problem with the objective of makespan minimization. Yeh et al. [38] addressed parallel machine scheduling with learning effects to minimize the makespan and proposed two heuristic algorithms, called the simulated annealing algorithm and the genetic algorithm.During the past decade, hybrid methods are also received significant interest to improve performance of evolutionary algorithms and to find the global optimum solutions [40–44]. The artificial immune algorithm is hybridized with hill climbing local search algorithm to solve optimization problems by Yildiz [40] and then applied to single objective test problem, multi-objective I-beam and machine-tool design optimization problems from the literature. In another study, Yildiz [43] proposed a hybrid robust differential evolution (HRDE) by adding positive properties of the Taguchi's method to the differential evolution algorithm and then applied for multi-pass turning operations to illustrate the effectiveness and robustness of the proposed algorithm in machining operations. The results found by the HRDE reveal that the proposed hybrid algorithm is more effective than particle swarm optimization algorithm, immune algorithm, hybrid harmony search algorithm, hybrid genetic algorithm, scatter search algorithm, genetic algorithm and integration of simulated annealing and Hooke–Jeevespatter search.In the present research, an efficient method based on hybrid of particle swarm optimization (PSO) and genetic algorithm (GA) is proposed for the minimization of the total machine load. The used operators in the proposed approach consist of PSO formula; selection, crossover and mutation from GA, in which PSO is applied to assign jobs on machines and to schedule jobs on each machine, and GA operators is applied to local search in each obtained solution.The rest of this paper is organized as follows. In Section 2, the problem description is given. In Section 3 the mathematical model of the problem is presented. Section 4 introduces the PSO and GA algorithms. Section 5 deals with the proposed method, hybrid of particle swarm optimization and genetic algorithm (HPSOGA). In Section 6 computational experiments are performed and the results are presented. Finally Section 7 concludes this work and points some directions for future research.There are n jobs (J={J1, J2, …, Jn}) to be processed on m parallel machines {M1, M2, …, Mm} where the machines are unrelated. Each machine can process at most one job at a time and preemption is not allowed. All jobs are not available for processing at time zero. Let nidenotes the number of jobs assigned to Miand∑i=1mni=n. We assume, as in practical situations, that m<n. Each job Jj(j=1, 2, …, n) has a release date rj. In addition, let Pjiand tji≥0 be the normal (basic) processing time and starting time of Jjwhen assigned to machine Mi. In this study, we consider a new model by adding release date to the model of Wang [18], in which the actual processing time of job Jjon machine Miif it is scheduled in position k in a sequence is given by:(1)PjikA=(Pji+α(tji−rj))kaj=1,2,…,n,i=1,2,…,m,k=1,2,…,niwhere α≥0 is the common deteriorating rate for all jobs, and a≤0 is the learning index, given as the (base 2) logarithm of the learning rate. In this model, the actual processing time of a job will be decreased due to the learning effect while it is scheduled in a later position. On the other hand, the actual processing time of a job will be increased due to the deterioration effect when the job starts after its release date.We also take setup times into consideration in the scheduling problem, as in Koulamas and Kyparisis [28], in which setup times are past-sequence-dependent (p-s-d), i.e., the p-s-d setup time of jobJkwhen scheduled on machine Miin position k is given by:(2)Si[1]=0andSi[k]=γi∑l=1k−1Pi[l]Ai=1,2,…,m,k=2,…,niwhere γi≥0 is a normalizing constant, andPi[l]Adenotes the actual processing time of a job if it is scheduled on machine Miin position l. For notational simplicity, we denote the p-s-d setup given in Eq. (2) by Spsd[28].For a given schedule π=[J1, J2, …, Jn], Cji=Cji(π) represents the completion time for job Jjon machine Mi. LetCmaxi=max{Cji|j=1,2,…,n,i=1,2,…,m}andTML=∑i=1mCmaxibe the makespan of machine Miand the total machine load, respectively.According to Eqs. (1) and (2), we can obtain the completion time and the actual processing time of kth job scheduled on ith machine, and the total machine load in a given schedule using Proposition 1 as follows:Proposition 1.For a given permutationπi=[Ji[1],Ji[2],…,Ji[ni]]on machine Mi, the actual processing times and the completion times of jobs on machine Mican be represented as follows:Therefore, from the above analysis, the sum of load on all machines can be calculated as the following equation:TML=∑i=1mCmaxi=∑i=1m(Pi[ni](ni)a+(1+α(ni)a)max(ri[ni],Ci[ni−1])−α(ni)ari[ni])+γi∑i=1m∑k=1ni−1(Pi[k]ka+αka(max(ri[k],Ci[k−1])−ri[k]))In this section, we consider the unrelated parallel machines TML problem with effects of deterioration and learning, past-sequence-dependent setup times, and release dates. Then, using the three-field notation introduced by Graham et al. [45], the corresponding scheduling problem can be denoted asRm|PjikA=(Pji+α(tji−rj))ka,Spsd|TML=∑i=1mCmaxi. In the following first the parameters and variables of the model are described and then the proposed model is given.nnumber of jobsnumber of machinesindex of machine, i=1, 2, …, mindex of job, j=1, 2, …, nindex of position, k=1, …, nirelease date of job jnormal processing time of job j on machine icommon deteriorating ratelearning indexnormalizing constant of machine iXjik1 if job j is assigned to machine i in position k and 0 otherwiserelease date of the job which is scheduled in the kth position on machine iactual processing time of the job which is scheduled in the k(l)th position on machine ip-s-d setup time of the job when scheduled in position k on machine istarting time of the job which is scheduled in the kth position on machine icompletion time of job j which is scheduled on machine imaximum completion time (makespan)on machine iObjective function:(3)Min∑i=1mCmaxiSubject to:(4)∑i=1m∑k=1niXjik=1,j=1,2,…,n(5)∑j=1nXjik≤1,i=1,2,…,m,k=1,2,…,ni(6)ri[k]=∑j=1nrjXjik,i=1,2,…,m,k=1,2,…,ni(7)Pi[k]A=∑j=1n(Pji+α(ti[k]−ri[k]))kaXjik,i=1,2,…,m,k=1,2,…,ni(8)Si[k]=γi∑l=1k−1Pi[l]A,i=1,2,…,m,k=2,3,…,ni(9)ri[k]≤ti[k],i=1,2,…,m,k=1,2,…,ni(10)ti[k−1]+Pi[k−1]A+Si[k−1]≤ti[k],i=1,2,…,m,k=2,3,…,ni(11)(ti[k]+Pi[k]A+Si[k])⋅Xjik≤Cji,j=1,2,…,n,i=1,2,…,m,k=1,2,…,ni(12)Cji≤Cmaxi,j=1,2,…,n,i=1,2,…,m(13)Xjik∈{0,1},j=1,2,…,n,i=1,2,…,m,k=1,2,…,ni(14)Si[1]=0,i=1,2,…,m(15)ri[k],Pi[k]A,Si[k],ti[k],Cji≥0j=1,2,…,n,i=1,2,…,m,k=1,2,…,niEq. (3) minimizes the total machine load. Constraint (4) ensures that each job is assigned to one of the existing positions on the machines. Constraint (5) ensures that either no job or one job can be assigned to each position. Constraints (6)–(8) measure the release date, actual processing time and p-s-d setup time of any job when scheduled in position k on machine i, respectively. Constraint (9) indicates that starting time of the scheduled job in the kth position on machine i be greater than or equal to its release date. Constraint (10) indicates that the begin processing time of the job in a position on each machine be greater than or equal to the sum of the starting time, actual processing time and p-s-d setup time of the scheduled job in previous position on the corresponding machine. Constraint (11) defines the completion time. Completion time of job j on machine i is calculated by sum of its starting time, actual processing time and p-s-d setup time. Constraint (12) defines the maximum completion time on machine i. Constraints (13)–(15) define the range of variables.Genetic algorithm (GA) was first introduced by Holland [46] and further its approach and applications in details described by Goldberg [47]. GA is a stochastic general search strategy premised on the natural evolution process. It has been widely applied to solve several diverse optimization problems such as facilities location problem [48], unrelated parallel machines scheduling problem [49], broadcast scheduling problem [50], optimization of neural networks [51], inventory control problem [52], and etc. In addition many studies have applied genetic algorithms to solve the parallel machine scheduling problems. Vallada and Ruiz [53] investigated the unrelated parallel machine scheduling problem with sequence dependent setup times and applied a genetic algorithm includes a fast local search and a local search enhanced crossover operator to solve this problem. Alcan and baslıgil [54] addressed a non-identical parallel machine scheduling problem with fuzzy processing times and suggested a kind of genetic algorithm based on machine coding for minimizing the makespan. They also used the triangular fuzzy processing times in order to adapt the GA to defined problem. Eroglu et al. [55] proposed a genetic algorithm with local search for the unrelated parallel machine scheduling problem with sequence-dependent set-up times to minimize maximum completion time.The GA generally starts with an initial set of random solutions, called initial population of chromosomes (individuals). In the population, chromosomes are a string or sequence of genes that displays the possible solutions of problems in the form of genes coding. The chromosomes of the population are evaluated using a pre-defined fitness function, so that chromosomes with higher fitness value will receive more chances to be selected for reproducing next generation. The aim of fitness evaluation is to calculate the competence of each chromosome in the population based on criterion that defines the objective function. In each iteration (or called generation), a new population is created using genetic operators; namely crossover and mutation. The crossover operator is applied at a time on two chromosomes (parents) and generates chromosomes (offsprings) by exchanging information between those two chromosomes. The main purpose of using the crossover operator is collecting of the good features of two selected parents in offspring chromosomes and thus generates the better chromosomes. The mutation operator is applied to one chromosome and creates a new combination of genes by displacing the position of the genes. The main purpose of using mutation operator is preventing convergence of chromosomes to local optimal solution and creating diversity of population to achieve better solutions. After a generation of new chromosomes, the new population is evaluated again, and the whole process is repeated until a specific stopping criterion is reached [47]. The pseudo-code of the GA is illustrated in Fig. 1, and details of how to implement the algorithm for the unrelated parallel machines scheduling problem are described as follows.The first step in the adoption and implementation of GA technique is considering a chromosome representation or solution structure. In this study, we divide the chromosome into two segments. The first segment of the chromosome is made up of n job symbols, denoted by integers, numbered from 1 to n. The second segment is made up of m-1 partitioning symbols, denoted by “*”, with distinct subscripts from 1 to m-1 and are used to separate the machines. Generally, for a problem with n jobs to be assigned to m machines, a solution consists of (n+m−1) distinct elements [56]. For example, for a problem with 8 jobs and 3 machines; the chromosome is shown in Fig. 2, where the jobs 7, 8 and 1 are assigned to machine 1, jobs 6, 4 and 2 are assigned to machine 3, and jobs 3 and 5 are assigned to machine 2. It should be noted that when there are no job symbols between two successive partitioning symbols, related machine of the second partitioning symbols will be unemployed in the schedule. Similarly, when there are no job symbols after the last partitioning symbol, the rest machine will be unemployed in the schedule.Generating a set of initial solutions is the first step for implementing GA. The initial population can be generated in a random procedure from the solution space or by a heuristic procedure. To generate random solutions for the initial population, the procedure is as follows:(1)Set t=1.Generate a job permutation from 1 to n and randomly assign them to genes of the chromosome.Generate (m−1) stars “*” and randomly assign to the rest of genes of the chromosome.Set t=t+1.Evaluate: if t≤pop-size go to Step 3, else if t>pop-size stop.The selection means how to choose chromosomes (parents) in the current population to reproduce next generations. In fact, selection is a process for selecting individuals within the population for mating purposes. In this study, the selection mechanism is implementing via two types of selection procedure. The first procedure is the roulette wheel selection, in which the chromosomes in each generation are sorted based on their corresponding fitness values. The second one is tournament selection procedure, in which a number of chromosomes (tour-size) are selected from the population randomly and chromosome with more favorable fitness value will receive a higher probability to participate in crossover operation. In this procedure when the tour-size is larger, weak chromosomes have a little chance to be selected and vice versa.The crossover operator is applied with a probability of Pcon two strings that were selected as the parents and generates two new offspring solutions. The basic mechanism used for the crossover operator is the standard single-point crossover which in the proposed algorithm a modified version of standard single-point crossover is employed. In this mechanism, as shown in Fig. 3, at first two parent strings are chosen, named parents 1 and 2, from the population and then randomly a crossover point (or cut point) for parent strings is selected. When crossover point is identified, all elements (genes) before the crossover point are copied from the parent 1 to the first segment of the primary offspring 1. The second segment of the primary offspring 1 is made up by copying all elements after the crossover point from the parent 2. Primary offspring 2 is made in a similar way by reversing the roles of the parents. Since the elements before and after the crossover point in the created offspring has repetitive elements, the generated primary offsprings are illegal solutions and should be reformed under a modification mechanism. Therefore, in order to modify generated offsprings, we reconnoiter the location of repeated elements which appear in primary offsprings before the crossover point. Finally, these repeated elements in primary offspring 1 and primary offspring 2 should be replaced by each other to generate final offsprings 1 and 2.The mutation operator creates a new solution by a random change on a selected chromosome, with the probability Pm. In this study, we consider three kinds of mutation operators, swap, inversion and shift, which are illustrated in Fig. 4. These mutation operations are defined as follows:1.Swap: two elements from a chromosome are chosen randomly, and then their positions will be exchanged.Inversion: two cut points from a chromosome are chosen randomly, and then the positions of the elements placed between these points will be reversed.Shift: two elements at positions i1 and i2 from a chromosome are chosen randomly. If 1≤i1<i2≤n, then right shift is applied. In this shift, new chromosome is generated as follows:New chromosome=[element(1:i1−1) element(i1+1:i2) element (i1) element (i2+1, n)]On the other hand, if 1≤i2<i1≤n, then left shift is applied. In this shift, new chromosome is generated as follows:New chromosome=[element(1:i2−1) element(i1) element(i2:i1−1) element (i1+1, n)].Reproduction is the process which selects the worthy chromosomes in a population and forms a mating pool. In this study, we use an elitism strategy to preserving the elite offspring generated during the evolutionary process. In this strategy, after creating offspring, the pop-size of best chromosomes from among the previous generation and lately generated offspring are directly copied into the new generation. Applying this mechanism in GA can increase performance of algorithm rapidly and help to accelerate the convergence with high reliability.Particle swarm optimization (PSO) is a population-based stochastic search technique for solving optimization problems with continuous search space that was introduced by Kennedy and Eberhart [57]. Although the most existing PSO applications are applied to continuous optimization problems, but it has been successfully applied in very few studies for solving discrete optimization problems such as cell formation problem [58], artificial neural networks [59], optimization of vehicle crashworthiness [60], resource constrained project scheduling problem [61], etc. In addition many studies have applied particle swarm optimization algorithms to solve the parallel machine scheduling problems. Husseinzadeh Kashan and Karimi [62] proposed a discrete particle swarm optimization (DPSO) algorithm to tackle the parallel machines scheduling problem and investigated the effectiveness of DPSO algorithm through hybridizing it with an efficient local search heuristic. Torabi et al. [63] considered a fuzzy multi-objective unrelated parallel machines scheduling problem with ready times, sequence and machine-dependent setup times, and secondary resource constraints for jobs. They also presented an effective multi-objective particle swarm optimization (MOPSO) algorithm to find a good approximation of Pareto frontier. Behnamian [64] suggested a discrete particle swarm optimization (DPSO) algorithm for fuzzy parallel machines scheduling problem, which comprises two components: a particle swarm optimization and genetic algorithm.This method has been inspired by the social behavior of animal groups such as a flock of birds in order to search food sources or a school of fish in order to avoid predators. PSO initializes a population of individuals (or potential solutions) with random positions and velocities and then, each individual, called as “particle”, flies around in the search space. The performance of each particle in different positions is evaluated by its fitness value, which the particle's fitness is computed by putting its position into a pre-defined objective function. During the evolutionary process, velocity and position of each particle will be changed according to the particle's own searching experience (i.e., best personal position) and other particles’ searching experiences (i.e., best global position). The position and velocity for particle i in an n-dimensional search space are represented by the vectors Xi=(xi1, xi2, …, xin) andVi=(vi1,vi2,…,vin), respectively. The velocity and position updating of the particle is shown by the following Eqs. (16) and (17):(16)vij(t+1)=w⋅vij(t)+c1⋅r1⋅(pbestij(t)−xij(t))+c2⋅r2⋅(gbestj(t)−xij(t))(17)xij(t+1)=xij(t)+vij(t+1)wherevij(t+1)represents the velocity and xij(t+1) represents the position of particle i in the jth (j=1, 2, …, n) dimension of the search space at the (t+1)th iteration. pbestij(t) is the best personal position of particle i in dimension j during iteration t, and gbestj(t) is the best global position visited so far among all particles in dimension j during iteration t.w, called the inertia weight, is a parameter to control the impact of the previous velocities on the current velocity. A large inertia weight facilitates the global search scope, while a small one leads to facilitate the local search scope. Therefore, a suitable value of this parameter(w)creates the balance between global and local search abilities of the swarm. c1 and c2 are the cognition and social learning factors, respectively; r1 and r2 are random numbers uniformly distributed between [0,1]. In system of PSO, the particles search continues in the multidimensional space to achieve the best solution until a pre-defined stopping criterion, which is usually a maximum number of iterations or a sufficiently good fitness, is reached. The pseudo-code of the PSO is shown in Fig. 5, and the steps of the proposed algorithm are explained in detail as follows:Generating a set of initial solutions is the first step of PSO implementation. The initial population is created randomly. In this paper, the random-key (RK) technique is employed. In the RK encoding method, a position in RK continuous space is converted to a discrete space. Every position in the RK virtual space is indicated by a vector of real numbers while every position in the problem solution space is indicated by a vector of integers. Suppose that n jobs are ready for assigning to m machines, the RK virtual space for a particle is created in n+m−1 dimensions. In the process of the random key encoding, to each of the dimensions of a particle a random number from [0, 1) is assigned, that is (δ1, δ2, …, δn+m−1), where δk, 1≤k≤n+m−1, is the corresponding weight of kth candidate Then, to decode the particle, we sort the corresponding weight of each dimension in an ascending order with the candidate indices. For example, for a problem with 8 dimensions, the process conversion of RK virtual space to a feasible permutation in the problem solution space is shown in Fig. 6. In this example, the position of a particle in RK virtual space is (0.72, 0.41, 0.12, 0.96, 0.38, 0.66, 0.15, 0.59). Thus, after sorting these weights in an ascending order, the RK encoding method convert it to (7, 4, 1, 8, 3, 6, 2, 5).The fitness values of particles will be measured by putting their positions into a pre-defined objective function. Forasmuch as the objective function of the model is minimization, in this study we consider the reverse of objective function value as the fitness value. As a result, fitness function value of each particle is calculated as the following:(18)Fkt=1ƒ(Xk(t))=1∑i=1mCmaxi(Xk(t)),k=1,2,…,Pop-sizewhereFktis the fitness value of particle k at iteration t, ƒ(Xk(t)) is the objective function value of Xk(position of particle k) at iteration t,∑i=1mCmaxi(Xk(t))is the total machine load of particle k at iteration t and Pop-size is the population size or the number of particles at each population. Accordingly, a particle with higher fitness value has more merit to reproduce next generation.After obtaining the fitness values of particles, pbest and gbest values are updated by comparing the current fitness value of each particle with its pbest and gbest. If particle's fitness value is better than its pbest, particle's pbest value and pbest position will be replaced. In the same way, if current pbest value of each particle is better than gbest, current gbest value and gbest position will be replaced. Therefore, from the above analysis, pbest and gbest can be computed with following Eqs. (19) and (20).(19)pbestk(t+1)=xk(t+1),ifFkt+1>Fkpbestpbestk(t),ifFkt+1≤Fkpbest(20)gbest(t+1)=argmaxFkpbest,1≤k≤Pop-sizeThe velocity and position of each particle will be updated according to Eqs. (16) and (17). In this study, the constriction coefficient (χ) [65] is utilized to restrict the particle velocity and to decrease some undesirable feedback effects. In the algorithm of PSO with constriction coefficient, velocities are constricted by the following change in the velocity update:(21)v′ij(t+1)=χ[vij(t)+c1⋅r1⋅(pbestij(t)−xij(t))+c2⋅r2⋅(gbestj(t)−xij(t))]wherevij(t+1)is the updated value of velocity vectorvijat iteration t+1, and χ is a constriction factor as reflected in Eq. (22).(22)χ=22−ϕ−ϕ2−4ϕ,ϕ=c1+c2>4Typically value of ϕ is equal to 4.1 so that c1=c2=2.05 and χ=0.729. As a conclusion, can be said that PSO algorithm with constriction coefficient can be applied as the equivalent of Eq. (16) with constants c1 and c2 are equal to 1.4944 andwis equal to 0.729 [66]. Also, the position of the particle is computed with the following formula:(23)x′ij(t+1)=xij(t)+v′ij(t+1)wherex′ij(t+1)is the updated value of position vector xijat iteration t+1. The results of adopting this strategy to update the velocity and position of particles indicate improving the performance of the algorithm to find the optimal solution and enhancing the ability of the swarm to prevent premature convergence on local optimal solution.At the end of iterations, in order to improve the generated solutions quality, a local search procedure can be utilized by exploring the neighborhood of a given solution. One of the main components of a local search is the choice of the neighborhood structure. In this study, we use a local search technique with two operators: swap and inversion, and because the local search is a time-consuming procedure, we will apply local search on global best. The detail description of the local search algorithm applied is as follows:1.Set t=1.Swap: randomly choose two elements from ith particle in tth iteration. Then, interchange the positions of two elements selected. If a better solution is found, it replaces the current solution (update particle i); otherwise, go to step 3.Inversion: randomly choose two cut points from ith particle in tth iteration. Then, the positions of the elements placed between these points will be reversed. If a better solution is found, it replaces the current solution (update particle i); otherwise, go to step 4.Set t=t+1. If t<MaxIt (maximum number of iterations), go to step 2; otherwise stop.In this study, a hybrid optimization approach is introduced to solve the unrelated parallel machines scheduling problem. The proposed approach incorporates particle swarm optimization algorithm and genetic algorithm. This integration is inspired by analyzing weaknesses of PSO algorithm for solving discrete problem, so that it can increase the global search capabilities and prevent from getting trapped in local optimal solution. In proposed method, the algorithm generates a population of random solutions (particles) initially and each particle searches the optima (optimal solution) in the search space by updating generations. During each generation, through applying the crossover operator, the exchange of information between two particles be improved and created a new solution. Additionally, mutation operator is employed to prevent the premature convergence of the algorithm and to increase diversity solution. However, if a more optimal point was found by applying these operators, the particles move towards the point and their velocity and position will be updated. The main process of implementing the HPSOGA algorithm proceeds as follows:1.Initialize a population of particles with random positions and velocities in the problem search space, where each particle contains d variables.The desired optimization fitness function of all particles are computed to measure their performance according to Eq. (18). These fitness values are evaluated for determining particles pbest and gbest as the following:2.1 Select pbest: pbest value and position of each particle are equal to its objective value and current position2.2 Select gbest: gbest value and position are equal to the objective value and position of the best particle obtained from the swarm so farThe velocity and position of the particle are adjusted according to Eqs. (21) and (23), respectively.A rank number k is assigned to each of particles based on their fitness values.The population of particles are updated according to the following steps:5.1 Select all of the old particles to generate population 1.5.2 Select two parent strings by using the roulette wheel selection procedure and apply crossover operator described in Section 4.1.4 to generate population 2.5.3 Select the best string (global best solution) achieved so far and apply mutation operator described in Section 4.1.5 to generate population 3.The created populations (population 1, 2 and 3) are combined and the fitness values of them are computed.The elitist strategy is performed for populations 1, 2 and 3 in order to keep best strings in the next generation.Return to Step 2 until the termination criterion is satisfied.The pseudo-code of the proposed algorithm is shown in Fig. 7.In this section, an experimental design is provided to evaluate the performance of algorithms that are applied to solve the presented model. All the meta-heuristic algorithms, namely HPSOGA, GA and PSO with and without a proposed local search, are coded in MATLAB R2009b and executed on a personal computer with 1.70GHz CPU and 1 GB RAM on Windows XP. Also, as a comparison, LINGO 9 is employed to solve the presented model with small-scale random instances which run on the same computer. Below the details of the doing experiments and also their results are described.

@&#CONCLUSIONS@&#
