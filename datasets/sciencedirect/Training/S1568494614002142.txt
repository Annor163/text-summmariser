@&#MAIN-TITLE@&#
Direct model of memory properties and the linear reservoir topologies in echo state networks

@&#HIGHLIGHTS@&#
A direct relationship between the memory of the linear reservoir and its connectivity is established.The reservoir topology can be constructed according to the given short-term memory.Some ESN's reservoirs presented in others works, the DSR and DLR, are special solutions of our model.The proposed model provides an easy way to adjust the reservoir to the edge of chaos.

@&#KEYPHRASES@&#
Echo state networks,Reservoir computing,Short-term memory,Reservoir topology,

@&#ABSTRACT@&#
Echo state networks (ESN), with a large number of randomly connected neurons (called “reservoir”) and a simple linear output neuron, are a kind of novel recurrent neural network (RNN). One of the most important properties of ESN is short-term memory (STM), which is indispensable for time varying information processing. However, due to the random connection of neurons in the reservoir, the relationship between the topological structure of the reservoir and STM in ESN is not fully understood. In this paper, we concentrate on ESN with a linear reservoir, which consists of neurons with identity activation functions. We transform the iterative mathematical model of ESN into a direct one. In the model, we establish a direct relationship between the memory capacity of ESN and its connectivity, which can obtain the reservoir topology through STM in ESN. We find that some reservoir topologies proposed by previous papers are the special solutions of our method. Furthermore, the proposed model can provide an effective way to adjust the reservoir to a state near the edge of chaos, where previous studies have shown that the reservoir can achieve maximum computational capabilities.

@&#INTRODUCTION@&#
In recent years, reservoir computing (RC) [1] has drawn great interest from the machine learning community. RC is a kind of novel recurrent neural network (RNN) proposed by Jaeger [2,3] for “Echo State Networks (ESN)” and by Maass [4] for “Liquid State Machines (LSM)”. These systems are mainly composed of two parts: a non-trainable sparse recurrent layer, known as “the reservoir”, and a simple linear readout. The reservoir has a large number of neurons (mostly from 50 to 2000) that are randomly connected with fixed interconnection weights. Only the readout neuron needs to be trained, with the use of linear regression in most cases. Generally speaking, ESN uses the analog neurons, whereas LSM uses binary (spiking) neurons. These approaches are easy to implement and free from common problems found in traditional RNNs such as local minima or slow convergence.One of the most important properties RC has is short-term memory (STM) [5], which is capable of storing information about recent input sequences in reservoir's transient responses. This property is also called “fading memory”. The performance of RC could be affected by the memory capacity [1,6–9] and might be a trade-off between memory capacity and so-called kernel quality [10]. However, because of the random connection of the neurons in reservoir, the theoretical principles underlying the memory capacity are poorly understood. There are several fundamental questions concerned about memory capacity and reservoir architecture. For example, how is memory capacity impacted by reservoir size, connectivity, or topology? What reservoir architecture is “optimal” to retain a short-term memory? Is there any upper bound on the memory capacity? If the upper bound exists, how can a reservoir achieve it?Previous analytical studies have made progress on these issues to some extent. Jaeger [5] first proved a sum-rule (we will show more details in Section 2.2) that the memory capacity cannot exceed the number of neurons in the reservoir given noiseless signals. White et al. [11] showed that two special topologies, “distributed shift register (DSR)” and “random orthogonal” linear neuronal networks, could have memory capacity scale with network size. Using the theory of Fisher information, Ganguli et al. [12] studied short-term memory in two different classes of linear neuronal networks: normal connectivity reservoirs and non-normal ones. The performance of normal networks is shown to be inferior to the non-normal ones. The method with which [12] evaluated memory is essentially not consistent with Jaeger's [5]. Hermans et al. explored memory properties of linear neuronal networks in continuous time [13] and those for high dimensional input signals [14]. However, all of the above works merely focused on the Gaussian input signals. Ganguli's further study [15] indicated that, when the input temporal signals are sparse (nongaussian signals), the memory capacity of linear neural networks could exceed the number of neurons in the network by using compressive sensing (CS) [16]. Verstraeten et al. [25] experimentally studied the interplay of memory and non-linear mapping with two measures—deviation from linearity and the maximal Lyapunov exponent. More recently, Rodan et al. [17] investigated three simple deterministically constructed reservoir topologies, and one of them, named linear simple cyclic reservoir's (SCR) memory capacity, can achieve an optimal value, which is the number of neurons in the reservoir. Furthermore, [17] found results different from those in [12] that the linear SCR (so-called normal networks in [12]) are not worse than non-normal ones. In addition, by designing some variant architectures of ESN, Gallicchio et al. [26] also argued that some architectural factors, such as input variability, multiple timescales dynamics, non-linear interactions among units and regression in an augmented feature space, have a major role in determining ESN predictive performances.Despite these recent progresses in RC memory properties and architectural factors, the direct relationship between the memory of ESN and its connectivity has not yet been established. Most of these methods discuss short-term memory and performance by constructing special reservoir topologies, such as random orthogonal networks [11], normal networks [12], delay line reservoirs or simple cyclic reservoirs [17]. However, they can only establish the functions of network state and connectivity, not those of network short-term memory and connectivity. Quoting [15], even with linear neuronal networks and Gaussian input statistics, “the relationship between the memory properties of a neural network and its connectivity is nonlinear, and so understanding this relationship poses an interesting challenge.” At the same time, it has been shown that the computational capabilities of the ESNs are maximized when the reservoir is close to the border between a stable and unstable dynamics regime, the so-called edge of chaos [6–9]. To the best of our knowledge, however, there are no methods mentioned on how to adjust the reservoir toward the edge of chaos according to the short-term memory of ESNs.In this paper, we concentrate on ESN with a linear reservoir, which consists of neurons with an identity activation function. Linear reservoirs, as a simple, effective and important form of reservoir, have been often used in other analytical methods [11–13,15,17]. We establish a direct relationship between the memory of the neural network and its connectivity. To be more specific, we transform the iterative mathematical model of ESN to a direct one. In this model, we can not only get the memory capacity (MC) by specifying the connectivity of the reservoir, but also obtain the reservoir topology based on short-term memory in ESN inversely. Interestingly enough, we find that the distributed shift register (DSR) network [11] and the delay line reservoir (DLR) [17] are the special solutions of our method. Furthermore, our experiment results show that our model can provide an effective way to adjust the reservoir to the edge of chaos through short-term memory, which shed light on designing reservoirs with proper computational capabilities.The rest of the paper is organized as follows. Section 2 provides a brief overview of ESN and a definition of memory capacity. In Section 3, we propose our model and conduct the theoretical analysis, in Section 4 we set up several experiments to explore the relationship between the memory capacity of different size and network connectivity and demonstrate how the reservoir at the edge of chaos can be obtained by tuning the STM of ESNs. Section 5 draws our conclusions.An ESN is a recurrent discrete-time neural network that comprises two basic components: a large and fixed RNN with N (mostly from 50 to 2000) internal neurons and a linear output. The untrained RNN part of an ESN is called the reservoir, from which the desired output is obtained by training suitable output weights. The structural diagram of ESN is briefly illustrated in Fig. 1.Here, we will focus on an ESN without output feedback to avoid instability problems [1,18] and simplify the mathematical analysis, as is also adopted by others [2,19,20]. The equations for updating internal units and computing the linear output can be written as:(1)x(t)=f(wx(t−1)+winu(t))(2)y(t)=woutx(t)where x(t) is the state variable in the reservoir at step t, u(t) is the input of ESN at step t, y(t) is the output of the ESN, and f is the neuron activation function (typically a tanh sigmoid function).w,win, andwoutare the internal connection weights of the reservoir with a size of N×N, the input weights with a size of N×Nin, and the output weights with a size of Nout×(N+Nin), respectively. According to [3], the sparse interconnectivity of the weights matrixwis 1–5%. Only when the spectral radius is set less than 1 does an ESN have the echo state property [3].wandwinare unchangeable after being initialized.Short-term memory (STM) is one of the most important properties in ESN. It is capable of storing information about recent inputs in the reservoir's transient responses and processing temporal context information. The memory capacity (MC) affects the performance of the ESN [1,6–10]. To study the short-term memory, one needs to know how much information on the input signals from k time steps ago can be reconstructed (or “recovered”) by the reservoir. Jaeger [5], for the first time, gave the formal definition MC of STM in ESN. When the ESN is fed with input u(t), the k-delay STM capacity of it is defined as(3)M(k)=cov2(u(t−k),y(t))σ2(u(t))σ2(y(t))where y(t) is the output of the ESN, cov denotes covariance, and σ denotes standard deviation. The range of memory function M(k) is within the interval [0,1]. M(k)=1 indicates the input is perfectly reconstructed by the network, namely y(t)=u(t−k), whereas M(k)=0 the network has no memory of input. The MC of the ESN is(4)MC=∑k=1∞M(k)This sum rule shows that the MC of ESN is the area under the memory function M(k). Jaeger [5] proved an upper bound on MC≤N for ESN with linear output units and independent identically distributed (i.i.d.) input, where N is the reservoir size.In this section, we first transform the iterative mathematical model of ESN to the direct form and then establish a direct relationship between the MC of ESN and its connectivity. In this model, we have the internal connection matrixwof the reservoir with a memory span value. Finally, we prove that the distributed shift register (DSR) network [11] and the delay line reservoir (DLR) [17] are the special solutions of our method.Here, activation function f in Eq. (1) is the identity function, and the reservoir is called a “linear” reservoir. A linear reservoir is a simple, effective and important form of reservoir, commonly used by other analytical methods of RC [11–13,15,17]. Eqs. (1) and (2) are rewritten as(5)x(t)=wx(t−1)+winu(t)(6)y(t)=woutx(t)wherewin∈RN×1,w∈RN×N, andwout∈R1×(N+1). N is the reservoir size. We transform the iterative Eq. (5) to a direct form(7)x(t)=win+wx(t−1)=winu(t)+wwinu(t−1)+w2x(t−2)=⋯=∑τ=0∞wτwinu(t−τ)According to the definition of memory, if the input signals from k-time steps ago u(t−k) are recovered by the reservoir, namely y(t)=u(t−k), then we can combine Eqs. (6) and (7) as(8)wout∑τ=0∞wτwinu(t−τ)=u(t−k)And Eq. (8) is rewritten as(9)∑τ=0∞woutwτwinu(t−τ)=u(t−k)We denote the optimal output weight vector bywout(k)when the network recovers the input from k-time steps ago. The optimal here means the outputs of the network store the maximum information of recent inputs in reservoir transient responses with givenwandwin. For example, if k=2, we can write Eq. (9) aswout(2)winu(t)+wout(2)wwinu(t−1)+⋯+wout(2)wτwinu(t−τ)+⋯=u(t−2).In practical terms, input data are not infinite. We assume the length of the input signals is n, and the maximum time step that can be recovered by the network is the r-time steps ago. We then have a set of equations from Eq. (9) in the finite case:(10)wout(0)winwout(0)wwin⋯wout(0)wnwinwout(1)winwout(1)wwin⋯wout(1)wnwin⋮⋮⋮⋮wout(r)winwout(r)wwin⋯wout(r)wnwinu(t)u(t−1)⋮u(t−n)=u(t)u(t−1)⋮u(t−r)To find a solution forw, we can use the optimization technique shown in Eq. (11)(11)minw∥RijU(t)−U′(t)∥2whereU(t)=[u(t)u(t−1)⋯u(t−n)]T,U′(t)=[u(t)u(t−1)⋯u(t−r)]TandRij=wout(i)wjwin(0≤i≤r,0≤j≤n).In the optimization problem, ((U(t), U′(t))) is a set of samples for obtaining approximate solutions, and with more samples, we will achieve better solutions. Therefore we construct two matrices U and U′ with m subsequences (u1(t), u2(t), ⋯,um(t), where ui(t)=u(t−i+1), (i=1, 2, ⋯, m)) from the input signals u(t) and adopt the pseudo-inverse that provided a least squares solution to the linear equations. Eq. (10) will consequently be transformed to(12)(Rij)(r+1)×(n+1)·(Uij)(n+1)×m=(Uij′)(r+1)×mwhereU=u1(t)u2(t)⋯um(t)u1(t−1)u2(t−1)⋯um(t−1)⋮⋮⋮⋮u1(t−n)u2(t−n)⋯um(t−n)(n+1)×m,U′=u1(t)u2(t)⋯um(t)u1(t−1)u2(t−1)⋯um(t−1)⋮⋮⋮⋮u1(t−r)u2(t−r)⋯um(t−r)(r+1)×m,Uij=uj+1(t−j) (0≤i≤n, 0≤j≤m−1), andUij′=uj+1(t−i)(0≤i≤r, 0≤j≤m−1).From Eq. (10), we have(13)[wout(0)winwout(0)wwin⋯wout(0)wnwin]u(t)u(t−1)⋮u(t−n)=u(t)Because the input data u(t)(t=0, 1, ⋯, n) are random i.i.d. sequence, there is no functionf:R→Rthat u(t)=f(u(t−1), u(t−2), ⋯, u(t−n)). Thus, we havewout(0)·win=1andwout(0)wiwin=0(i=1,2,⋯,n).Eq. (12) can be further rewritten as(14)Wopt·[winwwin⋯wnwin]·(Uij)(n+1)×m=(Uij′)(r+1)×min whichWopt(Wopt=[wout(0)wout(1)⋯wout(r)]T) is a matrix containing the optimal output weight vectorswout(k)(k=0, 1, ⋯, r). That the optimal output weight vector of network iswout(k)indicates the network can recover the input from k-time steps ago (the details of Wopt's initialization are shown in Algorithm 1). Eq. (14) is rewritten as(15)w[winwwin⋯wn−1win]·(Uij)n×m︸A=Wopt+(Uij′)−winu1(t)u2(t)⋯um(t)︸BwhereWopt+is the pseudo inverse matrix of Wopt. Then Eq. (15) can be rewritten as(16)wA=BIn fact, we can know that A and B are related to state variables x(t) in the reservoir:(17)A=x(t−1)x(t−2)⋯x(t−m)(18)B=x(t)x(t−1)⋯x(t−m+1)−winu1(t)u2(t)⋯xm(t)Finally, we have the solution of Eq. (16):(19)w=BA+where A+ is the pseudo inverse matrix of A.Thus, we build a model for obtaining reservoir topologies with three given parameters, the time step r (which means the network can recover an input signal r time steps ago, also called memory span), the input weightswin, and the output weights Wopt. By assigning a value to r, initializing Woptand constructingwinsubject towout(0)win=1, we can obtain the internal connection matrixw. This model provides a way to study the topological structure of reservoir from the amount of short-term memory capacity. From the above analyses, the algorithm for computingwis described in Algorithm 1.Algorithm 1Computing the internal connection matrixw.Input:The memory span r;Random i.i.d. input signals u(t)(t=0, 1, ⋯, n);Output:The internal connection matrixw;1:Initializewinwith random vectors subject to|win|=1;2:Initializewwith random values subject to the spectral radius ofw, which is less than 1;3:wout(0)=winT;4:k=1;5:whilek≤rdo6:Input u(t), set the target signal with u(t−k), train ESN and record the optimal output weight vectorwout(k);7:end while8:Wopt=[wout(0)wout(1)⋯wout(r)]T;9:Construct U and U′ by u(t), (t=0, 1, ⋯, n);10:Update state variables x(t) in the reservoir withWopt+·U′;11:Compute A and B according to Eqs. (17) and (18);12:returnwaccording to Eq. (19).The distributed shift register (DSR) network and the delay line reservoir (DLR) are the special reservoir topologies proposed by [11] and [17], respectively, to discuss the short-term memory and performance of networks. We find that DSR and DLR are the special solutions of our model. We state our results with the following Theorem 1.Theorem 1Let N be the number of neurons in a reservoir. Given the memory span r=N (that is MC=N), the reservoir's solutions attained in Algorithm 1 are equivalent to the distributed shift register (DSR) network and the delay line reservoir (DLR).Before we providing the proof of Theorem 1, we briefly introduce the result from [11] (for details refer to [11]). The DSR network is proposed by [11] withw=αΣk=1N−1v(k+1)v(k)Tandv(1)≡v, wherewis the matrix of recurrent connections, α is a parameter, which is 0<α≤1. In fact, α is exactly equal to the norm squared ofw's largest eigenvalue.vis a unit norm constant vector of connections from the input, andv(k)is an arbitrary set of N orthonormal vectors. Note thatαv(k+1)=wv(k). When α=1, the MC of DSR achieves its maximum, MC=N. Then we have(20)v(k+1)=wv(k)Proof of Theorem 1: If r=N andwout(i)⊥wout(j), i≠j, from Eq. (14), we have(21)wout(0)winwout(0)wwin⋯wout(0)wnwinwout(1)winwout(1)wwin⋯wout(1)wnwin⋮⋮⋮⋮wout(n)winwout(n)wwin⋯wout(n)wnwin=Iwherewout(i)(i=1, 2, ⋯,n) andwinare unit vectors. Then we havewout(0)=win,wout(1)=wwin, ⋯,wout(n)=wnwin.Next, letv(k)=w(k−1)win. Therefore, we have(22)v(i)⊥v(j),i≠j(23)v(i+1)=wv(i)Eq. (23) shows the same reslut as Eq. (20). Therefore, our solutionwis the same aswin the DSR of [11]. Under special circumstances, if we letv(1)=δ1,1, andwij=δi,j+1, where δi,jis the Kronecker delta function, our solution will be transformed to DLR in [17]. Thus, we finish our proof.In this section, we set up several experiments that are divided into three different parts. The first is r≤N, that is, the ESNs will completely “remember” all the signals less than or equal to N time steps ago. The second is r>N. In this case, we see whether the maximum MC of the ESNs withwobtained from our model can be larger than the reservoir size N. The last is to demonstrate that our model can adjust the reservoir to the edge of chaos by tuning the memory capacities. The reservoir of ESN has 100 neurons (N=100) in each experiment, the sparseness ofwis 2%, the spectral radius ofwis 0.93, and the input data u(t)(t=0, 1, ⋯, n) are random i.i.d. sequences containing random values drawn from the standard uniform distribution on the open interval (0, 1).If the network can perfectly reconstruct an input signal r time steps ago with N neurons, when r is less than or equal to N, Woptmust be an orthogonal matrix. Here, we further transform Woptto a pairwise orthogonal matrix. Input weightswinsatisfywout(0)win=1. Three experiments are conducted independently on different r (e.g., r=20, 60, 100). The memory function M(k) curves and eigenvalues distributions ofware shown in Fig. 2.In this situation, the values of M(k) are nearly equal to one when the time step k≤r. Therefore, the memory capacity of the ESN can be arbitrarily close to the proved optimal MC value, and ESN will remember all the signals less than or equal to r time steps ago. The right figure in Fig. 2 indicates that the eigenvalues ofwshow ring form distribution features, and the ring radius increases with the increase in r.When r is larger than N, we want the ESN to remember sequences lasting longer than N time steps with only N neurons. Although this has been achieved by [15] when the input signals are sparse, it seems impossible for non-sparse inputs because the ESN's MC cannot exceed the upper bound proved by Jaeger [2] of MC≤N. However, our question is whether there is any possibility for N<MC<r in our model.Eq. (14) can be rewritten as Wopt·X=U′, where X is the matrix of the reservoir units states. We can regard X as a set of point in N-dimensional space and U′ as a set of points in r-dimensional space. In this situation, we cannot have a set of r+1 orthonormal vectors in the N-dimensional space because of r>N. Thus, Woptcannot be constructed directly. But if Eq. (14) is further rewritten asWopt+·U′=X, we can consider this problem from another perspective, that is, to reduce r-dimensional set points to N-dimensional set points. Thus, we apply the PCA method [21] to obtainWopt+, and then we have Wopt. Seven experiments are conducted on different r (e.g., r=100, 120, 150, 170, 200, 250, 300), and the reservoir of ESN has 100 neurons (N=100). Each experiment is performed independently 20 times. The memory function curves and eigenvalues distributions ofware shown in Fig. 3(r=100, 120, 150).The left figure in Fig. 3 shows that the memory function curves decline slowly as k increases. The right figure indicates that the eigenvalues ofwalso show ringlike distribution, and the ring radius is close to 1 when r is larger than 100. The MCs of the ESNs in different r (from 120 to 300) are listed in Table 1.Here, σ is the standard deviation, and η is the percentage of variation corresponding to the first N principal components’. The definition of η is(24)η=∑i=1Nλi∑i=1rλi×100%where λi(i=1, 2, ⋯, r) are the eigenvalues ofU′U′T, and λ1≥λ2≥⋯≥λr. We can see from Table 1 that when r is larger than N(N=100), the values of MC exceed N. Furthermore, we can find an empirical equation(25)MC=η·rThis suggests, even with i.i.d. inputs, that the MC of ESN can exceed the upper bound proved by [5].11If the MC is defined only on test data, then MC≤N. But we have not find this constraint in [5].In fact, for one-dimensional finite inputs, they tend to be highly sparse when they are reconstructed in a high-dimensional space. For example, if the i.i.d. input signals with a size of 10000 are reconstructed in a 100-dimensional space and the signal only has a value of either 0 or 1, then the sampling ratio is 1000/2100=7.8×10−27. The data are sparse in the 100-dimensional space. Therefore, the distribution of the data, which are reconstructed from the one-dimensional finite inputs, is not an ideal hypercube in r-dimensional space. Under the circumstances, η≥N/r, when the r-dimensional space has been reduced to N-dimensional space with the method of PCA. Then we have(26)MC=η·r≥Nr·r=NIf and only if the i.i.d. input signals are infinite, the maximum MC of the ESN with linear output units in [5] is bounded by N. The reason is that the distribution of the data reconstructed from the infinite inputs is an ideal hypercube in r-dimensional space. Thus η=N/r, and then we have MC=(N/r)·r=N.The edge of chaos is a boundary region between the ordered and chaotic dynamics. When the system dynamics are ordered, the disturbances will quickly die out, whereas in chaotic dynamics, the disturbances will be amplified. Therefore, the edge of chaos is also a transition from ordered to chaotic. It has been shown that the RC will achieve the maximum computational capability when the reservoir operates near the edge of chaos [7–10]. Furthermore, previous literatures [22–24] have shown that ESNs will have different dynamics with various reservoir topologies. However, there is no method for how to adjust the reservoir toward the edge of chaos according to the STM of ESNs. As mentioned in Section 3, our model can obtain the reservoir topologies from STM in ESNs.To determine whether a network has ordered or chaotic dynamics, it is common to employ the Lyapunov exponent (LE) as a measure [6,9,10] of the sensitivity of the differences in the network's initial conditions. The sensitivity is the exponential divergence of two trajectories of a network in a state space with very small initial separation. Referring to [9], we use LE to evaluate the edge of chaos in our model. It is defined as(27)λ=limk→∞1klnγkγ0where γ0 is the initial distance between the perturbed and the unperturbed trajectory, and γkis the distance at time k. With a network in the ordered regime, we will have λ<0, whereas λ>0 holds for chaotic regime. Therefore, when λ≈0, the network is regarded as on the edge of chaos. We provide the computation process of LE as follows:(1) Given two identical ESNs with reservoirs obtained from Algorithm 1, denote them Xaand Xb. Introduce a small perturbation into Xabut not Xb. Then γ0 is the distance of the initial state of Xabetween that of Xb, γ0=∥Xa(0)−Xb(0)∥.(2) Record each step's state distance between Xaand Xb. For example, the distance of states in the two ESNs is γk=∥Xa(k)−Xb(k)∥ in the kth step.(3) Normalize Xa(k) withXa(k)=Xb(k)+γ0γk(Xa(k)−Xb(k))to keep the perturbed and unperturbed trajectories close in order to avoid numerical overflows [9].(4) Let k←k+1. If k<n, repeat step 2 to step 3, elseλ=1n∑k=1nlnγkγ0.In this experiment, the reservoir of ESN has 100 neurons (N=100), and the number of simulation steps n is 1000. The results are shown in the plot of the memory capacity versus memory span r and the plot of the Lyapunov exponent versus memory span r in Fig. 4.The two curves in Fig. 4 show similar behavior with the increasing memory span r. When the memory span r increases from 20 to 200, the MC will see a monotonic increase in the top plot of Fig. 4. That is consistent with the results in Sections 4.1 and 4.2. At the same time, the Lyapunov exponent λ increases with the memory span r in the bottom plot of Fig. 4. By comparing the two plots in Fig. 4, we notice that when the MC is less than 100, λ is less than 0, and the ESN is in the ordered regime. The more MC larger than 100, the closer the Lyapunov exponent λ will be to 0. Thus, the state of ESN is transforming from ordered to chaotic, in which case the ESN is regarded as on the edge of chaos. That is to say, for our model, given values of the memory span r larger than the number of neurons in the reservoir, we will have various reservoirs operating near the edge of chaos. Therefore, the proposed model can provide a convenient way to adjust the reservoir to the critical state.

@&#CONCLUSIONS@&#
