@&#MAIN-TITLE@&#
No-reference image quality assessment using interval type 2 fuzzy sets

@&#HIGHLIGHTS@&#
Human visual perception in assessing image quality using type 2 fuzzy sets.Entropy of visually salient regions measure uncertainty in feature space.Transformation of features to interval type 2 fuzzy feature space.Free from type reduction and defuzzification computation.Promising results compared to prominent subjective and objective image quality metrics.

@&#KEYPHRASES@&#
Visually salient regions,Mean opinion score,No-reference image quality,Interval type 2 fuzzy sets,

@&#ABSTRACT@&#
Image quality assessment of distorted or decompressed images without any reference to the original image is challenging from computational point of view. Quality of an image is best judged by human observers without any reference image, and evaluated using subjective measures. The paper aims at designing a generic no-reference image quality assessment (NR-IQA) method by incorporating human visual perception in assigning quality class labels to the images. Using fuzzy logic approach, we consider information theoretic entropies of visually salient regions of images as features and assess quality of the images using linguistic values. The features are transformed into fuzzy feature space by designing an algorithm based on interval type-2 (IT2) fuzzy sets. The algorithm measures uncertainty present in the input–output feature space to predict image quality accurately as close to human observations. We have taken a set of training images belonging to five different pre-assigned quality class labels for calculating foot print of uncertainty (FOU) corresponding to each class. To assess the quality class label of the test images, maximum of T-conorm applied on the lower and upper membership functions of the test images belonging to different classes is calculated. Our proposed image quality metric is compared with other no-reference quality metrics demonstrating more accurate results and compatible with subjective mean opinion score metric.

@&#INTRODUCTION@&#
Digital images are subjected to loss of information, various kinds of distortions at the time of compression [12] and transmission, which deteriorate visual quality of the images at the receiving end. Quality of an image plays fundamental role to take vital decision and therefore, its assessment is essential prior to application. Despite rapid advancement in technology, the characteristics of human vision are still considered best performer for quality assessment processes. Modeling physiological and psycho visual features of the human visual system (HVS) [16–18] are reported for developing image quality assessment (IQA) methods [23]. Due to limited knowledge of HVS, computational HVS modeling used in IQA is far from complete. The most reliable means of assessing image quality is subjective evaluation based on the opinion of the human observers [9,13]. However, subjective testing is not automatic, lengthy process and expensive too.Most objective image quality assessment methods [12,14,64–68] either require access to the original image as reference [8,14,67,68] or only can evaluate images, degraded with predefined distortions [64–66,76] and therefore, lacking generalization approach. Reduced reference IQA methods [13,64–66] provide a solution by extracting a minimal set of parameters from the reference image for cases where the reference image is not fully accessible. However, determination of the effective parameter set is an important issue. No-reference IQA algorithms [60–77] attempt to perform quality evaluation for specific type of distorted images [60–62] and designed specifically for JPEG or JPEG2000 compression artifacts [15]. Two prominent works by Wang et al. [10] and Sheikh [9] have been reported relating to no-reference image quality assessment. JPEG image quality index and quality metric based on natural scene statistics (NSS) model are the respective metrics proposed [9] for evaluation of image quality. Extreme learning machine classifier based mean opinion score (MOS) estimator [26], discrete cosine transform (DCT) domain statistics based metric [27] and blind image quality index [24,25] are the three no-reference methods are reported very recently to assess image quality. But none of them incorporated human centric computation methods that can exploit powerful judgment ability of human observers like subjective methods, best suited for assessing quality of images.In computer vision global and/or local features are extracted for image analysis and importance of the features depends on the area of application [37]. In subjective image quality evaluation methods, local features are more relevant [57,59,63] since human perceive the image based on salient points rather considering the image as a whole for predicting its quality. Salient features of images create varied degree of impression on human observers in assessing quality of the images without any reference image. Therefore, designing of a no reference image quality assessment method depends on human interpretation about the interrelation between the input features and output quality of the images and modeled as fuzzy rules using linguistic variables.Fuzzy modeling is used to transform knowledge of human experts into mathematical models [1–6]. It can be used as a tool to assist human perception about a given task by transforming human observations into mathematical understanding [30–33,77]. The proposed no-reference image quality assessment (NR-IQA) method using fuzzy techniques is described using Fig. 1, where subjective evaluation strategy of human beings are modeled with the help of salient features of the images. Type-1 fuzzy logic based inference system and fuzzy relational classifier [36] have already been built [40] considering human perceptual vagueness [38] in assessing quality of images using linguistic values. However, subjective judgment by human observers about quality of image is associated with uncertainty in input (feature) and output (quality class label) space for which an exact membership function is difficult to design.In type-1 fuzzy systems, the degree of membership of an image belonging to a particular class representing quality of an image is crisp. However, human perception has wide variation both on image features and image quality, therefore it is difficult to interpret and measure the linguistic variables by type-1 fuzzy set [58]. So to evaluate uncertainty associated with human perception in assessing image quality, type-2 fuzzy based systems has been proposed in the paper to depict the scenario. The membership functions of input–output variables of type-2 fuzzy systems are considered as type-1 fuzzy sets instead of crisp value.In the paper, we have handled uncertainty in human visual perceptual inference generating process by determining image quality using interval type-2 fuzzy sets. Interval type-2 fuzzy set is used within a bounded range for representing degree of membership values, called foot print of uncertainty. Input features are measured by evaluating the entropy of visually salient regions as proposed by Kadir–Brady algorithm [34,39]. In interval type-2 fuzzy set, membership grade of each feature has been represented as an interval of lower and upper bound instead of a crisp value to transform it into fuzzy feature space. We have taken a set of training images belong to five different classes and foot print of uncertainty (FOU) of each class is calculated by designing a fuzzy based NR-IQA method. Quality of a new image is assessed using fuzzy operator applied on the lower and upper membership functions of the test images obtained from the FOUs of the training images.The paper has been organized as: Section 2 describes feature selection procedure using Kadir–Brady algorithm, Section 3 presents transformation of features into type-2 fuzzy feature space, Section 4 proposes a NR-IQA method using interval type-2 fuzzy set, Section 5 shows results when applied to test images and finally conclusions are summarized in Section 6.Entropy values of visually salient regions are considered as features to evaluate subjective quality of the images into different classes by the proposed NR-IQA method. Kadir and Brady algorithm [34,39] invariant to affine transformation has been invoked in the paper to extract visually salient regions and Shannon's entropy [4] values of the salient regions are calculated to measure uncertainty in the local feature space. The entropies are ranked according to their saliency.Performance of subjective image quality assessment method depends on how quickly regions of interest in the image are identified by the method. Regions of interest are known as visually salient regions and it has been observed that in high quality images a large number of such regions exist. In the literature [43–46] different methods are reported to find visually salient regions out of an image. The earliest methods [48–56] of extracting edges from an image to build object contour is based upon the assumption that edges are more salient than other parts of an image. Gilles [21] reported that visually salient regions are rare in natural images and depends on the descriptor type used to define the regions. Highly differentiable descriptors treat every pixel rare whereas using non-differentiating descriptors more salient regions are identified. Schiele [20] demonstrated that visually salient regions posses a property which maximizes discrimination between the objects in an image. Fig. 2[47] shows salient regions marked by red color, which are rare in the image.Generally, visual saliency refers to the concept that certain parts of the scene are pre-attentively distinctive and create some form of significant visual arousal within the early stages of the Human Visual System [21]. Information theoretic entropies of visually salient regions are called regional entropy values, used in the paper as local features to develop no-reference image quality assessment (NR-IQA) method. Uncertainty in pixel intensity of local features is evaluated using Shannon's entropy [22] that determines stability of the system. Uniform image with peaked histogram indicate low complexity or high predictability while neighborhoods with flat intensity distributions demonstrate higher complexity of the image, as shown in Fig. 3[11]. On the other hand, local histograms with flat distributions represent higher entropy value and so the system becomes unstable and vice versa.To select visually salient regions, a range of scales in the image are chosen and Shannon entropy value with respect to the local image descriptor i.e. intensity of the pixels within the range are calculated. Scales at which entropies exhibit a peak value are selected as visually salient regions according to the Kadir and Brady algorithm. In the affine invariant to geometric transformation version of the algorithm, the scale shape is taken as elliptical while the scale parameters are scale size and axis ratio along with angle of orientation of the ellipse.Kadir–Brady AlgorithmInput: Training ImagesOutput: A matrix of dimension M×N// M is no. of parameters of a visually salient region and N represents number of visually salient regions.BeginFor each pixel location at positionx→BeginFor each scale size S in the range [Smax−Smin] // typical Smax value is 10 and Smin value is 3.(i) Measure the local descriptor D (gray level pixel intensity) within a window of scale size S.(ii) Estimate the local probability distribution functions (PDF) from the histograms considering the windows contained between Smin and Smax.(iii) Calculate local entropy using Eq. (1).(1)HD(S,x→)≜∫i∈DpD(S,x→)log2pD(S,x→).di//pD(S,x→)is PDF of scale S and positionx→while diindicates ith descriptor value in D.(iv) Select scale S at which the entropy value is highest, say,S→(S→may be empty):S→≜S:∂2HD(S,x→)∂x2<0(v) Weight (WD) the entropy value at scaleS→,as given by Eq. (2).// WDis the change of magnitude of PDF as a function of scale at each peak, known as inter-scale saliency.(2)WD(S,x→)≜S⋅∫i∈D∂∂x(pD(S,x→)).di(vi) Compute visual saliency measure as given in Eq. (3).(3)yD(S→,x→)≜HD(S→,x→).WD(S→,x→)// where yDis the saliency measure depending on scaleS→and positionx→,known as scale saliency.End;(vii) A space in ℜ3 (along x and y spatial dimension and scale) is generated.// ℜ3 is sparsely populated with scalar saliency values.End.Fig. 4shows salient points determined by Kadir and Brady algorithm and entropy of salient points are used as features to determine quality class label of the image.Recently, type-2 (T2) fuzzy sets and system theory is gaining popularity for managing uncertainty present in the input–output feature space, difficult to handle by type-1 fuzzy sets. The membership grades of a T2 fuzzy set are type-1 (T1) fuzzy sets in [0,1] instead of crisp numbers. Since the boundaries of T2 fuzzy sets are blurred, it is difficult determining of an exact membership grade [58]. There are two schools of thoughts of determining degree of membership value of type-2 fuzzy sets-generalized and interval type-2. Interval type-2 (IT2) fuzzy sets are used successfully in the paper to measure quality of distorted images when no prior information is available about the original images.It has been observed that there are at least four sources of uncertainties remain in type-1 fuzzy logic systems: (1) the words used in the antecedents and consequents of the rules may be uncertain because meaning of words are different to different people, (2) consequents contain uncertainty, especially when knowledge is extracted from a group of experts, (3) measurements that activate a type-1 FLS maybe noisy and therefore uncertain and (4) data used to tune the parameters of a type-1 fuzzy logic system often noisy. Type-1 fuzzy sets are not able to directly model such uncertainties using membership functions because their membership values are crisp. In type-2 fuzzy sets membership functions are themselves fuzzy [16] and so able to handle such kind of uncertainties.A type-2 fuzzy setA⌣is characterized by 3-D membership function μÃ(x, u) where x∈X, u∈Jx, Jx⊆[0,1] andA⌣=∫x∈X∫u∈JxμA⌣(x,u)(x,u),Jx⊆[0,1].An interval type-2 fuzzy set (IT2FS) Ã is described asA⌣=∫x∈X∫u∈Jx1/(x,u)where x is the primary variable, Jxis in the interval [0,1] representing the primary membership value of x, u is the secondary variable and∫x∈Jx1/(x,u)is the secondary membership function (MF) at x. The secondary membership function has constant value for IT2FS, here it is one [13]. Uncertainty about Ã is conveyed by the union of all primary memberships, called footprint of uncertainty of Ã [(FOU(Ã, u)], i.e.FOU(A⌣)=Ux∈XJx.Considering trapezoidal primary membership function of type-1 fuzzy set, corresponding IT2 fuzzy set is obtained, shown in Fig. 5[19] where the shaded region represents footprint of uncertainty (FOU). FOU is bounded by an upper membership function (UMF)μ¯A¯(x)and a lower membership function (LMF)μ_A¯(x),both are type-1 fuzzy sets. The membership grade of each element of an IT2 fuzzy set is represented by the interval[μ¯A¯(x),μ_A¯(x)].For evaluating image quality, interval type-2 fuzzy feature space is built using Kernel density estimation, a non-parametric way to estimate probability density function of a random variable. Kernel density estimation is a fundamental data smoothing problem where inferences about the population are made based on a finite data sample. In the proposed method entropies of visually salient regions of training images are utilized to build interval type 2 fuzzy feature space. The procedure is described by the following algorithms. Total numbers of samples of such entropies are determined by type of kernel used in kernel density estimation process (Fig. 6).Input: Training Images of a particular quality class // N is no. of images with different levels of distortions in a particular class.// each class represents quality of images like excellent, good etc.Output: UMF and LMF of features (entropy of visually salient regions) for different quality classes.LetIcjdenotes the input image, where c represents a particular quality class and j stands for image index out of N images.Step1: Extract local entropiesEvjof visually salient regionsRvjof the input images.//υ is the variable indicating visually salient region.Step2: Estimate distribution of entropies using “Normal Kernel Distribution”, given in Eq. (4).(4)ϕ(Evi)=12πe(1/2)Evj2Step3: Compute probability densities of the entropy values using Eq. (5).(5)f(Evj)=1σϕEvj−Evj¯σ//Evj¯and σ are mean and standard deviation respectively corresponding to the distribution of entropy values.Step4: Evaluate data distributions at each regional entropy value with respect to their maximum (Procedure Compute_MAX) and minimum (Procedure Compute_MIN) probability density.End.Procedure Compute_MAX and Compute_MIN describe calculation of maximum (MAX) and minimum (MIN) probability densities of the entropy values considering different training images of a particular quality class.Procedure Compute_MAXInput:Evjand correspondingf(Evj)// where j is the image index out of N training images belong to the same image quality class and v denotes visually salient regions.Output: Upper probability distribution UPD [.]Step1:RANGE_MAX=MAXv(MAXj(Evj))Step2:RANGE_MIN=MINv(MINj(Evj))Step3: i=RANGE_MINStep4: WHILE i<RANGE_MAXBeginFor each regional entropy valueEvjUPD[.]=MAXj(f(Evij))End;// UPD [.] is the set of maximum probability density of entropy values belonging to a particular quality class.Step5: i=i+1Step6: Ifi=RANGE_MAX Then ENDElse GOTO step4.ProcedureCompute_MINInput:Evjand correspondingf(Evj)// where j is the image index out of N images in the same image quality classOutput: Lower probability distributions LPD [.]Step1:RANGE_MAX=MAXv(MAXj(Evj))Step2:RANGE_MIN=MAXv(MINv(Evj))Step3: i=RANGE_MINStep4: WHILE i<RANGE_MAXBeginFor each regional entropy valueEvjLPD[.]=MINj(f(Evij))End;// LPD [.] is the set of minimum probability density entropy values belonging to a particular quality class.Step5: i=i+1Step6: Ifi=RANGE_MAXThenENDElse Go to Step4.The set of maximum probability density is called Upper Probability Density (UPD) distribution of the entropy values. Similarly, the set of minimum probability density is called Lower Probability Density (LPD) distribution of the entropy values. As a next step UPD and LPD are used to calculate upper and lower IT2 Primary membership functions respectively, as described below.ProcedureIT2primary _UMFInput: UPD [.] of training images belonging to a particular quality class.Output: UMF of that quality class.Step1:RANGE_MAX=MAXv(MAXj(Evj))Step2:RANGE_MIN=MINv(MINj(Evj))Step3: i=RANGE_MINStep4. μ(UMF)max=1Step5: WHILE i<RANGE_MAXStep6: For eachEvijBeginμ(UMF)=prob(UPD)/maxprob(UPD)End;Step7: i=i+1Step8: if i=RANGE_MAXEnd.Else Go to Step4.ProcedureIT2primary_LMFInput: LPD [.] of the training images for a particular quality class.Output: LMF of that quality classStep1:RANGE_MAX=MINv(MAXj(Evj))Step2:RANGE_MIN=MAXv(MINj(Evj))Step3: i=RANGE_MINStep4. μ(LMF)max=maxprob(LPD)/maxprob(UPD)Step5: WHILE i<RANGE_MAX,Step6: For eachEvijBeginμ(LMF)=prob(LPD)/maxprob(UPD)EndStep7: i=i+1Step8: ifi=RANGE_MAXEnd.Else Go to Step4.For both the algorithms maxprob(.) is the maximum probability density value and prob(.) represents probability density value of a particular entropy value. Therefore, the resulting data distributions provide the upper membership function (UMF) and lower membership function (LMF) for the corresponding image quality class.Example:Let there are three training images as shown in Fig. 7with quality class Good, i.e.Ic1jwhere j=1,2,3 and c1=Good.Let m1, m2, m3 are the maximum PDF values with respect to corresponding entropiesEv1,Ev2,Ev3and n1, n2, n3 are the minimum values of the same set of entropy values. Assume, M=maximum(m1, m2, m3), N=minimum(m1, m2, m3), H=maximum(n1, n2, n3), J=minimum(n1, n2, n3).Fig. 8(a) represents “normal” kernel probability density distributions of regional entropy values of the training images (Fig. 7). Upper (UPDc1) and lower (LPDc1) probability density functions of the images of class c1 are calculated using procedure compute_MAX and compute_MIN respectively and shown in Fig. 8(b). UPDc1 and LPDc1 are used to obtain degree of upper membership function UMFc1 and lower membership function LMFc1 of class c1 by applying procedure IT2primary_UMF and IT2primary_LMF respectively. Fig. 8(c) denotes the UMFc1 and LMFC1 that constitute the IT2 fuzzy feature space for quality class Good while Fig. 8(d) shows FOU of IT2 fuzzy set Good.To build the IT2 fuzzy system for assessing image quality, we have trained images for each five different quality classes (e.g. Excellent, Good, Average, Bad, Poor) from TAMPERE [28] and PROFILE (with added effect) Image Database [29], shown in Fig. 9.Using thirty training images, six images for each class (Fig. 9), respective IT2 fuzzy feature spaces are built as shown in Fig. 10.The proposed system is validated using different types of test images taking from TAMPERE and PROFILE (with added effect) image database (Fig. 11).Test image is converted to portable gray map format and visually salient regions of the image are obtained using Kadir and Brady algorithm. Shanon's theory [41] is applied to measure local entropy of the regions and mean of the entropies (Mean_local_entropy) is calculated using Eq. (6). Mean_local_entropy represents feature of the test image based on which a particular quality class is to be predicted.(6)H(Mean_local_entropy))=∑i=1NEvijNwhereEvijrepresents local entropy values of jth test image obtained using Kadir and Brady algorithm. N is the number of visually salient regions of the image.As a next step, the UMF and LMF of the test image corresponding to each different quality classes are calculated. The procedure to determine the UMF and LMF of a test image has been illustrated in Fig. 12.For a test image, maximum T-conorm [7] of the set containing UMF grades of five different classes (MaxTConorm(UMF)) and maximum T-conorm of the set containing LMF grades of same five different classes (MaxTConorm(LMF)) are obtained. Mean_conorm is calculated using Eq. (7), defined as average of maximum T-conorm of UMF and LMF grades for the test image. It has been observed that human eye can record increased value of contrast at the boundary having difference in intensity values than actual (Mach Band effect) [42]. To map the phenomenon, average of maximum T-conorms operator is chosen in the work to evaluate the effect of human vision in predicting quality of an image. Using Eq. (8), the Mean_conorm value is matched with the UMF grades of five different classes, obtained during the training phase.(7)Mean_conorm=MaxTConorm(UMF)+MAXTConorm(LMF)2(8)∀i(Absolute_difference)i=||UMFi−Mean_conorm||where i indicates five different image quality classes (Excellent, Good, Average, Bad, Poor).The lowest absolute_difference corresponding to a particular class is assigned as quality class label to the test image. It has been assumed that the degree of UMF is predominant in deciding the image quality class compare to LMF grades.

@&#CONCLUSIONS@&#
Type-2 fuzzy system has been designed in the paper to remove the limitation of type1 fuzzy systems for assessing quality of images. Shannon entropies of visually salient regions are used as features to measure uncertainty in the image features and modeled by Interval type-2 fuzzy sets. Human perception on visual quality of images is assigned by five different class labels. The variation of features is wide enough for capturing important information from the images. The proposed NR-IQA metric has been compared with the existing quality metrics producing satisfactory result. It has been observed long that human eye generally considers contrast difference at a higher range than actual at the boundary of intensity difference in an image (Mach band effect). So to predict the quality of an image we have considered UMF of output features while calculating the absolute_difference as given in Eq. (8). Moreover, for BIQI and JpegQualityScore metrics, numeric output is obtained as quality for the test images needs further intervention of human beings for subjective judgment, which is directly achieved by our proposed method.