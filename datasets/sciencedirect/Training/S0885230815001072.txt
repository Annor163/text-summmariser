@&#MAIN-TITLE@&#
Ensemble of deep neural networks using acoustic environment classification for statistical model-based voice activity detection

@&#HIGHLIGHTS@&#
We develop the voice activity detection based on statistical model.The DNNs are used for the voice activity detection.Ensemble of the DNN is devised for different noise environments.A separate DNN is built to detect the current environment.

@&#KEYPHRASES@&#
Voice activity detection,Statistical model,Acoustic environment classification,Deep neural network,Ensemble,

@&#ABSTRACT@&#
In this paper, we investigate the ensemble of deep neural networks (DNNs) by using an acoustic environment classification (AEC) technique for the statistical model-based voice activity detection (VAD). From an investigation of the statistical model-based VAD, it is known that the traditional decision rule is based on the geometric mean of the likelihood ratio or the support vector machine (SVM), which is a shallow model with zero or one hidden layer. Since the shallow models cannot take an advantage of the diversity of the space distribution of features, in the training step, we basically build the multiple DNNs according the different noise types by employing the parameters of the statistical model-based VAD algorithm. In addition, the separate DNN is designed for the AEC algorithm in order to choose the best DNN for each noise. In the on-line noise-aware VAD step, the AEC is first performed on a frame-by-frame basis using the separate DNN so the a posteriori probabilities to identify noise are obtained. Once the probabilities are achieved for each noise, the environmental knowledge is contributed to allow us to combine the speech presence probabilities which are derived from the ensemble of the DNNs trained for the individual noise. Our approach for VAD was evaluated in terms of objective measures and showed significant improvement compared to the conventional algorithm.

@&#INTRODUCTION@&#
Voice activity detection (VAD) which classifies the period of speech and non-speech from an input speech signal is an essential part of speech signal processing in tasks such as speech recognition, speech enhancement, and efficient variable-rate speech coding. Among the various VAD methods, we focus on a statistical model-based approach, which was originated from the work of Ephraim and Malah Ephraim and Malah (1984) for speech enhancement due to its high detection accuracy as well as low computational complexity. Sohn et al.Sohn et al. (1999) devised the VAD based on a Gaussian statistical model by employing the decision rule based on the geometric mean of the likelihood ratio (LR), which can be considered as a heuristic way. The novelty of the statistical model-based VAD was extensively reviewed, so that further improved methods based on the LR test according to two hypotheses for speech presence and absence have been presented in many studies. Kang et al. (2008) proposed a decision rule based on a discriminative weight training method which fuses the LR through a linear weighed combination in which weights are optimized by the minimum classification error (MCE) training method based on the gradient descent algorithm. Yu and Hansen (2010) further improved the method of Kang et al. (2008) by applying a multiple observation technique to the decision rule which reflects the LR of not only the current frame, but also previous few frames. On the other hand, Jo et al. (2009) found that the LR corresponding to speech absence and presence cannot be separated by a linear decision function such as the geometric mean and a linear weighted combination due to its considerable class overlap in the feature space. They thus applied a support vector machine (SVM) to the statistical model-based VAD as a robust decision function since the SVM makes it possible to build an optimal hyper-plane among many possible hyper-planes separating the two classes of speech absence and presence and especially has the advantages on addressing nonlinear properties of the input feature vector by applying the kernel function. However, the SVM cannot be considered as an effective method especially for the statistical model-based VAD since it cannot fully take the advantage of multiple features due to its shallow properties. Shallow architecture lacks the ability to take into account the diversity of the nonlinear distribution of the feature vector since it has zero or at most one hidden layer.Recently, the deep belief network (DBN) has been proposed, by Hinton and Salakhutdinov (2006), as a powerful hierarchical generative model not only for feature representation but also for classification by taking multiple-layered deep architecture. It is noted that the DBN is known to avoid the poor local-minima and over-fitting by the greedy layer-wise unsupervised learning process called pre-training. The superiority of the DBN compared to conventional shallow architecture-based machine learning techniques including the SVM has been reviewed, and thus the DBN has been successfully applied to various pattern recognition applications such as speech recognition (Mohamed et al., 2009, 2012; Hinton et al., 2012) and hand-written character recognition (Hinton and Salakhutdinov, 2006; Lee et al., 2007) as a state-of-the-art technique. The key idea behind the method of Zhang and Wu (2013) is to extract new features by transferring the acoustic features through deep nonlinear hidden layers since the deep model can combine multiple features in a nonlinear way to discover the regularity among the features. Thereafter, they proposed the denoising deep neural network (DNN)-based VAD approach (Zhang and Wu, 2013) which differs from Zhang and Wu (2013) in that it tries to minimize the reconstruction cross-entropy loss between the input noisy feature and its corresponding clean feature at the target while (Zhang and Wu, 2013) uses the noisy feature at the target in the pre-training step. In addition, Ryant et al. (2013) proposed the DNN-based VAD for web video such as YouTube by using 13 normalized MFCCs as feature vector. However, the DNN-based VAD is far from fully investigated yet in the area of the statistical model-based VAD under various noise environments, which is a main topic of interest in this study.Before presenting our work, it is worthwhile to mention the performance of the VAD by incorporating the acoustic environment classification (AEC) technique since it is useful to build and use a different DNN scheme for various acoustic environments. In the literature, Sangwan et al. (2007) proposed a technique to use the SVM for environmentally aware VAD and to find the best operating point for the competitive Neyman–Pearson VAD. Gaussian mixture model (GMM), in the method of Choi and Chang (2012), was applied to perform the AEC for speech enhancement to adaptively select the optimal parameters for a given noise type. Recently, Xia and Bao (2014) applied the GMM-based noise classification to speech enhancement, which is different from the work of Choi and Chang (2012) in that the weighted denoising auto-encoder (WDA) model is chosen among a number of models which were trained for each kind of noise in the training data-set. Unfortunately, these methods are restricted in tracking the subtle changes in acoustics, which cause nonlinear change in the feature space since the acoustic features are extracted through the fixed filters such as a Mel-filter bank or linear prediction filter. In addition, both the SVM and GMM belong to the shallow method, and they thus cannot represent the diversity of the feature yet in this AEC task. We note that the DNN can be successfully used in representing raw speech data to encapsulate the underlying information associated with various acoustic scenes.In this paper, we develop the statistical model-based VAD by employing the DNN with a multiple layer deep architecture as a novel decision rule in classifying the signal into speech or noise. The first step is to establish the baseline of the DNN by which the improved speech presence probability (SPP) is obtained based on the novel features of the statistical model-based VAD, namely the LR, the a priori signal-to-noise ratio (SNR), and the a posteriori SNR. As the key point that contributes to the success of DNN-based VAD, distinct DNNs according to different noise types are established via the separate training. Then different SPPs which correspond to a number of total noises are derived. As for the environmental awareness, an independent DNN module is constructed by a separate training process to offer the probabilities of occurrence for each noise type. The probability of occurrence for each noise environment is calculated to combine the SPPs, derived from the multiple DNNs and thus the final decision for VAD is obtained by comparing the combined SPP with a given threshold. The proposed VAD was evaluated in terms of an objective measure and found to have better results than the conventional SVM-based algorithm.We briefly review the statistical model-based VAD. It is assumed that a noise signal d(t) is added to a speech signal x(t) in a time domain, with their sum being denoted as the noisy speech signal y(t), that is(1)y(t)=x(t)+d(t).After taking a short-time Fourier transform (STFT) of the noisy speech y(t), we then have the following in the time-frequency domain as(2)Y(k,n)=X(k,n)+D(k,n)where Y(k, n), X(k, n) and D(k, n) denote the STFT coefficients of the noisy speech signal, clean speech signal and additive noise signal, respectively, where k and n denote the frequency-band index (k=0, 1, …, L−1) and frame index (n=0, 1, …), respectively. Given two hypotheses, H0 and H1, which respectively indicate speech absence and presence, it is assumed that(3)H0:speechabsence:Y(k,n)=D(k,n)(4)H1:speechpresence:Y(k,n)=X(k,n)+D(k,n).With the complex Gaussian probability distribution assumption, the probability density functions (PDFs) conditioned on the two hypotheses of H0 and H1 are given by(5)p(Y(k,n)|H0)=1πλd(k,n)·exp−|Y(k,n)|2λd(k,n)(6)p(Y(k,n)|H1)=1π(λx(k,n)+λd(k,n))·exp−|Y(k,n)|2λx(k,n)+λd(k,n)where λx(k, n) and λd(k, n) denote the variances of the clean speech and of the noise for the individual frequency band, respectively. The LR of the kth frequency-band is derived as(7)Λ(Y(k,n))≡p(Y(k,n)|H1)p(Y(k,n)|H0)=11+ξ(k,n)·expγ(k,n)ξ(k,n)1+ξ(k,n)where ξ(k, n)=λx(k, n)/λd(k, n) and γ(k, n)=|Y(k, n)|2/λd(k, n) which are called the a priori SNR and the a posteriori SNR, respectively. The a posteriori SNR γ(k, n) is obtained by λd(k, n), which is updated during the periods of speech absence. Indeed, the long-term smoothed noise power spectrum of the background noise is estimated by the soft decision method (Chang and Kim, 2001) at the previous frame and only during speech pauses. The a priori SNR ξ(k, n) is estimated based on the well-known decision-directed approach (Sohn et al., 1999) as follows:(8)ξˆ(k,n)≡α|Xˆ(k,n−1)|2λd(k,n−1)+(1−α)P[γ(k,n)−1]whereXˆ(k,n−1), the speech spectral amplitude estimate obtained in the previous frame, is obtained by a minimum mean square error (MMSE) estimator (Ephraim and Malah, 1984) and P[z]=z if z≥0 and P[z]=0 otherwise. Also, α is a weight that is usually determined in the range (0.95, 0.99) (Ephraim and Malah, 1984).The final decision in the statistical model-based VAD has been accomplished by taking the geometric mean of the LR computed from the individual frequency-bands and it is obtained by(9)logΛ(n)=1L∑k=0L−1logΛ(k,n)≷H0H1ηwhere L and η denote the total number of frequency-bands and a given threshold for speech detection, respectively. As in (9), an input frame is classified as the period of speech if the geometric mean of the LR is greater than the given threshold η and the period of noise otherwise.Since the shallow machine learning techniques such as SVM and geometric mean as in (9) are not desirable for the statistical model-based VAD as we mentioned earlier, the DNN, a powerful hierarchical generative model, is herewith applied to the decision function for the statistical model-based VAD.To fully take a consideration of nonlinear distribution of input features, the key parameters of the statistical model-based VAD such as the a priori SNR, the a posteriori SNR, and the LR are employed into the input layer of the DNN and go through the multiple hidden layers in the training and test steps. Specifically, as shown in Fig. 1, the microphone input signal is transformed to the STFT coefficients and then the a priori SNR, the a posteriori SNR, and the LR are estimated as in (7) and (8). The input feature vector Z is composed of the aforementioned three parameters and their delta and delta–delta components. Then, these are mapped into the target values, which are given in the training stage, through multiple nonlinear hidden layers as follows:(10)f(Z(n)|Θ)=g(g(g(Z(n)w(0)+b(0))w(1)+b(1))w(2)+b(2))w(3)+b(3)where Z(n) is the feature vector at the nth frame, Θ={w(0), b(0), w(1), b(1), w(2), b(2), w(3), b(3)}, and g(·) denotes the activation function. And, w(j) and b(j) denote the weight term and bias term between the jth layer and the (j+1)th layer, respectively. Note that all activation functions adopt the logistic function as in Hinton and Salakhutdinov (2006). One of the important issues in training the DNN is how to pre-train the weight and bias parameters of each layer. The solution adopted in this paper is to stack the multiple restricted Boltzmann machines (RBMs), which can be trained layer-by-layer in a unsupervised greedy fashion (Hinton and Salakhutdinov, 2006; Hinton et al., 2006). After the weights and biases of each RBM are initialized by the layer-wise unsupervised learning procedure, they are fine tuned by the gradient descent-based back-propagation algorithm to minimize the multi-class cross entropy error function (Hinton and Salakhutdinov, 2006). The number of output units for the VAD is two, speech presence and absence and thus target output values of the speech frames are [10]T and those of the non-speech frames are [01]T. When the fine-tuning process begins, the weights and biases between the last hidden layer and the output layer are set randomly, not yet initialized, whereas the other weights and biases have been already initialized through the pre-training process. Hence, only the weights and biases between the last hidden layer and the output layer are trained in first few epoches and then all weights and biases are optimized thereafter.Specifically, the output units of each DNN for speech detection is represented by the output of the DNN for the VAD such that(11)yout(n)=f0(Z(n)|Θ)−f1(Z(n)|Θ)where fm(Z(n)|Θ) is the output value of the mth output node and can be computed by(12)fm(Z(n)|Θ)=g(g(g(Z(n)w(0)+b(0))w(1)+b(1))w(2)+b(2))wˆm(3)+bm(3)wherewˆm(3)=[w0m(3),w1m(3),w2m(3),…,wNnodem(3)]T, which is the mth column vector of w(3), where Nnode and T denote the number of nodes on the third hidden layer and the vector transposition, respectively. And,bm(3)denotes the mth element of b(3). The SPP can be derived from the output of the DNN by adopting a parametric way to employ the sigmoid function as given by(13)p(H(n)=H1|yout(n))=11+exp(A·yout(n)+B).Here, the principal parameters such as the slope parameter A and the bias parameter B can be obtained by discriminative training in a way to minimize the negative log likelihood of the data, which is the cross-entropy error function (Bishop, 1995) defined as(14)F(A,B)=−∑n=1Nt(n)log(p(H(n)=H1|yout(n)))+(1−t(n))log(1−p(H(n)=H1|yout(n)))where t(n) is the target value of the parametric sigmoid function at the nth frame of the training dataset and given by manually labeling of every frame in the training stage. More specifically, t(n)=1 if the nth frame belongs to a speech frame and t(n)=0 otherwise. The model-trust algorithm based on the Levenberg–Marquardt algorithm (Platt, 2000) is performed to obtain the optimal parameters A and B, which minimize the cross-entropy error function for each noise type.Ensemble of DNN and AEC methods are adopted to statistical model-based VAD to adopt the best DNN scheme according to noise type. For this, first, the ensemble of DNNs used for VAD can be built through the layer wise pre-training process by using all type of the noisy speech dataset without a label, namely, the unsupervised learning. Then, each DNN is optimized, separately, through the fine-tuning process by using each type of the noisy training data with the label and by starting with the initialized weights and biases. As a result, the optimal parameters for estimating the SPP from the output of the DNNs are obtained by the model-trust algorithm in a separate manner. Specifically, the optimal DNN parameters Θ1, Θ2, …,ΘNDNNsand the parametersAVAD(1),BVAD(1),AVAD(2),BVAD(2), …,AVAD(NDNNs),BVAD(NDNNs)for estimating the SPP are obtained from which Θi,AVAD(i), andBVAD(i)are optimized for the ith type of noisy training data where NDNNs denotes the number of DNNs on the number of the acoustic environments.For the realtime implementation, each type of noise should be classified during the periods of non-speech, so the classification is updated only when the SPP of the previous frame is greater than a given threshold (i.e., 0.35). This process is performed by applying the DNN in a similar reason described earlier. Indeed, the separate DNN for acoustic environment classification is implemented by incorporating the log-power spectrum (LPS) as the feature vector. For this, the acoustic signal data manually labeled as noise frames was used, so optimal weights and biases set ΘAEC were found at the off-line training stage.Generally, each frame is classified from the value of output nodes of the DNN into the classes by taking the soft-max and winner-take-all methods. Instead of the hard decision, we adopt the parametric way to employ the sigmoid function, which was also used to predict the SPPs as in (14) at the ensemble of the DNNs, by simplifying the output of each DNN as follows:(15)yˆi(n)=fi(log(Y(n))|ΘAEC)−∑j=1i−1fj(log(Y(n))|ΘAEC)−∑j=i+1Nfj(log(Y(n))|ΘAEC)whereyˆi(n)is the final output value of the DNN for the ith type of noise condition at the nth frame and Y(n)=[Y(0, n), Y(1, n), …, Y(L−1, n)], which is a vector of power spectra at the nth frame. The probability of the ith type of noise given the observation Y(n) is derived by(16)p(s(n)=si)|Y(n))=11+exp(AAEC(i)·yˆi(n)+BAEC(i))whereAAEC(i)andBAEC(i)denote the slope parameter and bias parameter for the ith noise type, respectively, and can be optimized to the ith type of noise environment based on the model-trust algorithm (Platt, 2000). In the online classification stage, SPPs can be computed from each DNN by transferring the statistical feature vector into each well-trained DNN and then they are derived from the simplified output of each DNN by the parametric sigmoid function with specific optimal parametersAVAD(i),BVAD(i). A final SPP can be obtained by the linear weighted combination way with the weights derived from the result of the acoustic environment classification. Note that the weights must satisfy the following conditions:(17)∑i=1Nnoisewi(n)=1(18)wi(n)≥0.The weights satisfying the above conditions can be derived from the probabilities for each type of noise as follows:(19)wi(n)=exp(p(s(n)=si|log(Y(n)))∑j=1Nnoiseexp(p(s(n)=sj|log(Y(n))).The final SPP is estimated by fusing the probability of each DNN with the weights for each noise as follows:(20)p(H(n)=H1|Z(n))=∑i=1NDNNswi(n)·pi(H(n)=H1|yout(n))where pi(H(n)=H1|yout(n)) is calculated by using the parameters Θi,AVAD(i), andBVAD(i)used to estimate the SPP at ith DNN for VAD. In order to install the weights adaptively, weights have to be updated during the period of noise. Here, the threshold for the criterion of speech absence was set to 0.35 for updating the weights. This scheme is revised by the hang-over scheme method (Sohn et al., 1999) when the final SPP of the previous frame is less than the given threshold as given by(21)wi(n)=αwi(n−1)+(1−α)exp(p(s(n)=si|log(Y(n)))∑j=1Nnoiseexp(p(s(n)=sj|log(Y(n)))where α is a smoothing factor. Note that the AEC operates on the first few frames since those can be considered as the non-speech frames, which can be an acceptable rule (Choi and Chang, 2012). Finally, each frame is initially classified as the period of speech if the SPP as in (20) is greater than a given threshold η for speech detection or the period of noise otherwise.

@&#CONCLUSIONS@&#
In this paper, we have presented the novel VAD technique provided by the ensemble of the DNNs with AEC. The first contribution of this work is the use on the statistical model parameters such as the a priori SNR, the a posteriori SNR, and the LR in the feature extraction part and then they map to the target output value in a nonlinear way for transferring them into the multiple DNNs, which were trained for each kind of noise conditions. In order to implement the frame-by-frame pooling of the ensemble of the DNN, the SPPs are first derived from the output of the DNNs by using parametric sigmoid function with optimized parameters. Then, final SPP is computed by fusing the estimate of each DNN in a linearly weighed combination way by the use of weights derived from the result of the DNN-based AEC. The proposed method was evaluated in terms of an objective measure and was found to have significant improvement compared to the previous method under the various noise conditions in terms of types and SNR levels.