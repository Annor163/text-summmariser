@&#MAIN-TITLE@&#
A simultaneous sample-and-filter strategy for robust multi-structure model fitting

@&#HIGHLIGHTS@&#
Proportion of good/bad hypotheses is critical to fitting process.A sample-and-filter strategy for robust multi-structure model fitting.Simultaneously guides the sampling and removes irrelevant hypotheses.Output a small hypothesis set with a high concentration of good hypotheses.Improve the accuracy of mode seeking-based fitting methods.

@&#KEYPHRASES@&#
Robust model fitting,Multiple structures,Guided sampling,Hypothesis filtering,Preference analysis,

@&#ABSTRACT@&#
In many robust model fitting methods, obtaining promising hypotheses is critical to the fitting process. However the sampling process unavoidably generates many irrelevant hypotheses, which can be an obstacle for accurate model fitting. In particular, the mode seeking based fitting methods are very sensitive to the proportion of good/bad hypotheses for fitting multi-structure data. To improve hypothesis generation for the mode seeking based fitting methods, we propose a novel sample-and-filter strategy to (1) identify and filter out bad hypotheses on-the-fly, and (2) use the remaining good hypotheses to guide the sampling to further expand the set of good hypotheses. The outcome is a small set of hypotheses with a high concentration of good hypotheses. Compared to other sampling methods, our method yields a significantly large proportion of good hypotheses, which greatly improves the accuracy of the mode seeking-based fitting methods.

@&#INTRODUCTION@&#
Robust model fitting is required in various computer vision tasks such as the estimation of projective entities from multi-view images [1]. In practice, data are unavoidably contaminated by outliers due to sensing or preprocessing errors. Furthermore multiple instances (or structures [2]) of the geometric model often exist in the data, such as in motion segmentation [3]. This not only raises the outlier rate through pseudo-outliers [4], but also necessitates model selection to detect and estimate multiple structures.We distinguish two major paradigms among current robust estimators. The first is the so called “hypothesize-and-verify” methods (e.g., RANSAC [5], ASSC [6], pbM-estimator [7]) which repeatedly generate model hypotheses fitted on randomly sampled minimal subsets of the data, and then score the hypotheses using a robust criterion. To fit multiple structures, the inliers corresponding to the fitted structure are removed and the overall process is repeated. The second class of methods is mode seeking-based methods (e.g., Randomized Hough Transform [8], Mean Shift [9]), where each detected mode corresponds to a structure in the data. The mode seeking is accomplished in the parameter space using model hypotheses as samples. More recently, methods which cluster the data by constructing data similarity functions induced by model hypotheses [10,11] have also been proposed. These are also parameter space analysis methods based on using collections of hypotheses to cluster data, and so we include these methods in the second paradigm.Hypothesis sampling underpins both paradigms, and obtaining a good set of hypotheses is crucial for success. However, the difference in the sampling requirements of the two paradigms has not been generally emphasized. Methods in the first group focus on quickly retrieving a sufficiently good hypothesis which scores highly in the robust criterion; various guided sampling schemes have been proposed to this end [12–15]. Even when invoked under the fit-and-remove procedure (thereby turning an essentially “single dominant model plus outliers” approach into a “multiple models plus outliers” one); within each loop the underlying assumption is that the method is currently searching for only one structure of interest, and the aim of sampling is to recover a single good hypothesis of a genuine structure.On the other hand, the success of methods in the second group depends on retrieving a large proportion of good hypotheses (from all valid structures) relative to bad hypotheses, such that clusters or modes can be easily detected without being overwhelmed or swamped by bad or irrelevant hypotheses. Contrast this to the first group of methods where the relative proportions of good and bad hypotheses are inconsequential as long as a single good hypothesis (from one of the structures) is produced within reasonable time.Our aim in this paper is to improve hypothesis sampling for methods in the second group, which has been received comparatively less attention. Conventional guided sampling schemes [12–15] concentrate on rapidly yielding a hypothesis with high scores for methods in the first group, and ignore the danger posed by a large number of bad hypotheses towards mode detection. Moreover, producing these irrelevant hypotheses wastefully consumes large chunks of the time budget.To optimize hypothesis sampling for mode seeking-based methods, we present a novel sample-and-filter strategy for hypothesis generation. Our strategy aims to provide a hypothesis set with a high concentration of good hypotheses as input to mode seeking-based methods for achieving more accurate results. The basic idea of our strategy (illustrated in Fig. 1)) is to (1) identify and filter out bad hypotheses on-the-fly during sampling, and (2) use the remaining good hypotheses to guide the sampling to further expand the set of good hypotheses. Note that all the above steps are accomplished in the time budget for hypothesis sampling.A recent guided sampling scheme was proposed for multi-structure data (Multi-GS) [17]. However like other guided sampling schemes it ignores the proportion of bad hypotheses generated. Our work extends and improves Multi-GS in two ways: (1) In [17] the idea of exploiting the correlation of preference of data towards putative hypotheses was proposed. Here we show that a dual preference (i.e., preference of putative hypotheses towards data) is possible and computationally more attractive. (2) We introduce our sample-and-filter strategy based on the above dual preference to actively raise the proportion of good hypotheses generated. This significantly improves the accuracy of mode-seeking based fitting methods, without sacrificing computational efficiency. Initial versions of our work appeared in [18,19] – see the next section for an overview of the relationship to this paper.The rest of the paper is organized as follows: Section 2 provides a survey of related work. Section 3 introduces basic notions and notations for our two proposed sample-and-filter methods in Sections 4 and 5. Section 6 presents the experimental results and we draw conclusions in Section 7.

@&#CONCLUSIONS@&#
