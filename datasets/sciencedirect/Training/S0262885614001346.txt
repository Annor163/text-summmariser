@&#MAIN-TITLE@&#
Robust object tracking using least absolute deviation

@&#HIGHLIGHTS@&#
The representation error is modelled as a Laplacian distribution.We derive our new LAD–Lasso model based on a Bayesian MAP estimate.LAD–Lasso model is robust to outliers.The number of optimisation variable in the new model reduces greatly.We use ADMM algorithm to solve the new optimisation problem.

@&#KEYPHRASES@&#
Object tracking,Sparse representation,Least absolute deviation,

@&#ABSTRACT@&#
Graphical abstract

@&#INTRODUCTION@&#
Object tracking is an important component of many surveillance systems, such as transport systems (e.g., road traffic, airports and harbours), public spaces (e.g., shopping malls and parks), industrial environments, low-altitude rescue and military establishments. More efficient and robust object tracking remains a challenge due to the issues of image noise, complex object motion, partial or full occlusions, drastic illumination and pose changes [1].Methods based on particle filter are widely used for tracking. Under the particle filter framework, different image features such as colour [3,4,7], shape [2,13] and image structure [7,15] can be used to represent the appearance of object.Recently, there has been an increased interest in sparse representation and its applications in the field of computer vision. Under the particle filter framework, Xue Mei et al. [8,9] proposed a tracking method based on sparse representation, which was named the L1 Tracker. To find the tracking target in a frame, each target candidate (particle sample) is approximately expressed as a sparse linear combination of some target templates and trivial templates. The sparse representation coefficients of a target candidate are calculated by solving a L1-regularised least squares problem, which is high computational cost due to the high dimension of trivial templates.To further accelerate the L1 Tracker, Hanxi Li et al. [16] use the orthogonal matching pursuit (OMP) algorithm to search for a sparse solution. To reduce the number of particle samples that need to participate in solving the optimisation problems, Xue Mei et al. [10] improve L1 Tracker with a minimal error bounding strategy called the BPR-L1 Tracker. The APG-L1 Tracker proposed by Chenglong Bao et al. [24] used an accelerated proximal gradient (APG) approach to solve a new L1-norm related problem that added one term to control the energy of trivial templates. By regularizing the representation problem to enforce joint sparsity and learning the particle representations together, Tianzhu Zhang et al. [34] propose a computationally efficient multi-task sparse learning method to mine correlations among different tasks to obtain better tracking results than learning each task individually. The linear representation in their later work [37] incorporates background templates in the dictionary to discriminate the target from the background better and casts the tracking problem as an efficient low-rank matrix learning problem.To further accelerate the L1 Tracker, Hanxi Li et al. [16] use the orthogonal matching pursuit (OMP) algorithm to search for a sparse solution. To reduce the number of particle samples that need to participate in solving the optimisation problems, Xue Mei et al. [10] improve the computation speed of L1 Tracker with a minimal error bounding strategy called the BPR-L1 Tracker. The APG-L1 Tracker proposed by Chenglong Bao et al. [24] used an accelerated proximal gradient (APG) approach to solve a new L1-norm related problem that added one term to control the energy of trivial templates. By enforcing joint sparsity, Tianzhu Zhang et al. [34] propose a computationally efficient multi-task sparse learning method, which learns the particle representation solutions of all particle samples together.Some work based on sparse representation is devoted to improve the robustness of tracking. To alleviate the accumulation of errors during the self-updating, Baiyang Liu et al. [31] use a static sparse dictionary and a dynamically online updated basis distribution to model the target appearance. In order to deal with the challenge of drastic appearance change, Zhong Wei et al. [32] propose an appearance model exploiting both holistic templates and local representation. Jia Xu et al. [33] develop an appearance model, which exploits both partial information and spatial information of the target based on a novel alignment-pooling method. To discriminate the target from the background, Tianzhu Zhang et al. [37] incorporates background templates into the dictionary of sparse representation and reformulates the tracking problem as an efficient low-rank matrix learning problem.In the view of the model of the representation error, LSS [36] assumes that the representation error follows the Gaussian–Laplacian distribution. Dong Wang et al. [35,36] use classic principal component analysis (PCA) to learn effective appearance model. It needs to be stressed that there is no sparsity constraint on the representation coefficients in [35,36]. Consequently LSS [36] is not in the framework of sparse representation.In this paper, we propose a new tracking method under the framework of the L1 Tracker that can work more quickly and robustly. Our main contributions include:1)The representation error is modelled as a random variable following a Laplacian distribution. The representation error, which indicates corruption or noise, is random and unknown in advance. Thus, accurately modelling the representation error is a key to the robustness of tracking. After an elaborate analysis of the distribution of the corruption, we find that the distribution of the corruption is characterised by one spike and a long-tail, so we model the representation error as a Laplacian distribution.Based on the Laplacian representation error model and a sparseness-promoting prior of the representation vector, we derive our new LAD–Lasso model with a Bayesian Maximum A Posteriori (MAP) estimate. The number of optimisation variables in our new model is equal to the number of target templates, regardless of the dimensions of the feature. Thus, the computation cost can be reduced greatly compared with L1 Tracker and APG-L1 Tracker.After reformulating our proposed optimisation model, we use Alternating Direction Method of Multipliers (ADMM) to solve our proposed nonsmooth optimisation problem.We name our new method the LAD Tracker (Least Absolute Deviation). Experiments on challenging video sequences demonstrate our method performs well in computation speed and robustness.This paper is organised as follows: In Section 2, we briefly review the basic idea of trackers based on sparse representation. Section 3 introduces our LAD Tracker in detail. In Section 4, we make a theoretical analysis of the robustness and computation cost, compared with other trackers based on sparse representation. In Section 5, we demonstrate the performances of the LAD Tracker through numerous experiments. The conclusion is made in Section 6.In this section, we will briefly introduce the framework of trackers based on sparse representation. John Wright and Yi Ma et al. [20] addressed the problem of human face recognition via computing sparse linear representations with regard to a dictionary of different human faces. Then, Xue Mei et al. [8] extend the application based on sparse representation to tracking named the L1 Tracker. There are many later works that improve on this method [10,16,24]. All of these related trackers inherit the framework of the L1 Tracker.Generally, there are three main components in a tracking system [6]: the motion model, image representation and appearance model. L1 Tracker uses Markov process along with a Gaussian sampling strategy in a particle filter as its motion model, downsampling the image as its image representation and an adaptive updating template set as its appearance model. Both the particle filter and downsampling image allow L1 Tracker to adapt itself to scale variation. For more details about particle filters, please refer to the tutorial on particle filters [12,13]. The dynamic appearance model and model updating are related to the previous work on the appearance adaptive particle filter (AAPF) tracker [21].The basic idea of L1 Tracker is that each particle sample in a given frame can be approximately expressed with a sparse linear combination of some target templates and trivial templates. The target template is dynamically updated when a new target candidate appears to be different from the template set. The process of L1 Tracker is illustrated in Fig. 1.In the first frame, L1 Tracker chooses the first target template manually. Then, the other m templates are cropped by randomly moving only a few pixels around the first template, as shown in Fig. 1. These m templates form a positive template set. Each template is resized to a small p×q patch and stacked into a column vector with a size of d×1, where d=p×q, and normalised to zero mean and unit variance.Assume that the number of particle samples is n. LetT=t1t2⋯tm∈Rd×mdenotes the target template set at framet. Each of particle samples does the same pre-processing as the target template does. Considering occlusions, the ith target candidatey∈Rd×1 at frame t can be linearly represented by:(1)y=Tx+e,x≥0,where the column vectorx∈Rm×1 is composed of the coefficients corresponding to the target templates, ande∈Rd×1 is the representation error.To make all variables nonnegative, L1 Tracker enforces the nonnegativity constraints bye=e+−e−, wheree+,e−∈Rd×1≥0. Then, Eq. (1) can be rewritten as:(2)y=Tx+e+−e−=TI−Ixe+e−≜Aa,s.t.a≥0,whereA=TI−I∈Rd×m+2d, anda=xe+e−∈Rm+2d×1≥0. The identity matrixesI∈Rd×dand −I∈Rd×dare trivial templates.L1 Tracker computes the representation vectoraby solving the following L1-regularised least squares problem with a nonnegative constraint:(3)mina12Aa−y22+λa1s.t.a≥0.L1 Tracker solves Eq. (3) via an interior-point method.However, Chenglong Bao et al. [24] argued that the linear combination of trivial templates sometimes contains parts of the target, which will lead to a loss of accuracy. Thus, APG-L1 Tracker adds one item to control the energy of trivial templates as:(4)mina′12A′a′−y22+λa′1+μt2xI22s.t.x≥0,whereA′=TI∈Rd×m+d, anda′=xxI∈Rm+d×1. μtis a parameter that controls the energy in trivial templates.Both L1 Tracker and APG-L1 Tracker find the target with the minimum reconstruction error from the target template subspace by calculating the largest possibility of:(5)1Γexp−αTx−y22,where α is a constant controlling the shape of the Gaussian kernel, and Γ is a normalisation factor.At the end of the process in each frame, template updating occurs to decide whether the target we found replaces one of target templates. The updates to the templates should contribute to adapting to complex environments.For all the target templatesT=t1t2⋯tmat frame t, each of them has a weight wi. L1 Tracker makes a similarity measure between the target we obtained at frame t and the target template whose corresponding representation coefficient has the maximum value. If the similarity value is less than a given threshold value, then the target will replace the target template with the smallest wi. A medium value of current weights is assigned to the new template. We normalise weights at the end. In fact, the less a target template is used to represent tracking results, the lower its weight is. Template updating scheme can help tracker adapt complex environments and alleviate the problem of target drift in tracking.This section will introduce the details of our proposed LAD Tracker.As introduced in Section 2, the main purpose of a sparse representation is to estimate the representation vectorxefficiently and accurately under the givenTandy.The main idea of LAD Tracker can be described in Fig. 2. First, from the representation model in Eq. (1), the representation errorewill affect the estimate ofxand thus will affect the robustness of tracking when corruptions occur. Thus, how to accurately model the representation erroreplays an important role in estimatingx. We analyse the statistic characteristics of the representation erroreunder some typical tracking scenarios and model it as a Laplacian distribution. Second, by utilising Bayesian MAP theory, we derive our new optimisation model for the estimation ofx. Finally, ADMM is used to solve it.Section 3.1 discusses how to model representation error. The proposed optimisation model and ADMM optimisation algorithm will be introduce in Sections 3.2 and 3.3, respectively. Section 3.4 discusses how to achieve our tracking target afterxis determined.In the representation model in Eq. (1), the representation erroredenotes the occlusion, illumination variation, pose variation, blur and noise, which are random and unknown in advance. In practical tracking applications, the model of representation error should be resistant to these corruptions. In this section, by analysing numerous typical tracking video sequences, we ultimately use a Laplacian distribution to describe the reorientation error.We perform our analysis on some known datasets which can be downloaded in [25,26]. Both the real targets and the corrupted targets are manually selected. Without a loss of generality, we select a corrupted target which obviously changed under different corruptions we may encounter. We try to use the Gaussian distribution and Laplacian distribution to model the corruptions, respectively. From Fig. 3, it is obvious that the Laplacian distribution can match the corruptions better than the Gaussian distribution.Some typical examples are given in Fig. 3. The blue bar denotes the true statistical histogram of the corruptions, which have had a zero mean and unit variance normalisation applied to them. The green and red stair lines indicate the cumulative probabilities of the Laplacian distribution and the Gaussian distribution with the same variance as the corruptions, respectively. We use the same step size for the statistical histograms and stair lines.From Fig. 3(a) to (e), the distributions of the corruptions are characterised by two obvious features: one spike and a long tail. The spike occurs because the residuals between the target and corrupted target have many zero entries, as we can see from the images of the corruptions. The reason for the long-tail feature is the different ranges of corruptions.As we can see from Fig. 3, the Gaussian distribution could effectively cover the long-tail portion but fail to fit the spike. On the contrary, the Laplacian distribution could not only cover the long-tail part but also fit the spike well.A similar phenomenon has been found in Fig. 3(e) and (g). Both the Laplacian distribution and the Gaussian distribution can fit low-level Gaussian or impulse noise very well. For the case of high-level Gaussian noises in Fig. 3(f), however, the red line which indicates Gaussian distribution can fit better. These phenomenons make a lot of sense. Without impulse noise, a Laplacian assumption does no harm to model low-level Gaussian noise, as long as data do not contain a large amount of Gaussian noise [18]. It should be mentioned that there is almost no high-level Gaussian noise in practical tracking problems; the tracking system is usually corrupted by impulse noise [19]. In general, impulse noises satisfy the Laplacian distribution.In this section, we derive our optimisation programme for tracking from the viewpoint of Bayesian MAP.In the representation model in Eq. (1), the representation vectorxshould be sparse. In general, there are two definitions of sparsity. First, we can use the number of non-zero points inxto indicate the sparsity. Second, a widely used sparseness prior is thatxfollows the Laplace distribution [22] with a parameter a:(6)px=a2mexp−ax1.On the other hand, from the analysis in Section 3.1, a representation error can also be modelled by a Laplacian distribution with a parameter b:(7)pe=py|x=b2dexp−be1.From the viewpoint of Bayesian Maximum A Posteriori (MAP), we can estimatexthrough:(8)px|y=pxpy|xpy∝pxpy|x∝exp−ax1−be1∝exp−bλx1+e1,where λ=a/b. Ultimately,xis estimated by solving the following optimisation problem.(9)minxλx1+y−Tx1.Model (9) is a problem of least absolute deviation with a L1 regularisation and known as LAD–lasso model [27].LAD–Lasso model contains a fitting fidelity term measured by the least absolute deviation (LAD) and the L1 regularisation term controlling the sparsity. The fidelity term has a heavy impact on the final estimate because it ensures that the givenycan be sparsely represented by the target template setT[28].LAD–Lasso model in Eq. (9) is convex and nonsmooth. Unlike Lasso, LAD–Lasso consists of two nonsmooth L1-norm terms, so that the FISTA [29] or APG method [30] cannot be used. We could solve the problem in Eq. (9) using the interior point method or sub gradient method, but in practice they are too slow to be of interest. Alternatively, we use Alternating Direction Method of Multipliers (ADMM) to solve the LAD–Lasso problem.ADMM was proposed by Gabay, Mercier, Glowinski and Marrocco [17]. It is well suited to distributed convex optimisation and in particular to large-scale problems arising in statistics, machine learning, and related areas [11,23]. ADMM solves problems as follows:(10)minfx+gzs.t.Px+Qz=c,where the functions f, g are convex. The augmented Lagrangian function of Eq. (10) is:(11)Lρxzy=fx+gz+pTPx+Qz−c+ρ2Px+Qz−c22,wherepis a multiplier, and ρ>0 is a penalty parameter. Define the residualr=Ax+Qz−c. Then, we have(12)Lρxzu=fx+gz+ρ2Px+Qz−c22+ρ2u22,whereu=p/ρ. ADMM consists of an x-update, a z-update, and a dual variable update as follows:(13)xk+1=argminxfx+ρ2Px+Qzk−c+uk22zk+1=argminzgz+ρ2Pxk+1+Qz−c+uk22uk+1=uk+Pxk+1+Qzk+1−c.Note that ADMM cannot be used to solve our LAD–Lasso directly. If our LAD–Lasso model in Eq. (9) is solved directly via generic ADMM, then the x-update will turn into a new sub Lasso problem. That is clearly not what we want. Here, we reformulate Eq. (9) into another least absolute deviation problem so that it can be solved directly by ADMM. The L1 regularisation term in Eq. (9) is equivalent to:(14)λx1=0m×1−λIm×mx1,where 0m×1∈Rm×1 denotes the zero vector with all entries equal to 0. Substituting Eq. (14) into Eq. (9), we have:(15)λx1+y−Tx1=0m×1−λIm×mxy−Tx1=b−Dx1,whereb=0m×1y∈Rm+d×1andD=λIm×mT∈Rm+d×m. So, our new least absolute deviation problem is:(16)minxb−Dx1,s.t.x∈Rm×1≥0.Let f(x)=0, auxiliary variablez=b−Dx, g(z)=z1,P=D,Q=I, andc=b. Then, the augmented Lagrangian function of Eq. (16) is:(17)Lρxzu=z1+ρ2Dx+z−b+u22+ρ2u22.Finally,x,zanduupdate as follows:(18)xk+1=argminxDx+zk−b+uk22=DTD−1DTb−zk−ukzk+1=argminzz1+ρ2Dxk+1+z−b+uk22=S1/ρ−Dxk+1+b−uk=−Dxk+1+b−uk−1ρ−Dxk+1+b−uk>1ρ0−Dxk+1+b−uk≤1ρ−Dxk+1+b−uk+1ρ−Dxk+1+b−uk>−1ρuk+1=uk+Dxk+1+zk+1−b.At the end of each frame processing, we find the target with the largest possibility of:(19)1Γexp−y−Tx1β=1Γexp−e1β,where β is a constant controlling the shape of the Laplacian kernel, and Γ is a normalisation factor.It is equivalent to finding the target with the minimum reconstruction errore=y−Tx, which is known as representation error and modelled as a Laplacian distribution in our work.In this section, we will discuss the robustness and computation cost of our LAD Tracker.For the methods based on sparse representation, the robustness is mainly determined by two steps: searching for sparse solutionxand finding the tracking target afterxis found.Note that the model of representation error plays an important role in both of the steps above.In the step of searching for a sparse solution, our LAD Tracker models the presentation error as a Laplacian distribution and calculatesxby Eq. (9). As analysed in Section 3.1, the distribution of the corruptions always fits a Laplace distribution better. This distribution means that there are many outliers during the estimation ofx. For the purpose of convenience, we relax our example to 2-D. Fig. 4illustrates the impact of outliers on the estimation of coefficients where the bluex* is the true value, and the greenxis the estimate of our LAD Tracker. We can see that our LAD–Lasso model estimates the representation vectorxmore robustly when corruption occurs.In the step of finding the target, the tracking result of LAD Tracker is the particle with the largest probability of a Laplacian kernel in Eq. (19) or the minimum L1-norm of representation error. On the contrary, L1 Tracker and APG-L1 Tracker find the target by Eq. (5), which implies that the distribution of the representation error is Gaussian.Numerous experiments on challenging sequences in Section 5 demonstrated that our LAD Tracker performs robustly compared with L1 and APG trackers.Our LAD Tracker, L1 Tracker and APG-L1 Tracker work in the framework of particle filter. There are hundreds of particle samples to address in each frame. Consequently, for those methods based on sparse representation, the computational efficiency when solving the optimisation problem is critical for real-time applications.Generally speaking, the computational burden of solving an optimisation problem relates primarily to two factors: the number of optimisation variables and the optimisation algorithm.On the one hand, the number of variables in our LAD Tracker is far less than L1 Tracker and APG-L1 Tracker. As analysed in Section 3, the optimisation variable in our model in Eq. (9) or Eq. (16) isx∈Rm×1, whose dimension is equal to the number of target templates m. For APG-L1 Tracker, the optimisation variable in Eq. (4) isa'∈R(m+d)×1, and its dimension is equal to the total number of target and trivial templates. In L1 Tracker, the optimisation variable in Eq. (3) isa∈R(m+2d)×1, and its dimension is equal to the total number of target and trivial templates.Note that the number of target templates m is usually far less than the dimension of the feature d. We take the example of the sequences Occluded face2. The number of target templates is m=10, and the dimension of feature is d=320=20×16. Table 1show the number of optimisation variables in our LAD Tracker, L1 Tracker and APG-L1 Tracker.On the other hand, with the development of multi-core computers and parallel computing, ADMM that is suited to distributed convex optimisation plays an increasing role in speeding up the computation.The simulation results of the fps (frames per second) in Section 5.1 show the advantage of our method compared with L1 Tracker and APG-L1 Tracker.

@&#CONCLUSIONS@&#
In this paper, we proposed a new tracking method based on sparse representation. By modelling the corruption as a Laplacian distribution, we propose a new optimisation model for estimating the representation vector and use an ADMM optimisation algorithm to solve it. Numerous simulation results on challenging sequences demonstrated that our LAD Tracker performs very well.There is still one interesting question about the parameter λ=a/b in our model in Eq. (9). It is an important parameter that controls the balance of the sparsity of the representation vectorxand the representation errore. We set λ=1 empirically for all of the experiments. A more reasonable value for λ may improve the performance of our LAD Tracker. This will be our future work.