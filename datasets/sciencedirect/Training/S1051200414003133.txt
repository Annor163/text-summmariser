@&#MAIN-TITLE@&#
Robust least squares methods under bounded data uncertainties

@&#HIGHLIGHTS@&#
Introducing robust estimation algorithms based on a novel regret formulation.Performance tradeoff between best-case and worst-case optimal estimators owing to the regret formulation.Demonstrating the superior performance of the introduced novel algorithms.

@&#KEYPHRASES@&#
Data estimation,Least squares,Robust,Minimax,Regret,

@&#ABSTRACT@&#
We study the problem of estimating an unknown deterministic signal that is observed through an unknown deterministic data matrix under additive noise. In particular, we present a minimax optimization framework to the least squares problems, where the estimator has imperfect data matrix and output vector information. We define the performance of an estimator relative to the performance of the optimal least squares (LS) estimator tuned to the underlying unknown data matrix and output vector, which is defined as the regret of the estimator. We then introduce an efficient robust LS estimation approach that minimizes this regret for the worst possible data matrix and output vector, where we refrain from any structural assumptions on the data. We demonstrate that minimizing this worst-case regret can be cast as a semi-definite programming (SDP) problem. We then consider the regularized and structured LS problems and present novel robust estimation methods by demonstrating that these problems can also be cast as SDP problems. We illustrate the merits of the proposed algorithms with respect to the well-known alternatives in the literature through our simulations.

@&#INTRODUCTION@&#
In this paper, we investigate estimation of an unknown deterministic signal that is observed through a deterministic data matrix under additive noise, which models a wide range of problems in signal processing applications [1–14]. In this framework, the data matrix and the output vector are not exactly known, however, estimates for both of them as well as uncertainty bounds on the estimates are given [2,8,15–19]. Since the model parameters are not known exactly, the performances of the classical LS estimators may significantly degrade, especially when the perturbations on the data matrix and the output vector are relatively high [9,15,16,20–22]. Hence, robust estimation algorithms are needed to obtain a satisfactory performance under such perturbations. This generic framework models several real-life applications, which require estimation of a signal observed through a linear model [9,16]. As an example, this setup models realistic channel equalization scenarios, where the data matrix represents a communication channel and the data vector is the transmitted information. The channel is usually unknown, especially for wireless communications applications, and possibly can be time-varying. Hence, in practical applications, the communication channel is estimated, where this estimate is usually subject to distortions [9,16]. Under such possible perturbations, robust equalization methods can be used to obtain a more consistent and acceptable performance compared to the LS (or MMSE) equalizer. In this sense, this formulation is comprehensive and can be used in other applications such as in feedback control systems to estimate a desired data under imperfect system knowledge.A prevalent approach to find robust solutions to such estimation problems is the robust minimax LS method [8,9,16,23–27], in which the uncertainties in the data matrix and the output vector are incorporated into optimization framework via a minimax residual formulation and a worst-case optimization within the uncertainty bounds is performed. Although the robust LS methods are able to minimize the LS error for the worst-case perturbations, they usually provide unsatisfactory results on the average [15,23–27] due to their conservative nature. This issue is significantly exacerbated especially when the actual perturbations do not result in significant performance degradation. Another well-known approach to compensate for errors in the data matrix and the output vector is the total least squares method (TLS) [15], which may yield undesirable results since it employs a conservative approach due to data de-regularization. On the other hand, the data matrix usually has a known special structure, such as Toeplitz and Hankel, in many linear regression problems [9,15]. Hence, in [9,15], the authors illustrate that the performances of the estimators based on minimax approaches improve when such a prior knowledge on data matrix structure is integrated into the problem formulation. In all these methods, LS estimators under worst case perturbations are introduced to achieve robustness. However, due to this conservative problem formulation, in many practical applications, these approaches yield unsatisfactory performances [2,8,18,28–30].In order to counterbalance this conservative nature of the robust LS methods [9], we propose a novel robust LS approach that minimizes a worst case “regret” that is defined as the difference between the squared residual error and the smallest attainable squared residual error with an LS estimator [2,8,18,28–30]. By this regret formulation, we seek a linear estimator whose performance is as close as possible to that of the optimal estimator for all possible perturbations on the data matrix and the output vector. Our main goal in proposing the minimax regret formulation is to provide a trade-off between the robust LS methods tuned to the worst possible data parameters (under the uncertainty bounds) and the optimal LS estimator tuned to the underlying unknown model parameters. Minimax regret approaches have been presented in signal processing literature to alleviate the pessimistic nature of the worst case optimization methods [2,8,18,28–30]. In [18,29], linear minimax regret estimators are introduced to minimize the mean squared error (MSE) under imperfect knowledge of channel statistics and true parameters, respectively. In [28], a minimum mean squared error (MMSE) estimation technique under imperfect channel and data knowledge is investigated. In [2], these robust estimation methods are extended to flat fading channels to perform channel equalization. These methods are shown to provide a better average performance compared to the minimax estimators, whereas under large perturbations the robustness of the minimax estimators are superior to these competitive methods. On the other hand, in this paper, the optimization frameworks investigated here are significantly different than [9,16,23–27], where the regret terms are directly adjoined in the cost functions. In particular, unlike [2,18,28,29], where the uncertainties are in the statistics of the transmitted signal or channel parameters, in this paper, the uncertainty is both on the data matrix and the output vector without any statistical assumptions. While in [8], the authors have considered a similar framework, the results of this paper build upon them and provide a complete solution to the regret based robust LS estimation methods unlike [8]. We emphasize that perturbation bounds on the data matrix and the output vector heavily depend on the estimation algorithms employed to obtain them. Since our methods are formulated for given perturbation bounds, different estimation algorithms can be readily incorporated into our framework with the corresponding perturbation bounds [16].Our main contributions in this paper are as follows. i) We introduce a novel and efficient robust LS estimation method in which we find the transmitted signal by minimizing the worst-case regret, i.e., the worst-case difference between the residual error of the LS estimator and the residual error of the optimal LS estimator tuned to the underlying model. In this sense, we present a robust estimation method that achieves a tradeoff between the robust LS estimation methods and the direct LS estimation method tuned to the estimates of the data matrix and output vector. ii) We next propose a minimax regret formulation for the regularized LS estimation problem. iii) We then introduce a structured robust LS estimation method in which the data matrix is known to have a special structure such as Toeplitz or Hankel. iv) We demonstrate that the robust estimation methods we propose can be cast as SDP problems, hence our methods can be efficiently implemented in real-time [31]. v) In our simulations, we observe that our approaches provide better performance compared to the robust methods that are optimized with respect to the worst-case residual error [9,32], and the conventional methods that directly solve the estimation problem using the perturbed data.The organization of the paper is as follows. An overview to the problem is provided in Section 2. In Section 3.1, we first introduce the LS estimation method based on our regret formulation, and then present the regularized LS estimation approach in Section 3.2. We then consider the structured LS approach in Section 3.3 and provide the explicit SDP formulations for all problems. The numerical examples are demonstrated in Section 4. Finally, the paper concludes with certain remarks in Section 5.In this paper, all vectors are column vectors and represented by boldface lowercase letters. Matrices are represented by boldface uppercase letters. For a matrix H,HHis the conjugate transpose,‖H‖is the spectral norm,H+is the pseudo-inverse,H>0represents a positive definite matrix andH≥0represents a positive semi-definite matrix. For a square matrix H,Tr(H)is the trace. Naturally, for a vector x,‖x‖=xHxis theℓ2-norm. Here, 0 denotes a vector or matrix with all zero elements and the dimensions can be understood from the context. Similarly, I represents the appropriate sized identity matrix. The operatorvec(⋅)is the vectorization operator, i.e., it stacks the columns of a matrix of dimensionm×ninto anmn×1column vector. Finally, the operator ⊗ is the Kronecker product [33].We investigate the problem of estimating an unknown deterministic vectorx∈Cnwhich is observed through a deterministic data matrix. However, instead of the actual data matrix and the output vector, their estimatesH∈Cm×nandy∈Cmand uncertainty bounds on these estimates are provided. In this sense, our aim is to find a solution to the following data estimation problemy≈Hx,such thaty+Δy=(H+ΔH)x,for deterministic perturbationsΔH∈Cm×n,Δy∈Cm. Although these perturbations are unknown, a bound on each perturbation is provided, i.e.,‖ΔH‖≤δHand‖Δy‖≤δY,whereδH,δY≥0. In this sense, we refrain from any assumptions on the data matrix and the output vector, yet consider that the estimates H and y are at least accurate to “some degree” but their actual values under these uncertainties are completely unknown to the estimator.Even in the presence of these uncertainties, the symbol vector x can be naively estimated by simply substituting the estimates H and y into the LS estimator [10]. For the LS estimator we havexˆ=H+y,whereH+is the pseudo-inverse of H[33]. However, this approach yields unsatisfactory results, when the errors in the estimates of the data matrix and the output vector are relatively high [9,18,29,32]. A common approach to find a robust solution is to employ a worst-case residual minimization [9]xˆ=argminx∈Cnmax‖ΔH‖≤δH,‖Δy‖≤δY⁡‖(y+Δy)−(H+ΔH)x‖2,where x is chosen to minimize the worst-case residual error in the uncertainty region. However, since the solution is found with respect to the worst possible data matrix and output vector in the uncertainty regions, it may be highly conservative [15,18,29].Here, we propose a novel LS estimation approach that provides a tradeoff between performance and robustness in order to mitigate the conservative nature of the worst-case residual minimization approach as well as to preserve robustness [18,29]. The regret for not using the optimal LS estimator is defined as the difference between the residual error with an estimate of the input vector and the residual error with the optimal LS estimator, i.e.,(1)R(x;ΔH,Δy)≜‖(y+Δy)−(H+ΔH)x‖2−minw∈Cn⁡‖(y+Δy)−(H+ΔH)w‖2.By making such a regret definition, we force our estimator not to construct the symbol vector according to the worst possible scenario considering that it may be too conservative. Instead, we define the regret of any estimator by the difference in the estimation performances of that estimator and the “smartest” estimator knowing both data matrix and output vector in hindsight, so that we achieve a tradeoff between robustness and estimation performance.We emphasize that the regret defined in (1) is completely different than the regret formulation introduced in [18,29]. In (1), the uncertainty is on the data matrix where the desired data vector x is completely unknown, unlike [18,29]. We emphasize that we use the residual error‖(y+Δy)−(H+ΔH)x‖2instead of the estimation error‖xˆ−x‖since the estimation error directly depends on the vector x and cannot be used in the regret formulation since x is assumed to be unknown in the presence of data uncertainties. Moreover, in our formulation, the estimatexˆis not constrained to be linear unlike [18,29] since our regret formulation is well-defined without any limitations on the estimatedxˆ.In the next sections, the proposed approaches to the robust LS estimation problems are provided. We first introduce the regret based unstructured LS estimation method. We next present the unstructured regularized LS estimation approach in which the worst-case regret is optimized. Finally, we investigate the structured LS estimation approach.In this section, we provide a novel robust unstructured LS estimator based on a certain minimax criterion. We consider the most generic estimation problem(2)minx∈Cn⁡max‖ΔH‖≤δH,‖Δy‖≤δY⁡R(x;ΔH,Δy),whereR(x;ΔH,Δy)is defined as in (1). Now considering the second term in (1), we defineH˜≜H+ΔH,y˜≜y+Δy, whereH˜is a full rank matrix, and denote the estimation performance of the optimal LS estimator for some givenH˜andy˜byf(H˜,y˜)≜minw∈Cn⁡‖y˜−H˜w‖2.Since we consider an unconstrained minimization over w, we have [10](3)w⁎≜argminw∈Cn‖y˜−H˜w‖2=H˜+y˜,as the optimal data vector minimizing the residual error. Then we havef(H˜,y˜)=‖y˜−H˜w⁎‖2=(y˜−H˜w⁎)H(y˜−H˜w⁎)=y˜H(y˜−H˜w⁎)=y˜HP˜y˜,where the third line follows fromH˜HH˜w⁎=H˜Hy˜[10] andP˜≜I−H˜H˜+is the projection matrix of the space perpendicular to the range space ofH˜. If we use the Taylor series expansion based on Wirtinger calculus [33] forf(H˜,y˜)aroundH˜=Handy˜=y, then(4)f(H˜,y˜)=f(H,y)+2Re{Tr(∇f(H˜,y˜)|H˜=H,y˜=yH[ΔHΔy])}+O(‖[ΔHΔy]‖2).Note that the first order Taylor approximation is introduced in order to obtain a tractable solution. Clearly, the effect of using this approximation vanishes as‖[ΔHΔy]‖decreases and for distortions with larger‖[ΔHΔy]‖, one can easily use higher order approximations instead. However, we observe through our simulations that even for relatively large perturbations, a satisfactory performance is obtained using this approximation.We now introduce the following lemma in order to obtain the first order Taylor approximation in (4) in a closed form.Lemma 1LetH˜=H+ΔHbe a full rank matrix andy˜=y+Δy, whereH˜∈Cm×nandy˜∈Cm. Then definingf(H˜,y˜)≜y˜HP˜y˜, whereP˜≜I−H˜H˜+, we have∂f(H˜,y˜)∂H˜|H˜=H,y˜=y=−Py(H+y)H,and∂f(H˜,y˜)∂y˜|H˜=H,y˜=y=Py,whereP≜I−HH+.Proof of Lemma 1SinceH˜is full rank andm≥n, the pseudo-inverse ofH˜is found by [33]H˜+≜(H˜HH˜)−1H˜H.Hence, we have [33](5)D=∂∂H˜(y˜Hy˜−y˜HH˜(H˜HH˜)−1H˜Hy˜)|H˜=H,y˜=y=H(HHH)−1HHyyHH(HHH)−1−yyHH(HHH)−1=HH+y(H+y)H−y(H+y)H=−Py(H+y)H,and(6)b=∂∂y˜(y˜Hy˜−y˜HH˜(H˜HH˜)−1H˜Hy˜)|H˜=H,y˜=y=Py,where the last line of the equality follows sinceHH+is a symmetric matrix according to the definition of the pseudo-inverse operation. This concludes the proof of Lemma 1.  □Now turning our attention back to (4), we denoteD≜∂f(H˜,y˜)∂H˜|H˜=H,y˜=y,andb≜∂f(H˜,y˜)∂y˜|H˜=H,y˜=y,where we emphasize that the closed form definitions of D and b can be obtained from Lemma 1. We then approximate (4) and obtain the first order Taylor approximation as follows(7)f(H˜,y˜)≈f(H,y)+2Re{Tr([Db]H[ΔHΔy])}=κ+2Re{(vec(D)Hvec(ΔH)+bHΔy)}=κ+dHΔh+ΔhHd+bHΔy+ΔyHb,whereκ≜f(H,y),d≜vec(D), andΔh≜vec(ΔH). Hence we can approximate the regret in (1) as follows(8)R(x;ΔH,Δy)≈‖y˜−H˜x‖2−(κ+dHΔh+ΔhHd+bHΔy+ΔyHb).In the following theorem, we illustrate how the optimization (or equivalently estimation) problem in (8) can be put in an SDP form.Theorem 1LetH∈Cm×nandy∈Cmbe the estimates of the data matrix and the output vector, respectively, both having deterministic additive perturbationsΔH≤δHandΔy≤δY, respectively, i.e.,H˜=H+ΔHandy˜=y+Δy, whereH˜is the full rank data matrix,y˜is the output vector, andm≥n. Then the problem(9)minx∈Cn⁡max‖ΔH‖≤δH,‖Δy‖≤δY⁡R(x;ΔH,Δy),whereR(x;ΔH,Δy)is defined as in(8), is equivalent to solving the following SDP problem(10)min⁡γsubject toτ1≥0,τ2≥0,and[γ+κ−τ1−τ2(y−Hx)HδYbHδHdHy−HxI−δYIδHXδYb−δYIτ1I0δHdδHXH0τ2I]≥0,whereXis them×mnmatrix defined asX≜xH⊗I.The proof of Theorem 1 is provided in Appendix A.Remark 1In the proof of Theorem 1, we use Proposition 1 that relies on the lossless S-procedure. However, S-procedure is lossless with two constraints when the corresponding two quadratic (Hermitian) forms on the complex linear space [34]. However, classical S-procedure for quadratic forms is, in general, lossy with two constraints in the real case [35]. Hence, Theorem 1 cannot be extended for real linear space.Now we can consider two important corollaries of Theorem 1. First, a special case of Theorem 1 in which the uncertainty is only in the data matrix. We emphasize that the perturbation errors only in the data matrix are also common in a wide range of real life applications [10]. Here, we can define the regret as follows(11)R(x;ΔH)≜‖y−H˜x‖2−minw∈Cn⁡‖y−H˜w‖2,and similar to the previous case, we calculate the optimal estimation performance under a given uncertainty boundf(H˜)≜minw∈Cn⁡‖y−H˜w‖2≈κ+2Re{Tr(∇f(H˜,y)|H˜=HHΔH)}=κ+2Re{vec(DH)vec(ΔH)}=κ+dHΔh+ΔhHd.Hence we approximate the regret in (11) as follows(12)R(x;ΔH)≈‖y−H˜x‖2−(κ+dHΔh+ΔhHd).Corollary 1LetH∈Cm×nandy∈Cmbe the estimates of the data matrix and the output vector, respectively, wherem≥n. Suppose there is a bounded uncertainty on the full rank data matrixH˜, i.e.,H˜=H+ΔH,‖ΔH‖≤δH. Then the problem(13)minx∈Cn⁡max‖ΔH‖≤δH⁡R(x;ΔH),whereR(x;ΔH)is defined as in(12), is equivalent to solving the following SDP problem(14)min⁡γsubject toτ≥0and[γ+κ−τ(y−Hx)HδHdy−HxIδHXδHdδHXHτI]≥0.Outline of the proof of Corollary 1The proof of Corollary 1 can be explicitly derived from the proof of Theorem 1 by simply settingδY=0andτ1=0, hence is omitted.  □Second, we consider another special case of Theorem 1 in which the uncertainty is only in the output vector. We emphasize that similar to the previous case, this one is also a common case in a wide range of real-life applications [10], and studied under a similar framework in [18]. Here, we can define the regret as follows(15)R(x;Δy)≜‖y˜−Hx‖2−minw∈Cn⁡‖y˜−Hw‖2,and similar to the previous case, we calculate the optimal also performance under a given uncertainty boundf(y˜)≜minw∈Cn⁡‖y˜−Hw‖2≈κ+2Re{Tr(∇f(H,y˜)|y˜=yHΔy)}=κ+2Re{bHΔy}=κ+bHΔy+ΔyHb.Hence we approximate the regret in (15) as follows(16)R(x;Δy)≈‖y˜−Hx‖2−(κ+bHΔy+ΔyHb).Corollary 2LetH∈Cm×nandy∈Cmbe the estimates of the data matrix and the output vector, respectively, wherem≥n. Suppose there is a bounded uncertainty on the output vectory˜, i.e.,y˜=y+Δy,‖Δy‖≤δY. Then the problem(17)minx∈Cn⁡max‖Δy‖≤δY⁡R(x;Δy),whereR(x;Δy)is defined as in(16), is equivalent to solving the following SDP problem(18)min⁡γsubject toτ≥0and[γ+κ−τ(y−Hx)HδYbHy−HxI−δYIδYb−δYIτI]≥0.Outline of the proof of Corollary 2The proof of Corollary 2 can be explicitly derived from the proof of Theorem 1 by simply settingδH=0andτ2=0, hence is omitted. □Remark 2Corollaries 1 and 2 follow from the proof of Theorem 1, which relies on the lossless S-procedure. Under the frameworks presented in Corollaries 1 and 2, one can safely extend the same conclusions for the real case also, since S-procedure is lossless for quadratic forms with one constraint both in complex and real spaces [36,37].In this section, we introduce a worst-case regret optimization approach to solve the regularized LS estimation problem in [32]. The regret for not using the optimal regularized LS estimator is defined by(19)R(x;ΔH,Δy)≜{‖y˜−H˜x‖2+μ‖x‖2}−minw∈Cn⁡{‖y˜−H˜w‖2+μ‖w‖2},whereμ>0is the regularization parameter. We emphasize that there are different approaches to choose μ, however, for the focus of this paper, we assume that it is already set before the optimization so that these methods can be readily incorporated in our framework. Hence, we solve the regularized LS estimation problem for an arbitraryμ>0and note that we have already covered theμ=0case in Section 3.1.Similar to the previous case, we denote the estimation error of the optimal LS estimator for some estimated data matrix H and output vector y byf(H,y)≜minw∈Cn⁡‖y−Hw‖2+μ‖w‖2=‖P−1y‖2=yHP−1y,whereP≜I+μ−1HHH. Considering the first order Taylor series expansion based on Wirtinger calculus [33] forf(H˜,y˜)aroundH˜=Handy˜=yf(H˜,y˜)≈κ+2Re{Tr(∇f(H˜,y˜)|H˜=H,y˜=yH[ΔHΔy])},f(H˜,y˜)=κ+dHΔh+ΔhHd+bHΔy+ΔyHb,whered≜vec(DH),Δh≜vec(ΔH),(20)D≜∂f(H˜,y˜)∂H˜|H˜=H,y˜=y=−P−1yyHP−1H,andb≜∂f(H˜,y˜)∂y˜|H˜=H,y˜=y=P−1y,where the last line follows since P is symmetric. Hence we can approximate the regret in (19) as follows(21)R(x;ΔH,Δy)≈‖y˜−H˜x‖2+μ‖x‖2−(κ+dHΔh+ΔhHd+bHΔy+ΔyHb),similar to (8). In the following theorem, we illustrate how the optimization problem in (21) can be put in an SDP form.Theorem 2LetH∈Cm×nandy∈Cmbe the estimates of the data matrix and the output vector, respectively, both having deterministic additive perturbationsΔH≤δHandΔy≤δY, respectively, i.e.,H˜=H+ΔHandy˜=y+Δy, whereH˜is the full rank data matrix,y˜is the output vector, andm≥n. Then the problem(22)minx∈Cn⁡max‖ΔH‖≤δH,‖Δy‖≤δY⁡R(x;ΔH,Δy),whereR(x;ΔH,Δy)is defined as in(21), is equivalent to solving the following SDP problem(23)min⁡γsubject toτ1≥0,τ2≥0,and[γ+κ−τ1−τ2(y−Hx)HxHδYbHδHdHy−HxI0−δYIδHXx0μI00δYb−δYI0τ1I0δHdδHXH00τ2I]≥0.Proof of Theorem 2The proof of Theorem 2 follows similar lines to the proof of Theorem 1, hence is omitted here. □Remark 3Under the framework introduced in this section, one can straightforwardly obtain the corollaries similar to Corollaries 1 and 2 by considering cases in which the uncertainty is either only on the data matrix or only on the output vector, i.e.,δY=0andδH=0cases, respectively. The derivations follow similar lines to Corollaries 1, 2 and Theorem 2, hence is omitted. However, similar results can be readily derived from the result in Theorem 2 with suitable changes in the SDP formulations.There are various communication systems where the data matrix and the perturbation on it have a special structure such as Toeplitz, Hankel, or Vandermonde [9,15]. Incorporating this prior knowledge into the estimation framework could improve the performance of the regret based minimax LS estimation approach [9,15]. Hence, in this section, we investigate a special case of the problem in (2), where the associated perturbations for the data matrix H and the output vector y have special structures. The structure on the perturbations is defined as follows(24)ΔH=∑i=1pαiHi,and(25)Δy=∑i=1pβiyi,whereHi∈Cm×n,yi∈Cm, and p are known butαi,βi∈C,i=1,…,p, are unknown. However, the bounds on the norm ofα≜[α1,…,αp]Handβ≜[β1,…,βp]Hare provided as‖α‖≤δαand‖β‖≤δβ, whereδα,δβ≥0. We emphasize that this formulation can represent a wide range of constraints on the structure of perturbations of the data matrix and the output vector such as Toeplitz and Hankel [9,10]. Our aim is to solve the following optimization problemminx∈Cn⁡max‖α‖≤δα,‖β‖≤δβ⁡R(x;ΔH,Δy),where(26)R(x;ΔH,Δy)≜‖y˜−H˜x‖2−minw∈Cn⁡‖y˜−H˜w‖2,(27)H˜≜H+ΔH=H+∑i=1pαiHi,(28)y˜≜y+Δy=y+∑i=1pβiyi.After following similar lines to Section 3.1, and introducing the first order Taylor approximation tof(H˜,y˜)aroundα=0andβ=0, we obtain(29)f(H˜,y˜)≈κ+2Re{Tr(∇f(H˜,y˜)|α=0,β=0H[αβ])},wheref(H˜,y˜)=y˜HP˜y˜andP˜=I−H˜H˜+. We next introduce the following lemma to calculate the first order Taylor approximation in (29) in a closed form.Lemma 2LetH˜=H+ΔHbe a full rank matrix andy˜=y+Δy, whereH˜∈Cm×n,y˜∈Cm,ΔHandΔyare defined as in(24)and(25), respectively. Then denotingf(H˜,y˜)≜y˜HPy˜, whereP˜≜I−H˜H˜+, we have(30)∂f(H˜,y˜)∂α|α=0,β=0=[−yHPHH1H+y,…,−yHPHHpH+y]H,and(31)∂f(H˜,y˜)∂β|α=0,β=0=[yHPy1,…,yHPyp]H,whereP≜I−HH+.Proof of Lemma 2Note that the derivative off(H˜,y˜)is taken with respect to[αβ], hence we can use the Chain Rule to calculate the derivatives by using the results we have obtained in Lemma 1.First, we consider the derivative off(H˜,y˜)with respect toαi,i=1,…,p, i.e.,di≜∂f(H˜,y˜)∂αi|α=0,β=0=Tr((∂f(H˜,y˜)∂H˜)H∂H˜∂αi|α=0,β=0)=Tr(−H+yyHPHHi)=−yHPHHiH+y,where the last line follows from the cyclic property of the trace operator.Similarly, we next consider the derivative off(H˜,y˜)with respect toβi,i=1,…,p, i.e.,bi≜∂f(H˜,y˜)∂βi|α=0,β=0=Tr((∂f(H˜,y˜)∂y˜)H∂y˜∂βi|α=0,β=0)=yHPyi.This concludes the proof of Lemma 2.  □Now turning our attention back to (29), we denoted≜∂f(H˜,y˜)∂α|α=0,β=0,andb≜∂f(H˜,y˜)∂β|α=0,β=0,where we emphasize that the closed form definitions of d and b can be obtained from Lemma 2. We then approximate (29) and obtain the first order Taylor approximation as followsf(H˜,y˜)≈κ+dHα+αHd+bHβ+βHb.Therefore, we can approximate the regret in (26) as follows(32)R(x;ΔH,Δy)≈‖y˜−H˜x‖2−(κ+dHα+αHd+bHβ+βHb).In the following theorem, we illustrate how the optimization problem in (32) can be put in an SDP form.Theorem 3LetH,H1,…,Hp∈Cm×n,y,y1,…,yp∈Cm,δH,δY≥0,m≥n, whereH˜is the full rank data matrix defined as in(27),y˜is the output vector defined as in(28), with the corresponding estimatesHandy, respectively. Then the problem(33)minx∈Cn⁡max‖α‖≤δα,‖β‖≤δβ⁡R(x;ΔH,Δy),whereR(x;ΔH,Δy)is defined as in(32), is equivalent to solving the following SDP problem(34)min⁡γsubject toτ1≥0,τ2≥0,and[γ+κ−τ1−τ2(y−Hx)HδαdHδβbHy−HxI−δαGδβQδαd−δαGHτ1I0δβbδβQH0τ2I]≥0,whereG≜[H1x,…,Hpx]andQ≜[y1,…,yp].Proof of Theorem 3The proof of Theorem 3 follows similar lines to the proof of Theorem 1, hence is omitted here. □Remark 4Under the framework introduced in this section, one can straightforwardly obtain the corollaries similar to Corollaries 1 and 2 by considering cases in which the uncertainty is either only on the data matrix or only on the output vector, i.e.,δβ=0andδα=0cases, respectively. The derivations follow similar lines to Corollaries 1, 2 and Theorem 3, hence is omitted. However, similar results can be readily derived from the result in Theorem 3 with suitable changes in the SDP formulations.Remark 5The proofs of Theorem 2 and Theorem 3 follow from the results of Theorem 1, which relies on the lossless S-procedure. However, S-procedure is lossless with two constraints when the corresponding two quadratic (Hermitian) forms on the complex linear space [34]. However, classical S-procedure for quadratic forms is, in general, lossy with two constraints in the real case [35]. Hence, Theorem 2 and Theorem 3 cannot be extended for real linear space. On the other hand, under the frameworks described in Remark 3 and Remark 4, one can safely extend the same conclusions for the real case also, since S-procedure is lossless for quadratic forms with one constraint both in complex and real spaces [36,37].We provide numerical examples in different scenarios in order to illustrate the merits of the proposed algorithms. In the first set of the experiments, we randomly generate a data matrix of sizem×n, and an output vector of sizem×1, which are normalized to have unit norms. Then, we generate 1000 random perturbationsΔH,Δy, where‖ΔH‖≤δH,‖Δy‖≤δY,m=5,n=3, andδH=δY=1.2. Here, we label the algorithm in Theorem 1 as “rgrt-LS”, the robust LS algorithm of [9] as “rbst-LS”, the total LS algorithm [9] as “TLS”, and finally the LS algorithm tuned to the estimates of the data matrix and the output vector as “LS”, where we directly usexˆ=H+y.For each algorithm and for each random perturbation, we find the correspondingxˆand calculate the error‖H˜xˆ−y˜‖2. After we calculate the errors for each algorithm and for all random perturbations, we plot the corresponding sorted errors in ascending order in Fig. 1for 1000 perturbations. Since the rbst-LS algorithm optimizes the worst-case residual error with respect to worst possible disturbance, it usually yields the smaller worst-case residual error among all algorithms for these simulations. On the other hand, since the LS algorithm directly uses the estimates, it usually yields the smaller residual error when the perturbations on the data matrix and the output vector are significantly small.These results can be observed in Fig. 1, where in one extreme, the largest residual errors are observed as 2.9762 for the TLS estimator, 2.2557 for the LS estimator, 1.9275 for the rbst-LS estimator, and 1.9325 for the rgrt-LS estimator. In the other extreme, i.e., when there is almost no perturbation, the smallest estimation errors are observed as 0.3035 for the LS estimator, 0.4036 for the TLS estimator, 0.8727 for the rbst-LS estimator, and 0.6387 for the rgrt-LS estimator. While the LS estimator can be preferable when there is relatively smaller perturbations and the rbst-LS estimator can be preferable when there is significantly higher perturbations, the introduced algorithm provides a tradeoff between these algorithms and achieve a significantly smaller average error performance. The average residual error of the rgrt-LS estimator is observed as 1.1928, whereas this value is 1.2180 for the LS estimator, 1.2708 for the rbst-LS estimator, and 1.3826 for the TLS estimator. Hence, the rgrt-LS estimator is not only robust but also efficient in terms of the average error performance compared to its well-known alternatives. Owing to the competitive formulation of our estimators, we achieve such average performance gains especially when the perturbations are moderate.In the second set of experiments, we illustrate the performances of the proposed algorithms under variousδHandδYvalues. For these experiments, we generate 2000 random perturbationsΔH,Δy, where‖ΔH‖≤δH,‖Δy‖≤δY,m=5,n=3for different perturbation bounds and compute the averaged error over 2000 trials for the rgrt-LS, LS, rbst-LS, and TLS algorithms. In Fig. 2, we present the averaged residual errors of these algorithms for different values of perturbation bounds, i.e.,δ=δH=δY∈[0.5,1]. We observe that the proposed rgrt-LS algorithm has the best average residual error performance over different perturbation bounds compared to the LS, the rbst-LS and the TLS algorithms. Furthermore, in Fig. 3and Fig. 4, we present the averaged residual errors of these algorithms for different perturbation bounds, i.e., whenδH≠δY. Particularly, in Fig. 3, we setδH∈[0.5,1],δY=1and in Fig. 4, we setδH=1,δY∈[0.5,1].As can be observed from Fig. 2, as the perturbation bounds increase, the performances of the LS and the TLS estimators significantly deteriorate, whereas the rgrt-LS estimator provides an excellent performance. The residual error of the rbst-LS estimator, on the other hand, slightly increases as the perturbation bounds increase, i.e., it is the most robust algorithm against the perturbations due to its highly conservative nature. Yet, the performance of this estimator is significantly inferior to the rgrt-LS estimator. Furthermore, the rgrt-LS estimator provides the best performance under differentδHandδYvalues. Particularly, in Fig. 3, we observe a similar behavior to the one in Fig. 2, where our algorithm provides a robust performance while also providing the smallest residual error (especially for highδH). On the other hand, in Fig. 4, we observe that the performance of rgrt-LS estimator is less sensitive to the changes inδYcompared to the rbst-LS, LS, and TLS estimators.In the next experiment, we examine a system identification problem [15], which can be formulated asH0x=y0, whereH=H0+Wis the observed noisy Toeplitz matrix andy=y0+wis the observed noisy output vector. Here, the convolution matrix H (which is Toeplitz) constructed from h which is selected as a random sequence of ±1's. We then generate 1000 random structured perturbations forH0andy0, where‖α‖≤0.75‖H0‖, and plotted the sorted estimation errors in ascending order in Fig. 5.The average residual errors are observed as 1.1155 for the structured regret LS estimator “str-rgrt-LS” of Remark 4, 1.1807 for the structured robust LS algorithm “str-rbst-LS”, 1.1138 for the LS estimator, and 1.2576 for the structured least squares bounded data uncertainties estimator “SLS-BDU” of [15]. Therefore, we observe that the str-rgrt-LS algorithm yields a smaller average residual error with respect to other robust estimators and achieves the average performance of the LS estimator. In addition, we observe that the maximum residual errors are observed as 1.5554 for the str-rgrt-LS estimator, whereas it is 1.6659 for the LS estimator. Hence, the introduced algorithm can be used to obtain robustness without significant losses in the average estimation performance unlike the conventional robust estimation methods. Nevertheless, we emphasize that for a structured system, the performance of these algorithms are highly sensitive to the structures of the matrices and the vectors. If the perturbation bound is quite high, the robustness may not be preserved under large perturbations.In the fourth experiment, i.e., in Fig. 6, we provide errors sorted in ascending order for the algorithm in Theorem 2 as “rgrt-reg-LS”, for the robust regularized LS algorithm in [16] as “rbst-reg-LS” and finally for the regularized LS algorithm as “reg-LS” [10], where the experiment setup is the same as in the first experiment except the perturbation bounds are set to 0.65 and the regularization parameter is chosen asμ=0.5. In Fig. 6, we observe that the robustness and the performance tradeoff (between the rbst-reg-LS and the reg-LS algorithms) of the introduced rgrt-reg-LS algorithm.When there is small perturbations on the data matrix and the output vector, i.e., in the best-case scenario, the residual error of the reg-LS estimator is 0.1045, whereas it is 0.2416 for the rgrt-reg-LS estimator and 0.4282 for the rbst-reg-LS estimator. As can be observed from Fig. 6, for higher perturbations, the performance of the reg-LS estimator significantly deteriorates, whereas the rgrt-reg-LS and rbst-reg-LS algorithms provide a robust performance. On the other hand, the rgrt-reg-LS estimator significantly outperforms the rbst-reg-LS estimator in terms of the average error performance and achieves even a more desirable error performance compared to the reg-LS estimator. The average residual errors are calculated as 0.9059 for the rgrt-reg-LS estimator, 0.9177 for the reg-LS estimator, and 1.0316 for the rbst-reg-LS estimator. This experiment illustrates the sensitivity of the reg-LS estimator to the perturbations. On the other hand, the rgrt-reg-LS and rbst-reg-LS estimators provides more robust performances compared to the reg-LS estimator. Yet, the highly pessimistic nature of the rbst-reg-LS estimator deteriorates its estimation performance and yields an unacceptable performance. Our algorithm, on the other hand, not only yields a robust performance compared to the reg-LS estimator but also does not cause any average performance degradations unlike the conventional robust estimation methods.Finally, we illustrate the possible applications of our algorithm into different frameworks. Particularly, we consider the channel equalization problem and illustrate the bit error rate (BER) performance of our algorithm with respect to its well-known alternatives in the literature as follows.In these simulations, we define the signal-to-noise ratio (SNR) as followsSNR=20log⁡(‖x‖δ),where‖H‖=1andlog⁡(⋅)is the common (i.e., base 10) logarithm. For a given SNR, we generate 1 000 000 symbol vectors of x (having length 2) from a binary alphabet and 1 000 000 estimates of the (MIMO) channel matrix H (sized3×2) both having unit norms, randomly. For every symbol vector and channel estimate couple, we randomly generate perturbationsΔHandΔy, calculate the corresponding perturbed output vector, and feed this information to the algorithms. We quantize the estimate of the symbol vectorxˆand consider the number of incorrect bits as the BER (i.e., we consider the BER rather than the symbol error rate).In Fig. 7, we provide the BERs for various SNRs. We observe that the proposed algorithm outperforms its competitors in terms of equalization performance and successfully reconstructs the transmitted bits. While Fig. 7 illustrates the BER of the proposed algorithms averaged over a huge number of channel uses, we also illustrate the robustness of our algorithm over small number of channel uses in Fig. 8and Fig. 9. In these experiments, we perform 100 independent trials in each of which 10 000 symbol vectors and channel matrix estimates are generated and sent over the channel as in the previous experiment forSNR=20andSNR=25, respectively.In Fig. 8 and Fig. 9, we observe that our algorithm not only provides a superior averaged performance with respect to its well-known alternatives but also provides a robust performance. The conventional robust LS estimators provide unsatisfactory results since these algorithms adapt themselves to the worst-case scenario. However, the rgrt-LS estimator has a significantly smaller BER compared to the rbst-LS and TLS estimators, since our algorithm does not tune itself to the worst possible perturbation, but considers the worst possible regret. Particularly, when the perturbation on the estimates are relatively small, our algorithm provides significant performance improvements compared to the conventional methods as can be seen in Fig. 8 and Fig. 9.

@&#CONCLUSIONS@&#
