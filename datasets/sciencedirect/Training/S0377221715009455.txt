@&#MAIN-TITLE@&#
Zero-inefficiency stochastic frontier models with varying mixing proportion: A semiparametric approach

@&#HIGHLIGHTS@&#
We propose a semiparametric zero-inefficiency stochastic frontier model.We develop a backfitting local maximum likelihood estimation procedure.The proposed estimator is shown to be consistent and asymptotically normal.Monte Carlo results show that the proposed estimator performs well in finite sample.We examine the effects of total assets on the proportion of US banks being efficient.

@&#KEYPHRASES@&#
Zero-inefficiency,Varying proportion,Semiparametric approach,Backfitting local maximum likelihood,Sieve likelihood ratio statistics,

@&#ABSTRACT@&#
In this paper, we propose a semiparametric version of the zero-inefficiency stochastic frontier model of Kumbhakar, Parmeter, and Tsionas (2013) by allowing for the proportion of firms that are fully efficient to depend on a set of covariates via unknown smooth function. We propose a (iterative) backfitting local maximum likelihood estimation procedure that achieves the optimal convergence rates of both frontier parameters and the nonparametric function of the probability of being efficient. We derive the asymptotic bias and variance of the proposed estimator and establish its asymptotic normality. In addition, we discuss how to test for parametric specification of the proportion of firms that are fully efficient as well as how to test for the presence of fully inefficient firms, based on the sieve likelihood ratio statistics. The finite sample behaviors of the proposed estimation procedure and tests are examined using Monte Carlo simulations. An empirical application is further presented to demonstrate the usefulness of the proposed methodology.

@&#INTRODUCTION@&#
One drawback or restrictive assumption of estimating productivity and efficiency through stochastic frontier analysis originally proposed by Aigner, Lovell, and Schmidt (1977), Meeusen and van den Broeck (1977), (see also, Ondrich & Ruggiero, 2001) was recently pointed out by Kumbhakar, Parmeter, and Tsionas (2013, KPT hereafter) and Rho and Schmidt (2015, RS hereafter). The assumption that, a priori, all firms are inefficient and their inefficiency is modeled through a continuous density was shown to have considerable implications. When some firms are, in fact, fully efficient, a fact that we cannot preclude on prior grounds, applying stochastic frontier analysis with the familiar distributions (half-normal, exponential, etc.) results in biased estimates of inefficiency.To overcome this draw back, KPT (and independently by RS) proposed a new model for which they call “zero-inefficiency stochastic frontier (ZISF) model, that allows for the inefficiency term to have mass at zero with certain probability, π and a continuous distribution, with probability,1−π. In essence, their model takes a special form of the latent class model considered by, among others, Ivaldi, Monier-Dilhan, and Simioni (1995), Caudill (2003), Orea and Kumbhakar (2004) and Greene (2005). The interesting feature of the proposed model is that only non-existence and existence of inefficiency differs but not the frontier itself. KPT and RS also extend the ZISF model to allow for π to depend on a set of covariates via a logit or a probit function. Estimation of the model parameters can be carried out by using either standard maximum likelihood or E-M algorithm (see RS).In this paper we use a non-parametric formulation for the probability as a function of covariates, π(.) which does not impose restrictive assumptions on what determines full efficiency. The issue is important as misspecification of the parametric form of probability has implication for estimating technical efficiency and, more specifically, which firms are fully efficient. Although functional forms for production or cost functions are more or less established in applied studies, this is not so for the functional form of the probability of firms being fully efficient,π(.). This is quite important since the functional form of E(y|X) depends on the functional form of π(.) and the covariates.To accommodate for the unknown probability of firms being efficient function in the estimation, we develop an iterative backfitting local maximum likelihood procedure which is fairly simple to compute in practice. We also derive the necessary asymptotic theory of the proposed estimator. Specifically, we derive the asymptotic bias and variance of the proposed estimator and established its asymptotic normality. In addition, we discuss how to test for parametric specification of the probability function of firms that are fully efficient as well as how to test for the presence of fully efficient firms, based on the bootstrap sieve likelihood ratio statistics (Fan, Zhang, & Zhang, 2001).We use both Monte Carlo experiments and real-world data from U.S. banks to illustrate the applicability of the new model, and compare the results with the standard stochastic frontier models as well as the model proposed by KPT where the probability is a parametric function of covariates. Our Monte Carlo results indicate that the proposed estimation methods as well as the bootstrap sieve likelihood ratio statistics perform well in samples of the size typically used in applied econometric studies.The rest of the article is organized as follows. Section 2 introduces the semiparametric zero-inefficiency stochastic frontier model. Section 3 derives the backfitting local maximum likelihood estimator and discusses construction of inefficiency scores. Section 4 establishes the asymptotic properties of the proposed estimator. Hypothesis testing for the parametric specification of probability of firms being fully efficient as well as testing for the presence of fully inefficient firms are discussed in Section 5. Monte Carlo simulations are presented in Section 6, while Section 7 provides an empirical application to the U.S. banking industry. Section 8 provides concluding remarks. Proofs of the theorems are gathered in Appendix A.We consider the following semiparametric version of the zero-inefficiency stochastic frontier (SP-ZISF) model of KPT:(1)yi={xi′β+viwithprobablityπ(zi)xi′β+vi+suiwithprobablilty1−π(zi),where yiis a scalar representing output of firm i, xiis a d × 1 vector of inputs, viis random noise, uiis one-sided random variable representing technical inefficiency,s=+1for cost frontier ands=−1for production frontier, π(.) is an unknown smooth function representing the proportion of firms that are fully efficient and ziis a q × 1 vector of covariates which influence whether a firm is inefficient or not; and the zimay or may not be a subset of xi. Note that in (1) the technology is the same for both regimes, and the composed error isvi−ui(1−1{ui=0})where 1{.} is an indicator function andP(1{ui=0})=π(zi). For illustration purpose, we focus mainly on the production frontier. Cost frontier can be handled in the same way by replacing the negative sign on uiby a positive sign. In addition, to simplify our discussion, we consider univariate z. Extension to multivariate z is straightforward but at the expenses of increasing notational complexity and the “curse of dimensionality” problem.Under the standard stochastic frontier framework, there is no identification issue arise since the parameterσu2, the variance of uiis identified through the moment restrictions on the composed errorsɛi=vi−ui. However, in the context of model (1), we have an additional parameter π(.) which can be identified only if there are non-zero observations in each class. In addition, as KPT and RS point out, whenσu2→0, π(.) is not identified since the two classes become indistinguishable. Conversely, when π(.) → 1 for a given z,σu2is not identified. In fact, when a data set contains little inefficiency, one might expect thatσu2and π(.) to be imprecisely estimated, since it is difficult to identify whether little inefficiency is due to π(.) is close to 1 orσu2is close to zero. However, this identification issue is more relevant to the testing problem of all firms are efficient (or inefficient). We will return to the discussion of this hypothesis testing as well as other hypothesis testing problems in the later section. For the present discussion, we will assume thatσu2>0and 0 < π(.) < 1 so that all the parameters in model (1) are identified.To complete the specification of the model, let f(z) and f(y|x, z) denote the marginal density of z and the conditional density of y given x and z, respectively. In addition, we assume throughout the paper that f(y|x, z) is known and belongs to a class of parametric densities with parameter θ ∈ Θ⊂ℜkwhere k a positive integer and the function π(z): ℜq→ [0, 1] is a smooth function which is twice continuously differentiable.To make specific assumption regarding the conditional distribution of f(y|x, z), we follow the standard practice and assume thatvi|x,z∼i.i.d.N(0,σv2)andui|x,z∼i.i.d.|N(0,σu2)|, albeit other distributions such as exponential, truncated normal or gamma can also be considered for ui. The conditional probability density function ofɛi=vi−uiis given by(2)f(ɛ|x,z)=(π(z)σv)ϕ(ɛσv)+(1−π(z))[2σϕ(ɛσ)Φ(−ɛλσ)],whereσ2=σu2+σv2,λ=σu/σv, ϕ(.) and Φ(.) are the probability density (pdf) and cumulative distribution functions (CDF) of a standard normal variable, respectively. To avoid the non-negativity restrictions we make use of the following transformation:λ=exp(λ¯)=λ˜andσ2=exp(σ¯2)=σ˜2. Letθ=(β′,λ˜,σ˜2)′then it follows that the conditional pdf of y given x and z is(3)f(y|x,z)=(π(z)σ˜v)ϕ(y−x′βσ˜v)+(1−π(z))[2σ˜ϕ(y−x′βσ˜)Φ(−(y−x′β)λ˜σ˜)],and conditional log-likelihood is then given by(4)L*(π(zi),θ)=∑i=1nlogf(yi;π(zi),θ|x,z)From (4), it is clear that if π(z) is known and belongs a class of parametric function with finite dimensional parameter vector, then standard maximum likelihood (ML) estimator can be obtained by maxing (4) as discussed in KPT. However, π(z) is generally unknown in practice rendering the standard MLE infeasible. To make the MLE operational, we approximate the unknown function π(z) locally by a linear function, albeit in practice, one might wish to consider higher orders of local polynomials for π(z). For a given value of z0, and z in the neighborhood of z0, a Taylor series expansion of π(zi) at z0 givesπ(zi)≈π(z0)+π′(z0)(zi−z0)=a(z0)+b(z0)(zi−z0),where π′(z0) is the derivative of π(zi) evaluated at z0. Then, the conditional local log-likelihood function associated with (4) can be written as(5)L1(a(z0),b(z0),θ)=∑i=1n{logf(yi;a(z0),b(z0),θ|x,z)}Kh(zi−z0),whereKh(ξ)=h−1K(ξ/h)and K(.) is a kernel function and h is the appropriate bandwidth. Thus the conditional local log-likelihood depends on z. However, since the parameter θ does not depend on z, we suggest the following backfitting procedure which motivated by Huang and Yao (2012) for estimating semiparametric mixture regression models. Specifically, for a given value of z0, we first estimate π(.) locally by maximizing (5) with respect to a,  b and θ. Leta˜,b˜andθ˜be the solution to the maximization problem of (5), that is22In practice, the estimation is performed at a set of given z0 values. A simple approach is to setz0=z1,...,z0=znwhich yields a set ofπ˜(z0)values.(a˜,b˜,θ˜)=argmax(a,b,θ)L1(a,b,θ)Thenπ˜(z0)=a˜(z0)andθ˜(z0)=θ˜. Now note that the global parameter vector θ does not depend on z and since θ is estimated locally, it does not possess the usual parametricn-consistency. To preserve then-consistency and to improve the efficiency, given the estimate ofπ˜(z0), the parameter vector θ can be estimated globally by maximizing the following (global) log-likelihood function where we replace π(z0) with its estimateπ˜(z0)in (4)(6)L2(θ)=∑i=1nlogf(yi;π˜(zi),θ|x,z).Letθ^be the solution of maximizing (6). In the next section, we will show that, under certain regularity conditions,θ^retains itsn-consistency property. Given the estimates ofθ^and to improve efficiency, the function π(z0) can be obtained by maximizing the local likelihood function(7)L3(a(z0),b(z0),θ^)=∑n=1n{logf(yi;a(z0),b(z0),θ^|x,z0)}Kh(zi−z0)}.Letπ^(.)=a^(.)be the solution of maximizing (7). Finally,θ^andπ^(z)can further be improved by iterating until convergence. We will denote the finalπ^(z)andθ^as iterative backfitting local MLE.We summarize the above backfitting local ML estimation procedure with the following computational algorithm:Step 1:For eachzi,i=1,...,n, in the sample, maximize the conditional local log-likelihood (5) to obtain the estimate ofπ˜(zi). Note that if the sample size n is large, (5) could be performed on a random subsample Nswhere Ns≪ n to reduce the computational burden. Also, to ensure that the estimates of π(.) fall within the interval [0, 1], we reparameterizing the local linear parameters using logistic function.From Step 1, conditional onπ˜(zi), maximize the conditional global log-likelihood function (6) to obtainθ^.Conditional onθ^from Step 2, maximize the conditional local log-likelihood function (7) to obtainπ^(zi).Usingπ^(zi)repeat Step 2 and then Step 3 until the estimate ofθ^converges.RemarkFirst, in practice, one could stop at Step 3 to reduce the computational burden. However, iteration between Step 2 and Step 3 until convergence is highly recommended. Based on our limited experience, convergence is typically fast as it requires only a few iterations. Second, Step 1 requires specifications of the kernel function Kh(.) as well as bandwidth h. For the kernel function, an Epanechnikov or Gaussian function is a popular choice. As for the bandwidth selection, data driven methods such as cross-validation (CV) can be used (see for example Li & Racine, 2007). In our context, we use a likelihood version of CV which is given by(8)CV(h)=1n∑i=1nlogf(yi;π^(i)(zi),θ^(i)|x,z),whereπ^(i)(zi)andθ^(i)are the leave-one-out version of the backfitting local MLE described above. Third, it is important to note that, in semiparametric modeling, undersmoothing conditions (see Theorem 1 below) are typically required in order to obtainn−consistency for the global parameters. The optimal bandwidthh^selected by CV will be in the order ofn−1/5which does not satisfy the required undersmoothing conditions. However, a reasonable adjusted bandwidth which suggested by Li and Liang (2008) that satisfies the undersmoothing condition can be used, and it is given byh˜=h^×n−2/15=O(n−1/3). We will apply this adjusted bandwidth in our simulations and empirical application below.Finally, the iterative backfitting local MLE described in this section uses direct maximization of the log likelihood functions (5)–(7). An alternative approach is to use EM algorithm procedure. The main advantage of EM algorithm is that it is numerically stable and possesses the ascent property in the sense that when the sample size is large enough, each iteration raises the likelihood value (Greene, 2012; Huang & Yao, 2012). However, the main drawback of EM algorithm is that it requires extensive computation, especially for the model considers in this paper, and the convergence can be very slow. Huang and Yao (2012) give detail implementations of EM algorithms for normal mixture model which is very similar to our model. Interested readers are referred to their paper for more details.Follow the discussion of KPT, we can similarly consider several approaches to estimate firm-specific inefficiency. The first approach is based on the popular estimator of Jondrow, Lovell, Materov, and Schmidt (1982) where under our setting, the conditional density of u given ɛ isf(u|ɛ)={0withprobabilityπ(z)N+(μ*,σ*2)withprobability(1−π(z)),whereN+(.)denotes the truncated normal,μ*=−ɛσu2/σ2andσ*2=σu2σv2/σ2. Thus, the conditional mean of u givenɛ=y−x′βis:(9)E(u|ɛ,z)=(1−π(z))σλ1+λ2[ϕ(−λɛ/σ)Φ(−λɛ/σ)−λɛσ].A point estimator of individual inefficiency score could be obtained by replacing the unknown parameters in (9) by their estimates and ɛ byɛ^(x)=y−x′β^.The second approach is to use the modal estimator which defined as(10)M(u|ɛ,z)=df(u|ɛ,z)du=0,and (10) is known to have a zero at the value ofu=μ*whenever ɛ < 0, and zero otherwise. Hence, multiplying μ* by(1−π(z))yields the modal estimator.The final approach is to construct the posterior estimates of inefficiencyu˜i. To do this, letpi*(.)denotes the posterior estimate of the probability of being fully efficient where(11)pi*(z)=(π^(z)/σ^v)ϕ(ɛ^iλ^i/σ^v)(π^(z)/σ^v)ϕ(ɛ^iλ^i/σ^v)+(1−π^(z))(2/σ^)ϕ(ɛ^iλ^i/σ^)Φ(−ɛ^iλ^i/σ^).Then the posterior estimate of inefficiency can be defined asu˜i=(1−pi*(z))u^iwhereu^iis the estimated of inefficiency based on (9) or (10). KPT provide an intuitive explanation for why the estimator given in (11) would be particularly helpful for researchers and regulators in the merger case to determine the probability of a specific firm or a group firms in the industry is to being fully efficient.In this section, we derive the sampling property of the proposed backfitting local MLEπ^(z)andθ^=(β^′,σ^2,λ^)′. In particular, we will show that the backfitting estimatorθ^isn−consistent and follows an asymptotic normal distribution. In addition, we also provide the asymptotic bias and variance of the estimatorπ^(z), and show that asymptotically, it has smaller variance compared toπ˜(z). To this end, let us define the following additional notations.Letγ(z)=(π(z),θ′)′andℓ(γ(z),x,y)=logf(y|x,γ(z)). Defineqγ(γ(z),x,y)=∂ℓ(γ(z),x,y)∂γ,qγγ(γ(z),x,y)=∂2ℓ(γ(z),x,y)∂γ∂γ′and the terms qθ,  qπ,  qθθ,  qθπand qππcan be defined similarly. In addition, letψ(w|z)=E[qπ(γ(z),x,y)|z=w],Iγγ(z)=−E[qγγ(γ(z),x,y)|z]Iθθ(z)=−E[qθθ(γ(z),x,y)|z]Iππ(z)=−E[qππ(γ(z),x,y)|z]Iπθ(z)=−E[qπθ(γ(z),x,y)|z]Finally, letμj=∫ujK(u)duandκj=∫ujK2(u)du. We make the following assumptions:Assumption 1The sample{(xi,yi,zi),i=1,…,n}is independently and identically distributed from the joint density f(x, y, z) which has continuous first derivative and positive in is support. The support for z, denoted byZ, is a compact subset of ℜ and f(z) > 0 for allz∈Z.Assumption 2The unknown function π(z) is twice continuously differentiable in its argument. Furthermore, π(z) > 0 hold for allz∈Z.Assumption 3The matrix Iγγ(z) and Iθθare positive definite.Assumption 4The kernel density function K(.) is symmetric, continuous and has bounded support.Assumption 5For someζ<1−r−1,n2ζ−1h→∞and E(z2r) < ∞.All the above assumptions are relatively mild and have been used in the mixture models and local likelihood estimation literature. Given the above assumptions, we now ready to state our main results in the following theorems.Theorem 1Under Assumptions 1–5 and in addition, nh4 → 0 and nh2log (1/h) → ∞, we haven(θ^−θ)→DN(0,A−1ΣA−1),whereA=E{Iθθ(z)}andΣ=Var{∂ℓ(π(z),θ,x,y)∂θ−Iθπ(z)d(x,y,z)}with d(x, y, z) is the first element ofIγγ−1(z)qγ(γ(z),x,y).Theorem 2Under Assumptions 1–5 and in addition, as n → ∞,  h → 0 and, nh → ∞ we havenh{π^(z)−π(z)−B(z)+op(h2)}→DN{0,κ0f−1(z)Iππ−1},whereB(z)=12μ2h2Iππ−1(z)ψ′′(z|z).The proofs of Theorems 1 and 2 are given in Appendix A. Note that, the result from Theorem 2 shows that, as for common semiparametric model, the estimate of θ has no effect on the first-order asymptotic since the rate of convergence ofπ^(z)is slower than that ofn. Consequently, it is fairly straightforward to see thatπ^(z)is more efficient than the initial estimate ofπ˜(z).Given the structure of model (1), it is of great interest to ask whether the probability of a firm being efficient takes a specific parametric form such as those suggested in KPT or RS. This question leads to the following hypothesis testing problem:(12)H0:π(zi)=h(zi,δ),where h(zi, δ) is a specific parametric function and δ is a vector of unknown parameters. For example, as in KPT and RS, one can assumeh(zi,δ)=exp(zi′δ)/[1+exp(zi′δ)]orh(zi,δ)=Φ(zi′δ)where Φ(.) is the cumulative distribution function of a standard normal random variate. Under the null hypothesis, model (1) reduces to the parametric zero-inefficiency stochastic frontier model considered by KPT and RS. However, under the alternative hypothesis, model (1) is a semiparametric model and hence the number of parameters under the alternative is undefined. One useful approach to test for the above null hypothesis is to use sieve likelihood ratio (hereafter SLR) statistics suggested by Fan et al. (2001), and it is given by:(13)T=2{L*(H1)−L*(H0)},where L*(H0) and L*(H1) denote the log-likelihood function computed under the null and the alternative hypothesis, respectively. Fan et al. (2001) show that the SLR statistics are asymptotically distribution free of nuisance parameters and followχbn2distributions (for a sequence bn→ ∞) under the null hypothesis (i.e., Wilks phenomenon) for testing a number of useful hypotheses for a variety of useful models such as nonparametric regression, varying coefficient and generalized varying coefficient models. However, since model (1) belongs to the class of semiparametric mixture models, the asymptotic null distribution of the SLR may or may not followχbn2distribution. Thus, one approach is to derive the asymptotic distribution of T. Alternatively, we can use the conditional bootstrap procedure suggested by Cai, Fan, and Li (2000) to approximate the asymptotic null distribution. The conditional bootstrap can be conducted as follows. Let{β¯,σ¯2,λ¯,δ¯}be the MLE under the null hypothesis. Given xi, generate a bootstrap sample,yi*from a given distribution of y specified in (1) with {π(.), β, σ2, λ} are replaced by their MLE estimates{β¯,σ¯2,λ¯,δ¯}. For each bootstrap sample, calculate the test statistic T* in (13), and use the distribution T*as an approximation to the distribution of T.It is important to note that, the conditional bootstrap described above is valid only if the asymptotic null distribution is independent of nuisance parameter π(.)(i.e., Wilk's phenomenon). We investigate the Wilk's phenomenon via Monte Carlo simulation below. Our simulation results indicate that, indeed Wilk's type of phenomenon continue to hold for the model consider in this paper.Another interesting question that arises is whether all firms are inefficient. This question leads to the following testing hypothesis:H0:π(z)=0for all z. Under null hypothesis ofH0:π(z)=0, model (1) reduces to a standard stochastic frontier and this is simply a special case of the testing problem of constancy of π(.) which take on a specific value of 0. Thus, in principle, a simple modification of sieve likelihood ratio statistics in (13) can be used to test the null. However, since the value of 0 lies on the boundary of the parameter space of π, the asymptotic null distribution of the test statistics is no longer a χ2 distribution. Thus, one approach is to derive the asymptotic distribution of the test statistics under this null hypothesis along the line of Andrews (2001), which is very complicated, given the semiparametric nature of the alternative. In addition, it is beyond the scope of this paper and we will leave it for future research.Alternatively, since the null hypothesis ofH0:π(z)=0is a special case ofH0:π(z)=π, the conditional bootstrap described earlier can be used to approximate the asymptotic distribution of the test statistics, provided that the Wilk's type of phenomenon continues to hold. Our Monte Carlo results below show that the test which is based on the conditional bootstrap has approximately correct sizes.Finally, notethat we did not pursue the hypothesis testing problem ofH0:π(z)=1for all z (i.e., all firms are efficient) simply because under this null hypothesis, there is a technical problem related to the fact thatσu2is not identified, which invalidates the conditional bootstrap procedure, albeit the SLR test would remain valid. In this case, there is a need for deriving the asymptotic distribution of the test statistics, and this is beyond the scope of this paper. Further investigation for this case would be interesting and useful for future research.In this section, we conduct some simulations to study the finite sample performance of the proposed estimator and test statistics. To this end, we consider the following data generating process (DGP):yi={1+xi+viwithprobablityπ(zi)1+xi+vi−uiwithprobablilty1−π(zi),whereπ(zi)=0.05+0.6sin(πzi). We generate zifrom an uniform distribution on [0, 1] and the xiis generated from a N(0, 1). The random error term viis generated as N(0, 0.5) and the one-sided error uiis generated as |N(0, 0.5λ)|. For allof our simulations, we setλ={1,2.5,5}, and let the sample sizes vary overn=2500orn=5000. For each experimental design, 1000 replications are performed.We use the Gaussian kernel function and the bandwidth is chosen according toh˜=h^×n−2/15whereh^is the optimal bandwidth based on CV approach previously discussed in Section 3.1.We measure the performance of the estimate of the probability of firms being fully efficient function π(z) by computing the mean average squared errors (MASE):MASE=11000∑r=11000{1n∑j=1n[π^r(zi)−πr(zj)]2}.The performance of the estimates of the production parameters is measured by the mean squared errors (MSE)MSE=11000∑r=11000(θ^r−θ)2,whereθ^=β^,σ^2orλ^. The simulations were performed on the mainframe using FORTRAN 77 using G77 complier of GNU.The simulation results for the estimated MSE of the production parameter estimates and the estimated MASE ofπ^(zi), for various values of λ are presented in Table 1. From Table 1, first we observe that as the sample size increases, both estimated MSE for production parameter estimates,θ^and MASE forπ^(zi)reduces. Second, we also observe that as the sample size doubles, the estimated MSE for production parameter estimates reduces to about half of the original values; this is consistent with the fact that the back-fitting local ML estimator of θ isn-consistent as predicted by Theorem 1.Table 2 reports the empirical sizes of the bootstrap SLR statistics. From Table 2, we see that, there are little sizes distortions indicating the conditional bootstrap provides a good approximation for the asymptotic distribution of the SLR statistics.Table 3 summarizes the performance of the bootstrap approach for standard errors of estimate of parameters for two different samples, and three different bandwidths which correspond to under-smoothing (h˜=h^×n−2/15), appropriate amount (h^) and over-smoothing (2h^). In the table, the standard deviation of 1000 estimates are denoted by STD which can be viewed as the true standard errors, while the average bootstrap standard errors are denoted SE along with their standard deviations are given the parentheses. The SE are calculated as the average of 1000 estimated standard errors. The coverage probabilities for all the parameters are given the last column and they are obtained based on the estimated standard errors. The results from Table 3 show that the suggested bootstrap procedure approximates the true standard deviations quite well and the coverage probabilities are close to the nominal levels for almost all cases.Note that the bootstrap procedure also allows us to compute the point-wise coverage probabilities for the probability functions. Table 4 provides the 95 percent coverage probabilities of π(z) for a set of evenly space grid points distributed on the support of z. In the table, the row labeled withπ(θ^)gives the results using the proposed approach, while π(θ) provides the results assuming θ were known. For most cases, the coverage probabilities are close to the nominal level, especially when under-smoothing or appropriate smoothing is used. For the case of over-smoothing, the results are somewhat less satisfactory. Moreover, the coverage levels are slightly low for point 0.4 and slightly high for points 0.8 and 0.9.Next, we investigate whether Wilk's type of phenomenon hold for the proposed model. Under the null hypothesis of (12), we assume the probability of efficient firm takes a specific parametric form. The DGP is the same as above except now we generate π(z) as logistic function or standard normal CDF with parameter δ. For each function, we fixed the value ofλ=2.5and set 3 different values ofδ={−1,0,1}, and use nonparametric kernel density estimation to compute the unconditional (asymptotic) null distribution of SLR statistics withn=2500andh=0.06via 500 replications.33We also conduct simulations using other bandwidthsh=0.12andh=0.24. The results are very similar, and hence we do not report them here but available from the authors upon request.The resulting densities are plotted in solid lines in Fig. 1a–c. As can be seen from these plots, the resulting densities are very close indicating that the asymptotic distribution of the SLR statistics are not sensitive to the choice of function π(z). This suggests that Wilk's type of results continue to hold for our model.Finally, to validate the conditional bootstrap approach, for each assumed function, we select 3 typical samples generated from the 3 different values of δ, and compute the conditional null distribution based on its 500 bootstrap samples. The resulting densities are depicted as dotted curves in the same Fig. 1a–c. From these figures, we can observe that the proposed conditional bootstrap approach performed quite well to approximate the asymptotic null distribution.There exists a vast literature on measuring productivity and efficiency for the banking sectors in various countries (see for example, Galán, Veiga, & Wiper, 2015; Sathye, 2003; Tzeremes, 2015 and the articles in Volume 98, Issue 2 (1997) of the European Journal of Operational Research, just to name a few). However, all these applications typically do not allow for the presence of fully efficiency banks, and hence these results could potentially be misleading if in fact there are efficient banks in the sample.In this section, we provide an application of the U.S. banking sectors to illustrate the usefulness and merit of our proposed model and approach. The data we use are taken from Koetter, Kolari, and Spierdijk (2012) which consist of large number of individual U.S. commercial banks from Reports of Condition and Income of the Federal Reserve System.44See Koetter et al. (2012) for the issues involved as well as details construction of the data set. The data are available also from Restrepo-Tobon and Kumbhakar (2014).The data contain annual year-end from all U.S. insured banks between 1976 and 2007. After controlling for outliers and missing observations, the final sample use in the estimation consists of 342,868 observations.Following convention in the competition and efficiency literature, the regressors used in our model are logs of three input prices: price of fixed assets (w1), cost of labor (w2) and purchased fund costs (w3), levels of two outputs: loans (y1) and federal funds sold and securities purchased (y2), a time trend (t) and the log of total assets to control for size effects (z). In addition, to be in line with the intermediation approach, it is assumed that banks transform various saving of consumers and firms into loans and investment, and seek to minimize costs. Thus, the dependent variable is total operating costs implying a cost frontier approach is employed.Note that our proposed model and approach is designed for cross-section data, and since we are using panel data, we need to make some assumptions regarding the temporal behavior of the technical inefficiency and random noise. Following KPT, first we include a time variable in the SF function to allow for technical change or shift in the frontier; and second, for simplicity we assume that both u and v are independently and identically distributed.We employ the translog specification for the cost frontier which can be written as:lnCit=β0+∑j=12βjlnyj,it+∑j=12γjlnwj,it+12∑l=12∑k=12βlklnyl,itlnyk,it+12∑l=12∑k=12γlklnwl,itlnwk,it+∑l=12∑k=12αlklnyl,itlnwk,it+αtt+αttt2+∑l=12δlktlnyl,it+∑l=12θlktlnwl,it+vit+uit,where we assumeuit∼i.i.d.|N(0,σu2)|andvit∼i.i.d.N(0,σv2). We impose the usual symmetry conditions in which we assumeβlk=βklandαlk=αkl. In addition, we normalized the cost and input prices by one input price (here we use w3) to ensure that the linear homogeneous restriction of the cost function with respect to input prices hold.For comparison purpose, we also estimate a half-normal standard scholastic frontier model (HN-SFM) and a zero-inefficiency stochastic frontier (ZISF) model of KPT, which assumes the underlying probability of firms being fully efficient follows the logit specification of zi.Note that, since the estimated parameters of the translog frontier do not have any direct economic interpretation, for brevity, we do not report all the parameter estimates here but these are available from the authors upon request. Instead, we summarize the results for the estimated returns to scale (RTS), technical change (TC) as well as report the results that associated with the estimated probability function and the estimated technical inefficiencies.For a cost function, RTS measures the proportional increase in costs due to an increase in all outputs, that is, RTS can be defined as the reciprocal of∑j(∂lnC/∂yj). Thus, if RTS is larger than one then a proportional increase in all outputs will lead to a less than proportional increase in cost, implying that the scale operation is below optimum, and hence there are benefits from expansion (i.e., economies of scale). The opposite holds true when RTS is less than one. For TC, it is defined as the rate of change in cost over time, ceteris paribus, i.e., ∂ln C/∂t. Therefore, a negative value of TC suggests a reduction in cost overtime, implying technical progress, and a positive value of TC shows a technical regress, ceteris paribus.As previously mentioned in Section 1, it is important to recognize that the prominent feature of the ZISF model is that the frontier itself does not vary across the two classes of firms but only the existence or non-existence of inefficiency differs. Thus, we would expect that the estimated RTS and TC of the three models would not differ significantly. Indeed, our results indicated that the estimated RTS and TC are very similar across all models. For RTS, the estimated values ranging from 0.85 to 1.3 with the mean value of 1.11 and standard deviation of 0.22; while the estimated values of TC ranging from −0.091 to 0.019 with the mean value of −0.016 and standard deviation of 0.0068. These results indicate that most of the banks experienced economies of scale as well as technical progress.We now turn our attention to the results of the estimated probability function, and the estimated technical efficiencies. To present results for the probability function, we normalize the log of total assets aszit*=(zit−zmax)/(zmax−zmin)where zitis the log of total assets. The function is then evaluated at 100 points between 0 and 1, and presented (along with two standard error bands) in Fig. 2. From Fig. 2, we observe that banks attaining full efficiency are, most likely, concentrated near the 20 percent asset quantile where the probability peaks at about 80 percent while 95 percent confidence intervals indicate that the probability can be as high as one. Larger banks seem to be inefficient with large probability, although there is some evidence of marginal behavior near the 79–85 percent quantiles. Zero inefficiency seems to prevail for most banks roughly below the median (normalized) assets. These results are consistent with the hypothesis that larger banks are inefficient due, for example, to the “quiet life hypothesis” (Koetter & Vins, 2008; Koetter et al., 2012), albeit other reasons could also be responsible for inefficient larger banks and nearly efficient smaller banks.The estimated technical inefficiency distributions are displayed in Fig. 3. It can be seen from Fig. 3 that, the SFM based on the half-normal specification yields inefficiency results that display virtually no mass at zero, indicating that no banks are fully efficient. The inefficiency scores lie in the range of 0.5 percent to 14 percent with the mean value of 6.5 percent and the standard deviation of 0.021. In contrast, results from ZISF indicate that there is some mass at zero with a long right tail in the inefficiency distribution. This suggests that, albeit there are some fully efficiency banks, inefficiency can be as high as 20 percent for some other banks. The inefficiency scores lie in a wider range than in the case of half-normal SFM, ranging from 0 percent to 20 percent with the mean value of 4.5 percent and the standard deviation of 0.032. The semi-parametric specification places even more mass at zero, and the inefficiency distribution is much tighter than both half-normal SFM and ZISF. The inefficiency scores lies between 0 percent and 10 percent, with the mean value of 2.3 percent and the standard deviation of 0.012. Thus, from the results in Fig. 3, we can see that parametric models (HN-SFM or ZISF) deliver very different inefficiency distributions compared to the semi-parametric specification.To determine which specification is more appropriate for the data considered, we use the SLR test discussed in Section 5 to test for the hypotheses of (i)H0:π(z)=eαz/(1+eαz)(ZISF model) and (ii)H0:π(z)=0(HN-SFM), based on the conditional bootstrap critical values. Our SLR tests produce the conditional bootstrap p-values of 0.0236 for testing (i) and 0.0073 for testing (ii), suggesting that both null hypotheses are rejected at a 5 percent significant level in favor of the nonparametric specification of the probability function. Consequently, for this particular data set, our results provide evidence that a flexible specification of the probability function is critical and, in particular, material in terms of inefficiency estimation.In this paper, we propose semiparametric approach for estimating the zero-inefficiency stochastic frontier model (e.g., KPT, 2013; RS, 2015) by allowing for the proportion of firms that are fully efficient to depend on a set of covariates via unknown smooth function. In particular, we propose an iterative backfitting local maximum likelihood estimation procedure that achieves the optimal convergence rates of both frontier parameters and the nonparametric function of the probability of firms being efficient. We derive the asymptotic bias and variance of the proposed estimator and establish its asymptotic normality. In addition, we discuss how to test for parametric specification of the proportion of firms that are fully efficient as well as how to test for the presence of fully inefficient firms, based on the conditional bootstrap sieve likelihood ratio statistics. The finite sample behaviors of the proposed estimation procedure and tests are examined using Monte Carlo simulations. We apply the proposed method to data on a large number of individual U.S. commercial banks to examine the effects of total assets on the probability of banks being efficient as well as technical inefficiency measurements overall. Our analysis indicated that flexible specification of the probability function of banks being efficient is critical in efficiency estimation.Note that the estimation approach proposed in this paper can also be easily modified and extended to other models as well that allow for the distribution of uito depend on covariate zieither parametrically (e.g., Caudill & Ford, 1993; Reifschneider & Stevenson, 1991; Caudill, Ford, & Gropper, 1995) or nonparametrically. For example, if it is assumed thatσ2(zi)=exp(zi′α), then by simply redefining the finite dimensional parameter vector θ, the estimation algorithm proceeds as discussed. On the hand, if we assume σ2(zi) to be an unknown smoothing function, then by approximating this function locally at a point z0 and by modifying the local log likelihood function in (4), the estimation algorithm remains unaffected.Finally, it would be interesting to extend the current model to full nonparametric setting that includes both continuous and categorical variables in the frontier as well as in the probability function.We first introduce some additional notations. Letπ˜*=nh(π˜−π),β˜*=nh(β˜−β),λ˜*=nh(λ˜−λ),σ˜2*=nh(σ˜2−σ2)where π,  β,  λ and σ2 are the true values. Also, letθ˜*=(β˜*′,λ˜*,σ˜2*)′andγ˜*=(π˜*(.),θ˜*′)′Proof of Theorem 1: The proof of this theorem follows similarly to that of Huang and Yao (2012). We show the key steps of the proof.To derive the asymptotic properties ofθ^, we first examine the asymptotic behavior ofγ˜=(π˜,θ˜′)′which is the local MLE of (4). Letθ^*=n(θ^−θ)ℓ(π˜(zi),θ,xi,yi)=logf(yi|π˜(zi),θ)ℓ(π˜(zi),θ^+n−1/2θ˜,xi,yi)=logf(yi|π˜(zi),θ^+n−1/2θ˜)Thenθ^*is the maximize of(A.1)Ln(θ*)=∑i=1n{ℓ(π˜(zi),θ^+n−1/2θ˜,xi,yi)−ℓ(π˜(zi),θ,xi,yi)}By using a Taylor series expansion and after some calculation, yields(A.2)Ln(θ*)=Anθ*+12θ*′Bnθ*+op(1)whereAn=n−1/2∑i=1n∂ℓ(π˜(zi),θ,xi,yi)∂θBn=n−1/2∑i=1n∂2ℓ(π˜(zi),θ,xi,yi)∂θ∂θ′Next we evaluate the terms Anand Bn. First, expanding Anaround π(zi), we obtainAn=n−1/2∑i=1n∂ℓ(π(zi),θ,xi,yi)∂θ+n−1/2∑i=1n∂2ℓ(π(zi),θ,xi,yi)∂θ∂π×[π˜(zi)−π(zi)]+Op(n−1/2∥π˜(.)−π(.)∥∞2)=n−1/2∑i=1n∂ℓ(π(zi),θ,xi,yi)∂θ+D1n+Op(n−1/2∥π˜(.)−π(.)∥∞2)where the definition of D1nshould be apparent. Now, applying Lemma A.1 of Fan and Huang (2005), we haveγ˜(zi)−γ(zi)=n−1f−1(zi)Iγγ−1(zi)∑j=1n∂ℓ(γ(zj),xj,yj)∂γKh(zj−zi)+Op(δn)whereδn=n−1/2h3/2+(nh)−1log(1/h). Under the condition nh2/log (1/h) → ∞, we haveOp(n1/2δn)=op(1). Furthermore, sinceπ(zi)−π(zj)=O((zi−zj)2)and K(.) is symmetric about 0, we haveD1n=n−3/2∑j=1n∑i=1n∂2ℓ(π(zi),θ,xi,yi)∂θ∂πf−1(zi)d(xj,yj,zj)Kh(zi−zj)+Op(n1/2h2)=D2n+Op(n1/2h2)where d(xj, yj, zj) is the first element ofIγγ−1(zj)qγ(γ(zj),xj,yj)and the definition of D2nshould be apparent. LetD3n=−n−1/2∑j=1nIθπ(zj)d(xj,yj,zj), then it can be shown thatD2n−D3n→p0. Hence, under the condition nh4 → 0, we have(A.3)An=n−1/2∑i=1n{∂ℓ(π(zi),θ,xi,yi)∂θ−Iθπ(zi)d(xi,yi,zi)}+op(1)For Bn, it can be shown that(A.4)Bn=−E[Iθθ(x)]+op(1)=B+op(1)Thus, from (A.2) in conjunction with (A.4) and an application of quadratic approximation lemma (see for example Fan & Gijbel, 1996, p. 210), leads to(A.5)θ^*=B−1An+op(1)if Anis a sequence of stochastically bounded vectors. Consequently, the asymptotic normality ofθ^*follows from that of An. Note that since Anis the sum of i.i.d. random vectors, it suffices to compute the mean and covariance matrix of Anand evoke the Central Limit Theorem. To this end, from (A.3), we have(A.6)E(An)=n1/2E{∂ℓ(π(z),θ,x,y)∂θ−Iθπ(z)d(x,y,z)}The expectation of each element of the first term on the right hand side can be shown to be equal to 0 and further calculation shows thatE{Iθπ(z)d(x,y,z)}=0. ThusE(An)=0. The variance of AnisVar(An)=Var{∂ℓ(π(z),θ,x,y)∂θ−Iθπ(z)d(x,y,z)}=Σ. By the Central Limit Theorem, we obtain the desired result. □Proof of Theorem 2:Recall that, given the estimate ofθ^,π^(z)maximizes (5). Letη(zo,z)=a(zo)+a′(zo)(z−z0)andπ*=(nh)1/2{π−a(z0),h(π′−a′(z0))}′, thenπ^*maximizesLn*(π*)=∑i=1n{ℓ(η(z0,zi)+(nh)−1/2π*′wi,θ^,xi,yi)−ℓ(η(z0,zi),θ^,xi,yi)}Kh(zi−z0)wherewi=(1,(zi−z0)/h)′. Using Taylor expansion of ℓ(.)and after some calculation, we have(A.7)Ln*(π*)=Δ^n′π*+12π*′Γ^nπ*+op(1)whereΔ^n=(nh)−1/2∑i=1n∂ℓ(η(zi,z0),θ^,xi,yi)∂ηwiKh(zi−z0)Γ^n=(nh)−1∑i=1n∂2ℓ(η(zi,z0),θ^,xi,yi)∂η∂η′wiwi′Kh(zi−z0)By the SLLN, Assumption 3 and Lemma A.1 of Fan and Huang (2005), it can be shown thatE(Γ^n)→−f(z0)(100μ2)⊗Iπ=−Γ^andvar{(Γn)ij}=O((nh)−1)implying thatΓ^n=−Γ^+op(1). Thus (A.7) can be written as(A.8)Ln*(π*)=Δ^n′π*+12π*′Γ^π*+op(1)Using the quadratic approximation lemma, yields(A.9)π^*=Γ^−1Δ^n+op(1)ifΔ^nis a sequence of stochastically bounded random vectors. An expansion ofΔ^nleads toΔ^n=(nh)−1/2∑i=1n∂ℓ(η(zi,z0),θ,xi,yi)∂ηwiKh(zi−z0)+Gn+op(1)=Δn+Gn+op(1)whereGn=(nh)−1/2∑i=1n∂2ℓ(η(zi,z0),θ,xi,yi)∂η∂θwi(θ^−θ)Kh(zi−z0)Sincen(θ^−θ)=Op(1), it can be shown thatGn=−h−1/2Iηθ′(z)f(z)=op(1)whereIηθ(z)=−E[qηθ(η(z),θ,x,y)|z]. Thus, (A.9) becomes(A.10)π^*=Γ^−1Δn+op(1)The asymptotic normality ofπ^*follows from that of Δnso it suffices to calculate the mean and variance of Δn. Since K(.) is symmetric and bounded, we haveE(Δn)=n(nh)−1/2E{∂ℓ(η(z,z0),θ,x,y)∂ηwKh(z−z0)}=(nh)1/2f(z0)2(μ20)⊗Γ^(z0)ψ′′(z0){1+op(1)}andVar(Δn)=h−1E{(∂ℓ(η(z,z0),θ,x,y)∂η)2ww′Kh2(z−z0)}=f(z0)(κ0κ1κ1κ2)⊗Γ^(z0){1+op(1)Lete1=(10)′denote a (2 × 1) unit vector, then{e1′Var(Δn)e1}−1{e1′Δn−e1′E(Δn)}→DN(0,1)by using standard argument. □

@&#CONCLUSIONS@&#
