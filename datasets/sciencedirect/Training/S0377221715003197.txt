@&#MAIN-TITLE@&#
Circumventing the Slater conundrum in countably infinite linear programs

@&#HIGHLIGHTS@&#
Provides duality results for general countably infinite linear programs.Proves properties of the Lagrangian function and saddle points.Applies results to countable-state MDPs and to a robust auction-design problem.

@&#KEYPHRASES@&#
Infinite-dimensional linear optimization,Markov decision processes,Shadow prices,

@&#ABSTRACT@&#
Duality results on countably infinite linear programs are scarce. Subspaces that admit an interior point, which is a sufficient condition for a zero duality gap, yield a dual where the constraints cannot be expressed using the ordinary transpose of the primal constraint matrix. Subspaces that permit a dual with this transpose do not admit an interior point. This difficulty has stumped researchers for a few decades; it has recently been called the Slater conundrum. We find a way around this hurdle.We propose a pair of primal-dual spaces with three properties: the series in the primal and dual objective functions converge; the series defined by the rows and columns of the primal constraint matrix converge; and the order of sums in a particular iterated series of a double sequence defined by the primal constraint matrix can be interchanged so that the dual is defined by the ordinary transpose. Weak duality and complementary slackness are then immediate. Instead of using interior point conditions to establish a zero duality gap, we call upon the planning horizon method. When the series in the primal and dual constraints are continuous, we prove that strong duality holds if a sequence of optimal solutions to finite-dimensional truncations of the primal and dual CILPs has an accumulation point. We show by counterexample that the requirement that such an accumulation point exist cannot be relaxed. Our results are illustrated using several examples, and are applied to countable-state Markov decision processes and to a problem in robust optimization.

@&#INTRODUCTION@&#
We study infinite-dimensional linear programs (LPs) with countably infinite variables and countably infinite constraints. Following Ghate, Sharma, and Smith (2010) and Ghate and Smith (2013), we call them Countably Infinite Linear Programs (CILPs). CILPs arise in infinite-horizon planning applications under time-varying technological and economic conditions; examples include production planning, equipment replacement, and capacity expansion (Hopkins, 1969; Jones, Zydiak, & Hopp, 1988; Schochetman & Smith, 1989). They also subsume minimum cost flow problems on infinite networks (Romeijn, Sharma, & Smith, 2006; Sharkey & Romeijn, 2008); and LP formulations of countable-state Markov decision processes (MDPs) (Ghate & Smith, 2013; Lee, Epelman, Romeijn, & Smith, 2014; Puterman, 1994; Ross, 1983).We useN={1,2,…}to denote the set of all natural numbers and letℜNdenote the space of all real-valued sequences. Variables in CILPs reside in subspaces ofℜN. Duality results for CILPs have been scarce due to numerous mathematical pathologies inℜN. Several CILPs where weak duality fails, and where the primal and dual optimal values do not match, have been constructed (Anderson & Nash, 1987). An excellent recent exposition, replete with examples, of the difficulties in establishing duality results in CILPs (and more generally in infinite-dimensional LPs) is included in Martin, Ryan, and Stern (2014). We also refer the reader to Anderson and Nash (1987), Basu, Martin, and Ryan (2014), Grinold (1971), and Ponstein (1981) as examples of earlier works with similar discussions. We briefly review these challenges here.Unlike in finite-dimensional LPs, several options of subspaces ofℜNare available for embedding the variables, the columns of the constraint matrix, the right hand side vector, and the cost coefficient vector from the primal CILP. For instance, sometimes, a natural choice for the columns of the constraint matrix and the right hand side vector is to embed them inℜNitself. It may then be possible to write the so-called algebraic dual problem in the space of sequences with finitely many non-zero entries by using the ordinary transpose of the primal constraint matrix. One benefit of this approach is that weak duality can be easily established when we work with the algebraic dual (see Theorem 2.1 and its single-line proof on page 18 of Anderson and Nash (1987)). However, the trouble is that the non-negative orthant ofℜNwith its natural product topology has an empty interior. Thus, standard interior point conditions that ensure a zero duality gap do not hold in such CILPs (see Section 3.6.2 on page 54 of Anderson and Nash (1987), and Theorem 1 on page 217 of Luenberger (1969)). In an attempt to remedy this situation, one may be able to embed the columns of the constraint matrix and the right hand side in a space smaller thanℜN,such as the space l∞ of bounded sequences, and then use the norm topology. A benefit of this approach is that the non-negative orthant in this space does have a non-empty interior. So it might be possible to close the duality gap by embedding the dual variables in the dual of l∞. But unfortunately, this dual space is too large – the objective function and the constraints there do not even have a series representation (see Theorem 16.31 on page 542 of Aliprantis and Border (1994), and Problem 2 on page 223 of Taylor and Lay (1986)). Specifically, the linear operator (called the adjoint) that defines the constraints in the dual might not be the same as the ordinary transpose of the primal doubly-infinite matrix. This makes the dual structurally quite different from the familiar finite-dimensional LP dual and renders it unsatisfactory for applications. For instance, the standard finite-dimensional thought process of associating a dual variable with each primal constraint does not work, and the price-interpretation of dual variables is lost.In short, the process of selecting appropriate subspaces for the primal and the dual CILPs is torn between two competing alternatives. We want to select sequence spaces such that the ordinary transpose of the primal constraint matrix can be seen as its adjoint and so that weak duality is immediate; but we want to do this such that some sufficient condition for a zero duality gap holds. This difficulty can be viewed as a variation of what Martin et al. (2014) call the “Slater conundrum.” In fact, they prove the strong and elegant result that, in a large class of Riesz spaces, the conundrum cannot be resolved. If we insist on the existence of a core point (an algebraic counterpart of the topological notion of Slater’s interior point condition that is sufficient for a zero duality gap), then we must face the unsavory existence of a so-called singular dual functional that precludes writing the algebraic dual with a structure and interpretation similar to the finite-dimensional case!An alternative method that sometimes helps in proving duality results for CILPs avoids potential complications inℜNby working with their finite-dimensional truncations (Grinold, 1971, 1977, 1983, Grinold & Hopkins, 1973b; Romeijn & Smith, 1998; Romeijn, Smith, & Bean, 1992). The idea is to show that finite-dimensional duality properties are preserved in the limit. Grinold (1971) applied this “planning horizon” method to prove the existence of optimal dual solutions to a special class of infinite-horizon problems. Grinold (1977) also used it to establish weak duality in a stationary, infinite-stage linear program; he then later extended it to convex programs Grinold (1983). Grinold and Hopkins (1973a) applied it to a mathematical program with an arbitrary transient phase followed by an infinite-horizon stationary phase that had a dynamic Leontief structure. Jones et al. (1988) employed it to derive optimal solutions to an equipment replacement problem and its dual. The broadest applications of the planning horizon approach to CILPs have appeared in Romeijn and Smith (1998) and Romeijn et al. (1992). These two papers focused on primal problems where each constraint contained a finite number of variables, and in a dual manner, each variable appeared in a finite number of constraints. The resulting primal constraint matrix had a lower staircase structure that was exploited in duality proofs. Romeijn et al. (1992) explicitly included upper bounds on primal variables, then associated dual variables with these bound constraints, and applied the planning horizon technique to establish duality results under a transversality condition. Noting the algebraic difficulties that these upper bounds introduce in establishing transversality, Romeijn and Smith (1998) proved weak duality and complementary slackness without including explicit bounds in their formulation. Strong duality was then established under transversality between accumulation points of sequences drawn from the sets of finite-dimensional primal and dual optimal solutions. Ghate and Smith (2013) and Lee, Epelman, Romeijn, and Smith (2013) used the planning horizon method to prove duality results for non-stationary infinite-horizon Markov decision processes, heavily exploiting the structure of the constraints that appear in CILP formulations of these problems. As such, the papers that have used the planning horizon approach do not exploit the benefits (such as simpler statements of results, simpler and shorter proofs, similarities to finite-dimensional as well as to more general infinite-dimensional LPs) of a generic choice of sequence spaces. Instead, they appear to have focused either on problems that arise in specific applications or on constraint matrices with special structure to derive duality results.In light of this literature review, it is unclear whether it is even possible to select a generic pair of sequence spaces for CILPs such that the ordinary transpose of the primal constraint matrix can be seen as its adjoint, and still ensure that some sufficient condition for a zero duality gap holds. From a modeling perspective, it is unclear whether or not any potential choice of sequence spaces will be broad enough to encompass nontrivial applications. It would thus seem that there is no way around the Slater conundrum for general CILPs.In this paper, we attempt to put these concerns to rest. Our approach is to apply two methods to simultaneously chip away at the two sides of the conundrum. We combine the classical technique of choosing appropriate linear subspaces so that weak duality is immediate, with the planning horizon method. The former is known to work in abstract settings but has not been successfully applied previously to establish weak duality in general CILPs; the latter is known to work for establishing a zero duality gap in CILPs with a lower staircase structure. We show that, for general CILPs, the bridge between the two is a simple result in real analysis (Lemma 2.1).Specifically, our primal and dual sequence spaces are chosen to possess three properties: (i) the series in the primal and dual objective functions converge, (ii) the series defined by each row and each column of the primal doubly-infinite constraint matrix converge, and (iii) the order of sums in a particular iterated series of a double sequence that arises in all of our proofs can be interchanged. We show that CILPs defined this way behave essentially as finite-dimensional LPs do. We therefore call the resulting dual the natural dual. Because of this choice of sequence spaces, our proofs of weak duality and complementary slackness are immediate as in the finite-dimensional case and do not need any additional conditions. Also, unlike the planning horizon approach, these proofs do not use a limiting argument. In fact, we show that it is always possible to select primal and dual sequence spaces with the above three properties; thus, our weak duality and complementary slackness results hold in every CILP.We then switch to the planning horizon technique to prove strong duality using the natural product topologies on our sequence spaces. We prove that, when the series in the constraints in the original primal and dual CILPs are continuous, accumulation points (if they exist) of any sequence of optimal solutions to increasingly larger finite-dimensional truncations of the original CILPs satisfy complementary slackness conditions and are feasible. Thus, these accumulation points are optimal to the primal and dual CILPs, and they have identical objective function values; that is, strong duality holds. We show by counterexample that the requirement that such accumulation points exist cannot be relaxed. The continuity condition is trivially met in all CILPs where the constraints include a finite number of variables and variables appear in a finite number of constraints as in Romeijn and Smith (1998) and Romeijn et al. (1992). Thus, we recover strong duality in that paper as a special case. We also prove that the Lagrangian function of our CILPs is a linear support of the primal optimal value function. We conclude our theoretical results by deriving a saddle point property for our CILPs. To demonstrate in detail the steps involved in applying our theory, we study CILP formulations of countable-state Markov decision processes, and specifically, a stochastic equipment replacement problem within this framework. In countable-state MDPs, we are able to derive a pair of feasible primal-dual solutions using Bellman’s equations of dynamic programming to concretely demonstrate strong duality. As this may not always be possible in other applications, we provide further insights into our theory by explicitly checking our sufficient conditions for strong duality in a robust auction-design problem.Let b, c be inℜN,and for i = 1, 2, …, letAi·=(ai1,ai2,…)∈ℜNbe the ith row of a doubly-infinite matrix A. Here, A, b, c constitute our problem data and we do not place any restrictions on their structure. Unlike the abstract framework of Anderson and Nash (1987) for general infinite-dimensional LPs, we do not need to choose subspaces for these entities; instead, our primal and dual variable spaces will be chosen such that the two resulting CILPs are “well-posed” given this problem data. Specifically, as we explain in detail next, five hypotheses labeled H1–H5 hold for our variable spaces.LetX⊆ℜNbe the subset of all sequencesx∈ℜNfor whichH1.the seriesC(x)≜∑j=1∞cjxjconverges; andthe seriesAi·(x)≜∑j=1∞aijxjconverge for all i.H1 was used in Romeijn and Smith (1998). H2 was not needed in Romeijn and Smith (1998), Romeijn et al. (1992) as it holds for all sequences inℜNif each constraint includes a finite number of variables.Subset X is in fact a linear subspace ofℜNbecause it is easy to see that for any real numbers λ1, λ2, the sequenceλ1x(1)+λ2x(2)∈ℜNbelongs to X if the sequences x(1) and x(2) do (see Section 2.3 on page 14 of Luenberger (1969)). Now consider the CILP(1)(P)V(P)=inf∑j=1∞cjxj(2)∑j=1∞aijxj=bi,i=1,2,…,(3)xj≥0,j=1,2,…,(4)x∈X.We use F⊆X to denote the (possibly empty) feasible region of (P). Problems with inequality constraints and/or free variables can be converted into form (P) if needed as in the finite-dimensional case, and hence (P) includes all well-posed CILPs.We now choose a linear subspace ofℜNto embed dual variables. The idea, as in finite-dimensional LPs, is to associate a real-valued dual variable yiwith the ith constraint in (P). To do this in a rigorous manner such that the resulting pair of primal-dual CILPs behaves essentially exactly like its finite-dimensional counterpart, we put some restrictions on the subspace of dual variables. Specifically, let Y be the subset of ally∈ℜNfor whichH3.the seriesB(y)≜∑i=1∞biyiconverges;for every x ∈ X, the series∑j=1∞|aijxjyi|converges to some limit Li(x, yi), for i = 1, 2, …; andthe above limits Li(x, yi) have the property that∑i=1∞Li(x,yi)<∞.H3 was used in Romeijn and Smith (1998). H4 holds for all sequences y inℜNif each constraint includes a finite number of variables as in Romeijn and Smith (1998) and Romeijn et al. (1992). H5 need not hold for all sequences inℜNeven when each constraint includes a finite number of variables. H5 can be seen as playing the role of the transversality conditions used in Romeijn and Smith (1998) and Romeijn et al. (1992).Subset Y is in fact a linear subspace ofℜN. To see this, suppose λ1, λ2 are two real numbers and y(1), y(2) are two sequences in Y. We need to show that the sequence λ1y(1) + λ2y(2) belongs to Y; that is, to show that H3, H4, and H5 hold for λ1y(1) + λ2y(2). Establishing H3 is immediate. So we focus on H4, H5. We observe that∑j=1∞|aijxj(λ1yi(1)+λ2yi(2))|≤|λ1|∑j=1∞|aijxjyi(1)|+|λ2|∑j=1∞|aijxjyi(2))|=|λ1|Li(x,yi(1))+|λ2|Li(x,yi(2)). Thus, the series∑j=1∞|aijxj(λ1yi(1)+λ2yi(2))|converges to some limit Li(x, (λ1yi(1) + λ2yi(2))), for each i. Thus, H4 holds. Moreover,∑i=1∞Li(x,(λ1yi(1)+λ2yi(2)))≤|λ1|∑i=1∞Li(x,yi(1))+|λ2|∑i=1∞Li(x,yi(2))<∞as required for H5.The following result plays a central role throughout this paper.Lemma 2.1Fix any x ∈ X and any y ∈ Y. Then, (i) for each fixed j, the series∑i=1∞aijyixjconverges absolutely; and (ii) the two iterated series∑i=1∞∑j=1∞aijxjyiand∑j=1∞∑i=1∞aijyixjconverge with(5)∑i=1∞∑j=1∞aijxjyi=∑j=1∞∑i=1∞aijyixj.This result is a restatement of Theorem 8.43 in Apostol (1974) for the double sequence aijxjyi.□Eq. (5) can be interpreted as saying the desirable property that the usual transpose of the doubly-infinite matrix A is its adjoint (see Section 2.2.2 in Anderson and Nash (1987)). This property of X and Y allows us to write the dual of (P) using the usual finite-dimensional thought process (see Table 4.1 on page 143 of Bertsimas and Tsitsiklis (1997)). As we shall see, it permits us to interchange the order of sums that appear in our proofs of weak duality and complementary slackness. This renders these proofs identical to the finite-dimensional case. The importance of this type of interchange in the order of sums (akin to Fubini’s theorem) for establishing duality results is also discussed in the author’s doctoral dissertation (Ghate, 2006), in Martin et al. (2014), and by Ponstein (1981). We will also see later in Section 4 that Eq. (5) is the key link that enables us to replicate finite-dimensional proofs of standard properties of the Lagrangian function, primal optimal value function, and saddle points.Without loss of generality, let subspace X be such that for every j, there is at least one x ∈ X for which xj≠ 0. For j = 1, 2, …, letA·j=(a1j,a2j,…)∈ℜNdenote the jth column of A. The first conclusion in Lemma 2.1 then implies that the seriesA·j(y)≜∑i=1∞aijyiconverges for every y ∈ Y. We now write the dual of (P) using the ordinary transpose of A as(6)(D)V(D)=sup∑i=1∞biyi(7)∑i=1∞aijyi≤cj,j=1,2,…,(8)y∈Y.We call (D) the natural dual of (P) and use G⊆Y to denote its (possibly empty) feasible region.The natural dual yields a one-line proof of weak duality as in the finite-dimensional case (see Theorem 4.3 on page 146 of Bertsimas and Tsitsiklis (1997)) and as would the algebraic dual (see Theorem 2.1 on page 18 of Anderson and Nash (1987)). However, the crucial difference is that the constraints in the algebraic dual need not have a concrete series representation; the constraints in our natural dual do. Finally, we comment that H1–H5 can be seen as concrete sufficient conditions that ensure that the variable and constraint spaces in our problems are appropriately “paired” as, for example, required for weak duality and complementary slackness in Section 3.3 of Anderson and Nash (1987).Theorem 2.2Weak duality:For any x ∈ F and any y ∈ G,∑j=1∞cjxj≥∑i=1∞biyi. Hence ∞ ≥ V(P) ≥ V(D) ≥ −∞ (here, the infimum over an empty set is interpreted as + ∞ and the supremum over an empty set is interpreted as − ∞). Also, if x ∈ F and y ∈ G are such that∑j=1∞cjxj=∑i=1∞biyi,then x is optimal to (P) and y is optimal to (D), and thus strong duality holds.We have,∑i=1∞biyi=∑i=1∞(∑j=1∞aijxj)yi=∑i=1∞∑j=1∞aijxjyi=∑j=1∞∑i=1∞aijyixj=∑j=1∞(∑i=1∞aijyi)xj≤∑j=1∞cjxj,where the first equality holds by (2), the third equality is simply (5), and the last inequality follows from (7) because xj≥ 0. The other two conclusions then follow easily.□Before proceeding, we contrast this proof of weak duality with that in Romeijn and Smith (1998). The key intermediate step in their proof was their Lemma 3.2, which used a limiting argument that relied on the staircase structure of their constraint matrix and their Assumption 3.1. They explain that assumption as follows. The assumption “roughly requires that one be no worse off in the long run with respect to the infinite dimensional problem by ending the Nth period in any Nth period feasible state than by ending it in the 0-state.” The key step in our proof of weak duality is Eq. (5) in our Lemma 2.1 above, which is delivered directly by the properties of our sequence spaces X and Y as is more common in abstract weak duality results in infinite-dimensional LPs. We also contrast our proof of weak duality with that in Romeijn et al. (1992). In that paper, weak duality was established under the stronger (than that in Romeijn and Smith (1998)) assumption that off-diagonal matrices in the staircase eventually become non-negative. Specifically, this assumption implied that all feasible solutions of their dual CILP were feasible to sufficiently large truncations of that CILP; this fact was then used to show, via a limiting argument, that weak duality in the finite-dimensional truncations is inherited by the original CILPs in the limit.We will say that the (possibly infeasible) solutions x ∈ X and y ∈ Y are complementary if they satisfyxj(cj−∑i=1∞aijyi)=0for each j = 1, 2, ….Theorem 2.3Complementary slackness:1.Supposex ∈ F and y ∈ G, and suppose x and y are complementary. Then x is optimal to (P), y is optimal to (D), and V(P) = V(D). Thus, strong duality holds in this case.Suppose x is optimal to (P), y is optimal to (D), and V(P) = V(D). Then x and y are complementary.For the first claim, we have,∑j=1∞xj(cj−∑i=1∞aijyi)=0because every term inside the sum is zero. Therefore,∑j=1∞cjxj=∑j=1∞xj∑i=1∞aijyi=∑i=1∞∑j=1∞aijxjyi=∑i=1∞biyi,where the interchange of the order of summation is allowed by (5). Thus, we have that x ∈ F, y ∈ G and∑j=1∞cjxj=∑i=1∞biyi. Therefore x is optimal to (P) and y is optimal to (D) by weak duality; and V(P) = V(D).Now for the second claim, x and y are given as optimal to (P) and (D) respectively, and since V(P) = V(D), we have∑j=1∞cjxj=∑i=1∞biyi. Since x is feasible to (P),bi=∑j=1∞aijxj,for every i. Thus∑j=1∞cjxj=∑i=1∞∑j=1∞aijxjyi=∑j=1∞xj∑i=1∞aijyiagain by (5). Therefore,∑j=1∞xj(cj−∑i=1∞aijyi)=0. Since x is feasible to (P), xj≥ 0 for every j. Moreover, since y is feasible to (D),(cj−∑i=1∞aijyi)≥0for every j. Thus, for the infinite sum to be zero, each term must be zero. That is,(cj−∑i=1∞aijyi)xj=0for every j.□It is important to note that subspaces X and Y with the aforementioned properties always exist; specifically, all requisite properties hold if both X and Y are chosen to equal the space of sequences with finitely many non-zeros (although this is not in general advisable from a strong duality or even a feasibility perspective). Consequently, we say (although perhaps somewhat vacuously) that given any problem data A, b, c, it is always possible to choose X and Y such that weak duality and complementary slackness Theorems 2.2 and 2.3 hold as stated.We conclude this section by applying our results above to two examples.Example 2.4Our first example originally appeared in Sharkey and Romeijn (2008) and was also discussed in Ghate and Smith (2013). Consider the following primal CILP (that in fact corresponds to a minimum cost flow problem on an infinite network).(9)(P0)inf∑i=1∞(1/2i)xi,i+1(10)x1,2=1,(11)xi,i+1−xi−1,i=0,i=2,3,…,(12)xi,i+1≥0,i=1,2,….There is a single feasible solution to this CILP:xi,i+1*=1for i = 1, 2, …. We thus choose the space X = l∞ without loss of feasibility in this problem. Note then that the series in the objective function converges (absolutely) for every x ∈ X and thus H1 holds. H2 holds trivially because each constraint include a finite number of variables. In order to write the natural dual of (P0), we need to choose a dual sequence space Y. We make this choice using the thought process described above. The series∑i=1∞biyiin this case reduces simply to y1. Thus, we are assured that the dual objective function series will trivially converge and hence H3 will hold no matter what choice we make for Y. However, a further restriction must be placed on Y to obtain the natural dual of (P0). We argue that Y = l1, the sum of absolutely summable sequences, has the desired property. H4 holds because each constraint includes a finite number of variables. Moreover, for any x ∈ l∞ and any y ∈ l1, we have,|x1,2y1|≤2|y1|supi|xi,i+1|<∞. Similarly, for i = 2, 3, …,|xi,i+1yi|+|xi−1,iyi|≤2|yi|supi|xi,i+1|<∞. In addition, we have,2supi|xi,i+1|∑i=1∞|yi|<∞. Thus, H5 holds. In particular, Lemma 2.1 applies and we write the natural dual of (P0) as(13)(D0)supy1(14)yi−yi+1≤(1/2i),i=1,2,…,(15)y∈l1.Weak duality and complementary slackness Theorems 2.2 and 2.3 then hold for (P0) and (D0). Now consider the solutiony1*=1,y2*=1/2,y3*=1/4,and so on; that is,yi*=(1/2i−1),for i = 1, 2, …. It is easy to check that y* is feasible to (D0), and in fact, y* is complementary to the primal feasible solution x*. Consequently, the first item in our complementary slackness Theorem 2.3 (or the third claim in our weak duality Theorem 2.2) implies that x* and y* are optimal to (P0) and (D0), respectively, and strong duality holds (the objective value of these solutions in the two problems is 1).Note that if we had dropped the restriction y ∈ l1 from (D0), then even weak duality would have failed: solutions of the form yi= θ, for i = 1, 2, …, would be feasible to the resulting “dual” and then weak duality would fail for any θ > 1. This suggests that we cannot drop H5.Our second example is from Romeijn et al. (1992). Our choice of sequence spaces for this example is somewhat more complicated than that in the example above. Consider the following primal CILP that we obtained after adding/subtracting slack/surplus variables u, v, w, s, and t in the original CILP with variables x, y, and z that appeared in Romeijn et al. (1992).(16)(P1)inf∑i=1∞(1/2)i−1zi(17)x1−u1=1,(18)y1+z1−u2=1,(19)−2yi−1+xi−ui+1=0,i=2,3,…,(20)−2xi−1+yi+zi−vi−1=0,i=2,3,…,(21)xi+wi=2i−1,i=1,2,…,(22)yi+si=2i−1,i=1,2,…,(23)zi+ti=1,i=1,2,…,(24)x,y,z,u,v,w,s,t≥0.It is easy to see (and this is also mentioned in Romeijn et al. (1992)) that the solutionxi*=yi*=2i−1for i = 1, 2, …;zi*=ui*=vi*=wi*=si*=0for i = 1, 2, …; andti*=1for i = 1, 2, …, is optimal to this problem with optimal objective value 0. LetX⊂ℜNbe the subspace of real-valued sequences (x1, x2, …) with the property that |xi| ≤ τ(x)2i − 1 for some real number τ(x). Here, the motivation for including a scaling factor τ(x) in the bound is to ensure that the resulting subset is indeed a linear subspace. Without loss of optimality, we embed primal variables x, y, u, v, w, and s each in the subspace X; and z and t in l∞. It is easy to see that the series in the objective function converges (absolutely) for this choice of sequence spaces and thus H1 holds. H2 holds trivially because each constraint includes a finite number of variables. In order to write the natural dual of (P1), we need to first embed the dual variables in appropriate subspaces. Toward this end, letY⊂ℜNbe the subspace of real-valued sequences (α1, α2, …) with the property that∑i=1∞2i−1|αi|<∞; we observe that Y⊂l1 because this will be helpful later in our analysis. We embed dual variables α, β, γ, and δ each in the subspace Y; and ε in l1. Then, we have,|α1|+|α2|+∑i=1∞2i−1|γi|+∑i=1∞2i−1|δi|+∑i=1∞|ϵi|<∞.Thus, the series in the objective function of our natural dual converges (absolutely) and hence H3 holds. H4 holds trivially because each constraint includes a finite number of variables. Now, to see that H5 holds, we proceed as follows. First, we write,|x1α1|+|−u1α1|+|y1α2|+|z1α2|+|−u2α2|+∑i=2∞(|−2yi−1αi+1|+|xiαi+1|+|−ui+1αi+1|)+∑i=2∞(|−2xi−1βi−1|+|yiβi−1|+|ziβi−1|+|−vi−1βi−1|)+∑i=1∞(|xiγi|+|wiγi|)+∑i=1∞(|yiδi|+|siδi|)+∑i=1∞(|ziϵi|+|tiϵi|)≤|x1∥α1|+|y1∥α2|+|z1∥α2|+∑i=2∞|xi∥αi+1|+∑i=2∞2|yi−1∥αi+1|+∑i=2∞2|xi−1∥βi−1|+∑i=2∞|yi∥βi−1|+∑i=2∞|zi∥βi−1|+∑i=1∞|ui∥αi|+∑i=1∞|vi∥βi|+∑i=1∞|xi∥γi|+∑i=1∞|wi∥γi|+∑i=1∞|yi∥δi|+∑i=1∞|si∥δi|+∑i=1∞|zi∥ϵi|+∑i=1∞|ti∥ϵi|.Here, the inequality is obtained by first collecting similar terms and then noting that the absolute value of a product is bounded above by the product of absolute values. The above expression is in turn bounded as≤|x1∥α1|+|y1∥α2|+|z1∥α2|+τ(x)∑i=2∞2i−1|αi+1|+τ(y)∑i=2∞2i−1|αi+1|+τ(x)∑i=2∞2i−1|βi−1|+τ(y)∑i=2∞2i−1|βi−1|+(supi|zi|)∑i=2∞|βi−1|+τ(u)∑i=1∞2i−1|αi|+τ(v)∑i=1∞2i−1|βi|+τ(x)∑i=1∞2i−1|γi|+τ(w)∑i=1∞2i−1|γi|+τ(y)∑i=1∞2i−1|δi|+τ(s)∑i=1∞2i−1|δi|+(supi|zi|)∑i=1∞|ϵi|+(supi|ti|)∑i=1∞|ϵi|.This upper bound holds because x, y, u, v, w, s ∈ X and z, t ∈ l∞. By multiplying some of the terms by a factor of two, this expression is in turn bounded above as≤|x1∥α1|+|y1∥α2|+|z1∥α2|+τ(x)∑i=2∞2i|αi+1|+τ(y)∑i=2∞2i|αi+1|+2τ(x)∑i=2∞2i−2|βi−1|+2τ(y)∑i=2∞2i−2|βi−1|+(supi|zi|)∑i=2∞|βi−1|+τ(u)∑i=1∞2i−1|αi|+τ(v)∑i=1∞2i−1|βi|+τ(x)∑i=1∞2i−1|γi|+τ(w)∑i=1∞2i−1|γi|+τ(y)∑i=1∞2i−1|δi|+τ(s)∑i=1∞2i−1|δi|+(supi|zi|)∑i=1∞|ϵi|+(supi|ti|)∑i=1∞|ϵi|.By adding positive terms to the first two series in this expression, we further bound it above as≤|x1∥α1|+|y1∥α2|+|z1∥α2|+τ(x)∑i=0∞2i|αi+1|+τ(y)∑i=0∞2i|αi+1|+2τ(x)∑i=2∞2i−2|βi−1|+2τ(y)∑i=2∞2i−2|βi−1|+(supi|zi|)∑i=2∞|βi−1|+τ(u)∑i=1∞2i−1|αi|+τ(v)∑i=1∞2i−1|βi|+τ(x)∑i=1∞2i−1|γi|+τ(w)∑i=1∞2i−1|γi|+τ(y)∑i=1∞2i−1|δi|+τ(s)∑i=1∞2i−1|δi|+(supi|zi|)∑i=1∞|ϵi|+(supi|ti|)∑i=1∞|ϵi|<∞.Here, the last strict inequality follows from the definitions of subspaces Y and l1. Thus, H5 holds. Consequently, Lemma 2.1 applies and we write the natural dual as(25)(D1)supα1+α2+∑i=1∞2i−1γi+∑i=1∞2i−1δi+∑i=1∞ϵi(26)α1−2β1+γ1≤0,(27)αi+1−2βi+γi≤0,i=2,3,…,(28)α2−2α3+δ1≤0,(29)−2αi+2+βi−1+δi≤0,i=2,3,…,(30)α2+ϵ1≤1,(31)βi−1+ϵi≤(1/2)i−1,i=2,3,…,(32)α,β≥0,(33)γ,δ,ϵ≤0,(34)α,β,γ,δ∈Y,(35)ϵ∈l1.In particular, our weak duality and complementary slackness Theorems 2.2, 2.3 hold. Now consider the solutionαi*=βi*=γi*=δi*=ϵi*=0,for i = 1, 2, …. It is easy to check that this solution is feasible to the natural dual. Since its objective function value is equal to the value of the aforementioned feasible solution to the primal, the third conclusion in our weak duality Theorem 2.2 implies that this solution is optimal to the dual and that strong duality holds.We first present an example adapted from Martin et al. (2014) to show that the results up to this point are not sufficient to guarantee a zero duality gap between (P) and (D).Example 3.1Consider the following pair of countably semi-infinite linear programs.(36)(P2)infx1(37)x1+x3+x4+…=1,(38)−x2−(x3/3)−(x4/4)−…=0,(39)xj≥0,j=1,2,….(40)(D2)supy1(41)y1≤1,(42)−y2≤0,(43)y1−y2/i≤0,i=3,4,….Although these are semi-infinite programs, they can be seen as a (very) special case of CILPs. But note that the results in Romeijn and Smith (1998) do not apply here as the constraints in (P2) include infinitely many variables. We choose X = l1 as the subspace of primal variables. Here, H1 holds trivially. It is easy to see that the series in the two constraints of (P2) converge (absolutely) on l1 and hence H2 holds. For (D2), we choose Y = ℜ2. Then H3 holds trivially. Now, for any fixed x ∈ l1 and any fixed y ∈ ℜ2, we have,∑j=1∞|xjy1|≤|y1|∑j=1∞|xj|<∞. Thus, the series∑j=1∞|xjy1|converges to some limit L1(x, y1). Similarly,|x2y2|+∑j=3∞|xjy2|/j≤|x2y2|+|y2|∑j=3∞|xj|<∞. Thus, the series|x2y2|+∑j=3∞|xjy2|/jconverges to some limit L2(x, y2). That is, H4 holds. Finally, L1(x, y1) + L2(x, y2) is clearly finite. Thus, H5 holds and Lemma 2.1 applies. Thus, weak duality and complementary slackness hold for this example (although as we shall see now, the latter turns out to be vacuous because there are no feasible complementary solutions). Now observe that the only feasible solution of (P2) is where x1 = 1 and xj= 0 for j = 2, 3, …. Thus, the optimal value in (P2) is 1. In (D2), constraints (42) and (43) force y1 = 0 no matter what value y2 assumes. Thus, the optimal value in (D2) is 0 for a duality gap of 1 with (P2).We now provide general sufficient conditions that exclude CILPs with the undesirable outcome in Example 3.1.For any increasing sequences of positive integers Nnand Mnfor n = 1, 2, …, we define an Nn-constraint-Mn-variable truncation of (P) as(44)P(n)V(P(n))=inf∑j=1Mncjxj(45)∑j=1Mnaijxj=bi,i=1,2,…,Nn,(46)xj≥0,j=1,2,…,Mn.We denote the (possibly empty) feasible region of P(n) by F(n) and view it as a subset of X by setting variables beyond Mnto zero, that is,xMn+1=xMn+2=…=0. Let X*(n)⊆F(n)⊆X denote the set of optimal solutions to P(n). The dual of P(n) is given by(47)D(n)V(D(n))=sup∑i=1Nnbiyi(48)∑i=1Nnaijyi≤cj,j=1,2,…,Mn.We denote its (possibly empty) feasible region by G(n). We view G(n) as a subset of Y by setting variables beyond Nnto zero, that is,yNn+1=yNn+2=⋯=0. Let Y*(n)⊆G(n)⊆Y denote the set of optimal solutions to D(n).We comment on one difference between our truncations P(n), D(n), and the truncations typically used in the planning horizon method. The planning horizon method was traditionally motivated by infinite-horizon decision problems. In fact, this is what imparts the constraint matrix a staircase structure where the nth block corresponds to (a vector of) decisions made in the nth period. Thus, it is common to create truncations using n-horizon approximations of the original infinite-horizon problem. Since we do not assume such a structure or interpretation for our primal problem, it is not possible to attach a meaning to our generic truncations P(n) and D(n). This is also the reason why it is not possible to make explicit how Nnand Mndepend on n.We employ the product topology on (subspaces of)ℜN. Thus, a sequencex(n)∈ℜNconverges tox∈ℜNif and only if each real-valued component xi(n) converges to the corresponding real-valued component xiin the usual sense in ℜ. This topology, being a countable product of metric spaces ℜ, is metrizable (see Theorem 3.36 on page 89 of Aliprantis and Border (1994)) and hence the notions of compactness and sequential compactness coincide (see Theorem 3.28 on page 86 of Aliprantis and Border (1994)).Theorem 3.2Strong duality: Suppose there exist the aforementioned sequences P(n) and D(n) of finite-dimensional primal-dual problems and setsC⊆XandK⊆Ysuch thatC1.for each n,XC(n)≜(X*(n)⋂C)≠∅;for each n,YK(n)≜(Y*(n)⋂K)≠∅;for each i, the constraint functionAi·(x)=∑j=1∞aijxjfrom X → ℜ is continuous overC;for each j, the constraint functionA·j(y)=∑i=1∞aijyifrom Y → ℜis continuous overK; andthere exists a sequence of pairs(x*(n),y*(n))∈(XC(n)×YK(n))with a convergent (in the natural product topology on X × Y) subsequence (x*(nk), y*(nk))such thatlimk→∞(x*(nk),y*(nk))=(x¯,y¯)for some(x¯,y¯)∈(C×K).Then (P) and (D) have optimal solutionsx*∈Candy*∈Krespectively, and V(P) = V(D).For each i, we have∑j=1∞aijx¯j=bi. To see this, note that∑j=1∞aijx¯j=∑j=1∞aijlimk→∞xj*(nk)=limk→∞∑j=1∞aijxj*(nk)=limk→∞(∑j=1Mnkaijxj*(nk)+∑j=1+Mnk∞aijxj*(nk)︸0)=limk→∞∑j=1Mnkaijxj*(nk)=bi.The interchange of limit and infinite sum above is allowed by continuity of Ai ·(x) overC. The last equality follows from (45) because∑j=1Mnkaijxj*(nk)=bifor all sufficiently large k such thatNnk≥i.For each j, we have∑i=1∞aijyi¯≤cj. To see this, note that∑i=1∞aijyi¯=∑i=1∞aijlimk→∞yi*(nk)=limk→∞∑i=1∞aijyi*(nk)=limk→∞(∑i=1Nnkaijyi*(nk)+∑i=1+Nnk∞aijyi*(nk)︸0)=limk→∞∑i=1Nnkaijyi*(nk)≤cj.The interchange of limit and infinite sum above is allowed by continuity of A· j(y) overK. The last inequality follows from (48) because∑i=1Nnkaijyi*(nk)≤cjfor all sufficiently large k such thatMnk≥j. Thus,x¯is feasible to (P) andy¯is feasible to (D).We now show that the pair(x¯,y¯)satisfies complementary slackness, that is, thatx¯j(cj−(∑i=1∞aijy¯i))=0for each j. Observe thatx¯j(cj−∑i=1∞aijy¯i)=[limk→∞xj*(nk)](cj−∑i=1∞aijlimk→∞yi*(nk))=[limk→∞xj*(nk)][limk→∞(cj−∑i=1Nnkaijyi*(nk)−∑i=1+Nnk∞aijyi*(nk)︸0)]=limk→∞[xj*(nk)(cj−∑i=1Nnkaijyi*(nk))]︸0forlargek=0.Here, the term in square brackets is zero by complementarity of optimal solutions x*(nk) and y*(nk) to problems P(nk) and D(nk) for k large enough such thatMnk≥j. The result follows from the first conclusion in Theorem 2.3.□Conditions C1, C2, and C5 are similar to the hypotheses in Theorem 3.8 and Corollary 3.9 in Romeijn and Smith (1998), and in Lemma 4.1 in Romeijn et al. (1992). Conditions C3 and C4 do not appear in those papers because they trivially hold there as each constraint included a finite number of variables and each variable appeared in a finite number of constraints. A typical way to establish C3 and C4 would be to show that Ai ·(x) and A· j(y) are uniformly convergent onCandK,respectively. As we will demonstrate in Section 6, this could be done using, for example, the Weierstrass test Apostol (1974). Also observe that C5 holds if bothCandKare compact. By the Tychonoff product theorem (see Theorem 2.61 on page 52 of Aliprantis and Border (1994)), as in Corollary 3.9 in Romeijn and Smith (1998),Cis compact if it is given by∏j=1∞[0,uj]for some sequence u ≥ 0. Similarly,Kis compact if it is given by∏i=1∞[−vi,vi]for some sequence v ≥ 0. The central idea in our proof above is identical to that of Lemma 4.1 in Romeijn et al. (1992) and of Theorem 3.8 in Romeijn and Smith (1998). We had to be more careful in working out the details and needed the additional conditions C3 and C4 because unlike those papers, we did not assume that each constraint includes a finite number of variables and each variable appears in a finite number of constraints.In light of this strong duality theorem, we close this section by providing insight into why there is a duality gap between problems (P2) and (D2) presented in Example 3.1 above. A natural choice for the truncations of (P2) is to construct them using the first n variables, for n = 1, 2, …. This yields(P2(n))infx1x1+x3+x4+⋯+xn=1,−x2−(x3/3)−(x4/4)−⋯−xn/n=0,xj≥0,j=1,2,…,n,and its dual(D2(n))supy1y1≤1,−y2≤0,y1−y2/i≤0,i=3,4,…,n.The solution x*(n) = (1, 0, 0, …) ∈ l1 is optimal to (P2(n)) for all n; the solution y*(n) = (1, n) ∈ ℜ2 is optimal to (D2(n)) for all n. It is easy to see that conditions C1–C4 in Theorem 3.2 hold if we chooseC={(1,0,0,…)}(a singleton) andK=ℜ2. Specifically, C1 holds because (P2(n)) has an optimal solution inCfor every n; similarly, C2 holds because (D2(n)) has an optimal solution inKfor every n. C3 holds trivially becauseCis a singleton. Finally, C4 holds because each constraint in (D2) only includes a finite number of variables. However, the sequence y*(n) = (1, n) in ℜ2 has no convergent subsequence (and hence of course has no subsequence that converges to an optimal solution of (D2)). Specifically, condition C5 does not hold. This discussion shows that we cannot drop C5 and still hope for a zero duality gap.This failure of C5 to hold is not surprising. In fact, Martin et al. (2014) have shown that the duality gap between (P2) and its dual cannot be closed if the dual is written using the ordinary transpose of the constraint matrix in (P2) (as one might naturally prefer to do to replicate finite-dimensional intuition). It appears that the only way to close the duality gap between (P2) and its dual is to work in an algebraic dual space whose complete characterization is not known, where optimal dual variables cannot be interpreted as prices, and where the constraints are expressed using dual functionals that are not countably additive. Martin et al. (2014) call this structure of the algebraic dual problem that would close the duality gap “undesirable.”Our results here provide structure that is sufficient for CILPs to behave similar to finite-dimensional LPs. We further strengthen this statement in the next section by extending finite-dimensional properties of the Lagrangian function, primal optimal value function, and saddle points.In finite-dimensional LPs, one can define the so-called Lagrangian function and show that a primal-dual pair of solutions is optimal if and only if this pair is a saddle point of the Lagrangian function. One can also define a primal optimal value function (this is a function of the right hand side vector) and show that it is convex. Finally, the Lagrangian function is a linear support of the optimal value function – this last result allows a price interpretation of optimal dual variables. See Bertsimas and Tsitsiklis (1997), Dantzig (1963), Gale (1989), Luenberger (1969), Murty (1983) for detailed, insightful discussions of these elegant properties of finite-dimensional LPs. Romeijn and Smith (1998) provided a transversality condition and their aforementioned Assumption 3.1 under which these properties extend to CILPs where constraints included a finite number of variables and variables appeared in a finite number of constraints. Thus, the question arises as to whether or not these properties extend to our more general CILPs. We briefly answer this question in the affirmative in this section.Let subspaces X and Y be defined as in Section 2. Let Z be the subspace of allz∈ℜNsuch that the series∑i=1∞ziyiconverges. Note that b ∈ Z. We define the Lagrangian function as(49)Λ(x,y;z)=∑j=1∞cjxj+∑i=1∞yi(zi−Ai·(x)),∀x∈X,∀y∈Y,∀z∈Z.For any z ∈ Z, we define Fz≜{x ∈ X; Ai ·(x) = zi, i = 1, 2, …; x ≥ 0} as the set of feasible solutions to (P) when the right hand side is z. Let ϕ be the primal optimal value function given byϕ(z)=infx∈FzC(x),for z ∈ Z. Recall here that C(x) was defined in the statement of H1 in Section 2 as the series in the objective function of (P). One can show, by standard arguments in convex optimization, that ϕ is convex (see Proposition 1 in Chapter 8 on page 216 of Luenberger (1969)). This statement provides an extension of Lemma 4.1 in Romeijn and Smith (1998) to our setting. We then have the following extension of Theorem 4.2 in Romeijn and Smith (1998).Theorem 4.1Lagrangian support of the optimal value function:Suppose x*, y*is a pair of complementary solutions feasible to (P) and (D), respectively. Then, Λ(x*, y*; z) ≤ ϕ(z) for all z ∈ Z, and Λ(x*, y*; b) = ϕ(b).Replicates standard steps as in the finite-dimensional case. Specifically, we fix any z ∈ Z and first show that Λ(x*, y*; z) ≤ C(x) for all x ∈ Fz. We have,Λ(x*,y*;z)=∑j=1∞cjxj*+∑i=1∞yi*(zi−Ai·(x*))=∑j=1∞cjxj*+∑i=1∞yi*(zi−bi)(byfeasibilityofx*)=∑i=1∞ziyi*(bycomplementaryslackness)=∑i=1∞Ai·(x)yi*(becausex∈Fz)=∑i=1∞∑j=1∞aijxjyi*=∑j=1∞∑i=1∞aijyi*xj(by(5))≤∑j=1∞cjxj(byfeasibilityofy*,andbecausex≥0)=C(x).This implies thatΛ(x*,y*;z)≤infx∈FzC(x)=ϕ(z)as required for the first claim. Finally,Λ(x*,y*;b)=∑j=1∞cjxj*+∑i=1∞yi*(bi−Ai·(x*))=∑j=1∞cjxj*=ϕ(b)as required for the second claim.□The pair 0 ≤ x* ∈ X and y* ∈ Y is said to be a saddle point of the Lagrangian function for (P) and (D) if(50)Λ(x*,y;b)≤Λ(x*,y*;b)≤Λ(x,y*;b),∀0≤x∈X,∀y∈Y.We then have the following extension of Theorem 4.3 in Romeijn and Smith (1998).Theorem 4.2Saddle point property: The following two statements are equivalent.1.Solutions 0 ≤ x* ∈ X and y* ∈ Y are a saddle point of the Lagrangian.Solutions x* ∈ X and y* ∈ Y are feasible to (P) and (D), respectively, and complementary.The first half of the proof replicates standard steps as in the finite-dimensional case. To show that the first statement implies the second, we proceed as follows. Suppose the first statement holds. From the definition of the Lagrangian function in (49), the first inequality in (50) implies that(51)∑i=1∞yi(bi−Ai·(x*))≤∑i=1∞yi*(bi−Ai·(x*)),∀y∈Y.This means that Ai ·(x*) = bifor all i. Suppose not. Then there is an i, let us call it i*, such thatAi*·(x*)≠bi*. We consider three cases and in all three cases we define yisuch thatyi=yi*for all i ≠ i*; in all three cases, we setyi*so as to get a contradiction to inequality (51). The first case is whereyi**(bi*−Ai*·(x*))>0; here, we setyi*=2yi*. The second case is whereyi**(bi*−Ai*·(x*))<0; here, we setyi*=0. The third case is whereyi**=0; here we setyi*=−1oryi*=1,respectively, depending on whetherbi*−Ai*·(x*)<0orbi*−Ai*·(x*)>0. This discussion shows that x* is feasible to (P). From the definition of the Lagrangian function in (49), the second inequality in (50) implies that(52)∑j=1∞cjxj*−∑i=1∞yi*Ai·(x*)≤∑j=1∞cjxj−∑i=1∞yi*Ai·(x),∀0≤x∈X.That is,(53)0≤∑j=1∞cj(xj−xj*)−∑i=1∞yi*(Ai·(x)−Ai·(x*)),∀0≤x∈X,(54)=∑j=1∞(cj−∑i=1∞aijyi*)(xj−xj*)∀0≤x∈X(by(5)).Fix any j. Define a 0 ≤ x ∈ X such thatxk=xk*for k ≠ j andxj=xj*+1. Substituting this x in (54), we see thatcj−∑i=1∞aijyi*≥0. This shows that y* is feasible to (D). It remains to show that x* and y* are complementary. Toward this end, again fix any j. Define an 0 ≤ x ∈ X such thatxk=xk*for k ≠ j and xj= 0. Inequality (54) then implies that(cj−∑i=1∞aijyi*)xj*≤0. Now define another x such thatxk=xk*for k ≠ j andxj=2xj*. Inequality (54) then implies that(cj−∑i=1∞aijyi*)xj*≥0. This discussion shows that(cj−∑i=1∞aijyi*)xj*=0. Thus, x* and y* are indeed complementary. That is, the second statement holds.The second half of the proof replicates standard steps as in the finite-dimensional case. To show that the second statement implies the first, we proceed as follows. Suppose that the second statement holds. We first observe thatΛ(x*,y*;b)=∑j=1∞cjxj*+∑i=1∞yi*(bi−Ai·(x*))=∑j=1∞cjxj*=∑i=1∞yi*bi.Here, the first equality follows from the definition of the Lagrangian function, the second equality follows because bi= Ai ·(x*) by feasibility of x* to (P) and the third equality follows by strong duality as in the complementary slackness Theorem 2.3. Now consider any fixed 0 ≤ x ∈ X. We have,Λ(x,y*;b)=∑j=1∞cjxj+∑i=1∞yi*(bi−Ai·(x))=∑j=1∞cjxj+∑i=1∞yi*bi−∑i=1∞yi*∑j=1∞aijxj=∑j=1∞cjxj+∑i=1∞yi*bi−∑j=1∞∑i=1∞aijyi*xj(by(5))≥∑j=1∞cjxj+∑i=1∞yi*bi−∑j=1∞cjxj(byfeasibilityofy*,andbecausex≥0)=∑i=1∞yi*bi=Λ(x*,y*;b).Now consider any fixed y ∈ Y. We have,Λ(x*,y;b)=∑j=1∞cjxj*+∑i=1∞yi(bi−Ai·(x*))=∑j=1∞cjxj*(byfeasibilityofx*)=Λ(x*,y*;b).This shows that the pair x*, y* is a saddle point. That is, the first statement holds.□A countable-state MDP (Puterman, 1994; Ross, 1983) is a dynamic system that is observed by a decision maker at the beginning of each time-period to be in some states∈S,whereS≜{0,1,2,…}. The decision maker then chooses an action a from a finite setAwith cardinality|A|. When action a is chosen in state s, the system makes a transition to states′∈Swith probability p(s′|s, a), incurring cost 0 ≤ c(s, a; s′) ≤ c < ∞. Let c(s, a) be the expected cost incurred on choosing action a in state s. That is,c(s,a)=∑s′p(s′|s,a)c(s,a;s′),and note that 0 ≤ c(s, a) ≤ c. This procedure continues ad infinitum. The decision maker’s goal is to find a decision rule that minimizes the total infinite-horizon discounted expected cost when the discount factor is 0 < α < 1. Examples that fit this framework include stochastic decision problems in equipment replacement, quality control, inventory management, investment management, queuing, and numerous others.In this countable-state MDP, we concede that the assumption that costs are bounded by c, although not uncommon (see, for instance, Evans (1969), Puterman (1994), Ross (1983)), could be restrictive from a practical viewpoint. This assumption can be relaxed by following a more technically demanding approach as in Assumptions 6.10.1 and 6.10.2 in Puterman (1994). For expository simplicity, we limit our discussion below, as in Chapter 2 of Ross (1983), to the easier case where costs are bounded and later conclude this section with an example adapted from Puterman (1994) where this assumption holds. We believe that the simpler case here provides a sufficient demonstration of the ideas above.Let β(s) be a sequence of positive numbers indexed by statess∈Ssuch that∑s∈Sβ(s)=1. Let v(s) be the minimum infinite-horizon discounted expected cost incurred when starting in state s;v(·):S→ℜis called the optimal cost-to-go function. It is shown in Ross (1983) that the optimal costs-to-go equal the (unique) optimal values of variables y(s) in the CILP(D3)sup∑s∈Sβ(s)y(s)y(s)−α∑s′∈Sp(s′|s,a)y(s′)≤c(s,a),fors∈S,a∈A,y∈l∞.In particular, variables y(s) satisfy 0 ≤ y(s) ≤ c/(1 − α) for alls∈Swithout loss of optimality. We have chosen l∞, the space of bounded sequences, as the variable space Y in this problem. For any y ∈ l∞, the objective function series in (D3) converges (absolutely) because∑s∈S|β(s)y(s)|≤∑s∈S|β(s)∥y(s)|≤(sups∈S|y(s)|)∑s∈Sβ(s)=sups∈S|y(s)|<∞.Thus H3 holds. Problem (D3) is similar to (D) in structure, and as in Ross (1983), we see it as the (natural) dual of the CILP(P3)inf∑s∈S∑a∈Ac(s,a)x(s,a)∑a∈Ax(s,a)−α∑s′∈S∑a∈Ap(s|s′,a)x(s′,a)=β(s),fors∈S,x(s,a)≥0,fors∈S,a∈A.An abstract infinite-dimensional LP approach to MDPs is discussed in Chapter 12 of Hernandez-Lerma and Lasserre (2002). However, it seems difficult, if not impossible, to directly apply duality results there to countable-state MDPs. Specifically, the abstract results in Hernandez-Lerma and Lasserre (2002) rely on interior point, or more generally, closeness conditions to establish the absence of a duality gap (see Theorems 3.9 and 3.11 from Anderson and Nash (1987)). Unfortunately, it is well-known that such conditions are difficult to establish in subspaces ofℜN(Martin et al., 2014; Ponstein, 1981; Romeijn & Smith, 1998). To the best of the author’s knowledge, papers that apply generic CILP duality results specifically to CILP formulations of infinite-horizon countable-state MDPs under the discounted cost criterion are essentially non-existent. For instance, Ross (1983) presented our problems (P3) and (D3) as “duals” without proving any duality results. Puterman (1994) did not discuss the LP approach for countable-state MDPs. Evans (1969) proved duality results for countable-state MDPs but only for the finite-horizon case. Altman (1994) focused on constrained MDPs and presented a counterpart of our problem (P3) for that more general case, but did not explicitly analyze its relation with a counterpart of our problem (D3). Altman (1999), in Chapters 8–10, focused on constrained MDPs with the total cost and the discounted cost criteria and showed that there is no duality gap between the corresponding primal and the dual LPs (the author thanks Ilbin Lee of the University of Michigan, Ann Arbor, for bringing these chapters to his attention). Altman’s treatment was technically more demanding than and different from our approach here. Lee et al. (2014) simplified Altman’s approach and applied it to discounted countable-state MDPs with unbounded rewards. The analysis there is entirely different from our presentation here. Specifically, unlike us, Lee et al. do not use weak duality and then complementary slackness to prove strong duality between two explicitly derived complementary solution. Instead, they use the idea of an occupancy measure from Altman, which calls for transforming the discounted expected cost MDP into an expected cost MDP with an absorbing state. They then utilize various known results from Altman, and from Hernandez-Lerma and Lasserre (2002) about this transformed MDP to prove strong duality. As such, the approach in Lee et al. appears more complicated than our proof here. Hordijk and Lasserre (1994) and others considered infinite-dimensional LP formulations of countable-state MDPs but they focused on the average cost criterion. Ghate and Smith (2013) and Lee et al. (2013) derived weak duality, complementary slackness and strong duality for infinite-horizon, finite-state, non-stationary MDPs, which can be seen as a special case of countable-state MDPs. The proofs there exploited the acyclic nature of state transitions, which is a characteristic of non-stationary MDPs (and this does not hold here). In his doctoral dissertation Ghate (2006), the author proved duality results for CILP formulations of countable-state MDPs under the restrictive assumption that the set of states reachable in one transition from any state-action pair is finite. Finally note that the results in Romeijn et al. (1992) and Romeijn and Smith (1998) do not apply here because some of the constraints could include infinitely many variables and some of the variables could appear in infinitely many constraints. We show below that duality results between (P3) and (D3) follow easily from our general theory above.For any feasible variables x(s, a) in (P3), we have,∑s∈S∑a∈Ax(s,a)=1/(1−α). To see this, by adding the equality constraints in (MDP) over alls∈S,we get,∑s∈S∑a∈Ax(s,a)−α∑s∈S∑s′∈S∑a∈Ap(s|s′,a)x(s′,a)=∑s∈Sβ(s)=1.Since p(s|s′, a)x(s′, a) ≥ 0, we can interchange the order of summation in the above equation to get∑s∈S∑a∈Ax(s,a)−α∑s′∈S∑a∈A∑s∈Sp(s|s′,a)x(s′,a)=1.But since∑s∈Sp(s|s′,a)=1,the above equation yields∑s∈S∑a∈Ax(s,a)=1/(1−α)as claimed.In view of this observation, we choose X = l1, the space of absolutely summable sequences, as the variable space for (P2). For this choice, the series in the objective function converges (absolutely) because∑s∈S∑a∈A|c(s,a)x(s,a)|≤c∑s∈S∑a∈A|x(s,a)|<∞.That is, H1 holds. Similarly, for any x ∈ l1, the series∑s′∈S∑a∈Ap(s|s′,a)x(s′,a)converges (absolutely) because∑s′∈S∑a∈A|p(s|s′,a)x(s′,a)|≤∑s′∈S∑a∈A|p(s|s′,a)||x(s′,a)|≤∑s′∈S∑a∈A|x(s′,a)|<∞.Thus, H2 holds.We now show that H4, H5 hold for the pair X = l1 and Y = l∞. Specifically, consider any x ∈ l1 and any y ∈ l∞ and a fixeds∈S. We have,∑a∈A|x(s,a)y(s)|+∑s′∈S∑a∈A|−αp(s|s′,a)x(s′,a)y(s)|≤sups∈S|y(s)|(∑a∈A|x(s,a)|+∑s′∈S∑a∈A|x(s′,a)|)<∞.Thus, the series∑a∈A|x(s,a)y(s)|+∑s′∈S∑a∈A|−αp(s|s′,a)x(s′,a)y(s)|converges to some limit Ls(x, y(s)) and H4 holds. Moreover,Ls(x,y(s))≤sups∈S|y(s)|(∑a∈A|x(s,a)|+∑s′∈S∑a∈Ap(s|s′,a)|x(s′,a)|). Consequently,∑s∈SLs(x,y(s))≤sups∈S|y(s)|(∑s∈S∑a∈A|x(s,a)|+∑s∈S∑s′∈S∑a∈Ap(s|s′,a)|x(s′,a)|)=sups∈S|y(s)|(∑s∈S∑a∈A|x(s,a)|+∑s′∈S∑a∈A|x(s′,a)|∑s∈Sp(s|s′,a)︸1)=sups∈S|y(s)|(∑s∈S∑a∈A|x(s,a)|+∑s′∈S∑a∈A|x(s′,a)|)<∞.Thus,∑s∈SLs(x,y(s))converges as required for H5. Therefore, Lemma 2.1 applies, and consequently, our weak duality and complementary slackness results in Theorems 2.2 and 2.3 also hold.We now use Bellman’s equations to construct a pair of complementary feasible solutions to (P3) and (D3). The first item in Theorem 2.3 then implies that strong duality holds between (P3) and (D3). Suppose y*(s), fors∈S,is an optimal solution to (D3). Recall that such a solution is known to exist from the theory of countable-state MDPs in Ross (1983) (in fact, this can also be proven independently from scratch without relying on the relationship between (D3) and the corresponding MDP). As mentioned earlier, y*(s) equals the optimal cost-to-go v(s). It is also known that these optimal costs-to-go uniquely satisfy Bellman’s equationsy*(s)=c(s,a*(s))+α∑s′∈Sp(s′|s,a*(s))y*(s′),∀s∈S,where a*(s) is an optimal action in state s. By breaking ties arbitrarily, we thus associate a single optimal action a*(s) with each states∈Sto construct an optimal policy π*. LetA˜(s)denote the set of actionsa∈Athat are not optimal in states∈S. Then,y*(s)<c(s,a)+α∑s′∈Sp(s′|s,a)y*(s′),∀a∈A˜(s),∀s∈S.That is, the inequality constraint in (D3) for state s is not active ata∈A˜(s)(otherwise this action would be optimal in s). We definex*(s,a)≜∑s′∈Sβ(s′)∑n=1∞αn−1Pπ*(Sn=s,An=a|S1=s′),∀s∈S,∀a∈A.This definition is identical to Eq. (6.9.3) in Puterman (1994). Here, we have used the notationPπ*(Sn=s,An=a|S1=s′)to denote the probability that the (random) state Snat the n decision epoch will be s and the action Anchosen by policy π* in that state will be a, given that the (random) initial state S1 was s′. We make the key observation that x*(s, a) = 0 fora∈A˜(s)becausePπ*(Sn=s,An=a|S1=s′)=0as policy π* does not choose this action in state s. This discussion shows that y* and x* are complementary solutions. It remains to prove that x* is feasible to (P3). This in fact follows by replicating the algebra shown in the proof of Theorem 6.9.1(a) on page 225 of Puterman (1994) (note that although Puterman’s proof is for finite-state MDPs, the steps there are valid in our case as well; this is because all requisite interchanges in the order of sums are allowed by the non-negativity of the summands).We conclude this section by applying our duality results on countable-state MDPs to an example adapted from Section 6.10.4 in Puterman (1994). Consider a stochastic equipment replacement model where the equipment can be in any one of states {0, 1, 2, …} at the beginning of each time-period. State 0 corresponds to a new equipment; larger states represent poorer equipment conditions. At the beginning of each time-period, the decision-maker can either choose to replace the equipment with a new one (action 0) or keep the existing equipment (action 1). Between two decision epochs, the condition of the equipment worsens by i ≥ 0 states with probability q(i). This leads to the following transition probabilities:(55)p(s′|s,0)=q(s′),s′≥0,and(56)p(s′|s,1)={0,ifs′<sq(s′−s),ifs′≥s.Observe that these transition probabilities do not satisfy the finite-reachability property assumed in the author’s doctoral dissertation (Ghate, 2006). The costs in this model are given by(57)c(s,0)=γ+h(0),andc(s,1)=h(s),where γ > 0 is the cost of buying a new piece of equipment and h(s) is the cost of operating an equipment in condition s for one period. It is natural to expect that h(s) is non-decreasing in s as it should be cheaper to operate an equipment that is in a better condition. As a special case, we consider the bounded, non-negative, non-decreasing cost function h(s) = 1 − exp ( − s). Thus, the costs in (57) are bounded between zero and γ + 1. Then, weak duality, complementary slackness, and strong duality hold for CILP formulations of this stochastic equipment replacement example.CILPs in the existing literature have appeared mostly in the context of infinite-horizon planning problems. In this section, we highlight broader applicability of CILPs and also of our approach to duality through a robust optimization problem.We first describe the nominal version of this problem. Consider an auctioneer with I units of a product on hand that she wishes to sell in a single, multi-unit Vickrey auction. In this mechanism, the clearing price equals the highest losing bid. Suppose each bidder’s bid is independently uniformly distributed over the interval [0, 1], and that the number of participating bidders is a random variable N with support {0, 1, 2, …}. We use xjto denote the probability that N = j, for j = 0, 1, 2, …. The auctioneer needs to decide the number of units to put up for auction, that is, the auction’s lot-size, so as to maximize expected revenue. The key economic trade-off is roughly as follows. For any fixed number of participating bidders, a large lot-size reduces the clearing price for each unit whereas a small lot-size increases the clearing price for each unit; as shown in Chen, Ghate, and Tripathi (2011), the resulting expected revenue function is unimodal. Moreover, any remaining units (due to an insufficient number of bidders) can be scrapped at a salvage price of 0 ≤ s < 1 per unit and this adds another dimension to the economic trade-off. If the lot-size is l ∈ {0, 1, 2, …, I}, then the expected revenue is given byψ(l;x)≜l∑j=l+1∞(1−(l+1)/(j+1))xj+(I−l)s∑j=l+1∞xj+Is∑j=0lxj.In this expression, the first term is the expected revenue earned in a “successful” multi-unit Vickrey auction with l units. The term “successful” here means that the number of participating bidders is more than the lot-size. Specifically, in this first term, the expression (1 − (l + 1)/(j + 1)) is the expected value of the l + 1st largest among j ≥ l + 1 independent uniform[0, 1] bids. The second term is the expected revenue earned by selling the remaining units when the auction with l units is successful. The third term equals the expected revenue earned by selling the remaining units when this auction is “unsuccessful”. The term “unsuccessful” here means that the number of participating bidders is no more than the lot-size and hence the auction is cancelled, thus returning the l units back to the inventory. A detailed discussion of successful and unsuccessful auctions is included in Chen et al. (2011); Pinker, Seidmann, and Vakrat (2003). The above formula simplifies toψ(l;x)=l∑j=l+1∞((j−l)/(j+1)−s)xj+Is,and the auctioneer’s problem is given bymaxl∈{0,1,…,I}ψ(l;x). We refer the reader to Chen et al. (2011) for a detailed discussion of this problem and its many extensions to other auction mechanisms and other bid distributions.Here, we are interested in a robust counterpart of this nominal problem. In this robust counterpart, the auctioneer is uncertain about the probability mass function xj, for j = 0, 1, 2, …, of N. Following a standard approach in robust optimization (Ben-Tal, Ghaoui, & Nemirovski, 2009), we use an interval uncertainty model for these probabilities. Specifically, let λ > 0, and we define a Poisson reference distribution with probability mass function pj(λ) = e−λλj/j!, for j = 0, 1, 2, …. We assume that the probability mass function belongs to the uncertainty setX(λ;δ)given byX(λ;δ)≜{x≥0:|pj(λ)−xj|/pj(λ)≤δ,j=1,2,…;∑j=0∞xj=1},for some 0 < δ < 1. That is, the probabilities xj, for j = 1, 2, …, are within a relative absolute distance δ from the probabilities pj(λ) of the reference Poisson distribution. In our robust counterpart, the auctioneer maximizes the worst-case expected revenue over all probability mass functions inX(λ;δ). That is, we now wish to solvemaxl∈{0,1,…,I}minx∈X(λ;δ)ψ(l;x).This robust problem calls for solving a finite set of CILPs. Specifically, for each fixed l ∈ {0, 1, …, I}, the inner problemminx∈X(λ;δ)ψ(l;x)is a CILP that we will discuss in detail in this section.For each fixed lot-size l ∈ {0, 1, 2, …, I}, after ignoring the constant term Is and the multiplier l in the formula for ψ(l; x), the inner problem is given by(58)min∑j=l+1∞((j−l)/(j+1)−s)xj,(59)∑j=0∞xj=1,(60)(1−δ)pj(λ)≤xj,j=1,2,…,(61)xj≤(1+δ)pj(λ),j=1,2,…,(62)xj≥0,j=0,1,2,….We first note that when s = 0, the worst-case distribution is obtained by setting xj= (1 − δ)pj(λ) for j = 1, 2, … and setting x0 = 1 − (1 − δ)(1 − e−λ) because the coefficients of xjin the objective function are all non-negative. This is no longer the case when s > 0.We let cj(l)≜0, for j = 0, 1, …, l, and cj(l)≜(j − l)/(j + 1) − s, for j = l + 1, l + 2, …. Note that 0 ≤ cj(l) ≤ 1 for all j. After subtracting nonnegative surplus variables ujfrom constraints (60), and adding nonnegative slack variables vjto constraints (61), we obtain the following pair of primal and dual problems (P4l) and (D4l) that are identical in form to (P) and (D), respectively.(63)(P4l)min∑j=0∞cj(l)xj,(64)∑j=0∞xj=1,(65)xj−uj=(1−δ)pj(λ),j=1,2,…,(66)xj+vj=(1+δ)pj(λ),j=1,2,…,(67)xj≥0,j=0,1,2,…,(68)uj≥0,j=1,2,…,(69)vj≥0,j=1,2,….(70)(D4l)max(1−δ)∑j=1∞pj(λ)yj+(1+δ)∑j=1∞pj(λ)zj+θ,(71)θ≤c0(l),(72)θ+yj+zj≤cj(l),j=1,2,…,(73)yj≥0,j=1,2,…,(74)zj≤0,j=1,2,….Again notice that the results in Romeijn et al. (1992) and Romeijn and Smith (1998) do not apply because constraint (64) includes infinitely many variables.We first choose appropriate sequence spaces for the variables in the above primal-dual pair. Let X be a subspace ofℜNwith the property that for any x = (x0, x1, …) ∈ X, there exists a real number τ(x) such that |xj| ≤ τ(x)pj(λ) for j = 1, 2, …. Also let U be a subspace ofℜNwith the property that for any u = (u1, u2, …) ∈ U, there exists a real number τ(u) such that |uj| ≤ τ(u)pj(λ) for j = 1, 2, …. We choose the product space X × U × U for variables (x, u, v) in (P4l). Let Y be a subspace ofℜNwith the property that∑j=1∞pj(λ)|yj|<∞for any y = (y1, y2, …) ∈ Y. We choose the product space ℜ × Y × Y for variables (θ, y, z) in (D4l).For these choices of subspaces for the variables in (P4l) and (D4l), it is not difficult to confirm that the various series involved converge. For instance, note that, for every x ∈ X,∑j=0∞|cj(l)xj|≤∑j=0∞|cj(l)∥xj|≤∑j=0∞|xj|≤|x0|+τ(x)∑j=0∞pj(λ)=|x0|+τ(x),and hence the objective function in (P4l) converges (absolutely). Similarly, the series in constraint (64) converges (absolutely). So does the series in the objective function of (D4l). In short, H1, H2, H3 hold.Now consider any fixed (x, u, v) ∈ X × U × U and any fixed (θ, y, z) ∈ ℜ × Y × Y. We have,∑j=0∞|xj∥θ|≤|θ||x0|+|θ|τ(x)∑j=0∞pj(λ)=|θ||x0|+|θ|τ(x).Thus, the series∑j=0∞|xjθ|converges to some limit Lθ(x). Similarly, we let Lj(x, u, yj) = |xjyj| + |ujyj|, and Lj(x, v, zj) = |xjzj| + |vjzj|. Thus, H4 holds. Moreover,|θ||x0|+|θ|τ(x)+∑j=1∞|xjyj|+∑j=1∞|ujyj|+∑j=1∞|xjzj|+∑j=1∞|vjzj|≤|θ|τ(x)+τ(x)∑j=1∞pj(λ)|yj|+τ(u)∑j=1∞pj(λ)|yj|+τ(x)∑j=1∞pj(λ)|zj|+τ(v)∑j=1∞pj(λ)|zj|<∞.As a result, H5 holds and thus Lemma 2.1 applies, and (P4l) and (D4l) satisfy weak duality and complementary slackness as in Theorems 2.2 and 2.3.Recall that for the countable-state MDPs above, strong duality was established by explicitly deriving a pair of feasible complementary solutions and then applying the first item from Theorem 2.3. Here we pursue an alternate approach to prove strong duality. We show that conditions C1–C5 in Theorem 3.2 hold for (P4l) and (D4l).It is easy to see from the constraints in (P4l) that its variables belong to the setC(λ;δ)≜{(x,u,v)∈X×U×U:0≤x0≤1,(1−δ)pj(λ)≤xj,uj,vj≤(1+δ)pj(λ),j=1,2,…}.We also have,Lemma 6.1There exists a finite constant M(l) > 0 such that θ ≥ −M(l) without loss of optimality in (D4l). Similarly, there exist finite constants Kj(l) > 0 and Lj(l) > 0 such that yj≤ Kj(l) and − Lj(l) ≤ zj(l), for j = 1, 2, …, without loss of optimality in (D4l). As a result, solutions to (D4l) belong to the setKl≜{(θ,y,z)∈(ℜ×Y×Y):|θ|≤M(l);|yj|≤Kj(l),j=1,2,…;|zj|≤Lj(l),j=1,2,…}without loss of optimality.The objective function in (D4l) can be rewritten asθp0(λ)+∑j=1∞pj(λ)(yj+zj+θ)+δ∑j=1∞pj(λ)(zj−yj). For any feasible solution to (D4l), this objective is bounded above by 1 + θp0(λ). This follows since zj− yj≤ 0 and yj+ zj+ θ ≤ cj(l) ≤ 1. Also observe that the solution yj= zj= 0 for j = 1, 2, …, andθ=infj=0,1,…cj(l)=min{0,(1/(l+1))−s}is feasible to (D4l). Thus, the optimal objective value in (D4l) is bounded below by min {0, (1/(l + 1)) − s} ≥ −s. These upper and lower bounds imply that θ ≥ −(1 + s)/p0(λ) without loss of optimality, as required to establish the first claim.For the second claim, suppose y1 does not have the claimed upper bound. Thus, for each fixed number K1(l) > 0, there exists an optimal solution with y1 > K1(l). Similar to above, we rewrite the objective function asθp0(λ)+∑j=1∞pj(λ)(yj+zj+θ)+δ(z1−y1)p1(λ)+δ∑j=2∞pj(λ)(zj−yj). But since θ ≤ 0, y1 + z1 + θ ≤ 1, − θ ≤ M(l), z1 ≤ 0, and zj− yj≤ 0, the objective function is bounded above by 1 − δp1(λ)y1. Then, by choosing a large enough y1, we can make the optimal objective value smaller than − 2s. This contradicts the fact that the optimal objective value is at least − s.For the third claim, suppose z1 does not have the claimed upper bound. Thus, for each fixed number L1(l) > 0, there exists an optimal solution with z1 < −L1(l). Again, by algebra similar to above, the objective function value can be bounded above by 1 + δp1(λ)z1. By choosing a sufficiently negative z1, we can make the optimal objective value smaller than − 2s to get a contradiction.□We use (P4l(n)) to denote the finite-dimensional truncation of (P4l) that only includes non-negative variables x0, x1, …, xn; u1, u2, …, un; v1, v2, …, vn; constraint (64); and constraints (65) and (66) for j = 1, 2, …, n. We use (D4l(n)) to denote its dual. It is easy to see that both these problems have feasible solutions and the variable bounds in setsClandKlalso hold for (P4l(n)) and (D4l(n)). Thus, these problems have optimal solutions that belong to setsClandKl. Moreover,ClandKlare compact. This shows that conditions C1, C2, and C5 hold. Condition C4 holds because each constraint in (D4l) includes at most three variables. To show that condition C3 holds, we prove that the series∑j=0∞xjconverges uniformly overCl. To see this, first note that for any(x,u,v)∈Cl,we have, |x0| ≤ 1, and |xj| ≤ (1 + δ)pj(λ), for j = 1, 2, …. As a result,∑j=0∞|xj|≤1+(1+δ)∑j=1∞pj(λ)≤3. Uniform convergence then follows from the Weierstrass test.Our nominal problem here focused on choosing the lot-size in a single auction. An extension of this nominal problem to sequential auctions was studied in Chen et al. (2011). This extension used a stochastic dynamic programming formulation. The robust counterpart of this lot-size selection problem in sequential auctions could also be developed by applying the theory of robust dynamic programming. The inner problem in the resulting robust Bellman’s equations will also be a CILP, and our main ideas in this section will be applicable to that inner problem as well.Finally, although our results in Sections 2 and 3 were illustrated here via a robust auction-design problem, these ideas will also apply to inner problems in other more general robust optimization problems that employ the interval uncertainty model using a probability mass function with countable support.

@&#CONCLUSIONS@&#
