@&#MAIN-TITLE@&#
Automatic analysis of speech F0 contour for the characterization of mood changes in bipolar patients

@&#HIGHLIGHTS@&#
We present an automatic method for estimating features describing speech F0 contour.Analysis on acted emotional speeches distinguished high- from low-arousal emotions.Intra-subject analysis on bipolar patients found out differences between mood states.The results on bipolar patients are subject-specific and task-dependent.Results on controls confirmed a good specificity of the proposed features.

@&#KEYPHRASES@&#
F0 contour analysis,Bipolar disorders,Emotional speech,Voiced/unvoiced segmentation,

@&#ABSTRACT@&#
Bipolar disorders are characterized by a mood swing, ranging from mania to depression. A system that could monitor and eventually predict these changes would be useful to improve therapy and avoid dangerous events. Speech might convey relevant information about subjects’ mood and there is a growing interest to study its changes in presence of mood disorders. In this work we present an automatic method to characterize fundamental frequency (F0) dynamics in voiced part of syllables. The method performs a segmentation of voiced sounds from running speech samples and estimates two categories of features. The first category is borrowed from Taylor's Tilt intonational model. However, the meaning of the proposed features is different from the meaning of Taylor's ones since the former are estimated from all voiced segments without performing any analysis of intonation. A second category of features takes into account the speed of change of F0. In this work, the proposed features are first estimated from an emotional speech database. Then, an analysis on speech samples acquired from eleven psychiatric patients experiencing different mood states, and eighteen healthy control subjects is introduced. Subjects had to perform a text reading task and a picture commenting task. The results of the analysis on the emotional speech database indicate that the proposed features can discriminate between high and low arousal emotions. This was verified both at single subject and group level. An intra-subject analysis was performed on bipolar patients and it highlighted significant changes of the features with different mood states, although this was not observed for all the subjects. The directions of the changes estimated for different patients experiencing the same mood swing, were not coherent and were task-dependent. Interestingly, a single-subject analysis performed on healthy controls and on bipolar patients recorded twice with the same mood label, resulted in a very small number of significant differences. In particular a very good specificity was highlighted for the Taylor-inspired features and for a subset of the second category of features, thus strengthening the significance of the results obtained with patients. Even if the number of enrolled patients is small, this work suggests that the proposed features might give a relevant contribution to the demanding research field of speech-based mood classifiers. Moreover, the results here presented indicate that a model of speech changes in bipolar patients might be subject-specific and that a richer characterization of subject status could be necessary to explain the observed variability.

@&#INTRODUCTION@&#
Bipolar disorder is a pathology characterized by cyclic variations of mood. Subjects can experience very different states ranging from hypomania to depression, passing through euthymia. Physicians would strongly benefit from a tool that could support them in formulating diagnoses and monitor patients’ status between two successive visits when they are not hospitalized. Such a support system could alert physicians if patient status gets worst, optimize therapy and reassure patients. Several studies are concerned with the analysis of biomedical signals to detect physiological correlates of mood changes [1–3]. In particular, the analysis of speech signals could be used to obtain relevant information about patient mood state. In fact, speech-derived characteristics have been shown to vary in patients affected by psychiatric disorders with respect to healthy subjects. Currently, different categories of features are studied including glottal, prosodic, spectral and energy-related features. Each category of features can be tested on its own or in a combined approach. Prosodic and spectral features have been found to vary in patients suffering from depression with respect to healthy subjects [4–10]. Prosodic, glottal, cepstral, spectral and Teager Energy Operator (TEO) related features were used in detecting depression in adolescents [8] by analysing speeches during family interaction. Although good classification results were found combining the different categories of features, TEO was found to carry the most relevant information. In a related work, Ooi et al. [9] reported that prosodic and glottal features were effective in predicting the evolution of depression in adolescents over a time range of two years. Moore et al. [7] highlighted the relevance of glottal tract features for depression classification and showed that glottal features can improve depressed speech classification more than vocal tract features when used in combination with prosodic features.The analysis of speech features has also been proposed with the aim of identifying different emotions in speech [11,12]. Scherer sustained that speaker's emotional arousal is associated with physiological changes in respiration, phonation and articulation [13]. Such changes are responsible of the production of the emotion-specific patterns in acoustic parameters. Generally prosodic features as fundamental frequency, duration, intensity, and less frequently voice quality features as harmonics-to-noise ratio, jitter and shimmer are investigated in the field of emotion recognition [14]. The great availability of low-level descriptors and functionals has encouraged the use of a great number of features (e.g. brute-force extraction) carrying to the implementation of features selection methods [14,15].At the same time there is an interest in developing models that can improve the description of the dynamics of speech features. Recently, the relevance of shape, slope and range of F0 contour in emotional speech perception, synthesis and recognition has been described [11,16–18]. Moreover, local prosodic features that are related to the temporal dynamics description of prosody have been found to improve the information carried by global, static prosodic features [18].Rising and falling segments of the stylized fundamental frequency were taken into account in [19], where it was highlighted how F0 slope tends to be steeper in higher aroused emotions. The phenomenon of F0 declination across an utterance was studied in emotional speeches [20]. Moreover, it was found that the F0 slope in the last syllable can convey different moods [21].In this work we want to investigate whether a detailed description of F0 dynamics could be used to discriminate among different emotions in speech, and distinguish different mood states in bipolar patients. To achieve this goal we describe an automatic method for the analysis of F0 contour. The proposed method performs an automatic segmentation of running speech and the detection of voiced parts of syllables. A descriptive statistics of the F0 profile within each voiced segment is suggested. In particular, two categories of features are proposed. The first is borrowed from Taylor's Tilt Intonational Model [22] and it describes geometrically the voiced segments. Unlike the Taylor's model, the features here proposed are investigated in every voiced segment and not in only intonational events, as Taylor postulated in his model. The second category of features is related to the speed of F0 variations and estimates the steepness of both rising and falling F0 trend in each voiced segment. The results obtained from an emotional speech database are shown and preliminary results on bipolar patients, recorded in different mood states, are introduced and discussed.The proposed approach is a three-step procedure, consisting in a voiced segments detection step, a speech fundamental frequency (F0) estimation step and a final feature estimation step. In the first step, speech signal is analyzed to detect voiced part of syllables by using information about zero crossing rate [23] and signal intensity [24]. More details can be found in Fig. 1.In a second step a procedure based on the Camacho's swipe’ algorithm [25], is used to estimate the F0 contour. As can be seen in Fig. 2, for each voiced segment, the fundamental frequency is estimated using a moving window approach [26].In a third step the final features, that allow describing specific characteristics of the F0 profile within each voiced segment, are estimated. In particular some of the extracted features are borrowed from Taylor's Tilt Model [22] and are related to “relative sizes of the amplitude and durations of rises and falls for an event”. Within each voiced segment an eventual local maximum is detected and the features are estimated as in (1)–(3):(1)Amplitude*=|Arise|−|Afall||Arise|+|Afall|(2)Duration*=|Drise|−|Dfall||Drise|+|Dfall|(3)Tilt*=Amplitude*+Duration*2where Ariseand Afallare the F0 changes during the rising and falling section within a segment, respectively, Driseand Dfallare the duration of the rising and falling sections. The features we are estimating are different by those proposed by Taylor, even if functionally equivalent. This is due to the voiced segment and the identification processes utilized. In Section 4, this issue will be discussed further. Amplitude* (ampl*) feature is an index of the difference between the F0 amplitude excursions during rising and falling trend. Duration* (dur*) instead takes into account the time intervals in which the two trends, rising and falling, happen. Finally, tilt* is the mean value of the two previous features.The previously considered features can describe the shape of F0 contour in voiced segments, while they are insensitive to the temporal scale of the phenomena. Thus, a second category of features that takes into account the speed of F0 change has been considered. In particular, the steepness of the F0 contour during both rising (PosSlope) (4) and falling (AbsNegSlope) (5) phase of F0 change is estimated.(4)PosSlope=|Arise||Drise|(5)AbsNegSlope=|Afall||Dfall|Finally, other two features are estimated according to (6) and (7):(6)SumDer=Sloperise+Slopefall(7)GlobalSlope=|Arise|−|Afall||Drise|+|Dfall|SumDer (6) is estimated as the sum between the F0 slope during the rising variation and the F0 slope during the falling variation. GlobalSlope (7) instead is defined as the F0 slope between the first and the final F0 values in each voiced segment.The performances of the proposed method, in terms of voiced segments detection, were verified. To reach this aim, the segmentation step was applied on a database consisting of both audio and concurrent electroglottographic (EGG) recordings [27]. Specificity and sensitivity of the proposed method for the detection of voiced segments were estimated, considering the voiced segments as revealed by EGG as the ground truth.After the testing phase of the algorithm performances, the described features were firstly estimated on an emotional speech database [28]. Ten different sentences, acted by ten different actors (5 females) playing four different emotions (anger, boredom, happiness and neutral), were selected.Eleven psychiatric patients (5 females, 40±9) were recruited for this study. All patients had a clinical diagnosis of bipolar disorder. Namely they fulfil the criteria for one of the following mood episode: depressive, mixed, hypomanic. Patients were examined with the structured clinical interview for DSM-IV-TR (SCID) [29]. Clinical rating scales were used to evaluate the presence and the severity of mood symptoms. Particularly the Quick Inventory of Depressive Symptomatology-Clinician Rating (QIDS-C) [30] was used to assess depressive symptoms and the Young Mania Rating Scale (YMRS) [31] for the manic ones. Both scales were administered by a trained physician or psychologist. In this study four different mood states were identified, namely depressed, euthymic, mixed state and hypomanic state. Mood state was labelled according to the scores of the above-mentioned clinical rating scales. Particularly QIDS-C higher than eight indicates depressive state, YMRS higher than six indicates (hypo)manic state. Mixed state is labelled if both the scales were over the cut-off while if the patient scores under the cut-off in both of the scales he or she is labelled as euthymic.We also recruited 18 healthy control subjects (9 females, 30±5). Healthy control subjects did not refer any actual or past psychiatric disorder, and have no history of neurological or major somatic conditions. At the moment of the study they were not taking any medication.The experimental protocol, approved by the clinical ethical committee, consisted of two different tasks:•neutral text reading: subjects were asked to read a text that was supposed not to elicit a strong emotional reaction;commenting of TAT (Thematic Apperception Test) images: subjects were asked to comment a series of TAT images [32]. The images in this task represent social situations, and they require a personal interpretation of the scene by the subject.The signals were acquired with a sampling frequency equal to 48 KHz and a resolution of 32 bits by means of a high quality microphone (AKG Perception P220 Condenser Microphone, M-Audio Fast-Track). The mean recording length was 220s for neutral text reading and 350s for TAT. The recording sessions for all subjects took place in the afternoon. Each subject was recorded twice in two different days. For seven of the eleven patients one additional recording session took place in the morning of the same day when the afternoon recording was performed. The additional recording consisted in a neutral text reading task.Control subjects were asked to perform the same tasks of the patients for a total of 18 neutral text double recording sessions and 10 commenting of TAT double recording sessions.As regards the emotional speech database, statistical tests were performed to evaluate differences among the features estimated from speeches classified as having a different emotional content. The tests were performed both at single subject level (i.e. intra-subject) and at group level (i.e. inter-subject).As regards bipolar patients data, intra-subject analyses were performed to test for statistically significant features changes between records related to different mood states. The comparison was only performed between feature sets belonging to the same task. The limited number of subjects with the same mood labels did not allow performing a reliable inter-subject analysis.To test the specificity of the proposed features with respect to mood changes, features estimated from different recording sessions showing the same labels were compared. This was accomplished by comparing morning and afternoon recording sessions from bipolar patients, and by comparing acquisitions performed on control subjects, which were in the euthymic state in all the recordings.Parametric and non-parametric statistical tests were used according to feature distributions. Gaussianity of feature distribution was tested using a Lilliefors test. The non-parametric statistical tests were the Mann–Whitney U-test for intra-subjects analysis and the Kruskal–Wallis test for inter-subject analysis. The parametric test employed was the one-way ANOVA.

@&#CONCLUSIONS@&#
A method to estimate prosodic features of voiced segments is proposed in this work. Two categories of features are investigated. The first category is comprised of features that are functionally equivalent to those introduced by the Taylor's model. Otherwise than what indicated by the latter model, here those features are estimated on all the voiced events. This choice allows an easier approach but it results in features with a different meaning. The other category of features is chosen to describe the speed of F0 change.Statistically analysis on features estimated from an emotional speech database reveals how the proposed features could be able to highlight significant differences among the recordings. In particular emotional speeches can be classified according to the excitation level of the acted emotion.The analyses on bipolar patients and controls have highlighted that Taylor-inspired features and GlobalSlope have a very good specificity. In fact, statistically significant differences have not been detected in almost all the couples of records characterized by the same label. Statistically significant differences have been found out comparing speech records corresponding to different mood states.The main contributions of this work are that the directions of change estimated for different patients experiencing the same mood swing, are not coherent and that the changes seem to be task dependent. Only ampl* seems to have a coherent behaviour among patients experiencing a mood swing from or to hypomania. An increase in the number of enrolled patients, as well as the analysis of more time points per subject, would be useful for highlighting possible predominant trends of the proposed features with respect to mood changes.However, we believe that the results here described furnish helpful indications for planning further investigations. Moreover, they indicate that a model of speech changes might be subject-specific and that a richer characterization of subject status, for instance adding the anxiety dimension in the descriptors, could explain part of the observed variability. In conclusion, we believe that the proposed features might give a relevant contribution to the demanding research field of speech-based mood classifiers.