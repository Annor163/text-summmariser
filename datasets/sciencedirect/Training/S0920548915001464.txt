@&#MAIN-TITLE@&#
Measuring the quality of diff algorithms: a formalization

@&#HIGHLIGHTS@&#
We study the quality of deltas produced by diff algorithms, regardless of their speed.We present a formal way to compare the deltas and their properties.It includes a uniform delta model and a set of quantitative metrics.It makes it possible to compare algorithms with different aims and output formats.We show how the metrics capture peculiarities of some widespread XML diff algorithms.

@&#KEYPHRASES@&#
Diff algorithms,Output quality,Metrics,Delta model,XML diff,

@&#ABSTRACT@&#
The automatic detection of differences between documents is a very common task in several domains. This paper introduces a formal way to compare diff algorithms and to analyze the deltas they produce. There is no one-fits-all definition for the quality of a delta, because it is strongly related to the application domain and the final use of the detected changes. Researchers have historically focused on minimality: reducing the size of the produced edit scripts and/or taming the computational complexity of the algorithms. Recently they started giving more relevance to the human interpretability of the deltas, designing tools that produce more readable, usable and domain-oriented results. We propose a universal delta model and a set of metrics to characterize and compare effectively deltas produced by different algorithms, in order to highlight what are the most suitable ones for use in a given task and domain.

@&#INTRODUCTION@&#
The automatic comparison of two different versions of a document and the compilation of a list of changes between them is a common task. A diff algorithm is used for this purpose: it takes two files as input and computes their difference, according to a given set of change operations. The outputs of diff algorithms, usually called deltas, diffs or patches, are used for many purposes: programmers review source code diffs to avoid adding bugs and to understand which parts of the code has changed; editors highlight the changes made on drafts and pre-prints; law makers compare proposals during the discussion and approval of a bill; philologists use the differences between documents to recreate the stemma codicum of a text, the history of its development. An exhaustive survey on change detection and versioning tools can be found in [1].Historically the research on diff algorithms has been carried out by the database community, that has to deal with huge quantities of data and seeks to reduce space and time consumption. In fact, these algorithms have been evaluated mainly by comparing their time and space performance. Almost all the experiments in the literature follow the same pattern: the authors first compare the computational complexity and the execution time of the algorithms, then evaluate the quality of the results, see for instance [2–5].The quality is often expressed in terms of the ability to reduce the size of the produced delta. As summarized in [6]: “quality is described by some minimality criteria [...] Minimality is important because it captures to some extent the semantics that a human would give when presented with the two versions”.Surprisingly only a few other quality measures have been defined and applied. There is now a growing interest in characterizing more precisely the quality of deltas, in order to design algorithms that produce an output that is easier to interpret and more adequate for human readers, for instance specialized for literary documents [7] or ontological data [8].The focus of this paper is on comparing the quality of the deltas produced by diff algorithms.We introduce a framework for measuring the quality of diffs through an objective evaluation process. The basic idea consists of extracting numerical indicators from deltas (such as the number of detected changes, the number of high-level changes, the number of elements listed in the description of each change) and aggregating them into more complex quantitative metrics. These indicators can be associated to quality requirements and evaluated to decide whether or not the algorithm that produced that delta is ‘better’ than others in a given context.This work is built on top of UniDM [9,10], a unified conceptual model able to abstract the characteristics of deltas. Each diff algorithm uses its own strategy either in computing deltas or in serializing. Design choices are strictly dependent on the application domain and, very often, prescribed by the tools that are meant to apply deltas. Such a unified model, along with the evaluation metrics on top of it, gives users a powerful tool to analyze in a more precise way the behavior of the algorithms. It is also worth remarking that this model is general enough to deal with streams of text, lists, trees or graphs, so that the same evaluation process can be applied to heterogeneous algorithms and domains.The paper is structured as follows. Section 2 describes in more depth the solutions adopted to evaluate the quality of diff algorithms. Section 3 discusses how the same concept of ‘quality’ can have different meanings according to users' needs and preferences. Section 4 introduces our solution, that is detailed in each part in the following sections: Section 5 introduces UniDM (the model used to analyze the deltas) and some quantitative indicators, while Sections 6 describes the metrics in a formal way. The application of the metrics is presented in Section 7 where we introduce a two-phase method to evaluate the existing algorithms and we present experimental results on some well-known XML diff tools, before concluding in Section 8.

@&#CONCLUSIONS@&#
