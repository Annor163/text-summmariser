@&#MAIN-TITLE@&#
Time series interpolation via global optimization of moments fitting

@&#HIGHLIGHTS@&#
A new method for inputting values in time series is proposed.Variable Neighborhood Search successfully imputes missing values in time series.Contrary to standard procedures, our method matches targets for moments and autocorrelations.

@&#KEYPHRASES@&#
Missing values,Moments matching,Global optimization,Variable Neighborhood Search,

@&#ABSTRACT@&#
Most time series forecasting methods assume the series has no missing values. When missing values exist, interpolation methods, while filling in the blanks, may substantially modify the statistical pattern of the data, since critical features such as moments and autocorrelations are not necessarily preserved.In this paper we propose to interpolate missing data in time series by solving a smooth nonconvex optimization problem which aims to preserve moments and autocorrelations. Since the problem may be multimodal, Variable Neighborhood Search is used to trade off quality of the interpolation (in terms of preservation of the statistical pattern) and computing times.Our approach is compared with standard interpolation methods and illustrated on both simulated and real data.

@&#INTRODUCTION@&#
The existence of missing values is a very common yet critical issue in data from a variety of fields such as engineering, physics, hydrology, finance, marketing or transportation; see [26,30,18,36,12,19,8,3].Missing data may hide the pattern of the data, and they may considerably distort the results of any statistical analysis performed. In order to cope with data sets with missing values, two main strategies have been followed in the literature: one either modifies existing statistical techniques to accommodate the existence of missing data, e.g. [26], or one first estimates the missing values and then statistical methods are used for the completed data set.The latter approach has been widely used, in particular, in time series analysis, addressed in this paper. A number of interpolation methods have been suggested from long ago. Some assume a statistical parametric model, such as a MA(∞), [2,32], an ARMA, [6,15,23,27], an ARIMA, [17], or they impose a specific form for the interpolator, which ranges from piecewise polynomial-based approaches (linear, nearest-neighbor, logistic, B-splines), e.g. [4,20,38,37], to sinc interpolation or wavelets (in connection to filtering and signal processing), see [35] and the references given there, and Chapter 19 in [34] for a detailed description of such methods.Because of their simplicity, weak underlying assumptions and good empirical performance, simple polynomial-based interpolation methods such as linear, nearest neighbor, cubic or splines are implemented in general-purpose packages as Matlab or R.In the linear interpolation method, the missing data in an interval are imputed by the straight line that passes through the interval endpoints. The nearest neighbor algorithm assigns the closest known neighbor to a missing point, leading in this way to a piecewise constant interpolant. In a similar spirit, the cubic interpolation method fills a missing data interval by the third degree polynomial that passes through the four nearest neighbors. Finally, the spline interpolation method completes the series via a cubic spline under the requirements of smoothness and existence of derivatives. More specifically, if yiand yi+1 denote the known edges of the missing interval then, this will be completed by the functionSi(x)=ai(x-yi)3+bi(x-yi)2+ci(x-yi)+diwhere the constants ai, bi, ciand diare calculated to make the resulting function smooth enough. In other words, they are chosen according toSi(yi)=Si(yi+1),Si′(yi)=Si-1′(yi),Si″(yi)=Si-1″(yi).Though not too often mentioned, such interpolation methods may yield rather unsatisfactory results, since they may even provide interpolated values out of the possible range: the values may be known, for instance, to be non-negative (rainfall, exchange rates, etc.) or to range in a given interval, such as [0,1] if the time series represent the evolution of a ratio, (unemployment rate, disease prevalence, …), etc. As an illustrating example, which motivated our study, consider Fig. 1, where the series of daily precipitation amounts between 1976 and 1980 at the station of Níjar (Andalucía, Spain) has been analyzed. The top panel illustrates the real series, with several missing values, due to disfunctions in the measurement equipment. The central and bottom panels show the interpolated series according to the spline and cubic interpolation, respectively. It can be seen that under both methods negative values were obtained. Moreover, by construction, there is no guarantee that the moments and autocorrelations of the so-obtained time series remain close to their sample estimates. In other words, the statistical pattern of the time series may be strongly distorted when interpolation is done.The purpose of this paper is to explore how missing values in time series can be interpolated so that the imputed data are forced to belong to a specific interval, and thus the drawback of some interpolation methods illustrated by Fig. 1 is avoided, and the moments and autocorrelation function of the data are preserved as well.The paper is organized as follows. Section 2 formulates the problem of interpolating missing values preserving range, moments and correlations as a nonconvex optimization problem with box constraints. A well-known global-optimization metaheuristic, namely, Variable Neighborhood Search (VNS), is customized for this problem, as discussed in Section 3. Section 4 is devoted to illustrate the performance of our approach by comparing it with the benchmark interpolation methods previously described. Finally, Section 5 presents conclusions and prospects regarding this work.As commented in Section 1, the purpose of this work is to develop a method for interpolating the missing values in an incomplete time series so that the range of values, as well as moments and autocorrelation coefficients, are preserved. In this section we describe how this can be naturally modeled as an optimization problem.Consider a sequencey={yt}t=1Nof real values. The index set {1,2,…,N} is partitioned into two sets: the index set B of times t for which the value ytis missing, and the index set S={1,2,…,N}⧹B of times t for which ytis known. The aim is to recover the sequence y when only the values {yt}t∈Sare given.Any sequencex={xt}t=1Nsatisfying(1)xt=yt∀t∈S,interpolates the partially observed series y. However, not any such interpolating sequence will yield a reasonable estimate of y, as we illustrated with the example in Section 1. In order to avoid out-of-range problems, we will also impose x to satisfy(2)at⩽xt⩽bt∀t∈B.The constants at, btsatisfying −∞⩽at⩽bt⩽+∞ are assumed to be given by the user. A possible choice is obtained if we consider, for each t∈B, at=minn∈Synand bt=maxn∈Syn, and thus the range of x coincides with the range of y. However, if the time series is suspected to present outliers, such a choice of atand btmay lead to extreme values. In these cases, atand btcan be derived from Tukey’s fences, though, as discussed in Section 4, this choice may also be controversial.Together with forcing x to take values in a specified range, by imposing(2), we also wish to fix its statistical patterns by making moments and autocorrelation coefficients of x match given target values:(3)1N∑i=1Nxik=mk∀k=1,2,…,k0,and(4)ρj(x)=ρj∀j=1,2,…,j0.Here mkand ρjare given target values for the kth moment and lag-j autocorrelation coefficient, where(5)ρj(x)=∑t=1N-j(xt-x¯)(xt+j-x¯)∑t=1N(xt-x¯)2,is the estimation of the lag-j autocorrelation coefficient as defined in [9], andx¯is the sample mean of x.Target values mkand ρjallow one to preserve the statistical pattern of the time series. They can be either estimated from the observed series {yt}t∈S, or from a related but complete time series (even the same time series in a different time window). See Section 4.4 for a discussion on the effect of the selection of target values on the performance of the interpolation approach described in this paper.While (2)–(4) allow us to govern the statistical pattern and the range of the series, a smoothing criterion is also considered, imposing the missing values not to significantly differ from the adjacent values. In other words, if we define f as(6)f(x)=∑i=1N-1(xi+1-xi)2,we also wish to have f(x) as small as possible.The previous discussion leads us to model the problem of properly interpolating the incomplete time series y as the optimization problem of finding x minimizing f, as defined in (6), and satisfying the constraints (1)–(4). However, the feasible region defined by (1)–(4) is rather complex, and it may be hard for a numerical algorithm to find a feasible solution. Moreover, since analyst confidence on the target values mkand ρjmay be different, we find more convenient to consider (1) and (2) as hard constraints, and (3) and (4) as soft constraints. This way we obtain an optimization problem of the form(P)minF(x)s.tat⩽xt⩽bt∀i∈Bxt=yt∀i∈S.The objective function F in (P) is given byF(x)=f(x)f(p0)+∑k=1k0λk1N∑i=1Nxikmk-12+∑j=0j0μjρj(x)ρj-12,where p0 is a reference starting point for x, so that the quotientf(x)f(p0)is dimensionless, as the remaining terms in F, and the parameters λk,k=1,…, k0 and μj, j=1,…, j0, are positive scalars which trade off the deviations in the soft constraints.Problem (P) is a nonlinear problem with very simple constraints (just box constraints), but a nonconvex objective function. A closed-form solution to (P) seems hard to obtain due to the high nonlinearity of the objective. Hence, in order to cope with (P), numerical procedures are suggested. This will be discussed in Section 3.Problem (P) is a smooth optimization problem. Moreover, in contrast with what happens if (1)–(4) are all considered as hard constraints, obtaining starting feasible solutions for (P) is straightforward. Hence, obtaining a local minimum for (P) is a rather simple and cheap task, achievable by standard local-search numerical routines. However, due to the nonconvexity of the objective, there is no guarantee that the output of such local-search routines is a global optimum of (P).In order to scape from local optima, we propose to embed the local searches into a metaheuristic strategy, namely, the Variable Neighborhood Search (VNS), [29,28,10], which can successfully exploit the fact that the feasible region is rather simple, and thus neighborhoods are easily defined. It may be observed that other metaheuristics, e.g. [24,22,14,16], could have been used instead.The scheme of the VNS algorithm is summarized in Fig. 2.The VNS is customized to Problem (P) by defining the neighborhood structure, the random distributions for shaking, the starting solution and the stopping criterion.Since the feasible region of (P) is box-constrained, a set of nested boxes are chosen as neighborhoods, and sampling is performed by following a uniform distribution on the boxes. As stopping criterion, an upper bound on the number of iterations allowed is given.A more sophisticated strategy is followed for the selection of the starting point, since it is commonly accepted, and confirmed as well in our numerical experiments with (P), that choosing a good starting point is critical to guarantee an appropriate convergence speed for the VNS. Although, as already mentioned, it is unlikely to obtain a closed-form solution of problem (P), it is possible however to solve analytically a simpler relative of (P), in which constraints (2) and (4) are ignored, while, together with the interpolation constraints (1), the constraint (3) for k0=1 are put as hard constraint, (or, equivalently, we set λ1=+∞ in (P)). In other words, we consider the auxiliary problem (P∗) given by:(P★)min∑i=1N-1(xi+1-xi)2s.t.xt=yt∀t∈S,∑i=1NxiN=m1Problem (P∗) has a quadratic convex objective and one linear constraint. An optimal solution can be analytically derived, as discussed in the Appendix. Such optimal solution will be taken as starting solution x0 for the VNS algorithm for solving (P).In this section we illustrate the performance of the proposed methodology and compare it with the results provided by standard interpolation methods. All results have been obtained using R and, in the spirit of a reproducible research, the codes utilized in this paper are available as a stand-alone R toolbox from the authors upon request.Ten real times series from different contexts and presenting different statistical features have been selected to illustrate the performance of the proposed approach. The series considered in this Section do not present missing values (see Section 4.3. for a real incomplete series) and therefore, their sample moments and autocorrelation coefficients are known. As will be described in the experiments below, a chosen proportion of observations will be randomly removed in order to test different interpolation methods. The series, shown at the top panels of Figs. 3–7, are described next.•Exchange. This series represents the evolution of the exchange rates between the Hong Kong Dollar (HKD) and the US Dollar (USD). The number of moments and autocorrelation coefficients to be matched were k0=3 and j0=30, respectively. Data can be found at http://gtwavelet.bme.gatech.edu/datapro.htmlNorwegian. A total of 9181 consecutive fire claims values from 1972 to 1992 in a Norwegian insurance portfolio are considered here. Data are heavy-tailed (see [5,33] for a complete description, and analysis of the data concerning heavy-tailed distributions) and k0=3 and j0=10 were chosen. The series is available at http://lstat.kuleuven.be/Wiley/Internet. This time series, widely used in the literature (see [25,33], among others), represents 50,000 real interarrival times in a total of one million packet arrivals, recorded by Bellcore Morristown Research and Engineering facility. The series is highly variable and presents a non-negligible autocorrelation coefficients. Again, k0=3 and j0=10 have been set. Data are found at the InternetTraffic Archive: http://www.sigcomm.org/ITA/Coke. This series records a total of 4128 daily Coca-Cola stock market price on dollars. Fig. 4 shows that the series is clearly non-stationary and long-range dependent. In order to explore how the moments matching approach performs if only the fitting of the autocorrelation coefficients is considered, we set k0=0 and j0=1. Data can be found athttp://gtwavelet.bme.gatech.edu/datapro.htmlCordoba. This time series, provided by the AEMET (Spanish National Meteorology Agency, http://www.aemet.es) represents the total daily precipitation at the station of Córdoba’s airport (Spain) from 2004 to 2005. In this case k0=3 and j0=1 were fixed.The following series are provided by the Time Series Data Library found at http://datamarket.com/data/list/?q=provider:tsdl. All of them represent monthly observations and thus the number of autocorrelation coefficients to be matched was set to j0=12. In all cases, a value of k0=3 was chosen.•Gas. This series represents 106 monthly average gas usage (measured in cubic feet times 100) from 1971 to 1979at Iowa state. The series, analyzed by [1], presents a strong cyclical pattern.Riverflow. This time series records a total of 588 monthly riverflow from the Boise River (near Twin Springs, Idaho) measured in cubic meters per second. Data correspond to periods from October 1912 to September 1960. A more detailed description of the series can be found at [21].Unemploy. A number of 736 monthly civilian unemployment rates at the US from January 1948 to December 2011 are represented in this series. Data were provided by The Bureau of Labor Statistics (BLS).Gnp. This series, analyzed by [31], represents 176 quarterly US real GNP (in dollars) from the first quarter of 1947 to the first quarter of 1991.Milk. This time series relates to cow milk production. Specifically, it represents a total of 156 monthly pounds of milk per cow, from January 1962 to December 1975. It was listed and analyzed in [13]. This series is clearly non-stationary, see Fig. 7.Next, we describe the three conducted experiments. As commented previously the series do not have missing values and therefore they were artificially incompleted as follows.•Experiment 1: 30% of the observations, randomly selected, was deleted.Experiment 2: 15% of the observations, randomly selected, was deleted.Experiment 3: An interval of consecutive observations, representing the 30% of the total, was deleted.The values 30% and 15% were arbitrarily selected as examples of high and moderate percentages of missing data.This section presents the results obtained after interpolating the times series described in Section 4.1. by the interpolation method defined in this work, namely, the moments matching method (MMM, from now on), and compares the obtained performance with that provided by Linear, Nearest Neighbor, Cubic and Spline methods, which are easily found in common statistical packages such as R or Matlab. The chosen option ’cubic’ makes a cubic interpolation based on the four nearest neighbors; the Spline approach is encompassed in piecewise Cubic Interpolation. For more information about Spline interpolation the reader is referred to [7].We should point out here that, in order to implement our approach, the R-cran function optim was used with default options as local-search routine. The function implements the L-BFGS-B algorithm, which allows for box constrained problems. In practice, penalties have been considered constant and for convenience set to λk=λ=5000, for k=1,…, k0, and μj=μ=4000, for j=1,…, j0. The number of VNS neighborhoods and total of random points generated at each neighbor were 7 and 2, respectively. Finally, the target values are set as the sample moments of the complete series.Tables 1–3show the absolute percent error from target values in Experiments 1–3 obtained under the MMM (highlighted in gray color), Linear, Nearest Neighbor, Cubic and Spline interpolation methods, for all time series described in the previous section. Recall that m1, m2, m3 denote the moments of order 1, 2, and 3 of the series, and ρjthe jth-lag autocorrelation coefficient. Imputed values are required to belong to the interval defined by the minimum and maximum values in the series. Fitted values that lie out of such a range are starred (*). Although in some series j0 was set up to 30, for abbreviation, we only present the error when fitting ρ1.Several conclusions can be obtained from Tables 1–3. First, note how in the three experiments considered our approach provides interpolated series whose moments are either very close to, or exactly match the target values. Additionally, the imputed values always remain within the required interval. On the contrary, the rest of methods present a poorer performance (the results in Experiment 1 being poorer than Experiment 2, as expected, since less information is considered). In Experiment 1, MMM provides better fits than the rest of methods in 28 out of 37 fitted coefficients, and comparable results to the best of the alternative interpolators in 3 cases. In Experiment 2, these values turn into 29 and 5 (out of 37), respectively. Finally, in Experiment 3, MMM performs better than or equal to the best of the other methods in 35 cases; see the panels of the second row of Figs. 3–7 which depict the completion of the missing central gap under the MMM approach (data in between the bands are assumed to be missing). Note that, in appearance, no big differences exist between the real and interpolated series in all cases. Also, it is of interest to note from Table 3 how the performance of classic interpolation methods gets worse in Experiment 3, due to their strong dependence on neighbor data. Marked disparities between classic interpolation methods are also observed, note for example the results from Table 3 between Cubic and the remaining interpolators. Finally, it is worth pointing out that, for Spline and Cubic interpolators, it is a rule more than an exception to impute data outside the natural range of values and in some cases quite extreme values are obtained.In order to examine more in depth the performance of the MMM against that of other existing interpolation methods, it may be natural to apply the commonly used prediction error between the actual values of the time series and the estimated values. To this end, the absolute and quadratic differences, normalized by the smallest error, have been computed. They are shown in Table 4, in which the best obtained results are highlighted in bold style. Note that the best interpolation method according to these criteria presents an error equal to 1, and for the rest of methods this value is exceeded. Under the absolute error criterion, MMM outperforms the rest of methods in 12 out of 30 cases; presents a comparable performance in three cases and performs poorer than the best of the remaining benchmark approaches in the half of cases. If the quadratic difference criterion is used instead, MMM outperforms the other interpolators in 15 out of 30 cases, and performs similarly to the best in two cases. Panels of the third row of Figs. 3–7 show the interpolated series in Experiment 3 by the best method (if different from MMM), according to the quadratic error criterion. Note that in these cases the linear interpolation method is the approach that always outperforms the MMM. However, note too from Figs. 3–7 that, in spite of presenting a larger prediction error, the MMM approach provides a more reasonable fit in terms of the statistical pattern of the series than the linear method; see for example the cases of Internet, Riverflow and Gnp.Figs. 3–7 show how MMM and Linear approaches perform when fitting the missing values. Fig. 8illustrates how the rest of interpolators, namely, Nearest Neighbor (top right panel), Cubic (bottom left panel) and Spline (bottom right panel) behave for the time series Internet. Again, we emphasize the poor performance obtained under these methods.The optimization problem described in Section 2 has been solved using a global optimization technique, the VNS, which escapes from local optima, though at the expense of higher running times. Therefore, it is natural to gauge the benefits of such an approach instead of considering a simple local search, usually implemented by all statistical packages. To look more closely at this problem, the values of the objective function obtained with both a single run of the R-cran command optim, and the VNS approach (seven neighborhoods and two randomly generated points on each) are depicted in Table 5. The same starting point x0, namely the solution to Problem (P∗) discussed in Section 3, which preserves the sample mean, was used under both approaches. The improvements of the VNS on the local search are expressed as percentages. It can be observed that in eight/nine/six out of 10 series in Experiments 1/2/3, the results are similar, possibly due to the proper choice of the starting point. However, in the case of the heavy-tailed series Norwegian, and the highly variable series Riverflow and Gnp the VNS significantly improved the results in Experiment 3. Such an improvement in accuracy are obtained at the expense of an increase in running times. Indeed, the median running time for MMM among the ten considered series was, for the prototype code implemented in R, about 5.8minutes, in contrast to the couple of seconds taken by the classic methods. In other words, local search, with an appropriate choice of the starting solution, as the one proposed here, gives a quick and usually good solution, though the accuracy is substantially improved when more computational effort is made by plugging local search into a VNS method.As commented in Section 1, the MMM approach for completing a time series with missing values needs for the specification of target values, to which the moments and autocorrelation coefficients are matched. In the previous Section where the series were artificially made uncomplete by erasing some records, these target values were set as the empirical moments of the complete series (being the sample autocorrelation coefficients defined as in (5)). However, if the series really presents missing data, as will happen in practice, these sample moments may be seriously distorted. This section aims to investigate how the choice of target values influences the results of the proposed interpolation method.We first consider an example were 1000 synthetic data were simulated from an AR(1) process with parameters m1=1 and ρ1=0.6. Experiments 1, 2 and 3, as described in Section 4.1, were carried out with k0=j0=1, and the target values fixed as deviations from m1=1 and ρ1=0.6. Specifically, 0%, 5%, 10% and 15% deviations from the theoretical values were considered. Obtaining sample estimates for the autocorrelation coefficients is not a trivial task since the available information consists in different sequences spaced in time. Two different estimates for ρj, particularized for j=1, were also considered. First, a weighted autocorrelation coefficient estimator is proposed as follows: the series is split into sub-series of consecutive values with length higher than j; for each such sub-series, its sample jth lag autocorrelation coefficientρ¯j,nis computed according to (5), and then the estimator is given by(7)ρˆj=∑nωnρ¯j,n,where ωnrepresents the weight of the nth sub-series. However, a restriction worth to be mentioned while considering the jth lag autocorrelation is that it cannot be calculated if the length of the interval is less than j. Moreover, (5) does not behave properly when the length of the interval is not much larger than j. This could lead to unreliable estimations, thus the jth autocorrelation of the longest interval of known data can be considered as an estimator instead of the weighted one. Other estimates for ρjsuggested in the literature can be found in [11].Table 6shows the predictive errors, when interpolating using MMM, computed as the sum of the (absolute/squared) differences between the interpolated and real values, under the assortment of target values commented previously. In the last two columns the target values m1 are chosen as the sample mean of the uncompleted series, and the target values ρ1 are computed as the weighted estimator (7) for j=1, and the sample estimator (5) of the longest complete interval, respectively.From Table 6 it can be deduced that, in this example, the estimator of the autocorrelation function based on the longest known interval produces better results than the weighted one. It could be also said that, as expected, the larger the intervals of known data, the better performance is obtained by both estimators. Finally, and as expected too, the larger the deviations from the real values m1 and ρ1, the poorer the results obtained under both absolute and squared errors criterion.We conclude this section by interpolating via the MMM a time series which actually had missing values, and the pattern of such missing values is unknown. The series was described in Section 1 and represents the daily precipitation amounts in Níjar, Spain, from 1976 to 1980. It was shown in Section 1 how classic methods as Spline and Cubic failed in interpolating the series properly due to the fact that the imputed valued severely violated the range of admissible values. Values of k0=j0=3 were set; since the sample moments and autocorrelation coefficients are unknown, the target values m1,m2 and m3 were chosen as the sample moments of the available data, and the target values ρ1, ρ2 and ρ3 were computed as the weighted estimators, and as the sample estimators (5) from the longest interval.Níjar has sub-desertic Mediterranean climate and therefore rainfall days are very rare, although when they occur precipitation amounts may be extreme. This leads to interquartile ranges close to zero. In order to implement our approach we have considered lower and upper bounds as the minimum and maximum of the known data, as in the previous examples. From top panel of Fig. 9, it can be seen that the series does not present isolated missing data but located missing intervals, which motivates the use of the weighted estimator (7). Central and bottom panels show the completed series via MMM. In the central panel, the estimator (7) has been used while in the bottom panel, only the estimator corresponding to the longest interval has been considered. It can be observed from both figures how imputed values representing precipitation amounts are non-negative and do not exceed the maximum of the known series. Moreover, it is interesting to note that not all imputed data are close to zero: there exists a large imputed value, of the order of the larger values in the observed series, in the case of the weighted estimator interpolation. Note that, although the longest interval is completed with drain days in central panel, precipitations are allowed on other time instants.This work has considered the problem of interpolating missing values in a time series so that moments and autocorrelation coefficients are fitted to target values. This is done via a smooth nonconvex optimization problem, solved via a VNS approach in a continuous space.By several numerical examples, the suitability of the new approach has been highlighted, in comparison with classic interpolation methods, which clearly present a poorer performance. Regarding the optimization problem to be solved, the choice of a proper starting point and the use of a global optimization routine are shown to be relevant.For simplicity, just univariate time series have been considered, though the model naturally extends to the multivariate case, in which also the correlation between the different time series is governed by setting target values. Experimental analysis of such extension deserve further attention.In this section, how to obtain the optimal solution of (P★) is shown in detail. The problem to be solved is:(P★)min∑i=1N-1(xi+1-xi)2s.t.xi=yi,∑i=1NxiN=m1.∀i∈S,The Lagrangian function is given by(8)L(x,λ)=∑i=1N-1(xi+1-xi)2+λ∑i=1NxiN-m1.As shown next, the solution to (P★) depends on the number of intervals containing missing values and their lengths. In order to solve (P★), three cases are considered: the length of the interval of missing values is greater than or equal to 3, (case 1) or it is just 2 or just 1 (case 2).First, we first address the problem for a unique interval, and then we will extend the obtained results to the case of several intervals of missing values.(Case 1.a)A unique interval of missing values.Assume a unique interval of consecutive missing values, given by (xp,…, xp+r), of length R=r+1⩾3. The partial derivatives of the Lagrangian function are given by∂L∂xp=4xp-2yp-1-2xp+1+λN,ifj=0,∂L∂xp+j=4xp+j-2xp+j-1-2xp+j+1+λN,if1⩽j⩽r-1,∂L∂xp+r=4+xp+r-2xp+r-1-2yp+r+1+λN,ifj=r.Making the Lagrangian function equal to zero, we obtain(9)xp+j=(j+1)xp-jyp-1+j(j+1)4Nλ,for 1⩽j⩽r−1. On the other hand, xp+rcan be calculated in two different ways. First, from the partial derivative of (8) with respect to xp+r−1, we conclude that(10)xp+r=(r+1)xp-ryp-1+r(r+1)4Nλ.Also, from the partial derivative of (8) with respect to xp+r,(11)xp+r=r2xp-r-12yp-1+12yp+r+1+r2-r-28Nλ.Making (10) and (11) equal, we obtain(12)xp=r+1r+2yp-1-1r+2yp-1+-r2-3r-24N(r+2)λ.From the active constraint∑i=1NxiN=m1,it follows that(13)xp+∑j=1r-1xp+j+xp+r=c,wherec=Nm1-∑i∈Syifrom now on. Substituting (9), (11) and (12) into (13), yieldsλ=2c-(r+1)yp-1-(r+1)yp+r+1γ,whereγ=-r3-2r2-3r-2+2τ4Nandτ=∑j=1r-1j(j+1).L1⩾2 intervals of missing values.Assume in this case a number L1⩾2 of intervals of missing values, notedI1,…,IL1. An analysis similar to that in the previous case shows that, in the interval Inof length Rn=rn+1, for 1⩽n⩽L1,xpn=rn+1rn+2ypn-1-1rn+2ypn-1+-rn2-3rn-24N(rn+2)λxpn+rn=(rn+1)xpn-rnypn-1+rn(rn+1)4Nλxpn+rn=rn2xpn-rn-12ypn-1+12ypn+rn+1+rn2-rn-28Nλxpn+j=(j+1)xpn-jypn-1+j(j+1)4Nλ,1⩽j⩽rn,whereλ=2c-∑n=1L1(rn+1)ypn-1-∑n=1L1(rn+1)ypn+rn+1∑n=1L1γn,whereγn=-rn3-2rn2-3rn-2+2τn4Nandτn=∑j=1rn-1j(j+1).Now we address (P∗) for case 2, which corresponds to intervals of missing values of length 1 or 2.(Case 2.a)A unique interval of missing values of length 2Assume a single interval T of missing values xpand xp+1, for 2⩽p⩽N−2.In the same manner as in the previous calculations we can see thatxp=23yp-1+13yp+2-12Nλ,xp+1=23yp+2+13yp-1-12Nλ,where in this caseλ=(-c+yp-1+yp+2)N,L2 intervals of missing values of length 2.AssumeI1,…,IL2intervals of two missing values in the time series x. The reasoning above applies to this case to obtain, for 1⩽n⩽L2,xpn=23ypn-1+13ypn+2-12Nλ,xpn+1=23ypn+2+13ypn-1-12Nλ,whereλ=c-∑n=1L2(ypn-1+ypn+1)-L2N,A unique missing data.When there exists one single missing data xpin the time series x, it is easily seen that(14)xp=12yp-1+12yp+1-14Nλ,for 2⩽p⩽N−1, whereλ=(-2c-yp-1-yp+1)2N,L3 isolated missing data.Finally, assume a set of isolated missing data{xi}i∈I, where the length ofIis L3. Again, it is straightforward to obtainxpn=12ypn-1+12ypn+1-14Nλconpn∊I,for any 2⩽j⩽N−1 and whereλ=2c-∑n=1L3(ypn-1-ypn+1)-L32N,We end this Appendix by putting all the previous results together, giving a closed-form expression for the solution to (P★). Consider a time series which has L1 intervals of missing data of length less than or equal to 3, L2 intervals of length 2, and L3 isolated missing values. Let ∣B∣ represent the number of missing values. The expressions for the missing data are the same derived previously, where the only difference is the value of λ, which has to be the same for all expressions. Taking into account the active constraint to preserve the mean∑n=1L1∑j=1n(xpn+xpn+j+xpn+rn)+∑n=1L2(xpn+xpn+1)+∑n=1L3xpn=c,we obtainλ=2c-∑n=1L1((rn+1)ypn-1+(rn+1)ypn+rn+1)-∑n=1L2(2ypn-1+2ypn+2)-∑n=1L3(ypn-1+ypn+1)∑n=1L1γn-2L2N-L3N,wherec=Nm1-∑i∈Syiandγn=-rn3-2rn2-3rn-2+2τn4N,τn=∑j=1rn-1j(j+1).

@&#CONCLUSIONS@&#
