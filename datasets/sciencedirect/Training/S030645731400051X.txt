@&#MAIN-TITLE@&#
Supporting exploratory video retrieval tasks with grouping and recommendation

@&#HIGHLIGHTS@&#
Combine grouping of video search results with recommendation techniques to assist video retrieval.Evaluate grouping and recommendation techniques in separate evaluations to assess impact.Different recommendation approaches are relevant to the users at different stages of their search.Organisational and recommendation functionalities can result in a significant improvement on the users’ search performance.

@&#KEYPHRASES@&#
Video,Search,Interface,Collaborative,Implicit,Feedback,

@&#ABSTRACT@&#
In this paper, we present ViGOR (Video Grouping, Organisation and Recommendation), an exploratory video retrieval system. Exploratory video retrieval tasks are hampered by the lack of semantics associated to video and the overwhelming amount of video items stored in these types of collections (e.g. YouTube, MSN video, etc.). In order to help facilitate these exploratory video search tasks we present a system that utilises two complementary approaches: the first a new search paradigm that allows the semantic grouping of videos and the second the exploitation of past usage history in order to provide video recommendations. We present two types of recommendation techniques adapted to the grouping search paradigm: the first is a global recommendation, which couples the multi-faceted nature of explorative video retrieval tasks with the current user need of information in order to provide recommendations, and second is a local recommendation, which exploits the organisational features of ViGOR in order to provide more localised recommendations based on a specific aspect of the user task. Two user evaluations were carried out in order to (1) validate the new search paradigm provided by ViGOR, characterised by the grouping functionalities and (2) evaluate the usefulness of the proposed recommendation approaches when integrated into ViGOR. The results of our evaluations show (1) that the grouping, organisational and recommendation functionalities can result in an improvement in the users’ search performance without adversely impacting their perceptions of the system and (2) that both recommendation approaches are relevant to the users at different stages of their search, showing the importance of using multi-faceted recommendations for video retrieval systems and also illustrating the many uses of collaborative recommendations for exploratory video search tasks.

@&#INTRODUCTION@&#
As a result of the improving capabilities and the declining prices of current hardware systems, there are increasing possibilities to store and manipulate videos in a digital format. People now build their own digital libraries from materials created through digital cameras and camcorders, and use a number of systems to place this material on the web. However, the systems that currently exist to organise and retrieve these videos are insufficient for dealing with such large and increasing volumes of video. In particular, there is a growing need to develop tools and techniques to assist users in the complex task of searching for video; this is particularly true online with the increasing growth of online video search systems.Current state of the art video retrieval systems rely on textual descriptions or methods that use low-level features (e.g., visual features such as colour, shape, or texture; audio features such as the Fourier transform or pitch; and additional features such as automatic speech recognition (ASR) or optical character recognition (OCR)) to find relevant videos within a large collection. Neither of these methods is sufficient to overcome the problems associated with video search. On the one hand, query by text relies on the availability of sufficient textual descriptions of the video and its content, resulting in a heavy system dependence on users providing relevant text descriptions and annotations. The main drawback of this approach is that often users can have very different perceptions about the same video and annotate that video differently (Guy, Tonkin, & Folksonomies, 2006), which makes it difficult for different users to retrieve the same video. It has also been found that users are reluctant to provide an abundance of annotations unless there is some benefit to the user (Halvey & Keane, 2007), resulting in a lack of available textual annotations. On the other hand, the difference between the low-level data representation of videos and the higher level concepts users associate with video, commonly known as the semantic gap (Smeulders, Worring, Santini, Gupta, & Jain, 2002), provides difficulties for using these low-level features. Consequently, while these low-level features are used in some state of the art systems, most online video retrieval systems (e.g. YouTube1http://www.youtube.com/.1or Blinkx2http://www.blinkx.com/.2) rely only on query by text.In order to alleviate some of these problems associated with video search we have developed ViGOR, a video retrieval system that allows users to create semantic groups of results to help conceptualise and organise their results for complex video search tasks. This interactive grouping is a flexible means for a user to illustrate their multi-faceted information needs. Multi-faceted information needs mean that the task that the user is conducting can be considered to be multiple specific tasks; it could also be considered that multi-faceted search tasks/information needs can have multiple solutions. Specific information needs can be related to short term information needs as the user is focused on one particular aspect of their search task. The grouping facilities also allow the user to focus on one particular aspect of a global task as they can focus on specific (or short-term) information needs while still solving the overall multi-faceted (or long-term) information need as embodied by their search task. We believe that the semantic gap is narrowed by this abstraction to high-level semantic groupings reflecting an individual’s task-specific mental model of the data and a more flexible user interaction with the video collection, thus the user is focused more on interaction with the data and less on the mechanics of their search. We also believe that the use of this system can result in a number of desirable outcomes for users: improved user performance in terms of task completion and task exploration, and increased user satisfaction with their search and their search results.In addition, the interactions available in ViGOR make it an ideal system with which to integrate some recommendation techniques. We believe that many of the problems associated with searching large collections of video can be alleviated through the use of recommendation techniques. Recommendation techniques can offer a work around for the problems associated with the semantic gap and the unreliability of textual descriptions, as they utilise additional information about user interaction that is already available in many systems. However, it is also imperative that the recommendations relate to as many aspects of a user task as possible so as to ensure that the recommendations present the user with a diverse set of results that encompass as many interpretations of the user actions as possible. To that end, we have developed a recommendation approach that utilises the implicit actions involved in previous user searches to create a predictive model that can provide multi-faceted and diverse recommendations to assist users in completing their difficult search tasks. Our recommendations encompass many interpretations of user actions and numerous videos that users may not have seen using normal query methods. Providing these recommendations is not trivial, as due to the complex and difficult search process for video, implicit feedback from video search is quite noisy (Smeulders et al., 2002). However, we believe that this problem can be overcome by utilising collaborative recommendation techniques. In particular we believe that our approach of modelling many aspects of user needs via implicit user interactions can result in improved user performance in terms of task completion and reduce the user effort involved in finding relevant videos.Before proceeding, it should be noted ViGOR has been developed as an interface that can sit on top of any video retrieval system. The recommendation and grouping facilities provided as part of the ViGOR system can be created based on the log files stored by almost any system, website, etc. As such while the recommendation and interface are coupled in this system, in some ways they can be viewed as two distinct parts that can be applied to any video retrieval system, however in this case we are attempting to leverage the benefit of both. Thus as well as solving problems surrounding exploratory video search tasks, we have developed a scalable solution which can be deployed on top of almost any existing video retrieval system anywhere (this has been demonstrated by conducting evaluations using ViGOR in conjunction with YouTube, the largest online video storage and retrieval system (see Sections 6 and 7)).In order to test and validate the potential benefits of ViGOR in assisting with video search, we conducted two user studies, in which we tested two systems. The first ViGOR without recommendations, and the second a system based on ViGOR that provides recommendations based on a model of implicit actions. These systems were evaluated to determine whether any benefit to the users is achieved. The remainder of this paper is organised as follows: in the following section we will provide a rational for this work. Section 3 will describe the two systems that were used in our study. Subsequently, in Section 4 we will describe our approach for using implicit feedback to provide multi-faceted recommendations. In Section 5 we will describe our experimental methodology including our hypotheses, which is followed by the results of our experiments, presented in Sections 6 and 7. In Section 8 we will provide a discussion of our work and Section 9 will provide some final conclusion and a discussion of future directions for this work.

@&#CONCLUSIONS@&#
