@&#MAIN-TITLE@&#
Error-correction learning for artificial neural networks using the Bayesian paradigm. Application to automated medical diagnosis

@&#HIGHLIGHTS@&#
A novel Bayesian-based strategy for training MLPs is proposed.Six medical datasets (breast and lung cancer, heart attack and diabetes) were used for assessment.Statistical benchmark has revealed the effectiveness of the model.The corresponding algorithm is easy to understand and implement.The model is prone to easily adapt to different medical decision-making issues.

@&#KEYPHRASES@&#
Automated medical diagnosis,Bayesian-trained neural networks,Breast cancer,Lung cancer,Heart attack,Diabetes,

@&#ABSTRACT@&#
Automated medical diagnosis models are now ubiquitous, and research for developing new ones is constantly growing. They play an important role in medical decision-making, helping physicians to provide a fast and accurate diagnosis. Due to their adaptive learning and nonlinear mapping properties, the artificial neural networks are widely used to support the human decision capabilities, avoiding variability in practice and errors based on lack of experience. Among the most common learning approaches, one can mention either the classical back-propagation algorithm based on the partial derivatives of the error function with respect to the weights, or the Bayesian learning method based on posterior probability distribution of weights, given training data. This paper proposes a novel training technique gathering together the error-correction learning, the posterior probability distribution of weights given the error function, and the Goodman–Kruskal Gamma rank correlation to assembly them in a Bayesian learning strategy. This study had two main purposes; firstly, to develop anovel learning technique based on both the Bayesian paradigm and the error back-propagation, and secondly,to assess its effectiveness. The proposed model performance is compared with those obtained by traditional machine learning algorithms using real-life breast and lung cancer, diabetes, and heart attack medical databases. Overall, the statistical comparison results indicate that thenovellearning approach outperforms the conventional techniques in almost all respects.

@&#INTRODUCTION@&#
Medical diagnosis refers to the act of identifying a certain disease analyzing the corresponding symptoms. From the point of view of biomedical informatics, medical diagnosis assumes a classification procedure involving a decision-making process based on the available medical data. Thus, the utilization of automated medical diagnosis systems aims to minimize the physician’s error by taking advantages of both the intrinsic computation power when using a huge amount of data, and the fast processing speed as compared to that of the human. Such an “intelligent” system is fed with different symptoms and medical data of a patient, and, after comparing them with the observations and corresponding diagnoses contained in medical databases, will provide the most probable diagnosis based on the human knowledge embedded in the database.From the machine learning (ML) point of view, an automated medical diagnosis may be regarded as a classification problem. Neural networks (NNs) have become a popular tool for solving such tasks [1]. In [2], NNs have been applied to predict the severity of acute pancreatitis at admission to hospital. A competitive/collaborative neural computing decision system has been considered [3] for early detection of pancreatic cancer. Different NNs have been applied in breast cancer detection [4].Recent years have seen a large development of new approaches regarding NNs applied to the medical diagnosis. Hybrid NNs/genetic algorithms and partially connected NNs were used in breast cancer detection and recurrence [5,6]. NNs based on matrix pseudo-inversion have been applied in biomedical applications [7]. Swarm optimized NNs were used for detection of microcalcification in digital mammograms [8], and a fused hierarchicalNN wasapplied in diagnosing cardiovascular disease [9].The Bayesian paradigm could be used to learn the weights in NNs, by considering the concept of subjective probability instead of objective probability. NNs used as classifiers actually learn to compute the posterior probabilities that an object belongs to each class. Once the training data is presented to the NN, the posterior probabilities provide the measure that different weights are consistent with the observed data [10–12]. Some studies used Bayesian NNs to solve biomedical problems. In [13], a Bayesian framework for feed-forward neural networks to model censored data with application to prognosis after surgery for breast cancer has been proposed. A Bayesian NN has been used to detect the cardiac arrhythmias within ECG signals [14]. In [15], a Bayesian NN was able to provide early warning of EUSIG-defined hypotensive events.Different from other approaches dealing with the Bayesian paradigm in conjunction with network models, the current work proposes a novel technique to update the synaptic weights in a multi-layer perceptron (MLP). The underlying idea is to use the error-correction learning and the posterior probability distribution of weights given the error function, making use of the Goodman–Kruskal Gamma rank correlation. The synaptic weights belonging to the unique hidden layer are adjusted inspired by the Bayes’ theorem. Technically, in a subjective Bayesian paradigm, they are considered as posterior probabilities estimated using priors and likelihoods expressing only the natural association between object’s attributes and the network output, or the error function, respectively, through the non-parametric Goodman–Kruskal Gamma rank correlation. The statistical comparison indicates that thenovellearning approach outperforms the conventional techniques regarding both the decision accuracy and the computation speed. The main contributions of the paper are twofold: firstly, to develop anovel learning technique for MLP based on both the Bayesian paradigm and the error back-propagation, and secondly,to assess its effectiveness using real-world databases.The remainder of this paper is organized in five sections. Section 2 is devoted to the presentation of both the design and implementation of the novel model, and the real-world datasets for the benchmark process. Section 3 presents the experimental results of applying the model to six real-world datasets in terms of performance analysis and performance assessment. Section 4 briefly summarizes the main characteristic of the novel approach, while Section 5 deals with the conclusions and future work.The training dataset TS={x1, x2,…,xN} contains N objects. Each object is coded as a vectorxk=(x1k,…,xik,…,xpk;yj);xik, i=1, 2,…,p, represents the ith feature of the k object, k=1, 2,…,N, and yj, j=1, 2,…,q, represents the label of the decision class (category) Cjthe object xkbelongs to.For each k=1, 2,…,N, the attribute valuesxikbelonging to the attribute Ai, i=1, 2,…,p, are governed by a random variable (r.v.) Xi. Let Fi(x) be the probability distribution of Xi, i=1, 2,…,p. Statistically, the set{xi1,xi2,…,xiN}represents a random sample of lengthN corresponding to the r.v. Xi. One can consider, without loss of generality, the naïve assumption that all attributes are independent of each other, i.e., the parent r.v.’s Xi, i=1, 2,…,p are independent.For each object xk, the labels yj, j=1, 2,…,q, are governed by a categorical r.v. Y, whose (categorical) distribution is denoted by F(y). Statistically, the set{yj1,yj2,…,yjN}represents a random sample of lengthN corresponding to the categorical r.v. Y.TS contains objects xkcharacterized by input=features and output=category, providing valuable information in data, ready to be used in the learning phase. Since this information (subjective prior information) based upon data is available, the (subjective) Bayesian approach suggests its use to improve the way to find an acceptable solution [16].Two relationships have to be quantified: (a) the connection between attributes and the decision classes, and (b) the connection between attributes and the network error existing in the training process.A straight way to discover and use the potential information within data is to assess the statistical dependence between the parent r.v.’s Xi, i=1, 2,…,p, and either the decision class variable Y, or the network error E(n) at step n, using measures of association [17]. Assuming a common case in real-world applications, that is a non-linear monotonic relationships between variables and the existence of many tied observations in data, we chose between the traditional non-parametric approaches (e.g., Spearman rank ρ, Kendall Tau, etc.) the Goodman–Kruskal Gamma rank correlation Γ, which is based on the difference between concordant pairs (C) and discordant pairs (D), and computed as Γ=(C−D)/(C+D).From a Bayesian point of view, a decision-making process naturally combines prior knowledge with information extracted from observations. Intuitively, given a hypothesis h, the data or evidence D, the posterior probability P(h|D) of h given D, the likelihood P(D|h), the prior probability P(h), and the evidence P(D), then:(1)P(h|D)=P(D|h)·P(h)P(D).Probabilistically speaking, the Bayes’ formula is given by:(2)P{Ai|B}=P{B|Ai}P{Ai}∑i=1nP{B|Ai}P{Ai},P{B}>0,P{Ai}>0,i=1,2,…,n,where B is an arbitrary event and {A1, A2,…,An} is a partition of the sample space Ω. In this context, P{Ai|B} represents the posterior probability, P{Ai} represents the prior probability, P{B|Ai} represents the likelihood, and P{B} the evidence.The idea behind the Bayesian classification is so that one can predict the class label of an object given its attributes values by the use of the Bayes’ rule. Given an object with attributes {A1, A2,…,An}, we wish to classify it in class C. Accordingly, we choose the class C=Ckthat maximizes P{A1,A2,…,An|Cj} [18].In classification/decision-making problems, given an object with attributes {A1, A2,…,An}, belonging to class C, one often assume the so-called naïve Bayes classification (Idiot’s Bayes), stating the independence of attributes (obviously, a false assumption most of the time) for a given class C, namely:(3)P{A1,A2,…,An|C}=P{A1|C}·P{A2|C}·P{An|C}.The main elements of a feed-forward NN (or a multi-layer perceptron), seen as a classification model, are:•input vector x=(x1, x2,…,xp) formed by p feature components xi;related output/response (multivariate) variable yj, j=1, 2,…,q;(synaptic) weights wij, connecting the output of neuron i to the input of neuron j, where neuron j lies in a layer to the right of neuron i;activation non-linear function f, usually chosen sigmoidal.Mathematically, MLP as classifier is seen as a computational framework for defining a non-linear mapping between a p-dimensional Euclidian input (feature) space and a q-dimensional Euclidian output (decision) space. It generally consists of three or more layers: an input layer, an output layer, and one or more hidden layers.Popular examples of continuously differentiable non-linear activation functions commonly used in MLP are the logistic sigmoid and the hyperbolic tangent “tanh”, the latter often preferred because it converges faster in many instances. Suitable values for the parameters of the above functions can be heuristically estimated [11,19,20]. Practitioners recommend the use of normalized inputs, i.e., average of input variables close to zero, instead of the original ones in order to increase the convergence speed [19]. An appropriate synaptic weights initialization allows the training algorithm to produce a good set of weights and may improve the training speed [11]; the non-parametric Goodman–Kruskal Gamma rank correlation between attributes and decision classes was used for the synaptic weights initialization. A key observation in its practical use, based on the universal approximation theorem applied to MLP, is that a network with a single hidden layer is sufficient to uniformly approximate any continuous function [11,20].We introduce in this paper a novel error-correction learning strategy for MLP based on the Bayesian paradigm. The following features characterize the proposed Bayesian-MLP (B-MLP) model:–Architecture: one hidden layer with the number of hidden units equaling the number of decision classes.Activation function: the hyperbolic tangent:–Presentation of training examples: normalized inputs, shuffled examples and batch training mode [19,20].Initialization: using the Goodman–Kruskal Gamma rank correlation between attributes and decision classes.Network output computed using the winner-takes-all paradigm: the neuron with the largest output value gives the decision class.Stopping criterion: the testing/generalization performance is adequate to the problem at hand.The three-layer perceptron architectural graph is illustrated in Fig. 1.The original error-correction learning refers to the minimization of a cost function, leading, in particular, to the commonly referred delta rule. The standard back-propagation algorithm applies a correction to the synaptic weights (usually, real-valued numbers) proportional to the gradient of the cost function.In order to use the Bayesian paradigm to update the synaptic weights, we consider them from a different perspective. As it has been shown in literature [21], the prior probability and data from continuous quantities used in the standard Bayesian inference are, in reality, more or less fuzzy. The fuzziness refers, in some respects, also to the synaptic weights and the error involved in the neural network structure.Inspired by both the fuzziness perspective involved in the Bayesian paradigm and the debate concerning fuzziness vs. (subjective) randomness in the Bayesian field [22], we distinctly tackled the problem. Rather than consider them as fuzzy data, we can reinterpret them from a (subjective) probabilistic point of view.For each hidden neuron HNjbelonging to the hidden layer, denote by wij, i=1, 2,…,p, j=1, 2,…,q, the corresponding synaptic weight of the input attribute xibelonging to the feature vector x. Assume that the real values of the synaptic weights wijoccurring in the learning process represent, from a statistical point of view, the values of a statistical variable, denoted by Wij, i=1, 2,…,p, j=1, 2,…,q. From a probabilistic point of view, behind this statistical variable there is its parent r.v. denoted, naturally, Wij. The network error is perceived in a similar way for the same reason.Standard pre-Bayesian “training” of neural networks involves estimating the values for wijto minimize the network error E(D, wij) given (training) dataset D, which implies some drawbacks [12]. An alternative approach, used in this paper, deals with the conditional probability P(E|wij) considered as ‘likelihood’ in Bayesian terms. Suppose that the events Aijcorresponding to Wijprovide a partition of the “weight space” W, and let E(n) be the error of the network in iteration n. According the total probability formula, we have:(5)P{E(n)}=∑i,jP{E(n)|Aij}P{Aij},i=1,2,…,p,j=1,2,…,q.The standard synaptic weight refers to the strength of a connection between two units, perceived as a measure of this strength. From a subjective Bayesianism, the probability is interpreted as a measure/degree of belief [16,23]. Under these circumstances, assuming that the real-valued synaptic weights belong to the interval [0,1], the synaptic weights might be interpreted as probability-like measure encoding the strength of a connection; the stronger the strength, the larger the corresponding probability. In such a paradigm, we can use the Bayes rule in the updating process, by considering the synaptic weights as posterior (probabilities).Accordingly, the values of wij(n+1) are given by:(6)wij(n+1)=P{Aij|E(n)}=P{E(n)|Aij}P{Aij}∑i,jP{E(n)|Aij}P{Aij},i=1,2,…,p,j=1,2,…,q,where P{Aij} represents the prior (probability), P{E(n)|Aij} the likelihood, and P{E(n)} the evidence.A correlation-based approach is proposed to estimate both the priors, the likelihood, and the initial values of the synaptic weights, even though “correlation does not always imply causation”. The correlation coefficient, whatever the type, is a measure of the relationship between two variables of interest, i.e., a measure of the strength/intensity of association, with values ranging from −1 to +1. Thinking the same way and taking into consideration the modulus Γ of the coefficient, it might be interpreted in the Bayesian framework as probability-like measure encoding the strength of the relationship.Having in mind that the dilemma “Objective vs. Subjective” related to the choice of priors is still standing [24], we used a subjective-based approach to solve it. Assuming that the synaptic weights are naturally related to the attributes influence on the decision class, the priors P{Aij} are considered subjective (informative), expressing specific information about the object x through the correlation between attributes Xiand decision Y. P{Aij} can be expressed by the rank correlation Γ between Xiand Y:(7)P{Aij}=Γ(Xi,Y),i=1,2,…,p,j=1,2,…,q.The likelihood can be expressed in the same way by means of the rank correlation Γ between Xiand E(n):(8)P{E(n)|Aij}=Γ(Xi,E(n)).The synaptic weights of the network are adjusted inspired by the Bayes’ theorem, according to the formula:(9)wij(n+1)=wij∗=Γ(Xi,E(n))·Γ(Xi,Y)∑iΓ(Xi,E(n))·Γ(Xi,Y),i=1,2,…,p,j=1,2,…,q.We present below the steps of the training process of MLP inspired by the Bayesian paradigm.Algorithm (training B-MLP)1.For each decision class Cj, j=1, 2,…,q, and for each attribute Ai, i=1, 2,…,p, compute the corresponding mean attribute valuemij.For each hidden neuron HNj, j=1, 2,…,q, compute the synaptic weights wij(initialization), given by:3.For each hidden neuron HNj, j=1, 2,…,q, compute the linear discriminant uj, given by:4.For each hidden neuron HNj, j=1, 2,…,q, consider the non-linear activation function given by hyperbolic tangent:5.For each decision class Cj, j=1, 2,…,q, encode the corresponding label yjusing the “1-of-q” rule for nominal/categorical data, i.e., y1∼(0, 0,…,1), y2∼(0, 0,…,1, 0),…,yq∼(1, 0,…,0).The hidden layer can be seen as a discrete random variable, whose distribution is characterized by a probability mass function g, which values are given by gj=g(f(uj)), via the formula:7.For each input item xkof the training set TS, compute the corresponding error as follows:8.Build the error array E=(error1, error2,…,errorN), using the error at each step.Update the synaptic weights according to the formula (9):Repeat steps 3–9 for a certain number of epochs until the stopping criterion is satisfied.Return the synaptic weightswij∗, which will be used in real-world applications.The proposed B-MLP model has been applied on six real-world medical datasets related to: breast cancer (3), lung cancer (1), diabetes (1), and heart attack (1), described below.1.Breast Cancer Wisconsin (Diagnostic) – BCWD (UCI Machine Learning repository). BCWD consist of 569 cases, with two decision classes: benign 357 (62.74%) instances and malign 212 (37.25%) instances. From the total of thirty-two attributes, ten numerical attributes have been considered as the most relevant from medical point of view: radius, texture, perimeter, area, smoothness, compactness, concavity, concave points, symmetry, fractal dimension (detailed description of the BCWD database at: http://archive.ics.uci.edu/ml/datasets/Breast+Cancer Wisconsin+%28Diagnostic%29).Breast Cancer – BC (UCI Machine Learning repository). BC consists of 286 cases with two decision classes: non-recurrent-events 201 (70.27%) instances and recurrent-events 85 (29.72%) instances. The database contains nine mixed attributes, with three numerical attributes and six categorical attributes: age, tumor-size, inv-nodes, menopause, node-caps, deg-malig, breast, breast-quad,irradiat (detailed description of the BC database at: http://archive.ics.uci.edu/ml/datasets/Breast+Cancer).Breast Cancer Wisconsin (Prognostic) – BCWP (UCI Machine Learning repository). BCWP consists of 198 cases with two decision classes: non-recurrent-events 151 (76.26%) instances and recurrent-events 47 (23.73%) instances. From the total number of thirty-four attributes contained by the database, ten numerical attributes have been considered to be the most relevant from medical point of view: radius, texture, perimeter, area, smoothness, compactness, concavity, concave points, symmetry, fractal dimension (detailed description of the BCWP database at: http://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Prognostic%29).Lung Cancer – LC (UCI Machine Learning repository). LC consists of 32 cases with three decision classes, three types of pathological lung cancers: 9 cases of type I, 12 cases of type II, 11 cases of type III. The database contains 56 ordinal (categorical) attributes (detailed description of the LC database at: http://archive.ics.uci.edu/ml/machine-learning-databases/lung-cancer/lung-cancer.names).Echocardiogram survival rate after heart attack – ECHO (UCI Machine Learning repository). ECHO consists of 132 cases (one censored data) with two decision classes: died 88 (66.66%) instances and survived 43 (32.57%) instances. The database contains 10 numerical attributes: still-alive, uniformity of cell size, age-at-heart-attack, pericardial-effusion, fractional-shortening, E-point septal separation, left ventricular end-diastolic dimension, wall-motion-score, wall-motion-index, mult (detailed description of the ECHO database at: http://archive.ics.uci.edu/ml/machine-learning-databases/echocardiogram/echocardiogram.names).Pima Indian Diabetes – PID (UCI Machine Learning repository). PID consists of 768 cases with two decision classes: tested negative for diabetes 500 (65%) instances and tested positive for diabetes (35%). The database contains 7 numerical attributes: number of times pregnant, plasma glucose concentration, diastolic blood pressure, triceps skin fold thickness, body mass index, diabetes pedigree function, age (detailed description of the PID database at: http://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.names).

@&#CONCLUSIONS@&#
Automated medical diagnosis, developed as a collaborative paradigm involving both medical knowledge and Artificial Intelligence methods, has become a very important interdisciplinary technology in health care, yielding fast accurate diagnoses obtained with low costs. The effectiveness of a novel ML algorithm, based on a MLP trained using the Bayesian paradigm in conjunction with the error-correction learning was investigated on the task of providing a reliable real-time decision support for the medical diagnosis. The model was validated in real-world applications regarding breast cancer, lung cancer, heart attack and diabetes. Its performance equaled or exceeded the results reported in literature.Future research may lie in:•The use of alternative approaches to the Goodman–Kruskal Gamma rank correlation.The use of alternative non-linear activation function.The use of alternative network output computation to the winner-takes-all paradigm.