@&#MAIN-TITLE@&#
Spatially varying image based lighting using HDR-video

@&#HIGHLIGHTS@&#
We present a production ready systems pipeline for image based capture and processing of real world scenes.High dynamic range video is used for scene capture.We show algorithms for reconstruction and modeling of the geometric and radiometric properties of the scene.We show how our methods can be used to create highly realistic computer graphics renderings.

@&#KEYPHRASES@&#
High dynamic range video,Image based lighting,Scene capture and processing,Photo realistic rendering,

@&#ABSTRACT@&#
Graphical abstractIn this paper, we present a novel image based framework capturing and rendering with the lighting found in real world scenes. The framework is based on recent developments in HDR video capture, enabling efficient capture of the full dimensionality of the illumination in large environments exhibiting both complex spatial and angular variations. The image shows a comparison between traditional image based lighting (left), and our method (right). It is evident that the lighting complexity enabled by HDR video based scene capture increases the realism and visual interest in the resulting renderings significantly.

@&#INTRODUCTION@&#
The production of photo-realistic computer graphics renderings, seamlessly merging synthetic objects into real world scenes, is a fundamental goal in computer graphics. A difficult and time consuming challenge is to accurately model the synthetic lighting to match that of the real environment. This has motivated the development of Image Based Lighting (IBL) techniques [1], where the light in real world scenes is captured and used as a source of illumination in renderings. IBL has recently been extended from using a single HDR light probe measurement to include sequences of High Dynamic Range (HDR) images captured at different locations in the scene [2,3] capturing the full 5D spatial and angular dimensionality of the illumination. This generalization enables the production of highly realistic IBL renderings in environments exhibiting complex spatial variations with preserved parallax. However, these methods have previously been constrained to small scenes in controlled environments, and require extensive manual processing to produce good results. Furthermore previous methods are limited to simplistic representations which make post-processing and editing of the captured data difficult, and in some cases even impossible.In this paper, we present a novel framework for building detailed representations of real world illumination captured in general scenes that can be edited, and re-used, and which produce superior image quality compared to previous methods. The framework is based on recent developments in HDR video capture [4,5] which enables efficient capture of the full dimensionality of the illumination in large environments exhibiting both complex spatial and angular variations. From the comparison in Fig. 1between traditional IBL 1a and our method 1b, it is evident that the realism and visual interest in the resulting renderings increase significantly by the lighting complexity enabled by HDR video based scene capture.The HDR video sequences are often tens or hundreds of GB in size and pose great challenges for efficient storage, processing, and rendering. We present solutions to these challenges, and integrate the algorithms into a functioning software toolset and corresponding data structures. Our framework consists of a number of coupled hardware and software components for HDR video based scene capture and reconstruction and modeling of both the geometric and the radiometric properties of the scene, as well as data structures for efficient rendering of the reconstructed scene illumination. Our pipeline can, as illustrated in Fig. 2, be divided into five steps:1.Scene capture: The scene (indoor or outdoor) is captured using a combination of panoramic HDR light probe sequences and ordinary HDR video sequences with a smaller field of view.Scene reconstruction and modeling: The goal of the scene reconstruction and modeling step is to generate a geometric proxy model of the scene, and to extract the light sources in the scene. This is carried out as a semi-automatic, purely image-based process where we use a combination of structure from motion techniques and a novel interactive modeling approach based on shape from shading that we call focal volume modeling.Radiance re-projection: The radiance data captured in the HDR sequences is then re-projected onto the proxy geometry where it is stored as view dependent textures (light fields) in an adaptive 2D/4D data structure which enables fast lookup during rendering.Light field processing: All points on all objects have not been imaged from all directions, because the sample region is of finite size and resolution, and there may be occlusions in the scene. This leads to holes in the re-projected radiance data. The final step is to fill in such holes by the mean or interpolated value in the angular domain or, if the geometry is accurately recovered, by fitting a parametric BRDF to the captured data.Rendering: The resulting model can be directly used for rendering of highly realistic images of virtual objects placed into the real scene, using a rendering approach that is straightforward to incorporate in most rendering packages.To show the utility of the approach, we take our examples from IKEA Communications AB, the creators of the most widely distributed publication in the worldâ€”the IKEA Catalogue. We show how the presented framework can be applied to rapid generation of ultra realistic renderings of virtually furnished scenes.

@&#CONCLUSIONS@&#
