@&#MAIN-TITLE@&#
Weighted spectral features based on local Hu moments for speech emotion recognition

@&#HIGHLIGHTS@&#
HuWSF is computed from local regions of a spectrogram by Hu moments.HuWSF can evaluates how the energy aggregate to some frequencies in a spectrogram.HuWSF extracts the features among neighbor coefficients of Mel filters of a frame.HuWSF extracts the features among coefficients of Mel filters of neighbor frames.HuWSF can reduce the changes brought by sentences, speakers and speaking styles.

@&#KEYPHRASES@&#
Speech emotion recognition,Speech spectral features,Feature extraction,Hu moments,

@&#ABSTRACT@&#
Features greatly influence the results of speech emotion recognition, among which Mel-frequency Cepstral Coefficients (MFCC) is the most commonly used in speech emotion. However, MFCC does not consider both the relationship among neighbor coefficients of Mel filters of a frame and the relationship among coefficients of Mel filters of neighbor frames, which possibly leads to lose many useful features from spectrogram. This paper presents novel weighted spectral features based on Local Hu moments. The idea is motivated by that the energy on spectrogram would drastically vary with some emotion types such as angry and happy, while it would slightly change with other emotion types such as sadness and fear. This phenomenon would affect the local energy distribution of spectrogram in both time axis and frequency axis of spectrogram. To describe local energy distribution of spectrogram, Hu moments computed from local regions of spectrogram are used, as Hu moments can evaluate the degree how the energy is concentrated to the center of energy gravity of local region of spectrogram and can significantly vary with the speech emotion types. The conducted experiments validate the proposed features in terms of the effectiveness of the speech emotion recognition.

@&#INTRODUCTION@&#
Speech is one of the most important communication means of human beings. It is much expected that human emotions can automatically be recognized by machines to improve human machine interaction [1]. With the growth in the electronic and computer technologies, new spoken dialog systems with emotion recognition capability are needed. For example, a nursing robot that can continuously monitor the patients’ emotional state could provide better health-care services for patients [2]. However, it is very challenging to achieve this goal for the following reasons. Firstly, it is not clear which speech features are most powerful in distinguishing emotions [3]. Secondly, the acoustic variability introduced by the different sentences, speakers, and speaking styles adds another obstacle [2,3]. In addition, speech signals with noises can also change acoustics’ distribution greatly.To solve the above problems, many kinds of speech feature types have been presented. Among them, Mel-frequency Cepstral Coefficients (MFCC) is the most commonly used feature type [4,8,11,14–23,26,28,35–37]. It reduces some negative impact produced by Discrete Cosine Transform (DCT) for noisy and different sentences, speakers, and speaking styles. However, MFCC does not consider both the relationship among neighbor coefficients of Mel filters of a frame and the relationship among coefficients of Mel filters of neighbor frames, which may lose many useful features from spectrogram.There are some methods that can characterize both kinds of relationships among coefficients of Mel filters, such as mean, standard, entropy, skewness, kurtosis, and so on. However, these methods have two defects. One is that the differences among emotions could not be well discovered by mean, standard deviation, skewness and kurtosis. The other is that some methods can be easily influenced by the differences among the sentences, speakers, and speaking styles, which is much bad for speech emotion recognition. In this paper, Hu moments [25] are utilized to overcome these two problems.Hu Moments have been widely used as basic feature descriptors in image analysis, pattern and object recognition, image classification, and template matching [59–61]. Hu moments have two advantages: (1) the first absolute orthogonal invariant of Hu moments can evaluate the degree how the energy is concentrated to the center of energy gravity for two dimensional data; (2) Hu moments are invariant with respect to translation, scaling, as well as rotation [25,59–61]. It is reasonable to conclude that Hu moments could be good to characterize the relationship among coefficients of neighbor Mel filters within a frame, as well as the relationship among coefficients of Mel filters of neighbor frames.Firstly, Hu moments can be used to extract some differences among different emotions. Under different speech emotions, the pronounced strength, the articulation, the degree of changing pitch frequency, and the pronounced speed would obviously be changed [50–52]. These changes will alter the degree how the energy concentrated to some frequencies in a spectrogram. For example, energy is more concentrated in certain frequency in spectrogram when the utterance owns better articulation and higher pronounced strength. Obviously, if Hu moments are computed in local regions of a spectrogram, they can evaluate the degree how the energy concentrated to some frequencies in a spectrogram. It indicates that Hu moments have good ability to extract the differences among the emotions.Secondly, Hu moments can reduce the changes introduced by the sentences, the speakers, and the speaking styles, because Hu moments are invariant with respect to translation, scaling, as well as rotation. For example, different words may own different formant frequencies, easily leading to translation and rotation of energy. However, Hu moments computed in local regions of a spectrogram can effectively reduce these negative influences.The remainder of this paper is organized as follows. Section 2 introduces the proposed features. Section 3 presents the other features that can be combined with the proposed features to reach the better performance for speech emotion recognition. Section 4 presents the speech emotion framework used in our context. Experimental results are presented and discussed in Section 5, while Section 6 gives concluding remarks.This section introduces the proposed weighted spectral features based on Local Hu moments, denoted by HuWSF.Hu moments have been used as basic feature descriptors for images, for the reasons that they are invariant with respect to translation, scaling, as well as rotation. Here only the first absolute orthogonal invariant of Hu moments is used as a feature descriptor for speech, because it can also evaluate the degree how the energy is concentrated to the center of energy gravity of two-dimensional data. For a density distribution function g(u, v), the first absolute orthogonal invariant of Hu moments is defined as:(1)θ=η20+η02(2)ηpq=μpq(φ00ρ),ρ=(p+q)2+1(3)μpq=∑u=1U∑v=1V(u−u¯)p(v−v¯)qg(u,v),p,q=0,1,2⋯(4)φpq=∑u=1U∑v=1Vupvqg(u,v),p,q=0,1,2⋯where ηpq is (p+q)-th order normalized center moment, μpq is (p+q)-th order center moment, φpq is (p+q)-th order moment, and(u¯,v¯)is the center of energy gravity, whereu¯=φ10/φ00,v¯=φ01/φ00.The flowchart of extracting HuWSF is presented in Fig. 1, where the flowchart of extracting MFCC is also presented with an attempt to make comparison between HuWSF and MFCC. It can be seen from Fig. 1 that the main difference between HuWSF and MFCC lies in that MFCC is calculated directly from E while HuWSF is calculated from Q, where Q is constructed by θ which computed from local blocks on E.As described in Fig. 1, HuWSF includes six main steps. In the first step, the speech signal S is framed into short-term segments Skby multiplying wlsamples width Hamming window with wssamples shift, where k denotes the frame index,wl=1.81∗fs/b,ws=1.81/(4∗b)[33], fsis the sampling rate of S, and b is the frequency resolution of short time Fourier transform(STFT).In the second step, Tkis computed by STFT from Sk.In the third step, Mel frequency Melkand the log energy Ek(m) are computed by Eqs. (5) and (6) respectively,(5)Melk=2595∗lg1+Tk700(6)Ek(m)=ln∑f=flfhMelk(f)2Hm(f),0≤m<Mwhere fland fh, assigned by fl=0HZ and fh=4000HZ, are the low and high boundaries of the computed speech frequency, Hm(f) defines a triangular filter by Eq. (7), and M is the number of Mel filters.(7)Hm(f)=0,f<C(m−1)2(f−C(m−1))(C(m+1)−C(m−1))(C(m)−C(m−1)),C(m−1)≤f<C(m)2(C(m+1)−f)(C(m+1)−C(m−1))(C(m+1)−C(m)),C(m)≤f<C(m+1)0,f≥C(m+1)where∑m=0M−1Hm(n)=1, C(m), m=1, 2, ⋯M is the center frequency of triangular, and the interval between two C(m) increases with m.The fourth step computes the local Hu moments Q of E. To compute local Hu moments Q, E is firstly divided into(K−w+1)*(M/w)blocks Bij by Eq. (8), where w is the size of block. The blocks are square with the reason that b can control frequency resolution of STFT, which can balance the relationship between time domain and frequency domain. Secondly, θ of Bij is computed by Eq. (1), whereg(u,v)in Eqs. (3) and (4) can be defined by Eq. (9). Finally, all θ are used to constructQ∈R(k−w+1)*(M/w).(8)Bij=Ei(w*j)⋯Ei(w*j+w−1)⋮⋮⋮Ei+w−1(w*j)⋯Ei+w−1(w*j+w−1)i=1⋯K−w+1,j=1⋯M/w(9)g(u,v)=Bij(u,v)In the fifth step,TQk∈R1*(M/w)is computed by Eq. (10) from Qk. Here only the values from the 2-nd to 13-th coefficients of TQkare selected, as the high cepstral coefficients may be distorted by noise.(10)TQk(m)=Dm∑n=1M/wQk(n)cos(2n−1)mπ2(M/w)where Dmis defined as(11)Dm=1M/wm=12M/wm=2,⋯M/wIn the sixth step, HuWSF is generated by combining Q with TQ, whose dimension is(K−w+1)*(M/w+12).The algorithm of HuWSF is summarized as Algorithm 1. The code of HuWSF can be downloaded from: http://yunpan.cn/QCKNH3GyMR4wi (password: d5af).Algorithm 1HuWSFInput: a speech signal S, the frequency resolution of STFT b, the block width w, and the number of Mel filters M.Output: HuWSF features FAlgorithm:1: Frame S into short-term segments Skk=1, 2, ⋯K by hamming window, where the sampling rate fscan be achieved from S.2: For k=1 to KCompute STFT results Tkof all Sk.End For3: For k=1 to KCompute log energy Ek(m) by Eq. (6).End For//ComputeQ4: Divide E into(K−w+1)*(M/w)blocks by Eq. (8) and compute θ for each block by Eq. (1). As a result, θ of all blocks are constructed Q.//ComputedTQ5: For k=1 to K-w+1ComputeTQk∈R1*(M/w)from Qk. by Eq. (10)End For//Get HuWSF6: Compute HuWSF features F by connecting Q and TQ in frequency dimension as F=[Q TQ]Similar to the reference [14] which uses the dynamic feature vector of MFCC by appending the first and the second time derivatives of MFCC into the feature vector, we also use the dynamic feature vector of HuWSF, defined by FGΔ=[FG, ΔFG, ΔΔFG] where FGrepresents HuWSF, ΔFG represents the first-order derivative of FG, and ΔΔFG represents the second-order derivative of FG.It can be seen from Eqs. (1)–(4) that θ can evaluate the degree of how the energy concentrated to the center of gravity, which is proved in appendix. We compute each θ in local blocks of spectrogram, so that θ can evaluate the degree of how energy of frequencies concentrates to some frequencies in a spectrogram, which may significantly vary with the speech emotion types. Furthermore, θ is not only independent of position, size, and orientation, but also independent of the parallel projection [25], so that the differences among the sentences, speakers, and speaking styles can be reduced by θ. Because HuWSF is based on θ, HuWSF share the advantages of θ.To further illustrate the advantages of HuWSF, we present the visualization results of Q and TQ, which are components of HuWSF. We also give the visualization results of E that are the results of the third step of Algorithm 1. The visualization results of MFCC are also presented for comparison, as HuWSF are improved from MFCC. The visualization results of E, Q, MFCC, and TQ are presented respectively from the top line to the bottom line in Fig. 2. The pictures in each column of Fig. 2 are the experimental results of an utterance that are randomly selected from Berlin Speech Emotion Database (EmoDB), where the red box in the first and second rows of Fig. 2 indicates an example of vowel part, and the green box in the first and second rows of Fig. 2 indicates an example of voiceless part.By comparison with the visualization results of E and Q, Q has the following advantages. Firstly, in the vowel part of Q, strong energy peaks are more clearly and weak energy peaks have the opposite effect. This is very significant as the strong energy peaks are often more important for extracting features from speech signals [13,15,50–52]. Secondly, in the voiceless part of Q, the energy distributions of Q is smoother, showing that Q is also the more useful features for the part of voiceless of speech signal. The above advantages are illustrated in more detail in Appendix A.Compared with the visualization results of MFCC, TQ owns the advantages as follows. Firstly, the boundaries between the part of voice activity and silence are more clear. Secondly, in the part of voice activity, the coefficients among the neighbor frames of TQ are smoother than that of MFCC, which are more in line with the characteristics of the speech signal. These advantages are also illustrated in more details in Appendix A.As HuWSF is formed by Q and TQ, HuWSF shares the advantages of both Q and TQ.The currently used features in speech emotion recognition can be also applied to combine with HuSWF to further improve the performance. This is because most of the speech emotion recognition methods often use more than one feature extraction method to extract features [1,11,16,26,39,56,58] and the combined features can greatly enhance the effect of speech emotion recognition. These features can be divided into two categories: spectral features and prosodic features [3].In addition to MFCC and HuWSF, there are some other features belonging to spectral features, such as LPCC (Linear Predictor Cepstral Coefficients) [5,17,22,28,32,35,36], ZCPA(Zero Crossings with Peak Amplitudes) [13,15,38], Harmonic [4], MSF (Modulation Spectral Features) [8], LFPC (Log Frequency Power Coefficients)[6,28,35,40], PLP (Perceptual Linear Predictive) and RASTA-PLP(RASTA Perceptual Linear Predictive) [7,8,40], WMFCC (Weighted Mel-frequency Cepstral Coefficient) [14], and RSS (Ratio of a Spectral flatness measure to a Spectral center) [22]. Among them, some will be compared with HuWSF in the later experiments.Prosodic features are often used together with spectral features in speech emotion recognition, as they have good supplement effectiveness to spectral features. However, it can be seen from the second column of Table 1that the prosodic features used in different references are very different [1,11,16,26,39,56,58]. To prove that HuWSF has a better supplement effectiveness to prosodic features than the other spectral features, the combined prosodic features should be typical and complete. Fortunately, Björn Schuller proposed five different baseline acoustic feature sets for speech emotion recognition [45–49]. Here INTERSPEECH 2010 [45] feature set is selected, for the reasons that INTERSPEECH 2010 feature set contains most prosodic features that are used in the references [1,11,16,26,39,56–58].INTERSPEECH 2010 feature set results from a base of 34 Low-Level Descriptors (LLD) with 34 corresponding delta coefficients appended, and 21 functions applied to each of these 68 LLD contours (1428-features). In addition, 19 functions are applied to the 4 pitch-based LLD and their four delta coefficient contours (152 features), where 19 functions are selected from 21 functions mentioned by removing the minimum value and the range functions. Finally the number of pitch onsets (pseudo syllables) and the total duration of the input are appended (2 features). Table 2gives an overview of the low-level descriptors and associated feature statistics functions. The details of each item can be seen from reference [45]. This feature set can be obtained by the toolbox named OpenSmile [29].It can be seen from Table 2 that INTERSPEECH 2010 feature set contains MFCC features and prosodic features. In order to obtain the prosodic features, we remove MFCC features from INTERSPEECH 2010 feature set. The remaining features are prosodic features, which will be used in the later experiments. We name this feature set as PROS.To evaluate the HuSWF, typical speech emotion recognition framework is applied, which includes four steps [9,11,15–19,23]: feature extraction, feature statistics, dimensional reduction, and classification. HuSWF is performed for feature extraction. The remaining three steps in the framework will be simply introduced as follows.Feature statistics estimates the global statistics of the extracted features in order to form a feature vector for the speech signal. The global statistics are useful in speech emotion recognition, as they are less sensitive to linguistic information. Currently, many statistical methods have been applied to estimate the global statistics of the extracted features. Some of them are illustrated in the third column of Table 1.In our framework, the following feature statistics methods are selected: mean, std, max, min, kurtosis, skewness, and median. They are the most used feature statistics methods in the recent speech emotion recognition [1,11,16,26,39]. The feature statistics of the spectral features are computed by the above feature statistics methods, while the feature statistics of the prosodic features are computed directly by OpenSmile toolbox [29].There may be high correlation among the global statistics features, which could have negative influence on speech emotion recognition. Currently there are many approaches performing dimensionality reduction to solve the problem. For example, PCA (Principal Component Analysis) [38] and LDA (Linear Discriminant Analysis) [8] can be used to perform feature extraction. mRMR (Minimum Redundancy Maximum Relevance Feature Selection) [27,30,31], SFS (Sequential Forward Feature Selection) [3,8,9,17], SFFS (Sequential Forward float Feature Selection) [3], MCFS(multi-cluster feature selection) [44], and DISR(Double Input Symmetrical Relevance) [43] can be applied to perform the feature selection. Which one is the best in the framework will be discussed in the later experiments.Classification aims to obtain the emotion type for the input speech feature vector. Typical classification approaches include HMM [19,20,28], GMM [18–23,35,37], ANN [18,38], K-NN [17,18,21,26,42], SVM [8,10,11,15,16,18,23,26,38,54], and Bayesian classifier [12,15].In our framework, SVM is used for Classification, where the toolbox of SVM named LIBSVM is used [24] with the polynomial kernel, as polynomial kernel nearly owns the same results as that of RBF kernel.

@&#CONCLUSIONS@&#
