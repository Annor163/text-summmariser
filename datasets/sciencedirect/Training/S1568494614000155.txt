@&#MAIN-TITLE@&#
A type-2 neural fuzzy system learned through type-1 fuzzy rules and its FPGA-based hardware implementation

@&#HIGHLIGHTS@&#
We propose a new type-2 neural fuzzy system (FS) learned through structure and parameter learning.A type-1 FS is converted to a type-2 FS by merging highly overlapped type-1 fuzzy sets.A new hardware circuit is proposed to implement an interval type-2 FS with TSK-type consequents.

@&#KEYPHRASES@&#
Type-2 fuzzy systems,Fuzzy neural networks,Neural fuzzy systems,Fuzzy chips,Fuzzy hardware,

@&#ABSTRACT@&#
This paper first proposes a type-2 neural fuzzy system (NFS) learned through its type-1 counterpart (T2NFS-T1) and then implements the built IT2NFS-T1 in a field-programmable gate array (FPGA) chip. The antecedent part of each fuzzy rule in the T2NFS-T1 uses interval type-2 fuzzy sets, while the consequent part uses a Takagi-Sugeno-Kang (TSK) type with interval combination weights. The T2NFS-T1 uses a simplified type-reduction operation to reduce system training time and hardware implementation cost. Given a training data set, a TSK type-1 NFS is first learned through structure and parameter learning. The built type-1 fuzzy logic system (FLS) is then extended to a type-2 FLS, where highly overlapped type-1 fuzzy sets are merged into interval type-2 fuzzy sets to reduce the total number of fuzzy sets. Finally, the rule consequent and antecedent parameters in the T2NFS-T1 are tuned using a hybrid of the gradient descent and rule-ordered recursive least square (RLS) algorithms. Simulation results and comparisons with various type-1 and type-2 FLSs verify the effectiveness and efficiency of the T2NFS-T1 for system modeling and prediction problems. A new hardware circuit using both parallel-processing and pipeline techniques is proposed to implement the learned T2NFS-T1 in an FPGA chip. The T2NFS-T1 chip reduces the hardware implementation cost in comparison to other type-2 fuzzy chips.

@&#INTRODUCTION@&#
The neural-fuzzy approach to data-based modeling and prediction has drawn much research attention in the last two decades. While many neural fuzzy systems (NFSs) have been proposed, most studies have used type-1 fuzzy logic systems (FLSs) [1–4]. In recent years, studies on the theory and applications of interval type-2 FLSs have become a research focus. Interval type-2 FLSs are extensions of type-1 FLSs, where the membership value of an interval type-2 fuzzy set (FS) is an interval type-1 FS. Several advantages of using interval type-2 FLSs over their type-1 counterparts have been reported [5–7]. However, the footprint of uncertainty and operations with interval values in an interval type-2 FLS also lead to greater complexity in computing system outputs and assigning proper system parameters.To automate the design of interval type-2 FLSs, several interval type-2 NFSs have been proposed with claimed superiority over the type-1 NFSs used for comparison [7–16]. Parameter learning of interval type-2 FLSs using a gradient descent algorithm was proposed in [7]. The approach does not use structure learning to determine the number of rules and FSs. In other words, the structure is fixed and should be assigned in advance. Several studies on structure learning of interval type-2 FLSs have been proposed [8–16]. These studies use the idea of clustering to generate type-2 fuzzy rules. The general approach is using the maximum value of centers of interval firing strengths as a rule generation criterion for each incoming datum [9–11,13–16]. Because structure learning is easier when using type-1 fuzzy rules than type-2 fuzzy rules, type-1 NFSs with structure learning ability have been extensively studied. This paper proposes a new method that builds a type-2 NFS via extending a built type-1 NFS (T2NFS-T1). For previous interval type-2 NFSs [8–15], the type-reduction outputs are computed using an iterative procedure, such as the Karnik–Mendel procedure [5], which is computationally expensive. For this problem, the T2NFS-T1 uses a simple weighted sum operation [17] to simplify the type-reduction operation, which reduces both software training time and hardware implementation cost. In learning, the T2NFS-T1 can be used to convert well-trained type-1 NFSs learned through different types of learning algorithms to type-2 NFSs without regeneration of the type-2 rules from an empty set. This is different from previous type-2 NFSs that generate type-2 fuzzy rules from an empty set [9–16], which do not make good use of the learning results in extensively studied type-1 NFSs. Given a training data set, a TSK type-1 NFS is first learned through structure and parameter learning. The T2NFS-T1 is then initialized from extending the built type-1 FLS to its type-2 counterpart. The general approach to the learning of an interval type-2 FLS from a type-1 FLS is by extending all type-1 FSs to interval type-2 FSs. In this direct conversion approach, the number of interval type-2 FSs is simply equal to the number of type-1 FSs in each variable. In the proposed extension approach, highly overlapped type-1 FSs are merged to interval type-2 FSs. This approach reduces the number of FSs in each input variable. That is, this operation improves FS transparency which is an important factor in the design of an interpretable FLS [18,19]. This learning approach is also different from most previous type-2 NFSs where the number of type-2 FSs is simply set be equal to the number of fuzzy rules [10,11,13,15]. Similarity measure of type-1 FSs is well-defined and is easier to compute than that of interval type-2 FSs [20]. As such, it is easier to merge type-1 FSs based on a similarity measure degree to reduce the total number of FSs in each input variable. The interval type-2 FLS adds extra degrees of freedom to a learned type-1 FLS, which would help to obtain a T2NFS-T1 that outperforms a type-1 NFS using the same rule base size. After initialization, parameter learning using a hybrid of the gradient descent and rule-ordered recursive least square (RLS) is used to tune the T2NFS-T1 to an optimal extent.In addition to converting a type-1 FLS to a type-2 FLS using the T2NFS-T1, this paper proposes a new circuit to implement the learned type-2 FLS for the consideration of real applications. Several studies on the hardware implementation of interval type-2 FLSs have been proposed [10,16,21–27]. In contrast to type-1 fuzzy circuits [28–30], which process crisp values, interval type-2 fuzzy circuits process intervals, which require a relatively heavier computational load. In particular, finding the two interval boundary points in an interval type-2 FLS extended output requires an iterative procedure, such as the Karnik–Mendel procedure [5], and is costly in the designed chips [24–26]. To reduce the implementation cost, the circuit in [22] used Wu–Mendel closed form [31] for boundary point estimation. However, the computation load is still heavy for hardware implementation. A Look-Up-Table (LUT) circuit was proposed in Ref. [10,23] to store the left and right crossover points in advance for system output calculation, which avoids an iterative procedure. However, the LUT size grows exponentially with input dimensions and is most suitable to systems with low-dimensional inputs. In addition, this approach only applies to rules with constant consequent values. For the first-order Takagi-Sugeno-Kang (TSK)-type rules used in the hardware-implemented T2NFS-T1, the consequent values change with inputs, and therefore, the LUT circuit is not applicable. The simplified type-reduction operation [17] was used in an interval type-2 neural fuzzy chip with on-chip learning ability (IT2NFC-OL) to reduce the implementation cost [27]. This operation is also used in the hardware-implemented T2NFS-T1 (H-T2NFS-T1). In contrast to the IT2NFC-OL that uses Mamdani-type fuzzy rules, the H-T2NFS-T1 uses first-order TSK-type rules whose implementation using circuits is more complex and difficult. In addition, to accelerate the T2NFS-T1 chip execution speed, the paper uses four-pipeline in contrast to the two-pipeline technique in the IT2NFC-OL. The implementation of FLSs using personal computers is costly and inconvenient in some actual applications. In contrast to the personal computers, the H-T2NFS-T1 chip is much more suitable to actual applications that require small size and low-power consumptions in the implementation components, such as the fuzzy control problems [28,29,32]. In addition, the four-pipeline structure in the H-T2NFS-T1 chip is suitable to real-time operations that require high inference speed.This paper is organized as follows. Section 2 introduces functions of the T2NFS-T1. Section 3 introduces the type-1 NFS based on which the T2NFS-T1 is built. Section 4 introduces the conversion of a type-1 FLS to a type-2 FLS and the subsequent parameter learning algorithms in the T2NFS-T1. Section 5 introduces FPGA implementation of a software-designed T2NFS-T1. Section 6 presents the software simulation implementation result of the T2NFS-T1. Section 7 presents the hardware implementation result. Finally, Section 8 presents the conclusion.Each first-order TSK-type rule in the T2NFS-T1 has the following form:(1)Rulei:IFx1isA˜1iAND…ANDxnisA˜niTHENyisa˜0i+∑j=1na˜jixj,i=1,…M,where x1, ..., xnare input variables, y is output variable,A˜ji, j=1, ..., n are interval type-2 FSs, M is the number of rules, anda˜ji's, j=0, ..., n are interval sets with the following representation:(2)a˜ji=[cji−sji,cji+sji],j=0,…,n.The consequent value of each rule is an interval[wli,wri], where the indices l and r represent the left and right limits, respectively. According to (1) and (2), the consequent interval value is(3)[wli,wri]=[c0i−s0i,c0i+s0i]+∑j=1n[cji−sji,cji+sji]⋅xj.That is,(4)wli=∑j=0ncjixj−∑j=0nxjsji,wri=∑j=0ncjixj+∑j=0nxjsjiwhere x0≔1. The output of the T2NFS-T1 is obtained via fuzzifcation, fuzzy meet, type-reduction, and defuzzification operations introduced as follows.In the fuzzification operation, a Gaussian primary membership function having a fixed standard deviationσjiand an uncertain mean that takes on values in[mj1i,mj2i]is used, i.e.,(5)μA˜ji=exp−12xj−mjiσji2≡N(mji,σji;xj),mji∈[mj1i,mj2i].The footprint of uncertainty of this membership function can be represented as a bounded interval in terms of an upper membership function,μ¯A˜ji, and lower membership function,μ_A˜ji, where(6)μ¯A˜ji(xj)=N(mj1i,σji;xj)xj<mj1i1mj1i<_xj<_mj2iN(mj2i,σji;xj)xj>mj2i,μ_A˜ji(xj)=N(mj2i,σji;xj)xj<_mj1i+mj2i2N(mj1i,σji;xj)xj>mj1i+mj2i2.That is, the output of the fuzzification operation can be represented as an interval type-1 fuzzy set [μ_A˜ji,μ¯A˜ji].The fuzzy met operation finds the firing strength of each rule and is implemented using an algebraic product function [5]. The output is a firing strength, Fi, which is an interval type-1 FS given as follows:(7)Fi=[f_i,f¯i]where(8)f¯i=∏j=1nμ¯A˜jiandf_i=∏j=1nμ_A˜ji.In the type reduction operation, the T2NFS-T1 uses the following simple weighted sum operation [17] to find the extended output:(9)[yl,yr]=∫w1∈[wl1,wr1]⋯∫wM∈[wlM,wrM]∫f1∈[f_1,f¯1]⋯∫fM∈[f_M,f¯M]1/∑i=1Mfiwi.Eq. (9) is equivalent to compute:(10)[yl,yr]=∑i=1Mfiwi=∑i=1Mf_i,f¯i×wli,wriwhere yland yrdenote the minimum and maximum values, respectively. Based on the interval product result, the outputs yland yrin (10) can be expressed as follows:(11)yl=∑i=1Mfliwli,wherefli=f¯i,ifwli≤0f_i,ifwli>0and(12)yr=∑i=1Mfriwriwherefri=f_i,ifwri≤0f¯i,ifwri>0.In (11) and (12), the outputs yland yrare expressed in closed forms using only multiplication and addition operations, which avoids not only an iterative and computationally expensive procedure but also a costly division operation in chip implementations.The defuzzification operation computes the output variable y by taking the average of yland yr. Hence, the defuzzified output is(13)y=yl+yr2.The type-2 fuzzy rules in the T2NFS-T1 are initialized from their type-1 counterparts. For the construction of a type-1 FLS, different data-driven learning algorithms have been proposed [1–4] and these methods can be employed in the T2NFS-T1. This paper uses the learning algorithm in the self-constructing neural fuzzy inference network (SONFIN) [1] because it is easy in implementation and shows good learning performance [33,34]. This section describes the rules and the learning approach in the SONFIN for building a type-1 FLS.For the type-2 fuzzy rule in (1), its type-1 counterpart is represented in the following form:(14)Ri:IFx1isA1iAND…ANDxnisAni,THENyisa0i+∑j=1najixj,i=1,…,M,whereAjiis a type-1 FS andajiis a crisp value. The inputs x1, …, xnare scaled to lie in the range [−1, 1]. The type-1 FSAjiuses a Gaussian membership function given by(15)μji(xj)=exp−xj−mjibji2wheremjiandbjirepresent the center and width of the FS, respectively. Similar to (5) and (6), the rule firing strength Fi(x) of type-1 rule i is calculated by(16)Fi(x)=∏j=1nμji(xj)=exp−∑j=1nxj−mjibji2.The T2NFS-T1 uses the simple weighted sum operation in (9) to find the extended output of an interval type-2 FLS. For consistency, the output of the type-1 counterpart is also calculated by the simple weighted sum operation and is given as follows:(17)y=∑i=1MFi(x)∑j=0najixj.In structure learning, the rule firing strength Fi(x) is used as a criterion for type-1 fuzzy rule generation [1]. That is, for each piece of incoming datax=(x1, …,xn), find(18)I=argmax1≤i≤M(t)Fi(x)where M(t) is the number of existing rules at time t. If FI(x)≤ϕth, then a new rule is generated to cover the input data, where ϕth∈(0, 1) is a pre-specified threshold. Once a new rule is generated, the initial centers and widths of the new rule are assigned as follows:(19)mjM(t)+1=xj(t),bjM(t)+1=β∑j=1n(xj−mjI)2where β>0 decides the overlapping degree between two clusters. This paper sets β to 0.5 so that there is a suitable overlap between two clusters in the input space. Eq. (19) indicates that the number of type-1 FSs is equal to the number of type-1 fuzzy rules. The initial consequent parameter values of a new rule are set toa0M(t)+1=yd(t+1)andajM(t)+1=0.01, j=1, …, n, where ydis the desired output for inputx⇀.For each piece of training datum, the parameter learning phase follows the structure learning phase. Consider the single-output case for clarity; the objective is to minimize the error function:(20)E=12[y(t+1)−yd(t+1)]2where y(t+1) and yd(t+1) denote the real and desired outputs, respectively. As in [1], the consequent parameters of the constructed type-1 FLS are tuned by a RLS algorithm with a forgetting factor λ=0.9995 and the antecedent parameters are tuned by a gradient descent algorithm with a learning constant η=0.02.This section introduces how to convert the learned type-1 fuzzy rules in (14) to the corresponding interval type-2 fuzzy rules in (1). For type-2 fuzzy rule i, the initial consequent value of intervala˜jiis assigned based on the crisp consequent valueajiin type-1 fuzzy rule i. The centercjiof the intervala˜jiis set toajiand the initial interval width parametersjiis set to a small value of 0.00002. That is,(21)a˜ji=[cji−sji,cji+sji]=[aji−0.00002,aji+0.00002].Thesjiis set to a small value because a large value of it means a large deviation to the learned consequent parameters in a type-1 fuzzy rule, which will cause a large increment of learning error in the converted type-2 rules. The initial values of uncertain mean [mj1i,mj2i] and widthσjiin each interval type-2 FSA˜jiis assigned based on the centermjiand widthbjiof the type-1 FSAji. Direct assignment of these initial parameters is as follows:(22)[mj1i,mj2i]=[mji−0.1,mji+0.1],σji=bji.Eq. (22) sets an initial uncertain mean range of 0.2 for the scaled input range [−1, 1]. If the uncertain mean range is too small, then the initial type-2 fuzzy set becomes too close to a type-1 fuzzy set. In contrast, if this range is too large, then the uncertain mean covers most of the input range. Initial interval type-2 FSs assigned using this approach may be highly overlapped. To reduce the number of interval type-2 FSs in each input variable, highly similar type-1 FSs are merged into a single interval type-2 FS. The similarity measure degree between two type-1 Gaussian FSs,AjiandAjm, was approximately computed as follows [35]:(23)E(Aji,Ajm)=|Aji∩Ajm||Aji∪Ajm|=|Aji∩Ajm|bjiπ+bjmπ−|Aji∩Ajm|.The T2NFS-T1 uses this similarity measure to categorize type-1 FSs into different groups. If the similarity valueE(Aji,Ajm)is larger than a predefined threshold Sth, thenAjiandAjmare highly overlapped and are categorized into the same group. Two fuzzy sets are generally regarded as highly overlapped when the similarity measure value is at least greater than 0.5. This paper sets Sthto 0.5 so that more type-1 FSs can be merged to generate an T2NFS-T1 whose structure is as parsimonious as possible. Suppose that the γ type-1 FSsAj1, …,Ajγare categorized into the same group i*; then, these type-1 FSs are merged into a single interval type-2 FSA˜ji*. The initial values of the uncertain mean [mj1i*,mj2i*] and widthσji*inA˜ji*are assigned as follows:(24)mj1i*=min(mj1,...,mjγ),mj2i*=max(mj1,...,mjγ)and(25)σji*=1γ∑i=1γbji.If there is only one type-1 FS in a group, then the type-1 FS is extended to an interval type-2 FS using the direct assignment approach in (22). The following details the type-1 to type-2 FS conversion process.(1)Suppose that there are M type-1 FSs with indicesAj1, …,AjM, as shown in Fig. 1. The first FSAj1is used as the reference FS (denoted byAjR1) of group one G1. For the other FSsAji, 2≤i≤M, ifE(Aji,AjR1)≥Sth, thenAjibelongs to first group G1. The first FS, denoted as R2, that does not satisfy the similarity criterion is assigned as the reference FSAjR2of the second group G2.The FSs that have been grouped are removed for the next round of the grouping process. For the left FSsAji(2≤i≤M,Aji∉G1) in round two, ifE(Aji,AjR2)≥Sth, thenAjibelongs to the second group G2. Again, the first FS that does not satisfy the similarity criterion is assigned as the reference FS of group three.The process is repeated until all FSs are all grouped. For the groups containing more than one type-1 FS, the initial parameters in the merged interval type-2 FS are assigned using (24) and (25). For the groups containing only one type-1 FS, the initial parameters in the extended interval type-2 FS are assigned using (22).As an illustration, the type-1 FSs in Fig. 1 are categorized into four groups {G1=Aj1},G2={Aj2,Aj4,Aj5},G3={Aj3,Aj6}, andG4={Aj7}. Fig. 2shows distributions of the corresponding initial interval type-2 FSs generated from these type-1 FSs.After the type-1 to type-2 conversion process, the next step is to tune all of the parameters in the built type-2 rules. The objective of parameter learning is to minimize the error function in (20). The output y in (13) can be re-expressed as(26)y=12(flTwl+frTwr)=fˆTSKTwTSK,wTSK=[c01…cn1s01…sn1……c0M…cnMs0M…snM]T∈ℜ2M(n+1)×1where the rule-ordered consequent parameters,wTSK, are tuned using the rule-ordered RLS algorithm [9] described as follows:(27)wTSK(t+1)=wTSK(t)+S(t+1)fˆTSKT(t+1)(yd(t+1)−fˆTSKT(t+1)wTSK(t))S(t+1)=1λS(t)−S(t)fˆTSKT(t+1)fˆTSKT(t+1)S(t)λ+fˆTSKT(t+1)S(t)fˆTSKTwhere the value of the forgetting factor λ is that same as that in learning the type-1 rules. The antecedent parameters in the T2NFS-T1 are tuned by the gradient descent algorithm. That is,(28)θji(t+1)=θji(t)−η∂E∂θjiwhereθjidenotes one of the three antecedent parameters,mj1i,mj2i, orσji. The derived results are(29)∂E∂mj1i=12×(y−yd)×δ1×f¯i×(xj−mj1i)(σji)2,xj<_mj1i0,mj1i<_xj<_mj1i+mj2i212×(y−yd)×δ2×f_i×(xj−mj1i)(σji)2,xj>mj1i+mj2i2,(30)∂E∂mj2i=12×(y−yd)×δ1×f¯i×(xj−mj2i)(σji)2,xj>mj2i0,mj1i+mj2i2<_xj<mj2i12×(y−yd)×δ2×fi_×(xj−mj2i)(σji)2,xj<_mj1i+mj2i2,and(31)∂E∂σji=12×(y−yd)×[δ1×f¯i×(xj−mj1i)2(σji)3+δ2×f_i×(xj−mj2i)2(σji)3],xj<_mj1i12×(y−yd)×δ2×f_i×(xj−mj2i)2(σji)3,mj1i<xj<_mj1i+mj2i212×(y−yd)×δ2×f_i×(xj−mj1i)2(σji)3,mj1i+mj2i2<xj<_mj2i12×(y−yd)×[δ1×f¯i×(xj−mj2i)2(σji)3+(δ2)×f_i×(xj−mj1i)2(σji)3],mj2i<xj.where(32)δ1=wli,wli≤0,wri<0wli+wri,wli≤0,wri≥00,wli>0,wri≤0wri,wli>0,wri≥0,δ2=wri,wli≤0,wri<0wli+wri,wli≤0,wri≥00,wli>0,wri≤0wli,wli>0,wri≥0.Denote the total number of training iterations for building the type-1 and type-2 rules in the T2NFS-T1 as Max_iter. Then the numbers of iterations in training type-1 and type-2 FLSs are assigned to be 0.4×Max_iter and 0.6×Max_iter, respectively. The number of training iterations in the type-1 FLS training is smaller than that in the type-2 FLS because there are more tuned parameters in the latter than in the former. In addition, the type-1 FLS is built as an initial rule-base for the type-2 FLS and the major concern is the learning convergence of the type-2 FLS for obtaining an accurate T2NFS-T1. However, the number of iterations in building the type-1 FLS should be large enough to generate a nearly convergent and accurate rule-base for fuzzy set merging and parameter initialization of the type-2 FLS. Therefore, the ratio of the number of training iterations in the learning of type-1 and type-2 FLS is 2:3.The T2NFS-T1 is built through a type-1 NFS without the generation of new type-2 fuzzy rules. That is, the structure of the T2NFS-T1 is fixed after the type-1 to type-2 conversion process, which is one limitation of the algorithm. Thus, the T2NFS-T1 may be unsuitable for learning in non-stationary and evolving environments in which the ability of online generation and/or pruning of type-2 rules would show better adaptation performance for time-varying data.This section introduces the hardware implementation of the software-designed T2NFS-T1. Fig. 3shows the hardware implementation architecture of the H-T2NFS-T1. The architecture is specified to have n inputs, one output, and M rules. The H-T2NFS-T1 performs interval operations on membership values and firing strengths. The H-T2NFS-T1 computes the interval left and right end points (yland yr) using two parallel circuit modules. In Fig. 3, the circuit functions in the 2nd row modules for calculating the left end point ylare the same as the 3rd row modules for calculating the right end point yr.The H-T2NFS-T1 consists of six modules: (1) consequent module, (2) input fuzzifier module, (3) meet module, (4) extended output module, and (5) output module. The entire H-T2NFS-T1 is divided into four pipelines, T1, ..., T4, where the duration of each Tiis one clock. The chip sends an output y for every clock due to the pipeline function. The following are detailed descriptions of each module.(1)Consequent module: This module calculates the consequent values in (3) and (4) and consists of three sub-modules:(1.1) Sub-module one (multiplication sub-module): Fig. 4shows the sub-module for parallel computation of the productscjixjandsjixjin (4). There are M parallel multiplication sub-modules for computing the consequent values of M rules.(1.2) Sub-module two (2's complement sub-module): The main function of this module is to find the 2's complement of each of the product terms received from sub-module one.(1.3) Sub-module three (add sub-module): Eq. (4) can be re-written as follows:Fig. 5shows this sub-module, where adders “Add 1” and “Add 2” implement Aiand Bi, respectively. The value Biis transformed into 2's complement for the subtraction operation in (33). Eventually, adders “Add 3” and “Add 4” implement Ai+B1 and Ai−Bi, respectively, and send outputswliandwri.(2)Input fuzzifier module: This module stores the membership values of the m type-2 fuzzy sets in an input variable using LUTs. Each module receives input xiand implements (6) using LUTs. There are a total of 2m LUTs for storing the 2m membership valuesμ¯1,μ_1,...,μ¯m, andμ_m. The membership values are all in the range [0, 1] and are all represented by 8 bits, where the least significant digit is 2−8=0.00390625.Meet module:Fig. 6shows both the upper and lower meet modules used to calculate the upper firing strengthf¯iand lower firing strengthf_iin (8), respectively. There are M multiplier (MUL) circuits in a meet module. The ith MUL circuit in the upper meet module receives inputsμ¯1i, …,μ¯niand sends outputf¯i. In parallel, the ith MUL circuit in the lower meet module receives inputsμ_1i, …,μ_niand sends outputf_i.Extended output module:Fig. 7shows the upper extended output module for executing the termsfliwli, i=1, ..., M in (11). The left consequent valuewlireceived from the consequent module is sent to the multiplier to decide which term,f¯iorf_i, should be selected to multiplywli. In parallel, the lower type-reduction module implements (12) using the right consequent valuewrifrom the consequent module. Each output in type-reduction module is represented by 16 bits, where the least significant digit is 2−9=0.001953125 and the most significant digit is a sign bit.Output module: This module first adds the product terms from the extended output module to obtain yl+yr. A right shift circuit then implements (yl+yr)/2.The H-T2NFS-T1 chip implements an interval type-2 FLS with fixed parameter values. For the learning algorithms in Section 4, it is costly to implement all of them in a chip. One solution to address this problem is hardware-software co-design. For example, in a system-on-a-programmable chip that contains an embedded-processor core and a FPGA [36], the former provides flexibility and can be used to implement the learning algorithms and the latter can be used to implement the H-T2NFS-T1 chip to provide high-speed inference.This paper uses four examples to verify performance of the software-implemented T2NFS-T1. All simulations were conducted on an AMD Athlon 64X2 Dual-Core-Processor 2.59GHz central processing unit (CPU) running the Windows XP operating system with programs compiled by Microsoft Visual Studio.Net. To see the influence of the reduced FS number, the T2NFS-T1 without using the FS merging process in the type-1 to type-2 FLS conversion (i.e., the number of type-2 FSs is simply equal to the number of rules) was also applied to the same problems in the examples. This modified model considers only learning accuracy and is called T2NFS-T1(A). In addition, a type-1 version of the T2NFS-T1(A) (i.e., learning of a type-1 FLS with Max_iter iterations) is also used for comparison. This type-1 NFS is called T1NFS.Example 1System modelingIn this example, the T2NFS-T1 is used to model a nonlinear system. The plant to be modeled is guided by the difference equation [1,9]The training patterns are generated with u(t)=sin(2πt/100), t=1, …, 200. The inputs of the T2NFS-T1 are yd(t) and u(t), and the desired output is yd(t+1). Performance is evaluated using root-mean-square-error (RMSE). As in [9], the number of training iterations Max_iter was set to 500. Three and six rules were generated when ϕthwas set to 0.002 and 0.12, respectively. When there were only three rules, no type-1 fuzzy sets are merged. As a result, both T2NFS-T1 and T2NFS-T1(A) have the same performance. When there were six rules, these were six type-1 FSs in each input variable of the built type-1 FLS, as shown in Fig. 8(a). Fig. 8(b) shows the initially generated type-2 FSs from these type-1 FSs. The T2NFS-T1 had six and five type-2 FSs in inputs u(k) and y(k), respectively, after the type-1 to type-2 FS conversion process. That is, the two central type-1 FSs in input y(k) were highly overlapped and were therefore merged to a single interval type-2 FS. Fig. 8(c) shows the final type-2 FSs after learning, which shows slight tuning of the initial type-2 FSs in Fig. 8(b). Fig. 9shows the learning curve of the T2NFS-T1. A type-1 NFS was built after 200 (=0.4×Max_iter) training iterations. At iteration 201, the conversion from type-1 to type-2 FLS started; therefore; there was a small increase in the training error. After this conversion, parameter learning of the T2NFS-T1 was performed. Fig. 9 shows that the training error tends to decrease and converge with the increase of the iteration number. Table 1shows the number of rules and fuzzy sets, and RMSE of the T2NFS-T1(A) and T2NFS-T1. In contrast with the T2NFS-T1(A), the T2NFS-T1 reduces the FS number with a small increase of 0.00088 in RMSE.For the purpose of comparison, different type-1 and type-2 NFSs were applied to the same problem. The type-1 NFSs were a fuzzy adaptive learning control network (FALCON) [37] and T1NFS. The type-2 NFSs were an interval type 2 fuzzy logic system with the gradient descent algorithm (T2FLS-G) [7] and a SEIT2FNN [9] using the same number of training iterations as the T2NFS-T1. Table 1 shows the performances of these type-1 and type-2 NFSs. The T1NFS achieves not only a smaller error but also a much smaller number of training iterations than the FAQLCON, which verifies the advantage of using the T1NFS for type-1 to type-2 FLS conversion in the T2NFS. The RMSE of the T2NFS-T1 is comparable with the SEIT2FNN and is smaller than the other NFSs used for comparison.Table 2shows the CPU training times of the T2NFS-T1, SEIT2FNN and T2FLS-G. The T2NFS-T1 uses the simple weighted sum operation in (11) and (12) to find the type-reduction outputs while the SEIT2FNN and T2FLS-G use the complex Karnik–Mendel iterative procedure to find the outputs. Because of this simplification, Table 2 shows that the T2NFS-T1 achieves the advantage of using less training time than the SEIT2FNN and T2FLS-G. In contrast to the T2FLS-G, the SEIT2FNN uses a more complex parameter learning algorithm, the hybrid of rule-ordered RLS algorithm and gradient descent algorithm. For this reason, the SEIT2FNN shows longer training time than the T2FLS-G.Example 2Prediction of a chaotic time seriesThe time series prediction problem used in this example is the chaotic Mackey-Glass chaotic time series [2,4,9,38,39]. Four past values [x(t−24), x(t−18), x(t−12), x(t−6);x(t)] are used to predict x(t). As in [2,4,9,38,39], 1000 patterns were generated from t=124 to t=1123, with the first 500 patterns being used for training and the last 500 for testing. The number of training iterations Max_iter was set to 1000. Five rules were generated in the T2NFS-T1 when ϕthwas set to 0.2.There were five type-1 FSs in each input variable in the built type-1. There were 4, 3, 3, and 3 interval type-2 FSs in the inputs x(t−24), x(t−18), x(t−12), and x(t−6), respectively, after the type-1 to type-2 FS conversion process. Fig. 10(a) shows distributions of the type-1 FSs in the built type-1 FLS, where there were five type-1 FSs in each input variable. Fig. 10(b) and (c) shows distributions of the initial and final type-2 FSs in the built type-2 FLS, respectively, where there were 4, 3, 3, and 3 interval type-2 FSs in the inputs x(t−24), x(t−18), x(t−12), and x(t−6), respectively. The highly overlapped type-1 FSs in each input variable in Fig. 10(a) were merged to interval type-2 FSs, as shown in Fig. 10(b); therefore, the final number of FSs in each input variable was smaller than five in the T2NFS-T1. Table 3shows the performances of the T2NFS-T1(A) and T2NFS-T1. The result shows that the total number of interval type-2 FSs is reduced from 20 in the T2NFS-T1(A) to 13 in the T2NFS-T1 with a small increase of 0.0014 in RMSE. Fig. 11shows the training curve of the T2NFS-T1. A type-1 NFS was built after 400 (=0.4×Max_iter) training iterations. At iteration 401, there was a small increase in the training error due to the type-1 to type-2 conversion. Fig. 11 shows that the training error tends to decrease and converge with the increase of the iteration number.The performance of the T2NFS-T1 is compared with those of various type-1 [2,4,38,39] and type-2 NFSs [7,9]. The type-2 NFSs include the SEIT2FNN and T2FLS-G that use the same number of training iterations as the T2NFS-T1. Table 3 shows the performances of these NFSs used for comparison. The T1NFS achieves a smaller test error than the other type-1 NFSs used for comparison. The result shows the advantage of using the T1NFS for type-1 to type-2 FLS conservation in the T2NFS-T1. Table 3 shows that the SEIT2FNN achieves the smallest test error. However, as in Example 1, Table 2 shows that the training time of the T2NFS-T1 is much smaller than those of the SEIT2FNN and T2FLS-G. Other than the SEIT2FNN, the T2NFS-T1(A) and T2NFS-T1 show smaller test errors than the type-1 and type-2 NFSs used for comparison.Example 3Stock price predictionThe problem addressed in this example is the prediction of the price of a stock based on short-, mid-, and long-term financial indicators. In accordance with [8], 50 training and 50 test stock data sets were used, each with ten inputs and one output. As in Example 2, the number of training iterations Max_iter was set to 1000. Five rules were generated in the T2NFS-T1 when ϕthwas set to 0.015. Table 4shows the performances of T2NFS-T1 and T2NFS-T1(A). In contrast with the T2NFS-T1(A), the T2NFS-T1 significantly reduces the FS number from 50 to 29, with an increase of 2.852 in the test RMSE. Fig. 12shows the training curve of the T2NFS-T1. At iteration 401, there was an increase in the training error due to the type-1 to type-2 FLS conversion operation. Fig. 12 shows that the training error tends to decrease with the iteration number though are small oscillations in the learning process.For the purpose of comparison, Table 4 shows the performances of various type-1 NFSs, including adaptive neural fuzzy inference system (ANFIS) with grid-type partition (ANFIS-G) and subtractive clustering (ANFIS-S) [8], and T1NFS. The result shows that the T1NFS used for type-1 to type-2 conversion achieves a smaller test error than the type-1 NFSs used for comparison. Table 4 also shows the performances of various type-2 NFSs, including T2FLS-G and SEIT2FNN, and the reported result of a discrete interval type-2 fuzzy system with a TSK-type rule base (DIT2-TSFR) [8]. The best result for 2–20 rules was reported with the DIT2-TSFR. Table 4 shows that the T2NFS-T1(A) achieves a smaller test RMSE than the other type-1 and type-2 NFSs. The T2NFS-T1 shows a smaller test error than the type-2 NFSs used for comparison with the sole exception of the T2NFS-T1(A). As in Examples 1 and 2, Table 2 shows that training time of the T2NFS-T1 is much smaller than those of the SEIT2FNN and T2FLS-G.Example 4Real time series predictionThe series-A from the Sante Fe real time series [40] was used. It is a clean time series measured from a physical system in a laboratory experiment. In accordance with [41], 1000 samples were used, each with five inputs and one output. As in [41], 90% of the total data were used for training, and the remaining 10% were used for testing. As in Examples 2 and 3, the number of training iterations Max_iter was set to 1000. Five rules were generated in the T2NFS-T1 when ϕthwas set to 0.3. Fig. 13shows the test result of T2NFS-T1, where the predicted outputs using the T2NFS-T1 are very close the actual outputs of the actual physical system. Table 5shows the performances of the T2NFS-T1(A) and T2NFS-T1. There were a total of 25 and 12 interval type-2 FSs in the T2NFS-T1(A) and T2NFS-T1, respectively. The T2NFS-T1 significantly reduced the FS number with only a 0.052 increase in the test RMSE.For the purpose of comparison, Table 5 shows the reported results of various type-1 and type-2 FLSs from [42], including SONFIN [1], a support vector regression (SVR)-based fuzzy model (SVR-FM) [43], T2FLS-G, and SEIT2FNN. Table 5 also shows the reported results of different models from [41], including a neural network (NN) and a Pattern Modeling and Recognition System (PMRS). The results indicate that both the T2NFS-T1(A) and T2NFS-T1 achieve smaller test RMSEs than all of the models used for comparison. As in Examples 1–3, Table 2 shows that training time of the T2NFS-T1 is much smaller than those of the T2FLS-G and SEIT2FNN.The FPGA device uses a Xilinx Virtex-4 XC4vlx60-10ff1148 chip. The electrical design automation (EDA) software tools used in this paper are ISE 8.1i and ModelSim SE6.5. FPGA implementation of the software-designed T2NFS-T1 in Examples 1 and 2 was conducted in the following two examples to verify the performance of the H-T2NFS-T1 chip.Example 5H-T2NFS-T1 chip for modelingHardware implementation of the software-designed T2NFS-T1 with six rules in Example 1 was conducted in this example. The H-T2NFS-T1 chip contains two inputs for the system identification problem. The inputs and output of the H-T2NFS-T1 chip were represented by 8 and 16 bits, respectively. The H-T2NFS-T1 used 22 LUTs to implement the 11 interval type-2 fuzzy sets. Reduction of the number of interval type-2 fuzzy sets in the T2NFS-T1 helps reduce the number of LUTs in the H-T2NFS-T1 chip. The maximum execution speed is 85.668MHz clock frequency, and the total number of gate counts is 109,991. The hardware architecture used four pipeline stages. It took four clocks to get the output for the first input sample. Then the chip sent an output every clock for subsequent input samples due to the pipeline function. The RMSE between the actual output and the H-T2NFS-T1 outputs is 0.0163. Fig. 14shows the outputs of the software-implemented T2NFS-T1 and the H-T2NFS-T1 chip, both of which are very close except at the peak points. This difference is mainly due to the resolution lost in the LUT in the input fuzzifier module and degradation of the floating point representation accuracy in the circuits.The H-T2NFS-T1 uses a simplified operation for computing the extended output [yl, yr]. For implementation cost analysis, Table 6shows the number of additions, multiplications, and divisions in the H-T2NFS-T1 used to compute the outputs yland yrfor given inputswli,wri,f¯i, andf_i. For the purpose of comparison, Table 6 also shows the implementation costs of different interval type-2 fuzzy chips [16,22,24–26]. The chips in [24–26] use the Karnik–Mendel iterative procedure to find the extended output, and the cost shown in Table 6 is that when the maximum number of M iterations is performed. In these chips, additional circuits are required to implement comparisons and conditions evaluation in the iterative procedure; however, these studies do not show the implementation details. The chip in [22] uses the Wu–Mendel closed form approach to find the extended output. The chip in [16] uses weighted bound-set boundaries to find the extended output. Table 6 shows the number of additions, multiplications, and divisions used in the H-T2NFS-T1 chip are all smaller than the chips used for comparison. Particularly, the H-T2NFS-T1 chip is the only one that does not need any divisions among the chips. Table 7shows that the actual number of additions (multiplications) used in the H-T2NFS-T1 chip is smaller than a half of the chip in [16] and one-fifth of the chips in [24–26] when the rule number is M=6.Example 6H-T2NFS-T1 chip for predictionHardware implementation of the software-designed T2NFS-T1 in Example 2 was conducted in this example. The H-T2NFS-T1 chip consists of five interval type-2 fuzzy rules and uses 26 LUTs to implement the 13 interval type-2 fuzzy sets. The H-T2NFS-T1 chip contains four inputs and one output. As in Example 5, the inputs and output of the H-T2NFS-T1 chip were represented by 8 and 16 bits, respectively. The maximum execution speed is 75.038MHz clock frequency, and the total number of gate counts is 139,643. Fig. 15shows the outputs of the software-implemented T2NFS-T1 and the H-T2NFS-T1 chip, both of which are very close except at the peak points, as in Example 5. As in Example 5, Table 7 shows the number of additions, multiplications, and divisions used in the H-T2NFS-T1 chip are all much smaller than the chips used for comparisons when the rule number is M=5.

@&#CONCLUSIONS@&#
