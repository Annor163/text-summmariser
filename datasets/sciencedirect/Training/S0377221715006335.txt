@&#MAIN-TITLE@&#
Rough multiple objective programming

@&#HIGHLIGHTS@&#
We introduced a new MOP problem containing some roughly defined parts.We defined the model when the roughness exists only in the decision set.We characterized the rough complete, efficient and weak efficient solutions.We discussed the weighted sum problem in such new problem.We proposed an approach for solving the 1st-class RMOP problems, and presented a flowchart to clarify this new approach.

@&#KEYPHRASES@&#
Multiple objective programming,Rough sets,Rough programming,Rough efficient solution,

@&#ABSTRACT@&#
In this paper, we focused on characterizing and solving the multiple objective programming problems which have some imprecision of a vague nature in their formulation. The Rough Set Theory is only used in modeling the vague data in such problems, and our contribution in data mining process is confined only in the “post-processing stage”. These new problems are called rough multiple objective programming (RMOP) problems and classified into three classes according to the place of the roughness in the problem. Also, new concepts and theorems are introduced on the lines of their crisp counterparts; e.g. rough complete solution, rough efficient set, rough weak efficient set, rough Pareto front, weighted sum problem, etc. To avoid the prolongation of this paper, only the 1st-class, where the decision set is a rough set and all the objectives are crisp functions, is investigated and discussed in details. Furthermore, a flowchart for solving the 1st-class RMOP problems is presented.

@&#INTRODUCTION@&#
Decision making is a very important and much studied application of mathematical methods in various fields of human activity. In real-world situations, decisions are nearly always made on the basis of information which, at least in part, is vague in nature. In some cases (e.g. zooming out, granular computing and system complexity reduction), vague information is used as an approximation to more precise information. In such situations, this form of approximation is convenient and sufficient for making good enough decisions. In other cases (e.g. image processing and pattern recognition) and due to the limited precision in data acquisition phase, vague information is the only form of information available to the decision maker.Since it was pioneered by Pawlak, rough set theory (RST) (Pawlak, 1982, 1996) has become a hot topic of great interest in several fields. The capability of handling vagueness and imprecision in real-life problems has attracted researchers to use RST in many fields; one of them is the 'optimization'. Actually, most real-life problems involve (1) a process of optimizing simultaneously a collection of conflicting and competing objectives (i.e. a process of multiple objective programming (MOP)) and (2) vague or imprecise descriptions of some parts of the problem. Therefore, we usually need a suitable framework for handling this hyperdization of MOP and vagueness. For conventional MOP problem (Ehrgott, 2005; Hwang & Masud, 1979), the aim is to maximize or minimize a set of objectives over a certain decision set, both of which are precisely defined. But in many realistic situations, the available data lacke vagueness and inexactness and the decision maker may only be able to specify the objectives and/or the decision set imprecisely in a ‘rough sense’ using RST.Youness (2006) was the first who applied RST to the single-objective programming (SOP) problem and proposed a new optimization problem with rough decision set and crisp objective function, called “rough single-objective programming” (RSOP) problem. He also defined two concepts for optimal solutions, namely “surely optimal“ and “possibly optimal”. Then after, many attempts were made to overcome the concept of rough mathematical programming. For more details see (Xu & Yao, 2009a, 2009b; Osman et al., 2011; Lu, Huang, & He, 2011; Tao & Xu, 2012; Zhang, Shi, & Gao, 2009).Hence, for the sake of acquiring more realistic models and results of real-life MOP problems, we present a new extension of RSOP models presented in Osman et al. (2011), to the case of rough multiple objective programming (RMOP). A new framework in modeling and solving the RMOP problem is proposed without requiring any additional data.RST was proposed by Pawlak in the mid-1980s, and presents a new mathematical approach to imperfect (vague/imprecise) knowledge. The problem of imperfect knowledge has been tackled for a long time by philosophers, logicians and mathematicians. Recently, RST has been proven to be an excellent mathematical tool dealing with vague and imprecise descriptions of objects. It became a crucial issue for artificial intelligence and cognitive sciences, especially in the areas of machine learning, knowledge acquisition, decision analysis, knowledge discovery from databases, expert systems, inductive reasoning and pattern recognition.RST expresses ‘imprecision’ by employing a boundary region of the vague object (e.g. set, number, interval, function, etc.). If the boundary region of an object is empty it means that the object is crisp (exact); otherwise the object is rough (inexact). A nonempty boundary region of an object means that our knowledge about the object is not sufficient to define it precisely. The bigger the boundary the worse (i.e. the higher the imprecision of) the knowledge we have about the object.Let U be a non-empty finite set of objects, called the universal set, and E⊆U × U be an equivalence relation on U. The ordered pair A = (U, E) is called an approximation space generated by E on U. E generates a partitionU/E={Y1,Y2,….,Ym}whereY1,Y2,….,Ymare the equivalence classes of the approximation space A.In RST, any subset M⊆U is described by its lower and upper approximations in terms of the equivalence classes of A, as follows:E*(M)=∪{Yi∈U/E|Yi⊆M}E*(M)=∪{Yi∈U/E|Yi∩M≠ϕ}The sets E*(M) and E*(M) (or simply M* and M*) are called the lower and the upper approximations of M respectively, in the approximation space A. Therefore, M*⊆M⊆M*. The difference between the upper and the lower approximations is called the boundary of M and is denoted byBNE(M)=M*−M*(or simply MBN). The set M is crisp (exact) in A iffMBN=ϕ, otherwise M is rough (inexact) in A.In RST, each element x ∈ U is classified as ‘surely’ inside M iff x ∈ M* or ‘may be’ (I'm not sure if it is or not) inside M iff x ∈ MBN; otherwise x is surely outside M. Furthermore, an element x ∈ U is said to be “probably inside M”, iff x ∈ M*. On the other hand, each equivalence class Y ∈ U/E is classified as ‘completely’ included in M iff Y⊆M* or ‘partially’ included in M iff Y⊆MBN, otherwise Y is completely not included in M. Furthermore, an equivalence class Y ∈ U/E is said to be "possibly included in M", iff Y⊆M*.Consider the following crisp SOP problem(1)maxx∈Mg(x)where g(x) is the objective function, and M is the feasible set of the problem. In the conventional mathematical programming problem, it is assumed that all the parts (i.e. g(x) and M) are defined in a crisp sense and “max” is a strict imperative. However, in many practical situations it may not be reasonable to require that the feasible set or the objective function be specified in a precise crisp terms. In such situations, it is desirable to use some type of modeling that is capable of handling vagueness and imprecision in the problem. This led to the hybridization between SOP and RST to get the concept of “rough single-objective programming”. RSOP problems are broadly classified according to the place of roughness into three classes as follows:1st-Class: problems with rough feasible set and crisp objective function.2nd-Class: problems with crisp feasible set and rough objective function.3rd-Class: problems with rough feasible set and rough objective function.Unlike the crisp case (where the optimal value is a single crisp value), the optimal value in RSOP, denoted byg¯, is defined by its lower and upper bounds i.e.g¯*andg¯*respectively, such thatg¯*≤g¯≤g¯*. Therefore, in RSOP we can say that:•a solution x is surely-optimal, ifg(x)=g¯*,a solution x is probably-optimal, ifg(x)≥g¯*,a solution x is surely-not optimal, ifg(x)<g¯*.Also, in the 1st and 3rd classes of RSOP (where the feasible set is a rough set), it is remarkable that:•a solution x is surely-feasible, iff it belongs to the lower approximation of the feasible set,a solution x is probably-feasible, iff it belongs to the upper approximation of the feasible set,a solution x is surely-not feasible iff it does not belong to the upper approximation of the feasible set.Furthermore, in RSOP the optimal set is replaced by four optimal sets (See Fig. 1) covering all the possible degrees of feasibility and optimality of the solutions, as follows:•The set of all surely-feasible, surely-optimal solutions, denoted by FOss.The set of all surely-feasible, probably-optimal solutions, denoted by FOsp.The set of all probably-feasible, surely-optimal solutions, denoted by FOps.The set of all probably-feasible, probably-optimal solutions, denoted by FOpp.Therefore, we have:FOss⊆FOsp⊆FOpp,FOss⊆FOps⊆FOppandFOss=FOsp∩FOps.Suppose thatA=(U,E)is an approximation space generated by an equivalence relation E on the universe U, andU/E={Y1,Y2,….,Ym}is the partition generated by E on U. A RSOP problem of the 1st-class takes the following form:(2)maxx∈Mg(x)s.t.M*⊂M⊂M*M*,M*⊆U/Ewhere g: U → R is a crisp objective function. M⊂U is a rough set in the approximation space A, representing the feasible set of the problem. M is given only by its lower and upper approximations, M* and M* respectively, and the nonempty boundary region (MBN=M*−M*≠ϕ) of the feasible set indicates the notion of ‘rough-feasibility’ in problem (2). The lower and upper bounds of the optimal objective valueg¯in problem (2), are given byg¯*=max{a,b},g¯*=max{a,c}where (assuming the existence of the solution of the following crisp problems)a=maxx∈M*g(x),b=maxY∈U/E,Y⊆MBNminx∈Yg(x),c=maxx∈MBNg(x)Therefore, the optimal sets of problem (2) are given as follows:FOss={x∈M*|g(x)=g¯*}FOsp={x∈M*|g(x)≥g¯*}FOps={x∈M*|g(x)=g¯*}FOpp={x∈M*|g(x)≥g¯*}In this section, we present a new proposal to extend the concept of “rough programming” to the case of multiple objective programming. This proposal is called rough multiple objective programming (RMOP) and represents the hybridization of MOP and RST.Generally, the crisp MOP problem is defined as follows:(3)maxF(x)=(f1(x),…,fm(x))Ts.t.x∈M⊆Um≥2where U is a non-empty finite set of objects, called the universal set, M is the decision set and x is the decision variable. F: U → Rmis the objective vector and composed by m scalar objective functionsfi:U→R,i=1,…,m. The sets U and Rmare known as ‘decision variable space’ and ‘objective function space’, respectively.In contrast to SOP, a solution to a MOP problem is more of a concept than a definition. Typically, there is no single global optimal solution in MOP, and it is often necessary to determine a set of points that all fit a predetermined definition for an optimum. One property commonly considered as necessary for any candidate solution to the multiple objective problem is that the solution is not dominated by any other solutions in the feasible set. “The non-domination” (or best compromise, efficiency, Pareto-optimality) is the central concept in MOP only because an optimal solution for one objective function is not necessary an optimal solution for others. The predominant concept in defining an optimal point is that of Pareto optimality (Ehrgott, 2005; Hwang & Masud, 1979).Remark 1For any two vectors   F1,  F2 ∈ Rm, if at least one component of F1 is ‘greater than’ its corresponding one of F2 then we writeF1¬≤F2. Also, if at least one component of F1 is ‘greater than or equal’ its corresponding one of F2 then we writeF1¬<F2.Remark 2In the MOP problem (3), a solutionx^is said to be ‘not dominated’ by any member of set M if and only if ∀x ∈ M eitherF(x^)=F(x)orF(x^)¬≤F(x).In the conventional crisp scenario, it is assumed that all of the objectivesfi(x),i=1,2,…,mand the decision set M of problem (3) are defined precisely. However, practically this does not always be the case of study. Sometimes, in multiple objective programming, one or more of the problem components may be only defined through vague descriptions, and using rough set theory this could be handled easily. The roughness may appear in the MOP problem in the decision set and/or the goals, and then the problem is called rough multiple objective programming (RMOP) problem. According to the place of the roughness, the RMOP problems can be broadly classified into three classes as follows:1st-Class: problems with a rough feasible set and crisp objective functions.2nd-Class: problems with a crisp feasible set and at least one rough objective function.3rd-Class: problems with a rough feasible set and at least one rough objective function.Definition 1In problem (3), the objective vectorF¯, which is composed by the maximum values of the objectives over the feasible set M, is called ideal objective vector i.e.F¯=(f¯1,…,f¯m),f¯i=maxx∈Mfi(x)∀i=1,…,m.In RMOP, due to the roughness wherever it exists in the problem, the ideal objective vectorF¯is defined by its lower and upper bounds i.e.F¯*=(f¯1*,…,f¯m*)andF¯*=(f¯1*,…,f¯m*)respectively, whereF¯*≤F¯≤F¯*andf¯i*≤f¯i≤f¯i*∀i=1,…,m.f¯i*andf¯i*are called the lower and upper bounds off¯i∀i=1,…,m, respectively.Furthermore, only in the 1st and 3rd classes of RMOP problems (where the feasible set is a rough set), we have solutions with different degrees (e.g. surely and probably) of feasibility. While in the 2nd class, all the feasible solutions are surely-feasible.In this section, we define and discuss in details the RMOP problem in which the decision set is a rough set and all of the objectives are crisp functions. The roughness of the decision set appears when the feasible solutions have different degrees of feasibility (e.g. surely and probably). This case is mostly happening in the approximation process at zooming out and granulation operations (e.g. in pattern recognition and image processing). Also, due to insufficient and limited precision in data mining and information acquisition phases, we face some vagueness in data classifications and this causes rough feasibility too.Suppose thatA=(U,E)is an approximation space generated by an equivalence relation E on a universal set U, andU/E={Y1,Y2,….,Ym}is the partition generated by E on U. Then the general 1st-class RMOP problem is expressed as follows:(4)maxx∈MF(x)=(f1(x),…,fm(x))Ts.t.M*⊂M⊂M*M*,M*⊆U/Em≥2where M⊂U is a rough set in the approximation space A, and given only by its lower and upper approximations, M* and M*, respectively. x is the decision variable, and F: U → Rmis the objective vector which is composed by m scalar crisp objective functionsfi:U→R,i=1,2,…,m.Characterizing the optimality conditions, efficient solutions, weak efficient solutions, Pareto front and optimal sets for problem (4), is the core of the following discussion. It is remarkable that the key idea in the following definitions is to use the notions of "the smallest and largest probable feasible sets" in order to characterize "the probably and surely states" of the different types of solutions (e.g. efficient and weak-efficient solutions). The smallest probable feasible set refers to any set that consists of all the elements of the lower approximation M* together with only one element from each equivalence class Y ∈ MBN (since Y∩M ≠ ϕ    ∀Y ∈ MBN). So that the smallest feasible set is not a unique set, while the largest probable feasible set is always the upper approximation  M*.Now for problem (4), we have the following definitions and propositions.Definition 2The ideal objective vectorF¯=(f¯1,…,f¯m)is defined by its lower and upper bounds i.e.F¯*=(f¯1*,…,f¯m*)andF¯*=(f¯1*,…,f¯m*)respectively, where:f¯i*=max{ai,bi}andf¯i*=max{ai,ci}(assuming the existence of the solution of the following crisp problems)ai=maxx∈M*fi(x),bi=maxY∈U/EY∈MBN{minx∈Yfi(x)}andci=maxx∈MBNfi(x)∀i=1,…,mDefinition 3An objective vectorF¯¯=(f¯¯1,…,f¯¯m)is said to be a surely-utopian objective vector if and only if its components are marginally greater than that of the upper bound of the ideal objective vector, i.e.f¯¯i=f¯i*+ɛiwithɛi>0∀i=1,…,m.Definition 4An objective vectorF¯¯=(f¯¯1,….,f¯¯m)is said to be a probably-utopian objective vector if and only if its components are marginally greater than that of the lower bound of the ideal objective vector, i.e.f¯¯i=f¯i*+ɛiwithɛi>0∀i=1,2,…,m.Definition 5A pointx^∈M*is said to be a surely-complete optimal solution, if and only ifF(x^)=F¯*.Definition 6A pointx^∈M*is said to be a probably-complete optimal solution, if and only ifF(x^)≥F¯*.In problem (4), a pointx^∈M*is said to be a surely-efficient (or surely-Pareto optimal) solution, if and only if it is not dominated by the points of the largest-probable feasible set, i.e. M*. Also, a pointx^∈M*is said to be a probably-efficient (probably-Pareto optimal) solution, if and only if it is not dominated by the points of at least one of the smallest-probable feasible sets.Definition 7A pointx^∈M*is said to be a surely-efficient solution, if and only if ∀x ∈ M* eitherF(x^)=F(x)orF(x^)¬≤F(x).Definition 8A pointx^∈M*is said to be a probably-efficient solution, if and only if:(1)∀x ∈ M* eitherF(x^)=F(x)orF(x^)¬≤F(x), and∀Y ∈ MBN there exists at least one point x ∈ Y such that eitherF(x^)=F(x)orF(x^)¬≤F(x).Definition 9The set of all surely-efficient solutions is called the surely-efficient set and denoted byPs, while the set of all probably-efficient solutions is called the probably-efficient set and denoted byPp.Similarly, the surely and probably-weak efficient solutions are defined, as follows.Definition 10A pointx^∈M*is said to be a surely-weak efficient solution, if and only ifF(x^)¬<F(x)∀x∈M*.Definition 11A pointx^∈M*is said to be a probably-weak efficient solution, if and only if:(1)F(x^)¬<F(x)∀x∈M*,and∀Y ∈ MBN there exists at least one point x ∈ Y such thatF(x^)¬<F(x).Definition 12The set of all surely-weak efficient solutions is called the surely-weak efficient set and denoted byWs, while the set of all probably-weak efficient solutions is called the probably-weak efficient set and denoted byWp.Proposition 1Ps⊆Pp⊆WpandPs⊆Ws⊆Wp. (See Fig. 2)According to the above notions of optimality and feasibility, we can easily get that:•The set of all surely-feasible, surely-complete optimal solutions of problem (4), is defined asFsOsc={x∈M*|F(x)=F¯*}.The set of all surely-feasible, probably-complete optimal solutions of problem (4), is defined asFsOpc={x∈M*|F(x)≥F¯*}.The set of all probably-feasible, surely-complete optimal solutions of problem (4), is defined asFpOsc={x∈M*|F(x)=F¯*}.The set of all probably-feasible, probably-complete optimal solutions of problem (4), is defined asFpOpc={x∈M*|F(x)≥F¯*}.The set of all surely-feasible, surely-efficient solutions, is defined asFsOse=M*∩Ps.The set of all surely-feasible, probably-efficient solutions, is defined asFsOpe=M*∩Pp.The set of all probably-feasible, surely-efficient solutions, is defined asFpOse=M*∩Ps.The set of all probably-feasible, probably-efficient solutions, is defined asFpOpe=M*∩Pp.The set of all surely-feasible, surely-weak efficient solutions, is defined asFsOsw=M*∩Ws.The set of all surely-feasible, probably-weak efficient solutions, is defined asFsOpw=M*∩Wp.The set of all probably-feasible, surely-weak efficient solutions, is defined asFpOsw=M*∩Ws.The set of all probably-feasible, probably-weak efficient solutions, is defined asFpOpw=M*∩Wp.Generally in MOP problem (3), the image of the efficient set in the objective vector space is called ‘Pareto front set’. Also, the objective vector which is composed by the minimum values of the objectives over the efficient set is called ‘nadir objective vector’ and denoted by Fnad. Therefore, in problem (4) we can define both of the ‘Pareto front set’ and ‘nadir objective vector’ as follows.Definition 13•The Pareto front for the surely-feasible, surely-efficient set FsOse, is defined asFRss={F(x)|x∈FsOse}.The Pareto front for the surely-feasible, probably-efficient set FsOpe, is defined asFRsp={F(x)|x∈FsOpe}.The Pareto front for the probably-feasible, surely-efficient set FpOse, is defined asFRps={F(x)|x∈FpOse}.The Pareto front for the probably-feasible, probably-efficient set FpOpe, is defined asFRpp={F(x)|x∈FpOpe}.Proposition 2FRss⊆FRsp⊆FRpp, FRss⊆FRps⊆FRpp andFRss=FRsp∩FRps.Definition 14•The nadir objective vector for the surely-feasible, surely-efficient set FsOse, is denoted by Fssnad and defined withFssnad=(fss,1nad,…,fss,mnad)where:fss,inad=minx∈FsOsefi(x),∀i=1,2,…,m.The nadir objective vector for the surely-feasible, probably-efficient set FsOpe, is denoted by Fspnad and defined withFspnad=(fsp,1nad,…,fsp,mnad)where:fsp,inad=minx∈FsOpefi(x),∀i=1,2,…,m.The nadir objective vector for the probably-feasible, surely-efficient set FpOse, is denoted by Fpsnad and defined withFpsnad=(fps,1nad,…,fps,mnad)where:fps,inad=minx∈FpOsefi(x),∀i=1,2,…,m.The nadir objective vector for the probably-feasible, probably-efficient set FpOpe, is denoted by Fppnad and defined withFppnad=(fpp,1nad,…,fpp,mnad)where:fpp,inad=minx∈FpOpefi(x),∀i=1,2,…,m.Proposition 3Fppnad≤Fpsnad≤FssnadandFppnad≤Fspnad≤Fssnad.Theorem 1Recall from problem (4) thatF=(f1(x),…….,fm(x)). If for each i = 1,2,…,m, the setsFOssi,FOspi,FOpsiandFOppirepresent the four optimal sets of the following RSOP problem(5)maxx∈Mfi(x)s.t.M*⊂M⊂M*then:•The set of all surely-feasible, surely-complete optimal solutions of problem (4), is defined asFsOsc={x∈M*|F(x)=F¯*}=⋂i=1mFOssi.The set of all surely-feasible, probably-complete optimal solutions of problem (4), is defined asFsOpc={x∈M*|F(x)≥F¯*}=⋂i=1mFOspi.The set of all probably-feasible, surely-complete optimal solutions of problem (4), is defined asFpOsc={x∈M*|F(x)=F¯*}=⋂i=1mFOpsi.The set of all probably-feasible, probably-complete optimal solutions of problem (4), is defined asFpOpc={x∈M*|F(x)≥F¯*}=⋂i=1mFOppi.ProofThe proof is straightforward.□Theorem 2If FOss,  FOsp,  FOpsandFOpp represent the four optimal sets of the following weighted-sum RSOP problem(6)maxx∈M∑i=1mwifi(x)s.t.M*⊂M⊂M*wi>0∀i=1,…,mx^∈FOssis a surely-feasible, surely-efficient solution of problem (4).x^∈FOspis a surely-feasible, probably-efficient solution of problem (4).x^∈FOpsis a probably-feasible, surely-efficient solution of problem (4).x^∈FOppis a probably-feasible, probably-efficient solution of problem (4).ProofThe proof of this theorem in the crisp MOP form is well known. In RSOP problem (6), the solution feasibility and optimality characteristics (e.g. surely and probably degrees) are obviously transferred to the solution feasibility and efficiency in problem (4). Therefore, a surely (or probably) feasible solution of problem (6) is a surely (or probably) feasible solution of problem (4). Also, a surely (or probably) optimal solution of problem (6) is a surely (or probably) efficient solution of problem (4).□Theorem 3If FOss,  FOsp,  FOpsandFOpp represent the four optimal sets of the following weighted-sum RSOP problem(7)maxx∈M∑i=1mwifi(x)s.t.M*⊂M⊂M*wi≥0∀i=1,…,mthen:•x^∈FOssis a surely-feasible, surely-weak efficient solution of problem (4).x^∈FOspis a surely-feasible, probably- weak efficient solution of problem (4).x^∈FOpsis a probably-feasible, surely-weak efficient solution of problem (4).x^∈FOppis a probably-feasible, probably- weak efficient solution of problem(4).ProofThe proof is straightforward like that of theorem 2.□The proposed approach and methodology for solving the 1st class of RMOP problems could be summarized in the following flowchart, (See Fig. 3)Numerical example:Let U be a universal set defined byU={x∈R2|x12+x22≤16}where x = (x1, x2), and let K be a polytope generated by the following closed halfplanesh1=x1+x2−2≤0,h1=x2−x1−2≤0,h1=x2−x1+2≥0,h1=x1+x2+2≥0,Suppose that E is an equivalence relation on U such thatU/E={E1,E2,E3}E1={x∈U|xisaninteriorpointofpolytopeK}E2={x∈U|xisaboundarypointofpolytopeK}E3={x∈U|xisanexteriorpointofpolytopeK}Consider the following 1st class RMOP problem:maxx∈MF(x)=(f1(x),f2(x))Ts.t.M*⊂M⊂M*M*=E1∪E2,M*=E1∪E2∪E3f1(x)=−(x1−3)2−x22f2(x)=−x12−x22Solution:Step 1: Finding the complete optimal solutions (referring to Theorem 1):a1=maxx∈M*f1(x)=−1,b1=minx∈E3f1(x)=−49andc1=maxx∈E3f1(x)=0f¯1*=max{a1,b1}=−1andf¯1*=max{a1,c1}=0FOss1={x∈M*|f1(x)=0}=ϕ,FOsp1={x∈M*|f1(x)≥−1}={(2,0)}FOps1={x∈M*|f1(x)=0}={(3,0)}FOpp1={x∈M*|f1(x)≥−1}={x∈R2|(x1−3)2+x22=1}a2=maxx∈M*f2(x)=0,b2=minx∈E3f2(x)=−16andc2=maxx∈E3f2(x)=(−2)−where(−2)−=−2−ɛ,ɛ>0,ɛ≈0f¯2*=max{a2,b2}=0andf¯2*=max{a2,c2}=0FOss2={x∈M*|f2(x)=0}={(0,0)}FOsp2={x∈M*|f2(x)≥0}={(0,0)}FOps2={x∈M*|f2(x)=0}={(0,0)}FOpp2={x∈M*|f2(x)≥0}={(0,0)}•The set of all surely-feasible, surely-complete optimal solutions, is given byFsOsc=FOss1∩FOss2=ϕ.The set of all surely-feasible, probably-complete optimal solutions, is given byFsOpc=FOsp1∩FOsp2=ϕ.The set of all probably-feasible, surely-complete optimal solutions, is given byFpOsc=FOps1∩FOps2=ϕ.The set of all probably-feasible, probably-complete optimal solutions, is given byFpOpc=FOpp1∩FOpp2=ϕ.Step 2: finding the efficient solutions (referring to Theorem 2):We can get the efficient solutions by solving the following RSOP problem,maxx∈M∑i=1mwifi(x)s.t.M*⊂M⊂M*wi>0∀i=1,…,mw1w2FOssFOspFOpsFOpp0.50.5{(1.5,0)}{(1.5,0)}{(1.5,0)}{(1.5,0)}0.70.3ϕ{(2,0)}{(2.1,0)}{(x1,x2)|(x1−2.1)2+x22≤0.01}0.80.2ϕ{(2,0)}{(2.4,0)}{(x1,x2)|(x1−2.4)2+x22≤0.16}•The surely-feasible, surely-efficient solutions are {(1.5, 0)}⊂FsOse.The surely-feasible, probably-efficient solutions are {(1.5, 0), (2, 0)}⊂FsOpe.The probably-feasible, surely-efficient solutions are {(1.5, 0), (2.1, 0), (2.4, 0)}⊂FpOse.The set of all probably-feasible, probably-efficient solutions, is defined as{(1.5,0)}∪{(x1,x2)|(x1−2.1)2+x22≤0.01}∪{(x1,x2)|(x1−2.4)2+x22≤0.16}⊂FpOpe.Step 3: finding the weak-efficient solutions (referring to Theorem 3):We can get the efficient solutions by solving the following RSOP problem,maxx∈M∑i=1mwifi(x)s.t.M*⊂M⊂M*wi≥0∀i=1,…,mw1w2FOssFOspFOpsFOpp10ϕ{(2,0)}{(3,0)}{(x1,x2)|(x1−3)2+x22≤1}01{(0,0)}{(0,0)}{(0,0)}{(0,0)}•The surely-feasible, surely-weak efficient solutions are {(0, 0)}⊂FsOsw.The surely-feasible, probably-weak efficient solutions are {(0, 0), (2, 0)}⊂FsOpw.The probably-feasible, surely-weak efficient solutions are {(0, 0), (3, 0)}⊂FpOsw.The probably-feasible, probably-weak efficient solutions are{(0,0)}∪{(x1,x2)|(x1−3)2+x22≤1}⊂FpOpw.

@&#CONCLUSIONS@&#
