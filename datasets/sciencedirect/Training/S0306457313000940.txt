@&#MAIN-TITLE@&#
Bias–variance analysis in estimating true query model for information retrieval

@&#HIGHLIGHTS@&#
We study the retrieval effectiveness-stability tradeoff in query model estimation.This tradeoff is investigated through a novel angle, i.e., bias–variance tradeoff.We formulate the performance bias–variance and estimation bias–variance.We investigate various query estimation methods using bias–variance analysis.Experiments have been conducted to verify hypotheses on bias–variance analysis.

@&#KEYPHRASES@&#
Information retrieval,Query language model,Bias–variance,

@&#ABSTRACT@&#
The estimation of query model is an important task in language modeling (LM) approaches to information retrieval (IR). The ideal estimation is expected to be not only effective in terms of high mean retrieval performance over all queries, but also stable in terms of low variance of retrieval performance across different queries. In practice, however, improving effectiveness can sacrifice stability, and vice versa. In this paper, we propose to study this tradeoff from a new perspective, i.e., the bias–variance tradeoff, which is a fundamental theory in statistics. We formulate the notion of bias–variance regarding retrieval performance and estimation quality of query models. We then investigate several estimated query models, by analyzing when and why the bias–variance tradeoff will occur, and how the bias and variance can be reduced simultaneously. A series of experiments on four TREC collections have been conducted to systematically evaluate our bias–variance analysis. Our approach and results will potentially form an analysis framework and a novel evaluation strategy for query language modeling.

@&#INTRODUCTION@&#
Estimating query language model is an important task in language modeling (LM) approaches, since the query language model represents the underlying information need and has a significant impact on retrieval performance. Ideally, the estimation should be not only effective in terms of high mean performance over all queries, but also stable in terms of low variance of performance across different individual queries. In practice, however, improving effectiveness can sacrifice stability, and vice versa.For example, suppose that there are two queries q1 and q2, and for each query we use the average precision (AP) to measure the retrieval performance of a query model estimation method. Assume that we have two estimation methods A and B, where A and B can correspond to the original query model and an expanded query model, respectively. In Table 1, the mean average precision (MAP) over all queries for methods A and B are 0.2 and 0.34, respectively, meaning A is less effective than B. On the other hand, we compute the variance of AP across all concerned queries (denoted as VAP). Specifically, for A, VAP is 0.01, and VAP for B is 0.0676. It turns out the VAPBis greater than VAPA. The smaller VAP generally reflects the better retrieval stability. Thus, A is more stable than B. This shows a retrieval effectiveness-stability tradeoff between methods A and B.In this paper, we propose to study the tradeoff between retrieval effectiveness and stability from a new perspective, i.e., bias–variance tradeoff. The bias–variance tradeoff is fundamental in the estimation theory and has been extensively studied in density estimation (Zucchini, Berzel, & Nenadic, 2005), linear regression (Geman, Bienenstock, & Doursat, 1992), classification (Valentini, Dietterich, & Cristianini, 2004), and other areas (Bishop, 2006). In general, the bias represents the gap between the expectation (i.e., mean) of estimated values and the true target value, while the variance represents the variability over all estimated values.This motivates us to formulate the performance bias and variance, which are related to the retrieval effectiveness and stability, respectively. Specifically, assumes that we have a performance target (in practice, an upper bound performance). The performance bias represents the gap between the actual mean performance and the mean performance target. For the example in Table 1, assumes that the target AP (i.e., the upper-bound AP) can be 0.7 and 0.2 for queries q1 and q2, respectively. Then, for A, the bias is 0.45−0.2=0.25, where 0.45 is the target MAP and 0.2 is the MAP of A. Similarly, the bias for method B is 0.11 (see Table 1). On the other hand, the performance variance (denoted as Var in Table 1) is the variance of the retrieval performance across different queries, i.e., VAP. In Table 1, there is a bias–variance tradeoff between A and B, and the smaller performance bias and variance generally reflect the better retrieval effectiveness and stability, respectively. Therefore, we can investigate the problem of improving the retrieval effectiveness and stability from the perspective of reducing performance bias and variance, respectively.1We will also define additional performance bias–variance in Section 3.2.2.1In addition to the performance bias–variance, we also formulate the estimation bias–variance to measure the estimation quality of an estimated query model with respect to the true query model. Assume that the true information need can be represented by a set of truly relevant documents. Then, the true query model can be generated from truly relevant documents. Such a true query model is expected to give the upper-bound retrieval performance. The estimation error of an estimated model can be measured by the KL-divergence between the estimated model and the true model. The estimation bias is the expected estimation error over all queries, while the estimation variance is the variance of the estimation error across different individual queries. The sum of bias and variance (see Section 3.3) can yield the total estimation error which directly indicates the total estimation quality. The estimation bias–variance is important, in that it gives finer-grained insights on the estimated query model itself (i.e., its estimation quality).Our bias–variance analysis is based on general principles of bias–variance tradeoff and four query modeling factors (i.e., query model complexity, query model combination, document weight smoothness, non-relevant document removal). We investigate a series of estimated query models corresponding to the above factors, and analyze when and why the bias–variance tradeoff will occur and how the bias and variance can be reduced simultaneously. Based on the analysis, a set of hypotheses is formed. We then carry out extensive experiments based on TREC datasets to systematically evaluate the hypotheses based on the bias–variance analysis. Experimental results on performance bias–variance can generally verify the hypotheses. This shows that the retrieval effectiveness and stability can be studied via the performance bias–variance formulation and the general principles of bias–variance analysis. The experimental results on estimation bias–variance can verify the hypotheses on the occurrence of bias–variance tradeoff, but do not fully support hypotheses regarding the simultaneously reduction of the bias and variance. It is an interesting result though, since we find that the corresponding estimated query model may over-fit the relevant feedback documents, but may not fit the relevant documents that do not appear in the feedback document set. It can demonstrate that the improved retrieval performance cannot always guarantee the improvement of the estimation quality.The proposed bias–variance analysis is expected to form an analysis framework and potentially a novel evaluation strategy for the query language modeling. First, for a query modeling approach (or in general other IR models), we can analyze its modeling factors (e.g., model complexity or model combination) and propose hypotheses on the bias–variance tradeoff or even predict the bias–variance trends of the retrieval performance or estimation error/quality. Second, with respect to the evaluation strategy, the estimation bias–variance formulation can provide novel metrics (e.g., estimation bias, estimation variance, and the sum of them) to evaluate the estimation quality of an estimated model. In addition, the summed quantity of performance bias and variance (see Eq. (6) in Section 3.2.1), can naturally be a unified retrieval robustness metric combining retrieval effectiveness and stability.The rest of the paper is organized as follows. In the next section, we present a literature review on the query language model estimation. Then, in Section 3, we formulate the performance bias–variance as well as the estimation bias–variance. Section 4 presents and analyzes various estimated query models in relation to the bias–variance tradeoff. In Section 5, we move onto the evaluation of the hypotheses of the bias–variance analysis for the concerned query language models. Finally, in Section 6, we conclude our paper by summarizing the main contributions and highlighting the potential impact and future research directions.

@&#CONCLUSIONS@&#
In this paper, we propose a novel bias–variance analysis framework to study the tradeoff between the retrieval effectiveness and stability of query language modeling approaches in the pseudo relevance feedback context. Specifically, we propose a performance bias–variance formulation. This enables us to better analyze and understand the retrieval performance using the bias–variance analysis, which is a fundamental theory in machine learning and statistical estimation. We also go beyond the retrieval performance by directly measuring how closely an estimated query model can approach the true query model derived from the truly relevant documents. This leads to the estimation bias–variance formulation, which is based on the divergence between the estimated query model and the true query model.Based on four query modeling factors, i.e., query model complexity, query model combination, document weight smoothness and non-relevant documents removal, we analyze a number of representative query model estimation methods and present five hypotheses based on our analysis. In order to test the hypotheses, we then construct a systematic evaluation on four TREC datasets. Experimental results of the performance bias–variance (based on AP andρˆ′) generally support the hypotheses. This shows that the tradeoff between retrieval effectiveness and stability can be studied through the perspective of bias–variance tradeoff. In the experiments, we have explained when the bias–variance tradeoff can occur, and when the bias and variance can be reduced simultaneously. For example, in Section 4.2.4 and Section 5.4.2, we have explained why the performance bias–variance tradeoff will occur, and why a proper combination coefficient (e.g., λ=0.1) can reduce the performance bias and variance simultaneously.The experimental results of the estimation bias–variance support the hypotheses h1−h3, but do not support the hypotheses h4−h5. The hypotheses h1−h3 are about whether or not the bias–variance tradeoff will occur, and h4−h5 are about when the bias and variance can be reduced simultaneously. The results in Section 5.4.4 show that the non-relevant documents removal cannot reduce the estimation bias and variance simultaneously. As we explained, for some queries, this strategy may over-fit the relevant feedback documents, but may not fit the other documents that do not appear in the feedback document set, thus can move away from the true query model defined in Eq. (25). After we have removed all non-relevant documents in the feedback document set, we then use the document weight smoothing to improve the estimation quality. The corresponding results in Section 5.4.5 show that the estimation bias can be reduced, while the estimation variance almost remains unchanged. The total estimation error (summed over the estimation bias and variance) can be reduced by the document weight smoothing method.The above observations show that improving retrieval performance do not guarantee the improvement of the estimation quality. For example, in Section 5.4.4, in addition to the different trends between the performance bias/variance and estimation bias/variance, the trends of the overall retrieval performance (reflected by the sum of the performance bias and variance) are different from the trends of the overall estimation quality (reflected by the sum of the estimation and variance). This could lead to a future research direction to analyze the estimation quality of the query language modeling, rather than the retrieval performance only.This research may potentially lead to a novel evaluation strategy. Specifically, the estimation bias–variance formulation can provide novel metrics (e.g., estimation bias and estimation variance) to evaluate the estimation quality with respect to the true query model. In addition, the summed quantity of performance bias and performance variance (see Eq. (6)) can be a kind of robustness metric. We carried out a preliminary exploration on the bias–variance decomposition of the overall retrieval performance (taking into account both retrieval effectiveness and stability) based on Average Precision (AP) (Zhang, Song, Wang, & Hou, 2013). One may introduce different weights for bias and variance respectively, in the summation of them, to reflect how the retrieval robustness should be decomposed differently in different scenarios. Moreover, we will investigate other performance metrics (e.g., logAP) in the performance bias–variance analysis.Based on the proposed bias–variance analysis and evaluation methodology, we can study other query language model estimation methods (e.g., models in (Collins-Thompson, 2009b; Dillon & Collins-Thompson, 2010; Lv et al., 2011)). The proposed bias–variance analysis could also be applied to study the bias–variance of other IR models in terms of their retrieval effectiveness and stability. For instance, we may be able to study the model complexity of other IR models (e.g., ranking functions). The combination of two query models can be extended to the combination/ensemble of multiple (tens or hundreds) rankers in the web search scenario. In machine learning, ensemble learners can reduce both bias and variance simultaneously. In principle, the ensemble learners correspond to the combined rankers. As another example, the document weight smoothness can be related to the diversity of topic coverage of feedback documents. Further, we may explore non-relevant documents removal in the implicit feedback or interactive feedback scenario.Moreover, we can explicitly formulate the bias–variance for the social and personalized search where the tradeoff between the mean effectiveness over a collective user population and the variance across individual users needs to be balanced and properly modeled. We expect that the bias–variance analysis proposed in this paper can potentially serve as a start point for the above interesting research directions.