@&#MAIN-TITLE@&#
A new Mixture model for the estimation of credit card Exposure at Default

@&#HIGHLIGHTS@&#
New Mixture model proposed to estimate outstanding balance over loan and at default.Outstanding balance is function of spending and repayment amounts, and credit limit.Repeated events survival model to predict probability that balance greater than limit.Expected balance and limit estimated using panel models with random effects.Overall find more accurate predictions than most of those used in practice especially if short term predictions are required.

@&#KEYPHRASES@&#
Risk management,Forecasting,Panel models,Survival models,Macroeconomic variables,

@&#ABSTRACT@&#
Using a large portfolio of historical observations on defaulted loans, we estimate Exposure at Default at the level of the obligor by estimating the outstanding balance of an account, not only at the time of default, but at any time over the entire loan period. We theorize that the outstanding balance on a credit card account at any time during the loan is a function of the spending by the borrower and is also subject to the credit limit imposed by the card issuer. The predicted value is modelled as a weighted average of the estimated balance and limit, with weights depending on how likely the borrower is to have a balance greater than the limit. The weights are estimated using a discrete-time repeated events survival model to predict the probability of an account having a balance greater than its limit. The expected balance and expected limit are estimated using two panel models with random effects. We are able to get predictions which, overall, are more accurate for outstanding balance, not only at the time of default, but at any time over the entire default loan period, than any other particular technique in the literature.

@&#INTRODUCTION@&#
Predictions of Exposure at Default (EAD) are useful to banks for at least two reasons. First, the Basel Accords define expected loss as the product of Probability of Default (PD), Loss Given Default (LGD) and EAD, so predictions of EAD are needed to compute Regulatory Capital. Second, predictions of EAD are needed for the prediction of Economic Capital that a bank believes it needs to protect its depositors in the event of severe unexpected events. Since the credit crisis of 2008, there has been increased awareness of the models for these components, and in particular, for retail loans. However, these have been mainly focused on PD and LGD models, and how they should and can be improved (see Thomas (2010) for a review). The analysis and modelling of EAD at account level has so far been relatively neglected. For loans with fixed loan amounts over fixed terms and pre-agreed monthly repayment amounts, it is possible to estimate at least a reasonable range for EAD should the loan be expected to default in the following time horizon, e.g. in the next 12 months. However, in the case of revolving loans, the subject of this paper, i.e. loans with no fixed loan amount or term, debtors are given a line of credit, with a credit limit up to which they can draw upon at any time (as long as they have not gone into default). This could make it difficult for financial institutions to predict account level outstanding balance should an account go into default, especially if accounts deteriorate into default quickly and draw heavily on the card just before default.Another issue associated with the analysis and modelling of EAD is the measurement of EAD. EAD is similar to LGD in that its value is only of interest in the event default occurs (although its value still needs to be estimated for the calculation and preparation of economic capital). However, unlike LGD, where loss is predicted to be at some time point after default, EAD is known the very instant the account goes into default. Therefore, although default-time variables could be used in the modelling of LGD, they cannot be used for EAD models. As such, practitioners and the literature create various indicators to be estimated instead of EAD, taking into account the current balance and available limit. Unfortunately, each method has limitations.Our aim is to propose a new method to predict EAD for each loan in a portfolio and to demonstrate its accuracy by comparisons with methods currently in use and in the literature. Unlike conventional cross section methods, our proposed approach exploits the panel nature of a typical credit card dataset to model the values of balance and limit over time in a way that allows extrapolation from the time of prediction to the time of default. To evaluate our model, we use a large portfolio of defaulted loans and their historical observations, to directly estimate EAD at the level of the obligor by estimating the outstanding balance of an account, not only at the time of default, but at any time over the entire loan period, up to the time of default.Our methodology has several advantages over current methods. First for revolving credit loans, balance typically approaches the limit as an account moves over time towards default. We exploit this observation, to the extent that it is true, and the observation that modelling an account's limit at each time in its history can be done more accurately than the balance to more accurately predict the balance at default (that is EAD) than if this information is not used. Second we avoid several of the problems associated with current methods of modelling EAD which we describe in Section 2, for example the considerable sensitivity to very small values of a denominator. Third by using panel models we can more accurately include the effects of macroeconomic variables and so enable EAD estimates to be fixed as in a down-turn scenario than cross sectional models. Further our method yields predictions of balance at any time in an account's history and a bank would benefit from such predictions to estimate expected future interest income and so a component of expected profit from an account.The development and validation of the new Mixture model contributes to the literature in two ways. First, this is the first paper to predict the outstanding balance for defaulted loans at any time during the life of a revolving loan. Second, we incorporate macroeconomic variables into the model and so provide a framework suitable for stress testing later. The rest of this paper is structured as follows. Section 2 reviews the literature and Section 3 explains the model. In Section 4, we illustrate the use of the method and compare its performance to methods in the literature. Section 5 shows an empirical application and Section 6 concludes.Only a few papers have examined EAD and usually for corporate loans (see e.g. Araten and Jacobs (2001), Jacobs Jr. (2008), Jiménez and Mencía (2009), Jiménez, Lopez, and Saurina (2009), Yang and Tkachenko (2012) and Barkova and Pathasarathy (2013)). Few consider account level models, and they do not model EAD directly (e.g. see Risk Management Association (2004), Taplin, To, and Hee (2007)). Instead, they typically model the Loan Equivalent Exposure (LEQ) Factor, the Credit Conversion Factor (CCF) or the Exposure at Default Factor (EADF),11Note that LEQ, CCF and EADF are not universally defined. Basel II refers to a Credit Conversion Factor, “CCF”, but does not define it except to state that it is a factor of any further undrawn limit (see Basel Committee on Banking Supervision, 2004. International Convergence of Capital Measurement and Capital Standards: A Revised Framework., Paragraph 316, 474–478), so it is not clear that there is a standard industry practice towards EAD modelling.and then transform them back to an estimate of EAD (a more comprehensive review can be found in Moral (2006)). Thus, Jacobs Jr. (2008), using corporate data and a GLM modelling framework, models all three factors. Barakova and Parthasarrathy (2013) model four ratios using four algorithms applied to corporate level variables for large syndicated loans over 2007–2009. Yang and Tkachenko (2012) model EADF using eight account level variables and compare seven estimators applied to 500 commercial borrowers. The closest to our work is Qi (2009), who used unsecured credit card data, to model LEQ by looking at the level of credit drawn at 1 year before default. No macroeconomic variables were included in the above models. All come to the conclusion that EAD plays an important part in the calculation of the provision of capital and should be more carefully incorporated into risk and loss calculations.To define these terms, we adopt the definitions as in Jacobs Jr. (2008), Qi (2009), Barakova and Parthasarathy (2013) and Yang and Tkachenko (2012). In terms of nomenclature from here on, outstanding balance of account i at duration time τ is represented by Biτ, and limit of account i at duration time τ is represented by Liτ. We also construct a binary variable diτthat takes on the value 1 if account i defaults at time τ and dithat takes on the value 1 if account i defaults at some time in the future. To simplify the notation, the subscript i representing account i is dropped for the equations in this sub-section. The three variables EADFD, CCFDand LEQDare defined in Table 1.However, modelling EAD in terms of these ratios involves a number of difficulties, some of which are rehearsed by Jacobs Jr. (2008) and Qi (2009). In the case of EADFD, although we expect its value to range between 0 and 1, it is possible and quite common to see outstanding balances greater than the assigned limits, perhaps due to accumulated interest or banks allowing borrowers to go over their limits, giving values much greater than 1. This makes the choice of distribution slightly more challenging. A further problem noted by Qi (2009) is that as an account moves towards default and its balance increases, lenders may respond differently between accounts; in some cases increasing the limit, in others reducing the limit. This may introduce unexplained heterogeneity in a cross sectional model of EADFD.Considering CCFD, it is possible that the outstanding balance at the selected observation time happens to be £0, or even negative (the account is in credit), which would giveCCFD=0, and this raises the issue of the treatment of these accounts. It is also possible that some of these accounts then deteriorate quickly into delinquency and default. Also, should the account have a very low balance during observation time and defaults with a large balance, CCFDcould become an extremely large value, causing difficulties with data analysis and model estimation. Although on the one hand, it is likely that accounts that go into default have large balances on their account prior to default (for example, debtors who default due to behavioural issues), it is also possible that accounts go from a low or zero balance to default within a short period of time (for example, debtors who default due to unexpected circumstances), which could then imply a different set of predictors for each group. From the point of prediction, a value of 0 for CCFDdoes not make any sense as this would mean a prediction of £0 for balance at some time in the future, and possibly at default.Our method does not suffer from the theoretical inability to deal with zero or negative values of balance or the difficulty in modelling a dependent variable which is composed of a ratio where its value is very sensitive to different values of the denominator. In our approach we use panel data that incorporates unexplained heterogeneity unlike cross sectional models that have been used for the above ratios.The different values that the LEQDcan take could arise due to a number of different situations and which would give different implications. Should the account have zero undrawn limit, i.e. outstanding balance equal to limit, at the time of observation, we get an LEQDvalue of 0. This is a group of debtors who have used their maximum available limit and are likely to default, but would be difficult to include and handle in the modelling because the LEQDvalue computed does not have the same implications as the other LEQDvalues computed for when balance and limit are not equal.The majority of accounts would have a positive LEQD, which could be due to one of two situations: (a) when balance at default is greater than balance at observation, and balance at observation is below the credit limit at observation, which would be the most common progression into default; or (b) when balance at observation is greater than balance at default, and balance at observation is already greater than the limit at observation. The latter would represent debtors who are actually recovering from a large balance (and where perhaps extending the credit without putting the account into default might give lower loss). Although these two groups of debtors would have LEQDin the same range, we expect their characteristics and circumstances to be quite different. It is also possible to have negative LEQDagain in different situations.22These are: (a) when balance at observation is larger than limit at observation and balance at default is larger than balance at observation, which would represent debtors who are spiralling further into debt and default; or (b) when balance at observation is larger than balance at default, but both are below the limit at observation. Again, we have two groups of debtors with negative LEQDvalues but where they have arrived via different circumstances.The possible range of LEQD, coupled with the fact that different types of borrowers and circumstances could give LEQDin the same range, would make it difficult to estimate and model LEQD.One weakness of several of the above methods is that according to how they are defined, these variables could become unstable33These variables could have large volatility over short periods of time, most likely coinciding with the period just before default occurs as balance on accounts go from small to large in a short period of time.if the denominator is very small, so some restrictions have to be imposed on the range of values. Qi (2009) included only accounts at default time where undrawn limit is greater than 50 USD; Jacobs Jr. (2008) restricted the values of LEQ to between 0 and 1 and replaced outliers with the maximum and minimum values of his selected range. In his CCF model, he restricted the range of CCF to between the 1 and 99 percentiles, and replaced outliers with these maximum and minimum values. Both authors effectively ignored accounts that go from up-to-date to default suddenly or within a short time period, but this was the only way to get plausible results. Barakova and Parthasarathy (2013) winsorise LEQ and CCF at the 99th percentile. Yang and Tkachenko (2012) capped EADF at 1 and floored it at 0. Taplin et al. (2007) did not attempt to estimate LEQ (referred to as “CCF” in their paper) as they would have to exclude about 50 percent of their observations. They proposed regression models that estimate EAD as a function of balance and limit, but did not give any indication of covariates used or any performance measures. Note also that predictive results from most papers in the literature that used these dependent variables have generally been poor.We propose the prediction of outstanding balance using a Mixture model. The random variable, balance of account i at duration time τ could be above, equal to or below the account limit. The expected balance for account i at time τ is therefore given in Eq. (1):(1)E(Biτ|di=1)=(P(Biτ>Liτ|di=1)×E(Biτ|Biτ>Liτ,di=1))+(P(Biτ=Liτ|di=1)×E(Biτ|Biτ=Liτ,di=1))+(P(Biτ<Liτ|di=1)×E(Biτ|Biτ<Liτ,di=1)).Typically, as an account moves towards default, the balance increases towards and may exceed the limit. Often, borrowers stop increasing the balance when it reaches the limit. We exploit this occurrence in our method. Balance is less systematically governed by a model than is the limit, which is the result of a model. Instead of modellingE(Biτ|Biτ>Liτ,di=1)directly, we assume, as an approximation, that such accounts have an expected balance equal to their limit and replace Eq. (1) by Eq. (2).(2)E(Biτ|di=1)=(P(Biτ≥Liτ|di=1)×E(Liτ|Biτ≥Liτ,di=1))+(P(Biτ<Liτ|di=1)×E(Biτ|Biτ<Liτ,di=1)).We therefore propose the parameterisation of three models. First, a model of the probability that the outstanding balance of an account is larger than the credit limit, conditional on default; second, a model to predict the outstanding balance, conditional on default; and third, a model to predict the credit limit conditional on default, where the parameters to predict balance and limit are allowed to differ.There are cases where the limit may not increase and may even decrease as balance increases (see Qi (2009) for a good discussion of this). But our method is robust to this situation in that for such cases the survival model would be expected to predict a higher probability that in the next month the predicted balance will exceed the limit and so the weight on predicted balance in that month will be correspondingly lower and the weight on the predicted limit correspondingly higher, as Eq. (2) shows.From a training dataset based on only default accounts, i.e. accounts that eventually go into default, we propose the estimation of the probability that the outstanding balance at any duration time τ is equal to or greater than the limit at duration time τ. This is done by defining the event ‘overstretched’, Siτ, for account i at time τ which takes the value 1 if outstanding balance is greater than the limit at time τ; 0 otherwise, given in Eq. (3):(3)Siτ={1ifBiτ≥Liτ0otherwise.Given this definition, it is possible for an account to experience the event more than once (at different times of the loan), so a discrete-time repeated events survival model, given in Eq. (4), is estimated.(4)log(P(Siτ)1−P(Siτ))=ν+α(τ)+β1Xi+β2Yi,τ−l+β3Zτ−l,where ν is the intercept term; α(τ) is a function of time since the last event; Xiare account-dependent, time-independent covariates, i.e. application variables;Yi,τ−lare account-dependent, time-dependent covariates, lagged l months, i.e. behavioural variables;Zτ−lare account-independent, time-dependent covariates, lagged l months, i.e. macroeconomic variables; and β1, β2, β3 are unknown vectors of parameters to be estimated.To predict either balance or limit, we propose the estimation of two sub-models using two separate training datasets (where we use entire histories of the accounts in each training set). The datasets consist of accounts that at some time in their history defaulted as shown in Fig. 1. The training dataset is segmented according to whether accounts ever had balance exceeding limit (but not necessarily in default) at any point in the loan, or accounts that never had balance exceeding limit throughout the life of the loan. The subset consisting of accounts (represented by subscript a) where balance exceeded credit limit at some point during the loan is the limit training set, and used to estimate the limit at time τ, conditional on default. By structuring a sample in this way, our method involves parameterising the distribution of Laτgiven Baτ≥ Laτand given default. The other subset consisting of accounts (represented by subscript b) where balance never exceeded limit throughout the observation time of the loan is the balance training set, and used to estimate the balance at time τ. Hence, our method parameterises the Bbτgiven the Bbτ< Lbτdistribution. By segmenting the accounts in this way, we use the full history of each account in the estimation of either balance or limit as it changes over time and over the course of the loan period. This methodology, as well as the training and test sets created (details in the next section), is represented in Fig. 1.The limit, Laτ, and balance, Bbτ, for accounts a and b, respectively, at time τ could be estimated using panel models with random effects given in Eqs. (5) and (6) (see Cameron and Trivedi (2005), Gujarati (2003) and Verbeek (2004) for details).(5)[L^aτ|da=1]=μL+γ1LXa+γ2LYa,τ−l+γ3LZτ−l+αa+ɛaτ(6)[B^bτ|db=1]=μB+γ1BXb+γ2BYb,τ−l+γ3BZτ−l+αb+ɛbτwhere μL, μBare the intercept terms, Xa, Xbare account-dependent, time-independent covariates, i.e. application variables;Ya,τ−l,Yb,τ−lare account-dependent, time-dependent covariates, i.e. behavioural variables, lagged l months;Zτ−lare account-independent, time-dependent covariates, i.e. macroeconomic variables, lagged l months; γ1, γ2, γ3 are unknown vectors of parameters to be estimated; andαa+ɛaτ,αb+ɛbτare the error terms, with αa, αb∼ IID(0, σα2) and ɛaτ, ɛbτ∼ IID(0, σɛ2).The Mixture model could then be used to predict balance at any given time during the loan. This is done by first applying the survival model to all accounts to predict the probability of being overstretched at each duration time τ. Then, regardless of the estimated probability, one applies the balance panel model and the limit panel model onto all observations of all accounts to get an estimated balance and estimated limit, again at each time44When predicting balance and limit we set the random effect term at its mean (zero) in every case since its value is unknown for every case that is not in the training sample.τ. Because the models would be estimated for the subsets described above, these predicted values,B^iτandL^iτ, are the values of Biτgiven Bbτ< Lbτand Liτgiven Baτ≥ Laτrespectively, in both cases given default. The final predicted value for balance of an account i at duration time τ, given default,B˜iτ|di=1, is then a combination of the repeated events survival model estimating the probability of balance exceeding limit at time τ, and the panel models estimating either balance or limit at time τ. This is the expected value of balance and limit, given the probabilities of the balance exceeding the limit at time τ, and the assumed approximation, as defined in Eq. (7) (which is just Eq. (2) rewritten in a more efficient form):(7)[B˜iτ|di=1]=(P(Siτ)×(L^iτ|di=1))+((1−P(Siτ))×(B^iτ|di=1)),whereP(Siτ)=P(Biτ≥Liτ|di=1)and is the estimated probability that account i is overstretched at time τ, i.e. that the balance for account i at time τ exceeds the limit for account i at time τ; andL^iτandB^iτare the estimated values for limit and balance respectively, from their respective panel models.Data is supplied by a major UK bank and consists of a large sample of credit card accounts, geographically representative of the UK market. The accounts were drawn from a single product, and opened between 2001 and 2010. Accounts were observed and tracked monthly up to March 2011 or until it was closed, whichever is earlier. A minimum repayment amount is calculated in each month for each account and accounts progress through states of arrears depending on whether they are able to make the minimum repayment amount. We set the minimum repayment amount at 2.5 percent of the previous month's outstanding balance or £5, whichever is higher, unless the account is in credit, in which case the minimum repayment amount is £0, or the account has an outstanding balance of less than £5, in which case the minimum repayment amount would be the full outstanding amount. It is also possible for accounts to recover from states of arrears should the borrower make repayment amounts large enough to cover accumulated minimum repayment amounts that were previously missed. An account is then said to go into default if it goes into 3 months in arrears (not necessarily consecutive). For more details on the movement of accounts between states, see Leow and Crook (2014), but note that the percentage used here is different.Accounts that have a credit limit of £0 at any point in the loan are removed, based on the assumption that these accounts would have been singled out as problem loans by the bank. It is possible for accounts to be in credit, such that balance is negative, so balance is constrained such that observations that have negative balance have £0 balance. We experimented with various lags on the time-dependent covariates in all of the models and report results for lags of 12 and of 6 months. Because of these lags, and the minimum time required for accounts to go into default, we also removed accounts that have been on the books less than 15 and 9 months respectively.From the data, we see that some accounts go into default with an outstanding balance greater than their credit limit. This is illustrated in Fig. 2, which gives the distribution of the ratio of balance over limit at the time of default (only for ratios less than 3 for a clearer picture of the distribution). The peak in the graph corresponds to borrowers defaulting with a balance equal to their credit limit, but we also do see a sizeable proportion of borrowers who default with balances on either side of their credit limits.Common application variables are available, including age, time at address, time with bank, income, presence of landline and employment type. Behavioural variables are also available on a monthly basis, including repayment amount, credit limit, outstanding balance and number and value of cash withdrawals or card transactions. From these, further behavioural indicators can be derived, for example, the number of times an account oscillates between states of arrears and being up-to-date, the proportion of time the account has been in arrears and the average card transaction value. Any behavioural variables used in the model are lagged 12 (or 6) months.The macroeconomic variables considered here are listed in Table 2. The main source of macroeconomic variables is the Office of National Statistics (ONS), supplemented by data from Bank of England (BOE), Nationwide and the European Commission (EC) where appropriate. We use the non-seasonally adjusted series unless unavailable because the balance and limit data are also not seasonally adjusted. Any macroeconomic variables used in the model are also lagged 12 (6) months.Although we are interested in the prediction of outstanding balance of an account in each time step, these predictions of balance only become EAD values if and when accounts go into default. We also believe that balances of defaulted and non-defaulted accounts behave differently, and we see from Fig. 3that balances of non-default accounts are on average lower, and have more occurrences of 0 than the balances of default accounts. As such, we only use accounts that do (eventually) go into default. Because we only use observations from accounts that do go into default for the development of the EAD model, we do not need to be concerned with accounts that are inactive, e.g. have zero transactions and zero balance on the card for an extended period of time, but remain in the portfolio.In Section 1 we explained that the Mixture model will both predict balance at each time in the history of a defaulted account as well as at the time of default. The former is useful because a lender does not know when, or if, an account will default. We compare the performance of the established and Mixture model in these two settings by using two different test sets as follows. The dataset is divided to give the training set consisting of all accounts that do go into default at some time in their history and were opened on or before 31 December 2008, giving about 94,000 unique accounts. Test set I is an out-of-sample test set and is created using the remaining default accounts, consisting of all observations of all accounts opened on or after 01 January 2009. Test set I consists of about 12,000 unique accounts, giving more than 66,000 month-account observations. Test set II is created as a subset of Test set I, where only observations at the time of default are included. Test set I would give an indication of how well the model is able to predict balance for accounts that are likely to be delinquent but may not yet have gone into default at each time in their account history, whilst Test set II would be an indication of how well the model is able to predict at default-time, regulatory EAD. The relationship of the training and test sets are represented in Fig. 1. We calculate several performance measures including R-squared values, for the two test sets: Test set I, for all accounts, for all observation times; and Test set II, for all accounts, only at time of default.The portfolio of non-default accounts is not used in either the modelling or the testing as we estimate balance given default. Applying the Mixture model to observations of non-default accounts would give us the predicted balance should the account go into default, which is different to the observed balance, as seen in Fig. 3, which would mean that we will not be able to score how well the model is predicting.Both panel models were estimated using Generalised Least Squares (GLS) estimators. We estimated models with lags of 12 months and of 6 months lags. Covariates include application variables, lagged behavioural variables, and lagged macroeconomic variables, defined in Eqs. (5) and (6). We initially estimated the survival model and models for balance and limit, separately, using a very large number of application, behavioural and macroeconomic variables with 12 month lags. Covariates were then retained or deleted based on their level of statistical significance including that of other variables, their relevance and the predictive accuracy of the overall model. So for example in the limit and balance equations Time at address (TAAdd) and a binary variable indicator for missing or unknown time with bank (TWBank_MU) were not significant in the balance and limit equations and so were not included in the final equation (seeAppendix A) whereas they were significant in the survival model and so were included in that.55The omission of lagged utilisation in the limit and balance equations allows more flexibility in the estimated parameters concerning lagged balance and limit.Thus different sets of parameters are used in each model and between the lagged models.The survival model did not include utilisation or credit limit because although they were very statistically significant, the overall accuracy of the model at lag 12 months was slightly lower when the combination of variables that included these two was used. At lag 6 months, inclusion or exclusion of these two variables actually made little difference to predictive accuracy. We found the greatest predictive accuracy was gained when the training set of the balance model was restricted to cases when the minimum balance was over £200. Since each account typically has multiple observations (month-account observations), we adjusted for serial correlation by using a clustered sandwich estimator (on account ID) to estimate variance and standard errors Drukker (2003).To compare the predictive accuracy of the Mixture model with established methods, we use the training set with observations only at time of default to estimate the EADF, LEQ and CCF cross-sectional regression models (represented by the dotted square from the training set in Fig. 1). For all observations at time of default, EADF, CCF and LEQ are predicted based on observed covariates lagged 12 (6) months before default, according to the equations in Table 1. Similar to the EAD papers mentioned in the literature, some observations were further excluded from this subset due to some very extreme observations of CCF and LEQ. The final number of accounts and observations used in each training set when covariates were lagged 12 months is given in Table 3.EADF, LEQ and CCF were regressed on the same covariates as those used in the survival model.66Except for time varying duration time since last event, duration time squared and number of times event has happened which are all survival model specific and time on books that was included in the competing models but not the survival model.A variety of experimentation in terms of modelling functions and techniques was done for these competing models to improve the predictions for these variables. For the modelling of EADF, we tried several functional forms including a beta function and logit link functions but found that a liner model with OLS estimators gave the greatest predictive accuracy. For LEQ, various values of outliers were deleted but the greatest predictive accuracy was gained when we took only values in the range 0 < LEQ < 1 and adopted a generalised linear model with a logit link function with a maximum likelihood estimator. For the CCF model we took a loge transformation to transform the distribution to be close to normal, then deleted various sizes of outliers and used an OLS estimator. The predictive accuracy was very poor until we deleted all observations above the 80th percentile. These models are then applied onto the test sets (Test set I and Test set II) and performance measures are calculated. These three regression models are not further documented in this paper.

@&#CONCLUSIONS@&#
