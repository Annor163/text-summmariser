@&#MAIN-TITLE@&#
Assessing diagnostic complexity: An image feature-based strategy to reduce annotation costs

@&#HIGHLIGHTS@&#
Defined a metric for measuring the consensus difficulty in CT scan diagnosis.Developed a difficulty-predictive model, based solely on low-level image features.Reviewed most significant low-level image features for predicting case difficulty.Presented a radiologist assignment approach for reducing classification cost.

@&#KEYPHRASES@&#
Resource allocation,Computer-aided diagnosis,Image classification,

@&#ABSTRACT@&#
Computer-aided diagnosis systems can play an important role in lowering the workload of clinical radiologists and reducing costs by automatically analyzing vast amounts of image data and providing meaningful and timely insights during the decision making process. In this paper, we present strategies on how to better manage the limited time of clinical radiologists in conjunction with predictive model diagnosis. We first introduce a metric for discriminating between the different categories of diagnostic complexity (such as easy versus hard) encountered when interpreting CT scans. Second, we propose to learn the diagnostic complexity using a classification approach based on low-level image features automatically extracted from pixel data. We then show how this classification can be used to decide how to best allocate additional radiologists to interpret a case based on its diagnosis category. Using a lung nodule image dataset, we determined that, by a simple division of cases into hard and easy to diagnose, the number of interpretations can be distributed to significantly lower the cost with limited loss in prediction accuracy. Furthermore, we show that with just a few low-level image features (18% of the original set) we are able to determine the easy from hard cases for a significant subset (66%) of the lung nodule image data.

@&#INTRODUCTION@&#
Clinical radiology is at the center of modern medicine and can significantly impact patient outcomes [1]. Numerous advances in medical imaging technology continuously increase the volume and detail of diagnostic data available to radiologists; proliferation of cheap storage allows storing all of this image data indefinitely. However, human capacity to process collections of images has not kept up with the exponential growth in the available diagnostic data.Computer-aided diagnosis (CADx) systems can be used to assist radiologists in the interpretation of medical images. A CADx system classifies detected regions of interest with respect to their likelihood of malignancy and further helps radiologists in recognizing the next step, such as biopsy or short-term follow-up imaging examination.Once the region of interest is detected, a CADx system consists of two main steps: feature extraction and feature-based classification. In the feature extraction step, numerical descriptors quantifying the properties of the regions of interest are calculated and used to encode the raw pixel data. In the classification step, a machine learning or statistical modeling algorithm uses these image features to train a predictive model based on a dataset of annotated training images.Creating correctly annotated training datasets is a challenging task, especially in the medical imaging domain. When ground truth (such as pathology reports) is not available, the labels/annotations for the training sets are acquired through experts׳ interpretations. In many instances, the labels obtained from a single expert are different from the ground truth because of the uncertainty in the image data, lack of additional information at the time of interpretation, and error [2,3]. For these complex cases, panels of experts are preferred to provide an accurate and reliable label although it has been shown that the variability among experts׳ opinions can further introduce uncertainty in the labels [4]. For example, in a recent study [5] investigating the association of eye gaze pattern and diagnostic error in mammography, it was shown that there are significant differences even among individuals with the same level of training. Furthermore, the same study reported that the human perceived complexity of a case was consistently and moderately correlated with the risk of making a diagnostic error. Therefore, in order to obtain in the most accurate label while making efficient use of resources/experts, it is important to address how many experts to ask to annotate a medical image until a label consensus is reached and can be used to train and validate CADx systems.The first contribution of the work presented in this paper is in the area of efficient resource allocation, i.e. the process of reducing the overall cost of acquiring annotation labels (needed for the classification step of a CAD system) while maintaining comparable quality of image classification. We start from a hypothesis that some of the pathologies captured in the medical images will be necessarily more difficult to diagnose (or otherwise label) than others. We therefore assume that if we had a discriminator that could identify the different “categories” of medical cases, we could leverage that information and allocate annotation labels in a more efficient manner. Intuitively, a difficult case will often require multiple opinions (which may still not be enough for certainty), while an easy case may only require one or two evaluations. We define a threshold-based concept of estimated difficulty of the consensus of the diagnosis, which guides our custom label acquisition strategy that distributes radiologists׳ efforts in an efficient, non-uniform manner. Using this concept of case difficulty, we show that additional opinions have a different relative benefit in this context: additional opinions may be more valuable for “hard” cases than they are for “easy” cases or vice versa in some circumstances.Fig. 1 illustrates the relationship between the three concepts that are core to this paper: consensus (agreement among experts׳ aggregated opinion and CAD), reliable label (an aggregated label that does not change by adding a new label), and diagnostic complexity (difficulty to reach a consensus).The second contribution of the paper is in the area of reducing the semantic gap between low-level image features and high level interpretations of images. Having introduced a resource allocation strategy that relies on image content, we further evaluate the relationship between the low-level image features and the corresponding difficulty category of the image. This will serve two purposes: first, we can use that relationship to automatically predict the relative difficulty of the incoming image with just the low-level image features, without resorting to any of the radiologist opinions; second, we can provide feedback to radiologists and help identify the most important image features that should be guiding their decision. The knowledge of anticipated difficulty to achieve a consensus can also be helpful in choosing how much extra time to spend on each case. Using the NCI Lung Image Database Consortium (LIDC) data [6] where diagnosis annotations are provided by expert radiologists, we show that with just few low-level image features we are able to determine the easy from hard cases for a significant subset of the lung nodule image data.While we present our methodology and results for a particular medical application (lung nodule interpretation in Computed Tomography (CT) images) and diagnosis, same principles will apply for any dataset and medical task where annotations by multiple experts are required. The significance of relating image content to the human perception and cognition was also recently shown in [7] where certain image features where found to be important in predicting diagnostic error for mass interpretation in mammograms. Furthermore, segmentation of lesions can also benefit from the proposed approach given that in many cases the ground truth is derived from only experts׳ delineated boundaries [8]. Our approach can help determine how many outlines to aggregate (for example, using a p-map approach as defined in [9]) until a reliable region of interest is identified for the lesion. To our best knowledge, the presented work pioneers the idea of resource allocation for diagnostic imaging and shows strong support for the role of low-level image features in assessing diagnostic complexity.The rest of this paper is organized as follows. Section 2 discusses related research and background in the area of resource allocation and computer-aided diagnosis for lung nodules. In Section 3, we provide an introduction to our main dataset, LIDC, and discuss our methodology as well as define the image “difficulty” rating. Sections 4 and 5 present experimental results, and Section 6 concludes our findings and sketches further planned work.Classification mechanisms consider a collection of available item features and, based on the already known item labels, construct a model that can predict the labels for the unknown items. Therefore, resource allocation approaches may focus on reducing the cost of feature acquisition or label acquisition and the affected utility function is the resulting loss in prediction accuracy. Item features do not generally have any uncertainty associated with them. In contrast, only the simplest annotation schemes have no uncertainty at all; if available, the label of any item is known precisely. This may be accomplished by using a single annotator (when labels cannot be wrong) or simulated by requiring a single consensus-based label determined by a panel of annotators (i.e., through voting). Uncertainty in labeling can be introduced by noise, in which the inherent truth of a label is obscured by problems in the labeling process, or by imprecise knowledge of the label itself, in which there is no inherently true label, but the possibility of many labels. In recent years, the emerging popularity of crowdsourcing data acquisition created many situations where label quality is suspect (anonymous non-expert annotators). For example, crowdsourcing services like Amazon׳s Mechanical Turk [10], Games with a Purpose [11], and reCAPTCHA [12] provide inexpensive ways to acquire labels from a large number of annotators in a short time. Furthermore, websites such as Galaxy Zoo [13] and LabelMe [14] allow the public to label astronomical images and general purpose images, respectively, over the Internet.The bulk of available work had relied on opinion consensus when working with multiple labels. For example, the authors of [15] investigated the benefits of combining labels from multiple sources to improve data quality and achieve consensus. Their work assumes that the ground truth exists and that each annotator has a consistent noise level (e.g., annotator X is correct 75% of the time), which is not the case in our setting. They have concluded that repeated-labeling can improve the overall data quality, even when using noisy labelers and that the naïve round-robin approach for choosing what to label next is rarely the best strategy. A large number of non-expert volunteers were shown to achieve useful consensus results in [16], although these results are in a domain where a non-expert could produce good annotations (lunar crater annotation in this particular example) and ultimately such non-expert analysis serves as a pre-processing step to optimize experts׳ time. The work in [16] had mentioned but not investigated the possibility of incorporating expert evaluations into the same model thereby introducing multi-tiered cost to label acquisition. [17] directly considers the cost of acquiring features rather than labels; assuming that true labels are already known, but collecting additional item features may be costly. The work in [17] relies on the assumption that each feature has an inherent (independent) predictive value which can be weighed against its respective cost. In [18], studies were performed to analyze the cost of feature acquisition and the implications on the resulting classifier accuracy, concluding that round-robin label acquisition is not an efficient approach. Authors in [18] model the expected benefit of acquiring additional features, which is similar to the first phase of our work (though we look at additional labels and not features). However, in second phase we expand our work to estimate the expected benefit automatically, without looking at the labels.The work in [19] discusses the tradeoffs between acquiring annotation on images and information gain. The authors consider a more general case of labeling image contents, where both annotated and partially annotated images must be considered. Ground truth exists, but different labeling categories are available (e.g., “list items present in the image”, “estimate how difficult is this image to annotate”). The authors in [20] have found that a non-expert can estimate the image annotation difficulty, but it is important to note that difficulty in that context is “how long does it take to label the image?”, which is very easy to observe in practice. Our goal is to identify difficulty as it relates to experts׳ ability to arrive to an agreement; our difficulty measure had to be defined before it could be evaluated. Incorporating cost of annotation acquisition is similar to the work in [21] which controls the cost of label acquisition by limiting how much time the annotator is allowed to spend on the labeling process (by showing only a fraction of the text that needs to be annotated). In the long run, authors in [21] also intend to consider the incremental benefit of spending more resources on label acquisition to reduce the learning cost.Work in [22] focuses on using Random Forests and efficiently using a pool of individual predictive models for ensemble classification. It is similar to our work in that it investigates the cost of arriving to a consensus, but it once again assumes presence of ground truth and relies on a large pool of 100 trees to converge to an answer. In the same vein, the work in [20] studied strategies for applying crowdsourcing techniques (using tools such as LabelMe [14]) to improve image annotation, in order to reduce manual interaction required from specialists. Their goal is to improve non-expert labeling output by supplying detailed instructions and providing expert feedback. In [23], the authors use a semi-supervised learning algorithm variation called Co-Forest that relies on a combination of labeled and unlabeled items to reduce prediction errors of their models. Their approach relies on the idea that two classifiers can exchange newly predicted labels to mutually improve classification, as long as only the highest-confidence labels are shared.The approach presented in this paper pertains to a particular category of images, where no ground truth is available and annotations have to be provided by experts. Such situation is more common in the medical domain where, before getting pathology reports (forming ground truth), radiologists have to interpret medical images with respect to their likelihood of malignancy (generating reference truth). For diagnostically difficult cases, several opinions might be necessary, and therefore, additional experts need to be assigned to those cases. Our proposed approach aims to perform annotation assignment to minimize the effort required from the radiologists, while losing as little accuracy as possible (or, alternatively, given a fixed budget build a model with highest feasible accuracy). The approach is similar to the challenges of efficient resource allocation discussed in [24] and in several approaches discussed in this section. However, most of the time second opinion is not necessary since the label provided by an expert is the ground truth. When multiple opinions are used it is to estimate the underlying ground truth value based on unreliable (i.e., non-expert) annotators; in contrast, we have to use current expert consensus as the substitute for ground truth which has the potential to change any time (e.g., suppose first five medical experts agree on a diagnosis and then six more disagree changing the total consensus). Consequently, our definition of “difficulty” is fundamentally different from related work because it refers to the ease of achieving consensus and not to the difficulty of acquiring the label. One difference between these two measures is that the difficulty of acquiring the label is defined by some objective quantifiers (e.g., time to acquire the label, salary paid to the agent acquiring the label), while consensus difficulty cannot be quantified so easily. There is certainly some correlation between these two measures and if LIDC provided time taken for each diagnosis, we could have used it as a valuable predictor variable (it would also make the resource allocation phase of our work more precise). Some of the more expensive to acquire labels will be difficult to agree on, while some of the cheaper labels will correspond to easy consensus. However, it is the other two combinations that would be most interesting to study: the easy-to-acquire labels which result in disagreement between radiologists and the difficult-to-acquire labels that quickly coalesce into a consensus. Consensus interpretation of imaging studies is defined as the agreement reached when two or more radiologists report the imaging findings [25].In the computer-aided diagnosis (CAD) literature, consensus image interpretation is used as a standard of reference to provide the target class label to which the CAD method is compared [26]. For diagnosis of lung nodules, the application domain of this paper, most of the CAD systems use traditional classification techniques such as linear discriminant analysis [27–33], decision trees [34,35], and neural networks [36–39] to learn the class label from nodules׳ appearance, size, and shape image features. The CAD performance is generally evaluated using receiver operator characteristic (ROC) analysis and area under the ROC is used as a performance index [40,41]. A match between the predicted diagnosis by CAD and the standard reference truth for each one of the cases from the data will result in an area under the curve equal to 1.While all the above CAD studies relate to the prediction of malignancy, there are other studies that look into predicting specific characteristics of lung nodules that are important in the diagnosis process. For example, [42] proposed a patch-based context analysis to differentiate between well-circumscribed, vascularized, juxto-pleural, and pleural tail types of nodules. Further, [43] classified lung nodules into round, lobulated, densely spiculated, ragged, and halo based on their margin characteristic. In [44,45] Raicu et al. used image features to predict spiculation, lobulation, margin, subtlety, sphericity, and texture characteristics of lung nodules.An extensive literature review of CAD systems for lung nodules is presented in the review article by [46]. All of these studies have in common the assumption that a good class label is generated either by a single annotator or by the consensus among multiple annotators. Given that in the clinical practice of radiology consensus is hard to achieve, it is important to be able to determine how many annotators to use until a reliable and accurate label is obtained for each case. In the next section, using the NIH Lung Image Database Consortium Data, we propose a strategy for determining the level of difficulty to reach a consensus and show that image features can be used to determine the level of consensus difficulty.

@&#CONCLUSIONS@&#
