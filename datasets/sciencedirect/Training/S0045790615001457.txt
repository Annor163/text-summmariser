@&#MAIN-TITLE@&#
FPGA based accelerated 3D affine transform for real-time image processing applications

@&#HIGHLIGHTS@&#
In the proposed algorithm, simultaneously 4 pixel/voxel locations can be transformed to compute AT of the entire image.The proposed architecture requires only relevant frames instead of all the frames simultaneously.The faster implementation of the affine transform is useful during image registration of the 3D bio-medical images.

@&#KEYPHRASES@&#
Biomedical image processing,3D affine transform,Real-time,Hardware,

@&#ABSTRACT@&#
Affine Transform (AT) is widely used in high-speed image processing systems. This transform plays an important role in various high-speed image processing applications. AT, an important process during the intensity-based image registration, is applied iteratively during the registration. This is also used for the analysis of the interior of an organ and to get a better view of the organs from various angles in 3D coordinate system. Hence, for real-time medical image registration and visualization of the acquired volumetric images, acceleration of AT is very much sought for. In this paper, a parallel and pipelined architecture of the proposed AT algorithm has been presented. This will accelerate the transform process and reduce the processing time of medical image registration. The architecture is mapped in Field-Programmable Gate Array (FPGA) for prototyping and verification. The results show that the computational complexity of the proposed parallel algorithm is almost 4 times better than that of the conventional algorithm.

@&#INTRODUCTION@&#
AT consists of various operations such as rotation, scaling, shearing and translation. This algorithm is highly computationally intensive as it involves matrix multiplication of trigonometric functions and interpolation. In high-speed imaging systems, AT plays an important role. The transform is applied on each and every pixel location of the image to obtain a transformed image. Hence, the computation time of the transformation also increases along with the size of the image. A parallel algorithm will help in faster processing of the transform and this will led to faster implementation of the high-speed imaging systems. Image registration is the process of overlaying two or more images of the same scene taken at different times from different viewpoints and/or by different sensors. It geometrically aligns two images called the reference image and sensed image [1]. AT is widely used for geometric alignment during image registration [2–4]. During the process of mapping points from one image to the corresponding points in another image, correct alignment of images is obtained iteratively [5]. The basic operations of any registration algorithm are the image rotation and shifting methods [6]. These are possible by applying AT on the 3D image [7]. These operations must be carried out several times for image registration [5,7–9]. The time spent in measuring is negligible compared to that of the transformation. On the CPU, the execution time is dominated by the transform [10]. A real-time implementation of image registration is required [6,11,12]. So, a fast technique for AT of 3D images is needed. Table 1shows computation offered by different operations during image registration [10]. In computer graphics, 3D object is represented and displayed using polygon mesh model [10]. A polygon mesh model is a structure consisting of polygons represented by a list of coordinates called as polygon vertices. Thus the information stored to describe an object is a list of points or vertices [11]. But in medical imaging systems such as Computed Tomography (CT), an image is produced by physical measures such as X-ray absorption which has to be converted to 3D data by a mathematical reconstruction process. So, in the case of display of CT images, the data has to be stored as a 3D array of image elements called voxel. Each voxel contains a value that describes the material properties at its location. The volume data sets are acquired and pre-processed before display.Most of the previous researchers have put more emphasis on affine rotation. However, other operations of AT are used as well in medical image processing [13]. In implementing a rotation operation, non-separable and separable rotation methods are used. A separable rotation method decomposes the rotation into two or more 1D transformation along X and Y directions. There are several decompositions such as two-pass [14] and three-pass algorithms for 2D images. For 3D AT, several authors have also proposed separable rotations such as two-pass or three-pass algorithms which are nothing but shear based decompositions of the rotation operations [15–17]. Though, the separable rotation method reduces the use of memory, it also increases artefacts and computational complexity. In non-separable implementation, people have designed application specific chip for image rotation using CORDIC (acronym for Coordinate Rotation DIgital Computer) algorithm [18]. A multiplication free algorithm for 2D images has been proposed which uses the relationship between two neighboring pixels [19]. A similar algorithm named AT by Removing Multiplications (ATRM) has also been proposed [20]. A new idea of combining back-projection and interpolation into a single step has been proposed by Fischer and del Río [21]. In this method the basic idea is to employ a 3D Bresenham line interpolation algorithm while simultaneously traversing its back-projection in the original volume. In [22], a new fast algorithm has been proposed which calculates AT of two voxel locations using a single transform operation. The algorithm exploits the relation between two voxel positions by shifting the origin (0, 0, 0) to the center slice of the image. This helps in reducing the computations in terms of matrix multiplications to obtain the AT of a 3D image.In this paper, a complete hardware solution of our previously published algorithm [23] is presented. The algorithm in [23] was the modified version of our work [22,24]. For the registration of functional MRI, only small rotations and translations: 1–2° and 1–2 voxel dimensions are generally required [6]. So, without storing the complete image, transform operation can be performed by storing only some relevant frames of the image to minimize memory requirement. Moreover, this proposed modified algorithm is able to calculate AT of four voxel locations by performing a single transformation. This helps in further reducing the computations in terms of number of multiplications and also increasing the parallelism of the algorithm. The inherent parallelism of this algorithm reduces the hardware resources during implementation of the algorithm.The paper is organized as follows. Mathematical analysis of the proposed algorithm is explained in Section 2. Section 3 furnishes the detailed architectural design of the proposed algorithm. Image quality comparison is made between the proposed algorithm and conventional one as well as Verilog simulation results are discussed in Section 4. Finally, the paper is concluded in Section 5.The AT consists of four operations i.e. rotation, scaling, shearing and translation. All the operations of the 3D AT can be expressed by Eq. (1). However, to perform all operations with respect to the centre voxel location Eq. (1) is modified to Eq. (2). For different operations of AT the value ofmijonly changes.(1)p1q2r3=m11m12m13txm21m22m23tym31m32m33tz×pqr1(2)p1q2r3=m11m12m13txm21m22m23tym31m32m33tz×p-Xcentq-Ycentr-Zcent1+XcentYcentZcentwhereXcent,YcentandZcentare centre coordinates of the image.Due to shifting of the centre of the image, each voxel location has symmetry position in the image. By exploiting the replication nature of the pixel locations, the ATPR (acronym for Affine Transform by Pixel Replication) algorithm [22] was able to calculate transform of 2 pixel locations in parallel. In this paper, an algorithm named Modified ATPR (MATPR) is proposed which can calculate transform of 4 voxel locations from a single transformation operation. Here, 3D image is taken frame by frame as a 2D image and the transform is applied on that frame. The proposed algorithm is explained mathematically as follows.(3)p1(x,y,z)=u(x,y)+m13×z+tx+offset(1)q1(x,y,z)=v(x,y)+m23×z+ty+offset(2)r1(x,y,z)=w(x,y)+m33×z+tz+offset(3)whereoffset(1)=Xcent,offset(2)=Ycentandoffset(3)=Zcent(4)u(x,y)=m11×x+m12×yv(x,y)=m21×x+m22×yw(x,y)=m31×x+m31×yAT of pixel with coordinate(-x,-y,z)is(5)p2(-x,-y,z)=u(-x,-y)+m13×z+tx+offset(1)q2(-x,-y,z)=v(-x,-y)+m23×z+ty+offset(2)r2(-x,-y,z)=w(-x,-y)+m33×z+tz+offset(3)where(6)u(-x,-y)=-m11×x-m12×y=-u(x,y)v(-x,-y)=-m21×x-m22×y=-v(x,y)w(-x,-y)=-m31×x-m32×y=-w(x,y)Hence, while computing the AT of voxel with coordinate(x,y,z), the AT of the voxel with coordinate(-x,-y,z)can be obtained without any additional multiplication operation. AT of two more voxels are computed at the same time by performing only nine multiplications instead of eighteen.AT of voxel with coordinate(x,y+offset(2),z)is(7)p3(x,y+offset(2),z)=u(x,y+offset(2))+m13×z+tx+offset(1)q3(x,y+offset(2),z)=v(x,y+offset(2))+m23×z+ty+offset(2)r3(x,y+offset(2),z)=w(x,y+offset(2))+m33×z+tz+offset(3)where(8)u(x,y+offset(2))=u(x,y)+m12×offset(2)v(x,y+offset(2))=v(x,y)+m22×offset(2)w(x,y+offset(2))=w(x,y)+m31×offset(2)AT of voxel with coordinate(-x,-y-offset(2),z)is(9)p4(-x,-y-offset(2),z)=u(-x,-y-offset(2))+m13×z+tx+offset(1)q4(-x,-y-offset(2),z)=v(-x,-y-offset(2))+m23×z+ty+offset(2)r4(-x,-y-offset(2),z)=w(-x,-y-offset(2))+m33×z+tz+offset(3)where(10)u(-x,-y-offset(2))=-u(x,y)-m12×offset(2)v(-x,-y-offset(2))=-v(x,y)-m22×offset(2)w(-x,-y-offset(2))=-w(x,y)-m31×offset(2)Hence, the AT of 4 voxel locations are obtained from the AT of only one voxel location with some extra addition operations. In this way, the computation reduces by a factor of four to get the AT of the entire image. The proposed MATPR algorithm, explained below, is a good proposition for VLSI implementation due to its inherent parallelism and regularity.1.Input the image size and transform parameters. /∗L=No. of rows, M=No. of columns and N=No. of frame. The symbolsm11,m12,m13,m21,m22,m23,m31,m32,m33,tx,tyandtzare transform parameters and Q is the transformed image∗/calculate offset:(offset(1),offset(2),offset(3))=(Xcent,Ycent,Zcent)Calculate constant parameter:k1=offset(2)×m12,k2=offset(2)×m22,k3=offset(3)×m32and(e1,e2,e3)=(offset(1),offset(2),offset(3))+(tx,ty,tz).x=-L-12to 0y=-M-12to 0z=-N-12toN-12(p1,q1,r1)=(u,v,w)+(m13×z,m23×z,m33×z)+(e1,e2,e3)(p2,q2,r2)=-(u,v,w)+(m13×z,m23×z,m33×z)+(e1,e2,e3)(p3,q3,r3)=(u,v,w)+(m13×z,m23×z,m33×z)+(e1,e2,e3)+(k1,k2,k3)(p4,q4,r4)=-(u,v,w)+(m13×z,m23×z,m33×z)+(e1,e2,e3)-(k1,k2,k3)/∗(p1,q1,r1),(p2,q2,r2),(p3,q3,r3), and(p4,q4,r4)are Affine Transform of voxel locations(x,y,z),(-x,-y,z),(x,y+offset(2),z)and(-x,-y-offset(2),z)respectively.∗/If(pu,qu,ru)is within the source image then apply interpolation on voxels close to(pu,qu,ru)Pixval u=new voxel value after interpolation.Where u=1 to 4Q[x+offset(1),y+offset(2),z+offset(3)]=pixval1Q[-x+offset(1),-y+offset(2),z+offset(3)]=pixval2Q[x+offset(1),y+M2+offset(2),z+offset(3)]=pixval3Q[-x+offset(1),-y-M2+offset(2),z+offset(3)]=pixval4End ifEnd forEnd forEnd forThe MATPR algorithm, explained in Section 2, has been implemented in FPGA. The architectural block diagram of the complete system is shown in Fig. 1. According to the size of the input image, the coordinate generator unit generates values of x, y and z which are connected as inputs to the coordinate transformation unit.The coordinate generator unit consists of a cascaded counter. The input selection unit generates the transform parameters according to the 2-Bit Input selector signal. Among the four combinations, three are used to select the parameters, which are obtained from the LUT for different rotations, and one is for the rest of the operations of the AT. Both the cosine and sine values of all angles are stored in the LUT in a single address location. The Affine Transformation as mentioned in Eq. (2) is performed by the coordinate transformation unit. The transformed coordinates may fall outside the image. The coordinate checker unit checks the coordinate value to ignore the particular coordinate if it falls outside the range of the image. Each unit of the block diagram is explained in detail in the following sections.A 3D image can be rotated with respect toX,Yand Z axes. The AT parameters are different for rotation with respect to each axis. Hence, to store the rotation parameters, three LUTs i.e. LUT0, LUT1 and LUT2 have been used for rotation with respect toX,Yand Z axes respectively. For rotation operation, nine parameters are used. For any angle ‘θ’, all the nine parameters are pre-computed and stored in the same location of the LUT. To store each parameter, 11 bits have been considered with 9 fractional bits. So, every location of the LUT is of 99 bits. For other operations except rotation, affine parameters are passed from the input directly. An input selection circuit is used to select the elements those make up the transformation matrix for input to the coordinate transformation unit depending upon the input selector as mentioned in Table 2.Fig. 2shows the block diagram of the coordinate transform unit which transforms the destination voxel location to the corresponding source voxel location. The gray value of the source location is mapped to the corresponding destination voxel location of the output image. According to the proposed algorithm, four coordinates are computed simultaneously. Outputs of Block 11, Block 12 and Block 13 generate the coordinates of row, column and frame respectively. Internal architectures of these three blocks are the same but only inputs are changed. Fig. 3shows the internal architecture diagram of Block 11. The symbolsx,yand z are the outputs of three cascaded counters and they represent row, column and frame of destination voxel locations respectively.offset(1),offset(2), andoffset(3)are the centre location of the image. After receiving the input by the transformation unit, the same computation is performed irrespective of the transformation parameter. So, in the proposed architecture, the same modules are used for all the operations of the AT. In Fig. 3,k1ande1are not updated withx,yand z. So, it is calculated only once at the beginning of each operation andp1is updated with every input of x and y.In this architecture, the 3D image is considered as a stack of frames. The AT is applied sequentially frame by frame. Displacement will be very less for small angle rotation so we do not need to store total image in memory. When performing the transformation of theRth frame,(R+t)th to(R-t)th frames have to be stored in the system memory so that the required source voxel can be accessed directly. The value of ‘t’ depends on the input parameters. For implementation of the proposed algorithm, considering maximum rotation angle of6°, 16 consecutive frames are stored in the system memory. Before starting the transformation operation, first 8 frames of the image are stored in the system memory. After that transformation operation is applied on0th frame, both the transformation and storing of the next frame are performed simultaneously. It is very unlikely that(p1,q1,r1),(p2,q2,r2),(p3,q3,r3)and(p4,q4,r4)fall on the integer voxel points. To find out the exact gray value of these non-integer voxel locations, an interpolation technique is required. The proposed algorithm has been implemented using nearest neighborhood interpolation (NNI) as well as TriLinear Interpolation (TLI) techniques. In NNI, gray value of non-integer voxel point within the image is the nearest voxel gray value of that non-integer voxel. However, in TLI eight neighborhood voxel values of the non-integer voxel are required.Details of implementation of interpolation are discussed in Section 3.4. Fig. 4shows that the point ‘n’ is a non-integer voxel location within the image. To compute the gray value of voxel ‘n’ using TLI, gray value of eight voxel points A1_0, B1_0, A2_0, B2_0, A1_1, B1_1, A2_1 and B2_1 are required. In the proposed algorithm, transformations of four voxel locations are computed simultaneously. So, gray values of 32 voxel locations have to be accessed at single clock edge. The architecture for accessing 32 voxel locations is discussed below which helps to perform the interpolation in parallel.A single frame ‘k’, is divided into four sub memories like RAMk_00, RAMk_01, RAMk_10 and RAMk_11 depending on the row and column positions as shown in Table 3. The symbol ‘k’ is varying from 0 to 15 depending on the frame. These memories are dual port memory. Fig. 5shows complete block diagram of memory distribution, corresponding accessing and selecting block.The functionality of memory address generator is shown below(11)A_adds_00={Even(Int(p1),Int(p1)+1)[6:1]Even(Int(q1),Int(q1)+1)[6:1]}A_adds_01={Even(Int(p1),Int(p1)+1)[6:1]odd(Int(q1),Int(q1)+1)[6:1]}A_adds_10={odd(Int(p1),Int(p1)+1)[6:1]Even(Int(q1),Int(q1)+1)[6:1]}A_adds_11={odd(Int(p1),InT(p1)+1)[6:1]odd(Int(q1),Int(q1)+1)[6:1]}Int(p1)=Integer(p1)B_adds_00, B_adds_01, B_adds_10 and B_adds_11 can be generated in a similar way by puttingp2andq2in place ofp1andq1respectively in (11).Internal architectures of Block1 to Block8 (vide Fig. 5) are the same with different inputs. Fig. 6shows the internal structure of Block1. Initially, RAM0_00, RAM0_01, RAM0_10, RAM0_11 contain the data of 0th frame but after16×64×64clock cycle, RAM0_00, RAM0_01, RAM0_10, RAM0_11 are updated with the data of 16th frame. Simultaneously, the transform of 7th frame is computed. As the maximum rotation angle of6°is considered for designing the architecture, the 0th and 16th frames are not required for source voxels. Hence, this memory is not to be accessed. Mem_rotation_cntrl is used as a control signal to pass input to the second stage mux. This signal is generated at the output of a 4-bit counter. This counter starts counting after16×64×64clock cycles and incremented by one after each64×64clock cycles. So, after16×64×64clock cycles 0th to 15th input positions of second stage mux are connected with the outputs of the memory which originally contain 1st to 16th frames respectively. Again, after17×64×64clock cycles, 0th to 15th positions of second stage mux are connected with the outputs of the memory which originally contain 2nd to 17th frames respectively. Z1_sub_count is the difference between 7-bit counter output which starts after16×64×64clock cycles and incremented by one after64×64clock cycles each time, and integer part ofr1. Similarly, Z2_sub_count is the difference between 7-bit counter output and integer part ofr2. These two signals are used as selection line of the second stage mux. Z1_sub_count+1 and Z2_sub_count+1 are used to select just upper frames. For source voxel address(p3,q3,r3)and(p4,q4,r4), the same architecture is used as shown in Fig. 5.p1_q1_lm_0=accesseddatacorrespondingtoaddress(A_adds_lm,Z1_sub_count)p1_q1_lm_1=accesseddatacorrespondingtoaddress(A_adds_lm,Z1_sub_count+1)p2_q2_lm_0=accesseddatacorrespondingtoaddress(B_adds_lm,Z2_sub_count)p2_q2_lm_1=accesseddatacorrespondingtoaddress(B_adds_lm,Z2_sub_count+1)where(lm)=(00,01,10,11)The expressionspk_frac,qk_fracandrk_fracare fraction parts ofpk,qkandrkrespectively. Fig. 7shows block diagram for TLI interpolation. The internal diagrams of TLI_1 to TLI_4 are the same. Each block is working simultaneously and computing the gray value of one non-integer voxel point each. In the next paragraph, one of the block diagrams has been discussed. In TLI, suppose ‘n’ (vide Fig. 4) is a non-integer voxel location within the image. To compute gray value of this point, first gray values of pointsf1andf0have been computed using BiLinear Interpolation (BLI) followed by linear interpolation betweenf1andf0. Fig. 8shows block level circuit for TLI. BLI_20 and BLI_21 compute the gray values of points,f0andf1respectively. After getting output from BLI_20 and BLI_21, NNI_2 computes final gray value. Internal structure of BLI_20 and BLI_21 are the same. Fig. 9shows internal structure of BLI_20 (vide Fig. 8). Block Delay Element has been used to make a pipelined architecture. The fraction parts of row, column and frame aredx,dyand dz respectively. Lsb_row and Lsb_clm are LSB of integer part of the column. To compute the gray value of pointf0(vide Fig. 4), the gray values of pointsm1andm2are computed first using linear interpolation simultaneously. Then the gray value of pointf0is computed from the gray values of pointsm1andm2using linear interpolation. NNI_10 and NNI_20 in Fig. 9 compute gray values of pointsm1andm2respectively and NNI_30 computes the gray value of pointf0. Table 4shows the gray values of four nearest voxel locations of the pointfm.In Section 3.3, we have discussed how to accessf(A1_0),f(B1_0),f(A2_0)andf(B2_0). Similarly,f(A1_1),f(B1_1),f(A2_1)andf(B2_1)can be accessed by increasing only second stage select line by one.f(mi)=gray value at pointmi∈(1,2)(12)f(mi)=f(Ai_0)×(1-dy)+dy×f(Bi_0)=(f(Bi_0)-f(Ai_0))×dy+f(Ai_0)When integer of the column is even and(13)f(mi)=f(Bi_0)×(1-dy)+dy×f(Ai_0)=(f(Ai_0)-f(Bi_0))×dy+f(Bi_0)when integer of the column is odd. Gray value of point will be computed by Eqs. (12) or (13) depending on LSB of integer part column. Gray value of pointf0isF0, which can be computed from Eqs. (12) and (13) by replacingf(Ai_0),f(Bi_0)and dy withf(mi)and dx respectively and using integer part of row instead of integer part of column.SimilarlyF1, gray value of pointf1, can be computed.Either Eqs. (12) or (13) is selected by NNI_10 (vide Fig. 9). The internal circuit of NNI_10 (vide Fig. 9) is shown in Fig. 10. NNI_20 (vide Fig. 9) and NNI_30 (vide Fig. 9) are similar to NNI_10 (vide Fig. 9). However, inputs are different. In the same way we can compute the gray value of the pointf1, that isF1.After gettingF0andF1, NNI_2 (vide Fig. 8) computesPixval1, the gray value of non-integer voxel n.(14)Pixval1=(F1-F0)×dz+F0Figs. 10 and 11show the internal circuit diagram of NNI_10 and NNI_2 of Figs. 9 and 8 respectively.

@&#CONCLUSIONS@&#
In this paper, a 3D AT algorithm has been proposed and presented along with its LUT based parallel and pipelined hardware implementation. The architecture is designed by storing the relevant frames of the total image as the requirement of rotation angle is very small for image registration. A maximum of6°angle of rotation is considered in our architecture design which can be enhanced by increasing the number of frames in the system memory. From Fig. 13 this algorithm can be applied for any operation, although we have considered a maximum rotation of6°. This algorithm traverses only one fourth of voxel location of the image to obtain the transformation of complete image. By using the same architecture, all the operations of AT are possible. Due to faster implementation, the proposed algorithm is useful during image registration of 3D biomedical images, where iterative implementation of AT is required. It takes 16.7ms and 18.5ms to perform any operation of 3D AT for a 256×256×256image at frequencies of 258MHz and 233.65MHz using NNI and TLI techniques, respectively. The PSNR of the proposed algorithm is compared with that of the conventional algorithm to measure the quality of the transformed image and is found to be matching. The speed of the implementation in terms of volume rate is calculated and compared with that of the GPU implementation results. The results show that the proposed algorithm is faster compared to GPU while implemented in XC5VX95T FPGA and it can be made even faster by using advanced FPGAs. It can be conjectured that this faster implementation of the proposed 3D AT may be a good candidate for real-time medical image registration.