@&#MAIN-TITLE@&#
A parser for authority control of author names in bibliographic records

@&#HIGHLIGHTS@&#
A tool assisting authority control in large bibliographic collections.An efficient data structure accelerates the identification of name variants.The space search is pruned using a parser of complex temporal expressions.The algorithm and the parser’s grammar are distributed as open-source resources.

@&#KEYPHRASES@&#
Digital libraries,Cataloguing standards,Natural language processing,

@&#ABSTRACT@&#
Bibliographic collections in traditional libraries often compile records from distributed sources where variable criteria have been applied to the normalization of the data. Furthermore, the source records often follow classical standards, such as MARC21, where a strict normalization of author names is not enforced. The identification of equivalent records in large catalogues is therefore required, for example, when migrating the data to new repositories which apply modern specifications for cataloguing, such as the FRBR and RDA standards. An open-source tool has been implemented to assist authority control in bibliographic catalogues when external features (such as the citations found in scientific articles) are not available for the disambiguation of creator names. This tool is based on similarity measures between the variants of author names combined with a parser which interprets the dates and periods associated with the creator. An efficient data structure (the unigram frequency vector trie) has been used to accelerate the identification of variants. The algorithms employed and the attribute grammar are described in detail and their implementation is distributed as an open-source resource to allow for an easier uptake.

@&#INTRODUCTION@&#
Authority control in a catalogue is defined as the maintenance of the consistency of index terms for bibliographic material. In the particular case of the author names stored in a standard catalogue format, such as the MARC21 standard (Library of Congress, Network Development & MARC Standards Office, 2009), authority control serves two main purposes:•Distinguishing creators who have published under the same name—for example, Leopoldo Alas (1852–1901, aka Clarin) and his descendant Leopoldo Alas (1962–2008)—by adding titles or other words associated with the name, or by including information about the creator’s birth, death or active dates.Identifying variants of the name, for instance, pen names like Fígaro (Mariano José de Larra, 1809–1837) or alternative spellings like Pedro Fernandes de Queirós and Pedro Fernández de Quirós (1565–1614).Authority control is usually assisted by an index which helps cataloguers to identify previous occurrences of a name. However, this index cannot be guaranteed to be totally free from errors and the maintenance of catalogue entries is a routine task at libraries. Moreover, whenever open-access catalogues integrate records from different sources, it is possible that some libraries or cataloguing departments have employed different authoritative names. Remarkably, the metadata exchange protocols —such as OAI-MPH11Open Archives Initiative Protocol for Metadata Harvesting.(de Sompel, Nelson, Lagoze, & Warner, 2004) and SRU22Search/Retrieve via URL.(Network Development & MARC Standards Office, Library of Congress, 2007)—, neither address the quality control of metadata nor implement procedures to harmonize variants (Salo, 2009).Some frequent types of inconsistency found in catalogue names include:•Variants of the same name, like Jan Moretus catalogued sometimes as Joannes Moretus and also as Jean Moretus.Name permutations such as Carlos Borromeo and Borromeo, Carlos.Typos, for example, Alfonso Díaz de Montalvo instead of Alonso Díez de Montalvo.Stop word removal, as in Vicente Zea for Vicente de Zea or Belén Bañas replacing María Belén Bañas.Removal of diacritics, as in Schoner, Johann for Schöner, Johann or Salcedo Coronel, Garcia de for Salzedo Coronel, García.However, it is not necessarily the case that two names with minor differences in their spelling correspond to a unique person. For instance, Francisco de la Puente and Francisco de la Fuente are two different persons who coexist in the catalogue of the Biblioteca Virtual Miguel de Cervantes (BVMC, 1999).Clearly, these inconsistencies pose a challenge for the effective retrieval of bibliographic items. The application of a unique identification number for each author, such as the International Standard Name Identifier (ISO 27729), has been advocated for a long time as the solution to this problem—see, for instance, Snyman and van Rensburg (2000). Although some progress in this direction has been made (Hickey & Toves, 2014), the shared identifier approach is still far form being universally adopted and, of course, it is difficult to apply to the vast collection of former records.A large number of techniques and tools have been developed to assist the maintenance of the creator names in catalogue records (Ferreira, Gonçalves, & Laender, 2012). For instance, Chávez-Aragón, Ramirez Cruz, Reyes-Galaviz, Ayanegui-Santiago, and Portilla (2009) use a simple Euclidean distance between feature vectors to identify variants of a name. The components in the feature vector collect semantic information such as the keywords, title words or coauthor lists. A similar approach has been followed by Lee (2007) to trace evolving names and to design a system supporting the maintenance of metadata that change over time. The cosine distance (using normalized names, titles and publication venues as features) has been used instead to measure the similarity between records by de Carvalho, Ferreira, Laender, and Gonçalves (2011) and by Cota, Ferreira, Nascimento, Gonçalves, and Laender (2010).Some methods for name authority control are based on Bayesian classification (Warnner & Brown, 2001) and transform every text field into a vector of words and follow an incremental, supervised approach: a subset is manually disambiguated and then used as the seed for a clustering process guided by author names and their variants (for example, as found in the Library of Congress authority file), and also exploit contextual evidences such as the publication dates (which should be consistent with author’s lifetime). For instance, the Levy II suite (DiLauro, Choudhury, Patton, Warner, & Brown, 2001) employs an adaptive Bayesian probability model and a threshold that triggers human intervention. Tang, Fong, Wang, and Zhang (2012) applied Hidden Markov Random Fields trained with Expectation Maximization as an alternative to traditional clustering algorithms. Machine learning techniques have been also traditionally applied to this problem (Han, Xu, Zha, & Giles, 2005; Torvik & Smalheiser, 2009). More recently, Ferreira, Veloso, Gonçalves, and Laender (2014) implemented a bootstrapping procedure for those cases where training data are missing or scarce. Following this line of work, a specific similarity function, based on terms appearing in the list of coauthors, publication and venue titles, was defined (Santana, Gonçalves, Laender, & Ferreira, 2015) and used in combination with several heuristics to improve upon previous results.Alternatively, names can be considered as sequences of characters to which string similarity measures can be applied. For instance, Cohen, Ravikumar, and Fienberg (2003) compared different metrics to evaluate the similarity between author names and conclude that hybrid methods, such as the Monge–Elkan measure (Monge & Elkan, 1997), improve the results over the traditional Levenshtein distance (Levenshtein, 1966).This paper presents a method which parses temporal annotations in bibliographic records in order to disambiguate author names. Due to the weak normalization of such expressions, a variety of forms must be interpreted. We have defined for this purpose an attribute grammar (Aho, Sethi, & Ullman, 1986) which parses valid dates and gives them a non-ambiguous interpretation as a temporal range or period. This grammar, described in more detail in Section 2, interprets a date expression either as a year, a century or period and associates an uncertainty to each temporal unit. The output is later used as complementary information to check the compatibility between authors.The comparison of author names, described in Section 3, is based on simple measures for string similarity and employs a compact data structure to accelerate the search for name variants. The full method is specifically designed for those cases where additional features—such as the publication venue or the cross citations—are missing, a common case in humanistic and literary libraries such as the Miguel de Cervantes digital library (BVMC, 1999). Section 4 analyses the results obtained when the method was applied to a real collection of bibliographic records in that library and, finally, Section 5 presents the conclusions.Traditionally, birth dates, and other dates associated with the person, have been used to disambiguate authors with identical names. For example, Hickey and Toves (2014) use name and title similarity and, for people, matching birth and death dates to find clusters of matching authority records. Authority control supported by dates does not however provide a perfect solution since, on the one hand, the maintenance of date records requires consultation of diverse (sometimes divergent or evolving) references and, on the other hand, library users cannot often recall dates as distinguishing features. An additional difficulty in the exploitation of date records is that they include keywords, abbreviations and characters with special meaning which have not been fully standardized —furthermore they are often dependent on the language—, for example:•The prefixes b. (or n.) and d. (or m.) introduce birth and death dates respectively.The prefix fl. (or f.) abbreviates floruit (flourished) and precedes dates or periods when the creator is known to be active.The prefix ca. (or c.) for circa (approximately) indicates an uncertain date or period.The dash between dates, as in 1780–1833, defines a range or period.The suffix BC (and variants such as BCE, bC., aC., adC and a.J.C.) specifies a date before the common era.The suffix AD (and variants such as CE, aD., dC., dJC and d.J.C.) specifies a date in the common era.The prefix s. (sometimes followed by Roman digits) and the suffix cent. (usually preceded by ordinal) indicate a century.Ordinals are customarily used for centuries, as in 3rd, 18th and 15o.A question mark often indicates an uncertain date, for example, 1492?.The RDA consortium (Resource Description and Access, Joint Steering Committee for Development of RDA (2010)) promotes the standardization of digital descriptions. The proposed vocabulary33See, for example, http://www.rdatoolkit.org.recommends using normalized forms instead of abbreviations. For example, “century”, “approximately” and “active” should be used instead of “cent.”, “ca.” and “fl.” respectively while hyphens are to be used for date ranges instead of the “b.” and “d.” prefixes.Formal grammars (Rozenberg & Salomaa, 1997) provide a useful instrument to analyze and interpreter natural language text. In particular, an unambiguous attribute grammar for temporal expressions provides a single interpretation for every string of characters which is a valid expression and produces a non-accepted output whenever it is not well formed. In our case, four types of basic temporal expressions will be accepted: unknown, year, century, and period. Every date of a simple type (unknown, year or century) has three attributes: value, uncertainty (or delta, for short), and type. In contrast, a period will consist of a pair of dates—called, respectively, low and high components—, and has therefore six attributes (low.value, low.delta, low.type, high.value, high.delta, and high.type). In order to identify temporal expressions, some basic tokens are defined by the grammar:•A year number is a sequence of Latin digits, optionally followed by a simple or double question mark (as in 16??) denoting uncertainty.44At least a default uncertainty is assigned to every date (which should be larger for older dates) in order to allow for discrepancies between different sources of information.A century number contains either Latin digits, optionally followed by characters marking them as ordinal, or Roman numerals. They are accompanied with specific prefixes or suffixes such as cent.A period is usually denoted with a dash between dates although, sometimes, one of the components may be missing in the expression (for example, –1560), indicating an unknown higher or lower boundary.The meaning of these basic tokens can be modified by optional prefixes and suffixes:•Year and century numbers may be followed by a suffix, such as the (language-dependent) strings representing BC or AD. The BC case reverses the sign of the value attribute. Furthermore, when this modifier is applied to the second component in a period (the high date), the negative sign must be propagated also to the first component (the low date).Year and century numbers can be preceded by prefixes denoting approximate dates (for example, circa or active) which introduce additional uncertainty on the exact date.The prefixes born and dead often define implicitly a period where the exact value of one component is unknown and can be only inferred with added uncertainty.With these ingredients, an unambiguous attribute grammar is built whose rules are listed in Tables 1–3. As seen there, every production has associated a translation which transforms data into the rule’s output, that is, a temporal expression with associated uncertainty. The default uncertainty is an attribute with a value which depends on the terms found in the expression. For example, a “born” (respectively “dead”) annotation does not provide an upper (respectively lower) bound for the author’s life-time span, which is therefore estimated to extend for 60 ± 30 years, while a date such as 15?? is translated into 1550 ± 50. An example of the parse tree and the output produced by the grammar is illustrated in Fig. 1.Since every date is assigned an uncertainty, temporal annotations can be used to filter matches as a preprocess where only pairs of creators with compatible ranges are considered as candidates to matching entries: only in such case their similarity is measured with the procedure described in next section.Traditionally, similarity measures based on the edit distance (Wagner & Fischer, 1974) have been applied to identify strings (sequences of characters) which could be representations of the same entity. However, records in a catalogue have some characteristic features. For example, most standards recommend (Joint Steering Committee for Development of RDA, 2010) that titles in names—such as Saint or King—, are marked (using the $c sub-field in MARC21 or the titleOfThePerson property in RDA) even when they are in the preferred variant of the name. However, it is relatively common to find records where these type of elements have not been explicitly tagged. Some other issues which one must take care of and require some preprocessing are:•Name components with minor significance (stop-words such as de, del, de la, von, or van) are sometimes missing.Foreign characters are often normalized and punctuation and non-letter diacritics (such as ñ, ü) are substituted; furthermore, uppercase characters are sometimes lowercased (or vice-versa), mainly in titles and foreign names (as in Al-Idrissí and al-Idrīsī).Names are sometimes expressed through different rotations of the component words.In order to identify related names, their similarity can be measured using standard edit distances (Navarro, 2001) normalized to the number of words in the name. The normalization avoids penalizing long names with respect to shorter ones by assuming a small and roughly uniform probability that one word contains a typo. The higher weight of substitution operations in the indel edit distance55The indel edit distance is particular case of edit distance where the cost of a single substitution exceeds the added costs of an insertion plus a deletion.agrees with the observation that substitutions are not as frequent as accidental deletions or insertions (such as duplication of characters): preliminary results showed that substitutions accounted only for 17% of the differences found with a Levenshtein distance between variants of names in the collection. In order to deal with name permutations, the minimum distance between the first string and all word-level rotations of the second string is computed.Our procedure works incrementally by adding creators to a set of authoritative names. Before a new creator is added, the creators which are already in the authority set are examined and a list is presented containing those compatible by date with the new one and ranked by their similarity (up to a predefined threshold). The cataloguer can then mark the current entry as a new creator or assign it as a variant or preferred form of one of the names in the list.Computing the edit distance is expensive (in terms of computation time) and thus the exhaustive comparison of all pairs of strings becomes slow for large sets (for example, our collection contains over 80 thousand name entries) and more efficient procedures are called for. As shown in the extensive review by Boytsov (2011), the unigram frequency vector trie provides fast results on natural language data (comparable to forward-backward trees and length-normalized q-gram indices) for the retrieval of medium length strings (10 characters and longer) with multiple errors. This method provides quick access to data since entries are partitioned under signature vectors which only store the number of characters of each type in one text (the character unigram frequencies). Therefore, in order to accelerate the identification of similar names and to compare the effectiveness of different thresholds and variations of the edit distance, our procedure implements a radix tree storing unigram frequencies: every leaf in the radix tree contains the entries with identical unigram frequency vectors and every subtree under an internal node expands entries sharing some components (a suffix in the example depicted in Fig. 2) of a unigram frequency vector. The search consists essentially of a branch-and-bound procedure traversing the trie and pruning those subtrees where the lower bound for the distance—provided by the differences in unigram frequencies with respect to the reference string; some details of the pruning process are described in the Appendix—exceeds the maximal number of errors allowed. Every time a leaf node is reached, the pattern is compared with all the strings referenced by the node and those with an edit distance below the predefined threshold are selected.

@&#CONCLUSIONS@&#
The exploitation of temporal annotations in traditional bibliographic records requires parsing expressions which are not fully normalized and may be considered in some cases as a natural language processing task.We have implemented a system to assist the maintenance of the catalogue which exploits name similarity and temporal compatibility to suggest possible clusters of creators.The exploitation of the temporal information reduces a 97% in average the number of comparisons needed to identify variants of an author name. When this information is not available, the system employs similarity measures combined with data structures providing quick access to the data. The procedure has shown efficient enough to revise a catalogue containing above 80,000 author entries with an effort below one person-month.The precision obtained with this method depends on the threshold used for the similarity between author names: the lower the similarity threshold, the higher the number of false positives. This is a relevant question since the tool will be only used by cataloguers to find inconsistencies in the catalogue if they find that the number of false positives is a small fraction of the total candidate pairs they must revise. Fortunately, the results show that the method works with a precision above 60% when distances are not much higher than 1 character per word. This relatively low number of rejections makes the cost of using this software much lower than that of analyzing manually thousands of entries to identify duplicated authors.The full documentation and the source code are distributed under the terms of the GNU General Public License.1010https://github.com/transducens/authority.