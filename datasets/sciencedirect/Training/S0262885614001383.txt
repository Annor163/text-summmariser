@&#MAIN-TITLE@&#
Automatic usability and stress analysis in mobile biometrics

@&#HIGHLIGHTS@&#
We made usability stress tests on mobile biometrics successfully.We optimized resources through automation.Stress is not a major drawback for handwritten signature recognition.The use of colours as a feedback of the recognition benefits usability and performance.

@&#KEYPHRASES@&#
Usability,Mobile devices,Biometrics,Stress,Handwritten signature recognition,

@&#ABSTRACT@&#
Graphical abstract

@&#INTRODUCTION@&#
One of the main drawbacks users find in biometric recognition [1] systems is the lack of usability. Almost all the work done in biometrics is devoted to improving algorithm performance and bringing the Equal Error Rate (EER) close to zero. But while this kind of research is necessary, working on improving user interaction with systems is also extremely important, as a lack of usability could mean not only the rejection of the system by the users, but also a reduction in the expected performance of the biometric system. There are previous usability works in biometrics in the literature [2] and most of them come from the usability definition given by the ISO 13407:1999 [3]: “the extent to which a product can be used by specified users to achieve specified goals with effectiveness, efficiency, and satisfaction in a specified context of use”. One of the most complete models published up to now is the Human Biometric System Interaction (HBSI) [4], which proposes methods and measures (including the ones recommended by ISO 13407:1999) to analyse the user–sensor interaction deeply. As this model has not yet been tested empirically in dynamic modalities [5], our work goes a step further, proposing some modifications to it, and therefore, the results obtained can be considered a novelty. Furthermore, this study includes stress tests where users sign under pressure conditions. The inclusion of these tests in the evaluation is motivated by some common scenarios where users are indirectly encouraged to sign quickly and carelessly (e.g. post offices, banks or supermarkets). Therefore, the main intention in this study is to measure the influence of stress in the recognition process, as this is one of the major concerns regarding usability and performance. These tests mean a novelty and an important advance in the improvement of security in mobile environments. Another relevant factor recently studied in handwritten signature recognition is the effect of ageing, which has been demonstrated to decrease the performance [6] [7]. In those works authors suggest different strategies to maximize the system accuracy over time, making the template updating less critical than expected.It is important to note that the current tendency is to move from desktop computers to mobile devices, using them in mobile scenarios. Therefore the migration of biometrics to these scenarios has become an important topic nowadays. There are several published works focusing on the adaptation of biometrics to mobile devices, using different modalities such as the iris [8], hand [9] or fingerprint [10]. In our previous works with mobile devices and dynamic handwritten signature recognition [11], the algorithm applied was tested under different conditions but the evaluation of its usability was left for a future work, being covered by this paper.In this experiment, 56 users (54 finished the whole process) signed in 2 sessions on a Samsung Galaxy Note [12] using a stylus. The process was split into user training, enrolment, verification and stress tests. Finally, the users had to complete a satisfaction questionnaire where they were asked about various usability aspects of the evaluation such as easiness or comfort. All signatures captured were real signatures (i.e. not invented, as they are the same ones that the users write when shopping with a credit card).This paper is divided into 6 sections: in Section 2 the state-of-the-art is presented, while Section 3 explains the evaluation set-up, followed by a description of the experiments performed (Section 4). Results are shown and discussed in Section 5, finishing the paper with Section 6, where the conclusions obtained and the proposed future works are explained.The three best known first usability studies in biometrics were: an enrolment trial in the UK [13] conducted by Atos, the guidelines drawn up by NIST [14] and the HBSI model [4]. All of these measure the efficiency, effectiveness and satisfaction as defined by ISO 13407:1999, but HBSI goes further, focussing on the human–system interaction and the potential errors this relationship involves. Therefore, this is the baseline model applied to this work. The following subsections provide an explanation of the HBSI model and its application to handwritten signature recognition.This model includes several measurements in order to complete a full usability analysis covering ergonomics and signal processing, as shown in Fig. 1. The HBSI was applied to various usability evaluations including most of the best known biometric modalities such as fingerprints [15] or hand geometry [16]. Nevertheless, the approaches to dynamic modalities were only theoretical [3], so this work proves the practical viability of the HBSI in behavioural biometrics. Additionally, this model incorporates interaction metrics in order to categorize the FTA (Failure to Acquire) errors during the sample acquisition process [3].There are previous relevant studies in the literature about handwritten signature recognition in mobile devices [17] [18] and there are also commercial products using these kinds of algorithms [19]. Works previously carried out by authors regarding usability show interesting outcomes that have been applied to this experiment. These conclusions [8] are:Mobile deviceRegarding performance and users' opinions, capacitive devices are the preferred ones. Moreover, users prefer signing with a stylus to using the fingertip. The screen size and the device operative system were not considered as influential parameters.None of the tested user positions for signing (seated or standing up; with the device resting on a table or held by the user) involves better performance results than the others and neither does the device situation. The users' training process has been demonstrated to be a highly influential factor for both effectiveness and efficiency [20], so we have considered this process indispensable. The expected time for users to complete the evaluation was reduced in this experiment (fewer signatures and sessions) with respect to previous usability evaluations [21] as many users complained about it.This evaluation set-up is in accordance with the conclusions obtained in previous works and the necessity to accomplish users' requirements in order to develop usable systems.The experiment was divided into 2 sessions one week apart [22]. Training, enrolment and Verification 1 (V1) were done during session 1 (with pauses between them). Verification 2 (V2) and Stress-Influence Tests (SIT) were done in session 2 (also including pauses between them). Users could pause at any time to rest, except during the stress-influence testing.The evaluation crew was composed of 56 users (37 men and 19 women) chosen without any special requirement. The only condition for taking part in the evaluation was age (over 16years old as it was demonstrated that the handwritten signature is not stable in children [23]). There were 54 right-handed and 2 left-handed. Most of them (41) were between 18 and 35years old, 8 were between 35 and 50, and 7 were older than 50. Regarding the level of studies, only one user did not have a minimum qualification. The rest of them had primary education studies (2), secondary (2), high school (13) and university degrees (38). Several (29) did not have previous experience of signing on mobile devices. Almost all the users completed the evaluation successfully (54).Currently one of the leading representatives in the smartphones market, the Samsung Galaxy Note,11http://www.telegraph.co.uk/technology/picture-galleries/9818080/The-20-bestselling-mobile-phones-of-all-time.html.was chosen for this experiment. Its capacitive screen has 5.3-inches and 1280×800 pixels WXGA at 285ppi (HD Super AMOLED). Its processor is a Dual Core at 1.4GHz and it has 1GB RAM. The Android version at the time of the evaluation was 4.1 (Ice Cream Sandwich). One of the reasons for using this smartphone is its proprietary stylus used in the signing evaluation. It also covers the initial requirements fixed by previous works: capacitive screen, stylus and considered comfortable by users.None of the participants received any previous information from the operator before starting the experiment. Nevertheless, the application offered guidance during the process. At the beginning, a video explaining the whole process was shown. In addition, reminders (e.g. text messages) were shown at all the stages. The evaluation was supposed to be completed without an operator, so users were encouraged not to ask for help unless they could not continue with the evaluation.Before starting the enrolment, a training process was to be completed by users. The application required at least one accepted signature to move forward to the next stage (interface details are explained in 3.5), although the user could stay in training as long as desired. During training, all deleted and user-accepted signatures were accounted for to measure the training that each of the users needed.A dynamic handwritten signature recognition algorithm based on DTW [24] was applied in this evaluation. The input signals used were X and Y time series coordinates from two samples and the returned value was a similarity score between those two samples. That similarity score was normalized from 0 (totally different from the original) to 5 (the same signature), and this score was compared with a pre-defined threshold so as to accept the signature only if the score was higher than that threshold. The algorithm was used in two different situations. First, it was embedded in the mobile device application for giving feedback to users in real time (only genuine comparisons); this is explained in Section 3.4.1. Then, once all signatures had been acquired from all the subjects in the test crew, the algorithm was also used to provide the error rates FRR and FAR; this is explained in Section 4.The algorithm applied was translated into Java for Android [25] and embedded in the smartphone. It was used in the mobiles' application to calculate similarities between the signatures as feedback for users just after performing a signature and pressing the “Accept” button. The use of the algorithm differs from the enrolment in the verification.During enrolment, the algorithm compares each signature with all of those previously acquired, starting with the second one (2nd vs 1st; 3rd vs 2nd and 1st; 4th vs 3rd, 2nd and 1st; and 5th vs 4th, 3rd, 2nd and 1st) and returns the best result. This verification is performed in order to check if the user is making a different signature or if the acquisition process has not captured a significant number of sample points. As a result, the application shows a red or green square above the signing space as feedback to users. The feedback square becomes red if the similarity score is under the threshold 3; otherwise it becomes green. If the provided signature returns a similarity score below 3, users are required to repeat the process. If after the third attempt to acquire a signature for the enrolment, the similarity score is not at least 3, the enrolment ends and the user is not allowed to complete the evaluation, increasing the rate of Failure to Enrol (FTE).After enrolment, the verification stages occur. In these phases, the algorithm compares each signature with the five signatures obtained through the enrolment and returns the best result. In this case, if the signature is under the threshold, the error is stored and the user shall not repeat the attempt, but continues with the next one.The application guides the user through the different menus intuitively, so the user is supposed to complete the experiment without additional help. As the application has been executed in Spain, all guidance is written in Spanish, and therefore the captured screens are in that language. These screens fit with the evaluation roadmap pictured in Fig. 2. All the signatures and data gathered were locally stored in the mobile device until the end of the evaluation. Once the experiment was finished they were transferred to a PC in order to obtain the final performance and usability rates. As in a previous approach to a real application, critical concerns for a final implementation such as security have been untreated because the main target is the usability analysis.As previously mentioned, the first session starts with a video explaining the whole process in detail. Then, users have to accept the evaluation conditions, covering National Data Protection Law requirements, as well as a participation agreement. After this, the user introduces his personal data, including name, surname, age range, profession, gender and laterality. A personal number is generated for each user, keeping personal data unlinked to signatures.The next step is the training process where the user can practise until he feels ready to start the evaluation. The training screen has 3 buttons (accept, delete and continue) and a blank space to sign. All the deleted and accepted signatures are counted at this point. Once the user feels comfortable with the system, the enrolment starts and he is required to provide 5 signatures correctly (as mentioned above).Next, V1 starts and 12 signatures are requested. The verification screen is similar to the enrolment one: 2 buttons (accept and delete) and a blank space for signing. A screenshot is shown in Fig. 3. This is the end of session 1.This session starts with the V2 phase, which follows the same process as the V1. Once the user completes 12 signatures again, the SIT starts, by showing a new screen with the message “the interface will change slightly, please keep signing normally like up to now”. Then, the interface starts blinking, changing between yellow and red. At the same time, an intermittent annoying sound is played loudly and a countdown from 5 to 0 starts. When the countdown reaches 0 a text message appears saying “you are so slow, please go faster”. All these signals are intended to provoke stress in the users. At this stage, no feedback about the signature similarity is provided and only when the user completes 12 signatures does the stress test finish. When all the signatures are completed, a satisfaction questionnaire is provided to the user to express his opinion about several aspects of the test and the evaluation ends.A user can finalize each signing attempt at any time either in V1 or in V2. The signing process finishes by accepting or deleting the signature performed. Once the delete button is pressed, a pop-up window with the following options is shown:–Why did you delete the signature?oI did not like itI repeated strokesI made it partially in the airI placed my wrist on the screenI made it out of the blank spaceIf the user presses the accept button, the algorithm works as explained in Section 3.4.1. Furthermore, if the similarity score obtained is 1 or less (in the 0–5 scale), a new pop-up window appears, asking the user whether the signature is correct or not. By pressing yes, the user continues with the process and pressing no means repeating the signature. These steps were made in order to categorise both errors and deleted signatures into the HBSI metrics.The intention of this work was to save resources during evaluation, such as video recording and data processing, making the system automatic. Recording the whole evaluation on video is a method used in the HBSI to better understand the user–system interactions. In such a case, at the end of the evaluation, operators have to review the videos carefully to categorise interaction errors. This process takes a very long time and requires several personnel to minimize categorisation errors during video replaying. In this work, it is the user who decides whether a signature is correct or not (deleting or accepting the signature), and also the reason for deleting. Therefore, the categorisation is automatic. This decision process proposed by the HBSI was modified and it is shown in Fig. 4.The experiment involves measuring several usability parameters. This evaluation returns three kinds of outcomes: HBSI rates, stress-influence and system performance. As these outcomes are quite inter-related, this analysis is done separately first and the correlations found are then analysed in the conclusions section.Following the diagram shown in Fig. 1, the HBSI model is divided into usability, ergonomics and signal processing. Therefore, the test done is in accordance with these three categories.The usability analysis included in HBSI considers the three main parameters proposed by the ISO 13407:1999, satisfaction, efficiency and effectiveness:–Satisfaction: Defined as the percentage of satisfied users. This parameter is measured and studied through the satisfaction questionnaires. The questions concerning satisfaction included in the experiment questionnaire are:•Would you use this system in your daily life?Do you consider the received instructions enough?Time spent. Score 0–5 (0—very annoying to 5—very satisfactory)Easiness. Score 0–5 (0—very annoying to 5—very satisfactory)Privacy. Score 0–5 (0—very annoying to 5—very satisfactory)Global opinion of the evaluation. Score 0–5 (0—very annoying to 5—very satisfactory)Intrusion (how intrusive the application is). Score 0–5 (0—very annoying to 5—very satisfactory)Efficiency: The time spent on performing tasks. When a user deletes a signature, it also decreases the efficiency as the time performing that task is increased. For that reason, in this particular case the efficiency is calculated with two parameters: the time spent signing and the rate of non-deletions:Non‐DeletionsRate=1−NumberofdeletedsignaturesTotalnumberofsignatures×Users×100.This involves a change to the HBSI proposal, as the possibility of deleting an acquired sample is not considered in HBSI.Effectiveness: Defined as the task completion rate by users.TaskCompletion=NumberofusersabletocompletetheprocessTotalnumberofUsers×100.These three measurements are inter-related as the decrease of any of them usually involves the decrease of the rest of them (i.e. when a user deletes a signature several times the efficiency decreases and at the same time the user feels annoyance so the satisfaction decreases too).Ergonomics in the HBSI includes cognitive and physical categories.–Cognitive: Defined as the percentage of users that1.Know how to use the capture sensor. Obtained in session 1Learn how to use the capture sensor (also known as learnability). This is obtained in session 2 once the users have acquired skills previously in session 1Remember how to use the capture sensor. This parameter is also observed in session 2.Physical: The percentage of users that can use the capture sensor.This measurement includes sample quality metrics and processing capability (number of segmentations and feature extraction errors). In the handwritten signature recognition state-of-the-art there is no method to measure the sample quality, so this parameter was not measured in this evaluation. However, as aforementioned, a similarity metric between signatures was given to users as feedback by the mobile application. There are only a few works using similarity metrics as a measure of quality in the state-of-the-art [26].The HBSI metrics are divided into two types depending on whether the presentation is successful or not [3]. As already mentioned in the previous section, the application allows the user to delete signatures, and also asks for a reason for doing so. This, together with the newly defined flow chart, limits the possibilities of HBSI metrics to the following two:–FI (False Interaction): “Incorrect presentation is detected by the system and classified as correct”.CI (Concealed Interaction): “Incorrect presentation is detected by the system, but not classified as correct”.In the handwritten signature recognition state-of-the-art there is no method to measure the sample quality to determine its correctness, so new definitions for FI and CI are needed. It is important to note that the user is aware of whether the signature meets the similarity requirements or not through the feedback provided by the application. Therefore we propose the following definitions:–FI (False Interaction): “Presentation with a similarity score below the defined threshold that is classified by the user as correct (accepted)”.CI (Concealed Interaction): “Presentation with a similarity score below the defined threshold that is classified by the user as incorrect (deleted)”.This kind of measurement is not included in the HBSI model, as there are no measurements related to the user's mood, so it is necessary to include them in the evaluation.It is important to remember that during the acquisition of signatures under stressful conditions, no feedback on the similarity of the signatures is provided. Then, these signatures are used to calculate performance, being compared with the ones obtained in V1 and V2, extracting from these the new metrics for the influence of stress on the biometric behaviour.Most of the main outcomes of the evaluation are extracted from the performance results, which are the following:–Error rates in V1, obtaining the FRR when comparing each of the 12 genuine samples with the corresponding template, and doing this with each of the users enrolled in the system. The FAR is obtained by comparing the template from each user with the whole set of 12 signatures of the other 53 users, which are considered here as impostor signatures.–Error rates in V2. Learnability information is obtained by comparing these rates with the ones obtained during the V1. The FAR and FRR are calculated in the same way as in V1.Error rates under stress conditions. Information about the influence of stress is obtained by comparing these rates with the two previous rates. The FAR and FRR are calculated in the same way as in V1 and V2.Error rates of the whole system in normal conditions: considering the signatures of the V1 and the V2 jointly. The FRR and the FAR are obtained in the same way as the V1, V2 and SIT, but considering 24 signatures instead of 12.Error rates considering the signatures from the V1, V2 and SIT jointly. This reports a result of a possible real environment where the stress factor is present sometimes. The FRR and the FAR are obtained in the same way as the V1, V2 and SIT but using 36 signatures instead of 12. The complete set of error rates is summarized in Table 2.

@&#CONCLUSIONS@&#
Several conclusions have been extracted from this experiment. As the HBSI model was the starting point to design the usability evaluation, let us proceed with the conclusions related to the HBSI first.The whole model was applied to this evaluation including the proposed metrics. However, because this is the first empirical approach to the analysis of a behavioural modality, the metrics adopted had to be adapted to this new context. These changes are impacted by the fact that the application allowed users to interact freely with the system (i.e. deleting signatures) and by the impossibility of knowing whether a signature is correct or not. Then, it is noticeable that the HBSI flow chart to decide the result metric is highly dependent not only on the modality but also on the characteristics of the evaluation. The ergonomic results demonstrate that the application is easy to use and the procedure is easy to learn and remember, as only two users could not complete the experiment. The sample similarity metrics applied as users' feedback provided some interesting outcomes in various aspects: users proved to be comfortable with an acknowledgement of their signature and there were not too many errors. Furthermore, similarity scores on average were quite good in all the phases.The aim of causing stress in users was achieved, as all of them were anxious to finish this phase (as they expressed at the end of the experiment). In addition, the time spent completing the signatures was reduced ostensibly and the similarity scores were worse than in other phases. Nevertheless, the error rates obtained in the SIT reveal better performance than the V2 at several operating points in the DET curves, indicating that the stress factor is not a major drawback for recognition. Regarding general performance results, these are similar to those obtained in our previous works [8] with the same mobile device (EER=0.17%). Note that, in this case, most of the users had no previous knowledge of signing on mobile devices. According to the good results obtained, both in usability and performance, authors recommend the use of handwritten signature recognition in mobile devices as a trustable and comfortable method for biometric recognition.