@&#MAIN-TITLE@&#
On the impact of excitation and spectral parameters for expressive statistical parametric speech synthesis

@&#HIGHLIGHTS@&#
Analysis of speech parameters for expressive statistical speech synthesis.Different spectral and excitation features analyzed.Spectral parameters have bigger impact on the expressive synthetic speech.

@&#KEYPHRASES@&#
Speech synthesis,Statistical parametric speech synthesis,Expressive speech synthesis,Speech parameterization,

@&#ABSTRACT@&#
This paper presents a study on the importance of short-term speech parameterizations for expressive statistical parametric synthesis. Assuming a source-filter model of speech production, the analysis is conducted over spectral parameters, here defined as features which represent a minimum-phase synthesis filter, and some excitation parameters, which are features used to construct a signal that is fed to the minimum-phase synthesis filter to generate speech. In the first part, different spectral and excitation parameters that are applicable to statistical parametric synthesis are tested to determine which ones are the most emotion dependent. The analysis is performed through two methods proposed to measure the relative emotion dependency of each feature: one based on K-means clustering, and another based on Gaussian mixture modeling for emotion identification. Two commonly used forms of parameters for the short-term speech spectral envelope, the Mel cepstrum and the Mel line spectrum pairs are utilized. As excitation parameters, the anti-causal cepstrum, the time-smoothed group delay, and band-aperiodicity coefficients are considered. According to the analysis, the line spectral pairs are the most emotion dependent parameters. Among the excitation features, the band-aperiodicity coefficients present the highest correlation with the speaker's emotion. The most emotion dependent parameters according to this analysis were selected to train an expressive statistical parametric synthesizer using a speaker and language factorization framework. Subjective test results indicate that the considered spectral parameters have a bigger impact on the synthesized speech emotion when compared with the excitation ones.

@&#INTRODUCTION@&#
The research on expressive speech synthesis has gained considerable importance in the last few years. Usually, controllability of the expression of the synthetic speech according to the user's intentions or feedback is desirable. To achieve this, text tags can be used to represent the user's input. These tags are eventually conveyed to the frontend of a text-to-speech system to synthesize speech with the desired style. The synthesis of the speech signal with a desired style or expression can be highlighted depending on the choice of prosodic, spectral and excitation parameters, since some parameters are more emotion dependent than others (Maia and Akamine, 2012). It is common knowledge that long-term prosodic information have a significant effect on the emotion of the synthesized speech, e.g. (Tao et al., 2006). However, when it comes to the family of statistical parametric speech synthesizers (Zen et al., 2009) the question to be asked is how short-term spectral and/or excitation parameters, the commonly used features which represent a source-filter model of speech production (Deller et al., 2000), affect the desired style or expression of the synthesized speech.Some studies have reported on the connection of glottal source parameters with the speaker identity (Plumpe et al., 1999), the speaker's phonation style (Airas and Alku, 2007), and the speaker's emotion (Airas and Alku, 2006; Székely et al., 2011). These analyses have been mostly based on parameters of the Liljencrants–Fant (LF) model (Fant et al., 1985) or the normalized amplitude quotient, extracted directly from samples of glottal pulses (Alku and Backstrom, 2002). As for the short-term spectrum, in (Salvi et al., 2010) a cluster analysis indicates that the spectral envelope changes when speech shifts from neutral to sad or happy states, and in (Přibilová and Přibil, 2009) a spectral modification approach for unit concatenation-based expressive speech synthesis is proposed. These works indicate that there might be a connection between short-term spectral and excitation parameters and the speaker's emotion under the framework of statistical parametric speech synthesis.This paper presents an investigation on the importance of the choice of the spectral and excitation parameterization for conveying the appropriate style in emotional statistical parametric speech synthesis. Spectral parameters commonly used, such as Mel cepstrum and Mel line spectral pairs (LSP) were analyzed. Among the excitation parameters, features related to the breathiness of speech and that control the mixture of voiced and unvoiced components, such as the band-aperiodicity parameters (Zen et al., 2007) were taken into account. In addition, parameters that are related to the glottal flow, such as the anti-causal cepstrum (Quatieri, 2002) and time domain group delay (Banno et al., 1998) were also considered. The analysis is conducted on a high-quality emotional corpus, recorded on studio by a professional actress. Initially, analyses using K-means clustering and Gaussian mixture models (GMMs) are employed to determine the most emotion dependent features among the ones considered. Based on this analysis, the parameters that have the strongest connection with the speaker's emotion are then utilized to train an expressive statistical parametric synthesizer based on a speaker and language factorization (SLF) framework (Zen et al., 2012). The SLF framework allows better estimation of the statistical models by combining all the common factors on the emotional data (Wan et al., 2012) through the combination of cluster dependent decision trees (Yu et al., 2011). The synthesis of speech with different styles at the stream level can be performed by selecting the cluster weights appropriately (see Section 4.6.1 for a short explanation of SLF). Finally, subjective tests were conducted to verify whether spectrum or excitation parameters have significant influences on the speaking style of the synthesized speech.This work is a follow up to our previous study on this subject (Maia and Akamine, 2012). This time the parameters are extracted with higher precision by using higher complex cepstrum analysis orders followed by frequency warping. The group delay feature is calculated directly from the complex cepstrum for consistency and to avoid phase unwrapping mistakes. Furthermore, a different emotional database with higher quality, recorded by a professional actress with supervised distinct emotions is utilized here. Finally, in order to synthesize speech with higher quality by combining the common properties of all the emotions, the SLF framework was used rather than emotion dependent models, trained on small databases, as performed in Maia and Akamine (2012). In this way, the artefacts related to the poor quality of training on a small corpus are reduced, and focus can be directed to the speaking style itself during the subjective evaluation.The rest of this paper is organized as follows. Section 2 describes the procedures of analytical tests of emotion dependency based on K-means clustering and GMM; Section 3 introduces the assumed speech model, shows how speech can be factorized using complex cepstrum analysis, describes the spectral and excitation parameters utilized in the analysis, and discuss their relationship with the speech production mechanism. Section 4 shows experimental results, in Section 5 we discuss the experimental results, and the conclusions are presented in Section 6.Two different methods are employed to analyze which features are more dependent on the speaker's emotion: (1) K-means emotion clustering; (2) GMM emotion classification.Fig. 1illustrates how we can utilize the K-means clustering algorithm (Bishop, 2006) to obtain an index of separability of emotions according to the parameters utilized in the clustering. Assuming thatX=x0,…,xT−1corresponds to short-term speech parameters extracted from the entire emotional data (including all the emotions), with T being the number of frames,Xis split into K clusters Ω={Ω0, …, ΩK−1} so as to minimize the following distortion(1)D=argminΩ∑k=0K−1∑xt∈Ωkxt−vk2,wherevkis the centroid in Ωk. Since the intention is to verify how the speech features represented byXare dependent on the speaker's emotion, the number of clusters K must be set to the number of emotional classesC=C0,…,CK−1.The performance of the clustering, which will tell us how separable are the emotions, can be measured by the normalized mutual information (Estévez et al., 2009),(2)U(Ω;C)=2I(C;Ω)H(C)+H(Ω),where(3)I(Ω;C)=∑k=0K−1∑i=0K−1p(Ωk,Ci)log2p(Ωk,Ci)p(Ωk)p(Ci),is the mutual information betweenCand Ω, and(4)H(Ω)=∑k=0K−1p(Ωk)log2p(Ωk),(5)H(C)=∑i=0K−1pCilog2pCi,are respectively the entropies of Ω andC. A highU(Ω;C)indicates thatΩ≈C, and consequently the emotions are separated through the clustering process.In GMM modeling (Reynolds and Rose, 1995), the likelihood that a feature vectorxtbelongs to a given classC, represented by the GMM model Λ={κj,mj, ϒj} is(6)p(xt∣C)=∑j=0J−1κjN(xt∣mj,ϒj),whereNx∣m,ϒmeans a Gaussian distribution ofxwith mean vectormand covariance matrix ϒ. In (6),mjand ϒjare respectively the mean vector and covariance matrix of the jth Gaussian component of Λ, κjis its respective weight, and J is the number of components.Fig. 2illustrates our GMM-based approach to measuring which features are more emotion dependent. For each feature, emotional GMMs are separately trained. By using the emotional test data, a confusion matrix can be constructed by evaluating the likelihood of each test data given each GMM, where the more diagonal is the matrix the more emotion dependent is the feature. Each element of a feature dependent confusion matrix can be calculated as(7)L(s,g)=∑p∈PL(p)(s,g),withL(p)(s,g)being the likelihood of the emotional test datasgiven the emotional GMMgfor phonep, and(8)L(p)(s,g)=∑t=0Z−1ln∑j=0J−1κj(g,p)Nyt(s,p)∣mj(g,p),ϒj(g,p),where Z is the number of frames (the same for all test data),yt(s,p)is the tth feature vector of phonepin emotions, andκj(g,p),mj(g,p)andϒj(g,p)are respectively the jth mixture weight, mean, and covariance of emotional GMMgof phonep.Pis the phone set on which the analysis is performed.Frame accuracy identification rates can also be used to confirm the results inferred from the confusion matrices. The average frame accuracy for each emotion on phonep,d¯(s,p), according to each feature can be calculated as(9)d¯(s,p)=∑t=0Z−1d(s,p)(t)Z,where(10)d(s,p)(t)=1,s=argmaxgpyt(s,p)∣Λ(g,p),0,s≠argmaxgpyt(s,p)∣Λ(g,p),andΛ(g,p)=κ0(g,p),…,κ0(g,p),m0(g,p),…,mJ−1(g,p),ϒ0(g,p),…,ϒJ−1(g,p).The parameters used in this work compose a digital source-filter model of speech production (Deller et al., 2000).We assume a model in which a two-pitch period speech, s(n), segment centered at the glottal closure instant, is produced by the convolution of a causal minimum phase impulse response, hm(n), and excitation signal, e(n)(11)s(n)=hm(n)*e(n),where hm(n) accounts for the effects of the vocal tract resonance, glottal flow spectral tilt and lip radiation. The excitation signal is assumed to be composed of a mixture of pulse and noise(12)e(n)=ha(n)*hv(n)*t(n)+hu(n)*w(n),where ha(n) is a non-causal all-pass impulse response accounting for the phase information of the glottal flow, t(n) is an Dirac sequence representing the voiced portion of the excitation, andw(n)is a white noise sequence with power one and mean zero representing unvoiced portion of the excitation. The zero-phase impulse responseshv(n)and hu(n) control the mixture between pulse and noise and are derived from aperiodicity measures in the frequency domain (Fujimura, 1968; Kawahara et al., 2001). Fig. 3shows the speech model, which is widely used in statistical parametric synthesizers (Zen et al., 2007). Assuming that s(n) is mostly mixed phase (Deller et al., 2000; Quatieri, 2002; Rabiner and Schafer, 1978), the all-pass filter, Ha(z), contains the additional phase information that when added to the minimum-phase phase response of the synthesis filter, Hm(z), results in the phase response of the speech signal, s(n).In this work complex cepstrum analysis is used to derive the impulse responses hm(n) and ha(n).Synthesis filter and glottal flow parameters can be derived by complex cepstrum analysis (Quatieri, 2002). Assuming that s(n) is a two-pitch segment of speech, selected through an appropriate window with center at the glottal closure instant (GCI), the complex cepstrum of s(n) is given by(13)hˆ(n)=12πlnSe0+2∫0+πlnSeωcosθ(ω)+ωndω,for −C≤n≤C, where C is the cepstral order, andSeωis the discrete-time Fourier transform (DTFT) of s(n), withθ(ω)=∠Seω. For (13) to be valid,Seωmust be even on the frequency axis, andθωmust be odd, continuous (unwrapped), andθ0=θπ=022Actually, θ(0)=0 only if ∑ns(n)≥0, and θ(π)=0 only if∑nevens(n)−∑nodds(n)≥0, respectively., which is true when s(n) is a real sequence (Maia et al., 2013a).The cepstral coefficientshˆ(n)encapsulate the effects of the vocal tract resonance, glottal flow, and lip radiation.hˆ(n)can be converted to a corresponding impulse response, h(n), through the following operation(14)h(n)=12π∫−ππexp∑k=−CChˆ(k)e−ωk+ωndω,−N2≤n≤N2,where N is the impulse response order. Fig. 4shows examples of natural and synthetic waveforms for the vowel “e” uttered on different emotions. The synthetic waveforms were produced through overlap-and-add of h(n) at the corresponding analysis instants. Note that the similarity between the natural and synthetic versions indicates that most of speech information is indeed represented by h(n).33The reader may be interested to compare with the waveforms in the right side of Fig. 5, produced through minimum-phase synthesis.However, for statistical parametric synthesis it is interesting to factorize h(n) because the glottal flow excitation and the vocal tract resonance play different roles in the speech production mechanism, and consequently should have separate decision trees (Maia et al., 2013a). In the following we show two factorizations of the complex cepstrum that can be suitable for statistical parametric synthesis.Any stable system represented by its impulse response h(n) can be decomposed as a convolution of a minimum-phase and an all-pass impulse responses (Oppenheim, 2010): h(n)=hm(n)*ha(n). In the cepstral domain this convolution becomes a summation:hˆ(n)=hˆm(n)+hˆa(n). Assuming that the complex cepstrum of a minimum-phase sequence44Commonly referred to as the minimum-phase cepstrum.with the same amplitude response is given by (Oppenheim, 2010)(15)hˆm(n)=0,n<0,hˆ(0),n=0,hˆ(n)+hˆ(−n),1≤n≤C,then the so-defined all pass cepstrum becomes(16)hˆa(n)=hˆ(n)−hˆm(n)=hˆ(n),n<0,0,n=0,−hˆ(−n),n>0,where it can be noticed thathˆa(n)contains solely samples of the anti-causal cepstrum, i.e.hˆ(n)∣n<0. The literature shows that the anti-causal cepstrum is one possible representation of the glottal pulse, e.g. (Drugman et al., 2011). However, for this decomposition the all-pass impulse response ha(n) represents the phase information of the glottal excitation since its amplitude response is one.Fig. 5shows examples of waveforms created through the overlap-and-add of ha(n) and hm(n) at the GCIs, for the vowel “e” uttered in different emotions. The gain term has been removed by makinghˆ(0)=0. It can be seen that in this case the responses to the minimum-phase filters produce waveforms with more distinctness across the emotions. Amplitude spectra of the waveforms in Fig. 5 are shown in Figs. 6and 7.Another way to factorize the complex cepstrum is simply through its anti-causal/causal components(17)hˆac(n)=hˆ(n),−C≤n<0,0,0≤n≤C,(18)hˆca(n)=0,−C≤n<0,hˆ(n),0≤n≤C.In this case it can be shown that the impulse response hac(n) is related to the glottal flow. Fig. 8shows examples of waveforms created through the overlap-and-add of hac(n) and hca(n) at the GCIs, for the vowel “e” uttered in different emotions. The gain term has been removed by makinghˆ(0)=0. It can be noticed that in this case the glottal pulses produced by hac(n) seem more distinctive across the emotions. Amplitude spectra of the waveforms in Fig. 8 are shown in Figs. 9and 10, where the spectral tilt of the of the amplitude response of hac(n) changes from the more intense emotions: anger and happiness, to the least intense ones, neutral and sadness.Basically, the difference between the two factorizations regards the effect of the glottal flow. In the all-pass/minimum-phase decomposition the glottal flow amplitude response is included in the minimum-phase cepstrum, while the all-pass cepstrum represents the phase response of the glottal flow. In the causal/anti-causal decomposition the separation between vocal tract and glottal flow parameters is more explicit.55A more detailed comparison of the two factorizations in terms of amplitude and phase responses can be found in Appendix A.In this work the spectral parameters represent the minimum-phase impulse response hm(n). Two different parameterizations for hm(n) are considered: the Mel cepstrum and the Mel line spectral pairs.The minimum-phase filter can be represented by M+1 Mel cepstral parameters,c˜(0),…,c˜(M)as follows(19)Hmeω=exp∑n=0Mc˜(n)e−ω˜n,whereHmeωis the frequency response of the minimum-phase filter. The warped angular frequencyω˜can be regarded as the phase response of an all-pass filter whose input is ω (Oppenheim and Johnson, 1972)(20)ω˜=(1−α2)sinω(1+α2)cosω−2α,with α controlling the degree of warping. For α=0.42, (20) results in a good approximation to the Mel scale (Fukada et al., 1992).Mel cepstral coefficients can be obtained from the complex cepstrum through two simple steps. Firstly, a minimum phase cepstrum,hˆm(n), is obtained from the complex cepstrum according to (15). After that, the sequencehˆm(0),…,hˆm(C)can be warped onto the Mel cepstral parameters,c˜(0),…,c˜(M), by using the following recursive formula(21)c˜(i)(n)=hˆm(−i)−αc˜(i−1)(0),n=0,1−α2c˜(i−1)(0)−αc˜(i−1)(1),n=1,c˜(i−1)(n−1)−αc˜(i−1)(n)−c˜(i)(n),2≤n≤M,for −C≤i≤0, where i is the recursion index. The recursive formula above is a direct application of the bilinear transform to a causal sequence (Oppenheim and Johnson, 1972; Tokuda et al., 1994).The minimum-phase synthesis filter can also be represented by M Mel linear prediction coefficients,a˜(1),…,a˜(M), and a gain term σ, as follows(22)Heω=σ1−∑n=1Ma˜(n)e−ω˜n=σAeω,whereω˜is the warped angular frequency as shown in (20), where α=0.42 gives a good approximation to the Mel scale for speech sampled at 16kHz.A vector of Mel linear prediction coefficientsa=[a˜(1)⋯a˜(M)]⊤can be derived from a speech segment s(n) by making(23)a=argmaxaϵ,where the cost function ϵ is given by (Markel and Gray, 1982)(24)ϵ=12π∫−ππSeω2Hmeω2−lnSeω2Hmeω2−1dω,withSeω2andHmeω2being respectively the power spectrum of the sequences s(n) and hm(n). The gain σ is the power of the residual – what is left after passing the speech signal s(n) through the inverse filterAeω(25)σ=12π∫−ππSeωAeω2dω.The line spectral pairs (LSP) basically represent an efficient form of encoding the linear prediction coefficients. The LSP usually present better quantization and interpolation properties (Kabal and Ramachandran, 1986). They also have a strong relationship with the formants of the short-term spectrum of s(n) (Kleijn and Paliwal, 1995). Considering the M-order all pole model of (22), two polynomials of order M+1 can be created in the warped z transform domain as follows(26)Pz˜=1−∑n=1Ma˜(n)−a˜(M+1−n)z˜−n+z˜−M−1,(27)Qz˜=1−∑n=1Ma˜(n)+a˜(M+1−n)z˜−n+z˜−M−1,wherez˜=reω˜n. The zeros ofPz˜andQz˜are interlaced on the warped unit circle (Kabal and Ramachandran, 1986). By eliminating the obvious zeros atz˜=±1, and by counting only the zeros at the first and second quadrants (the other ones can be given by their complex conjugates), the Mel LSP are the corresponding angular frequencies of the zeros ofPz˜andQz˜. The zeros ofPz˜andQz˜can be found through a simple root-finding algorithm (Kleijn and Paliwal, 1995).Improved speech models that use sophisticate excitation signals have been largely used in statistical parametric synthesizers recently, e.g. (Yoshimura et al., 2001; Zen et al., 2007; Maia et al., 2007; Cabral et al., 2007; Drugman et al., 2009; Raitio et al., 2011). Some methods are based on models of the glottal flow, e.g. (Cabral et al., 2007; Raitio et al., 2011), others attempt to model the residual signal, e.g. (Maia et al., 2007; Drugman et al., 2009), and others rely on the application of vocoding techniques to statistical parametric synthesis, e.g. (Yoshimura et al., 2001; Zen et al., 2007). According to (12), in this work the excitation signal consists of: (1) glottal flow phase representation through the filter ha(n); (2) mixture of the voiced and unvoiced excitation components through the zero-phase filtershv(n)and hu(n), respectively. In the following we present two parameterization for the phase information of the glottal flow: the anti-causal cepstrum and the time domain group delay. Finally, the band-aperiodicity parameters is used for controlling the breathiness of speech.We have seen in Section 3.2 that the anti-causal cepstrum contains information on the glottal flow. At synthesis time this information can be used to implement the all-pass filter Ha(z), as shown in Section 3.2.1. Alternatively, the same set of parameters can be used to implement a “glottal filter”, according to shown in Section 3.2.2. In the first case, the amplitude response of ha(n) would be one, and it can be demonstrated that its phase response would be66See Appendix A for more details.(28)θa(ω)=2∑n=1Chˆ(−n)sinωn.In the second case, the amplitude and phase responses of hac(n) would be, respectively(29)Haceω=∑n=1Chˆ(−n)cosωn,(30)θac(ω)=∑n=1Chˆ(−n)sinωn.Therefore, in order to account for the glottal excitation, we define some parameters ϕ(n) as(31)ϕ(n)=hˆ˜(−n−1),0≤n≤M−1,wherehˆ˜(n)is the complex cepstrum warped onto the Mel scale. ϕ(n) can be obtained through three steps: (1) derive the all-pass cepstrum,hˆa(n), fromhˆ(n)according to (15) and (16); (2) warp its causal part,hˆa(0),…,hˆa(C), onto M+1 parameters,hˆ˜a(0),…,hˆ˜a(M), by using the recursive formula of (21) (sincehˆa(n)is anti-symmetrical); (3) regard the glottal parameters asϕ(n)=hˆ˜a(n+1), for 0≤n<M.The group delay represents the rate in which a given phase response θa(ω) changes on frequency, and here is used to represent the phase response of the all-pass filter Ha(z)(32)τa(ω)=−dθa(ω)dω.τa(ω) can be calculated directly fromHaeωby making (Oppenheim, 2010; Banno et al., 1998)(33)τa(ω)=−ReHaeωImHa′eω−ImHaeωReHa′eωHaeω2,withRe·andIm·meaning respectively the real and imaginary parts of the respective arguments, andHa′eωbeing the derivative ofHaeωwith respect to ω,(34)Ha′eω=d∑n=−M/2M/2ha(n)e−ωndω=−∑n=−M/2M/2nha(n)e−ωn.The term∑n=−N/2N/2nha(n)e−ωnis the discrete-time Fourier transform of the sequence nha(n). In a similar way to the approach proposed by Banno et al. (1998), we utilize the group delay in the time domain as a phase feature(35)ν(n)=12πτa(0)+2∫0+πτa(ω)cosωndω,0≤n≤M−1,where the dimension M controls the smoothness of ν(n).The filtershv(n)and hu(n) control the mixture of voiced and unvoiced excitation in e(n), respectively. Their frequency responses are given by, respectively(36)Hveω=1−b(n),∣ω∈Δωn,(37)Hueω=b(n),∣ω∈Δωn,where the band-aperiodicity coefficients, b(n), are calculated as(38)b(n)=∑ω∈Δωnρ(ω)SeωSeω,0≤n≤B−1,with Δωnbeing the nth pre-defined spectral bandwidth, and B the number of bands.ρωis an aperiodicity index at frequency ω, and among several possibilities (e.g. Kawahara et al., 2001), they can also be derived as(39)ρ(ω)=SueωSueω+Sveω,withSueωandSveωbeing respectively the amplitude spectra of the unvoiced and voiced speech components.

@&#CONCLUSIONS@&#
An analysis of the impact of short-term parameterizations for expressive statistical parametric speech synthesis has been presented. Two parameterizations for the minimum-phase synthesis filter, regarded here as spectral parameters, were tested: the Mel cepstrum and the Mel line spectral pairs. Three parameters that are used to construct the excitation signal were also analyzed: the anti-causal cepstrum, which is a representation of the glottal flow; the time domain group delay, which represents solely the phase response of the glottal flow; and the band-aperiodicity parameters, which are related to the breathiness of the speech signal. Ideas for analyzing the connection of each one of these parameters with the speaker's emotion were proposed, based on K-means emotion clustering and GMM emotion identification. The results of this analysis indicated that: (1) spectral parameters are more emotion dependent than parameters that are used to construct the excitation signal; (2) the Mel line spectral pairs appear as the most emotion dependent features; (3) the band-aperiodicity parameters have the strongest connection with the speaker's emotion among the excitation parameters. Subjective results on speech synthesized by a system trained under a speaker and language factorization framework have shown that: (1) Mel LSP are more important for enhancing expressiveness of synthesized speech than excitation parameters; (2) the impact of the anti-causal cepstrum and band-aperiodicity parameters on the speaking style is not as relevant as that of the parameters of the minimum-phase synthesis filter.Future work includes the use of complex cepstrum analysis methods that are less sensitive to the GCIs (Maia et al., 2013b), and use of other parameterizations for the glottal flow.