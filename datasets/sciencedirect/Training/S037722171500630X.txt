@&#MAIN-TITLE@&#
An alternative approach for proving the NP-hardness of optimization problems

@&#HIGHLIGHTS@&#
A new approach for proving the NP-hardness of optimization problems is proposed.The new approach includes the “classical” approach as a special case.NP-hardness of a problem is proved, for which the classical approach is not applicable.Polynomially solvable problem with NP-hard computable objective is formulated.

@&#KEYPHRASES@&#
Combinatorial optimization,Computational complexity,Scheduling,

@&#ABSTRACT@&#
We provide a new reduction-based approach for proving the NP-hardness of optimization problems and establish that it includes the “classical” approach as a special case. We apply our alternative approach to prove the NP-hardness of a problem for which the classical approach is not applicable. Besides, we construct a special case of the problem with the property that finding an optimal element takes polynomial time despite that computing the objective function values is NP-hard.

@&#INTRODUCTION@&#
We consider a general combinatorial optimization problem Poptin the following setting, which is consistent with the definitions given in standard texts such as Garey and Johnson (1979), Papadimitriou and Steiglitz (1982), Papadimitriou (1995), and Schrijver (2003). We confine our consideration to minimization problems although all the results are also valid for maximization problems.Given a set DP of problem instances, a finite set X(I) of feasible elements for each I ∈ DP, and a real-valued objective function F(I, x), x ∈ X(I). The aim is to find x* ∈ X(I) such thatF(I,x*)=min{F(I, x)|x ∈ X(I)} for the given instance I ∈ DP if X(I) ≠ ∅ or report thatX(I)=∅.The elements x* are called the optimal elements.Hereinafter, x* means either argmin{F(I, x)|x ∈ X(I)} if X(I) ≠ ∅ orX(I)=∅.Following the conventional notion (see, e.g., Garey & Johnson, 1979), we say that problem Poptis NP-hard if there is a Turing reduction from any problem in NP to Popt.Consider the following problem associated with Popt:Standard Decision ProblemInstance: I ∈ DPand a number y.Output: “yes” if there is an x0 ∈ X(I) such that F(I, x0) ≤ y; “no” otherwise.In particular, ifX(I)=∅,then the output is “no”.To show the NP-hardness of an optimization problem using the “classical” approach, one constructs a Standard Decision Problem that corresponds to the original problem Poptand proves the NP-hardness of the former problem. We refer to this procedure as the standard scheme for proving the NP-hardness of an optimization problem.Since x0 ∈ X(I) such that F(I, x0) ≤ y exists if and only if F(I, x*) ≤ y, the validity of the standard scheme is based on the following Turing reduction of the Standard Decision Problem to the optimization problem.Step (a).Solve the optimization problem Popt.Given x* (found in Step (a)), calculate F(I, x*) (if X(I) ≠ ∅).Compare F(I, x*) and number y. Return “yes” if F(I, x*) ≤ y; otherwise (if F(I, x*) > y or ifX(I)=∅), return “no”.If both Step (a) and Step (b) can be executed in polynomial time, then the Standard Decision Problem is polynomially solvable. If the Standard Decision Problem is proved to be NP-hard and Step (b) can be executed in polynomial time, then Step (a) cannot be executed in polynomial time (unlessP=NP). The latter fact allows us to conclude that the optimization problem is NP-hard. On the other hand, if we have no polynomial algorithms to implement Step (b), then we can say nothing about the NP-hardness of problem Popt.Thus, the standard scheme works correctly if and only if Step (b) can be executed in time polynomial of the input size of problem Popt.Following Garey and Johnson (1979), we denote by Length[I] the length of a reasonable encoding of instance I.By and large, the standard scheme works quite well. However, it cannot cope with some odd cases where, e.g., given x* ∈ X(I) for Poptproblem, the value of F(I, x*) cannot be computed in time polynomial of Length[I].It may be argued that such odd cases need no additional research because the NP-hardness of computing F(I, x*) implies the NP-hardness of searching for x*. To counter this argument, we present a polynomially solvable optimization problem with an objective function that cannot be calculated in polynomial time if P ≠ NP. To construct an example of such a problem, we formulate a scheduling problem involving parallel machines and jobs with start-time and position dependent processing times.It seems that some researchers bear in mind the standard scheme in defining special classes of optimization problems. Vazirani (2001, Appendix A) considers a class of NP-optimization problems consisting of all the optimization problems with polynomially computable objective functions. The same restriction is used by Ausiello, Crescenzi, Gambosi, Kann, Marchetti-Spaccamela, and Protasi (1999, Chapter 1) to define a class NPO of so-called constructive problems.We do not confine our discussion to problems with polynomially computable objective functions. So the problems under consideration are in general outside the class NPO and they are not NP-optimization problems.To outline the basic idea of our approach for proving the NP-harness of optimization problems, we first analyze the standard scheme from the following point of view. The Standard Decision Problem is formulated in such a way that it has answer “yes” if and only if F(I, x*) ≤ y for a given number y. If we have the optimal element x*, then we can easily check whether F(I, x*) ≤ y holds (if F(I, x*) is easily computable). On the other hand, if we do not have x*, then the Standard Decision Problem should be NP-hard, which implies the NP-hardness of the corresponding optimization problem.We develop our approach for analyzing problems with hard computable objective functions. This means that we should avoid the calculation of the objective function values in our process of proving NP-hardness. In the standard scheme, they use a property of the problem instance I to provide the correctness of the inequality F(I, x*) ≤ y. The particular instance I may have this property (then the answer is “yes” for the Standard Decision Problem) or it may not have the property (then the answer is “no”). Note that the checking of the property for I should be done without knowing the element x* for instance I.The idea of our approach is to replace the property based on the inequality F(I, x*) ≤ y with a different property that does not involve the calculation of the objective function values. The new property should possess two features. It should be easily checkable if we are given an optimal element x* and its checking should be an NP-hard problem if we have instance I and do not have x*. Then the NP-hardness of the optimization problem will follow immediately from an argument similar to that used in case of the standard scheme.The idea looks quite simple and almost evident upon its discovery. As with any discovery, the most challenging aspect is to stumble upon an innovative insight into a problem, which may take a long time to emerge.The rest of the paper is organized as follows: Section 2 gives a short review of some related results. In Section 3 we formulate our alternative approach to proving the NP-hardness of optimization problems and show that the classical approach is a special case of it. In Section 4 we consider a scheduling problem with start-time and position dependent processing times of jobs (Problem 1) and propose a simple but non-polynomial algorithm to solve it. In Section 5 we show that the objective function values of Problem 1 cannot be computed in polynomial time if P ≠ NP. Besides, we construct a polynomially solvable special case of Problem 1 with the same property of having a hard computable objective function. We use the alternative approach to show the NP-hardness of Problem 1 in Section 6.In the literature, there are different techniques for proving the NP-hardness of optimization problems. It appears that the first attempt to provide a general technique was presented by Garey and Johnson (1979, Section 3.2).Subsequent results concern some specific classes of problems. For example, Creignou (1995) proposes a technique for proving the NP-hardness of a number of graph theory problems, which are shown to be linearly equivalent to the known satisfiability problem.Kovalyov and Pesch (2010) provide a unified approach for proving the NP-hardness of unconstrained partition-type problems (partitioning a given set of elements into several subsets such that a certain objective function of the partition is optimized).Sevastianov (2005) develops a so-called multi-parameter complexity analysis of a discrete problem. The proposed technique aims to establish the complexity of some subproblems of a discrete problem (decision or optimization) by analyzing possible combinations of constraints on the key parameters of the problem.The mentioned approaches either work with decision problems or (in case of optimization problems) are based on the standard scheme.As noted in the previous section, to illustrate the proposed alternative approach for proving the NP-hardness of optimization problems, we formulate a scheduling problem (Problem 1) involving parallel machines and jobs whose processing times are variables that depend on the start-times of the jobs and on the positions of the jobs in the sequence on a particular machine. The mentioned Problem 1 relates to the so-called high multiplicity problems. We discuss this type of problems in Section 4.3, where we give additional references.Scheduling research in which the processing times of the jobs depend on their start times was initiated by Melnikov and Shafransky (1980), who considered two-machine flow shop and single-machine problems to minimize the makespan. Many papers have since been published presenting results on scheduling problems with jobs whose processing times depend on their starting times (see the reviews by Alidaee and Womer (1999), and Cheng, Ding, and Lin (2004) for details). A comprehensive review of the problems with start-time dependent processing times can be found in the book by Gawiejnowicz (2008).Some papers present results that unite the technique for proving NP-hardness of optimization problems and scheduling research with jobs having start-time dependent processing times. Ng, Barketau, Cheng, and Kovalyov (2010) establish the strong NP-hardness of the Product Partition problem (which differs from the well-known Partition problem by considering the product of item weights instead of their sum). As an offshoot, the NP-hardness of some scheduling problems with start-time dependent processing times is established. Gawiejnowicz, Kurc, and Pankowska (2009) introduced a general transformation of scheduling problems with parallel machines and start-time dependent processing times. The transformation gives rise to the definition of a class of equivalent problems (a schedule is optimal for the initial problem if and only if the schedule constructed by this transformation is optimal for the resulting problem). Obviously, if the initial problem is NP-hard, then the transformed problem is NP-hard as well.There are a number of papers studying scheduling problems in which the processing time of a job depends on its position in the processing sequence. In fact, such a dependence may be considered as a dependence on the state of the machine, since the position of a job indicates the number of jobs that the machine has processed before it. It appears that Gawiejnowicz (1996) was the first to pursue research in this direction. Among pioneers in this stream of research are Biskup (1999), and Cheng and Wang (2000).Both types of scheduling problems (with start-dependent and position-dependent processing times) are reviewed by Biskup (2008), and Janiak and Kovalyov (2006). Some recent results are presented by Hsu and Yang (2014), Jiang, Chen, and Kang (2013), Qian and Steiner (2013), Wang and Wang (2014), and Zhao, Yin, Cheng, and Wu (2014), amongst many others. The most recent review of the results for problems with start-dependent and position-dependent processing times is given in book by Agnetis, Billaut, Gawiejnowicz, Pacciarelli, and Soukhal (2014, Chapter 6).We consider a general optimization problem Poptsuch that the values of its objective function F(I, x) cannot be calculated in polynomial time (unlessP=NP). For such a problem, the standard scheme for proving its NP-hardness cannot be used. This situation calls for the development of a new scheme for proving the NP-hardness of optimization problems. One of the main features that the new scheme should possess is that it gives a possibility to avoid calculating the objective function values.For an instance I of problem Popt, denote by X*(I) the set of all the optimal elements x* for I.Recall that, from a mathematical point of view, the phrase “a property” means a unary relation, i.e., it is a set. Saying that an object has a property, we mean that the object belongs to a set that defines the property.For a particular instance I ∈ DPand any property Π* ⊂ DP, we mean that I ∉ Π* ifX(I)=∅.Lemma 1An optimization problem Popt is NP-hard if there exist a polynomial p and a property Π* ⊂ DP that satisfy the following conditions:10.Given an instance I of problem Popt, recognizing whether I ∈ Π*is an NP-hard problem.Given any optimal element x* ∈ X*(I) for an instance I of problem Popt, recognizing whether I ∈ Π*is solvable in time p(Length[I]).Denote by Z the decision problem from condition 10. Suppose that we have an algorithm Aoptfor solving problem Popt. Using algorithm Aopt, we can find an optimal element x* ∈ X*(I) for any instance I of Popt.In view of condition 20, problem Z may be solved in time p(Length[I]) given any optimal element x* for I; denote the corresponding algorithm by AR. Since the time complexity of ARis bounded by p(Length[I]) and x* and Π* are parts of algorithm ARinput, the lengths of their descriptions should be bounded by some polynomial in Length[I]. Construct an algorithm AZfor solving problem Z as a composite of algorithms Aoptand AR(algorithm Aoptfinds x*, then algorithm ARrecognizes whether I ∈ Π* holds). The time complexity of AZisO(T(Length[I])+p(Length[I])),where T(Length[I]) is the complexity of Aopt. This means that algorithm AZdefines a Turing reduction from the NP-hard problem Z to the optimization problem Popt. In particular, if Aoptis a polynomial algorithm, then Z is a polynomially solvable problem. Thus, Poptis an NP-hard problem.□Lemma 1 provides a basis for formulating an alternative scheme for proving the NP-hardness of optimization problems.To prove the NP-hardness of Popt, it suffices to construct a special case of problem Poptwith some property Π* that satisfies conditions 10 and 20.The NP-hardness of recognizing the validity of the inclusion I ∈ Π* without the benefit of a known x* ∈ X*(I) may be established through a polynomial reduction of some known NP-hard problem. To prove the validity of condition 20, one should use some specific properties of the constructed special case of problem Popt.There is a clear relationship between the standard scheme that is based on the classical approach and the alternative approach that we have just presented.Lemma 2The standard scheme for proving the NP-hardness of optimization problems is a special case of the alternative scheme.As mentioned above, the standard scheme works correctly if and only if the value of F(I, x*) for given x* ∈ X*(I) can be calculated in time polynomial of Length[I] for instance I of problem Popt.Define property Π* in the following way:Π*={I∈DP|F(I,x*)≤y,x*∈X*(I)}. If F(I, x*) is polynomially computable, then the alternative scheme becomes the standard one under such definition of Π*. In fact, x0 ∈ X(I) such that F(I, x0) ≤ y exists if and only if x* ∈ X*(I) satisfies the condition F(I, x*) ≤ y. This means that the proof of the NP-hardness of problem Z is equivalent to the proof of the NP-hardness of recognizing the existence of element x0.In addition, the inequality F(I, x*) ≤ y holds and does not hold for all x* ∈ X*(I) simultaneously. Therefore, the checking of the validity of the inclusion I ∈ Π* reduces to the checking of the validity of the inequality F(I, x*) ≤ y. According to the condition 20, the complexity of the mentioned checking should be bounded by a polynomial p(Length[I]) if we are given x* ∈ X*(I). This implies that the value F(I, x*) should be polynomially computable.□Next we present a problem with a hard computable objective function.We consider a multi-machine processing system where the processing time of job j on machine H depends on the starting time of processing job j and on the state of the machine at that time.We consider a processing system consisting of M unrelated parallel machines. Each of the n identical non-preemptive jobs may be completely processed on any of the machines. It is supposed that the state of each machine H depends on the number of jobs that have been processed on the machine by the current time. For each machine H, there is a given cost WH> 0, being the cost of processing a job for one time unit on this machine. The goal is to construct a schedule s to process the n jobs in the described machine environment such that the total cost F(s) of processing all the jobs is minimized.The processing time pH(k, t) of job j on machine H depends on the position k of job j in the corresponding sequence and on its starting time t (recall that all the jobs are identical). The value of pH(k, t) is given by the relationpH(k,t)=λH(k−1)+φH(k−1)t,where the functions λH(k) and φH(k) are supposed to satisfy the following conditions: λH(k) > 0 and φH(k) ≥ 1 forH=1,…,M,k=0,…,n−1. Note that many decreasing, increasing, and non-monotone functions readily satisfy these conditions.Here λH(0) represents the “nominal” processing time of a job (the processing time of a job if it is processed first on machine H and starts at time 0). The longer a job waits for processing, the greater is its processing time. The valueφH(k−1)tcharacterizes the additional time needed for processing job j if its processing starts at time t, and the valuesφH(k−1)andλH(k−1)express the influence of the state of machine H on the length of job processing.Since φH(k) > 0 for any k, an idle time on a machine before processing any job assigned to the machine increases the processing times of the jobs that follow the idle period. This leads to the unreasonable growth of F(s) value. So we consider schedules without the mentioned idle times.All the jobs are identical, so any schedule s without idle times is defined by the set of numbersn1(s),…,nM(s),where nH(s) is the number of jobs processed on machine H under schedule s,∑H=1MnH(s)=n,nH(s) ≥ 0. Let LMH(ν) be the completion time of the first ν jobs assigned to machine H. For schedule s represented by numbersn1(s),…,nM(s),we can calculate the total load LMH(nH(s)) of machine H. Thus the objective function can be expressed in the following form:F(s)=∑H=1MWH·LMH(nH(s)).We denote the above problem of finding a schedule that minimizes F(s) as Problem 1.The starting time of job j in position k on machine H ist=LMH(k−1),so the processing time of job j equals(1)pH(k)=λH(k−1)+φH(k−1)LMH(k−1).The setting of Problem 1 is motivated by the following real-life situation. In electro-chemical treating of parts, the composition of the electrolyte used for the treatment changes over time (its concentration decreases). For the scheduling aims, the producer needs to take into account the changing conditions of the treatment (the processing time of a job (a part) increases with the growth of the number of jobs that have been processed earlier on the machine). Besides, there is a stage of preliminary processing of the parts that can process practically an unbounded number of parts simultaneously. If a preprocessed part waits for the main treatment, its properties acquired during the preliminary stage deteriorate over time, which results in an increase in the time needed for the treatment of the part at the main stage. This increase may be modeled by making the processing time of a job dependent on the instant at which the job begins processing in the main stage.In short, we present the formulated problem in the following form.Problem 1Instance: numbers M, n, and WH, and functions λH(k) and φH(k),H=1,…,M. Output: schedules*=(n1(s*),…,nM(s*))that minimizes the total processing cost F(s).We assume that the functions λH(k) and φH(k) are presented in formulas and that each of these formulas may be described in a polynomial space with respect to M and log n (e.g., using LaTeX encoding for formulas). This assumption is very weak since usually the description of a formula needs a constant number of symbols. Besides, we assume that the complexity of calculating the values of the functions is bounded by some polynomial p1 in M and log n. We assume also that each WHis bounded by Mq(M) for some polynomial q.To describe any instance of Problem 1, it suffices to present M functions of λH(k) and M functions of φH(k), as well as M numbers of WHand number n.The above assumptions allow us to conclude that the length of the input of Problem 1 is bounded by C × M × p(M, log n) for some polynomial p and constant C.We propose a simple greedy algorithm to solve Problem 1. The algorithm assigns the next job to the machine on which the processing cost of the job is minimum.Let nHbe the current number of jobs assigned to machine H. Then the processing cost of the next job assigned to machine H equalsWHpH(nH+1).We look through all the machines, find one where the cost is minimum, and assign a job to this machine. The process is completed when all the jobs are assigned to the machines.Algorithm AForH=1,…,MsetnH=0,LMH(0)=0and calculate the valuespH(nH+1)according to (1).Setn0=0.While n0 < n do the following:FindH0=argmin{WHpH(nH+1)|H=1,…,M}; break ties arbitrarily.SetLMH0(nH0+1)=LMH0(nH0)+pH0(nH0+1),nH0=nH0+1.Calculate the valuepH0(nH0+1)according to (1).Setn0=n0+1.End while.Any schedule s*constructed by Algorithm A minimizes the function F(s) for Problem1.Let s(1) be an optimal schedule and s* ≠ s(1). Denote bynH*andnH(1)the numbers of jobs assigned to machine H in schedules s* and s(1), respectively. Introduce the functionΔ(s*,s(1))=∑H=1M|nH*−nH(1)|.Find an index Q such thatnQ(1)>nQ*. The existence of Q implies the existence of an index R such thatnR(1)<nR*. Therefore Δ(s*, s(1)) ≥ 2.Our aim is to transform schedule s(1) into s* without increasing the value of the objective function. The aim is achieved if we obtainΔ(s(r),s*)=0for a schedule s(r) obtained from s(1) afterr−1transformation steps.Note thatpH(k+1)>pH(k)for any k ≥ 1. Indeed, we have φH(k) ≥ 1 and λH(k) > 0, sopH(k+1)=λH(k)+φH(k)LMH(k)≥λH(k)+φH(k)pH(k)>pH(k).Consider the step of Algorithm A for constructing schedule s*, where the number of jobs assigned to machine R changes fromnR(1)tonR(1)+1. By this step, let the number of jobs assigned to machine Q be equal to l. Evidently,l≤nQ*<nQ(1). It follows from the description of Algorithm A thatWRpR(nR(1)+1)≤WQpQ(l+1).Suppose thatl<nQ(1)−1. Then frompH(k)<pH(k+1),we haveWQpQ(l+1)<WQpQ(nQ(1)). Combining this inequality and the last inequality from the previous paragraph, we obtainWRpR(nR(1)+1)<WQpQ(nQ(1)),yieldingδ=WQpQ(nQ(1))−WRpR(nR(1)+1)>0.Transform schedule s(1) into schedule s(2) by re-assigning the last job on machine Q  to machine R. SinceF(s(2))=F(s(1))−δ,we obtain a contradiction with the optimality of schedule s(1). Note that the contradiction arises under the assumption thatl<nQ(1)−1. Ifl=nQ(1)−1,then instead of δ > 0 we have δ ≥ 0. Then, for schedule s(2), we have F(s(2)) ≤ F(s(1)) andΔ(s(2),s*)=Δ(s(1),s*)−2. Having fulfilled the necessary number of steps of the described form, we obtain schedule s(r) such thatF(s(r))≤F(s(r−1))≤⋯≤F(s(1))andΔ(s(r),s*)=0,i.e., s(r) coincides with s*.□It is easy to check that the running time of Algorithm A is O(n × p1(M, log n)). Recall that p1(M, log n) is a polynomial that bounds the time complexity of calculating the values of the functions λH(k) and φH(k). Therefore, Algorithm A is not a polynomial algorithm since the length of the problem input is C × M × p(M, log n).The presented estimation of the running time of Algorithm A establishes the validity of the following statement.Note 1Problem 1 is a pseudo-polynomially solvable problem.If Algorithm A with such a low time complexity is a non-polynomial algorithm, it is reasonable to hypothesize that Problem 1 has no polynomial-time algorithm at all (if P ≠ NP). Yet in trying to prove the NP-hardness of Problem 1, we found it impossible to use the standard scheme since the computation of F(s*) is not polynomial if P ≠ NP (see Theorem 2 in Section 5 below).Problem 1 is a typical example of the so-called high multiplicity (HM) problem, where there are large groups of identical input elements (see Hochbaum and Shamir (1990) and Hochbaum and Shamir (1991) for details, in which the notion of high multiplicity is first explicitly introduced but no formal definition of HM problems is given). A formal definition of HM scheduling problem is given by Brauner, Crama, Grigoriev, and van De Klundert (2007): a scheduling problem is an HM problem if the number of jobs n is not polynomially bounded in the input size of the problem.Comprehensive reviews of results on the HM scheduling problems are provided by Clifford and Posner (2001), and Brauner et al. (2007).To create a concise encoding for a high multiplicity problem, we cannot give a description of each of the identical elements. Instead, we should describe a representative of each group of identical elements and provide the capacity of the group in the encoding string. For Problem 1, there is only one such group, i.e., the set of jobs.Since in Problem 1 we are dealing with functions λH(k) and φH(k) that are in a relatively general form, we should provide an encoding scheme to describe them. As mentioned in Section 4, we suppose that each of the formulas that present functions λH(k) or φH(k) may be described in a polynomial space with respect to M and log n (e.g., using LaTeX encoding for formulas).For HM scheduling problems, one important issue about the construction of polynomial-time algorithms is to come up with a compact presentation of a schedule. Some basic ideas of such a presentation are given by Hochbaum and Shamir (1991), Brucker, Kovalyov, Shafransky, and Werner (1998), Clifford and Posner (2000), 2001), and Filippi and Romanin-Jacur (2009). The essence of the ideas is conventionally presented in two approaches. The first approach is to construct an optimal schedule in which each set of identical jobs is partitioned into a relatively small (polynomial in the problem size) number of groups and within each of these groups the jobs are processed sequentially. Such a schedule may then be presented in a compact form since for each of the found groups of jobs it suffices to state the particular machine on which the jobs should be processed, the starting time of the group, and the number of identical jobs in the group. The second approach is to construct an optimal schedule that has a regular structure. The compact presentation (by a set of formulas and relations) of such a schedule is captured by the regular structure. The two approaches are often combined. It should be noted that the second approach is also useful when we deal with problems (not necessarily HM problems) with unit-time operations or with preemptions. The number of unit-time operations may not be polynomial in problem size, which makes it impossible to present an operation-by-operation schedule in polynomial time without additional research. Finding an optimal schedule that has a regular structure may lead to the possibility of converting a non-polynomial algorithm into a polynomial one (see, e.g., the peculiar method proposed by Timkovsky (2001) for the two-machine job-shop problemJ2|pij=1|Lmaxwith unit-time operations to minimize the maximum lateness).Sometimes one is unable to propose a compact presentation of a solution for an HM problem or there is a need to present a solution in some prescribed form that is not polynomial in the size of the problem instance. In such cases, an algorithm that generates a solution cannot be considered as polynomial. Nevertheless, it is possible to devise an algorithm that is polynomial in the length of the solution encoding. Some approaches in this field are presented by Brauner, Crama, Grigoriev, and van De Klundert (2005) and Brauner et al. (2007).In case of Problem 1, we have a relatively simple situation with respect to the compact presentation of a schedule. The first approach works quite well here. As mentioned at the beginning of this section, to define a schedule for Problem 1, it suffices to give the numbers of jobs assigned to the machines.Our next goal is to show that computing the F(s) values is an NP-hard problem. First, we consider a special polynomially solvable case of Problem 1.Problem 2Under the conditions of Problem 1, suppose thatλH(k)=λ(k),φH(k)=φ(k),for allH=1,…,M,and for anyWH1<WH2,at least one of the conditions(2)WH2WH1<min{λmin(1+φmin)/λmax,φmin(1+φmin)/φmax}or(3)WH2/WH1≤φminhold, whereλmin=min{λ(k)|k=0,…,n−1}. The values λmax, φmin, and φmaxare defined in a similar way. The objective is to find a schedule s* that minimizes F(s).In short, we present the formulated problem in the following form.Instance: numbers M, n, λmin, λmax, φmin, φmax, and WH,H=1,…,M,and functions λ(k) and φ(k).Output: schedules*=(n1(s*),…,nM(s*))that minimizes F(s).In Problem 2 we have two functions instead of 2M functions in the input of Problem 1. Nevertheless, the input length of Problem 2 is almost the same: it is bounded by some polynomial in M and log n.Without loss of generality, we suppose that W1 ≤ W2 ≤ ⋅⋅⋅ ≤ WM(otherwise, we re-number the machines). For a positive number K, denote by ⌊K⌋ the largest integer D not greater than K.Letμ=nmodM. Schedule s*is an optimal schedule for Problem2if(a)nH(s*)=n/MforH=1,…,Mwheneverμ=0andnH(s*)=⌊n/M⌋+1forH=1,…,μ,andnH(s*)=⌊n/M⌋forH=μ+1,…,Mwhenever μ ≠ 0.Since Problem 2 is a special case of Problem 1, we can use Algorithm A to construct s*. For certainty, we suppose that having a choice (whenmin{WHpH|H=1,…,M}is achieved for some H), Algorithm A assigns the next job to the machine with the minimum H. Denote by nH(v) the number of jobs assigned to machine H after v steps. Since all the machines have the same functions λ(k) and φ(k), it follows from the description of Algorithm A that n1(v) ≥ n2(v) ≥ ⋅⋅⋅ ≥ nM(v).We now analyze the number of jobs assigned to the machines after v steps of Algorithm A, 1 ≤ v < n. We omit the trivial casen1(v)=⋯=nM(v).Find the maximum H such thatnH−1(v)>nH(v). Evidently, machine H has the minimum number of assigned jobs. For any G < H, we have LMG(nG(v)) > LMH(nH(v)). We show that Algorithm A assigns the next job to machine H, i.e.,nH(v+1)=nH(v)+1.Denote by α and β the costs of possibly assigning the next job to machines H and G, respectively. Thenα=WHpH(nH(v)+1)andβ=WGpG(nG(v)+1). We show that α < β. If this inequality is valid, Algorithm A should setnH(v+1)=nH(v)+1.We assume that WG< WH, since otherwise (ifWG=WH) the validity of α < β  is evident.If (2) is valid, thenWH<WGλmin(1+φmin)/λmaxandWH<WGφmin(1+φmin)/φmax. Having in mind these two inequalities, we getα=WH[λ(nH(v))+φ(nH(v))LMH(nH(v))]≤WH[λmax+φmaxLMH(nH(v))]<WGλmin(1+φmin)+WGφmin(1+φmin)LMH(nH(v)). On the other hand,nG(v)≥nH(v)+1implies thatβ=WG[λ(nG(v))+φ(nG(v))LMG(nG(v))]≥WG[λmin+φminLMG(nG(v))]≥WG[λmin+φmin[LMH(nH(v))+λmin+φminLMH(nH(v))]]. Therefore, α < β.Suppose now that (3) is valid. Estimate the value of β:β≥WG(λmin+φminLMG(nG(v)))≥WG[λmin+φmin(LMH(nH(v))+pH(nH(v)+1))]>WGφminpH(nH(v)+1)). In accordance with (3), we have WH≤ WGφmin. Sinceα=WHpH(nH(v)+1),we obtain α < β.Thus, we conclude that|nH1(s*)−nH2(s*)|≤1for each pair of machines H1 and H2.□Let s* be an optimal schedule for instance I of Problem 1 and, for some positive integer d, the valueF(s*)=10d−1a1(s*)+10d−2a2(s*)+⋯+10ad−1(s*)+ad(s*),wherea1(s*),…,ad(s*)∈{0,1,…,9}. Here ad(s*) is the last digit in the decimal presentation of the number F(s*).Consider the following auxiliary decision problem.Problem LastDigInstance: instance I of Problem 1, optimal schedule s* for I, and numberξ∈{0,1,…,9}.Output: “yes” ifad(s*)=ξ;“no” otherwise.Theorem 2Problem LastDig is NP-hard even ifM=2andλH(k)=φH(k)=λ(k)=φ(k)forH=1,2,k=0,…,n−1.Problem Partition is used as the basic problem for the proof. Partition: given positive integerse1,…,eρ,does there exist a 0-1 vectorz=(z1,…,zρ)such that∑l=1ρelzl=E,where∑l=1ρel=2E? Partition is NP-hard problem (see, Garey & Johnson, 1979).Given an instance of Partition, we construct an instance LD of Problem LastDig in the following way: we use for the instance construction an idea from Cheng and Kovalyov (2002). SetM=2,W1=2,W2=3,andn=2·2ρ. Let b(k) be the binary representation ofk∈{0,1,…,n/2−1}using ρ bits, i.e., a ρ-dimensional 0-1 vectorb(k)=(b1(k),…,bρ(k))such thatk=∑l=1ρ2l−1bρ−l+1(k). Fork∈{n/2,n/2+1,…,n−1},defineb(k)=b(k−n/2).DefineλH(k)=φH(k)=λ(k)=φ(k)forH=1,2,whereφ(k)={4,if∑l=1ρelbl(k)≠E,3,if∑l=1ρelbl(k)=Efor allk∈{0,…,n−1}. Setξ=5.Note that the length of n encoding does not exceedρ+1and the binary representation b(k), as well as the values of λ(k) and φ(k), may be found in O(ρ) time for each k. Besides, the presentation of the above formula for φ(k) takes a constant space. Thus the described reduction takes polynomial time in the length of the Partition input and results in a description of an input for Problem LastDig ofO(logn)=O(ρ)length. Here we again use the property of high multiplicity of Problem LastDig and a presentation of functions by formulas.Let s* be an optimal schedule for the constructed instance LD. It is easy to check that instance LD is simultaneously an instance of Problem 2. So, according to Lemma 3, we haven1(s*)=n2(s*)=n/2(recall that n is an even number in our instance). We show that Partition has a solution if and only ifad(s*)=5,where ad(s*) is the last digit in the decimal representation of the F(s*) value.Sufficiency. Suppose that Partition has no solution. Then,λ(k)=φ(k)=4,k=0,…,n−1. Consider a schedule s withn1(s)=N1andn2(s)=N2,whereN1+N2=n. We show thatLMH(NH)=5NH−1.ForNH=1,we haveLMH(1)=5−1. Suppose that forNH=ν≥1,the formula is valid. ForNH=ν+1,we haveLMH(ν+1)=(5ν−1)+(4+4(5ν−1))=5ν+1−1.Then,F(s)=2(5N1−1)+3(5N2−1). For any integer N > 0, the number5N−1is even. This means that ad(s) ≠ 5 for any schedule s.Necessity. Suppose now that there exist r integers0<k1<⋯<kr≤n−1,1 ≤ r < n, such that∑l=1ρelbl(kν)=E,ν=1,…,r,i.e., Partition has a solution. We show that the last digit of F(s*) equals 5 (heren1(s*)=n2(s*)=n/2).For any 0 < k ≤ k1, the relationLM1(k)=LM2(k)=5k−1holds. This means that the last digit in the decimal representation of LMH(k) is 4. Then the last digit of the number3+3LMH(k1)is 5. Here,3+3LMH(k1)is the processing time of the(k1+1)th job assigned to machine H. The last digit ofLMH(k1+1)=LMH(k1)+3+3LMH(k1)is evidently 9. Now we have two possibilities:k2=k1+1andk2>k1+1. In the former case, we haveLMH(k1+2)=LMH(k2+1)=LMH(k1+1)+3+3LMH(k1+1),which means that the last digit ofLMH(k1+2)is 9. In the latter case, we haveLMH(k1+2)=LMH(k1+1)+4+4LMH(k1+1)and the last digit ofLMH(k1+2)is 9, too. Thus, in any case, the last digit of LMH(n/2) is 9 and this regularity holds forLMH(k1+3),…,LMH(n/2).As a result, the last digit ad(s*) of the numberF(s*)=2LM1(n/2)+3LM2(n/2)is 5.□Let s*be an optimal schedule for an instance of Problem1. The value of F(s*) cannot be calculated in polynomial time unlessP=NP.Since the constructed instance LD is an instance of Problem 2, the following proposition holds:Corollary 2Calculating the objective function values of Problem2cannot be performed in polynomial time (if P ≠ NP). On the other hand, it follows from Lemma3that the construction of an optimal schedule for any instance of Problem2takes O(Mlog M) time, i.e., it takes polynomial time.We use the alternative scheme to show that Problem 1 is NP-hard.Consider the following special case of Problem 2Problem 1RInstance: instance I of Problem 1 withM=2,W1=1,W2=5/2,n=2gfor some g ≥ 4,λ1(k)=λ2(k)=1,and φ1(k) ∈ {1, 2} withφ1(0)=φ1(1)=φ1(2)=2andφ2(k)=2,k=0,…,n−1.Output: schedule s* that minimizes F(s).Note that the length of the input of Problem 1R is Clog n for some constant C.We formulate the property Π* for Problem 1R as follows:Π*={I∈D1R|n1(s*)>n2(s*),s*∈X*(I)},where D1Ris the set of all the instances of Problem 1R.The definitions of the values W1, W2, λ1(k), λ2(k), φ1(k), and φ2(k) imply the impossibility of the existence of an optimal schedule s* such that n1(s*) < n2(s*).In fact, W1 < W2,λ1(k)=λ2(k),and φ1(k) ≤ φ2(k) for allk=0,…,n−1.In case n1(s*) < n2(s*), we would have F(s′) < F(s*) for schedule s′ withn1(s′)=n1(s*)+1andn2(s′)=n2(s*)−1.Lemma 4An optimal schedule s*for instance I ofProblem 1Ris defined by the equalityn1(s*)=n2(s*)if and only if the relationφ1(k)=2holds for allk=0,…,gin the instance I.Let s be a schedule for instance I of Problem 1R such thatn1(s)=n2(s)=g. Along with schedule s, we consider a schedule s0 for the instance I such thatn1(s0)=g+uandn2(s0)=g−ufor some 1 ≤ u ≤ g. Calculate and compare values F(I, s) and F(I, s0). SetΔ=F(I,s)−F(I,s0).For Problem 1R, we havep1(k)=1+φ1(k−1)LM1(k−1)andp2(k)=1+2LM2(k−1). ThenF(I,s)=∑k=1gp1(k)+2.5∑k=1gp2(k),F(I,s0)=∑k=1gp1(k)+p1(g+1)+⋯+p1(g+u)+2.5∑k=1g−up2(k),andΔ=2.5∑k=g−u+1gp2(k)−∑k=1up1(g+k).We show that s is an optimal schedule ifφ1(k)=2holds for allk=0,1,…,g. FrompH(k)<pH(k+1),we haveΔ<Δ′=2.5up2(g)−up1(g+1). Ifφ1(k)=2,thenp1(k)=p2(k),k=1,…,g,and we haveΔ′=u[2.5(1+2∑k=1g−1p2(k))−(1+2∑k=1gp1(k))]=u[1.5+3∑k=1g−1p2(k)−2p1(g)]=u[−0.5−∑k=1g−1p1(k)]<0. This implies that s is an optimal schedule and there are no other optimal schedules for the instance I.Suppose now that there exists an index r, 3 ≤ r ≤ g, such thatφ1(r)=1. Let r be the minimum index that satisfies this condition. Setu=1and estimate the valueΔ=2.5p2(g)−p1(g+1).Letr=g,thenΔ=2.5p2(g)−[1+∑k=1gp1(k)]=1.5p2(g)−[1+∑k=1g−1p1(k)]=0.5+2∑k=1g−1p2(k)>0. This means that s cannot be an optimal schedule.Consider now the situation where r < g. Note thatΔ=2.5p2(g)−p1(g+1)reaches its minimum value atr=g.Indeed, in Problem 1R we havep2(k)=1+2LM2(k−1)and p2(g) does not depend on r, whereasp1(k)=1+φ1(k−1)LM1(k−1)andp1(g+1)reaches its maximum value atr=g.This means that for r < g, we again have Δ > 0 and s cannot be an optimal schedule.□Given an instance I ofProblem 1R, the inequality n1(s*) > n2(s*) either holds for all the optimal schedules for the instance I or does not hold for any of them.The validity of Corollary 3 follows directly from the proof of Lemma 4 and Note 2.Theorem 3Problem 1Ris NP-hard.Construct a polynomial reduction from Partition to the problem of recognizing the validity of the relation I ∈ Π* for an instance I of Problem 1R. Recall thatΠ*={I∈D1R|n1(s*)>n2(s*),s*∈X*(I)}.Given an instance of Partition, we construct an instance of Problem 1R in the following way: setn=2ρ. Similar to the proof of Theorem 2, let b(k) be the binary representation ofk∈{0,1,…,n−1}using ρ bits, i.e., b(k) is a ρ-dimensional 0-1 vectorb(k)=(b1(k),…,bρ(k))such thatk=∑l=1ρ2l−1bρ−l+1(k). Fork∈{0,…,n−1},we defineφ1(k)={2,if∑l=1ρelbl(k)≠E,1,if∑l=1ρelbl(k)=E.Note that the length of n encoding does not exceed ρ and the values of b(k), as well as the values of φ1(k), may be found in O(ρ) time. Thus, the described reduction takes polynomial time in the length of the Partition input.We may assume that el< E for alll=1,…,ρ(otherwise, Partition is trivially solvable). The condition el< E implies thatφ1(0)=φ1(1)=φ1(2)=2.Problem Partition has a solution if and only if the optimal schedule s* for the constructed instance of Problem 1R satisfies the condition n1(s*) > n2(s*). Indeed, the existence of a solution for Partition is equivalent to the existence of an index r such thatφ1(r)=1.At the same time, it follows from Lemma 4 that the relation n1(s*) > n2(s*) holds if and only if there exists r such thatφ1(r)=1.In view of Corollary 3, we see that the NP-hardness of Problem 1R follows directly from Lemma 1.□Problem1is NP-hard even forM=2.

@&#CONCLUSIONS@&#
The main result of the paper is the alternative approach to proving the NP-hardness of optimization problems. The alternative approach includes the “classical” approach as a special case.While the alternative approach is especially useful for proving the NP-hardness of optimization problems with a hard computable objective function, it can be used for problems with a polynomially computable objective function as well.We give an example of a scheduling problem (Problem 2) where the identification of an optimal element for any instance of the problem takes polynomial time, while the calculation of the objective function values is an NP-hard problem. The example shows that there is no direct relationship between the hard computability of the objective function and the NP-hardness of an optimization problem Popt.For future research, it is worth developing a similar approach for proving the strong NP-hardness of optimization problems.The first author was supported in part by The Hong Kong Polytechnic University under the Fung Yiu King – Wing Hang Bank Endowed Professorship in Business Administration. The second author was partially supported by the ISTC project B-986 and the scientific program “Mathematical Models 28” of the National Academy of Sciences of Belarus. We are grateful to the anonymous referees for their many valuable comments and suggestions that have helped us to improve the presentation of the results .