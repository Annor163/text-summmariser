@&#MAIN-TITLE@&#
3D faces in motion: Fully automatic registration and statistical analysis

@&#HIGHLIGHTS@&#
A new landmark prediction method for 3D facial motion sequence is proposed.We fully automatically register 3D facial motion sequences.We statistically analyze the registered motion sequences.We synthesize new motion sequences.

@&#KEYPHRASES@&#
Statistical shape space,Statistical model fitting,Motion sequence registration,Statistical analysis,

@&#ABSTRACT@&#
This paper presents a representation of 3D facial motion sequences that allows performing statistical analysis of 3D face shapes in motion. The resulting statistical analysis is applied to automatically generate realistic facial animations and to recognize dynamic facial expressions. To perform statistical analysis of 3D facial shapes in motion over different subjects and different motion sequences, a large database of motion sequences needs to be brought in full correspondence. Existing algorithms that compute correspondences between 3D facial motion sequences either require manual input or suffer from instabilities caused by drift. For large databases, algorithms that require manual interaction are not practical. We propose an approach to robustly compute correspondences between a large set of facial motion sequences in a fully automatic way using a multilinear model as statistical prior. In order to register the motion sequences, a good initialization is needed. We obtain this initialization by introducing a landmark prediction method for 3D motion sequences based on Markov Random Fields. Using this motion sequence registration, we find a compact representation of each motion sequence consisting of one vector of coefficients for identity and a high dimensional curve for expression. Based on this representation, we synthesize new motion sequences and perform expression recognition. We show experimentally that the obtained registration is of high quality, where 56% of all vertices are at distance at most 1mm from the input data, and that our synthesized motion sequences look realistic.

@&#INTRODUCTION@&#
The human face plays an important role in our daily life, since non-verbal communication by facial expression has a significant impact on all kinds of human interactions. This motivates many different application areas like human computer interaction, entertainment, medicine, ergonomic design, and security, to be also interested in faces. There, faces are used to control virtual avatars e.g. [33,53,34,16], to generate realistic physical deformable face models e.g. [9], to plan surgeries e.g. [26], to recognize certain diseases e.g. [25], to design best fitting gear e.g. [54] or to recognize faces e.g. [36]. Since many of these works are based on the 3D geometry of the face, several new methods have been developed during the last years to acquire static or dynamic 3D faces [15,50,6,14,7]. With the improved ability of capturing 3D scans, the number of publicly available 3D face databases has increased [59,46,58]. These databases aim at capturing a wide variety of facial shapes and facial expressions, including facial dynamics.To further process these face scans, they need to be annotated. A sparse annotation of a face scan can be achieved by manually selecting a set of points, but this is quite tedious and takes between 30s and several minutes per scan, depending on the number of selected points. For a database of 606 motion sequences like the BU-4DFE [58], each sequence contains about 100 frames, and it would therefore take more than 21days to obtain a manually selected set of points. To decrease this manual effort, several data-driven methods exist for the fully automatic prediction of facial landmarks, requiring a labeled set of faces for training [17,43,24]. Even worse, a dense annotation of a facial scan cannot be achieved manually. However, for a single scan, such an annotation can be computed using a given sparse set of landmarks [43,36,24]. These methods cannot be used for 3D facial motion sequences, since they are not stable and the temporal coherence is not preserved. In the following document, we say that a set of faces, which are all annotated in the same way, is registered or in correspondence. We propose an approach to robustly compute correspondences between a large set of facial motion sequences in a fully automatic way. The approach is stable, preserves the temporal coherence, and does not require any manual annotation.With a registered database that captures a wide variety of shapes, methods that aim at extracting geometric facial characteristics become possible. These statistical analysis methods are used to analyze the shape of 3D faces of different identities of the same, or across several ethnicities [11,10], or to analyze shape and expression changes simultaneously [52,18,55]. All these methods analyze static data, and to the best of our knowledge, there are so far no general methods to statistically analyze 3D faces in motion. Our approach allows to statistically analyze large datasets of facial motion sequences in a semi-supervised manner, where the only input required is the type of motion to be analyzed (e.g. “happy”). Due to the importance of facial dynamics, analyzing 3D faces in motion has numerous applications. Our approach allows to animate static face scans, which is challenging, since the performed expressions are subject specific. Furthermore it allows to synthesize large-scale datasets of facial motion sequences labeled by the type of motion as well as landmarks in a fully automatic way. While the resulting facial animation does not contain fine-scale geometric details, it can be combined with texture- and bump-maps and used in video games, for instance. Another potential application is the recognition of dynamic facial expressions.Performing statistical analysis of 3D motion data is a challenging problem, since it requires a robust registration method that establishes spatial and temporal correspondence for motion sequences of different identities performing different expressions. While it is possible to apply the previously mentioned facial registration methods [43,36,24] for each frame of the sequence individually, these methods do not preserve the temporal coherence of the motion, and do not yield a compact representation, which is required for statistical analysis.To compute a registration, we use a multilinear model as statistical prior. Fig. 1shows an overview of our method. To be robust to fast motions, we need a good initialization for our motion registration. For this, we fully automatically predict landmarks for an entire motion sequence using a method based on a Markov Random Field (MRF). We then use a learned multilinear model for a fully automatic registration of motion sequences of 3D faces. We have previously learned this model from a registered 3D face database that contains static 3D faces of different identities performing several expressions in four intensity levels each. To be independent of illumination changes, our overall approach only depends on geometric information, but appearance information could be added using a higher-dimensional multilinear model. After registration, each motion sequence is represented by a vector of coefficients for identity and a high dimensional curve for expression. This representation allows to use standard techniques to perform statistical analysis on 3D faces in motion.In summary, the main contributions of our work are:•We propose a new MRF-based landmark prediction method for entire motion sequences of 3D faces.We propose a fully-automatic approach to register motion sequences of 3D faces both spatially and temporally using a multilinear model as statistical prior that is more robust with respect to fast motions and computationally faster than in the previous version [13].We introduce a general framework to analyze 3D face shapes in motion.We apply the framework to four applications, namely we propose different ways to synthesize new motion sequences, and recognize dynamic expressions.We register a large number of facial motion sequences and show that our registration result is of high quality, where 56% of the vertices are at distance at most 1mm from the input data. We further show that our synthesized motion sequences look realistic. For the expression recognition, we obtain classification rates of 90.71% for the expressions anger, happiness, surprise, and 90.60% for the expressions happiness, sadness, surprise.The main novelty compared to our previous version [13] are (1) the use of a multi-resolution framework for model fitting, (2) the use of fully-automatically predicted landmarks for improved registration stability, and (3) a more extensive experimental validation with additional application scenarios.This paper is organized as follows. Section 2 summarizes some relevant work. Section 3 presents a novel fully automatic landmark prediction for facial motion data. Section 4 presents the multilinear model, describes its usage as a statistical prior, and evaluates the model. Section 5 describes our fully automatic registration technique for facial motion data. This registration approach gives a compact representation for each motion sequence that consists of a vector of coefficients for identity and a high-dimensional curve for expression. In Section 6, we use this representation to perform statistical analysis of 3D facial motion sequences. Finally, Section 7 evaluates all steps of our registration approach.

@&#CONCLUSIONS@&#
