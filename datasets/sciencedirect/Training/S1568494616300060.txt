@&#MAIN-TITLE@&#
Opposition chaotic fitness mutation based adaptive inertia weight BPSO for feature selection in text clustering

@&#HIGHLIGHTS@&#
A feature selection method based on binary particle swarm optimization is presented.Fitness based adaptive inertia weight is integrated with the binary particle swarm optimization to dynamically control the exploration and exploitation of the particle in the search space.Opposition and mutation are integrated with the binary particle swarm optimization improve it's search capability.The performance of the clustering algorithm improves with the features selected by proposed method.

@&#KEYPHRASES@&#
Text clustering,Binary particle swarm optimization,Mutation,Chaotic map,Opposition-based learning,

@&#ABSTRACT@&#
Due to the ever increasing number of documents in the digital form, automated text clustering has become a promising method for the text analysis in last few decades. A major issue in the text clustering is high dimensionality of the feature space. Most of these features are irrelevant, redundant, and noisy that mislead the underlying algorithm. Therefore, feature selection is an essential step in the text clustering to reduce dimensionality of the feature space and to improve accuracy of the underlying clustering algorithm. In this paper, a hybrid intelligent algorithm, which combines the binary particle swarm optimization (BPSO) with opposition-based learning, chaotic map, fitness based dynamic inertia weight, and mutation, is proposed to solve feature selection problem in the text clustering. Here, fitness based dynamic inertia weight is integrated with the BPSO to control movement of the particles based on their current status, and the mutation and the chaotic strategy are applied to enhance the global search capability of the algorithm. Moreover, an opposition-based initialization is used to start with a set of promising and well-diversified solutions to achieve a better final solution. In addition, the opposition-based learning method is also used to generate opposite position of the gbest particle to get rid of the stagnation in the swarm. To prove effectiveness of the proposed method, experimental analysis is conducted on three different benchmark text datasets Reuters-21578, Classic4, and WebKB. The experimental results demonstrate that the proposed method selects more informative features set compared to the competitive methods as it attains higher clustering accuracy. Moreover, it also improves convergence speed of the BPSO.

@&#INTRODUCTION@&#
The volume of the text documents in digital format is gradually increasing and the text clustering has become a key paradigm to arrange digital text documents. The primary aim of a text clustering algorithm is to create clusters of the documents based on their intrinsic characteristics. Typical text clustering framework consists of a vector space model (VSM) representation that represents the documents in a common standard format and a clustering process that performs grouping of the text documents.Majority of the text clustering paradigm employs bag-of-words approach [69], where each distinct term present in the documents’ collection is considered as a feature for the document's representation. Hence, a document is represented by a multi-dimensional feature space where the cell value of each dimension corresponds to a weighted value, e.g., TF-IDF [68], of the concerning term within the document. Since the features are generated from distinct terms, even the moderate-sized documents in a text collection would result in hundreds of thousands of features. The clustering algorithms that do not perform feature reduction fail miserably not only because of large number of features but also because the non-informative (irrelevant, redundant, and noisy) features mislead and slow the algorithms. Therefore, feature selection, which removes non-informative features and selects a discriminative subset of features, has become a crucial preprocessing step in the text clustering to improve performance of the underlying algorithm and to speed up the computation. Feature selection is a combinatorial optimization problem with an aim to maximize accuracy of the underlying algorithm and minimize the number of features. As exhaustive search, a brute and force paradigm to solve combinatorial optimization problems, is not practically possible for larger problems owing to unreasonably high computational complexity, a number of feature selection methods have been proposed and their effectiveness have been studied to simplify the task.According to the search strategy to obtain an informative subset of the features, the existing feature selection methods are classified into three categories: filter, wrapper, and hybrid. Filter methods perform statistical analysis of the feature set to select a discriminative subset of the features without considering an interaction with the learning algorithm. Methods in this category include the information gain [63], document frequency [47], term strength [91], mean-median [27], mean absolute difference [27], mutual information [61], chi-square [46], and odd ratio [52]. Due to less computational complexity, these methods are widely used for feature selection, especially in the case of very high dimensional feature space, e.g., text. Wrapper methods employ search strategy to obtain subsets of the features and use learning algorithm to evaluate effectiveness of the obtained subsets and select the optimal feature subset. Methods in this category include sequential forward selection [62], sequential backward elimination [62], and plus-l-take-away-r-process [54,75]. Though these methods are computationally expensive in comparison to the filter methods, they attain comparatively higher accuracy. Another category of the feature selection is the hybrid method. Hybrid methods integrate different feature selection methods for informative feature subset selection. It takes the advantage of one method and lessen the drawback of the other for the feature subset selection. Recently, hybrid methods have received considerable attention for feature selection due to their feature selection characteristics [11,12,34,88,92,93].Feature selection is a NP-hard combinatorial optimization problem [14,26,53,84]. An exhaustive search is the best way to obtain ideal solution(s) for the combinatorial optimization problems. However, performing an exhaustive search over the entire solution space is not practical as it involves unreasonably high computational complexity. In recent decades, many researchers have explored meta-heuristic algorithms to solve the combinatorial optimization problems [41,65,73,96] and these algorithms are gaining popularity day by day owing to their global search capability. Genetic algorithm [31], ant colony optimization (ACO) [20], and artificial bee colony (ABC) [38] are some well explored methods of this category and these algorithms have been widely used for feature selection problem [30,37,74,76,83].Particle swarm optimization (PSO) is a swarm-intelligence based meta-heuristic search and optimization method. It simulates the social behavior of organisms such as bird flocking and fish schooling. Owing to its simplicity, information sharing capability, fast converge, and population based natures, the PSO-based algorithms are successfully explored in different domain such as power flow analysis [2], bioinformatics [1], image processing [66], financial decisions [50], data clustering [18]. In this paper, BPSO is used for feature selection problem. However, it has been used by other researchers to solve the feature selection problem in other domains. Recently, several of its variants such as the chaotic BPSO [18], the catfishBPSO [17], the correlation and taguchi chaotic BPSO [19], and the hamming distance BPSO [7] have been proposed for the feature selection. Chuang et al. [18] propose chaotic maps (logistic map and tent map) based methods to improve search ability of the BPSO. Chaotic map has an ergodicity, stochastic and regularity properties. These properties of a chaotic system provide a good exploration of the search space at the cost of exploitation. Chuang et al. [17] present an improved version of the BPSO for feature selection problem. The authors introduce a concept of catfish effect, which replaces 10% worst fit particles by the particles initialized at extreme points in the search space when the fitness of the global best particle (gbest) does not improve for a defined number of iterations. Banka and Dara [7] present a hamming distance based BPSO (HDBPSO) for feature selection problem. The authors use hamming distance as a proximity measure to update velocities of the particles in the BPSO. Experimental analysis is conducted on three different benchmark text datasets. The results and study favour their proposed method HDBPSO to the competitive methods.Tan et al. [77] use genetic algorithm for feature selection problem. Akadi et al. [24] use GA for gene selection in a microarray data. Crossover and mutation are strong components of the GA. These components are responsible for exploration and exploitation, respectively in the algorithm. Some researchers integrate crossover and/or mutation to improve the search ability of the PSO. Ghamisi et al. [30] integrate GA with PSO for feature selection problem. Zhang et al. [95] use bare bone PSO for feature selection problem. In this approach, a reinforced memory strategy is used to update local leaders of the particles. Moreover, a uniform combination of crossover and mutation is proposed to balance the exploration and exploitation of the algorithm.As discussed above, the filter methods are computationally more efficient than the wrapper methods, however, the wrapper methods attain better accuracy compared to the filter methods. Therefore, to integrate advantage of the one method and lessen drawback of the other, various researchers present integrated methods named hybrid or two stage methods for feature selection [24,83]. These methods select informative subset of features from the original feature space, thereby decrease computational complexity and increase performances of the underlying algorithms.Chunag et al. [19] present correlation and taguchi chaotic binary particle swarm optimization for feature selection. In this approach, correlation approach is used to reduce dimensionality of the feature space and then taguchi chaotic binary particle swarm optimization is used to further refine the selected feature subspace. Zhang et al. [94] combine reliefF [42] and MRMR for gene selection problem. Akadi et al. [24] introduce a two-stage selection process for gene selection. They combine MRMR with GA to create an informative genes’ subset. Initially, they use MRMR to filter out noisy and redundant genes from high dimensional space and then use GA to select a subset of relevant discriminative features. In their study, the authors use support vector machine (SVM) and naive bayes (NB) classifiers to estimate fitness of the selected features. Their experimental results show that their model is able to select the smallest gene subset that obtains the most classification accuracy in leave-one out cross-validation (LOOCV).Initialization of the particles’ positions, update of the particles, and parameters’ values update play an important role to control search capability of the BPSO. Hence, in this paper, we aim to cover each possible aspect of the BPSO to improve its search capability. Population initialization plays a crucial role in the population based methods as it affects convergence speed and quality of the final solution. Different initialization strategies have been tested with the PSO to improve its performance [60,89]. In this paper, we use opposition-based strategy for population initialization. Moreover, it is also used to avoid stagnation of the swarm at the local optimal solution when the value of the gbest does not change for a defined number of iterations. The parameters of the BPSO play an important role to improve its search capability. Inertia weight is one of the key parameter to achieve this objective. Various adaptive/dynamic inertia weight criteria have been introduced by the researchers. A comprehensive review of different inertia weights is presented by [8,55]. In this paper, a new fitness based dynamic inertia weight is introduce to dynamically control the value of inertia weight of each particle based on its current status. It assigns high inertia weights to the low fit particles to explore distinct regions in the search space (exploration) and assigns low inertia weights to the high fit particles to exploit the region in their vicinity (exploitation). Instead of using uniform random number generation function, a chaotic map is used in velocity update equation for random number generation due to its ergodicity, stochastic and deterministic dynamic behavior. At the end, mutation and opposition-based strategy are applied to avoid stagnation of the particles at a local optimal solution and improve global search capability of the algorithm.In nutshell, the BPSO is fortified with the following five modifications to avoid stagnation of the swarm at a local optimal solution and obtain a (near) global optimal solution: (1) opposition-based initialization strategy to start with a population of good solutions, (2) fitness based adaptive inertia weight to update the position of a particle intelligently, (3) logistic chaotic map to improve the search capability of the algorithm, (4) mutation to explore nearby location of the gbest solution, and (5) opposite position of gbest particle to replace the worst fit particle if the value of gbest does not change for a defined number of iterations.As BPSO is computationally expensive, it cannot be directly applied on very high dimensional feature space like text, where the number of features varies from thousands to tens of thousands. To reduce its computational overhead and to employ it to the text clustering problem, we apply our improved BPSO on top ranked features obtained by term variance (TV). The k-means clustering [49] with dunn index [22] is used to evaluate effectiveness of the features subsets. It is also used to create final clusters of the documents with selected features subset. To evaluate effectiveness of the proposed method, empirical analysis is conducted on three different benchmark text datasets: Reuters-21578, Classic4, and WebKB. Experimental results show that the proposed method (PM) not only reduces the number of features, but also significantly improves performance of the clustering algorithm; the obtained results are comparatively better than the competitive methods in most of the cases.The rest of the paper is organized as follows. Section 2 presents a theoretical motivation behind this work. A detailed description of the proposed methodology is presented in Section 3. Section 4 illustrates the empirical results to show effectiveness of the proposed dimension reduction method. Finally, conclusions and recommendations for future directions are summarized in Section 5.In this section, we briefly introduce the preliminaries of the presented work.The volume of the text documents in the digital format is gradually increasing and the text clustering has become a key paradigm to arrange the text documents. The primary aim of a text clustering algorithm is to create clusters of the documents based on their intrinsic characteristics. Before creating clusters, the documents are preprocessed with standard preprocessing steps such as tokenization, stop words removal, stemming, and term weighting to convert the documents into a suitable format. A brief description of these preprocessing steps are given below.In tokenization, a document gets split into independent terms called tokens. The length of the token varies from a single term (unigram) to a consecutive sequence of n-terms (n-grams). In this paper, we use single terms for documents representation.The common words, e.g., a, an, the, who, be, and other common words that carry less weightage in the document clustering are known as the stop words. There are no specific rules; it is completely left to the user to write in as many stop words as required. It becomes necessary to remove them as they usually cover a substantial part of a document, hence they not only increase the number of features unnecessarily but also mislead and deteriorate performance of the underlying clustering method. In this paper, stop words are removed in accordance with the stop word list, which is available at the website11http://jmlr.org/papers/volume5/lewis04a/a11-smart-stop-list/english.stop.of Journal of Machine Learning Research; it consists of 571 stop wordsStemming converts inflectional/derivationally related forms of a word to their root form. For example, introduction, introduce, and introducing all have the common root ‘introduc’. Various stemming methods have been proposed in the literature to achieve this task [35]. The most commonly used stemming method is the porter stemmer.22http://tartarus.org/martin/PorterStemmer/.In this paper, we also use the same for the stemming purpose.This scheme is used to map the textual documents into a compact numeric vector representation. Various term weighting schemes have been proposed in the literature [13,67,68] to map textual content into a numeric format. The most widely used term weighting scheme is the term frequency inverse document frequency (tfidf). Mathematically, it is formulated as follows:(1)tfidfpq=(|Ypq|)lnndfq,ifYpq≥10,otherwisewhere tfidfpqis tfidf of the qth term in the pth document, Ypqis frequency of the qth term in the pth document, n is total number of documents in the corpus and dfqis document frequency of the qth term, i.e., the number of documents in the corpus that include the qth term.Particle swarm optimization (PSO) is a population based meta-heuristic algorithm proposed by Kennedy and Eberhart in 1995 [39] and is motivated by the social behavior of organisms, i.e., birds in a flock, fish in a school. The population in the PSO is called a swarm and each individual in the swarm is referred as a particle. A swarm consists of S particles, where each particle represents a potential solution in a d-dimensional search space. In the population, the ith particle is characterized by its position vector Xi=(xi1, xi2, …, xid) and velocity vectorVi=(vi1,vi2,…,vid). Each particle moves around in the search space to search for the global optimal solution. The movement of a particle is guided by its own ever known best position known as personal best position (pbest) as well as the swarm's ever known best position known as global best position (gbest). The personal best position of the ith particle is represented by pbesti=(pbesti1, pbesti2, …, pbestid) and the best position of the swarm is depicted by gbest=(gbest1, gbest2, …, gbestd). A fitness value is used to assess the quality of a particle. It helps the algorithm to select a better position for the particle. In the process of update, the current position and velocity of the ith particle are updated according to the following equations.(2)vidnew=w×vidold+c1×rand(0,1)×(pbestid−xidold)+c2×rand(0,1)×(gbestd−xidold)(3)xidnew=xidold+vidnewIn Eq. (2), rand is a random number uniformly distributed in the interval [0.0, 1.0]. The c1 and c2 are cognitive weight coefficient and social weight coefficient, respectively, that are usually set to a fixed value between [0, 2]. Parameterwis an inertia weight between [0.0, 1.0] to control the influence of the previous velocity. A large value ofwfacilitates global exploration (searching new regions), whereas a small value allows a local exploitation (refinement in the searched region). An appropriate value of the inertia weight usually provides a good balance between the exploration and the exploitation and thus results in a better optimal solution. Thevidoldis the particle's previous velocity and thevidnewis the particle's updated velocity. Thexidnewandxidoldare updated and previous positions, respectively, of the particles. For each particle, the velocity is confined within a range of [Vmin, Vmax] to prevent the particle from flying out of the search space. If sum of the accelerations causes the velocity of that dimension to exceed a predefined limit, the velocity of that dimension is clamped to a defined range.As discussed above, the PSO was introduced by Eberhart and Kennedy as a search and optimization method to solve continuous problems [23]. Later, Kennedy and Eberhart [40] modified, and named it as binary PSO (BPSO), to accommodate binary variables and operate in a discrete search space. Here, a particle moves in a discrete search space in each dimension. The update of a particle represents changes of its bits and the velocity represents the probability with which a certain bit of particle's position will change to “1”. A Sigmoid function as shown in Eq. (4) is used to map the continuous valued velocity as shown in Eq. (2), in the range [0, 1]. The sigmoid function value of the velocity is given in Fig. 1.(4)Sig(vidnew)=11+e−vidnewwhereSig(vidnew)represents the probability that the dth bit is 1. The new position of a particle is updated by Eq. (5):(5)xidnew=1,ifrand(0,1)<S(vidnew)0,elsewhere rand is a random number between [0, 1].The BPSO has been widely used in a variety of applications owing to its flexibility and efficiency. The recent successful applications include the flowshop scheduling problem [58,59], and the vehicle routing problem [32]. In this paper, we use BPSO for feature selection problem. Hence, the position of a particle is defined as the binary vector Xi=(xi1, xi2, …, xid), where xid=1 if feature d is to be included in the feature subset, and 0 otherwise. The pseudo code of BPSO is given in Algorithm 1.Algorithm 1Binary particle swarm optimizationInput:c1, c2: Acceleration factors.w:Inertia weight.rand: Random number generation function.pbest, gbest: Personal and global best positions.X: A set of current positions of the particles in the swarm.S: Size of the swarm.Output:gbest particle and its fitness valueAlgorithm:1: Initialize algorithm parameters.2: Randomly initialize X of size S.3: Evaluate fitness of particles in X.4: Set pbest for each particle and gbest of the swarm.5: while maximum number of iterations do6:   forp=1 to Sdo7:     forq=1 to ddo8:       Update velocity of qthdimension of pthparticle using Eq. (2).9:       Update position of the pthparticle using Eqs. (4 and 5).10:     end for11:   end for12:   forp=1 to Sdo13:     iff(xp)>f(pbestp) then14:       pbestp=xp15:       iff(xp)>f(gbest) then16:         gbest=xp17:       end if18:     end if19:   end for20: end while21: Report gbest particle of the X as an output of the algorithm.Chaos is a stochastic, ergodicity, dynamic, deterministic, non-linear, and non-repetitive system that demonstrates a sensitive dependence on the initial conditions and also includes infinite unstable periodic motions [29,82]. Due to the nature of ergodicity, mixing properties of chaos, and non-repetition nature, it potentially carries out overall search at higher speeds than the stochastic ergodic search that is probabilistic in nature. Various researchers [18,19] integrate the chaotic systems with the optimization algorithms to enhance the search capability and prevent them from being trapped at the local minima solution. A variety of chaotic maps are introduced in the literature [25,33,51,78]. In this paper, we use logistic map [51] for chaotic sequence generation. Mathematically, logistic map is formulated as(6)Chiter+1=β×Chiter×(1−Chiter)Ch generates chaotic value in the range 0.0 and 1.0. In Eq. (6), Ch0 is generated randomly for each independent run, with Ch0 not being equal to 0, 0.25, 0.5, 0.75, 1 and β equal to 4. In Eq. (6) the parameter β controls the behavior of Chiter(as iter goes to infinity). The behavior of the logistic map for varying values of the parameter β is shown in Fig. 2. For low values of β(β<3), Ch eventually converges to a single number. When β=3, Ch oscillates between two values. This characteristic change in behavior is called bifurcation. For β>3, Ch goes through further bifurcations, eventually resulting in chaotic behavior. In fact, the bifurcation diagram is itself a fractal [43]. The change in chaotic number over 100 number of iterations is shown in Fig. 3.In biology, a mutation is basically a permanent alteration of the value of gene to some other value. In GA, a common approach to implement the mutation to change allel value of a gene involves a user-defined probability, known as pm, to decide whether a mutation shall take place or not. The purpose of mutation in GA is retaining and introducing diversity, which helps it to approach a better solution in the given search space. It is widely explored in the other meta-heuristic algorithms such as PSO [80,87], and artificial bee colony [45] to take its advantage and obtain a better solution to the problem. In this paper, we integrate mutation operator with the BPSO for feature selection.Opposition-based learning (OBL) is introduced by Tizhoosh [79] to improve quality of the candidate solution by simultaneously considering the solution as well as it's opposite solution.Traditionally, meta-heuristic algorithms start with some randomly generated initial solutions (initial population) and aim to improve the population toward the (near) global optimal solution(s). The process of searching terminates when some predefined criterion/criteria is/are satisfied. The convergence rate, hence the computation time is related to the distance of the optimal solution from the initial population. In absence of any information, the initial population is generated using the random guess. In the worst case, if the optimal solution is too far away from the random guess, it may not be reached in a reasonable time. However, the convergence rate can be improved by simultaneously considering the solution and it opposite solution [3,48,70] as the empirical study of Tizhoosh [79] indicates that 50% of the time a random guess is too far away from the optimal solution than its opposite guess. Therefore, starting with the initial population which consists of the best of the two guesses (random and its opposite) is far better. It increases the possibility of rate of convergence [48,70]. In this paper, a novel version of the BPSO is introduced which on one hand employs OBL strategy to start with good potential solutions and on the other hand uses it to diversify the search moves in case of stagnation of the best particle. The concept of opposite number, opposite points, and opposition-based initialization is defined as follows.Definition 1(Opposite number) Let x be a real number in an interval [l, u] (x∈[l, u]); the opposite numberx¯is defined by(7)x¯=u+l−xThis definition can be extended to higher dimensions [79,64] as follows.Definition 2(Opposite point) Let Xi=(xi1, xi2, …, xid) be a candidate solution in d-dimensional space, where (xi1, xi2, …, xid)∈X and xi1∈[li, ui] ∀i∈1, 2, ···, d. The opposite point of Xiis defined by(Xi¯)=((xi1¯),(xi2¯),…,(xid¯)).(8)Xi=ui+li−XiNow, with the opposite point definition, the opposition-based optimization can be defined as follows.Let Xi=(xi1, xi2, …, xid), a point in an d-dimensional space with xi∈[li, ui] ∀i∈1, 2, …, d, be a candidate solution. Assume f(x) is a fitness function, which is used to compute the candidate's optimality. According to opposite point definition, the candidate solutionXi¯=(xi1¯,xi2¯,…,xid¯)is the opposite of Xi=(xi1, xi2, …, xid). Now, iff(Xi¯)≥f(Xi), the candidate solution Xican be replaced by the solutionXi¯else continue with the solution Xi. Hence, the candidate solution and its opposite candidate solution are evaluated simultaneously to continue with the fitter one.Population initialization is the first and a crucial step in any optimization algorithm as it affects the convergence speed and quality of the final solution. The random initialization is the most frequently used approach to generate initial population in absence of any information about the solution. However, in this case too, an intelligent initialization method based on realistic approach may help. It has been shown experimentally [3,15,29,70,86] that the simultaneous consideration of the random positions and their opposite positions increase the chance of selecting good initial solutions and lowers the chance of exploring unfruitful regions in the search space. Therefore, integrating OBL with the BPSO is worth investigating. Here, a combined initial population of size 2S is generated using uniform random distribution and the OBL strategy and finally the fittest S particles (Out of the 2S particles) are included in the initial population. The pseudo code of opposition-based population initialization is given in Algorithm 2.Algorithm 2Opposition-based population initializationInput: NilOutput:X: a set of S particlesAlgorithm:1: {X} = Randomly generated S particles2: fori=1 to Sdo3:   forj=1 to ddo4:Xij¯=uj+lj−Xij5:   end for6: end for7:{OX}={X}∪{X¯}/* {OX} consists of 2S particles */8: Compute fitness of particles in {OX} using Eq. (9).9: Sort {OX} with fitness values.10: {X} = top({OX}/2) particles11: Return {X}In BPSO, the velocity update equation has three major components; previous velocity(vidold), cognitive component(pbestid−xidold), and social component(gbestd−xidold). Previous velocity stores the value of particle's velocity at the previous iteration, the cognitive component represents the information posses by the particle itself and social component represents the information retained in the swarm. Each of these components play an important role in the velocity update of the particle and consequently affect position of the particle. For a detailed description on this topic, refer [72]. In BPSO, the influence of previous velocity on the current velocity is controlled by inertia weightw. The {c1, rand} and {c2, rand} control weightage of the cognitive component and social component, respectively. In this paper, instead of using a fix value for these parameters, we use a dynamic strategy to update the value of these parameters.To improve search capability of the PSO, Shi and Eberhart [72] integrate an inertia weightwwith the particle's previous velocity to control exploration and exploitation in the search space. In this paper, a novel fitness based dynamic inertia weight strategy is introduced in to the BPSO to control the value of the inertia weight based on the current status of the particle. It uses fitness of each particle to dynamically set the value of the inertia weight for the particle. Here, the fitness value of the ith particle is measured as shown in Eq. (9).(9)fi=min1≤l≤Kmin1≤g≤K,l≠gd(cl,cg)max1≤k≤Kdiam(ck)where K is the number of clusters, d(cl, cg) is the distance between clusters cland cg(as shown in Eq. (10)), and diam(ck) is the diameter of the kth cluster (as shown in Eq. (11)).(10)d(cl,cg)=minx∈cl,y∈cgdist(x,y)(11)diam(ck)=maxx,y∈ckd(x,y)As it seeks clusters with high intra-cluster similarity and low inter-cluster similarity, particles that produce clusters with high f are considered more preferable.Fitness based probability computation has been used by researchers in the artificial bee colony algorithm [36,45]. The fitness based dynamic inertia weight in the BPSO assigns high inertia weights to the low fit particles to facilitate exploration of the unknown regions in the search space and assigns low inertia weights to the high fit particles to exploit the searched region in the search space. Here, the inertia weight of each particle is adjusted as(12)Propi=0.9×fifbest+0.1(13)fwi=1.1−Propiwhere fiand fbestare fitness of the ith and global best (gbest) particles, respectively. The modified velocity update equation based on the fitness based dynamic inertia weight is formulated as(14)vidnew=fwi×vidold+c1×rand(0,1)×(pbestid−xidold)+c2×rand(0,1)×(gbestd−xidold)Integration of the fitness based dynamic inertia weight with velocity update equation facilitates dynamic adjustment of the velocity based on the current status of a particle. A higher value of the fw increases the velocity (step size) of the low fit particle, which facilitates global exploration in the search space, whereas, a lower value of the fw lowers the step size of the high fit particle, which facilitates exploitation of nearby place in the searched region. The introduction of fitness based adaptive inertia weight in the BPSO has been attempted by other researchers too [6,90].As discussed above, the second and third parts of the velocity update equation reflect the particle's cognitive and social information, respectively. Acceleration factors used with the cognitive and the social components affect movement of the particle significantly. A large value of the acceleration factor c1 makes a particle to fly towards its own ever known best position (pbest) faster and a large value of the acceleration factor c2 makes the particle to move towards the best particle (gbest) faster. Usually, c1 and c2 are set equal to give equal weightage to both the cognitive component and the social component [16,17,39]. In this paper, we vary the value of acceleration factors to control the influence of cognitive information and social information on the velocity update equation. To avoid stagnation of the particles at the local optimal solution and explore the undiscovered regions in the search space an opposition-based replacement strategy is used. Here, opposite position of the gbest particle is generated and is used to replace the worst fit particle. It helps to retain the good solution in the form of gbest particle and explore the undiscovered region in the search space using the worst fit particle (gworst). For all particles, except for the (gworst) particle, c1 and c2 are set to 2 and for the (gworst) particle c1 and c2 are set to 3 and 1, respectively. It draws movement of the particle in the direction of the updated particle rather than the gbest, which has been stagnated at a (probably) local optimal solution. This strategy helps to avoid stagnation of the particle at a local optimal solution and increases the possibility of attaining (near) global optimum solution. The unbalanced setting of c1 and c2 is also used by [81] to draw the direction of the newly updated particle towards its own pbest.In the area of optimization, chaotic strategies have been exploited in meta-heuristic methods for the global optimization problem [5,9,10,16,18,19]. This strategy avoids entrapment of an individual at an undesirable local minimum solution by making use of the chaotic behavior. Due to the non repetition of the chaos, search speed of the chaotic maps are generally faster than the stochastic ones. Recently, various researchers have investigated the use of chaotic dynamic in the various versions of the PSO [5,16,18,19]. In this paper, chaotic map is integrated with the BPSO. Generating stochastic sequences with a long period and good uniformity is very important for a meta-heuristic algorithm. In BPSO, the stochastic parameter rand is the key component affecting the convergence behavior. Here, chaotic sequence generated by the logistic map (refer Eq. (6)) [5] is used to replace the stochastic parameter rand in the velocity update equation (refer Eq. (2)). The modified velocity update equation based on the logistic map is formulated as(15)vidnew=w×vidold+c1×Ch×(pbestid−xidold)+c2×(1−Ch)×(gbestd−xidold)The value of Ch varies between 0 and 1. The value of chaos over 100 number of iteration is shown in Fig. 3.As discussed above, the particle's position update strategy is significantly influenced by its own best solution found so far, denoted as pbest, and the swarm's best solution, represented as gbest. The steering of the particles, by the gbest, towards its direction may be advantageous or disadvantageous depending on the condition. It proves to be effective when gbest is near to the global optimum solution in the search space and is ineffective, rather destructive, if it is near to a suboptimal solution. In the latter case, it will direct the swarm in the direction of the suboptimal solution. In order to explore unexplored regions of the search space and get rid of the suboptimal solution, it is suggested in [29,64,70,80,86,87] to bring out small random mistakes or consider opposite direction of the particle. In this paper, we apply mutation or opposition on the gbest particle to achieve the task. Here, we replace the worst fit particle with the mutated or the opposite gbest. It helps the algorithm to explore undiscovered regions using the worst fit particle (updated with the mutated or opposite gbest particle) and at the same time retains the global optimal solution with the gbest particle because no alteration is done in the gbest particle itself. The algorithmic flow of the mutation strategy and opposition are given in Algorithms 3 and 4, respectively.Here, the applicability of the mutation or opposition is tested based on a defined condition. A parameter count is used for this testing. If the value of count is less (<) than the δ then mutation is applied. In the mutation (refer, Algorithm 3), the gbest and the worst fit particle gworst are submitted as input. The variable temp is used to store the gbest particle, which is mutated to replace the gworst. Each dimension of the temp is checked for the mutation. For this, a random number is generated between 0 and 1. If a randomly generated number is less than the probability of mutation pm (=1/d), the selected dimension is mutated. As the rate of mutation decides the number of dimensions to be mutated, it should be chosen carefully to introduce some variation in the particle instead of introducing too much randomization in it. It has been shown effective by the Lee el al. in the BPSO [44]. In this paper, the mutation probability pm is set at 1/d, which means that at least one of the bits in the position vector is changed [85].Algorithm 3Mutation strategyInput:gbest: Best particle in the swarm.gworst: Worst particle in the swarm.Output:gworst: Updated gworst particle (mutated gbest).Algorithm:1: temp=gbest2: forj=1 to ddo3:   ifrand≤pmthen4:tempj=tempj¯5:   end if6: end for7: gworst=temp8: return updated worst fit particle gworst.For count equal to or greater than (≥) the δ opposition strategy is used. In opposition algorithm (refer, Algorithm 4), the gbest and the worst fit particle gworst are submitted as input and the temp is used to store the opposite position of the gbest particle, which is used to replace the gworst after the update. First, gbest is copied into a temporary variable temp then opposite position of the gbest particle is generated, which replaces the worst fit particle in the swarm.Algorithm 4Opposition strategyInput:gbest: Best particle in the swarm.gworst: Worst particle in the swarm.Output:gworst: Updated worst fit particle (opposite of the gbest).Algorithm:1: temp=gbest2: forj=1 to ddo3:   tempj=uj−lj+tempj4: end for5: gworst=tempj6: return updated worst fit particle.This section presents a detailed description of the proposed method for the feature selection. The motivation is to use the fortified global search algorithm BPSO for the feature selection in the text clustering. A descriptive summary of the proposed methodology is presented below and an algorithmic flow is given in Algorithm 5.The algorithm starts with setting of the BPSO's parameters and chaotic number (line 1–2). At the next step, a population is initialized to start with. Here, opposition-based initialization is used for the population initialization (line 3). For algorithmic flow of the opposition-based initialization strategy, refer Algorithm 2. It obtains a set of good solutions (particles) spread over the search area. Each particle contains a combination of the feature subset selected from the original feature space. Next, the pbest, the gbest and the gworst are selected based on the fitness information (line 4). Initially, current position is treated as the personal best position (pbest) as there is no history of traverse at initial step, and the best and the worst positions in the swarm are selected as the global best position (gbest) and the global worst (gworst) positions, respectively. Next, the velocity and position of each particle is updated using Eq. (15), and Eqs. (4) and (5), respectively. As discussed in the previous subsections, the velocity update equation contains three parts velocity, cognitive component, and social component. The parameter inertia weight is attached with the velocity component, (c1, rand) is attached with the cognitive component, and (c2, rand) is attached with the social component. As the parameters’ values directly affect searching capability of the BPSO, different strategies are used by researchers to adaptively and intelligently set these values [18,19]. In this work also, different strategies are employed for the same. Here, the inertia weight is updated adaptively based on the fitness of the particle using Eq. (12). Next are the acceleration factors. The acceleration factors c1 and c2 play a major role in the movement of its own known direction (pbest) or in the neighbourhood known direction (gbest), respectively. An equal value of c1 and c2 gives equal weightage to both the components. In case of unequal value, either of the two components get higher chance to influence the particle movement. Here, two different settings are used for the acceleration factors. If count is greater than (>) δ, and the selected particle is gworst, condition 2 is used, else condition 1 is used (line 8-12), i.e., the unequal weightage is applied only to the gworst particle if the population stagnates. In condition 1, both the cognitive and the social components are given equal weightage by setting equal value to both c1 and c2 (c1=c2=2), whereas in condition 2, by setting c1 and c2 to 3 and 1, respectively, the cognitive component is given more weightage. The unequal weightage is applied only in the case of stagnation of the particle. It helps the newly generated particle to avoid drifting towards the gbest and explore a new region in the search space. Hence, it avoids stagnation of the swarm at a suboptimal solution. Next, the chaotic number that replaces the parameter rand in the velocity update equation (refer, Eq. (2)), inertia weight, c1, and c2 are computed for each particle using the above discussed methods to update their velocities and positions using Eq. (15), and Eqs. (4) and (5) (line 13–17). It is repeated for each of the particle (line 6–18). The pbest, the gbest, and the gworst are updated as follows. If fitness of a particle is better than its pbest, it replaces the pbest (line 20–21). In the above case, it is compared with the gbest also and replace it if it is better than the gbest (line 22–24). Afterwards, the gbest is observed for an improvement. The worst fit solution in the swarm is set as the gworst particle (line 26-28). If there is no improvement in it the value of count is incremented by 1 else it is reset to 0 (line 30–34). Next, the gworst particle is considered for a mutation or an opposition-based initialization (line 35). If count is less than δ then mutation is applied (line 37) otherwise the opposition-based initialization is used (line 39). Refer Algorithm 3 and 4 for mutation and opposition-based initialization, respectively. The process of updating the velocity, position, pbest, gbest, gworst, and count is repeated until a predefined number of iterations is met (line 5–41). At the end gbest particle is reported as a final solution of the algorithm, which contains the selected combination of the feature subset.Algorithm 5Proposed BPSO-based method for feature selectionInput:c1, c2: Acceleration factorsfw: Inertia weight.Ch: Chaotic random number generator.pbest, gbest, gworst: Personal best positions and global best and worst position, respectively.X: A set of current positions of the particles in the swarm.count: Holds gbest status. /*Initially set to 0 */S: Size of the swarm.Output:gbest particle and its fitness valueAlgorithm:1: Initialize algorithm parameters.2: Randomly generate Ch0.3: Apply opposition-based initialization strategy using Algorithm 2.4: Set pbest for each particle and gbest and gworst in the swarm.5: while maximum number of iterations (MaxIter) do6:   forp=1 to Sdo7:     Compute inertia weight fwpusing Eqs. (12 and 13)8:     ifcount>δ and xpis gworst particle then9:       Assign unequal weight to c1 and c2 (c1=3, c2=1 (refer condition 2 in Table 2)).10:     else11:       Assign equal weight to c1 and c2 (c1=c2=2 (refer condition 1 in Table 2)).12:     end if13:     forq=1 to ddo14:       Update Chqusing Eq. (6).15:       Update the velocity of each dimension using Eq. (15).16:       Update the position of each dimension using Eqs. (4) and (5).17:     end for18:   end for19:   forp=1 to Sdo20:     iff(xp)>f(pbestp) then21:       pbestp=xp22:       iff(xp)>f(gbest) then23:         gbest=xp24:       bf end if25:     bf end if26:     iff(xp)<f(gworst) then27:       gworst=xp28:     end if29:   end for30:   if there is no change in the gbestthen31:     Increment count by 1.32:   else33:     Reset count to 0.34:   end if35:   Select the worst fit particle gworst.36:   ifcount<δthen37:     Apply mutation using Algorithm 3.38:   else39:     Apply opposition-based initialization using Algorithm 4.40:   end if41: end while42: Report gbest particle as final solution of the algorithm.Initially, datasets are pre-processed with the standard preprocessing steps such as tokenization, stop word removal, and stemming to convert the documents into a suitable format. After processing the documents with the preprocessing steps (refer, Section 2.1), the proposed feature selection method (refer, Section 3.4) is applied to select an informative subset of the features. A subset of good discriminative features not only increases efficacy of the underlying algorithm but also reduces its computational complexity [4,57,71,83]. In this paper, we use meta-heuristic algorithm BPSO for the same. Like other meta-heuristics, BPSO is also computationally expensive on high dimensional feature space. It is especially true for the text clustering as text documents are very high dimensional by nature. Therefore, to prove efficacy of the proposed method in time constraint, we test our method on 1% dimensions of the original feature space. Instead of random selection, we use term variance measure (TV) to rank each feature. TV is one of the effective and efficient unsupervised feature selection measure [47]. It quantifies relevance of each feature based on the deviation of the feature from its mean value. Suppose, the corpus consists of op{p=1, 2, 3, …, n} documents and tf{f=1, 2, 3, …, L} terms. The TV of the fth term is defined as(16)TVf=1n∑p=1n(Tfj−Tf¯)Here n is total number of the documents andTf¯=((1/n)∑p=1nTfp)is mean of the feature Tf.Next, features are sorted in the descending order based on their relevance score and the proposed BPSO based feature selection method (refer, Section 3.4) is applied on top 1% of the features to obtain a discriminative/informative feature subset. Finally, the clustering is performed using the k-means with the above obtained informative features subset. A complete process to cluster the documents, hereinafter referred as the proposed method (PM), is presented in Algorithm 6.Algorithm 6Proposed method (PM) to cluster the documents.Input: Set of the documents.Output: Clusters of the documents.Algorithm:1: Preprocess the documents with preprocessing steps (refer, Section 2.1).2: Convert textual documents into numerical format using Eq. (1).3: Select top ranked features using TV (refer, Eq. (16)).4: Apply proposed BPSO based feature selection method (refer, Algorithm 5).5: Apply the k-means on selected feature subspace.6: Return the set of clusters.This section presents results and discussion along with the datasets, setting of the parameters and evaluation criteria. All experiments are performed in Windows 7 environment on a machine having core i7 processor and 2GB RAM. All the algorithms are coded in MATLAB.All experiments are conducted on three different benchmark datasets Reuters-21,578, Classic4, and WebKB. These datasets are pre-classified into several categories. However, this information is not included during the clustering process; it is used only to assess worthiness of the clustering results. A descriptive summary of the these datasets is presented below and a tabulated summary is presented in Table 1.The Reuters-21,578 collection33http://www.daviddlewis.com/resources/testcollections/reuters21578/.is a set of news published by Reuters newswire in 1987. It consists of 21,578 documents, which are distributed non-uniformly over 135 thematic categories. We randomly select 1339 documents belonging to eight different categories for our experiments.Classic dataset has two versions Classic3 and Classic4.44http://www.dataminingresearch.com/index.php/2010/09/classic3-classic4-datasets/.In this study, we use Classic4 dataset. The dataset contains 7095 documents, where each document belongs to any of the four categories CACM, CRAN, CISI, and MED. We randomly select 2000 documents (500 documents from each category) for our experimental analysis.The WebKB55http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/.(World Wide Knowledge Base) is collected by Craven in 1998. It contains 8,282 web pages gathered from the four academic domains. The original dataset has seven categories, but only four of them course, faculty, project and student are used. In our experiment, we select randomly 2803 documents belonging to the four categories.We randomly select subsets of documents from original feature space to reduce computational requirements of the algorithms; it does not alter properties of the dataset. The statistical summary of all datasets used in this paper is presented in Table 1. Here, sparsity of the dataset is evaluated by a sparsity measure proposed by [11]. As it is evident from the Table 1, all the datasets considerably differ in the number of documents, features, skewness, and sparsity; it makes them a suitable choice for varied and valid experiments.Experiments are conducted to compare five meta-heuristic algorithms binary particle swarm optimization (BPSO)[18], chaotic binary particle swarm optimization (CBPSO) [18], simple genetic algorithm (SGA) [56], adaptive inertia weight particle swarm optimization (AIWPSO) [55], and the PM. The considered algorithms BPSO, CBPSO, SGA, AIWPSO, and the PM use different adjustable parameters. The parameters for the competitive methods have been considered from the respective papers as researchers suggest them based on their empirical analysis. However, the parameters, which are specific to the PM, are set through our empirical analysis. The parameters values adopted in the paper are listed in Table 2.To evaluate effectiveness of the PM four clustering accuracy measures are used. A detailed description of these measures is presented in the subsequent section. All the experiments are repeated ten different times and the best (B), worst (W), average (A), and standard deviation (SD) of the results are reported for a comparative analysis.Precision (P), recall (R), and F-score (F) are widely used metrics in the text mining literature for the text categorization, e.g., [83,28]. Precision measures total number of correct positive predictions to the total numbers of positive predictions and recall measures total number of correct positive predictions to the total number of positive documents. F-score is a harmonic combination of P and R. For a category i, the P, R, and F are defined as shown in Eqs. (17)–(19), respectively.(17)Pi=TPiTPi+FPi(18)Ri=TPiTPi+FNi(19)Fi=2×Pi×RiPi+RiIn this paper, we use four evaluation metrics macro precision (Pmacro), macro recall (Rmacro), macro F-score (Fmacro), and micro F-score (Fmicro) to evaluate effectiveness of the proposed algorithm. The macro and micro average scores summarize effectiveness of the considered algorithm over all categories. Mathematically, macro and micro average scores are computed as shown in Eqs. (20)–(23).(20)Pmacro=∑i=1kPiK(21)Rmacro=∑i=1kRiK(22)Fmacro=∑i=1kFiK(23)Fmicro=2×P¯×R¯P¯+R¯HereP¯andR¯are precision and recall values, respectively, across all the classes.

@&#CONCLUSIONS@&#
