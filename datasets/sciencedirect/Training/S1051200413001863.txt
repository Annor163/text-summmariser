@&#MAIN-TITLE@&#
Context-aware Discriminative Vocabulary Tree Learning for mobile landmark recognition

@&#HIGHLIGHTS@&#
Visual content and mobile device context are integrated for image recognition.A server-client prototype for mobile image recognition is developed.A Context-aware Discriminative Vocabulary Tree Learning algorithm is proposed.

@&#KEYPHRASES@&#
Mobile landmark recognition,Content and context integration,Weighted hierarchical clustering,

@&#ABSTRACT@&#
Recently, mobile landmark recognition has become one of the emerging applications in mobile media, offering landmark information and e-commerce opportunities to both mobile users and business owners. Existing mobile landmark recognition techniques mainly use GPS (Global Positioning System) location information to obtain a shortlist of database landmark images nearby the query image, followed by visual content analysis within the shortlist. This is insufficient since (i) GPS data often has large errors in dense build-up areas, and (ii) direction data that can be acquired from mobile devices is underutilized to further improve recognition. In this paper, we propose to integrate content and context in an effective and efficient vocabulary tree framework. Specifically, visual content and two types of mobile context: location and direction, can be integrated by the proposed Context-aware Discriminative Vocabulary Tree Learning (CDVTL) algorithm. The experimental results show that the proposed mobile landmark recognition method outperforms the state-of-the-art methods by about6%,21%and13%on NTU Landmark-50, PKU Landmark-198 and the large-scale San Francisco landmark dataset, respectively.

@&#INTRODUCTION@&#
Recent years have witnessed the phenomenal growth in the usage of mobile devices. Nowadays, most mobile phones in use have the camera feature. The built-in camera and network connectivity of current mobile phones makes it increasingly appealing for the users to snap pictures of landmarks, and obtain relevant information about the captured landmarks. This is particularly useful in applications such as mobile landmark identification for tourists guide, mobile shopping, and mobile image annotation. An illustration of a mobile recognition system for tourist guide is shown in Fig. 1. The mobile user captures a landmark image but he/she has no idea about the landmark. Then the user uploads it to the server and, in a moment, the related information of the captured image is returned to the mobile user, e.g. the landmark name, the location in the map, the description of the landmark, recent events, etc. The returned shortlist contains the best matched images and multiple images corresponding to the same category are only shown with best matched one. In such a way, the user will probably find the correct landmark category by simply scrolling the screen in a few times. Landmark recognition (e.g. [1–3]) is closely related to landmark classification [4] and landmark search [5] problems, and can be broadly categorized into non-mobile and mobile landmark recognition systems.In recent years, a number of non-mobile landmark vision systems have been developed [6,4], which focused on learning a statistical model for mapping image content features to classification labels. In [6], a scalable vocabulary tree (SVT) is generated by hierarchically clustering local descriptors as follows: (1) an initial k-means clustering is performed on the local descriptors of the training data to obtain the first-level k-clusters, (2) the same clustering process is applied recursively to each cluster at the current level. This will lead to the formation of a hierarchical k-means clustering structure, also known as a VT in this context. Based on the VT, the high-dimensional histograms of image local descriptors can be generated, which enables efficient image recognition. In [4], non-visual information such as textual tags and temporal constraints are utilized to improve the performance of content analysis.However, these landmark recognition methods underutilize the unique features of mobile devices, such as location, direction and time information. These context information can be easily acquired from mobile devices, which can further enhance the performance of content analysis without introducing much additional computational cost. In the mobile landmark recognition scenario, utilizing either content or context analysis alone is suboptimal. GPS location is able to help discriminate visually similar landmark categories that are far away from each other. However, location information should not be used alone due to the GPS error. As such, the context information must be used in conjunction with visual content analysis in mobile landmark recognition.In order to further enhance the performance, it is therefore imperative to combine the context information with content analysis in mobile systems. In [7], the content of mobile images is analyzed based on local descriptors, and the user interaction as a source of context information is considered. In [8–11,1], the GPS location information is utilized to assist in content-based mobile image recognition. In these methods, the content analysis is essentially filtered by a pre-defined area centered at the logged GPS location of the query image. With the aid of the location information, the challenge in differentiating similar images that are captured in different areas can be reduced substantially. Recent works in [5,12] demonstrate that GPS can be utilized to improve SVT-based mobile landmark recognition [6]. In these systems, the candidate images for the query image are obtained from the landmark image database by using GPS information. This is done by selecting those landmark images that are located close to the mobile device when taking the query image. Visual content analysis is then performed in the GPS-based shortlist for the final recognition. This approach makes it easy to differentiate visually similar images of different landmark categories that are far away from each other.However, the approach of performing content analysis within the GPS-based shortlist still has some drawbacks that make it nonideal in real applications: (1) it is elusive to determine the optimal search radius since the GPS error of the captured image may vary significantly from open areas (about 30 meters) to dense build-up areas (up to 150 meters). In the event that GPS error of the captured image is too large, the correct landmark category may be filtered out, which will result in wrong recognition, and (2) direction information is not utilized to further improve the recognition performance, which can be easily acquired from the digital compass on mobile devices.In order to circumvent the approach of performing content analysis within a GPS-based shortlist as in most existing mobile recognition systems as well as to have the flexibility of incorporating different types of context, we propose to integrate various content and context information by Context-aware Discriminative Vocabulary Tree Learning (CDVTL). As compared to the well-known approach of using SVT built upon SIFT descriptors [6], we improve the recognition performance by incorporating various context descriptors. Context information adopted in this work includes location information (obtained through built-in GPS or WiFi localization) and direction information (obtained from digital compass).In this paper, we present a context-aware mobile landmark recognition framework as shown in Fig. 2, which enables efficient mobile landmark recognition in both context-aware and context-agnostic environments.In Fig. 2, the application on the mobile device provides the user interface which can upload the photo along with its context information such as GPS and direction to the server, while all the feature extraction and recognition tasks are processed in the server side. In such a way, both feature extraction and recognition are fast and the efficiency is independent to the computational capacities of different mobile devices. The battery energy is also saved by avoiding the computation on the mobile devices. Moreover, this architecture makes it easier to migrate the mobile image recognition across different mobile platforms. In this work, a prototype for context-aware mobile image recognition is developed and its efficiency is validated using the developed Android application.The recognition procedure is different from the traditional mobile image recognition pipeline: confine the candidate images to a radius of several hundred meters centered at the location of the mobile device, and then perform content-based recognition within the shortlist. This pre-filtering is less than ideal because it is elusive to determine the search range centered at the logged location and the range should not be a constant. In this work, the pre-filtering by location is circumvented, that is, image content descriptors and context information are pre-combined without the pre-filtering step. The proposed framework has achieved superior recognition precision with low computational cost comparing with some state-of-the-art mobile vision systems to recognize landmarks from university-scale to city-scale landmark datasets.We also propose a Context-aware Discriminative Vocabulary Tree Learning (CDVTL) algorithm that can be embedded in the proposed framework. In landmark recognition scenarios, the location information can be more important within a region with a radius of hundreds of meters, while less reliable in a region within one hundred meters due to the GPS error. In contrast, image content descriptors are usually more discriminative in small regions for landmark recognition. In addition, for regions of the same scale, the image content descriptors may not be the same discriminative, e.g. it is more discriminative in suburban areas but less discriminative in build-up areas. In view of these observations, for each codeword in each level of the hierarchical clustering when training the vocabulary tree, we introduce a weighting scheme which evaluates the relative discriminative power between the context information and the image content descriptors. Intuitively, a small region with visually distinct landmarks is supposed to yield more weighting for image descriptors and less weighting for location, and vice versa. In this work, we use 128-D SIFT as the content descriptor, and 2-D GPS location (longitude and latitude) and 1-D direction as the context descriptors. The context descriptors are concatenated to each of its SIFT descriptor to form a 130-D (only GPS is available) or 131-D (both GPS and compass are available) hybrid descriptors, where SIFT, GPS and direction components have their respective weights.In order to demonstrate the effectiveness of combining content and context features for image recognition, we randomly selected 60 images from the categories “Administrative building”, “Nanyang auditorium” and “Cafe express” in NTU landmark dataset. It is a typical dataset to compare different methods although the classification on this small-scale dataset is relatively easier than that on the whole dataset. The example images for the 3 landmarks are shown in Fig. 3(a)–(c), respectively, and are briefed as categories 1, 2 and 3 here. We can notice that categories 1 and 2 are visually very similar, however, they are different buildings far away from each other. In contrast, categories 2 and 3 are located nearby sharing similar GPS coordinates, however, they are visually dissimilar.In Fig. 4(a)–(b), we show 2D feature for the purpose of illustration, where different shapes indicate the feature extracted from different categories. In Fig. 4(a), the average pixel values in R and B channels are adopted as the 2D content feature for each image, which are clustered by using K-means clustering. By observing the Voronoi graph, we can see that the 2D content features are not well partitioned between category 1 and category 2 because the images are visually similar, while the content features from category 3 are well separated from categories 1 and 2. In Fig. 4(b), the 2D GPS coordinates of images are clustered by K-means clustering. By observing the Voronoi graph, we can observe that the 2D GPS features are not well partitioned between categories 2 and 3 because the images are geometrically nearby, while the GPS feature from category 1 is distinguished. In Fig. 4(c), the 4D composite feature is formed by concatenating the 2D R–B feature and 2D GPS coordinates, and is also clustered by K-means clustering. In order to illustrate the clustering results, the 4D composite feature is only shown in 2D geometrical point of view in Fig. 4(c). For visualization, the feature points allocated to different clusters are marked by different colors. We can observe that the feature points are correctly grouped into their own category. In contrast, in Fig. 4(a) and (b), the feature points are not well grouped into their own category overall. Therefore, by considering both the content and context information, we can obtain a better representation of the mobile image for the subsequent recognition.Using K-means as the flat clustering method in each layer of hierarchical clustering, a VT is generated from the composite content–context image features. VT trees are illustrated In Fig. 5, where the gray level of each node indicates the weight for GPS component (the darker the higher). In Fig. 5(a), the traditional VT-based mobile landmark recognition [5,12] is shown, where the coarse clusters are partitioned by location first, and the finer clusters are obtained by soly visual content descriptors. However, as shown in Fig. 4, this scheme is not optimal. In Fig. 5(b), GPS is considered together with content feature in each cluster in each level, and the weight for each component is generally non-zero values. For example, in the coarse cluster I, the visual content is important and location is less useful, so the node I has very low location weight; in the cluster III, location weight is high since it can perfectly separate the patterns; while in the cluster II, content and GPS weights are almost equal so the node is gray. We can notice that the existing mobile vision systems based on VT matching are essentially a special case of content–context weighing in the proposed hybrid recognition framework.

@&#CONCLUSIONS@&#
We proposed a new mobile landmark recognition framework that can integrate various mobile content and context information by Context-aware Discriminative Vocabulary Tree Learning (CDVTL). As compared to content analysis based on GPS shortlist that is adopted in most existing mobile landmark recognition systems, the proposed CDVTL scheme can significantly improve the recognition performance while not introducing much additional computational cost. In addition, the proposed content and context integration framework has the flexibility to incorporate any types of content and context information when they are available in the mobile device. Future work may include integrating other context information (e.g. time) to further improve the recognition performance.