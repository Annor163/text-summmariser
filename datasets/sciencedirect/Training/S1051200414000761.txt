@&#MAIN-TITLE@&#
Robust feature extraction based on an asymmetric level-dependent auditory filterbank and a subband spectrum enhancement technique

@&#HIGHLIGHTS@&#
We propose cGCFB-based robust cepstral feature (RCGCC) for speech recognition tasks.A sigmoid-shaped suppression rule is introduced for auditory spectrum enhancement.Short-term cepstral mean and scale normalization is proposed to reduce mismatch.Performance evaluation is carried out on the AURORA-2, -4 and -5 corpora.Proposed RCGCC outperformed other front-ends in real-time reverberant environment.

@&#KEYPHRASES@&#
Speech recognition,Compressive gammachirp,Auditory spectrum enhancement,Feature normalization,

@&#ABSTRACT@&#
In this paper we introduce a robust feature extractor, dubbed as robust compressive gammachirp filterbank cepstral coefficients (RCGCC), based on an asymmetric and level-dependent compressive gammachirp filterbank and a sigmoid shape weighting rule for the enhancement of speech spectra in the auditory domain. The goal of this work is to improve the robustness of speech recognition systems in additive noise and real-time reverberant environments. As a post processing scheme we employ a short-time feature normalization technique called short-time cepstral mean and scale normalization (STCMSN), which, by adjusting the scale and mean of cepstral features, reduces the difference of cepstra between the training and test environments. For performance evaluation, in the context of speech recognition, of the proposed feature extractor we use the standard noisy AURORA-2 connected digit corpus, the meeting recorder digits (MRDs) subset of the AURORA-5 corpus, and the AURORA-4 LVCSR corpus, which represent additive noise, reverberant acoustic conditions and additive noise as well as different microphone channel conditions, respectively. The ETSI advanced front-end (ETSI-AFE), the recently proposed power normalized cepstral coefficients (PNCC), conventional MFCC and PLP features are used for comparison purposes. Experimental speech recognition results demonstrate that the proposed method is robust against both additive and reverberant environments. The proposed method provides comparable results to that of the ETSI-AFE and PNCC on the AURORA-2 as well as AURORA-4 corpora and provides considerable improvements with respect to the other feature extractors on the AURORA-5 corpus.

@&#INTRODUCTION@&#
Speech intelligibility as well as the performance of speech recognition systems degrades in practical environments due to a variety of signal variabilities. Additive noise and reverberation are the important causes of signal variabilities. Additive noise from interfering noise sources and convolution noise arising from acoustic environments mainly cause a reduction of speech recognition performance.The acoustic features most commonly used in speech recognition systems are Mel Frequency Cepstral Coefficients (MFCC) [1] and Perceptual Linear Prediction (PLP) [2] features. Both MFCC and PLP front-ends perform well in matched environments, where speech data are collected from reasonably clean environments. However, their performance degrades severely when the testing environment is different from the training environment. Degradation of performance due to mismatched environments has been a barrier for deployment of speech recognition technologies. Various sources give rise to this mismatch, such as background noise, channel/handset distortion, room reverberation. Most of the sources of speech variability produce additive distortion (e.g., background noise) and/or convolutional distortion (e.g., channel/handset mismatch) in the speech signal. Among all different additive noises, a multi-speaker or babble noise environment, where the interference is speech from speakers in the vicinity, is one of the most challenging noise conditions. Therefore, it is necessary to address this problem, i.e., performance degradation due to mismatch environments, to enable the deployment of recognition systems in real world conditions.Various research has been reported in the literature to improve the robustness of speech recognition systems under additive noise and reverberation. The methods to compensate for the effects of environmental mismatch can be implemented at the front-end (feature domain techniques) or at the back-end (model domain techniques) or both. The model domain methods adapt each acoustic model to make it fit better to the mismatched acoustic environment so that the adapted models will be able to classify the mismatched speech features collected in the testing environment. The typical examples of this category include the well-known noise masking [20], speech and noise decomposition (SND) [21], vector tailor series (VTS) [22], maximum a posteriori (MAP) [23,24], maximum likelihood linear regression (MLLR) [25], model-based stochastic matching [26], statistical reestimation (STAR) [27–29], parallel model combination (PMC) [30], optimal likelihood weighting based on the criteria of minimum classification error (MCE) and maximum mutual information (MMI) [31], etc.The objective of feature domain techniques is to make features more consistent in diverse environmental conditions. Feature domain methods can be classified into two subgroups. One subgroup of feature domain techniques aims at modifying the test features and making those features match the acoustic conditions better for the trained models: speech enhancement methods [32–37], codeword dependent cepstral normalization (CDCN) [38], feature-based stochastic mapping [26], multivariate Gaussian based cepstral normalization (RATZ) [39], feature normalization methods, such as cepstral mean normalization (CMN) [7,11,40], the stereo-based piecewise linear compensation for environment (SPLICE) [41,42]. The other subgroup of feature domain techniques, on the other hand, aims at making a special robust speech feature representation, which is used for both training and testing, to reduce the sensitivity to the various acoustic conditions. This paper deals with the latter subgroup. Robust feature extractors are usually obtained either by appending a pre-processing step, like speech enhancement [8,9,14,15], or by incorporating algorithms in an MFCC or PLP computation framework such as PNCC [3], amplitude modulation-based cepstral features [43,44], frequency masking [9], or by adding a post-processing step, like feature normalization techniques [7,11] (e.g., cepstral mean normalization (CMN)) or by combining any two or all of the above mentioned steps [3,4]. Most of the front-ends use, in addition to other techniques for environmental mismatch compensation, a feature normalization technique, at the least CMN, as a post-processing scheme.Additive noise reduction approaches usually have a tradeoff between the amount of noise reduction and speech distortion induced due to processing of a speech signal. At very low SNR the intensity of this induced distortion is high, thereby deteriorating the performance of the speech recognition systems. Compensation of reverberant noise is usually done by dereverberation, which can be obtained by inverse filtering the impulse response of the room [12,13]. However, room impulse response is dependent on the distance between the speaker and the microphone and on the conditions of the room. Therefore, extracting a common set of robust features, which can perform well at low SNRs and also can handle various room impulse responses, is a difficult and challenging task.To deal with additive noise distortion various speech enhancement methods have been proposed in [8,9,14,15]. In [10,12,13] several approaches have been proposed for handling convolutional noise distortions. The ETSI-AFE, described in [4], uses a two-stage Wiener filter and a blind equalization technique, which is based on the comparison to a flat spectrum and the application of the LMS algorithm, for improving robustness of ASR systems against additive noise distortions and channel effects. The PNCC technique, proposed in [3], includes the use of a gammatone filter-bank (GTFB) and a power law nonlinearity in place of the Mel filter-bank and log nonlinearity, used in conventional MFCCs framework, a medium duration power bias subtraction technique, for noise reduction, based on the arithmetic mean (AM)–geometric mean (GM) ratio and cepstral mean normalization as a post-processing scheme for DC offset removal, for robust feature extraction.In this work, for robust features extraction, we propose to enhance the speech auditory spectrum using a weighting rule based on the subband a posteriori signal-to-noise ratio (SNR). In order to allow a realistic and controllable frequency-domain asymmetry and to model most of the level dependency observed in basilar membrane (BM) filtering, the proposed method includes the use of a compressive gammachirp filter-bank (cGCFB) [16] for auditory spectral analysis. We use a power function nonlinearity as it has been found in [3] that it is more robust than the logarithmic nonlinearity used in a conventional MFCC framework. As a post-processing scheme for the normalization of the features, we use the short-time cepstral mean and scale normalization (STCMSN) technique, proposed in [7]. Feature normalization is normally performed over the whole utterance with the assumption that the channel effect is constant over the entire utterance, such as CMN (or CMVN). Also, normalizing a feature vector over the entire utterance is not a feasible solution in real-time applications as it causes an unnecessarily long processing delay. To relax this assumption and to reduce the processing delay, cepstral features in the proposed method are normalized over a sliding window of 1.5 s duration. In this paper, we denote this proposed feature extractor as the Robust Compressive Gammachirp filterbank Cepstral Coefficients (RCGCC). Replacing cGCFB with the GTFB in the proposed RCGCC feature extraction framework, we also present Robust Gammatone Filterbank Cepstral Coefficient (RGFCC) features to show the effectiveness of using cGCFB for the auditory spectral analysis in the proposed front-end. Experimental recognition results on the AURORA-2, AURORA-4, and AURORA-5 corpora demonstrate that the proposed RCGCC feature extractor outperforms the MFCC and PLP front-ends and provides comparable (and sometimes better) results to most state-of-the-art front-ends used in this work.The complete block diagram of the proposed robust compressive gammachirp filterbank cepstral coefficient (RCGCC) feature extractor for robust speech recognition is shown in Fig. 1. In the RCGCC feature extractor, processing of a speech signal begins with pre-processing (including DC removal and pre-emphasis, typically using a first-order high-pass filter). Short-time Fourier Transform (STFT) analysis is performed using a finite duration (25 ms) Hamming window with a frame shift of 10 ms to estimate the power spectrum of the signal. Compressive gammachirp filter-bank (cGCFB) integration is performed on both speech and noise power spectra for auditory spectral analysis. A sigmoid-shaped weighting rule is applied to enhance the auditory spectrum. The 13-dimensional static features, obtained after applying a power function nonlinearity with a coefficient of 1/15 and the discrete cosine transform (DCT), are normalized using the short-time cepstral mean and scale normalization (STCMSN) technique. A detailed description of various stages of the proposed method is given below.A Hamming-windowed direct spectrum estimator (i.e., the squared magnitude of the Fourier transform of the signal) is the most often used power spectrum estimation method for speech processing applications. For the m-th frame (frame length is 25 ms with a frame shift of 10 ms) and k-th frequency bin an estimate of the windowed periodogram can be expressed as:(1)Xˆ(m,k1)=|∑j=0N−1w(j)x(m,j)e−i2πjk1N|2,wherek1∈{0,1,…,K−1}denotes the frequency bin index, i is the imaginary unit, N is the frame length,x(m,j)is the time domain speech signal andw(j)denotes the time domain window function called a taper, which usually is symmetric and decreases towards the frame boundaries (e.g., Hamming). Direct spectrum estimators, such as a Hamming-windowed periodogram, are attractive as these estimators are completely independent of data and therefore do not suffer from problems arising due to modeling deficiencies. However, these methods are not robust to noise environments.The estimation of a noise power spectrum from the noisy speech signal plays a very important rule in noise reduction/speech enhancement algorithms. For relatively stationary noises, an accurate estimation of the noise spectrum can be done using minimum statistics (MS)-based approaches [49,50]. These algorithms lead to less satisfying results for rapidly changing noises. In this paper, for accurate estimation of noise power spectra, we employ a minimum mean square error (MMSE) – soft speech presence probability (SPP) (MMSE-SPP)-based noise estimation approach, proposed in [17]. In this method, the initial estimate of the noise power spectrum is computed by averaging the first ten frames of the speech spectrum. The advantage of this method is that it does not require a bias correction term as required by an MMSE-based noise spectrum estimation method; it also results in less overestimation of noise power and is computationally less expensive [51]. The MMSE-SPP based noise estimation procedure is summarized in Algorithm 1. The reason behind choosing MMSE-SPP-based noise spectrum estimation method is that it is computationally simple and requires only one parameter to be tuned.Dominant speech analysis techniques for speech recognition, such as MFCC [1] and PLP [2], try to emulate human auditory perception. The widely used MFCC front-end [1] was proposed more than two decades ago as a representation based on a crude approximation of the auditory response, with little rigorous justification for its implementation [45]. A more general approach for auditory spectral analysis is based on gammatone filterbanks, which involve dedicated design of mathematical forms of frequency response that match physiological experimental results. The gammatone filterbank (GTFB) consists of a series of non-uniform overlap bandpass filters, which model the frequency selectivity property of human hearing. Its name is due to the nature of its impulse response, which is a gamma envelope modulated by a tone carrier centered atfcHz(2)gt(t)=atn−1e−2πbtcos(2πfct+φ)fort>0,where a, b, n, t, φ, andfcare the amplitude normalization constant, bandwidth of the filter in Hz, order of the filter, time, phase shift and center frequency, respectively.Some attempts have been made to utilize GTFB in ASR for auditory spectral analysis of the speech signal, for instance [3,43,44,46–48], which show significant performance gains in various noise types and signal-to-noise ratio (SNR) levels. The compressive gammachirp filter bank (cGCFB), introduced by Irino et al. in [16], is a generalization of the GTFB. The frequency response of a gammatone filter is very nearly symmetric, which is not a good match to the auditory data, and it cannot model the level-dependent properties observed in basilar membrane (BM) filtering. The cGCFB allows a realistic and controllable frequency-domain asymmetry and has also been shown to model most of the level dependency observed in the BM filtering [16].In this work, we have adopted cGCFB for the auditory spectral analysis of the speech signal. The frequency response of the cGCFB is given by:(3)Hcgc=aΓ⋅|Hgt(f)|⋅ec1θ1⋅ec2θ2,whereHgt(f)is the frequency response of the GTFB,θ1=tan−1(f−fr1b1B(fr1)),θ2=tan−1(f−fr2b2B(fr2)),fr1andfr2are the center frequencies of the low-pass asymmetric function (LP-AF)ec1θ1and high-pass asymmetric function (HP-AF)ec2θ2, respectively.aΓis an amplitude normalization factor,fr2is defined in terms of a frequency ratiofratioasfr2=fratio⋅fp1, withfp1=fr1+c1b1B(fr1)n, the peak frequency of the passive gammachirp filter [16]. So, the frequency response of the cGCFB can be obtained from the GTFB by multiplying the frequency response|Hgt(f)|of the GT filter with HP- and LP-AFs. The high-pass asymmetric function makes the pass-band of the cGCFB more symmetric at lower levels. Fig. 2shows how cGCFB gains (or weights) are computed from the GTFB weights. The number of filters used in this work is 64 andb2=2.17,c2=2.20,b1=1.81,c1=−2.96. Fig. 3presents a comparison of shapes and weights of the Mel filterbank (MelFB), GTFB and compressive GTFB (cGCFB).In this work, we propose a sigmoid-shaped weighting ruleH(k,m)to enhance the auditory spectrumSas(k,m). Proposed weighting rule is based on the subband a posteriori SNR (in dB)γsb(k,m)and is formulated as [18]:(4)H(k,m)=11+e−ϑ(k,m)τ,where k is the subband index, m is the frame index, τ is a parameter that controls the lower limit of the weighting function andϑ(k,m)is the subband instantaneous SNR (in dB) defined as:(5)ϑ(k,m)=γsb(k,m)−4.5,whereγsb(k,m)=max(10log10(Sas(k,m)Nas(k,m)),−4.0),Nas(k,m)is the noise power spectrum mapped onto the auditory frequency axis. Here,τ=4.5is chosen experimentally.In order to remove the outliers from the weighting function, as given by Eq. (4), due to noise variability, we use a two-dimensional median filter. For smoothing the decision regions, a two-dimensional moving average filter is also applied [18]. In order to achieve optimal performance it is desirable to have a feature extractor that is well suited both for clean and adverse acoustic conditions.While the auditory filtering (e.g., Mel-scale filterbank) approximates the nonlinear characteristics of the human auditory system in frequency, the natural logarithmic nonlinearity or power function nonlinearity deals with the loudness nonlinearity. It approximates the relationship between a human's perception of loudness and the sound intensity [62]. Power function nonlinearity or root compression (which can be expressed asy1=ya,0<a<1) followed by DCT leads to better compaction of energy and hence more robust to mismatch condition [3,63,64]. In this paper, similar to [3], we use a power function nonlinearity with a coefficient ofa=1/15to approximate the loudness nonlinearity of human perception. The static features, obtained after applying the discrete cosine transform (DCT), are normalized using the short-time cepstral mean and scale (STCMSN) approach [7].In order to compensate for the effects of environmental mismatch feature normalization strategies are employed in speech (and speaker) recognition systems. These techniques are preferred because a priori knowledge and adaptation are not required under any environment. Most of the normalization techniques are applied as a post-processing scheme on the cepstral coefficient features. Normalization techniques can be classified as model-based or data distribution-based techniques. In model-based normalization techniques, certain statistical properties of speech such as mean, variance, and moments, are normalized to reduce the residual mismatch in feature vectors, e.g., short-time mean and variance normalization (STMVN). Data distribution-based techniques aim at normalizing the feature distribution to the reference, such as short-time Gaussianization (STG) [54,55]. Some feature normalization techniques have been proposed in the past for speech and speaker recognition systems, including feature warping [54], STG [55], cepstral mean normalization (CMN) [56,57], cepstral variance normalization (CVN) [58], histogram equalization [59] and RASTA filtering [60,61]. Almost all the feature extractors include feature normalization technique as a post-processing scheme. Feature normalization is normally performed over the whole utterance with the assumption that the channel effect is constant over the entire utterance, such as CMN, mean and variance normalization (MVN). Also, normalizing a feature vector over the entire utterance is not a feasible solution in real-time applications as it causes unnecessarily long processing delay. To relax this assumption and to reduce the processing delay MFCC features are normalized over a sliding window of more than 1 s duration. The feature vector to be normalized is located at the center of the sliding window.In the proposed feature extractor the static features, are normalized using the short-time cepstral mean and scale normalization (STCMSN) approach [7] over a sliding window of 1.5 s duration (i.e., 150 frames for a frame shift of 10 ms). In STCMSN approach the n-th feature space and m-th framec(n,m)are normalized as(6)cstmsn(n,m)=c(n,m)−μst(n,m)dst(n,m),whereμst(n,m)anddst(n,m)are the short-time mean and short-time difference between the upper and lower bound, respectively. Short-time meanμst(n,m)of the n-th feature space is defined for a short-time window ofL=150frames as:(7)μst(n,m)=1L∑j=m−L/2m+L/2c(n,j).Short-time difference between the upper and lower bounddst(n,m)of the n-th feature space is defined for a short-time window ofL=150frames as:(8)dst(n,m)=max(m−L/2)⩽j⩽(m+L/2)(c(n,j))−min(m−L/2)⩽j⩽(m+L/2)(c(n,j)).The main idea behind STCMSN technique is that under mismatched conditions a difference of log spectrum between the training and test environments is removed by adjusting the short-time mean and short-time scale. The advantage of short-time feature normalization technique is that it does not use constant channel assumption as used in the full utterance-based feature normalization method, and reduces the unnecessary long processing delay [7]. Fig. 4presents the zero-th cepstral coefficient feature (c0) of a clean speech signal and that of a noisy speech signal, having a signal-to-noise ratio (SNR) of 5 dB, (a) before applying feature normalization and (b) after normalizing by the STCMSN method. It is observed from Fig. 4(a) that the effects of additive noise on a clean speech signal are:•The minimum values of the cepstral features are elevated.The valleys of the cepstra are affected by the additive noise energy while the peaks remain almost unaffected.

@&#CONCLUSIONS@&#
In this paper we presented a robust feature extractor, denoted in this paper as robust compressive gammachirp filterbank cepstral coefficients (RCGCC), that includes the use of an asymmetric level-dependent compressive gammachirp filterbank (cGCFB) for mapping the speech spectrum onto the auditory frequency axis, a simple auditory domain spectrum enhancement technique based a sigmoid type suppression rule, an MMSE – soft speech presence probability (SPP)-based noise spectrum estimator method [17], and a short-time cepstral mean and scale normalization (STCMSN) method [7], for speech recognition tasks. Speech recognition experiments are performed on the AURORA-2 and AURORA-5 TIdigits corpora and the AURORA-4 large vocabulary continuous speech recognition (LVCSR) corpus to cover matched as well as various mismatch conditions due to additive background noise, channel distortion and real time reverberation. It is evident from the reported results that the proposed method provided comparable results to that of the ETSI-AFE and PNCC front-ends under additive noise distortions (AURORA-2 and AURORA-4 tasks) and performed better under reverberant environments (on the AURORA-5 task). The proposed robust feature extractor performed well both in matched and mismatched training/test environments and was also found computationally, measured in terms of execution time, less expensive.