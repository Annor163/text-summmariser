@&#MAIN-TITLE@&#
Automatic classification of seismic events within a regional seismograph network

@&#HIGHLIGHTS@&#
Fully automatic method for classification of seismic events.The method is based on Support Vector Machine.Effective in filtering out blasts and spurious events from automatic event bulletins.The method is flexible and easily adjustable to denser or wider networks.

@&#KEYPHRASES@&#
Classification,Regional seismic events,Automatic method,Support Vector Machine,Sparse seismic network,Fennoscandia,

@&#ABSTRACT@&#
This paper presents a fully automatic method for seismic event classification within a sparse regional seismograph network. The method is based on a supervised pattern recognition technique called the Support Vector Machine (SVM). The classification relies on differences in signal energy distribution between natural and artificial seismic sources. We filtered seismic records via 20 narrow band-pass filters and divided them into four phase windows: P, P coda, S, and S coda. We then computed a short-term average (STA) value for each filter channel and phase window. The 80 discrimination parameters served as a training model for the SVM. We calculated station specific SVM models for 19 on-line seismic stations in Finland. The training data set included 918 positive (earthquake) and 3469 negative (non-earthquake) examples. An independent test period determined method and rules for integrating station-specific classification results into network results. Finally, we applied the network classification rules to independent evaluation data comprising 5435 fully automatic event determinations, 5404 of which had been manually identified as explosions or noise, and 31 as earthquakes. The SVM method correctly identified 94% of the non-earthquakes and all but one of the earthquakes.The result implies that the SVM tool can identify and filter out blasts and spurious events from fully automatic event solutions with a high level of accuracy. The tool helps to reduce the work-load and costs of manual seismic analysis by leaving only a small fraction of automatic event determinations, the probable earthquakes, for more detailed seismological analysis. The self-learning approach presented here is flexible and easily adjustable to the requirements of a denser or wider high-frequency network.

@&#INTRODUCTION@&#
Many seismological observatories use automatic event detection and location procedures for monitoring local and regional seismicity. Fully automatic event solutions provide a cost-effective, nearly real-time snapshot on seismic activity within a target area. However, the events are often unclassified or poorly classified. The next and crucial step is to apply reliable automatic or semi-automatic methods for classifying the huge database of fully automatic event solutions. Automated event classification is necessary in monitoring natural hazards when rapid and reliable information to local authorities and media is of essence. Moreover, it helps to maintain the quality of regional earthquake catalogs, in particular among the low magnitude events. Namely, if unclassified or poorly classified event solutions end up in the catalog, the earthquake data will become increasingly contaminated with anthropogenic activity. Investigations relying upon such data will yield erroneous estimates of the rate of seismicity and, consequently, of seismic hazard.The classification of seismic events requires the integration of physical and statistical techniques. The task is challenging in low-seismicity areas where natural and anthropogenic seismicity often overlap in magnitude, space and time. A sparse coverage of the monitoring network further complicates event classification. The Finnish National Seismic Network, operated by the Institute of Seismology, University of Helsinki (ISUH) is a typical example of a sparse regional network. To supplement the near real time automatic detection and location capability of the national network, ISUH utilizes also available on-line stations of the partner networks (Fig. 1). The area monitored by ISUH covers central and eastern parts of Fennoscandia, including Finland, parts of Estonia, Norway, Sweden, Russia and the adjoining seas (Fig. 1). The region is characterized by a relatively low rate of natural seismicity intermingled with a high rate of anthropogenic activity. Several large-scale underground mines along with numerous open pits operate in the area on a daily basis. The greatest mine blasts have magnitudes exceeding ML 3. Rock bursts and other mining-induced or -triggered events occur frequently, the largest recorded so far are of magnitude ML 4+ (e.g., Roth and Bungum (2003)). Continual explosions from e.g. military exercises present a challenge to seismic monitoring of the sea areas.In addition to fully automatic near real-time bulletins, ISUH releases reviewed event bulletins where the automatic event solutions have been manually reviewed: all events are classified, spurious events are cleaned out and possible earthquakes (~1% of the data) as well as events with questionable seismic origin are subjected to a detailed reanalysis. The visual screening phase is time-consuming and labor-intensive and calls for automation.Many automatic seismogram classification methods reduce the waveform data to a set of parameters and these parameter vectors are then classified. Parameters commonly used in classification of regional events are spectral amplitude ratios of different seismic phases, complexity of the signal and autoregressive moving average (ARMA) coefficients. Recent examples are e.g. Fäh and Koch (2002), Zeiler and Velasco (2009) and Yilmaz et al. (2013). More complex methods have also been applied. Allmann et al. (2008) compared spectral fit to a simple ω−2 source model, whereas Lyubushin et al. (2013) used multi-fractal singularity spectrum properties.Our study originates from an idea to translate the guidelines for manual spectral analysis into automatic classification parameters. According to our experience the time-frequency distribution plots, i.e., spectrograms, are the most powerful tool in discriminating weak local and regional events. We exploit the total duration of seismic signals by forming “numerical spectrograms” of the automatically detected and located events. In order to present the information contained in the spectrogram plots in numerical form, a large parameter set is needed. We therefore search for a classification method that is effective in high dimensional spaces.Both statistical and machine learning methods have been applied in seismic classification previously. Examples of statistical methods are linear Bayesian discriminator (Lyubushin et al., 2013), linear discrimination analysis and its variants (Che et al., 2007; Kuyuk et al., 2014) and multivariate statistical analysis (Fäh and Koch, 2002). Examples of machine learning methods include the use of supervised Artificial Neural Networks (ANN) (Tiira, 1996; AllamehZadeh, 2011). Kuyuk et al. (2011) have used Self Organizing Maps and ANN combined with unsupervised learning in classification of small earthquakes and quarry blasts. Two unsupervised machine learning methods, k-means and Gaussian mixture model, were applied by Kuyuk et al. (2012) for classification of seismic activities in Istanbul. Support Vector Machine (SVM) is a popular application for a wide range of supervised pattern recognition problems (e.g., Boser et al., 1992; Cortes and Vapnik, 1995; Vapnik, 1995). Giacco et al. (2009) have applied SVM to automatic classification of seismic signals in volcano environment and Zhao (2007) for seismic discrimination within hydrocarbon reservoirs. The advantages of using SVM are handleability of large number of features and effectiveness in high dimensional spaces. SVM also gives unambiguous result to an ambiguous problem, which is easily implementable into automatic processing.Our approach, the numerical spectrogram, is a set of parameters calculated over time and frequency space of seismic records. Classification of the parameter set, in turn, is basically a pattern recognition problem (Joachims, 1999). For solving the problem, we have chosen to use the SVMlight package (svmlight.joachims.org), which is an implementation of Vapnik's Support Vector Machine (Vapnik, 1995).Section 2 summarizes the basis for our parameterization, i.e. the guidelines used in visual spectral analysis. In Section 3 we present data and methods applied to design an automatic SVM tool capable of identifying and filtering human-made and spurious events from automatic seismic event bulletins with a high level of accuracy. The goal is that only a small fraction of the events, i.e., the probable earthquakes, are left for manual screening and revision. We will apply the tool to fully automatic regional seismic event solutions produced by ISUH and we will show that the SVM based tool performs well within the network setting and relevant boundaries.Manual discrimination of seismic events relies on judgments made by individual analysts. To increase objectivity in the decision making ISUH has listed the following guidelines for visual seismogram analysis.Earthquakes are volume sources extended both in time and space and they generate a larger fraction of energy in S waves than in P waves. The P and S wave radiation patterns are, however, strongly dependent on rupture directivity. For earthquake sources the strength of P and S wave signals may vary significantly at stations located at approximately the same epicentral distance but in different azimuth directions. Seismic waves of earthquakes have wide frequency content and their energy is evenly distributed over the whole recorded frequency band. Earthquakes also produce rather complex waveforms because of secondary depth-sensitive seismic phases in their P and S coda.In contrast to earthquakes, explosions are compressive point sources from which P wave energy radiates evenly to all azimuth directions. S waves are presumably generated by mode conversions close to the source. They have smaller energy content as well as lower dominant frequencies than the corresponding P waves. In comparison to an earthquake of similar magnitude, the explosions have narrower frequency content as well as shorter duration of P and S wave coda. In addition, the P and S coda look more uniform because the most depth-sensitive secondary phases are missing. One important exception in explosion coda at short epicentral distances and low frequencies, i.e., up to 100km and 2.5Hz, is the existence of short-period surface wave Rg. Although excitation of Rg wave is merely an indicator of event depth, large Rg to S amplitude ratio at short distances may also work as a discriminant between surface blasts and earthquakes (cf. Uski et al., 2006).Ripple-firing is common practice for economic blasts, and ripple-fired explosions constitute the majority of human-made events in ISUH database as well. Ripple-firing comprises a series of small explosions spaced few meters apart and fired with small time delays between individual detonations. The technique enhances some signal frequencies while dampening others. The overall effect depends on several factors such as the shooting configuration and time-lag between subsequent blasts (e.g., Smith, 1989). In time-frequency distribution of explosion spectra, the ripple-firing effect is seen as spectral scalloping, i.e., time-invariant variation of energy minima and maxima. Records of underwater explosions may also display strong spectral scalloping. Bubble pulse effect together with interference from surface reflections generates impulsive signals with very distinctive scalloping pattern even for relatively small charges.In regional seismic analysis, the time-frequency distribution plots, spectrograms, are the most useful tool for discrimination between natural and artificial seismicity. They reveal time and frequency dependent variations in the signal energy distribution and also display relative amplitudes of regional seismic phases (P[g/b/n], S[g/b/n], Lg, Rg). Spectral scalloping typical for ripple-fired and underwater explosions becomes clearly visible on spectrograms.Fig. 2 shows examples of spectrograms and STA traces (see Section 3.1) computed for (a) single, (b) ripple-fired and (c) underwater explosion as well as for (d) shallow and (e) mid-crustal earthquake.In Fig. 2a the spectrogram of a single explosion displays bigger P to S ratios, shorter duration of P and S wave coda and more concentrated distribution of signal energy content than a shallow earthquake of similar size in Fig. 2d. The ripple-fired quarry blast in Fig. 2b has a rather limited energy distribution. Spectral scalloping is most pronounced between 8 and 20Hz, while the low and high frequency ends of the spectra carry little energy. Fig. 2c shows spectrogram of an underwater explosion. Note the relatively weak S waves, low dominant frequencies and strong spectral banding generated by the event. Finally, an example of mid-crustal (depth 26km) earthquake in Fig. 2e exhibits relatively small P to S ratios and relatively even energy distribution throughout the frequency band recorded.In addition to spectral analysis, seismic analysts use all available sources of information in event discrimination, e.g. clear first motion polarities of P waves and a database of active mines including commercial and military explosion sites.As yet, ISUH does not have reliable means to distinguish between mining-induced events and explosions from the same site. The most typical mining-induced events are rock bursts and other collapse type events with seismic energy confined to very low frequencies. Clear P wave polarities show downward first motion and the ripple-firing effects are missing. However, some of the mining-induced events display earthquake-like spectral features (e.g., larger fraction of S wave energy than pure collapse type events) suggesting a shear-slip component in the rock failure process. Due to the ambiguity in spectral characteristics, many of the small-magnitude induced events are probably labeled as explosions.For this study we utilized data from 19 high-frequency online stations in Finland (Fig. 1). The selected stations had a sufficient number of earthquake recordings during a period of unchanged instrumentation and a minimum sampling rate of 100Hz. High sampling rate was necessary for a good resolution at high frequencies, the part of signal spectra that provides the most relevant information for regional seismic discrimination (Bowers and Selby, 2009).The event data comprised mainly fully automatic, unclassified event solutions produced by ISUH since 2009. For this study, the automatic solutions were supplemented with classification code from the manual analysis. An event was included in the data set if it was located by a minimum of four stations and the epicenter was within 15–270km distance from at least one of the stations in this study. The lower distance limit ensured that the P- and S-phases could be separated and there were enough data samples in the phase windows used. The upper distance limit guaranteed good signal-to-noise ratio (SNR) for small magnitude events at high frequencies.Three independent event data sets were compiled for this study. The first set was used for SVM training, the second for testing the network processing rules and the third for evaluating the SVM classification performance. Geographical distribution of the events is shown inFig. 3a–c.The training data set was recorded between Jan 2009 and Oct 2012. For the SVM classification, the events were divided into two categories: positive including earthquakes and negative including non-earthquakes, i.e. anthropogenic events as well as spurious events generated by erroneous phase and/or noise associations. In order to avoid a spatial bias in the training models, it was necessary to limit the number of small explosions originating from the most active mining sites (cf. Figs. 1 and 3a). In addition, the earthquake training data set was supplemented with manually located earthquakes from 2006 to 2008 to increase the number of earthquake solutions available. The final training data set included 918 earthquakes and 3469 non-earthquakes. The SVM training models were station-specific; the number of earthquake examples ranged from 11 in one station up to 302 in another station and the number of non-earthquake examples from 227 to 1048.The first step in data parameterization was to determine, for each event-station pair, the length of the waveform segment and phase windows. The four phase windows containing the first P wave, P coda, the first S wave and S coda had the same length; half of the theoretical S–P arrival time difference. The waveform segment included also two noise windows, one preceding the first P and one following the S coda. The length of the noise windows was equal to the theoretical S–P arrival time difference.In the second step, the waveform segment was filtered via 20 narrow frequency bands using zero-phase second-order Butterworth filter (seeTable 1). The filters covered frequencies between 1 and 41Hz, i.e., an optimal band for analyzing weak regional signals sampled at 100Hz. Several narrow pass-bands were needed to preserve and highlight spectral scalloping effects in the STA traces.In the next step, a time series of short term averages (STA) was derived from every filtered channel. The STA values were calculated with the conventional method (e.g., Ruud and Husebye, 1992)(1)STA=1N∑i=1Nyi2,where N is the length of the STA window in samples, and yithe ith sample from the filtered time series y.The STA trace was formed by calculating STA values over the filtered time series using overlapping time windows with step length shorter than the STA window length. In the final step, average STA was calculated for each of the four phase windows and, to minimize the effect of event magnitude, divided with the overall STA average of the waveform segment. Thus, every event-station pair was assigned with a set of 80 parameters referred to as “numerical spectrogram”. An example of a training explosion and its numerical spectrogram is shown inFig. 4.We used the SVMlight in its simplest form, i.e., the classification option with linear kernel function. We left most of the learning parameters in default settings or let the program automatically determine them from the input data. The cost factor, which deals with the unequal number of positive and negative training events was, however, chosen with care. In SVMlight the cost factor is an implementation of Morik's study (Morik et al., 1999), where the factor is optimized so that the potential total cost of the false positives, C+, equals the potential total cost of the false negatives, C−. In other words, the parameters C+ and C− are chosen to obey the ratio(2)j=C+C−.We optimized the parameter j by first splitting the model accuracy test into two parts: accuracy of positive training examples and accuracy of negative training examples. Then we searched the value of j that maximizes the sum Atot of the accuracy tests(3)Atot(j)=NEC(Mj)NE+PEC(Mj)PEwhere NE is the number of negative training examples, NEC the number of correctly classified negative training examples, PE the number of positive training examples, PEC the number of correctly classified positive training examples and Mjthe model calculated with parameter j. In Eq. (3) the minimum value for j was set to 1 (the costs are equal) and the maximum value was obtained from Eq. (2).The second data set, recorded between November 13th, 2012 and August 13th, 2013, comprised 2342 fully automatic event solutions of which 48 were identified as earthquakes (Fig. 3b). The SVM models derived from the training data were applied to these events. Outcome of the test period determined method and rules for combining the station-specific classification results into a network discriminant.For a given test event, the network discriminant was defined as the weighted mean of all its prediction values. The weights were empirical and somewhat arbitrary and were determined by dividing the training examples into six groups on the basis of their P wave SNR and epicentral distance (seeTable 2). The lowest SNR range was below the detection threshold of the network. Associated with the first P at short source-receiver distances, such a low SNR usually implies misplaced P onset due to poor or erroneous location. Very strong signals were generally noise spikes and therefore also the highest SNR values were assigned reduced weights. The shortest and longest source-receiver distances were also considered less suitable for spectrogram type analyses than the mid-range, distances 30–250km.The level of the discrimination limit was adjusted so that none of the test earthquakes fell below the limit. The test data set indicated that an optimal choice for discrimination limit was zero and therefore no further adjustments on the shape or level of the limit were made. When the testing was completed, earthquakes in the test data set were added to the training data as positive examples. In addition, the misidentified non-earthquakes were included in the training data as negative examples.Finally, an additional data set recorded between August 14th, 2013 and January 31st, 2014 was used for evaluation of the SVM classification performance (Fig. 3c). The updated training models and the network classification rules were applied to these events. In this phase, SVM classification was performed for a total of 5435 events. According to the manual review, 31 of the events were earthquakes and the rest anthropogenic or spurious events.The weighted network discriminants calculated for the test and evaluation events are shown inFig. 5a–b. In both figures the discriminants are plotted against the number of classifying stations.Fig. 6 shows the results from the evaluation period on a map.

@&#CONCLUSIONS@&#
i)We have designed a Support Vector Machine (SVM) based tool, which classifies automatically generated numerical spectrograms. The tool is effective in distinguishing natural earthquakes from the bulk of human-made and spurious seismic events. For a sparse regional network used in this study, the false alarm rate is 6% and the risk of losing an earthquake is 3%.The performance of SVM tool depends on the number and on the spatial coverage of the recording stations. The classification should be based on several stations located at different azimuth directions around the epicenter.The balance between misidentified earthquakes and non-earthquakes may be further improved by introducing more sophisticated rules for the compilation of network discriminants.