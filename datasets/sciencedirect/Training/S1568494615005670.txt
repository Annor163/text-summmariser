@&#MAIN-TITLE@&#
GPFIS-CLASS: A Genetic Fuzzy System based on Genetic Programming for classification problems

@&#HIGHLIGHTS@&#
Novel Genetic Fuzzy System based on Multi-Gene Genetic Programming called GPFIS-CLASS.GPFIS-CLASS builds fuzzy rule premises by using Multi-Gene Genetic Programming.In 22 datasets GPFIS-CLASS compares favorably to other GP-based Genetic Fuzzy Systems.In other 23 datasets GPFIS-CLASS outperforms other state-of-the-art EFSs.In general, GPFIS-CLASS provides more compact rule bases.

@&#KEYPHRASES@&#
Genetic Fuzzy System,Classification,Multi-Gene Genetic Programming,

@&#ABSTRACT@&#
Genetic Fuzzy Systems (GFSs) are models capable of integrating accuracy and high comprehensibility in their results. In the case of GFSs for classification, more emphasis has been given to improving the “Genetic” component instead of its “Fuzzy” counterpart. This paper focus on the Fuzzy Inference component to obtain a more accurate and interpretable system, presenting the so-called Genetic Programming Fuzzy Inference System for Classification (GPFIS-CLASS). This model is based on Multi-Gene Genetic Programming and aims to explore the elements of a Fuzzy Inference System. GPFIS-CLASS has the following features: (i) it builds fuzzy rules premises employing t-norm, t-conorm, negation and linguistic hedge operators; (ii) it associates to each rule premise a suitable consequent term; and (iii) it improves the aggregation process by using a weighted mean computed by restricted least squares. It has been evaluated in two sets of benchmarks, comprising a total of 45 datasets, and has been compared with eight different classifiers, six of them based on GFSs. The results obtained in both sets demonstrate that GPFIS-CLASS provides better results for most benchmark datasets.

@&#INTRODUCTION@&#
Genetic Fuzzy Systems (GFSs) [16,17,23,29] have been widely employed to solve classification [4,11,49], regression [2,5] and control [15,54] problems. The main feature that highlights GFSs in respect to other mathematical, statistical and artificial intelligence models is its capability of extracting knowledge from datasets or industrial plants and state it in linguistic terms with reasonable accuracy. This is provided by the bond between a Fuzzy Inference System (FIS) and a Genetic Based Meta-Heuristic (GBMH), which is based on Darwinian concepts of natural selection and genetic recombination. Therefore, a GFS provides fair accuracy and linguistic interpretation (FIS component) through the automatic learning of its parameters/rules (GBMH component), using information extracted from a dataset or a plant.In GFSs literature, most works focus on developing or modifying methods in the GBMH component, such as:•Modification of the Genetic Fuzzy Rule-Based Systems in the GBMH structure (codification, selection and evaluation) to generate fuzzy rule bases different from the standard Pittsburgh-style [29,36,44], such as Michigan [15,43], Genetic Cooperative-Competitive Learning (GCCL) [11,32,40,45] and Iterative Rule Learning (IRL) [18,26,28] approaches;Application of Multi-Objective Evolutionary Algorithms (MOEAs) to search for solutions that satisfy both accuracy and interpretability criteria in GFSs development [3,6,21,22,33];Employment of a GBMH to fine tune membership functions in the post-processing stage [49,50]; andUse of other Evolutionary Algorithms outside the GBMH scope (Particle Swarm and other Bio-inspired algorithms), in order to attain better results [13,25,37,42].However, few researchers have focused on developments of the Fuzzy Inference component (e.g., operators such as negation, linguistic hedges, aggregation, etc.) to improve the performance obtained by the GFS. Additionally, there is a lack of works using Genetic Programming (GP) [39,47] as a GBMH for a GFS [16], despite its adequacy to problems that demand a non-fixed size codification – such as a fuzzy rule-based system. Therefore, to satisfy both conditions – FIS structure developments and application of Genetic Programming –, this work proposes a new GFS model: the Genetic Programming Fuzzy Inference System for Classification problems (GPFIS-CLASS). GPFIS-CLASS is based on Multi-Gene Genetic Programming (sometimes called Multi-Tree Genetic Programming) [24,52], a generalization of Koza-style GP [39,47] with three main features: (i) it builds fuzzy rule premises by employing t-norm, t-conorm, negation and linguistic hedge operators; (ii)it associates a given premise to a suitable consequent term; and (iii) it improves the aggregation procedure by using operators other than the maximum t-conorm. GPFIS-CLASS should be seen as an improvement to the GPF-CLASS model [38]. This does not present reasonable results due to the lack of a suitable procedure for consequent term definition and membership degrees aggregation.In order to assess its capability, GPFIS-CLASS is evaluated in two sets of experiments, extracted from Berlanga et al. [11] and Antonelli et al. [6] respectively. Briefly, the former [11] proposes a new GFS based on GP, in which each individual is encoded in a GCCL scheme for fuzzy rule base learning. This model was applied to 24 datasets and its results have been compared to those of four other GFSs, two of them based on GP. Antonelli et al. [6], on the other hand, presents a novel approach for learning concurrently the rule and data bases of fuzzy rule-based classifiers based on a multi-objective evolutionary approach. This system is evaluated in 24 datasets, mainly by comparing its results with those from two Evolutionary Fuzzy Systems (EFSs) and two other state-of-the-art classifiers. GPFIS-CLASS performance was then evaluated in most datasets; results have been compared to those provided by other GP-based and state-of-the-art EFSs models.This paper has four additional sections. The next section describes the main concepts of Multi-Gene Genetic Programming. Section 2 presents the proposed GPFIS-CLASS model in five steps: fuzzification, fuzzy inference, decision, evaluation and selection & recombination. Section 4 deals with the model evaluation for different benchmark classification problems, and Section 5 concludes the work.Genetic Programming (GP) [39,47] belongs to the Evolutionary Computation field. Typically, it employs a population of individuals (or solutions), each of them denoted by a tree structure that codifies a mathematical equation, which describes the relationship between a set of input features Xj(j=1, ..., J) and the output Y. Based on these ideas, Multi-Gene Genetic Programming (MGGP) [20,24,31,52] generalizes GP, as it denotes an individual as a set of tree structures, commonly called genes, that also receives the set of features Xjand tries to predict Y (Fig. 1).Each individual is composed of D functions (d=1, ..., D) that map Xjfeatures to Y through user-defined mathematical operations. Those functions are composed of simple unary, binary or ternary operators, which have to satisfy some constraints detailed in [39]. It is easy to verify that, when D=1, MGGP generates solutions similar to GP. In GP terminology, the Xjinput features are included in the Terminal Set, while the mathematical operations (plus, minus, etc.) are inserted in the Function Set (or Mathematical Operations Set).MGGP considers three recombination operators: a mutation and two different crossover operators. The mutation in MGGP is similar to that in GP. As for crossover, the level at which the operation is performed must be specified: it is possible to apply crossover at high and low levels. The low level is the space where it is possible to manipulate the structures (Terminals and Mathematical Operations) of equations present in an individual. The high level, on the other hand, is the space where expressions can be manipulated in a macro way.Fig. 2a presents a generic example of the application of a mutation operator to a multi-gene individual with five equations (D=5), where the first tree has been randomly modified. Fig. 2b shows the low level crossover operation. As can be seen from Fig. 2(a) and (b), the mutation and low level crossover are similar to those performed in GP. The high-level crossover, on the other hand, differs from the standard GP crossover, as shown in the example presented in Fig. 2c. The dashed lines indicate that functions 2, 3 and 4 in individual 1 have been switched with functions 3, 4 and 5 from individual 2. The cutting point can be symmetric – the same number of equations is exchanged between individuals –, or asymmetric. Intuitively, high level crossover has a deeper effect on the output than low level crossover or mutation operators.In general, the evolutionary process in MGGP differs from GP due to the addition of two parameters: maximum number of trees per individual and high level crossover rate. For the first parameter, a high value is usually employed in order not to obstruct the evolutionary process. On the other hand, the high level crossover rate, similar to other genetic operators rates, needs to be adjusted and its value is usually determined by experiments.The next section presents the proposed GPFIS-CLASS model, which uses MGGP as its GBMH to evolve the premise term of each fuzzy rule in the FIS component.GPFIS-CLASS is a typical Pittsburgh-type GFS [29], in which each individual represents a fuzzy rule base. Fig. 3exhibits the main modules of the GPFIS-CLASS model.Modeling begins by mapping crisp values into membership degrees of fuzzy sets (Fuzzification). Then, a fuzzy inference procedure is performed in three substeps: (i) ggeneration of fuzzy rule premises (Formulation); (ii) assignment of the best suited consequent term for each premise (Association) and (iii) aggregation of activated fuzzy rules (Aggregation). Finally, Decision, Evaluation and Selection & Recombination are performed. The following subsections provide a detailed description of each module of the GPFIS-CLASS model.In classification problems, the main source of information consists of a dataset containing n patterns xi=[xi1, xi2, ..., xiJ], each of them containing values of J features Xj(i=1, ..., n and j=1, ..., J). The fuzzification step involves the association of fuzzy sets to each input feature. Therefore, each j-th feature has L associated fuzzy sets:Alj={(xij,μAlj(xij))|xij∈Xj}, whereμAlj:Xj→[0,1]is a membership function that assigns to each value xija membership degreeμAlj(xij)to the fuzzy set Alj(see Fig. 4). Finally, each i-th pattern in the dataset belongs to a class C of K possible classes, i.e., C∈{1, 2, ..., k, ..., K}.The specification of fuzzy sets involves the definition of three factors: (i) functional description (triangular, trapezoidal, etc.); (ii) support and granularity of membership functionsμAlj(xij); and (iii) linguistic terms, in order to qualify the subspace defined by the membership function with an appropriate adjective. In theory, this should be specified by an expert. In practice, however, due to the difficulty of having an expert available, membership functions are usually defined as strongly partitioned [7,11,18,32,35]. This is shown in Fig. 4, where all membership function have supports of equal width along the universe of discourse of a feature Xj.The inference process in GPFIS-CLASS is subdivided into 3 steps: (i) Formulation – responsible for combining the linguistic terms of each feature to build a fuzzy rule premise (antecedent creation - MGGP); (ii) Association – given a set of premises, this step verifies the consequent class that is most suited to each premise (fuzzy rule creation and screening); and (iii) Aggregation – receives as input the activated fuzzy rules and computes a consensual value for each consequent class. Fig. 5illustrate this whole process described in details in the following sub-sections.The GPFIS-CLASS model makes use of MGGP to obtain a set of fuzzy rule premises. A fuzzy rule premise is commonly defined as:IfX1isAl1,and...,andXjisAlj,and...,andXJisAlJ″or, in mathematical terms:(1)μAd(xi)=μAl1(xi1)*...*μAlj(xij)*...*μAlJ(xiJ)whereμAd(xi)is the joint membership degree of pattern xiwith respect to the d-th premise (d=1, ..., D), computed through a t-norm “*” that combines each marginal membership degreeμAlj(xij). In general terms, a fuzzy rule premise can be described as a combination of eachμAlj(xij)by using t-norms, t-conorms, negation and linguistic hedge operators. However, the search space grows exponentially as the number of features and available operators increases. MGGP is employed in order to deal with this large search space. As described in Section 2, GP and MGGP require the definition of a set of terminals and mathematical operations for the evolutionary process. Table 1shows those components in the proposed GPFIS-CLASS model.The set ofμAlj(xij)represents the Input Fuzzy Sets (the Terminal Set), while the Mathematical Operations Set contains the Fuzzy Operators Set. Fig. 6exhibits an example of a solution provided by an individual of an MGGP population. In Fig. 6, based on the components presented in Table 1, premise 1 is analytically represented by:μA1(xi)=μA21(xi1)*μA32(xi2), which denotes in linguistic terms: “If X1is A21, and X2is A32”. Note thatμAd(xi)stands for the d-th premise encoded in a d-th tree of an MGGP individual.The next step to fully develop a fuzzy rule base is the definition of the consequent term (in this case the k-th class) best suited to eachμAd(xi).In the association step the consequent class most compatible to a given premiseμAd(xi)is determined. LetμAd(k)(xi)denote a premise associated to class k (i.e., a fuzzy rule). In linguistic terms,μAd(k)(xi)describes:“IfX1isAl1,and.˙.,andXJisAlJ,thenxiisClasskA simple and intuitive association technique is the so-called Uniform Division, where, given a set of D premises and K possible classes,U=⌊DK⌋is computed for each MGGP individual. Then,the D premises are divided as:•Class 1:μA1(1)(xi),...,μAU(1)(xi);Class 2:μAU+1(2)(xi),...,μA2×U(2)(xi);...Class K:μA(K−1)×U+1(K)(xi),...,μAK×U(K)(xi)This method is illustrated in Fig. 7.The use of this simple method, however, may cause the association of a premiseμAd(xi)to an incorrect consequent class. In order to avoid this and also promote the reduction of the search space, each premiseμAd(xi)should be assigned to the k-th class that maximizes a certain compatibility measure between the specific premise and a consequent class.A very simple and widely used compatibility measure is the Confidence (Certainty) Degree (CDk) [11,34]:(2)CDk=∑i∈kμAd(xi)∑i=1nμAd(xi)∈[0,1]where the numerator∑i∈kμAd(xi)is the compatibility degree of d-th premise with respect to the k-th class, and∑i=1nμAd(xi)is its compatibility degree to all classes. If CDk=0 for a certain premise, then no consequent is associated to it.In a fuzzy inference system, an input patternxi*may activate several rules, from different classes. The Aggregation step aims to merge the activation degrees ofxi*over fuzzy rules from the same class, in order to generate a consensual value per class.Consider D(k) the number of fuzzy rules associated to the k-th class. Given an aggregation operator [9,14],g:[0,1]D(k)→[0,1], the merged membership degree ofxi*to each of the K classes (μˆCi∈k(xi*)) is:(3)μˆCi∈1(xi*)=g[μA1(1)(xi*),...,μAd(1)(xi*),...,μAD(1)(xi*)](4)μˆCi∈2(xi*)=g[μA1(2)(xi*),...,μAd(2)(xi*),...,μAD(2)(xi*)](5)...μˆCi∈K(xi*)=g[μA1(K)(xi*),...,μAd(K)(xi*),...,μAD(K)(xi*)]The most commonly used aggregation operator is the maximum t-conorm [46]. An alternative would be the weighted mean [10], where the weights are computed by solving a Restricted Least Squares problem:(6)min:∑i=1n(μCi∈k(xi)−∑d(k)=1D(k)wd(k)μAd(k)(xi))2subjectto.:∑d(k)=1D(k)wd(k)=1andwd(k)≥0wherewd(k)is the weight or the influence degree ofμAd(k)(xi)in order to discriminate patterns from k-th class. Note thatμCi∈k(xi)∈{0,1}is the target result for the membership degree of xito the k-th class. This is a typical Quadratic Programming problem, the solution of which is easily computed by algorithms discussed in [51]. This aggregation operator is called Weighted Average Restricted Least Squares (WARLS), and its mathematical expression is presented below:(7)μˆCi∈1(xi*)=∑d(1)=1D(1)wd(1)μAd(1)(xi)(8)μˆCi∈2(xi*)=∑d(2)=1D(2)wd(2)μAd(2)(xi)(9)...μˆCi∈K(xi*)=∑d(K)=1D(K)wd(K)μAd(K)(xi)Once the merged membership degree ofxi*for each k class has been computed, it is necessary to verify which classxi*belongs to, by using the information given byμˆCi∈1(xi*),...,μˆCi∈K(xi*). This is performed in the subsequent step.Given a new patternxi*, the decision on the k-th class (k=1, ..., K) it belongs to is performed by:(10)Cˆi=argkmax{μˆCi∈1(xi*),...,μˆCi∈k(xi*),...,μˆCi∈K(xi*)}whereCˆiis the predicted class: a result of the k-th argument with the maximum value in expression (10). Therefore, this method associates to patternxi*the class that provides the highest merged membership degree, according to the available rule base. When a tie occurs, either a heuristic can be applied (e.g., impose toxi*the class that has more patterns in the dataset) or no class is defined toxi*.

@&#CONCLUSIONS@&#
This work has proposed a new Genetic Fuzzy System based on Genetic Programming, called Genetic Programming Fuzzy Inference System for Classification problems (GPFIS-CLASS). Its building blocks have been described, mainly the Fuzzy Inference process (Formulation-Association-Aggregation), which is what distinguishes it more from other Evolutionary Fuzzy Systems in the area.In order to assess the performance of GPFIS-CLASS, several experiments and comparisons with other GFSs haven been carried out by using 45 classification datasets, some of them with high dimension and scalable. GPFIS-CLASS has outperformed state-of-the-art GFSs based on Genetic Programming (Case 1) in terms of accuracy and complexity reduction, and its accuracy has not been significantly different from that of MOEFS, although its results in terms of compactness and accuracy have been superior in most datasets. This greater compactness and accuracy of GPFIS-CLASS model is due to several factors: Lexicographic Parsimonious Pressure (tiebreaker criterion), high level crossover, addition of negation operator, application of CD to better associate a premise to a consequent term and the use of WARLS as an aggregation operator, which turns to be a fuzzy rule selection as well as an error minimization procedure.Future works will consider further investigations with GPFIS-CLASS, mainly in three directions: (i) use of other t-norm, negation and linguistic hedges operators, inclusion of t-conorm operators for premises formulation (such as in the GP-COACH model); (ii) develop new association methods through other similarity measures, or applying Restricted Least Squares procedure with some adaptation, to address a more suitable consequent class to a given premise; (iii) evaluate other aggregation operators, such as nonlinear operators (weighted geometric mean, logistic regression, etc.), which may bring better results in terms of accuracy. Finally, fine-tuning of GPFIS-CLASS model parameters (membership functions and Genetic Programming set up parameters) shall be considered, as well as its use in real applications, such as in the medical area, image recognition and economics analysis.