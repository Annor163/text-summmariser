@&#MAIN-TITLE@&#
Use of graphics processing units for automatic synthesis of programs

@&#HIGHLIGHTS@&#
A new quantum-inspired linear genetic programming system that runs on the GPU.Allows the synthesis of solutions for large-scale real-world problems.Eliminates the overhead of copying the fitness results from the GPU to the CPU.Proposes a new selection mechanism to recognize the programs with best evaluations.Improves performance of the GP execution through exploiting the GPU environment.

@&#KEYPHRASES@&#
Genetic programming,GPU acceleration,Machine code,Quantum-inspired algorithms,Massive parallelism,

@&#ABSTRACT@&#
Genetic programming (GP) is an evolutionary method that allows computers to solve problems automatically. However, the computational power required for the evaluation of billions of programs imposes a serious limitation on the problem size. This work focuses on accelerating GP to support the synthesis of large problems. This is done by completely exploiting the highly parallel environment of graphics processing units (GPUs). Here, we propose a new quantum-inspired linear GP approach that implements all the GP steps in the GPU and provides the following: (1) significant performance improvements in the GP steps, (2) elimination of the overhead of copying the fitness results from the GPU to the CPU, and (3) incorporation of a new selection mechanism to recognize the programs with the best evaluations. The proposed approach outperforms the previous approach for large-scale synthetic and real-world problems. Further, it provides a remarkable speedup over the CPU execution.

@&#INTRODUCTION@&#
The idea of enabling the computer to automatically create programs that solve problems establishes a new paradigm for developing reliable applications. The field of genetic programming (GP) has demonstrated that devising computer programs on the basis of a high-level description is viable.Genetic programming extends conventional evolutionary algorithms to deal with computer programs. The essence of GP is to use the Darwinian principle of natural selection in which a population of computer programs is maintained and modified, according to genetic variation. A GP system progresses toward a solution by stochastically transforming populations of programs into better populations of programs until a stopping criterion is met.In the past few years, GP has been successfully applied to a wide variety of problems, including automatic design, pattern recognition, financial prediction, robotic control, data mining, image processing, and synthesis of analog electrical circuits [1–7]. However, a major drawback of GP is that the search space of candidate programs can become enormous. For example, to solve the 20-bit Boolean multiplexer problem, a total of 1,310,720,000 candidate programs have to be evaluated [8]. In addition, the evaluation of the fitness of a single program in the search space may demand testing this program with numerous different combinations of input data. Consequently, the time required to evaluate the programs may be unreasonable. The computational power required by GP to evaluate billions of programs with hundreds or thousands of input data can be a huge obstacle to solving large real-world problems.The last few years have witnessed remarkable advances in the design of parallel processors. In particular, graphics processing units (GPUs) have become increasingly popular. Further, their high computational power, low cost, and reasonable floating-point capabilities have made them attractive platforms for speeding up GP. Modern GPUs contain thousands of cores, and the massive parallelism provided is highly suitable to process GP in parallel.The power of the GPU has been previously exploited to accelerate GP by using different methodologies. The compilation methodology [9–12] generates programs using the GPU high-level language, and each of these programs has to be compiled before its evaluation. The pseudo-assembly methodology [13,14] creates programs in the pseudo-assembly code of the GPU, and a just-in-time (JIT) compilation is performed for each individual before the evaluation. The interpretation methodology [15–17] interprets programs during its evaluation. The machine code methodology [18] generates programs in the GPU machine code and does not require any compilation step before evaluation.The above mentioned methodologies have been used with different levels of success. However, the machine code methodology, called GPU machine code genetic programming (GMGP) [18], has exhibited significant performance gains over the others. Avoiding the compilation overhead without including the cost of parsing the evolved program is the key for severe reductions in the computational time. In addition, GMGP implements linear genetic programming by using a quantum-inspired evolutionary algorithm. The use of a quantum-inspired evolutionary algorithm provides a more efficient evolutionary algorithm that includes the past evaluation history to improve the generation of new programs. The linear genetic programming approach is more appropriate for machine code programs, as computer architectures require programs to be provided as linear sequences of instructions.This work extends GMGP in two directions: First, we propose a new approach where the power of the GPU is fully exploited in the GP algorithm. Second, we assess the impact of selecting programs with the best evaluations on the best final solutions (i.e. programs). We have developed two approaches: GMGP-gpu and GMGP-gpu+. GMGP-gpu implements all the GP steps in the GPU and provides the following: (1) significant performance improvements in the GP steps and (2) elimination of the overhead when copying the fitness results from the GPU to the CPU through the PCIe bus. GMGP-gpu+ incorporates a new selection mechanism to recognize programs with the best evaluations. The new selection mechanism produces a more efficient comparison of the past population with the current population, bringing more diversity to the search.The two proposed approaches were compared to GMGP and found to outperform GMGP for large-scale synthetic and real-world problems. The speedups ranged from 1.3 to 2.6. They also provided substantial speedups over CPU execution; the parallel GPGMGP-gpu+ executed up to 325.5 times faster than the CPU version.The remainder of this paper is organized as follows: Section 2 presents previous work on parallel GP. Section 3 briefly introduces GP and quantum-inspired GP. Section 4 describes the GMGP system. Section 5 describes our approach to fully exploit the power of the GPU in GMGP. Section 6 presents the experimental results obtained for synthetic and real-world problems. Finally, Section 7 draws some conclusions and presents future research directions.

@&#CONCLUSIONS@&#
The synthesis of programs based on user-defined requirements is an exciting field of evolutionary computation. At present, however, this ambition is bounded by the computational power needed to perform the evaluation of billions of programs. This work focuses on accelerating genetic programming (GP) to support the synthesis of large problems. We worked on exploiting the highly parallel environment of the GPU to tackle the huge computational effort required by GP.We provide a new perspective on the implementation of GP on the GPU. We worked on a quantum-inspired linear GP system that generates programs using the GPU machine code. Our approach implements all the GP steps in the GPU and offers the following advantages over the previous approach: (1) the parallelization of all the GP steps, improving the GP performance; (2) the elimination of the overhead associated with copying the fitness results from the GPU to the CPU through the PCIe bus; and (3) the incorporation of a new selection mechanism to recognize the programs with the best evaluations.Completely exploiting the power of the GPU to synthesize programs produces impressive accelerations. Our approach outperformed the previous approach for synthetic and real-world problems and provided remarkable speedups over the CPU execution. We obtained speedups ranging from 1.3 to 2.6 as compared to the previous approach, and speedups of up to 325.5 as compared to the CPU execution. We were the first to evolve the intrusion detection problem by using all of the input data during the evolution, rather than samples.This work opens up the possibility of applying GP to a variety of relevant problems. Evolutionary machine learning techniques present a considerable potential for big data learning tasks owing to their flexibility in knowledge representations, learning paradigms, and their innate parallelism.