@&#MAIN-TITLE@&#
Analysis of named entity recognition and linking for tweets

@&#HIGHLIGHTS@&#
We analyse the named entity recognition and disambiguation performance on tweets.Multiple state-of-the-art systems are included.Commercial and academic systems suffer the same range of problems.Lack of context is a major problem, demanding new, custom NER & NEL approaches.A named entity linking corpus is released with the paper.

@&#KEYPHRASES@&#
Information extraction,Named entity recognition,Entity disambiguation,Microblogs,Twitter,

@&#ABSTRACT@&#
Applying natural language processing for mining and intelligent information access to tweets (a form of microblog) is a challenging, emerging research area. Unlike carefully authored news text and other longer content, tweets pose a number of new challenges, due to their short, noisy, context-dependent, and dynamic nature. Information extraction from tweets is typically performed in a pipeline, comprising consecutive stages of language identification, tokenisation, part-of-speech tagging, named entity recognition and entity disambiguation (e.g. with respect to DBpedia). In this work, we describe a new Twitter entity disambiguation dataset, and conduct an empirical analysis of named entity recognition and disambiguation, investigating how robust a number of state-of-the-art systems are on such noisy texts, what the main sources of error are, and which problems should be further investigated to improve the state of the art.

@&#INTRODUCTION@&#
Information Extraction (IE) (Cardie, 1997; Appelt, 1999) is a form of natural language analysis, which takes textual content as input and extracts fixed-type, unambiguous snippets as output. The extracted data may be used directly for display to users (e.g. a list of named entities mentioned in a document), for storing in a database for later analysis, or for improving information search and other information access tasks.Named Entity Recognition (NER) is one of the key information extraction tasks, which is concerned with identifying names of entities such as people, locations, organisations and products. It is typically broken down into two main phases: entity detection and entity typing (also called classification) (Grishman & Sundheim, 1996). A follow-up step to NER is Named Entity Linking (NEL), which links entity mentions within the same document (also known as entity disambiguation) (Hirschmann & Chinchor, 1997), or in other resources (also known as entity resolution) (Rao, McNamee, & Dredze, 2013). Typically, state-of-the-art NER and NEL systems are developed and evaluated on news articles and other carefully written, longer content (Ratinov & Roth, 2009; Rao et al., 2013).In recent years, social media – and microblogging in particular – have established themselves as high-value, high-volume content, which organisations increasingly wish to analyse automatically. Currently, the leading microblogging platform is Twitter, which has around 288million active users, posting over 500million tweets a day,1http://news.cnet.com/8301-1023_3-57541566-93/report-twitter-hits-half-a-billion-tweets-a-day.1and has the fastest growing network in terms of active usage.2http://globalwebindex.net/thinking/social-platforms-gwi-8-update-decline-of-local-social-media-platforms.2Reliable entity recognition and linking of user-generated content is an enabler for other information extraction tasks (e.g. relation extraction), as well as opinion mining (Maynard, Bontcheva, & Rout, 2012), and summarisation (Rout, Bontcheva, & Hepple, 2013). It is relevant in many application contexts (Derczynski, Yang, & Jensen, 2013), including knowledge management, competitor intelligence, customer relation management, eBusiness, eScience, eHealth, and eGovernment.Information extraction over microblogs has only recently become an active research topic (Basave, Varga, Rowe, Stankovic, & Dadzie, 2013), following early experiments which showed this genre to be extremely challenging for state-of-the-art algorithms (Derczynski, Maynard, Aswani, & Bontcheva, 2013). For instance, named entity recognition methods typically have 85–90% accuracy on longer texts, but 30–50% on tweets (Ritter, Clark, Mausam, & Etzioni, 2011; Liu, Zhou, Wei, Fu, & Zhou, 2012). First, the shortness of microblogs (maximum 140 characters for tweets) makes them hard to interpret. Consequently, ambiguity is a major problem since semantic annotation methods cannot easily make use of coreference information. Unlike longer news articles, there is a low amount of discourse information per microblog document, and threaded structure is fragmented across multiple documents, flowing in multiple directions. Second, microtexts exhibit much more language variation, tend to be less grammatical than longer posts, contain unorthodox capitalisation, and make frequent use of emoticons, abbreviations and hashtags, which can form an important part of the meaning. To combat these problems, research has focused on microblog-specific information extraction algorithms (e.g. named entity recognition for Twitter using CRFs (Ritter et al., 2011) or hybrid methods (van Erp, Rizzo, & Troncy, 2013)). Particular attention is given to microtext normalisation (Han & Baldwin, 2011), as a way of removing some of the linguistic noise prior to part-of-speech tagging and entity recognition.In light of the above, this paper aims to answer the following research questions:RQ1How robust are state-of-the-art named entity recognition and linking methods on short and noisy microblog texts?What problem areas are there in recognising named entities in microblog posts, and what are the major causes of false negatives and false positives?Which problems need to be solved in order to further the state-of-the-art in NER and NEL on this difficult text genre?Our key contributions in this paper are as follows. We report on the construction of a new Twitter NEL dataset that remedies some inconsistencies in prior data. As well as evaluating and analysing modern general-purpose systems, we describe and evaluate two domain specific state-of-the-art NER and NEL systems against data from this genre (NERD-ML and YODIE). Also, we conduct an empirical analysis of named entity recognition and linking over this genre and present the results, to aid principled future investigations in this important area.The paper is structured as follows.3Some parts of Sections 3.2, 5.1, 5.2 and 5.3 appeared in an earlier form in Derczynski et al. (2013).3Section 2 evaluates the performance of state-of-the-art named entity recognition algorithms, comparing versions customised to the microblog genre to conventional, news-trained systems, and provides error analysis.Section 3 introduces and evaluates named entity linking, comparing conventional and recent systems and techniques. Sections 2 and 3 answer RQ1.Section 4 examines the performance and errors of recognition and linking systems, making overall observations about the nature of the task in this genre. This section addresses RQ2.Section 5 investigates factors external to NER that may affect entity recognition performance. It introduces the microblog normalisation task, compares different methods, and measures the impact normalisation has on the accuracy of information extraction from tweets, and also examines the impact that various NLP pipeline configurations have on entity recognition.In Section 6, we discuss the limitations of current approaches, and provide directions for future work. This section forms the answer to RQ3.In this paper, we focus only on microblog posts in English, since few linguistic tools have currently been developed for tweets in other languages.Named Entity Recognition (NER) is a critical IE task, as it identifies which snippets in a text are mentions of entities in the real world. It is a pre-requisite for many other IE tasks, including NEL, coreference resolution, and relation extraction. NER is difficult on user-generated content in general, and in the microblog genre specifically, because of the reduced amount of contextual information in short messages and a lack of curation of content by third parties (e.g. that done by editors for newswire). In this section, we examine some state-of-the-art NER methods, compare their performance on microblog data, and analyse the task of entity recognition in this genre.A plethora of named entity recognition techniques and systems is available for general full text (cf. Nadeau & Sekine, 2007; Roberts, Gaizauskas, Hepple, & Guo, 2008; Marrero, Sanchez-Cuadrado, Lara, & Andreadakis, 2009). For Twitter, some approaches have been proposed but they are mostly still in development, and often not freely available. In the remainder of this section, we evaluate and compare a mixture of Twitter-specific and general purpose NER tools. We want to eliminate the possibility that poor NER on tweets is systematic – that is, related to some particular approach, tool, or technology. To this end, we evaluate a wide range of tools and describe their operation. It is also important to establish factors that contribute to making an entity difficult to recognise, and so we use the results of this multilateral comparison in a later analysis.For our analyses of generic NER systems, we chose those that take different approaches and are immediately available as open source. The first system we evaluate is ANNIE from GATE version 8 (Cunningham, Maynard, Bontcheva, & Tablan, 2002), which uses gazetteer-based lookups and finite state machines to identify and type named entities in newswire text. The second system is the Stanford NER system (Finkel, Grenager, & Manning, 2005), which uses a machine learning-based method to detect named entities, and is distributed with CRF models for English newswire text.Of the NER systems available for Twitter, we chose Ritter et al. (2011), who take a pipeline approach performing first tokenisation and POS tagging before using topic models to find named entities, reaching 83.6% F1 measure.In addition to these, we also include a number of commercial and research annotation services, available via Web APIs and a hybrid approach (van Erp et al., 2013), named NERD-ML, tailored for entity recognition of Twitter streams, which unifies the benefits of a crowd entity recognizer through Web entity extractors combined with the linguistic strengths of a machine learning classifier.The commercial and research tools which we evaluate via their Web APIs are Lupedia,4http://lupedia.ontotext.com.4DBpedia Spotlight,5http://dbpedia.org/spotlight.5TextRazor,6http://www.textrazor.com.6and Zemanta.7http://www.zemanta.com.7DBpedia Spotlight and Zemanta allow users to customise the annotation task, hence we applied the following settings: (i) DBpedia Spotlight={confidence=0, support=0, spotter=CoOccurrenceBasedSelector, version=0.6}; (ii) Zemanta ={markup limit:10}.8We wanted to include AlchemyAPI, but its terms and conditions prohibit evaluation without permission, and requests for permission were not answered.8Their evaluation was performed using the NERD framework (Rizzo & Troncy, 2012) and the annotation results were harmonised using the NERD ontology.9http://nerd.eurecom.fr/ontology/nerd-v0.5.n3.9The NERD core ontology provides an easy alignment with the four classes used for this task. The high-performance system reported by Microsoft Research (Liu et al., 2012) is not available for evaluation, so we could not reproduce these figures.In Table 1, we present the main characteristics of the different NER systems, as well as the NEL systems that will be evaluated in Section 3.We evaluated the tools described above on three available datasets. The first is the corpus of tweets developed by Ritter et al. (2011). This corpus consists of 2400 tweets (comprising 34K tokens) and was randomly sampled. Tweet IDs are not included in this dataset, making it difficult to determine the nature of the sampling, including the relevant time window. Examining datestamps on image URLs embedded in the corpus suggest that it was collected during September 2010. The second is the gold standard data created through a Twitter annotation experiment with crowdsourcing at UMBC (Finin et al., 2010) (441 tweets comprising 7037 tokens). The third is the dataset developed for the Making Sense of Microposts 2013 Concept Extraction Challenge (Basave et al., 2013), consisting of a training and test set. For our evaluations, we use the test set (4264 tweets comprising 29,089 tokens). These datasets are all anachronistic to some degree, making them susceptible to entity drift, a significant problem in tweet datasets that we touch on in more detail in Section 3.1.1. Strict matching is used to determine scores. Due to the short document lengths, single entity mistakes can lead to large changes in macro-averaged scores, and so we use micro-averaging; that is, scores are calculated where each entity has equal weight, instead of weighting at document level.These datasets use disparate entity classification schemes, which we mapped to person, location, organisation, and miscellaneous using the mappings shown in Table 2. It also needs to be noted that the different Twitter NER approaches that we evaluate and compare are trained and evaluated on small and custom datasets. This complicates carrying out a comparative evaluation and establishing the state-of-the-art in Twitter NER performance.We exclude Ritter’s T-NER system from the Ritter corpus evaluation, as the released version of the system used this data for training and development.

@&#CONCLUSIONS@&#
