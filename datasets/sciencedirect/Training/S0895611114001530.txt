@&#MAIN-TITLE@&#
Robust multi-scale superpixel classification for optic cup localization

@&#HIGHLIGHTS@&#
Glaucoma is a leading cause of vision loss, & optic cup detection is of great interest.An optimal model integration framework to robustly localize the optic cup is presented.It addresses performance variations from random repeated training.Multiple superpixel scales are also integrated for better cup boundary adherence.It outperforms the intra image learning approach in cup localization accuracy.

@&#KEYPHRASES@&#
Optic cup localization,Glaucoma,Model selection,Superpixel classification,Sparse learning,

@&#ABSTRACT@&#
This paper presents an optimal model integration framework to robustly localize the optic cup in fundus images for glaucoma detection. This work is based on the existing superpixel classification approach and makes two major contributions. First, it addresses the issues of classification performance variations due to repeated random selection of training samples, and offers a better localization solution. Second, multiple superpixel resolutions are integrated and unified for better cup boundary adherence. Compared to the state-of-the-art intra-image learning approach, we demonstrate improvements in optic cup localization accuracy with full cup-to-disc ratio range, while incurring only minor increase in computing cost.

@&#INTRODUCTION@&#
As the global leading cause of irreversible blindness, glaucoma is estimated to visually impair 80 million people by 2020 [1]. It is the most prevalent of eye diseases in the United States and around the world. There are approximately 2.2 million diagnosed cases of glaucoma in the United States [2] and 60 million worldwide; however, this figure is significantly underreported due to the slow, asymptomatic progression of the disease, resulting in low disease detection until vision lost sets in. Undiagnosed rates are reported to be nearly 50% in United States and Australia [2] and as high as 90% in Singapore [3,4]. Glaucoma, which is a group of ocular disease, causes permanent damage to the optic nerve. Vision loss due to glaucoma usually begins from the peripheral vision and progress inwards, resulting in a “tunnel vision”. This “sneaky thief of sight” is a silent disease as most people do not develop onset symptoms, and do notice the lost of their peripheral vision. When glaucoma patients are referred to the ophthalmologists, severe irreversible visual impairment has often already occurred.Although vision loss due to this disease is permanent and cannot be restored, studies have shown that early detection and proactive medical intervention are effective in preserving vision and slowing, or even halting, the progression of the disease [5]. A recent economic study has found that technology-based assessment would be a cost-effective option in improving glaucoma detection for eye examination [6].One of the methods to detect glaucoma is to study the structural damage in the optic nerve head in retinal fundus photographs. In order to gauge the health of the optic nerve in a fundus image, the optic nerve has to be inspected and delineated to identify the optic disc, optic cup and neuroretinal rim [7]. Vertical elongation of the optic cup is a characteristic feature of glaucomatous optic neuropathy [8]. Assessment of the optic cup and optic disc provides important information for glaucoma evaluation, such as the cup-to-disc ratio (CDR). Clinically, these are manually annotated by a trained ophthalmologist.Several automated methods have been proposed to reduce the labor intensive manual workload. Computerized segmentation techniques for optic disc includes Hough transform [9], template matching [10,11], pixel feature classification [12], vessel geometry [13], deformable models [14] and level sets [15].In this paper, we address the challenging problem of optic cup detection. Broadly, existing approaches for optic cup segmentation can be categorized into image processing-based strategies and machine learning models. Examples of techniques employing image processing approaches includes thresholding based on the image's intensity [16], active contour [17], level sets [18], active shape models [19]. To further improve on the segmentation, domain priors such as vessel kinks [20], and r-bends [21] have been incorporated. Recently, [22] proposed using graph cuts to segment the optic cup in fundus images, using location and shape priors obtained from OCT.In recent years, machine learning based approaches have become popular in this domain due to its better accuracy. The use of machine learning methods can be subcategorized into pixel-based, sliding windows and superpixel classification approaches. In [12], the authors proposed using color opponency gaussian filter banks and stereo features on a k-nearest neighbour classifier for pixel-level feature classification. Xu et al. [23] presented a machine learning framework based using a regression model for optic cup segmentation. However, the method is reported to take a long time as it seeks to identify the optic cup as a whole, using a bundle of sliding windows of varying sizes for feature extraction and a RBF support vector regression model to rank each candidate region.Recently, the use of superpixels for feature classification has been widely adopted in retinal imaging for diseases like pathological myopia [24], age-related macula degeneration (AMD) [25] and glaucoma [26]. Examples of such image over-segmentation methods to acquire superpixels includes graph-based segmentation [27], SLIC superpixels[28], and TurboPixel [29,30]. In particular, for optic cup detection, the supervised superpixel-based classification [26,31] has shown to achieve state-of-the-art performance against other existing approaches [18,21,19]. In [31], a centre surround statistics (CSS) feature descriptor based on biological inspired features were used as features for a non-linear support vector machine (SVM) to perform the task of optic cup segmentation. In [26], besides using the general supervised superpixel classification method, domain prior based on intra-image learning, where each model is learnt from samples from each test image without pre-labeling, is introduced to overcome the illumination differences between training and testing images. A refinement scheme is then used to include structural priors and local context for the final cup detection to further boost its performance. However, the approach assumes a CDR ranging between 0.2 and 0.9, which leads to low accuracy in small and large cup localization. Furthermore, at larger superpixel scales, this will lead to fewer training samples, thus, affecting the overall performance. Using an unsupervised approach, [32] proposed using domain priors, derived from cup pallor and optic cup origin, together with features extracted from superpixels. A label refinement using k-means clustering is then used to achieve the task of optic cup localization. Nonetheless, the accuracy of the supervised approaches [26,31] outperforms the unsupervised method, using fewer assumptions (i.e. domain priors).In this work, we identify two limitations in the existing superpixel-based framework. First, an alternative strategy to reduce the effects of varying illumination between images is proposed and experimentally validated. Next, a general multi-scale superpixel classification strategy is proposed to improve the accuracy and robustness for optic cup localization. We build upon the existing single scale superpixel classification framework in [26,32], and incorporated two novel contributions. First, a multiple model selection and integration scheme using sparse learning is proposed to provide stability in performance variations, arising from repeated random samples selection of training data. Second, multiple superpixel resolutions are integrated for better optic cup boundary adherence and localization. We believe this general framework is also adaptable to other similar classification-based ocular disease CAD applications (Fig. 1).Similar to the setup of the work in [26], we start with a disc image to localize the optic cup. First, blood vessels are extracted and the input disc image is enhanced by a contrast normalization scheme. Next, the contrast enhanced image is then segmented into superpixels and blood vessels which overlap with the superpixels are removed. Features are then extracted across multiple superpixel scales, and multiple classification models are trained for each respective scale. To obtain an unique label for each pixel with higher accuracy, optimal superpixel classification models are selected and integrated using a sparse learning approach. The final optic cup area is then identified by using an ellipse to enclose all the pixels predicted as ‘cup’. The framework of our overall approach is illustrated in Fig. 2.To reduce the effects on rim/cup misclassification, we used a multi-scale difference-of-closing vessel extraction algorithm by [34] for blood vessel removal. The benefits of this method lies in its flexibility in quick parameter tuning to extract only thicker blood vessels, and the vessel extraction process is simple and fast, requiring only basic morphological operations. A disk structuring element of radius of 16 pixels and 3 pixels for the dilation and erosion operations respectively. As the images are processed at superpixels scale, a blood vessel mask is generated based on superpixels that overlap the vessel extraction mask by at least 75%.The influence of illumination variances between training and learning images can affect the performance of the trained machine learning models. To reduce these effects, a pre-processing using histogram stretching is performed. For each RGB channel in a disc image, the image intensities are represented as a histogram and stretched into [0, 255] to expand the dynamic range and achieve consistency across all the images. This pre-processing histogram normalization accomplishes two goals, 1) it provides better illumination alignment for all images, 2) it helps to enhance the contrast between the optic cup and rim.Instead of pixel-level classification, labels are assigned for each superpixel segment for efficiency. This bottom-up segmentation of superpixels not only reduces the complexity of our problem, but also summarizes image redundancy and provides highly desirable spatial and local boundaries adhering properties. Superpixels are acquired using the SLIC (Simple Linear Iterative Clustering) algorithm [28] which groups pixels by adapting a k-means clustering approach. Each SLIC superpixel corresponds to a cluster in a five-dimensional color (CIELAB) and image location plane space. The advantages of using the SLIC algorithm to perform superpixel segmentation are 1) its efficiency to generate compact, nearly uniform superpixel segments with O(N) complexity, and, 2) only requiring a single parameter, which is to specify the number of superpixels.Experimental studies in [26] demonstrated the effects of different superpixel scales and their resulting performance accuracy. In particular, it was found that larger superpixel segments provides richer and more discriminative features compared to smaller superpixels, but are prone to over-segmentation in ambiguously-labeled boundary superpixels. In contrast, smaller superpixel regions offers less distinctive features but are able to adhere to the cup boundary with greater precision. This trade-off is unavoidable in a single scale classification framework. Instead of using a “one-size-fits-all” strategy, our approach proposes to merge multiple superpixel scales segments to unify their advantages for better boundary fit.Using the superpixels as building blocks, features can be extracted to describe each segment. Examples of features that had been used to represent superpixels in object recognition and human body segmentation, includes size, shapes, texture, colors, boundary contours, and thumbnail appearances [35,36]. Following the features extracted in [26], for each j-th superpixel at superpixel scale Sc(i.e. Sc∈{S1, …, Sβ}), we extract a feature vectorfjcthat consists of position information (coordinates and distance of each superpixel centroid with respect to the disc centre) denoted by (xj, yj, ρj), mean RGB colors (rj, gj, bj) and a 256-bin histogram (hjr,hjg,hjb) for each color channel. To avoid magnitude differences among the features, they are each normalized to the range of [0, 1], with L1-normalization of each histogram. Finally, for each scale Sc, the feature matrix of all superpixelsFccan be obtained.At each scale Sc, α linear SVM models are trained to predict each test superpixel to be either disc(−1) or cup (+1). For each test superpixel, with featurefjc, using a pre-learnt linear SVM modelwjcat scale Sc, the predicted label can be obtained by(wjc)⊤fjc(i.e., the superpixel is classified as cup(+1) when(wjc)⊤fjc≥0with this pre-learned model, otherwise it is classified as disc(−1)).Using a one-time random sampled superpixels with manual labels (determined by an overlapping ratio with ground truth cup), a base model can be learned using a linear SVM. However, since one time random sampling only learns a model which represents a small area in the feature space, its performance may vary significantly. To overcome this bias, multiple base models can be obtained by repeated random sampling. Furthermore, in order to avoid unbalanced training data distribution, equal positive and negative samples are selected for each training round. In total, αβ base models are trained for β scales. At each scale, the α base models can be used to get α predictions simultaneously without much additional cost. This is performed simply by concatenating each single model projection vectorwjc, as columns, to form a projection matrixWc. The prediction labels for a single scale can then be found by(Wc)⊤Fjc.In the training phases, for each pixel, αβ base models are used to classify it as disc or cup. To get a unique and accurate classification with minimal computation cost, we solve the following problem:(1)minω∑a=1τ∥la−ω⊤da∥22+λ∥ω∥1wheredais the αβ predictions from the linear SVM base models and, lais the training label. The first term is to minimize the prediction error, and the second term is regularized by λ to enforce the sparsity ofω, which means, only a few linear base models are selected for the unique final prediction for each pixel. In our implementation, the SLEP [37] toolbox is used to train the integration model. The solution ofωindicates which base models are chosen to generate the final feature Dsof each pixel, i.e., the ith base model is selected when |ωi|>10−3. A unique integration modelΩis then obtained by consolidating the weight parameters of these K selected base models inω. In real applications, only a small number of selected models are used to compute the intermediate results for making final classification. This allows a boost to the performance without much additional cost.To summarize, in the testing phase, four steps are sequentially performed to obtain the final pixel-level label from the original image. 1) Preprocessing for blood vessel extraction and contrast enhancement, 2) Superpixel segmentation, blood vessel removal and feature extraction at multiple scales, Sc∈{512, 1024, 2048}, 3) Using the selected base models, a feature vector for the s-th pixel,Ds, is formed by concatenating the respective classification decision values of selected models indicated byω. 4) The integration modelΩis then used to assign each pixel with a unique labelΩ⊤Ds.As the given ground-truth for optic cup segmentation is an elliptical reference [33], after obtaining the final labels for each pixel, a minimum ellipse that encloses all positive labeled pixels is computed to produce the final optic cup boundary estimate. Another major reason to introduce ellipse fitting is to include superpixels lying on the blood vessels which had been previously removed. Moreover, the use of ellipse-fitting to produce an accurate single optic cup region has also been widely used and reported in related works [18,23,26,31,32]. In our implementation, the ellipse-fitted optic cup is represented by its centre and elongation parameters(u,v,υ,ν). Fig. 3illustrates the major steps of our approach.

@&#CONCLUSIONS@&#
