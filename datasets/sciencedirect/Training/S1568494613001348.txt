@&#MAIN-TITLE@&#
Fuzzy neural network for solving a system of fuzzy differential equations

@&#HIGHLIGHTS@&#
Graphical abstractWe consider a system of fuzzy differential equations.We employ partially fuzzy neural network for solving system of fuzzy differential equations.We propose a learning algorithm from the cost function for adjusting of weights.

@&#KEYPHRASES@&#
Partially fuzzy neural network,System of fuzzy differential equations,Learning algorithm,

@&#ABSTRACT@&#
Fuzzy neural network (FNN) can be trained with crisp and fuzzy data. This paper presents a novel approach to solve system of fuzzy differential equations (SFDEs) with fuzzy initial values by applying the universal approximation method (UAM) through an artificial intelligence utility in a simple way. The model finds the approximated solution of SFDEs inside of its domain for the close enough neighborhood of the fuzzy initial points. We propose a learning algorithm from the cost function for adjusting of fuzzy weights. At the same time, some examples in engineering and economics are designed.

@&#INTRODUCTION@&#
Proper design for engineering applications requires detailed information of the system-property distributions such as temperature, velocity, and density in space and time domain [8,9]. This information can be obtained by either experimental measurement or computational simulation. Although experimental measurement is reliable, it needs a lot of labor efforts and time. Therefore, the computational simulation has become a more and more popular method as a design tool since it needs only a fast computer with a large memory. Frequently, those engineering design problems deal with a set of DEs or SDEs, which have to numerically solved such as heat transfer, solid and fluid mechanics. Numerical methods of predictor–corrector, Runge-Kutta, finite difference, finite element, boundary element, spectral and collocation provide a strategy by which we can attack many problems in applied mathematics, where we simulate a real-word problem with a system of differential equations, subject to some initial or boundary conditions. In the finite difference and finite element methods we approximate the solution by using the numerical operators of the function's derivatives and finding the solution at specific preassigned grids [52]. The linearity is assumed for the purposes of evaluating the derivatives. Although such an approximation method is conceptually easy to understand, it has a number of shortcomings. Obviously, it is difficult to apply for systems with irregular geometry or unusual boundary conditions. Predictor–corrector and Runge-Kutta methods are widely applied over preassigned grid points to solve SDEs [31]. In the spectral and collocation approaches a truncated series of the specific orthogonal functions (basis functions) are used for finding the approximated solution of the SDEs. In the spectral and collocation techniques the role of trial functions as a basis function is important. The trial functions used in spectral methods are chosen from various classes of Jacobian polynomials [17], still the discretization meshes are preassigned. Neural network model is used to approximate the solutions of SDEs for the entire domains. In 1990 the authors of [33] used parallel computers to solve a first order differential equation with Hopfield neural network models. Meade and Fernandez [37,38] solved linear and nonlinear ordinary differential equations using feed forward neural networks architecture and B1-splines. Leephakpreeda [34] applied neural network model and linguistic model as universal approximators for any nonlinear continuous functions. With this outstanding capability, the solution of SDEs can be approximated by the appropriate neural network model and linguistic model within an arbitrary accuracy.When a physical problem is transformed into a deterministic system of initial value problems(1)dy1(x)dx=f1(x,y1,…,yn),dy2(x)dx=f3(x,y1,…,yn),⋮dyn(x)dx=fn(x,y1,…,yn),y1(a)=A1,…,yn(a)=An,x∈[a,b].We usually cannot be sure that this modeling is perfect. The initial values may not be known exactly and the function fimay contain unknown parameters. If the nature of errors is random, then instead of a deterministic problem (1) we get a random differential equation with random initial value and/or random coefficients. But if the underlying structure is not probabilistic, e.g., because of subjective choice, then it may be appropriate to use fuzzy numbers instead of real random variables.The topic of FDEs has been rapidly growing in recent years. The fuzzy initial value problem have been studied by several authors [1,2,6,7,12,15,29,40,41,47]. The concept of fuzzy derivative was first introduced by Chang and Zadeh [11], it was followed up by Dubois and Prade [13] who used the extension principle in their approach. Other methods have been discussed by Puri and Ralescu [46] and by Goetschel and Voxman [16]. Fuzzy differential equations were first formulated by Kaleva [27] and Seikkala [48] in time dependent form. Kaleva had formulated fuzzy differential equations, in terms of Hukuhara derivative [27]. Buckley and Feuring [10] have given a very general formulation of a fuzzy first-order initial value problem. They first find the crisp solution, make it fuzzy and then check if it satisfies the FDE. In [14], investigated the existence and uniqueness of solution for fuzzy random differential equations with non-lipschitz coefficients and fuzzy differential equations with piecewise constant argument.During the past few years, neural networks have received much attention Mosleh et al. [39,40,42–44]. Mosleh and Otadi [40] considered FDEdy(x)dx=f(x,y),y(a)=A,and they proposed FNNM for the approximated solution. In this work, we generalize a new solution method for the approximated solution of SFDEs (1) using innovative mathematical tools and neural-like systems of computation. This hybrid method can result in improved numerical methods for solving fuzzy initial value problems. In this proposed method, FNN is applied as UA. We use fuzzy trial function, this fuzzy trial function is a combination of two terms. A first term is responsible for the fuzzy initial while the second term contains the fuzzy neural network adjustable parameters to be calculated. The main aim of this paper is to illustrate how fuzzy connection weights are adjusted in the learning of FNN by the back-propagation-type learning algorithms [23,26] for the approximated solution of SFDEs.In this section the most basic notations used in fuzzy calculus are introduced. We start by defining the fuzzy number.Definition 1A fuzzy number is a fuzzy setA:ℝ1⟶I=[0,1]which satisfiesi.A is upper semi-continuous.A(x)=0 outside some interval [a, d].There are real numbers b, c:a≤b≤c≤d for whichA(x) is monotonic increasing on [a, b],A(x) is monotonic decreasing on [c, d],A(x)=1, b≤x≤c.The set of all the fuzzy numbers (as given by Definition 1) is denoted by E1.An h-cut of A, written [A]h, is {x|A(x)≥h}, 0<h≤1. We need to define [A]0 separately, but since all our fuzzy sets will be A=(a/b/c/d), we define [A]0=[a, d]. A0 is called the support of A. If A=(a/b/c/d), then[A]h=[[A]hL,[A]hU]a closed, bounded, interval for all h in [0, 1].A collection of intervals[[A]hL,[A]hU],0≤h≤1,defines a trapezoidal (shaped) fuzzy number if: (1)−∞<[A]0L<[A]1L<[A]1U<[A]0U<∞; and (2)[A]hL([A]hU)is a continuous monotonically increasing (decreasing) function of h for 0≤h≤1.In E we define the metricD(A,B)=sup0≤h≤1d([A]h,[B]h),where d is the Hausdorff metric.In the sequel we use the concepts of differentiability, which generalize those of Hukuhara differentiability. The definition of the derivative is from Puri and Ralescu [46].Let X=[a, b] with a>0 and A, B∈E. If there exists a C∈E such that A=B+C then we call C the H-difference of A and B, denoted A−B.Definition 2A mapping f:X→E is differentiable at x∈X if there exists a f′(x)∈E such that the limitslimh→0+f(x+h)−f(x)handlimh→0+f(x)−f(x−h)h,exist and are equal todf(x)dx.Here the limit is taken in the metric space (E, D). At the end points of X we consider only the one-sided derivatives.Artificial neural networks are an exciting form of artificial intelligence which mimic the learning process of the human brain in order to extract patterns from historical data [49,4]. For many years this technology has been successfully applied to a wide variety of real-word applications [45]. Simple perceptrons need a teacher to tell the network what the desired output should be. These are supervised networks. In an unsupervised net, the network adapts purely in response to its inputs. Such networks can learn to pick out structure in their input. Fig. 1shows typical three-layered perceptron. Multi-layered perceptrons with more than three layers, use more hidden layers [20,28]. Multi-layered perceptrons correspond the input units to the output units by a specific nonlinear mapping [51]. From Kolmogorov existence theorem we know that a three-layered perceptron with n(2n+1) nodes can compute any continuous function of n variables [35,21]. The accuracy of the approximation depends on the number of neurons in the hidden layer and does not depend on the number of the hidden layers [32].Before describing a fuzzy neural network architecture, we denote real numbers and fuzzy numbers by lowercase letters (e.g., a, b, c, …) and uppercase letters (e.g., A, B, C, …), respectively.We briefly mention fuzzy number operations defined by the extension principle [54,55]. Since input vector of feedforward neural network is fuzzy in this paper, the following addition, multiplication and nonlinear mapping of fuzzy numbers are necessary for defining our fuzzy neural network:(2)μA+B(z)=max{μA(x)∧μB(y)|z=x+y},(3)μf(Net)(z)=max{μNet(x)|z=f(x)},where A, B, Net are fuzzy numbers, μ*(.) denotes the membership function of each fuzzy number, ∧ is the minimum operator and f(.) is a continuous activation function (like sigmoidal activation function) inside hidden neurons.From interval arithmetic [5], the above operations of fuzzy numbers are written for h-level sets as follows:(4)[A]h+[B]h=[[A]hL+[B]hL,[A]hU+[B]hU],(5)w·[A]h=[w·[A]hL,w·[A]hU],ifw≥0,w·[A]h=[w·[A]hU,w·[A]hL],ifw≤0.(6)f([Net]h)=f([[Net]hL,[Net]hU])=[f([Net]hL),f([Net]hU)],where f is a activation function which is normally nonlinear function, the usual choices of the activation function [18] are the sigmoid transfer function.Let us consider a fuzzy three-layer feedforward neural network with 1 input units, m hidden units and 1 output units. In order to derive a crisp learning rule, we restrict connection weights by four types of numbers: real numbers, symmetric triangular fuzzy numbers, asymmetric triangular fuzzy numbers and asymmetric trapezoidal fuzzy numbers. We can use any type of fuzzy numbers for fuzzy targets. For example, an asymmetric triangular fuzzy connection weight is specified by its three parameters asVkj=(vkjL,vkjC,vkjU).When an 1-dimensional input x is presented to our fuzzy neural network, its input–output relation can be written as follows, wheref:ℝ⟶E:Input units:(7)o=x.Hidden units:(8)zij=f(netij),j=1,2,…,m,i=1,…,n,(9)netij=o·wij+bij.Output unit:(10)Ni=f(Neti),(11)Neti=∑j=1mzij·Vij,where f is a sigmoidal activation function. The architecture of our FNN is shown in Fig. 1. The input–output relation in Eqs. (7)–(11) is defined by the extension principle [54] as in Hayashi et al. [19] and Ishibuchi et al. [25].The input–output relations of our FNN can be written for the h-level sets:Input unit:(12)o=x.Hidden units:(13)zij=f(netij),j=1,2,…,m,i=1,…,n,(14)netij=∑i=1no·wij+bij,Output unit:(15)[Ni]h=[[Ni]hL,[Ni]hU]=[f([Neti]hL),f([Neti]hU)],where f is a sigmoidal activation function.(16)[Neti]hL=∑j∈azij·[Vij]hL+∑j∈bzij·[Vij]hU,[Neti]hU=∑j∈azij·[Vij]hU+∑j∈bzij·[Vij]hL,where a={j∣zij≥0}, b={j∣zij<0} and a∪b={1, …, m}.We are interested in finding the approximate solution of SFDEsdy1(x)dx=f1(x,y1,…,yn),dy2(x)dx=f3(x,y1,…,yn),⋮dyn(x)dx=fn(x,y1,…,yn),where yiis a fuzzy function of x, fi(x, y1, y2, …, yn) is a fuzzy function of the crisp variable x and the fuzzy variables yi, i=1, 2, …, n, anddyi(x)dxis the fuzzy derivative of y (Definition 2). If an initial value yi(a)=Aiis given, we obtain a fuzzy Cauchy problem of first order:(17)dy1(x)dx=f1(x,y1,…,yn),dy2(x)dx=f3(x,y1,…,yn),⋮dyn(x)dx=fn(x,y1,…,yn),y1(a)=A1,⋯,yn(a)=An,x∈[a,b],where Ai, i=1, 2, …, n are fuzzy numbers in E with h-level sets[Ai]h=[[Ai]hL,[Ai]hU],i=1,2,…,n,0<h≤1.Sufficient condition for the existence of the solution of SFDEs (17), where Aiis a fuzzy number, are given in [53].Let us assume that a general approximation solution to Eq. (17) is in the formyTi(x,Pi)foryTias a dependent variable to x and Pi, where Piis an adjustable parameter involving weights and biases in the structure of the three-layered feed forward FNNM (see Fig. 1). The fuzzy trial solutionyTiis an approximation solution to yifor the optimized value of unknown weights and biases. Thus the problem of finding the approximated fuzzy solution for Eq. (17) over some collocation points in [a, b] by a set of discrete equally spaced grid pointsa=x1<x2<⋯<xg=b,is equivalent to calculate the functionalyTi(x,Pi).In order to obtain fuzzy approximate solutionyTi(x,Pi), we solve unconstrained optimization problem that is simpler to deal with, we define the fuzzy trial function to be in the following form:(18)yTi(x,Pi)=αi(x)+β[x,Ni(x,Pi)],i=1,2,…,n,where the first term in the right hand side does not involve with adjustable parameters and satisfies the fuzzy initial condition. The second term in the right hand side is a feed forward three-layered FNN consisting of an input x and the output Ni(x, Pi). The crisp trial function was used in [36]. In the next subsection, this FNNM with some weights and biases is considered and we train in order to compute the approximate solution of problem (17).Let us consider a three-layered FNNM (see Fig. 1) with one unit entry x, one hidden layer consisting of m activation functions and one unit output Ni(x, Pi).Here, the dimension of FNNM is 1×m×1.One drawback of fully FNN with fuzzy connection weights is long computation time. Another drawback is that the learning algorithm is complicated. For reducing the complexity of the learning algorithm, we propose a partially fuzzy neural network (PFNN) architecture where connection weights to output unit are fuzzy numbers while connection weights and biases to hidden units are real numbers [24,40]. Since we had good simulation results even from partially fuzzy three-layer NN, we do not think that the extension of our learning algorithm to NNM with more than three layer is an attractive research direction.For every entry x the input neuron makes no changes in its input, so the input to the hidden neurons is(19)netij=x.wij+bij,j=1,…,m,i=1,…,n,wherewijis a weight parameter from input layer to the jth unit in the hidden layer, bijis an jth bias for the jth unit in the hidden layer for approximate yi. The output, in the hidden neurons is(20)zij=s(netij),j=1,…,m,i=1,…,n,where s(.) is a sigmoidal activation function, and the output neuron make no change its input, so the input to the output neuron is equal to output(21)Ni=Vi1zi1+⋯+Vijzij+⋯+Vimzim,where Vijis a weight parameter from jth unit in the hidden layer to the output layer.We can be rewritten for h-level sets of the Eqs. (19)–(21). In this paper we assume that the input x is a non-negative real number, i.e., 0≤x[23]:Input unit:(22)o=x.Hidden units:(23)zij=s(netij),j=1,…,m,i=1,…,n,(24)netij=o·wij+bij.Output unit:(25)[Ni]h=[[Ni]hL,[Ni]hU]=∑j=1m[Vij]hL.zij,∑j=1m[Vij]hU.zij.A PFNN4 (partially fuzzy neural network with crisp set input, crisp number weights and biases to hidden units, fuzzy number weights to output unit and fuzzy number output) solution to Eq. (17) is given in Fig. 1. How is the PFNN4 going to solve the SFDEs? The training data are a=x1<x2<⋯<xg=b for inputs. We propose a learning algorithm from the cost function for adjusting weights.Consider the following fuzzy initial value problem for a first order system of differential equation (17), the related trial function will be in the form(26)yTi(x,Pi)=Ai+(x−a)Ni(x,Pi),i=1,2,…,n,this solution by intention satisfies the initial condition in (17). In [23], the learning of our fuzzy neural network is to minimize the difference between the fuzzy target vector B=(B1, …, Bs) and the actual fuzzy output vector O=(O1, …, Os). The following cost function was used in [23,3] for measuring the difference between B and O:(27)e=∑heh=∑hh·∑k=1s([Bk]hL−[Ok]hL)22+∑k=1s([Bk]hU−[Ok]hU)22,where ehis the cost function for the h-level sets of B and O. The squared errors between the h-level sets of B and O are weighted by the value of h in (27).In [24], it is shown by computer simulations, the fitting of fuzzy outputs to fuzzy targets is not good for the h-level sets with small values of h when we use the cost function in (27). This is because the squared errors for the h-level sets are weighted by h in (27). Krishnamraju et al. [30] used the cost function without the weighting scheme:(28)e=∑heh=∑h∑k=1s([Bk]hL−[Ok]hL)22+∑k=1s([Bk]hU−[Ok]hU)22.In the computer simulations included in this paper, we mainly use the cost function in (28) without the weighting scheme.The error function that must be minimized for problem (17) is in the form(29)e=∑i=1n∑k=1geik=∑i=1n∑k=1g∑heikh=∑i=1n∑k=1g∑h{eikhL+eikhU},where(30)eikhL=dyTi(xk,Pi)dxhL−[fi(xk,yT1(xk,P1),…,yTn(xk,Pn))]hL22,(31)eikhU=dyTi(xk,Pi)dxhU−[fi(xk,yT1(xk,P1),…,yTn(xk,Pn))]hU22.and{xk}k=1gare discrete points belonging to the interval [a, b] and in the cost function (29)eikhL,can be viewed as the squared errors for the lower limits andeikhU,the upper limits of the h-level sets.It is easy to express the first derivative of Ni(x, Pi) in terms of the derivative of the sigmoid function, i.e.,(32)∂[Ni]hL∂x=∑j=1m[Vij]hL·∂zij∂netij·∂netij∂x=∑j=1m[Vij]hL·zij·(1−zij)·wij,(33)∂[Ni]hU∂x=∑j=1m[Vij]hU·∂zij∂netij·∂netij∂x=∑j=1m[Vij]hU·zij·(1−zij)·wij.Now differentiating from trial function yiT(x, Pi) in (30) and (31), we obtain∂[yTi(x,Pi)]hL∂x=[Ni(x,Pi)]hL+(x−a)·∂[Ni(x,Pi)]hL∂x,∂[yTi(x,Pi)]hU∂x=[Ni(x,Pi)]hU+(x−a)·∂[Ni(x,Pi)]hU∂x,thus the expressions in (32) and (33) are applicable here. A learning algorithm is derived in Appendix A.To illustrate the technique proposed in this paper, consider the following examples.Example 1Consider the following FDEsdy1(x)dx=y2(x),dy2(x)dx=y1(x),x∈[0,0.5]and defining the initial values to be y1(0) and y2(0) are equal to about 2 which can be done by setting, for example,[y1(0)]h=[h2+h,4−h3−h],[y2(0)]h=[1+h,3−h],0≤h≤1.The exact solution is[y1(x)]h=12+h+h22ex+h22−12e−x,72−h32−hex+12−h32e−x,[y2(x)]h=12+h+h22ex+12−h22e−x,72−h32−hex+h32−12e−x,0≤h≤1.Here, the dimension of PFNN is 1×5×1. The error function for the m=5 sigmoid units in the hidden layer and for g=6 equally spaced points inside the interval [0, 0.5] is trained. In the computer simulation of this section, we use the following specifications of the learning algorithm.(1)Number of hidden units: five units.Stopping condition: 200 iterations of the learning algorithm.Learning constant: η=0.1.Momentum constant: α=0.2.Initial value of the weights and biases of PFNN are shown in Table 1, that we supposeVij=(vij(1),vij(2),vij(3))for i=1, 2, j=1, …, 5.We apply the proposed method to the approximate realization of solution of problem (17). A comparison between the exact and approximate solutions at x=0.2 is shown in Tables 2and 3and Figs. 2and 3.Example 2Consider the following FDEsdy1(x)dx=y1(x)+y2(x),dy2(x)dx=−y1(x)+y2(x),x∈[0,0.5]and defining the initial values to be y1(0) and y2(0) are equal to about 2 and about 1, respectively which can be done by setting, for example,[y1(0)]h=[1+h,3−h],[y2(0)]h=[h,2−h],0≤h≤1.The exact solution is[y1(x)]h=[(h−1)e2x+exsin(x)+2excos(x),(1−h)e2x+exsin(x)+2excos(x)],[y2(x)]h=[(h−1)e2x+excos(x)−2exsin(x),(1−h)e2x+excos(x)−2exsin(x)],0≤h≤1.Here, the dimension of PFNN is 1×5×1. The error function for the m=5 sigmoid units in the hidden layer and for g=6 equally spaced points inside the interval [0, 0.5] is trained. In the computer simulation of this section, we use the following specifications of the learning algorithm.(1)Number of hidden units: five units.Stopping condition: 200 iterations of the learning algorithm.Learning constant: η=0.1.Momentum constant: α=0.2.Initial value of the weights and biases of PFNN are shown in Table 1, that we supposeVij=(vij(1),vij(2),vij(3))for i=1, 2, j=1, …, 5.Example 3Vibrating mass [10]Consider the vibrating mass in Fig. 6. The mass m=1 slug, the spring constant k=4lbs/ft and there is no, or negligible, damping. The forcing function is 2cos(βx), for 0<β<2. The system of differential equation of motion isy2′(x)+4y1(x)=2cos(βx),y1′(x)=y2(x).The uncertain initial conditions are[y1(0)]h=[2h,4−2h],[y2(0)]h=[−2+2h,2−2h],0≤h≤1.The unique solution is[y1(x)]hL=(2h)cos(2x)+(−1+h)sin(2x)+Φ1(x),[y1(x)]hU=(4−2h)cos(2x)+(1−h)sin(2x)+Φ1(x),[y2(x)]hL=(−4h)sin(2x)+(−2+2h)cos(2x)+Φ2(x),[y2(x)]hU=(−8+4h)sin(2x)+(2−2h)cos(2x)+Φ2(x),whereΦ1(x)=24−β2[cos(βx)−cos(2x)],Φ2(x)=24−β2[−βsin(βx)+2sin(2x)].The graph of y1(x) and y2(x), h=0 and h=1 cuts, for β=1.9, are shown in Figs. 7and 8, respectively (the outside curves are the h=0 cut and the center curve the h=1 cut).Here, the dimension of PFNNM is 1×5×1. The error function for the m=5 sigmoid units in the hidden layer and for g=4 equally spaced points inside the interval[0,π14]is trained. In the computer simulation of this section, we use the following specifications of the learning algorithm.(1)Number of hidden units: five units.Stopping condition: 200 iterations of the learning algorithm.Learning constant: η=0.2.Momentum constant: α=0.1.Initial value of the weights and biases of PFNNM are shown in Table 1, that we supposeVi=(vi(1),vi(2),vi(3))for i=1, …, 5. We apply the proposed method to the approximate realization of solution of problem (17). A comparison between the exact and approximate solutions atx=π14are shown in Figs. 9and 10.Example 4Dynamic supply and demandThis application is adopted from the example on pages 250–252 in [50].We are considering an item such that increasing its price P results in an increase in supply S but that increasing its supply S will ultimately decrease its price P. We will assume there are two factors that influence price: inflation and supply. The differential equation for price is(34)P′=θ−k1(S−S0),where θ is the rate of inflation and S0 is equilibrium supply, and k1>0 is a constant. In Eq. (34) we have: (1) if S>S0, the supply is too large and price is to decrease; and (2) if S<S0, supply is too low and price tends to increase. This is the reason we put −k1(S−S0), k1>0, into the equation.We also assume that the rate of change of S is proportional to (P−P0) for equilibrium price P0. The equation for supply is(35)S′=k2(P−P0),for some constant k2>0. The factor K2(P−P0) means: (1) if P>P0, price is high and supply increasing; and (2) when P<P0, price is low and supply decreases.Given fuzzy initial conditions [P(0)]h=[20+5h, 30−5h] and [S(0)]h=[550+50h, 650−50h] we solve Eqs. (34) and (35) for their unique solutions. To simplify the algebra we will now use the following values for the constants: θ=0.05, k1=k2=0.5, P0=25 and S0=1200. The unique solution is[P(x)]hL=452r−452e−x2−552−552rex2+Φ1(x),[P(x)]hU=−452r+452e−x2+552−552rex2+Φ1(x),[S(x)]hL=452−452re−x2−552−552rex2+Φ2(x),[S(x)]hU=452r−452e−x2+552−552rex2+Φ2(x),whereΦ1(x)=−600110cosx2−225sinx2+12,00110,Φ2(x)=600110sinx2−225cosx2+250.Here, the dimension of PFNNM is 1×5×1. The error function for the m=5 sigmoid units in the hidden layer and for g=10 equally spaced points inside the interval [0, 1] is trained. In the computer simulation of this section, we use the following specifications of the learning algorithm.(1)Number of hidden units: five units.Stopping condition: 200 iterations of the learning algorithm.Learning constant: η=0.2.Momentum constant: α=0.1.Initial value of the weights and biases of PFNNM are shown in Table 1, that we supposeVi=(vi(1),vi(2),vi(3))for i=1, …, 5. We apply the proposed method to the approximate realization of solution of problem (17). A comparison between the exact and approximate solutions at x=1 are shown in Figs. 11and 12.Solving SFDEs by using UA, that is, PFNN is presented in this paper. The problem formulation of the proposed UA is quite straightforward. To obtain the “Best-approximated” solution of SFDEs, the adjustable parameters of PFNN are systematically adjusted by using the learning algorithm.In this paper, we derived a learning algorithm of fuzzy weights of tree-layer feedforward FNN whose input–output relations were defined by extension principle. The effectiveness of the derived learning algorithm was demonstrated by computer simulation on numerical examples. Since we had good simulation result even from partially fuzzy three-layer neural networks, we do not think that the extension of our learning algorithm to neural networks with more than three layers is an attractive research direction.

@&#CONCLUSIONS@&#
