@&#MAIN-TITLE@&#
Normalization of informal text

@&#HIGHLIGHTS@&#
Normalization of abbreviations in noisy, informal text.Collection, filtering and annotation of Twitter status messages.Comparison of statistical and machine translation approaches.Effects of language model order on accuracy.Combination of methods to achieve best results.

@&#KEYPHRASES@&#
Text normalization,Noisy text,NLP applications,

@&#ABSTRACT@&#
This paper describes a noisy-channel approach for the normalization of informal text, such as that found in emails, chat rooms, and SMS messages. In particular, we introduce two character-level methods for the abbreviation modeling aspect of the noisy channel model: a statistical classifier using language-based features to decide whether a character is likely to be removed from a word, and a character-level machine translation model. A two-phase approach is used; in the first stage the possible candidates are generated using the selected abbreviation model and in the second stage we choose the best candidate by decoding using a language model. Overall we find that this approach works well and is on par with current research in the field.

@&#INTRODUCTION@&#
Text messaging is a rapidly growing form of alternative communication for cell phones. This popularity has caused safety concerns leading many US states to pass laws prohibiting texting while driving. The technology is also difficult for users with visual impairments or physical handicaps to use. We believe a text-to-speech (TTS) system for cell phones can decrease these problems to promote safe travel and ease of use for all. Normalization is the usual first step for TTS.SMS lingo is similar to the chatspeak that is prolific on forums, blogs and chatrooms. Screen readers will thus benefit from such technology, enabling visually impaired users to take part in internet culture. In addition, normalizing informal text is important for tasks such as information retrieval, summarization, and keyword, topic, sentiment and emotion detection, which are currently receiving a lot of attention for informal domains.Normalization of informal text is complicated by the large number of abbreviations used. Some previous work on this problem used phrase-based machine translation (MT) for abbreviation normalization; however, a large annotated corpus is required for such a method since the learning is performed at the word level. By definition, this method cannot make a hypothesis for an abbreviation it did not see in training. This is a serious limitation in a domain where new words are created frequently and irregularly.This work is an extension of our work in Pennell and Liu (2010, 2011, 2011). In this paper, we establish two sets of baseline results for this problem on our data set. The first uses a language model for decoding without use of an abbreviation model, while the second utilizes a state-of-the art spell checking module, Jazzy Idzelis (2005). We then compare the use of our two abbreviation models for decoding informal text sentences. We also determine the effects on decoding accuracy when more or less context is available. Finally, we combine the two systems in various ways and demonstrate that a combined model performs better than both systems individually.

@&#CONCLUSIONS@&#
In this paper, we have provided an extensive comparison of two abbreviation models for normalizing abbreviations found in informal text. Both models yield improvements over two baselines – using a language model alone for decoding and a state-of-the-art spell-checking algorithm – even when using the score models with no context. With context and an LM, we significantly outperform both baselines. Our MT model vastly outperforms our CRF model, even on the deletion-type abbreviations for which the CRF model was designed. It will be interesting future work to use our data to compare our MT model to the extended CRF model used by Liu et al. (2011).Increasing the order of the language model yields a fairly large increase in performance from a unigram to a bigram model, with a small increase in performance when moving to trigram models. When combining the abbreviation and language models, giving the language model much higher priority gives the best performance on abbreviations. This explains why using our systems to prune the hypotheses checked by the language model does not cause a large decrease in performance on abbreviations. However, the pruning only method leads to worse overall results due to false positives.We also tested combinations of the two models to create a single system that performs better than either model alone. During tests on abbreviations with context words in standard form, we see a slight increase in performance when taking a weighted average of the two models (with MT weighted higher than CRF). However, a higher false positive rate means this does not translate to overall improvement at the message level.False positives are a major area for future work. One heuristic is to leave a token as-is if we find it in a dictionary. Preliminary tests using this heuristic improved precision but greatly decreased recall because many abbreviations are themselves words, (e.g.. “cat” for “category” or “no” for “number”). Additionally, it is difficult to find an appropriate dictionary. The dictionaries we tried are either missing some common words or contain many acronyms and chat slang, defeating the purpose of the heuristic. Alternatively, we are considering using the dictionary not as a definitive source of whether a token should be expanded, but rather if the word is found we do not expand it unless the LM score, AM score, or their combination is “high enough”. This work is still in progress, but preliminary results look promising.In addition, the MT model still needs some improvement. The correct translations did not appear in the 20-best list generated for over 20% of the abbreviations in this data. We hope that optimizing the parameters in Moses or moving to a factored model will help decrease this percentage. We also have the potential to improve our system by reranking the results that Moses generates using a length model ot other metric as with the CRF system.Finally, we believe that the machine translation method is mainly language independent and are expanding our work to languages other than English, with preliminary results on Spanish tweets. We are also investigating its use for deromanization of non-Latin-script languages that have been informally transliterated for use in social media applications.