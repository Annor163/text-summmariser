@&#MAIN-TITLE@&#
Rotation-invariant texture image retrieval using particle swarm optimization and support vector regression

@&#HIGHLIGHTS@&#
A rotation-invariant texture image retrieval technique adopts the PSO and the SVR.LPM-FFT, Gabor filter, and Zernike moment are utilized to extract image features.The PSO algorithm is utilized to select a nearly optimal parameter set for the technique.The technique uses the SVR to promote its image-retrieval efficiency.The technique outperforms other existing rotation-invariant image retrieval methods.

@&#KEYPHRASES@&#
Content-based image retrieval,Log-polar mapping,Fast Fourier transform,Zernike moment,Particle swarm optimization,Support vector regression,

@&#ABSTRACT@&#
This paper presents a novel rotation-invariant texture image retrieval using particle swarm optimization (PSO) and support vector regression (SVR), which is called the RTIRPS method. It respectively employs log-polar mapping (LPM) combined with fast Fourier transformation (FFT), Gabor filter, and Zernike moment to extract three kinds of rotation-invariant features from gray-level images. Subsequently, the PSO algorithm is utilized to optimize the RTIRPS method. Experimental results demonstrate that the RTIRPS method can achieve satisfying results and outperform the existing well-known rotation-invariant image retrieval methods under considerations here. Also, in order to reduce calculation complexity for image feature matching, the RTIRPS method employs the SVR to construct an efficient scheme for the image retrieval.

@&#INTRODUCTION@&#
With the advancement in information technology, people can readily take pictures using digital cameras, camcorders, web camera videos and smart mobile phones anytime and anywhere. This phenomenon makes image databases get an explosive growth. Thus, users need to employ efficient and effective tools to search for the images they want in the huge image database [7]. Current popular websites, such as Google and Yahoo, provide the function for image searching based on the text annotations of images. As a result, users often spend a lot of time and effort to edit these text annotations for each image. However, in many cases, text annotations cannot clearly describe the image contents. Moreover, a query request is difficult to be precisely described by text annotations. Therefore, the content-based image retrieval (CBIR) technique has been proposed to overcome these limitations mentioned above. The index in every image in the CBIR is popularly formed by the composition of image's own visual contents [16]. Subsequently, researchers improve the CBIR with rotation-invariant. At present, rotation-invariant texture image retrieval becomes important issue in image retrieval.The texture descriptor of images plays an important role in computer vision and image pattern recognition, especially in representing the contents of an image [4,18]. Several image retrieval systems, which extract the rotation-invariant texture features of images, have been developed recently [5,7,8,11,19]. The feature extractions of several proposed methods have been devised in frequency domain for extracting rotation-invariant features [5,7,11,16]. In Ref. [7], Kokare combines a dual-tree rotated complex wavelet filter (DT-RCWF) and a dual-tree complex wavelet transform (DT-CWT) to obtain the texture features for rotation-invariant from 12 different angles. However, the similarity measurement formula is not optimized. Thus, the method cannot get better discrimination between two different images [7]. In [16], Tzagkarakis presents the kullback-leibler-divergence (KLD) method which employs the Gaussianized steerable pyramids to extract the texture features of images. Nevertheless, it is insufficient to search for the optimal number of outputted images. In Ref. [5], the rotation-invariant Gabor (RIG) method is proposed which combines the Gabor filters with same scales and different angles to extract the rotation-invariant texture features of images [5]. In Ref. [11], Rallabandi presents wavelet-based hidden Markov trees (WBHMT) which combines the wavelet transformation and the hidden Markov tree to extract the rotation-invariant texture features of images. Unfortunately, the feature extraction algorithm of the method requires high computational complexity. In Ref. [12], Sim presents the modified Zernike moments (MZM) which combine the discrete Fourier transformation and the Zernike moments to construct the rotation-invariant texture features of images. Nevertheless, the similarity measurement formula is not optimized. Hence, the method cannot get nearly optimal solutions in the number of outputted images, various feature weights, etc.Aforementioned phenomena motivate us to develop a novel rotation-invariant texture image retrieval method, called the RTIRPS method which can overcome the drawbacks of the above methods [5,7,11,12,16]. The RTIRPS method employs the PSO algorithm which searches for a set of nearly optimal thresholds and enlarging constants in order to improve the retrieval performance. Fig. 1depicts the conceptual design for the RTIRPS method. Note that the RTIRPS method has following properties: (1) simultaneously employing three rotation-invariant features to design dis-similarity measurement algorithm, (2) optimizing the dis-similarity measurement algorithm by using the PSO algorithm, (3) searching for an optimal number of outputted images, (4) reducing the computational complexity of feature matching by employing the SVR.The remainder of this paper is arranged as follows. Section 2 introduces the background knowledge including the PSO algorithm, the SVR, the LPM, the Gabor filter, and the Zernike moment. Subsequently, the RTIRPS method is described in Section 3. Section 4 presents the experimental results. Finally, conclusions are given in Section 5.The PSO algorithm was proposed by Kennedy in 1995. It is employed to solve the optimization problems in many applications, such as, image processing and vehicle routing problem [3,10]. In the PSO algorithm, birds decide the next direction and the distance of movement by referencing the past directions of movement and the current location when they search for foods. A particle in the algorithm is simulated as a biological individual in a hyper-dimensional search space to find out nearly optimal solution of more complex problems [2,6]. More specifically, the training procedure for the PSO algorithm is briefly described as follows:Step 1.Initialize the beginning parameters: swarm populations, weights, ranges of coordinates of locations for particles, and the number of training iterations. Also, the particles are randomly located and the movement vector is randomly assigned.Store Gbest and all Pbest locations at the current iteration by using an evaluation process employing the fitness function for all particles.If the number of training iterations is terminated or the accuracy is satisfied, then output Gbest and Pbest locations, and the algorithm terminates. Otherwise, go to Step 4.Calculate the movement vectors in Eq. (1) for all particles.Modify the locations of all particles utilizing Eq. (2) and then go to Step 2.The movement vector (location) is specified as follows:(1)Vi(t+1)=wVi(t)+c1×r1×(Pbesti−Xi(t))+c2×r2×(Gbest−Xi(t)),where Vi=(Vi1, Vi2, ⋯, Vim)∈ℜm, Vi(t+1) represents the movement vector of particle i at the (t+1)th iteration, w indicates the inertia weight, c1 and c2 denote the acceleration coefficients which are random numbers in [0,1], r1 and r2 are also two randomly generated values in [0,1]. Moreover, in Eq. (1), the first term wVi(t) denotes the particle's inertia, the second term c1×r1×(Pbesti−Xi(t)) indicates the particle's cognition-only model, and the third term c2×r2×(Gbest−Xi(t)) stands for the particle's social-only model. The location of particle i is modified by Eq. (2).(2)Xi(t+1)=Xi(t)+Vi(t+1)where Xi(t+1) represents the location of particle i at the (t+1)th iteration, Vi(t+1) denotes the movement vector of particle i at the (t+1)th iteration. Hence, the new location of particle i is to add its current location vector to its movement vector.The SVR is a learning algorithm which is based on the statistical learning theory and the structural risk minimization principle [14]. Because the SVR possesses the feature of the global optimal solution and considers the structured risk, it has famously been utilized in the data mining and the pattern recognition [14].Suppose that a set of training samples be represented as{(xk,yk)}k=1N,xk=(xk1,xk2,…,xkn)t∈ℜn, yk∈ℜ, where xkand ykrespectively indicate the input vector and its corresponding desired output of the kth training sample and t stands for the transpose operation, ℜ expresses the set of real numbers. The linear regression function, f, is described in Eq. (3).(3)f(x)=(wtx+b)wherew=(w1,w2,…,wn)t∈ℜnand denotes the weight vectors, and b∈ℜ indicates the bias quantity, respectively [14]. Here, the linear SVR can be optimized and the optimized problem can be rewritten in Eq. (4).(4)Minimize12wtwSubjecttoyk−wtxk−b≤εwtxk+b−yk≤ε,k=1,2,…,Nwhere ɛ is nonnegative which denotes the maximum error between the tolerant prediction output and the desired output. Because errors and noises may exist in the sample data in the most applications, the extra items have to be included in the application and some sample data are allowed to stay outside of ɛ. Hence, Eq. (4) can be rewritten as Eq. (5).(5)Minimize12wtw+C∑k=1N(ξi+ξk*)Subjecttoyk-wtxk−b≤ε+ξkwtxk+b−yk≤ε+ξk*,k=1,2,…,Nwhere the slack variables,ξkandξk*, represent nonnegative numbers. Also, the regularization parameter C is positive. Here, Eq. (5) can be converted into the Wolfe dual problem, shown in Eq. (6).(6)MaximizeL(α,α*)=−ε∑k=1N(αk+αk*)+∑k=1Nyk(αk−αk*)−12∑k=1N∑l=1N(αk−αk*)(αl−αl*)(xktxl)Subjectto∑k=1N(αk−αk*)=0,αk,αk*∈[0,C]where α=(α1, …, αN)t,α*=(α1*,…,αN*)t, α1, …, αNandα1*,…,αN*represent the nonnegative Lagrange multipliers. In Eq. (6), suppose that the optimal solutionsα¯andα¯*be obtained. Subsequently, the optimal weight vectorw¯can be calculated. Through the KKT theory, whenα¯,α¯*andw¯are substituted into Eq. (6), the optimal biasb¯and the linear regression function, f, can be obtained in Eq. (7).(7)f(x)=(w¯tx+b¯)=∑k=1N(αk¯−αk*¯)(xktx)+b¯A nonlinear SVR can be decided by solving the following problem defined in Eq. (8).(8)MaximizeL(α,α*)=−ε∑k=1N(αk+αk*)+∑k=1Nyk(αk−αk*)−12∑k=1N∑l=1N(αk−αk*)(αl−αl*)K(xk,xl)Subjectto∑k=1N(αk−αk*)=0,αk,αk*∈[0,C]where K(·) denotes the Kernel function. Therefore, the nonlinear regression function, f, can be regarded as the nonlinear SVR which is specified in Eq. (9)(9)f(x)=(w¯tx+b¯)=∑k=1N(αk¯−αk*¯)K(xk,x)+b¯Here, the radial basis function (RBF) kernel is considered as the Kernel function K which is specified in Eq. (10).(10)K(xk,x)=exp−γx−xk2where γ indicates a parameter for Kernel function K and⋅represents the L2 norm [14].The LPM can map an original coordinate plane into a polar coordinate plane, shown in Eq. (11)[15].(11)r=(x−xc)2+(y−yc)2θ=tan−1y−ycx−xcwhere x and y indicate the position of a point in the original coordinate plane, xcand ycdenote the position of origin of this coordinate plane, r represents the distance for a point from the origin in the polar coordinate plane, θ stands for the angle in the polar coordinate plane.Fig. 2illustrates that the LPM can convert rotation distortion into translation distortion [15]. Moreover, the FFT is less sensitive to translation and scaling distortions. Thus, in many studies, the integration of the LPM and the FFT is often utilized to extract the rotation-invariant features in frequency domain [15]. More specifically, Fig. 2 depicts the results of an example while applying the combination of the LPM and the FFT for an image. Fig. 2(a) shows the original image. Fig. 2(b), (c), and (d) respectively rotate clockwise 90, 180, and 270 degrees for Fig. 2(a). Fig. 2(e)–(h) respectively represent the LPM versions for Fig. 2(a)–(d). Also, Fig. 2(i)–(l) respectively indicate the FFT versions of Fig. 2(e)–(h). It is significantly found that the output images, Figs. 2(i)–(l), are very similar after the input image, Fig. 2(a), is rotated and the rotated images are transformed by the LPM and the FFT sequentially.The Gabor filter is represented by a two-dimensional Gabor function g(x,y) and its Fourier transformation G(u,v) in Eqs. (12) and (13), respectively [5]:(12)g(x,y)=12πσxσyexp−12(x2σx2+y2σy2)+2πjWx(13)G(u,v)=exp−12(u−W)2σu2+v2σv2whereσu2=12πσx,σv2=12πσy, σxand σyrepresent the spatial length and the frequency width in the Gabor filter, respectively, (W,0) denotes the frequency center that the Gabor filter is located on (u, v).The method of Han presents a rotation-invariant Gabor filter [5]. Some aggregations are performed to produce a set of new Gabor filters with rotation-invariant. Using the aggregations for KGfilters with same scales and different angles, the family of rotation-invariant Gabor-filter-based can be obtained, shown in Eq. (14).(14)gmG(R)(x,y)=∑n=0KG−1gmG,nG(x,y),mG=0,1,…,SG−1.For the input image I(x, y), the corresponding Gabor filter imageJm(R)(x,y)is represented by Eq. (15).(15)Jm(R)(x,y)=∑x1=1,2,…,SL∑y1=1,2,…,SWI(x1,y1)gmG(R)(x−x1,y−y1),mG=0,1,…,SG−1.The mean value and the standard deviation, respectively in Eqs. (16) and (17), are employed to construct the feature vector, shown in Eq. (18).(16)umG(R)=1SL×SW∑x=1,2,…,SL∑y=1,2,…,SWJmG(R)(x,y),(17)σmG(R)=1SL×SW∑x=1,2,…,SL∑y=1,2,…,SWJmG(R)(x,y)−umG2,(18)fGb(R)=u0(R),σ0(R),u1(R),σ1(R),…,uSG−1(R),σSG−1(R).When a texture image is inputted into the Gabor filter, the output results are shown in Fig. 3, where the scale parameter SGis set to four. Fig. 3(a) depicts an original image. Fig. 3(b)–(d) exhibit the clockwise rotation versions of Fig. 3(a) with 90, 180, and 270 degrees, respectively. Figs. 3(a1)–3(a4) display four scale parameters for SG=0, 1, 2, 3, respectively. In each column of Table 1, these four images are obtained using four Gabor filters with the same rotation degree. For example, Fig. 3(a1)–(a4) are obtained using zero degree. More specifically, each image of Fig. (a)–(d) has four output images and obtains its two rotation-invariants,μmG(R)andσmG(R). Table 1 presents the feature extraction using the Gabor filter in four rotation degrees and four scales, where rotation-invariantμmG(R)andσmG(R)are respectively a kind of the features of images. Moreover,μmG(R)andσmG(R)are respectively the mean and the standard deviation. Observing Table 1, an important finding is that same texture images still have similarμmG(R)andσmG(R)after rotating.The method of Teague employs the Zernike moment to perform the image analysis [13]. Here, a series of complicated polynomials is constructed in Eq. (19).(19)VnZkmZk(x,y)=VnZkmZk(ρZk,θ)=RnZkmZk(ρZk)exp(jmθ)where nZkrepresents a vector, mZkdenotes a real number or negative number, nZk−|mZk| indicates the restriction formula, that is |mZk|≤nZk, ρZkshows the vector length, from the origin of coordinate to the primitive (x, y), θ means the angle between the vector ρZkand the counterclockwise x-axis direction, andRnZkmZk(ρZk)is the radiation polynomial of Zernike moment, which is shown in Eq. (20) and satisfies Eq. (21).(20)RnZkmZk(ρZk)=∑s=0(nZk−mZk)/2(−1)snZk−s!ρZkn−2ss!nZk+|mZk|2−s!(nZk−|mZk|2−s)!(21)∫∫x2+y2≤1VnZkmZk(x,y)Vpq(x,y)dxdy=πnZk+1δnpδmp,δ=1,ifa=b,0,otherwise.Zernike repeats the order of nZkfor mZktimes to give a continuous image I(x, y). He calculates the number of features employing Eq. (22). For digital images, combining Eqs. (21) and (22) can obtain Eq. (23).(22)ZnZkmZk=nZk+1π∫∫x2+y2≤1I(x,y)[VnZkmZk(ρZk,θ)]dxdy.(23)ZnZkmZk=nZk+1π∑x∑yI(x,y)VnZkmZk(ρZk,θ),x2+y2≤1Fig. 4shows that a texture image rotates through different angles and performs the Zernike moment. Zernike ignores the partial regions around the images and only processes the regions inside red frames. Fig. 4(a) represents the original image. Figs. 4(b)–(d) respectively rotate clockwise 90, 180, and 270 degrees for Fig. 4(a). Table 2depicts the example of the Zernike feature values for the images in Fig. 4. Observing these feature values, it can be found that Zernike features possess the ability of image rotation-invariant because row 1 is almost the same as rows 2, 3, and 4, respectively.Fig. 5shows the block diagram for the RTIRPS method with rotation-invariant. First, for all images in the database, three kinds of feature extraction methods with rotation-invariant are employed to obtain image features which are stored in the feature database. Subsequently, when a query image is inputted, its feature extraction process is the same as that for the images in the database. Finally, the features of the query image are compared with the features of images in the feature database by using the dis-similarity measurement. Also, the PSO algorithm is utilized to find out a set of nearly optimal feature weights and various thresholds to enhance the retrieval performance of the RTIRPS method.Three kinds of image rotation-invariant feature extraction methods are utilized, including the LPM combined with the FFT, the Gabor filter, and the Zernike moment. For convenience, they are shortly called the LF, the Gb, and the Zk, respectively. Let the image database consist of N images, shown in Table 3. Each image has three image features, the LF, the Gb, and the Zk features, which are extracted by the above three kinds of extraction methods. Here, the numbers of these three features are not completely the same. MLF, MGb, and MZkrespectively indicate the number for the LF features, the number of the Gb features, and the number of the Zk features. Observing the empirical data, the performance of the RTIRPS method is better when MLF, MGb, and MZkare set to 3, 8, 20, respectively. Let fiand fqdenote the feature vectors of the Iith image and the query image Iq, respectively. fiand fqare specified in Eqs. (24) and (25).(24)fi=fjkij∈LF,Gb,Zk,k=1,2,…,Mj,i=1,2,…,N,(25)fq=fjkqj∈LF,Gb,Zk,k=1,2,…,Mj,wherefjkiandfjkqrespectively denote the kth feature value in the jth category of features for the ith image in the feature database and the query image. In addition, the total number for the features of query image Iqisfq=∑j∈LF,Gb,ZkMj.The component, Difference computing, in Fig. 5 is utilized to compute the dis-similarity degree between the features of query image Iqand each image Iiin the database. The function, d(Iq, Ii), in Eq. (26) is employed to perform the dis-similarity measurement in Difference computing component [9]. Employing Eq. (26), the RTIRPS method calculates the dis-similarity degree dqibetween Iqand Iiin the database. Finally, the RTIRPS method outputs images whose dis-similarity degrees are less than the threshold T.(26)dqi=d(Iq,Ii)=∑j∈LF,Gb,Zk∑k=1,2,…,Mjwjkfjkq−fjkiαjkwhere i=1,2,…,N,wjkindicates the weight for the kth feature in the jth class of features for all images, αjkdenotes the standard deviation for the kth feature in the jth class of features for all images in the database, which is employed in the feature normalization, and |·| represents L2 norm.In the design of image retrieval method, it is very difficult to decide an optimized number, NSim, of outputted images. If NSimis too small, this phenomenon is not easy to retrieve all similar images correctly. Fig. 6(a) depicts an example for above phenomenon. It displays nine images and the ordering of their dis-similarity degrees. Let the query image be symbolized with a red circle. If NSimis set to 2, then a similar image symbolized with red circle is missed to be retrieved. Thus, too small NSimvalue makes an incorrect retrieval result, although the retrieval result completely contains similar images, as shown in Fig. 6(b). Accordingly, too large NSimvalue, 8, as displayed in Fig. 6(c), causes that the RTIRPS method outputs various dissimilar images, although all similar images in the database are retrieved out. Therefore, the RTIRPS method employs the PSO algorithm to select an optimized threshold T0. Fig. 7exhibits an example for this optimized threshold. While using the threshold T0 to get the retrieval result shown in Fig. 7(a), the RTIRPS method can obtain a better retrieval performance than that of the phenomenon shown in Fig. 6.Most image retrieval methods do not take into account the notable differences of two feature vectors for individual feature. The individual threshold T0mentioned above still has shortcoming in image retrieval for all features of all image classes in the database. Thus, the RTIRPS method proposes a novel procedure of difference calculation. Suppose that there be four images, that is, N=4. Each image has five features, that is, M=5. Here, Table 4presents the feature database formed by four images, I1, I2, I3, and I4. Fig. 8(a)–(d) display these four images, respectively. Visibly, I1 is similar to I2, and I3 is similar to I4. Fig. 9(a) and (b) respectively depict two query images Iq1 and Iq2. Moreover, Table 5exhibits the features for Iq1 and Iq2. Apparently, the similar images for Iq1 are images I1 and I2, those for Iq2 are images I3 and I4, respectively.If the query image Iq1 is inputted for image retrieval, the dis-similarity degrees between Iq1 and I1, I2, I3, and I4, respectively, can be calculated by Eq. (26). These dis-similarity degrees are sorted by an ascending order, depicted in Table 6. Similarly, if the image Iq2 is inputted for image retrieval, the dis-similarity degrees between Iq2 and I1, I2, I3, and I4, respectively, can be also obtained. These dis-similarity degrees are also sorted by an ascending order, shown in Table 7. If the threshold T is set to 1, the similar images can be completely and correctly determined for Iq1. For the case of the query image Iq2, the retrieval result cannot be completely and correctly obtained, shown in Fig. 10.Let the threshold T=1 in Fig. 10(a). When the query image is Iq1, the similar images can be completely decided and images I1 and I2 can be correctly retrieved out. However, in Fig. 10(b), when the query image is Iq2, the similar image(s) cannot be correctly determined. That is, image I3 cannot be correctly found out. In order to resolve the shortcoming mentioned above, the RTIRPS method proposes a novel procedure of calculating the difference of two feature vectors. The procedure searches for a set,Tnew, of thresholds for each feature, which is specified in Eq. (27).(27)Tnew=Tjkj∈LF,Gb,Zk,k=1,2,…,Mjwhere Tjkindicates the threshold for the kth feature value of the jth feature class. Fig. 11(a) shows that the similar images can be accurately found out for Iq1. Also, Fig. 11(b) demonstrates that the similar images can be correctly retrieved out for Iq2.The novel procedure first exploits the setTnewin the difference calculation of two feature vectors to produce differences, and then these differences are employed in the dis-similarity degree computations via Eq. (26). More specifically, the procedure can be formulated as Eq. (28). That is,fjkq−fjkiin Eq. (26) is replaced by the function diff defined by Eq. (28).(28)fdjkq,i=diff(fjkq,fjki)=fjkq−fjki,iffdjkq,i>TjkL0,else.where L0 is a large constant. Moreover,fdjkq,idenotes the feature difference in terms of the dis-similarity degree between two features,fjkqandfjkifor Iqand Iiwhile utilizing Tjk.A set of parameters for charactering the RTIRPS method can be represented by Eq. (29).(29)ρ=(wjk,Tjk,αjk)j∈LF,Gb,Zk,k=1,2,...,Mj∪{NSim}wherewjkand αjkare used in Eq. (26), and Tjkis employed in Eq. (27). Moreover, NSimrepresents the number of retrieval images. Fig. 5 demonstrates the RTIRPS method is optimized using the PSO algorithm to search for a nearly optimal parameter set, ρ0. Namely, ρ0 is utilized to characterize the RTIRPS method.The SVR is employed to reduce the computational complexity for image retrieval. Suppose that there be totally N images in the image database. There are Γ classes of images. Each class averagely has Ncimages. In this case, the time complexity without SVR is θ(N). Note that it is reduced to θ(Γ)+θ(Nc) when the SVR is employed. For instance, Fig. 12(a) shows the required computational number without the SVR is 12. Visibly, Fig. 12(b) demonstrates the computational number is reduced to eight when the SVR is utilized.In recent years, the SVR is popular and powerful to construct a regression model. In the design of the RTIRPS method, several SVRs are constructed to form a classification model so as to reduce the computational complexity for image-similarity calculation. Let nCrepresent the number of classes in the database. Each class of the images in the database is utilized to build its corresponding SVR. Let Pjdenote the set of the training patterns for the jth class of the image database. While building SVRj, Pj={(fj,r, j)} in Eq. (24), where fj,rindicates the feature vector of the rth image in the jth class of the image database. The following algorithm is to construct the SVR models for nCclasses.Input: a set of feature vectors fj,rOutput: a set of the SVR modelsStep 1For j=1 to nCfor the jth class, construct its corresponding SVRjusing Eq. (24)end-forStep 2Output{SVRjj=1,2,…,nC}.The following algorithm performs an image classification while using the trained SVR models for nCclasses.Input: a set of feature vectors fqfor a query image.Output: which class a query image belongs to.Step 1For j=1 to nCcalculate the value βjwhile feeding SVRjwith fq.end-forStep 2j0=argmin{βj}Step 3Decide that fqbelongs to the j0th class.

@&#CONCLUSIONS@&#
This paper has proposed the RTIRPS method which can be employed to retrieve rotation-invariant gray-level texture images. First, it respectively utilizes the LPM combined with the FFT, the Gabor filter, and the Zernike moment to extract three kinds of rotation-invariant features for the gray-level texture images. These three feature classes are subtly integrated to form a combined feature vector. The main contribution of this paper is to develop a novel procedure which calculates feature difference for each individual feature component. Subsequently, the PSO algorithm is utilized to optimize the RTIRPS method via finding out a nearly optimal parameter set, ρ0, which is employed to character the RTIRPS method. It is utilized to optimize the RTIRPS method for these parameters while using the measurement procedure of dis-similarity degrees for two different images. Finally, experimental results demonstrate that the RTIRPS method outperforms other existing well-known rotation-invariant image retrieval methods under considerations here. Furthermore, in order to reduce computational complexity for image feature matching, the RTIRPS method also utilizes the SVR to construct an efficient scheme for the image retrieval.