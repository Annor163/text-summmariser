@&#MAIN-TITLE@&#
An improved electromagnetism-like mechanism algorithm and its application to the prediction of diabetes mellitus

@&#HIGHLIGHTS@&#
The utilization of ROST technique on EM algorithm with 1NN shows its superiority.Statistical analysis reveals that our method outperforms all compared methods.We have identified four risk factors for this disease.Our research can help the diagnosis and prognosis of Type 2 DM.The findings can help to reduce the morbidity and mortality rate caused by DM.

@&#KEYPHRASES@&#
Electromagnetism-like mechanism algorithm,Nearest-neighbor heuristic,Opposite sign test,Feature selection,Diabetes mellitus,

@&#ABSTRACT@&#
Recently, the use of artificial intelligence based data mining techniques for massive medical data classification and diagnosis has gained its popularity, whereas the effectiveness and efficiency by feature selection is worthy to further investigate. In this paper, we presents a novel method for feature selection with the use of opposite sign test (OST) as a local search for the electromagnetism-like mechanism (EM) algorithm, denoted as improved electromagnetism-like mechanism (IEM) algorithm. Nearest neighbor algorithm is served as a classifier for the wrapper method. The proposed IEM algorithm is compared with nine popular feature selection and classification methods. Forty-six datasets from the UCI repository and eight gene expression microarray datasets are collected for comprehensive evaluation. Non-parametric statistical tests are conducted to justify the performance of the methods in terms of classification accuracy and Kappa index. The results confirm that the proposed IEM method is superior to the common state-of-art methods. Furthermore, we apply IEM to predict the occurrence of Type 2 diabetes mellitus (DM) after a gestational DM. Our research helps identify the risk factors for this disease; accordingly accurate diagnosis and prognosis can be achieved to reduce the morbidity and mortality rate caused by DM.

@&#INTRODUCTION@&#
The overwhelming amount of data makes most of the existing data mining algorithms inapplicable and inefficient to many real world problems, particularly those data with high dimensionality and features [14]. Feature selection depends on the determination of an optimal feature subset among all of the full features, and is constantly studied to improve computational efficiency and effectiveness by optimizing criteria, such as accuracy and total feature subset costs. However, this possesses another problem called exponential complexity [43] due to the exponential number of candidate subsets. Researchers addressed this problem through exhaustive search [31,1]. Yet, exhaustive search is computationally impractical, except if the total number of features is quite small.The use of metaheuristic algorithms as a solution to feature selection problems has gained significant attention lately. Among them, EM is a new popular optimization algorithm that has been known for its efficiency. EM is a heuristic approach proposed by Birbil and Fang [5] that mimics the attraction–repulsion mechanism of electrically charged particles in the magnetic field. EM is capable of optimizing continuous and discreet problems.Chen et al. [9] first introduced opposite sign test (OST). It is a new local search technique that have the capability to handle the local optima trap problem. In their study, they demonstrated the abilities of OST to increase population diversity in the Particle Swarm Optimization (PSO) algorithm and to escape from local optimal trapped by refining the jump ability of flying particles. OST is implemented by carrying out simple test on the particular states of particle. The objective is to obtain a better solution for each particle. There are two kinds of OST: forward opposite sign test (FOST) and random opposite sign test (ROST). The FOST algorithm works by changing the particular states of particles in a forward manner, while the ROST algorithm randomly changes the particular states of particles.In this study we combine EM algorithm with nearest neighbor classifier (1NN) as the wrapper method. OST method is utilized as local search procedure to help EM algorithm explore for its best.This novel feature selection approach (denoted as IEM) can achieve high classification accuracy and Kappa index. A thorough investigation is conducted to evaluate the performance of the proposed IEM. Initially, we apply IEM on 54 public datasets that consist of 46 datasets from the UCI Repository [2] and 8 public microarray dataset. We conducted this preliminary test to justify our method can be applied to any kind of data size and dimensions. Subsequently, the results are compared with nine other popular feature selection techniques using a non-parametric statistical test.Diabetes mellitus (DM) is one of the disease that contributes to the high mortality rate in the world and also in Taiwan [13,35]. Gestational diabetes is one type of DM that occurs during or after pregnancy and are recognized as one of the most common pregnancy’s complication. Therefore, this study aims at applying IEM on a real clinical dataset to predict the occurrence of Type 2 DM from a gestational diabetes mellitus (GDM) that occurs during or after pregnancy in Taiwan.The remainder of the paper is organized as follows: In Section 2, we review the related literature. Section 3 presents the proposed IEM algorithm. The experimental results and discussion are presented in Section 4. Section 5 concludes the paper.Selecting an optimized subset from original features is the major task in the feature selection problem. In feature selection one needs to remove irrelevant or redundant features from our dataset. This process influences on the performance of the classification algorithm, particularly in improving accuracy and efficiency [37]. For a specific a dataset, feature selection helps analysts in understanding which features are important and how they are related [36]. As the dimensionality of a domain increases, the number of features N also increases. Finding an optimal feature subset is usually is not an easy task [21]. Many problems related to feature selection have been shown to be NP-hard [6]. This phenomenon is also known as the curse of dimensionality or the Hughes phenomenon.Several researchers have utilized feature selection based methods to predict the occurrence of a disease. Ban et al. [3] used SVM to predict the Type 2 DM in Korean population. Chandra and Gupta [8] used a features weight with Naïve Bayes and SVM classifier to identify the most relevant genes that associate with diseases like leukemia, colon tumor, lung cancer, diffuse large B-cell lymphoma (DLBCL), and prostate cancer. Peng et al. [25] integrated filter and wrapper methods into a sequential search procedure to improve the classification performance of the selected features by using breast cancer and heart disease dataset from UCI repository. Cho et al. [10] proposed a linear SVM combined with wrapper or embedded feature selection methods and applied it to predict the onset of diabetic nephropathy. Huang et al. [18] proposed a feature selection method via supervised model construction for features ranking to predict the occurrence of Type 2 diabetes.The EM algorithm was first introduced as a stochastic global optimization technique on Birbil’s PhD dissertation in 2002. Since then it has gained popularity due to its simplicity and efficiency in solving many engineering problems.There are four main phases exist on the EM algorithm: initialization, local search, force calculation, and particle movement. In the initialization phase, m particle points are randomly selected from the feasible region, which is an n dimensional hyper-solid features. Each feature of a point is assumed to be scattered between the corresponding upper bound and lower bound. The next step is local search. Hence the OST technique was used as the local search for EM algorithm. In the third phase, the total force exerted on each point is calculated based on the principle of electromagnetism theory. Each particle will calculate the charged value by using Eq. (1). For instance, F1 is the force exerted by q3 on q1 (Fig. 1); that is, q1 is repulsed by q3 if the value of the objective function of q1 is better than that of q3. F2 is the force exerted by q2 on q1; that is, q1 is attracted by q2 if the objective function value of q1 is worse than that of q2. Thus we can count the eventual force exerted on q1 as F1=F21+F31. Each point is viewed as a magnetic particle and contains a force of attraction or repulsion with respect to other particles. If the objective function value of one point is better than that of the other points, then it is attracted to those points.On the other hand, if the objective function value of one point is worse than that of the other particles, it is repulsed by them. The total force exerted on every point is calculated after all of the forces of attraction and repulsion are determined. The total force is calculated using Eq.(2):(1)qi=exp-n×f(xi)-f(xbest)∑k=1m(f(xk)-f(xbest)),∀i(2)Fi=∑j≠imqiqjxj-xi|xj-xi|2iff(xj)<f(xi)qiqjxi-xj|xj-xi|2iff(xj)⩾f(xi),∀iThe last phase implicates the movement according to the direction of the force. The total force of each point is calculated point, then this point moves at random step length in the direction of the force and causes the particles to move into any unvisited area in the search space. These procedure is repeated until a termination criterion is reached. The termination criterion can be set as the maximum number of iterations [5].Despite the popularity of EM algorithm in solving engineering optimization problems, the utilization of EM algorithm in data mining is still few. Su and Lin [34] are the first to apply EM for feature selection. They use the EM algorithm to solve binary problems. In their proposal, EM is combined with the nearest-neighbor heuristics as the wrapper method. Experimental results on 13 public datasets indicate that their method outperforms other well-known algorithms in classification accuracy and feature selection efficiency for balanced data. Lin and Su [22] propose an EM algorithm combined with robust Bayes classifier, and apply it for feature selection with incomplete data. They conduct experiments on 11 public datasets, and show that the results from their proposed method are better than those from other methods, including GA. In our study, the proposed IEM algorithm is embedded with the OST techniques to help the algorithm explore for the global optima solution and escape from local optima trapped.The nearest neighbor algorithm (specifically, 1NN) [12] is one of the successful techniques used in classification task in data mining area [32] and has been widely applied to solve various classification problems. It becomes a popular classifier due to the simplicity and high convergence speed. The 1NN using Euclidean distance to calculate the difference between attributes for continuous data [19]. The algorithm classifies the object by simply assigning it to the class of the single nearest neighbor. Despite its benefit, 1NN can cause the effects of curse of dimensionality which is one of the major issues in biomedical data analysis and mining [25]. In order to avoid that effect, 1NN is usually combined with some optimization algorithm, such as genetic algorithm (GA), PSO and other kind of search methods to improve classification performances. In this paper, we propose the EM algorithm to combine with 1NN as the classifier.The OST technique can help avoid local optima by improving the jump ability of flying particles. This technique has been utilized to exploit the region for a probable global optimization. Some recent literatures indicate the promising results of using OST techniques [38]. The OST technique entails the testing of the current state and the subsequent change of a feature (0→1 or 1→0). For particle 10100, for example, the first bit 1 was changed to 0 in the first state. Subsequently, the second bit 0 was changed to 1 in the second stage. This process continued until all bits are changed sequentially.The OST procedure is described as follows:Step 1: Set d: 1.Step 2: SetXinew:Xi.Step 3:xid:1thenxidnew:0. Ifxid:0thenxidnew:1.Step 4: If the fitness value ofXi<Xinewthenxid:xidnewandXi:fitnessvalueofXinew.Step 5: d: d+1.Step 6: If d<D, proceed to step 2; otherwise, stop.There are two OST techniques namely, FOST and ROST. The FOST algorithm is a simple test that aims to acquire better optimization for each particle. The algorithm sequentially changes the particular states of particle in a forward manner. In particle 10,100, for example, the first bit changed from 1 to 0 in the first step. If the fitness value of the old particle (Xi) is less than that of the new particle (Xinew), we accept the alteration of the new particle. Otherwise, we retain the alteration of the old particle. The second bit was then changed from 0 to 1 in the second step, and its fitness value is tested. This process continues until all bits were tested sequentially. Meanwhile, the ROST algorithm randomly changes particular particle states. Fig. 2shows the illustration of ROST technique. In particle 10,100, as an example, we first generate a random key such as ‘35,421’. Following the random key order, the third bit changes first from 1 to 0. If the fitness value of the old particle (Xi) is less than that of the new particle (Xinew), we accept the alteration of the new particle. If the results illustrate otherwise, we accept the alteration of the old particle. Subsequently, the fifth bit changed from 0 to 1. We test for the fitness value of the fifth bit. We continues this process until all bits are changed.By performing the OST technique, our proposed algorithm is able to find better particle before the particle movement. The aim of using it is to promote the correct direction. The OST technique has increased the population diversity in metaheuristics such as the PSO [9]. It is helpful for the EM algorithm to avoid local optima traps as well.Chen et al. [9] utilized both FOST and ROST in their proposed approach and exposed that ROST outperforms FOST. Therefore, this study will utilize the ROST technique to embed in our proposed method.To justify the performance of our proposed IEM method, we compared it with different types of feature selection and classification methods. There are nine popular methods that are used as comparisons, i.e. improved genetic algorithm (IGA), standard version of EM with 1NN classifier, support vector machine (SVM), back propagation neural network (BPNN), logistic regression (LR), C4.5, radial basis function (RBF), self-organizing map (SOM), and learning vector quantitation (LVQ).The IGA is an improved version of genetic algorithm that utilized ROST techniques. The reason to make such comparison is to see the performance difference of GA and EM algorithms when the ROST is adopted. The comparison to a standard EM algorithm is to investigate the effect of ROST on a naïve EM and the improved version. The SVM algorithm is a powerful machine learning tool using kernel. The SVM maps nonlinear inputs to a high-dimension feature space where a linear classification surface is constructed [11]. The BPNN is a common method of training artificial neural networks (ANNs). It was first introduced by Paul Werbos in the 1970s. The BPNN trains multi-layered neural networks such that it can learn the appropriate internal representations to allow it to learn any arbitrary mapping of input to output [28]. The LR is a type of probabilistic statistical classification model proposed in the 1970s as an alternative technique to overcome the limitations of ordinary least squares regression. It has been used extensively in numerous disciplines, including the medical and social science fields [30,16,41]. The C4.5 is a decision tree based classifier developed by Quinlan [27] and an extension of Quinlan’s earlier ID3 algorithm. The decision trees generated by C4.5 can be used for classification, therefore it is often referred to as a statistical classifier. The RBF appears as a variant of neural network in the late 1980s. The RBF is embedded in a two-layer neural network, where each hidden unit implements a radial activated function. The input into an RBF network is a nonlinear while the output is linear. Its capabilities has been studied [24,26]. The self-organizing feature map (SOFM) or known as self-organizing map (SOM) is a type of ANNs that is trained using unsupervised learning to produce a low-dimensional, discretized representation of the input space of the training samples, called a map. The SOM is different from other ANNs since it uses a neighborhood function to preserve the topological properties of the input space. The model was first described as an ANN by the Teuvo Kohonen, and is sometimes called a Kohonen map or network [20]. SOM may be considered a nonlinear generalization of principal components analysis (PCA). It has been shown, using both artificial and real geophysical data, that SOM has many advantages over the conventional feature extraction methods [29,42]. LVQ is a prototype-based supervised classification algorithm that considered as a special case of an ANN. LVQ applies a winner-take-all Hebbian learning-based approach. It is a precursor to self-organizing maps (SOM). The LVQ was developed by Teuvo Kohonen. It can be applied to multi-class classification problems and has been widely used in a various practical applications.We use binary digits for feature representation in applying IEM to the feature selection problem. The bits consist of 0 and 1 digit, corresponding to non-selected and selected features respectively. Each particle is coded as a binary alphabetical string. For instance, particle 10,100 contains five features where only the first and the third features are selected.Accuracy is the most commonly used parameter in classifier performance assessment [40]. In the present study, the classification accuracy is determined through 1NN classification rate. Accuracy is used to evaluate classifier performance by defining the total number of good classifications over the total number of available examples. The classified test points can be divided into the following four categories, which are usually represented in the well-known confusion matrix [33]: true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN). The column of the matrix denotes the instances in a predicted class, while the row denotes the instances in an actual class. Given the four categories of the confusion matrix, accuracy is defined as the fitness function in 1NN. We calculate the fitness function using Eq. (3):(3)Accuracy=TP+TNTP+TN+FP+FNFig. 3shows the pseudo-code of the proposed IEM algorithm in solving feature selection problem and Fig. 4shows the flowchart of IEM.

@&#CONCLUSIONS@&#
