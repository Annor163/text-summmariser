@&#MAIN-TITLE@&#
Integrating articulatory data in deep neural network-based acoustic modeling

@&#HIGHLIGHTS@&#
We test strategies to exploit articulatory data in DNN-HMM phone recognition.Autoencoder-transformed articulatory features produce the best results.Pre-training of phone classifier DNNs driven by acoustic-to-articulatory mapping.Utility of articulatory information in noisy conditions and in cross-speaker settings.

@&#KEYPHRASES@&#
DNN-HMM,Acoustic-to-articulatory mapping,Deep neural networks,Acoustic modeling,Electromagnetic articulography,Autoencoders,

@&#ABSTRACT@&#
Hybrid deep neural network–hidden Markov model (DNN-HMM) systems have become the state-of-the-art in automatic speech recognition. In this paper we experiment with DNN-HMM phone recognition systems that use measured articulatory information. Deep neural networks are both used to compute phone posterior probabilities and to perform acoustic-to-articulatory mapping (AAM). The AAM processes we propose are based on deep representations of the acoustic and the articulatory domains. Such representations allow to: (i) create different pre-training configurations of the DNNs that perform AAM; (ii) perform AAM on a transformed (through DNN autoencoders) articulatory feature (AF) space that captures strong statistical dependencies between articulators. Traditionally, neural networks that approximate the AAM are used to generate AFs that are appended to the observation vector of the speech recognition system. Here we also study a novel approach (AAM-based pretraining) where a DNN performing the AAM is instead used to pretrain the DNN that computes the phone posteriors. Evaluations on both the MOCHA-TIMIT msak0 and the mngu0 datasets show that: (i) the recovered AFs reduce phone error rate (PER) in both clean and noisy speech conditions, with a maximum 10.1% relative phone error reduction in clean speech conditions obtained when autoencoder-transformed AFs are used; (ii) AAM-based pretraining could be a viable strategy to exploit the available small articulatory datasets to improve acoustic models trained on large acoustic-only datasets.

@&#INTRODUCTION@&#
The steady increase of training data and computational resources combined with the use of new machine learning strategies for acoustic modeling has been continuously improving ASR performance in the last few years. Deep neural networks (DNNs) (Hinton et al., 2006), either combined with HMMs or used in a recurrent architecture, are the best strategy for acoustic modeling (Mohamed et al., 2012; Dahl et al., 2012; Graves et al., 2013).However, despite the impressive results shown by DNN-based ASR, there are several real usage scenarios where ASR technology still needs large improvements. In general, ASR accuracy significantly decreases in mismatched training-testing conditions, as it has been shown for traditional Gaussian mixture model (GMM)-HMMs systems in, e.g., speaking style mismatched conditions (Yu et al., 1999), and for DNN-HMM systems in, e.g., environment and microphone mismatched conditions (Seltzer et al., 2013).Other than simply increasing the number of training conditions we can explicitly address the speech modeling limitations responsible for the lack of generalization underlying the mismatched conditions problem. For example, context-dependent (CD)-DNN-HMMs, as well as GMM-HMMs, handle context effects (like, e.g., coarticulation effects) using hundreds/thousands of tied context dependent sub-phonetic states, i.e., senones (Dahl et al., 2012). The selection, either automatic or manual, of the number of senones (and, consequently, of learning parameters) may be affected by the number of conditions in the training dataset and, at the same time, by the invariance of the input feature set to those conditions (see, e.g., (Schaaf and Metze, 2010) where the portion of gender-dependent senones depends on the feature set used).The senones themselves result from the need to reduce learning parameters and are created by exploiting some speech production knowledge in the form of speech production-based questions in the state clustering tree. However ASR may benefit from a more explicit use of speech production knowledge where speech production can be used as, e.g., additional observations appended to the vector of acoustic observations, or as hidden structure connecting the phonological level (i.e., the HMM hidden phonetic states) to the observed speech acoustics.Such approaches are motivated by the fact that complex phenomena observed in speech, for which a simple purely acoustic description has still to be found, can be easily and compactly described in speech production-based representations (notably Browman and Goldstein, 1992; Jakobson et al., 1952; Chomsky and Halle, 1968). For example, in Articulatory Phonology (Browman and Goldstein, 1992) or in the distinctive features framework (Jakobson et al., 1952; Chomsky and Halle, 1968) coarticulation effects can be compactly modeled as temporal overlaps of few vocal tract gestures. The vocal tract gestures are regarded as invariant, i.e., context- and speaker-independent, production targets that contribute to the realization of a phonetic segment. Obviously the invariance of a vocal tract gesture partly depends on the degree of abstraction of the representation but speech production representations offer compact descriptions of complex phenomena and of phonetic targets that purely acoustic representations are not able to provide yet (see, e.g., Maddieson, 1997).Additional motivations to the use of speech production in ASR come from theories of speech perception such as the well known Motor Theory of speech perception (Liberman et al., 1967; Galantucci et al., 2006) which assumes that the perception of speech is the perception of motor gestures and involves access to the motor system. Such claims are partly supported by neurophysiological studies that show the contribution of the activity of the motor cortex to speech perception (DAusilio et al., 2009; Bartoli et al., 2013).In the last two decades several strategies have been proposed for an explicit use of speech production knowledge in ASR (see King et al., 2007, for an extensive review). Here we review studies where measured articulatory data are used for ASR. Such studies require simultaneous recordings of audio and articulatory data. Articulatory movements are recorded using techniques such as electro-magnetic articulography (EMA) (Wrench, 2000), X-rays (Westbury, 1994), ultrasounds (e.g., Grimaldi et al., 2008), and MRI (Narayanan et al., 2004).The approaches that use measured articulatory data can be roughly grouped into two categories. In the first category (e.g., Stephenson et al., 2000; Markov et al., 2006; Mitra et al., 2012) articulatory information is represented as discrete latent variables which are observed during training but hidden during testing. The idea behind this approach is to explicitly and compactly model speech production processes that are among the main causes of acoustic variability (e.g., variability due to coarticulation effects). In the second category (e.g., Zlokarnik, 1995; Wrench and Richmond, 2000), which the present work belongs to, articulatory features (AFs) are recovered from speech acoustics and then appended to the vector of observed acoustic features. In this case the working hypothesis is that the recovered articulatory domain (combined with the acoustic domain) represents a transformation of the acoustic domain into a new speech-production constrained domain which is more invariant over different conditions and where phonetic-articulatory targets can be more easily discriminated.We first review some of the studies belonging to the first category. In Stephenson et al. (2000) the articulatory information is represented by a single discrete articulatory variable within a dynamic Bayesian network (DBN). Its values are computed by clustering data points in a space defined by eight articulator sagittal positions (upper lip, lower lip, four tongue positions, lower front teeth, lower back teeth). The acoustic observation probability distribution is both conditioned on the phone state and on the articulatory variable which in turn depends on the phone state and the previous articulatory value.In Markov et al. (2006), not only the articulator position but also velocity and acceleration are taken into account and a latent discrete variable is used for each of them within a Bayesian Network that substitutes the traditional GMM to model the state-dependent observation probability distributions in HMM-based ASR. Contrary to Stephenson et al. (2000), the articulatory variables are not conditioned on their previous values. Both Stephenson et al. (2000) and Markov et al. (2006) show an increased phone recognition accuracy when latent articulatory variables are used.In the Gesture-based DBN (G-DBN) proposed by Mitra et al. (2012), articulatory features are derived from the Articulatory Phonology theory (Browman and Goldstein, 1992). The most interesting contribution of the paper is the use of articulatory features that attempt to explicitly describe the phonetic-articulatory targets. The G-DBN integrates articulatory information at two levels, which can be seen as a motor planning and a motor execution level. The motor planning is represented as six latent binary variables, where each variable encodes the activation state of an Articulatory Gesture (e.g., glottis constriction, tongue body constriction, lip aperture). The motor execution level is represented as observed tract variables (TVs) appended to the acoustic observation vector. The TVs define the kinematics of the vocal tract determined by the activations of the articulatory gestures. In realistic ASR settings the TVs, although represented as observed features, are not available during testing and need to be recovered from acoustics through an acoustic-to-articulatory mapping (AAM). Results on the Aurora-2 corpus (Pearce and Hirsch, 2000) showed that the G-DBN is more robust to noise than the acoustics-only DBN. A current limitation of the approach is that the ASR system can be trained on synthetic (acoustic and articulatory) speech, and consequently, the speech variability of the training data is quite limited.The first studies belonging to the second category, where measured articulatory data are only used as observations, are Zlokarnik (1995) and Wrench and Richmond (2000) where recovered AFs are appended in a GMM-HMM system. The two studies report conflicting results, AFs are of no utility in Wrench and Richmond (2000) whereas produce a large WER reduction in Zlokarnik (1995). The improvement in Zlokarnik (1995) may be due to the very large acoustic context (51 frames) used to reconstruct the AFs. The WER reduction may be simply due to the implicit observation of a larger acoustic context.A critical factor for the success of measured AFs used as observation is the accuracy of the Acoustic-to-Articulatory mapping (AAM, also referred to as speech inversion problem). Some studies on AAM have proposed methods that appropriately address the non-uniqueness of the AAM problem (Richmond et al., 2003; Richmond, 2006; Toda et al., 2007). The non-uniqueness implies that identical sounds can be produced by posing the articulators in a range of different positions (Lindblom et al., 1979). As a consequence the conditional probability density function of the position of an articulator given a speech sound can exhibit more than one mode (Roweiss, 1999). In other words, the AAM can be a one-to-many mapping. However, Qin and Carreira-Perpiñán (2007) showed that, although the non-uniqueness of AAM is normal in human speech, most of the time the vocal tract has a unique configuration when producing a given phone. Non-linearity seems to be a more relevant aspect to address. That is partly supported by the fact that feed-forward neural networks, which cannot properly approximate one-to-many mappings but can approximate non-linear functions, are one of the best performing methods for AAM (Mitra et al., 2010).The successful learning of the AAM can depend on the type of representation of the articulatory data. For example, the representation may affect the degree of non-uniqueness and non-linearity of the AAM. Representations where a feature encodes the coordinated movement of two or more vocal tract parts can facilitate the learning of the AAM as opposed to representations where each feature encodes the movement of one single vocal tract part (e.g., tract variables vs. articulator flesh points (Mitra et al., 2011)).The idea of transforming the acoustic domain by using measured articulatory data can also be accomplished without an explicit AAM. Multi-view learning based on canonical correlation analysis (CCA) has been proposed (Bharadwaj et al., 2012; Arora and Livescu, 2013, 2014) to find pairs of maximally correlated projected data in the acoustic and articulatory view. Then the acoustic projection is retained and appended to the acoustic observation vector. CCA-extracted features reduce PER in both speaker-dependent, and cross-speaker and cross-domain settings in a GMM-HMM phone recognition system (Arora and Livescu, 2013, 2014).The present work follows up our previous work (Badino et al., 2012; Canevari et al., 2012) where we showed that appending AFs to the acoustic observation vector reduces the error rate of a speaker-dependent hybrid DNN-HMM phone recognition system, in the MOCHA-TIMIT corpus (Badino et al., 2012) and over different datasets (Canevari et al., 2012).Compared to previous work, Badino et al. (2012) is the first attempt to integrate measured articulatory data in DNN-HMM acoustic models. In Badino et al. (2012) DNNs serve different purposes. A first DNN is used to learn the AAM and two different DNN pretraining strategies are compared. A second DNN is trained to compute phone state posteriors (henceforth shortened to phone posteriors) given the combined acoustic and recovered (through AAM) articulatory observations. Additionally a third DNN, in the form of a deep autoencoder (AE), is used to extract, from the space of single independent movements of each articulator, a new articulatory feature space that captures the most relevant “gestures” of the vocal tract and ignores the irrelevant ones. Such domain transformation aims at facilitating the learning of the AAM and at improving phone posterior estimation. That is in the same spirit of Mitra et al. (2012) where features derived from Articulatory Phonology represent the articulatory domain, although our approach is entirely data-driven and theory-free.Here we advance (Badino et al., 2012; Canevari et al., 2012) in many important aspects, both algorithmic and experimental. Our goals are: improve (DNN-based) methods that perform the AAM and asses the impact of pretraining in the AAM task; find better (autoencoder based) data-driven transformations of the articulatory domain and understand what they represent; assess the utility of the articulatory data in mismatched conditions where the phone recognizer is trained on clean speech and tested in different noisy environments; find DNN-based methods to exploit the very limited availability of articulatory data in cross-speaker and cross-domain settings, i.e., settings where the articulatory data of one or few speakers are used to improve DNN-based acoustic models, trained on purely acoustic corpora.The remainder of this paper is organized as follows. Section 2 briefly introduces deep neural networks and autoencoders. In the first part of Section 3 we review the pretraining strategies for DNNs performing AAM described in Badino et al. (2012) and propose some new variants. In the second part of Section 3 we review deep autoencoder-based transformations of the articulatory domain and propose supervised autoencoding, where autoencoders exploit information about the phonetic class associated to their input vector. Section 4 describes the two strategies applied to exploit articulatory data for phone posterior estimation: the standard approach where reconstructed AFs are appended to the observation vector and a novel approach, which we have named AAM-based pretraining, where articulatory features are not explicitly observed to compute phone posteriors. Section 5 describes the experimental setup. Section 6 shows results in AAM accuracy, autoencoder-based articulatory feature extraction, and phone recognition accuracy. Finally we analyze results and discuss future directions in Section 7.In their standard formulation DNNs are feed-forward neural networks whose parameters are first “pre-trained” using unsupervised training of deep belief networks (Hinton et al., 2006). DNNs can be seen as an improved version of feed-forward neural networks that exploits the knowledge of the statistical properties of the input domain (i.e., P(X)) to effectively guide the search for input-output relations (i.e., P(Y|X)).The DNN training is carried out as follows. First a deep belief network (DBN, henceforth DBN refers to deep belief network and not to dynamic Bayesian network as above) is trained in an unsupervised fashion. Subsequently the DBN is transformed into a DNN by converting the stochastic activation function of each node into a deterministic function. If the DNN is used to perform regression or classification an output layer is added on top of the deterministic net. Finally, supervised fine-tuning of the parameters is applied, typically using backpropagation.The DBN can be trained by approximating it to a stack of restricted Boltzmann machines (RBMs). An RBM (Smolensky, 1986) is an undirected graphical model with a layer of visible nodes (v) and a layer of hidden nodes (h) with intra-layer connections and without any within-layer connection. The joint probability of an RBM is:(1)P(v,h)=1Zexp(−E(v,h))where Z is the partition function and the energy function E(v, h) for an RBM with both binary visible and hidden variables is:(2)E(v,h)=−∑i,jviWijhj−∑ibivi−∑jcjhjwhere Wijare the connection weights and biand cjare the biases on the visible and hidden nodes respectively.The unsupervised learning of the parameters is performed by maximizing the log(P(v))=log(∑hP(v, h)). The update rule for a parameter θkis:(3)Δθk∝〈∂E(v,h)∂θk〉data−〈∂E(v,h)∂θk〉modelwhere 〈…〉datastands for expected value under the empirical distribution and 〈…〉modelfor expected value under the model distribution (Hinton and Sejnowski, 1986). They are computed using contrastive divergence (Hinton, 2002). RBMs with Gaussian distributed visible (or hidden) variables can be also trained by applying simple changes to some of the equations above (Welling et al., 2005).An AE is a particular neural network that consists of an encoding and a decoding part. The encoder maps an input vector x into a hidden/encoding representation h:(4)h=fθ(x)=s(Wx+b)where W is a weight matrix, b a bias vector and s is typically the sigmoid function.The decoder maps back the hidden vector h to a “reconstructed” input y:(5)y=gθ′(h)=l(W′h+b′)The AE is trained to minimize the distance between its input and its output (Fig. 3a), i.e., the reconstruction error. If the input data are assumed to be Gaussian distributed, as in the present work, l is typically an identity function and the AE is trained, usually through backpropagation, to minimize the squared error function ||x−y||2.An AE can be either used to reduce the dimensionality of the input domain or to generate overcomplete representations where the number of encoding nodes (i.e., extracted features) is larger than the number of input features.Single-layer AEs can be stacked to create a deep AE. An effective strategy to train deep AEs was proposed by Hinton and Salakhutdinov (2006) where a DBN corresponding to the encoding part of the deep AE is first trained (Fig. 3a), then it is “unrolled” to create the decoding part of the deep AE (Fig. 3b) and the resulting unrolled net is fine-tuned to minimize the reconstruction error.In the present work we use deep AEs where encoding nodes are as many as the input nodes and their values lie in the [01] range.A simple variant of the standard AE is the denoising AE (DAE) (Vincent et al., 2010), where the training input to the AE is transformed inx¯by corrupting the input, while the training objective is kept unaltered (||x−y||2, Fig. 3b). For Gaussian distributed input the input is corrupted by adding Gaussian noise (e.g., with 0 mean and 0.5 standard deviation as in the present work). The expectation is that the corruption of the input will not only make the AE more robust to noise but will also force the AE to capture the most stable and relevant dependencies between input features and ignore the irrelevant ones.In this section we review methods proposed in our previous work (Badino et al., 2012) and propose variants and novel methods to: (i) learn the AAM with DNNs (Section 3.1); (ii) extract, through an AE, a new set of articulatory features (Section 3.2).We experimented with three different DNNs to perform AAM. We named the three nets as aDNN1, jDNN1 and jDNN2 (depicted in Fig. 1c, d and e respectively). The lower case letter preceding DNN indicates the domain on which the DNN is pretrained, where letter a stands for acoustic domain and letter j stands for joint acoustic and articulatory domain. The number following DNN indicates the type of architecture (thus, e.g., aDNN1 and jDNN1 share the same architecture but are differently pretrained).As in Uria et al. (2011) and Badino et al. (2012), aDNN1 is pretrained with a DBN trained on the acoustic domain (Fig. 1a) and then trained to perform AAM after adding a layer of linear regressors on top of the DBN (Fig. 1c).jDNN1 (first proposed in Badino et al., 2012) is trained as follows. The pretraining is carried out by training three different RBMs (Fig. 1b). The first RBM (acoustic RBM) is trained on the acoustic domain while the second RBM (articulatory RBM) is trained on the articulatory domain. The third RBM (joint RBM) is trained on the joint output of the first two RBMs. The stochastic activities of each RBM are then replaced by deterministic activities. Subsequently the three (deterministic) RBMs are combined to create the jDNN1 shown in Fig. 1d, which in turn is trained to learn the AAM (note that no linear output layer is added). When combining the RBMs, some edges of the joint RBM are removed, resulting into a “pruned” RBM. Specifically the input to the joint RBM is v=[xz] where x is the input vector of acoustic features and z is the input vector of AFs. The input to the pruned joint RBM only consists of x and its weight matrix WRis a reduced version of the W weight matrix of the full joint RBM as all edges linking z to h are removed. Below we propose a novel strategy to “preserve” the RBM training in the pruned joint RBMs.jDNN2 is pretrained exactly as jDNN1 but the RBMs are differently combined. As for jDNN1 the joint RBM is used twice, but its “transpose” is not pruned, and the acoustic RBM is used twice (its transpose is on top of the transpose joint RBM). jDNN2 can be regarded as a multi-task network that tries to both perform AAM and reconstruct the acoustic domain (like an autoencoder). The idea behind multi-task learning is that of improving generalization by leveraging the domain-specific knowledge learned from related tasks.A common goal of jDNN1 and jDNN2 pretraining is to leverage articulatory information in the pretraining phase by driving the representation of the acoustic domain towards a speech production-constrained representation. Both DNN types are largely inspired by Ngiam et al. (2011). However in Ngiam et al. (2011) no action is taken to recompute the weights of the joint RBM in order to preserve its training when some of its edges are removed. Here we propose an alternative recomputation strategy to that proposed in Badino et al. (2012).If, given the input acoustic vector x, we want the weight matrix of the pruned joint RBM WRto generate hidden activationsh˜as much as possible similar to those generated by the full joint RBM (h) when fed with input v=[xz], then a possible strategy consists in updating the pruned RBM parameters to maximize log(P(h|x))=log(exp(−E(h, x))/∑hexp(−E(h, x))) (where h is the target value, obtained from the full joint RBM). That requires the computation of ∂logP(h|x)/∂θ(6)∂logP(h|x)∂θ=∂(−E(h,x)−log∑hexp(−E(h,x)))∂θ(7)∂logP(h|x)∂θ=−∂E(h,x)∂θ+∑hexp(−E(h,x))∑hˆexp(−E(hˆ,x))∂E(h,x)∂θ(8)∂logP(h|x)∂θ=−∂E(h,x)∂θ+∑hP(h|x)∂E(h,x)∂θConsidering that in a RBM P(h|x) factorizes, the partial derivative∂logP(h|x)/∂wij, wherewijis the weight parameter of the edge that connects node xito node hjis:(9)∂logP(h|x)∂wij=xihj+∑h˜1P(h˜1|x)⋯∑h˜jP(h˜j|x)⋯∑h˜HP(h˜H|x)(−xih˜j)(10)∂logP(h|x)∂wij=xihj−∑h˜jP(h˜j|x)xih˜j(11)∂logP(h|x)∂wij=xihj−〈xih˜j〉P(h˜|x)and we can use a simplified version of contrastive divergence to approximate the partial derivative where xihjis already given by the full RBM and for the second term we sampleP(h˜|x)from the pruned RBM just once to approximate〈xih˜j〉P(h˜|x).Contrary to jDNN1 and jDNN2, the topmost layer of aDNN1 is not pretrained as it is added after pretraining. A rule of the thumb to properly train aDNN1 is that of first training the topmost layer for few training epochs and than fine-tuning the entire net.Rather than simply representing the articulatory domain as the set of independent movements of each articulator flesh point recorded, e.g., by an EMA, we may aim at representing the articulatory domain as a set of features that encode the coordinated movements of different flesh points. In other words features that represents gestures of the vocal tract, where here gesture is meant as a statistically relevant movement or configuration of the vocal tract. Such transformation may facilitate the DNN-based AAM. In fact, in a DNN performing the reconstruction of all AFs, the topmost layer is a bank of independent regressors, each predicting one output feature value independently of all the other output features (given the values of the topmost hidden layer, which provides a representation of the acoustic domain shared by all regressors). When using flesh point movements, DNN maps speech acoustics on each single articulator flesh point movement/position whose effect on speech acoustics is marginal and strongly depends on the other flesh point movements/positions. By using features that encode the combined movements/positions of different flesh points we attempt to reconstruct features that may have a more direct relation to speech acoustics.The tract variables of Articulatory Phonology are an example of such kind of features, where the vocal tract behaviour is described as a set of constriction degrees and locations (e.g., lip aperture, tongue tip constriction degree and location). Here, rather than extracting theory-derived AFs, we follow a data-driven approach where new features are automatically extracted by an AE.As in Badino et al. (2012) we first train an AE to extract new AFs given the flesh point features and then learn the AAM on the new AFs as shown in Fig. 2. Henceforth we will refer to the flesh point positions velocities and acceleration as raw articulatory features (rAFs) and to the AE-extracted features as autoencoder articulatory features (aAFs).In this paper we do not only experiment with standard AEs but also with denoising AEs (DAEs) and AEs that exploit supervised information, i.e., information about the phonetic class associated to the vector of raw AFs.The use of DAEs is partly motivated by the fact that EMA measurements, which are the measurements used in the present work, are noisy, both because of intrinsic noise in the coil trajectory measurements and because of occasional displacements of the coils (Richmond et al., 2011).Both standard and denoising AEs are trained in an unsupervised fashion. However some supervised information can be used to force the AE to only learn segmental articulatory feature dependencies, i.e., dependencies that are related to phonetic articulatory targets, and discard all the other dependencies that are related to non-segmental aspects (e.g., supra-segmental aspects, speaker peculiarities, etc). Here we propose two types of supervised AEs, a segmental AE (SAE) and a segmental contractive AE (SCAE).The rationale behind the first supervised AE, SAE, is largely inspired by DAEs. When training a SAE, randomly selected input vectors are substituted by vectors that belong to the same phone state s, i.e.,xtsmay be transformed, with probability p (p=0.33 in the present work), intoxts¯=xns(where t and n specify some points in time) while the training objective is kept unaltered (||xt−yt||2, Fig. 3b). The working assumption is that the substitutionxts→xnsthat we (randomly) apply, forces the AE to learn dependencies that most characterize that phonetic unit and remove the phonetically irrelevant differences.We experimented with two different strategies to selectxns, which turned out to produce almost identical results. In the first strategy,xnsis the next vector if it falls within the same phone state, i.e.,xns=xt+1s(otherwise, if xt+1belongs to a different phone state, thenxns=xt−1s). In the second strategy we randomly selectedxns.The SAE training does not necessarily force the autoencoder to have similar encodings for input vectors sharing the same phone state and different encodings for input vectors of different phone states. A training that (rein-)forces such similarity can be a desirable property. The segmental contractive AE that we propose enjoys such property (Fig. 3c).Using binary encoding units, which have a Bernoulli conditional probability distribution, we can define the SCAE training error function as:(12)Et=||xt−yt||2−λ∑i=1Hht,islog(hn,is)+(1−ht,is)log(1−hn,is)where λ is a constant, H is the number of encoding nodes, and the second term of the error function is a sum of cross-entropies CE(ht,i, hn,i).CE(ht,i, hn,i) can also be interpreted as a lower bound of the Kullback–Leibler divergence between two Bernoulli variables with means ht,iand hn,irespectively.An additional positive term (λ2∑i=1Hht,islog(hn,il)+(1−ht,is)log(1−hn,il)) could be added to the error function to penalize similar encoding activations for vectors belonging to different phones (rather than phone states). However, that may require to weight phone similarity (meaning a different λ2 for each phoneme pair). That might explain why, when adding such term, the SCAE training did not converge unless we used very small λ2 values.The computation of the partial derivatives ∂CE(ht,i, hn,i)/∂θ·,i, where θ·,iis an element of the encoding W matrix or the b vector affecting the encoding node hi, is complicated by the fact that a change of θ·,iaffects both ht,iand hn,i.(13)∂CE(ht,i,hn,i)∂θ·,i=∂ht,i∂θ·,ilog1−hn,ihn,i+∂hn,i∂θ·,iht,i−hn,ihn,i(1−hn,i)Using sigmoidal activation units for the encoding layer we have:(14)∂CE(ht,i,hn,i)∂θ·,i=∂at,i∂θ·,iht,i(1−ht,i)log1−hn,ihn,i+∂an,i∂θ·,i(ht,i−hn,i)where ht,i=sigmoid(at,i).The previous section described approaches to learn the AAM. This section describes two strategies that exploit articulatory data for DNN-HMM acoustic modeling. Both strategies require that the AAM is learned first.In a DNN-HMM phone recognition system each phone is modeled as a n-state (n=3 in the present work) hidden Markov model which is typically context-independent (Mohamed et al., 2012). The phone state observation probabilities are approximated by scaling the phone state posteriors (here shortened to phone posteriors) computed by a DNN-based phone (state) classifier. Scaling consists in dividing by the phone state priors.The two approaches that we implemented in order to exploit measured articulatory data only affect the DNN-based phone classification. The first approach is the well-known approach where AFs are recovered from speech acoustics through AAM and then appended to the observation vector of the DNN phone classifier (Zlokarnik, 1995; Wrench and Richmond, 2000; Badino et al., 2012). Reconstructed AFs are both used when training and testing the phone classifier.Here we propose an alternative approach which we named AAM-based pretraining. In this approach the use of articulatory data is not direct. The DNN trained to learn the AAM is not used to recover the AFs appended to the observation vector but is instead used to initialize the parameters of the phone classifier DNN. Once a DNN is trained to learn the AAM (1) its topmost layer is removed; (2) a new layer, in which each node has a softmax activation function, is added on top of the net; (3) the net is finetuned to compute phone posteriors. The change of the topmost layer is necessary since both tasks (regression vs. classification) and targets (AFs vs. phone state posteriors) change.The AAM-based pretraining substitutes the “standard” DBN-based initialization of the phone classifier DNN. The expected difference of the initialization values provided by the two pretraining strategies is due the backpropagation applied to learn the AAM (in the AAM-based pretraining).In the AAM-based pretraining not only statistical properties of the acoustic domain (given by the DBN-based pretraining of the AAM DNN) but also acoustic–articulatory dependencies are used to drive the search for dependencies between acoustic features and phone classes. Similarly to the appended AFs approach the hypothesis is that phone classification can be improved by reverting the speech production process. Contrary to the appended AFs approach there is no explicit transformation of the acoustic space (into an acoustic+reconstructed articulatory space), however an implicit articulatory-driven transformation is carried out in the hidden layers of the phone classifier DNN.We used two British English datasets, the msak0 male voice of MOCHA-TIMIT (Wrench, 2000) and the single male voice mngu0 dataset (Richmond et al., 2011). Both consist of simultaneous recordings of speech and electromagnetic articulographic (EMA) data (plus, in the msak0 case, other types of articulatory data that we did not consider). msak0 is smaller than mngu0, with the first consisting of 460 utterances and the second of 1354 utterances.EMA data are the x and y positions of upper incisor (UI) (except for the mngu0 corpus), lower incisor (LI), upper lip (UL), lower lip (LL), tongue tip (TT), tongue blade (TB) and tongue dorsum (TD).Speech was segmented into 25ms Hamming windows sampled every 10ms, from which we extracted 20 mel-scaled filterbank coefficients (fbanks) plus their deltas and delta-deltas (for an overall vector of 60 fbanks). Contrary to Badino et al. (2012) we used fbanks as acoustic input for both AAM and phone posterior estimation.We used 3-state monophones and state boundaries were computed using the HInit, HRest and HERest functions of HTK (Young et al., 1999).Concerning the articulatory data, we used 42 AFs (36 in the mngu0 dataset) consisting of the x and y trajectories, plus their first and second derivatives. The EMA trajectories were first downsampled (to have a sample every 10ms as for the acoustic coefficients) and smoothed using an elliptic lowpass filter with 20Hz cutoff frequency. Then deltas and delta-deltas of the resulting trajectories were computed.All acoustic and articulatory features were normalized to have 0 mean and unit variance.To evaluate our systems in noisy speech conditions we corrupted the audio signal by adding 3 different kinds of noise: white Gaussian noise, noise in a cafeteria and noise produced by a subway train. The SNR ranged from 30 to 0dB. The noisy audio data were generated following the same procedure as for the Aurora-2 database (Hirsch and Pearce, 2000) and using the FANT software (Hirsch, 2005). The SNR was calculated after filtering the clean audio and the noise with the G.712 characteristic. The speech energy was determined using the ITU recommendation P.56.Most of the DNNs trained to learn the AAM received an input of 5 consecutive fbank vectors to reconstruct the AF vector corresponding to the central fbank vector. The 5-frame context is much smaller than that proposed in previous work (Zlokarnik, 1995; Wrench and Richmond, 2000). Since reconstructed AFs may convey information about the acoustic input on the DNN performing AAM, improved phone posterior estimation that uses reconstructed AFs may not be entirely due to articulatory information but also to an implicit larger acoustic context for phone posterior estimation. Our choice of a 5-frame context is a tradeoff between a reduced implicit context for phone posterior estimation and a context that guaranties good AF reconstruction. The number of nodes per hidden layer was manually set to be the same as the number of input nodes (5×60=300). Since aDNN1 and jDNN1 only differ in terms of learning parameters initialization (i.e., pretraining strategy) they were compared on same-size networks. jDNN2 is the multi-task counterpart of jDNN1 thus resulting in a larger number of learning parameters due to a larger topmost hidden layer (with 600 nodes).Concerning the DNNs that computed phone posteriors, the input covered a window of 9 consecutive frames where each frame consisted of 60 fbanks plus 42 or 36 AFs (when AFs were used). The DNN had 3 hidden layers with 1500 nodes per layer and 132 (=44 phonemes×3 states) softmax output units.111500 was approximately the maximum number of nodes per hidden layer that our graphical processing units could sustain.The input to the deep AEs used to transform the articulatory domain consisted of one single vector of 42 (for msak0) or 36 (for mngu0) AFs. They had a 300-42-300 (or 300-36-300) structure meaning that the hidden layers had 300 nodes each, with the exception of the middle (encoding) layer that had as many nodes as the input and output layers. By constraining the AEs to extract as many features as the input features we ensure that possible improvements due to AE-extracted features were not due to dimensionality reduction effects. Contrary to Badino et al. (2012) the encoding nodes were binary nodes, with values ranging in the [01] interval, which were then normalized to have 0 mean and unit variance. In the denoising AE the input nodes were corrupted with Gaussian noise with 0 mean and 0.5 standard deviation. Concerning the segmental AEs, the input vector could be substituted, with probability p=0.33, with a vector sharing the same phone state. In the Segmental Contractive AE λ was set to 0.3 (higher values of λ typically did not guarantee training convergence). Most of the AE's hyperparameters (e.g., Gaussian noise for DAE, p for SAE, etc) were validated by observing the AE reconstruction errors in the first fold validation of the MOCHA-TIMIT msak0 dataset.The DBN-based pretraining of the DNNs, including AEs, was implemented using a recipe very similar to that proposed in Mohamed et al. (2012). DNNs were pretrained using stochastic gradient descent and a mini-batch size of 100 training cases. RBMs with Gaussian units were trained for 225 epochs and a 0.001 learning rate, RBMs with binary units only were trained for 75 epochs and a 0.1 learning rate. The weight cost was fixed to 0.0002 and the momentum switched from 0.5 to 0.9 after 5 epochs.When applying AAM-based pretraining to the phone classifier DNN, the phone classifier DNN was not DBN-pretrained, the AAM DNN was used to initialize its learning parameters. In that case the AAM network needs to have the same structure of the phone classifier network, with the exception of the output layer. Thus, for the AAM-based pretraining case, we trained AAM nets that received an input of 9 acoustic vectors and had 1500 nodes per hidden layer. For AAM-based pretraining we only experimented with DNNs trained to reconstruct raw AFs.Finally, concerning DNN finetuning, the DNNs were fine-tuned with conjugate gradient and batch size of 1000 training cases (which turned out to be the best compromise between DNN performance and training time). The number of training epochs was 100 for AAM and 50 for phone classification. Training epochs were validated on the first fold validation of the MOCHA-TIMIT msak0 dataset where we observed that: (i) after 40 epochs the phone classifier DNNs usually stopped improving (on both training and testing data); (ii) after 100 epochs the regression error reduction of the AAM DNNs was negligible.When training aDNN1 and the phone classifier DNNs, the weights of the topmost layer were first updated for few epochs (e.g., 5), and then all the net parameters were updated. That was done to better preserve pretraining (Hinton et al., 2006).To accelerate DNN pretraining and fine-tuning we used the GPUmat Matlab toolbox (GPUmat, 2014) running on Tesla S2050 Graphical Processing Units.In the DNN-HMM phone recognizer each phone was represented as a 3-state HMM whose observation probabilities can be approximated by the phone posteriors provided by the DNN-based phone classifier divided by state priors. However, scaling phone posteriors often increased PER, thus the PERs reported in the results section refer to the lowest PER between the scaled and the non-scaled case.Speech was decoded by feeding the sequence of vectors of DNN estimated state posteriors into a Viterbi decoder. The probabilities of phone unigrams and bigrams used by the Viterbi decoder were computed on the speech training data only (as well as those of state bigrams). The probabilities of phone bigrams were computed using Good-Turing discounting, and back-off for missing bigrams.Training and evaluation on the MOCHA-TIMIT msak0 voice (consisting of approximately 20min of speech) was carried out by applying the same 5-fold cross-validation as in Wrench and Richmond (2000) and Badino et al. (2012), while the mngu0 dataset (consisting of approximately 1h of speech) was divided as in (Richmond et al., 2011) into 1225 training utterances and 65 testing utterances (the validation utterances were excluded).

@&#CONCLUSIONS@&#
