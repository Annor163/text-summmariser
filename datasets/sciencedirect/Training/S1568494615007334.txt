@&#MAIN-TITLE@&#
Cognitive behavior optimization algorithm for solving optimization problems

@&#HIGHLIGHTS@&#
A new swarm intelligence algorithm, COA, is developed for the optimization problems.The novel behavior model in COA makes the algorithm more effective and intelligent.Performance on 53 different benchmark problems is considered.The problem solving success of COA is compared with 8 state-of-the-art algorithms.

@&#KEYPHRASES@&#
Nature-based algorithms,Global optimization,Cognitive behavior model,Exploration,Exploitation,

@&#ABSTRACT@&#
Nature-based algorithms have become popular in recent fifteen years and have been widely applied in various fields of science and engineering, such as robot control, cluster analysis, controller design, dynamic optimization and image processing. In this paper, a new swarm intelligence algorithm named cognitive behavior optimization algorithm (COA) is introduced, which is used to solve the real-valued numerical optimization problems. COA has a detailed cognitive behavior model. In the model of COA, the common phenomenon of foraging food source for population is summarized as the process of exploration–communication–adjustment. Matching with the process, three main behaviors and two groups in COA are introduced. Firstly, cognitive population uses Gaussian and Levy flight random walk methods to explore the search space in the rough search behavior. Secondly, the improved crossover and mutation operator are used in the information exchange and share behavior between the two groups: cognitive population and memory population. Finally, the intelligent adjustment behavior is used to enhance the exploitation of the population for cognitive population. To verify the performance of our approach, both the classic and modern complex benchmark functions considered as the unconstrained functions are employed. Meanwhile, some well-known engineering design optimization problems are used as the constrained functions in the literature. The experimental results, considering both convergence and accuracy simultaneously, demonstrate the effectiveness of COA for global numerical and engineering optimization problems.

@&#INTRODUCTION@&#
Global optimization which can be defined as the process of searching for the global optimum in an optimization problem is a hotspot in applied mathematics [1,2]. However, the classical methods are difficult to find the global optimum for the problems as the dimension size of search space increased [3]. How to solve such complex problems becomes a key issue for global optimization.The behaviors observed in creatures such as reproduction, foraging food, defending oneself, communication and information exchange are very promising and complex. It is because of the behaviors that the problem-solving skills of super organisms are highly developed. As a result, the nature-based algorithms (NAs) come next, enabling non-linear and non-differentiable optimization problems to be solved effectively [4–6]. In addition, such optimization algorithms have been applied to various fields such as dynamic optimization [7], inverse geophysical problems [8], twin-screw configuration problem [9], IIR filter design [10], image processing [11], distribution transformer design problem [12], mechanical design optimization problems [13], sensor deployment problems [14], task scheduling [15], data mining applications [16], chemical processes [17] and many other engineering problems.Generally speaking, NAs can be divided into three main classes: swarm intelligence algorithms (SIAs), evolutionary algorithms (EAs), and physical phenomenon algorithms (PPAs) [18]. EAs are inspired by the genetic or evolution behaviors of creatures. Genetic algorithm (GA) [19] and differential evolution (DE) [20] algorithm can be described as representative algorithms in EAs. Firstly, the EAs evolve an initial random population for optimization. Then the combination and mutation strategies are used to generate the new population. Finally, greedy selection method is usually used to select a better solution between the new population and original population. In contrast, DE has a relative simpler structure and is more efficient for optimization. Many researchers have proposed a number of advanced variants of DE to improve its optimization performance, such as self-adapting control parameters in differential evolution (jDE) [21], the strategy adaption self-adaptive differential evolution algorithm (SADE) [22], the parameter adaptive differential evolution algorithm (JADE) [5], the DE based on covariance matrix learning and bimodal distribution parameter setting (CoBiDE) [23], and differential search algorithm (DSA) [24].The second main class of NAs is swarm intelligence algorithms. The SIAs as well as EAs are usually inspired by the behaviors of intelligent nature creatures. But most of SIAs use genetic rules only, which are different from the EAs. On the other hand, SIAs always take fully advantages of each solution in search space to provide better solutions to the problem. Some of the popular SIAs are particle swarm optimization (PSO) inspired by the social behavior of bird flocking [25], symbiosis organisms search (SOS) that simulates the interactive behavior seen among organisms in nature [26], artificial bee colony (ABC) that mimics the honey-bees food searching behavior [27], cuckoo search (CS) inspired by parasitic bio-interactions in living creatures [28], animal migration optimization (AMO) algorithm that gets the idea from the behavior of the animal migration [29], gray wolf optimizer (GWO) inspired by hunting mechanism of gray wolves [30].Physical phenomenon algorithms are the third class of nature-based algorithms. Different from the other two nature-inspired algorithms EAs and SIAs, the PPAs are mostly inspired by physical rules in the nature. There are some popular PPAs such as central force optimization (CFO) [31], big-bang big-crunch (BBBC) [32], charged system search (CSS) [33] and gravitational search algorithm (GSA) algorithm [34]. These algorithms in the three classes of the nature-based algorithms have been used to solve complex computational optimization problems. However, fast convergence along with accuracy is still a challenge need to be solved for the NAs.As a result, we develop a new nature-inspired algorithm named cognitive behavior optimization (COA) in this paper. Firstly, according to analyzing the classic behavior model of ABC and DE, the common phenomenon of finding food source is summarized as the process of exploration–communication–adjustment. Secondly, combining with social behaviors of human, a relative detailed cognitive behavioral model of swarm intelligence is proposed. Finally, based on the developed model, which contains three main behaviors: rough search, information exchange and share, and intelligent adjustment, COA is proposed which satisfy both fast convergence and accuracy for the selected global optimization problems in this paper.Preliminary studies show that the NAs such as ABC, DE, GSA, CoBiDE, DSA, GWO, AMO, CMA-ES [35] and other well-known optimization algorithms are very promising and could be used as the compared algorithms for evaluating COA's performance in solving global optimization problems.The rest of this paper is organized as follows. Section 2 reviews the two typical behavior models of NAs. Then the COA is presented in Section 3. The experiment sets and experimental analysis for different tests are presented in Section 4. Finally, Section 5 concludes the work and suggests some directions for the future.Many researchers are looking forward to a perfect way to model the intelligence of nature creatures’ behaviors so as to solve real world complex problems. Herein DE is a kind of evolutionary algorithm which has the ability of information exchange. Besides, ABC is typical bionic algorithm based on the self-organized model and swarm intelligence of bee colony in nature.Based on optimization technique that models the foraging behavior of the honey bees in the nature, Karaboga proposed the ABC algorithm [4]. The model of artificial bees in ABC contains three groups namely employed bees, onlookers and scouts. It can be concluded from the behavior model of ABC algorithm that based on the division of the swarm. The model is composed of three kinds of behavior for three kinds of bees: preliminary search, accurate search and abandoning a food source. ABC algorithm finds the global optimal value through various individuals of local optimization behavior. Compared with other nature-based algorithms, the detailed behaviors of foraging food and cooperation in ABC are emphasized so as to make it have faster convergence rate. However, it is simplified by using selected individuals’ information as the whole population's information to exchange. It may result in loss of useful information. Meanwhile, the primary search stage for employed bees and the accuracy search stage for onlooker bees have the same search equation. It unfits the demand of different search behavior in ABC model.On the other hand, DE, proposed by Storn and Price [20], is a simple, yet powerful, evolutionary algorithm with the generate-and-test feature for global optimization. It can be summarized that the behavior model of DE is mainly composed of the mutation and crossover operation, which is a process of information exchange. In DE, each individual in the population is called a target vector. DE produces a mutant vector by making use of the mutation operator, which perturbs a target vector using the difference vector of other individuals in the population. Then, the crossover operator is applied to the target vector and the mutant vector to generate a trial vector. Finally, the trial vector competes with its target vector for survival according to their objective function values. In a word, the function of information exchange in DE, which is explained by the mutant and crossover operator, is emphasized and it plays a very important role in the DE model.In this section, we propose a novel swarm intelligence algorithm named cognitive behavior optimization algorithm (COA) inspired by ABC and DE. Based on the behavior model of ABC and DE, cognitive behavior model is proposed and it contains three main behaviors: rough search, information exchange and share and intelligent adjustment.As we have known, the exploration refers to the ability to investigate the various unknown regions in the solution space to discover the global optimum, while the exploitation refers to the ability to apply the knowledge of the previous good solutions to find better solutions [36]. From the perspective of balancing the performance of exploration and exploitation, based on the biological behavior model of ABC and DE, the common process of finding food source for social insects is summarized as exploration–communication–adjustment. Meanwhile, combining with social behaviors of human, the cognitive behavior model is generalized as Fig. 1shown. The model contains three main behaviors: rough search, information exchange and share, and intelligent adjustment, which compose the main parts of COA. As ABC designed, the population of COA is also divided into two important groups namely cognitive population (Cpop) and memory population (Mpop) in this model, which play different roles in the COA algorithm. The abstract behavior model of foraging food source is introduced as follows:1)Rough search. For cognitive population Cpop, the individuals use the Levy flight and Gaussian random walk method to explore the search space randomly (RS and RS1 in Fig. 1). Gaussian random walk method used in the optimization algorithm has a promising performance in finding global minima, while Levy flight converges faster than that of Gaussian random walk. As a result, the two random walk methods are used in this search behavior.Information exchange and share. Based on the results of first stage, the memory population Mpop is introduced to store the food position of previous population for this behavior. In addition, the new crossover and mutation operator inspired by the DE algorithm is proposed to realize the behavior of information exchange and share (IES in Fig. 1).Intelligent adjustment. The exploitation and exploration are balanced by the selection and adjustment in this stage (IA, IA1 and IA2 in Fig. 1). In this part, with the ranks of the individuals among the population, the chance of passing better food position to the next generation will increase. On the other hand, the chance of adjusting the position of individuals which have not obtained a good food position will increase.In the early generations, the population prefers to explore a greater scope (RS in Fig. 1) because of lacking of useful information or avoiding loss information (Food Source C in Fig. 1) of the problems (B1 expands to B2 and A2 expands to A3 in Fig. 1). While, with the generation increased and useful information exchanged, more suitable solutions are remembered and the population trends to exploit some specific locations around the best food position (A2 narrows to A1 in Fig. 1). Rough search can random explore the search space. The information of individuals in Cpop and Mpop are exchanged by the information exchange and share operation. Meanwhile, the third behavior enables the individuals to fine-tune the position so as to retain the fairly performance to the next generation. In a word, these three behaviors and two groups in this model are collaboratively used to realize the foraging food phenomena and the detailed illustration of the three behaviors can be shown as follows.Before the rough search, according to the cognitive behavior model, the two important groups in COA are initialized as ABC designed by the Eqs. (1) and (2).(1)Cpopi=rand⋅(up−low)+low(2)Mpopi=rand⋅(up−low)+lowwhere Cpopiis the ith cognitive population and Mpopiis the ith memory population (i=1,2,3,…,N/2). And the size of Cpop=Mpop=N/2. N is the population size, low and up are the lower and upper bounds of the search space and rand is a random value selected from the range [0,1].In this stage, there are two main steps. Firstly, the methods of Gaussian random walk and the Levy flight are used to generate the new individual around the current individual. As we known, Levy flight converges faster than Gaussian walk in some generations, but Gaussian random walk is more promising in finding the global best solutions. As a result, the two different random walk methods are used in this behavior to balance the exploration and exploitation as Eqs. (3) and (4) described.(3)Cpop′i=Gaussian(Gbest,σ)+(r1⋅Gbest−r2⋅Cpopi),rand≤0.5(4)Cpop′i=Cpopi+α⊗Levy(s)⊗(Cpopi−Gbest)|s=μυ1/β,rand>0.5where r1and r2 are random numbers produced in the range [0,1]. Gbsetis the current best solution. The step size σ is calculated by the search direction (Cpopi−Gbest). Meanwhile, in order to balance the exploration and exploitation, the term log(g)/g is used to decrease the step size with the generation increased, where g is the current generation. As a result, the step size σ in Eq. (3) is calculated by Eq. (5)(5)σ=log(g)g⋅(Cpopi−Gbest)In Eq. (4), α>0 is the step size scaling factor. μ and υ are selected from the normal distributionN(0,σμ2)andN(0,συ2), where(6)σμ=Γ(1+β)sin(πβ/2)Γ(1+β)/2β2(β−1)/21/β,συ=1We use α=0.01 and β=3/2 as CS set in [27]. If the new individual is better than the old solution, it is replaced by the new one; otherwise it keeps unchanged.In this behavior, the improved crossover and mutation operation realized by the select probability Pc is proposed. Meanwhile the memory population Mpop which is used to store the individuals of previous population is introduced. The main process of the information exchange and share is introduced as follows.Step 1: Calculate the crossover probability Pc by Eq. (7).(7)Pci=rank(fitCpopi)N/2where fitCpopiis the fitness value of Cpopiand rank(fitCpopi) is considered as the rank of the individual Cpopiamong the other individuals in the population.Step 2: Confirm the Mpop by the ‘if-then’ rule and change the order of it:(8)ifr1<r2,thenMpop=Cpop(9)Mpop=permuting(Mpop)Step 3: Generate the new individual.(10)Cpop″i,j=Cpopk,j+rand⋅(Gbest,j−Cpopi,j+Mpopi,j−Cpoph,j),rand≤PciCpop″i,j=Cpopi,j+rand⋅(Mpopi,j−Cpopk,j),rand>Pciwhere i, k, h∈{1, 2, 3,…,N/2}, i≠k≠h, j∈{1, 2, 3,…,D} and D is the dimension of the problem. Gbestis the current global best solution.In this process, instead of the control parameter crossover rate CR in DE, Eq. (7) wants to state that the better the individual, the higher the probability. This equation is used to increase the chance of exchange the elements of individuals which have not obtained a good solution. Meanwhile, the chance of passing better solutions in the next behavior will increase. As a result, according to Eq. (10) without the scaling factor F in the standard DE, if rand≤Pciis held for a new individual Cpopi, the element of Cpopiis exchanged guided by the adjusted ‘DE/best/2’ so as to improve the exploitation performance, otherwise it is exchanged by the modified ‘DE/rand/1’ so as to improve the exploration performance. In addition, Eq. (8) ensures that Mpop belonging to a randomly selected previous generation as the memory population and remembers this historical population until it is changed. Eq. (9) changes the order of Mpop to enhance the capability of memory.The final operation is processed in the intelligent adjustment behavior. It is used to improve the quality of exploitation and the diversification property. In this process, the crossover probability Pc is also used. Different from the information exchange and share, if the condition Pci>rand is held for a new individual, the current position of Cpopiis adjusted according to Eqs. (11) and (12), otherwise no adjustment occurs.(11)Cpop‴i=Cpopi+φ⋅(Cpopi−Gbest)|rand<0.5(12)Cpop‴i=Cpopi+φ⋅(Cpopi−Cpopj)|rand>0.5where i is random selected index from [1, 2, 3,…,N/2] and i≠j. φ is uniformly distributed random number restricted to [−1,1]. The adjusted individual will replace Cpopiif its fitness value is better than that of Cpopi.Some individuals of the population may overflow the allowed search space after the generation of new population. The individuals beyond the search-space limits are regenerated using the ‘boundary control’ strategy. Its procedure is introduced as follows.Step. 1 if Cpopi,j<low or Cpopi,j>upStep. 2 then Cpopi,j=rand×(up−low)+lowBy combining the above three behaviors, the pseudo code of COA based on the fitness evaluations is presented as shown in Algorithm 1. From Algorithm 1, it is necessary to illustrate that the number of fitness evaluations for first two processes is N per one generation (N/2+N/2). But for the last process (Intelligent Adjustment), the function is evaluated if rand>Pci. As a result, the total number of fitness evaluations per one generation cannot be statically determined.To study our COA algorithm, experiments are carried out on both unconstrained and constrained global optimization problems. For unconstrained problems, two types of benchmark functions including classic and modern benchmark functions (CEC 2014) are employed. For constrained benchmark functions, three engineering design optimization problems commonly used in literature are used. In addition, all the experiments in this section are performed on computer with 3.20GHz Intel(R) Core(TM) i5-3470 processor and 4GB of RAM using MATLAB 2013a.To illustrate the relative success of COA on classic benchmark functions, a series of 20 widely used benchmarks selected from literature [29,37] are introduced in this test. Table 1presents the detailed information about the different types of benchmarks, such as unimodal, multimodal, separable, non-separable. Our COA algorithm has been compared with six well-known nature-based algorithms: (1) EAs: DE, CoBiDE and DSA; (2) SIAs: GWO and ABC; (3) PPAs: GSA. Appendix A shows the download link of all algorithms used in this test.In this test, the population size N is set to 50. The maximum function evolution numbers (MaxFEs) is set to 400,000 for f1–f12 and 10,000 for f13–f20. For each test function, 30 independent runs were carried out with random seeds. To have a fair comparison, ABC is also modified according to [38]. For COA, the size of Cpop=Mpop=N/2 inspired by ABC algorithm. The parameter settings for the other compared algorithms in this test are detailed as follows:1)DE: F=0.5, CR=0.9 as in [20] and the DE/rand/2/exp strategy is used;CoBiDE: pb=0.4, ps=0.5 and mutation strategies and crossover strategies as in [23];DSA: p1=0.3×rand, p2=0.3×rand and objective direction method is used as in [24];GWO: a=2-1×(2/MaxCycle) as in [30];GSA: Go=100 and a=20 [34];ABC: limit=(N·D)/2; size of employed-bee=onlooker-bee=(colony size)/2 as in [38].Table 2summarizes the statistical results including Mean, Worst, Best and SD of objective function values obtained by 30 independent runs for high-dimensional functions in Table 1. From Table 2, it can be found that COA nearly finds all the optimal values of the high-dimensional functions except function f05, f07, f11 and f12. Compared with other 6 algorithms, COA exhibits the best performance on the unimodal nonseparable function f02 and f03. For f05, COA has the best mean value. For unimodal separable functions f01, f04 and f06, COA performs better than the compared algorithm in the terms of the accuracy and robustness. GWO has a promising performance for optimizing f07 and COA exhibits the second best performance on this function. With regard to the multimodal nonseparable functions (f10–f12), COA performs better than GSA, ABC and DE in the terms of the accuracy and robustness. And COA performs very similar with DSA, CoBiDE and GWO for f10. However, DSA and CoBiDE perform best on f11 and f12 compared with other algorithms. According to the test results of multimodal separable functions, COA outperforms the other algorithms on f09. Meanwhile, COA performs comparable to DSA, CoBiDE and GWO on f08.For the functions f13–f20 with few local minima due to low dimensions, Table 3reports the final results obtained by 30 independent runs. Different from the high-dimensional multimodal functions, these functions deal with low and fixed dimensions. As shown in Table 3, almost all algorithms could reach optimal or near optimal solutions for all test functions. However, COA performs best on f14, f15, f17, f18 and f20 according to Mean and SD values. DE is similar to CoBiDE for optimizing f13 and it performs best on f16. For f16 and f19, GSA exhibits the promising performance.From a graphical point of view, Figs. 2 and 3present the convergence curves abstained by the seven algorithms with 30 independent runs. When examined Figs. 2 and 3, COA converges rapidly for most test functions. Especially for the unimodal functions which are used to test the performance of the convergence for the NAs, COA has faster convergence rate for f02, f03, f04 and f07 compared with other algorithms. GWO performs better for f01 on convergence rate. For the most multimodal functions, COA also presents the promising performance in convergence rate as shown in Figs. 2 and 3.For detailed comparison, the Wilcoxon Signed Ranks Test [39], in which Iman–Davenport's procedure is used as a post hoc procedure, is firstly used to make a pairwise comparison so as to illustrate the superiority of COA on the 20 test functions. The statistical results are summarized in Table 4. In Table 4, the ‘R+’ represents the sum of ranks for the problems which COA algorithm outperformed the compared one, and the ‘R−’ refers to the sum of ranks for the opposite. ‘+’ indicates COA having superior performance than the compared algorithm. ‘−’ indicates the opposite. The sign ‘≈’ means there is no statistical difference between COA and the compared algorithm. According to the values of ‘+’, ‘≈’ and ‘−’ in Table 4, we can conclude that COA algorithm outperforms the compared six algorithms in solving the problems used in the test. According to the last line of Table 4, overall COA is the statistically best compared with other six algorithms at a 0.05 significance level.To further detect the significant differences between COA and the six competitors, the Friedman's test is carried out, in which Iman–Davenport's procedure is used as a post hoc procedure. It is necessary to emphasize that the Friedman's test is accomplished in this paper by using the KEEL software [40]. Table 5summarizes the average rankings of Mean and SD values among seven algorithms obtained by the Friedman's test. The average rankings can be used as indicators to illustrate how successful the algorithm is. In other words, the lower the rank, the more successful the algorithm is [41]. From Table 5, it can be seen that the COA ranks first on Mean and SD value. For the mean value, Friedman statistic considering reduction performance distributed according to chi-square with 6 degrees of freedom is 9.208929. And the Iman and Davenport statistic considering reduction performance distributed according to F-distribution with 6 and 114 degrees of freedom is 1.579276 at α=0.05, and the critical value is 2.17 which can be found in the statistical tables [42]. According to [41], Critical difference CD is calculated byCD=qαk(k+1)/6NandCD=2.177⋅8/6⋅20=1.48, where qαis the critical value, N is the number of problems, k the number of algorithms. The comparisons of all algorithms against each other using the critical difference from the Friedman and Iman and Davenport tests for Mean and SD are shown in Fig. 4. From Fig. 4(a), it can be found that there is significantly difference between COA and GSA on mean values. However, the group of the algorithms including COA, CoBiDE, ABC, DSA, GWO and DE is not significantly different each other. From Fig. 4(b), there is significantly difference between COA and GSA, DSA, GWO on SD values. However, the group of the algorithms including COA, CoBiDE, ABC, and DE is not significantly different each other.With regard to the time consumption for optimization of these 20 test functions, Table 6presents the results of mean time obtained by the COA and other compared algorithms with 30 independent runs. We also rank the algorithms from smallest mean time of the test functions to the highest mean time. As it can be concluded from Table 6, DSA performs the best among the algorithms. Compared with ABC and DE, COA ranks fifth and it performs better than DE. As COA's information exchange and share behavior realize the information exchange among all the individuals and the intelligent adjust process is performed for the selected individuals per generation, while the selected or special individuals in ABC perform the information exchange operation for onlooker bees and scout bees. As a result, COA consumes more time than ABC.In this section, the effect of COA parameter, as well as three behaviors of COA is experimentally studied. To evaluate the effect of each behavior in COA cognitive behavior model, three configurations are designed. The first one configuration (Configuration 1) is designed without rough search behavior. In the second configuration (Configuration 2), the information exchange and share behavior is removed from the COA. The last configuration (Configuration 3) is developed without the intelligent adjustment behavior. These three configurations along with COA are tested on the above 10 typical classic benchmark functions. The MaxFEs of each function are set as its previous condition. Table 7represents the average results of 30 runs for each configuration. From Table 7, it can be observed that COA performs the best performance for the accuracy and robustness. Configuration 1 presents the worst optimization performance for the selected unimodal and multimodal benchmark functions. It means that the rough search behavior is rather important for COA because of the successful use of the random walk method.Fig. 5shows the convergence curves of the three configurations and COA algorithm. From Fig. 5, COA converges faster for the multimodal functions than the other configurations, while Configuration 3 performs better for the unimodal function Sphere and Schwefel 2.22. The obtained results from Table 7 and Fig. 5 prove that each behavior in COA can significantly affect the quality of final results, and it also shows that COA with three behaviors together make a coherent system to solve optimization problems more successfully.As mentioned before, different from DE algorithm, COA does not have any special control parameters such as CR or F in DE. However, COA is sensitive to the population size N designed from the idea of ABC. In order to analyze the effect of N for the optimizing performance of COA, it is tested on the above 10 typical functions and the MaxFEs of each function are set as its previous condition. Table 8lists the average results of 30 independent runs based on the different N values. From Table 8, it can be found that the optimization performance of COA is considerably improved as N is decreased under the same MaxFEs, while the time consumption is increased as a result. Figs. 6 and 7show the effect of N on the convergence rate for several test functions and mean time consumption of the previous ten test functions. From Fig. 6, it can be found that COA with N=30 converges faster than the other conditions. However, its time consumption is larger than that of other conditions. From the above results, N is better to set as 50–100 based on a trade-off between convergence rate and time consumption.We also used 30 single objective and complex real-parameter unconstrained numerical optimization problems in IEEE CEC2014 to evaluate the performance of COA for solving complex problems in this section. Table 9summarizes several features of the benchmark problems in this test. Detailed information on these problems is provided in [43]. To evaluate CEC 2014 benchmark functions, the results of COA are compared with state-of-the-art algorithms such as: DE [20], CoBiDE [23], AMO [29], GWO [30], CMA-ES [35] and ABC [38]. AMO is s novel swarm intelligence algorithms and it is similar with COA on the term of the algorithm structure. CMA-ES is a famous evolutionary algorithm and it is often used as a compared algorithm for complex benchmark functions. In this test, the population size N is set to 100, function's dimension is 30, MaxFEs is set to 300,000 with 51 independent runs [43]. It is necessary to emphasize that the Matlab codes of CMA-ES and AMO are directly taken from Appendix A. The other parameter settings for DE, CoBiDE, ABC, GWO and COA are the same as Test I used. The parameter settings of other two algorithms are detailed as follows:1)AMO: The number of animals in each group was set to 5 [29];CMA-ES: σ=0.25 andμ=4+2⋅logN2as in [35].Tables 10–13summarize the Mean and SD of the objective function values over 51 independent runs for each problem. We rank the algorithms from smallest mean solution to the highest solution. At last of the tables, the average ranks and the overall ranks obtained by algorithms are concluded.(1)Unimodal functions f01CEC∼f03CEC: Table 10 presents the results of the algorithms for optimizing the unimodal functions. From Table 10, it can be found that AMO can find the global best global optimum for f02CEC. Although COA cannot reach the best global optimum for the three functions, it exhibits the best performance on the three unimodal functions. CMA-ES shows the worst performance on f02CEC, GWO performs the worst for f01CE and f03CEC. When examined the last line of Table 10, COA ranks first for these three functions.Simple multimodal functions f04CEC∼f16CEC: Table 11 summarizes the results obtained by the algorithms for simple multimodal functions. Clearly, CMA-ES exhibits the best performance on f06CEC, f09CEC, f11CEC, f12CEC and f15CEC compared with other six algorithms. COA shows the best performance among the algorithms for f04CEC. For f14CEC, GWO outperforms than the other six algorithms. CoBiDE presents the best performance on f08CEC, and AMO performs better than the other algorithms for f05CEC, f07CEC and f13CEC. DE shows the worse optimization performance for the multimodal functions. However, when examined the last line of Table 11, COA ranks first for these multimodal functions as a whole because the three behaviors effectively balance the performance of exploration and exploitation.Hybrid functions f17CEC∼f22CEC: The solution of these 6 functions is more difficult than that of previous 16 functions. The final results of hybrid functions are reported in Table 12. When examined Table 12, we can observe that the performance of CoBiDE outperforms the compared algorithms for f17CEC, f18CEC and f21CEC. COA performs better on f22CEC than the other compared algorithms. As can be seen from Table 12, CoBiDE and DE rank toward the top for these 6 hybrid functions, while COA performs overall better than GWO, ABC, AMO and CMA-ES.Composition functions f23CEC∼f30CEC: These eight functions are the most difficult in this test. Table 13 summarizes the obtained results of the algorithms with 51 independent runs. As seen from Table 13, the results provided by the seven algorithms are far away from the global optima for these 8 test functions. However, according to overall rankings of Table 13, the COA still ranks first, while CoBiDE and AMO rank second and third, respectively. CMA-ES performs the worst performance among the algorithms.We also perform the Wilcoxon Signed Ranks Test in this test, in which Iman–Davenport's procedure is used as a post hoc procedure [39]. The statistical results are summarized in Table 14. When examined the last line of the Table 14, we can observe that COA provides higher ‘+’ values than ‘−’ or ‘≈’ values compared with other six algorithms. It means that, COA performs better than the compared algorithms. As ABC's mechanism, COA and AMO also divide the population into different functional parts or have different operations for the population. However, COA outperforms the ABC and AMO according to the results of Table 14. As a result, the behaviors of COA are superior to that of ABC and AMO.In addition, Friedman test is used to detect the significance differences between COA and the compared algorithms on both 30 benchmarks of CEC 2014, in which Iman–Davenport's procedure is used as a post hoc procedure. Also, the Friedman's test was used by KEEL software [40]. The detailed test results of the seven algorithms on Mean and SD are shown in Table 15. From Table 15, it can be found that COA ranks first on Mean and SD among the seven algorithms. We can also observe that Friedman statistic considering reduction performance distributed according to chi-square with 6 degrees of freedom for mean value is 49.035714, and the Iman and Davenport statistic considering reduction performance distributed according to F-distribution with 6 and 174 degrees of freedom is 10.8582 at α=0.05, the critical value is 2.15 which can also be found in the statistical tables [42]. Again, we calculate critical difference CD for this test andCD=2.157⋅8/6⋅20=1.199. Fig. 8shows the comparisons of all algorithms against each other using the critical difference from the Friedman and Iman and Davenport tests for Mean and SD rankings. From Fig. 8(a), it can be found that there are not significantly differences among the COA, CoBiDE and AMO for 30 CEC2014 benchmark functions. Compared with the rest four algorithms, COA is significantly different. As seen from Fig. 8(b), the group of the algorithms including COA, CoBiDE, GWO and AMO is not significantly different each other.From a graphical point of view, Figs. 9 and 10present the convergence curves for 16 typical functions in CEC 2014. When examined Fig. 9, COA algorithm converges rapidly for most test functions compared with other 6 algorithms. For f02CEC, AMO converges faster than COA. From Fig. 10, it is clear that COA performs better performance in convergence than the compared algorithms for f14(CEC), f16(CEC), f22(CEC), f23(CEC) and f28(CEC). For f20(CEC), DE converges little faster than COA. In a word, the superior experimental and analysis results of COA on these 30 CEC2014 test functions owe to its cooperative behaviors of the cognitive behavior modal.So far, the COA is invested on types of unconstrained problems. For the engineering design optimization problems, which are the most important and ubiquitous types of global constrained optimization problems, COA is also employed to evaluate its preference of solving such problems in this section.Generally, the global constrained optimization problem is defined as follows:(13)minf(x)s.t.gk(x)≤0,k=1,2,3,…,qhj(x)=0,j=1,2,3,…,mlowi≤xi≤upi,i=1,2,3,…,Dwhere f(x) is the objective function, x=(x1, x2,…,xD) is an D dimensional vector, gk(x) is the inequality constraints and hj(x) is the equality constraints. lowiand upiare the lower and upper bound of xi, respectively.In order to study the performance of solving the engineering design optimization problems, COA is applied to three well-known constrained engineering design optimization problems: tension/compression spring, welded beam and pressure vessel designs. The above algorithms including ABC, GSA, CoBiDE, DSA, GWO, DE and AMO are used as the compared algorithms. In addition, the same constraint handling mechanism used in [44] was used for the problems as Eq. (14) shown, and the results of 30 independent runs on COA and other algorithms are placed at Tables 16–21. The population size for these problems is set to 20 and the MaxFEs is set to 50,000 for each problem.(14)fP(x)=f(x)+104∑kmaxk{gk(x),0}+∑j|hj(x)|Spring design optimization problem is a well-known engineering design problem. The main goal of this problem is to design a spring for a minimum weight by achieving optimum values of the variables as shown in Fig. 11. It contains four nonlinear inequality constrains and three continuous variables: the wire diameter w (x1), the mean coil diameter d (x2) and the length (or number of coils) L (x3). This problem can be modeled as follows:(15)minfx1,x2,x3=x3+2x12x2Subject to(16)g1(X)=1−x23x371785x14≤0g2(X)=x2(4x2−x1)12566x13(x2−x1)+15108x12−1≤0g3(X)=1−140.45x1x22x3≤0g4(X)=2(x1+x2)3−1≤0Variablerange:0.05≤x1≤2,0.25≤x2≤1.3,2.0≤x3≤15.0For this problem, Table 16 lists the best solutions of this problem obtained by COA and other seven compared algorithms with 30 independent runs and the statistical results of them are presented in Table 17. When examined Tables 16 and 17, it can be found that the best result obtained by COA is 0.01266523278, which is equal to that of DE algorithm. However, the standard deviation of the results by COA is clearly smaller than that of compared six algorithms. In addition, the optimization results of inequality constraint g1 and g2 of COA reach to the 0 based on 50,000 NFEs.The welded beam design optimization problem (see Fig. 12) involves four design variables including the width h (x1) and length l (x2) of the welded area, the depth t (x3) and thickness b (x4) of the main beam. The main goal is to minimize the overall fabrication cost, under the bending stress σd(30,000psi), appropriate constraints of shear stress τd(13,600psi), maximum end deflection δd(0.25in) and loading condition P (6000lb). The problem can be written as:(17)minf(x1,x2,x3,x4)=1.10471x12x2+0.04811x3x4(14.0+x2)Subject to:(18)g1(X)=x1−x4≤0g2(X)=δ−0.25≤0g3(X)=τ−13,600≤0g4(X)=σ−30,000≤0g5(X)=0.10471x12+0.04811x3x4(14+x2)−5.0≤0g6(X)=0.125−x1≤0g7(X)=6000−F≤0where(19)σ=50400/x32x4Q=600014+x2/2D=12x22+(x1+x3)2β=QD/JJ=2x1x2x22/6+(x1+x3)2/2δ=65856/3000x4x33α=6000/2x1x2τ=α2+αβx2/D+β2F=0.61423⋅106x3x4361−x330/4828The variable ranges are 0.1≤x1, x4≤2.0, 0.1≤x2, x3≤10.With 30 independent runs, the COA and the other compared algorithms are used to find the best solution of the design problem based on 50,000 NFEs per run. The comparison results are provided in Table 18 and the statistical results are listed in Table 19. The results from Tables 18 and 19 show that COA obtained the best cost value. Although, CoBiDE gets the 0 value for the inequality constraint g3 and g4, its cost value (1.72485442453) is larger than that of COA (1.72480865692). In addition, the best result obtained by DE is comparable to COA, but it performs worse on the SD value as seen from Table 19.Pressure vessel design optimization problem is a mixed type of optimization and it has also been often used as a benchmark problem for evaluate different optimization algorithms. Fig. 13shows a cylindrical pressure vessel capped at both ends by hemispherical heads. The object is to minimize the total cost, including the cost of forming, material and welding. There are four variables: the thickness Ts (x1), thickness of the head Th (x2), the inner radius R (x3) and the length of the cylindrical section of the vessel L (x4). The problem can be stated as follows:(20)minf(x1,x2,x3,x4)=0.6224x1x3x4+1.7781x2x32+3.1661x12x4+19.84x12x3Subject to:(21)g1(X)=−x1+0.0193x3≤0g2(X)=−x2+0.00954x3≤0g3(X)=−πx32x4−43πx32+1,296,000≤0g4(X)=x4−240≤0Variableranges:1×0.0625≤x1,x2≤99×0.0625,10≤x3,x4≤200For this problem, Table 20 summarizes the best results obtained by COA and other seven compared algorithms based on 30 independent runs and the statistical results of them are presented in Table 21. From Table 20, COA, CoBiDE and DE have the similar optimization performance according to the best results. However, the statistical results from Table 21 show that CoBiDE and DE have the worse standard deviation, while the SD of COA is 0.For detailed comparison, the Wilcoxon Signed Ranks Test and Friedman test [39], in which Iman–Davenport's procedure is used as a post hoc procedure, are used to illustrate the superiority of COA on the three engineering optimization problems. The statistical results are summarized in Table 22. According to CD, which is equal to 4.3086 in this test, there are not significantly differences among the eight algorithms. However, when examined the Table 22, we can observe that COA provides higher ‘+’ values than ‘−’ or ‘≈’ values compared with other algorithms. Meanwhile, COA and DE tied for first on the average rankings according to the Friedman test. As a result, COA still performs better optimization performance for these three engineering optimization problems.

@&#CONCLUSIONS@&#
