@&#MAIN-TITLE@&#
A comprehensive review of background subtraction algorithms evaluated with synthetic and real videos

@&#HIGHLIGHTS@&#
We propose a wide review of background subtraction (BS) algorithms.We have tested them thanks to the BGSLibrary and the BMC benchmark.This study leads to a global discussion about the comparison of BS algorithms, and the way to test them.

@&#KEYPHRASES@&#
Background subtraction,Motion detection,Foreground segmentation,

@&#ABSTRACT@&#
Background subtraction (BS) is a crucial step in many computer vision systems, as it is first applied to detect moving objects within a video stream. Many algorithms have been designed to segment the foreground objects from the background of a sequence. In this article, we propose to use the BMC (Background Models Challenge) dataset, and to compare the 29 methods implemented in the BGSLibrary. From this large set of various BG methods, we have conducted a relevant experimental analysis to evaluate both their robustness and their practical performance in terms of processor/memory requirements.

@&#INTRODUCTION@&#
Background subtraction (BS) is a crucial step in many computer vision systems, as it is first applied to detect moving objects within a video stream, without any a priori knowledge about these objects [40]. BS has been widely studied since the 1990s, and mainly for video-surveillance applications, since they first need to detect persons, vehicles, animals, etc. before operating more complex processes for intrusion detection, tracking, people counting, etc. Many algorithms have been designed to segment the foreground objects from the background of a sequence, and generally share the same scheme [4]:Background initialization:They first aim to build a background model thanks to a fixed number of frames. This model can be designed by various ways (statistical, fuzzy, neuro-inspired, etc.).In the next frames, a comparison is processed between the current frame and the background model. This subtraction leads to the computation of the foreground of the scene.During this detection process, images are also analyzed in order to update the background model learned at the initialization step, with respect to a learning rate. An object not moving during long time should be integrated in the background for example.Even if the evaluation of such algorithms is an important issue, only a few articles deal with this problematic in the literature [2,13,33]. Moreover, since they are conducted on a reduced period of time, these works do not cover a large set of algorithms of the literature. Recently, this lack of durable reference has lead to the emergence of several benchmarks, fully available on the Web, as ChangeDetection.net [20], SABS (Stuttgart Artificial Background Subtraction Dataset) [8], or BMC (Background Models Challenge) [42]. These datasets allow authors to download challenging videos, and to compare their work with both classical and recent contributions.In this article, we propose to use the BMC dataset, which has the originality to contain both synthetic and real sequences (more precisely, it is composed of 20 synthetic videos and 9 real videos). In this benchmark, a software (BMC Wizard) is employed to compute four quality measures. Some of them are very significant for BS, and not usual in the other benchmarks (namely, SSIM [43] and D-Score [28]). We have compared the 29 methods implemented in the BGSLibrary (Background Subtraction Library) [36]. Thanks to this library, covering a large set of algorithms from the literature, we thus propose here a very large comparison of BS methods, with various methodologies.In Section 2, we present a pragmatical state of the art about background subtraction, related to BGS library. Both BMC and BGSLibrary are described shortly in Section 3. The evaluation process is explained in Section 4. We finally discuss the results obtained and possible further works in Section 5.In this section, we propose an overview of the most famous methods for BS, related to the BGS library. Any reader could also refer to more complete states of the art in [3,7]. In the following, we adopt that bold letters represent vectors or matrices in the equations, upper case letters are collections (matrices, images, etc.), while lower case letters mean canonical elements (like pixels, scalars), and that we consider by default colored pixels in the presentation of the algorithms, meaning that pixels are composed of three components (R, G, B for example). The associated variables may thus be vectors, and images are always considered as matrices. The use of a special colorspace (as gray scale images for example) will be specified in the text if necessary.Essentially the BS process consists in creating a background model. In a simple way, this can be done by setting manually a static image that represents the background, and having no moving object. For each video frame, we then compute the absolute difference between the current frame and the static image. This method is called Static Frame Difference in the rest of the article. However, a static image is not the best choice, if the ambient lighting changes, then the foreground segmentation may fail dramatically. Alternatively, it is possible to use the previous frame rather than a static image. This approach, called further Frame Difference, works with some background changes but fails if the moving object stops suddenly. In [27], the authors suggest the initialization and maintenance of the background model by the arithmetic mean (or weighted mean) of the pixels between successive images. So, given a videoVwith length l containing gray scale images defined byV={Z1,…,Zl}, the background modelBcan be defined by:(1)B=1l∑t=1lZt.Typically Eq. (1) is used to initialize the background model.1Other authors such as McFarlane and Schofield [31], Prati et al. [33] and Calderara et al. [9] suggests to use the median filter instead of the mean or average filter.1However, after the initialization, to perform the background model maintenance, it is also common to use Eq. (1) recursively by:(2)Bt=(1-α)Bt-1+αZt,whereBtis the background model at timet∈{1,l}⊂Zandα∈[0,1]⊂Ris the learning rate. The main advantage of this method is the adaptive maintenance of the background model while changes occur in the scene (see Fig. 1). Afterwards, [34] clarify that some foreground pixels are included in the background model update. To solve this issue, an adaptive-selective method is proposed. In this approach, only the regions with no moving object are updated.After building the background model, the next step is the foreground detection. The first and most common way is to compute the absolute difference between the current frame and the background model, similarly to Static Frame Difference method. However, in this case the background model is continuously adapted instead of a static image. The foreground detection can be performed in other ways. More recent methods, such as [1,24,23,26,46] suggest the use of color, texture and edges features to improve the foreground detection. In [46], the authors present a novel approach that fuses the texture and color features for BS. In [26], the authors investigate how different color spaces affect the segmentation result in terms of noise and shadow sensitivity. In [23] is proposed an approach to model the background of images in a video sequence based on sub-pixel edge map. A new background subtraction algorithm based on a combination of texture, color and intensity information is presented in [24]. In [1] the authors propose a new technique for background modeling and subtraction by fusing texture feature, RGB color feature and Sobel edge detector.One of the most popular BS method is based on a parametric probabilistic background model proposed by Stauffer and Grimson [38], and improved by Hayman and Eklundh [21]. In this algorithm, distributions of each pixel color is represented by a sum of weighted Gaussian distributions defined in a given colorspace: the Gaussian Mixture Model (or GMM). These distributions are generally updated using an online expectation-minimization algorithm, and enhance the use of one Gaussian distribution, as in [44] (see also [2]).More precisely, as a new image is processed, the GMM parameters (for all pixels) are updated to explain the colors variations. In fact, at time t, we consider that the modelmtgenerated for each pixel from the measures{z0,z1,…,zt-1}of a pixel is correct. The likelihood that a pixel is a background pixel is:(3)P(zt|mt)=∑n=1Nαn(2π)d/2|Σn|1/2e-12(zt-μn)TΣ-1(zt-μn)where d is the dimension of color space of the measuresztand each Gaussian n is described by its meanμnand covariance matrixΣn. The Gaussians are weighted by factorsαnwhere∑n=1Nαn=1.|·|denotes the matrix determinant. The channels (e.g. R, G, B) of each pixel are considered independent, that is:(4)Σn=1σn20002σn20003σn2In the original paper, left subscripts refer to the channel number. However, it is common to use a vector to describe the variance of the three components of a pixel. In this case, to update the GMM, we first associate the measuresztto one Gaussiann′if(5)‖zt-μn‖<kσnwhere k is 2 or 3, andσna vector representing the variance of the Gaussian distribution of index n. The operator<is true if all vector components at the left are smaller thankσn. This measure represents the background if the Gaussiann′explains the background of the scene. In fact, the weightαn′is high. This Gaussian is then updated:(6)αn′←(1-δ)αn′+δ(7)μn′←(1-ρn′)μn′+ρn′zt(8)σn2′←(1-ρn′)σn2′+ρn′(zt-μn′)T(zt-μn′)(9)ρn′←δN(zt|μn′,σn′)whereδis the learning coefficient, and represents adaptation time of the model. For all Gaussiansn≠n′, mean and variance are not modified, but:αn←(1-δ)αn. If the test of Eq. (5) fails, pixel is associated to the foreground. The Gaussian with the smallest weight is reinitialized with current measure:αn=δ,μn=zt,σn2=σ¯2, whereσ¯2is a high variance. We also apply those affectations for GMM initialization.GMM has shown good performance for the analysis of outdoor scenes, and has become a very popular BS algorithm. Even if this method is able to handle with low illumination variations, rapid variations of illumination and shadows are still problematic. Furthermore, the learning stage can be inefficient if it is realized with noisy video frames. To tackle these problems, many authors have studied the various ways to improve GMM-based BS.Kaewtrakulpong and Bowden [25] propose to modify the update equations in this model to improve the adaptation of the system to illumination variations. Then, the layer model introduced by Tuzel et al. [41] is based on 3D multivariate Gaussian distributions. With a recursive Bayesian learning approach, they are able to estimate the distribution of the variance and the mean of the model. Chen et al. [10] propose a hierarchical and block-based BS algorithm combining GMM and a contrast histogram. Zivkovic [48,49] automatically computes the correct numbers of Gaussian distributions for each pixel, instead of setting it constant. More than simply use color to describe pixels’ contents, [45] use a combination of features.Recently, some authors have introduced fuzzy concepts in the different steps of the background subtraction process [4]. In [46], the authors perform the background subtraction by the similarity measure of color and texture features of the input image and the background model using Sugeno Integral [39]. Later, [14] got better results using Choquet Integral [11]. Subsequently [1] have used the Choquet Integral with color, edge and texture features. In [34], the authors have proposed a fuzzy function to compute the foreground extraction and to update the background model. To update the background model with Fuzzy Running Average, [34] suggests the use of a linear saturation function instead of the crisp limiter function defined by applying:(10)Sn(x,y)=1ifd(Zt(x,y),Bt-1(x,y))>TZt(x,y)-Bt-1(x,y)Totherwise,where T is the threshold. However, Eq. (10) results in a foreground mask with real values[0,1]. To determine the binary foreground mask, [34] suggests the use of a low-pass filter (LPF) by:(11)Ft(x,y)=1if|LPF(St(x,y))|>T0otherwise.The advantage of the Eq. (11) is the noise reduction in the foreground mask. In [5,16,17], the authors suggest a type-2 fuzzy method to deal with uncertainties of the multimodal background. [35] have used a set of fuzzy rules to improve the detection of moving objects. One could consult [4] for a full review of the fuzzy based approaches in the BS field.Basically the neural network learns to classify each pixel of the image. For each pixel, the neural network determines if one pixel belongs to the foreground or to the background [3]. In [12], the authors have used a multilayered feed-forward neural network with 124 neurons. The background segmentation approach proposed by [12] relies on an adapted PNN (Probabilistic Neural Networks). The background model is learned by neural network while a Bayesian classifier identifies if one pixel belongs to the foreground or to the background. [29] have used a Self-Organizing Map (SOM) network to perform BS. Each pixel has one neural map given by weight vectors. Later, [30] has improved the previous work by adding a fuzzy function in background learning step. The previous authors also apply a spatial coherence analysis on the SOM network of each pixel to enhance robustness against false detections and to deal with decision problems typically arising when crisp settings are involved.BS has also been achieved by many other methodologies. We can cite for example the computation of eigen values and eigen vectors [32] or the combination of histograms and Bayesian inference [18]. An original approach, VuMeter, proposed by [19], a non-parametric model, is based on a discrete estimation of the probability distribution. It is a probabilistic approach to define the image background model. We recall thatZtis an image at time t, andztgives the color vector RGB of a given pixel inZt. A pixel can take two states,ω1if the pixel is background,ω2if the pixel is foreground. This method tries to estimatep(ω1|zt). With 3 color component i (Red, Green, Blue for example), the probability density function can be approximated by:(12)p(ω1|zt=∏i=13p(ω1|zti),with(13)p(ω1|zti)≈ki∑j=1nπtijδ(bti-j),whereδis the Kronecker delta function,btgives the bin index vector associated tozt,jis a bin index, andkiis a normalization constant to keep at each moment(14)∑j=1nπtij=1.πtijis a discrete mass function which is represented by a bin. At first image(t=1), we set bins values,π0ij=1/nto have a sum to 1 like in Eq. (14). At each new pixel, its value match to a binπtij. The level of this bin is updated as follows:(15)πt+1ij=πtij+α·δ(bt+1i-j),whereαis the learning rate. After the learning phase, the bins which are modeling the background have a high value. To choose at each moment if a pixel is background or not, a threshold t is set by an supervised way. Each new pixel with corresponding bins under this threshold will be detected as background. In RGB mode each pixel will be modeled by 3 VuMeter (1 for each color). To consider a pixel as background, it must be detected as background with each VuMeter.The BGSLibrary [36] provides an easy-to-use C++ framework to perform background subtraction. Currently, the library offers 29 background subtraction algorithms. The source code is available under GNU GPL v3 license and the library is free, open source and platform independent. The BGSLibrary also provides one Java based GUI (Graphical User Interface) allowing the users to configure the input video-source, region of interest, and the parameters of each BS algorithm. To use the library, it is necessary to have the OpenCV2http://opencv.org/.2library installed.Fig. 2shows one Windows MFC (Microsoft Foundation Class) application using the BGSLibrary. Table 1shows all algorithms with respective author(s) available in the library. The algorithms are grouped by similarity. The column Method ID is the implementation reference of the method name. Some algorithms of BGSLibrary have been used successfully in [37].The BMC (Background Models Challenge) data set proposed by Vacavant et al. [42] are composed of both synthetic and real videos, representing urban scenes acquired from a static camera. The data set is composed of 20 urban video sequences rendered with the SiVIC simulator [13]. These videos are representations of two scenes: a street and a rotary. We sum up in Table 2the different situations that are generated from these two scenes in the BMC data set. The Learning set is composed of the 10 synthetic videos representing the use case 1. Each video is numbered according to scene number (1 or 2), the presented event type (from 1 to 5), and the use case (1 or 2). For example, the video 311 of our benchmark describes a sunny street, under the use case 1, as illustrated in Fig. 3(top-left).The Evaluation set first contains the 10 synthetic videos with use case 2. This set is also composed of real videos acquired from static cameras in video-surveillance contexts. This dataset has been built in order to test the algorithms reliability during time and in difficult situations such as outdoor scenes. Real long videos (about one hour and up to four hours) are available, and they may present long time change in luminosity with small density of objects in time compared to previous synthetic ones. This dataset allows to test the influence of some difficulties encountered during the object extraction phase. Those difficulties have been sorted according to:1.The ground type (bitumen, ballast or ground).The presence of vegetation (trees for instance).Casted shadows.The presence of a continuous car flow near to the surveillance zone.The general climatic conditions (sunny, rainy and snowy conditions).Fast light changes in the scene.The presence of big objects.Samples of images extracted from those real videos are presented in Fig. 3. For each of these videos have been manually segmented some representative frames that can be used to evaluate a BS algorithm. In the evaluation phase of the BMC data set, no ground truth image is publicly available, and authors should test their BS method with the parameters they have set in the learning phase.In this section, the benchmark and performance evaluation of all BS algorithms are shown. Firstly, the parameter tuning process is presented, followed by the benchmark and performance evaluations.As explained above in Section 3.2, the BMC data set is composed of two distinct sets of sequences: learning and evaluation. The learning videos are used for parameter tuning of the BGSLibrary algorithms. The BMC data set provides the ground truth of each video, where background pixel are set to 0, and foreground to any other value. The foreground masks generated by each BS method were stored, and then the BMC Wizard software3http://bmc.univ-bpclermont.fr/?q=node/7.3has been used to compute all the quality measures. The parameters of each BS algorithm were adjusted manually to get the best measures. Table 3shows the parameter settings of each BS algorithm after the learning phase.When selecting a background subtraction method, it is important to evaluate its memory consumption, execution time and CPU occupancy. In embedded systems or real time applications, these features can be critical. In our experiments, we have used an Intel Core i5-2410M 2.3GHz processor with 4GB DDR3 RAM and Windows 7 Home Premium x64 SP1. In Fig. 4is presented the average occupancy of CPU, RAM and execution time of each BS algorithm with all features normalized. Note that in Fig. 4 there are 26 methods rather than 29 methods available in BGSLibrary. In this experiment the benchmark of T2FGMM method includes T2FGMM_UM and T2FGMM_UV, and the T2FMRF method was not evaluated because its implementation has a memory leak failure4A memory leak is unnecessary memory consumption by a computer program. If a program with a memory leak runs long enough, it can eventually run out of usable memory.4with some very long videos provided by BMC.In general, the basic BS methods are fast (low computational cost), but have a low precision. In our performance evaluation, several criteria have been considered, and represents different kinds of quality of a BS algorithm. We have taken into account the measures computed thanks to the BMC Wizard: F-score, PSNR (Peak Signal–Noise Ratio), SSIM (Structural SIMilarity) [43], and D-Score [28].The Precision, Recall and F-Measure are based on the amount of false positives (FP), false negatives (FN), true positives (TP) and true negatives (TN) as shown in Table 4.We also compute the PSNR, defined by:(16)PSNR=1n∑i=1n10log10m∑j=1m‖Si(j)-Gi(j)‖2whereSi(j)is the jth pixel of image i (of size m) in the sequence S (with length n).We also consider the problem of background subtraction in a visual and perceptual way. To do so, we use the gray-scale images of the input and ground truth sequences to compute the perceptual measure SSIM (Structural SIMilarity), given by [43]:(17)SSIM(S,G)=1n∑i=1n(2μSiμGi+c1)(2covSiGi+c2)(μSi2+μGi2+c1)(σSi2+σGi2+c2),whereμSi,μGiare the means,σSi,σGithe standard deviations, andcovSiGithe covariance ofSiandGi. In our benchmark, we setc1=(k1×L)2andc2=(k2×L)2, where L is the size of the dimension of the signal processed (that is,L=255for gray-scale images),k1=0.01andk2=0.03(which are the most used values in the literature).We finally use the D-Score [28], which consists in considering localization of errors according to real object position. As Baddeleys distance, it is a similarity measure for binary images based on distance transform. To compute this measure we only consider mistakes in BSA results. Each error cost depends on the distance with the nearest corresponding pixel in the ground-truth. As a matter of fact, for object recognition, short or long range errors in segmentation step are less important than medium range error, because pixels on medium range impact greatly on object’s shape. Hence, the penalty applied to medium range errors is heavier than the one applied to those in a short or large range.More precisely, the D-Score is computed by using:(18)D-score(Si(j))=exp(-log2(2.DT(Si(j))-5/2)2whereDT(Si(j))is given by the minimal distance between the pixelSi(j)and the nearest reference point (by any distance transformation algorithm). With such a function, we punish errors with a tolerance of 3 pixels from the ground-truth, because these local errors do not really affect the recognition process. For the same reason, we allow the errors that occur at more than a 10 pixels distance. Details about such metric can be found in [28]. Few local/far errors will produce a near zero D-Score. On the contrary, medium range errors will produce high D-Score. A good D-Score has to tend to 0.To perform the ranking of all BS algorithms evaluated here, in the first place the F-Measure, D-Score and SSIM have been normalized in the range[0,…,1]. The global quality of a BS algorithm is then obtained thanks to the FSD score defined by:FSD(a)=n(FMeasure‾(a))+n(SSIM‾(a))+(1-n(DScore‾(a)))3where a is the BS method,FMeasure‾(a),SSIM‾(a)andDScore‾(a)are the average F-Measure, SSIM and D-Score of the BS algorithm throughout the data set.Table 5shows global score of the BS algorithms. Moreover, Fig. 5shows the F-Measure, D-Score and SSIM values of the top 5 best methods on BMC data set, and Fig. 6shows the F-Measure versus SSIM values. Finally, Fig. 7and Fig. 8display the foreground masks obtained from the top 5 BS algorithms on all sequences of the street scene.In this section we analyze the performance of the best BS algorithms in four specific situations. We evaluate them while moving objects stop suddenly, while sudden light changes occur, when complex cast shadows appear and with moving trees or dynamic background. These four specific situations are very hard to deal and very common in indoor and outdoor environments. All methods are evaluated with their default parameters and we propose then to tune them so that they handle these pathological cases at best. The parameter tuning has been performed experimentally in order to maximize the number of true positives pixels and minimize the number of false positive pixels.In this evaluation we have used one video where a train stops for a certain period of time. Several background subtraction algorithms fail in this situation because the train can be included in the background model while it is updated. Table 6shows the number of true positive (TP), false negative (FN), false positive (FP) and true negative (TN) pixels before and after the parameter tuning. The Fig. 10shows the number of TP pixels versus FP pixels for each algorithm. As it can be observed in Table 6, the number of TP pixels have increased after parameter tuning as expected. However, sometimes the number of FP pixels increases also (see MultiLayerBGS). The best method must have few FP and many TP pixels. In this experiment, the LBAdaptiveSOM and AdaptiveBackgroundLearning algorithms show interesting results, since it is possible to improve their good detection without increasing false ones too much. However, no algorithm was completely able to handle this very hard event in the video, even with an adequate adjustment of their parameters. Figs. 11and 12illustrate the foreground masks obtained from the BS algorithms.In this evaluation we have used one video where a sudden light changes occur. Some BS algorithms fail in this situation due their low noise tolerance. As we can notice in Table 7, the number of TP pixels has increased after parameter tuning as expected and most of FP pixels have decreased. All algorithms (except the PBAS and DPEigenbackgroundBGS) have achieved good results with their default parameters. The PBAS only have achieved a better result after the parameter tuning. In this experiment, the LBAdaptiveSOM and MultiLayerBGS algorithms have shown interesting results, since it has been possible to find a good compromise between increase of TP pixels and the increase of FP ones. Fig. 13shows the number of TP pixels versus FP pixels for each algorithm, Figs. 14and 15show the foreground masks obtained from these BS algorithms.The occurrence of hard and soft shadows is also one of the challenging situations for many BS algorithms. In this evaluation we have used one video where one person walks in a dark environment and the lights produce a large shadow. In Table 8, we can observe that the number of FP pixels have decreased after parameter tuning as expected but the number of TP pixels have decreased slightly. In this experiment, the MultiLayerBGS algorithm have shown interesting results, as in the previous complex cases we have evaluated. It is important to note that MultiLayerBGS algorithm has one parameter called SW (Shadow Rate) specific for the occurrence of shadows. The MultiLayerBGS has successfully discarded the shadow in this experiment. The other methods have achieved only slight better results. For all the tested methods, we have not succeeded to remove the shadow completely without harming the detection of the TP pixels. The Fig. 16shows the number of TP pixels versus FP pixels for each algorithm, Figs. 17and 18present the foreground masks obtained from the BS algorithms.Dynamic backgrounds and moving trees are still an open challenge in BS. In this evaluation we have used one video where moving trees produce a dynamic background. We can notice in Table 9that the number of FP pixels have decreased after parameter tuning as expected but the number of TP pixels have changed slightly for some methods. In this experiment, the PBAS, DPWrenGABGS, LBAdaptiveSOM and T2FGMM_UM algorithms have shown interesting results since a good compromise was found between the increase of TP and FP pixels. The other methods have achieved only a moderate improvement of their quality. Fig. 19shows the number of TP pixels versus FP pixels for each algorithm, while Figs. 20and 21illustrate the foreground masks obtained from these BS algorithms.

@&#CONCLUSIONS@&#
