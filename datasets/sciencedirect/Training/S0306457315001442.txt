@&#MAIN-TITLE@&#
Multi-view clustering via spectral partitioning and local refinement

@&#HIGHLIGHTS@&#
A new multi-view clustering algorithm is proposed.The proposed MVNC algorithm uses spectral partitioning and local refinement.MVNC is compared to state-of-the-art algorithms using three real-world datasets.MVNC significantly outperforms the other algorithms.MVNC is parameter-free unlike existing multi-view clustering algorithms.

@&#KEYPHRASES@&#
Multi-view clustering,Spectral clustering,Local refinement,Normalized cuts,

@&#ABSTRACT@&#
Cluster analysis using multiple representations of data is known as multi-view clustering and has attracted much attention in recent years. The major drawback of existing multi-view algorithms is that their clustering performance depends heavily on hyperparameters which are difficult to set.In this paper, we propose the Multi-View Normalized Cuts (MVNC) approach, a two-step algorithm for multi-view clustering. In the first step, an initial partitioning is performed using a spectral technique. In the second step, a local search procedure is used to refine the initial clustering.MVNC has been evaluated and compared to state-of-the-art multi-view clustering approaches using three real-world datasets. Experimental results have shown that MVNC significantly outperforms existing algorithms in terms of clustering quality and computational efficiency. In addition to its superior performance, MVNC is parameter-free which makes it easy to use.

@&#INTRODUCTION@&#
In many real-world applications, datasets are characterized by multiple sets of features. Web pages and scientific papers are typical examples of such datasets where documents can be represented using not only their textual content but also other modalities such link information. Cluster analysis using multiple representations (or views) of data is known as multi-view clustering and has attracted much attention in recent years (see, e.g., Cai, Nie, & Huang, 2013; Chaudhuri, Kakade, Livescu, & Sridharan, 2009; Greene & Cunningham, 2009; Liu, Wang, Gao, & Han, 2013; Zhao, Evans, & Dugelay, 2014; Zhuang, Karypis, Ning, He, & Shi, 2012). Multi-view clustering seeks to take advantage of the complementarity of views to achieve better clustering performance than when relying on a single view. Bickel and Scheffer (2004), for example, show that exploiting both the textual content of web pages and the anchor text of inbound links improves clustering quality over the use of a single modality; Chikhi, Rothenburger, and Aussenac-Gilles (2008) show that combining text and citation information improves document clustering.To cluster multi-view data using single-view clustering techniques (such as K-means), one has first to combine the available sets of features in an ad-hoc way to form a single view. This can be achieved either by concatenating the sets of features into a single set, or by building a similarity matrix from each view and then computing the overall affinity matrix by averaging the different similarity matrices. In practice, though, these simple combination techniques have been shown to give poor results in comparison to more elaborate techniques such as the convex K-means algorithm described in (Modha & Spangler, 2003). Convex K-means is a generalized version of the classical K-means algorithm which combines views, in a convex fashion, during the assignment step. In the same vein, Zhou and Burges (2007) proposed a multi-view extension of the spectral clustering algorithm of Meila and Shi (2000), where views are combined using a mixture of Markov chains. In (Kumar, Rai, & Daume, 2011), a co-regularized approach to multi-view clustering is presented. Co-regularization consists in introducing constraints in the clustering process to ensure that the clusterings on different views agree with each other. In (Liu et al., 2013), the authors proposed an adaptation of the non-negative matrix factorization technique to work with multiple sets of features. Their algorithm uses a joint factorization process to find a consensus clustering across the views. More recently, Xia, Pan, Du, and Yin (2014) proposed a multi-view spectral algorithm based on Markov chains and noise handling. The basic idea of their algorithm is to combine the transition probability matrices constructed from each view into a shared transition probability matrix via low-rank and sparse decomposition.The major drawback of existing multi-view clustering algorithms is that they have hyperparameters which are difficult to set and which affect significantly the clustering performance. For instance, the convex K-means algorithm (Modha & Spangler, 2003) and the mixture model of Zhou and Burges (2007) use a weighting parameter to balance the importance of each view. The approach of (Kumar et al., 2011) has a co-regularization parameter which trades-off a spectral (dis)agreement term and a spectral clustering objective during the optimization process. There is also a regularization parameter in the multi-view non-negative matrix factorization (Liu et al., 2013) and the robust multi-views spectral clustering (Xia et al., 2014) algorithms.In this paper, we propose the Multi-View Normalized Cuts (MVNC) approach, a parameter free multi-view spectral clustering algorithm. MVNC is described in Section 2. Section 3 describes the experimental environment, while Section 4 reports and discusses the experimental results. Section 5 concludes the paper and gives an outlook to future work.In this section, we present MVNC, a new multi-view clustering algorithm which works in two phases. In the first phase, an initial partitioning is performed using a spectral technique. In the second phase, a local search procedure is used to refine the initial clustering.Given a set of N data pointsX={x1,x2,…,xN},the single-view normalized cut algorithm proposed by Ng, Jordan, and Weiss (2001) partitions X into K clusters by solving the following minimization problem:(1)minU∈RN×Ktr(UTLU),s.t.UTU=Iwhere tr is the matrix trace and L is the normalized Laplacian. Cluster memberships are then obtained by clustering the rows of matrix U using the K-means algorithm.When the dataset X is represented using V different sets of features (i.e. views), the co-regularized multi-view spectral clustering algorithm of Kumar et al. (2011) divides X into K clusters by solving the following joint optimization problem:(2)minU(1),…,U(V)∈RN×K∑v=1Vtr(U(v)TL(v)U(v))+λ∑1≤i,j≤Vi≠jD(U(i),U(j))s.t.U(v)TU(v)=I,∀1≤v≤Vwhere L(v) is the normalized Laplacian constructed from view v, D(U(i), U(j)) is a measure of disagreement between the clusterings of views i and j, and λ is a hyperparameter to be set by the user.If we constrain the clusterings of all views to be identical, i.e.U(1)=U(2)=…=U(V),then Eq. (2) reduces to the following minimization problem:(3)minU∈RN×K∑v=1Vtr(UTL(v)U),s.t.UTU=Ior, equivalently, to(4)minU∈RN×Ktr(UT(∑v=1VL(v))U),s.t.UTU=IThe motivation behind the imposed constraint on U(v), 1 ≤ v ≤ V is twofold. First, it allows us to get rid of the co-regularization parameter λ, since the disagreement term in Eq. (2) vanishes. Second, the optimization problem is simplified, as it involves a single matrix, in contrast to the original co-regularization framework which involves V matrices.Eq. (4) is similar to the single-view spectral clustering problem of Eq. (1), where the Laplacian is formed by the sum of the normalized Laplacians constructed from each view. This suggests that the algorithm of (Ng et al., 2001) can be easily extended to multi-view data. Based on this idea, we propose a new multi-view spectral clustering algorithm. The proposed algorithm, summarized in Algorithm 1, is used in the first phase of MVNC.Spectral clustering is a discrete optimization problem which is known to be NP-hard. Existing spectral clustering algorithms find an approximate solution by adopting the following approach: First, the original intractable (discrete) problem is relaxed into a tractable (continuous) optimization problem; the relaxed problem is then solved, and the resulting continuous solution is discretized using some clustering heuristic (Yu & Shi, 2003).Discretization techniques used by spectral clustering algorithms, including the K-means algorithm used by MVNC in the first phase, do not guarantee that the obtained clustering corresponds to the optimal solution of the original discrete problem. Therefore, in the second phase of MVNC, we propose to use a greedy algorithm to improve the clustering obtained from the first phase.The proposed refinement strategy is similar in spirit to the Kernighan–Lin heuristic (Kernighan & Lin, 1970). It consists in parsing the list of data points in a random order, and then moving each data point to its best cluster, that is, the one which decreases the objective function of Algorithm 1 the most. This procedure is repeated until no further improvement of the objective function can be achieved.The objective function minimized by Algorithm 1 is given by:(5)f(C)=∑v=1V(K−∑k=1Kg(v)(Ck)vol(v)(Ck))withg(v)(Ck)=∑i:xi∈Ckj:xj∈CkSij(v),vol(v)(Ck)=∑i:xi∈Ckj=1→NSij(v),S(v) the similarity matrix constructed from view v, N the number of data points, V the number of views, andC=⋃k=1KCka set of K clusters.Note that this objective function corresponds to the sum of the normalized cut values over the V views, and will be referred to as the overall normalized cut.Moving a data point xifrom its current cluster Cjto another cluster Ckchanges the overall normalized cut value by(6)ΔN(xi,Ck)=f(D)−f(C)=∑v=1V(g(v)(Cj)vol(v)(Cj)+g(v)(Ck)vol(v)(Ck)−g(v)(Cj−{xi})vol(v)(Cj−{xi})−g(v)(Ck∪{xi})vol(v)(Ck∪{xi}))whereC=⋃k=1KCkis the current clustering and D is the clustering obtained by moving xifrom Cjto Ck.Algorithm 2summarizes the main steps of the local refinement technique used by MVNC.Theorem 1Algorithm2is guaranteed to convergeSincevol(v)(Ck)=g(v)(Ck)+∑i:xi∈Ckj:xj∈C∖CkSij(v),we have vol(v)(Ck) ≥ g(v)(Ck).It follows that the objective function f is bounded below by 0.Since f is monotonically non-increasing over iterations and is bounded below, Algorithm 2 is guaranteed to converge.□In practice, we consider that Algorithm 2 has converged when the difference of the overall normalized cut values between two consecutive iterations is below a threshold, or when a maximum number of iterations is reached.In Algorithm 1, steps 1, 4 and 6 are the most time-consuming. Step 1 involves the calculation of V N × N similarity matrices, which costs O(MVN2), where M is the average number of features over the V views. In step 4, K eigenvectors of an N × N symmetric matrix are computed, which costs O(KN2). In step 6, K-means clustering is performed on a K × N matrix, which costs O(IK2N), where I is the number of iterations. The overall time complexity of Algorithm 1 is thenO((MV+K)N2+IK2N).The bottleneck in Algorithm 2 is the evaluation of the effect of moving data points (step 3). This step can, however, be carried out efficiently by avoiding redundant computations. For instance,vol(v)(Cj−{xi})can be easily obtained, sincevol(v)(Cj−{xi})=vol(v)(Cj)−deg(v)(xi),wheredeg(v)(xi)=∑j=1NSij(v). deg(v) is a N-dimensional vector which can be computed at the beginning of the algorithm. Similarly, vol(v)(Cj) can be computed at the beginning of the algorithm, and updated if cluster Cjis modified in step 6.If Algorithm 2 is implemented as suggested, computing the effect of moving a data point from its current cluster to another cluster using Eq. (6) can be performed in linear time with respect to the number of views V since the terms inside the summation can be evaluated in constant time. The cost of step 3 is therefore O(KV), yielding a time complexity of O(JKNV) for Algorithm 2, with J being the number of iterations required for convergence.To evaluate the performance of MVNC, we conducted experiments using three real-world datasets:•Digit dataset: it is a set of images where each image corresponds to a handwritten digit (0–9) (Asuncion and Newman 2007). In this dataset, view-1 corresponds to the Fourier coefficients of the character shapes, and view-2 corresponds to the pixel averages in 2 x 3 windows. The similarity between images in each view is computed using a Gaussian kernel, i.e.,Sij=exp(−∥xi−xj∥22σ2)where∥xi−xj∥is the Euclidean distance between data points xiand xj; σ is the kernel width which, in our experiments, is set to the median distance between data points.Cora dataset: it is a collection of scientific papers, with view-1 being the textual content of documents and view-2 being citation links between documents (Sen et al., 2008). Each document belongs to one of the following categories: case based reasoning, genetic algorithms, learning theory, neural networks, probabilistic learning methods, reinforcement learning, and rule learning. The similarity between documents in each view is computed using the cosine measure, which is better suited to text and link data than the Gaussian kernel (Dhillon & Modha, 2000). The cosine similarity of two documents xiand xjis defined as:Sij=xi·xj∥xi∥∥xj∥where ‖xi‖ is the Euclidean norm of vector xi.3sources dataset: it is a set of news stories collected from 3 popular online news sources (Greene & Cunningham, 2009). Of these stories, 40% were reported in all three sources, 47% in two sources, and 13% appeared in a single news source. Each story is about one of the following topics: business, entertainment, health, politics, sport, and technology. The cosine measure was used to compute the similarity between news articles in each view.Characteristics of the three datasets are summarized in Table 1.To show the effectiveness of the proposed approach, we evaluated and compared the following algorithms:•Best Single View (BSV): Running single-view spectral clustering on each view, and then reporting the results of the view that achieves the best performance.Feature Concatenation (FeatCon): Concatenating the features of all views to form a single representation, and then applying single-view spectral clustering.Average Similarity (AvgSim): Combining different similarity matrices into a single matrix by taking their average, and then performing single-view spectral clustering.Mixture of Markov Chains (MixMC): Using a mixture model whose components are Markov chains constructed from each view of the data (Zhou & Burges, 2007).Convex K-means (ConvKM): It is a multi-view version of the classical K-Means algorithm (Modha & Spangler, 2003).Co-Regularizerd multi-view spectral clustering (CoReg): Adopting a co-regularization approach to multi-view spectral clustering (Kumar et al., 2011).Multi-view Nonnegative Matrix Factorization (MultiNMF): This is a multi-view variant of the Nonnegative matrix factorization algorithm (Liu et al., 2013).Robust Multi-view Spectral Clustering (RMSC): Applying the idea of low-rank and sparse decomposition to multi-view spectral clustering (Xia et al., 2014).Multi-View Normalized Cuts (MVNC): This is our proposed algorithm.MVNC-Lite: It is a “lite” version of MVNC in which no local refinement is performed. This algorithm allows us to study the effect of the local improvement step.In our experiments, all the algorithms were implemented under the Matlab environment. For CoReg, RMSC and MultiNMF, we used the Matlab code available on the websites of A. Kumar11http://www.umiacs.umd.edu/~abhishek/papers.html., Y. Pan22http://ss.sysu.edu.cn/~py/.and J. Liu33http://jialu.cs.illinois.edu/., respectively.Clustering evaluation using a gold standard dataset consists in assessing how well the output of a clustering algorithm matches the gold standard classes. In the literature, many measures have been proposed to quantify this matching. In our experiments, we used three indices: purity (PUR), F-measure (FM) and normalized mutual information (NMI).The purity of a single cluster is the fraction of member points belonging to the dominant class in that cluster. The purity of a clustering is obtained by computing the weighted average of the cluster-wise purities (Manning, Raghavan, & Schtze, 2008):Purity=∑ininPiwhere n is the total number of data points, niis the size of cluster i, and Piis the purity of cluster i.The F-measure is defined as the harmonic mean between precision and recall (Manning et al., 2008):F−measure=2×precision×recallprecision+recallThe NMI is an information theoretic measure for the mutual dependence of two random variables. In the context of clustering evaluation, the two random variables correspond to the clustering and the gold standard classification. Given a clustering X and a gold standard classification Y, the NMI between X and Y is defined as (Strehl & Ghosh, 2002):NMI(X,Y)=H(X)+H(Y)−H(X,Y)H(X)×H(Y)where H(X) and H(Y) are the entropy of X and Y, respectively; H(X, Y) is the joint entropy of X and Y.

@&#CONCLUSIONS@&#
