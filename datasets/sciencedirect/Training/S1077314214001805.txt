@&#MAIN-TITLE@&#
Online action recognition using covariance of shape and motion

@&#HIGHLIGHTS@&#
A descriptor for describing action in terms of shape and motion is proposed.The descriptor is employed in the online action recognition scenario.Two runtime optimizations are introduced to enable applicability in real time.The method is simple, fast and accurate enough to be of practical value.

@&#KEYPHRASES@&#
Online action recognition,Gesture recognition,Covariance descriptor,Incremental covariance computation,

@&#ABSTRACT@&#
We propose a novel approach for online action recognition. The action is represented in a low dimensional (15D) space using a covariance descriptor of shape and motion features – spatio-temporal coordinates and optical flow of pixels belonging to extracted silhouettes. We analyze the applicability of the descriptor for online scenarios where action classification is performed based on incomplete spatio-temporal volumes. In order to enable our online action classification algorithm to be applied in real time, we introduce two modifications, namely the incremental covariance update and the on demand nearest neighbor classification. In our experiments we use quality measures, such as latency, especially designed for the online scenario to report the algorithm’s performance. We evaluate the performance of our descriptor on standard, publicly available datasets for gesture recognition, namely the Cambridge-Gestures dataset and the ChaLearn One-Shot-Learning dataset and show that its performance is comparable to the state-of-the-art despite its relative simplicity. The evaluation on the UCF-101 action recognition dataset demonstrates that the descriptor is applicable in challenging unconstrained environments.

@&#INTRODUCTION@&#
Automatic recognition of actions performed by humans is a challenging task with applicability to video surveillance, video summarization, natural user interface and gaming. Typically, a distinction is made between “action” and “activity” [1]. Action is an atomic and self contained motion performed by a single subject, e.g., jumping, hand waving, whereas an activity is a series of actions bearing a more complex meaning typically involving more than one subject, e.g., a team scoring a goal in a soccer game. In this work we consider the problem of action recognition. Even more specifically, we are interested in interactive applications such as natural user interface (NUI) and gaming, where actions are to be detected and classified on the fly and the system response time is of great importance.Action recognition has been extensively studied during the past two decades [1]. In the early works on action recognition the body or the hand appearance were learned and later recognized in video sequences. The action’s temporal aspect was handled by training a parametric model such as the Hidden Markov Models (HMM) [2,3]. Later on the community switched its attention to nonparametric approaches describing actions in terms of shape and motion patterns. Bobick and Davis [4] represent actions using Motion History Image (MHI) temporal templates, a weighted sum of binary silhouette masks, describing the action as a shape evolution in time. This representation was found to be discriminative enough for several simple action classes such as aerobics exercises. Blank et al. [5] use silhouette masks to represent an action as a space–time volume and extract features using the Poisson equation to compare these volumes. The major drawback of using shape is the assumption of availability of the silhouette masks, commonly extracted using background subtraction. To overcome this issue, Efros et al. [6] suggest motion descriptors based on optical flow computed over the entire bounding box in low resolution video sequences. Ali and Shah [7] derive kinematic features from optical flow and use them to represent spatio-temporal volumes in low dimensional space. Zelnik-Manor and Irani [8] use a nonparametric descriptor, based on normalized spatio-temporal gradient histograms extracted in different temporal scales and successfully apply it for action detection and clustering.Following their success in still images, several extensions of local descriptors [9] to the spatio-temporal case were employed for action recognition. Laptev and Lindeberg [10] use an extension of Harris interest points to represent and recognize actions. Dollar et al. [11] learn a dictionary of spatio-temporal interest points and represent the action as a bag-of-words over the learned dictionary. Niebles et al. [12] use a similar approach, combining the bag-of-words approach with probabilistic graphical models.Several recent works use tensors to represent space–time volumes. Kim and Cipolla [13] extend the Canonical Correlation Analysis (CCA) to tensors and use it to measure the similarity between two video volumes avoiding an explicit motion estimation. Lui et al. [14] represent the video volume tensor as a point on a product manifold and classify actions using the nearest neighbor in this space. In a follow up work [15] the authors employ least squares regression achieving impressive results in application to gesture recognition including the challenging problem of one-shot-learning [16], where the action class has to be recognized based on a single action instance.The covariance descriptor [17,18], originally designed for object detection, was recently successfully employed for action recognition [19–21]. Guo et al. [19] use the covariance descriptor in kinematic feature space [7] to represent spatio-temporal volume. In their work on pedestrian detection, Tuzel et al. [18] show that covariance descriptors lie in a nonlinear Riemannian space. Harandi et al. [20] use a Riemannian Locality Preserving Projection (RLPP) technique to map the covariance descriptors into the euclidean space and to apply standard classification methods. In a follow up work by Sanin et al. [21] the RLPP was used with AdaBoost to learn a set of covariance descriptors giving impressive results in classification of a specific set of actions. A significant drawback of this approach is its learning phase making it inapplicable to the one-shot-learning scenario.While classical action recognition is typically concerned with classifying actions using a full video volume, the goal of online action recognition is to classify actions on the fly. Albanese et al. [22] propose a system for online complex activity recognition using a specially designed logical language to describe the activity. In [23] the authors present a method for low latency online action recognition based on detecting distinctive canonical poses in the 3D motion of the subject’s skeleton.In this work we present an efficient descriptor for action recognition and propose an algorithm for online action detection and recognition applicable in natural user interface and gaming scenarios. Motivated by the prior art [4–7], we use the silhouette shape and motion properties to derive our action descriptor. Fig. 1(a–c) show a frame from a video capturing an action, a corresponding silhouette and the computed optical flow vector field. The optical flow [24] is computed between the current and the consecutive frames assuming translation only. Fig. 1(d and e) depict heat maps representing the optical flow’s horizontal and vertical components. The hotter (more red) the pixel is, the more dynamic it is in terms of its horizontal or vertical velocity. Our action descriptor is based on the heat maps’ evolution over time. We represent each pixel belonging to the silhouettes’ series as a vector of its spatio-temporal coordinates and velocity. The action is represented as an empirical covariance matrix of such vectors [17]. Although the resulting descriptor is of low dimensionality (15 unique values only), we show experimentally that its discriminating capabilities are comparable and sometimes favorable to much more complex descriptors used by the state-of-the-art algorithms.Our approach differs from previous works in several important aspects:1.We use an extension of the covariance descriptor to the temporal domain to represent a spatio-temporal volume in an exceptionaly low dimensional (15D) space. We analyze the applicability of the descriptor to the online scenario where action classification is performed based on incomplete spatio-temporal volumes.We demonstrate that rough statistics of shape and motion (spatio-temporal coordinates and optical flow), computed on pixels belonging to regions of interest, provide a sufficiently rich representation for human body action and hand gesture classification. This is in contrast with [13] which uses a much richer method of Canonical Correlation Analysis (CCA), to represent the statistics of the volume and [7,19] who use more complex kinematic features based on optical flow.Unlike [14,13] we do not assume a fixed number of frames in the spatio-temporal volume. Also, our approach does not rely on extensive learning procedures applied, for example, in [21,20,13]. Thanks to this our approach may be applied in the challenging one-shot-learning scenario [16].Our online action recognition algorithm uses an incremental covariance descriptor computation and on demand nearest neighbor classification, which significantly improve its performance and make it suitable for real time applications. Finally, the algorithm is relatively simple, easy to implement and computationally efficient.Additional contributions of this work are the experimental validation of our approach on two standard, publicly available datasets for gesture recognition, on one publicly available dataset for action recognition and on our own private dataset for online action recognition. We propose several quality measures for action recognition in the online scenario, which differ from those usually used in the classical action categorization, and use them for the evaluation of our algorithm. We will make both our dataset and the code available for public use.We will now define the CovAct, the covariance descriptor for action recognition, and show its applicability for online action recognition. LetV={It}t=1Mdenote a sequence of M video frames constituting an action and letS={st}t=1Mdenote a corresponding sequence of M binary silhouette masks of the figure performing the action, e.g., Fig. 1(b). Let(Lt,Tt,Wt,Ht)denote thest’s bounding box, where(Lt,Tt)are its top left corner coordinates and(Wt,Ht)are its width and height. We normalize the spatio-temporal coordinates(x,y,t)of each point insidestrelative to the bounding box size and the action’s duration(x′,y′,t′)=x-LtWt,y-TtHt,tM. This normalization step provides partial invariance to object-camera distance and action duration. Besides the normalized spatio-temporal coordinates, for each point(x,y)belonging tost, we compute its Lucas-Kanade [24] optical flow(ut(x,y),vt(x,y))usingItandIt+1, assuming translation only. Fig. 2(a and b) show an example of normalized horizontal and vertical optical flow heat maps corresponding to the frame in Fig. 1. Thus, each pixel(xi,yi,ti)belonging to the silhouette sequence S of the spatio-temporal volume V is represented as a 5-dimensional vector:(1)zi=xi′,yi′,ti′,uti(xi,yi),vti(xi,yi),wherexi′,yi′,ti′are the normalized spatio-temporal coordinates and(uti(xi,yi),vti(xi,yi))are the horizontal and vertical optical flow components. Only pixels belonging to the silhouettes and having optical flow greater than zero are represented:(2)Z={zi|(xi,yi)∈sti,(uti(xi,yi),vti(xi,yi))2>0}.We label the observations in Z according to their normalized spatial origin – each observationziis labeled with(3)li=1,xi′⩽12,yi′⩽122,xi′>12,yi′⩽123,xi′⩽12,yi′>124,xi′>12,yi′>12,differentiating between observations generated by the four parts of the silhouette (see Fig. 2(d)).The purpose of region covariance descriptor, first introduced in [17], was for object recognition in still images. Since then, the covariance descriptor has proved useful also for representing spatio-temporal volumes [19,21]. We employ the covariance descriptor for representing shape and motion statistics of a spatio-temporal buffer of video frames.In the case of a still image, each pixel is represented by a point in feature space. Possible features are the spatial coordinates of the pixel, its color and gradients, just to name a few. An image region consisting of m pixels is described using the covariance matrix of their feature representationsZ={zi}i=0m:(4)CZ=1m-1∑i=1m(zi-μ)(zi-μ)T,whereμ∈Rnis the mean value ofzisand n is the number of features. An image is described by a set of covariance matrices, of sizen×n, each corresponding to an image region. Covariance matrices are compared using a metric for covariance matrices [25]:(5)d(C1,C2)=∑i=1nln2λi(C1,C2),whereλi(C1,C2)are the generalized eigenvalues ofC1andC2, both of sizen×n, computed fromλiC1ui=C2ui, whereui≠0are the generalized eigenvectors. Intuitively, the metric for covariance matrices measures the extent to which the matrixC1-1C2deviates from the identity. For a pair of equal matricesC1andC2,d(C1,C2)=0since the logarithms of all eigenvalues of I are zero.Our action descriptor is based on the region covariance descriptor extended to the spatio-temporal domain. LetZkdenote a set of observations in Z generated by part k,Zk={zi|zi∈Z,li=k}. Similarly to [17] we represent the action with covariance matrices computed on observations generated by five overlapping regions:(6)A(Z)=CZ,CZ1∪Z2,CZ3∪Z4,CZ1∪Z3,CZ2∪Z4.The proposed representation encodes the correlation of measurements extracted from different regions.The group of covariance (symmetric positive definite) matrices may be represented as a connected Riemannian manifoldM. In this case, the distances between a pair of pointsC1andC2on the manifold is defined as the geodesic distance between these points, and is computed using the Affine-Invariant distance metric [26]:(7)dAI(C1,C2)=logC1-12C2C1-12F=tracelog2C1-12C2C1-12,where‖·‖Fis the matrix Frobenius norm andlog(·)is the matrix logarithm operator. LetC=UΛUTdenote the eigenvalue decomposition ofC∈M, then the matrix logarithm operator maps C to a point on a tangent (Euclidean) space, and is defined as:(8)log(C)=Ulog(Λ)UT,wherelog(Λ)is a diagonal matrix with the logarithms of the eigenvalues of C. It turns out that the metric in Eq. (5), proposed by [25], is actually equivalent to the one from Eq. (7), proposed by [26].One of the drawbacks ofdAIis its relatively high computational burden, especially for high dimensional feature spaces. To overcome this issue, the Log-Euclidean metric was proposed in [27]:(9)dLE(C1,C2)=log(C1)-log(C2)F.This metric maps both covariance matrices from the Riemannian manifoldMto the tangent Euclidean spaceTIat point I on the manifold using the logarithm operator. Besides the difference in several theoretical aspects (see [26,27] for details), thedLEand thedAImetrics differ in some practical aspects as well. First,dLEis typically faster since it does not compute generalized eigenvalues asdAIdoes. Second, mapping the covariance descriptors to the Euclidean space opens possibilities to employ various general machine learning techniques, e.g., [28,29]. We have evaluated both metrics in terms of computational efficiency and recognition accuracy while comparing the proposed descriptors. Our experiments showed thatdLEis roughly two times faster compared todAI, but the speedup comes at the expense of a slight degradation in the recognition accuracy. Therefore, despite the advantages and considering the low dimensionality of our descriptor, we decided that the modest gain in the computational efficiency does not justify the preference ofdLEoverdAI. We note that similar results were reported in [29] regarding the speed versus accuracy for these two metrics.We suggest three possible distance measures for the comparison between action descriptors. The first measure compares only theCZof both actions, thus disregarding the observations’ spatial origin while the second measure compares pairs of covariance matrices generated using observations of similar spatial origin. LetA1=Ci1i=15andA2=Ci2i=15denote two action descriptors as defined in Eq. (6). The two measures are defined as:(10)D1(A1,A2)=dC11,C12,and:(11)D2(A1,A2)=∑i=25dCi1,Ci2,whered(·,·)is the metric for covariance matrices (Eq. (5)). The sum in Eq. (11) starts withi=2becauseC1corresponds to the covariance descriptor of the entire region. The third distance measure uses the whole set of covariance matrices of A. It combinesD1andD2in a way which gives more robustness to partial occlusions and to silhouette imperfections. Similarly to [17], we define the third measure as:(12)D(A1,A2)=D1(A1,A2)+D2(A1,A2)-maxjdCj1,Cj2.The purpose of the subtracted element is to compensate for an accidentally large distance computed between a certain pair of covariance descriptors ofA1andA2which may be due to partial occlusion or a missing silhouette part in one of the descriptors. In our experiments we have found that the distance measure defined in Eq. (12) gives the best performance in action categorization tasks. However, for online action recognition task,D1is preferable because of its lower computational cost.An important property of our approach is its extendability to the online scenario of action recognition. In addition to its computational efficiency this fact enables it to be successfully applied in real time interactive applications such as gaming and natural user interface. The challenge in the online action recognition scenario is that the decision regarding whether a specific action is taking place at the moment has to be made based on an incomplete information available up until that moment. Making the decision as soon as possible is crucial because it immediately affects the system’s behavior.A useful property of the CovAct descriptor is that its dimension does not depend on the dimensions of the underlying spatio-temporal volume. That is to say, two descriptors, one extracted from a complete spatio-temporal volume describing an action and the second one extracted from only the first temporal half of this volume may be naturally compared. This is in contrast with methods reported in [13,14] where the volumes have to be normalized to a fixed temporal size. This observation allows us to apply the nearest neighbor classifier on descriptors extracted from different spatio-temporal volumes. To see the covariance descriptors’ behavior, we have used spatio-temporal volumes of gradually increasing temporal length (incremented one frame at a time), to extract a set of covariance matrices belonging to five different action types. The actions we used were punch, kick, bend forward, bend left and bend right and were performed by a single actor while playing an interactive video game. All covariance matrices in the set have5+(5×4)/2=15unique values due to symmetry. We have applied principal component analysis (PCA) on these 15D vectors. Fig. 3depicts the cumulative energy as a function of the number of principal components. It can be seen from the graph that two major principal components contain around 80% of the overall energy. Fig. 4shows clusters of trajectories for each one of the five actions plotted in 2D using the two major principal components. Each trajectory represents the descriptor’s transformation with the accumulation of frames in the underlying spatio-temporal volume. Each trajectory begins with “∘” and ends with “□” corresponding to action’s complete covariance descriptorCZ. It can be noticed from the 2D illustration that the trajectories belonging to each action class tend to cluster together, and diverge in different directions as more frames are used. Since adding more principle components will intensify this divergence we conclude that in the original 15D space the trajectories are clustered as well.Another interesting observations is that the trajectories’ intermediate points corresponding to descriptors extracted from incomplete information, at some point in time, tend to be closer to the trajectories’ end points belonging to the same class than to those belonging to different classes. To support this intuition, we used Eq. (10) to compute the distances between covariance descriptors extracted from complete and incomplete spatio-temporal volumes. LetCiX%denote a covariance descriptor computed using theX%of the ith spatio-temporal volume, and leta(Ci)denote its corresponding action class. We define the distribution of distances between descriptors belonging to the same class, extracted from volumes of X’s degree of completeness and those extracted from the entire volumes (100%-complete), aspsameX%D1CiX%,Cj100%|i≠j,a(Ci)=a(Cj). Similarly, we define the distribution of distances between descriptors belonging to different classes aspdiffX%D1CiX%,Cj100%|a(Ci)≠a(Cj). Fig. 5shows the distributionspsameX%andpdiffX%in green and red respectively, forX=25,50,75,100. As expected the modes separation increases with the increased information completeness. Note that while it may be difficult to correctly predict the action based on 25% of the information, it is much easier based on 50%. Thus, the distance between a descriptor extracted online, based on an incomplete spatio-temporal volume, to those extracted during the training phase, based on complete volumes, may serve as a predictor on what action is currently being performed. Thanks to this, we avoid the explicit modeling of actions’ fragments, significantly reducing the number of exemplar descriptors required for the nearest neighbor classifier. On the other hand, if necessary, we may seamlessly augment the exemplars set with descriptors extracted from incomplete spatio-temporal volumes, at the price of runtime performance.Based on the above derivations we propose an algorithm (summarized in Algorithm 1) for online action recognition. The algorithm uses a temporal sliding window ofLwframes to buffer measurements for covariance descriptor computation. For each new frame the data buffer is updated, a new covariance matrixCtis calculated and compared with exemplar matrices collected during training. The distance to the nearest neighbor is used to detect whether or not the action of the nearest neighbor’s class took place. Rather than applying a threshold on the absolute distance to the nearest neighbor we have found it useful to apply it on the ratio between the nearest and the farthest neighbor. Let(Ci,ai)i=1Ndenote the descriptors stored in the database together with their class labels taking values fromA={a1,…,aNa}. Letdtmaxdenote theCt’s distance to the farthest neighbor –dtmax=maxi=1…Nd(Ct,Ci). Letγadenote the threshold on the nearest-to-farthest distance ratio for each classa∈A. Thus, a covariance descriptorCtis classified as belonging to class a ifC∗=argmini=1…Nd(Ct,Ci)belongs to class a and:(13)d(Ct,C∗)⩽thta=dtmaxγa,whered(·,·)is the covariance distance defined in Eq. (5).Algorithm 1Online action recognition (CovActOL)Input: Video framesV={It}t=0M, maximum buffer sizeLw,action descriptors databaseDB={(Ci,ai)}i=1N(ai∈A),nearest-to-farthest distance ratio threshold for each action class{γa|a∈A}1. InitializeZ=∅.2. For t=1:M2.1. Extract silhouette maskst.2.2. Compute LK optical flow(ut,vt)using framesIt,It-1.2.3. BuildZtas described in Eq. (2).2.4.Z=Z∪Zt.2.5. Ift⩾Lw2.5.1.Z=Z⧹Zt-Lw.2.5. Compute covariance matrixCtofZ.2.6. For i=1:N2.6.1. Computed(Ct,Ci)using Eq. (5).2.7.i∗=argminid(Ct,Ci).2.8. Ifd(Ct,Ci∗)<thai∗t, Output “Actionai∗”.2.9. Else, Output “No Action”.One of the deficiencies of the CovActOL algorithm as described in Algorithm 1, is that its runtime depends on the buffer sizeLwand on the database size N. This dependency is because for each frame we need to recompute the covariance descriptor and to compute its distance to each one of the descriptors in the database. In this section we outline two modifications to the CovActOL algorithm dealing with each one of these deficiencies independently, with no impact on the algorithm’s correctness. In the first modification we show that rather than recomputing the descriptor for each frame we can use a simple and efficient update rule. In the second modification we use the fact that the descriptors computed in the consecutive frames are not likely to be very different one from each other, and thus the distances between them and the descriptors in the database are not likely to change much as well. Using this observation and the properties of the metric for covariance matrices, Eq. (5), we derive a criterion which allows us to skip over the distance computations which are certainly not going to affect the classifier’s final decision.In the online scenario the temporal sliding window is of a fixed size, and so for each new frame the data buffer has to be updated, namely observations extracted from the new frame should be appended while observations of the oldest frame should be dropped. However, the requirement of recomputing the covariance matrix at each iteration induces a significant computational burden, especially if a large number of pixels are extracted from each frame. To overcome this issue we propose a simple and efficient update rule for incrementally computing the covariance descriptor instead of recomputing it entirely after each frame.LetDt-1=∪i=1LwZidenote a set of observations, currently in the buffer, extracted from theLwframes preceding framet-1, whereLwis the buffer size andZi=xjij=1Nidenotes a set of observations extracted from framet-1-i. LetNiandNt-1denote the number of samples inZiandDt-1respectively. LetμiandCidenote the mean and the covariance ofZi, andMt-1andCt-1denote the mean and the covariance ofDt-1. As a new frame, t, arrives, its observations are appended to the buffer:Dt=Dt-1∪Zt. If the buffer is full,1By full buffer we mean that it contains observations extracted fromLwlast frames.1the least recent frame’s observations are dropped:Dt=Dt⧹Z1. Thus, the number of observations inDtisNt=Nt-1-N1+Nt+1, whereN1=0when the buffer is not full. Our goal, therefore, is to efficiently compute the new meanMtand the covarianceCtof samples inDtusingMt-1andCt-1, without explicitly recomputing them from the dataDt. In Sections A and B we derive these efficient update rules forMtandCt, which are:(14)Mt=1NtNt-1Mt-1-N1μ1+Ntμt,and(15)Ct=1Nt-1Nt-1-1Ct-1+ΔCt,where(16)ΔCt=-(N1-1)C1+(Nt-1)Ct+Nt-1Mt-1Mt-1T-N1μ1μ1T+NtμtμtT-NtMtMtT.Thus the update takesONt2operations instead of theONt2operations, which would be the case had we explicitly recomputed the covariance from the data for each frame. For example, ifLw=30, then we improve the descriptor computation speed by a factor ofLw2=900. We note that a similar approach was used in [30] to improve the performance of real time tracking using the covariance descriptor. The authors used an efficient update rule to build an adaptive appearance model of the tracked target.Using m recursive expansions of expression Eq. (15), we getCtas a function ofMt-mandCt-m, andMt-iandCt-icomputed from{Zt-i}i=0m-1. (see Section B for details):(17)Ct=1Nt-1Nt-m-1Ct-m+∑i=0m-1ΔCt-i.Letd(Ct-m,C′)denote the distance between the covariance descriptorCt-m, computed in the(t-m)’th iteration of the CovActOL algorithm(m<t), and some covariance descriptor in the database, C′. Let us also assume that C′ belongs to class a andd(Ct-m,C′)-tht-ma⩾0, wheretht-mais as defined in Section 2.4.1. Thus, action a was certainly not detected based on theCt-m’s proximity to C′ in iterationt-m. We would like to define a condition which, when held, guarantees thatd(Ct,C′)-thta⩾0, meaning that action a is not detected based on theCt’s proximity to C′ in iteration t, without explicitly computing the distance. Note that the thresholdstht-maandthtadepend on the distances to the farthest neighbor in iterationst-mand t respectively. However, since the farthest neighbor’s distance in iteration t could not shrink by more thand(Ct,Ct-m)relative to the farthest neighbor’s distance in iterationt-m, we can say thatdtmax⩾dt-mmax-d(Ct,Ct-m), and thus:(18)thta=dtmaxγa⩾γadt-mmax-d(Ct,Ct-m)=tht-ma-γad(Ct,Ct-m).The triangle inequality holds for the covariance distance and therefore:(19)d(Ct,C′)⩾d(Ct-m,C′)-d(Ct,Ct-m).Using the inequalities (18) and (19), we obtain the following inequality:(20)d(Ct,C′)-thta⩾d(Ct,C′)-tht-ma+γad(Ct,Ct-m)⩾d(Ct-m,C′)-d(Ct,Ct-m)-tht-ma+γad(Ct,Ct-m)⩾d(Ct-m,C′)-tht-ma-(1-γa)d(Ct,Ct-m).This lower bound allows us to derive a criterion to whether or not we should recompute the distanced(Ct,C′)in iteration t for some C′ in the database, or we can say for sure that it is above the threshold. Also, we definemmaxindicating the maximum number of frames allowed to skip the distance computation. Thus, in iteration t we require a computations ofd(Ct,C′)only if(21)m>mmaxOrd(Ct-m,C′)-tht-ma-(1-γa)d(Ct,Ct-m)<0,otherwise we conclude thatd(Ct,C′)>thtawithout the actual computation ofd(Ct,C′). In our experiments we have found thatmmax=Lw/2, whereLwis the sliding window length, provides a good choice in terms of performance.In iteration t we evaluate the criterion from Eq. (21) for each one of the N descriptors in the database. The criterion computation involves at mostmmaxcovariance distance computations to compareCtwith each one of theCt-m,m=1,…,mmax. Also once in a while, when the criterion does not hold, the distance computation comparingCtwithC′is performed. Thus, the overall number of covariance distance computations per frame is reduced from N, as in the CovActOL algorithm, tommax+pN, where p is the probability that the condition in Eq. (21) does not hold. According to our experiments,p≈0.1whenLw=30is used. This is much better considering that typicallyN≫mmax. Experimental validation showed that about 70–90% of the covariance distance computations are saved when using the criterion.We have modified the CovActOL algorithm based on the improvements described in Sections 2.5.1 and 2.5.2. The resulting algorithm, Fast-CovActOL, is summarized in Algorithm 2. The algorithm uses a temporal sliding window ofLwframes to extract the CovAct descriptor. For each new frame t the CovAct descriptorCtis computed using the updated rule in Eq. (15), and is compared with the exemplar descriptors meeting the criterion (inequality (21)), using the covariance distance Eq. (5). Action a is detected ifCt’s nearest neighbor is of class a, and its distance is smaller than the threshold,thta. We present an experimental validation of the algorithm in Section 3.1.Algorithm 2Fast online action recognition (Fast-CovActOL)Input: Video framesV={It}t=0M, maximum buffer sizeLw,action descriptors databaseDB={(Ci,ai)}i=1N(ai∈A),nearest-to-farthest distance ratio threshold for each action class{γa|a∈A}1. Initializemi=0,∀i=1,…,N2. For t=1:M2.1. Extract silhouette maskst.2.2. Compute LK optical flow(ut,vt)using framesIt,It-1.2.3. Build Z as described in Eq. (2) withti′=t.2.4. Compute the meanμtand the covariance matrixCtof Z (Nt=|Z|).2.5. UpdateCtusing Eq. (15).2.6. For i=1:N2.6.1. Ift=1Ormi>mmaxOrd(Ct-mi,Ci)-tht-mia-(1-γai)d(Ct,Ct-mi)<02.6.1.1. Computed(Ct,Ci)using Eq. (5).2.6.1.2.mi=02.6.2. Else2.6.2.1.d(Ct,Ci)=d(Ct-mi,Ci)2.6.2.2.mi=mi+12.7.i∗=argminid(Ct,Ci).2.8. Ifd(Ct,Ci∗)<thai∗t, Output “Actionai∗”.2.9. Else, Output “No Action”.

@&#CONCLUSIONS@&#
We have presented a method for action recognition for the online scenario. The method recognizes actions on the fly by applying a nearest neighbor classifier on descriptors extracted from an incomplete spatio-temporal volume available at the moment. The proposed descriptor encodes the statistics of temporal shape and motion changes in a low dimensional space using the covariance descriptor. The method was successfully validated on a privately collected dataset, using quality measures, e.g. latency, especially designed for the online scenario. The descriptor was evaluated on standard gesture recognition datasets showing performance comparable to the state of the art, despite its relative simplicity and compactness. Our fast online action recognition algorithm exploits two computational speed-ups, namely the incremental covariance update and the on demand nearest neighbor computation, to meet real time requirements.In our method we use the Affine-Invariant distance metric to compare covariance matrices. As noted in Section 2.2, this metric gave us better results than the Log-Euclidean metric, in terms of accuracy. That said, the advantage of the Log-Euclidean metric is that it can be seen as a projection of points on the manifold to the Euclidean space, followed by the regular Euclidean metric computation. This may improve the accuracy beyond what we have obtained with the nearest neighbor classifier combined with the Affine-Invariant metric, using standard machine learning techniques. One of the possible extensions of this work could be deriving a more suitable classifier for our problem using either the Riemannian kernel [28] or the Riemannian pseudo-kernel [20].Considering group actions is another possible direction for future work. The required adjustments depend on the type of the action. If the action is a collective motion of a group of people in some region of interest such as the case with the crowd actions, e.g., demonstrations, parades, concerts, the CovAct descriptor may be employed as is. For more complex scenarios involving a structured interaction between several actors, the method has to be adapted according to the available information. For example, if we are given the bounding boxes of each actor we may describe the motion of each one of them separately, and represent the action as a collection of such descriptors. Otherwise, if we are given a probability distribution of each actor’s position inside the region of interest, we may still model actors separately by employing soft association of samples to actors based on the given distribution. Finally, if no additional information is given we may represent the complex action as a collective motion, and for moderately complex actions, e.g., a mutual handshake, this may be just enough.