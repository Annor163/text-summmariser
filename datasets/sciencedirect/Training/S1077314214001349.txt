@&#MAIN-TITLE@&#
Background subtraction for the moving camera: A geometric approach

@&#HIGHLIGHTS@&#
We present a geometric approach to background subtraction for freely moving cameras.We introduce a 2.5D background model that describes the background scene.The algorithm does not rely on restrictions on camera motions or scene geometry.Moving objects are detected in complex scenes with significant camera motion.

@&#KEYPHRASES@&#
Background subtraction,Object detection,Camera motion,View geometry,

@&#ABSTRACT@&#
Background subtraction is a commonly used technique in computer vision for detecting objects. While there is an extensive literature regarding background subtraction, most of the existing methods assume that the camera is stationary. This assumption limits their applicability to moving camera scenarios. In this paper, we approach the background subtraction problem from a geometric perspective to overcome this limitation. In particular, we introduce a 2.5D background model that describes the scene in terms of both its appearance and geometry. Unlike previous methods, the proposed algorithm does not rely on certain camera motions or assumptions about the scene geometry. The scene is represented as a stack of parallel hypothetical planes each of which is associated with a homography transform. A pixel that belongs to a background scene consistently maps between the consecutive frames based on its transformation with respect to the “hypothetical plane” it lies on. This observation disambiguates moving objects from the background. Experiments show that the proposed method, when compared to the recent literature, can successfully detect moving objects in complex scenes and with significant camera motion.

@&#INTRODUCTION@&#
Background subtraction is the process of detecting objects (foreground) residing in the static scene (background). Generally speaking, background subtraction involves building a scene representation referred to as the background model, which is compared against incoming video frames to detect the objects. In the case when camera moves, it is important to compensate the camera motion, such that the background model and a new frame are registered. This requirement imposes significant constraints on the types of motions the camera can undergo.A typical setting commonly adopted by many in the field is when the camera is stationary. In this setting, the static background pixels maintain their locations, such that the transformation from the incoming frame to the background model is an identity transformation, which geometrically implies that the static scene can be considered a single 3D plane. There are too many papers to cite in the literature that assume a stationary camera [1–3]. These papers and many others employ similar statistical models to represent the color distributions of pixels which include parametric [1,2] and non-parametric models [3]. There are also a few studies that perform background subtraction for stationary cameras but with dynamic backgrounds [4–10]. For more detailed discussion on background subtraction for stationary cameras, we refer the reader to comprehensive surveys in [11–13].Another typical but less studied camera setting is when the camera is restricted to only pan, tilt and zoom, such that the optical center of the camera does not move [14–17]. This limited motion imposes a rotational homography, which is also known as the plane projective transformation, between the background model and the incoming frame. Similar to the stationary camera, this setting lets construction of a panoramic background model that has a wide scene coverage. The moving object detection proceeds by first registering the video frame to the panoramic background model and employing the background subtraction with this registered model.When the optical center of the camera moves, the one-to-one mappings between the background model and the video frame can no longer be computed. In fact, the image regions that belong to different planes in the 3D scene create parallax and require different transformations and background models. This requirement is emphasized when the 3D scene contains significant depth variations. In order to address this limitation, several methods assume existence of a dominant scene plane with wide spatial coverage [18–22]. These so-called plane+parallax methods register images to the dominant plane and use the epipolar constraint [18,19], shape constancy constraint [20] or the structure consistency constraint [21] to resolve incorrect registrations due to parallax and object motion. Authors of [22] handle parallax by applying robust statistics [23] to motion estimation [24].While plane+parallax methods perform well for scenes that can be approximated by a single plane, such as in aerial imagery, difficulties arise in cases when the scene contains more than one dominant plane. For such cases, researchers resort to estimating a set of physical planes in the scene [25–29]. In context of background subtraction, Jin et al. [30] adopt the multi-plane representation and generate different background models for each scene plane. Given a sequence of images, the scene planes and respective homography transformations are estimated by applying a cascade of RANSAC steps. This process assumes that the scene planes contain a high number of matching salient points1We refer to images of static scene points as salient points.1between consecutive video frames. All the planes selected through this process, however, may not necessarily provide full spatial coverage of the scene. In addition, selecting multiple scene planes through a cascaded RANSAC process practically results in non-existing scene planes leading to incorrect background models. In order to address this shortcoming, Monnet et al. [31] formulate the problem of background subtraction as the complement of saliency detection, such that, background model becomes insensitive to the camera motion due to its locally discriminant nature. A recent study by Lim et al. [32] proposes an online iterative algorithm where the geometry of the scene is approximated by dividing the image into multiple blocks and estimating the background/foreground motion of each block using optical flow. As observed in experimental results, the background labeling in [32], however, is prone to complex scene geometry and small objects that inhibit the planar scene assumption for each the block. Similarly, Kwak et al. [33] use the same block-wise approach but adopt a non-parametric belief propagation with Gaussian mixtures for motion estimation.Alternative to multi-layer representations, Sheikh et al. [34] applied factorization based shape from motion to a set of tracked points. They labeled the trajectories as background or foreground based on the assumption that background trajectories form a 3D subspace. Their approach, however, requires offline processing and generates a sparse model due to disjoint trajectories. A recent method proposed by Elqursh et al. [35] represents trajectories in a low-dimensional space and groups them by relearning the Gaussian Mixture Model at each frame. The decision of which trajectory groups belong to background or foreground is given by a set of heuristics such as compactness, surroundedness, and spatial closeness, which may not always hold. Another method for moving object detection with hand held cameras was proposed in [36]. The authors combine color and locality cues to detect moving objects when the object motion is sparse and insufficient. This method assumes that the global background motion can be modeled by a single homography transformation. Existence of a single homography transform, however, does not hold when the camera translation is significant for a complex scene. Zhang et al. [37] improved the single homography assumption by introducing a bilayer segmentation approach for handheld cameras. Their approach, however, requires a manual preprocessing step to indicate the foreground layer each time a new foreground object enters the scene. Background from moving cameras can also be estimated by video grounding that removes the moving objects and reconstructs the occluded parts of the background in a sequence of images. In the case of video grounding using moving cameras, Evangelidis et al. [38] exploit information obtained from multiple image sequences capturing the same scene and use a set of background/foreground classifiers with temporal coherence constraints to estimate the background.In this paper, we adopt the idea of multi-layer representation for detecting moving objects when the camera moves freely in the space. Unlike [30], we do not select a number of scene planes. Instead, we select a single plane with the highest number of salient points as the reference plane, and generate a set of hypothetical planes that slice the 3D scene creating object cross-sections (see Fig. 1). This treatment overcomes the aforementioned problems of plane selection for multi-layer approaches. In plane-sweep stereo, researchers have used such hypothetical planes for projective 3D recovery [39] and surface visibility tests [40] in context of volumetric scene recovery. For the background subtraction problem, we conjecture that all pixels in an image are projections of 3D points lying on respective hypothetical planes. This conjecture suggests that given sufficient number of planes, pixels that are members of the static scene only register to the images of the hypothetical planes they belong. These memberships generate a 2.5D background model which contains color statistics for each hypothetical plane (see Fig. 2). The residual pixels that do not satisfy the statistics of any hypothetical plane belong to the moving objects. In our approach, a set of points lying on the same physical plane may be associated with a number of hypothetical planes. This property provides the flexibility to generate background models for nonplanar static scene structures, which is otherwise impossible for typical multi-layer based scene models. In summary, the main contributions of the paper include: (1) ability to detect moving objects for freely moving cameras; (2) generating 2.5D background model for a generic scene containing nonplanar structures; and (3) implicit disambiguation of scene structures with high parallax in consecutive video frames based on the scene geometry.The remainder of the paper is organized as follows. In the next section, we elaborate on image transformations induced by different types of camera motion and introduce the plane topology used to model the scene. Based on this topology, the details of proposed background subtraction method are sketched in Section 3. Section 4 provides the experimental evaluations and discussions. Finally, we conclude the paper and provide future directions in Section 5.Pinhole camera provides a simple yet powerful model that projects the 3D object space to 2D image plane. The transformation is algebraically governed by the perspective projection in the homogenous coordinates and is given by:(1)sx=PX=KR[I|-C]X,where a 3D pointX=(X,Y,Z,1)⊤maps to a 2D pointx=(x,y,1)⊤with scale s. Based on central projection, the3×4projection matrixPin (1) can be decomposed into the camera calibration matrixK, camera rotation matrixRand the optical center vectorC. Let two imagesxandx′of a 3D pointXbe acquired at consecutive time instants, as the camera rotates, translates and zooms in on objects. For relative camera motion, the initial camera center can be assumed to reside at the origin and have its principle axis aligned with the z-axis of the coordinate system, such thatsx=K[I|0]Xands′x′=K′[R|t]Xare the projections for two consecutive cameras. In the following discussion, we will analyze different camera motions to understand its effect on the static scene in context of generating background models.(1)Stationary camera: When camera is stationary,R=Iandt=0, with a fixed focal length, the projection matrices between consecutive frames become identical:P=P′. In this setting, a 3D point projects to the same pixel in both images,sx=KK-1s′x′, such that the transformation between the two images simplifies to identity matrixx′=Ixdue to the constant scale. A desired outcome of the identity transformation is that despite the complexity of the scene geometry, one can directly use the color observations at a particular pixel for generating/updating the background model without image registration.Pan-tilt-zoom camera: Pan-tilt-zoom (PTZ) camera is commonly used in moving object detection. This camera undergoes only rotational changes in its external configuration due to the pan/tilt motion, and focal length changes due to zooming. Since the camera does not translate, projections from 3D to images becomesx=KXands′x′=K′RX. These projections result in direct image registrationsx=s′KR⊤K′-1x′by means of rotational homography [41, p. 326], such thatsx=HRx′. Disregarding the complexity of the scene structure, rotational homography provides a one-to-one mapping between the images which can be used to generate panoramic background models. The added complexity of this configuration compared to the stationary camera is the additional step for detecting and matching/tracking a minimum of four salient points across frames.Free camera motion+planar scene: When the scene is assumed to be planar, such as in aerial imagery [15], 3D points can be conjectured to lie on a ground planeπ. Without loss of generality, selectingπasZ=0plane, using the projection matrixP=[p1,p2,p3,p4]in (1), pointsXπ=(X,Y,1)⊤onπare projected to image bysx=[p1,p2,p4]Xπ=H(Xπ), where s is the scale factor andHis the homography transform between the scene plane and the image. The registration between two frames with respect to the planeπbecomessx=HXπ=HH′-1x′=(Hπ)x′, whereHπ, similar toHR, is a homography transform. The complexity of this configuration is the same as in PTZ camera case and it requires detection and matching of salient points for registration of images to detect moving objects. This approach, however, will not work for images where the scene parallax causes registration errors.Free camera motion+complex scene geometry (2.5D Representation): In this configuration, we conjecture that the 3D scene can be decomposed into a stack of N “hypothetical planes” slicing the scene into a set of disjoint 2D subspaces. Considering that the choice of the coordinate system in Euclidean geometry is arbitrary, we let a reference planeπ0generating the hypothetical planes coincide with theZ=0plane:π0=[0,0,1,0]⊤. Note that the reference plane does not need to be a physical scene plane. Translating the reference plane bykΔZin Z direction generates a parallel hypothetical planeπk=[0,0,1,-kΔZ]⊤:k∈Nwith a pencil at infinity (see Fig. 3). The pointsXi,0lying on the reference planeπ0can be transferred to the hypothetical planeπkby a4×4homography matrix:Considering that there exists a single vanishing line for the hypothetical planes generated by translatingπ0, estimating the images of points for these non-existing planes becomes the problem of estimating pixel scales using (5) and (6). Using these points, the image registration with respect to any hypothetical plane is achieved by computing its respective homography transform:(7)si,kxi,k′=Hkxi,k,whereH0,H1,…,HN-1are transformations providing one-to-one mappings between consecutive images for each planeπ0,π1,…,πN-1.The multi-layer scene model given above requires the vanishing linelvand the vertical vanishing pointvz. Vanishing lines can be estimated using different approaches. One of these approaches is to use cascaded Hough transform to first estimate vanishing points such that their join becomes the vanishing line [42] (see Fig. 3). Alternatively, vanishing line can be estimated by computing the relative camera motion between consecutive frames. In our case, since we use the ground plane as the reference plane, the camera matrices of the first few frames can be estimated using [43]. Note that the vanishing line needs to be estimated only for the first frame. Using the dual of the homography in (7), the vanishing line estimated initially simply transfers to the following frame by(8)lv′=H0-⊤lv,whereH0is the homography transform in (7) for planeπ0.Given the vanishing linelvof the plane, the vanishing pointvzis computed in closed form using the topology illustrated in Fig. 4, whereCCandPPare the camera center and the principal point, respectively. This geometry provides the vertical pointxlvonlv, such thatxlv=lv×lv⊥, wherelv⊥is the line passing throughPPand perpendicular tolv. Using this relationvzis calculated as:(9)vz=PP+(PP-xlv)b/a,where(10)a=||xlv-PP||,andb=f2/a.In order to generate hypothetical planes, we usexg, which is the image of an arbitrary salient pointXi,0, andxv, which is the image of the corresponding pointXi,k. The planeπkis selected by setting an arbitrary projective scale of a pointxvin the first frame lying on the line connecting the vanishing pointvzto any pointxgon the reference planeπ0. In our experiments, we used 100 as the reference projective scale. In the next frame, the vanishing linelv′is estimated using (8) andvz′is calculated fromlv′using (9). The ground pointxgis transformed using the ground homographyH0to findxg′. Based on the geometry illustrated in Fig. 5, the vertical pointxv′can be estimated as the intersection of the of linelgv′and the epipolar lineFxv, such that(11)xv′=(vz′×xg′)×Fxv,whereFis the fundamental matrix between consecutive frames.Our objective is to estimate a binary labelingL∗={l1,l2,…,ln}, which denotes if the pixel belongs to the background or the foreground. Motivated by the MAP-MRF formulation outlined in [44,45], the labeling can be achieved by findingL∗=argminLE(L,x), which minimizes the energy function given by:(12)E(L,x)=∑xi∈ID(xi)+∑xi,xj∈N(I)V(xi,xj).In this equation,D(xi)andV(xi,xj)respectively represent the data and smoothness terms in local neighborhoodN(·). The data term can be written as:D(xi)=-lnpli|xi=-lnp(b|xi)ifli=0ηfifli=1,wherep(b|xi)is the probability ofxibeing a background pixel,ηfis the parameter determining the cost of labeling the pixelxias foreground andli∈{0,1}represents the label for background (0) and foreground (1).We compute the probabilityp(b|xit)by representing the background scene as a stack of hypothetical planes as described in Section 2. With this representation, points in the scene belong to one of the hypothetical planes. Moreover, points belonging to the static scene structures register between consecutive frames with one of the homography transformsH0,H1,…,HN-1. LetXibe a 3D point andxit,xit-1be its images in framesItandIt-1, respectively. We compute N possible registrationsxi,0t-1,xi,1t-1,…,xi,N-1t-1ofxitwith respect to each plane:(13)si,kxi,kt-1=(Hkt)-1xit,whereHkis the homography transform in (7), and the subscript k inxi,kt-1indicates thatxitis transformed withHkt. IfXibelongs to a planeπm, we expectxi,mt-1=xit-1ifXibelongs to a static scene, andxi,mt-1≠xit-1ifXibelongs to a foreground region. As a result, background points will result in consistent mappings across the frames with respect to one of the hypothetical planes (see Fig. 6), while the foreground pixels will result in inconsistent mappings. We exploit this fact to distinguish background and foreground.Finding the plane on whichXilies is not trivial. We eliminate the requirement of implicit estimation of the plane on which a point resides by maintaining a background model for each hypothetical planeπkas if all points in the image belongs to that plane. With this setting, a pixelxitcan be labeled as background by applying maximum a posteriori probability (MAP) estimate, such that the probability ofxitbeing a background pixel,p(b|xit)is computed by:(14)p(b|xit)=max0⩽k<Np(b|xi,kt-1,It(xit)),wherep(b|xi,kt-1,It(xit))is the conditional probability ofxitbeing a background pixel with respect to planeπkgiven the corresponding positionxi,kt-1computed using (13) and color valueIt(xit)of the pixel. Recall that the background model for each hypothetical planeπkmodels all pixels assuming that they belong to this plane. When a physical static scene pointXiresides on a planeπm, this assumption, while not valid for most of the planesπk≠m, is guaranteed to hold for at least one planeπk=m. The use of MAP estimate requires the presence of at least one distribution modelingXicorrectly. Applying the Bayes’ rule, the expressionp(b|xi,kt-1,It(xit))in (14) becomes(15)p(b|xi,kt-1,It(xit))=c·p(It(xit)|xi,kt-1,b)︸appearancep(xi,kt-1|b)︸geometry,wherec=p(b)p(xi,kt-1,It(xit))-1is constant due to the fact thatp(b)andp(xi,kt-1,It(xit))are drawn from uniform distributions. The first probability terms in (15) are computed from the appearance and geometry distribution introduced in Sections 3.1 and 3.2, respectively, and provide the likelihood of the observation to be a member of that distribution.In order to enforce the spatial smoothness during the labeling process, the smoothness term can be formulated as [46]:(16)V(xi,xj)=λ(1-δ(li-lj))exp-||I(xi)-I(xj)||t22β,whereλcontrols the effect of the smoothness term,δ(·)is a Kronecker delta function. As suggested by [32], the constantβrepresents the average of intensity variations in the neighborhood of each point and is defined as:(17)β=1n∑i=1n∑xj∈G(xi)||I(xi)-I(xj)||2,whereG(xi)is a set of neighboring pixels aroundxi. The solution of the energy minimization can be efficiently computed using the graph-cut algorithm [47–49].LetXibe a static scene point andxitbe its image at frameIt, whereIt(xit)is the appearance function at time t. In order to learn the underlying appearance distribution forXi, and compare an incoming observation to see whether it is drawn from this distribution, we need to know whereXiprojects across images. This operation is not explicitly possible in case when the camera parameters are unknown. However, with our 2.5D formulation, the appearance of the background pixels can be modeled without explicit knowledge of which plane a point belongs to. We start with the assumption thatXilies on all N hypothetical planes, such that at frame t, we have N different mappingsxi,0t-1,xi,1t-1,…,xi,N-1t-1of a pixelxitto pixels in framet-1, computed using the homography transformsHktfrom imageItto imageIt-1with respect to the planeπk:k=0,…,N-1. Considering that our goal is to generate a scene model for moving object detection, this assumption is exercised by creating N appearance distributions{Ψi,0t,Ψi,2t,…,Ψi,N-1t}using the mapping given in (13) (see Fig. 2). An appearance distributionΨi,kfor a pixelxiis maintained with the assumption thatxilies on a planeπk. In other words, by adding the new observations from incoming frames,Ψi,ktat frame t models distribution of values,(18){I1(xi,k1),I2(xi,k2),I3(xi,k3),…,It(xi,kt)},wherexiis transformed with a corresponding homography transformationHk1,Hk2,…,Hkt.In our implementation, we adopt the Gaussian mixture model of [2] as the distributionΨi,kfor kth hypothetical plane, and estimate the posterior probability by:(19)p(It(xit)|xi,kt-1,b)=∑j=1NgwjN(It(xit),μj,Σj),where b denotes background label,wj,μjandΣj=σj2Iare respectively the weight, mean and covariance of thejth Gaussian distribution inΨi,kt-1, andIis3×3identity matrix. A pixel valueI(x)is checked against each of the Gaussian components inΨuntil a match is found. A pixel matches a component if its value is within 2.5 standard deviations [2]. The parameters of a distribution that matches the current pixel are updated as:(20)μ←(1-α)μ+αI(x),(21)σ2←(1-α)σ2+α(I(x)-μ)⊤(I(x)-μ),whereαis the learning rate. The mean and covariance of the unmatched distributions remain unchanged. The weights of the distributions are updated as follows:(22)w←(1-α)w+αifthedistributionmatches,(1-α)wotherwise.If none of theNgdistributions matchI(x), the distribution with lowest weight is replaced by a new distribution withμ=I(x), low prior weight and initial high variance. As discussed in [2], the mixture model makes background appearance distribution multimodal and allows accounting for illumination changes, camera flicker, and repetitive motion.The marginal updates performed by introducing observations to the model result in evolution of the background model as illustrated in Fig. 7for the reference plane of the desk sequence. In this figure, the top of the desk and the bottom parts of all objects placed on it belong to the reference plane and are successfully modeled in the background distribution with highest weighted Gaussian component (Fig. 7, row 2). On the other hand, the other pixels that do not belong to the reference plane, e.g., the wall and the white box, are modeled with the lowest weighted Gaussian component.For the stationary camera, since the transformations become identity matrices, the proposed background model reduces to the traditional background model with the added complexity of retaining the appearance models for eachπk. For the moving camera case,Ψi,kuniquely models the appearance of the scene featureXiwith respect to the hypothetical planeπkthat it lies on. On the other hand, the appearance distributions forXiwith respect to the other hypothetical planesπm≠kwill model observations obtained due to wrong mappings between consecutive frames. In our model, moving objects change their positions from one frame to the next and their registration with respect to any of the hypothetical planes will cause misalignments and will not conform to learned appearance models. Fig. 6 presents the background model of the desk sequence for selected planes aligned with one of the frames. It can be noted thatπ0represents the top of the desk and bottom parts of the objects residing on it. The top of the small box is a part of planeπ17. Planeπ33correctly models the hypothetical plane containing top of the large box and the top part of the mug. Planeπ72corresponds to the plane passing through the pen and papers on the top shelf of the paper rack.The second probability term in (15) relates to the proposed 2.5D background model composed of multiple background distributions due to projections onto multiple planes as shown in Fig. 8(a). Pixelxican be drawn from a distribution of hypothetical planes defining the background, which will be referred to as the geometry distribution.If a physical scene pointXiresides on a planeπm, its projection in images will be mapped correctly with the homographyHm. This mapping consistently results in the same appearance observation, leading to a high value ofp(I(xit)|xi,k=mt-1,b)and low values ofp(I(xit)|xi,k≠mt-1,b)in all the frames where the pointXiis not occluded by a foreground object. One can keep track of consistent mappings for the hypothetical planes by maintaining a1×NvectorΓifor each pixelxit. We start with a vector ofΓt=0of all zeros. Then, for each k, we calculate the probabilityp(It(xit)|xi,kt-1,b)using (19). If the probability is sufficiently high,Γitis updated as following:(23)Γit(x)←Γit(x)+Kh(x-k),wherex=1,…,NandKh(x)=exp-||x||22h2denotes a kernel function with bandwidth parameter h. Intuitively, h should decrease as N decreases, since with fewer planes we expect less confusion in plane association. Motivated by the model update in [2], the construction ofΓitis finalized by an exponential update based on the existing modelΓit-1:(24)Γit←(1-α)Γit-1+αΓit,whereαis the learning rate used for appearance model update in (20)–(22). The values ofΓitare further normalized to be in the range[0,1]. With this design, the second probability termp(xi,kt-1|b)in (15) is proportional toΓit(k). In Fig. 8(b), we plot the geometric distribution as a function of the plane index for the pixel marked in red color in part (c). It can be observed that the probability is maximum for the plane that the pixel belongs to.Algorithm 1Background subtraction process.The steps of the proposed method are detailed in Algorithm 1. First, a set of coplanar pixelsS01is detected by robust homography estimation using RANSAC. In our experiments, we use dense optical flow [24] to establish point correspondences. For very low frame rates, point detection or tracking (for example, KLT tracking [50]) can also be used. To construct the 2.5D representation, we estimate the vanishing line and the vertical vanishing point for the selected plane using the procedure described in Section 2. With each new frameIt, the tracked set of points is augmented by introducing new points to the inlier setS0t-1which satisfy the homography transformH0t, allowing gradual change of the scene content. The points inS0tare used to obtain the setsSktfor each hypothetical planeπkusing (4). The corresponding points inSkt-1andSktare used to estimate the homographyHktfor the planeπk.Pixelsxitin a novel image are projected to each layer of the background model governed by N homography transforms asxi,0t-1,xi,1t-1,…,xi,N-1t-1using (13). To eliminate occasional small errors in homography estimation, we perform a local search in a small window around each pixel, selecting the position which induces the highest probability for each planeπk:(25)x̃i,k=argmaxxj,k∈N(xi,k)p(I(xi)|xj,k,b),wherex̃i,kis the corrected position ofxi,k, andN(xi,k)is a set of pixels in the small window centered aroundxi,k. In our experiments, we use a3×3window. This formulation allows us to avoid erroneous updates of the background models, while the spatial smoothing using MRF affects the labeling output only.The labeling likelihood is computed using (14). Some of the pixelsxi,kt-1may not have corresponding appearance distributionsΨi,kt-1. This usually happens because of new regions appearing on the boundaries of the image as the camera views unseen parts of the scene. For these regions, we start a new distributionΨi,ktand initialize it withμ=It(xit), low prior weight and high initial variance values.In order to ensure the temporal consistency of estimated probabilities, we compute the weighted average of the probabilitiesp(b|xit),p(b|xit-1)andp(b|xit-2)from the last three frames [51] using:(26)p(b|xit)=g1p(b|xit)+g2p(b|xit-1)+g3p(b|xit-2),where(g1,g2,g3)denote the weights. In our implementation, we used(0.7,0.2,0.1)as probability weights which emphasizes the current frame more than the previous frames. Note that application of the temporal consistency on probabilities may not guarantee consistency of labels. This is primarily due to the fact that MRF, which is applied after temporal consistency, suppresses small objects.For hypothetical planes that are close to the plane which passes through camera center, the points converge to the horizon line (see Fig. 9). As a result, the induced homographies become degenerate and do not provide mappings between the frames. Therefore, when the vanishing linelvcan be seen in the image, we ignore the image regions that are spatially close tolv, and assume that they belong to background. This assumption is not considered to be a drawback of the proposed algorithm since objects close to horizon are infinitesimally small.

@&#CONCLUSIONS@&#
