@&#MAIN-TITLE@&#
Stereo fusion: Combining refractive and binocular disparity

@&#HIGHLIGHTS@&#
A stereo fusion system that combines binocular and refractive stereo is presented.Our stereo fusion outperforms traditional binocular and refractive stereo.An efficient calibration method for refractive stereo is proposed.

@&#KEYPHRASES@&#
Stereo fusion,Refractive stereo,Multi-view stereo,

@&#ABSTRACT@&#
The performance of depth reconstruction in binocular stereo relies on how adequate the predefined baseline for a target scene is. Wide-baseline stereo is capable of discriminating depth better than the narrow-baseline stereo, but it often suffers from spatial artifacts. Narrow-baseline stereo can provide a more elaborate depth map with fewer artifacts, while its depth resolution tends to be biased or coarse due to the short disparity. In this paper, we propose a novel optical design of heterogeneous stereo fusion on a binocular imaging system with a refractive medium, where the binocular stereo part operates as wide-baseline stereo, and the refractive stereo module works as narrow-baseline stereo. We then introduce a stereo fusion workflow that combines the refractive and binocular stereo algorithms to estimate fine depth information through this fusion design. In addition, we propose an efficient calibration method for refractive stereo. The quantitative and qualitative results validate the performance of our stereo fusion system in measuring depth in comparison with homogeneous stereo approaches.

@&#INTRODUCTION@&#
There have been many approaches to acquiring depth information of real scenes such as passive stereo [1], active stereo [2], time-of-flight imaging [3], depth from defocus [4], etc. Among them, passive stereo imaging has been commonly used for distant measurements to understand scene shapes. Classical stereo algorithms employ a pair of binocular stereo images. Such stereo algorithms estimate depth by evaluating the distance of corresponding features, so-called disparity, via computing matching costs and aggregating the costs [1]. However, owing to the nature of triangulation in depth estimation, depth accuracy strongly depends on the baseline between a stereo pair. For instance, a wide baseline elongates the range of the correspondence search so that the matching problem cannot be solved with high precision in typical locally-optimizing approaches [5]. On the contrary, a narrow baseline shortens the resolution of disparity; therefore, the accuracy of estimated depth could be degraded [6,7].Recently, Gao and Ahuja [8,9] introduced a single-depth camera based on refraction. Chen et al. [10] further extended this refractive mechanism. Such refractive stereo systems estimate depth from the change of light direction; therefore, the disparity in refractive stereo in general is smaller than that in binocular stereo, i.e., its performance is similar to that of binocular stereo with a narrow baseline.We take inspiration from refractive stereo to combine these two heterogeneous stereo systems, where a stereo fusion system is designed with a refractive medium placed on one of the binocular stereo cameras. In this paper, we introduce a novel optical design that combines binocular and refractive stereo and its depth process workflow that allows us to fuse heterogeneous stereo inputs seamlessly to achieve fine depth estimates. Our system comprises a pair of stereo cameras, one of which is covered with a transparent medium, allowing for enhanced depth accuracy. The proposed approach offers benefits compared to the typical multiview stereo [7] in terms of building cost as our system employs just the same number of cameras as a binocular stereo does. It is also more advantageous than multiview stereo, which consists of two cameras on a linear slider [11], by providing physical stability, i.e., spinning the medium is less undemanding than moving a camera on the slider frequently at different distances.The refractive calibration process that we propose in this paper is a natural evolution of our previously published research [12]. We increase the efficiency of the tedious refractive calibration process in the prior work [12]. In this paper, we propose a novel refractive calibration method that requires fewer angle samples (at least three angles), rather than the dense angle samples, from 0 to 360° at 10-degree intervals. As such, the novel calibration method can accelerate the cumbersome calibration process that hinders the usefulness of refractive stereo. We believe that this calibration method increases the usefulness of the proposed stereo fusion method.Fig. 1shows a brief overview of our method. The following contributions have been made:•A stereo fusion system that combines refractive and binocular stereo.We propose a stereo fusion system that combines a refractive medium on a binocular base. The medium is placed in front of a camera in binocular stereo.Calibration methods for stereo fusion.We develop a workflow of calibration for this fusion system that includes radiometric, geometric and refractive calibration methods. In particular, we propose an efficient calibration method of refractive stereo based on xyz-Euler angles, which requires a smaller number of angle measurements (at least three angles), rather than dense measurements of complete angle variation. This calibration enables us to obtain the essential points of the entire angles from sub-sampled angle measurements.Depth fusion workflow that combines two heterogeneous stereo images.Our calibration methods allow us to estimate depth from two heterogeneous stereos. The resulting depth map achieves a higher depth resolution with fewer artifacts than that of traditional homogeneous stereo.This section describes the foundational differences of binocular and refractive stereo, surveying state-of-the-art depth-from-stereo methods.Binocular disparity in stereo imaging describes pixel-wise displacement of parallax between corresponding points on a pair of stereo images taken from different positions. Searching correspondence on an epipolar line is necessary prior to computing disparity. As disparity d depends on its depth, we can recover the depth z using simple trigonometry as follows:(1)z=fb/d,where f is the focal length of the camera lens, and b is the distance between the center of projections for the two cameras, the so-called baseline. In particular, the baseline b determines the depth resolution of the stereo system, and b is also related with occlusion error. Therefore, baseline must be adapted to the scene configuration for optimal performance. There is no universal configuration of baseline for real-world conditions.Wide-baseline stereo reserves more pixels for disparity than narrow-baseline stereo does. Therefore, wide-baseline systems can discriminate depth with a higher resolution. On the other hand, the search range of correspondences increases, and in turn, it increases the chances of false matching. The estimated disparity map is plausible in terms of depth, but it includes many small regions without depth as spatial artifacts (of holes) on the depth map. This missing information is caused by occlusion and false matching in featureless or pattern-repeated regions, where the corresponding point search fails.Narrow-baseline stereo has a relatively short search range of correspondence. The search range of correspondence is shorter than that of wide-baseline stereo. There are fewer chances for false matching, so accuracy and efficiency in cost computation can be enhanced. In addition, the level of spatial noise in the disparity map is low because the occluded area is small. However, narrow-baseline stereo reserves a small number of pixels for depth discrimination. The depth-discriminative power decreases accordingly, whereas the spatial artifacts in the disparity map are reduced. It trades off the discriminative power for the reduced spatial artifacts in the disparity map.This fundamental limitation of the baseline in binocular stereo has been addressed by the use of more than two cameras, so-called multi-baseline or multi-view stereo. Okutomi and Kanade [6] proposed a multi-baseline stereo method, which is a variant of multi-view stereo. The proposed system consists of multiple cameras on a rail. They presented the matching cost design for the multi-baseline setup. Instead of computing the color difference of a pixel on the reference view and the corresponding point on the other view, the color differences of all views are summed up. This multi-baseline stereo gives more accurate depth estimates than binocular stereo does.Furukawa and Ponce [13] presented a hybrid patch-based multi-view stereo algorithm that is applicable to objects, scenes, and crowded scene data. Their method produces a set of small patches from matched features, which allows the gaps between neighboring feature points to be filled in, yielding a fine mesh model. Gallup et al. [14] estimated the depth of a scene by adjusting the baseline and the resolutions of images from multiple cameras so that depth estimation becomes computationally efficient. This system exploits the advantages of multi-baseline stereo while requiring the mechanical support of moving cameras. Nakabo et al. [11] presented a variable-baseline stereo system on a linear slider. They controlled the baseline of the stereo system in relation to the target scene to estimate the accurate depth map.Zilly et al. [7] introduced a multi-baseline stereo system with various baselines. Four cameras are configured in multiple baselines on a rail. The two inner cameras establish a narrow-baseline stereo pair while two outer cameras form a wide-baseline stereo pair. They then merge depth maps from two different baselines. The camera viewpoints in the multi-baseline systems are secured mechanically at fixed locations in general. This design restricts the spatial resolution along the camera array while the depth map is being reconstructed. Refer to [15] for the in-depth investigation of other multi-view methods.Compared to the previous multi-baseline systems, we utilize a refractive medium on a rotary stage that is installed ahead of one of the binocular cameras. Our system requires only two cameras for binocular stereo, which is more efficient than other multi-baseline systems [7] that employ more than two cameras. In multi-baseline systems [11], it is cumbersome to move a camera along a linear slider. This manual operation may suffer from misregistration and broken calibration of multiview images, and such systems require a large space to operate for the camera movement. In the proposed system, we simply rotate a medium, instead of a camera, and can avoid any problems caused by the change of camera position. The form factor of our system is much smaller than that of multi-baseline systems [7,11].Refractive stereo estimates depth using the refraction of light via a transparent medium. We follow the derivation of Gao and Ahuja [9] to formulate the optical geometry in refractive stereo. Suppose point p in a three-dimensional scene is projected to pdon an image plane through the optical center of an objective lens C directly without any transparent medium (see Fig. 2a). Insertion of a transparent medium in the light path changes the transport of the incident beam from p, and it reaches pron the image plane with a lateral displacement d (between with and without the medium). The displacement between pdand pron the image plane is called refractive disparity.Now we can compute the depth z of p using simple trigonometry following [8,9]:(2)z=fRr,where r is a refractive disparity completed by searching a pair of corresponding points, f is the focal length, and R is the ratio of lateral displacement d to sin (θp):(3)R=dsin(θp).Here θpis the angle betweenprC→and the image plane. To obtain the value of R, we first compute cos (θp) as(4)cos(θp)=pre→·prC→|pre→||prC→|.Then we plug sin (θp) into Eq. (3) after computing sin (θp) with this equation:(5)sin2(θp)+cos2(θp)=1.Lateral displacement d, the parallel-shifted length of the light passing through the medium, is determined as [16](6)d=(1−1−sin2(θi)n2−sin2(θi))tsin(θi),where t is the thickness of the medium, n is the refractive index of the medium, and θiis the incident angle of the light. Here, sin (θi) can be obtained in a similar manner as the case of sin (θp) using the following equation:(7)cos(θi)=prC→·eC→|prC→||eC→|.The refracted point prlies on a line, the so-called essential line, passing through essential point e (an intersecting point of the normal vector of the transparent medium to the image plane) and pd(see Fig. 2b). This property can be utilized to narrow down the search range of correspondences onto the essential line, allowing us to compute matching costs efficiently. It is worth noting that disparity in refractive stereo depends on not only the depth z of p but also the projection position pdof light and the position of the essential point e, whereas disparity in traditional stereo depends on only the depth z of the point p. Before estimating a depth, we calibrate these optical properties in refractive stereo in advance.Nishimoto and Shirai [17] first introduced a refractive camera system in which a refractive medium is placed in front of a camera. Rather than computing depth from refraction, their method estimates depth using a pair of a direct image and a refracted one, assuming that the refracted image is equivalent to one of the binocular stereo images. Lee and Kweon [18] presented a single camera system that captures a stereo pair with a bi-prism. The bi-prism is installed in front of the objective lens to separate the input image into a stereo pair with refractive shift. The captured image includes a stereo image pair with a baseline. Depth estimation is analogous to the traditional methods. Gao and Ahuja [8,9] proposed a seminal refractive stereo method that captures multiple refractive images with a glass medium tilted at different angles. This method requires the optical calibration of every pose of the medium. It was extended by placing a glass medium on a rotary stage in [9]. The rotation axis of the tilted medium is mechanically aligned to the optical axis of the camera. Although the mechanical alignment is cumbersome, this method achieves more accurate depth than the previous one does.Shimizu and Okutomi [19,20] introduced a mixed approach that combines refraction and reflection phenomena. This method superposes a pair of reflection and refraction images via the surface of a transparent medium. These overlapping images are utilized as a pair of stereo images. Chen et al. [10,21] proposed a calibration method for refractive stereo. This method finds the pairs of matching points on refractive images with the SIFT algorithm [22] to estimate the pose of a transparent medium. They then search corresponding features using the SIFT flow [23]. By estimating the rough scene depth, they recover the refractive index of a transparent medium.We propose a novel stereo fusion system that exploits the advantages of refractive and binocular stereo. This section describes technical details of the hardware design and calibration methods for the proposed system.Our stereo fusion system consists of two cameras and a transparent medium on a mechanical support structure. The focal length of both camera lenses is 8 mm. The cameras are placed on a rail in parallel with a baseline of 10 cm to configure binocular stereo. We place a transparent medium on a rotary stage for refractive stereo in front of one of the binocular stereo cameras. Fig. 3(a) presents our system prototype. Fig. 3(b) and (c) compare disparity changes by the refractive medium (b) and the baseline in stereo (c), respectively. Suppose we have two points,pandq∈R3,where point p is farther from the camera than point q. In Fig. 3(b), if there is no transparent medium, points p and q will be projected at pixels pdand qdvia perspective projection, respectively. However, as we have the medium, rays are refracted and projected at pixels prand qrdue to refraction caused by the medium. Note that the refractive disparity (the offset distance) depends on the depth of point, i.e., the refractive disparity of distant point p is shorter than that of q, and the orientation of refractive disparity depends on the pose of the medium. In Fig. 3(c), the left camera is installed next to the right camera with a baseline. The corresponding pixels pdand qdare projected on pband qbwith binocular disparities. Comparing the refractive and the binocular disparity, the refractive disparities prand qrare smaller than those of binocular disparities pband qb. Refractive stereo is equivalent to narrow-baseline stereo while binocular stereo is equivalent to wide-baseline stereo in our system.Our transparent medium is a block of clear glass. The measured refractive index of the medium is 1.41 (η=sin(20.00∘)/sin(14.04∘)), and the thickness of the medium is 28 mm. We built a customized cylinder to hold the medium, cut in 45° from the axis of the cylinder. The tilted medium spins around the optical axis from 0° to 360° with angle intervals while capturing images. The binocular stereo baseline and the tilted angle of the medium are fixed rigidly during image capturing. For the input images of a scene, multiple images refracted by the medium are captured on a camera and another image is obtained from the other camera without the glass. Note that the refractive medium is not detached while capturing the input.Our stereo fusion system requires several stages of prior calibration to estimate depth information. This section summarizes our calibration processes.We first calibrate the extrinsic/intrinsic parameters of the cameras, including the focal length of the objective lens, the center point of the image plane and the lens distortion in order to convert the image coordinates into the global coordinates. For the geometric calibration, we captured 14 different positions on a chessboard. This allows us to derive an affine relationship between the two cameras and rectify the coordinates of these cameras with respect to the constraint epipolar line [24].Refractive stereo demands several optical calibrations related with the glass medium, such as thickness, its refractive index, and the essential points of glass orientation. This section presents our novel calibration method for refractive stereo.Analogous to the rectification of the epipolar line in binocular stereo, refractive stereo requires calibration of the essential point e, where essential lines converge to the essential point e outside the image plane (see Section 2.2 for details on essential points and lines); i.e., the refracted point prpasses through the virtually unrefracted pixel pdand reaches the essential point e on the essential line (see Fig. 2b).Gao and Ahuja [8,9] estimate essential points by solving an optimization problem with a calibration target at a known distance. They precompute the positions of the essential points at the entire angles by mechanically changing the normal orientation of the glass to each angle. Chen et al. [10] estimate essential points directly from a target scene without pre-calibration of essential points. Instead of capturing the calibration targets, they capture a target scene with and without the glass medium and apply the SIFT algorithm [10] to search correspondences of the refracted and unrefracted points. Their calibration process is simpler than that of previous works [8,9]. However, the accuracy of the calibration depends on the SIFT performance in searching correspondences.Recently, Baek and Kim [12] calibrate essential points through dense measurements of essential lines at each medium orientation using a calibration target. They take an image of a chessboard without the medium first in order to compare it with other refracted images at different poses of the medium. Once they take a refracted image in a pose, they extract corner points from both the direct and the refracted images, where corresponding feature points appear at different positions due to refraction. Superposing these two images, they draw lines by connecting the corresponding points with all feature corners following Chen et al. [10]. They then compute the arithmetic mean of the intersection points’ coordinates to approximate essential point eϕper angle ϕ. They repeat this process for every angle ϕ ∈ Φ, where Φ is the set of angles for calibration. Fig. 4(a) presents the calibrated essential points for Φ measured from 36 different orientations.Whereas Gao and Ahuja [8,9] require the measurement between the target and the camera in addition to measuring essential lines, the method proposed by Baek and Kim [12] does not require measurement of the distance from the camera to the chessboard, and thus is more convenient. Note that Gao and Ahuja [9] should capture four angles of the medium iteratively, until the rotation axis of the medium meets the principal axis of the camera. Contrary to Chen et al. [10], Baek and Kim [12] employ a calibration target to enhance the reliability of refractive calibration. However, since their calibration requires rigorous measurements of the entire angle variation, their measurement step in refractive calibration is very cumbersome and introduces any measurement errors.In order to overcome the problem of cumbersome measurements in the calibration process, we propose a modified approach of the previous refractive calibration introduced by Baek and Kim [12]. We were motivated to reduce the number of per-angle measurements of correspondences while estimating the entire essential points.We propose a novel parametric approach of refractive calibration on essential points. The key idea is to approximate the entire essential points of every angle by using a parametric rotation of xyz-Euler angles, where the rotation axis vector is optimized from a subset of measured essential points.Suppose we already estimated a certain number of essential points eϕfor sampled angles ϕ ∈ Φ following Baek and Kim [12]. Let the rotation axis of the medium be a unit vectoru=[ux,uy,uz]⊺,where∥u∥2=1. We denote the unit normal vector of the medium at angle φ asn(φ)=[nx(φ),ny(φ),nz(φ)]⊺,where∥n(φ)∥2=1. Without loss of generality, we set the reference angle (one of the measured angles) as zero degree. When we rotate the medium by degree φ from the reference angle with respect to the rotation axis u, the corresponding normal vector n(φ) of the medium can be computed as follows:(8)n(φ)=T(φ,u)n(0),where T(φ, u) is a rotation matrix that rotates a given vector n(0) by degree φ with respect to u. T(φ, u) is defined as an xyz-Euler angle rotation matrix with a rotation axis u following [25]:(9)T(φ,u)=[ux2v+cuxuyv−uzsuxuzv+uysuyuxv+uzsuy2v+cuyuzv−uxsuzuxv−uysuzuyv+uxsuz2v+c],where c is defined as cosφ, s is sinφ, andv:=1−c.Let eφ be the essential point for a pose of the medium, rotated about degree φ from the reference pose with respect to u. The essential point eφ in the image plane is located on a line that passes through the center of optics C. By definition, the unit normal vector of the medium n(φ) is on the same line (see Fig. 5). We then formulate this relation between the essential point eφ and the normal n(φ) as follows:(10)n(φ)=eφ−C∥eφ−C∥2.We denote the right-hand side in Eq. (10),(eϕ−C)/∥eϕ−C∥2,as Kϕ. Our goal is to formulate essential points from any given angle rotation as a parametric calibration model. This axis vector u and the reference normaln^(0)satisfy Eqs. (8) and (10) from known values Kϕand can be formulated as an objective function:(11)minu,n^(0)∑ϕ∈Φ∥T(ϕ,u)n^(0)−Kϕ∥2s.t.∥n^(0)∥2=1and∥u∥2=1,wheren^(0)is the optimized reference normal of the medium. Note that it is feasible to use n(0) directly instead of introducingn^(0)using Eq. (10). However, we found that when one of direct measured normals is used as n(0) in optimization of Eq. (11), optimized essential points can be biased occasionally upon an initial measurement error of n(0). We therefore choose to apply a joint optimization approach to find both optimaln^(0)and u in order to enhance global accuracy.We solve this non-linear objective function using a non-linear optimization algorithm [26]. Note that we have six unknown variables of u andn^(0). Since the unit normal vector n(φ) has rank 2, Eq. (11) gives us two equations per angle ϕ. Therefore, we need at least three samples to solve Eq. (11). See Fig. 12 for the impact of the number of input angles.After optimizing u, we can compute the essential point eφ at any arbitrary angle φ of the medium. The essential point eφ is the intersection point of a line that passes through the center of optic C displaced with f along the−zaxis (see Fig. 5). Therefore, we can compute the essential point eφ as follows:(12)eφ=T(φ,u)n^(0)−Tz(φ,u)n^(0)f+C,where Tz(φ, u) is the z-axis vector of T(φ, u).In our experiment, we select a set of angles Θ to be used to estimate corresponding essential points from the estimated u with Eq. (8). We denote the set of essential points as E for depth estimation.Matching costs are calculated by comparing the intrinsic properties of color at feature points. Since we attach a transparent medium on one of the stereo cameras, it is critical to achieve consistent camera responses with and without the medium. In our system, the right camera is attached with the glass medium while the left camera is without any medium. We found that there are mismatches of colors captured in the same scene. We therefore characterize these two cameras via radiometric calibration to match colors with each other. To do this, we employed a GretagMacbeth ColorChecker target of 24 color patches. We first captured an image from the refractive module with the medium and an image from the other camera without the medium. Then, we linearized these two RGB images with known gamma values as inverse gamma correction. Since we had two sets of the linear RGB colors for the 24 patches, A and B (with and without the medium), of which the dimensions were 24 × 3 each, we determined an affine transformation M of A to B as a camera calibration function (a 3 × 3 matrix) using least-squares [27]. Once we characterized two camera responses, we applied this color transform M for the linear RGB image which was reconstructed from the images taken by the camera with the medium (see Section 4.1.3). This color characterization allowed us to evaluate matching costs for disparity through identical color reproductions of the two different cameras.Our stereo fusion workflow is composed of two stages. We first estimate an intermediate depth map from a set of refractive stereo images (from the camera with the refractive medium) and reconstruct a synthetic direct image. Then, this virtual image and a direct image (from the other camera without the medium in a baseline) are used to estimate the final depth map referring to the intermediate depth map from refractive stereo. Fig. 6presents the workflow of our stereo fusion method.Depth reconstruction from binocular stereo has been well-studied regarding matching cost computation, cost aggregation, disparity computation, and disparity refinement [1], whereas depth reconstruction from refraction has been relatively less discussed. In this section, we describe our approach for refractive stereo for reconstructing an initial depth map.Binocular stereo algorithms often define the matching cost volumes of every pixel per disparity [1], where a disparity value indicates a certain depth distance directly in binocular stereo. This linear relationship can be universally applied for all the pixels in stereo images. However, it is worth noting that the refractive disparity of a pixel depends on not only depth but also on its image coordinates and the pose of the medium in refractive stereo; i.e., the refractive disparities of a point on the object’s surface could be different when the pixel position or the pose of the medium is different. We therefore define the matching cost volumes based on the depth, rather than the disparity, in our refractive stereo algorithm, following a plane sweeping stereo method [28]. This approach allows us to apply a cost volume approach for refractive stereo.Suppose we have a geometric position set P of the refracted points pr(pd, z, e) of direct point pdat depth z (see Fig. 2) with essential point e (e ∈ E):(13)P(pd,z)={pr(pd,z,e)|e∈E}.This set P can be derived analytically by refractive calibration (Section 3.2.2) so that we precompute this set P for computational efficiency.We denote L as the set of colors observed at the refracted positions P, where l is a color vector in a linear RGB color space (l ∈ L). Assuming that the surface of the direct point pdis Lambertian, the colors of the refracted points L(pd, z) would be the same. We use the similarity of L(pd, z) for the matching cost C of pdwith hypothetical depth z[29]. Note that our definition of the matching cost is proportional to the similarity, different from the typical definition of the matching cost in traditional stereo algorithms. The definition of the matching cost is as follows:(14)C(pd,z)=1|L(pd,z)|∑l∈L(pd,z)K(l−l¯).K is an Epanechnikov kernel [30] following:(15)K(l)={1−∥l/h∥2,∥l/h∥≤10,otherwise,where h is a normalization constant (h=0.01). Here,l¯is a mean color vector of all elements in a set of L. We computel¯with five iterations in L(pd, z) using the mean shift method [31] as follows:(16)l¯=∑l∈L(pd,z)K(l−l¯)l∑l∈L(pd,z)K(l−l¯).z in our refractive stereo is a discrete depth, the range of which is set between 60 cm and 120 cm at 3 cm intervals. Note that we build a refractive cost volume per depth for all the pixels in the refractive image.To improve the spatial resolution of the intermediate depth map in refractive stereo, we aggregate the refractive matching cost using a window kernel G.Advanced cost aggregation techniques, such as guided image [32] and bilateral weights [33], require a prior knowledge of the scene, i.e., a unrefracted direct image. However, we do not capture the direct image in our experiments because this requires detachment of the medium for every scene. Therefore, we first aggregate the refractive matching costs using a Gaussian kernel G:(17)G(pd,qd)=12πσ2exp(−∥pd−qd∥22σ2),where σ is set to 9.6 as a parameter.We filter the refractive matching cost at a pixel pdin a depth z, where this kernel convolves C(pd, z) with the matching costs of neighboring pixels with a weighting factor G(pd, qd) [34]:(18)CA(pd,z)=∑qd∈wG(pd,qd)C(qd,z),where qdis a pixel inside a squared window w, the size of which is 7 × 7.Finally, we compute the optimal depth Z(pd) of the point pdthat maximizes the aggregated matching costs:(19)Z(pd)=argmaxzCA(pd,z).Even though the levels of the two cameras are the same on a rail as traditional binocular stereo, our stereo pair includes more than horizontal parallax due to the refraction effect. Prior to combining the estimated refractive depth and the binocular stereo input, we reconstruct a synthetic image Id(a direct image without the medium) by computing the mean radiance of the set L(pd, Z(pd)) using the mean shift method (Eq. (16)). Note that this set L consists of colors gathered from the refracted images.Fig. 7presents the initial depth map Z (a) and the reconstructed synthetic direct image Id(d), which is compared with a ground truth image (e) that was captured while the medium was detached. If the refractive depth estimates Z(pd) contains some errors, the resulting synthetic image Idalso contains errors.Reconstructing the direct image allows us to apply a depth refinement algorithm with a weighted median filter [35] by treating the synthetic direct image as guidance to fill in the holes of the estimated depth map. The weighted median filter replaces the depth Z(pd) using the median from the histogram h(pd, ·):(20)h(pd,z)=∑qd∈wW(pd,qd)f(qd,z),where f(qd, z) is defined as follows:(21)f(qd,z)={1,ifZ(qd)−z=00,otherwise.Here, W is a weight function with a guided image filter [32], defined as(22)W(pd,qd)=1|w|2∑k:(pd,qd)∈wk(1+(ld(pd)−μk)(Σk+ϵU)−1(ld(qd)−μk)),where ld(pd) is the linear RGB color of pdon the direct image Id, U is an identity matrix, k is the center pixel of window wkincluding pdand qd, |w| is the number of pixels in wk, and μkand Σkare the mean vector and covariance matrix of Idin wk. In our experiments, we set the size of wkas 9 × 9, and we set ϵ as 0.001.This median filter allows us to refine the hole artifacts in the depth map while preserving sound depth. Once we obtained this refined depth map, we iteratively build a synthetic image again using the refined depth map. Fig. 8compares the initial synthetic image and the refined synthetic image from the second iteration.We next complete the synthetic direct image from the refracted images to be used as the input of binocular stereo. Since these two cameras’ color reproductions are different due to the insertion of the glass medium, we apply the color calibration matrix (described in Section 3.2) to the synthetic image.The number of input refractive images is critical to the quality of depth estimation in refractive stereo. We were motivated to identify the effects of number of input refractive images; therefore, we evaluated point-wise errors in estimating the depth on a planar surface at a known distance. We computed the root-mean-square error (RMSE) of depth estimates on a planar surface, indicated as a red rectangle in Fig. 9(a). Fig. 9(b) shows that the RMSE decreased very fast, while the input increased to six refractive images. Hence, we determined that we could utilize six refractive images as input considering the tradeoff between computation cost and depth accuracy. Note that we use six refractive images with 60° intervals for capturing results in this paper.Our binocular stereo with a wider baseline allows us to discriminate depth with a higher resolution than refractive stereo (equivalent to narrow-baseline stereo). We take inspiration from a coarse-to-fine stereo method [36,37] to develop our stereo fusion method. Our refractive stereo yields an intermediate depth map with a high spatial resolution that is on a par with that of narrow-baseline stereo. However, it is not surprising that the z-depth resolution of this depth map is discrete and coarse. We utilize the fine depth map from refractive stereo to increase the z-depth resolution as high as possible with a high spatial resolution by limiting the search range of matching cost computation in binocular stereo using the refractive depth map. To this end, we can significantly reduce the chances of false matching while estimating depth from binocular stereo between direct and synthetic images. This enables us to achieve a fine depth map from binocular stereo, taking advantages of a high spatial resolution in refractive stereo.Now we have a direct image Ibfrom the camera without the medium in the binocular module and the synthetic image Idreconstructed from the refractive stereo module (Section 4.1.4) with its depth map. Depth candidates with uniform intervals are not related linearly to the disparities with pixel-based intervals. Hence, we define a cost volume for stereo fusion on the disparity instead in order to fully utilize the image resolution. To fuse the depth from binocular and refractive stereo, we build a fusion matching cost volume F(pd, d) per disparity for all pixels as follows. The fusion matching cost F is defined as a norm of the intensity difference:(23)F(pd,d)=∥ld(pd)−lb(pd′)∥,wherepd′is a pixel shifted by a disparity d from pd, andlb(pd′)is a color vector ofpd′on image Ib.In order to aggregate sparse matching costs, we first tried to use a guided image filter that consists of multiple box filters, as done when refining refractive depth. Prior to applying this filter, we reduce the search range of correspondence differently per each pixel using the depth map obtained from refractive stereo. Since the guided filter exploits integral images, which need to be constructed for every pixel and every depth candidate, its computational cost increases significantly due to the wide range of valid depth candidates for high depth resolution. Instead of the guided filter, we use a bilateral filter W in Eq. (24), as the filter can be applied to the different ranges of depths per pixel independently. We achieve a significant improvement in computational cost by applying the bilateral image filter within a narrowed search range using the depth prior from refractive stereo, while maintaining high depth resolution. The size of the kernel w is 9 × 9, and the value of ϵ is 0.001 in our experiments. The aggregated cost of the fusion matching costs in our method is defined as(24)FA(pd,d)=∑qd∈wW(pd,qd)F(qd,d).Here W is the bilateral image filter [34] defined as(25)W(pd,qd)=exp{−d(pd,qd)σs2−c(pd,qd)σc2},where d(pd, qd) is the Euclidean distance between pdand qd, c(pd, qd) is the sum of differences of colors of RGB channels, and σsand σcare the standard deviations for spatial distance and color difference, respectively. In our experiment, we selected the window size, σs, and σcas 9, 7, and 0.07 respectively.Suppose the depth of point pdis estimated as Z(pd) from refractive stereo. As we compute the refractive matching cost and aggregate the cost per discrete depth interval Δz in refractive stereo, let the actual depth of pdbe between(Z(pd)−Δz)and(Z(pd)+Δz)as Zprevand Zpost. The corresponding disparities of Zprevand Zpostcan be computed as dprevand dpostusing Eq. (1). Note that dpostis smaller than dprev. We therefore estimate the optimal disparity D(pd) by searching the aggregated cost volume FA(pd, d) within the range [dpost, dprev] as follows:(26)D(pd)=argmindFA(pd,d).Note that we compute Eq. (24) within the range of [dpost, dprev] exclusively for computational efficiency.The ground true disparity of an orange pixel in Fig. 10(a) is approximately 200. However, the disparity from binocular stereo was estimated as 160 because the minimum aggregated cost of binocular disparity has a local minimum, yielding a wrong depth estimate. As a result, we were motivated to take a coarse-to-fine approach using both binocular and refractive disparity maps. As shown in Fig. 10(b), the refractive depth map tends to have fewer spatial artifacts. We use this refractive disparity map as a guide map for searching aggregated disparities. The search range is set to [dpost, dprev] around the refractive depth estimate with a threshold. To this end, we are capable of preventing faulty estimates in our stereo fusion.

@&#CONCLUSIONS@&#
We proposed a novel optical design combining binocular and refractive stereo and introduced a stereo-fusion workflow. Our stereo fusion system is capable of estimating depth information with a high depth resolution and fewer artifacts at a speed that is competitive with other local and global binocular methods. We validated that our proposed method combines the advantages of both traditional binocular and refractive stereo. Also, our refractive calibration method makes our system more efficient than the previous method [12] by reducing the calibration cost of refractive stereo. Our quantitative and qualitative evaluation demonstrates that our fusion method outperforms the traditional homogeneous methods in terms of artifacts and depth resolution. In addition to these advantages, our stereo fusion can be easily integrated into any existing binocular stereo systems by simply introducing a transparent medium in front of a camera, allowing for a significant improvement in a depth map with fewer artifacts.