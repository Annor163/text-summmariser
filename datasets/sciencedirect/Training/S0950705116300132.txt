@&#MAIN-TITLE@&#
Item-based relevance modelling of recommendations for getting rid of long tail products

@&#HIGHLIGHTS@&#
The liquidation of long tail items can be assisted by recommender systems.We propose a probabilistic item-based Relevance Model (IRM2).IRM2 outperforms state-of-the-art recommenders for long tail liquidation.

@&#KEYPHRASES@&#
Recommender systems,Collaborative filtering,Relevance models,Long tail,

@&#ABSTRACT@&#
Recommender systems are a growing research field due to its immense potential application for helping users to select products and services. Recommenders are useful in a broad range of domains such as films, music, books, restaurants, hotels, social networks, news, etc. Traditionally, recommenders tend to promote certain products or services of a company that are kind of popular among the communities of users. An important research concern is how to formulate recommender systems centred on those items that are not very popular: the long tail products. A special case of those items are the ones that are product of an overstocking by the vendor. Overstock, that is, the excess of inventory, is a source of revenue loss. In this paper, we propose that recommender systems can be used to liquidate long tail products maximising the business profit. First, we propose a formalisation for this task with the corresponding evaluation methodology and datasets. And, then, we design a specially tailored algorithm centred on getting rid of those unpopular products based on item relevance models. Comparison among existing proposals demonstrates that the advocated method is a significantly better algorithm for this task than other state-of-the-art techniques.Recommender systems are becoming increasingly popular to help users with the finding of relevant items. On the one hand, users are becoming more demanding and, on the other hand, more amount of information is available. For this reason, many e-commerce companies have started to use these systems for product recommendation with the intention of increasing the number of sales. From the business perspective, recommenders are effective tools to improve user satisfaction and, thus, sales revenue. The former is achieved through high quality recommendations whilst the latter is more dependent of the seller circumstances. It is hard to establish what is a good recommendation. Research efforts have focused on accuracy since the very beginning of the field of recommender systems. However, recent studies have pointed out the importance that other properties such as serendipity, novelty or diversity may have as key features to deliver good recommendations [14,23].There exist several aspects that should be taken into account with respect to revenue enhancement. Fleder and Hosanagar have analysed thoroughly the effect of recommender systems in sales diversity [12]. These authors concluded that recommenders can increase sales if they discount popularity appropriately generating more diverse suggestions. The key idea behind this study is that recommenders should not focus solely on popular products but also on long-tail items. This proposition is the main idea of Anderson’s book The Long Tail: Why the Future of Business Is Selling Less of More[2]. He coined the term long tail to refer to those less popular products that have a low demand in large catalogues. In the current context of e-commerce, he declared that promoting long tail items is crucial for both the user and the business: customers may discover new and unexpected relevant products while the companies may sell the majority of their stock. He claimed that a retailing strategy based on selling a large number of products in small quantities is more profitable than a business centred on selling large amounts of a small set of popular products.Traditionally, recommendation approaches in the long tail [27,39] have explored ways for promoting long tail items in the recommendation process. However, addressing the long tail problem in that way is not particularly novel because a high performance recommender should recommend both popular and long tail items according to what fits best to the users’ tastes. A growing body of literature has focused on improving the diversity of recommender systems [1,34]. Since more diverse recommendations are expected to lead to a larger catalogue coverage, more long tail products are likely to be recommended. In the same way, many studies have analysed the importance of novelty in recommendation. A recommendation is considered novel when the recommended item is unpopular (such as a long tail). Although enhancing diversity and novelty is a way for recommending more long tail products, we argue that there will still be items that vendors will be unable to sell.The long tail nature of a product can be produced by very different factors: it is rarely sold and therefore has very few ratings, it is sold but it has almost no ratings given its embarrassing nature (e.g., sex toys) or it is barely recommended by the existing recommender. All these aspects make recommending in the long tail an extremely difficult task. Thus, effectively suggesting all the products of companies’ catalogues is almost impossible following a unified approach.In business terminology, the items from the inventory of a company that cannot be successfully sold are called excessive stock or overstock. Overstock can be the result of poor prediction of product demand but it can also be a consequence of market fluctuations.Thus, an effective process management and a diverse recommender may minimise overstock effects. However, as we stated before, despite the efforts oriented towards improving sales of long tail products, there will be situations when companies will not be able to sell some items. In this case, we think that a specially designed recommender may provide interesting benefits. Therefore, in this paper, we develop a formal recommender model for dealing with these products which may help companies to liquidate their excessive stock.To the best of our knowledge, the specific problem of getting rid of long tail items (i.e., liquidating stock) has never been addressed systematically. Thus, first, we state the overstock clearance problem formally, we study how to evaluate this task and how this process differs from the classic recommendation problem. Next, we propose three methodologies for estimating which items are part of the excessive stock of businesses given a standard recommendation dataset. Then, we adapt a collaborative filtering approach whose roots lie in pseudo-relevance feedback (a well-known task in the Information Retrieval field) to the overstock liquidation problem. This approach builds a statistical relevance model of each of the long tail items which enables the identification of target users for selling those particular items. Finally, we conduct a series of thorough experiments to analyse and compare the performance of our proposal with other collaborative filtering algorithms. The results confirm our intuition that a probabilistic item-based relevance model enables to build an effective recommender to get rid of the long tail products.In brief, the contributions presented in this article are: (1) the formulation of a novel recommendation task consisting in liquidating long tail items, (2) the proposal of three different methodologies for estimating the overstock products of a dataset and its critical analysis, (3) the design of a collaborative filtering algorithm based on relevance modelling for the task of liquidating long tail items and (4) the empirical comparison of different collaborative filtering algorithms (including the proposed one) under this novel task.The objective of a recommender system is to elaborate personalised rankings of products for each user. Every recommendation task involves a set of products or items (we will use these two terms interchangeably), which we denote asI,and a set of possible customers or users, denoted asU. Recommendation in the long tail refers to the generation of item recommendations to users including not only popular products but also long tail ones. However, here we propose a different approach: for those items in the long tail we want to get rid of, we want to identify those potential users that will buy the product, even when that item is not on the top of the users’ preferences. Of course, this approach does not replace the classic recommendation. On the contrary, it is specially designed to address a business concern and should be used as a complement to current methods in a operational setting. We claim that recommenders may help to get rid of long tail products which can be seen as a proxy representation of the overstock phenomena.Recommender systems are usually classified in three main categories: content-based, collaborative filtering and hybrid techniques [6,28]. While content-based recommenders employ the properties of the items to recommend similar items to those appreciated by the user [20], collaborative filtering techniques rely on data about past interactions between users and items (ratings, purchases, clicks, etc.) [11]. And, finally, hybrid methods combine algorithms from both families. Content-based algorithms are usually preferred for improving novelty because they find similar items based on content, not on popularity [24]. However, sometimes the available information about items is not adequate for using this family of methods. Additionally, content-based recommenders may lead to over-specialisation suggesting only items that are very similar to those rated by the target user [11]. Therefore, we propose a collaborative filtering approach specially designed for the task of getting rid of the long tail.Since we are focusing on a pure collaborative filtering approach, we are interested in the users’ feedback. We are assuming that we have explicit feedback, i.e., ratings from users to items. However, our proposal is also suitable for dealing with implicit feedback such as clicks or purchase information that can be represented with binary ratings. We indicate the relationship between the user u and the item i with the rating ru, i. We can organise these ratings in a user-item matrix R. Additionally,Uirefers to the subset of users that rated the item i. Similarly,Iudenotes the subset of items that have been rated by the user u.Collaborative filtering algorithms exploit the past interactions between users and items to create meaningful recommendations. We can classify these techniques in two main categories [11]. Model-based approaches build a predictive model from the ratings. In contrast to these systems, which learn patterns from historical data, memory-based methods (also known as neighbourhood-based) actually employ all the ratings stored in the system. Memory-based algorithms can compute suggestions using the information of like-minded people (user-based) or, on the other hand, recommend items that are similar to the ones the user liked in the past (item-based).The classic task in the field of recommender systems consists in estimating, given a particular user, the top items for which this user is most likely to be interested in. This task can be formulated as finding a scoring functions:U×I→Rsuch that, for each user u, we can generate a ranked list of k itemsLuk∈Iksorted by decreasing score order.In this article, we propose a novel recommendation problem: instead of generating the best item suggestions for each user, we aim to find the best users for each long tail product. The inversion of the classic recommendation task (recommending users to items in the place of suggesting items to users) was recently studied to improve sales diversity [34]. These authors explored the inverted task and proposed a probabilistic approach that enhances sales diversity. Still, at the end, their intent was to improve the original recommendation problem (suggesting items to users). In contrast, our intention is to address a very different problem: how to get rid of the excessive stock suggesting the most suitable users for each item.We use the notationI′to identify the subset of items (I′⊂I) that are the part of the catalogue that conforms the products we want to liquidate. For a particular itemi∈I′,we intend to find a ranked list of k users,Lik,that are most likely interested in such item i. Thus, using the user-item matrix R, our objective is to find a scoring functions′:I′×U→R. It is important to remark that we use the ratings emitted to all the items,I,but we are only interested in generating recommendations for the overstock products,I′,exclusively.Sales on e-commerce sites depend on multiple factors. Seasonality is especially relevant because some products become outdated while new items appear. Additionally, other factors such as price fluctuations or changes in the users’ needs may affect sales numbers. In order to deal with these issues, we can liquidate stock at the end of the season. Therefore, the recommendation algorithms should use the available data of the season to generate stock liquidation suggestions at the end of this period. Users’s needs should not change extremely during the season and price should be managed according to the liquidation policy of the company. In this way, we can effectively apply a collaborative filtering approach without using outdated information from past seasons.To evaluate the performance of recommender under this novel task of liquidating stock, we have to adapt current evaluation methodologies for classic recommendation. We describe this process in Section 4.3. Additionally, we need to define what is the set of long tail productsI′. To the best of our knowledge, there is no public collection available for research that specifies excessive stock. Since we lack of datasets with such kind of information, we designed three approaches to estimate the subset of overstock items. We should note that, for this purpose, we use all the information in the datasets (without training and testing splits) because we are merely designating which items are excessive stock. The splits of the collections will be used to train and evaluate the recommenders. The experiments based on the collections built with the methods here described will be presented in Section 4.We can argue that the least rated items conform the set of overstock products we want to get rid off because few users have shown interest in them. This is the most common approach used in recent studies [9,39]. Thus, given the threshold c1 we can select a subset of items that have less than c1 ratings/purchases:(1)I′={i∈I||Ui|<c1}We remind thatUirefers to the set of users that rated the item i. We decided to use a fixed threshold instead of relying on a percentage (e.g., taking the 5% of least rated products) because in this way we can assure that the selected items are rarely sold (if we choose a proper threshold). In contrast, there will be always items in the bottom percentiles, but we cannot assure that those are long tail items, only that they are the least sold in the dataset.Another approach to the estimation of the overstock products is to suppose that the items with the lowest ratings are the ones that cannot be sold by the company. We only need to choose a value for c2 to specify the threshold of what is a low rating.(2)I′={i∈I|∑u∈Uiru,i|Ui|<c2}Again, the same motivation as in the preceding strategy favours the use of a fixed threshold instead of a percentage. Please note that it is not advisable for a traditional recommender system to recommend low quality items (probably those with low ratings) to their users. However, in this paper, we are tackling a very different problem: we want to get rid of long tail items, no matter the cost. Thus, we are devising a recommendation technique that is able to select which users are the potential buyers of a long tail item. We leave to the manager the responsibility of choosing if liquidating items with very low ratings is a good idea (perhaps offering an important discount). In this work, our objective is to present a recommendation algorithm that is capable of doing so if needed.Finally, according to a recent study which has pointed out the effect of recommendations diversity in e-commerce sales [12], it is plausible to state that those products that a standard recommender system does not suggest are not going to be sold. Thus, given a particular threshold c3, we can construct a set of items that are not present in the top c3 results of any user recommendation list:(3)I′={i∈I|i∉Luc3,∀u∈U}In this section we address our proposal for dealing with long tail items. For this purpose, we employed Relevance-based Language Models (frequently abbreviated as Relevance Models or RM). This method is a state-of-the-art technique for performing automatic query expansion via pseudo-relevance feedback [19]. In text retrieval, a user introduces a query into the system and the outcome is a list of documents ranked according to an estimated relevance measure between the documents and the query. Since the query is an imperfect representation of the user’s information need, research efforts have focused on expanding the query with new meaningful terms [8]. Pseudo-relevance feedback is a technique that aims to expand the user’s query automatically extracting terms from a set of pseudo-relevant documents [38]. These documents are obtained as the top results of an initial retrieval process that is carried out with the original query.Language Models follow the Probability Ranking Principle which states that documents should be ranked in descending order of probability of relevance [29,40]. Despite Language Models do not explicitly include the concept of relevance, they can be derived from a generative relevance model [18]. In contrast, Relevance-Based Language Models estimate a relevance model for each query [19].Recently, [26] adapted RM to the collaborative filtering task achieving high accuracy figures. The key idea is to adapt the pseudo-relevance feedback paradigm to recommendation. This involves mapping a triadic space (queries, documents and terms) to a dyadic one (user and items). Queries and documents are both mapped to users while terms play the role of items. Thus, instead of expanding a query with new terms, user profiles are expanded with items from a pseudo-relevant set. We can compute this set calculating the target user’s neighbourhood using some standard techniques in the field of Recommender Systems.In this paper, we designed a different approach to collaborative filtering using also Relevance Models. We propose the construction of a Relevance Model for each long tail product. We intend to expand item profiles with relevant users using information from similar items. The objective is to estimate the probability of a user u under the Relevance Model of an item i, p(u|Ri). Note that our approach to collaborative filtering recommendation using RM is not just the item-based version of the model proposed in [26]. In [26], the authors built a Relevance Model for each user and estimated the probability of relevance of each item, p(i|Ru). We cannot apply directly this model to the long tail liquidation task because probabilistic relevance estimates across users are not comparable. Given two users u and v and the long tail item i we cannot generate recommendations of users to liquidate that item sorting p(i|Ru) and p(i|Rv) since we are comparing estimates from different Relevance Models. Likewise, we cannot apply Bayes’s Theorem to get the bayesian inversion because the estimation of p(Ri|u) o p(Rj|u) does not make sense within this probabilistic framework. Therefore, we need to build a RM for each item if we want to model long tail items and choose the best users for them.There exist two methods for approximating a Relevance Model: assuming independent and identically distributed sampling (RM1) or conditional sampling (RM2) [19]. In this paper, we focus on the latter because it has demonstrated better performance on the recommender task in our experiments. We refer to the adaptation of the RM2 model to the long tail liquidation problem as IRM2 (Item Relevance Modelling 2). Next, we present the derivation of the IRM2 algorithm based on the previous work on Relevance-Based Language Models for pseudo-relevance feedback [19].IRM2 aims to build a relevance model Rifor each long tail producti∈I′of the collection. In this way, we can estimate the relevance of each user for a given long tail product. Thus, in our stock liquidation task, we can define an Item Relevance Model as a formalism that enables to compute the probability that a user u rates a long tail item i, p(u|Ri). We ignore which users will rate certain product, but we can use the history of ratings of that item and other relevant items to estimate it.We assume that the relevance model Rigenerates the long tail item i but also a set of relevant items Ji. Fig. 1illustrates how the item i and the set of relevant items Jiare random samples from an unknown relevance model Ri. As we will see below, the sampling process for the target item i can be different from the sampling for the relevant products Ji. With these assumptions, we are going to estimate p(u|Ri) for each useru∈U.First, we sample item i from the underlying relevance model Ri. This process consists in repeatedly sampling k usersv1⋯vkfrom Ri. These users correspond to the clients that bought the item i,Ui={v1⋯vk}. Since we sample k users for item i,k=|Ui|. To estimate p(u|Ri), which is unknown, we rely on what we observed before: the set of usersUi. We need to answer what is the probability that the next user we sample from the relevance model will be u. Formally, we can formulate this question in the following way:(4)p(u|Ri)≈p(u|v1⋯vk)Applying the definition of conditional probability, we can reformulate the previous equation in terms of the joint probability of observing the user u along with usersv1⋯vkdivided by the joint probability of observing usersv1⋯vk. Note that we can safely ignore the denominator because it remains constant for the same item i (it would not affect the final ranking):(5)p(u|Ri)≈p(u,v1⋯vk)p(v1⋯vk)∝p(u,v1⋯vk)Following on the conditional sampling method proposed by Lavrenko and Croft [19], we estimate the joint probability of observing the user u along with the usersv∈Uibased on the ratings distribution of the relevant items j ∈ Ji. We present a diagram of this sampling scheme in Fig. 2.First, to estimate the relevance model of item i, Ri, we pick a user u given the prior probability p(u). This user u will condition the selection of item distributions from which we will sample the usersv1⋯vk:(6)p(u|Ri)∝p(u,v1⋯vk)=p(u)∏v∈Uip(v|u)Following the scheme from Fig. 2, to estimate the conditional probability p(v|u), we repeat the following process k times: we pick an item distribution j according to p(j|u) and, then, we sample a userv∈Uifrom the item distribution j with probability p(v|j). Note that this sampling strategy considers that usersv1⋯vkare sampled independently of each other, but they are dependent on u. Applying the law of total probability we obtain the following:(7)p(v|u)=∑j∈Jip(v|j,u)p(j|u)If we assume that usersv∈Uibecome independent of u after choosing an item distribution j ∈ Ji, we can simplify Eq. 7 obtaining the following estimate:(8)p(v|u)=∑j∈Jip(v|j)p(j|u)Applying Bayes’ Theorem, we can estimate p(j|u) as follows:(9)p(j|u)=p(u|j)p(j)p(u)Finally, plugging Eqs. 8 and 9 into Eq. 6, we obtain the final IRM2 estimate:(10)p(u|Ri)∝p(u)∏v∈Ui∑j∈Jip(v|j)p(u|j)p(j)p(u)For estimating the probability of user u under the Relevance Model Ri, IRM2 iterates over the users who rated that item,Ui,and over the set of relevant items to that item i, Ji. For the sake of simplicity, we consider prior probability estimates, p(u) and p(j), uniform. However, these priors open the door to explore new estimations that may improve the performance or even include business aspects as it was done in [32] with the original Relevance Model recommender proposed in [26]. We leave this possibility for future work.With the estimate from Eq. 10 we can generate the list of recommendations Lifor each long tail itemi∈I′. Thus, for each useru∈U,we build the list Lisorting those users by decreasing estimated relevance p(u|Ri).Additionally, we need to provide two final estimation details. The first question is which items conforms the set of relevant items Ji. We explain this issue in Section 3.2. The other issue is how to calculate the probability of a user u given the item distribution j, p(u| j). For this purpose, we use the maximum likelihood estimate of a multinomial distribution over the ratings:(11)pml(u|j)=ru,j∑v∈Ujrv,jHowever, this estimate suffers from data sparsity. Thus, we need to use smoothing techniques, which are the way to solve this problem in the language modelling framework [41]. We discuss about smoothing in Section 3.3.In Eq. 10, we can observe that IRM2 leverages the information from a set of relevant items to estimate the users’ relevance. Since we ignore which items are relevant to some item i and we do not have explicit relevance feedback information, we need to compute approximation using the available data. We assumed that the similar items to item i are the pseudo-relevant items for the relevance model Ri. We use the term pseudo-relevant because we want to emphasize that these products are an approximation of the real relevant items.We employ clustering techniques for computing the set of pseudo-relevant items based on the ratings. In our case, we used the kNN algorithm which consists in taking the k nearest neighbours (in this case, the k most similar items) according to a pairwise similarity [11]. Even though Pearson’s correlation coefficient is the most common measure, we employed cosine similarity which yielded better results in our experiments—this outcome is consistent with results reported for state-of-the-art top-N recommenders [9]. The cosine similarity s between two items i and j is given by the following formula:(12)s(i,j)=∑u∈Ui∩Ujru,iru,j∑u∈Uiru,i2∑v∈Ujrv,j2The three most popular methods for smoothing Language Models are Jelinek-Mercer, Dirichlet priors and Absolute Discounting [41]. These methods smooth the maximum likelihood estimate pml(u|i) (Eq. 11) with a background model which, in our case, is the user probability in the collection:(13)p(u|C)=∑j∈Iru,j∑v∈U,j∈Irv,jThe main goals of smoothing are: on the one hand, preventing the apparition of zeros due to the data sparsity and, on the other hand, adding the effect of the idf (inverse document frequency) [41]. Additionally, some smoothing techniques also provide the effect of length normalisation [21].The impact of smoothing methods for RM has been studied before in the context of the classic recommendation task [33] concluding that Absolute Discounting is the best choice. Despite our work is devoted to a different task—liquidating excessive stock—, we found the behaviour of these techniques under this new problem is similar. The optimal values of the smoothing parameters vary, but Absolute Discounting is still the best option.Absolute Discounting subtracts the same constant, δ, from the count of all the seen ratings. Then, a count proportional to the probability in the collection is added to each user:(14)pδ(u|j)=max(ru,j−δ,0)+δ|Uj|p(u|C)∑v∈Ujrv,jThus, the above estimate is used to calculate p(u|j) and p(v|j) in the computation of the item relevance model (Eq. 10).

@&#INTRODUCTION@&#


@&#CONCLUSIONS@&#
In this paper, we proposed a new unstudied problem in the field of recommender systems: how to liquidate long tail items (sometimes called overstock). Vendors usually have remaining products in their catalogue they wish to get rid of. Thus, an algorithm that computes which users would be interested in a certain item is worthwhile for targeting users in liquidation campaigns. We described this task formally and we designed a collaborative filtering algorithm to cope with this problem.We proposed three strategies to estimate the long tail items from a recommendation dataset based on the number of ratings, on the average value of the ratings or on the recommendation frequency. Additionally, we used a dataset with purchase information. Nevertheless, it would be very valuable to obtain a public available dataset with real information about excessive stock instead of elaborating approximated evaluation sets.We designed a probabilistic collaborative filtering algorithm, IRM2, that builds a relevance model for each long tail item. This approach outperformed a set of representative state-of-the-art recommendation algorithms in our experiments. We also found that traditional item-based approaches worked better than user-based ones.As future work, we think that it would be interesting to analyse the effect of IRM2 priors in the proposed task. Previous studies have demonstrated that different prior probability estimates can improve the accuracy of the recommendations in the traditional scenario [32]. Moreover, the probability distributions of the user and item priors from Eq. 10, p(u) and p(j), can be modified to introduce business rules into the model. For example, we can demote the probability of certain VIP users because we do not want to overflow them with liquidation advertisements.Another critical aspect of IRM2 is how to compute neighbourhoods. In this work, we have used kNN with cosine similarity for this purpose. Further investigation on more sophisticated approaches—such as matrix factorisation methods [26] or cold-start clustering techniques [35]—might shed light on enhancing the quality of the recommendations.