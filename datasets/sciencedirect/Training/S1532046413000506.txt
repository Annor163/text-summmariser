@&#MAIN-TITLE@&#
With how many users should you test a medical infusion pump? Sampling strategies for usability tests on high-risk systems

@&#HIGHLIGHTS@&#
Legacy approaches to sample size prediction in usability studies are faulty.Late control means adding new users until a preset target for discovery is met.The LNBzt model predicts completeness of discovery.Low effectiveness and high volatility call for large samples.A novel idiosyncrasy score discovers atypical users (for embracing them).

@&#KEYPHRASES@&#
Usability,Infusion pump,Patient safety,Ergonomics,User testing,Sample size,

@&#ABSTRACT@&#
Usability testing is recognized as an effective means to improve the usability of medical devices and prevent harm for patients and users. Effectiveness of problem discovery in usability testing strongly depends on size and representativeness of the sample. We introduce the late control strategy, which is to continuously monitor effectiveness of a study towards a preset target.A statistical model, the LNBzt model, is presented, supporting the late control strategy. We report on a case study, where a prototype medical infusion pump underwent a usability test with 34 users. On the data obtained in this study, the LNBzt model is evaluated and compared against earlier prediction models.The LNBzt model fits the data much better than previously suggested approaches and improves prediction. We measure the effectiveness of problem identification, and observe that it is lower than is suggested by much of the literature. Larger sample sizes seem to be in order. In addition, the testing process showed high levels of uncertainty and volatility at small to moderate sample sizes, partly due to users’ individual differences. In reaction, we propose the idiosyncrasy score as a means to obtain representative samples. Statistical programs are provided to assist practitioners and researchers in applying the late control strategy.

@&#INTRODUCTION@&#
Well-designed medical devices of good quality are necessary for providing safe and effective clinical care for patients. Capturing the user requirements and incorporating them into the design is essential. Therefore, the field of Human Factors has an important role to play in the development of medical devices, all the more so because numerous reports show clear links between hazards and usability problems [1,2].The field of usability engineering has developed an array of methods to identify usability problems, most importantly empirical usability testing. However, the practices for usability testing have been established for evaluating non-critical systems, such as commercial websites. A major impact factor for effective usability tests is the sample size, but the prevalent recommendations for usability testing studies may not be adequate for safety critical systems, such as medical devices. Moreover, due to mathematical misconceptions [3], prevalent recommendations generally understate the adequate sample size. In consequence, a considerable number of usability problems can go unnoticed, placing severe risks on patients and users. In this paper we present a rigorous approach to sample size estimation, that, in essence, continuously tracks the completeness of problem discovery.The presented approach bases on an updated mathematical model for sample size estimation (previously introduced in [4]). We applied it in a case study, where the prototype of a medical infusion pump is tested. First, we measure the observed effectiveness and compare it to classic models. Next, we examine the reliability of predictions and compare it to the volatility of the usability testing progress. Finally, we compare the impact of two professional groups (nurses and anesthesiologists) and extend the approach to also assist in compiling representative user samples.The report “To err is human” from the Institute of Medicine [41] greatly increased people’s awareness about the frequency, magnitude, complexity, and seriousness of medical accidents. As many as 100,000 deaths or serious injuries each year in the US result from medical accidents. Similar reports have been issued by other authorities, e.g. France [42] and the UK [43]. Between 2005 and 2009, the FDA collected approximately 56,000 reports of adverse events associated with the use of infusion pumps, which are medical devices that deliver fluids into a patient’s body in controlled amounts [21]. A significant number of reported adverse events, many of them led to injuries and deaths, is due to device use errors. These are errors in how a medical device is used, rather than a technical malfunction. It is now widely recognized that poorly designed user interfaces induce errors and operating inefficiencies [44], even when operated by well-trained, competent users.The recognition of the role of good design has resulted in a number of studies investigating the usability of medical devices, most notably infusion pumps [1,2,5–7]. User interfaces of medical equipment demand a high level of reliability in order to create prerequisites for safe and effective equipment operation, installation and maintenance [8]. Poorly designed human–machine interfaces in medical equipment increase the risk of human error [1,9], as well as incidents and accidents in medical care. Medication errors are estimated to be the major source in those errors that compromise patient safety [10–15]. These, together with other common problems with infusion pump design, may predispose health care professionals to commit errors that lead to patient harm [16]. The most common cause in erroneous handling during drug delivery tasks stems from the fact that operators have to remember (recall) everything that was previously entered, as well as detecting and recovering from errors in confusing and complex programming sequences [1,17]. Not surprisingly, most reported problems are identified as originating from lack of feedback during programming, even though interfaces should function as an external mental map (cognitive artifact) in supporting monitoring and decision making processes [17]. Infusion pumps contain numerous modes of functioning, and often present poor feedback about the mode in which they are currently set. Also, buttons are often illogically placed and marked [6]. Previous research indicated that causes for programming and monitoring difficulties resulted from infusion device complexity (flexibility), hidden behind simplified pump interfaces not designed from a human performance and fallibility point of view [18]. Users therefore become more and more a victim of clumsy automation [19], loss of situational awareness and mode confusion, often unrecognized as cause in many of the problems reported.That user-interface issues with infusion pumps are widely regarded serious, is reflected by the FDA’s recent initiative to improve pump safety [21]. In order to assure that use-related hazards have been adequately controlled, the FDA states that three central steps are essential [22]:1.Identify anticipated use-related hazards (derived analytically, for instance by heuristic analysis) and unanticipated use-related hazards (derived through formative evaluations, for instance simulated use testing).Develop and apply strategies to mitigate or control use-related hazards.Demonstrate safe and effective device use through human factors validation testing (either simulated use validation testing or clinical validation testing).The analytical approaches and formative evaluations are complementary, each having unique strengths and weaknesses with respect to identifying, evaluating, and understanding use-related hazards early in the design process. Formative evaluations can demonstrate sufficient use-safety for an infusion pump. Formative evaluation has its strengths in a focus on critical tasks, challenging or unusual use scenarios and the follow-up to determine the cause of task failures. Potential limitations of formative evaluation include artificial testing conditions and limited range of users and use conditions. Clinical validation testing has its strengths in realistic testing conditions (e.g., time pressure, distractions, noise, glare), a broader range of users, and unanticipated use conditions, but potential limitations include lack of control over use scenarios and testing conditions.In reaction to what one could call the “ergonomic crisis”, numerous works have aimed at transferring established concepts of user-centered design to the domain of medical devices. In particular, usability evaluation gained attention: Martin et al. review a number of user-centered methods for requirements analysis and usability evaluation of medical devices [17]. In their conclusion, they clearly favor usability testing over expert inspection methods such as heuristic evaluation and cognitive walkthrough. Liljegren reports on a survey on the importance of several usability criteria for medical equipment and found ‘difficulty to make errors’ ranked highest [20]. From a subsequent assessment of several evaluation methods it was concluded that usability testing is the most effective evaluation method.In the current paper, we take the position that measuring and controlling the effectiveness of formative evaluation, usability testing in particular, is crucial for risk reduction in the development of medical devices. Undiscovered design faults decrease performance (e.g., by imposing higher cognitive workload) and raise the probability of hazard (e.g., mistakes made when inserting or modifying the dosage), harming peoples’ health. While many studies have addressed various impact factors on effectiveness of usability evaluation, there is general agreement on one factor: the sample size. However, Bastien [23] reviews the usability testing method for medical applications and concludes that the “question of the number of users to test is far from being solved and requires further research” (p. 20). While the importance of sample size is beyond doubt, quantifying its impact has seen a long and heated discussion [24]. Several authors suggested so-called magic numbers [25,26]. Others introduced mathematical models to estimate the required sample size and a third fraction claims the whole issue practically irrelevant [27].A central assumption in this paper is that usability researchers in the domain of medical devices have at least three good reasons to strive for effective discovery of usability problems: First, medical devices are high risk systems: many past incidents have shown that poor usability can cause use-related hazards and, in consequence cost lives [16]. Second, authorities have acknowledged the problems and manufacturers are now liable for thorough testing of the devices [28]. And third, medical devices are embedded devices, with much of the functionality still provided in hardware. It is well known fact in systems engineering that late fixes of safety–critical embedded devices are extremely costly [29].In the following sections we give an overview on possible strategies for sample size managements, as well as basic statistical ideas to estimate effectiveness and sample size. The mathematical background of these ideas is introduced in Section 2.The question of sample size is typically posed as: how many subjects are required for testing so that at least, say 85%, of the existing usability problems are discovered? The usability researcher aiming for effective usability testing of a medical device, in principle has three approaches at her disposal for controlling the sample size. The magic number approach assumes that all studies are similar in how fast they reach completeness with increasing sample size, hence it sets the sample size a priori. Lewis [30] introduced early control where the sample size gets estimated from the first few sessions, which may still be early enough for assigning resources to a project. Unfortunately, it seems that with small samples, early estimates are far too uncertain to be of practical value [4]. In consequence, a late control strategy has been suggested that can guide the process towards the targeted completeness of discovered usability problems [3]. The usability researcher continuously monitors the progress, and invites further participants to the testing lab, until the preset target is reached with sufficient confidence. In the current paper, we show how the late control strategy applies to usability testing of high risk systems, by example of a prototype medical infusion pump.All above-mentioned strategies ground on mathematical estimators for the effectiveness of a usability evaluation process, in order to predict the required sample size. Virzi [31] was among the first to propose that the discovery rate d of usability problems follows a geometric series, depending on the probability of detection p and sample size n.(1)d=1-(1-p)nReviewing 11 usability evaluation studies, Nielsen and Landauer [32] found that p averages to approximately .31. However, p seemed to vary considerably between studies (sd=.12), leaving considerable uncertainty about the effectiveness of a particular study. Still, many practitioners and academics have come to the belief that this average p holds for any usability study, hence the often made recommendation that five users suffice to find 85% of the problems. Hwang and Salvendy [33] set out to correct the number 5 by an updated meta study, concluding that the magic number is to be found in the range of 10±2. In a recent review of this debate, the first author reached the conclusion that magic numbers are simply meaningless [3].Two mathematical misconceptions led past researchers to overrate the average effectiveness of usability testing: assuming homogeneity of problem visibility (i.e., a problem’s likelihood of being discovered) and completeness of problems. Lewis [30] pointed out the problem of incompleteness with the original geometric series model. The naïve estimator for p depends on the true number of problems as:(2)pˆ=#successfuldiscoveryevents#problems×samplesizeIn principle, the true number of problems is unknown to the researcher, since the set of usability problems progressively emerges with increasing sample size. In the early stages of an evaluation study, the number of known usability problems can be much lower than the true number of usability problems. Hence, using the number of so-far-discovered problems in (1) will grossly overstate p. In (1), an overstated p yields an overly optimistic estimation of discovery rate d and an underestimation of sample size necessary for a preset target.A second insufficiency of the geometric series model is that it assumes homogeneous visibility of all usability problems. That is, p has the same value for every type of problem [34]. This is an unrealistic assumption and several researchers have expressed their disbelief of homogeneous visibility [35,36]. What has long been overlooked is that variance in visibility of problems substantially decelerates the progress of finding problems. The 2006 edition of the International Encyclopedia of Ergonomics and Human Factors says, “There is no compelling evidence that a probability density function would lead to an advantage over a single value for p.”[37]. The opposite seems to hold: the assumption homogeneous visibility is typically false and ignoring visibility variance results in severely overestimating the true progress in problem discovery [34].The now classic geometric series model for sample size estimation is optimistically biased for two reasons: incompleteness is not regarded and the model assumes homogeneous visibility. Lewis [30] suggested a first solution to incompleteness, a smoothing method for binomial data known as the Good-Turing adjustment. Another solution is to use zero-truncated distributions. Zero truncation is the more general solution than the Good-Turing adjustment as it applies to a wider range of count data models, especially those incorporating visibility variance.Earlier [4], we introduced the zero-truncated logit-normal binomial model (LNBzt), accounting for both issues: incompleteness and visibility variance.With the LNBzt model it is possible to•estimate the proportion of usability problems that rest undiscovered at a given point in time,extrapolate the evaluation process and predict the required sample size for a given discovery target, say 85% of the usability problems,determine the accuracy of predictions by constructing confidence intervals.In Section 2, the mathematical background is explained in more detail. Furthermore, the appendix of this paper provides the basic statistical programs necessary to perform the late control strategy by virtue of the LNBzt model.The current EU guidelines on usability of medical devices NEN-EN-IEC 62366 [28] recognizes that diversity of users is an issue (p. 48) and explicitly asks for representative user samples.As Caulton [35] argued, completeness of problem discovery can be very much a question of representative sampling. Many factors may play a role for users’ expectation, interaction style and performance in operating a device. Different professional groups may use a device with different backgrounds, have different tasks and work under different conditions. Previous experience may have positive or negative consequences on performance with a newly designed device [38]. While domain expertise may prevent a user from making certain mistakes, experience with legacy devices may cause a negative transfer [39].Following the arguments of Caulton [35], discovery of usability problems likely is incomplete, if a certain subpopulation of users is omitted. If a user type is omitted or under-represented in the sample, the usability researcher is at risk to overlook usability problems that in practice may cause hazards.The differences between professional groups is explicitly mentioned in the FDA draft guidelines [40], pointing out that members of professional groups potentially differ in their requirements and, in consequence, experience different problems when working with a device. The draft guidelines recommend testing 15 users of each major user group during validation testing.The current FDA guidelines [22] are less explicit about the sample size per user group, but make another important point about user diversity: “Outlier data from performance measures is often informative and should be investigated to determine the nature and pattern of the use scenarios associated with them.” (p. 26). Not all user traits influencing interaction with the device can be known in advance, and sampling by professional groups may not capture the full diversity. Later in this paper, we suggest a procedure to discover under-represented user groups by identification of untypical subjects in the sample. This lends itself to an improved, adaptive sampling strategy, beyond pure sample size considerations.The aim of this paper is to advance strategies for rigorous usability testing of medical devices. In the past, much has been said about effectiveness of usability evaluation methods, see [45] for a critical review. The focus here is on strategies for managing the sample. More specifically, we examine how the LNBzt model applies to late control of usability testing studies.First, the overall fit of the LNBzt model will be compared to the legacy geometric series model. Second, a Monte-Carlo sampling experiment shows how well the LNBzt model interpolates the observed progress of problem discovery. Third, we examine how consistent sample size predictions are. Fourth, we evaluate reliability of the discovery process and precision of estimates, by assessing the amount of uncertainty and volatility in problem discovery. Fifth, we examine the differences between professional groups and, finally, propose a procedure to identify untypical subjects (“outliers”) in the sample.The following sections explain the mathematical background of the LNBzt model.2This section can safely be skipped by the impatient or mathematically inapt reader.2The statistical programs for doing basic sample size control with the LNBzt model are provided with the electronic copy of this paper and demonstrated in Appendix A.The classic geometric series model (1) for sample size prediction can be derived in a number of ways: first, it is a growth curve with diminishing returns. The number of discovered problems asymptotically reaches the true number of problems (preview Fig. 3). A consequence of this asymptotic behavior is that with increasing number of test sessions the gain in terms of newly discovered problems decreases. Second, the geometric series model is the cumulative distribution function (CDF) of the geometric probability distribution. The geometric probability function expresses the probability for a certain number of failures before the first success in a Bernoulli experiment. In the usability evaluation process the Bernoulli trials are the participants’ “attempts” to stumble upon a usability problem.Finally, the geometric series model can be derived from the better known binomial distribution. The binomial probability distribution function pdfBin(k|p,n) expresses the probability of k successes with n trials and a basic probability of success p.(3)pdfBin(k|n,p)=nppk(1-p)n-kThe binomial pdf predicts the number of successes. In usability testing, however, progress of discovery occurs when a problem has been observed at least once. The relevant question is: how likely is it that a problem is discovered at least once, hence k>0? The mathematical problem simplifies by taking the opposite event: how likely is it that a problem remains undiscovered after n sessions, hence k=0. Let PD(n|p) denote the probability of successful discovery with basic probability p and sample size n, obtains the geometric series formula:(4)PD(n|p)=1-pdfBin(k=0|n,p)=1-(1-p)nThe binomial model, from which the geometric series formula derives, has a remarkable property: variance depends strictly on the parameter p, as var=np(1−p). If the observed variance exceeds this term, this is called overdispersion. Overdispersion indicates that probability of success is not fixed for all observations and instead varies.When overdispersion occurs, the distribution has fatter left and right tails compared to the binomial distribution.3This is formally expressed by the Two-Crossings Theorem, see [69]. See [4] for an illustration.3A fatter left tail means that there is an excess in zero successes, k=0, problems that have not been discovered at all. In consequence, when the basic probability p varies over problems, the binomial model underestimates the number of zero successes, i.e. the unseen problems (preview Fig. 1). It is easily seen from (4), that the geometric series model overestimates the progress in presence of an excess in zero, arising from overdispersion.The issue is solved by adding a prior to the binomial distribution. The prior is another probability distribution to underlie parameter p. Priors are commonly used in Bayesian statistics to model previous belief. Here, the prior reflects the random variation of the parameter in the population of problems. A prior representing a random effect, not a belief, is often called empirical Bayesian prior. Statistical models with parameters allowed to vary by a prior distribution are also referred to as hierarchical models or mixture models.The prior for p has to satisfy the range of p, which is the interval [0;1]. A commonly used prior for binomial problems is the beta distribution [46]. Here, we chose another distribution as prior, the logit-normal (LN) distribution pdfLN(x|m,s2) [47]. The LN distribution features a parameter m for the central tendency and s2 for the variance. It is less common for modeling mixture models than the beta distribution, but has a few advantages. In several pilot trials of modeling evaluation process data, the LN estimation predicted very similar to the beta distribution, but yielded better precision for the parameters of interest (especially the number of remaining defects). Another useful model for researching usability evaluation processes is the Rasch model from psychometric test theory [48]. The logit is the inverse of the logistic function in the Rasch model. Under the assumption that the latent variable is normally distributed, both mathematical models are fully compatible. Last but not least, interpretation of the LN parameters m and s2 as average and variance of visibility is quite natural for a majority of researchers being familiar with the normal distribution.Letting p vary according to the LN distribution results in the logit-normal binomial (LNB) probability distribution of the form:(5)pdfLNB(k|n,m,s)=nk12πs2∫01pk-1(1-p)n-k-1e(logit(p)-m)22s2dpThis function does not simplify further, hence solving it requires quadrature methods for integration. Still, the LNB is a discrete probability distribution; therefore, deriving the cumulative distribution and quantile (percentile) functions is straightforward. The cumulative logit-normal geometric distribution function applies for predicting the rate of discovery. It is derived from the LNB in the same way the geometric series model was derived from the binomial distribution in (4).Given the frequency distribution of how many times usability problems were encountered, the LNB model allows to estimate the parameters m (reflecting the average visibility) and s2 (reflecting the variation in visibility) using the method of maximum likelihood. By virtue of the added variance parameter, the LNB model accounts for over-dispersion in the observed frequency of detection. In particular, it captures the fat left tail of the distribution, resulting in a more plausible estimate for the number of unseen problems, which is the point k=0 (preview Fig. 2).The LNB distribution, as introduced so far, has a valid range of zero to the maximum possible number of discoveries, which is the sample size n. However, with zero successes as its lower bound, the model “expects” that the observed data contains the problems that have not yet been discovered. This is insufficient, because the researcher does not have any knowledge on the number of undiscovered problems. And, as will be shown soon, estimating the number of undiscovered problems, basically is the same as estimating effectiveness. In the following it is outlined, how the model is adjusted accordingly, which naturally leads to an estimator for the number of undiscovered problems.In contrast to the smoothing methods suggested by Lewis [30] (see Section 1.3.3), here the issue is resolved by limiting the range of the LNB distribution to exclude k=0 and re-adjusting the probability mass to 1. The so called zero-truncated LNBzt function derives as follows:(6)LNBzt(k|n,m,s2)=pdfLNB(k|n,m,s2)1-pdfLNB(k=0|n,m,s2)k>00k=0By virtue of the LNBzt probability function, the parameters m and s2 can be estimated from the observed frequencies of problem encounters, excluding the undiscovered problems. These estimates are useful in two ways: First, one can derive a function for progress of discovery in the same way as the geometric series function is derived from the binomial distribution (see Eq. (4)). Second, one can easily obtain an estimator for the number of not yet discovered usability problems4Throughout, D and d are used as designators for usability problems, as p too easily is confused with probability or the binomial parameter. The reader may imagine “d” as denoting “defect” or “design flaw”.4d0 by entering the obtained estimates for m and s2 into the non-truncated distribution function. By solving the equation with k=0 and multiplying by the number of discovered problems d, one obtains an estimate d0 for the number of not yet discovered problems:(7)d0=pdfLNB(k=0|n,mˆ,sˆ)dIn the late control strategy, the estimator for undiscovered problems serves as an indicator for incompleteness, or in the opposite, the current level of effectiveness is calculated as 1−d0/d.

@&#CONCLUSIONS@&#
