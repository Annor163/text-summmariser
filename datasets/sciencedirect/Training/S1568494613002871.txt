@&#MAIN-TITLE@&#
AINet-SL: Artificial immune network with social learning and its application in FIR filter designing

@&#HIGHLIGHTS@&#
An artificial immune network with social learning (AINet-SL) for optimization is proposed.Two social learning mechanisms, i.e., stochastic social learning (SSL) and heuristic social learning (HSL), are proposed.The numerical simulation results confirm the superiority of the proposed AINet-SL in solution accuracy and convergence speed.

@&#KEYPHRASES@&#
Artificial immune network,FIR filter,Optimization,Self learning,Social learning,

@&#ABSTRACT@&#
This paper proposes an artificial immune network with social learning (AINet-SL) for complex optimization problems. In AINet-SL, antibodies are divided into two swarms. One is an elitist swarm (ES) where antibodies experience self-learning and the other is a common swarm (CS) where antibodies experience social-learning with different mechanisms, i.e., stochastic social-learning (SSL) and heuristic social-learning (HSL). The elitist antibody to be learned is selected randomly in SSL, while it is determined by the affinity measure in HSL. In order to obtain more accurate solutions, a dynamic searching step length updating strategy is proposed. A series of comparative numerical simulations are arranged among the proposed AINet-SL optimization, Differential Evolution (DE), opt-aiNet, IA-AIS and AAIS-2S. Five benchmark functions and a practical application of finite impulse response (FIR) filter designing are selected as testbeds. The simulation results indicate that the proposed AINet-SL is an efficient method and outperforms DE, opt-aiNet, IA-AIS and AAIS-2S in convergence speed and solution accuracy.

@&#INTRODUCTION@&#
As an extremely complex life protection system, the biological immune system (BIS) is able to protect our bodies from the intrusion of external pathogens. Inspired by some biological mechanisms of BIS, the artificial immune system (AIS) has been developed into a promising method in theoretical researches and engineering applications [1], such as function optimization [2,3], pattern recognition [4,5], data mining [6], anomaly detection [7,8] and RFID networks [23]. AIS provides an intelligent method with the features of learning, memorization and feature extraction. In the last two decades, many variations of AIS have been designed to solve specific problems, and they have been developed into four families: clone selection[9], negative selection[10], danger theory[11] and artificial immune network[12]. In the artificial immune network, a B lymphocyte can recognize not only antigens but also other B lymphocytes. This causes a negative response of suppressing the recognized B lymphocytes.The earlier version of artificial immune networks is aiNet proposed by Castro [12]. Further, aiNet was modified as opt-aiNet to solve multi-modal optimization problems [13]. The opt-aiNet algorithm has a well-designed convergence criterion and is able to automatically determine the population size, to combine exploitation with exploration of the affinity landscape, and to locate and maintain stable local optimum solutions. Subsequently, opt-aiNet becomes popular for global optimization in many applications [1], such as pattern classification, filtering, control systems, RFID systems. Considering the data compression quality of opt-aiNet, Stibor and Timmis presented a closeness measure between the input data set and the compressed output data set [14]. Further, inspired by omni-optimization and immune evolution process, Coelho and Zuben proposed an improved algorithm named omni-aiNet [15] to solve single and multi-objective problems. The omni-aiNet optimization is able to adjust the population size dynamically and avoid high redundancy within the population. Cob-aiNet [16] employs the concept of concentration to guide the evolution of antibodies. Inspired by PSO, an aiNet algorithm with elite learning (aiNet-EL) [17] and its enhanced version [18] were proposed. Both algorithms selectively discriminate the elitist antibody and other non-elitist antibodies during the mutation operation, and the experimental results indicate that the learning mechanisms are effective to improve the convergence speed. In order to enhance the adaptability of AIS, an improved adaptive artificial immune system (IA-AIS) [19] was designed to focus on the affinity between the antibody and the antigen. In IA-AIS, three major immune operators are revised as the affinity-based cloning operator, the affinity-based mutation operator and the dynamic suppression operator, respectively. To make use of the information on elitist antibodies, an adaptive AIS algorithm with two swarms called AAIS-2S [20] was proposed. AAIS-2S separates the candidate antibodies into an elitist swarm (ES) and a common swarm (CS) by their affinity, where the antibodies experience self-learning and elitist-learning, respectively. In essence, IA-AIS and AAIS-2S are both derived from artificial immune networks. All these achievements indicate that artificial immune networks have been studied deeply and have a broad application foreground. It is not difficult to find that in AAIS-2S, antibodies in CS only learn from the elitist antibody in ES. However, it cannot guarantee that the current elitist antibody will be able to evolve into the global optimum in the future. As a result, especially in optimizing some complex multi-modal functions, AAIS-2S has greater risk of falling into the local optimum.Considering the above problem, this paper attempts to propose an efficient artificial immune network with social-learning (AINet-SL) for complex optimization problems. In the proposed AINet-SL, the antibody population is separated into two swarms (i.e., ES and CS) based on their affinity. In the mutation operator, two different social learning mechanisms (i.e., stochastic social learning and heuristic social learning) are employed. A dynamic searching step length updating strategy is introduced to improve the solution accuracy. The proposed AINet-SL expects to quickly capture the optimal solution of complex optimization problems. Some comparative numerical simulations on benchmark functions are arranged, and the application in FIR filter designing is simulated. The results will prove the high-performance and the effectiveness of the proposed AINet-SL in convergence speed and solution accuracy.The rest of this paper is organized as follows. Section 2 reviews the artificial immune network and its derivations, i.e., opt-aiNet, IA-AIS and AAIS-2S. Subsequently, Section 3 presents the theoretical analysis and the technical details of the proposed AINet-SL with two different social learning mechanisms (i.e., AINet-SSL and AINet-HSL). And Section 4 compares the performance of the proposed AINet-SL with DE, opt-aiNet, IA-AIS and AAIS-2S by a series of numerical simulations and an industrial application. Finally, Section 5 draws some conclusions.The powerful biological immune system is able to kill the invasive pathogens by means of recognition, learning, memorization, etc., and keep the body healthy and secure. By imitating the principles of BIS based on the immune network model, artificial immune network is designed to solve real-world problems. The new significances of the terms in BIS for optimization are abstracted as follows:•Antigen: the objective problem to be solved, including constraint conditions;Antibody: the feasible solution of the objective problem;Affinity: the value of the feasible solution to the objective problem.In the basic artificial immune network, let Ab(t)∈RN*Dbe a population of candidate antibodies and Abi(t) be an individual antibody at time epoch t following(1)Ab(t)={Ab1(t),Ab2(t),…,AbN(t)},where N is the number of the candidate antibodies and D is the dimension of the candidate antibodies. The affinity function Aff(·) is used to evaluate each candidate antibody's fitness with the antigen. And in continuous space, the similarity Sim(·) between candidate antibodies is measured by Euclidian distance. After these above definitions, the pseudocode of the basic artificial immune network is presented in Fig. 1[19].The opt-aiNet [13] is an earlier version of the artificial immune network, which is used to solve multi-modal function optimization problems. In the cloning operator, opt-aiNet employs the uniform cloning strategy, where each parent antibody is multiplied by a fixed number of Nc, and all the cloned offspring antibodies but the parent one goes through an affinity-based Gaussian mutation (AGM) following(2)Abi(t+1)=Abi(t)+G⋅exp(−Aff*(Abi(t)))/β,but only one antibody with the highest affinity among Nc cloned antibodies is selected to enter the child population. In Eq. (2), β controls the decay degree of the inverse exponential function, Aff*(Abi(t)) is the normalized affinity in the interval [0,1] of the antibody Abi(t), and G is a Gaussian random variable with the zero mean and the standard deviation 1. Once the current average affinity is not significantly different from that of the previous time epoch, a similarity-based suppressor will be activated. All but the highest affinity of those antibodies whose Euclidean distances are less than a suppression threshold Thsare suppressed. And then, a number of randomly generated antibodies are introduced. Repeat the above iterative process until the termination condition is met. For more details, please see [13].During the opt-aiNet optimization, the simple proximate linear relationship between the mutated antibodies and their affinity impairs the convergence speed and the solution accuracy. In addition, the suppression threshold, which has a great effect on the performance of an artificial immune system, is just determined by trial and error. Taking account of these disadvantages, an improved adaptive artificial immune system (IA-AIS) is proposed to solve complex optimization problems [19]. In IA-AIS, an affinity-based cloning operator is proposed. In the cloning operator, the antibody with higher affinity has more offspring antibodies, and the number of the cloned antibodies is related to the normalized affinity nonlinearly, which follows(3)Nci(t+1)=round(r⋅Ncmax⋅Aff*(Abi(t))n+Nc0),where Nci(t+1) is the cloned number of Abi(t), r is a uniform random variable, Ncmax is the maximum variable scalar, n is the power factor of the control function, and Nc0 is a base integer number which encourages the contribution from antibodies with worse affinity. And a controlled affinity-based Gaussian mutation (CAGM) operator is designed to make antibodies with lower affinity mutate much more than those with higher affinity, which follows(4)Abi(t+1)=Abi(t)+γ⋅exp(−Aff*(Abi(t))/η)⋅G,where γ is the mutation scalar, and η is the control factor. In addition, IA-AIS proposes a dynamic suppressor, where the threshold Thscan be adjusted dynamically and is proportional to the similarity of antibody population following(5)Ths=min{Dij|i≠j}+ξ⋅(max{Dij|i≠j}−min{Dij|i≠j}).where Dijis the Euclidean distance between the ith antibody and the jth antibody, and ξ∈(0, 1). For more details, please see [19].Both opt-aiNet and IA-AIS use a single candidate swarm, and all the antibodies evolve independently. This will degrade the convergence speed. Thus, AAIS-2S [20] is proposed to introduce a learning mechanism. In AAIS-2S, the candidate antibodies are separated into an elitist swarm (ES) and a common swarm (CS) by their affinity, respectively. The antibodies in ES experience a self-learning mutation operator, while those in CS go through an elitist-learning mutation operator to accelerate the convergence process. The self-learning and elitist-learning mutation operators are expressed as(6)Abi(t+1)=Abi(t)+G1⋅λ(t),and(7)Abi(t+1)=Abi(t)+G2⋅exp(−Aff*(Abi(t))/η),ifAff(Abi(t))≥Aff(Abe(t))r⋅(Abe(t)−Abi(t)),ifAff(Abi(t))<Aff(Abe(t)),respectively, where G1 and G2 are Gaussian random variables with the zero mean and the standard deviation 1, r is a uniform random variable, and Abe(t) is the best antibody in ES. And these two swarms are updated every other a fixed number of time epochs following a dynamic swarm updating strategy, which guarantees that those better antibodies in CS can be upgraded into ES. In addition, the searching step length in the elitist-learning mutation operator is adjusted adaptively based on the distance among the antibodies in ES. For more details, please see [20].In the mutation operator of AAIS-2S [20] as you know, the antibodies in CS only learn from the elitist antibody in ES. Clearly, it cannot guarantee that the elitist antibody will evolve to be the global optimum in the future. Once the current elitist antibody falls into the local optimum, it is difficult for AAIS-2S to escape. This will decrease the solution accuracy.In order to overcome the above shortcoming, this paper proposes an artificial immune network with social learning (AINet-SL) for optimization. In AINet-SL, all the candidate antibodies are also separated into ES and CS, which is the same as AAIS-2S. Those antibodies with higher affinity step into ES while the remainder antibodies belong to CS. Unlike AAIS-2S, the proposed AINet-SL employs social learning mechanisms, i.e., stochastic social learning (SSL) and heuristic social learning (HSL), instead of the elitist learning mechanism. In addition, a well-designed dynamic searching step length updating strategy is introduced to improve the solution accuracy. The overall operating process and its technical details are presented as follows.An initialized population Ab(t) with N antibodies, i.e., Ab(t)={Ab1(t), Ab2(t), …, AbN(t)}, are randomly produced, where Abi(t) is the ith antibody at the tth time epoch. Note that the produced initial antibody population must satisfy the constraint conditions of the specific problem.In the clone phase, a nonlinear affinity-based cloning operator is executed. Antibodies with higher affinity are more reproductive during the clone operation. And the number of offspring antibodies is nonlinear to the affinity of the parent antibody. Thus, the cloning operator of the proposed AINet-SL follows:(8)Nci(t+1)=round((Ncmax−Ncmin)⋅Aff*(Abi(t))n+Ncmin),where Ncmax and Ncmin are the maximum and minimum number of the offspring antibodies, and n is the power factor of the control function. Fig. 2shows the curves of cloned number related with the normalized affinity. It is obvious that the curve is heavily nonlinear when n is greater than 1.Because ES owns the antibodies with higher affinity and plays an important role in local search, those antibodies in ES experience a self-learning as mutation operator following(9)Abi(t+1)=Abi(t)+G1⋅λi(t),where Abi(t+1) is the offspring antibody of Abi(t), G1 is a Gaussian random variable with the zero mean and the standard deviation 1, and λi(t) is the searching step length at the tth time epoch. The canonical artificial immune networks, such as opt-aiNet and IA-AIS, employ the fixed step length in the mutation operators. As a result, the convergence speed and the solution accuracy are effectively reduced. This is because, any elitist antibody is easy to step out of the optimum if it appears close to the optimum and the step length is too large. Or if the step length is too small, the convergence speed will be slowed down. So, a dynamic searching step length updating strategy is introduced for self-learning, and λi(t) is determined by(10)λi(t)=λi(t−1),ifAff(Abi(t))>Aff(Abi(t−1))λi(t−1)/2,elseifλi(t−1)/2≥λminλ0(k),otherwise,where(11)λ0(k)=λ0(k−1)/2,ifλ0(k−1)/2≥λminλ0(0),otherwise,and λmin is the minimum threshold of the step length.As shown in Eqs. (10) and (11), the initialized step length is set to be λ0(0) which is equal to the half of the threshold ThS. After the suppressor, the distance of any two antibodies is less than ThS, so λ0(0)=Ths/2 can maximize the searching area without overlap. If the affinity of an antibody Abi(t) is improved after the mutation, λi(t) maintains its value. Otherwise, λi(t) will be halved. However, the step length cannot be less than a lower bound λmin, because too small step length will decrease the convergence speed severely. As a result, when λi(t) is less than λmin, λi(t) will be set to λ0(k) which is decreased to the half of its previous value λ0(k−1). Also, λ0(k) cannot be less than λmin. And λ0(k) will be reset to λ0(0) when it is less than λmin. The pseudocode of the dynamic searching step length updating strategy is shown in Fig. 3.Fig. 4shows the curve of λ0(k) related with k. It is obvious that λ0(k) is halved after updated and is reset to λ0(k) when it is less than the lower bound λmin.An example of the behavior of the dynamic searching step length updating strategy is shown in Fig. 5, which illustrates the evolution of the searching step length λi(t) for the benchmark function F9 (see Section 4) in 2D during a single execution.As shown in Fig. 5, λi(t) is initialized to be λ0(0). In the case of t=4 or t=80, because the affinity of Abi(t) is equal to that of Abi(t−1), λi(t) is halved to be λi(t−1)/2. In the case of t=6 or t=9, because the affinity of Abi(t) is higher than that of Abi(t−1), λi(t) keep its previous value λi(t−1). In the case of t=77, although the affinity of Abi(t) is higher than that of Abi(t−1), the half of its previous value λi(t−1) is less than the lower bound λmin. Thus, λi(t) is set to be λ0(1)=λ0(0)/2 following Eq. (11).Because CS plays a role of global search under the guidance of ES, all the antibodies in CS experience social-learning from a selected antibody in ES. If the affinity of the antibody from CS is lower than that of the selected one from ES, it learns from the selected one. If its affinity is not lower than the selected one but lower than the best one in ES, it selects the best one to learn. Otherwise, it executes the self-learning mutation which is the same as that in ES. The mutation operator for CS follows:(12)ΔAbi(t)=r1⋅(Abj(t)−Abi(t)),ifAff(Abi(t))<Aff(Abj(t))r2⋅(Abe(t)−Abi(t)),elseifAff(Abi(t))<Aff(Abe(t))G2⋅λi(t),otherwise,Abi(t+1)=Abi(t)+ΔAbi(t),where ΔAbi(t) is the difference between two neighboring-generation antibodies, G2 is a Gaussian random variable with the zero mean and the standard deviation 1, r1 and r2 are uniform random variables, Abj(t) is the selected antibody from ES, and Abe(t) is the best antibody in ES, which follows(13)Abe(t)=argmaxAbi(t)∈ESAff(Abi(t)),Two social learning mechanisms are proposed to select the antibody from ES to learn. They are:•Stochastic social learning (SSL), andHeuristic social learning (HSL).In SSL, the antibody to be learned is selected randomly and the selection probability of every antibody in ES is uniform. In other words, the selection probability is not related to the antibody individual but the population size of ES, i.e., NElitist(NElitist=pElitist·N, 0<pElitist<1).In HSL, the antibody to be learned is selected based on the affinity. In detail, the selection probability p(Abj(t)) for the antibody Abj(t) in ES follows(14)p(Abj(t))=Aff*(Abj(t))∑i=1NElitistAff*(Abi(t)).Clearly, the selection probability is nonuniform. And the higher the affinity is, the greater selection probability the corresponding antibody has. And then, the roulette is employed for HSL to select the antibody from ES to learn.For a multi-modal function, if the optima are significantly different in affinity, the higher the antibody's affinity is, the greater probability the antibody has to evolve to be the global optimum. So, AINet-HSL will have faster convergence speed. However, if ES does not include the global optimum, AINet-HSL has potential risk to fall into some local optima. If the optima are similar in affinity, the affinity-based selection mechanism in AINet-HSL will be similar in effect to the random selection mechanism in AINet-SSL.The dynamic suppressor is used, which is the same to IA-AIS. The threshold ThSis proportional to the similarity of the antibody population, which is determined by Eq. (5). After the suppressor, a number of randomly generated antibodies are recruited to keep the population size at the number of N.After learning from ES, some antibodies in CS will probably become better in affinity than some in ES. Thus, it is necessary to update swarms and allow these better antibodies upgraded into ES. Thus, in the swarm updating, all the antibodies are sorted by their affinity in a descending order. And then, the first NElitistantibodies step into ES and the other antibodies step into CS.Note that after the initialization, all the antibodies experience the self-learning mechanism until the suppressor is triggered. This allows all the antibodies to evolve around themselves and is benefit for finding the peaks of the problem. So, in the first swarm updating, all the candidate antibodies step into ES, while CS is empty.The flowchart of the proposed AINet-SL is shown in Fig. 6. The self-learning and social-learning mechanisms are denoted in the red boxes. The candidate antibodies are separated into ES and CS based on their affinity in the swarm updating phase. For ES, the antibodies experience the affinity-based cloning and the self-learning mutation operator, where the searching step length is updated adaptively by a well-designed mechanism. And for CS, each antibody selects a target antibody randomly in SSL or by means of the affinity in HSL. And then, each antibody undergoes the social-learning mutation operator. The suppressor is excited, if the suppression condition is satisfied. Repeat this process until it is terminated.In this section, a series of numerical simulations are arranged to examine the solution quality and the convergence performance of the proposed AINet-SL with two different social learning mechanisms by comparing with DE (SaDE/best/1/bin) [21], opt-aiNet, IA-AIS and AAIS-2S. Five benchmarks are selected from 2005 CEC Special Session on Real-Parameter Optimization[22] to examine the solution accuracy and the convergence speed. Furthermore, as a practical industrial application, the optimal designing for FIR filter is considered to evaluate the proposed AINet-SL. The initialized population size of these five algorithms is 100, the maximum number of time epochs is 5000, and all the simulations are repeated for 50 trials.Five selected benchmark functions are listed as follows:•F1 – Shifted Sphere Function: unimodal;F2 – Shifted Schwefel's Problem 1.2: unimodal;F6 – Shifted Rosenbrock's Function: multi-modal;F9 – Shifted Rastrigin's Function: multi-modal;F12 – Schwefel's Problem 2.13: multi-modal.In the proposed AINet-SL, three key parameters, i.e., the population size of ES (pElitist), the power factor in the cloning operator (n) and the suppression scale (ξ), need to be tuned. In this subsection, five benchmark functions in 10-D are used to discuss the impact of these three parameters. Because F1 is so simple that it is easy for AINet-SL to capture the global optimum in most conditions with different parameters, only F2, F6, F9 and F12 are selected to discuss the parameter sensitivity.Fig. 7represents the impact of the population size of ES, i.e., pElitist, on error, where n=2 and ξ=10. Seen from Fig. 7(A) and (B), pElitisthas little impact on error for F2, F6 and F12. However, the error is much larger when pElitistis smaller than 20% for F12. The impact is significant for F9, where the error is much greater when pElitistis greater than 60% for SSL and 70% for HSL. The reason can be got in Fig. 8which shows the impact of pElitiston convergence speed for F9. The greater pElitistis, the slower convergence speed the proposed AINet-SL has. When pElitistis greater than 60% for SSL and 70% for HSL, AINet-SL is difficult to obtain a desired solution. Take a tradeoff between the error and the convergence speed, pElitistis usually set to be 20%.Figs. 9 and 10represent the impact of the power factor in the cloning operator n on error and number of function evaluations, respectively, where pElitist=20% and ξ=10. Seen from Fig. 9(A) and (B), it is obvious that n has little impact on error for all benchmark functions. However, as shown in Fig. 10, it impacts on the number of function evaluations significantly. The greater n is, the fewer function evaluations the proposed AINet-SL needs. Note that the number of function evaluations keeps almost unchanged when n is greater than 3 for F9 optimized by HSL. Consequently, pElitistis usually set to be 3 to 5.Fig. 11represents the impact of the suppression scale ξ on error, where pElitist=20% and n=5. Seen from Fig. 11(A) and (B), ξ has little impact on error for all benchmark functions except F12. For F12, the error increases with ξ, when ξ is greater than 30% for SSL and 20% for HSL. Above all, ξ is usually set to be 20–30%.For the sake of fairness, in the cloning operators, the number of clones for opt-aiNet is 20, while the maximum and minimum number of clones are 20 and 4 for the other immune algorithms, respectively. According to the results of parameter sensitivity analysis, all other parameters of the proposed AINet-SL are listed in Table 1, and please see [13], [19], [20] and [21] for the rest parameters of other four algorithms.Tables 2 and 3present the numerical simulation results for five benchmark functions, including the best, worst, mean and standard deviation (Std) of the error in 10D and 30D, respectively, where the best results are typed in bold. According to the results, the proposed AINet-SL has obviously remarkable performance. For all benchmark functions in both 10D and 30D, the proposed AINet-SSL and AINet-HSL are able to obtain desired solutions which are much better than those of DE, opt-aiNet, IA-AIS and AAIS-2S. Especially in 10D, AINet-SSL and AINet-HSL can both capture the global optimum in every trial for F1, and the best error for F2 and F9 are 0. It is worth pointing out that AINet-SL has the best results in best, worst, mean and std of the error, which proves that AINet-SL has better robustness. It is obvious that the proposed AINet-SL has much better convergence accuracy than DE, opt-aiNet, IA-AIS and AAIS-2S.Figs. 12–16show the average convergence processes for five benchmark functions, respectively. In order to distinguish the results obviously, the results are expressed in lg (log10) scale. In every sub-figure of Figs. 12–16, it is obvious that the proposed AINet-SSL and AINet-HSL have much faster convergence speed than DE, opt-aiNet, IA-AIS and AAIS-2S. Especially for F12 in 10D, the AINet-SSL and AINet-HSL can both capture the optima easily within 1500 time epochs, while the curves of the other algorithms descend much slower.A digital filter is applied to obtain the desired spectrum for a discrete input signal. As an attractive choice of digital filters, the finite impulse response (FIR) filter is one of the basic components in digital signal processing. FIR filter has the property of strict linear phase for any amplitude–frequency characteristic. Because its impulse response is finite, FIR filter is absolutely stable. In addition, the lack of feedback in the design makes FIR filter more reliable and robust. So, FIR filter is widely used in various signal processing applications such as communication, image processing and pattern recognition.The structure of a typical FIR filter is shown in Fig. 17, where the FIR filter is made up with a series cascaded sub-modules. Each sub-module consists of the units of multiplier, adder and delay. Each multiplying coefficient or impulse response h(n) has a great effect on the performance of an FIR filter.The z transform of an N-point FIR filter is given by(15)H(z)=∑n=0N−1h(n)z−n,where h(n) is finite, i.e., 0≤n≤N−1. So, the frequency response can be obtained by(16)H(ω)=∑n=0N−1h(n)e−jωn.Here, the optimal filter design method is employed, which is aiming at designing the filter coefficients to minimize a particular error. The least squared error, the error measured as a sum of the squared differences between the actual frequency response and the desired frequency response over a set of M frequency samples, is selected with the expression of(17)E=∑k=1MH(ωk)−Hd(ωk)2=∑k=1M∑n=0N−1h(n)ejωkn−Hd(ωk)2,where ωk=(2πk)/M and Hd(ωk) are M samples of the desired response. The designing of the FIR filter is focused on determining the set of {h(0), h(1), …, h(N−1)} to minimize the least squared error.In this application, a designing of a band-pass filter is selected, where the lower cut-off frequency ωL=0.32π, the higher cut-off frequency ωH=0.68π, the amplitudes in the passband and the stopband are 1 and 0, respectively. In addition, N and M are set to be 30 and 100, respectively. The proposed AINet-SL is compared with DE, opt-aiNet, IA-AIS and AAIS-2S, and the maximum of time epoch is 1000.Table 4presents the simulation results optimized by six algorithms. The indices include the best, worst, mean and standard deviation (Std) of the error. The best result of each index is typed in bold. Seen from the results, the best solutions of AINet-SSL and AINet-HSL are both 1.122553, and the mean errors of them are 1.122675 and 1.122631, respectively, while the results obtained by the other four algorithms are much greater. It is obvious that the proposed AINet-SSL and AINet-HSL are able to capture much better solutions than DE, opt-aiNet, IA-AIS and AAIS-2S.Designed by these six methods, the frequency responses using the best solutions are illustrated in Fig. 18. Seen from Fig. 18, in the pass band, all the methods except IA-AIS have similar frequency responses approaching the desired one. However in the stop band, it is obvious that the amplitudes of the proposed AINet-SSL and AINet-HSL are much smaller than those of the other four methods. As a result, the passband filter is more well-designed by the proposed AINet-SL.

@&#CONCLUSIONS@&#
In this paper, an artificial immune network with social learning (AINet-SL) for complex optimization problems is proposed. Considering that the elitist-learning has great risk to fall into the optima, the proposed AINet-SL uses two social-learning mechanisms, i.e., stochastic social learning (SSL) and heuristic social learning (HSL). In addition, a dynamic searching step length updating strategy is proposed to improve the solution accuracy. In the numerical simulations, the sensitivity analyses of parameters are studied and the best values are recommended. For the sake of performance evaluation, the proposed AINet-SL is compared with DE, opt-aiNet, IA-AIS and AAIS-2S in five benchmark functions and a practical application of FIR filter designing. According to the numerical simulation results, both AINet-SSL and AINet-HSL can obtain the global optimum much more quickly and accurately, and are more effective in such an application than DE, opt-aiNet, IA-AIS and AAIS-2S.This work is supported by the National Science Foundation of China (#61201087), and the Guangdong Natural Science Foundation under Grant (#S2011010001492). In addition, a famous professor of South China University of Technology, Mr. Zongyuan Mao should be respected for earlier leading the authors to enter the research field of artificial immune systems and his professional guidance in industrial applications. We would like to thank three anonymous (unknown) reviewers and the editor for their comments.Supplementary data associated with this article can be found, in the online version, at http://dx.doi.org/10.1016/j.asoc.2013.08.013.The following are the supplementary data to this article: