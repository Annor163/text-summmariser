@&#MAIN-TITLE@&#
A typology of different development and testing options for symbolic regression modelling of measured and calculated datasets

@&#HIGHLIGHTS@&#
Symbolic regression is used to develop and test two types of predictive model.A new typology distinguishes between development/testing data and model type.Test period data has more influence than development period data on model accuracy.Sensitivity analysis of simulator and emulator models revealed similar mechanisms.

@&#KEYPHRASES@&#
Simulation,Emulation,Pan evaporation,Gene expression programming,Data driven modelling,Emulation simulation typology,Symbolic regression,

@&#ABSTRACT@&#
Data-driven modelling is used to develop two alternative types of predictive environmental model: a simulator, a model of a real-world process developed from either a conceptual understanding of physical relations and/or using measured records, and an emulator, an imitator of some other model developed on predicted outputs calculated by that source model. A simple four-way typology called Emulation Simulation Typology (EST) is proposed that distinguishes between (i) model type and (ii) different uses of model development period and model test period datasets. To address the question of to what extent simulator and emulator solutions might be considered interchangeable i.e. provide similar levels of output accuracy when tested on data different from that used in their development, a pair of counterpart pan evaporation models was created using symbolic regression. Each model type delivered similar levels of predictive skill to that other of published solutions. Input–output sensitivity analysis of the two different model types likewise confirmed two very similar underlying response functions. This study demonstrates that the type and quality of data on which a model is tested, has a greater influence on model accuracy assessment, than the type and quality of data on which a model is developed, providing that the development record is sufficiently representative of the conceptual underpinnings of the system being examined. Thus, previously reported substantial disparities occurring in goodness-of-fit statistics for pan evaporation models are most likely explained by the use of either measured or calculated data to test particular models, where lower scores do not necessarily represent major deficiencies in the solution itself.

@&#INTRODUCTION@&#
For model-based approaches to remain credible tools in problem solving, a systematic and repeatable approach to iterative model development and evaluation tasks is required (Alexandrov et al., 2011; Bennett et al., 2012; Jakeman et al., 2006). Many data-driven modelling studies focus almost exclusively on goodness-of-fit metrics to determine the efficacy of solutions and little attention is paid to data provenance and/or to the conceptual underpinnings of the natural system being investigated, which consequently has a detrimental effect on scientific robustness and overall transparency of any findings. The present study has been inspired by an enthusiasm for more standardised approaches to environmental modelling explorations, including improved model evaluation (Abrahart et al., 2010; Bennett et al., 2012; Blocken and Gualtieri, 2012; Jakeman et al., 2006; Robson et al., 2008), resulting in our exposition of a data-driven modelling protocol that is able to answer our principal research question: does it really matter what type of data is modelled? A key outcome of this study is a new categorisation for research outcomes termed Emulation Simulation Typology (EST). This descriptor will enable researchers to distinguish between model type (simulator or emulator), as well as differences in model performance arising from the quality and type of data used in model development and testing.Two different types of predictive model are recognised in our study: a simulator, a model of a real-world process developed on a conceptual understanding of physical relationships using measured records (SMOD), and an emulator, an imitator of some other model developed on predicted outputs calculated by that source model (EMOD). This paper summarises the regulated exploration of two counterpart model types: i) a simulator, SMOD, used to estimate measured pan evaporation (EPAN); and ii) an emulator, EMOD, used to estimate pan evaporation values originally calculated by means of the Nordenson–Fox equation (Burman and Pochop, 1994; Kohler et al., 1955) (ECAL). Four independent model testing scenarios were envisaged in which each model that is developed is tested twice, once using EPAN data, and then again using ECAL data, as depicted by means of a 2 × 2 matrix in Fig. 1. The matrix illustrates that models may be developed using either measured EPAN or calculated ECAL data, and that subsequent testing could also be performed using either measured or calculated data, thus leading to four possible sets of findings. This study questions whether or not model performance is affected by which of the four development/testing combinations a given model can be assigned and examines the question of to what extent goodness-of-fit performance is related to particular scenarios. To assist with our explanation of matters, two types of model combination are defined: Type 1 models that are developed and tested using the same sort of data i.e. measured and measured or calculated and calculated; Type 2 models that are developed using one sort of data and tested on another.It must be stressed that this analysis is not intended to establish whether emulators are better than simulators, or vice versa. Moreover, our study is not about searching for a superior predictive model, more accurate than anything published to date, and our models are not intended to represent general purpose solutions but rather specific fits to a particular dataset and study site. Finally, our study does not compare or benchmark its prediction accuracies against other types of data-driven daily EPAN or ECAL model such as those created by means of Multiple Linear Regression (MLR) (e.g. Abudu et al., 2011; Almedeij, 2012; Cooke et al., 2008; Tabari et al., 2010), Neural Network (NN) (e.g. Almedeij, 2012; Kim et al., 2012a, 2012b; Kisi, 2009; Kişi and Tombul, 2013; Moreno et al., 2010; Piri et al., 2009; Shiri et al., 2011; Shiri and Kisi, 2011; Shirsath and Singh, 2010; Tabari et al., 2010; Terzi and Keskin, 2010), Adaptive Neuro Fuzzy Inference System (ANFIS) (e.g. Chung et al., 2012; Dogan et al., 2010; Keskin et al., 2009; Kişi, 2006; Kişi et al., 2012; Sanikhani et al., 2012; Shiri et al., 2011) or Symbolic Regression (SR) (e.g. Guven and Kisi, 2010; Shiri et al., 2011; Shiri and Kişi, 2011; Shiri et al., 2013; Terzi, 2011, 2012). Such explorations have already been reported in past studies and need not be repeated. This study instead distinguishes between the different ideological contexts that underpin and separate simulator and emulator models in a new typology characterised by the different types of data that can be used for either model development and/or model testing purposes. This is done successfully by applying a standardised modelling protocol to examine EST and so in doing offers a mechanism for providing greater intelligibility of subsequently reported findings, for scientists, modellers, software developers and end users. We make this distinction between the different groups of people involved because the role each plays in inspiring, developing, evaluating and using outputs from data-driven modelling explorations will vary, as conveyed by means of a simple flow diagram in Fig. 2.Each stakeholder in Fig. 2 could take home a different message from our study. In common, is that EST is a clear and concise way of illustrating the influence that data used for development and testing operations has on data-driven modelling outputs.Natural evaporation of surface water is routinely estimated by field measurements using a U.S. Weather Bureau Class-A Evaporation Pan (World Meteorological Organization, 2008). EPAN is a useful observation for the purpose of water resources management, especially in forecasting lakes levels by hydrologists and ecologists. It is also regularly used to estimate evapotranspiration – a type of evaporation that takes account of aerodynamic and surface effects influenced by vegetation (Allen et al., 1998). EPAN, if not effected by precipitation or other extraneous events, is considered to be the product of synergistic interactions occurring between air temperature, relative humidity, solar radiation and wind on a water surface (Allen et al., 1998). The relative importance of each variable is debated, but it is generally agreed that a combination of meteorological factors drives this process (Fu et al., 2009).To overcome certain systematic errors and limitations associated with the collection of EPAN records (Moreno et al., 2010) it is often calculated, ECAL, by means of an appropriate equation using meteorological data. Indeed pan evaporation databases, such as those made available by the United States Environmental Protection Agency (EPA), might reasonably be expected to contain one or other, or a mix of both measured and calculated estimates (Burns et al., 2007). Deriving EPAN surrogates from simpler measurements is an active area of investigation in data-driven modelling (e.g. Abudu et al., 2011; Guven and Kisi, 2010; Keskin and Terzi, 2006; Kisi, 2009; Shiri et al., 2011; Terzi, 2011). Lately, these approaches have created a discussion about the level of accuracy that can be achieved by modelled solutions. Kisi (2009), for example, used downloaded EPA data to develop and test a daily pan evaporation model that delivered an exceptional performance that was measured using the goodness-of-fit metric, R-squared. Moreno et al. (2010) commented that such performance is difficult to achieve for models developed on measured EPAN given large amounts inherent error and uncertainty in the data, believing that the source data used by Kisi (2009) was more likely to be computed ECAL and not, as stated, observed EPAN, an argument side-stepped by Kisi (2010). This conflict and potential misunderstanding could have been avoided with an improved, more transparent, approach to modelling. The issue is that published models can sometimes be misleading, partly due to a lack of comprehensive reporting on the provenance of the data and the methods applied to them, and partly because of insufficient model evaluation. This research addresses these problems by proposing EST in conjunction with a simple six stage modelling protocol.

@&#CONCLUSIONS@&#
1.Gene expression programming has been used to create simulator (SMOD) and emulator (EMOD) models with reasonable performance, as evaluated using four goodness-of-fit metrics, scatter plots and OAT sensitivity analysis.Model development data has less influence over model performance than model testing data, providing that in each case the dataset is representative of the conceptual underpinnings of the natural system being investigated (for this study, pan evaporation in New Mexico).Models tested using calculated data tend to outperform those tested using measured data.Model inadequacy as determined by OAT sensitivity analysis is an issue in symbolic regression analysis conducted using GEP.Data-driven modelling outputs can easily be ascribed to one or other of the newly presented EST scenarios (Type 1 or Type 2) using an appropriate modelling protocol.Scientists and modellers should understand their data before embarking on data-driven modelling challenges – by using a systematic modelling protocol and EST, this is made easier, more transparent and increases the chances of the study being repeatable and results being reproducible.Model end-users should be aware of the type of data used to create a model before they applying it to practical problems.In the interest of future scientific explorations and maintaining research transparency and repeatability, the dataset used for this study is freely available from the corresponding author.