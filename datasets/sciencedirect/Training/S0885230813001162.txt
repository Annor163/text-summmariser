@&#MAIN-TITLE@&#
Improved open-vocabulary spoken content retrieval with word and subword lattices using acoustic feature similarity

@&#HIGHLIGHTS@&#
Acoustic feature similarity between spoken segments can improve spoken term detection.Pseudo-relevance feedback and graph-based re-ranking approach using acoustic feature similarity are proposed.A generalized framework for these approaches is presented.Significant improvements with both in-vocabulary and out-of-vocabulary queries were observed.

@&#KEYPHRASES@&#
Spoken content retrieval,Spoken term detection,Pseudo-relevance feedback,Random walk,

@&#ABSTRACT@&#
Spoken content retrieval will be very important for retrieving and browsing multimedia content over the Internet, and spoken term detection (STD) is one of the key technologies for spoken content retrieval. In this paper, we show acoustic feature similarity between spoken segments used with pseudo-relevance feedback and graph-based re-ranking can improve the performance of STD. This is based on the concept that spoken segments similar in acoustic feature vector sequences to those with higher/lower relevance scores should have higher/lower scores, while graph-based re-ranking further uses a graph to consider the similarity structure among all the segments retrieved in the first pass. These approaches are formulated on both word and subword lattices, and a complete framework of using them in open vocabulary retrieval of spoken content is presented. Significant improvements for these approaches with both in-vocabulary and out-of-vocabulary queries were observed in preliminary experiments.

@&#INTRODUCTION@&#
In the Internet era, digital content over the Internet covers almost all the information and activities of human life. The most attractive form of network content is multimedia, which includes audio signals. The subjects, topics, and core concepts of such multimedia content can very often be identified based on the speech information within the audio part of the content. Hence, in the future, spoken content retrieval will be very important in helping users retrieve and browse efficiently across the huge qualities of multimedia content (Lee and Chen, 2005). Spoken term detection (STD) is a subtask of the above spoken content retrieval, in which the query is a term (a word or a phrase of a few words) in text form and a spoken segment is taken as relevant if it includes the query term. The work of this paper is primarily for STD, although it is certainly possible to generalize the discussions here to other tasks in spoken content retrieval.Substantial research has been conducted in spoken content retrieval, and many successful techniques have been developed. Lattice-based approaches taking into account multiple recognition hypotheses (Saraclar, 2004; Chelba et al., 2007) have been used to mitigate the relatively low accuracy of 1-best transcriptions. Lattices were usually converted into sausage-like structures to facilitate indexing and to reduce memory requirements. Examples of such sausage-like lattice-based structures include position-specific posterior lattices (PSPL) (Chelba and Acero, 2005; Pan and Lee, 2007) and confusion networks (CN) (Mangu et al., 2000; Hori et al., 2007; Pan and Lee, 2007). As an alternative, the weighted finite state transducer (WFST) algorithm can also be used to index and retrieve lattices (Allauzen et al., 2004). The out-of-vocabulary (OOV) query is another important issue because queries often contain OOV words (Logan et al., 2000). The most fundamental approach for handling the OOV problem is to represent both the queries and the spoken segments by subword units and then match them on the subword unit level (Akbacak et al., 2008; Pan et al., 2007; Logan et al., 2005; Wallace et al., 2007; Turunen, 2008; Turunen and Kurimo, 2007; Wang et al., 2008; Itoh et al., 2007; Garcia and Gish, 2006; Ng, 2000). Word-based and subword-based indexing can be further integrated to yield better performance (Pan et al., 2007). Many successful applications of spoken content retrieval have been demonstrated including those for broadcast news (Pan et al., 2012), course lectures (Kong et al., 2009; Glass et al., 2007), historical spoken archives (Hansen et al., 2004; Oard et al., 2004), podcasts (Goto et al., 2007), and YouTube videos (Alberti et al., 2009).In general, there are two stages in conventional spoken content retrieval (Chelba et al., 2008). In the first stage, the audio content is recognized and transformed into transcriptions or lattices by a recognition engine using a set of acoustic models and language models. In the second stage, after the user enters a query, the retrieval engine searches through the recognition output and returns a list of relevant spoken segments to the user. The returned segments are usually ranked by the relevance scores derived from the recognition output. In the above two-stage framework, the spoken content retrieval techniques were actually applied on top of ASR output, either 1-bests or lattices. The performance of spoken content retrieval is thus inevitably limited by ASR performance, and in many cases the ASR performance is still unpredictable today, especially for spontaneous speech produced under adverse environments, and speech produced in the languages with limited resources for acoustic and language model training (Akbacak, 2009). Also, in many application tasks, it is practically very difficult, if not impossible, to obtain acoustic and language models that are robust enough to produce good ASR performance for the huge quantities of target spoken archives which are generated under different acoustic conditions by larger number of speakers for different scenarios with different subject domains. It is not surprising that such mismatched models may result in relatively poor recognition output. In such cases even very robust retrieval approaches are not able to compensate for the recognition errors.In text-based information retrieval, even if the texts to be retrieved include all precise words, it is still difficult to retrieve precisely all documents relevant to the query. One major reason for this is many queries are too short to completely represent the user's intent. However, related documents may have many words in common; thus if a given document contains many words that also appear in documents judged to be relevant in the first retrieval pass, this document may have a higher probability to be relevant. In other words, it is possible to get better retrieval results by considering the “similarity” (common words) between documents. Pseudo-relevance feedback (PRF) (Kurland et al., 2005; Tao and Zhai, 2006; Cao et al., 2008; Lv and Zhai, 2009, 2010), also known as blind relevance feedback, is one way to accomplish this. PRF assumes that a small number of top-ranked documents in the first-pass retrieved results are relevant (or “pseudo-relevant”), and sometimes also assumes that low-ranked documents are irrelevant (or “pseudo-irrelevant”); the documents retrieved in the first pass are then re-ranked based on their similarity (and dissimilarity) to the pseudo-relevant (and/or -irrelevant) documents.If the spoken content to be retrieved from can be transcribed into text, the PRF methods developed for text information retrieval can be directly applied on the transcriptions (Zhou, 2003; Lee et al., 2012a). However, since the transcriptions may include many recognition errors, when transcribing speech signals into text, much information is lost and not recoverable. A better approach of utilizing the idea of PRF is not directly applying it on the transcriptions but on the speech signal level with a hope to better use the information carried by the signals (Parada et al., 2009; Chen et al., 2010). The basic idea is if a spoken segment has an acoustic feature vector sequence very similar in some way to those of other spoken segments judged to include the target query term in the first retrieval pass, it may have a higher probability to include the target query term. In this approach, given a user query, the retrieval engine first searches through the lattices to produce a first-pass returned list ranked according to a relevance score directly derived from the lattices. The returned segments with the highest and lowest relevance scores are then respectively defined as the pseudo-relevant and -irrelevant sets. The similarities between each first-pass retrieved spoken segment and the pseudo-relevant and -irrelevant sets can be computed based on the acoustic feature vector sequences of the query hypotheses, and the first-pass returned list is re-ranked accordingly. Furthermore, it is also possible to take the pseudo-relevant and -irrelevant sets respectively as positive and negative examples to train a binary classifier by machine learning techniques, and then use the binary classifier to determine the relevance of the spoken segments retrieved in the first pass (Lee and Lee, 2013).The PRF approach can be taken one step further with graph-based re-ranking (Chen et al., 2011). In this approach, for each query entered we construct a graph for the first-pass retrieved spoken segments, in which each node represents a spoken segment and the edges represent the acoustic feature similarity between the segments’ query hypotheses. With this graph, the above concept now translates to the concept that segments strongly connected to many segments with higher/lower scores on the graph should have higher/lower scores. The relevance scores for the segments therefore propagate over the graph, and the segments are re-ranked accordingly. In this way, all the spoken segments in the first-pass returned list are considered globally, rather than assuming pseudo-relevant and -irrelevant sets in the PRF approach. For STD utilizing machine learning models to determine the relevance of the spoken segments, similar graph-based concept have been applied for selecting pseudo training examples not restricted to top/bottom-ranked spoken segments in the first-pass retrieved results (Lee and Lee, 2013). These approaches are similar to the very successful PageRank (Langville and Meyer, 2005; Brin and Page, 1998) used to rank web pages; PageRank considers the hyperlink between every two pages and computes a converged importance score for each page. Similar approaches have also been found useful in video search (Hsu et al., 2007; Tian et al., 2008) and extractive summarization (Otterbacher et al., 2009; Lee et al., 2011), in which the similarities between each pair of videos or sentences11Or utterances for spoken documents.are respectively used to formulate the ranking problem over graphs.Although the approaches utilizing acoustic feature similarity in STD were shown to be helpful, in the prior works (Chen et al., 2010, 2011; Tu et al., 2011; Lee and Lee, 2013) they were formulated based on a relatively limited task in which the query includes only a single in-vocabulary (IV) word, and the whole retrieval process was based on word lattices. Here in this paper we focus on a generalized framework for these approaches for a more complete task: the query can be shorter or longer, including one to several words, IV or OOV, and the retrieval is considered on both word- and subword-based lattices. The formulations of the approaches proposed previously (Chen et al., 2010, 2011) are modified to consider the case that the query can include several words or be represented as a sequence of subword units. Furthermore, in this paper it is verified that these approaches can improve the retrieval performance of OOV queries, which has not been investigated before.Part of the generalized framework was mentioned previously (Lee et al., 2012b), but here the influences of the sizes of pseudo-relevant/-irrelevant sets and different graph construction methods are explored, and deeper analysis based on detection error trade-off (DET) curves and the discussion of time complexity are also included in this paper. Below, the generalized framework for both approaches using PRF and graph-based re-ranking is presented in Section 2. Experiments are presented in Sections 3 and 4, and we conclude in Section 5.The framework for the proposed approach for the task considered here is shown in Fig. 1. The spoken segments are first transcribed into word or subword lattices by a speech recognizer. When the user enters a query, which can be shorter or longer including IV or OOV words, the retrieval engine searches over the lattices and produces the first-pass returned list as described in Section 2.1. The acoustic feature similarity between every two retrieved segments is then computed as presented in Section 2.2. Based on this similarity, the list is re-ranked using either pseudo-relevance feedback (PRF) in Section 2.3 or graph-based re-ranking in Section 2.4.Given query Q, the system returns the spoken segments xiwith relevance scores R(xi, Q) higher than a threshold, and ranks these segments according to the values of R(xi, Q) as the first-pass retrieval resultX. The relevance score R(xi, Q) used to produce the first-pass returned list can be derived from either word or subword lattices. Relevance scores from word lattices are usually more accurate than those from subword lattices, but we must rely on the latter when the query Q consists of OOV words. Below we first show how to determine R(xi, Q) using word lattices, and then show that the subword-based scores can be similarly obtained from subword-based lattices.We are given the query Q represented as one to several words,Qw={wj,j=1,2,…,N},wjbeing the jth word and N the number of words in Q. To compute the word-based relevance scoreR(xi,Qw)for a segment xifrom the word lattice, we calculate the expected count for each n-gram {wk,…,wk+n−1}, k=1, …, N−n+1, in the query from the segment's lattice as in (1), and then aggregate the results for all such n-grams to produce the word-based n-gram scoreRn-gram(xi,Qw)for each order of n in (2).(1)E[wk,…,wk+n−1|xi]=∑u∈W(xi)P(xi|u)P(u)C(u,{wk,…,wk+n−1})∑u∈W(xi)P(xi|u)P(u),where W(xi) is the set of all allowed paths in the lattice of xi, u one of the allowed paths, P(xi|u) the likelihood for the observation sequence of xigiven the path u based on the acoustic model set, P(u) the prior probability of u from the language model, andC(u,{wk,…,wk+n−1})the occurrence count of the n-gram{wk,…,wk+n−1}in u, and(2)Rn-gram(xi,Qw)=∑k=1N−n+1E[wk,…,wk+n−1|xi].The different proximity types, one for each n-gram order n allowed by the query length, are finally integrated in a weighted sum to yield word-based relevance scoreR(xi,Qw)for word lattices as(3)R(xi,Qw)=∑n=1NanRn-gram(xi,Qw),where anis a weight parameter. SinceR(xi,Qw)here is the aggregation of all the possible n-grams in the query, segments that only partially match the query can still be retrieved; this may increase the recall rate of the retrieval results but not necessary decrease the precision if anare properly set (Meng et al., 2009).To retrieve the subword-based lattices, the query Q is represented as a sequence of subword units instead, Qs={sj, j=1, 2, …, M}, where sjis the jth subword unit and M the number of subword units in Q. The subword-based relevance score R(xi, Qs) can be obtained in exactly the same way as that in (1)–(3), except that E[sk, …, sk+n−1|xi] is computed on a subword lattice.(4)E[sk,…,sk+n−1|xi]=∑u∈W(xi)P(xi|u)P(u)C(u,{sk,…,sk+n−1})∑u∈W(xi)P(xi|u)P(u),(5)Rn-gram(xi,Qs)=∑k=1M−n+1E[sk,…,sk+n−1|xi],(6)R(xi,Qs)=∑n=1Man′Rn-gram(xi,Qs).Here (4), (5) and (6) are exactly the same as (1), (2) and (3) except that the wordwjis replaced by the subword unit sj, Rn-gram(xi, Qs) and R(xi, Qs) are subword-based n-gram score and subword-based relevance score respectively, and an′ is the corresponding parameter.Here the similarity S(xi, xj) between the acoustic feature vector sequences for two retrieved segments xiand xj, referred to as acoustic feature similarity below, is computed, which will be used in both PRF and graph-based re-ranking in the next two subsections. S(xi, xj) can be obtained again based on either word or subword units; here we show the word-based version first for demonstration.Given query Q consisting of a sequence of wordsQw={wj,j=1,2,…,N}, for each n-gram{wk,…,wk+n−1}in Q where k=1, …, N−n+1, the dynamic time warping (DTW) distance (Aradilla et al., 2006) is first calculated between the acoustic feature sequences corresponding to the subpaths with word hypothesis sequences{wk,…,wk+n−1}in the lattices of xiand xj22If there are multiple subpaths whose word hypotheses are {wk,…,wk+n−1} in a lattice, only the one with the highest posterior probability is considered. Instead of picking the subpaths with the highest posterior probability, there are other reasonable alternatives. For example, first cluster the subpaths with the same hypothesis sequences and similar time spans into groups. Use the time span of the subpath with the highest posterior probability in each group to represent the time span of the group, and take the summation of the posterior probabilities of all the elements in a group as its score. Then use the time span of the group of subpaths {wk,…,wk+n−1} with the highest score to compute the DTW distances. However, this alternative did not lead to too much difference from picking the highest subpaths in terms of the experimental results, but required extra computing efforts, so for simplicity we do not report its results here.. An example is shown in Fig. 2. This yieldsd(xi,xj;{wk,…,wk+n−1}), the word-based DTW distance between xiand xjconsidering the n-gram {wk,…,wk+n−1} in the query. The word-based similarity between xiand xjconsidering {wk,…,wk+n−1} is then(7)S(xi,xj;{wk,…,wk+n−1})=1−d(xi,xj;{wk,…,wk+n−1})−dmindmax−dmin,where dmaxand dminare the largest and smallest values ofd(xi,xj;{wk,…,wk+n−1})for all pairs of segments in the first-pass returned list. Eq. (7) simply linearly normalizes the DTW distance and transforms it into a similarity score between 0 and 1. If the n-gram {wk,…,wk+n−1} does not exist in the lattice of either xior xj,S(xi,xj;{wk,…,wk+n−1})is set to 0. We then aggregate the similarities considering all such n-grams to produce word-based scoreSn-gram(xi,xj;Qw)for each order of n as(8)Sn-gram(xi,xj;Qw)=∑k=1N−n+1S(xi,xj;{wk,…,wk+n−1}).The different proximity types are finally integrated as a weighted sum to yield the word-based similarity between xiand xj:(9)S(xi,xj;Qw)=∑n=1NbnSn-gram(xi,xj;Qw),where bnis another weight parameter.To retrieve subword-based lattices, the query Q is represented as a sequence of subword units instead, Qs={sj, j=1, 2, …, M}. The computation of subword-based similarity S(xi, xj;Qs) is exactly the same as that in (7)–(9), except that the word sequenceQwis replaced by Qs, and each wordwiis replaced by subword unit sj.(10)S(xi,xj;{sk,…,sk+n−1})=1−d(xi,xj;{sk,…,sk+n−1})−dmin′dmax′−dmin′,(11)Sn-gram(xi,xj;Qs)=∑k=1M−n+1S(xi,xj;{sk,…,sk+n−1}),(12)S(xi,xj;Qs)=∑n=1Mbn′Sn-gram(xi,xj;Qs).Here (10), (11) and (12) are exactly the same as (7), (8) and (9), except that the word sequenceQwis replaced by Qs, and each wordwiis replaced by subword unit sj, and dmax′, dmin′ and bn′ are the corresponding parameters.Although we can obtain the relevance scoreR(xi,Qw)and R(xi, Qs), and similarityS(xi,xj;Qw)and S(xi, xj;Qs) based on different units and use them together – for example, it is possible to deriveR(xi,Qw)in (3) from word lattices but compute S(xi, xj;Qs) in (12) on subword lattices and use them together – for simplicity in the experiments below, we always useR(xi,Qw)/R(xi, Qs) andS(xi,xj;Qw)/S(xi, xj;Qs) obtained from the same type (word or subword) of lattices together. Below for simplicity in notation, we simply use R(xi, Q) to denote relevance score and S(xi, xj) to denote similarity, regardless of whether they are obtained from word or subword lattices.Although we here report only experiments using MFCC features for the DTW distances, other acoustic features could be used, such as phone posteriorgrams (Hazen et al., 2009) or Gaussian posteriorgrams (Zhang and Glass, 2010, 2009), and evaluating the acoustic similarity between two acoustic feature sequences based on models is even preferred (Chan and Lee, 2011; Wang et al., 2012). These approaches may provide less speaker-dependent DTW distance measures and could thus be useful if the target spoken segments are produced by many different speakers.In PRF, the top-ranked y segments with the highest relevance scores R(xi, Q) in (3) or (6) are selected as pseudo-relevant setY; the bottom-ranked z segments with the lowest R(xi, Q) are selected as pseudo-irrelevant setZ. The similarity between each segment xiin the first-pass retrieved resultsXand the pseudo-relevant and -irrelevant sets is then defined as(13)SIM(xi)=1y∑x∈YS(xi,x)−1z∑x∈ZS(xi,x),where y and z are the sizes of the pseudo-relevant and -irrelevant sets.The value of SIM(xi) is then linearly normalized into a number between 0 and 1 as SIM′(xi),(14)SIM′(xi)=SIM(xi)−SminSmax−Smin,where Smaxand Sminare the largest and smallest values of SIM(xi) among all spoken segments xiretrieved.The relevance score R(xi, Q) for each segment xiis then updated into a new relevance score(15)Rp(xi,Q)=R(xi,Q)1−δ1SIM′(xi)δ1,where δ1 is a weight parameter between 0 and 1. The segments inXare then re-ranked according to Rp(xi, Q), and then displayed to the user.Usually in the literature the sizes of the pseudo-relevant/-irrelevant objectsYandZare fixed for all of the queries. However, in text information retrieval, it has been found that the optimal number of pseudo-relevant objects varies from query to query (Montgomery et al., 2004). This observation suggestsYandZshould be determined dynamically for different queries based on their properties, which is not an easy task.33In the preliminary experiments, we have investigated different alternatives for dynamically determining the size of pseudo-relevant set for each queries. For example, take the segments whose relevance scores higher than a threshold as pseudo-relevant. In this way, different queries have different pseudo-relevant sets with different sizes. However, due to the diverse properties of the queries, these alternatives did not outperform that of simply taking a fixed number of top-ranked segments as pseudo-relevant, so they are not reported here.The graph-based re-ranking presented in the next subsection may achieve this goal to some extent. Instead of selecting a set of pseudo-relevant/-irrelevant spoken segments, in the graph-based re-ranking, the contribution of a spoken segment to the final scores of other segments is based on the acoustic similarity structure between the segments in the first-pass retrieved result (represented as a graph). For instance, the spoken segments ranked in the first place of the first-pass retrieved results of two different queries have different influences to the other spoken segments in the retrieved results because the acoustic similarity structures are different for the two first-pass results.An alternative to PRF for using acoustic feature similarity is graph-based re-ranking, which involves first constructing a graph for the first-pass retrieved segments for each query (Section 2.4.1) and then applying a random walk for relevance score propagation over the graph (Section 2.4.2).Here for each query a directed graph is constructed from the first-pass returned listX, in which each node represents a segment. A simplified example for such a graph is shown in Fig. 3. Because directions are needed for score propagation over the graph, the edges between nodes need to have directions. There can be at least several approaches for connecting the edges with directions for the graph. In the first two cases below, we first connect each pair of nodes for segments xiand xjwith a pair of edges in both directions (xi→xjand xi←xj). The weight of edge from xito xj(xi→xj) is S(xi, xj) in (9) or (12).44Because S(xi, xj)=S(xj, xi), the edges connecting xiand xjin both directions (xi→xjand xi←xj) have equal weights.We then prune those edges with lower weights in two different ways as listed below.•Fixed Number of Outgoing Edges (OUT): Each segment (or node) xionly keeps the K outgoing edges with the highest weights. In this way, each node in the graph has a fixed number K of outgoing edges but a variable number of incoming edges. In other words, during the score propagation, each node influences equal number of nodes.Fixed Number of Incoming Edges (IN): Each segment (or node) xionly keeps the K incoming edges with the highest weights. Thus each node in the graph has a fixed number K of incoming edges but a variable number of outgoing edges. In other words, the score of each node is influenced by equal number of nodes during the score propagation.K-nearest Neighbor (KNN): Nodes xiand xjare connected to each other in both directions (xi→xjand xi←xj) if xiis among the K-nearest neighbors of xj(or given xj, S(xi, xj) is among the K highest of all xi), or if xjis among the K-nearest neighbors of xi(or given xi, S(xi, xj) is among the K highest of all xj).Mutual K-nearest Neighbor (M-KNN): Similar as the above, except both directions of K-nearest neighbor relationships are required. Nodes xiand xjare connected to each other in both directions if xiis among the K-nearest neighbors of xj, and xjis among the K-nearest neighbors of xi.A new set of graph-based relevance scores Rg′(xi, Q) for all xiin the first-pass returned listXcan be obtained via score propagation on the graph, which can be expressed as(16)Rg′(xi,Q)=(1−α)R(xi,Q)+α∑xj∈BiR′g(xj,Q)Sˆ(xj,xi),where R(xi, Q) is the relevance score in (3) or (6), α is an interpolation weight between 0 and 1, Biis the set of all segments connected to xiby incoming edges as in Fig. 3, and xjis a node in Bi.Sˆ(xj,xi)is the normalized edge weight S(xj, xi) over all edges outgoing from node xjon the graph55Note thatSˆ(xj,xi)≠Sˆ(xi,xj).:(17)Sˆ(xj,xi)=S(xj,xi)∑xk∈AjS(xj,xk),where Ajis the set of segments connected to xjby outgoing edges as in Fig. 3. In (16) the graph-based score Rg′(xi, Q) of a segment xidepends on two factors interpolated by α: the relevance score in (3) or (6) (the first term on the right hand side of (16)) and the score propagation over the graph from all nodes xjin Bito xivia incoming edges based on the normalized edge weightsSˆ(xj,xi)(the second term on the right hand side).Based on (16), a segment xiwould have large Rg′(xi, Q) under the following two conditions:1.Original relevance score R(xi, Q) is large, or the confidence of the occurrence of the query in xiis high based on its lattice.xiis connected to other nodes xjwith large Rg′(xj, Q), or xiis acoustically similar to other spoken segments xjwith larger probabilities of containing the query.The normalization in (17) formulates (16) as a random walk problem on the graph; random walk theory guarantees that a set of unique solutions of Rg′(xi, Q) can be found. For all retrieved spoken segments xi, Rg′(xi, Q) in (16) can be found efficiently by power method (Langville and Meyer, 2005).66It is also possible to obtain Rg′(xi, Q) in (16) by searching for the eigenvalues of a matrix, but this approach has much larger time complexity than power method.Each node xiis first given an initial valueRg0(xi,Q).77The initial values would not influence the final results (Meye, 2000).Then at each iteration t,Rgt−1(xi,Q)obtained in the last iteration are updated toRgt(xi,Q)as below:(18)Rgt(xi,Q)=(1−α)R(xi,Q)+α∑xj∈AiRgt−1(xj,Q)Sˆ(xj,xi).Eq. (18) is parallel to (16), except thatRgt(xi,Q)is at the left hand side of the equation, andRgt−1(xj)at the right hand side. Whenever the results converge, that is,Rgt−1(xi,Q)andRgt(xi,Q)are sufficiently close,Rgt(xi,Q)can be taken as the scoresRg′(xi,Q)satisfying (16).Rg′(xi, Q) is finally integrated with R(xi, Q) to become a new relevance score for re-ranking,(19)Rg(xi,Q)=R(xi,Q)1−δ2R′g(xi,Q)δ2,where δ2 is a parameter between 0 and 1. The final retrieval results ranked according to Rg(xi, Q) in (19) are then displayed to the user.88Although the original scores R(xi, Q) have been considered when computing Rg′(xi, Q) in (16), integrating R(xi, Q) and Rg′(xi, Q) again in (19) empirically lead to better performance. Because R(xi, Q) and Rg′(xi, Q) are added in (16) but multiplied in (19), considering both integration mechanism leads to optimal performance.There are two stages in graph-based re-ranking: graph construction in Section 2.4.1 and random walk in Section 2.4.2. In the following, the complexity of these two stages is analyzed.During the graph construction in Section 2.4.1, the system first constructs a fully connected graph, and then prunes the edges with low weights. Suppose the number of spoken segments retrieved in the first pass is G, or there are G nodes in the graph. The system should compute the distances between the G nodes, or the weights of G(G−1) edges. G is usually small because the number of segments retrieved in the first pass is usually limited, although there are a large amount of spoken segments in the spoken archive. In real implementation, it is possible to construct the graph in more effective way. The system can use the coarse but fast approaches (Jansen and Durme, 2012) to first compute the approximate weights for all of the G(G−1) edges in the fully connected graph to decide the edges to be pruned, and then use the fine but slow ways to exactly compute the weights of the remaining edges.The complexity of the random walk in Section 2.4.2 is low. For each iteration in power method, there are G equations like (18) for every xi, and in general each equation has at most G additions (because the number of elements in Aican never exceed G). Therefore, if power method has T iterations, the complexity of random walk is less than O(G2T). G is a small number as described in the last paragraph, and T is also small because empirically tens of iterations is sufficient for the power method to converge (even if there are millions of nodes in a graph) (Langville and Meyer, 2005). In fact, in the experiments here, O(G2T) excessively overestimates the complexity. For example, based on the graph construction with Fixed Number of Incoming Edges (IN) in Section 2.4.1, because the size of Aiis always K, there are only K+1 additions in (18), and K is usually much smaller than G. Therefore, one of the G in O(G2T) should be replaced by K, and thereby the complexity of random walk is only O(KGT) in such case. There are other approaches to speed power method (Kamvar et al., 2003; Manaskasemsak and Rungsawang, 2005), but it is out of the scope here.The testing spoken archive is a corpus of 45h of recorded lectures for a course offered at National Taiwan University taught by a single instructor; the corpus is quite noisy and spontaneous (Lee et al., 2009).The spoken archive was divided into about 23,000 spoken segments based on silences, and the lengths of the segments were 3.6s on average.The lectures were given primarily in Mandarin Chinese but with some English terms and phrases embedded within the Mandarin utterances.Those embedded English words or phrases are usually very short, very often with a length of only one to three words. The English words or phrases occurred in the utterances in the following two conditions. In the first case, almost all terminologies for this course are directly produced by the instructor in English without trying to translate them into Chinese. For example, in the utterance, “speech recognition,indexingretrieval” (Except for speech recognition technology, we also need technologies about indexing and retrieval.)”, the phrase “speech recognition” and the words “indexing” and “retrieval” were produced in English, while other parts of the utterance are in Mandarin. In the second case, the instructor may prefer to use some commonly used English words in his utterances, which are not terminologies at all, probably because the concept can be easily or naturally expressed in English in this way. For example, in the utterance, “somehow handle(I can somehow handle this problem)”, the words “somehow handle” were produced in English, but other parts of the utterance were in Mandarin.The ratio of the number of Mandarin characters to that of English words is nine to one in the spoken archive we used. Many course lectures are presented in this code-switching way in Taiwan. In fact, such code-switching speech is very common for speakers whose native languages are not English but speak fluent English in the daily lives. They naturally speak the native languages as the daily language, but spontaneously embed some English words in their native language utterances. For example, many Asian whose native languages are not English speak in this way. Hence, the task domain considered here is very important and representative, although not yet investigated extensively.We split the corpus into two parts: 12 hours for acoustic and language model training and 33h for retrieval testing.In the following experiments, mean average precision (MAP) (Garofolo et al., 2000) was used as the retrieval performance measure. The pair-wise t-test with a significance level of 0.05 was used to gauge the significance of performance improvements. Here the STD system only returns the spoken segments containing the query terms without locating their exact positions in the spoken archive, because the spoken segments here were short enough to be used as the pointer to the positions. This task definition is the same as the STD task in the 9th NTCIR workshop (Akiba et al., 2011), but slightly different from that in NIST 2006 (http://www.itl.nist.gov/iad/mig/tests/std/2006/index.html), in which the positions of the query terms in the spoken archive should be located. For computing the DTW distances in Section 2.2, MFCCs were used as the acoustic features, and Euclidean distance was applied as the distance measure between two acoustic features. Some parameters in the experiments were set empirically as below. anand an′ in (3) and (6) were both set to 105nto favor longer n-grams. The expected term frequencies of longer n-grams should have more influence on the relevance scores in (3) and (6) because the observation of a query's longer n-grams in the lattices provides more confidence about the existence of the query than shorter n-grams. Due to the same reason, bnand bn′ in (9) and (12) were set equal to anand an′.The influence of δ1 in (15), δ2 in (19) and α in (16) has been explored in previous studies on the same audio but with another query set (Chen, 2011; Chen et al., 2011). In the previous studies (Chen, 2011), larger δ1 and δ2 implied better results unless they were too close to 1 (for example, larger than 0.99) because SIM′(xi) in (14) andRgt(xi,Q)in (16) were more reliable than the original relevance scores R(xi, Q). Therefore, δ1 and δ2 were both set to 0.9 here. For graph-based re-ranking, α close to 1 was optimal for not only spoken term detection (Chen et al., 2011) but also video search (Hsu et al., 2007), so α was set to 0.9 here.In order to evaluate the retrieval performance with respect to acoustic models of different matched conditions, we used three sets of acoustic models:•Speaker-independent models (SI) trained on a Mandarin corpus of 24.6h of read speech, produced by 100 male and 100 female speakers, plus the Sinica L2 Taiwanese English corpus with 59.7h of English read speech, produced by 229 male and 256 female Taiwanese speakers.Speaker-adaptive models (SA) adapted by MLLR with 256 classes cascaded with the maximum a posterior estimation from the above SI model based on 500 utterances taken from the training set of the lecture corpus.Speaker-dependent models (SD) trained on the 12-h training set of the lecture corpus.Two sets of experiments respectively with in-vocabulary (IV) and OOV queries were performed as mentioned below.The IV query set included 275 Chinese queries, each composed of 1–3 words, or 2–7 Chinese characters. The number of relevant spoken segments for each IV query ranged from 5 to 714 with an average of 38.2. In the experiments here, a language model trained with the manual transcriptions of the training set of the lecture corpus was used. A close-to-oracle lexicon was used which included 11K Chinese words plus 2K English words covering all words in the testing archive. Each utterance was transcribed into a bilingual word lattice. Then we transformed each Chinese word arc into a sequence of concatenated corresponding Chinese character and Mandarin syllable arcs to respectively form character and syllable lattices. The English word arcs remained unchanged. Therefore, for each utterance there were three lattices: word-, character-, and syllable-based. The word recognition accuracies for Chinese characters and English words evaluated together were 49.7%, 80.8%, and 88.0% respectively for the SI, SA, and SD models, and the inclusion rates of the lattices99The highest accuracy among the path hypotheses in each lattice.were 72.7%, 86.8%, and 92.0% respectively for the SI, SA, and SD models. Note that the three different sets of acoustic models together with the relatively matched language model and lexicon gave different levels of recognition accuracies. In this way, we wish to show that the proposed approaches can offer performance improvements regardless of whether the recognition accuracies are lower or higher.For the OOV query set we used 110 English queries, each consisting of a single word. The number of relevant spoken segments for each OOV query ranged from 2 to 268 with an average of 39.8. First, we assume we do not know the pronunciation of these OOV words, so we trained a 6-gram joint-sequence model from the CMU dictionary with 130K words (http://www.speech.cs.cmu.edu/cgi-bin/cmudict) to be used as the grapheme-to-phoneme converter to predict the pronunciations for the OOV queries (Bisani and Ney, 2008).1010The terms used in OOV queries were excluded from the CMU dictionary during training.The canonical pronunciation for each OOV query was also used in the experiments for comparison. Using the canonical pronunciation as the reference, the pronunciation was estimated perfectly (exactly the same as the reference) for 81 of the 110 OOV queries, or with an accuracy of 73.6%, while the pronunciation estimation accuracies on syllable and phoneme levels were 85.8% and 93.8% respectively.We used a word/subword hybrid system to transcribe each spoken segment, which is a widely used approach for handling the OOV problem (Rastrow et al., 2009; Akbacak et al., 2008; Szoke et al., 2008). In this experiment, we used a lexicon composed of 11K Chinese words, 5K English words from the standard Aurora-4 lexicon,1111This lexicon is quite disjoint from the content of the target corpus for retrieval, so the 110 English queries were not included in this lexicon.and 10K English syllables automatically generated from the CMU dictionary based on some syllable segmentation rules. 20,000 English documents from the 20Newsgroups corpus1212http://people.csail.mit.edu/jrennie/20Newsgroups/.were then used to train an English language model based on the above lexicon of 5K English words and 10K English syllables, which included mixed trigrams for English words and syllables (for example, a word following two concatenated syllables can be a trigram item). In other words, those words in the English training documents but not within the above selected lexicon of 5K English words were segmented into syllable sequences to be used together with the other words in the 5K English word lexicon to train an English trigram language model for mixed words and syllables. A Chinese word-based trigram language model was trained on the lecture corpus training set. These two language models were then interpolated to produce the lattices composed of a mixture of arcs for Chinese words, English words, and English syllables. We further substituted the Chinese and English word arcs in the lattices with their corresponding syllables to obtain a set of syllable-based lattices. Thus for each spoken segment we generated two lattices: one composed of Chinese and English words plus English syllables, and the other composed solely of Chinese and English syllables. Because the English recognition accuracies for SI and SA were not good enough to offer reasonable results, we used only SD models for the OOV query experiments. Since the accuracy for the word/syllable hybrid recognition output is not easy to define, we only evaluated the English syllable accuracy here. The English syllable accuracy of the one-best transcriptions and English syllable inclusion rate on the lattices were respectively 43.6% and 59.5% for the SD models.

@&#CONCLUSIONS@&#
