@&#MAIN-TITLE@&#
From spoken narratives to domain knowledge: Mining linguistic data for medical image understanding

@&#HIGHLIGHTS@&#
The medical images that can be described more consensually by physicians tend to be less diagnostically challenging.The medical images that can be described with highly related concepts by most physicians tend to be less diagnostically challenging.We develop a method based on anchor concepts to cluster diagnostic narratives by image.

@&#KEYPHRASES@&#
Unified Medical Language System,Lexical consensus,Semantic relatedness,Clustering algorithm,Image-based diagnostic reasoning,Medical data analysis,

@&#ABSTRACT@&#
ObjectivesExtracting useful visual clues from medical images allowing accurate diagnoses requires physicians’ domain knowledge acquired through years of systematic study and clinical training. This is especially true in the dermatology domain, a medical specialty that requires physicians to have image inspection experience. Automating or at least aiding such efforts requires understanding physicians’ reasoning processes and their use of domain knowledge. Mining physicians’ references to medical concepts in narratives during image-based diagnosis of a disease is an interesting research topic that can help reveal experts’ reasoning processes. It can also be a useful resource to assist with design of information technologies for image use and for image case-based medical education systems.Methods and materialsWe collected data for analyzing physicians’ diagnostic reasoning processes by conducting an experiment that recorded their spoken descriptions during inspection of dermatology images. In this paper we focus on the benefit of physicians’ spoken descriptions and provide a general workflow for mining medical domain knowledge based on linguistic data from these narratives. The challenge of a medical image case can influence the accuracy of the diagnosis as well as how physicians pursue the diagnostic process. Accordingly, we define two lexical metrics for physicians’ narratives—lexical consensus score and top N relatedness score—and evaluate their usefulness by assessing the diagnostic challenge levels of corresponding medical images. We also report on clustering medical images based on anchor concepts obtained from physicians’ medical term usage. These analyses are based on physicians’ spoken narratives that have been preprocessed by incorporating the Unified Medical Language System for detecting medical concepts.ResultsThe image rankings based on lexical consensus score and on top 1 relatedness score are well correlated with those based on challenge levels (Spearman correlation >0.5 and Kendall correlation >0.4). Clustering results are largely improved based on our anchor concept method (accuracy >70% and mutual information >80%).ConclusionsPhysicians’ spoken narratives are valuable for the purpose of mining the domain knowledge that physicians use in medical image inspections. We also show that the semantic metrics introduced in the paper can be successfully applied to medical image understanding and allow discussion of additional uses of these metrics.

@&#INTRODUCTION@&#
Image understanding is an important topic studied in imaging, computing, and the cognitive sciences and incorporates the domain knowledge of target images, human vision and psychophysics, and data mining. In order to better perform computational image understanding tasks, such as object detection [1–3], shape estimation [4], or depth estimation [5], common knowledge from humans is borrowed and injected into a variety of algorithms. However, hard-coded human knowledge cannot be easily and directly applied for complex tasks such as image classification and retrieval, which involve a wide range of images and require human expertise. For example, content-based image retrieval (CBIR), as an application of image understanding, is subject to the “semantic gap” between visual image features and the richness of human understanding [6–8]. Because image-feature representations often require high dimensionality and are described by complex semantics, incorporating human knowledge and high-level image representation into computational image classification is necessary [9]. Since language is the primary conduit of expressing meaning, researchers are currently integrating verbal metadata with image features [10] and moving toward new directions in CBIR, such as association-based image retrieval (ABIR) and perception-based image retrieval (PBIR) [11–13]. We take these insights in new directions by using novel linguistic data for medical image understanding.Classification of medical images can benefit from experts’ input of domain knowledge, because the semantics are not necessarily described by salient low-level visual features captured by computer vision algorithms [14]. Physicians in the specialties such as radiology and dermatology have developed perceptual expertise. They know where to look and what to look for and perform better than unsupervised algorithms that lack guidance from medical knowledge [15,16]. However, it is time consuming and impractical for physicians to manually annotate medical images since these images are usually stored in large-scale image databases with a large and rapidly growing number of digital images. We address this issue by collecting data from physicians as they engage in medical image inspection. Our approach to gathering data for analysis during physician image inspection involves a more natural task for experts to perform in contrast to asking them to conform to predefined image annotation labels and rules. Therefore, properly tracing the use of human knowledge—in our case physicians’ knowledge during the diagnostic reasoning process—becomes an interesting and meaningful topic. It also furthers the theoretical understanding of experts’ reasoning and decision making in medical domains and thus facilitates the adequate use of medical images in medical applications. In order to make these contributions, we conducted an experiment that recorded physicians’ spoken descriptions during medical image inspection. The descriptions were then transcribed to diagnostic narratives and preprocessed by programs that use the Unified Medical Language System (UMLS), an ontology of medical concepts [17]. We subsequently applied computational techniques to the narratives obtained in order to benefit medical image understanding.Since many medical images are inherently complex and noisy due to both photographic inconsistency and different presentations of even the same medical condition, grouping relevant medical images into semantically related and meaningful groups has been a long-standing challenge. Researchers have made efforts to incorporate domain knowledge in image clustering [18]. However, understanding physicians’ use of knowledge remains a challenging task that must be studied in the cognitive and biomedical domains. This is especially difficult in our case as we try to trace human knowledge used during image-based diagnostic reasoning, because visual diagnostic reasoning is a complex interaction of domain knowledge, perceptual expertise, reasoning processes [19], and idiosyncratic visual information in the image case being inspected by physicians. The scope of this paper is not to understand the reasoning process itself but to analyze experts’ expressions of domain knowledge during the diagnostic reasoning process, because it is difficult to know and clearly define which reasoning strategies are used in a given reasoning process [20].Currently, studies such as work by Evered et al. [21] use non-specific measures, such as participating physicians’ response time, to predict diagnostic accuracy and thus overlook much detailed information from physicians during diagnostic reasoning. We exploit human experts’ knowledge to facilitate medical image grouping by applying a methodology that is more objective and automated than current research [21–23]. The intuition is that the meaning of a medical image is expected to be mirrored by the spoken narrative of a physician when s/he describes the image during a diagnostic process. In this way, we incorporate physicians’ domain knowledge obtained from years of systematic study and clinical training to achieve more effective medical image grouping. However, the language data—our collected materials that contain expert knowledge—still provide multiple challenges:•Language is, by nature, sparse [24]. In most linguistic data sets, the vast majority of lexical items tend to occur rarely, and speakers can express similar meaning in a variety of ways, both syntactically and lexically.Semantic ambiguity occurs in language data. The understanding and successful professional interpretation of ambiguous language data depends on knowledge in the field. For example, the medical term plaque refers to a primary lesion type in the dermatology domain, while it could be interpreted as dental plaque by a dentist.Segmentation of medical text, such as in lexical tokenization, requires a combination of language processing techniques and medical knowledge. For example, contact dermatitis is a multiword expression jointly referring to a specific medical condition as opposed to two individual word tokens.Medical concepts11By concepts in this paper, we mean the level of meaning.have different levels of semantic scope and specificity. For example, contact dermatitis is a representative example of dermatitis that is manifested by reactivity to materials or substances coming in contact with the skin.22https://uts.nlm.nih.gov.In order to distinguish the diagnoses from broad (e.g., dermatitis) to specific (e.g., contact dermatitis), we need ontological knowledge and an understanding of relations between concepts.The difference in narration styles among multiple human experts results in variability that obscures common strategies of diagnostic reasoning.Language data are influenced by the mode in which they were produced. Naturally occurring speech data differ substantially from standard written text data.In this paper, we propose a novel framework that exploits physicians’ domain knowledge in a systematic fashion to facilitate the understanding of highly complex and oftentimes noisy medical images. In particular, we leverage physicians’ spoken narratives during the diagnostic process as sensors for domain knowledge collection and introduce two new lexical metrics that derive new insights into medical images. The use of spoken narratives also lead to novel ways of clustering medical images, which open a gate for organizing large-scale images generated in the medical domain on a daily basis. Following the idea of building a hidden conceptual space by defining quality dimensions [25,26], we define anchor concepts in a latent space for clustering spoken narratives based on the work in service-oriented computing by Yu [27]. Our anchor concept-based algorithm achieves much better clustering performance than latent semantic analysis (LSA) [28] by setting constraints on the sparseness of latent space.Based on prior work and exploration of an experimentally collected data set (see Section 2), we have developed the following coherent research questions to explore the potential benefits of analyzing physicians’ verbal input. The aim of these research questions is to test the value of the spoken narratives and to verify the usefulness of the metrics proposed for analyzing the linguistic data. The answers to these research questions could be used to devise more meaningful ways to automatically group medical images in new image databases that can be used for medical training or diagnostic comparison.RQ1 (physicians’ lexical consensus): Are the images with more consensus descriptions among multiple physicians more likely to represent easy diagnostic cases? We define the lexical consensus of an image as having similar tokens, including single words and multiword expressions [29], among the physicians participating in our experiment.RQ2 (overall semantic relatedness): Does the relevance of descriptions with regard to the correct diagnosis from the participating physicians indicate the difficulty levels of the medical images? Intuitively, it makes sense that difficult images tend to lead to physicians’ irrelevant descriptions, but we still would like to know if descriptions that are scored as more relevant represent easier image cases for the physicians participating in our experiment.RQ3 (clustering spoken descriptions): Are the vocabulary sets used for describing different groups of images largely disjointed? How could we leverage this property in grouping images in new ways (i.e., characterizing a set of images using a specific group of terms)?To study the usefulness of the collection of spoken narratives and address the identified research questions, we define two novel metrics for mining physicians’ spoken descriptions, namely lexical consensus score and top N relatedness score. In order to test the usefulness of these metrics, we conduct a case study that uses these metrics to understand the diagnostic challenge levels of medical images. We also make efforts to cluster the images in order to extract and represent physicians’ use of domain knowledge during image-based reasoning and apply it in the future for organizing medical images automatically. Due to the sparsity of spoken narratives under term-narrative representation (i.e., representing narratives using term vectors), we compute a set of anchor concepts, which provide a more compact representation than using term vectors [27]. Each spoken narrative can be expressed using a few anchor concepts instead of a large and sparse term vector. In particular, the anchor concepts are computed via a principled optimization process to ensure that they best capture the underlying semantics of the spoken narratives. As a spoken narrative is usually focused on a small number of medical concepts based on a specific image, we further integrate a sparsity constraint into the optimization process to ensure that each narrative is represented by a small number of anchor concepts.The remainder of this paper is organized as follows: Our language data are described in Section 2, as is the necessary preprocessing. Section 3 reports on the use of two novel metrics for understanding diagnostic challenge levels of images at the level of tokens (consensus metric) versus at the level of concepts (relatedness metric). Section 4 explores clustering results of spoken descriptions of dermatology images.To collect experts’ data for construction of a medical corpus, we conducted a data elicitation experiment that used 48 dermatology images as visual stimuli for inspection. These images represent a wide range of dermatology diagnoses [30]. Figs. 1 and 2present one of our 48 images and a corresponding diagnostic description. We chose dermatology as it is a medical specialty that requires perceptual expertise. In addition, dermatology images can be difficult to inspect even for experts, because these photographic images often contain many and complex visual features. Moreover, in some cases the information in the image is not sufficient to support a final diagnosis.In our data elicitation experiment, we followed a modified master-apprentice model [31] and asked each of 16 participating physicians to describe the visual content of each image aloud, as if teaching a student who was seated nearby. The physicians’ speech was recorded. This study was approved by Rochester Institute of Technology's Institutional Review Board, and all observers provided informed consent before participating in the data elicitation experiment.Fig. 2 contains a diagnostic narrative that was transcribed from a physician's spoken diagnostic description record. Similar to this example, all the spoken narratives were comprehensively transcribed with tokens and time-stamps included using the speech analysis tool Praat [32]. These data will be released in the future. Overall, the dataset comprises 768 transcribed spoken narratives from 16 physicians, each inspecting 48 images. Whereas the diagnostic narratives/descriptions in our corpus were manually transcribed, in the future a speech recognizer could be trained to perform the process automatically.The UMLS constitutes the largest existing semantic network of medical terms and lexical relations [17,33]. It is widely used as a knowledge source for medical terminology research and information retrieval [34]. MetaMap is a knowledge-intensive tool developed at the National Library of Medicine for semantic reference in language processing in the medical domain [35]. It automatically annotates biomedical text tokens by UMLS Metathesaurus concepts. For example, in the sentence This is a basal cell um carcinoma, where um is a disfluency marker, the multiword expression basal cell carcinoma (BCC) is detected by MetaMap as a medical concept. Multiword expressions (e.g., basal cell carcinoma) were quite common in our data. For this reason, following the transcription of narratives we used MetaMap to detect and reconstruct medical multiwords by joining adjacent medical words and discarding disfluency markers and pause markers within a medical multiword. So, in the example (SIL) um this is a basal cell um carcinoma shown in Table 1, the multiword after this is a is detected as basal cell carcinoma with the disfluency marker um discarded. An abstract illustration of a narrative after medical multiword reconstruction can be seen in Table 2, as compared with the original narrative shown in Table 1. The process of detecting multiword expression segments each spoken narrative into a sequence of words and multiword expressions. Disfluencies occurring within the span of each multiword expression are dropped, and corresponding time intervals are combined into one. The analysis methods described in the following sections use these preprocessed spoken narratives from physicians.In this section, we propose two metrics, lexical consensus score and top N relatedness score, which could be applied to the narratives in order to answer various questions that benefit medical image understanding. As a proof of concept, and to verify the usefulness of these two metrics for characterizing medical images, we conduct a case study that evaluates the challenge levels of diagnosis with images in our dataset by measuring the consensus of descriptions from different physicians and overall semantic relatedness of these descriptions to the image content (see Section 3.3). We select image challenge levels in this case study as an intuitive problem with implications for image use. Different studies using these two metrics can be conducted in future work in order to benefit the medical community in other ways.In order to know whether physicians agree in their image understanding and description, and to use this information as a new feature for image grouping, we propose a lexical consensus score of an image to evaluate the degree of agreement among descriptions by all physicians. This approach was adapted in modified form from cohesion scores typically used in clustering [37]. High agreement among K physicians leads to a high consensus score.We define the lexical consensus score of an image to be the average pairwise cosine similarity among descriptions given by K physicians {p1, p2, …, pK} in terms of their use of medical terms. We adopt cosine similarity in our study, because cosine similarity is widely used in text analysis [38]. We assume that we have a set of images {m1, m2, …, mR}. For each image mifrom the R images, we gather the spoken narratives from K physicians and form a vocabulary Vi, which contains all the medical words/multiwords used for describing this image. This vocabulary is treated as a feature space for this image, so that the K corresponding narratives could be expressed using feature vectors in this space. Because a physician cannot be compared to him/herself, K physicians form K(K−1)/2 pairs, thus K(K−1)/2 cosine similarity scores can be computed. Since the feature spaces may not be of the same length across R images, the similarity scores per image are normalized by the length of feature vector, |Vi|. The whole vocabulary set V={V1, V2, …, VR}, in the space of which we denote each narrative by a term vector, will be used in Section 4.3. We average the K(K−1)/2 normalized similarity scores and define it to be the lexical consensus score of the image, which measures K physicians’ level of agreement on describing the image.(1)SC=2K(K−1)×|Vi|∑j=1K∑k=j+1Ksim(xj,xk)where xjand xkare narratives from the narrative set {x1, x2, …, xK} that describes the same image, and sim(xj, xk) refers to the similarity between xjand xk.We defined the overall relatedness score of a spoken narrative at the concept level, in contrast to the lexical consensus score at the token level (in Section 3.1), to capture the level of semantic relevance of a physician's description. In the case study, we relate each narrative to the correct diagnosis of the corresponding image. More targets other than correct diagnoses can also be used for other purposes, e.g., using primary morphology of dermatological lesions to group dermatology images by primary morphology.Now assume we have a target medical concept with which to relate. Since we need to know how a narrative is related to a target concept, an intuitive way is to first know how each concept within the narrative is related to the target concept and average their relatedness. For this purpose, we used an open source software package UMLS::Similarity [39], which uses UMLS to calculate the semantic relatedness between two medical concepts. The semantic relatedness quantitatively measures the degree to which the semantic features overlap between the two terms. To result in high relatedness, two terms may be related through collocation as needle and thread but not necessarily be synonyms or hyponym/hypernym in a hierarchical semantic relationship [40]. For example, the sensation of itchy can be caused by rashes, so these two terms are highly related. Examples can be explored with this tool's web interface (http://atlas.ahc.umn.edu/cgi-bin/umls_similarity.cgi). This package builds upon UMLS to calculate similarity and relatedness between medical concepts and has been used for studies involving senses and relations of medical terms [41,42]. In order to return a normalized score in the range [0, 1] so that the scores can be effectively compared across or averaged by narratives, we select the vector measure method [43]. Generally, this method utilizes the definitions of both medical concepts in UMLS and their second-order co-occurrence. This captures the chance that both concepts will occur in the same context, given a large medical corpus (see discussion of details by Liu et al. [43]). Specifically for broader coverage and an empirical setting, we used the extended definitions of concept unique identifiers (CUIs) for both terms. The definition extensions are based on has parent/child relationship, as well as the relationship of has a broader/narrower, given the complete Metathesaurus vocabulary inventory, which includes resources such as MeSH and SNOMED CT. According to the above configuration, the relatedness score is within the range of [0, 1], 0 being not relevant at all and 1 being the same concept (either the exact word/multiword or its synonym).Because of homonymy and polysemy, there are situations where a word or a multiword has multiple medical meanings indexed by multiple CUIs in UMLS. In these situations, word sense disambiguation [44] is necessary. Before calculating the relatedness score to the image's correct diagnosis, we disambiguate the intended sense. We employ a package, UMLS::SenseRelate [45], for this purpose. Since not all words in narratives are medical terms, the scores of non-medical terms are set to zero. To infer which words and multiwords represent medical terms, we used UMLS packages to search for the CUI of each word/multiword in the narrative. If there is a CUI (or CUIs) related to this word/multiword, then we consider it as a medical term; otherwise we do not, although the UMLS is not fine-tuned in our context of the dermatology domain. Since each narrative can be annotated by several medical concepts that are, more or less, related to the image content, we define the top N relatedness scores to capture multiple levels of semantic relevance.The top 1 relatedness score in a narrative is the highest score calculated from the relatedness of all medical concepts to the correct diagnosis within a given narrative. This score thus corresponds to the most relevant medical concept from the physician describing this image and can intuitively serve as a good indicator of a physician's understanding of a particular image case. Likewise, the top N relatedness score is the average of the highest N scores in this narrative. We use the top N relatedness score to measure, for a specific narrative, the subject's understanding of the image s/he described. The reason for calculating the top N relatedness score, rather than averaging all the scores within one narrative, is that different physicians use different narration styles [20], either comprehensive or brief. What we want here is to reduce the variation among subjects to identify what they share.After calculating the relatedness score for each term, we retrieved the top N relatedness scores from each narrative. Averaging the top N relatedness scores from K narratives {x1, x2, …, xK} that correspond to a same image gives us the top N relatedness score for that image. After computing the top N relatedness scores for all images, we rank these images by their top N relatedness scores arranged in order.(2)SRN=1K×|Nj|∑j=1K∑ni∈Njrel(ni,c)where Njis the set of terms in narrative j that have the top N relatedness scores. For example, if the size of the set Njis 1, only the top term is included. If the size of the set is 3, the top 3 terms are included. Additionally, niis a term in this set; rel(ni) is the relatedness score of ni; and c is the target concept. For example, in the case study the target concept is the correct diagnosis of the image case.It is important that medical education systems be designed to provide a trainee with cases at a proper challenge level based on his/her performance, thus allowing the trainee's learning curve to be shortened [46]. For this reason, we select challenge levels of images as the topic of this case study and apply our metrics of lexical consensus score and top N relatedness score. Unlike André et al.'s approach [46], our metrics place more weight on physicians’ knowledge and expertise applied to the image inspection tasks.In image-based diagnosis, easy tasks versus difficult tasks require different levels of expertise and trigger different reasoning strategies [47]. Intuitively, images that are easier to diagnose will be described with more consensus lexical choices by physicians, and the concepts referred to by physicians’ utterances will be more closely related to the correct diagnosis. In order to verify this intuition, we evaluate the proposed metrics below, given the topic of challenge levels of image-based diagnosis.Physicians (human experts) perceive different levels of challenge when inspecting different image cases, which can be reflected in their spoken descriptions. We are interested in studying human behaviors and extracting human knowledge. For these reasons, it is important to identify a ground truth provided by human experts. We thoroughly examined the ground truth of different challenge levels in our image set. We have generated two different ranked lists as ground truth. One list was generated from a dermatologist who did not participate in the data collection experiment as a subject. This physician ranked difficulty levels of image cases based on personal expertise without knowing the performance of the participating physicians in the prior experiment. The other ground truth list was obtained from data analysis of judgments (made by the same dermatologist and two other non-involved dermatologists) of the performance of the 16 participating physicians, including the correctness of their described primary morphology, differential diagnosis, and final diagnosis. Prior inter-annotator analysis with the kappa metric indicates good inter-annotator agreement of correctness judgments for differential diagnosis as well as final diagnosis but less agreement on medical lesion morphology [31]. This ranking is linked to participants’ performance based on data analysis. We compared the two ranked lists and found that they are highly correlated. The Spearman correlation score between the two lists is 0.77, and the Kendall correlation score is 0.56. The equations of Spearman and Kendall correlation measurements are shown below, respectively:•Spearman coefficient:rs=(1/2SXSY){SX+SY−∑idi2}, where X and Y are ranked lists, Xiand Yiare ordinals, i=1, …, N.SX=∑ixi2,xi=Xi−X¯,X¯=(1/N)∑iXi, and diis the difference between ordinals Xiand Yi[48,49].Kendall coefficient: τ=((C−D)/(C+D)), where C is the total number of concordant pairs between the two ranked lists and D is the total number of discordant pairs [50].In order to confirm that the lexical consensus score metric indicates the challenge level of an image and to answer research question RQ1, we computed the pairwise cosine similarity among descriptions given by the 16 physicians in our experiment. We then normalized the 16×15/2=120 similarity scores and averaged them, thus obtaining the consensus score of the image. We ranked the 48 images based on their consensus scores and compared this ranking with the ground truth rankings suggested by non-participating physicians.The ranking of the 48 images based on the consensus score is correlated with both the difficulty-based ground truth and the correctness-based ground truth. The Spearman correlation score between the ranking based on consensus scoring and the difficulty ground truth is 0.64 (well correlated), and that between the consensus-based ranking and correctness-based ground truth is 0.57. According to Kendall correlation metrics, the consensus-based ranking is well correlated to both ground truth rankings. See Table 3.Besides predicting the challenge levels of image-based diagnosis, the lexical consensus metric could be further applied for other purposes, such as to infer the domain competence of clinicians in training.Intuitively, to measure a physician's understanding of a medical image content, we could judge whether each of his/her spoken terms is relevant to the correct diagnosis of the image. To attempt this, we related each medical concept that occurred in narratives to the correct image diagnosis and computed the relatedness score.Since each narrative contains more than one medical concept, we defined the top N (with N=1, 3, 5, 10) relatedness scores to capture multiple levels of semantic relevance. In order to answer research question RQ2, we compared the rankings of 48 images based on the top 1, 3, 5, and 10 relatedness scores with the rankings suggested by non-participating physicians.The correlation scores between the relatedness-based rankings and the ground truth rankings are also measured by Spearman and Kendall matrices and listed in Table 3. We find that among all relatedness-based rankings only the top 1 relatedness-based ranking is positively correlated with at least one of the ground truth rankings. It is also interesting to note that the top 3, 5, and 10 scores, though correlated to each other, are either poorly or even negatively correlated with the ground truth. Our observations of some special image cases explain this finding: (1) Fig. 3is a detailed illustration of a typical easy case and a typical difficult case, respectively, according to both the difficulty-based ground truth and correctness-based ground truth. Intuitively, the easy image case (see Fig. 3(a)) should be scored as having a high relatedness, since its correct diagnosis was uttered by most physicians. However, its lack of richness of highly related terms causes the overall relatedness of this image to be affected by the terms with low scores. On the contrary, the difficult case (see Fig. 3(b)), whose correct diagnosis was not uttered widely, benefits from its language richness, thus obtaining a score that is higher than our intuitive expectation. This observation informs us that considering a larger N for top N relatedness score calculation may suffer from noisy data caused by poorly related terms. (2) Also, in some narratives the physician only uttered a few medical terms that can be detected by UMLS. Out of 768 narratives, 134 narratives contain fewer than 10 medical terms, and 5 of them even contain fewer than 5 medical terms. In such cases, we could only average the relatedness scores of these few terms, as opposed to other cases in which the physician uttered more than 10 medical terms and we could average the top 3, 5, and 10 as we intended. Such difference in narration styles among subjects introduced the data mining problem of missing values[51] in the top 3, 5, and 10 relatedness-based rankings. Therefore, it is reasonable that only the top 1 relatedness score fairly represents the physicians’ best understanding of each image case. This could deviate in a different situation with other data properties.Besides predicting the challenge levels of image-based diagnosis, the relatedness metric could be used for other purposes, such as selecting keywords for a new database, indexing medical terms in metadata, or grouping dermatological images by their relatedness to primary morphology terms (instead of by their diagnoses). It could be further applied in other areas, such as keyword expansion and diagnostic error detection.Since we are interested in new ways of grouping and describing medical images derived from experts’ domain knowledge, we need to know whether physicians use different sets of medical terms to describe medical images showing different diseases and whether they use similar sets of terms to describe similar images. To begin to explore this, we examined the third research question by clustering diagnostic narratives based on physicians’ use of terms (words and detected multiwords). This provides evidence about whether physicians tend to use different terms as they describe images displaying different diagnoses. In this section, we first visualize the narratives to be clustered in a term-narrative matrix to stress the outstanding clusters of our data. After that, the details of clustering algorithms we use and the clustering results are provided. They provide a new perspective on how to group medical images.If shown similar images, the physicians should describe these images similarly. Since our image set represents a wide range of dermatology diagnoses, rather than numerous examples for each diagnosis, we cluster narratives by images instead of clustering images by diagnoses. However, the goal of clustering physicians’ narratives remains to advance image grouping in the future. Therefore, the ground truth for narrative clustering consists of 48 image labels, which are the correct diagnoses of the 48 dermatology images. The narratives corresponding to a single image are labeled the same in the ground truth. Once we have a larger image set with multiple image cases presenting each diagnosis, we could use physicians’ diagnostic narratives to cluster images by diagnoses.To best visualize the patterns of term usage in different image descriptions, we organize the medical terms containing words and multiwords uttered by physicians to describe any one (or more) of the images into a term-narrative matrix. The term-narrative matrix specifies each term's frequency of occurrence in a specific narrative. The values within the term-narrative matrix are tf-idf scores (term frequency multiplied by inverse document frequency) [52]. In order to make the patterns of term usage as clear as possible, the values are illustrated in Fig. 4. In Fig. 4(a), chunk 27 contains non-zero values that are densely distributed. chunk 27 is powerful for differentiating image 27 from the rest. In Fig. 4(b), the region contains loosely distributed non-zero values that form vertical dotted lines. These dotted lines represent the occurrence of medical terms that can be used to describe the manifestation of multiple different diseases, such as papules and plaques, which indicate the primary morphology of dermatological lesions, or scale and erosion which indicate the secondary morphology. These terms are also helpful in differentiating the images, since they each are shared by a subset of all diseases, though they are not tightly linked to and specifically used for describing one disease.The terms that are useful for clustering medical images should only occur in a few images but in a large enough number out of all the narratives corresponding to these few images. For example, the names of the correct diagnoses and their synonyms are medical terms that are labeled as the correct diagnoses of the 48 images in our experiment, such as lichen planus, BCC, and melanoma. Each of these terms occurs in a small number of images, because the 48 images in our experiment cover a wide range of dermatology diagnoses. Some of them only occur in a small number of narratives, since not all physicians managed to utter the correct diagnosis. The primary morphology terms, such as papule, plaque, nodule, and patch, are also useful for distinguishing among medical images. Primary morphology is highly informative in categorizing a medical image into a certain disease type. Other terms describe information such as patient demographics, lesion body location, lesion distribution, and secondary morphology. The occurrences of major categories of medical terms in our experiment are analyzed and visualized in Fig. 5.Since physicians’ understanding of a medical image is contained in the spoken narrative, we exploit the spoken narratives to incorporate physicians’ domain knowledge in medical image grouping. Narrative preprocessing as described above helps identify the medically related terms. Following traditional document clustering, each narrative diis denoted by a term vector of length |V|, in which di(j) is set to the normalized frequency (or other metrics such as tf-idf) of term tjif tj∈diand 0 otherwise. The length of diis equal to the size of the term dictionary, which consists of the distinct medical terms (words or multiwords) over all narratives. As different physicians may have distinct medical backgrounds, as well as preferences for the use of terms, various naming conventions may be used by different narratives. This variation results in a large number of distinct terms especially when scaling to a massive number of images.Partitioning-based clustering algorithms (e.g., K-means) have good clustering performance and can scale to a large number of medical images. Two term vectors are regarded as similar if they share a large enough number of terms. Directly applying these algorithms to spoken narrative clustering may lead to poor clustering quality, because the term vectors for narrative descriptions can be very sparse and hence less likely to share common terms. Advanced algorithms, such as those based on matrix factorization (e.g., SVD co-clustering [53] and NMTF [54]), have been demonstrated to be more effective in dealing with the limitations of data sparsity. However, the high complexity of these algorithms may lead to a computational bottleneck when clustering large-scale collections of medical images.Inspired by recent advances in latent factor models and sparse coding [55,56], we devise a novel strategy to discover a set of latent medical concepts, referred to as anchor concepts. The anchor concepts aim to capture the high-level medical concepts of the spoken narratives. Ideally, the anchor concepts are expected to map to the true medical concepts in the spoken narratives. For example, an anchor concept that corresponds to a body part may include terms like arm, leg, and face. Similarly, another anchor concept may correspond to the primary morphology, which covers terms like papule and plaque. Nonetheless, due to the various naming conventions and the diverse usage of terms by different physicians, such a perfect mapping is almost impossible for real-world verbal descriptions of medical images. Therefore, the anchor concepts are better understood as latent medical concepts that capture the underlying semantics of the verbal descriptions. In this regard, the anchor concepts are in line with the “quality dimensions” in Gärdenfors’ conceptual spaces theory [25,26]. In particular, by computing the anchor concepts, we are able to discover the “hidden conceptual space”, which corresponds to the underlying semantics of the verbal descriptions.Specifically, we compute a matrix C={c1, …, ck}, where eachci∈ℝndenotes an anchor concept and is a linear combination of a set of term vectors (represented by a matrix D), where the jth column denotes the term vector of narrative dj:(3)C={c1,…,ck}=DW(4)ci=Dwi=∑j=1mWijdj,∀i=1,…,kwhereW={w1,…,wk}∈ℝm×kis a weight matrix. Each entry Wijdenotes how much narrative djcontributes to anchor concept ci.The desired anchor concepts are expected to capture high-level medical concepts of the spoken narratives that can be used to recover the original term vectors of the narratives. Meanwhile, since a narrative is used to describe a specific medical image, it is common for the narrative to concentrate on a small number of medical concepts. Stated differently, a spoken narrative is expected to be only related to a small subset of anchor concepts. Therefore, a desired anchor concept set can be identified by optimizing the following objective function:(5)minC,zi≥0∑i=1m∥di−Czi∥2+λ∥zi∥1subjectto∥cj∥2≤a,∀j=1,…,kwherezi∈ℝ+kis the coefficient vector with zi(j) signifying the correlation between diand anchor concept cj, and a is a constant. Since each narrative should concentrate on a small number of medical concepts, ziis expected to be sparse. We achieve sparse zi(i=1, …, m) by incorporating an l1 norm (i.e.,∥zi∥1) and λ is the penalty parameter that controls the sparsity ratio. l1 norm has been demonstrated to be effective in finding sparse solutions. Hence, the second term of (5) corresponds to a sparsity constraint on zi. The norm constraint on the size of the anchor concept, i.e., ∥cj∥2≤a where ∥·∥ is the vector l2 norm, avoids arbitrarily large anchor concept vectors that keep Cziunchanged while making ziarbitrarily close to zero.The coupling between C and Z in the first term of (5) makes the overall objective function non-convex. Nonetheless, we can resort to an efficient iterative algorithm to find a local optimum. More specifically, by fixing Z, (5) is convex in C, and an optimal C* can be found by solving a constrained least square problem. Then, C* is kept fixed, which turns (5) into a convex function over Z. An optimal Z* can be achieved by solving an l1 regularized least square problem. This iterative process continues until a local optimum is achieved.Once C and Z are computed, the coefficient vector zican be regarded as the projection of tionto the anchor concept space C={c1, …, ck}. As the number of anchor concepts is much smaller than the number of terms, the anchor concept space C essentially forms a lower dimensional space to represent the verbal descriptions. This makes it similar to other dimensionality reduction techniques, such as LSA [28], which is widely employed in language understanding. The key difference between the proposed anchor concept-based approach and existing dimensionality reduction technique lies in the addition of the sparsity constraint to the coefficient vectors. In particular, the coefficient vector zicaptures the relevance between the ith narrative and all the k anchor concepts. The sparsity constraint on ziensures that each narrative is only related to a small number of anchor concepts. This is in line with the fact that when a physician describes a specific medical image, his/her spoken narrative is usually focused on a small number of medical concepts. Furthermore, spoken narratives can be easily separated based on their distinct relationships with the anchor concepts. Clustering of spoken narratives can be achieved by applying a simple clustering algorithm (e.g., K-means) to the coefficient matrix Z.We adopt two metrics to measure the clustering quality: accuracy (i.e., AC) and mutual information (i.e., MI). Both AC and MI are widely used metrics to assess the performance of clustering algorithms.•Accuracy: For a given narrative di, assume that its cluster label is liand its ground truth label is gi. The AC metric is defined as follows:(6)AC=∑i=1mδ(gi,map(li))mwhere m is the total number of narratives in the dataset. δ(x, y) is the delta function that equals 1 if x=y and equals 0 otherwise. Moreover, map(li) is the permutation mapping function that maps each assigned cluster label to the equivalent ground truth label. The best mapping between the two sets of labels is achieved by the Kuhn–Munkres algorithm [57].Mutual information: LetDbe the set of true narrative groups andCbe the narrative clusters obtained from a clustering algorithm. The mutual information metricMI(D,C)is defined as follows:(7)MI(D,C)=∑di∈D,cj∈Cp(di,cj)log2p(di,cj)p(di)p(cj)where p(di) and p(cj) are the probabilities that a randomly selected narrative belongs to group diand cluster cj, respectively. Furthermore, p(di, cj) is the joint probability that the randomly selected narrative belongs to both group diand cluster cj.To demonstrate the effectiveness of the proposed narrative clustering strategy, we include two widely used clustering algorithms, K-means and spectral clustering [58], for comparison purposes. We also include one widely used dimensionality reduction algorithm for comparison: LSA [28]. The K-means algorithm is then applied to the low-dimensional space to achieve clustering. Table 4summarizes the clustering results from all four algorithms. The proposed anchor concept-based clustering achieves the best results in both AC and MI categories. A confusion matrix visualization based on the anchor concept algorithm is shown in Fig. 6. The labels are mostly aligned with ground truth.

@&#CONCLUSIONS@&#
In this paper, we developed three research questions to explore the potential benefits of analyzing experts’ spoken inputs. We first explored the value of physicians’ spoken narratives by using the lexical consensus metric at the token level and the top N relatedness metric of terms’ relations to the correct diagnoses of images at the concept level. These two metrics were applied in the case study of the diagnostic challenge levels of medical images in order to verify the usefulness of the metrics and to answer research questions RQ1 and RQ2. Next, we clustered narratives using anchor concepts based on use of medical terms by physicians during medical image inspection (RQ3). The results show that if the use of medical terms by all experts is in agreement, the more likely the image was an easy case. In addition, the highest relatedness score computed from the most related medical concept uttered by physicians could help indicate the challenge levels of an image case. Finally, the images used in our experiment can be differentiated by term usage from the physicians’ utterances.