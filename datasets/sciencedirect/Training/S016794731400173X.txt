@&#MAIN-TITLE@&#
Reducing the computational cost of the ECF using a nuFFT: A fast and objective probability density estimation method

@&#HIGHLIGHTS@&#
A nuFFT method reduces the computational cost of the ECF by 100x.This enables a fast, robust, and objective kernel density estimation technique.We estimate PDFs of a huge ensemble of atmospheric model velocity gradients.The PDFs are self-similar and demonstrably non-Gaussian.

@&#KEYPHRASES@&#
Empirical characteristic function,ECF,Kernel density estimation,Histogram,Nonuniform FFT,nuFFT,

@&#ABSTRACT@&#
A nonuniform, fast Fourier transform can be used to reduce the computational cost of the empirical characteristic function (ECF) by a factor of 100. This fast ECF calculation method is applied to a new, objective, and robust method for estimating the probability distribution of univariate data, which effectively modulates and filters the ECF of a dataset in a way that yields an optimal estimate of the (Fourier transformed) underlying distribution. This improvement in computational efficiency is leveraged to estimate probability densities from a large ensemble of atmospheric velocity increments (gradients), with the purpose of characterizing the statistical and fractal properties of the velocity field. It is shown that the distribution of velocity increments depends on location in an atmospheric model and that the increments are clearly not normally distributed. The estimated increment distributions exhibit self-similar and distinctly multifractal behavior, as shown by structure functions that exhibit power-law scaling with a non-linear dependence of the power-law exponent on the structure function order.

@&#INTRODUCTION@&#
Research often calls for the estimation of probability distribution functions (PDFs) derived from empirical data. For instance, information about a distribution may be necessary to assess whether differences between two sets of data are statistically significant, or it may be required to estimate the probabilities that outliers come from the distribution of a given dataset. A variety of PDF approximations (e.g., histograms) are frequently used to represent the relative occurrence of data values.This paper describes a computationally efficient method to estimate probability distributions based on the recent work by Bernacchia and Pigolotti (2011). We have developed this technique to support research on scaling in the Earth’s atmosphere, but the method should be generally applicable across the physical and engineering disciplines. We have initially applied this method to aid the development of a theory about resolution dependence in atmospheric models. The following discussion necessarily makes heavy use of some terms that are commonly used in the atmospheric sciences but that may be unfamiliar to researchers from other fields. Appendix provides definitions for some of these terms.Many studies also require strictly non-parametric estimation procedures so that the resulting PDFs are free of a priori assumptions regarding their underlying functional forms. In our particular application, the normality of velocity gradients is a key hypothesis that should be proven or disproven from the emergent properties of the data itself without recourse to Gaussian fitting. The traditional methods for estimating PDFs, e.g. binning methods and kernel density techniques, require specification of a bandwidth parameter that heavily influences the shape of the resulting PDF (Silverman, 1986; Wilks, 2006; Srihera and Stute, 2011). While methods exist for estimating an optimal bandwidth, these methods usually require some assumption about the shape of the underlying PDF (Wand and Jones, 1995; Srihera and Stute, 2011; Bernacchia and Pigolotti, 2011). Given that our application requires an unbiased determination of the normality of the velocity increments, estimation methods utilizing such assumptions would not be suitable for our analysis. While methods do exist for testing normality (e.g., Murota and Takeuchi, 1981), our analysis additionally requires that we estimate various moments of the distribution. One can readily and efficiently perform tests for normality and estimate moments if an estimate of the underlying distribution is available.For these reasons, the method of Bernacchia and Pigolotti (2011) for estimating PDF distributions should in principle be well suited for such an application because it provides an objective PDF estimate that requires no prior assumptions regarding the underlying distribution. Bernacchia and Pigolotti (2011) derive an expression for a data-derived, optimal kernel (Watson and Leadbetter, 1963) and the resulting and self-consistent kernel density estimate; their kernel derivation method is even optimal for multi-modal data. This ‘self-consistent’ estimate converges on the true distribution at a faster rate than traditional binning or kernel density estimation methods (Bernacchia and Pigolotti, 2011).However, during our initial attempts to apply the PDF estimation method of Bernacchia and Pigolotti (2011) (hereafter BP11), we discovered that its computational performance is not practicable. Most of the BP11 algorithm is implemented in inverse Fourier space and is based on transforming the data under analysis to inverse Fourier space by computing its empirical characteristic function,Cn, given by:(1)Cn=∑j=1Nei⋅χj⋅τn,whereχjis a collection of data points that are presumed to come from a random distribution,Nis the number of data points, andτnare the frequencies at which the empirical characteristic function is calculated. Calculation ofCnis equivalent to an inverse discrete Fourier transform in which the Fourier coefficients areaj=1for each of theχjdata points. Since the direct calculation of the discrete Fourier transform is notoriously slow, it would be preferable to evaluate this discrete Fourier transform using the fast Fourier transform (FFT) method of Cooley and Tukey (1965). However, the FFT is not directly applicable since it requires that the Fourier coefficients are specified on an evenly-spaced grid. This requirement is violated since theχjdata points are presumed to be randomly distributed, and so their spacings are also random.In this paper, we show how to accelerate the computational performance of the BP11 density estimation method using the nonuniform FFT (nuFFT) method of Greengard and Lee (2004) to approximate the empirical characteristic function (Section  2). We demonstrate that this method substantially improves the speed of the BP11 density estimation method without compromising its accuracy or convergence properties (Section  3). We apply this method to estimate the PDF of velocity increments from atmospheric model output in support of a hypothesis relating velocity increments to model resolution dependence (Section  4). We show that the increments from a specific atmospheric model are generally bell-shaped but demonstrably non-Gaussian. Further, we use the estimated distributions to show that the velocity field is self-similar and multifractal. This ability to rapidly characterize increment distributions has thus proved invaluable in our development of a robust theory on resolution dependence in atmospheric models.Kernel density estimation is a widely used method for estimating the probability distribution function (PDF) of a given dataset (e.g., Silverman, 1986, Wilks, 2006), in which the PDF is approximated as a normalized sum of kernel functionsK(χ)centered on each data pointχj:fKDE(χ)=1N∑j=1NK(χ−χj).The choice ofK(χ)–particularly the width ofK–can heavily influencefKDE, and there is a host of literature devoted to choosing the kernel width. Except in some specific circumstances (e.g., the data are known to be normally distributed (Silverman, 1986)), the choice of the kernel and the kernel width are subjective (Liao et al., 2010; Bernacchia and Pigolotti, 2011; Srihera and Stute, 2011). Bernacchia and Pigolotti (2011) recently derived a method for objectively estimating the probability distribution function (PDF) of a univariate dataset. They show that the dataset itself can be used to derive a kernel (both its shape and width) in an objective, data-driven way. We summarize the essential details of the derivation and the method here.The inverse Fourier transform of the KDE estimate is simply the product of the transform kernel and the ECF of the data; we derive this relationship here, since it is relevant for understanding the role of the nuFFT in the BP11 method. Recognizing that a kernel density estimate is equivalent to a sum of convolutions between a kernel function and delta functions centered on the data:fKDE(χ)=1N∑j=1NK(χ−χj)=1N∑j=1N∫−∞∞K(s)⋅δ(χ−χj−s)ds=1N∑j=1NK(χ)∗δ(χ−χj),the kernel density estimate can readily be transformed to its inverse Fourier-space representation,ϕKDEusing the convolution theorem:ϕKDE(τ)=ℱτ−1[fKDE]=ℱτ−1[1N∑j=1NK(χ)∗δ(χ−χj)]=κ(τ)⋅1N∑j=1Neiχjτ=κ(τ)⋅C(τ),whereℱτ−1represents the inverse Fourier transform from data space,χ, to inverse Fourier space,τ;κrepresents the inverse Fourier transform ofK; andCrepresents the empirical characteristic function of the data.Bernacchia and Pigolotti (2011) use this relationship and the result of Watson and Leadbetter (1963), which states that the mean squared error of a kernel density estimate is minimized if the kernel satisfies the equation:κˆ=N⋅(N−1+|ϕ|−2)−1. They use this optimal kernel to provide an equation for the optimal PDF estimate (in inverse Fourier space):(2)ϕˆ(τ)=κˆ(τ)⋅C(τ)=C(τ)⋅NN−1+|ϕ|−2.Since the underlying distribution (and its transform,ϕ) is assumed to be unknown, they derive a solution to Eq. (2) using an iterative procedure in which an initial guess atϕ,ϕ0, is used to estimateϕˆ1which is then used as the next guess atϕto estimateϕˆ2, and so on. They show that if this iterative procedure converges, that it will converge to a solutionϕsc=κsc⋅C(which provides a self-consistent solution to Eq. (2):ϕsc(τ)=C(τ)⋅N⋅[N−1+|ϕsc(τ)|−2]−1), providedκscsatisfies the following equation, which is a function of the ECF amplitude:(3)κsc(τ)=N2(N−1)[1+1−4(N−1)N2|C(τ)|2]IA(τ),whereIA(τ)represents a frequency filter that is 1 for the set of accepted frequenciesA(defined below), and 0 otherwise.In order for Eq. (3) to provide a stable solution to Eq. (2), the set of accepted frequencies must be specified such that|C(τ)|2≥4(N−1)N−2forτ∈A. Further, the frequency setAmay exclude an arbitrary additional subset of otherwise acceptable frequencies, which reflects the arbitrariness of the initial guessϕ0of the iterative solution. Bernacchia and Pigolotti (2011) show thatϕscconverges to the true underlying distribution asNincreases, provided that a number of conditions are met, including the integrability of the characteristic function and boundedness ofA. The stability condition onAforcesκsc(τ)to be real-valued, implying that its data space representationKsc(χ)is symmetric.Finally, this self-consistent estimate can be Fourier transformed to obtain the data-space estimate of the PDF:fSC(χ)=ℱχ[κsc(τ)⋅C(τ)]. Provided that the ECF has been calculated, calculation ofκsc(τ)is trivial, so the bulk of the cost of computingfSC(τ)comes from the computation of the ECF.While exploring this BP11 density estimation method, it became clear that the ECF itself is a type of direct Fourier transform (DFT):C(τ)∝∑j=1Naj⋅eiχjτ,whereχjrepresents abscissa values in data space,τrepresents abscissa values in inverse Fourier space, and theajFourier coefficients are all 1. Since theχjvalues are assumed to be randomly distributed, they presumably are not regularly spaced, which excludes the possibility of using a standard FFT method to evaluate the DFT. However, the nonuniform FFT (nuFFT) method described by Greengard and Lee (2004) is specifically designed to reduce the computational cost of DFTs on irregularly-spaced data. The nuFFT method can be summarized as follows.An arbitrary dataset of abscissa and ordinate pairs,χjandaj, can be viewed as a continuous function that is a sum of weighted delta functions:a(χ)=∑j=1Naj⋅δ(χ−χj).Convolution ofa(χ)with a Gaussianghspreads the delta functions across the abscissa, which results in a smooth curve:a′(χ)=a(χ)∗gh(χ). By the convolution theorem, the Fourier transform ofa′(χ),c′(τ), is proportional to the Fourier transform ofa(χ),c(τ):c′(τ)=ℱτ(a′(χ))=ℱτ(a(χ)∗gh(χ))=c(τ)⋅g̃h(τ),whereg̃h(τ)is the Fourier transform ofgh.If the abscissa is sampled at regular intervals,χk, then a FFT technique can readily be used to approximate the Fourier transform ofa′(χk). Finally, the convolution theorem is used to deconvolvec′(τn)(dividec′byg̃h), which results in an approximation of the discrete Fourier transform of the irregularly-spaced (χj,aj) data. Greengard and Lee (2004) show that the nuFFT can approximate the DFT with arbitrary accuracy, which is controlled by the interaction of three main factors: the widthhof the convolving Gaussian; the number surroundingχkvalues at which the convolution is calculated for each (χj,aj) point; and the spacing of theχkgrid. The speed of the nuFFT method, which is a trade-off for accuracy, is also controlled by these three factors.With respect to using the nuFFT to calculate the ECF, the (χj,aj) abscissa/ordinate pairs are identically (χj,1), whereχjrepresent the random (irregularly spaced) data. With all theajvalues set to 1, the convolution step effectively reduces to a (unnormalized) kernel density estimate of the data:a′(χ)=∑j=1Ngh(χ−χj).So in statistical terms, the essential steps of the nuFFT approximation of the ECF can be summarized as: (1) perform a kernel density estimate (on a regular grid); (2) use an inverse FFT to transform the kernel density estimate to inverse Fourier space; and (3) divide the transformed density by the inverse Fourier transform of the kernel function.The following steps summarize the algorithm that we use to perform a fast and efficient calculation of the BP11 density estimate (for conciseness, we hereon express functions at a given grid point using the function symbol and the corresponding grid subscript: e.g.,Cn≡C(τn)):(1)configure a regular grid and its transform grid:χkandτn.specify a Gaussian kernelgh(x)=exp[−(x/h)2].convolve the data with the Gaussian to obtain a (unnormalized) kernel density estimate:fk′=1N∑j=1Nχj⋅gh(χk−χjh).perform an inverse FFT of the kernel density estimate to obtain its transform:ϕ′n=ℱτn−1(fk′).divideϕ′nby the transform of the Gaussian kernel to deconvolve the FFT and obtain an estimate of the empirical characteristic function:Cn≈ϕ′n⋅[g̃(τn)]−1.calculate the self-consistent kernel transformκnsc(Eq. (3)).calculate the self-consistent PDF transform:ϕnSC=κnsc⋅Cn.perform an FFT to obtain the self-consistent PDF estimate:fksc=ℱ(ϕnsc).If applied naïvely, the convolution in step (3) can be as expensive as the direct DFT calculation (or more so). ForNdata points, a full calculation of the convolution requires풪(N2)calculations, whereas the direct DFT calculation requires풪(N⋅M)calculations forMfrequency points and hence would be faster ifM<N. The speed of the convolution can be dramatically improved if the Gaussian contribution from each of theχjdata points is only applied to a limited set ofqsurrounding points. To this end, Dutt and Rokhlin (1993) provide an expression for specifying the width of the Gaussianhand the point-widthqof the convolution such that the resulting FFT is the same as the direct DFT within a specifiable accuracy. The convolution part of this algorithm requires풪(N⋅q)calculations and the FFT portion requires풪(M⋅logM)(Cooley and Tukey, 1965). Simple algebraic manipulation can show that ifq<MandlogM≪N, thenN⋅q+M⋅logM<N⋅M, and so the nuFFT is theoretically faster than the direct DFT calculation. These conditions also imply that the nuFFT-based calculation is theoretically풪(M/q)times faster than the direct calculation.To simplify the analysis of velocity increment PDFs and to provide a static grid on which all of the estimated PDFs can be stored, we standardize the data (i.e.,χj=(χj′−χ̄)⋅σχ−1) prior to applying the density estimation algorithm (χ̄andσχare the mean and standard deviation of the originalχj′data respectively). We specifyχkas 4097 evenly-spaced points from−20to 20 unit standard deviations. Since in our analysis theχjdata points are all real, the Fourier transform of these points has Hermitian symmetry and hence the redundant negative frequency components of the transform may be ignored. Therefore, theχkgrid yields a transform gridτnwith 2049 evenly-spaced frequency points. We only consider the lowest half of the frequency points (i.e., we setCn=0forn>1025) since the nonuniform FFT method is only guaranteed to provide a good approximation over this range (Dutt and Rokhlin, 1993). Following Dutt and Rokhlin (1993), we specify the width of the convolution kernel ash=1.5629, and we apply the convolution to theq=28χknearest points surrounding eachχjdata value. We find that this configuration produces an approximation of the ECF that differs from the exact DFT calculation by less than 10−7 over all considered frequencies (see Section  3 and Fig. 1).We also note that we implemented the selective frequency filter,In, in a slightly different manner than Bernacchia and Pigolotti (2011). They show that the self-consistent density estimate converges on the true density provided the filterInis set to 1 for some subset of the frequencies for whichCis above the estimate stability threshold given by|Cn|2≥4⋅(N−1)⋅N−2and set to 0 for all other frequencies. Whereas they choose the subset based on a frequency cut-offt∗such thatCis above the stability threshold for half of the frequencies within[−t∗,t∗], we choose a cut-off frequency based on the occurrence of three consecutiveCvalues below the stability threshold. In our implementationIn=0for alln>n∗, wheren∗is the index of the lowest frequency for whichCn∗+1,Cn∗+2, andCn∗+3are below the stability threshold. We choose this criterion because it is fast to implement and we find that it avoids an occasional, spurious leakage of high-frequency components that manifests as high-frequency waves superimposed on the density estimate. Bernacchia and Pigolotti (2011) note that the selection of the subset of frequencies is arbitrary and corresponds to the arbitrary choice of initial density estimate in the iterative procedure that they use to derive the expression forϕˆ. As long as the subset is bounded and the bound grows withN, a self-consistent estimate will converge. Our filter choice satisfies these criteria for integrable characteristic functions, since the stability threshold decreases with increasingNand therefore higher frequencies are permitted asNincreases. Therefore our implementation of theInfilter maintains the convergence properties of the BP11 density estimate (see Section  3 for verification of this).To show that the FFT-based approximation of the empirical characteristic functionC(FFT)reproduces the exact and direct calculationC(DFT)at high precision, we compare the two quantities calculated from the samples drawn from a normal distribution with sample sizes ranging from 64 to 4096. Fig. 1(a) shows the absolute difference between the two quantities,|C(FFT)−C(DFT)|, as a function of frequency for several different sample sizes. The FFT-based estimate differs from the true estimate by less than 10−7 or less over the entire frequency range. For reference, the inset of Fig. 1 showsC(FFT).Because the FFT-based approximation of the ECF differs from the true calculation of the ECF by such a small amount, the convergence properties of the BP11 density estimate are unaffected. Fig. 1(b) shows the mean squared error,E2(N)=∑jN|N(χi)−fˆ(χi)|⋅Δχ, whereN(χ)is the normal distribution andΔχis the grid spacing.E2(N)declines followingN−1for sample sizes ranging from21to 219 in agreement with the convergence rate presented by BP11. While the convergence-rate of the FFT-based method is in accord with the convergence rate from BP11 over the range of sample sizes shown, the FFT-based method should have a lower-bound onE2that is controlled by the approximation error,ϵ=|C(FFT)−C(DFT)|∼10−7. If the approximation-error of the density estimate,|fˆ(FFT)−fˆ(DFT)|, is larger than the nuFFT approximation error in the ECF, thenE2will be dominated byϵ, soE2will have a lower bound ofE2∼풪(ϵ2)∼풪(10−14). In this case,E2for the FFT-based method should flatten out for sample sizes larger thanN∼풪(1014), sinceE2∝N−1. This nonconvergence for extremely large sample sizes could be mitigated by increasing the width of the Gaussian kernel (bothhandq) to achieve a more precise estimate of the DFT. For the analysis in this manuscript, however, the sample sizes will not be so large thatE2approaches its limit.Fig. 1(b) also shows the time required to perform the density estimates from both the FFT-based method and the DFT method. As described in Section  2, the FFT-based method scales as풪(N⋅q+M⋅logM)whereas the direct method scales as풪(N⋅M). Because we useM=2049for both methods and for all sample sizes, both methods scale proportionally toN1for large sample sizes, as evinced by the parallel lines in Fig. 1(b). For sample sizes larger than풪(103), the FFT-based method is approximately 100 times faster than the DFT method. For an풪(106)sample size, the FFT method takes풪(1)second versus the풪(102)sfor the DFT calculation.In anticipation of the analysis presented in Section  4, we show a sample version of the same analysis applied to a dataset with well-known properties that mimics the data to which this method is applied in Section  4. The analysis in Section  4 has two goals: (1) to determine whether velocity increments are distributed normally; and (2) to show that the width and moments of the increment PDFs scale as a power-law of increment distance (as expected for a field with self-similar, fractal behavior).Because atmospheric velocities are known to exhibit statistical self-similarity in reality and in models (Nastrom and Gage, 1985; Skamarock, 2004; Rauscher et al., 2013), we apply the analysis to a realization of a fractional Brownian motion, which is a type of self-similar field (Mandelbrot, 1983). Fractional Brownian motion (fBm) can be categorized as a type of ‘red-noise’ field where the power spectrum of the field decays following a power-law: i.e.,P(f)∼f−β, wherePis the spectral power of the fBm field,fis the Fourier frequency, andβis the scaling exponent. fBm fields are characterized by their Hurst parameterH(Mandelbrot, 1983), which is directly related toβfor fBm fields by the relationshipH=(β−1)/2(Davis et al., 1994).Davis et al. (1994) define themth-order structure function of a fieldFasSmF(Δx)≡〈|F(x)−F(x+Δx)|m〉, which is themth (absolute) moment of the PDF of increments calculated at distanceΔx(we define〈…〉as the average). For fBm fields, the structure functions scale as a power-law of increment distance:SmF(Δx)∼ΔxHm. The exponent for themth-order structure functionHmis simply related to the Hurst exponent of the fBm field byHm=H⋅m(Davis et al., 1996). If we define the increment asΔxF≡F(x)−F(x+Δx), then themth-order structure function can be calculated from the increment PDF by(4)SmF(Δx)=∫−∞∞|ΔxF|m⋅P(ΔxF)dΔxF∼ΔxH⋅m.Since the fBm field is generated based on samples drawn from a normal distribution, it can be shown that the distribution of increments is also normally distributed (Davis et al., 1996). Therefore we expectP(ΔxF)to be a normal distribution with varianceS2F(Δx)∼Δx2H, implying that(5)P(ΔxF)=1σoΔxH2π⋅e−(ΔxF)2/2(σoΔxH)2,whereσois a constant of proportionality related to the total variance of the fieldF. This is the form of the PDF for any self-similar field with increments that are normally distributed.We use the method of Wood and Chan (1994) to generate an fBm field withH=0.6and 217 points. We apply the fast, self-consistent density estimation method described in Section  2 to estimate the PDF of increments at distances of21to29grid points, with distance intervals that are integer powers of two. Fig. 1(c) shows the PDF estimates of the standardized incrementsfˆ(ΔxF). The standardized increment PDFs (colored curves) overlap strongly and are consistent with a normal distribution with zero mean and unit variance (the thick gray curve). The inset of Fig. 1(c) shows that the moments of the PDFs scale as power laws (e.g., straight lines given log–log axes) of the increment distance. The structure functions are well-described by power laws as expected from Eq. (4). We estimate the exponents of the structure functions using the York et al. (2004) maximum likelihood method in log–log space, and we show in Fig. 1(d) that the exponents vary asHm=0.6⋅mas expected for an fBm field withH=0.6(Davis et al., 1996).As noted at the beginning of this section, the goal of this analysis is to show whether (1) increments are normally distributed, and (2) the moments of the increment PDFs scale as a power-law of increment distance. This analysis technique uses the fast, self-consistent density estimation method as an efficient way of verifying that an fBm field has these characteristics. The standardized PDFs overlap and are all consistent with a normal distribution, which provides evidence that the increments are distributed normally. The approximate linearity of the structure functions in the log–log inset of Fig. 1(c) provides evidence that the increment PDFs scale as a power-law of increment distance. And finally, the linearity of theHmvs.mpoints shown in Fig. 1(d) provides further evidence that the increment PDFs are normally distributed. It can be shown that the moments of the normal distribution follow the relationship∫−∞∞|x|mN(x)dx∼σm, whereσis the width of the normal distribution. From Eq. (5), the PDF width isσ=σo⋅ΔxH, so the moments should follow the relationshipMm∼σm∼ΔxH⋅m. Therefore, theHm=H⋅mrelationship demonstrated in Fig. 1(d) is consistent with increments that are normally distributed and have PDF widths that vary asΔxH.In a forthcoming manuscript (O’Brien, T.A., W.D. Collins, S.A. Rauscher, T.D. Ringler, M. Martini, W. Gustafson, and P. Ullrich, Fractal velocity fields cause resolution dependent updrafts in variable resolution atmospheric models. Journal of Geophysical Research. In Prep.), we develop a theory relating the distribution of vertical velocities (updrafts) in an incompressible atmospheric model to the probability distribution of horizontal velocity increments. We show that this theory predicts a resolution-dependent broadening of the vertical velocity distribution in a variable-resolution atmospheric model. In particular, for a self-similar horizontal velocity field with normally distributed horizontal increments, the theory predicts that the mean magnitude of vertical velocities〈|w|〉is simply related to the grid spacingΔxby〈|w|〉∼ΔxH−1, whereHis the Hurst exponent that characterizes the self-similarity of the horizontal velocity field.Our analysis of model output shows that the vertical velocity distribution broadens consistent with thisΔxH−1relationship. However, we have no a priori reason to expect that the horizontal velocity increments are distributed normally, and so it is unclear whether the observed broadening of the vertical velocity is truly consistent with our prediction. In order to characterize the distribution of horizontal velocity increments to evaluate this finding, it is necessary to estimate the PDF of풪(105)sets of풪(106)increment values. Given the amount of data reduction required in our analysis, and in fact in many other applications, a suitable method for estimating the PDFs should be as fast as possible to minimize the computational cost. The nuFFT-based improvement introduced in Section  2 reduces the computational cost of the BP11 method from approximately102sper estimated PDF to 1 s per estimated PDF (when applied to106data points). This reduces the computational cost of our analysis from풪(103)CPU hours (e.g., a month on a serial processor) to풪(10)CPU hours.We apply the analysis presented in Section  3 to output from an atmospheric model with an idealized setup. We use output from the Community Atmosphere Model 4 (CAM4) (Neale et al., 2010), which is a modular hydrostatic atmospheric model with a variety of parameterizations that simulate various processes important for atmospheric dynamics (e.g., radiative transfer, convection, precipitation, etc.). We use a version of CAM4 that includes the Model for Prediction Across Scales atmospheric (MPAS-A) dynamical core, which predicts the evolution of the atmosphere by evaluating conservation laws (e.g., conservation of mass, momentum, etc.) on a centroidal Voronoi tessellation of the sphere (Rauscher et al., 2013; Skamarock et al., 2012).The MPAS-A dynamical core is capable of operating on nonuniform grids that can effectively zoom in on an area of interest, which is one of the model’s distinguishing features. Initial evaluation of CAM4 with the MPAS-A dynamical core showed that the model exhibits some distinctly resolution-dependent artifacts (Rauscher et al., 2013; O’Brien et al., 2013). Subsequent analysis has shown that this resolution-dependence may be linked to resolution dependence of the vertical velocity field (Yang et al., 2014), and we have recently developed a theory linking the resolution dependence of the vertical velocity field to the self-similarity of the horizontal velocity field. The theory relates the PDF of vertical velocities to the PDF of horizontal velocity increments.To characterize the distribution of horizontal velocity increments at the model’s highest resolution, we use the uniform-resolution 30 km simulation described by Rauscher et al. (2013). We use one year of model output that is recorded for every 6 model hours. To facilitate this analysis, we have interpolated the CAM4 output from the its native, unstructured grid to a grid with uniform latitudinal and longitudinal spacing that has approximately the same 30 km resolution as the native grid; in this grid, the globe is divided into 768 latitudes and 1152 longitudes. The model is configured in accord with the aquaplanet protocol specified by Neale and Hoskins (2000), in which the surface of the simulated planet is covered with water, and all boundary conditions are specified with rotational (in the direction of planetary rotation) and hemispheric symmetry. We leverage the rotational symmetry by treating latitudinal bands at a given level (altitude) as statistically identical, which we use to improve our sampling statistics.At each time, latitude, and level in the model output, we calculate zonal velocity increments in the zonal direction (i.e.,ΔxU, whereUis the zonal wind velocity andxis the distance in the zonal direction). We calculate increments at all grid spacings that are powers of 2 between 20 and 210. We use the FFT-based density estimation method described in Section  2 to calculate the empirical characteristic function for each set of increments.We parallelized the algorithm described in Section  2 by performing steps (1)–(5) in parallel for each time slice. We perform an additional step (60), in which we add the empirical characteristic functions from each time slice (treating values at each specific latitude and level separately) to obtain the empirical characteristic function for zonal velocity increments for the full year of model output. We then apply steps (6) and (8) to obtain the estimate of the probability density of zonal velocity increments for each latitude and level. We use these probability densities to estimate the 1st through 9th absolute moments of each distribution, which yield the 1st through 9th order structure functions of the zonal velocity field (see Section  3.2).Fig. 2(a), (d), and (g) shows the estimated probability densities of the zonal velocity incrementsfˆ(ΔxU)from three distinct regions of the atmosphere: 40° S at the 700 hPa level (approximately 3 km altitude), 0° N at the 510 hPa level (approximately 5 km altitude), and 30° N at the 970 hPa level (approximately 400 m altitude). These increment probability distributions, which are all standardized, overlap relatively well, which is consistent with self-similar behavior. Fig. 2(b), (e), and (h) shows the first absolute moment of the increment distributions as a function of increment distance (i.e., the first-order structure functions). In all three figures, the first order structure functions exhibit approximate power-law scaling over a relatively wide range of increment distances, which is also consistent with self-similar behavior. The dashed gray lines in the figures show a power-law fit, using the York et al. (2004) maximum likelihood method, to the structure functions for increment distances ranging between approximately 100 and 500 km. We choose these bounds for two separate reasons. For the lower bound, it is well known that the diffusive properties of atmospheric models tend to dampen variability for length scales ranging from one grid cell to ten grid cells (Skamarock, 2004). This effect manifests as a steepening of the first order structure functions for the two smallest increment distances (distances corresponding to 1 and 2 grid cells), so we restrict the fit to increment distances that are greater than or equal to 4 grid cells, which is approximately 100 km. Additionally, since it is hypothesized that there should be a scale-break for distances greater than approximately 500 km (e.g., Nastrom and Gage, 1985), we restrict our fit to increment distances less than or equal to this value.We perform a similar power-law fitting procedure for the 1st through 9th order structure functions. Fig. 2(c), (f), and (i) shows the estimated power-law slopes (the structure function exponents)Hmas a function of structure function orderm. As discussed in 3.2, a self-similar field with normally-distributed increments should have structure function exponents that scale linearly with the structure function order, i.e.,:Hm=H1⋅m. Such monofractal scaling is shown as a solid gray line in Fig. 2(c), (f), and (i). The zonal velocity structure function exponents approximately follow this monofractal scaling for the 1st and 2nd order structure functions, but they diverge rapidly for the higher order exponents. This divergence is characteristic of a multifractal field (Davis et al., 1994), and it indicates that the zonal velocity increments are not distributed normally.It is also clear from comparing the estimated distributions in Fig. 2(a), (d), and (g) with that of a unit normal distribution (shown as a gray dashed curve in all three figures) that the estimated distributions do not overlap well with the normal distribution. In exploring other potential distributions, we found that the increment distributions closely matched a standard logistic distribution–f(x)∼sech(x/2)–over a wide portion of the atmosphere (shown as a solid gray curve in all three figures). However, it is apparent in Fig. 2(g) that some areas of the atmosphere have zonal increment distributions that are quite positively skewed and are therefore inconsistent with symmetric distributions like the logistic distribution.To demonstrate that the scaling properties of the zonal velocity field vary throughout the atmosphere, Fig. 3(a) and (b) shows latitude-versus-height maps ofH1, and the excess kurtosis of the increment PDFs. (It is conventional in atmospheric sciences to express heights in terms of atmospheric pressure, which decreases monotonically with height.) We calculate the excess kurtosis,γ2as follows:(6)γ2=〈|ΔxU|4〉〈|ΔxU|2〉2−3,and we average the excess kurtosis from increment distributions with increment distances ranging from approximately 100 to 500 km (the same range as used in the power-law fit described previously).Fig. 3(a) shows that the first order structure functionH1varies systematically throughout the atmosphere, with relatively small values near 0° latitude and relatively large values near 40° N/S. This is consistent with the first-order structure function of the water vapor field reported by Pressel (2012) for a similar model configuration. It shows that the (modeled) atmosphere is not well-characterized by a single scaling exponent, as suggested by Nastrom and Gage (1985), but that the fractal behavior of the atmosphere ranges from anti-persistent (H1<0.5) to persistent (H1>0.5) depending on location.Further, the excess kurtosis,γ2, which is a parameter that describes the ‘peakedness’ of a distribution relative to the normal distribution, also varies throughout the atmosphere. Fig. 3(b) shows thatγ2varies from approximately 1 at 0° latitude to approximately 7 near 30° N/S. A normal distribution is characterized by zero excess kurtosis, whereas distributions with sharper peaks and fatter tails (relative to the normal distribution) have positive excess kurtosis. The logistic distribution hasγ2=1.2, which is consistent with values over a wide area of the equator.Interestingly there are zones of high kurtosis near 1000 mb at approximately 30° N/S; these leptokurtic zones are associated with positive skew. The examination of Fig. 2(g), which shows the estimated increment distributions from this high-kurtosis zone, reveals that the negative half of the distribution overlaps reasonably well with the normal distribution, whereas the positive half of the distribution has wide tails. This positively skewed distribution reflects an abundance of zones in which the wind speed tends to accelerate in the eastward direction, which is indicative of a force acting in that direction. That this skewed distribution occurs in the midlatitudes (near 30° latitude), where the effect of Earth’s rotation becomes important, suggests that the Coriolis force may be the cause of the skewed distribution.

@&#CONCLUSIONS@&#
