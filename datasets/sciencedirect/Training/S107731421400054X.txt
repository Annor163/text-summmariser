@&#MAIN-TITLE@&#
Cross-modal domain adaptation for text-based regularization of image semantics in image retrieval systems

@&#HIGHLIGHTS@&#
In-depth literature review on related subjects.Two regularization strategies: interpolation and classification.Regularization on images lacking auxiliar information.Extensive evaluation shows astonishing results on CBIR.

@&#KEYPHRASES@&#
Content-based image retrieval,Query-by-example,Domain adaptation,Semantic representation,Cross-modal regularization,Class-specific regularization,

@&#ABSTRACT@&#
In query-by-semantic-example image retrieval, images are ranked by similarity of semantic descriptors. These descriptors are obtained by classifying each image with respect to a pre-defined vocabulary of semantic concepts. In this work, we consider the problem of improving the accuracy of semantic descriptors through cross-modal regularization, based on auxiliary text. A cross-modal regularizer, composed of three steps, is proposed. Training images and text are first mapped to a common semantic space. A regularization operator is then learned for each concept in the semantic vocabulary. This is an operator which maps the semantic descriptors of images labeled with that concept to the descriptors of the associated texts. A convex formulation of the learning problem is introduced, enabling the efficient computation of concept-specific regularization operators. The third step is the selection of the most suitable operator for the image to regularize. This is implemented through a quantization of the semantic space, where a regularization operator is associated with each quantization cell. Overall, the proposed regularizer is a non-linear mapping, implemented as a piecewise linear transformation of the semantic image descriptors to regularize. This transformation is a form of cross-modal domain adaptation. It is shown to achieve better performance than recent proposals in the domain adaptation literature, while requiring much simpler optimization.

@&#INTRODUCTION@&#
Image representation is a central component of computer vision problems such as image classification or content-based image retrieval (CBIR). In this context, the design of visual features has been a subject of substantial interest. Early representations relied on explicit representation of low-level image properties such as color, texture, or shape, through color histograms [1], color moments [2,3], Gabor wavelets [4], Fourier features [5], stochastic models [6], or shape contexts [7], among others. More recently, substantial effort has been devoted to the extension and robustification of these representations, through operations like normalization and spatial pooling, leading to modern descriptors such as SIFT [8], HoG [9], SURF [10], spatial pyramids [11], or Fisher vectors [12].It was also realized, early on, that one of the limitations of these representations is a semantic gap[13] between strict visual similarity, i.e. similarity in terms of patterns color or texture, and human judgments of image similarity. This spurred significant interest in the development of representations that account for semantic abstraction[14–22]. In CBIR, such representations are designed by identifying a vocabulary of concepts of interest for the retrieval operation and learning classifiers for the detection of these concepts. Images are then classified and mapped to a space where each feature is a score for the detection of a concept. Several methods have been proposed to implement this approach, under different terminology. In this work, we adopt the framework of [18], which refers to the representation as a semantic representation, and relies on the vector of posterior probabilities of the image under the concepts in the vocabulary, as semantic feature vector. This feature vector is denoted a semantic multinomial (SMN). As illustrated in Fig. 1, this representation maps each image into a point on the probability simplex. It should be noted that other implementations of semantic representation have been proposed in the literature, e.g. the query-by-example semantic retrieval method of [17], the classeme representation of [21], or the object bank of [22].The representation of images in a semantic space has several advantages. First, the generalization from low-level features to semantic concepts enables similarity measures that correlate much better with the expectations of CBIR users [15,18,23]. Second, because semantic features are, by definition, discriminant for tasks like image categorization, the semantic representation enables the solution of these tasks with low-dimensional classifiers [24,25]. Third, the semantic representation is naturally aligned with recent computer vision interest on contextual modeling[26–30]. This is of importance for tasks such as object recognition, where the detection of contextually related objects has been shown to improve the detection of certain objects of interest [31–33], or semantic segmentation, where the coherence of segment semantics can be exploited to achieve more robust segmentations [34–36]. Finally, due to their abstract nature, semantic spaces enable a unified representation for data from different content modalities, e.g. images, text, or audio. This opens up a new set of possibilities for multimedia processing, enabling operations such as cross-modal retrieval, where an image is used to search a database of texts and vice versa [37], or where an audio clip is used to rank a set of images [38].In this work, we exploit this support for cross-modal processing to design an improved image representation for CBIR. The basic idea is to leverage the fact that most images exist in a rich multimodal context, e.g. web-pages, which provide contextual information about the image content. In fact, some of this information may be much easier to model or classify than the image itself. For example, text classifiers tend to have higher accuracy than state-of-the-art image classifiers. Due to this, an SMN inferred from an image is likely to be more noisy than an SMN derived from an associated text document. This is illustrated in Fig. 1, where SMNs derived from images scatter through the semantic space much more than those derived from text.A question that arises naturally is whether it would be possible to exploit the presence of this text to denoise the semantic representation of the image. One possibility would be to simply replace the image SMN with the associated text SMN. This would reduce to the cross-modal retrieval scheme of [37], where a query image is matched to a database of texts. While effective, this solution is not fully general, since it assumes the availability of text for all images in the CBIR database. A more general solution is to collect a dataset of image-text pairs and learn a transformation that maps the ambiguous image semantics on the left of Fig. 1 to the less ambiguous text semantics on the right. This transformation can then be applied to images that have no complementary text. Because this denoising operation is likely to enable better generalization for all retrieval operations we denote it as a regularization of the semantic image representation. Since text information is used to regularize visual information, the process is denoted cross-modal regularization. The denoised semantic representation is denoted as regularized image semantics.We propose a cross-modal regularizer of image semantics (RIS) composed of three steps, illustrated in Fig. 2. Training images and texts are first mapped to the semantic space. A regularization operator is then learned for each concept in the semantic vocabulary. This operator maps SMNs of images labeled with that concept to the SMNs of the associated texts. Because the transformation is linear on an affine space (probability simplex), and the objective function is to minimize the mean squared error of the mapping, the problem can be framed in a convex formulation, which lends itself to efficient optimization. The process results in a set of concept-specific regularization operators. The final step is a procedure for the selection of the most suitable regularization operator for the image to regularize. This can be seen as a quantization of the probability simplex, where each quantization cell is associated with a regularization operator. Overall, the proposed regularizer is a non-linear mapping, implemented as a piecewise linear transformation of the image SMN to regularize. This is shown to enable better performance than other recent proposals in the domain adaptation literature [39–43], and requires a much simpler optimization.The paper is organized as follows. Section 2 discusses previous related work. Section 3 reviews the fundamental concepts of semantic representation. The proposed operator is then introduced in Section 4. Section 5 presents an extensive experimental evaluation of the regularizer in the context of CBIR. Finally, some conclusions are presented in Section 6. A preliminary version of this work appeared in [44].

@&#CONCLUSIONS@&#
In this work, we have proposed a cross-modal domain adaptation method that exploits training text to learn a regularizer of image semantics. The resulting regularization was shown beneficial for image retrieval, where it led to significant performance improvements on various challenging datasets. While the largest gains (up to double mAP) were obtained for retrieval problems where all database images are complemented by text, the method was also shown successful when this is not the case. In fact, for some datasets, it enabled gains even when no text was available to the retrieval operation.This robustness was justified by two properties of the proposed regularizer. The first is the semantic nature of the underlying image and text representation. This enables the modeling of contextual relationships between semantic concepts and establishes a unified space for image and text data. In result, the cross-modal regularization problem is reduced to one of adaptation between two homogeneous domains, i.e. there is no need to learn a translator between images and text. It was shown that, when compared to previous proposals to cross-modal regularization, this significantly simplifies the learning problem, enabling better generalization. The second is the implementation of the regularizer as a combination of class-specific regularizers. This leads to a piecewise-linear transformation of the image descriptors to regularize, which is highly non-linear but can be learned efficiently. When compared to previous approaches to domain adaptation in computer vision, the resulting regularizer is both more flexible and naturally aligned to the semantics of images and text. This was shown to enable significant gains in regularization performance.