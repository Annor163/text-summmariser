@&#MAIN-TITLE@&#
Integer programming models for feature selection: New extensions and a randomized solution algorithm

@&#HIGHLIGHTS@&#
Feature Selection (FS) is modelled as a (mixed) integer optimization problem.To solve this problem, a new FS algorithm (FSA) with short memory is proposed.This algorithm has been already successfully applied to life science data.New experiments on randomly generated and real biological datasets are reported.The results are compared w.r.t. other FSA confirming the validity of our approach.

@&#KEYPHRASES@&#
Data mining,Heuristics,Integer programming,

@&#ABSTRACT@&#
Feature selection methods are used in machine learning and data analysis to select a subset of features that may be successfully used in the construction of a model for the data. These methods are applied under the assumption that often many of the available features are redundant for the purpose of the analysis. In this paper, we focus on a particular method for feature selection in supervised learning problems, based on a linear programming model with integer variables. For the solution of the optimization problem associated with this approach, we propose a novel robust metaheuristics algorithm that relies on a Greedy Randomized Adaptive Search Procedure, extended with the adoption of short memory and a local search strategy. The performances of our heuristic algorithm are successfully compared with those of well-established feature selection methods, both on simulated and real data from biological applications. The obtained results suggest that our method is particularly suited for problems with a very large number of binary or categorical features.

@&#INTRODUCTION@&#
Feature Selection (FS) addresses a class of methods used to extract relevant information from data. FS has always been a central topic in Multivariate Statistics and Data Analysis, but has received important contributions also from mathematicians and computer scientists; at the same time, the ever increasing amount of data that are being collected in many real world applications, jointly with the evolution of technology, poses new challenges for FS methods. An example of such challenges can be found in the study of biological and genomic data, where interesting data sets may be composed of a few hundreds of tissue samples on which the activity of tens of thousands of genes is measured. The analysis of such data requires to identify a limited number of genes (i.e., features) able to identify an interesting model. Similarly, new data collection techniques based on cheap sensors and on internet activity are creating very large repositories where precious information may be hidden and needs to be mined out.In the general setting, FS can be described as follows: given a data matrix defined by a finite set of features measured on a finite number of objects, select a subset of the feature set that is particularly relevant, with respect to all the other possible subsets, for the analysis that is to be conducted on the data under study. In this work, we focus on supervised learning (i.e., classification), where data are analysed to identify a model able to predict if an observation belongs to one of two or more classes, based on the values of its features. In supervised learning, FS operates to select a relevant – and possibly small – subset of features to be used by the classifier.We propose a method designed to treat directly integer or binary features, keeping in mind that discretization methods are often used to transform continuous measures into discrete or binary ones; such a process is adopted in many settings to control noise, to ease the interpretation of the classification model, and, last but not least, to apply specific logic-based classifiers also in the presence of continuous features.The proposed approach is based on an optimization problem derived from the data matrix, where each feature is associated with a binary variable. Such an approach is not new in the literature; it stems from the minimum test collection originally described in Garey and Johnson (1979). We show that such optimization problems are still not tractable – even with state-of-the-art mixed integer solvers – and propose a new heuristic algorithm for their solution.The performances of our method are tested on different data sets, and compared with other established FS methods, in combination with classifiers of different nature. The tests are run both on simulated data sets, composed of binary features, and on two real genomic data sets, composed of continuous variables. For the latter, we adopt a simple discretization procedure. The results appear to be very satisfactory both from the standpoint of solution quality and of solution time, particularly when applied to data sets of large dimension.The paper is organized as follows. Section 2 provides a brief introduction to the different approaches to FS and the related literature. Integer programming models for FS are treated in Section 3. In Section 4 we describe the new Greedy Randomized Adaptive Search Procedure with memory proposed for the solution of the optimization problem associated with FS. The comparison of all the FS methods and their performances on real and simulated data sets are treated in Section 5 and its subsections, jointly with the description of the classifiers that we use to compare the different FS methods, and the main motivations of our experiments. Conclusions and future lines of work are drawn in Section 6.FS can be looked at from different angles. One may simply evaluate the features according to their individual merit, order them accordingly, and then select the desired number of them, possibly controlling the quality of the solution when the number of selected features increases. Such an approach is the one adopted by Ranker methods (Kira & Rendell, 1992a). Conversely, one may want to evaluate a subset of the features according to their integrated contribution, and thus is faced with a more complex subset selection problem, which has an intrinsic combinatorial nature and is recognized to be a complex problem. In the latter case, some methods are designed to construct a solution set by adding features iteratively, paying attention to evaluate the feature to be added conditionally to those that are already in the set; such a forward selection approach is paired with a backward approach, where features are, iteratively, eliminated from the current set.Another way of looking at FS methods is to distinguish them according to how the feature sets are evaluated and used in data analysis. This defines Filter, Wrapper, and Embedded methods (Bolón-Canedo, Sánchez-Maroño, & Alonso-Betanzos, 2013; Forman, 2003). Methods of the first group select features according to a score function; methods of the second group iteratively test feature sets performing data analysis, until a satisfactory result is obtained; to the third group belong those methods that automatically select the features that appear to be good for the purpose of their analysis. As recognized in Bolón-Canedo et al. (2013), filters are often to be preferred for their stand-alone nature and their speed when compared to wrappers. Indeed, analysing the performances of different methods on several synthetic data sets, the authors of Bolón-Canedo et al. (2013) conclude that filter methods seem to perform better. Also in Forman (2003), where the analysis is restricted to text classification problems, filter methods stand out – in particular, the Bi-Normal Separation proposed by the authors.FS problems of large size can be solved efficiently also with embedded methods; among the most successful ones are Support Vector Machines (SVM; Cristianini and Shawe-Taylor, 2000), where some proper modifications of the underlying optimization model can efficiently combine the choice of the separating hyperplane with the selection of good features (see, among others, Carrizosa, Martin-Barragan, & Morales, 2008; Maldonado, Pérez, Weber, & Labbé, 2014). For an additional overview of FS, the interested reader may refer to Guyon and Elisseeff (2003), John, Kohavi, and Pfleger (1994), Kira and Rendell (1992b), Liu and Motoda (2000), Liu, Li, and Wong (2002) and Swiniarski and Skowron (2003); a more specific analysis of FS methods for data mining is presented in Piramuthu (2004). As far as FS applications are concerned, a very actual battlefield is to be found in medical and bioinformatics data analysis, where supervised learning problems with very large number of features abound; here, data mining applications strongly rely on FS methods – some examples are in Dagliyan, Uney-Yuksektepe, Kavakli, and Turkay (2011), Lan and Vucetic (2013) and Peter and Somasundaram (2012).Particularly relevant for the scope of this paper are the methods that adopt a mathematical formulation of the FS problem based on integer variables, able to exploit its combinatorial nature. The most representative and seminal work in this line of research is the minimum test collection problem, stated in Garey and Johnson (1979), based on a Set Covering formulation where binary variables are associated with the features, and a covering constraint is defined for each pair of elements that belong to different classes. In these constraints the feature variable is present only if it exhibits a different value in the two addressed elements.Also in embedded methods, mathematical optimization is largely used. In Rubin (1990), the solution to a linear program is used to find a separating hyperplane between two sets of points; the linear program is then augmented with binary variables associated with features, resulting in a difficult problem for which several heuristics have been proposed. Similarly, in Bradley and Mangasarian (1998) linear separating hyperplanes are derived via linear programming, and then developed into the well-established theory of the already mentioned SVM (Cristianini & Shawe-Taylor, 2000). Iannarilli and Rubin (2003) adopt an optimization model, where additional packing constraints on binary variables control the dimension of the feature set, while the objective function takes care of maximizing a quality measure of the features based on the Kullback–Leiber divergence.In this paper, we propose a method based on some variants of the minimum test collection problem, that is guaranteed to provide a separation between the classes, but does not rely on the choice of a specific classification method. A similar approach is used, among others, in Boros, Ibaraki, and Makino (1999) and in previous applications to biological and genomic data (Bertolazzi, Felici, Festa, & Lancia, 2008; Weitschek et al., 2012; Weitschek, Velzen, Felici, & Bertolazzi, 2013). Such an approach is substantially different from methods based on the search of separating hyperplanes such as Bradley and Mangasarian (1998), Carrizosa et al. (2008), Iannarilli and Rubin (2003), Maldonado et al. (2014) and Rubin (1990).The adoption of a model where integer variables are associated with the choice of a feature sets results in computationally challenging problems, that become intractable for general purpose solvers when the dimensions of the problem increase. We thus propose a properly designed greedy randomized adaptive heuristic, usually referred to as GRASP (Feo & Resende, 1989; 1995), as a viable strategy to obtain good solutions for large FS problems that arise in supervised learning. As already mentioned above, the adoption of properly designed heuristics is frequent in FS problems: a similar GRASP approach is proposed, in a different framework, in Bermejo, Gámez, and Puerta (2011) to control the choice of the feature sets evaluated by a wrapper method; in Unler and Murat (2010) the importance of good heuristics for large sized FS problems is acknowledged, proposing a particle swarm optimization algorithm, while in Meiri and Zahavi (2006) simulated annealing is used to deal with FS problems arising in marketing applications.According to the distinction of FS into filter, wrapper, and embedded approaches, the method proposed in this work can be considered as a filter method, and therefore the main filter FS algorithms will be taken into account for a computational assessment of the quality of the results of our method. A more detailed description of these methods – namely, Relief (Kira & Rendell, 1992a), Las Vegas Filter (LVF) (Liu & Setiono, 1996), FOCUS (Almuallim & Dietterich, 1994), Correlation-based Feature Selection (CFS) (Hall, 1999), Sequential Forward (Backward) Selection (Elimination) SFS (SBE) (Devijver & Kittler, 1982), and Information Gain (InfoGain) (Hall & Smith, 1998) – is provided in Section 5.1.Following, we describe the integer programming models (Section 3) and the algorithm designed for their solution 4.In this section, we provide a general framework that motivates the use of Integer Programming (IP) models for FS. As a starting point, we assume to deal with continuous features; then, we specify our model for supervised learning and for integer or discrete features.Assume that a real-valued matrix A is available, composed of m rows and n columns, where the rows are associated with objects or samples drawn from one or more populations, and each column is associated with a measure of a feature. Therefore, aikwould represent the value of feature k on sample i. By denoting with M (resp. N) the index set of the rows (resp. columns) of A, it results that:m=|M|,n=|N|,A∈Rm×n.We are interested in maximizing the amount of information withheld by a small number of features (columns of A). The literature proposes alternative quantitative definitions of information, usually related to the extent to which the characteristics – or features – of the observed objects vary among each other, starting from the universally shared assumption that a measure that does not vary among the observations contains no information. Since a complete discussion of the definition of information is out of the scope of the paper, we point the interested reader to Hastie, Tibshirani, Friedman, and Franklin (2005), Phua, Lee, Smith-Miles, and Gayler (2010) and Witten and Frank (2005). Here, we consider a measure of information for real data, based on the Euclidean distance:(1)I(A)=∑i=1m∑j=i+1m∑k=1n(aik−ajk)2.I(A) is directly related to the variance expressed by A, a widely used measure in statistics and data analysis, or else to a measure of the entropy (Shannon, 1948) when the features can be described by qualitative classes.As anticipated, the task of FS is to reduce the dimension of the original matrix. Assume that we fix a target dimension β and consider the projection of A on a subset of columns N′ with|N′|=β<n. Such a problem can be easily formalized by introducing a binary variable xkdefined as follows:(2)xk={1,ifk∈N′;0,otherwise.Then, we have that(3)Ix(A)=∑i=1m∑j=i+1m∑k=1n(aik−ajk)2xkrepresents the portion of information preserved by the projection of the points of A on β columns, projection represented by the values of the binary vector x. The FS problem may then be formulated as follows:(4)maxIx(A)=∑i=1m∑j=i+1m∑k=1n(aik−ajk)2xk∑k=1nxk≤βx∈{0,1}n.An optimal solution for problem (4) can be found by applying a greedy strategy, by ordering the n features in decreasing order of contribution to the function to be maximized, and then selecting the first β features, similarly to the approach adopted by the Rankers described in Section 2.Problem (4) is therefore easy to solve, but presents a major limitation: its objective function is the sum (or, equivalently, the average) of the distances between all pairs, where the contributions of all pairs are mixed together to obtain the final value. In this way, few distant pairs of points may largely influence the projection, while the distance among many closer points may not be given the proper attention in the solution.Such a scenario is not infrequent in data analysis, mainly motivated by the presence of measurement errors, outliers, or different measures scales among the features. A trivial example is depicted in Fig. 1. Fig. 1(a) represents a very simple data set composed of 4 samples described by 2 real valued features x and y. In such a simple case, ifβ=1the only projections that may be considered are the one on the x axis and the one on the y axis. The first one, represented in Fig. 1(b), has the property of maximizing the objective function of problem (4), but is not able to separate the first 3 samples on the left from each other. On the other hand, the projection on the y axis (Fig. 1(c)) would not represent an optimal solution for (4) but is able to maintain a good spacing among all points. Although one could not argue that one projection is better than the other without knowing the final purpose of the analysis, this simple example brings to our attention one main point: the simple model (4) may result in projections that do not identify important information for separating points in the target space.An alternative approach to the problem consists in requiring – directly within the optimization model – that all pairs of points are distinct by at least a minimal threshold quantity α > 0, and then requiring such quantity to be as large as possible:(5)maxα∑k=1n(aik−ajk)2xk−α≥0∀i,j,i<j∑k=1nxk≤βx∈{0,1}nα∈R+.We now turn the focus on supervised learning problems, where each object belongs to one out of two or more classes and the selected features should support models that are able to tell the correct class of an object. In this case, the row vectors of A are partitioned into two or more different groups and the purpose is to project the objects onto a space where the classes are easily separable. In this case, a simple relaxation of model (5) would serve the purpose: if constraints associated with objects that belong to the same class are eliminated, then we only require points of different classes to be distant from each other. Model (6) reported below represents this problem. Here, the class of a sample is indicated by a mapping c from the set of the objects M into the index set of the p classes{1,…p},p≤Mand we writec(i)=c(j)if and only if objects i and j belong to the same class.(6)maxα∑k=1n(aik−ajk)2xk−α≥0,∀i,j,c(i)≠c(j),i<j∑k=1nxk≤βx∈{0,1}nα∈R+.Note that the relaxation above reduces only linearly the number of constraints of the model, without affecting its computational complexity.An additional variant is obtained in the case of objects represented by logic or categorical data (corresponding to binary or integer features) – a quite common case in many data mining problems. In this case, we do not define a continuous distance function among samples, but simply check if two samples are equal or different on a given feature. Therefore, we define the generic element of the constraint matrixdijkas:(7)dijk={1,ifaik≠ajk;0,otherwiseand problem (6) becomes:(8)maxα∑k=1ndijkxk−α≥0,∀i,j,c(i)≠c(j)∑k=1nxk≤βx∈{0,1}nα∈R+.The particular interest of model (8) resides in the fully binary nature of its constraints matrix. We finally note that model (8) has a strong connection with the minimum test collection of Garey and Johnson (1979): formally, the latter would be obtained from (8) simply by eliminating α from the constraints of the model, substituting their right hand side with 1, dropping the constraint involving β, and adopting the sum of thexk,k=1,…,nas the objective function to be minimized. While (8) fixes the dimension of the target space (i.e., β) and then maximizes the separating information withheld by the features, the minimum test collection problem identifies the smallest set of features with sufficient separating information.As a concluding remark, it is worth noting that model (8) has solution with α > 0 (or equivalently, the minimum test collection problem has feasible solution) only if the classes in which data is divided do not overlap, i.e., there are no observations of different classes that are exactly equal one another. In the case of overlapping classes, a natural strategy adopted in many methods is to eliminate the overlapping elements from the data sets, or else expand the feature set to obtain separation.Problems of the type described in (5)–(8) turn out to be very hard to solve (actually, they can be easily proved to be NP-hard). Moreover, real instances may have large size and, as confirmed by the experiments reported later, a fast heuristics needs to be used. Among different strategies, we propose to use a Greedy Randomized Adaptive Search Procedure (GRASP) (Feo & Resende, 1989; 1995) with some original modifications (among which, the use of a short memory). A GRASP meta-heuristic approach has been used also in Bermejo et al. (2011) in a FS context, to speed up the feature selection process in a wrapper approach. GRASP is a multi-start or iterative method, in which each iteration consists of two phases: construction of a solution and local search. The construction phase builds a solution x. Once x is obtained, its neighbourhood is explored by the local search until a local minimum is found. The best overall local optimum solution is kept as the result. The pseudo-code in Fig. 2 illustrates the main blocks of a generic GRASP procedure for minimization, in which MaxIt iterations are performed and Seed is used as the initial seed for the pseudo-random number generator.An extensive survey of the literature on this class of meta-heuristics is given in Festa and Resende (2002, 2011).Starting from an empty solution, the construction phase iteratively constructs a complete solution, one element at a time. At each iteration, the choice of the next element to be added is determined by ordering all the elements that can be added to the solution in a candidate list C with respect to a greedy functiong:C→Rthat measures the (myopic) benefit of selecting each element. The heuristic is adaptive because the benefits associated with every element are updated at each iteration of the construction phase, and thus reflects the changes brought on by the selection of the previous elements. The probabilistic component of GRASP is characterized by randomly choosing one of the best candidates in the list, but not necessarily the top candidate. The list of best candidates is called the restricted candidate list (RCL). In this context, a column covers a row of a binary matrix when that row exhibits a 1 in correspondence of that column. Since the selection involves candidate columns, the greedy function for a given column (say, column j) is based on the number of rows that have not yet been covered that are covered by column j.As a countermeasure, we introduce 2 elements of short memory in the construction process:•At each iteration, we memorize when each row has been covered (a row is cover at order h if it has been covered by the column selected at the h-th step of the solution construction process). Then, in each step of each iteration, we use the order derived in the previous iteration as follows: we consider all rows that have not been covered and have largest order. Among them one is picked at random; then, all columns that cover that row are placed in the RCL.The selection of a column c from the RCL is done sampling from a weight distribution. Such weights are higher for columns that have appeared in one of the (so far) best solutions; namely, they are proportional to the ratio between the number of current best solutions in which a column appears and the total number of current best solutions.As it is the case for many deterministic methods, the solutions generated by a GRASP algorithm are not guaranteed to be locally optimal with respect to simple neighbourhood definitions. Hence, it is always beneficial to apply a local search to attempt to improve a solution. A local search algorithm works in an iterative fashion by successively replacing the current solution by a better one in its neighbourhood. It terminates when no better solution is found in the neighbourhood. The neighbourhood structureNfor a problem relates a solution s of the problem to a subset of solutionsN(s). A solution s is said to be locally optimal if inN(s)there is no better solution in terms of objective function value. The key to success for a local search algorithm consists of the suitable choice of a neighbourhood structure, efficient neighbourhood search techniques, and the starting solution.In this case, we adopt a local search that, starting from a feasible solution, tries to find a better one, i.e., a new set of columns with lower cardinality (removal of redundant columns) and/or corresponding to a higher coverage.It is difficult to formally analyse the quality of solution values found by the GRASP methodology. However, there is an intuitive justification that views GRASP as a repetitive sampling technique. Each GRASP iteration produces a sample solution from an unknown distribution of all possible results. The mean and variance of the distribution are functions of the restrictive nature of the candidate list. For example, if the cardinality of the restricted candidate list is limited to one, then only one solution will be produced and the variance of the distribution will be zero. Given an effective greedy function, the mean solution value in this case should be of good quality, but probably suboptimal. If a less restrictive cardinality limit is imposed, many different solutions will be produced implying a larger variance. Since the greedy function is more compromised in this case, the mean solution value should degrade. However, by order statistics and the fact that the samples are randomly produced, the best value found should outperform the mean value. Indeed, often the best solutions sampled are optimal (see more details in Festa & Resende, 2002, 2011).

@&#CONCLUSIONS@&#
This paper presents an approach to solve feature selection problems in supervised learning. The contribution of the paper is to be found in the use of integer programming models, and in a new randomized heuristics to solve them. Using a set of randomly generated test problems, we provide experimental evidence of the practical challenge of solving our models at optimality, showing that even with state-of-the-art commercial solvers and significant computing resources, we fail in finding solutions that can be proven to be optimal. On the other hand, we show that the feasible solutions identified with the ad-hoc GRASP here proposed are good in quality. The performances of the method are compared with those of several well-known FS algorithms in their established implementations, on the simulated instances and on two real data sets. Our method always provides solution of comparable quality, and often outperforms the others. In particular, it provides good solutions for the simulated instances with binary features and when the bias introduced by the classifier is kept at a minimum (i.e., the case of Nearest Neighbour classifier).A distinguishing feature of the proposed model is to be found in its capability of incorporating, as linear constraints, additional knowledge to drive the selection of the features, e.g., requiring the covering of certain subsets of features in the solution, adding extra penalties for certain features, or controlling the error in the data by requiring larger separation thresholds for outliers.The extension of the proposed randomized heuristic to accommodate the management of additional constraints in the model is the object of currently ongoing research; an interesting direction is indeed related to the search for multiple feasible feature subsets, which could be modelled with additional constraints that cut out the already found solutions.