@&#MAIN-TITLE@&#
Efficient dictionary learning for visual categorization

@&#HIGHLIGHTS@&#
An efficient dictionary learning method is proposed.Fast kNN graph construction is well integrated with submodular dictionary learning.The proposed method finds a balance between accuracy and efficiency.

@&#KEYPHRASES@&#
Visual categorization,Efficient dictionary learning,Submodular optimization,Fast graph construction,

@&#ABSTRACT@&#
We propose an efficient method to learn a compact and discriminative dictionary for visual categorization, in which the dictionary learning is formulated as a problem of graph partition. Firstly, an approximate kNN graph is efficiently computed on the data set using a divide-and-conquer strategy. And then the dictionary learning is achieved by seeking a graph topology on the resulting kNN graph that maximizes a submodular objective function. Due to the property of diminishing return and monotonicity of the defined objective function, it can be solved by means of a fast greedy-based optimization. By combing these two efficient ingredients, we finally obtain a genuinely fast algorithm for dictionary learning, which is promising for large-scale datasets. Experimental results demonstrate its encouraging performance over several recently proposed dictionary learning methods.

@&#INTRODUCTION@&#
Visual categorization is a challenging task in pattern recognition and machine learning, and it has become a hotspot due to the large number of potential applications of categorizing large-scale data in real-world scenarios [1,2]. Intermediate feature representation serves as an important part in visual categorization. As the low-level visual features are often noisy and redundant, the intermediate features are widely exploited to obtain compact and discriminative representations for visual data, which can be leveraged to bridge the semantic gap between low-level visual features and high-level categories [3]. In particular, the sparse coding based approaches have achieved many impressive results [4]. Generally speaking, sparse coding approximates a query datum as a sparse linear combination of items from an over-complete dictionary, which coincides with the fact discovered in neural science that the human vision system utilizes a small number of words in a visual vocabulary to represent the input image [5].The performance of sparse coding is closely related to the quality of the dictionary. In [6,7], the dictionary is obtained using methods that minimize the reconstruction error. In [8,9], the local coding methods are proposed by imposing the locality constraints on the data-sets. Many efforts have been dedicated to embedding the discriminative information into the representations via supervised learning [10â€“12]. However, most of these methods are of high time complexity. Efficient dictionary learning on large-scale data remains a challenging task.Recently, submodular optimization has emerged as a powerful optimization facility in a variety of computer vision tasks such as superpixel segmentation [13], data clustering [20] and video retrieval [21]. A submodular function can be intuitively characterized as a diminishing return property, i.e., adding an element to a smaller set is more helpful than adding it to a larger set. On the other hand, submodularity can be viewed as a discrete analog of convexity, which shares some properties with convex optimization. Maximization of a submodular function is a typical instance of discrete combinational optimization, and finding the global optimum is very difficult due to its NP-hard property. However, provided that the solution is of a matroid structure, the greedy algorithm can be used to obtain a bound of12on the optimal solution. This useful property has led to some efficient algorithms for superpixel segmentation [13] and dictionary learning [14].The work in [14] can be viewed as a supervised extension of [13] for dictionary learning. In [14], the entropy rate term of its submodular objective function is the same as that in [13], which is used to enable compact and homogenous clusters. Besides, the discriminative term instead of the balancing term is employed to ensure class purity of each cluster. As their objective functions are submodular and subject to a matroid constraint, the greedy solution is quite efficient for clustering (see the analysis of time complexity in Section 4). The clustering procedure in [13,14] is essentially graph partition so that both of these two works involve the issue of graph construction. In [13], the construction of a kNN graph can be achieved with linear time complexity as the neighboring points can be promptly obtained via image grids. However, as for the data points, the construction of a kNN graph is not a trivial task and its time complexity is even higher than that of the clustering procedure with the implementation in [14], where all the possible pairwise distances need to be computed and then the k-nearest neighborhoods are exhaustively searched by comparing the pairwise similarities. Therefore, the algorithm presented in [14] does not meet the demand of efficient dictionary learning on large-scale data, although the time performance of its clustering procedure is fairly good.To address the aforementioned issues, we present an efficient submodular dictionary learning approach based on an approximate kNN graph, where the divide-and-conquer strategy is utilized to speed up the graph construction. As a result of the low time complexity of the approximate kNN graph construction, the computational time of the proposed approach is far less than that of [14] when evaluated on several canonical datasets. Moreover, the classification accuracies delivered by our approach are also comparable to those produced by the method presented in [14]. In Fig. 1, we show the pipeline of the classification procedure based on our dictionary learning approach.The rest of this paper is organized as follows. An overview of related work is presented in Section 2. In Section 3, we introduce some preliminaries, including sparse coding and submodular dictionary learning. In Section 4, we elaborate the details of the proposed approach. Some experimental evaluations are provided in Section 5. Finally, we summarize this paper in Section 6.

@&#CONCLUSIONS@&#
