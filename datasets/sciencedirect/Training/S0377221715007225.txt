@&#MAIN-TITLE@&#
Augmenting measure sensitivity to detect essential, dispensable and highly incompatible features in mass customization

@&#HIGHLIGHTS@&#
Complex variability models (VM) are commonly used in mass customization.The limitations of current measures and algorithms to identify essential and dispensable features in VMs are shown.We redefine existing measures by taking into account a sensitivity parameter.We propose an algorithm to efficiently compute the measures using Binary Decision Diagrams.We report empirical evidence of the goodness of our approach.

@&#KEYPHRASES@&#
Mass customization,Product platform,Variability modeling,Binary decision diagram,

@&#ABSTRACT@&#
Mass customization is the new frontier in business competition for both manufacturing and service industries. To improve customer satisfaction, reduce lead-times and shorten costs, families of similar products are built jointly by combining reusable parts that implement the features demanded by the customers. To guarantee the validity of the products derived from mass customization processes, feature dependencies and incompatibilities are usually specified with a variability model. As market demand grows and evolves, variability models become increasingly complex. In such entangled models it is hard to identify which features are essential, dispensable, highly required by other features, or highly incompatible with the remaining features. This paper exposes the limitations of existing approaches to gather such knowledge and provides efficient algorithms to retrieve that information from variability models.

@&#INTRODUCTION@&#
Companies have shifted from mass production to mass customization in order to increase product variety, improve customer satisfaction, reduce lead-times, and shorten costs (Liou, Yen, & Tzeng, 2010; Ngniatedema, Fono, & Mbondo, 2015; Simpson, Siddique, & Jiao, 2005; Takagoshi & Matsubayashi, 2013). For instance, van der Linden, Schmid, and Rommes (2007) report successful experiences of large companies such as Bosch (Gasoline Systems), Nokia (Mobile Phones), Philips (Consumer Electronics Software for Televisions), Siemens (Medical Solutions), etc.Mass customization enriches the mass production economies of scale with the flexibility of custom manufacturing by developing families of related products instead of single products. From this perspective, designing a product family requires developing a generic architecture, named product platform, that supports the creation of customized products, named derivatives, to satisfy different market niches. Derivatives are specified as combinations of features demanded by the customers (e.g., get a car with cruise control, speed limiter, directional stability control, etc.) (Apel, Batory, Kästner, & Saake, 2013; van der Linden et al., 2007).Product platforms usually offer a high number of features whose combination can produce a large quantity of derivatives (Sternatz, 2014). For instance, the BMW 7-Series platform supports 1017 derivatives (ElMaraghy et al., 2013). Typically not all feature combinations are valid. There may be feature incompatibilities (e.g.,“manual transmissions are not compatible with V8 engines”), feature dependencies (e.g., “sport cars require manual gearbox”), etc. As product platforms grow and evolve, the need for feature variability increases, and managing that variability becomes increasingly difficult (Bachmann & Clements, 2005). Variability models (also known as configuration models, feature models, etc.) are widely used to support variability management by modeling the dependencies and incompatibilities among features (Pohl, Bockle, & Linden, 2005; Zhang & Tseng, 2007).Operational research methods are currently being used to tackle several problems regarding the automated management of variability models. For instance, they are applied to search which derivative best fulfills the requirements of a given customer in terms of costs and/or utilities (Du, Jiao, & Chen, 2014; Yang et al., 2015), to diagnose and re-factor variability models (Zhang, 2014) (e.g., by removing redundant constraints), etc. In particular, the goal of this paper falls into the model diagnosis domain.Product variety creates both challenges and opportunities. Customers prefer broad product variety and, therefore, marketing managers are rewarded with greater revenue when product platforms are extended (Jacobs, 2013). Nevertheless, this may increase costs and reduce profits (ElMaraghy et al., 2013; Patel & Jayaram, 2014; Salvador, Chandrasekaran, & Sohail, 2014; Takagoshi & Matsubayashi, 2013; Yenipazarli & Vakharia, 2015). In order to provide guidance on how maximize the profits and minimize the costs associated with product platforms, this paper describes how to analyze variability models to identify which features are essential, dispensable, highly required by other features, or highly incompatible with the remaining features. That is, features in a product platform have usually varying degrees of importance. Some features may be highly demanded by the market and so most derivatives should include them. Other features may become dispensable as the market demand evolves. In addition, there may be features that indirectly become of key importance because other essential features need them. Finally, product platforms may include highly incompatible features whose presence disable many other features, hindering feature combinability.Since variability models specify how features can be combined to get the valid derivatives, it is possible to identify the incompatibility and the relative importance of the features by directly inspecting the models. Nevertheless, existing approaches to carry out such identification have the following limitations:1.Measures are rigid. Measures are needed to account for feature incompatibility and relative importance. Although some measures have been proposed (Benavides, Segura, & Ruiz-Cortes, 2010; Boender, 2011; Cosmo & Boender, 2010; van Deursen & Klint, 2002; Zhang, Zhao, & Mei, 2004), their sensitivity is not adjustable and thus they are often too rigid in practice. For instance, the dead measure is commonly used to detect if a feature is expendable for a product platform (Benavides et al., 2010). Its traditional definition is: “a feature is dead if, due to its dependencies and incompatibilities with the remaining features, it cannot be included in any derivative”. Imagine a feature that can only be included in 1 percent of the derivatives. The high dispensability of such feature would go unnoticed for the current definition of dead feature.Algorithms to compute the measures are inefficient. Calculating the measures by hand is unfeasible for all but the most trivial variability models, so their automated computation is required. The usual way to perform that computation is to translate the models into Boolean formulas and use off-the-self logic tools, such as SAT solvers (Batory, 2005) or Binary Decision Diagrams (BDDs) (Mendonça, 2009), to get the measures. Unfortunately, current algorithms have poor time performance (Fernandez-Amoros, Heradio, Cerrada, & Cerrada, 2014; Heradio, Fernandez-Amoros, Cerrada, & Abad, 2013; Mendonça, 2009).To overcome the aforementioned limitations, this paper contributes with:1.The redefinition, by taking into account a sensitivity parameter, of the existing measures. In particular, the following ones have been extended: dead and core features (Benavides et al., 2010), impact and exclusion sets (Boender, 2011), feature necessity and incompatibility (Boender, 2011). As we will see, sensitivity is a number between 0 and 1. For instance, if the dead sensitivity is set at 0.01, a feature passes to be considered dead if its reusability is restricted to 1 percent of the derivatives.Algorithms to efficiently compute the measures from a variability model. The input of our algorithms is the Propositional logic codification of a variability model. Since more complex logics than the Propositional one, which include integer arithmetic, transitive closure, etc., can be reduced to Boolean functions (Huth & Ryan, 2004; Jackson, 2012), our algorithms are general enough to support most variability model notations. In fact, the paper includes an experimental validation of our algorithms processing models specified in three different notations: the Configit language (http://configit.com/), feature models (Kang, Cohen, Hess, Novak, & Peterson, 1990), and decision models (Reuse-driven software processes, 1993).This paper provides empirical evidence of the usefulness of our measure redefinition to detect essential, dispensable and highly incompatible features that go unnoticed using current measures. It also shows that our algorithms not only support the efficient computation of the flexibilized measures, but also have better time performance than existing algorithms when computing the rigid measures.The remainder of this paper is structured as follows. Section 2 introduces the background required to understand our work, i.e., variability models and current measures. Section 3 summarizes related work to our approach, identifying its limitations. Section 4 describes our approach, defining the flexibilization of the considered measures thanks to the sensitivity parameter, and presenting the algorithms that support the computation of the measures. Section 5 reports the experimental validation of our approach. Finally, Section 6 outlines the conclusions of our work.

@&#CONCLUSIONS@&#
The development and maintenance of product platforms require the management of complex variability models. The role each feature plays in such models is unclear to the naked eye and so automated support is needed to identify which features are essential, dispensable, highly required by other features and highly incompatible with the remaining features. We have exposed the drawbacks of existing approaches to provide that support.Due to the rigidness of the current measures to account for feature interrelations, relevant information is frequently overlooked. Moreover, existing approaches to automatically compute the measures from variability models have poor time performance.To overcome such problems, we have increased measure sensitivity by introducing the concepts of feature probability and conditional probability. It has been empirically shown that our measure flexibilization unveils critical information that current measures cannot detect.Finally, we have provided new algorithms to support the computation of these measures. It has been shown, both theoretically and experimentally, that our algorithms not only can take into account different levels of sensitivity, but also are more time-efficient than related work even for computing the rigid measures.