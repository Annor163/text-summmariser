@&#MAIN-TITLE@&#
Re-parameterization reduces irreducible geometric constraint systems

@&#HIGHLIGHTS@&#
A new re-parameterization for reducing and unlocking irreducible geometric systems.No need for the values of the key unknowns and no limit on their number.Enabling the usage of decomposition methods on irreducible re-parameterized systems.Usage at the lowest linear Algebra level and significant performance improvement.Benefits for numerous solvers (Newton–Raphson, homotopy,p-adic methods, etc.)

@&#KEYPHRASES@&#
Geometric constraints solving,Geometric modeling with constraints,Re-parameterization,Reduction,Decomposition,

@&#ABSTRACT@&#
You recklessly told your boss that solving a non-linear system of sizen(nunknowns andnequations) requires a time proportional ton, as you were not very attentive during algorithmic complexity lectures. So now, you have only one night to solve a problem of big size (e.g., 1000 equations/unknowns), otherwise you will be fired in the next morning. The system is well-constrained and structurally irreducible: it does not contain any strictly smaller well-constrained subsystems. Its size is big, so the Newton–Raphson method is too slow and impractical. The most frustrating thing is that if you knew the values of a small numberk≪nof key unknowns, then the system would be reducible to small square subsystems and easily solved. You wonder if it would be possible to exploit this reducibility, even without knowing the values of these few key unknowns. This article shows that it is indeed possible. This is done at the lowest level, at the linear algebra routines level, so that numerous solvers (Newton–Raphson, homotopy, and alsop-adic methods relying on Hensel lifting) widely involved in geometric constraint solving and CAD applications can benefit from this decomposition with minor modifications. For instance, withk≪nkey unknowns, the cost of a Newton iteration becomesO(kn2)instead ofO(n3). Several experiments showing a significant performance gain of our re-parameterization technique are reported in this paper to consolidate our theoretical findings and to motivate its practical usage for bigger systems.

@&#INTRODUCTION@&#
Geometric modeling by constraints  [1–7] leads to large systems of non-linear (algebraic most of the time) equations. In their seminal work, Gao et al.  [8] automatically generated all the possible irreducible and structurally well-constrained 3D systems of geometric constraints (which they called basic configurations) that involve up to six geometric primitives (points, lines, and planes). These basic configurations correspond to 3D sub-problems that often occur in geometric constraint solving problems, in variational modeling, or in CAD/CAM applications. Most of the time, and contrarily to the 2D case, there is no closed-form solution for such 3D basic configurations. Gao et al. proposed the Locus Intersection Method (LIM) for solving these basic configurations and showed that among these possible 683 systems, 614 ones can be solved by using one key unknown (also called the driving parameter), while solving the remaining 69 ones requires two key unknowns. They referred to these two re-parameterization solving methods as LIM1 and LIM2.Re-parameterization consists in identifying or introducing a small number of key unknowns, also called parameters in the literature (hence the re-parameterization term), which have the following property: “if the values of these key unknowns were known, then the system would be reducible to much smaller structurally irreducible subsystems and thus it would be easily solved”.The work presented in  [9] relied on properties of bipartite graphs underlying systems of equations, to polynomially decompose large systems into well-, over-, and under-constrained subsystems. The same paper  [9] also proposed an efficient method to decompose or reduce well-constrained systems into irreducible and well-constrained (having as many equations as unknowns) subsystems. These decompositions have considerably speeded-up the solving process, and also allowed debugging systems of constraints in a constraint programming context. However, the reduction methods proposed in  [9] have limitations. For instance, they do not apply to re-parameterized systems proposed in  [8,10–12]. This inability to reduce re-parameterized systems is due to the fact that the methods of  [9] are unable to reduce irreducible systems, and that re-parameterized systems are irreducible. Later on, after the locus intersection method of Gao et al.  [8] became popular, several techniques for the decomposition of geometric systems with re-parameterization have been proposed  [12,10,11]. These methods decompose well-constrained 3D systems into re-parameterized subsystems with a small set of key unknowns per subsystem, perform in polynomial time, and provide sub-optimal but good results.In spite of the breakthrough made by the re-parameterization technique and the aforementioned methods seeking to find small sets of key unknowns, there are two major limitations. First of all, since re-parameterized systems are irreducible, the decomposition methods proposed in  [9] cannot apply to them. Second, with basic configurations involving more than six geometric primitives or for systems of geometric constraints involving more complex geometric primitives (cylinders, spheres, cones, tori, etc.), using one or two key unknowns is not enough, even when employing the best re-parameterization techniques known so far  [12]. In other words, although LIM1 (Locus Intersection method with one key unknown) is very fast and simple, its variantsLIMk(Locus Intersection methods involvingk≥2key unknowns) become much less convenient.Contribution. Our work addresses the aforementioned major limitations of re-parameterization/decomposition techniques. It proposes a technique for efficiently reducing or unlocking irreducible re-parameterized systems of equations like those proposed in  [8,10–12] and resulting from geometric constraint systems and geometric modeling applications, so that the decomposition methods proposed in  [9], which were unable to reduce such re-parameterized irreducible systems, become applicable. Furthermore, this work shows that it is possible to benefit from these decomposition techniques even when the values of the key unknowns are not known and the number of these key unknownskis greater than 2. For this purpose, we propose to exploit re-parameterization at the lowest level, which is the level of the underlying linear algebra routines: solving a linear system or inverting a matrix. The focus on the lowest level for exploiting re-parameterization is not hazardous. It is pertinent and highly motivated by the fact that most existing solvers, like Newton–Raphson or homotopy rely on the aforementioned low-level linear algebra routines. Consequently, this level seems to be the best place to exploit re-parameterization. Although, doing so does not prevent using re-parameterization at some higher level.Our paper focuses on exploiting the re-parameterization technique for reducing and thus efficiently solving well-constrained irreducible re-parameterized systems which are determined in advance and for which the key unknowns or parameters (even if their values are unknown) are already identified. It does not seek to find the best decomposition or re-parameterization of a system, a problem that has already been investigated in the literature  [12] (cf. Section  10).Although this paper focuses on solving well-constrained irreducible re-parameterized systems of equations and not on directly solving (under-constrained) geometric constraint systems involved in geometric modeling, the latter easily translate into well-constrained systems of equations which are perfectly handled by our technique. For instance, all the basic configurations enumerated in  [8] can be solved by our technique which goes beyond the locus intersection method as the latter is limited to one or two key unknowns, while we do not have such limitation. Other examples of geometric constraint systems can be found in  [12], while the particular case of the pentahedron problem and the way it is more efficiently solved through re-parameterization are discussed in Section  8.The rest of this paper is organized as follows: we first introduce the re-parameterization technique in Section  2 through examples, with a particular emphasis on the LIM involving one parameter. In Section  3, we briefly present matching theory and show how combinatorial decomposition methods are applied in order to improve the performance of linear algebra routines. After that, we show in Section  4 how decomposition speeds-up linear algebra routines. In Section  5, we show how re-parameterization speeds-up linear algebra routines for re-parameterized systems, so that Newton and homotopy methods can straightforwardly benefit from re-parameterization. This section also draws a complexity study. Section  6 explains how Hensel lifting inp-adic methods can take advantage of re-parameterization as well. This is an important result as it shows that not only numerical analysis, but symbolic computations, like Gröbner bases, may also benefit from re-parameterization. Section  7 shows that interval solvers may also benefit from re-parameterization, however the wrapping effect requires further research. Section  8 presents an experimental study of the performance of our re-parameterization technique at the lowest level of linear algebra routines involved in numerous solvers (Newton–Raphson, homotopy, and alsop-adic methods relying on Hensel lifting) and shows an important speed-up. This section also presents a CAD example showing the benefits of our re-parameterization technique when applied to geometric constraint systems. Section  9 examines the issues of using re-parameterization at a higher level. Finally, Section  10 presents future works and open questions before Section  11 concludes the paper.In this section, we first illustrate the re-parameterization technique by means of two examples in 2D and 3D.Fig. 1depicts a system of geometric constraints in 2D. For this system, the lengths of all the edges are given. It is easy to see that this system is under-constrained because it involves twelve unknowns (2D coordinates of its six vertices) and nine (distance) constraints. To make this system well-constrained, we employ placement rules commonly used in the literature, to constrain the placement of a particular subset of a geometric system  [13], and transform it into a well-constrained system, without affecting the set of possible solutions. For our 2D system, we fix three coordinates in 2D, which is equivalent to fixing the positions of the three points of one triangle, sayA′B′C′. PointA′(xA′=0,yA′=0)is placed at the coordinates origin, pointB′(xB′>0,yB′=0)is placed on the positivex-axis, and pointC′(xC′,yC′>0)is placed in thexy-plane with positiveycoordinate. By doing so, the placement of triangleA′B′C′has a unique solution which can be easily found, and this placement rule transforms the original under-constrained system into a well-constrained one of six equations (expressing the lengths of the six edgesAB,BC,CA,AA′,BB′, andCC′) in six unknowns (the coordinates of the vertices of triangleABC, those of triangleA′B′C′are known). This system has a finite number of solutions, at least for generic values of the lengths (e.g., non-null values). Unfortunately, this system is structurally irreducible, i.e., it is not decomposable into smaller well-constrained subsystems.We observe that if the lengthuof edgeAB′called a key unknown or parameter is known (cf. Fig. 1 right), then the whole system would be easy to solve. In fact, it would be easy to determine the coordinates of pointAas a first step, by solving a system of two equations in two unknownsxAandyA. Geometrically speaking,Ais one of two intersection points of two circles, the first circle has centerA′and radiuslAA′(length of edgeAA′), while the second one has centerB′and radiusu. Then in a second step, the coordinates of pointBcan be computed in a similar way, as the intersection of two circles with known centers and radii. Third and finally, pointCcan be computed in three different ways because three equations are available (lengths of edgesAC,BC, andCC′), but any of the three combinations of two of them is enough for determining pointC. One of these equations can be “ignored”, still assuming thatuis known of course. We will assume that the ignored equation is the one corresponding to the length ofCC′. So to summarize, ifuis known, then the system becomes decomposable (or reducible) and thus very easy to solve.The aforementioned reasoning holds when the value ofuis known. However,uis in fact unknown. In order to solve the system foru, i.e., to compute the correct value ofu, it is possible to plot the functionsA(u),B(u),C(u)(through sampling for example), and to detect when the ignored equation determining the length of edgeCC′is satisfied. Note that by construction, all the other constraints are satisfied. Some iterations of the Newton–Raphson method or the dichotomy should be enough to tune the value ofu. Since in some way, re-parameterization transforms an initial system of six equations in six unknowns into a system of one equation (the ignored equation) in one unknown (parameteru), it comes with no surprise that the re-parameterized system can be solved more efficiently than the initial one. However, many complications arise in the re-parameterized system due to the facts that: (1)A(u),B(u), andC(u)are actually multi-functions (algebraically, square roots are involved; geometrically, two circles intersect in two points), and (2) the construction ofA(u),B(u), andC(u)may fail for some values ofu(algebraically, for the square root of a negative number; geometrically, for the intersection of two disjoint circles). Finally, a third issue concerns the choice of the interval of the possible values ofu. In this example, because of the triangle inequality of triangleAA′B′,ucan be bounded asu≤lAA′+lA′B′andu≥|lAA′−lA′B′|. Similarly, the triangle inequality of triangleABB′implies thatu≤lAB+lBB′andu≥|lAB−lBB′|. When the computed intervals for the values ofuare disjoint, there is no real solution for the system.We define the big re-parameterized system as the concatenation of the initial system and another equationu2−(A−B′)⋅(A−B′)=0that defines the key unknownu. The big system has one more equation and one unknown (u) than the initial system of six equations in six unknowns. It is also well-constrained as it has as many equations as unknowns. Its equations are independent (almost everywhere the Jacobian has a full rank), so the big re-parameterized system has a finite number of real solutions, at least for generic values of the given lengths.Sinceuis an unknown of the big system, the latter is irreducible. Indeed, the equations of the big system are the following:(1)0=dist2(A,B′)−u2equation definingu(2)0=dist2(A′,A)−lA′A2(3)0=dist2(B′,B)−lB′B2(4)0=dist2(A,B)−lAB2(5)0=dist2(A,C)−lAC2(6)0=dist2(B,C)−lBC2(7)0=dist2(C′,C)−lC′C2ignored equation ,wherelA′A,lB′B,lAB,lAC,lBC,lC′Cdenote the lengths of the six edges anddist2(A,B)=(xA−xB)2+(yA−yB)2. The Jacobian of this system has the structure of the table depicted in Eq. (8), where the first column corresponds to the key unknownu, the last row corresponds to the ignored constraint, and anXis used in place of a non-zero entry.(8)If the first column corresponding touand the last row corresponding to the gradient of the ignored equation were removed, i.e., if the value ofuwas known so that the last equation becomes redundant and thus may be discarded, then the remaining system would be reducible. The order of unknowns and equations in the above system was deliberately chosen to emphasize and make visible this reduction in three blocks of two equations for each one, with a block lower triangular structure.This development leads to the main question addressed in this work: “is it possible to exploit re-parameterization, i.e., is it possible to reduce the big system in some way, even when the value of the key unknownuis indeed unknown?” At first glance, it seems impossible to use decomposition because the big system is irreducible.Remark 1Actually,uand its defining equation are useless.xAcan be used as the key unknown and the distancelC′Cas the ignored constraint. Removing the first row (equation definingu) and the first column (derivatives w.r.t.u) results in the Jacobian with a visible block lower triangular structure in the upmost rightmost sub-square of the table depicted in Eq. (9), where the last row corresponds to the ignored constraint.By symmetry, any of the unknownsxA,yA,xB,yB,xC, andyCcan be used as the key unknown. However, rows and columns must be permuted for the block lower structure of the Jacobian to be visible.The main idea of re-parameterization is illustrated through the 3D example of the hexahedron problem (a generalization of the cube) in Fig. 2. The lengths of the twelve edges of the hexahedron are given as input. The system of constraints is completed with six coplanarity constraints (one for each face of the hexahedron), and a non-degeneracy constraint ensuring that the hexahedron is not flat. The last constraint is an inequality that eliminates a continuum of degenerate solutions of topological dimension 1, i.e., a curve of flat hexahedra  [14]. To simplify, we will ignore this constraint in the sequel.This system has a finite number of solutions, modulo rigid body motions, i.e., up to isometries. It has eighteen equations. The eight vertices are represented by 3×8 coordinates, six of which can be arbitrarily fixed as done for the 2D example in the previous section. For instance, vertex 0 is fixed at the origin, vertex 1 on the positivexaxis, and vertex 2 on theOxyplane with positiveycoordinate. Thusx0=y0=z0=y1=z1=z2=0. This placement rule yields a well-constrained system of eighteen (24-6) equations in eighteen unknowns. In addition, if we consider thatz3=0, which comes from the coplanarity of vertices 0, 1, 2, and 3, and constraintx1=l01(wherel01is the specified length for the edge 01 andx1denotes the abscissa of vertex 1), then we can satisfy and thus discard two additional constraints. At the end, we are left with a system of sixteen equations in sixteen unknowns, which is irreducible according to the methods proposed in  [9].The idea of re-parameterization when illustrated for the hexahedron problem is the following: if the lengths of the edges of the triangle numbered 124 are known, then it would be easy to compute the coordinates of triangle 012. Vertex 4 is the intersection of three spheres of known centers and radii. Then, it would be possible to compute the coordinates of vertex 3 as a function of the coordinates of vertices 0, 1, and 2. Similarly it would be possible to compute the coordinates of vertex 5 as a function of the coordinates of vertices 0, 1, and 4. It would also be possible to compute the coordinates of vertex 6 as a function of the coordinates of vertices 0, 2, and 4. Finally, the equations of the three planes 135, 236, and 456 can be computed and their intersection is vertex 7.The distance constraints for edges 37, 57, and 67 have not been used as they are redundant. In this example, the key unknowns do not represent unknowns of the initial system, as they represent the lengths of the edges of triangle 124. The distance constraints for edges 37, 57, and 67 are called the ignored constraints. The Jacobian of the hexahedron has the structure depicted in Table 1.In some sense, re-parameterization transforms the initial systemS(X)=0into a systemX=F(U),G(X)=G(F(U))=0, whereUare the key unknowns,X=F(U)stands for the non-ignored constraints that are satisfied by construction, andG(F(U))=0represents the ignored constraints. The unknowns of the small re-parameterized system are the parametersUand its equations areG(F(U))=0, unknownsXdo not appear in this formulation. A difficulty arises unfortunately,Fis a multi-function, e.g., it involves±, and it may be undefined for some values ofU. It may be difficult to find an explicit functionFso thatX=F(U). In this case, one may prefer the implicit formulationF(U,X)=0,G(X)=0, orF(U,X)=0,G(U,X)=0, which is more general. It is the latter which is used in this paper, for the sake of generality.When there is only one parameterU, computing the correct values for the parameterUis simple, because it reduces to following a parametric curveX=F(U)or the implicit curveF(U,X)=0, and to detect when the ignored constraint is satisfied (e.g., when the sign ofG(U)changes). Some iterations of the Newton method or the dichotomy allow to tune the solution values ofU.WhenUcontains more than one parameter, the methods proposed in the literature to solve the small re-parameterized systems forUare much more difficult and more involved. In their pioneering work, Gao et al.  [8] handle the case of one and two key unknowns and show that hundreds of irreducible 3D problems may be solved by using this re-parameterization. Our work presents a simple method, which applies for any number of key unknowns. The only restriction is that the numberkof these key unknowns should remain small compared to the numbernrepresenting the size of the system to solve.The main motivation behind involving matching theory in our discussion comes from the fact that the decomposition of systems of equations mainly relies on this theory. In this paper, we briefly discuss this theory, while more details can be found in Lovász and Plummer’s book  [15]. The reader is referred to this book for more information about matroids, König–Hall’s marriage theorem, etc.Each iteration of the Newton–Raphson method either performs a Jacobian matrix inversion, solves some linear systems, computes an LU decomposition, or performs similar linear algebra computations. Other solvers rely on similar linear algebra routines, like homotopy, also called continuation.The methods proposed in  [9] reduce well-constrained systems of (linear, or non-linear) equations to irreducible well-constrained subsystems. These methods are purely combinatorial, they study the bipartite graph of the equations and unknowns of the system. Each equation is represented by a graph vertex belonging to a first set of vertices, while each unknown is represented by a graph vertex belonging to a second set of vertices. An edge links an equation-vertex to an unknown-vertex if and only if the represented equation depends on that unknown.The aforementioned decomposition methods apply to both linear and non-linear systems. They are based on maximum matchings. A matching is a subset of edges, so that each vertex in the graph is the vertex of at most one edge in the matching. A vertex which belongs to an edge in a matching is said to be saturated, or covered, by this matching. A matching is maximum if it is maximum in cardinality. A matching is maximal if it is maximum for inclusion, i.e., it is not included in a larger matching. Maximal matchings are not always maximum, but every maximum matching is of course maximal. A matching is perfect when all vertices are saturated. A perfect matching is both maximal and maximum.The bipartite graph captures structural properties of the system. A system of non-linear or linear equations and its Jacobian matrix share the same bipartite graph. These properties depend only on that graph, regardless of the values of the coefficients of the system. For instance, the matrix has full rank (assuming generic values for the non-zero entries) if and only if the bipartite graph has (at least) one perfect matching. More generally, the rank of the matrix, i.e., the number of independent equations of the system, cannot be greater than the cardinality of the maximum matching in the bipartite graph associated to this matrix. For some degenerate cases, the matrix rank is smaller than the cardinality of the maximum matching. This one-to-one correspondence between the terms of the determinant and the perfect matchings of the bipartite graph is illustrated in Fig. 3for the matrix:(10)M=(ab0cd00ef),which has the determinantadf−bcf, while the perfect matchings areadfandbcf. The graph edges are labeled with the corresponding matrix coefficient. Note that in this example, the determinant is independent of the value ofeand that the corresponding edge does not belong to any perfect matching.This correspondence between a perfect matching and a determinant term holds for any square matrix. Indeed, letMbe a square matrix and letGbe the corresponding bipartite graph, i.e.,Gis the bipartite graph corresponding to the linear systemMx=b, wherebis a given generic vector (non-null). An edge links the vertex representing the equation of the linelto a vertex representing an unknowncif and only ifMl,cis non-null. Therefore, the determinant ofMis∑σparity(σ)∏l=1nMl,σ(l), whereσruns through all permutations1,…,nandparity(σ)is either+1or−1depending on whether the permutationσis even or odd, respectively. We remind that a permutationσis even (resp. odd) if and only ifσis the composition of an even (resp. odd) number of swaps of two distinct elements. Any non-null term∏l=1nMl,σ(l)contains exclusively non-nullMl,σ(l), so the permutationσgives a perfect matching inG: this matching links the vertex of an equationlto a vertex of an unknownσ(l).This correspondence between a perfect matching and a determinant term explains why the determinant is null when there is no perfect matching as for the structurally not well-constrained system of Fig. 4. The qualifier structurally means that the system is not well-constrained regardless of the values of its coefficientsa,b,c,d, ande.The methods of  [9] strongly polynomially (cf.  [16] for the difference between strongly and weakly polynomial time) compute a maximum (perfect for well-constrained systems) matching of the bipartite graph corresponding to a system of equations. Then, they orient the edges of the bipartite graph according to whether they belong or not to the maximum matching. This process is illustrated in Fig. 5for the example depicted in Fig. 3. For a well-constrained system, the only case discussed in this work, each strongly connected component of the oriented graph represents an irreducible well-constrained subsystem. The strongly connected components are independent of the used maximum matching. Moreover, the edges of the graph that link two distinct strongly connected components, i.e., the edges of the reduced acyclic graph, reflect the dependencies between the different subsystems, i.e., they provide the order for solving the different subsystems.Some data structures are not only very convenient, but almost necessary in order to fully and easily benefit from the decomposition during the solving process, even for linear systems. These structures are the equation-unknown bipartite graphs and the Directed Acyclic Graphs (DAG for short) of subsystems, whose edges indicate the existing dependencies between the subsystems. These DAGs are the reduced graphs illustrated in Fig. 5. These data structures are even more indispensable in the case on non-linear systems, where we have to handle the multiplicity of the solutions of the non-linear subsystems, or on the other hand the absence of their solutions.The reduction or decomposition methods proposed in  [9] reorder unknowns and equations in polynomial time, so that the Jacobian matrix becomes block lower triangular, and the decomposition into well-constrained subsystems becomes more visible. As a result, the processes of solving a linear system involving the Jacobian or inverting the Jacobian matrix become more efficient, thanks to the use of forward block substitution. Remember that solving a linear system ofnequations andnunknowns has a computational complexity ofO(n3)in the general case by using Gauss pivoting, while solving a lower triangular system has a smaller complexity ofO(n2)[17].Let us assume that matrixMis block lower triangular as follows:(11)M=(M1,100M2,1M2,20M3,1M3,2M3,3).We will show how to exploit the decomposition ofMin order to solve a systemMX=Bor to invert matrixM. The proofs of what follows are straightforward and may be found in  [17].Remark that the matrices and the vectors mentioned in this section may either involve floating point arithmetic when manipulated by the Newton–Raphson or homotopy methods (Section  5), interval arithmetic when manipulated by interval solvers (Section  7), or rational/modular arithmetic when handled by somep-adic methods (Section  6).The matrixKinverse of the square non-singular block lower triangular matrixMis also a block lower triangular matrix. It has the same block structure as matrixM(12)K=M−1=(K1,100K2,1K2,20K3,1K3,2K3,3),where its blocksKl,care defined as:(13)Kl,c=0whenc>l(14)Kl,l=Ml,l−1(15)Kl,c=−Kl,l∑i=cl−1Ml,iKi,cwhenl>c.In fact, it is always possible to avoid the inversion of matrixMas it suffices to just solve less thannlinear systems (nis the number of rows or columns ofM) as discussed in the following.To solve a linear systemMX=B, whereX=(X1,X2,…)tandB=(B1,B2,…)thave a structure compatible with the block structure of matrixM, we have to successively solve the subsystemsX1≔solve(M1,1X1=B1), thenX2≔solve(M2,2X2=B2−M2,1X1), thenX3≔solve(M3,3X3=B3−(M3,1X1+M3,2X2)), etc. In other words, we have to solve the series of the followinglsystems in ascending order ofl:(16)Xl≔solve(Ml,lXl=Bl−∑i=1l−1Ml,iXi).The smaller are the blocksMl,land therefore numerous, the greater is the number of null blocks under the diagonal, and the more important is the speed-up in the solving process. In order to evaluate this speed-up, let us study the complexity for a simplified case, whereMis of sizenbynand all diagonal blocks are of equal sizetbyt. This implies that there existsb=n/tdiagonal blocks. If we denote byβthe number of non-null matrix blocks below the diagonal, i.e.,0≤β≤b(b−1)/2∈O(b2)=O(n2/t2), then solvingMX=brequiresβproducts of a matrix of sizetbytby a column vector of sizet, which costsO(βt2), and the same number of additions of column vectors of sizet, which costsO(βt), and finallybtimes solving linear systemsMl,lXl=Bl−∑i=1l−1Ml,iXi,l=1,…,b, which costsO(bt3), at a cost ofO(t3)per inversion (Strassen inversions are not pertinent for small matrices of sizetbytwhentis small). Therefore, the overall complexity isO(βt2+bt3)=O((β+bt)t2). But sincebt=n, the complexity becomesO((β+n)t2).Now, let us assume thattis small as well, e.g.,t≤10, so that it can be considered as constantt∈O(1). The overall complexity of solvingMX=BisO(β+n). We remind that0≤β≤b(b−1)/2=O((n/t)2)=O(n2), so even if there is no null block below the diagonal, the complexity drops fromn3ton2. Ifβis of the order ofn, then the complexity drops fromn3ton. This may happen for geometric constraint systems because each constraint like the distance between two points depends on a constant number of variables. Therefore, the speed-up induced by the decomposition may be considerable.In the following, we will putα=β+n, whereαis the number of non-null block matrices. We will say that the complexity of the solving process isO(α)and this holds because we assume thattis constant.The optimal cost for solving a sparse linear system and the optional solving strategy (e.g., how to choose Gauss pivots that allow the most efficient system solving?) are discussed in more detail in Bomhoff’s Ph.D. thesis  [18]. The exposed problems and the used combinatorial methods consider only the bipartite graph of the linear system.This section shows that re-parameterization speeds-up linear algebra computations involved in the Newton–Raphson method or its variants, like for example the damped Newton method or the homotopy. In the sequel, we only consider the Newton–Raphson method.We assume that the system of equations is well-constrained, the set of key unknowns is known, and the structure of the Jacobian matrix of the system has already been computed by the methods proposed in  [9] and remains unchanged along the iterations of the Newton–Raphson method. Each Newton–Raphson iteration solves a linear system whose matrix has the structure (already seen in Eq. (8) or in the example of Fig. 1) depicted in Fig. 6.Translating the Jacobian structure into formulas yields:(17)HU+AX=R,CU+LX=SwhereAis a square non-singular block lower triangular matrix,Uis the vector of parameters or key unknowns, andXis the vector of the other unknowns. Eq.HU+AX=Rfollows from the derivative of the non-ignored constraints, while Eq.CU+LX=Sfollows from the derivative of the ignored constraints.Let us now define the concept of anR-structure. Consider the following matrix extracted from the above equations:(18)(HACL).This matrix is said to have anR-structure (Rfor re-parameterization). Two matrices are said to have the sameR-structure if and only if the sizes of their four blocksH,A,C, andLare equal, and the block structures of theirAparts are equal.From Eq. (17), we deduce:(19)(C−LA−1H)U=S−LA−1Rwhich is proved as:(Eq. (17))⇒CU+LX=S,X=A−1(R−HU)⇒CU+LA−1(R−HU)=S⇒(C−LA−1H)U=S−LA−1R.We will solve the last linear equation forU, then deduceX, avoiding at the same time the inversion ofA. We assume thatAis of sizen×n. The re-parameterization makesAhighly reducible and this is the goal of re-parameterization. This decomposition ofAallows to speed-up the solving process for linear systemsAx=Bas shown in the previous section. SolvingAx=B(for a givenB) costsO(α), i.e., the number of non-null blocks of matrixA. Eqs. (17) and (19) are solved forUandXas follows:1.Zn×1≔solve(An×nZn×1=Rn×1)costsO(α). This yieldsZ=A−1Rwhich appears in Eq. (19), avoiding at the same time the inversion of matrixA.Kn×k≔solve(An×nKn×k=Hn×k). This requires solvingklinear systems, one for each columnc=1,…,kofK, and the corresponding column ofH. This costsO(kα). This yieldsK=A−1Hwhich appears in Eq. (19), avoiding at the same time the inversion of matrixA.ComputeC−LA−1H=Ck×k−Lk×nKn×kcostsO(k2n). This term is involved in step 5.ComputeS−LA−1R=Sk×1−Lk×nZn×1costsO(kn). This term is involved in the right side of Eq. (19) and in step 5.Uk×1≔solve((C−LA−1H)k×kUk×1=(S−LA−1R)k×1), where onlyUis unknown, costsO(k3). This is Eq. (19). It is useless to optimize this step becausekis small. This givesU.ComputeRn×1−Hn×kUk×1costsO(nk). This term is involved in step 7.Xn×1≔solve(An×nXn×1=(R−HU)n×1), where onlyXis unknown, costsO(α), avoiding at the same time the inversion of matrixA. This is the first equation of the system (17). This yieldsX.Now,U​andXare known. Consequently, the overall cost of solving a re-parameterized linear system isO(k(α+kn+k2)). Ask≪nand becausen≤α≤n(n−1)/2∈O(n2), this cost is therefore in-betweenk2(n+k)in the best case andO(kn2+kn+k3)=O(kn2)in the worst case. This cost is always less thann3which is the cost of solving a linear system by classical methods (Gauss pivoting or LU decomposition). It is even less than the cost of the inversion by the Strassen method which isO(nlog27)≈O(n2.8…)and also less than the cost of the inversion by the Coppersmith–Winograd method which isO(n2.375…)(cf.  [17] for more details, where it is shown that the inversion of a matrix of sizen×nhas asymptotically the same cost as the product of two matrices of sizen×n).Another remark deserves to be mentioned. We have assumed for simplicity reasons thatk≪nand we do not discuss the problem of determining the maximum value ofkin order for the speed-up of the re-parameterization to remain interesting or significant. The aforementioned complexity study suggests a bound fork: ifk=O(n0.375…), then our method has the same complexity as that of Coppersmith–Winograd method which equalsO(n2.375…). This bound forkis interesting because it is probably easier to find sets of key unknowns of sizeO(n0.375…)rather than sets of sizeO(1)orO(logn).p-adic methods are used inp-adic analysis, but also to solve diophantine problems  [19–23] or in computer algebra computations, e.g., greatest common divisor of polynomials, polynomial factorization  [24,25], or Gröbner bases computation  [26].Re-parameterization can also benefit Hensel lifting used inp-adic methods. Let us assume thatX0is a root of some re-parameterized algebraic systemF(X)=0modulop(a prime number or a power of a prime number). Our goal is to computeX1such thatX0+pX1is a root ofF(X)=0modulop2. In the Taylor series expansionF(X0+pX1)=F(X0)+pF′(X0)X1+⋯, we can clearly discard powerspk,k>1(the dots at the end of the expansion) because they vanish modulop2. Then,F(X0)=0modpimplies thatF(X0)(taken modulop2) is a multiple ofp. We compute the vectorλ=F(X0)/p. ThenF(X0+pX1)=0⇒F(X0)+pF′(X0)X1=0[modp2]⇒λ+F′(X0)X1=0[modp], which is a linear system, solvable modulop. In fact, Hensel lifting is nothing else than the Newton–Raphson method but for thep-adics  [26].The proposed method can also be applied to compute thep-adic expansion of the root, i.e., the roots modulop,p2,p3,…(orp2,p4,p8,…), starting from the rootX0modulop.To summarize, ifF(x)=0is a re-parameterized system, then each Hensel lifting may benefit from it. Note however that re-parameterization does not help to find the initial rootX0modulop.This section shows that interval solvers (ALIAS-C++   [27], RealPaver   [28], IBEX   [29], and QUIMPER   [30]) can benefit from re-parameterization. Interval solvers use methods of interval analysis  [31–34] that compute all the real roots of a systemf(x)=0within a given initial box, wherefdenotes a large well-constrained re-parameterized system.Typically, interval solvers handle a stack of boxes and work as follows: the initial box is pushed into this stack of boxes to be processed. While this stack is non empty, the top boxBis popped and processed. This box processing tries to reduce the box without missing any root. One of the following cases may happen for a boxB:•BoxBis proven to contain no roots (e.g.,Bis already an empty box).BoxBis proven to contain only one regular root (the Jacobian is non-null), e.g., by some tests using Kantorovich theorem  [35]. In this case, this unique root is tuned by some iterations of the standard (non-interval version) Newton–Raphson method and inserted into a list of roots.BoxBis too small to be subdivided again and it is impossible to prove that it contains no roots. BoxBmay contain multiple roots, close roots, or no root at all, but the used arithmetic precision is insufficient. In this case, boxBis inserted into a list of residual boxes.BoxBhas not been significantly reduced, so it may contain several real roots. In this case,Bis bisected along its largest interval (other bisection choices have also been studied  [27,31,32]) and the resulting boxes are pushed into the stack of non-processed boxes.Several methods have been proposed in the interval analysis community for processing a boxB, i.e., finding the roots off(x)=0withinB, or reducingB(maybe to the empty box) without missing any of its roots. One may look for the Krawczyk–Moore or the Sengupta–Hansen operators  [32], just to cite few ones. The simplest approach just solves with interval computations the linear system. If we denote byx0is the center of boxB, then:(20)f(x∈B)=f(x0+(x−x0))∈f(x0)+f′(B)(x−x0).We are looking forx∈Bsolution off(x0)+f′(B)(x−x0)=0. Fig. 7left provides a geometric interpretation of this equation for a 1D problem. Let us putΔx=x−x0.Δxis the solution of the interval-wise linear systemf′(B)Δx=−f(x0).f′(B)is first computed by intervals, by exploiting the sparsity of the Jacobianf′. Then, we computeΔx≔solve(f′(B)Δx=−f(x0))by exploiting theR-structure of the Jacobian, through the use of the methods described in Section  5.A main difficulty arises when any of the sub-matrices of the diagonal of the Jacobianf′(B)is a singular matrix. This is the case of the 1D example of Fig. 7.A solution to the aforementioned problem can be found by using the centered evaluation form. Let us considerJ=f′(B)and denote byJ0the center ofJ, which is a non-singular matrix with probability one. It is also possible to useJ0=f′(x0). Let us putΔJ=J−J0. It follows that:f(x0)+JΔx=0⇒f(x0)+(J0+ΔJ)Δx=0⇒f(x0)+J0Δx+ΔJΔx=0⇒f(x0)+J0Δx+ΔJ(x−x0)=0,x∈B⇒f(x0)+J0Δx+ΔJ(B−x0)=0(21)⇒J0Δx=−f(x0)−ΔJ(B−x0).This last equation represents a linear system having an unknownΔx. The elements of matrixJ0are floating point numbers, not intervals. With probability one,J0is non-singular. In addition,J0has theR-structure off, so solving the linear system can benefit from re-parameterization. Only the vector at the right side of the equation−(f(x0)+ΔJ(B−x0))has interval elements. Geometrically, the hypersurface of each equation is bounded by a thick hyperplane of constant thickness, it is the convex hull of the cones of the previous linearization by intervals.A simple 1D example is depicted in Fig. 7, whereB=[−2,2],x0=0,f(x0)=−1, andf′(B)=[−1/3,2/3]. The first linearization by intervals isf(x0+Δx)∈f(x0)+f′(B)Δx, which gives−1+[−1/3,2/3]Δx=0. Geometrically, the arc of the curve is bounded by (or enclosed in) the two gray-shaded triangles, cf. left side of Fig. 7.f′(B)is singular. The middle side of Fig. 7 shows the second linearization by intervals: the centered form. The arc of the curve is covered by a thick straight line, i.e., by the thick band of equationJ0Δx=−f(x0)−ΔJ(B−x0). Numerically,1/6x−[0,2]=0. The right side of Fig. 7 superimposes the left and middle side coverings or bounds of the same figure. We observe that the intersection of the straight lineOxwith the thick line is greater than its intersection with the two gray-shaded triangles. Fig. 8illustrates a 2D example.OnceΔxis computed, boxBis updated asB≔B∩(x0+Δx). If this intersection is empty, thenB​does not contain any root. If the box is not reduced, then it is split into two boxes, e.g., using the largest side. Bisection is unavoidable because it is the only way to separate the roots.The Gauss–Seidel idea can be used to optimize the computation ofB∩(x0+Δx): eachith coordinateBiofBmay be reduced withBi≔Bi∩(x0+Δx)i, as soon as it is available. The following computations use the latest and more precise value ofBi. IfBiis empty, then the processed boxBcontains no root. This idea is used in the Hansen–Sengupta operator.Another classical optimization used in the Hansen–Sengupta operator is preconditioning, which limits the wrapping effect  [36] of interval computations. The preconditioned systemg(x)=f′(x0)−1f(x)=0is solved instead of the systemf(x)=0. By construction,g′(x)=f′(x0)−1f′(x), sog′(x0)is the identity matrix. Iffandgwere linear, then thehth hyperplane of equationgh′(x)=0would be perpendicular to the axisxh. The smallest is the box, the closer is the hypersurfacegh′(x)=0to a hyperplane. Is it possible to exploit both re-parameterization and preconditioning at the same time? The difficulty comes from the fact thatg′​does not have the sameR-structure asf′. Indeed, the matrix product of two matrices having the sameR-structure does not have the sameR-structure in general.To summarize, interval solvers may in principle benefit from re-parameterization, but further work has to be done to address the wrapping effect issue for re-parameterized systems.

@&#CONCLUSIONS@&#
