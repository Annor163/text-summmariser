@&#MAIN-TITLE@&#
Classification of microarray using MapReduce based proximal support vector machine classifier

@&#HIGHLIGHTS@&#
Feature selection and classification of microarray data using Hadoop framework.MapReduce based statistical tests are proposed for feature selection (FS).MapReduce based PSVM (mrPSVM) classifier is proposed for classification of microarray.Comparative analysis of results is performed with FS methods in permutation with mrPSVM.

@&#KEYPHRASES@&#
Big data,Classification,Gene selection,Hadoop,Proximal support vector machine,MapReduce,Microarray,Statistical test,

@&#ABSTRACT@&#
Microarray-based gene expression profiling has emerged as an efficient technique for classification, diagnosis, prognosis, and treatment of cancer disease. The nature of this disease changes frequently, this generates a huge volume of data. The data retrieved from microarray covers its varieties (veracity) of nature, and changes observed as time changes (velocity). Therefore, the analysis of microarray dataset in a very short period is essential. The major drawback of microarray data is the ‘curse of dimensionality problem’, this hinders the useful information of dataset and leads to computational instability. Therefore, selecting relevant genes is an imperative in microarray data analysis. Most of the existing schemes employ a two phase process: feature selection/extraction followed by classification. In this paper, various statistical methods (tests) based on MapReduce are proposed to select the relevant features. After feature selection, MapReduce based proximal support vector machine (mrPSVM) classifier is also proposed to classify the microarray data. These algorithms are successfully implemented on Hadoop framework. A comparative analysis is done on these feature selection methodologies using microarray datasets of various dimensions. Experimental results show that the ensemble of mrPSVM classifier and various feature selection methods produces a better accuracy rate on the benchmark dataset.

@&#INTRODUCTION@&#
Microarray based gene expression profiling has emerged as an efficient technique for cancer diagnosis, prognosis, and treatment purposes [1]. In recent years, DNA microarray technique has a great impact on determining the informative genes that cause cancer [2,3]. The major drawback that exists in microarray data analysis is the curse of dimensionality problem; this hinders the useful information of dataset and leads to computational instability [4]. Therefore, the selection/extraction of relevant features (genes) remains as an imperative in the analysis of microarray data of cancer, which is an important step towards effective classification. A good feature subset is one that contains features highly correlated with (predictive of) the class, yet uncorrelated with (not predictive of) each other.A good number of feature (gene) selection/extraction techniques have been proposed by various researchers and practitioners in the past. These techniques are based on methods of projection or compression, where the original dataset gets altered. Meanwhile, recent developments in microarray chip technology help in studying thousands/millions of genes simultaneously, generating a huge amount of data. Processing it, is a difficult task using a conventional system having standard computational power. Various machine learning methodologies have been proposed in the area of bioinformatics (typically, microarray data) by different researchers [5–9]. But these consume a lot of time to analyze and explore large datasets. To counter this, the concept of distributed computing has been adopted, where the data are distributed on various nodes in a cluster and processed using various parallel processing paradigm [10]. The MapReduce programming model and its implementation on Hadoop framework has a substantial base for processing large datasets, in particular for high dimensional genomic data such as microarray data, in a distributed manner.Hadoop framework was developed by Doug Cutting in 2008 [11]. Apache Hadoop is an open source software, and provides an effective way of storing and processing big data in a distributed fashion on large clusters of commodity hardware. It employs a master/slave architecture for both distributed storage and distributed computation, thus, accomplishing two tasks, i.e., massive data storage and faster processing [12].In this era, big data applications are increasingly becoming the main focus of attention because of the enormous increment of data generation and storage that has taken place in the last years. This situation becomes a challenge when huge amounts of data are processed to extract knowledge because the data mining techniques are not adapted to the new space and time requirements. To overcome these challenges, the new paradigms have been considered to develop scalable algorithms. The researchers have accomplished a significant relevance of intelligent techniques to the development of Data Science for dealing with imprecision, uncertainty, learning, and evolution, in posing and solving computational problems [13].A parallel hierarchical attribute reduction method can be applied on big data to analyze the intended data more efficiently [14]. This model is able to mine decision rules under different levels of granularity. The proposed algorithms are implemented on Hadoop framework using MapReduce paradigm that can parallelize the data and task and efficiently deal with big data. Li et al. [15] have developed the Dominance-based Rough sets Approach which is an extension of classical rough sets theory, for selecting relevant features efficiently. It processes information with preference-ordered attribute domain and then can be used for multi-criteria decision analysis. Ayadi et al. [16] have used the concept of biclusters using DNA gene expression for microarray data. Initially, they have considered a new tree structure, called Modified Bicluster Enumeration Tree (MBET), on which biclusters are represented by the profile shapes of genes. In the next phase, they have proposed an algorithm called BiMine+ which uses a pruning rule to avoid both trivial biclusters and the combinatorial explosion of the search tree. The performance of BiMine+ is assessed on both synthetic and real DNA microarray datasets. An algorithm called MROSEFW-RF, based on MapReduce parallelization strategy, proposed by Triguero et al. [17]. This algorithm ensembles several highly scalable, re-processing and mining methods, that performs the balancing of class distribution, detects the cost relevant features and builds an appropriate Random Forest model. Islam et al. [18] have proposed a MapReduce based parallel gene selection method, that utilizes sampling techniques to reduce irrelevant genes by using Between-groups to Within-groups sum of square (BW) ratio. The BW ratio indicates the variances among gene expression values. After gene selection, it applies MRkNN technique to execute multiple kNN in parallel using MapReduce programming model. Finally, the effectiveness of the method is verified through extensive experiments using several real and synthetic datasets. Wang et al. [19] have proposed a new method for calculating correlation and introduced an efficient algorithm based on MapReduce to optimize storage and correlation calculation. This algorithm is used as a basis for optimizing high throughput molecular data (microarray data) correlation calculation. He et al. [20] have proposed the parallel implementation of several classification algorithms like k-nearest neighbor, Naive Bayesian model, and decision tree which are executed concurrently on various clusters using iris dataset.The parametric and non-parametric statistical tests are elegant procedures to analyze the behavior of data [21]. The statistical tests are applied as a feature selection method by assuming the hypotheses, i.e., Null hypothesis and alternate hypothesis. Based on the correctness of the hypothesis, the features are either selected or rejected.As the size of the data samples increases, the training time increases and also the computational complexity increases in case of support vector machine (SVM) [22]. In order to overcome the drawbacks of SVM, proximal support vector machine (PSVM) was developed. It is based on Least Square support vector machine (LS-SVM) [23,24]. The training time required by PSVM is less as compared to large training time in case of standard SVM. PSVM assigns the classes to the data points by measuring its proximity from the two parallel hyperplanes and the data points are clustered around the two parallel hyperplanes; whereas SVM divides the space into two half spaces such that datapoints of different classes are separated. The hyperplanes are designed such that the margin of separation between the two classes is maximized. The idea behind proposed work is to maximize the margin between the hyperplanes or the decision surface such that data points lie on the correct side of the hyperplanes in order to increase the generalization ability of the classifier or to minimize the generalization error.In this paper, MapReduce based statistical tests like ANOVA test, Kruskal–Wallis test, and Friedman test have been proposed to select the relevant features from microarray dataset. Along with these feature selection techniques, MapReduce based proximal support vector machine (mrPSVM) classifier has been also proposed to classify the microarray dataset. These proposed algorithms are not only processing large datasets, but also can be extended to execute on a cluster. The performance of the algorithms are tested on Hadoop cluster with four slave (data) nodes and a conventional system.The rest of the paper is organized as follows: Section 2 presents the proposed work for selecting features and classifying the microarray data using statistical tests and proximal support vector machine based on the MapReduce programming paradigm. Section 3 highlights the basic concepts of Hadoop and its components. Section 4 presents the implementation details for the proposed approach. Section 5 discusses on the results obtained, interpretation drawn from it and also presents the comparative analysis for classification of various microarray datasets. Section 6 concludes the paper with scope for future work.The presence of a huge number of insignificant and irrelevant features degrades the quality of analysis of diseases like ‘cancer’. To counter this, it is essential to analyze the dataset from proper perspective. This section presents an approach for classification of microarray data, which consists of three phases:i.The input data is preprocessed using methods such as missing data imputation and normalization.Feature selection is done using statistical tests based on MapReduce programming model.After selecting the relevant features, MapReduce based proximal support vector machine (mrPSVM) has been applied to classify microarray dataset into their respective classes, i.e., (cancerous/non-cancerous).Fig. 1shows the graphical representation of proposed approach. A brief description is, as follows:a.Data collectionThe dataset for classification analysis, which acts as requisite input to the models is obtained from Kent Ridge Bio-medical Data Set Repository [1] and National center of Biotechnology Information (NCBI GEO, http://www.ncbi.nlm.nih.gov/gds/).Missing data imputation and normalization of datasetMissing data of a feature (gene) in microarray dataset are imputed by using the mean value of the respective feature. Input feature values are normalized over the range[0,1]using Min–Max normalization technique [25,26].Feature selectionMapReduce based statistical tests viz., ANOVA-test, Kruskal–Wallis test, and Friedman test have been applied to select the features having high relevance value and thus the curse of dimensionality issue has been addressed.Division of datasetThe dataset is divided into two categories: training set and testing set as discussed in Section 5.Building classifierMapReduce based proximal support vector machine (mrPSVM) classifier has been built to classify the microarray dataset.Testing the classifierClassifier is tested using the test dataset and the performance of the classifier is evaluated.Distributed computing techniques such as grid computing and cluster computing are based on the message-passing model, while systems that run on parallel algorithms, such as the graphics processing unit (GPUs), are based on shared-memory model. The efficient working of these machines, calls for an interface enabling them to access shared file systems as well as maintain proper communication with other machines in the grid/cluster. As a distributed framework, Hadoop enables many applications that stores the massive amount of data in a distributed manner and benefit from parallelization of data processing.Hadoop is an open-source software framework that provides a abstraction for storing and processing of large data sets in a distributed fashion across clusters of commodity hardware [11]. It has been designed to store data across all the nodes (servers) in a cluster, in a distributed manner by dividing the file into smaller entities called blocks. This enables each node to work according to the principle of proximity, i.e., to process the data available locally, leading to less network transmissions. In addition to this, it provides a low-cost and scalable architecture designed to scale up from a single node to a thousand servers. Its design allows for detecting and handling failures at the application layer; thus, providing service even in error prone systems. There are three core components of the Hadoop framework, included in the package, managed and maintained by Apache Software Foundation. They are:Hadoop Distributed File Systems (HDFS): It is a Java-based distributed file system that can store all kinds of data without prior organization, unlike in relational databases. Though it is similar to the existing distributed systems, there are significant differences. It can be deployed on low-cost commodity hardware and its architecture allows for speedy recovery from failure. It consists of a Namenode and several Datanodes. The Namenode keeps the directory tree of all files in the file system, and tracks where, across the cluster, the file is stored. It does not store these files itself. In addition to this, the files stored in a Datanode are replicated on other Datanodes, thereby, allowing for speedy recovery in case of failure of a node [27]. Fig. 2shows the architecture of HDFS.Yet Another Resource Negotiator (YARN): It is a resource management framework for scheduling and handling resource requests from distributed applications. YARN combines a central resource manager that manages the way applications use Hadoop system resources with node-manager agents that monitor the processing operations in individual cluster nodes. In the case of failure of a node, it reschedules the job to some other node [28].MapReduce: It is a programming model, developed by google, for processing large datasets in a distributed manner on clusters of commodity hardware [29]. It consists of three steps:•Map step: Each data node applies the “map()” function to the local data, and writes the output to a temporary storage with a key value associated with the output.Shuffle step: Here the nodes redistribute data based on the output keys (produced by the map function), such that all data belonging to one key is located on the same node.Reduce step: Each node now processes each group of output data, per key, in parallel and the resulting output is then written to the HDFS.Fig. 3shows the architecture of MapReduce and its life cycle.The execution of a job on Hadoop framework is carried out in two phases viz. map and reduce. In the map phase, a mapper reads a file (stored as blocks in the node) and calculates the required output (as discussed in Section 4). These values, along with their corresponding keys (Feature Id. in case of feature set), are written to an intermediary file; where they are sorted, shuffled and are sent to the reducers. The reducers process the input and write the obtained results into the HDFS. The working of MapReduce is explained in Fig. 3.For instance, if the input file is of size 550MB and the HDFS has a block size of 128MB, when the dataset (input file) is uploaded to HDFS, it gets divided into 5 (⌈550/128⌉) blocks. The first four blocks will be of size 128MB each, and last block will have a size of 38MB. These blocks are distributed on all the Datanodes with replication factor three. Further assuming that, split size is equal to block size, corresponding to five blocks, five mappers are formed. Mappers are executed on all Datanodes simultaneously and write their output to an intermediary file in the HDFS. The reducers perform further operations on the result obtained from the mappers and write the final result into the HDFS.In this section, the time taken for execution of an algorithm on a Hadoop cluster and a conventional system is discussed.The time taken for execution on Hadoop cluster is equivalent to the sum of time taken by each mapper (tm), each reducer (tr), and the time taken for communication between the Datanodes and the Namenode (tc), i.e.,(1)THadoop=tm+tr+tcLet M be the total number of data entries (feature sets) in a file which are distributed among n Datanodes; so the total number of entries per mapper would beM/n. Let the time taken to execute one entry is (tfm). So(2)tm=(M/n)∗tfmIn reducer, let the time taken per entry be (tfr). So total time taken by a reducer would be (assuming r reducers).(3)tr=(M/r)∗tfrThe Datanode communicates with the Namenode only after the execution of the whole batch of entries assigned to it. Assuming that it takestcdtime for communication between the Namenode and Datanode, the total time taken for communication is(4)tc=n∗tcd+r∗tcd=(n+r)∗tcdreplacing (1) by (2)–(4),(5)THadoop=((M/n)∗tfm)+((M/r)∗tfr)+(n+r)∗tcd=(M)∗(tfm/n+tfr/r)+(n+r)∗tcdin a conventional system (conv), all the M entries are stored as a single entity on one system. Hence the time taken for complete execution would be(6)Tconv=tm1+tr1where,(7)tm1=M∗(tfm)(8)tr1=M∗(tfr)Thus,(9)Tconv=tm1+tr1=M∗(tfm+tfr)For the sake of analysis, it is assumed that the time for communication, which depends on network quality, is constant. In the case of small datasets, due to their small size they will be divided into a less number of blocks, as a result, they will be distributed on a less number of systems, i.e., the value of n and r will be small (n,r=1for size less than block size). Eqs. (5) and (9) show that in the case of small datasets, the time taken by a Hadoop cluster is more than the time taken by a conventional system. But in the case of a large dataset, the data divided into a number of blocks, therefore the time taken by a Hadoop cluster will always be less than that of a conventional system. To confirm the above inferences, we have conducted experiment on five case studies with different datasets, as discussed in Section 5.

@&#CONCLUSIONS@&#
