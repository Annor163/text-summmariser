@&#MAIN-TITLE@&#
Multi-label learning with label-specific feature reduction

@&#HIGHLIGHTS@&#
We propose two multi-label learning approaches with LIFT reduction.The idea of fuzzy rough set attribute reduction is adopted in our approaches.Sample selection improves the efficiency in feature dimension reduction.

@&#KEYPHRASES@&#
Feature reduction,Fuzzy rough set,Label-specific feature,Multi-label learning,Sample selection,

@&#ABSTRACT@&#
In multi-label learning, since different labels may have some distinct characteristics of their own, multi-label learning approach with label-specific features named LIFT has been proposed. However, the construction of label-specific features may encounter the increasing of feature dimensionalities and a large amount of redundant information exists in feature space. To alleviate this problem, a multi-label learning approach FRS-LIFT is proposed, which can implement label-specific feature reduction with fuzzy rough set. Furthermore, with the idea of sample selection, another multi-label learning approach FRS-SS-LIFT is also presented, which effectively reduces the computational complexity in label-specific feature reduction. Experimental results on 10 real-world multi-label data sets show that, our methods can not only reduce the dimensionality of label-specific features when compared with LIFT, but also achieve satisfactory performance among some popular multi-label learning approaches.

@&#INTRODUCTION@&#
Nowadays, multi-label learning problem has received an increased attention in real-world applications. For example, in semantic annotation of images [3,16,26,49], a picture can be annotated as camel, desert and landscape. In text categorization [5,11,17,29], a document may belong to several given topics, including economics, finance or GDP. In bioinformatics [6,13,50], each gene may be associated with a set of functional classes, such as metabolism, transcription and protein synthesis. In all cases above, each sample may be associated with more than one label simultaneously and predefined labels for different samples are not mutually exclusive but may overlap. This situation is distinct from the traditional single-label learning where predefined labels are mutually exclusive, each sample only belongs to a single label.Over the last decade, many multi-label learning approaches have been witnessed [12,28,58]. Generally, the existing methods can be grouped into two main categories [43], i.e., algorithm adaptation methods and problem transformation methods. Algorithm adaptation methods extend specific single-label learning algorithms to directly handle multi-label data by modifying some constraint conditions, such as AdaBoost.MH [40], ML-kNN [59], MLNB [60], and RankSVM [9]. Problem transformation methods, transform the multi-label task into one or more corresponding single-label ones and then handle them one by one through traditional methods. The well-known problem transformation methods include binary relevance (BR), label power set (LP) and pruned problem transformation (PPT). BR [3] learns a binary classifier for each label independently and predicts each of the labels separately, so it cuts up the relationship among different labels. LP [44] considers each unique set of labels that exists in a multi-label training set as a new single-label multi-value class. Though this method considers the correlations among different labels, it easily leads to a higher time consumption since the number of new classes is increased exponentially with the increasing of labels. Meanwhile, some new classes created by a few samples may lead to class unbalance problem. PPT [34] abandons the new classes associated with extremely small number of samples or assigns these samples with new labels that can create accepted classes, while some abandoned classes will lead to the loss of multi-label information. Although above methods have achieved good performance in multi-label learning, they make use of the same features to achieve the learning purposes in different labels. Actually, different labels may have distinct characteristics of their own, and these characteristics are more inclined to judge whether labels belong to a specific sample. Fortunately, Zhang [61,62] has proposed the representative LIFT algorithm and validated the effectiveness of constructing label-specific features. For each label, LIFT employs clustering analysis in the positive and negative samples respectively, and then constructs label-specific features by checking the distances between the sample and all the clustering centers. (There is not any semanteme for constructed label-specific features, which can be regarded as a set of distances.) However, construction of label-specific features may encounter the increasing of feature dimensionalities, and a large amount of redundant information exists in feature space. As a result, the structure information between different samples will be disrupted, and even more be destroyed, which leads to the decreasing of the performance of multi-label learning approach. To alleviate this problem, an effective solution is to perform dimension reduction in label-specific features.Rough set theory is a good mathematical tool for describing incomplete and uncertain data. With over 30 years of development, it has been widely applied in attribute reduction [18,30], feature selection [20,22,31,42,55], rule extraction [25,38] and uncertainty reasoning [46]. Numerous researchers [31,32] have used the various rough set models for dealing with single-label data analyses in real-world applications. Recently, some researchers [53,54,56,57] begin to attempt at carrying out multi-label classification via rough set approaches, however, all of them determine different labels in the same feature space, which contradicts the fact that different labels may have distinct characteristics of their own. In this paper, with the idea of attribute reduction based on fuzzy rough set, we will develop a multi-label learning approach with label-specific feature reduction (FRS-LIFT), which uses the approximation quality to evaluate the significance of specific dimension and takes the forward greedy search strategy. Furthermore, sample selection is an effective data compression technique, which can reduce the time and memory consumption in attribute reduction. On the basis of FRS-LIFT, another multi-label learning approach with label-specific feature reduction by sample selection (FRS-SS-LIFT) will be presented at the same time. To validate the effectiveness of FRS-LIFT and FRS-SS-LIFT, we conduct comprehensive experiments on 10 real-world multi-label data sets. Experimental study shows clear advantages of FRS-LIFT and FRS-SS-LIFT over various multi-label learning algorithms.The rest of this paper is organized as following. Section 2 introduces the formal definition of multi-label learning’s framework and LIFT approach. Section 3 provides some background materials on fuzzy rough set and sample selection, and then the details of our FRS-LIFT and FRS-SS-LIFT are presented. Section 4 describes data sets, evaluation metrics, experimental settings, and then analyzes the results of comparative studies on 10 multi-label data sets. Finally, Section 5 summarizes and sets up several issues for future work.LetX=Rdbe the d-dimensional sample space andL={l1,l2,…,lm}be the finite set of m possible labels.T={(xi,Yi)|i=1,2,…,n}denotes the multi-label training set with n labeled samples, where xi∈ X is a d-dimensional feature vector such thatxi=[xi1,xi2,…,xid],Yi⊆L is the set of labels associated with xi.The goal of multi-label learning is to produce a real-valued functionf:X×P(L)→R. In detail, for each xi∈ X, a prefect learning system will tend to output larger values for labels in Yithan those not in Yi[59], i.e., for any l, l′ ∈ L, if l ∈ Yiand l′ ∉ Yi, f(xi, l) > f(xi, l′) holds.LIFT aims to improve the learning performance of multi-label learning system through generating distinguishing features which capture the specific characteristics of each label lk∈ L. To achieve this goal, LIFT takes into account intrinsic connection between different samples in all labels. Specifically, with respect to each label lk, the training samples are divided into two categories, i.e., the set of positive training samples Pkand the set of negative training samples Nk, such that:(1)Pk={xi|(xi,Yi)∈T,lk∈Yi};(2)Nk={xi|(xi,Yi)∈T,lk∉Yi}.In other words, the training sample xibelongs to Pkif xihas label lk; otherwise, xiis included in Nk.To consider intrinsic connection among different samples, LIFT employs clustering analysis on Pkand Nk, respectively. Following Zhang’s research [61,62], k-means algorithm [21] is adopted to partition Pkintomk+disjoint clusters whose clustering centers are denoted by{p1k,p2k,…,pmk+k}. Similarly, Nkis also partitioned intomk−disjoint clusters whose clustering centers are{n1k,n2k,…,nmk−k}. LIFT treats clustering information gained from Pkand Nkas equal importance, and then the numbers of clusters on Pkand Nkare set to be the same, i.e.,mk+=mk−=mk. Specifically, the number of clusters for both positive samples and negative samples is:(3)mk=⌈δ·min(|Pk|,|Nk|)⌉,where | · | represents the cardinality of a set, δ ∈ [0, 1] is the ratio parameter for controlling the number of clusters.The above two groups of clustering centers describe inner structures of positive samples Pkand negative samples Nk, on this basis, label-specific features can be constructed in the form of:(4)φk(xi)=[d(xi,p1k),…,d(xi,pmkk),d(xi,n1k),…,d(xi,nmkk)],where d( ·, ·) represents the distance between two samples. In literatures [61,62], Euclidean metric is used to calculate sample distance. Actually, φkis a mapping from the original d-dimensional sample space X to a new 2mk-dimensional label-specific feature space LIFTk, i.e., φk: X → LIFTk.LIFT induces a family of m classification models{f1,f2,…,fm}in the constructed label-specific feature spaces LIFTk(1 ≤ k ≤ m). Formally, for each lk∈ L, a binary training setTk*with n samples is created from the training set T according to the mapping φk, such that:(5)Tk*={(φk(xi),ϕ(Yi,lk))|(xi,Yi)∈T},whereϕ(Yi,lk)=+1if lk∈ Yi; otherwise,ϕ(Yi,lk)=−1. Based on the binary training setTk*,any binary learner can be employed to induce a classification model fk: LIFTk→Rfor lk.Given an unseen sample x′ ∈ X, the predicted label set for x′ isY′={lk|f(φk(x′),lk)>0,1≤k≤m}.To fuse rough set approaches into machine learning problems, we will introduce the classification learning task instead of the notion of information system. Formally, a classification learning task can also be considered as the 3-tuple < U, A, D >, in whichU={x1,x2,…,xn}is the finite set of n samples called the universe of discourse,A={a1,a2,…,ac}is the set of condition features, D is the decision.Let U ≠ ∅ be a universe of discourse. F: U → [0, 1] is a fuzzy set [8] on U, F(x) is the membership function of F, F(U) is the set of all fuzzy sets on U. A given fuzzy binary relation R can be a fuzzy equivalence relation if and only if R is reflexive, symmetric and transitive. Equivalently, ∀x, y, z ∈ U,R(x,x)=1,R(x,y)=R(y,x)and⋀y(R(x,y),R(y,z))≤R(x,z).Definition 1[8,18]Let U ≠ ∅ be a universe of discourse, R is a fuzzy equivalence relation on U, ∀F ∈ F(U), the fuzzy lower and upper approximations of F are denoted byR(F) andR¯(F),respectively, ∀x ∈ U, the membership functions are defined as:(6)R̲(F)(x)=infy∈Umax(1−R(x,y),F(y));(7)R¯(F)(x)=supy∈Umin(R(x,y),F(y)).The pair[R̲(F),R¯(F)]is referred to as a fuzzy rough set of F.Definition 2Let < U, A, D > be a classification learning task, ∀B⊆A, RBis the fuzzy equivalence relation on U in feature subset B,U/IND(D)={d1,d2,…,dp}is the partition induced by the decision D, then approximate quality of U/IND(D) based on fuzzy rough set is represented in form of:(8)γ(B,D)=|⋃i=1pRB̲(di)||U|=∑j=1|U|(⋁i=1pRB̲(di)(xj))|U|,where | · | denotes the cardinality of a set, diis a decision class.γ(B, D) reflects the approximation abilities of the granulated space induced by feature subset B to characterize the decision D. Obviously, 0 ≤ γ(B, D) ≤ 1 holds. In literatures [18,19], it is proved that approximate quality is monotonic with the increasing or decreasing of condition features in a classification learning task, i.e, γ(B1, D) ≤ γ(B2, D) if B1⊆B2.Definition 3Let < U, A, D > be a classification learning task, ∀B⊆A, B is referred to as a reduct of A if and only if1.γ(B,D)=γ(A,D);∀C ⊂ B, γ(C, D) ≠ γ(B, D).By Definition 3, we can see that a reduct of A is a minimal subset of A, which preserves the approximate quality. However, in the majority of real-world applications, the above definition is much too strict. To expand the application scope of attribute reduction (dimension reduction, feature selection), Hu et al. [18,19] introduced the threshold ε to control the change of approximate quality for loosening the restrictions of reduct. In reality, we can also consider B as a reduct of A when satisfying the following conditions: (1)γ(A,D)−γ(B,D)≤ɛ; (2) ∀C ⊂ B,γ(A,D)−γ(C,D)>ɛ. Note that, ε is aimed at reducing redundant information as much as possible, while maintaining the change of approximate quality in a smaller range. In general, ε is recommended to be [0, 0.1].In fuzzy rough set, with the number of features increasing, the fuzzy similarity between samples will decrease, and then the lower approximation of decision will increase, namely, the size of positive region will be enlarged. As is well known, the samples in positive region are usually regarded as to be certain, and then the degree of certainty in the classification learning task will be improved. It is consistent with our intuition that new features will bring new information about granulation and classification.Let < U, A, D > be a classification learning task, ∀ai∈ B⊆A, we define a coefficient(9)Sigin(ai,B,D)=γ(B,D)−γ(B−{ai},D)as the significance of aiin B relative to decision D.Sigin(ai,B,D)reflects the change of approximate quality if aiis eliminated from B. Accordingly, we can also define(10)Sigout(ai,B,D)=γ(B+{ai},D)−γ(B,D),whereai∈A−B,Sigout(ai,B,D)measures the change of approximate quality if aiis introduced into B. On the basis of above, large numbers of researchers [24,45,47,51,52,63] iteratively select the most significant features with forward greedy algorithm until no more deterministic rules generating with the increasing of features. Feature selection (dimension reduction) based on approximate quality can greatly reduce redundant and irrelevant information in feature space, while remaining the degree of certainty in the classification learning task.In this subsection, we will propose a multi-label learning approach with label-specific feature reduction based on fuzzy rough set (FRS-LIFT). In the multi-label training set T, FRS-LIFT firstly constructs the label-specific feature space LIFTkfor each label lk(Steps 2–4); then, dimension reduction in LIFTkis implemented with fuzzy rough set (Steps 5–10); next, m classification models are built in the dimension-reduced label-specific feature space FRS-LIFTk(Steps 13 and 14); finally, the unseen sample is predicted in the multi-label learned system (Step 16).Formally, FRS-LIFT can be designed as following.The time complexity of FRS-LIFT mainly comprises of three components: clustering on Pkand Nkin Step 3, forming the label-specific feature space in Step 4, and dimension reduction for the label-specific feature space in Steps 5–10. The cost of performing clustering on Pkand Nkusing k-means isO(mk(t1|Pk|+t2|Nk|)),where t1 and t2 are the iterations of k-means on Pkand Nk, respectively. Forming the label-specific feature space requires O(2mk|T|) time. Finally, the time complexity of dimension reduction isO(4mk2|T|2). Therefore, in general the time complexity of FRS-LIFT isO(mk(t1|Pk|+t2|Nk|)+2mk|T|+4mk2|T|2).Although FRS-LIFT improves the performance of multi-label learning via reducting redundant label-specific feature dimensionalities, its computational complexity is high. To alleviate this problem, sample selection technique will be introduced in next subsection.In the field of machine learning, sample selection is considered as a better data compression technique [23]. The ultimate goal of sample selection is to reduce the size of training samples without losing any extractable information, while simultaneously insisting that a learning approach built on the reduced training samples is good or nearly as good as a learning approach built on the original training samples [7]. It is obvious that removing some samples from the training set decreases the computational complexity of learning approach. Several methods of sample selection have been explored and studied, such as condensed nearest neighbor (CNN) algorithm [15], instance-based learning (IB) algorithm [1], selective nearest neighbor (SNN) algorithm [36], and edited nearest neighbor (ENN) algorithm [48].According to many research [4,48], the uncertainty for samples in the boundary is larger than in other places, which means that the information provided by boundary samples will be more important. Accordingly, the majority of methods of sample selection tend to choose samples in boundary. Similarly, in dimension reduction, we compute a series of similarity matrices in the sample space constructed by boundary samples instead of the original sample space. The time and memory consumption of constructing fuzzy equivalence relations will be reduced greatly. For this purpose, some clustering algorithms, for example, k-means or fuzzy k-means [10,27] algorithm can be employed to seek the samples far away from the center of similar samples. Specifically, we suppose that Cjis a cluster,Cj*is the clustering center of Cj,dist(x,Cj*)denotes the distance between x ∈ CjandCj*,and the average distance between ∀x ∈ CjandCj*is represented as following:(11)dist¯(Cj*)=1〈Cj〉∑x∈Cjdist(x,Cj*),where ⟨ · ⟩ represents the number of samples in a cluster.The sample x ∈ Cjwhosedist(x,Cj*)is larger thandist¯(Cj*)is considered as a boundary sample. With all selected boundary samples, a new classification learning task can be constructed and its computational complexity is reduced in some extent.In this subsection, we will propose a multi-label learning approach with label-specific feature reduction by sample selection (FRS-SS-LIFT). In the multi-label training set T, FRS-SS-LIFT firstly constructs the label-specific feature space LIFTkfor each label lk(Steps 2–4); then, sample selection is adopted to reduce the number of samples in LIFTk(Steps 5–8); next, dimension reduction in the sample-selected label-specific feature space SS-LIFTkis implemented with fuzzy rough set (Steps 9–14); after that, m classification models are built in the dimension-reduced label-specific feature space FRS-SS-LIFTk(Steps 17 and 18); finally, the unseen sample is predicted in the multi-label learned system (Step 20).Note that k-means algorithm [21] is used to partition all samples into k clusters, where k represents the number of decision classes. In multi-label learning’s framework, for each label l ∈ L, the value of decision for any xi∈ X equals+1if xihas l; otherwise, the value equals−1. Therefore, k is set to be 2 in k-means clustering. The samples far away from their own clustering centers are selected to form a new label-specific feature space.Formally, FRS-SS-LIFT can be designed as following.The time complexity of FRS-SS-LIFT mainly comprises of four components: clustering on Pkand Nkin Step 3, forming the label-specific feature space in Step 4, sample selection on the label-specific feature space in Steps 5–8, and dimension reduction for the sample-selected label-specific feature space in Steps 9–14. The cost of performing clustering on Pkand Nkusing k-means isO(mk(t1|Pk|+t2|Nk|)),where t1 and t2 are the iterations of k-means on Pkand Nk, respectively. Forming the label-specific feature space requires O(2mk|T|) time. Then, sample selection on the label-specific feature space needs O(2t3|T|) time, where t3 is the iterations of k-means on |T|. Finally, the time complexity of dimension reduction isO(4mk2|Ts|2),where |Ts| is the number of selected samples (boundary samples). Therefore, in general the time complexity of FRS-SS-LIFT isO(mk(t1|Pk|+t2|Nk|)+2mk|T|+2t3|T|+4mk2|Ts|2).In the majority of data sets, we have|T|−|Ts|>t3/(2mk2),then(1+|Ts|/|T|)(|T|−|Ts|)>t3/(2mk2)⇔2mk2(|T|+|Ts|)(|T|−|Ts|)>t3|T|⇔4mk2|T|2>2t3|T|+4mk2|Ts|2holds. Therefore, it is shown that the time complexity of FRS-SS-LIFT is lower than that of FRS-LIFT.To evaluate the performances of our multi-label learning methods, 10 real-world multi-label data sets have been employed in this paper. For each multi-label data setS={(xi,Yi)|1≤i≤p},symbol |S|, dim(S), L(S) and F(S) represent the number of samples, number of features, number of possible labels, and feature type, respectively. Moreover, for better describing the characteristics of data sets, some other multi-label properties [33,58,61,62] also have been adopted such as:•LCard(S)=1p∑i=1p|Yi|:measures the average number of labels in each sample;LDen(S)=LCard(S)L(S):normalizes LCard(S) with the number of possible labels;DL(S)=|{Yi|(xi,Yi)∈S}|:counts the number of distinct label combinations in S;PDL(S)=DL(S)|S|:normalizes DL(S) with the number of samples.Table 1summarizes some detailed statistics of multi-label data sets used in our experiments. The 10 data sets are chosen from five distinct practical application domains, such as music, audio, biology, text and image. Therefore, the multi-label data sets used in our experiments are more comprehensive.Sinceeach sample is simultaneously associated with several labels, the performance evaluation in multi-label learning is more complicated than traditional single-label learning. Some popular evaluation metrics in the single-label learning system, such as accuracy, precision, recall and F-measure [41], can’t well adapt to the multi-label learning system. In this paper, five widely used multi-label evaluation metrics proposed in [14,37,39,40] are employed, including average precision, coverage, hamming loss, one error and ranking loss.Given a multi-label testing setT′={(xi,Yi)|1≤i≤t},the real-valued function f( ·, ·) produced from the multi-label learning system can be transformed into a ranking function rankf( ·, ·) [59]. For each l ∈ L, rankf(xi, l) maps f(xi, l) to the grades {1,2,...,m}, i.e., for f(xi, l) > f(xi, l′), rankf(xi, l) < rankf(xi, l′) holds. The detailed multi-label evaluation metrics are presented as following.•Average Precision [37]: evaluates the average fraction of labels ranked above a particular label l ∈ Yiwhich actually are in Yi. The bigger the value of AveragePrecision(f), the better the performance. Specially, the performance is perfect whenAveragePrecision(f)=1.(12)AveragePrecision(f)=1t∑i=1t1|Yi|×∑l∈Yi|{l′|rankf(xi,l′)≤rankf(xi,l),l′∈Yi}|rankf(xi,l).Coverage [40]: evaluates the average depth of going down the list of labels for covering all the possible labels of sample. The smaller the value of Coverage(f), the better the performance. Specially, the performance is perfect whenCoverage(f)=0.(13)Coverage(f)=1t∑i=1tmaxl∈Yirankf(xi,l)−1.Hamming Loss [14]: evaluates the average time of misclassification in each sample, i.e., a label not associated with the sample is predicted or a label associated with the sample is not predicted. The smaller the value of HammingLoss(h), the better the performance. Specially, the performance is perfect whenHammingLoss(h)=0.(14)HammingLoss(h)=1t∑i=1t|h(xi)⊗Yi|,where h(xi) is the predicted label set which is associated with xi, ⊗ represents symmetric difference between two sets.One Error [39]: evaluates the average fraction of top-ranked label which is not in the set of possible labels associated with the sample. The smaller the value of OneError(f), the better the performance. Specially, the performance is perfect whenOneError(f)=0.(15)OneError(f)=1t∑i=1tΨ([argmaxl∈Lf(xi,l)]∉Yi),where for any predicate τ,Ψ(τ)=1if τ holds; otherwise,Ψ(τ)=0.Ranking Loss [59]: evaluates the average fraction of label pairs which are reversely ordered for the sample. The smaller the value of RankingLoss(f), the better the performance. Specially, the performance is perfect whenRankingLoss(f)=0.(16)RankingLoss(f)=1t∑i=1t1|Yi∥Yi¯||{(l,l′)|f(xi,l)≤f(xi,l′),(l,l′)∈Yi×Yi¯}|,whereYi¯denotes the complementary set of Yi.In this paper, our FRS-LIFT and FRS-SS-LIFT are compared with three well-established multi-label learning algorithms, including ML-kNN [59], MLNB [60] and LIFT [62]. According to Ref. [59], in ML-kNN, the number of nearest neighbors k and smoothing parameter s are set to be 10 and 1, respectively. For LIFT, FRS-LIFT and FRS-SS-LIFT, we adjust the parameter δ by increasing it from 0.1 to 1.0 (stepsize 0.1), and finally assign δ to 0.2. Note that, to our best knowledge, no theoretical bases have been reported to specify the threshold ε for controlling the change of approximate quality. The optimal value of threshold ε is dependent on the nature of a specific application. Therefore, we conduct a large number of experiments which help us determine an optimal change range of approximate quality. Consequently, our methods achieve better classification performance when ε is between 0.001 and 0.05. Furthermore, all experiments are run on a workstation equipped with a 3.10 Hz processor and a 8.00G memory.

@&#CONCLUSIONS@&#
Different labels may have distinct characteristics of their own, and then construction of label-specific features for each label is great necessary for multi-label learning. However, the construction of label-specific features may cause the increasing of feature dimensionalities with redundant information. In this paper, we have developed two approaches named FRS-LIFT and FRS-SS-LIFT for multi-label learning, which effectively remove some redundant information existing in label-specific feature space with the idea of fuzzy rough set based attribute reduction. In addition, FRS-SS-LIFT using sample selection comes with the equivalent predictive performance while achieving the low computational complexity in comparison to FRS-LIFT. In other words, FRS-SS-LIFT can be considered as an evolution of FRS-LIFT. The experimental study on 10 data sets from five different application domains demonstrates the superiorities of our proposed two approaches to other three typical multi-label learning approaches, including ML-kNN, MLNB and LIFT.It is worth noting that FRS-LIFT and FRS-SS-LIFT do not take full account of the correlations between different labels. To further improve the performances of our multi-label learning approaches, we can attempt to fuse them into dimension-reduced label-specific features in the future study.