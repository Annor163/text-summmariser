@&#MAIN-TITLE@&#
Simulating non-stationary operators in search algorithms

@&#HIGHLIGHTS@&#
Selecting most suitable operators in search algorithms.Simulating search operators whose behavior often changes continuously during the search.Comparison of operator selection policies for non-stationary search scenarios.Specific restless bandits whose actions values decrease according to frequent uses.

@&#KEYPHRASES@&#
Adaptive operator selection,Operator-based algorithms,Multi-armed bandit,Island models,

@&#ABSTRACT@&#
In this paper, we propose new scenarios for simulating search operators whose behaviors often change continuously during the search. In these scenarios, the performance of such operators decreases while they are applied. This is motivated by the fact that operators for optimization problems are often roughly classified into exploitation and exploration operators. Our simulation model is used to compare the performances of operator selection policies and to identify their ability to handle specific non-stationary operators. An experimental study highlights respective behaviors of operator selection policies when faced to such non-stationary search scenarios.

@&#INTRODUCTION@&#
Using a search algorithm to tackle an optimization problem mainly consists in applying basic solving operators—heuristics—in order to explore and exploit the problem search space for retrieving solutions. Well-known metaheuristics [18,12,23] are based on this principle, as well as sophisticated metaheuristics which have been developed through numerous metaphors [46]. Advanced neighborhood-based or evolutionary algorithms share some common principle according to this notion of basic search operators, especially when they have been combined to get hybrid metaheuristics. The use of appropriate operators is decisive to tackle effectively combinatorial (discrete) optimization problems for which the topology of the search space has to be carefully defined among different possible encoding of solutions. For instance, variation operators (mutation and recombination) are used in evolutionary algorithms, neighborhood-based operators are used in local search (several operators in variable neighborhood search [38]), local improvement operators are used in ant colony optimization [10] or velocity and position update functions, and sometimes local optimization processes, have to be defined in particle swarm optimization [26]. Most of the time, the designer of such algorithms has many choice concerning these search components. Therefore, the study or design of strategies for selecting the most suitable operators in search algorithms is an active research area [13,30]. Usually, the choice of the successive operators along the search process is driven by means of parameters. The improvement of the algorithm performance thus relies on an adequate setting of these parameters. An optimal setting may then be achieved by an optimal operator selection policy. Unfortunately, according to the underlying intuitions provided by the No Free Lunch theorems for optimization [54], this optimal policy may strongly depend on the problem instances to be solved.Initial parameters setting can be achieved by automated tuning algorithms [25,39]. In this case the set of benchmarks chosen for tuning can greatly impact the algorithm performance [29]. Nevertheless, the values of parameters may require more continuous control [15] and should rather not be fixed during the whole search process. Note that such an adaptive operator selection is strongly related to reinforcement learning problems, and especially to multi-armed bandit problems [16,7]. Finally, managing the famous exploration vs. exploitation balance in search heuristics have been greatly investigated in the literature through various methods; see for instance [33,30,48].The performance of adaptive selection policies depends on the characteristics of the problem's search space, as well as on the specificities of the search operators. Therefore different families of practical problems have been handled (e.g., permutation based problems [51], satisfiability problems [35]), but also more abstract operators models in order to provide more general and comprehensive testing frameworks. Note that abstract frameworks have been widely used in order to experiment and analyze the behavior and the performance of search algorithms, especially in evolutionary computation [37], with the use of Onemax, Royal Road or NK functions [52], or more generally combinatorial fitness landscapes [3].Focusing now on the adaptive selection of operators, Thierens [48] proposes epoch based scenarios in which rewards, drawn from a constant uniform distribution, are assigned to operators during each epoch. In Costa et al. [7], Boolean scenarios are proposed in order to consider null rewards for some operators and outliers scenarios that produce higher variance in the rewards. Fialho et al. [17] introduced two-values benchmarks in order to consider two possible levels of rewards for operators. Nevertheless, changes of operators efficiency are only considered by means of successive epochs.In this paper, we propose a new model for simulating search operators whose behavior often change continuously during the search. In these scenarios, the more an operator is applied during a period, the more its performance will decrease. At the contrary, performance of operators can increase when other operators are applied. This is motivated by the fact that operators for optimization problems are often roughly classified into exploitation and exploration operators. Exploitation operators aim at focusing on the evaluation of the visited solutions (configurations) of the search space in order to converge quickly to a local optimum. Exploration operators aim at diversifying the search trajectory by visiting sparse areas of the search space. Unfortunately, it is not possible to always exploit nor explore the search space. For instance, it is unlikely that an exploitation operator will always improve a configuration and find directly an optimal solution (except for simple problems). Therefore, decreasing performance may be observed along the search as well as changing behaviors of operators, according to the current state of the search and to possible interactions between operators. For example, the relative improvements or expected improvements of an intensification operator are likely to decrease when approaching a local optimum. We already observed such behaviors while studying adaptive operator selection mechanisms on OneMax functions within an evolutionary process [6], as well as for permutation problems with different neighborhoods in local search algorithms [51,8]. Similarly, Ochoa et al. [41] observed that the crossover role may change during the search with regards to the notion of exploitation and exploration.While metaheuristics algorithms have been widely developed using different formulations [46], we choose here to focus on a simple common scheme. Hence, the general description of operator based search algorithms proposed in the following may be helpful in this design process when the user has to precisely identify the components and performance criteria that are used in the adaptive process.To compare operator selection policies, we propose the use of specific non-stationary operators modeled by an original gain function where previous actions affect gain distributions. Such a simulation model can be used as a surrogate operator model when designing new adaptive search algorithms, since it makes possible to identify the ability of operator selection policies to manage exploitation and exploration operators. The aim is to provide abstract scenarios for modeling sets of operators whose behaviors may be subjected to progressive changes and interactions that require more intricate combinations when being applied and to evaluate different operator selection policies on these scenarios. Note that such adaptive policies for selecting operators may easily be introduced in various solving algorithms in order to schedule dynamically their basic search heuristics, as it has been already investigated for solving different optimization and combinatorial problems with evolutionary algorithms [9,20], local search [50] or constraint solvers [31].More precisely, our scenarios consider operators whose gains decrease if they are too much applied, which corresponds, as mentioned above, to intensification operators approaching a local optimum. Of course two different operators with such decreasing gain may use different heuristics as thus should be alternated in order to improve their performance. For instance, local search intensification operators that use different neighborhood may be used complementary in order to avoid getting stuck into a local optima (since may have different local optima with regards to their neighborhood). Moreover, we also consider operators with null gain that may be used for diversification or that correspond to inefficient operators for the current instance of the problem. This may be very useful when considering generic solvers which include numerous possible solving strategies. Therefore, the basic model of scenarios described here—and more sophisticated variants which can easily be designed—allows the simulation of very different type of search situations, focusing on the operator management.The experimental study gives information on the respective behaviors of operator selection policies when faced to such non-stationary search scenarios. We show that none of the selection policies achieve the best performance in every situation but results highlight that their respective performances rather depend on the specificities of the operators. Therefore, such comparison may help a user to determine the most appropriate selection policy according to her/his problem at hand.At last, considered as a multi-armed bandit problem, the adaptive non-stationary operator selection problem corresponds to a specific restless bandit problem that could be used to model different real applications as soon as the efficiency of a given action decreases according to successive frequent uses. For instance, such reinforcement learning techniques are now widely used for recommendation on the web [28] in order to manage adaptive content. Our model could be pertinent in this context since it may be clear that the relevance of an advertisement decreases if it is too much shown to the same user. Other cases of such repeated decreasing actions may actually be observed in various application domains.In Section 2, we formally describe optimization algorithms that are based on applications of basic search operators. We also define the problem of designing the best possible operator selection policy and show its relationship with multi-armed bandit problems. Section 3 is dedicated to review different operator selection policies. Section 4 presents our model for simulating non-stationary operators. Experiments are presented in Section 5.In this section, we propose to precisely define the components of a search algorithm in the context of solving optimization problems, in order to be able to manage their behavior. We focus on algorithms that use operators—also commonly called heuristics—which aims at determining pertinent solutions in the search space. Our purpose is to progressively introduce and discuss the different aspects that must be taken into account when one wants to improve search algorithms.We first propose a generic algorithmic scheme that allows us to precisely define the main components of an operator based search algorithm. We discuss the notion of performance with regards to the operational semantics of the algorithm and recall two general methodologies for improving this performance, namely tuning and control. Focusing on the dynamic control of search algorithms, we focus then on the notion of policy, that defines how operators should be scheduled in a search process. Some criteria are introduced in order to compare different policies and thus to characterize the notion of optimal policy. The following Table 1summarizes the main notations used in this section.Definition 1Optimization problemAn optimization problem is a pair(S,f)whereSis a search space whose elements represent solutions of the problem (also called configurations or points), andf:S→ℝis an objective function. An optimal solution (for maximization problems) is an elements*∈Ssuch that∀s∈S,f(s*)≥f(s).Example 1As a simple running example, let us consider the toy One-Max problem, whereS={0,1}nfor a given size n and,∀s=(s1,…,sn)∈S,f(s)=∑i=1nsi. Note that siis the value of the ith bit of s. The optimal solution is s*=(1, …, 1), with f(s*)=n.Defining a search algorithm A general solving algorithm may be abstracted according to the following components (see Algorithm 1).Algorithm 1Operator based algorithm (OBA).Require: an optimization problem(S,f), a set of operators Ω={o1, …, on}, a parameter vector θ∈Θ, a policyπ:Θ×ℕ→Ω.1.t←02.Sol(t)←Init(S)3.s*←best(Sol(0))4.while not (stop condition) do5.t←t+16.o←π(θ, t)7.Sol(t)←o(Sol(t−1))8.b←best(Sol(t))9.iff(b)>f(s*) then10.s*←b11.end if12.end while13.returns*Typically, the initialization function (line 2) takes as input11This input represents indeed the knowledge that is required by the initialization function.the whole search spaceS—or a part of it—and returns a single element (e.g., in local search algorithms) or a set of elements of this search space (e.g., in population based algorithms). From a practical point of view, initialization is often insured by a random generation of points of the search space or by a greedy algorithm that builds a suboptimal solution. Note that we do not want to distinguish the different solving paradigms that use different data structures for their search processes (e.g., tree-based search, neighborhood search or population-based search). We only pay attention to the fact that these algorithms use basic search operators (e.g., branching heuristics, constraint propagation, hill-climbing or recombination of solutions) that they apply on a sub part of the search space (stored into Sol(t)). The policy π determines the operator to apply at each iteration of the algorithm (line 6). π can be called an operator selection (OS) policy and may depend on the parameters vector θ22According to the classification proposed in Smit and Eiben [45], parameters can be numerical or symbolic.and the iteration t, as it will be illustrated later. The notion of iteration is here directly linked to the application of one operator in a steady state fashion; this general model can anyway be adapted for coarser granularities of iterations. Note that the stop condition (line 4) may refer to a global optimum that has been reached (e.g., for maximal satisfaction problems) or to a maximal number of iterations.Example 2Back to our running example, let us consider a simple evolutionary algorithm for the One-Max problem. The algorithm includes an initialization function such that Sol(0) is an initial population, randomly generated. The set of operators may include flipping (mutation) operators (e.g., 1-flip is an operator that flips one bit—i.e., one position—of a configurations∈S) and crossover operators (e.g., uniform crossover). In this case, let us consider an operatoro1−flip:S→S. Note that here, operators are applied on a population and are supposed to include their own selection and insertion processes, in a steady state fashion. For instance, let Sol(t)={000, 111, 110}, o1−flip(Sol(t))={010, 111, 110}, which means that the individual 000 of Sol(t) has been selected for mutation and the resulting mutated (by flipping one bit) individual has replaced the selected one. Of course, many selection and replacement methods exist in the literature. An example of parameter vector could be θ=(psize, σ1, …, σn) where psize is the size of the population Sol(t), and σiis the probability of application of an operator oi. The OS policy may be π(θ, t)=oiwith a probability σi, that corresponds to a roulette wheel selection (as described later in Section 3). Note that, in this case, we consider that θ remains constant over iterations, i.e., π(θ, t) does not depend on t.Running the algorithm: The operational semantics of a search algorithm can be defined by means of its runs. We defineS¯(resp.Ω¯) as the set of sequences over2S(resp. Ω).Definition 2Run of an OBAFor an optimization problemP=(S,f), a run of an OBA A, defined as in Algorithm 1, is a pair(s¯,o¯)∈S¯×Ω¯where:•s¯=Sol(0),Sol(1),…,Sol(l)withSol(0)=Init(S)and Sol(l) satisfies the stop condition.o¯=o(1),…,o(l).∀1≤t≤l, o(t)=π(θ, t).∀1≤t≤l, Sol(t)=o(t)(Sol(t−1)).Performance evaluation: Given a run r∈Run(A, P), we suppose that there exists a performance function such thatPerfP(A,r)∈ℝevaluates the performance of r. Note that we use the notation PerfPsince this performance function has to be defined according to the problem P. Typically, the performance of a run can be defined by the objective value of the best solution obtained during this run, i.e., s* (line 13 of Algorithm 1). A run r is considered better than a run r′ if PerfP(A, r)≥PerfP(A, r′). This performance function can be extended to a subset of runs R⊆Run(A, P). Note that this extension has to be defined carefully. For instance one may consider an average performance of all runs in R, or the maximal run performance. Based on this notion of performance, we may now turn to the optimization of the algorithm itself by managing its operators. We use here the classical terminology of tuning and control [30,21].Tuning of the algorithm: Now let us define a notion of optimality for the performance of the algorithms, in a tuning point of view [24]. Various methods have been proposed to this aim [4,45,25]. From these considerations, the optimal tuning only considers the parameters vector θ. For an optimization problem P, an OBA A with parameter vector θ is optimally tuned for a problem P iff, for all algorithm A’ with parameters vector θ′, PerfP(A′, Run(A′, P))≤PerfP(A, Run(A, P)). In practice the tuning of the algorithm can only be estimated on a subset R of runs. Note that we consider here tuning by means of parameters vector θ and not directly on the possible policies.Control of the algorithm: Only algorithms with fixed parameters and policy have been considered in the previous definitions. We have to take into account adaptive algorithms in which parameters may change during the search. As previously, a policy uses parameters θ and may be then intrinsically submitted to adaptive changes by means of these parameters. Therefore, we may generalize our definition of an OBA to an adaptive OBA by considering a control functionK:Θ×ℕ→Θthat updates the parameter vector with regards to the current iteration.Kis the set of control functions.We must insert in Algorithm 1, between lines 11 and 12, the following line: θ←K(t, θ). As above for tuning purposes, one can theoretically determine that an adaptive OBA is optimally controled by function K in comparing its performance against all possible control functions K′.We focus now on the control of the algorithms through their OS policies and control functions. According to the previous notations, an adaptive control policy is a pair (π, K), which associates a policy and a parameter control function. Following [30,21], such a policy is called an adaptive operator selection (AOS) policy. Note that the case where K is the identity function corresponds to a non-adaptive policy. Therefore, in the remaining of the paper, we consider OS policies, including adaptive policies and fixed policies cases.Example 3Back to the example, we may consider an adaptive roulette wheel instead of the fixed one as previously mentioned. The control function may thus update the probability according to the observed performances of operators by means of the function K, which is known as probability matching (see Section 3.1.1).From performance to operator gains: Given an optimization problem P, our purpose is to generate a policy (π, K) that produces optimal runs(s¯,o¯), i.e., an optimal sequence of operator applicationso¯. It is clear that the impact of the operators depends on the elements ofs¯on which they are applied. In order to consider a more abstract point of view, we consider that each operator provides a gain when it is applied; this gain may be estimated by a gain functiongP:Ω×S¯→ℝ. In practice the gain of an operator can be computed using the fitness function of the problem, e.g., fitness improvements, or other criteria, e.g., diversity of the population in population based algorithms (see [34,36] for instance). Of course the gain has to be defined according to the notion of performance. The gain obtained by a policy (π, K) for a run r can be defined asGP((π,K),r)=∑o(k)∈o¯gP(o(k),(Sol(0),…,Sol(k)))wherer=(s¯,o¯)∈Run((Init,Ω,θ,π,K),P). This gain function should be defined such that for two runs r and r′ obtained by an algorithm A using a policy (π, K), we have GP((π, K), r)≤GP((π, K), r′)iffPerfP(A, r)≤PerfP(A, r′) (i.e., the gain function should estimate the performance of the algorithm).Optimizing the algorithm: At last, the operator selection policy problem can be defined as follows.Definition 3OS policy problemGivenP=(S,f), the components Init, Ω and θ of an OBA, the operator selection policy problem consists in finding a policy(π,K)opt=argmaxπ∈Π,K∈K∑r∈Run(A,P)GP((π,K),r)with A=(Init, Ω, θ, π, K)Again, since most of the time the whole set Run(A, P) cannot be extensively computed, the optimal policy has to be approximated on a subset of Run(A, P). From a practical point of view, there is a connection between the notion of optimal control and the notion of optimal policy. Therefore, searching for an optimal policy which chooses at each step operators that maximize the overall gain may be related to bandit problems [7,15].The initial stochastic multi-armed bandit (MAB) [42,5,43] is formulated as follows. Given several possible actions—usually called arms according to the gambling machine analogy—that have different individual gains (or rewards), one has to select a sequence of actions that maximizes the total gain. Definition 4 proposes a more formal definition of this general problem.Definition 4Stochastic MABLet us consider n independent arms. For each arm i∈{1, …, n}, we have:•a set of possible states Si;a set of probabilitiesProbi={σj→ki|j,k∈Si}such thatσj→kiis the probability of being in state k if the arm i is played33We use the verb play according gain to the gambling analogy.from state j;a set of gainsGi={gji|j∈Si}wheregjiis the gain obtained when arm i is played from state j.Given a stochastic MAB, the problem is to find a policy that maximizes over a finite44Note that we restrict the problem to finite horizon MAB. The most general problem is often presented over infinite horizon.horizon T,∑t=0Tgtγt, where gtis the expected gain of the policy at time t and γ∈[0, 1] is a discount factor.Four features can be identified to characterize a MAB problem [32]:1only one arm is played at each time;states of unplayed arms do not change;arms are independent;arms that are not played do not contribute any gain.Many variants of the initial stochastic MAB have been studied in the literature. In this paper we focus on the restless MAB, first introduced in Whittle [53]. In this formulation, the gains of the arms change over time, while they are supposed to be fixed—but of course unknown—in the initial stochastic MAB formulation. In fact, restless bandits may be defined as in Definition 4 except that, when an arm is not played, its state may change, which corresponds to a relaxation of Feature 2. Hence, restless MABs involve two kinds of probabilities in Probi, namelyσj→ki, which represents the probability of being in state k if the arm i is played, andσ˜j→ki, that is the probability of being in state k if the arm i is not played.OS policy problems can be stated as MAB problems, where arms represent operators. In order to simulate the behavior of OS policies, one may define specific scenarios within the MAB formalism that specify gain and probabilities of gains of operators.In this section we first explain how operator selection in operator based algorithms can be directly related to the choice of the most suitable sequence of actions in the context of multi-armed bandit problems. We review then different possible selection policies that can be used to achieve an optimal schedule of the operators.Let us consider an adaptive OBA A=(Init, Ω, θ, π, K). Here, we are not interested in the initialization function Init. Let Ω={o1, …, on} be the set of n operators. We have to define the control policy (π, K) which selects an operator at each iteration of the algorithm in order to build a run(s¯,o¯). We review here different policies and we distinguish between policies based on probabilities of application of the operators and policies based on upper confidence bounds.The gain of an operator (see Definition 3) is generally specific to the problem, since it uses the notion of performance of a run. In order to have a more general approach, a general notion of utility,55Note that we use the term utility here, which should be clearly related to the notion of action value in reinforcement learning [47].which reflects the successive gains obtained by the operators, can be introduced.Considering a run(s¯,o¯), such thats¯=s(0),…,s(n)ando¯=o(1),…,o(n), an utilityui(t)is associated to each operator i∈{1..n} for any iteration t∈{1..n}. This utility has to be re-evaluated at each time, classically using a formulaui(t)=(1−α)ui(t−1)+α·g(oi,s(0),…,s(t−1)), withui(0)=0. This utility uses the gain associated to the application of operator i (which corresponds thus to the immediate utility) and α which is a coefficient that controls the balance between past and immediate utilities, as in classic reinforcement learning techniques [47]. If an operator is not selected at iteration t, its gain is 0 for this iteration.In this context, given the set of operators Ω={o1, …, on}, we use the parameter vector θ to associate a probability of selecting the operator, θ=(σ1, …, σn) such that∑i=1nσi=1. The OS policy π is then a roulette selection wheel that selects each operator oiaccording to its probability of selection σi. Different operator selection policies have been proposed in the literature [30,21]; we review here some of the most used of them.•Fixed roulette wheel:A first possibility consists in keeping θ fixed during the run, i.e.,∀t∈ℕ,K(θ,t)=θ. Note that these values can be determined by an automated tuning process [24,11].Adaptive roulette wheel:Contrary to a static tuning of the operator application rates, adaptive operator selection consists in selecting the next operator to apply at iteration t+1 by adapting the selection probability during the search. In this case, we haveθ(t)=(σ1(t),…,σn(t)). The control functionK:Θ×ℕ→Θis defined as K(θ(t), t+1)=θ(t+1). Defining K consists in defining the probabilitiesσi(t+1)with regards to the evolution of the operator's utilities.A classic mechanism is the probability matching selection rule:(1)σi(t+1)=pmin+(1−n·pmin)ui(t+1)∑k=1nuk(t+1)where a non-negative value pmin insures a non-zero selection probability for all operators. Note that, in order to insure a coherent behavior, pmin should be in the interval[0,1n].Adaptive pursuit:An alternative proportional selection rule has been proposed in Thierens [48], called adaptive pursuit (AP), that distinguishes the best current operator from the others:(2)σi*(t+1)=σi*(t)+β(pmax−σi*(t))σi(t+1)=σi(t)+β(pmin−σi(t))wherei*∈argmaxi∈{1,…,n}ui(t+1), pmax=1−(n−1)pmin and β is a parameter to adjust balance of this winner-take-all strategy.Optimal strategies have been initially proposed by Feldman [14] and Gittins [19] for the multi-armed bandit problem. Later, Auer [1] proposed to use this problem to manage the compromise between exploration and exploitation in optimization algorithms. The following policies consists in computing an upper confidence bound of the expected gain and to select thus the most promising arm.•UCB (upper confidence bound):The UCB1 criterion [2] is defined as:(3)∀oi∈Ω,UCB1(oi,t)=ui(t)+2log(∑1≤k≤nnbk(t))nbi(t)wherenbi(t)denotes the number of times operator oihas been applied. Note that this formula is defined for gains that should be normalized between 0 and 1. The left term of the formula uses the successive utilities that are obtained by the arms in order to focus of the best arm, while the right term aims at providing the opportunity to be selected for less used arms. This formula attempts thus to achieve a compromise between exploitation and exploration.Therefore, we may define the control policy (π, K) for a given iteration t as:(4)π(θ(t),t)=argmaxi∈{1,…,n}θi(t)K(θ(t),t)=θ(t+1)=(UCB1(o1,t+1),…,UCB1(on,t+1))Here no parameter is required. Note that UCB has originally be designed for fixed gain distributions. Since the gain of the operators is likely to change along the search, UCB has been extended to dynamic multi-armed bandit has to be considered.DMAB (Dynamic MAB algorithm based on UCB):UCB has been revisited in Costa et al. [7]. A standard test—known as Page Hinkley [22]—for the change hypothesis is used. We may add a parameter in θ which indicates if the process has to be restarted. In this case the control function K use the Page Hinkley test to detect statistical changes in the successive utilities of the operators and may re-initialize the values of the operators utilities. Moreover, it can be useful to add a scaling factor to the right term of the UCB1 formula in order to take into account the value range for utilities. The test is parametrized by γ that controls its sensitivity and δ that manages its robustness. We refer the reader to Fialho et al. [17] for more details.Based on previous work on island models for operators selection [6], we present here a selection policy based on a probabilistic transition matrix. The underlying motivation is not only to detect the best possible operators but also possible relationships between operators.Given an OBA A, we define a matrix M of size |Ω|×|Ω|. M(oi, oj) represents the probability of applying operator ojafter having applied oi. According to our previous notations, M corresponds to the θ parameter of the selection policy (introducing a two-dimensional parameter structure).From an operational point of view, contrary to previous policies, this policy uses simultaneously several possible runs of the OBA in order to acquire its knowledge. Let us consider a set P(t) of psize runs of length t+1 produced by the algorithm A at iteration t (remind that runs are numbered from index 0 in definition 2). psize is a parameter of this policy. For each operator oi∈Ω, letPi(t)be the set of runs (s(0)…s(t), o(1)…o(t))∈P(t) where o(t)=oi. Hence, this set is the set of all runs whose last applied operator is oi, and we haveP(t)=⋃o∈ΩPo(t).The control policy (π, K) can thus be defined from M. Firstly, the policy π is defined by a roulette selection whose probabilities are given by M. Secondly, the update of M is performed by the control function K as:(5)K(M(t)(i,k),t)=M(t+1)(i,k)=(1−β)(α·M(t)(i,k)+(1−α)Ri(t)(k))+βN(t)(k)where N(t) is a stochastic vector such that ||N(t)||=1 andRi(t)is the reward vector that is computed by using the utility of the operators that have been used after applying i. More precisely:(6)Ri(t)(k)=1|B|ifk∈B,0otherwise,withB=argmaxok∈Ω(max{s¯∈S¯|∃r∈Pk(t),r=(s¯,o(1)…o(t−1)=oi,o(t)=ok)}g(ok,s¯))B is the set of the best operators that have been applied after an operator oi, i.e., that have provided the best gain. We could also updateRi(t)(k)by using the mean of the improvements. Remark also that this policy does not use the utility which is indeed computed within the update process.The parameter α represents the importance of the knowledge accumulated (inertia or exploitation) and β is the amount of noise, which is necessary to explore alternative possibilities. The influence of these parameters has been studied in Candan et al. [6].This approach can be related to reinforcement techniques for MDP (Markov Decision Processes) [47]. Nevertheless island models use several populations in order to learn simultaneously from several sequences of operators.In practice, the gain function g may be difficult to compute and is very specific to the problem at hand. Therefore we may approximate such a gain function by using distributions. Comparisons of operator selection policies would be then easier and faster. As previously defined, we use a gain function g(o, t) which represents an estimation of the gain of an operator o if it is applied at iteration t. Several scenarios for modelling g can be envisioned.In Section 2, the gain associated to an operator is defined according to the performance improvements that it provides during a given run of the algorithm. Remind that, for an optimization problemP=(f,S), the effect of operators can be defined according to the evaluation function f. For instance, given an elements∈S, the variation of evaluation f induced by an operator o is given as f(o(s))−f(s). This function can be extended toS¯. Of course different performance criteria could be taken into account.For each operator o, the gain g is defined independently from iteration t by a binomial probability distribution (po, go), i.e., ∀i∈I, Pr[g(o, i)=go]=p0 and Pr[g(o, i)=0]=(1−po). Such distributions have been proposed in Costa et al. [7] for studying operator selection policies.In this context, if the values (po, go) are fixed during the whole run of the algorithm, then determining the best policy just consists in finding the operator that has the greatest expected gain po.go. This is indeed a basic stochastic multi-armed bandit problem (Definition 4), where operators correspond to arms with only one state. Of course, it is unlikely that, in real optimization problems, the effect of an operator remains unaltered in a whole run. Therefore it would be more realistic and interesting to consider gain functions that may evolve during the run.In Thierens [48], uniform distributions are associated to each operator in different overlapping intervals of values. The distributions are fixed during a given number of iterations—called epoch—and then are re-assigned according to a permutation. The gain of the operator is thus non-stationary during the algorithm's execution and the AOS has to discover the best operator to apply at each epoch. Such scenarios have been studied using various techniques including adaptive pursuit [49], dynamic UCB [16] and genetic algorithms [27].In this section we define a new scenario in order to model operators whose behavior evolves more continuously during the solving process. We want to consider more continuous changes in the gain distributions. The idea is to provide a model where the gain of an operator decreases proportionally to its use. In such a model, the AOS policy must not detect the best operator during an epoch but rather identify suitable sequences of operators.Within a run(s¯,o¯)of length n of an OBA, such thato¯=o1,…,on, we compute the gain of operator o at iteration t thanks to a gain function gwsize(o, t), defined as:(7)gwsize(o,t)=go·1−Occwsize(s¯,o,t)wsizegois a fixed maximal gain of operator o, andOccwsize(s¯,o,t)is the number of applications of operator o during the last wsize iterations. More formally,Occwsize(s¯,o,t)=|{i∈1..|w¯|,ωi=o}|, wherew¯is the subsequence ot−wsize, …, ot−1 that records the wsize last applied operators. wsize is a fixed parameter of the scenario. This scenario can be formalized as a restless MAB problem:•An arm i is associated to each operator oi∈Ω.For each operator oi∈Ω, we define a set of states Si={0..(2(wsize+1)−1)}, such that each state represents the previous use of the arm by a binary number. For instance, for a window of size wsize=4, the state 1001 means that the arm has been played at iteration t−1 and t−4 only.The set of transition probabilities Probibetween states can be defined as:1σj→ki=1ifk=Lshift(j)+10otherwiseσ˜j→ki=1ifk=Lshift(j)0otherwiseLshift(j)= denotes the logical left shift of a binary number of fixed size wsize. Note that we needσ˜probabilities since we are modelling a restless MAB problem (see Definition 4 and Section 2.3).When an arm i is played from state j, the reward can be straightforwardly defined asrji=go·(1−#1(j)wsize), where #1(j) is the number of bits being equal to 1 in state j.This restless bandit problem involves a two states transition matrix whose size is inO((n·2wsize)2). Of course in practice, one has to memorize the wsize previous applications of operators.In optimization algorithms, operators are often classified intuitively in two classes: diversification operators and intensification operators, whose behaviors should be understood as orthogonal. In order to better understand such scenarios we consider here two types of operators, according to the previous notations: (1, 1) and (1, 0), that we denote here respectively1andO. The1operators theoretically always gain 1 but their efficiency will decrease proportionally to their use according to our sliding window.Oalways gain 0. We choose here probability of 1 for all operators in order to avoid probability side effects in our analysis. We also use gain values whose range is in [0, 1].An instance of a non-stationary binary scenario can be fully defined by a triple (Nop, N1, wsize) where Nopis the number of operators, N1 is the number of1operators and wsize is the length of the window used for computing the decay of the operator's gains (see Section 3).It can be shown that, independently from the size of the windows, the score obtained by a uniform choice of the operators is constant:Property 1Given an instance (Nop, N1, wsize), the expectation of gain for a1operator is:(8)∀t∈ℕ,E[g(1,t)]=1−1NopMore generally, we have then the following property:Property 2Given an instance J≡(Nop, N1, wsize), the gain expectation per iteration is equal toN1Nop(1−1Nop).Therefore the behavior of a uniform selection of the operators is independent from wsize, for a given Nop. The total expected gain for N1 operators can easily be computed according to the number of allowed iterations. A uniform choice policy may thus serve as baseline for other selection processes (see Section 5).We may suppose that the1operators are numbered 1..N1 andOare numbered N1+1..Nop. The optimal policy problem can be formulated as a discrete constraint optimization problem.Definition 5Optimal policy for binary non-stationary scenariosGiven a non-stationary binary scenario (Nop, N1, wsize) and an horizon T (the number of allowed iterations) we define:•the set of variablesX={x1,…,xT}, such that xiis the operator applied at iteration i (decision variables);∀i∈{1, …, T}, xi∈{1, …, Nop} (domains of the decision variables);∀j∈{1,…,N1},gj=1wsize∑k=1T−wsize(∑i∈k..k+wsize|xi=ji)(gains for all operators whose value is 1);the objective function ismax∑j∈{1..N1}gj.Due to the size of the induced search space, this problem cannot be solved practically for large values of T. Nevertheless, we may restrict this problem to a limited window and compute suboptimal policies. We consider a restricted model. Note that, since we are only interested in maximizing the gain, theOoperators are all equivalent and we may consider only oneOoperator, i.e., Nop=N1+1 or Nop=N1 if noOoperator is considered. We consider here a circular scenario of length Sc on which the total gain is computed in order to simulate the successive repetitions of this scenario. Since we want to be able to compute circularly the total gain induced by such a scenario, we have to consider a computation sequence comps that is both a multiple of the length of the windows wsize and of the length of the scenario Sc (i.e., the least common multiple lcm).Definition 6Suboptimal policy for binary NS scenariosGiven a non-stationary binary scenario (Nop, N1, wsize) and scenario length Sc (the number of allowed iterations of operators in the scenario) we define:•comps=lcm(wsize, Sc) (the length of the computation sequence);nbseq=compsdivSc (the number of sequence of length Sc in the total computation windows);the set of variablesX={x1,…,xcomps}, such that xiis the operator applied at iteration i (decision variables);∀i∈{1, …, comps}, xi∈{1, …, Nop} (domains of the decision variables);∀i∈{1, …, comp}, k∈{1, …, nbseq−1}, (xi=xi+(k*Sc)) (the scenario is repeated nbseq times in the computation sequence);∀i∈{(wsize+1), …, comps} such that xi∈{1, …, ..N1},gi=1wsize∑{k∈(i−wsize)..i|xk=xi}k(standard gains for operators in the computation sequence);∀i∈{1, …, wsize} such that xi∈{1, …, ..N1},gi=1wsize(∑{k∈1..i|xk=xi}k+∑{k∈(comps−wsize+i)..comps|xk=xi}k)(circular case);the objective function ismax∑j∈{1..N1}gj.This model can be used to compute sub-optimal policies. We have used the Minizinc [40] constraint modelling and solving framework in order to compute such policies. Table 2shows the expected gain per iteration for different scenario window sizes, considering oneOoperator and one1operator. Note that this is a complete exhaustive search performed by a tree based constraint solver for discrete variables.

@&#CONCLUSIONS@&#
