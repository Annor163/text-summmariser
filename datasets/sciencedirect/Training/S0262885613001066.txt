@&#MAIN-TITLE@&#
Fast perspective recovery of text in natural scenes

@&#HIGHLIGHTS@&#
We present a method for perspective recovery of text in natural scenes.It relies on the characters' geometry to estimate a rectifying homography.The proposed method is efficient and fast.Comparative results show improved recognition accuracy against the state-of-the-art.

@&#KEYPHRASES@&#
Scene text extraction,Perspective recovery,Homography rectification,

@&#ABSTRACT@&#
Graphical abstract

@&#INTRODUCTION@&#
Efficient and fast comprehension of text in our environment is an important aspect of scene understanding for a variety of application areas, e.g. for automatic and assisted navigation of robots and humans respectively [1–4]. Images from a mobile camera, indoors or outdoors pose considerable challenges to text understanding, such as blurred or out of focus frames, uneven lighting, complex backgrounds, and lens distortion. One of the main issues is perspective distortion as the camera may not necessarily have a fronto-parallel view of the text. There have been rare attempts to directly recognize characters with perspective deformation, e.g. [5], but in general, even when the region of text can be delineated reasonably well, the accuracy of OCR engines degrades quickly with increasing perspective effects.The focus of this work then is on perspective recovery of text in natural scenes. Our aim is to obtain a fronto-parallel reconstruction of an image patch with scene text that improves the text recognition accuracy by off-the-shelf OCR software. The characteristics of scene text are fundamentally different from those of document images with text appearing in various orientations including differing orientations within the same image. Such texts usually appear in the form of isolated words or short sentences in diverse typefaces.We wish to recover text (e.g. on posters, billboards, shops and street signs) from images with enough resolution to make the segmentation of individual characters possible. We expect only a single frame – so no temporal information – and no camera parameters, e.g. the focal length would be unknown. The 3D orientation of the text should not matter (except for extreme views), and the only assumption we make is that the text is laid out in a straight line on a planar surface.The method proposed here computes a rectifying homography to reconstruct a fronto-parallel image for a line of text that may have been affected by perspective transformations, such as horizontal perspective foreshortening, shearing, and vertical perspective foreshortening. We correct horizontal perspective foreshortening by fitting two lines to the top and bottom of the text. The shearing and vertical perspective foreshortening are rectified by first estimating a shearing value for each character to then perform a linear regression on the shear variation across the text line.Our experimental results include a systematic test of texts, obtained from the ICDAR 2011 Robust Reading Competition datasets [6,7], synthetically regenerated at various orientations to establish a ground truth for performance comparison, followed by results on natural scene images. We present performance evaluation showing significant increase in recognition accuracy, across a wide range of viewing angles, compared against the unrectified image and the scene text perspective recovery technique by Myers et al. [8].Next, in Section 2 the problem is formally defined and set in context with respect to related works, followed by a detailed description of our proposed method in Section 3. Experimental results are presented in Section 4. We conclude our work in Section 5 and point to future directions.Before the emergence of camera based document acquisition, in-plane rotation or skew was the main geometric correction that document analysis systems had to deal with. Extensive literature exists in the area of document skew estimation, for example for some surveys see [9–11].For camera based rectification of text, there are more degrees of freedom to consider. Assuming text lies on a planar surface, the process of perspective recovery of text can be modeled as a projective transformation [12] between the source image and a target image. As the projective transformation preserves linearities, a rectangle enclosing the text in its original plane and orientation is seen as a quadrilateral in the source image and would need to be mapped to a rectangle in the target image (see Fig. 1). This projective transformation or homography is represented by a 3×3 mapping matrix:(1)p′=Hp,where p=[x y 1]⊤ and p′=[cx′ cy′ c]⊤ are homogeneous coordinate points in the source and target images respectively and H is the homography matrix.The homography has 8 degrees of freedom which can be decomposed into: translation and scale along each axis, Euclidean rotation, shear and two perspective foreshortenings along each axis respectively. As pointed out by Myers et al. [8], some of the degrees of freedom affect recognition more than others: OCR engines can deal with translation and scaling well, and rotation (or skew) is also handled by current OCR systems (albeit for a limited range of angles). Therefore, OCR-wise, the problem can be reformulated as correcting the distortions produced by shear and the two perspective foreshortenings, or alternatively, as estimating the location of two vanishing points within the image plane.Pilu [13] and Clark and Mirmehdi [14,15] were among the first to look at perspective recovery of camera acquired document images. Pilu [13] looked at the high level organization of text within documents as a basis for extracting illusory visual clues and computing the vanishing points to perform rectification. He employed a saliency measure between text connected components to form lines of text and estimate the horizontal vanishing point. Then, he used a set of carefully chosen rules of association between components in different lines to construct a set of candidate vertical lines which defined the vertical vanishing point. However, given that vertical clues are more scarce and difficult to get, Pilu [13] acknowledged that his vertical vanishing point estimation is not as reliable as the horizontal one. Clark and Mirmehdi [14] proposed two distinct perspective correction techniques based on extracting cues from higher level structures of text within document images. In their first technique, they searched for quadrilaterals in the image that would enclose text (e.g. paper borders, frames) and use it to compute the projective transformation. In their second and more complex approach, they considered the text itself only to infer the two vanishing points. The horizontal vanishing point was estimated by computing a projection profile for every possible vanishing point in a 2D polar search space around the image center. Then, the vertical vanishing point was obtained by projecting lines from the left and right margin lines, which restricts this technique to fully justified paragraphs. This process was later refined [15] to include left or right-only justified paragraphs by employing the spacing between lines in the computation of the vertical vanishing point. In a similar fashion to the first technique of [14], Cambra and Murillo [16] also looked for borders enclosing text regions for rectification and implemented it on a mobile phone.More recent works focused on document images include Stamatopoulos et al. [17] and Liang et al. [18] where perspective recovery was considered along with dewarping. In [17], after a word and line detection stage, text was rectified in two steps: a coarse correction to remove the global distortions of the image and a fine correction to restore the local deformations. The coarse rectification proceeds by warping an area delimited by two curves fitted to the top and bottom text lines of the document, along with the left and right boundaries of the text. Another text detection stage precedes the fine rectification step, in which a baseline is fitted to every word and used to rotate and translate each of them independently in the output image. Liang et al. [18] used the notion of texture flow, where certain patterns in textured surfaces can give a sense of continuous flow. Two texture flow orientations (named major and minor) were found in document images, aligned with the directions of the text line and the vertical strokes respectively. The major texture flow was determined by applying projection profiles locally, where directional filters were used to obtain the minor texture flow. The method differentiates between flat and curved document images, the latter involving not only rectification, but document dewarping. In the case of flat documents, the lines projected by the two texture flow directions converge into vanishing points that were then used to compute the rectification.The methods described above cannot be applied to scene text, since, to find orientation cues, they rely on how text is structured and organized within documents, i.e. as groups of lines. The most relevant work, specifically dealing with 3D scene text recovery, is by Myers et al. [8] whose method deals with individual or isolated text lines found in everyday scenes, particularly outdoors. In that work, images are first segmented and individual lines of text are extracted. The text lines are rotated at various angle increments and horizontal projection profiles for each angle are computed. By measuring the slope on the sides of the projection profile, top and bottom angles can be estimated, allowing for the estimation of the horizontal vanishing point and a partial rectification of the text by removing the horizontal foreshortening.As expressed earlier, correcting shear and vertical foreshortening is a challenging problem due to the difficulty of obtaining accurate vertical cues for text — even more so when only one text line is being considered. Myers et al.'s [8] view of this is that a weak perspective deformation is expected in the vertical axis on natural scenes, as cameras are usually oriented closely to the horizontal and, in the real-world, text is laid out on vertical surfaces. Therefore, assuming that the vertical vanishing point lies at infinity, they estimate a single shear angle for the whole line by also employing vertical projection profiles. However, they also acknowledge that, when the perspective distortion is significant, their method of correcting shear produces (after rectification) a line of text where the vertical strokes vary in angle with respect to their horizontal position. This is more apparent when images obtained with hand-held or wearable cameras are considered, since the camera could be pointing to text at more extreme orientations. Furthermore, in Myers et al. [8], a large number of possible shear angles within an interval have to be evaluated, which involves a whole image transformation and the computation of a projection profile for each angle. This makes their method inefficient, or if the number of evaluated angles is reduced, inaccurate.Several systems have been proposed where only an affine rectification of text was performed. In the work by Chen et al. [19], text was segmented using an edge detector combined with a Gaussian mixture model (GMM) to separate the text from the background. Characters were grouped together by means of a similarity function and a Hough transform on the character's center points was used to detect linear distribution patterns. A minimum area rectangle was then fitted around each character, aligned with the main direction of the character's group. The most salient rectangle of each group – selected as the one with maximum average edge intensity inside the rectangle – was used to compute two (top and bottom) parallel lines for the group. Two additional parallel lines were also computed using the left- and right-most character rectangles. With these lines an affine rectification of the text group was then computed. Yamaguchi et al. [20] employed a two step rectification for recognizing digits in natural scenes. They made the assumption that the text plane is close to a fronto-parallel view from the camera, thus considering that the vanishing points are far away or close to infinity. Under these conditions, the text was rectified by applying two affine transformations in sequence: one to remove the skew (or in-plane rotation) and a second to remove the slant (or shear). The skew angle was obtained with a modified Hough transform on the center points of each character. Then, rotated minimum area rectangles were circumscribed to each character to obtain an average slant angle for the whole line. Therefore, as a true projective transformation was not being performed on either of these methods, they will also produce incorrect rectifications when significant perspective distortions are in play.A completely different approach was employed by Li and Tan [5] by recognizing characters with perspective distortion directly. This technique was aimed at recognizing symbols (e.g. single characters, traffic signs, logos). For this purpose they introduced an image descriptor which is invariant to projective transformations. The authors demonstrated the increased recognition accuracy of their method over state-of-the-art image matching algorithms for symbols with severe perspective deformations. However, when considered for scene text recognition, this approach lacks all the technical advances that current OCR engines apply besides character recognition: joining or splitting of components to form characters, text line and word formation, statistical dictionary search, etc. If these techniques were to be adapted and applied directly on the unrectified image, they would certainly benefit from having an estimation of the true orientation of the text line.In this work we perform a full perspective rectification of the text, relying only on the geometry of the characters themselves. We do not assume the vertical vanishing point to lie at infinity, thus allowing for bigger variations of shear within the line of text, and our method deals efficiently and accurately with a larger range of view angles.

@&#CONCLUSIONS@&#
