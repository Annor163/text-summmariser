@&#MAIN-TITLE@&#
Texture classification and discrimination for region-based image retrieval

@&#HIGHLIGHTS@&#
The fusion of the Gabor and curvelet filters is used for texture representation.Rotation invariant texture features are obtained by rearranging the sub-bands.Encoding the sub-bands’ information by polynomial makes unique vectors.The classification rate is increased while the texture feature space is reduced.

@&#KEYPHRASES@&#
Region-based image retrieval,Texture feature extraction,Texture classification,Gabor wavelet,Curvelet filters,Polynomials,ImageCLEF,Outex,

@&#ABSTRACT@&#
In RBIR, texture features are crucial in determining the class a region belongs to since they can overcome the limitations of color and shape features. Two robust approaches to model texture features are Gabor and curvelet features. Although both features are close to human visual perception, sufficient information needs to be extracted from their sub-bands for effective texture classification. Moreover, shape irregularity can be a problem since Gabor and curvelet transforms can only be applied on the regular shapes. In this paper, we propose an approach that uses both the Gabor wavelet and the curvelet transforms on the transferred regular shapes of the image regions. We also apply a fitting method to encode the sub-bands’ information in the polynomial coefficients to create a texture feature vector with the maximum power of discrimination. Experiments on texture classification task with ImageCLEF and Outex databases demonstrate the effectiveness of the proposed approach.

@&#INTRODUCTION@&#
The rapid growth of image data on the internet has spurred the demand for methods and tools for efficient search and retrieval. Although many researches have been done in the field of image search and retrieval, there are still many challenging problems to be solved. As the semantic gap is considered to be the main issue, recent works have focused on semantic-based image retrieval. Most of the proposed approaches learn image semantics by extracting low-level features from entire image. However, such approaches fail to take into consideration the semantic concepts that occur in the images. In this paper, we focus on the high-level semantic identification at the region level. This is because analyzing the visual features included in the images gives more intuition about images. By learning these features at the region level, high-level semantics of images can be built. The approaches in which region information is employed to extract semantic concepts of images are known as region-based image retrieval or RBIR [1–3].One issue in semantic understanding of image regions in RBIR is the extraction of effective and discriminatory features. Many researches have been done in global image features extraction and representation, but not much attention has been paid to region-based features [2–6]. Ideally, the extracted features must match the human perceptions of images. Most image retrieval systems apply three well-known color, shape and texture features. Color is the most common feature since it is invariant to distortion and scale. Although color feature is well-defined and widely used in image retrieval systems, it is unable to distinguish between different objects with the same color. Shape feature is not as important as other features in RBIR as regions’ shapes are more vulnerable than regions’ color and texture features [2,7].Texture is an important determinant of region class in RBIR due to its capability to distinguish regions with similar colors and shapes. Although texture feature is very useful in RBIR, they are difficult to model. Ideally, a texture feature of an object should be consistent with human perceptual intuitions of the object, like directional/chaotic and smooth/rough [8].Basically, texture analysis has four main categories: (1) texture feature, (2) texture discrimination, (3) texture classification, and (4) shape from texture. In this paper, we consider only texture discrimination and classification. Many different methods have been proposed for texture feature extraction. In general, these methods are categorized into spatial and spectral approaches [9,10].The spatial approaches are further classified into structural, statistical and model-based approaches. In structural approaches such as Voronoi tessellation [11], texture feature is described using a set of texture primitives and their placement rules. Statistical texture features are usually based on low level statistics of grey level co-occurrence matrices (GLCM) [12,13]. Although these features are compact and robust, they are insufficient to describe a large variety of textures. Model-based approaches such as Markov random fields (MRF) [14] and fractal dimensions (FD) [15] apply stochastic (random) or generative models to describe texture features. As these models fall into an optimization problem, they usually need complex computations [16].In spectral methods such as discrete cosine transform [18], Fourier transform [19], wavelet filters [20], Gabor [21] and curvelet features [22], texture images are transformed into the frequency domain using a set of spatial filters. Then, the statistics of the spectral information at different scales and orientations form the texture descriptor. Due to the large neighborhood support of the filters, spectral methods can generate sufficient number of features to classify variety of texture images.However, the varying rotations of real-world textures suggest the need for rotation–invariant methods. Among the many spatial methods that can be considered, LBP [17] is the most widely used. It combines structural and statistical approaches by computing the occurrence histogram for rotation–invariant texture classification. In LBP, the values of neighboring pixels are turned into binary values using the central pixel as the threshold. This local binary grayscale information is encoded to characterize a structural pattern. Although rotation invariance is achieved by only selecting rotation–invariant uniform local binary patterns, it is not scale-invariant. The LBP-based approaches often also fail in detecting large-scale textural structures. Many Gabor- and wavelet-based algorithms were also proposed for rotation–invariant texture classification [23–25]. Han and Ma [25] proposed to create texture features from a rotation–invariant and a scale-invariant Gabor representation by summations of the conventional Gabor filters. Recently, [2,26] presented a circular shifting of the curvelet texture features to generate rotation–invariant texture representations.However, both the Gabor wavelet and the curvelet filters capture a large volume of unnecessary information which reduces their distinguishing power in texture classification. To overcome this issue, sub-band coefficients are produced in multiple orientations and scales and analyzed in the pre-processing step. In some earlier works, generalized Gaussian density was used to model wavelet coefficients [20,27,28]. In most of the texture extraction methods, texture feature vectors consist of statistical information, which are calculated from all sub-bands generated by applying either Gabor or curvelet transforms on a given image [23,29,30]. Zhang et al. [2] used mean and standard deviation to create a texture feature vector from curvelet sub-bands for each image region. Although both Gabor and curvelet transforms represent image texture features sufficiently by sub-band coefficients, using mean and standard deviation and other statistical information can lead to misclassifications as the discrimination power reduces. For instance, each image pair in Fig. 1shows two different textures being classified into the same group as they have similar or slightly different means and standard deviations.Mohamadzadeh and Farsi [7] used down-sampling to create a reduced size feature vector. However, random down-sampling increases the risk of losing the key information in the respective sub-band.To overcome these problems, we propose the application of polynomial coefficients that are unique in representing data points. It is a combination of sub-band coefficients computed by the Gabor wavelet and curvelet transforms. Using polynomial coefficients can therefore classify similar textures in the same group which leads to effective discrimination of different textures when both Gabor and curvelet features are combined. The block diagram of the proposed texture classification method is shown in Fig. 2.Experiments show that the proposed method performs well in comparison to the several other methods, namely that of Zhang et al.’s [2], LBP [17] and systems that only use the Gabor wavelet or curvelet features.The rest of the paper is organized as follows. In Section 2, we describe the basic concepts required for region-based texture classification. In Section 3, we propose a method to combine both Gabor wavelet and curvelet transforms and create texture feature vector by polynomial coefficients. In Section 4, the evaluations and comparisons are presented. Finally, Section 5 summarizes and concludes the paper.In this section, the definitions of Gabor wavelet, curvelet transform and polynomial function required for region-based texture classification are described.The Gabor transform is a type of wavelet filter. According to [14], a two dimensional Gabor function g(x,y) can be written as:(1)g(x,y)=12πσxσyexp-21x2σx2+y2σy2+2πjWx,where W is the modulation frequency. The 2-D Fourier transform which is the frequency response of the Gabor function is then defined as:(2)G(u,v)=exp-21(u-w)2σu2+v2σv2,where σu=(2πσx)−1 and σv=(2πσy)−1. Let g(x,y) be the mother Gabor wavelet, then a set of self-similar filter functions can be generated by appropriate dilations and rotations of g(x,y) through the following generating function:(3)gmn(x,y)=a-mg(x′,y′),where x′=a−m(xcosθ+ysinθ),y′=a−m(−xsinθ+ycosθ), a>1,θ=nπ/N, and n=0,1,2,…,N−1 and m=0,1,2,…,M−1 are the orientation and the scale of the Gabor wavelet, respectively. Accordingly, for a given image region I(x,y) with the size X×Y, its discrete Gabor wavelet transform is computed as:(4)Gmn(x,y)=∑i∑jI(i,j)gmn∗(x-i,y-j),where ∗ indicates the complex conjugate of gmn. The Gabor wavelet filter is then applied on the image region in different orientations and scales to obtain an array of magnitudes:(5)E(m,n)=∑x∑y|Gmn(x,y)|,These magnitudes are the sub-bands which represent the energy contents at several scales and orientations for a given region. It has been proven that they are descriptive image texture features similar to how human vision is invariant to scale and orientation.A curvelet transform is an extension of the ridgelet transform. The ridgelet differs from other wavelet transforms in which it detects lines instead of points. According to [31], a continuous ridgelet transform at scale a, translation b, and orientation θ is given by:(6)CRTf(a,b,θ)=∬ψa,b,θ(x,y)I(x,y)dxdy,where I(x,y) is an image region, and ψ is the ridgelet function which is defined as:(7)ψa,b,θ(x,y)=a-1/2ψxcosθ+ysinθ-ba.Curvelet feature extraction function for digital image region I[x,y] can then be computed as:(8)CTa,b,θD=∑0⩽x⩽X∑0⩽y⩽YI[x,y]ψa,b,θD[x,y].As explained in [32], this formula can be extended in the frequency domain and defined as:(9)CTa,b,θD=IFFT(FFT(I[x,y])×FFT(ψa,b,θD[x,y])).The curvelet sub-bands for every region are created by adjusting the ridgelets to different scales and orientations. Then, the energy of each curvelet sub-band can be computed as:(10)E(a,θ)=∑x∑y|Sba,θ(x,y)|.where Sba,θis the sub-band at scale a and orientation θ.Though Gabor filters are similar in this way, they do not cover entire frequency spectrums due to their oval shape. Fig. 3(a) represents the holes between ovals in the frequency plane of Gabor wavelet. The complete coverage of the frequency spectrum by curvelet can be observed in Fig. 3(b). In this figure, sistands for scale i, and the numbers 1,2,…,etc. show the orientations or sub-bands.Although curvelet can cover spectral domain completely due to the wedge shape of its frequency response, the Gabor wavelets are very effective in representing objects with isolated point singularities. Thus, Gabor wavelets and curvelets are mutually complementary. Recently, the combination of wavelet and curvelet transforms has been reported in some applications for image denoising and feature extraction [33–36], which their findings suggest the potential of the combination. Thus, we apply both methods to create texture feature vectors for the best classification accuracy.Polynomial functions have flexible shapes, and changes of location and scale in the raw data result in the same polynomial model. That is, the underlying metrics do not influence the polynomial functions. They are frequently applied as an empirical method for curve and surface fitting, so they can encode information about some other objects. A polynomial function is a data set of n paired (x,y) members such as (x1,y1),(x2,y2),(x3,y3),…,(xn,yn), which are evaluated using a least-squares technique to generate a predictive polynomial equation y as illustrated in the following:(11)y=a0+a1x+a2x2+a3x3…apxp,p<n,where p stands for the degree of the polynomial, which is a non-negative integer. In this equation, a0,a1…apare the polynomial coefficients, which are constants. These unknown coefficients are calculated by minimizing the sum of the deviations’ squares of the data from the model, which is called least-squares fit. This mathematical procedure is described in detail in [37].Multivariate polynomial functions can also be defined. Polynomials in two variables are algebraic expressions taking the form of axnyminstead of axn. The degree of these polynomials is the largest sum of the exponents in all terms.In this paper, we use a polynomial function with two variables to encode each Gabor or curvelet sub-band in the spectral domain of an image region texture. The following equation has been created for a curvelet transform sub-band. The constants p00,p10,p01,p20,p11,p02,p30,p21,p12 and p03 are coefficients which properly code the information in the respective sub-band:(12)f(x,y)=p00+p10x+p01y+p20x2+p11xy+p02y2+p30x3+p21x2y+p12xy2+p03y3.Fig. 4illustrates a texture and its corresponding curvelet sub-band which has been fitted by a polynomial function.Prior to texture classification task, images in the training set are segmented into regions using the JSEG in [38]. Since image regions are irregular, they must be transferred to a regular shape in order to apply Gabor wavelet and curvelet transforms.An irregular shaped region is transformed into an appropriate regular shaped region for texture extraction by finding either the largest internal square or the bounding box. Due to the size variation in the regions, the largest internal square of some regions may not include enough texture information. Although a bounding box is large enough, but it always includes non-region pixels. The accuracy of the extracted texture features from a bounding box highly depends on the values of these pixels. The common approach to fill these regions is via ‘zero-padding’, where non-region pixels are filled by zeros. Nevertheless, differences between zero padded regions and the original region result in false and inaccurate texture information. Recently, Zhang et al. [2] proposed a mirror padding method to fill non-region areas in the bounding box with the mirrored textures inside the region. Although this method outperforms zero padding, it adds false information of the region edges to the real texture of the region. In contrast, an internal square consists of only valid region-pixels. Thus the features extracted from an internal square are more accurate than the features extracted from a bounding box. Thus we propose an improved version of the mirror padding which extract both real and mirrored textures from the largest internal square instead of the bounding box. It also avoids mirror padding when the size of the largest internal square is large enough to carry texture information of the region. Fig. 5shows the wrong reflected region edges using mirror padding method, which have been removed in the improved mirror padding method.Once all the training image regions are transformed into the regular shapes using the improved mirror padding method, they are categorized into predefined concept classes according to their textures. For a particular concept with a distinct texture, multiple regular shaped regions are gathered to create an image region class. In every class, Gabor and curvelet filters are applied on each region to obtain texture features in different orientations and scales.Our proposed algorithm employs 40 Gabor filters in five scales and eight orientations as shown in Fig. 6(b) where we illustrate the texture detection steps along with the image region after applying Gabor filters. We also apply curvelet decomposition on the image regions in four levels. Therefore, 50(=1+16+32+1) different sub-bands of curvelet coefficients are calculated. Fig. 6(c) shows the texture representation of an image region using curvelet transform. Furthermore, a description of image region texture is obtained with 90 different sub-bands.However, although these sub-bands comprehensively describe the textural structures, they are not reliable enough when the image is rotated. Recently, some works proposed to overcome this limitation using the curvelet rotation shifting property [2], or alternatively, generating a dominant orientation by summation over all the sub-bands at different scales, and then circular shifting [26]. In this work, given that the energy of the dominant orientation usually spreads between two neighboring sub-bands, different sub-bands at each scale are rearranged based on their energies. This makes a unique sub-band set for each texture, even though it is rotated, and therefore, encoding a texture at different rotations generates similar coefficients. More specifically, although both the Gabor and the curvelet coefficients reflect the effect of the rotation at different scales, the rotation of the texture cannot be captured by only considering these coefficients. However, there is a dominant orientation at each scale of decomposition in the Fourier spectrum which is detectable by finding the highest energy sub-band. Consequently, rearranging the other sub-bands makes a relatively unique set of coefficients for each texture even though it is rotated. Using Eqs. (5) and (10), the highest energy sub-bands can be detected.Fig. 7shows a texture image at different rotation angles of 0°, 15°, and 30°, which their sub-band energies are listed in Table 1. It is observable that these energies are concentrated at the dominant values and they are circularly shifted when the texture is rotated. Rearranging the respective sub-bands and fitting them by polynomials (using Eq. (12)) result in a unique feature vector consisting of the polynomial coefficients as shown in Table 2. To encode the rearranged coefficients using polynomial fitting, first we obtain the degree of the bivariate polynomial that best fits the data. From our preliminary experiments, this varies between 3° and 6° for different sub-bands. We finally choose the value 6 so that the feature vectors with the same size are created for all regions and sub-bands.Using polynomials to encode texture information brings several benefits in texture classification and discrimination. First, same textures have the same coefficients after fitting the polynomials. It is also observed that the sub-bands obtained from the Gabor and curvelet transforms of various textures in the same class can be coded into similar codewords with the coefficients of their fitted polynomials. Fig. 8illustrates two different textures for the class ‘grass’ with very similar coefficients of their fitted curvelet sub-band polynomials. The higher discrimination power is another benefit of using polynomials. It is described by the Fisher’s linear discriminant or FLD which is a widely used discrimination criterion that measures the “between-class scatter” normalized by the “within-class scatter” [39]. It tries to maximize the scatter ratio between classes to the scatter within classes. Let us assume we have sets Di, i=1,2,3…,c which represent c classes, each containing nielements that |Di|=ni. The scatter within classes is given by:(13)SW=∑SDi,whereSDiis the sum of all classes scatters and is defined as:(14)SDi=∑x∈Di(x-mDi)(x-mDi)T,whichmDiis the mean vector for class Di. The total mean that is in fact the centers of gravity of all classes means is defined as:(15)m=1N∑i=1cnimDi.The scatter between classes is also computed by:(16)SB=∑i=1cni(mDi-m)(mDi-m)T.And finally, we define the discrimination as following:(17)Dis=SBSW.As a result, the higher values of this metric show lower scatter within classes and higher scatter between classes, which result in a better discrimination. This leads to compact and well-separated clusters.In the next step, all texture features in each training texture class are used to create the representative feature vector for that class, by averaging all vectors in the texture category. We then build up a texture dictionary from these feature vectors where every entry includes a codeword to represent a particular texture. A texture dictionary is a set of codewords which are representative texture features. The number of entries in the texture dictionary demonstrates the number of texture classes. In other words, every region texture class is coded into a codeword in the texture dictionary. These codewords are generated using the vector quantization method which discretizes texture feature vectors and computes the centroid of each cluster by averaging the cluster of textures [40].Once the texture dictionary is created, a mapping between a semantic concept and codewords from the dictionary needs to be established. Therefore we applied a k-nearest neighbor (k−NN) classifier based on the proposed classification method in [41] to perform classification step. This classifier projects the extracted region texture to the appropriate texture class according to the texture descriptors of the regions. To make predictions with k−NN, we measure the distance between the texture vector and the codewords using the Euclidean distance. We find the index of the closest representative feature vector from the texture dictionary by:(18)ind=argmin(dist(v,vi)),where i is an integer between 1 and the dictionary size, and dist(v,vi) is the Euclidean distance between the texture vector and the codewords from the texture dictionary.

@&#CONCLUSIONS@&#
