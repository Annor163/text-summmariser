@&#MAIN-TITLE@&#
Solving the minimum labelling spanning tree problem by intelligent optimization

@&#HIGHLIGHTS@&#
Minimum labelling spanning tree problem on graphs and networks.Solution approach for the problem using intelligent optimization concepts.Hybrid metaheuristic with probability-based local search and automated parameters setting.Computational experiments on randomly generated graphs and statistical analysis of results.State-of-the-art solutions for the minimum labelling spanning tree problem.

@&#KEYPHRASES@&#
Combinatorial optimization,Graphs and networks,Minimum labelling spanning trees,Intelligent optimization,Hybrid methods,Variable neighbourhood search,

@&#ABSTRACT@&#
Research on intelligent optimization is concerned with developing algorithms in which the optimization process is guided by an “intelligent agent”, whose role is to deal with algorithmic issues such as parameters tuning, adaptation, and combination of different existing optimization techniques, with the aim of improving the efficiency and robustness of the optimization process. This paper proposes an intelligent optimization approach to solve the minimum labelling spanning tree (MLST) problem. The MLST problem is a combinatorial optimization problem where, given a connected, undirected graph whose edges are labelled (or coloured), the aim is to find a spanning tree whose edges have the smallest number of distinct labels (or colours). In recent work, the MLST problem has been shown to be NP-hard and some effective metaheuristics have been proposed and analysed. The intelligent optimization algorithm proposed here integrates the basic variable neighbourhood search heuristic with other complementary approaches from machine learning, statistics and experimental soft computing, in order to produce high-quality performance and to completely automate the resulting optimization strategy. We present experimental results on randomly generated graphs with different statistical properties, and demonstrate the implementation, the robustness, and the empirical scalability of our intelligent local search. Our computational experiments show that the proposed strategy outperforms heuristics recommended in the literature and is able to obtain high quality solutions quickly.

@&#INTRODUCTION@&#
Today a wide range of metaheuristic methods for the solution of relevant combinatorial problems have steadily gained success. The practical challenges that the Operations Research community needs to face for the design of heuristic solution strategies are technical and scientific issues regarding the efficient tuning, adaptation, combination and hybridization of the different existing techniques [1,2]. The potential in terms of efficiency or robustness of the obtained metaheuristics is large, but the task is also quite complex. First, the performance of these algorithms depends on a number of components and parameters which need to be tuned by the user through a lengthy trial and error process every time the algorithm has to face different instances of the considered problems [3]. Second, the scientific intent consists also in comprehending the contribution of the different components with respect to the whole algorithm and at discerning the basic principles for achieving successful metaheuristics [1,2]. Consequently, there is a great interest in developing intelligent optimization algorithms which make use of mechanisms from machine learning, statistics and experimental soft computing, integrate exact techniques of mathematical programming, hybridize existing metaheuristics, in order to produce effective optimization strategies with high-quality performance and with completely automated parameters tuning processes [4,5,1]. In particular, the present paper considers probability-based components within self-tuned local search to solve the minimum labelling spanning tree (MLST) problem with state-of-the-art results. These algorithmic components allows the local search to achieve a proper balance of diversification (exploration) and intensification (exploitation) during the search process, a fundamental objective for any effective heuristic solution approach. The diversification capability of a metaheuristic refers to its aptitude of exploring thoroughly different zones of the search space in order to identify promising areas. When a promising area is detected, the metaheuristic needs to exploit it intensively to find the relative local-optimum, but at the same time without wasting excessive computational resources. This is referred as the intensification capability of the metaheuristic. Finding a good balance between diversification and intensification is indeed an essential task for the proper effectiveness of a metaheuristic [6,7,2].In the MLST problem we are given an undirected, labelled (or coloured) graph as input, with a label assigned to one or more edges, but with each edge having only one label allocated, and the aim is to find a spanning tree of the graph having the minimum overall number of labels [8]. The MLST problem has many real-world applications in different fields, such as in data compression [9], telecommunications network design [10], and multimodal transportation systems [11]. For example, in multimodal transportation systems there are often circumstances where it is needed to guarantee a complete service between the terminal nodes of the network by using the minimum number of provider companies [12]. This situation can be modelled as a MLST problem, where each edge of the input graph is assigned a label, denoting a different company managing that link, and one wants to obtain a spanning tree of the network using the minimum number of labels. This spanning tree will reduce the construction cost and the overall complexity of the network. A practical example in this context is given by multimodal transportation networks of large territories, from regions to states, or even continents, during humanitarian crisis events like, for example, volcanic eruptions, terrorist threats, floods, tsunamis, etc [13]. In these very delicate crisis management situations, amongst different types of human intervention, it is also necessary to reorganize dynamically the entire transportation network of the damaged area, taking into account the upcoming inaccessible or forbidden zones, and guaranteeing a minimal working transport service among main cities, hospitals, airports, principal way outs, and others, with the minimum number of different transportation carriers and companies.It is possible to express the MLST problem in a more formal way as a network or graph problem as follows [14]:Definition 1.1Minimum labelling spanning tree problem:GIVEN: A labelled connected undirected graph G=(V, E, L), where V is the set of nodes, E is the set of edges, and L is the set of labels.GOAL: Find a spanning tree T of G such that min|LT|, where LTis the set of labels used in T.The left graph of Fig. 1is an example of an input graph, whose MLST solution is shown on the right.It has been demonstrated by Xiong et al. [15] that any spanning tree of a feasible optimal solution for the MLST problem is a minimum labelling spanning tree. A feasible solution is defined as a set of labels, C⊆L, such that all edges with labels in C represent a connected subgraph of G and span all the nodes in G. If C is a feasible solution, then any spanning tree of C has at most |C| labels. Therefore to solve the MLST problem, it is easier to get firstly a feasible solution with the least number of labels, and then to use any polynomial time algorithm already known in the literature to extract from the obtained feasible solution a spanning tree with the minimum number of labels [15].The structure of the paper is as follows. In Section 2 the MLST algorithms in the literature are reviewed. In particular this section will give the details of an exact method [14], and those of the heuristics recommended in the literature [14]: greedy randomized adaptive search procedure (GRASP) and variable neighbourhood search (VNS). Section 3 describes the intelligent algorithm that we propose, which derives from the basic VNS heuristic and is extended by other complementary approaches in order to improve the effectiveness and robustness of the optimization process. Section 4 contains a computational analysis and statistical evaluation of the results. Finally, our conclusions are described in Section 5. For a survey on the basic concepts of metaheuristics and combinatorial optimization, the reader is referred to [6,16,7,1].Chang and Leu [8] first introduced the MLST problem, along with the proof of its NP-hard complexity. They also presented the Maximum Vertex Covering Algorithm (MVCA), a polynomial time heuristic for the problem successively refined by Krumke and Wirth [17]. Starting from an empty graph, MVCA iteratively adds at random unused labels to the partial solution, by greedily minimizing the number of connected components at each step. The procedure continues until only one connected component is left, i.e. when only a connected subgraph is obtained.Krumke and Wirth [17] also proved that MVCA yields a solution with a value no greater than (1+2logn) times optimal, where n is the total number of nodes. Later, Wan et al. [18] obtained a better bound for the greedy algorithm introduced by Krumke and Wirth [17]. The algorithm was shown to be a (1+log(n−1))-approximation for any graph with n nodes (n>1).Brüggemann et al. [19] used a different approach; they applied local search techniques based on the concept of j-switch neighbourhoods to a restricted version of the MLST problem. In addition, they proved a number of complexity results and showed that if each label appears at most twice in the input graph, the MLST problem is solvable in polynomial time.Xiong et al. [20] derived tighter bounds than those proposed by Wan et al. [18]. For any graph with label frequency bounded by b, they showed that the worst-case bound of MVCA is the bth-harmonic number Hbthat is:Hb=∑i=1b(1/i)=1+(1/2)+(1/3)+⋯+(1/b).Subsequently, they constructed a worst-case family of graphs such that the MVCA solution is Hbtimes the optimal solution. Since Hb<(1+log(n−1)) and b≤(n−1) (since otherwise the subgraph induced by the labels of maximum frequency contains a cycle and one can safely remove edges from the cycle), the tight bound Hbobtained is, therefore, an improvement on the previously known performance bound of (1+log(n−1)) given by Wan et al. [18].The usual rule of Krumke and Wirth [17] to select the label that minimizes the total number of connected components at each step, results in fast and good quality solutions. However, a difficulty arises when more than one label with same resulting minimum number of connected components is detected in a specific step. Since there may be many labels with this minimum number of connected components, the results mainly depend on the rule chosen to select a candidate from this set of ties. If the initial label from this set is chosen, the results are affected by the sorting of the labels. Therefore, different executions of the algorithm may result in different solutions, with a slightly different number of labels.Several other heuristic approaches to the MLST problem have been proposed in the literature. For example, Xiong et al.[15] presented a Genetic Algorithm outperforming MVCA in most cases. Subsequently, Cerulli et al. [21] applied to the MLST problem the Pilot Method, which is a greedy heuristic developed by Voß et al. [22]. Considering different sets of instances of the MLST problem, Cerulli et al. [21] compared this method with other metaheuristics: Tabu Search, Simulated Annealing, and a variable neighbourhood search attempt. Their Pilot Method obtained the best results in most of the cases. It generated high-quality solutions to the MLST problem, but running times were quite large.Xiong et al. [23] implemented modified versions of MVCA focusing on the initial label added. For example, after the labels were sorted according to their frequencies, from highest to lowest, the first modified version tried only the most promising 10% of the labels at the initial step. Afterwards, it run MVCA to determine the remaining labels and then it selected the best of the |L|/10 resulting solutions. Despite the potential of this approach in terms of computational running time, it did not perform as well as the Pilot Method with respect to solution quality, since a higher frequency label may not always be the best place to start. Another modified version by Xiong et al. [23] was similar to the previous one, except that it tried the most promising 30% of the labels at the initial step. Then it run MVCA to determine the remaining labels. Moreover, Xiong et al. [23] proposed another way to modify MVCA. They considered at each step the three most promising labels, and assigned a different probability of selection that was proportional to their frequencies. Then, they randomly selected one of these candidates, and added it to the incomplete solution. In addition, Xiong et al. [23] presented a Modified Genetic Algorithm that was shown to have the best performance for the MLST problem in terms of solution quality and running time.Successively some heuristics based on greedy randomized adaptive search procedure and variable neighbourhood search were proposed by Consoli et al. [14]. A comparison with the results provided by the other MLST heuristics in the literature showed that these methods were able to obtain high-quality solutions in short computational time, resulting in the best performing metaheuristics to date for the MLST problem. Furthermore, Chwatal and Raidl [24] implemented an Ant Colony Optimization algorithm for the problem. This seems to be an interesting alternative although further study is needed to get a performance comparable to that of the best MLST algorithms in the literature. In addition, recently the same authors in [25] presented some exact mixed-integer programming methods, including branch-and-cut and branch-and-cut-and-price, which were able to find exact solutions to small instances of the MLST problem.In the following an exact method for the problem is described [14]. It will be useful to obtain exact solutions to small instances of the MLST problem and to produce practical benchmarks for the analysis of the considered metaheuristics. In addition the details of the best MLST heuristics in the literature, i.e. greedy randomized adaptive search procedure and variable neighbourhood search, will follow.This exact approach for the MLST problem is based on an A* or backtracking procedure to test the subsets of L. It performs a branch and prune procedure in the partial solution space based on a recursive procedure that attempts to find a better solution from the current incomplete solution [14]. The main program that solves the MLST problem calls the recursive procedure with an empty set of labels, and iteratively stores the best solution to date, say C*. In order to reduce the number of test sets, it is more convenient to use a good approximate solution for C* in the initial step, instead of considering all the labels. Note also that if we are evaluating an incomplete solution C′ with a number of labels |C′|=|C*|−2, we should try to add the labels one by one to check if it is possible to find a better solution for C* with a smaller cardinality, that is |C′|=|C*|−1. To complete this solution C′, we need to add a label with a frequency at least equal to the current number of connected components minus 1. If this requirement is not satisfied, the incomplete solution can be rejected, speeding up the whole search process [14]. Another improvement that avoids the examination of a large number of incomplete solutions consists of rejecting every incomplete solution that cannot result in a single connected component. The running time of this exact method grows exponentially, but if either the problem size is small or the optimal objective function value is small, the running time is “acceptable” and the method obtains the exact solution [14]. The complexity of the instances increases with the dimension of the graph (number of nodes and labels), and the reduction in the density of the graph.Greedy randomized adaptive search procedure (GRASP) is a recently exploited method combining the power of greedy heuristics, randomization, and local search [26]. It is a multi-start two-phase metaheuristic consisting of a “construction phase” and a “local search phase”. The construction phase is aimed at building an initial solution using a greedy randomized procedure, whose randomness allows solutions in different areas of the solution space to be obtained. Each solution is randomly produced step-by-step by uniformly adding one new element from a candidate list (RCLα: restricted candidate list of length α) to the current solution. Subsequently, the local search phase is applied to try to improve the current best solution. This two-phase process is iterative and halts when the user termination condition is satisfied [26].The GRASP implementation for the MLST problem proposed in [14] is specified in the following. The greedy criterion of the construction phase of GRASP is based on the number of connected components produced by the labels, and a value-based restricted candidate list is used [26]. This involves placing in the list only the candidate labels having a greedy value (number of connected components) not greater than a user-defined threshold, α, whose values can vary dynamically during the search process. The value of the threshold α and its tuning during the iterations of the algorithm need to be chosen in an appropriate way. Indeed, a small value of α results in few labels in the restricted candidate list, giving a large intensification capability and a small diversification capability. This means that the resulting algorithm is very fast, but it can easily become trapped at a local optimum. Conversely, a large value of α produces an algorithm with a large diversification capability, but a short intensification capability, because many candidate labels are included in the restricted candidate list. In this implementation for the MLST problem, in order to get an adequate trade-off between intensification and diversification capabilities with respect to the set of problem instances considered, [14] used the following scheme. In order to fill the restricted candidate list, the threshold is set equal to the minimum number of connected components produced by the candidate labels. This means that only the labels producing the least number of connected components constitute the restricted candidate list. Furthermore, after two iterations, complete randomization is used to choose the initial label to add. This corresponds to setting the threshold to +∞, i.e. including all the labels of the graph within the restricted candidate list (length α = total number of labels, ℓ). To intensify the search for the remaining labels to add, the list is filled considering only the labels leading to the minimum total number of connected components, as in the previous iterations [14].At the end of the construction phase, a local search phase is included. It simply consists of trying to drop some labels, one by one, from the current solution C whilst retaining feasibility. Local search gives a further improvement to the intensification factor of the algorithm. The entire procedure is repeated until the user termination conditions are satisfied.Variable neighbourhood search (VNS) is an explorative metaheuristic for combinatorial optimization problems based on dynamic changes of the neighbourhood structure during the search process [27]. The guiding principle of VNS is that a local optimum with respect to a given neighbourhood may not be locally optimal with respect to another neighbourhood. Therefore VNS looks for new solutions in increasingly distant neighbourhoods of the current solution, jumping only if a better solution than the current best solution is found [27]. The process of changing neighbourhoods when no improvement occurs is aimed at producing a progressive diversification.Algorithm 1VNS for the MLST problemInput: A labelled, undirected, connected graph G=(V, E, L) with n vertices, m edges, ℓ labels;Output: A spanning tree T;Initialisation:- Let C←0 be the global set of used labels;- Let H=(V, E(C)) be the subgraph of G restricted to V and edges with labels in C, where E(C)={e∈E:L(e)∈C};- Let C′ be a set of labels;-Let H′=(V, E(C′)) be the subgraph of G restricted to V and edges with labels in C′, where E(C′)={e∈E:L(e)∈C′};- Let Comp(C′) be the number of connected components of H′=(V, E(C′));beginC←Generate-Initial-Solution-At-Random()repeatSet k←1 and kmax←(|C|+|C|/3)whilek<kmaxdoC′←Shaking phase(Nk(C))Local search(C′)if |C′|<|C|thenMove C←C′Restart with the first neighbour: k←1elseIncrease the size of the neighbourhood structure: k←k+1endenduntiltermination conditionsUpdate H=(V, E(C))⇒ Take any arbitrary spanning tree T of H=(V, E(C)).endAlgorithm 2Procedure Shaking phase(·)Procedure Shaking phase(Nk(C)):Set C′←C;fori←1 to kdoSelect at random a number between 0 and 1: rnd←random[0, 1]ifrnd≤0.5 thenDelete at random a label c′∈C′ from C′, i.e. C′←C′−{c′}elseAdd at random a label c′∈(L−C) to C′, i.e. C′←C′∪{c′}endUpdate H′=(V, E(C′)) and Comp(C′)endThe VNS for the MLST problem implemented by Consoli et al. [14] is described in Algorithm 1. Given a labelled graph G=(V, E, L) with n vertices, m edges, and ℓ labels, each solution is encoded by a binary string [14], i.e. C=(c1, c2, …, cℓ) where(1)ci=1iflabeliisinsolutionC0otherwise(∀i=1,…,ℓ).Denote with Nk(C) the neighbourhood space of the solution C, and with kmaxthe maximum size of the neighbourhood space. In order to impose a neighbourhood structure on the solution space S, comprising all possible solutions, the distance considered between any two such solutions C1, C2∈S, is the Hamming distance [14]:(2)ρ(C1,C2)=|C1−C2|=∑i=1ℓλiwhere λi=1 if label i is included in one of the solutions but not in the other, and 0 otherwise, ∀i=1, …, ℓ. Then, given a solution C, its k-th neighbourhood, Nk(C), is considered as all the different sets having a Hamming distance from C equal to k labels, where k=1, 2, …, kmax, and kmaxis the maximum dimension of the shaking. In a more formal way, the kth neighbourhood of a solution C is defined as Nk(C)={S⊂L:ρ(C, S)=k}, where k=1, …, kmax.Algorithm 1 starts from an initial feasible solution C generated at random and lets parameter k vary during the execution. In the successive shaking phase (Shaking phase(Nk(C)) procedure, see Algorithm 2) a random solution C′ is selected within the neighbourhood Nk(C) of the current solution C. This is done by randomly adding further labels to C, or removing labels from C, until the resulting solution has a Hamming distance equal to k with respect to C[14]. Addition and deletion of labels at this stage have the same probability of being chosen. For this purpose, a random number is selected between 0 and 1 (rnd←random[0, 1]). If this number is smaller than 0.5, the algorithm proceeds with the deletion of a label from C. Otherwise, an additional label is included at random in C from the set of unused labels (L−C). The procedure is iterated until the number of addition/deletion operations is exactly equal to k.Algorithm 3Procedure Local search(·)Procedure Local search(C′):whileComp(C′)>1 do- Let S be the set of unused labels which minimize the number of connected components, i.e. S={e∈(L−C′): min Comp(C′∪{e})}Select at random a label u∈SAdd label u to the set of used labels: C′←C′∪{u}Update H′=(V, E(C′)) and Comp(C′)endfori←1 to |C′|doDelete label i from the set C′, i.e. C′←C′−{i}Update H′=(V, E(C′)) and Comp(C′)ifComp(C′)>1thenAdd label i to the set C′, i.e. C′←C′∪{i}endUpdate H′=(V, E(C′)) and Comp(C′)endThe successive local search (Local search(C′) procedure, see Algorithm 3) consists of two steps [14]. In the first step, since deletion of labels often gives an infeasible incomplete solution, additional labels may be added in order to restore feasibility. In this case, addition of labels follows the MVCA criterion of adding the label with the minimum number of connected components. Note that in case of ties in the minimum number of connected components, a label not yet included in the partial solution is chosen at random within the set of labels producing the minimum number of components (i.e. u∈S where S={e∈(L−C′): min Comp(C′∪{e})}). Then, the second step of the local search tries to delete labels one by one from the specific solution, whilst maintaining feasibility [14].After the local search phase, if no improvements are obtained (|C′|≥|C|), the neighbourhood structure is increased (k←k+1) giving a progressive diversification (|N1(C)|<|N2(C)|<⋯<|Nkmax(C)|). Otherwise, the algorithm moves to the improved solution (C←C′) and sets the first neighbourhood structure (k←1). Then the procedure restarts with the shaking and local search phases, continuing iteratively until the user termination conditions are satisfied.It is worth noting that the performance of VNS is highly influenced by the setting of the kmaxvalue, which needs to be tuned by the user in order to balance properly the intensification and diversification factors of the search process. Choosing a small value for kmaxproduces a high intensification capability and a small diversification capability, resulting in a fast algorithm, but with a high probability of being trapped at a local minimum. Conversely, a large value for kmaxdecreases the intensification capability and increases the diversification capability, resulting in a slower algorithm, but able to escape from local minima. Computational experience by Consoli et al. [14] indicates that for the set of MLST problem instances faced, the value kmax←(|C|+|C|/3) gives a good trade-off between these two factors.The performance of the heuristics developed for the MLST problems by Consoli et al. [14] depends on a number of parameters which are tuned by the user through lengthy trial-and-error processes in relation to the set of instances that they need to solve. Furthermore, these heuristics reveal sometimes a lack of diversification capability as they usually find good-quality for medium-small instances of the problem in relative short computational time, but for large instances of the problem they tend to be generally slow and less robust.Therefore, in this section we consider an intelligent optimization algorithm[28,4] used to attack the MLST problem, trying to obtain a powerful optimization strategy able to solve effectively the problem, and to adapt automatically to the instances class to face without any human intervention from the final user. The intelligent algorithm that we propose here is the result of challenging empirical studies that we have experimented recently [29,30]. In particular, in [29] we developed a preliminary version consisting of a hybrid algorithm for the MLST problem, with results only for small instances (n = 100) since for large instances we detected low performance in terms of computational running time. We found that the algorithm, as the other prior MLST heuristics, was unable to adapt efficiently to changes on the size of the problem instances to face, since its performance depends on a number of parameters which need to be manually tuned every time the algorithm has to face different problem instances. From that hybrid algorithm, in order to improve the efficiency and robustness of the optimization process, we have started to improve the whole approach in order to make it “intelligent”, i.e. trying to have high performance and with a completely automated tuning of the parameters. In [30] we presented an initial attempt for solving the MLST problem with this possibility. In particular this short contribution contains some details on the work-in-progress regarding our intelligent algorithm for the MLST problem, which we tested only with the small problem instances in [29], producing improved results especially with respect to computational running times. These encouraging results provided us the motivations to keep on this direction and further exploring these preliminary studies, by further improving and exploiting the algorithm, providing statistical analysis on the performance and explanations for the success keys, and progressing on the state-of-the-art of the MLST problem being able to solve very large problem instances. The intelligent local search that we propose in this paper enriches, formalizes, and more rigorously tests the early implementations in [29,30].The proposed intelligent optimization algorithm used to attack the MLST problem integrates the variable neighbourhood search metaheuristic with other complementary approaches and self-tuning mechanisms to improve the effectiveness and robustness of the optimization process. The first introduced extension is a further local search mechanism that is included on top of the Variable Neighbourhood Search heuristic. The resulting local search method is referred to as complementary variable neighbourhood search and will be described in Section 3.1. Then, Complementary Variable Neighbourhood Search is improved by replacing the inner local search based on the deterministic MVCA heuristic with a probability-based local search inspired by a “Simulated Annealing cooling schedule” [31], with the view of achieving a proper balance between intensification and diversification capabilities. The strength of this probabilistic local search is tuned by an automated process which allows the intelligent strategy to adapt on-line to the problem instance explored and to react in response to the search algorithm's behaviour [32]. The resulting metaheuristic, summarized in Fig. 2, represents the intelligent optimization algorithm that we propose for the MLST problem, and will be described in detail in Section 3.2.Complementary variable neighbourhood search (COMPL) is a VNS-based local search aimed at increasing the diversification factor of the basic VNS for the MLST problem described in Section 2.3. Given a labelled graph G=(V, E, L) with n vertices, m edges, and ℓ labels, COMPL replaces iteratively each incumbent solution C=(c1, c2, …, cℓ), where, ∀i=1, …, ℓ, ci=1 if label i∈C, ci=0 otherwise, with another solution selected from the complementary space of C.Definition 3.1Complementary space:GIVEN: A solution C of a labelled connected undirected graph G=(V, E, L).DEFINE: Let (L−C) be the complementary space of C, i.e. the set of all the labels that are not contained in C.In COMPL, the iterative process of extraction of a new solution from the complementary space of the current solution helps to escape the algorithm from possible traps in local minima, since the complementary solution produced by COMPL lies in a very different zone of the search space with respect to the incumbent solution. This process yields an immediate peak of diversification capability of the whole local search procedure. To get a complementary space solution, COMPL uses the MVCA as constructive method applied to the subgraph of G with labels in (L−C). Complementary variable neighbourhood search stops if either a final infeasible solution is obtained, i.e. the set of unused colours contained in the complementary space is empty (i.e. (L−C)=∅), or a final feasible solution is produced (i.e. Comp(C)=1). Successively, the same basic variable neighbourhood search described in Section 2.3 is applied to the incumbent solution in order to search for further improvements, therefore increasing the intensification of the whole procedure.In order to illustrate complementary variable neighbourhood search, consider the example shown in Fig. 3. Given an initial random solution X0, the algorithm searches for new solutions in increasingly distant neighbourhoods of X0. In this example, no better solutions are detected, and the current solution is still X0. Now, COMPL extracts a solution from the complementary space of X0, defined as (L−X0). Let the new solution beX0compl. Then, the algorithm searches for new solutions in the neighbourhoods ofX0compl. In this example, a better feasible solution X1 is found. The algorithm continues with this procedure until the termination conditions are satisfied. In the example, the final solution is denoted by X2.As we will see from our computational experience (Section 4), complementary variable neighbourhood search has been compared with the previous algorithms, resulting in good performance. However, in order to seek further improvements and to automate on-line the search process, complementary variable neighbourhood search has been modified by including a probability-based local search with a self-tuning parameters setting, as shown in the next section.The proposed intelligent metaheuristic (INTELL) for the MLST problem is built from complementary variable neighbourhood search, with the insertion of a probability-based local search as constructive method to get the complementary space solutions. In particular, the probability-based local search is a modification of MVCA (see Section 2), obtained by introducing a probabilistic choice on the next label to be added into incomplete solutions. By allowing worse components to be added to incomplete solutions, this probabilistic constructive heuristic produces a further increase on the diversification of the optimization process. The construction criterion is as follows. The procedure starts from an initial solution and iteratively selects at random a candidate move. If this move leads to a solution having a better objective function value than the current solution, then this move is accepted unconditionally; otherwise the move is accepted with a probability that depends on the deterioration, Δ, of the objective function value.This construction criterion takes inspiration from Simulated Annealing (SA) [31]. However, a main difference is that the probabilistic local search works with partial solutions which are iteratively extended with additional components until complete solutions emerge. Our resulting algorithm that combines complementary variable neighbourhood search and this probabilistic local search can be seen in fact as a hybridization between VNS and SA metaheuristics [1,2]. Variable neighbourhood search provides a general framework and today a current trend consists of the integration of good characteristics from one or more metaheuristics within the basic VNS implementation, in order to improve its performance [1,2]. In our problem, a basic VNS implementation which uses the probabilistic local search has been tested. However, the best performance has been obtained by integrating COMPL with the probabilistic local search, resulting in the optimization method explained in the following.Algorithm 4Intelligent optimization algorithm for the MLST problemInput: A labelled, undirected, connected graph G=(V, E, L), with n vertices, m edges, ℓ labels;Output A spanning tree T;Initialization:- Let BestC←0 be the global set of labels;- Let HBEST=(V, E(BestC)) be the subgraph of G restricted to V and edges with labels in BestC, where E(BestC)={e∈E:L(e)∈BestC};- Let C←0 be the set of used labels;- Let H=(V, E(C)) be the subgraph of G restricted to V and edges with labels in C, where E(C)={e∈E:L(e)∈C};- Let Comp(C) be the number of connected components of H=(V, E(C));- Let C′ be a set of labels;- Let H′=(V, E(C′)) be the subgraph of G restricted to V and edges with labels in C′, where E(C′)={e∈E:L(e)∈C′};- Let Comp(C′) be the number of connected components of H′=(V, E(C′));- Let Compl_Space=(L−BestC) the complementary space of the best solution BestC;beginBestC←Generate-Initial-Solution-At-Random()Set kmax←|BestC|repeatExtract a solution from the complementary space of BestC: C←Complementary(BestC)while (|C|<|BestC|) AND (C is a feasible solution) doMove BestC←CExtract another complementary solution: C←Complementary(BestC)endSet k←1whilek<kmaxdoC′←Shaking phase(Nk(C))Local search(C′)if |C′|<|C| thenMove C←C′Restart with the first neighbour: k←1elseIncrease the size of the neighbourhood structure: k←k+1endendif |C|<|BestC| thenMove BestC←CDecrease the maximum size of the shaking: kmax←max(kmax−1;|BestC|/2)elseIncrease the maximum size of the shaking: kmax←min(kmax+1;2·|BestC|)enduntiltermination conditionsUpdate HBEST=(V, E(BestC))⇒ Take any arbitrary spanning tree T of HBEST=(V, E(BestC)).endIn the probabilistic local search, the acceptance probability of a worse component into a partial solution is evaluated according to the usual SA criterion by the Boltzmann function exp(−Δ/T), where the parameter T, referred to as temperature, controls the dynamics of the function. Initially the value of T is large, so allowing many worse moves to be accepted, and is gradually reduced by the following geometric cooling law:(3)Tk+1=α·TkwhereT0=|BestC|,α=1/|BestC|∈[0,1],with BestCbeing the current best solution, and |BestC| its number of colours. This cooling law is very fast for the MLST problem and produces a good balance between intensification and diversification capabilities. In addition, this cooling schedule does not requires any intervention from the user regarding the setting of its parameters, as it is guided automatically by the best solution BestC. Therefore the whole optimization process is able to react in response to the search algorithm's behaviour and to adapt its setting on-line according to the instance of the problem under evaluation [32].The probabilistic local search has the purpose of allowing also the inclusion of less promising labels to incomplete solutions. Probability values assigned to each label are inversely proportional to the number of connected components they give. In this way, at each step labels with a lower number of connected components will have a higher probability of being selected than those labels having a larger number of connected components. In addition, the progressive reduction of the temperature parameter in the adaptive cooling law produces, step by step, an increasing of this diversity in probabilities. It means that the difference between the probabilities of two labels giving different numbers of components is higher as the algorithm proceeds. As the algorithm iterates, the probability of a label having a large number of components tends to zero, making the entire search MVCA-like.The details of the implementation of our intelligent metaheuristic are specified in Algorithm 4. Note that the probabilistic local search is applied both in complementary variable neighbourhood search, to obtain a solution from the complementary space of the current solution, and in the inner local search phase, to restore feasibility by adding labels to incomplete solutions. At the beginning of INTELL, the algorithm generates an initial feasible solution at random, denoted by BestC, and set parameter kmaxto the number of colours of the initial solution (kmax←|BestC|). Then the Complementary(·) procedure is applied to BestC, as shown in Algorithm 5, to obtain a solution C from the complementary space of BestCby means of the probabilistic local search.Algorithm 5Procedure Complementary(·)Procedure Complementary(BestC):Set C←0while (Comp(C)>1)AND((Compl_Space−C)≠0) doLet s∈(Compl_Space−C) be the label that minimizes Comp(C∪{s})Temperature cooling schedule: T|C|+1=α·T|C| whereT0=|BestC|α=1/|BestC|foreachc∈(Compl_Space−C) doCalculate the probabilities P(c) for each label, normalizing the values given by the Boltzmann function:exp−Comp(C∪{c})−Comp(C∪{s})T|C|+1where s∈(Compl_Space−C) is the label which minimizes Comp(C∪{s})endSelect at random an unused label u∈(Compl_Space−C) following the probabilities P(·)Add label u to the set of used labels: C←C∪{u}Update H=(V, E(C)) and Comp(C)endThe complementary procedure stops if either a feasible solution C is obtained, or the set of unused colours contained in the complementary space is empty (i.e. (Compl_Space−C)=0), producing a final infeasible solution. Subsequently, the same shaking phase used for the basic VNS (Section 2.3) is applied to the resulting solution C (Shaking phase(Nk(C)) procedure, see Algorithm 2). It consists of the random selection of a point C′ in the neighbourhood Nk(C) of the current solution C, where Nk(C)={S⊂L:(ρ(C, S))=k}, k=1, 2, …, kmax. At each iteration of the shaking phase, in order to select a solution in the kth neighbourhood of a solution C, the algorithm randomly adds further labels to C, or removes labels from C, until the resulting solution has a Hamming distance equal to k with respect to C. Addition and deletion of labels at this stage have the same probability of being chosen.The successive local search (Local search(·) procedure, see Algorithm 6) consists of two steps. Since either the complementary variable neighbourhood search, or the deletion of labels in the shaking phase, can produce an infeasible incomplete solution, the first step of the local search consists of including additional labels in the current solution in order to restore feasibility, if needed. The addition of labels at this step is according to the probabilistic procedure. Then, the second step of the local search tries to delete labels one by one from the specific solution, whilst maintaining feasibility.Algorithm 6Procedure Local search(·)Procedure Local search(C′):whileComp(C′)>1Let s∈(L−C′) be the label that minimizes Comp(C′∪{s})Temperature cooling schedule:T|C′|+1=α·T|C′|whereT0=|BestC|α=1/|BestC|foreachc∈(L−C′) doCalculate the probabilities P(c) for each label, normalizing the values given by the Boltzmann function:exp−Comp(C′∪{c})−Comp(C′∪{s})T|C′|+1where s∈(L−C′) is the label which minimizes Comp(C′∪{s})endSelect at random an unused label u∈(L−C′) following the probabilities P(·)Add label u to the set of used labels: C′←C′∪{u}Update H′=(V, E(C′)) and Comp(C′)endfori←1 to |C′| doDelete label i from the set C′, i.e. C′←C′−{i}Update H′=(V, E(C′)) and Comp(C′)ifComp(C′)>1 thenAdd label i to the set C′, i.e. C′←C′∪{i}endUpdate H′=(V, E(C′)) and Comp(C′)endAt this step, if no improvements are obtained (|C′|≥|C|), the neighbourhood structure is increased (k←k+1) producing progressively a larger diversification. Otherwise (i.e. if |C′|<|C|), the algorithm moves C to solution C′ restarting the search with the smallest neighbourhood (k←1). This iterative process is repeated until the maximum size of the shaking phase, kmax, is reached. The resulting local minimum C is compared to the current best solution BestC, which is updated in case of improvement (BestC←C). Note that at this point a reactive setting for the parameter kmaxhas been used [33]. In case of an improved solution, kmaxis decreased (kmax←max(kmax−1;|BestC|/2)) in order to raise the intensification factor of the search process. Conversely, in case of none improvement, the maximum size of the shaking is increased (kmax←min(kmax+1;2·|BestC|)) in order to enlarge the diversification factor of the algorithm. In each case, the adaptive setting of kmaxis bounded to lie in the interval between |BestC|/2 and 2·|BestC| to avoid a lack of balance between intensification and diversification factors.The algorithm proceeds with the same procedure until the user termination conditions are satisfied, producing at the end the best solution to date, BestC, as output.To test the performance and the efficiency of the algorithms presented in this paper, we performed an experimental evaluation on instances of the MLST problem having different number of nodes (n), density of the graph (d), and number of labels (ℓ). In particular, for our experiments we considered well-known benchmark instances from the MLST literature [15,21,23,14] as well as completely new instances of the problem having larger cardinality. All the considered instances are available from the authors in [34]. These consists of 60 different datasets, each one containing 10 randomly generated instances (for a total of 600 problem instances), having high (0.8), medium (0.5), and low (0.2) density values, d. These instances were selected with number of vertices, n, and number of labels, ℓ, ranging from 20 up to 1250, while the number of edges, m, is obtained indirectly from the density d. Note that the complexity of the instances increases with the dimension of the graph (i.e. increasing n and/or ℓ), and the reduction in the density of the graph. For each dataset, solution quality is evaluated as the average objective function value (i.e. the number of colours of the solution) among the 10 problem instances. A maximum allowed CPU time, that we call max-CPU-time, was chosen as stopping condition for all the metaheuristics, determined with respect to the dimension of the problem instance. Selection of the maximum allowed CPU time as stopping criterion was made in order to have a direct comparison of the metaheuristics with respect to the quality of their solutions.Our results11The computations have been performed on an Intel Xeon(R) microprocessor at 4.0GHz with 6GB RAM.are reported in Tables 1– 5. In each table, the first three columns show the parameters characterizing the different datasets (n, ℓ, d), while the remaining columns give the computational results of the considered algorithms, which are identified with the abbreviations: EXACT (exact method), GRASP (greedy randomized adaptive search procedure), VNS (variable neighbourhood search), COMPL (complementary variable neighbourhood search), INTELL (intelligent local search algorithm).All the metaheuristics run for the max-CPU-time specified in each table, and for every problem instance the best solution is recorded. The computational times reported in the tables are the average times at which the best solutions were obtained, except in the case of the exact method, where the exact solution is reported unless a single instance computed for more than 3h of CPU time.22All the reported computational running times have precision of ±5ms.In the case that no solution was obtained within this time by the exact method, a not found status (NF) is denoted in the tables. The performance of an algorithm can be considered better than another if either it obtained a smaller average objective function value, or an equal average objective function value but in a shorter computational running time.Tables 1 and 2examine relatively small instances of the MLST problem. Watching Table 1, all the metaheuristics show very good performance for this dataset class with small problem instances (n=ℓ=20, 30, 40, 50, and d=0.8, 0.5, 0.2). All the metaheuristics were able to find the exact solutions for these instances in very shorter computational running time than the exact method. A view on the performance of the heuristics for the datasets reported in Tables 2 and 3, having slightly larger cardinality (n=100, 200, ℓ=0.25n, 0.5n, n, 1.25n, and d=0.8, 0.5, 0.2), begins to show some differences between the algorithms. GRASP obtained in general a worse objective function value than the other metaheuristics for the instances where the exact solution was not known (NF). It was faster in some instances (see for example [n=200, ℓ=200, d=0.2] and [n=200, ℓ=100, d=0.2] in Table 3) but because it obtained worse quality solutions. The other metaheuristics obtained approximately the same performance with respect to solution quality, but INTELL was considerable faster than VNS and COMPL.If the differences in performance among the algorithms begin to reveal in the previous tables, within Tables 4 and 5 these become remarkable. These two tables contain larger instances of the problem (respectively n=500 and n=1000) where it is shown that INTELL was able to obtain very high performances. INTELL always outperformed the other algorithms in terms of solution quality and, in general, also with respect to computational running time. GRASP exhibits a limited diversification capability which sometimes did not allow the search process to escape from local optima. Indeed, GRASP was faster than the other algorithms in some circumstances but it produced poorer solutions with respect to solution quality, as for example in the instances [n=500, ℓ=500, d=0.2] and [n=500, ℓ=625, d=0.2] in Table 4, and [n=1000, ℓ=1000, d=0.5] in Table 5. As in the previous analysis, VNS and COMPL shown the same relative behaviour for the considered instances. However, COMPL sometimes showed excessive diversification and poor intensification capabilities in some problem instances, obtaining worst performance than VNS in terms of solution quality and computational running time, as shown for example in the instances [n=500, ℓ=250, d=0.5] and [n=500, ℓ=500, d=0.8] in Table 4, and [n=1000, ℓ=250, d=0.2] and [n=1000, ℓ=500, d=0.5] in Table 5. Nevertheless, this unbalanced ratio towards diversification allowed COMPL to find better objective function values than VNS for larger problem instances (see for example [n=500, ℓ=625, d=0.8] in Table 4 and [n>=1000] in Table 5).The last consideration by observing the results reported in all the tables is that, in almost all the problem instances for which EXACT produced the solution, the considered metaheuristics also obtained the exact solution. Solely in the problem instance [n=200, ℓ=250, d=0.8] in Table 3, GRASP and COMPL obtained a slightly worse solution quality than the exact method, although they were extremely faster. In addition, the exact solutions reached by all the considered metaheuristics required a shorter computational time than that required by the exact method. From this point of view, within the 32 datasets, out of the total of 60 datasets, for which the exact method obtained the exact solution, the best performance was obtained by INTELL which required less computational time than that required by the other algorithms. Summarizing, the proposed intelligent algorithm was the best performing method in our computational experiments. To further confirm this evaluation the metaheuristics were ranked for each dataset, with a rank of 1 assigned to the best performing algorithm, a rank of 2 to the second best one, and so on. The average ranks of the algorithms among the considered datasets are (ordering from the best to the worst rank): INTELL = 1.47, VNS = 2.68, GRASP = 2.88, COMPL = 2.97. To analyse the statistical significance of differences between the evaluated ranks, we used the Friedman Test[35] and its corresponding Nemenyi Post-hoc Test[36]. For more details on the issue of statistical tests for comparison of algorithms over multiple datasets see [37,38]. According to the Friedman Test, a significant difference between the performance of the metaheuristics, with respect to the evaluated ranks, exists (at the 5% of significance level). Since the equivalence of the algorithms is rejected, the Nemenyi post-hoc test is applied in order to perform pairwise comparisons. It considers the performance of two algorithms significantly different if their corresponding average ranks differ by at least a specific threshold critical difference. In our case, considering a significance level of the Nemenyi test of 5%, this critical difference is 0.61. The differences between the average ranks of the algorithms are reported in Table 6.From this table, it is possible to identify two groups of algorithms with different performance. The best performing group consists of just INTELL, because it obtains the smallest rank which is significantly different from all the other ranks. The remaining group consists of VNS, GRASP, and COMPL, which have comparable performance according to the Nemenyi test (since, in each case, the value of the test statistic is less than the critical difference 0.61), but worse than that of INTELL. Summarizing, according to the statistical evaluation, INTELL is the best performing algorithm, further confirming the superiority of our intelligent optimization strategy with respect to the other approaches.All the results demonstrate that INTELL is an effective metaheuristic for the MLST problem, producing high quality solutions in short computational running times. The superiority of this intelligent local search with respect to the other algorithms is further evidenced by its scalability, robustness, automated parameters configuration, and easy adaptivity to new problem instances to solve.

@&#CONCLUSIONS@&#
This paper presented the state-of-the-art optimization algorithms to solve the minimum labelling spanning tree (MLST) problem. An exact method has been described, along with the best heuristics recommended in the literature [14]: Greedy randomized adaptive search procedure and variable neighbourhood search. Additionally, an intelligent optimization method for the problem has been proposed. It integrates variable neighbourhood search with other complementary approaches and self-tuning mechanisms which improve the effectiveness and robustness of the optimization process.Computational experiments were performed using different instances of the MLST problem to evaluate how the algorithms are influenced by the parameters and the structure of the network. From our experience we concluded that the proposed intelligent metaheuristic has significantly better performance than the other algorithms with respect to solution quality and computational running time. Our approach was able to get a large number of optimal or near-optimal solutions quickly, showing a high balanced ratio between diversification and intensification capabilities for the problem. This study provides further evidence of the ability of intelligent optimization and reactive search strategies to deal with hard combinatorial optimization problems.