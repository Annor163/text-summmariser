@&#MAIN-TITLE@&#
Detecting, segmenting and tracking unknown objects using multi-label MRF inference

@&#HIGHLIGHTS@&#
We present a framework for detection, segmentation and tracking of multiple objects.The framework has minimal requirements on input for initialization.The choice of MRF inference method is less important, than how scenes are modeled.Proximities are more important than colors as cues for segmentation.For real-time application message passing is more feasible, than graph cuts.

@&#KEYPHRASES@&#
Figure-ground segmentation,Active perception,MRF,Multi-object tracking,Object detection,GPU acceleration,

@&#ABSTRACT@&#
This article presents a unified framework for detecting, segmenting and tracking unknown objects in everyday scenes, allowing for inspection of object hypotheses during interaction over time. A heterogeneous scene representation is proposed, with background regions modeled as a combinations of planar surfaces and uniform clutter, and foreground objects as 3D ellipsoids. Recent energy minimization methods based on loopy belief propagation, tree-reweighted message passing and graph cuts are studied for the purpose of multi-object segmentation and benchmarked in terms of segmentation quality, as well as computational speed and how easily methods can be adapted for parallel processing. One conclusion is that the choice of energy minimization method is less important than the way scenes are modeled. Proximities are more valuable for segmentation than similarity in colors, while the benefit of 3D information is limited. It is also shown through practical experiments that, with implementations on GPUs, multi-object segmentation and tracking using state-of-art MRF inference methods is feasible, despite the computational costs typically associated with such methods.

@&#INTRODUCTION@&#
Objects play a central role in computer vision, and different fields are dedicated to recognize or classify objects in images, or track such objects over time. The former tasks assume models of objects or classes of objects to be learned from sets of training examples. Given a test image, extracted features are associated to the learned models, with the goal of deducing whether a particular object or class exists in the image or not. Object tracking is a general problem, but can be facilitated by taking advantage of similar models, if the tracked object is previously known. While much focus has been given to these fields during the past decades, less focus has been given to that of discovering unknown objects in scenes and tracking these over time. Such a problem assumes that hypotheses of what could constitute real physical objects are first generated from images and then modeled, so that tracking can be initiated. Once tracked the hypothesis of an object can then either be confirmed or rejected, given multiple observations in sequence.The benefits of doing so include: (1) allowing for finding and modeling of objects outside currently learned classes, and (2) assisting classification and recognition problems by limiting the search space. The task is related to the fields of visual attention and segmentation. Computational models of attention can be used to find regions of interest in images [1], regions that potentially have some semantic meaning to the observer. Few such models, however, are capable of segregating physical objects from the surrounding scene. Work in object segmentation [2,3] on the other hand, aim to segregate foreground objects from their backgrounds, but usually do so with no real concept of what constitutes an object. Instead, a user is needed to indicate the object in the image by for instance framing it [3]. The presented work is more related to recent work by Mishra and Aloimonos [4], where the concept of an object is more central. In their work they segment “simple”objects defined as compact regions enclosed by edge pixel that arise either due to discontinuities in depth or from contact with supporting surfaces. Similarly, we define an object as something that occupies a portion of 3D space, and exhibits some continuity in appearance and shape. Compared to [4] our framework has three main advantages though. First, it runs in real time, thus enabling tracking. Second, as a side effect of the object definition, it produces a model of an object, in terms of its size, rough shape and color distribution. Third, it allows for simultaneous segmentation and tracking of multiple objects, not just a single one.A motivating goal of the presented work is a system that facilitates active scene and object understanding in realistic indoor settings [5,6]. Given an observed scene containing unknown objects of interests, the system should allow for objects to be modeled and refined over sequences of observations, while the camera pose changes or objects are interacted with by e.g. a robotic manipulator. In such a scenario, segmentation and tracking serve little purpose in themselves, but are used as a means to extract attributes for object understanding over time and guide exploratory actions. The system should, however, be open also to other applications, such as semi-autonomous annotation of image and video data, an application that also requires precision and high speed. Care has thus been taken not to introduce assumptions specific to particular applications.Tracking previously unseen objects has gained some attention lately [7,8]. Similarly to our work, these methods create models of foregrounds and backgrounds, and segment objects based on measurements of e.g. image intensities and positions. Unlike our work, however, they use level sets for object tracking, and gain their speed from propagating only contours around objects, something that is possible even for multiple objects in real time, as demonstrated in [9]. However, they cannot effortlessly be used in unsupervised scenarios, as they require initial boundaries for initialization and are sensitive with respect to how these boundaries are drawn [10]. We instead take a graph based approach, similar to [4], which allows for more robust segmentation, while enabling unsupervised initialization. Our method performs tracking by modeling and segmenting each frame using not just boundary pixels, but every pixel, and letting model parameters evolve as functions of estimates from previous frames. This enables robustness to changes in object appearance, shape and topology. For additional robustness, in particular for cases when the boundary between an object and its supporting surface is ambiguous, we propose an heterogeneous scene representation that uses a combination of flat surfaces and random clutter for background modeling, with foreground objects modeled as 3D ellipsoids.The main contributions of this work is a unified framework for principled active object segmentation by modeling the problem over a Markov Random Field (MRF) that•allows for multi-object detection, modeling and tracking,considers all image pixels for classification, not just those around objects of interest, andhas minimal requirements on user input for initialization.In an effort not to sacrifice accuracy for speed, we present a thorough analysis of alternative MRF inference methods for segmentation. We study these in terms of both segmentation performance and computational costs, in particular when implemented on massively parallel GPU architectures. We also evaluate different tracking scenarios, where model parameters are predicted and tracked using sets of Kalman filters, and by doing so demonstrating the feasibility for tracking.The outline of the presentation is as follows. In Section 2 we give an overview of the work related to our study. The theoretical basis for the tested MRF inference methods is given in Section 3. A heterogeneous scene representation for segmentation is presented in Section 4, with foreground objects modeled as 3D ellipsoids and background as a combination of planar surfaces and uniform clutter. In Section 5 the initialization procedure is described and in Section 6 the framework is extended for tracking, using Kalman filters for forward predictions of model parameters. A large series of off-line experiments, testing alternative MRF inference methods, are presented in Section 7, whereas in Section 8 these are studied from a computational point of view. Finally, the presentation is concluded in Section 9 with a discussion on future work.

@&#CONCLUSIONS@&#
We have studied the problem of multi-object segmentation and tracking, and done so without letting the real-time constraint prevent us from using state-of-the-art energy minimization techniques for labeling. From experiments is has been concluded that the modeling of the problem is more important than the choice of MRF inference method. We have proposed a heterogeneous scene representation that consists of a background model of planar surfaces and uniform clutter, with foreground objects hypotheses modeled as 3D ellipsoids. It has further been concluded for grouping of pixels, distance measures in either 2D or 3D are just as important as colors, and that planar background models help to disambiguate objects from the surfaces that are placed on.In experiments it has been shown that modern maximization based methods, such as max-product belief propagation (LBP-M), tree reweighted message passing (TRW-S) or primal–dual graph cuts (GC), lead to very similar end results, when run in an EM-like iterative framework with labeling and modeling interleaved. Unlike labeling problems with fixed energy terms, the dynamics prevent methods, LBP-M in particular, from being stuck in local minima. What cannot be directly observed from the quantitative experiments, however, is that the paths to the final segmentation can be quite different. Foreground object seeds tend to evolve faster, but sometimes more erratic, with TRW-S and GC, which is reasonable given their better convergence properties for energy minimization. However, the minimization problem solved in each iteration is based only on statistics from the previous iteration, which might still be incomplete, if the full extent of an object has not yet been captured. Thus better convergence in the energy minimization does not necessarily translate into faster convergence overall.Given that LBP-S was the only marginalization based approach in our study, it has properties rather different from those of the others. By considering all possible labeling in each iteration, not just the current MAP labeling, is shows the best ability to grow from very small initial foreground seeds to full object segmentation. Unfortunately, this also means that it is more likely to connect objects located close to each others, especially if these have overlaps in their color distributions. However, if competing seeds can be simultaneously initiated on each object, LBP-S do just as well as the top contenders, and should thus not be disregarded as a candidate method. The same is true if the end goal is object modeling, not segmentation per se, since with marginalization the full space of labeling is taken into consideration for modeling.As soon as you apply segmentation in a real-time settings, computational speed becomes an issue. Due to the costs associated with multi-label MRF inference methods, in terms of both speed and memory, such methods are rarely used for real-time purposes. We have in this study shown that MRF methods are in fact feasible, even for the purpose of object tracking, using GPUs for accelerations and given recommendations on how to use the hardware to its full potential. It has been concluded that, given the pattern of memory accesses, message passing methods are more suitable for high levels of parallelism, even if graph cut based methods are faster on single threaded CPUs. Given the similarities in segmentation results, message passing methods are thus recommended for real-time purposes.A choice made in designing the real-time system was to let the segmentation in one frame only depend on the scene part models of the previous frame, but not on the previous segmentation, despite the temporal consistency that can often be expected. From one frame to the next, only the model parameters remain. The motivation for this modeling centric approach, was to allow for initialization of previously seen objects in new scenes, using as little prior information as possible. In later studies we intend to study the problem of re-initialization of such objects, when the scene has undergone different degrees of changes, from gradual changes in view points, to larger changes where objects temporarily leave the visual field, and to complete scene changes. For tracking we have already made an exception from the general rule and do in fact exploit segmentation from previous frames. The question is what information to use when more radical changes occur and no previous segmentations can be applied. A possible avenue is to combine SIFT features, extracted during earlier tracking, and the intrinsic scene part parameters, the shape and color information, for detection and re-initialization.A segmentation system can never perform better than what its models and prior assumptions allow. As soon as you introduce abstractions in its representations, you may run into problems, if the object to be represented diverges from what can typically be assumed. What frequently occurs in multi-object segmentation is the difficulty to disambiguate objects in direct contact. Are there two objects placed next to each others or is there just one of an unusual shape or texture? We believe that such questions cannot always be resolved from single images alone and are exploring methods for active object hypothesis testing and validation [75]. We also use the presented system to learn more about objects through tactile exploration of object shape [76] and study objects in terms of object-related sensorimotor contingencies [77].This leads to the more general question of active scene understanding, the theme of our current research, which involves the problem of representing scenes with objects and their relations, updating representations over time, as well as planning and executing hypothesis testing through actions to understand the scene well enough to solve whatever task is placed on the system. In the presented experiments, foreground models were automatically inserted on start-up, given the number of models and expected size. This does not necessarily have to be the case. In a fixating system, e.g., new models can sequentially be inserted each time a part of the scene that has not yet been visited is fixated on. If a previously fixated, and currently tracked, object is to be revisited, the corresponding model keeps information on where it is located in the scene and how to make a saccade towards it. Thus even if the presented multi-object segmentation and tracking system can be used for many different applications, applications vary in the way scene part models should be inserted, exploited and eventually discarded.Supplementary data associated with this article can be found, in the online version, at http://dx.doi.org/10.1016/j.cviu.2013.10.007.Supplementary Video 1Supplementary Video 2Supplementary Video 3