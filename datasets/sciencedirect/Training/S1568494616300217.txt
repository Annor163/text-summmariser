@&#MAIN-TITLE@&#
Clustering by propagating probabilities between data points

@&#HIGHLIGHTS@&#
We study the problem of data clustering.We propose a clustering algorithm by propagating probabilities between data points.We use local densities of the data points to initialize the probabilities.Experiments on synthetic and real data show that the proposed clustering algorithm performs well.

@&#KEYPHRASES@&#
Affinity propagation,Data clustering,Graph-based clustering,Markov clustering,Probability propagation,

@&#ABSTRACT@&#
In this paper, we propose a graph-based clustering algorithm called “probability propagation,” which is able to identify clusters having spherical shapes as well as clusters having non-spherical shapes. Given a set of objects, the proposed algorithm uses local densities calculated from a kernel function and a bandwidth to initialize the probability of one object choosing another object as its attractor and then propagates the probabilities until the set of attractors become stable. Experiments on both synthetic data and real data show that the proposed method performs very well as expected.

@&#INTRODUCTION@&#
Data clustering or cluster analysis is a fundamental tool for data analysis. The goal of data clustering is to divide a set of items into groups or clusters such that items in the same cluster are more similar to each other than to items form other clusters [12,32]. As a result, data clustering has found applications in a wide range of areas such as bioinformatics [7,22], pattern recognition [20], health care [33], insurance [13,15], to just name a few.In the past 60 years, many clustering algorithms have been developed to achieve the task of data clustering [19]. These algorithms differ significantly in terms of how clusters are defined and how the clusters are identified. The k-means algorithm [24] is one of the most popular and classical clustering algorithms. Used to find groups of objects with small distances among cluster members, the k-means algorithm starts from k initial cluster centers and repeats updating cluster members and cluster centers until some stopping criterion is met. The number of clusters, k, is a parameter of the algorithm. One drawback of the k-means algorithm is that it is quite sensitive to initial cluster centers, which affect clustering results and the convergence speed. For example, [26] compared four initialization methods for the k-means algorithm and found that random initialization is not the best method.To address the cluster center initialization problem, Frey and Dueck [11] proposed an efficient clustering method called affinity propagation. The Affinity Propagation (AP) algorithm starts with the similarities between pairs of data points and repeats passing real-valued messages between data points until a high-quality set of exemplars (i.e., cluster centers) and corresponding clusters are found. Unlike the k-means algorithm [24], the AP algorithm considers simultaneously all data points as cluster centers and thus does not suffer from the cluster center initialization problem.One drawback of the AP algorithm is that the rules of passing messages between data points are complicated. In the AP algorithm, two types of messages are exchanged between data points: the responsibility and the availability. As we will see in Section 2, the rule for updating the responsibility involves calculating the maximum of sums of the availability and the similarity; the rule for updating the availability involves calculating the sum of positive responsibilities.Motivated by the AP algorithm, we propose in this paper a novel clustering algorithm called “probability propagation,” which is able to identify clusters having spherical shapes as well as clusters having non-spherical shapes. The probability propagation (PP) algorithm starts with a matrix of probabilities calculated from local densities and keeps propagating probabilities until the set of attractors become stable. Here we use the term “attractor” to represent a cluster center because the clusters found by the PP algorithm can have non-spherical shapes.The PP algorithm we proposed is similar to the AP algorithm and the Markov Clustering (MCL) algorithm in that all three algorithms involve certain message-passing mechanism. One major difference between the PP algorithm and the AP algorithm is that the rules of message-passing in the former are simpler than those in the later. Another difference is that the PP algorithm is able to identify clusters of non-spherical shapes but the AP algorithm cannot. One major difference between the PP algorithm and the MCL algorithm is that the PP algorithm does not use the inflation operator, which is required by the MCL algorithm. Another difference is that the stochastic matrix initialization of the PP algorithm is different from that of the MCL algorithm.The remaining of the paper is structured as follows. In Section 2, we give a brief description of the AP algorithm, the MCL algorithm, and spectral clustering. In Section 3, we present the PP algorithm in detail. In Section 4, we demonstrate the performance of the PP algorithm by conducting experiments on both synthetic and real data sets. In Section 5, we conclude the paper and point out some areas for future research.

@&#CONCLUSIONS@&#
