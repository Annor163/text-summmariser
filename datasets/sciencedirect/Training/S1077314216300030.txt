@&#MAIN-TITLE@&#
Toothbrush motion analysis to help children learn proper tooth brushing

@&#HIGHLIGHTS@&#
A novel approach to help children learn proper tooth brush based on Computer Vision is proposed.3D toothbrush tracking is performed based on MOSSE algorithm and a colored marker.Hidden Markov Models are adopted for tooth brushing gesture recognition.A tooth brushing gamification based on avatars induces kids to follow their hints.A test on volunteer kids was performed providing good results.

@&#KEYPHRASES@&#
Target tracking,Hidden Markov models,Color segmentation,

@&#ABSTRACT@&#
Toothbrush training is a complex and not fun task for the child nor for the parents or for the dental stuff. Parents and hygienists often report that they are frustrated by poor responses to the training and in most of cases children go home and resume wrong brushing habits, if any. In this paper we present a novel approach where the tooth brushing procedure can become a fun and enjoyable task for kids using a cheap toothbrush accessory and a tablet or a smartphone. The main idea is to apply a simple and cheap 3D colored target at the end of the toothbrush and to track and analyze its motion, imparted by the child. In particular, from the tablet camera it is possible to track both the toothbrush target and the child’s facial parts in order to estimate the brushed dental side. The proposed approach has been tested on seven kids showing good results both in propensity and accuracy after a 20 days period.

@&#INTRODUCTION@&#
Thanks to the widespread of handheld powerful devices, there is a plenty of opportunities where they can permeate our daily life with novel and helpful applications. In particular, thanks to the significant computational resources and the large number of sensory input/output devices they fulfill the mobile cyber-physical systems paradigm of intelligent embedded systems (Alippi, 2014; Dziri et al., 2014; Yoon and Ahn, 2006). In this article, we describe a novel application where mobile devices can fruitfully perform as an oral hygiene supervisor for kids. The usage of integrated cameras, high computational power and handless user interaction make these devices the perfect tooth-brushing assistant in order to make the procedure interactive, stimulating and fun. Most of kids find tooth brushing an annoying procedure and even if their parents/caregivers spend a lot of time trying to teach them the correct oral hygiene, they can easily acquire wrong habits. A correct tooth brushing training requires a constant supervision and the ability to create stimulating tricks to make children greet with enthusiasm. In order to simplify training and supervision tasks, many companies such as Oral-B (2014) and Kolibree (2014) are introducing new electric toothbrushes; these devices, equipped with Bluetooth connectivity, gyroscopes and accelerometers, communicate with smartphones or tablets capable of registering all the toothbrush motions in order to keep track of the brushed dental parts and the time dedicated to each of them. Electric toothbrushes equipped with advanced sensors cost significantly more than a manual toothbrush, they require to be charged or to replace batteries and, even if they could be more fun and entertaining, it is easy to accidentally break them: dropping an electric toothbrush can be fatal. Furthermore, kids could try to cheat the system just mimicking the tooth brushing gestures keeping the toothbrush head out of their mouths since no visual analysis is performed.In order to keep the whole system for tooth brushing analysis as cheap and simple as possible, we decided to track the toothbrush and the face of its user, by the smartphone frontal camera. The smartphone display plays the role of a “virtual mirror” where the image of the person using the toothbrush is replaced by an avatar that, thanks to the facial features tracking, is able to completely mimic his gestures and expressions and points out wrong movements. The toothbrush itself can be replaced by a virtual tool and an interactive videogame can be realized where experience points and level progression are steered by a proper brushing procedure and timing. In our approach, we simply evaluated the effect of an avatar that mimics the children movements and correct their wrong gestures or timing on each side of the dental arches. We also evaluated in Section 5.1 the results of removing this tool after 10 days of usage to analyze the learning effect independently from the app usage.The most relevant aspects, related to Computer Vision and Pattern Analysis, regard the tracking of the toothbrush, the estimation of its spatial position and orientation with respect to the user mouth and the analysis of its motion.The toothbrush spatial position and orientation is obtained applying a colored target at the end of the toothbrush handle: this is the only portion of the toothbrush that will always be visible during tooth brushing since the head could be inside the mouth or could be covered by toothpaste and foam while the user’s hand, grasping the handle, will cover it. The colored target is a ball in order to avoid dangerous sharp corners and is painted with vivid colors that can be easily segmented from the background using a suitable color space. In literature we found some other approaches based on target tracking for toothbrushing analysis, in particular Flagg et al. (2011) adopted color segmentation for toothbrush localization together with face parts localization but in this preliminary work a simple planar localization is considered without any 3D rotation or translation tracking or analysis in Chang et al. (2008) a shining marker equipped with LEDs is adopted on the tail of the toothbrush but no movements analysis is performed (a frame by frame analysis is performed), furthermore the mouth of the user is not tracked so it is impossible to know if the child is brushing his teeth or just moving the toothbrush in the air. Most of other approaches Lee et al. (2007); 2012); 2011) are based on accelerometers, magnetometers and/or gyroscopes which represent a different approach to the problem requiring instrumented toothbrushes with batteries and transmission interfaces. Furthermore bulky and heavy devices do not appear particularly appealing to young people for improving their inclination towards tooth brushing (Strickland, 2013). In our approach the idea is to reuse as much as possible of the computational power and acquisition capabilities of common tablets and smartphones and, at the same time, with the same hardware, we can also monitor facial parts position getting a more accurate estimation of the brushed dental side. Results with other approaches are compared in Section 5. In our approach the target tracking is then performed using a modified version of the MOSSE (Bolme et al., 2010) algorithm based on correlation filters: the aim is to account for the blurring due to the rapid motion of the target. In fact the toothbrush during the brushing procedure is usually subject to fast and periodic translations and rotations. The analysis of the toothbrush motion is performed using a Hidden Markov model (Bashir et al., 2005; Rabiner, 1989), this will allow us to estimate motion paths imparted by the user.In order to evaluate the correctness of the brushing procedure the user face and its parts are also tracked; in particular we used the open source Active Shape Model code (Xing, 2010) in order to get an efficient estimation of eyes, nose and mouth position. Using this information we will estimate which surface of the dental arches is being brushed and in which direction.The article is articulated as follows. In Section 2 we describe the target and how we can localize it inside an image through color segmentation. Section 3 describes the tracking procedure adopted for blurred images. Section 4 shows the procedure to recognize the dental arch brushed surface and evaluate the correctness of the brushing motion. The article concludes with Sections 5 and 6 where results on a set of children are presented and commented.The first step for the tooth brushing analysis consists of a robust toothbrush tracker, in order to get this an easy-to-track target was adopted: we used a ball with vivid and highly saturated colors in order to easily segment and localize the target from the background even in unconstrained and poorly illuminated environments. Many techniques were proposed for robust color segmentation in noise environments and Fuzzy C-means (FCM) has been widely adopted (Kim et al., 2004) together with its variants (Xess and Agnes, 2013) like SFCM (Spatial FCM) (Chuang et al., 2006) or THFCM (Thresholding FCM) (Jassim, 2012). FCM approach introduced the fuzzy concept so that the same object can belong to more than a single class at the same time. For every object, the belonging degree to a class is related to the association strength between the object and the class. The largest drawbacks related to the FCM approach are that the number of clusters must be set before clustering, the resulting quality is strictly connected to initial clusters seeds and the overall approach is not spatial dependent. In our application, we can a priori define the target colors; the color drift in the acquired frames is due to different illuminants, shadows or different acquisition settings (white profile). The a priori knowledge of the approximate searched colors allow us to simplify the initial target localization and its further segmentation in the tracking phase. In particular we followed a supervised Region Splitting and Merging (RSM) approach starting from the unsupervised approach proposed in Tan et al. (2013). Accordingly to Agoston (2005), we adopted, as a color space the Hue, Saturation, Value (HSV); further details on this color space conversion can be found in Gonzalez and Woods (2008).In order to be able to estimate the toothbrush spatial orientation the target is painted with a specific color sequence. In particular, the target is composed of two hemispheres whose joining circle (the equator) is orthogonal to the main toothbrush axis. Every hemisphere is divided into six slices based on six colors chosen in order to be maximally distant in the Hue dimension: Red, Green, Blue, Cyan, Magenta,Yellow. The two hemispheres, to avoid ambiguities, where painted (see Fig. 1) following two different color sequences:•the top hemisphere: Red, Yellow, Blue, Magenta, Green, Cyan.the bottom hemisphere: Green, Magenta, Red, Cyan, Blue, Yellow.The target colors present a high saturation and are painted on a dull surface in order to prevent highlights. The first screening to localize possible pixels belonging to the target is based on a Value threshold and a Saturation threshold.In real applications, where the environment is usually a bathroom and the user keeps the smartphone or the tablet in front of him, the light sources are quite intense and are placed above the camera. This can generate the presence of self-shadows on the target image seen from the camera (see Fig. 2): this aspect, together with automatic exposure function of the camera induce a high variability in the target pixels intensities (Value) even in the same frame. For this reason, as a first rough classification, we remove from possible target points candidates, pixels with a Value below 0.1 and with a Saturation below 0.5. The further steps to remove false positive target pixels are•Connected components (blobs) extraction based on the pixels above the previously described thresholds in the HSV color space.Morphological processing based on a Morphological Opening and Closing (Gonzalez and Woods, 2008) in order to remove noise and small clusters and to connect close clusters.Removal of too small candidates (we adopted as a threshold the area below 256 pixels but it also depends on the camera sensor resolution and the minimum target size that we want to detect)Removal of blobs touching the image border: even if the target could belong to them its partial occlusion would prevent the correct estimation of its position and rotation.The next threshold is based on the roundness of the remaining blobs: it uses the eccentricity which is closer to zero as the blob is closer to a perfect circle.The eccentricity is obtained by analyzing the second order central moments of every blob:(1)μpq=∑x∑y(x−x¯)p(y−y¯)qf(x,y)wherex¯andy¯represent the centroid of the blob and f(x, y) is a boolean function equal to 1 for points belonging to the blob and 0 otherwise. The covariance matrix of the blob, defined as(2)cov[I(x,y)]=[μ20μ11μ11μ02]It represents the parameters of an ellipse approximating the blob and, in particular, the eccentricity of the blob can be defined as(3)ɛ=1−λ2λ1where λ1 and λ2 represent the eigenvalues of Eq. (2). A reasonable eccentricity threshold for our application is to accept values below 0.5. If more than one blob satisfies the above conditions, the one with the largest area is assumed as the best target candidate.The next step aims to ascribe every pixel belonging to the target to its proper colored portion: the attribution is based on the Hue value but accordingly to the aforementioned reasons, significant drifts could occur from tint nominal value. In particular, different lighting conditions, different target paintings together with color unbalanced cameras may give hue values far form their nominal Hue value; where, for our target, colors are indicated in Table 1. In order to find the Hue range for every cluster of pixels belonging to different target portions we processed the Hue histogram using an RSM (Region Splitting and Merging) approach (Tan et al., 2013). In our approach the histogram thresholds to find the different target regions is based on the following criteria: in the first step the RSM determines the relevant weighted Hue histogram peaks starting from seeds in Table 1; in the second step the valleys between peaks are used to define the endpoints of each color. The histogram that we adopted is not the simple one obtained from the Hue channel but every sample is weighted with its saturation value. It means that in every histogram bin we place the sum of the saturation values of all the target pixels belonging to the specific Hue value of the bin. This will give less relevance to poorly saturated pixels that usually are in darker target regions where the Hue value estimation is less accurate.The number of adopted histogram bins is 360 according to the possible values of the Hue channel. In order to increase the statistics of the target portion centroids the histogram adopted is the average of 15 histograms acquired from target in different image positions and different out-of-plane target rotations. The first step of the RSM algorithm follows the pipeline below:1.Reduce the histogram quantization to 180 bins grouping two contiguous bins in order to reduce noise.Apply a moving average filter with a span of seven elements on the histogram:(4)s(k)=∑i=−33n(k+i)7where n(k) is the histogram value at the kth bin. The filter span could be smaller but for small targets (below 300 pixels) the quantization noise in the histogram could thwart a proper clusterization.Localize all the peaks and valleys in s(k) according to the following rule:(5)IF(s(k)>s(k−1))and(s(k)>s(k+1))THENkisapeakIF(s(k)<s(k−1))and(s(k)<s(k+1))THENkisavalleyLower the value of peaks to the highest neighbor and rise the valleys to the lowest neighbor:(6)IF(kisapeak)THENs(k)=max(s(k+1),s(k−1))IF(kisavalley)THENs(k)=min(s(k+1),s(k−1))Identify the significant peaks by examining the turning points with positive to negative derivative changes in s(k).Evaluate the distance and the number of peaks between the six nominal peaks (reference hue values) defined in Table 1.If more than two peaks fall between two nominal peaks merge the two with the lower distance.The second step simply defines the valleys in the histogram by taking the minimum value between two adjacent peaks in the histogram. These values define the threshold between different target clusters (portions of different colors). We are then able to define the centroids of every target portion and their area (in pixels) in every image. Applying the above version of the RSM we are then able to define the Hue values for actual target colors corresponding to nominal values of Table 1 and the range extension, in the Hue histogram, for each of them. We are then able to process a target image defining a six-bins histogram where every bin contains the amount of pixels between two consecutive valleys. Each bin is then associated to the name of the reference color closest to the peak, if that reference color is not already associated to a closer peak: a result is shown in Fig. 3We have then defined a target feature descriptor in a six dimensions space: it is based on the sum of the normalized Hue histogram values between the thresholds defined above. The value in every dimension represents the proportional amount of pixels in the target image belonging to a reference color accordingly to the new Hue space partitioning obtained by the RMS algorithm. Summing the values of homogeneous color in Fig. 3 we get the value along each direction of the six dimensions feature descriptor as shown in Fig. 3.The descriptor is normalized in order to get the sum of all its values equal to 1: this allows us to consider it as a Probability Density Function (pdf) of colors in the target. This descriptor will be used in the tooth brushing analysis phase to estimate out-of-plane rotations of the target and, consequently, of the toothbrush.The toothbrush tracking is the next step in the pipeline for the tooth brushing analysis. In this task the major difficulties arise from•Partial or total occlusions due to the target movement outside of the frame or when the user is changing the hand used to grasp the toothbrush.Rotation of the target out-of-plane; in this case the colors and their disposition will change rapidly between frames.Blurring due to the fast target motion in conditions where the shutter speed could be low or unknown due to automatic exposure setting of the adopted acquisition devices.The last aspect is the most disturbing in an accurate tracking; in order to account for it we adopted a modified version of the enhanced correlation filter (Minimum Output Sum of Squared Error, MOSSE) (Bolme et al., 2010). This tracker, to our knowledge, outperforms in many cases most of other trackers as depicted in Visual Object Tracking Challenge (VOT2014) (Kristan and Others, 2014) where its scale invariant implementation (Discriminative Scale Space Tracker, DSST) (Danelljan et al., 2014) got the highest score.In the MOSSE filter the target appearance is modeled by adaptive correlation filters, and tracking is performed via convolution. The main drawback of simple correlation filters is that, even if they produce strong peaks for the target they also falsely respond to the background. The MOSSE filter works in the frequency domain in order to speed up the correlation process; we indicate with F, G and H the Fast Fourier Transforms (FFT) of the input image f, the correlation output g and the filter h respectively. The correlation of the input image with the correlation filter(7)g(x,y)=f(x,y)*h(−x,−y)(where the * indicates the convolution operator), becomes in the frequency domain(8)G(ωx,ωy)=F(ωx,ωy)⊙H*(ωx,ωy)where ⊙ indicates the element wise multiplication and H* indicates the complex conjugate of H. g(x, y), the filter output, should give the position of the target center and in theory could be an impulse but to lower noise at the high frequencies is quite common to adopt a compact 2D Gaussian shaped peak (usually σ ≃ 2). Given a set of training images fiand their center gi, the filter should be different for each of them(9)Hi*=GiFiwhere the division is performed element-wise. The aim of MOSSE filter is to find a single filter H that minimizes the sum of squared error between the actual output of the convolution and the desired output of the convolution i.e.:(10)minH*∑i|Fi⊙H*−Gi|2Solving this minimization problem is not difficult but care must be placed since H is a real valued function of complex variables: accordingly to Bolme et al. (2010) and Messerschmitt (2006) we get(11)H*=∑iGi⊙Fi*∑iFi⊙Fi*A typical online update is based on a learning rate η and a running average on the numerator and denominator:(12)Ht*=AtBtAt=ηGt⊙Ft*+(1−η)At−1Bt=ηFt⊙Ft*+(1−η)At−1where the index t is an integer indicating the actual frame. So Ftrepresents the FFT of the actual target image and Htis the new filter estimate. In our application the target typically undergoes rapid oscillating movements, and the blurring is minimum at the inversion points, where the velocity is minimal and the acceleration is maximal. In these points the target can be accurately analyzed and its rotation can be estimated while, during its trip, the blurring could prevent an accurate estimation of its rotation. In order to account for large blurring effects during target tracking (Wu et al., 2011), we integrated the estimated blurring convolution in the MOSSE filter. In particular we adapt the correlation output gtaccordingly to the target displacement estimated at the previous frame. Assuming thatv→tis the displacement (or motion) vector of the target center between framet−2and framet−1we generate a displacement matrix vtwith values different from zeros only along a short segment of pixels whose length and orientation is equal tov→. The value of every pixel of vtdifferent from zero is1/(#pixels≠0)(where#means: the number of... ) in order to keep the sum of all pixels values of vtalways equal to 1. We then convolve g with v obtaining the effect of applying the blurring to expected filter output:(13)gt=g*vtwhere g is, again, the expected filter output, i.e. an impulse representing the target position convolved with a 2D gaussian withσ=2. This approach will relax the forcing of the filter h to adapt to blurred target images, keeping all the tracking capabilities of the MOSSE filter when the target slows down reducing the risk of losing the target. In our approach g is always a gaussian with aσ=2.0and the learning rate ηvis also related to the magnitude ofv→t:(14)ηv=ηe−|v→|τwhere we adopted a value forτ=10. This approach allows us to keep memory of the target when it was moving slow, minimizing its update rate when large blurring is present. In our case Eq. (12) then transforms into(15)Ht*=AtBtAt=ηvGt⊙Vt⊙Ft*+(1−ηv)At−1Bt=ηvFt⊙Ft*+(1−ηv)At−1where Vtis the is the FFT of vt. Accordingly to Fig. 5the proposed approach for blurred target images performs better than MOSSE algorithm but only when almost linear motion is present (like in our periodic motion case) and the displacement direction and amount can be estimated. These results were obtained comparing our approach with the open source implementation of MOSSE provided by Raman, however we want to stress that these improvements are related to the a priori assumption that when the target is moving fast, and its image is blurred, it follows an almost straight path which can be estimated from the early motion steps. This constraint suits the oscillatory tooth brushing motion and could be applied in similar scenarios when the camera is fixed.The last step is the analysis of the toothbrush motion, in particular two aspects must be checked during tooth brushing: the correct motion of the toothbrush and the proper time devoted to every dental arch. Both of these two tasks require further information regarding the facial parts position in the image.We performed the facial parts localization task using the open source software (Xing, 2010) which is based on Active Shape Model technique, a widely used approach to extract facial parts that works very well in images with frontal faces even in case of partial occlusions. For further details about ASM we refer the reader to the literature on the topic (Huang et al., 2011; Karuppusamy et al., 2014; Wang and Song, 2011). In particular we used the eyes centers in order to estimate the facial axis, it will be useful to estimate the side of the dental arch that the user is brushing. The other relevant element from the facial analysis is the mouth position in the image and it has a high probability to be occluded by the toothbrush and toothpaste foam. The ASM algorithm, anyway, is quite robust in estimating the mouth position, even if it is completely occluded: effectively it bases its mouth position estimation also on all the other recognized facial parts. The placement of the target with respect to the facial axis indicates that the toothbrush head is insisting on the dental arch on the opposite side of the facial axis with respect to the target or it is insisting on the frontal incisor teeth. In Figs. 6and 7there are two examples of tracked faces together with the target.The proper tooth brushing technique depends on the tooth side that is being brushed, we refer to Fig. 10 for different parts. In particular for the vestibular and the oral surfaces, the vertical ones oriented towards the outer and the inner mouth parts respectively, the toothbrush motion has to be vertical, possibly with a slight rotation of the toothbrush along its axis. This further aspect will facilitate the removal of food remains trapped between teeth; a horizontal movement, on the contrary, has to be avoided since it facilitates food remains to stick in the oral pits or fissures. For the chewing surface (occlusal tooth surface), on the contrary, the horizontal toothbrush movement is correct and it regards only molar and premolar teeth. In Fig. 8 the correct and wrong movements are depicted.In order to analyze the toothbrush motion independently from the gesture speed we used a Hidden Markov Model (HMM) framework (Baum and Petrie, 1966; Rabiner, 1989). We chose this approach since HMM are versatile and robust in managing temporal pattern recognition tasks (such as speech, handwriting, gesture recognition, etc.), tasks where the same sequence (the same spoken word, the same gesture...) can have different time lengths or can be executed at different speeds. A HMM can be considered a generalization of a mixture model where the hidden variables (or latent variables), which control the mixture component to be selected for each observation, are related through a Markov process rather than independent of each other. Assuming a planet-like notation for the spherical target, we can define the rotation axis coincident with the toothbrush handle, the equator as the great circle separating the upper hemisphere from the lower one and six principal meridians separating slices of different colors (Fig. 9).In particular, accordingly to Section 4.2, we can see that a correct vestibular or oral surface brushing implies a translation of the target from lower to upper dental arch and vice versa, a further rotation along the Rotation axis of the target will further improve the cleaning quality. On the contrary a simple translation of the target along the Rotation axis with or without a slight rotation along a meridian is a wrong procedure for vestibular or oral brushing while is a correct path for occlusal brushing. For this reason estimating the orientation of the toothbrush head with respect to the dental arches is crucial both to evaluate the correctness of the procedure and the time devoted to each side. In particular, the knowledge of the toothbrush head orientation together with the facial axis allows us to recognize and analyze the 10 brushed surfaces (see Fig. 10) defined in Table 2.Using the actual approach we are unable to distinguish, both for the vestibular and the oral surfaces, the upper from the lower arch since the toothbrush head orientation and the motion is the same when brushing those surfaces.In order to estimate toothbrush head orientation from the feature descriptor we set up an HMM where the latent variables represent the target rotation angles and our aim is to infer their sequences from the observations. The observations are based on the six dimensional color descriptor detailed in Section 2.3. In particular this descriptor allows us to estimate the out-of-plane rotation angles of the target (with “plane” we intend the image plane), that are the most relevant for our scopes, while the in-plane rotation could be estimated from the orientation of the vector joining the target centroid with the estimated mouth position: obtained from the ASM (see Section 4.1). This vector represents the projection of the toothbrush handle on the image plane, but we will not use this angle in the recognition phase. We will follow the same notation of Rabiner (1989) and for any further detail about the HMM we refer the reader to that article. Our HMM graph is represented in Fig. 11where we have 25 hidden states S. In particular every hidden state represents a different target orientation: assuming that the graph in Fig. 11 represents a sort of virtual trellis around the target, we can assume that every hidden state represents a virtual camera pointing towards the target center. Then, for example, the hidden state A1 indicates a camera placed along the “rotation axis” on the opposite side with respect to the toothbrush handle; this camera is assumed to be oriented towards the south pole of the target; the hidden states Cx, wherex=1...8identify toothbrush position when the toothbrush handle is almost parallel to the image plane. The hidden states Bx and Dx represent possible views when the toothbrush handle orientation is around−45∘and 45° with respect to the optical axis, respectively. The A1 hidden state represents the situation when the camera is looking at the south pole of the target: the handle is approximately aligned to the camera optical axis. There is not a symmetrical hidden state with respect to A1 placed on the North Pole since it would be meaningless to consider a camera looking towards the target from the toothbrush head. Therefore our aim is to estimate from the color features vector the best fitting view between the 25 available. This, dually, will estimate the target rotation with respect to the real camera. Anyway, as previously mentioned, this approach will not allow us to estimate camera rotations (or, dually, target rotations) along the optical axis leaving the camera roll angle unknown. The HMM state transition probability distributionA={aij}is a 25 × 25 sparse matrix where values different from zero represent the edges in Fig. 11. The notation aijindicates the probability to jump from state Sito state Sj. Therefore, keeping in mind that every column of matrix A must sum to 1, since every edge represents a transition probability, we give to every allowed transition the same probability e.g.:(16)aC1→C2=aC1→D1=aC1→C8=aC1→B1=aC1→C1=15(17)aD3→D4=aD3→D2=aD3→C3=aD3→D3=14Note that we have always to consider the fact that the system could persist in the actual state. In our case the faced problem is what Rabiner (1989) defines as problem 2: i.e., Given the observation sequenceO=O1,O2,...,OTcorresponding to a time sequence of features vectors, and given a HMM model λ (based on the aforementioned state transition probability distribution A) how do we choose a corresponding state sequenceQ=q1,q2,...,qTthat best explains the observations? Our aim is then to uncover the hidden part of the model, the toothbrush head rotation, in a sequence of frame. In our case, we can simplify some general assumptions, since the movement that we analyze, when the user is brushing a specific dental arch, is a periodic motion. Furthermore, due to the blurring presence, in most of practical cases the target orientation can be estimated only in motion inversion points. We then decided to use as observations only the frames where the target velocity, estimated by the MOSSE tracker, is very low or the target changes direction.The observation symbol probability distribution bj, i.e. the Hue pdf associated to a specific hidden state j, is defined as the six dimensional color feature (see Section 2.3) and was obtained using a ray tracing software Lightwave. In particular, every hidden state represents all the possible viewpoints in a spherical sector of 45° in latitude and 45° in longitude. We sampled that angular sector in a matrix of 5 × 5 angles changing viewpoint at each step of 9° in yaw or pitch (see Fig. 12). For each camera position we also considered different light positions: starting from the light aligned to the camera optical axis we moved the light at ± 30° with respect to the optical axis both in vertical and in horizontal direction obtaining five different light positions for each virtual camera view. Every hidden state is then represented by a cluster of 125 histograms extracted from the 125 images rendered accordingly to the aforementioned criteria. Every histogram can be considered a vector in a six dimensional space. The usage of a ray tracing software allowed us to accurately model the different features considering self-shadows and the presence of the toothbrush handle in the framed targets.The probability of each hidden state is then evaluated, according to Rabiner (1989) notation, following this formula:(18)P(qt=Sj|xt,λ)=maxl(e−γ|xt−hl|)where xtrepresents the actual feature vector, j represents the considered hidden state (1 ≤ j ≤ 25), the “max” ranges over the 125 feature vectors of the considered (hl) hidden state, γ is the exponential decay constant (we adopted values in the range of 15 < γ < 25).The first step of the brushing analysis is based on the output from the MOSSE Tracker, analyzing the motion direction with respect to the facial axis we can get the following information: if the angle between the target velocity and the facial axis is below 30° it means that we are considering a vestibular or oral surface cleaning (vertical motion). If this is not the case we cannot say anything just from the target tracking since if the motion has an almost orthogonal direction with respect to the facial axis, we could be facing a correct occlusal brushing or a wrong vestibular or oral brushing. Considering the side of the target with respect to the facial axis we can infer that the toothbrush head is insisting on the opposite facial part with respect to the target but this will not allow us to differentiate between frontal and side parts (furthermore, for the frontal case, the target position depends from the hand, left or right, used to grasp the toothbrush). Accordingly to these considerations the HMM is crucial in order to distinguish the 10 sides of the dental arches depicted in Table 2. As described in Section 4.3 we feed the HMM with a new sample only when the target velocity estimated by the tracker is low, i.e. the periodic brushing movement is close to an inversion point. From experiments, we have seen that, accordingly to the angular quantization of the target rotation, during a periodic brushing gesture we usually have an oscillation between two or three hidden states (depicted in Fig. 11). In particular for correct vestibular and oral brushing we have a rigid target displacement (no rotations) or slight rotations along parallels of Fig. 11: a rotation along the toothbrush handle axis (rotation axis in the “planet-like” formulation of the HMM graph) is reflected in changes of hidden states along the same parallel (the capital letter indicating the hidden state does not change, e.g. qC7 → qC8). On the contrary, horizontal movements are pure translational movements or slight a rotation appears along meridians of Fig. 11: the hidden states keep the same number but change their capital letter, e.g. qC7 → qB7). This is mainly the case of a wrong vestibular brushing when the user drags the toothbrush head horizontally along the teeth external surfaces. When the user imparts large rotations to the toothbrush it could happen that the two inversion points are not connected in the HMM graph (see Fig. 11). This problem will also arise when the user (usually kids) gives unusual rotations to the target which do not correspond to parallel or meridian rotations but transitions like, qB1 → qC2 which are not allowed in our state transition matrix A. In order to overcome these problems and to get a robust estimation of the brushing procedure we used the Viterbi algorithm accordingly to Rabiner (1989). In all the comparisons the closeness of the actual target feature descriptor to a hidden state descriptor is evaluated using the L1 distance between the two vectors. This procedure allows us to properly partition the descriptors time sequence evaluating the time devoted to each part of the dental arches defined in Table 2.

@&#CONCLUSIONS@&#
In this article we present a novel approach to analyze the tooth brushing procedure. In particular, the application is oriented towards analyzing and teaching kids a correct oral hygiene. To get this result we propose a simple 3D color target placed at the end of the toothbrush handle that allows us to analyze the 3D motion imparted to the toothbrush. The effectiveness of our approach was proved in some tests where we got analysis results very close to a manual processing performed by a dentist. The described pipeline has been integrated in an game/app for tablets oriented to stimulate kids to properly brush their teeth. The results showed the success of this approach where an avatar follows the kid’s gestures in a “virtual mirror” approach indicating proper gestures and timing when wrong movements are performed. However, after 10 days of usage, when the app was removed, some bad habits in terms of timing and movements, reappear; this suggests that a longer adoption of our game has to be considered in order to increase natural inclination towards proper tooth brushing.