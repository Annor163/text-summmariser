@&#MAIN-TITLE@&#
A formal semantics of nested atomic sections with thread escape

@&#HIGHLIGHTS@&#
Semantics of languages with nested atomic sections and thread escape.A precise definition of atomicity and well-synchronisation on program traces.A mechanised proof that well-synchonisation implies strong atomicity.

@&#KEYPHRASES@&#
Atomic sections,Well-synchronisation,Atomicity,Program traces,Formal semantics,Proof assistant,

@&#ABSTRACT@&#
The multi-core trend is widening the gap between programming languages and hardware. Taking parallelism into account in the programs is necessary to improve performance. Unfortunately, current mainstream programming languages fail to provide suitable abstractions to do so. The most common pattern relies on the use of mutexes to ensure mutual exclusion between concurrent accesses to a shared memory. However, this model is error-prone and scales poorly by lack of modularity. Recent research proposes atomic sections as an alternative. The user simply delimits portions of code that should be free from interference. The responsibility for ensuring interference freedom is left either to the compiler or to the run-time system.In order to provide enough modularity, it is necessary that both atomic sections could be nested and threads could be forked inside an atomic section. In this paper we focus on the semantics of programming languages providing these features. More precisely, without being tied to a specific programming language, we consider program traces satisfying some basic well-formedness conditions. Our main contribution is the precise definition of atomicity, well-synchronisation and the proof that the latter implies the strong form of the former. A formalisation of our results in the Coq proof assistant is described.

@&#INTRODUCTION@&#
The multi-core trend in architectures development is widening the gap between programming languages and hardware. Improving performances now come at the price of a deep software renewal because it cannot be done without taking parallelism on board. Unfortunately, current mainstream programming languages fail to provide suitable abstractions to do so. The most common pattern relies on the use of mutexes to ensure mutual exclusion between concurrent accesses to a shared memory. It is widely accepted that this model is error-prone and scales poorly by lack of modularity. In this context, the user is responsible for preserving some sequences of operations from interference. This is typically done by mapping the target (a collection of data) of such operations to a lock to be held when executing the sequence. Different mappings correspond to different choices of granularity, e.g. during a list update one can choose to protect the whole list or simply the updated item and its neighbourhood. A set of coarse-grained locks helps to keep the code simple, but in general leads to poor performances. On the opposite, fine grain locks lead to better performance but the complexity growth is inversely proportional. Despite an important effort of the community to help users in specifying such mappings, mainstream programming languages still do not offer support for doing so mainly because current proposals fail in handling programs in which the mapping changes dynamically. Quoting O×³Hearn et al. [1], ownership is in the eye of the asserter.Recent research proposes atomic sections as an alternative. In this context, the user simply delimits portions of code that should be free from interference; the responsibility for ensuring interference freedom is left either to the compiler or to the run-time system. Proposals for implementing atomic sections fall in two categories, depending on the choice of an optimistic or pessimistic approach to concurrency. The former relies on transactions [2,3], a well established mechanism in database management systems. Intuitively, in this approach, sections are executed optimistically, assuming no interference, but cancelled if any interference occurs. For a discussion on issues raised by the implementation of transactions in a programming language, readers are referred to [4,5]. The latter relies on lock inference [6,7], sections are executed pessimistically, enforcing mutual exclusion between sections by means of locks. With [8] we consider that transactions are a mean, an implementation technique, to ensure atomicity. The same remark holds for lock inference and the two approaches could even be combined. For example, lock inference could be used to help a transaction-based system to deal with I/O.At first glance, and independently of the underlying implementation, atomic sections seem simpler to learn compared to more classical synchronisation primitives [9]. However, it is not yet clear whether they can be efficiently implemented [10] and whether they are really simpler, considering formal semantics and reasoning about programs. Some systems allow the nesting of transactions [11,12] while others do not [2], thus leading to poor modularity. When nesting is possible one needs to define precisely the meaning of spawning threads within an atomic section. In [13], two primitives for thread creation are proposed: The first one delays the creation until the end of the section (if it is created inside), and the second one forces the thread to live entirely inside the transaction. In [6], nesting is allowed but the lock inference scheme prevents an atomic section inside another one to run before the enclosing section terminates when they access the same memory location. More importantly, one needs to define precisely the meaning of atomicity in this context.In this paper we do not consider implementation issues but focus on the semantics of atomic sections. We consider a simple imperative language with fork/join parallelism and lexically scoped atomic sections. It supports the nesting of atomic sections, inner parallelism where threads are allowed to escape from surrounding sections, meaning that there is no synchronisation between the end of a section and the termination of threads started within this section. The semantics of the language is as permissive as possible and is not tied to any particular implementation. More precisely, we consider program traces satisfying some basic well-formedness conditions and, more importantly, satisfying the weak atomicity property, i.e there is no interference between concurrent sections. In this context, our contribution is the precise definition of atomicity, well-synchronisation and the proof that the latter implies the strong form of the former (up-to an equivalence relation over traces). A formalisation in Coq[14,15] of our results is available.We first motivate our work with some examples using parallelism and nested atomic sections (Section 2). We expose the context of this work with some related work (Section 3). We then describe the semantics domains of our work including the characterisation of well-formed traces (Section 4). We present our notion of well-synchronised traces in Section 5. Section 6 is devoted to our notion of atomicity and to the proof sketch for the main result. In Section 7 we discuss the design choices for the formalisation in the Coq proof assistant and provide a matching between the definitions and results stated in the paper and their counterparts in Coq. We conclude and give future research directions in Section 8.To motivate the need of parallelism inside atomic sections and nesting, let us see some examples of simple programs using such constructions. We need to consider an imperative language. It has the classical set of imperative instructions (loops, conditional, etc.), primitives for creation and synchronisation of threads (fork, join), and the possibility to delimit syntactically portion of code to protect with atomic section (atomic{ }). We expect this language to produce traces verifying the properties defined in the rest of the paper. We already defined a similar language with its operational semantics in [16] and proved such a result.Let us see some examples of programs where parallelism inside atomic section and nesting are useful. For example, consider a shared array list that provides a resize operation. During the resize operation, the list needs to be isolated from the other threads. So an atomic section surrounds the process. However, in order that resizing is not a performance bottleneck, the copy must be done in parallel, where each thread copies a part of the array. That is an example of parallelism in atomic section.Now suppose one wants to copy two arrays into a third one. These arrays are shared. So there must be an atomic section to protect the target array. Threads read the source arrays and write in the destination one. To read the array, threads must create an atomic section. So this section is nested in the first one.This following program does this operation. The 5-size arrays a1 a2 are merged into to the 10-size array a thanks to the method merge. This method creates an atomic section in order to protect the array a, and spawns two threads in charge of the copy, thanks to the method copy. This method copies the content of b inside a, starting at the index k of the array a. These threads do not interfere with each other because each thread writes in its own portion of array a. However, it reads a shared array, so this reading must be protected by an atomic section.

@&#CONCLUSIONS@&#
