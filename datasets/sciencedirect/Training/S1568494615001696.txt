@&#MAIN-TITLE@&#
On the evolution of ellipsoidal recognition regions in Artificial Immune Systems

@&#HIGHLIGHTS@&#
An Artificial Immune System was developed based on ellipsoidal recognition regions.Clonal selection principle was utilizes for generating best ellipsoidal regions.Different mutation procedure was applied for the speed of the algorithm.Applications were done on some benchmark data and real-world datasets taken from UCI machine learning repository.Good and promising results have been obtained.

@&#KEYPHRASES@&#
Classification,Artificial Immune Systems,Nonlinear classification,Ellipsoidal recognition regions,Clonal selection principle,

@&#ABSTRACT@&#
Using different shapes of recognition regions in Artificial Immune Systems (AIS) are not a new issue. Especially, ellipsoidal shapes seem to be more intriguing as they have also been used very effectively in other shape space-based classification methods. Some studies have done in AIS through generating ellipsoidal detectors but they are restricted in their detector generating scheme – Genetic Algorithms (GA). In this study, an AIS was developed with ellipsoidal recognition regions by inspiring from the clonal selection principle and an effective search procedure for ellipsoidal regions was applied. Performance evaluation tests were conducted as well as application results on some real-world classification problems taken from UCI machine learning repository were obtained. Comparison with GA was also done in some of these problems. Very effective and comparatively good classification ratios were recorded.

@&#INTRODUCTION@&#
Since its beginning, shape space representation scheme has taken a general acceptance in Artificial Immune Systems (AIS) community. In many AIS based algorithms, system units which are usually called as Antibodies (Abs) have some recognition regions and any input datum in an Ab's region is recognized by these Abs. Thus, input space should be carefully covered by these Abs. So far, except from some studies [1–4], spherically shaped recognition regions – this is why a term of recognition ball is used – have been utilized. In algorithms which use spherical recognition regions, a threshold that is equal to the radius of ball should be passed to enter the recognition region of an Ab and this threshold is same in all directions. However, two data points can be very near in one direction whereas they are far from each other in another direction. So, for an effective algorithm, threshold may be different with respect to the direction and this is only possible by using ellipsoidal recognition regions.Classifying data with ellipsoidal detectors is not a new finding. Some studies used this issue in classification and clustering problems. For example in [5], authors generated Ellipsoidal Adaptive Resonance Theory (E-ART) and Ellipsoidal ART-MAP (E-ART-MAP). They concluded that, depending on the problem, E-ART and E-ART-MAP can be good classifiers with respect to their fuzzy counterparts. In another study, minimum volume ellipsoids (MVE) covering data in a class were found and Hopfield Neural Network was utilized to find these ellipsoids [6]. Authors of [7] proposed MVE clustering as an alternative clustering technique to k-means for data clusters with ellipsoidal shapes. They saw that very effective clustering performances were obtained with ellipsoidal k-means and it is worth to continue studying. Many other studies were conducted related with MVE [8–12]. A similar classification method which uses ellipsoids is the study of [13,14]. In their study, authors developed an algorithm to find best ellipsoidal regions covering the input space. From the findings on some benchmark data, their system can said to be good and comparable with state-of-art works. Authors of [15] presented an effective stream clustering algorithm called Hyper-Ellipsoidal Clustering for Evolving data Stream (HECES) by making a few changes in the recently proposed Hyperellipsoidal Clustering for Resource-Constrained Environments (HyCARCE) algorithm [16], which is a strong clustering technique, in particular designed for low dimensional static data. Hsiao et al. in [17] presented a neural network based on the Ellipsoidal Function Modulated Adaptive Resonance Theory (ART) (EFM-ART). In [18], a novel method was introduced to diagnose power transformer faults based on Ellipsoidal Basis Function (EBF) neural network and this method was compared with Radial Basis Function (RBF) neural network. In another study, a convex quadratic programming representable Minimum Mahalanobis Enclosing Ellipsoid (QP-MMEE) was presented for generally unbalanced dataset classification [19]. Forghani et al. [20] investigated an Extended Support Vector Data Description (ESVDD) which describes data by using a hyper-ellipse and as a result, ESVDD can represent data better than SVDD in the input space.Similar to the above studies conducted in machine learning area, some researchers in AIS field have also used the idea of ellipsoidal recognition regions in their algorithms. In their study, authors of [1,2] have used Genetic Algorithm (GA) to evolve ellipsoidal detectors in negative selection algorithm. However they used negative selection as an inspiration source from the immunology, most of the work is done by GAs in that ellipsoidal recognition regions were evolved by GA. So, the origin of these studies can be regarded to GAs more than immune system. Some other studies can also be cited here like them but they are all the same with regard to their origin – GA finds best detectors [3,4]. In the study of [3], two synthetic datasets (star and multi-cluster) were used with four types of detector shapes which are hyper-ellipses, hyper-rectangles, hyper-spheres and mixed shapes. According to ROC curves of this study, different results were obtained for each detector shape, and low error rates were produced by mixed shaped detectors as compared to other single shaped detectors in both datasets. In the study of [4] however, 2-dimensional synthetic data was used with six types of shapes which are cross, triangle, circle, stripe, intersection and pentagram. Hart in [21] presented a study to improve and expand the work of [22]. As a result, choosing recognition area can affect the elimination of antigen and the memory capacity of emergent networks. In work of [22], they mentioned the effects of changeable forms of the recognition area of cells in an idiotypic network simulation. Stibor et al. [23] investigated the behavior of the negative selection algorithm on artificial datasets by different-sized detectors. Classification performances of negative selection, positive selection and statistical anomaly detection techniques were analyzed on a high-dimensional KDD (Knowledge Discovery and Data Mining) dataset.Whereas GA can be an effective search technique to find optimum ellipsoidal shapes, it is a generic search procedure with random mutation and recombination procedures. So, finding optimum ellipsoids can take time and sometimes ellipsoids which are indeed not optimum but seem to be locally optimum can be found (phenomenon called as catching to local optimum). Because of these negative points in GA, we developed an AIS algorithm that uses ellipsoidal recognition regions which are evolved with clonal selection principle in AIS. To fasten the search process, directed mutations depending on affinities are proposed in changing ellipsoidal shapes. At first, performance of developed system was evaluated and compared with GA on some artificially generated data. Then, comparisons with state-of-art works and GA were done on some real world classification problems. These problems are Pima Indians Diabetes Disease classification problem, Statlog Heart Disease classification problem and BUPA Liver Disorders classification problem whose datasets were taken from the UCI machine learning repository. Besides of these problems, the proposed system was also run for 5 more datasets which were again taken from UCI. Results were given in tabulated form.Before giving the details of the developed Ellipsoidal AIS system, preliminary information related with ellipsoidal shapes in higher dimensional space are given in the following.The equation for two-dimensional ellipse is given in the following [24]:(1)(x−x0)2a2+(y−y0)2b2=1where (x0, y0) are the center points and a and b are the lengths of semi-axes of x and y respectively. The following equation shows a matrix formulation of Eq. (1)[25]:(2)(x−w)TVΛVT(x−w)=1Eq. (2) gives the same formula in (1) if,x=xy,Λ=1/a2001/b2,w=x0y0andV=1001If, VΛVTin Eq. (2) is represented with ∑, then general form of an ellipsoid for n dimension can be written as:(3)(x−w)T∑(x−w)=1where w is a n×1 vector representing the center of the ellipsoid and ∑ is a real symmetric positive-definite n×n matrix. Here, V is a n×n matrix whose columns are orthonormal eigenvectors of ∑ and Λ is a n×n diagonal matrix whose entries are eigenvalues associated with the eigenvectors in V. The ith column of matrix V stands for the orientation of the ellipsoid in ith dimension. Besides, Λ defines the lengths of ellipsoid's semi-axes as the following:(4)ℓi=1Λi,iwhere ℓiis the length of the ith semi-axes [25].Changing the orientation of the semi-axes means rotating the ellipsoid. A rotation in n-space is defined by a n×n orthonormal matrix. V is an orthonormal matrix, and defines this rotation. If x−w is a point on the surface of some ellipsoid, then V(x−w) is a point on the surface of an ellipsoid that has been rotated by V. This is shown with the substitution (x−w)→V(x−w), leading to:(5)V(x−w)TVΛVTV(x−w)=1This equation can be re-arranged as:(6)(VTV(x−w))TΛVTV(x−w)=1Since V is orthonormal, VTV=I. Thus, Eq. (6) simplifies to Eq. (2). Rotation preserves the relative positions of points on the ellipsoid. Hence, if point p is on a semi-axis in the un-rotated ellipsoid, then Vp is on the rotated ellipsoid. Therefore, the orientation of the semi-axes is the columns of V [25].Tee showed that the volume of an n dimensional ellipsoid is calculated by the Eq. (7)[26]:(7)Volume=Ωn∏i=1n1Λj,iwhere Λi,iis 1/(ℓi)2 in which ℓiis the length of ith semi-axes. Ωnis the volume of an n-dimensional unit hyper-sphere. Smith and Vamanamurthy [27] show that the volume of an n-dimensional unit hypersphere is calculated by the following equation:(8)Ωn=πn/2Γ(1+(1/2)n)Here in Eq. (8), Γ() is the Gamma function. The Γ() function is a mathematical extension of the factorial function from positive integers to real numbers [28].To determine whether a p point is inside the ellipsoid or not, the following squared Mahalanobis distance can be used [29]:(9)(x−w)T∑(x−w)<1If the criterion in Eq. (9) holds, the point p is inside the ellipsoid.To obtain an orthonormal V matrix from a randomly generated matrix, Gram-Schmidt orthonormalization technique is utilized [30].Let {x1, ……, xn} be a set of n linearly independent vectors and let {y1, ……, yn} be the orthogonal set of vectors to be determined [30]. Then,(10)y1=x1y2=x2−x2,y1y1,y1y1y3=x3−x3,y2y2,y2y2−x3,y1y1,y1y1...yk=xk−∑i=1k-1xk,yiyi,yiyiAfter obtaining {y1, ……, yn} orthogonal set by using Eq. (10), each vector in this set is divided to its length to obtain the orthonormal set of {z1, ……, zn}:(11)zi=yiyiAIS takes the advantage of learning mechanisms in the human immune system to form intelligent problem solvers. The major actors of immune system are T and B cells (or lymphocytes). The foreign particles on the other hand for which immune system is activated are named as Antigens (Ags). The mechanisms in the immune system such as positive and/or negative selection, clonal selection, affinity maturation and self/non-self discrimination motivated AIS studies in such a manner that T cells and B cells form the main units of the system individually or together [31]. Especially clonal selection principle has taken a wide application range in optimization, classification and clustering problems [32]. Clonal selection algorithm (CSA) is based on two principles:(1)Only cells that recognize Ag are selected for proliferation.Selected and proliferating cells increase their sensitivity to the Ag by maturation process.The working procedure of CSA is shown in Fig. 1[33].The algorithm works as follows [33]:(1)Generate a set (P) of candidate solutions, composed of the subset of memory cells (M) added to the remaining (Pr) population (P=Pr+M).Determine (select) the n best individuals of the population (Pn), based on an affinity measure.Reproduce (clone) selected individuals giving rise to a temporary population of clones (C), the clone size is the increasing function of the affinity.Submit clones to a hypermutation sceme which is proportional to their affinity. A maturated antibody population is generated (C*).Re-select the improved individuals from C* to compose the memory set M. Some members of P can be replaced by other improved members of C*.Replace d antibodies by novel ones. The lower affinity cells have higher probabilities of being replaced.Here the affinity is a measure how an Ab (solution) fits to the Ag (problem). Many measures are used in literature such as distance measures, similarity measures, fitness functions (especially in optimization problems), etc. The key point here is that Abs are the possible solutions to the problem (to the presented Ag) and CSA tries to find best solution via processes in Fig. 1.In our proposed system we utilized from this CSA method but with some modifications. The following subtitles describe our proposed scheme.In this study, Abs were modeled as system units to recognize presented inputs which are taken as Ags. The formation of memory Abs was organized in such a manner that a standard CSA [31] was used with ellipsoidal recognition regions. The applied procedure is shown in Fig. 2. The whole procedure is run iteratively for one Ag in each time.The input data was presented to the algorithm as Ags which are vectors with N dimension:Ag=〈Ag1Ag2…AgN〉TThat is each sample in an N-dimensional feature space is represented as an Ag vector given above. An input data matrix with dimension M×N (M is the number of data, N is the number of features) was prepared in experiments.Our aim is to find memory Abs that best covers problem space (or input space) such as in Fig. 3. In the figure, Abs represents ellipsoids covering three class data. Because we are trying to find ellipsoids (Abs) covering an area in input space we represent each Ab with its center-w, V and Λ in Eq. (2) (see Section 2.1.1). w is a N-length vector, V is a N×N matrix giving the orientation of Ab in each dimension by its columns and Λ is a N×N matrix including lengths of each dimension in its diagonal. By definition, V should be an orthonormal matrix. To obtain an orthonormal matrix from a randomly generated matrix, Gram-Schmidt orthonormalization procedure was applied given in Section 2.1.3.The whole algorithm in Fig. 2 can be explained as the following:(1)If Ag is in the recognition area of a memory Ab whose class is the same with Ag,Then, take next Ag and go to step (1) again. Else, continue.(2)Selected_Abs=selection(Ags, Ab_pop)(3)Cloned_Abs=clone(Selected_Abs,cl)(4)Mutated_Abs=mutate(Cloned_Abs)(5)Ab_cand=bestAb(Mutated_Abs)(6)If fitness of Ab_cand=1Then add Ab_cand to memory Ab population (Ab_mem). Take next Ag and go to step (1).Else, form a new Ab_pop by adding some randomly generated Abs to memory population and go to step (2)Here, there is two Ab population: one is memory Ab population-Ab_mem (includes final Abs) and the other is Ab_pop which includes memory Abs and some randomly generated Abs to have diversity. In the first place of the algorithm, a control mechanism is run to find out if there is any memory Ab recognizing presented Ag, that is, including Ag in its ellipsoidal region whose class is the same with presented Ag? (step 1). If there is, the remaining procedure in the algorithm isn’t run because there is no need to form a new Ab to recognize this Ag. After this control process, selection, cloning and mutation processes, explained in the following subsections, take parts in the algorithm (steps 2–4). Then a candidate Ab (Ab_cand) is determined whose fitness value is the highest (step 5). If the fitness value of this Ab is equal to 1, then the iteration process for presented Ag is stopped and this Ab_cand is added to the Ab_mem. In other case iterations continue with returning to the step (2) again. But, before conducting same iterative process, random Abs are added to the Ab_pop to have a different Ab population from the previous iteration.Selection, cloning and mutation processes are explained in the following subsections. In these procedures and in the whole algorithm, fitness calculation of Abs is required. And we applied the following fitness calculation function for this purpose:Ab_fit=fit_calculate(Ags, Abs)%Ab_fit is a vector, each element is the fitness value of the ith AbFor each AbicorrectAg=0; % number of same class Ags, initial value is set to 0incorrectAg=0; % number of different class Ags, initial value is set to 0For each Agjif (Agj−wi)T∑i(Agj−wi)<1 AND Agj is in the same class with Abj%that is if Agjis in the region of Abiand their class are same, correctAg=correctAg+1;else if (Agj−wi)T∑i(Agj−wi)<1 AND Agj is in the different class with AbjincorrectAg=incorrectAg+1;endendif correctAg and incorrectAg are both equal to 0Ab_fit(i)=−1;% this means Abi doesn’t involve any Ag in its regionelse(12)Ab_fit(i)=correctAg−incorrectAgcorrectAg+incorrectAgendendAccording to this fitness calculation, an Ab can have a value between [−1,+1]. A value of −1 means that Ab does not involve any Ag in its recognition region and +1 means all Ags in its recognition region belong same class with it. By using Eq. (12), Abs including correct class Ags are rewarded.The selection procedure can be regarded as a function given in the following:Selected_Abs=selection(Ag,Ags,Ab_pop)%Ab_pop is an Ab population formed with memory Abs and randomly generated Abs-Ab_forAg=[]; %an empty population is prepared to store that Abs which involves presented Ag;-For each Abi in Ab_pop %Abs in Ab_pop involving presented Ag are selected in this loopif (Ag−wi)T∑i(Ag−wi)<1add Abi to Ab_forAg;end-end-Ab_fit=fit_calculate(Ags, Ab_forAg);%fitness of each Ab in Ab_forAg is calculated-Ordered_Abs=Order(Ab_fit,Ab_forAg); %Ab_forAg is ordered according to the fitness values in decreasing order-select first m Abs in Ordered_Abs as Selected_Abs % m is determined by the userCloning is simply copying Abs selected in previous step (step-3) by a number of times with using cl parameter determined by the user. This number is called as cloning parameter (this parameter is selected by trial-error fashion like GA).Cloned_Abs=clone(Abs,cl)For each Abiadd Abi to the Cloned_Abs population cl timesendThe next step in the algorithm is mutation explained in the next section. The procedures up to mutation are similar to the conventional CSA procedures. But, we applied a new mutation procedure for Ellipsoidal Abs.In many AIS algorithms using hyper-sphere recognition regions, mutation is done through expanding or narrowing recognition region. Generally, a procedure named as hyper-mutation is conducted which provides higher mutations to bad Abs and lower mutations to good Abs. By doing so, an Ab, which is very near to the optimum, changes its recognition region slightly to come closer to the optimum. On the other side, an Ab far from the optimum finds the opportunity to have major changes in its recognition region to approach optimum rapidly. This is very straightforward with hyper-spheres but in case of ellipsoids, the situation gets harder. Three important parameters affect a recognition region of an Ellipsoid: its center, its length in each dimension and its orientation. So, changing recognition region of an ellipsoid means changing one or more of these parameters. In the proposed mutation procedure of this study, an Ab can go through any of three kinds of mutation. The selection of which type of mutation will be realized is done by generating a random number which can be 1, 2 or 3 for center, length or orientation mutations respectively. The overall mutation procedure is given in the functional form as the following:Mutated_Abs=mutate(Cloned_Abs,Ags)For each Abi in the Cloned_Abs populationw=Abi.w % w: center vector to be mutated, Abi.w: center vector of the ith AbV=Abi.V %V: orientation matrix to be mutated, Abi.V: orientation matrix of the ith AbΛ=Abi. Λ % Λ: length matrix to be mutated, Abi. Λ: length matrix of the ith Abrn=generate_random(3); %a random number 1, 2 or 3 is generated and saved in rnif rn=1w_new=center_mutate(w)else if rn=2Λ_new=length_mutate(Λ)elseV_new=orient_mutate(V)endNewAb.w=w_new, NewAb. Λ=Λ_new, NewAb. V=V_newAdd NewAb in place of ith Ab in Mutated_Absend(1)Center mutation: In this kind of mutation, an Ab changes its place in input space by changing its center. The new value of the center point is determined by the mean value of same class Ags in Ab’ s recognition region. That is, letw={w1,w2,…,wn}be a vector of center point of an Ab in N-dimensional space and Ag1, Ag2, …, AgMare M Ags each of which is represented by N×1 vectors. The new center point of Ab (w_new) is determined as:w_new=center_mutate(w)same_class_Ags=[];%To involve same class Ags in the region of related Ab whose center (w) is to be mutatedfor each Agiif (Agi−w)T∑(Agi−w)<1 and the class of Agi is same with presented Ag,then add Agi to the same_class_Ags populationendw_new=mean(same_class_Ags);where w is center vector and ∑ is the VΛVTmatrix of related Ab which is to be mutated. An example of center mutation is shown in Fig. 4. Here Ab's class is class-2 and Ags represented with circular shaped data are class-2 Ags. The dashed ellipsoid is the first place of the ellipsoid and by mutating, it moves towards the mean point of class-2 data.Length mutation: The information related with lengths of ellipsoids is hold in the diagonals of Λ matrix (see Eq. (4)). So, increasing length of an axis means decreasing the related value in diagonal of Λ. The length mutation procedure in this study was different from the previous studies in that the optimal length value was searched by decreasing and increasing length iteratively. The search procedure is given in the following:Λ_new=length_mutate(Λ)For length of ith axis in N-dimensional space:ℓi=1Λi,i(1) store the beginning length and fitness value of Ab to be mutated as ℓbeginand old_fitt respectively:(2) increase the length of ith axis by the formula:(13)ℓinew=ℓiold+b∗ℓioldwhereℓinewis the new andℓioldis the old value of ith axis, while b is user defined parameter selected randomly from the interval [0-1]. Calculate the new fitness value of Ab with its new length value in its ith axis: new_fitt.(3) if new_fitt<old_fitt, stop the length increasing process and take the previous length. Otherwise old_fitt=new_fitt, return to the step (2).(4) take the beginning length and old_fitt again (ℓiold=ℓbegin). But this time decrease the length of ith axis by the formula:(14)ℓinew=ℓiold−b∗ℓiold(5) Calculate the new fitness value of Ab with its new length value in its ith axis: new_fitt. If new_fitt<old_fitt, stop the length decreasing process and take the previous length. Otherwise old_fitt=new_fitt, return to the step (4).(6) Λi,i=loptimal: Take the optimal length value after the above length increasing and decreasing processes whose fitness value is highest.By applying the above procedure in length mutation, the optimal point in fitness function is searched two directions as shown in Fig. 5by increasing or decreasing lengths. Fig. 6on the other hand presents the search process for finding optimal ellipsoids in each dimension of a 2-dimensional space by decreasing lengths.Orientation mutation: The last type of mutation that Abs can face is the orientation mutation. As also stated in [2], there is no need to change every vector in V. Because vectors in V are orthonormal, changing two randomly chosen vectors from V and rotating them will be sufficient to rotate ellipsoid. The mathematical proof for this is given in [2] and the orientation mutation of an Ellipsoid is done by using the procedure in that study as the following:V_new=orient_mutate(V)(1) Select two randomly chosen vectors from V as s1 and s2.(2) Chose a small angle θ randomly from a Gaussian distribution with mean μ=0 and deviation σ=π/2 radians. The mutated vectors s1mand s2mare determined by [2]:(15)s1m=cos(θ)s1+sin(θ)s2(16)s2m=−sin(θ)s1+cos(θ)s2(3) replace s1mand s2mto the V_new in place of old s1and s2vectors.Fig. 7shows an example of orientation mutation which resulted with success.The training algorithm gives memory Abs and class information of them as the output after training phase. In classification (test) phase, these outputs are used for determination of the classes of test Ags as the following:(1) For every test Ag (Agt, t:1, 2, ....):(1.1) Find memory Abs whose ellipsoidal recognition region involves presented Ag. (If there is not, find the ellipsoid whose center is the nearest to the Agt). Among these Abs, find the one whose center is the nearest to the presented Agt.(1.2) determine the class of memory Ab found in step (1.1) as the class of Agt→s_test(t).With this test classification process, the class of every Ag is determined (s_test). Classification accuracy of the algorithm is calculated as the following:(17)Classification_accuracy=∑i=1Nttrue_s(i)Nt(18)true_s(i)=1s_test(i)=s(i)0otherwiseHere; s_test(i): the predicted class of Agiby the algorithm, Nt: total number of test data, true_s(i): real class of Agi. Besides of classification accuracy, sensitivity and specificity values were also calculated by the formulas:Sensitivity and specificity (%):(19)sensitivity=TPTP+FN×100,specificity=TNFP+TN×100where TP: True Positive, TN: True Negative, FP: False Positive and FN: False Negative rates [34].For test results to be more valuable, k-fold cross validation is used. It minimizes the bias associated with random sampling of training data [35]. In this method, whole data is randomly divided to k mutually exclusive and approximately equally sized subsets. The classification algorithm trained and tested k times. In each case, one of the folds is taken as test data and remaining folds are added to form training data. Thus k different test results exist for each training-test configuration. The average of these results gives the test accuracy of the algorithm [35]. Besides of giving average results, statistical dispersion measures such as standard deviation, range, mean absolute deviation (MAD) and interquartile range (IQR) are given. For more information related with these measures, see [36].To evaluate the performance of the developed algorithm, three artificially generated datasets were used. The first one is a three-class, linearly separable dataset in a two-dimensional space. There are 228 data points in total. 80 data are in one class. 45 of them were used in training and 35 data are used in for the test. 74 data are in the second class and 39 of them were taken for training and remaining 35 data were used in test. Lastly, 74 data are in class-3 and 44 of them were reserved for training and 30 of them were processed as test data. The population size during the iterations was selected as 50. In every cycle 20 Abs were selected for cloning and mutation. Through choosing cloning parameter as 3, 60 Abs (20×3) go to the mutation process in iterations. Ellipsoids in the beginning and the resulted best ellipsoids in the input space are shown in Fig. 8.As shown in Fig. 8, only three Abs were produced for three class data and this will decrease the necessary extra processes in test phase when more than three Abs would be produced. The second artificially generated dataset is a two-class linearly non-separable dataset in two-dimensional space. In training, 46 data in one class and 39 data in the other form training data whose size is 85 in total. In test on the other hand, 32 data in the former class and 23 data in the latter class were used. The whole dataset including training and test datasets involves 140 data. Again, the program was run with the same parameters. Abs in the beginning and result of this run is given in Fig. 9.In this time, 6 Abs were produced for one class and 7 Abs were produced for the other. It can be seen from Fig. 9 that, ellipsoids had arranged their size adaptively to include maximum same class data but in the same time not to include different class data. With this procedure, homogenous regions can be covered with smaller number ellipsoids meanwhile heterogeneous regions are covered with many ellipsoids having smaller sizes.The last dataset to evaluate the developed system is the well-known two-spirals dataset. It consists of 190 data in which 95 data belong to one class while the remaining 95 data are of the second class. Again it is a two-dimensional data. 70 data from the first class and 70 data from the second class were chosen randomly to form the training dataset. The distribution of resulted memory Abs is shown in Fig. 10in addition to the ellipsoids in the beginning.As can be deduced from Fig. 10, 10 Abs were formed for the first class (shown with dashed lines) and 10 Abs were formed for the other (shown with straight lines).To assess the difference of our system from the GA-based ellipsoidal region evolution method, the method in [2] was applied on the above artificially generated datasets. Population size in GA-based method was determined as 50, 100 and 200 for the first, second and third datasets respectively. Both algorithms were run 10 times for all datasets. Results are given in Table 1for both method with respect to the classification accuracy.The training times and number of memory Abs with ellipsoidal regions were recorded in Table 2for both methods. As can be seen from Tables 1 and 2, there is not too much difference for linearly separable data but for nonlinear datasets (second and third one) training time increases remarkably in GA method.To evaluate the applicability and value of the developed Ellipsoidal-AIS system on real-world data, eight classification problems whose dataset were taken from the UCI machine Learning Repository [37] were used. These are Pima Diabetes, BUPA Liver disorder, Statlog Heart Disease, İris, Wisconsin Breast Cancer, Thyroid, Ionosphere and Wine classification problems. Among these, for the first three datasets, detailed explanation of the experimental procedure and results were given in the following subsections in which GA-based method in [2] was also conducted for comparison.The first used dataset was the Pima Indians Diabetes dataset. It contains 768 samples taken from healthy and unhealthy persons. 500 of these samples belong to persons with no diabetes problem while the remaining 268 samples are of persons with diabetes. The class information contained in this dataset is given by 0 for healthy persons and by 1 for diabetic patients. The number of attributes in samples is 8 in which all attributes are numerical and real valued [37].This is a very hard classification problem to solve because most of the feature values are not distinguishable for two classes. The dataset was divided into 5 folds to give the opportunity of applying 5-fold cross validation. The numbers of produced Ellipsoidal Abs, duration of the training and the resulted test classification accuracy, sensitivity and specificity values for each fold are given in Table 3. It is worth to note here that the algorithm was run 10 times for each fold and so statistical dispersion measures were also noted in the table.In Fig. 11(a), the change of fitness value along the iterations for one Ag is shown. The algorithm is stochastic. Thus, with mutation operations there can be formed such a good Ab that a sharp change like in Fig. 11(a) can occur even in a single iteration. Also, soft changes in fitness value were also common as in Fig. 11(b). If the Abs in the population are in a region near to the decision surfaces, the number of iterations to find an Ab with fitness value 1 can be very high. For example, in the experimentations for Pima diabetes classification, 120 iteration was conducted to reach fitness value 1. This iteration number can be seen as high but it took only a few seconds.The confusion matrix for a run in the fold-4 is given in Table 4as the following.To have an idea about the place of Ellipsoidal-AIS among the state-of-art methods for this problem, other applications for Pima Indians Diabetes classification were compared with this study. The results are shown in Table 5. Generally the results for this dataset lay between 71% and 80% interval. But, in Table 5, only some of them especially the ones conducted in last years are given. It should be also stated that, not all of the results were obtained with different versions of cross-validation method. Thus, we cannot compare our study with them on the same base but at least it can be seen from the table that Ellipsoidal-AIS has a very successful result, especially for a distance-based classification algorithm.Besides of the above comparison, to see the effect of evaluating ellipsoids with AIS more than GA, GA-based ellipsoidal detector generation algorithm in [2] was applied to the Pima dataset with beginning population size of 500. Its classification result was also given in Table 5. 5-CV method with 10 run in each fold was applied in GA-based method, too. Standard GA toolbox in Matlab 7.0 programming language was utilized while evolving ellipsoids. As seen from the table, the accuracy is quite lower than AIS method for this dataset. Another point to be emphasized here is that the training times between two methods were very far from each other. Experimentations with AIS method was lasted in 4h, 32m, 15s while this duration was 6h, 12m, 54s for GA-based method in the same machine.The second benchmark problem from real world to assess the value of Ellipsoidal-AIS is the BUPA Liver Disorder classification problem. The similar application steps with Pima were followed for this dataset, too. BUPA Liver Disorders dataset prepared by BUPA Medical Research Company includes 345 samples consisting of six attributes and two classes [37]. Each sample is taken from an unmarried man. 200 of these samples are of one class (healthy) with remaining 145 are belong to the other (liver disorder). First five attributes of the collected data samples are the results of blood test while the last attribute includes daily alcohol consumption [37].The situation is a bit harder for this dataset. There seems to be no evident feature that is distinguished in healthy and patient subjects. This is also the reason of low classification rates in literature for this dataset. Because of lower number of training and test data at hand, 3-fold cross validation scheme was preferred for this dataset. Again, the numbers of produced Ellipsoidal Abs, duration of training and resulted test classification accuracies for each fold are presented in Table 6. The reason for lower training times and lower number of Abs can be attributed to lower number of training data and features. But the lower classification results should be concluded from the nature of the non-distinguishing features. Although the result seems to be low, it gives Ellipsoidal-AIS a value when compared with other methods in Literature. This comparison is done in Table 6 and it can be deduced as a success for Ellipsoidal-AIS to be a good-performing method when compared with very effective state-of-art methods like ANN, Bayesian Classifier, etc. Again, like Pima, comparison with GA-based method [2] was also applied as in the same fashion (with 300 beginning population). The classification result for GA-based method, as given in Table 7, is very low with respect to our proposed method. The training time difference is again very big for two methods: Ellipsoidal-AIS: 2h, 18m, 37s GA-based method: 3h, 54m, 2s.The last real world application was done by using Statlog heart disease dataset [37]. 120 samples belong to patients with heart problem while the remaining 150 samples are of healthy persons. The samples taken from patients and healthy persons include 13 attributes [37]. The class information is included in the dataset as 1 and 2 regarding absence and presence of disease respectively. Again, because of the limited number of data, 3-fold Cross Validation was applied through dividing the whole dateset into three folds. Many undistinguishable features exist in this dataset, too. The results of 3-fold CV experiments are given in Table 8.One point that should be emphasized here that, the number of Ellipsoidal Abs and the training times were very high for this dataset. The reason for this is the excess of nominal features in this dataset. In the training phase of the Ellipsoidal Abs, Abs are mutated until no Ag in different class resides in that Ab’ s recognition region. But when nominal features are taken into account, there is a complication to include or exclude different-class Ags, because Ags in different classes can have same nominal values. Thus, including that value also causes to include different class Ags, and excluding that nominal value also causes to exclude same class Ags. For that reason the algorithm produced many Abs such that almost one Ab was produced for one Ag. This made the algorithm impractical whereas high classification rates were achieved. So, it can be deduced from this application that, the algorithm should be modified to be applicable for also nominal feature spaces, too. The results obtained with Ellipsoidal-AIS for Heart disease are compared with some of other methods in literature in Table 9. As in the same manner with previous datasets, mean and standard deviation result of GA-based method's runs was also given in Table 9. Unlike previous problems, the difference in classification accuracy is not too much between two methods for this dataset. The reason may be the nominal features as explained in the above paragraph. When training time comparison was done between two experimented methods, it was seen that Ellipsoidal-AIS had the score of 6h, 28m, 3s while GA-based method over-performed with training time of 4h, 2m, 56s.Besides of these three datasets, Ellipsoidal AIS algorithm was run for 5 more UCI datasets which are Iris, Wisconsin Breast Cancer, Thyroid Disease, Ionosphere and Wine datasets. The algorithm was run 10 times for each fold in applied 3-fold CV method in the experimentations. In the following Table 10, the mean and standard deviation results of classification accuracy values in test are given.

@&#CONCLUSIONS@&#
