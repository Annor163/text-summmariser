@&#MAIN-TITLE@&#
Dynamic scaling on the limited memory BFGS method

@&#HIGHLIGHTS@&#
Updates of the well known LBFGS method are approximately equilibrated.The initial diagonal matrix is modified to equilibrate and to approximate the Hessian.Numerical results indicate that the proposed scaling strategy is very effective.

@&#KEYPHRASES@&#
(B)Large scale optimization,(I)Nonlinear programming,Limited memory quasi-Newton methods,Column scaling,Equilibrated matrix,,

@&#ABSTRACT@&#
This paper describes a limited-memory quasi-Newton method in which the initial inverse Hessian approximation is constructed based on the concept of equilibration of the inverse Hessian matrix. Curvature information about the objective function is stored in the form of a diagonal matrix, and plays the dual role of providing an initial matrix and of equilibrating for limited memory BFGS (LBFGS) iterations. An extensive numerical testing has been performed showing that the diagonal scaling strategy proposed is very effective.

@&#INTRODUCTION@&#
In this paper, we consider minimization of a function of a large number of variables,minf(x),wheref:Rn→Ris a nonlinear and twice continuously differentiable objective function. Limited-memory BFGS method is one of the most versatile, effective and widely used classes of methods in tackling large scale problems. This approach uses Hessian approximations that can be stored compactly by using the information gathered via a few vectors of length n.A related issue is concerned with the formulation of an effective limited-memory quasi-Newton method, motivating this work. The problem is how to select the initial approximation to the Hessian at each iteration to improve the behavior of this method. Several authors studied the use of diagonal initial matrices. Diagonally scaling a matrix is intended to cluster eigenvalues or to reduce the condition number of the matrix. To compute these conversion matrices a minimal additional work is required. Moreover diagonally scaling preserves the sparsity of the matrix, if it is the case.The simplest way of diagonal scaling is setting the initial matrix to be a multiple of the identity matrix. Liu and Nocedal (1989) propose such multiple as yTs/yTy that attempts to estimate the size of the true inverse Hessian matrix along the most recent search direction. Such starting matrix approximates an eigenvalue of the inverse Hessian and appears to be the most successful in practice, see Nocedal and Wright (2006). Al-Baali (1995) investigated alternative choices for initial approximation of the inverse Hessian to yield improvement over the LBFGS method with the Liu and Nocedal choice. In one of his approaches, he argues that an alternative choice for the scalar is better than the usual one.Another approach is Jacobi scaling of a symmetric matrix A. It is defined byΔ=diag{a111/2,…,ann1/2}. It is known that such scaling is effective for sparse matrices whenever they are very diagonally dominant, see Van der Sluis (1969).Quasi-Newton diagonal updating, due to Zhu, Nazareth, and Wolkowicz (1999) is based upon weak quasi-Newton relation with matrices further restricted to be diagonal. The interpretation of a diagonal initial matrix, as a matrix that attempts to capture curvature information is incompatible with subsequently updating this matrix to a full one. This method performs poorly and suffers from a tendency to generate search direction numerically orthogonal to the gradient one, see Auroux (2000) and Roma (2005).Far more effective scaling procedure of the Hessian matrix is equilibration so that all its columns (rows) have the same length in some norm. Other equilibration algorithms exist to achieve a number of different objectives such as unit diagonal elements, equal row or column p-norms, bounded elements. Note to that the objectives may be achieved either exactly or approximately. Let us note also that there is a recommendation to equilibrate the matrix in some norm before applying any algorithm for solving a system of linear equations, since the most effective results occur for equilibrated matrices because a small perturbation of one column (or row) is then of the same magnitude as that of any other column (or row), see Wilkinson (1961) and Bunch (1971).One of the most frequently used norms in the numerical solution of linear equations is the l1-norm under which column scaling is carried out by dividing each column of the positive definite matrix by the sum of the absolute values of the column entries. In this way, required equilibration means normalization to 1.Roma (2005) proposed a diagonal preconditioner based on l1-norm equilibration. In his paper the quantities that are a lower estimate of the l1-norm of the jth column vector of the inverse Hessian were computed. These values used to build the diagonal preconditioner and no scaling is performed on those columns for which the estimate of their l1-norm is small. It is found appropriate to apply similar equilibration in the frame of the LBFGS method. The reason is that the LBFGS method can be regarded as numerical solution of linear system in which the search direction is computed so as to satisfy the secant equation. The main difference between the approach proposed in this paper and Roma’s is in the strategy taken for those columns for which the estimate of their l1-norm is small. To be precise, instead of performing no scaling for such columns, the scaling multiplier proposed by Liu and Nocedal (1989) is used to replace the corresponding diagonal entries of the initial scaling matrix.In the following section, we shall present some details about theoretical properties and practical implementation of modified LBFGS algorithm employed by the method of this paper. In Section 3, we will describe the detailed algorithm of equilibrated limited memory BFGS method based upon two loop recursive procedure. The global and local convergence results of the equilibrated limited memory BFGS method are presented in Section 4. In Section 5 we compare the Dolan and Moré (2002) performance profile of the new algorithm with LBFGS by Liu and Nocedal (1989). The conclusion is finally outlined in Section 6.In this section, the limited-memory BFGS method is briefly recalled, a more precise description of the LBFGS method can be found in Liu and Nocedal (1989) and Nocedal and Wright (2006). Then the focus will be on diagonal scaling technique which can be efficiently used in the context of LBFGS method.It is well known, the ordinary LBFGS method, proposed by Liu and Nocedal (1989), is adopted from the BFGS method to solve large scale problems. Each step of the BFGS method has the formxk+1=xk−αkHkgk,where αkis the step size, gkdenotes the gradient of f at xk, and Hkis updated at every iteration to obtain Hk + 1, defined by(1)Hk+1=(I−ρkskykT)Hk(I−ρkykskT)+ρkskskT,ρk=1ykTsk,that satisfies the secant equation Hk + 1yk= sk, in which sk= xk + 1 − xkand yk= gk + 1 − gk.To explain the strategy of the limited memory BFGS method, we first assume that m is a given small positive integer. The LBFGS method keeps the m most recent correction pairs,{si,yi}i=k−mk−1,to update m times, the initial matrixHk0=sk−1Tyk−1/yk−1Tyk−1I,and computes the productpk=−Hkgk,efficiently via performing two-loop recursion procedure as follows.Algorithm 1LBFGS two loop recursionq ← gk;For    i = k − 1, k − 2, …, k − mComputeρi=1yiTsi;αi←ρisiTq;q ← q − αiyi;end(for)r←Hk0q;For    i = k − m, k − m − 1, …, k − 1χ←ρiyiTr;r ← r + si(αi− χ);end(for)Stop with result Hkgk= r.This recursion has advantages of reasonable implementation cost. On one hand, the multiplication by the initial matrixHk0is isolated from the rest of the computations, allowing this matrix to be chosen frequently and to vary between iterations. We consider this feature of great importance because the inverse Hessian matrix Hkcan change drastically during the course of the optimization iteration and to use the same initial approximation to the inverse Hessian throughout the run is questionable. Therefore, the idea of having a dynamic initial matrices that can be handle inexpensively is appealing. On the other hand, within recursion approach, it is not necessary to form the Hessian matrix explicitly because it requires only the computation of matrix–vector products involving this matrix, see Nocedal and Wright (2006). These features motivate several practical modification strategies. Al-Baali (1995) suggests an initial matrix in the form of a scalar multiple of the identity asHk0=vkIwherevk=max{sk−1Tyk−1yk−1Tyk−1,sk−mTyk−myk−mTyk−m}.This strategy is justified by the fact that under certain conditions such as vk≥ 1, self scaling methods converge globally and superlinearly on convex objective functions, see Al-Baali (1992); 1995).In general the performance of a method involving the solution of a generic linear system, e.g. Ax = b, can be accelerated by equilibrating the linear system to improve the eigenvalue distribution of A, see Schneider and Zenios (1990). It is worthwhile noticing the fact that computing the LBFGS search direction is equivalent to solving the linear system H−1p = −g.Therefore in the sequel of this section we seek for an initial scaling matrix which equilibrates the Hessian updates while being able to imbed in the framework of the LBFGS method. For this purpose we use an equilibration strategy based on columns scaling of the inverse Hessian matrix H(xk) which uses the ℓ1-norm. For simplicity, we drop the iteration index k in the linear system H−1p = −g or equivalently p = −Hg. If the entries and the columns of the inverse Hessian matrix are denoted by hijand aj, respectively, for i, j = 1, …, n, then we have aj= Hejwhere ej= (0, …, 1, …, 0)Tis the jth unit vector. We can choose diagonal matrix M such thatM=diag{∥a1∥1,…,∥an∥1},and we deduce that the matrix HM−1 is a column equilibrated matrix in the ℓ1-norm.The following results analyze the condition number, denoted by κ1 in the ℓ1-norm, and the eigenvalues distribution of the equilibrated matrix HM−1. Their proof can be found in the paper of Roma (2005) and Higham (2002).Proposition 1If H is nonsingular and Dn denotes the set of n × n nonsingular diagonal matrices, thenκ1(HM−1)≤minD∈Dnκ1(HD).Therefore in terms of condition number in ℓ1-norm, the proposed column scaling matrix is the best among all column scaling matrices.Proposition 2Let ρ(HM−1) = max   { |λi|: λieigenvaluesofHM−1 } be the spectral radius of the matrix HM−1. Thenρ(HM−1) ≤ 1.This proposition implies that the use of the proposed scaling matrix enables to have clustered eigenvalues and hence the algorithm will identify the solution in smaller number of iterations.We turn our attention now to embedding this diagonal scaling strategy in the framework of the LBFGS method. The idea to attain a practical algorithm is focusing on the computation of the Hessian–vector product. By simply setting(2)σj=|∑i=1nhij|,for each jth column, j = 1, …, n we get a reasonable estimate (a lower estimate) of the ℓ1-norm of the columns of the Hessian matrix. We do not compute σj,  j = 1, …, n explicitly, but rather, it requires only to supply Hessian–vector products of the form He, where e = (1, …, 1)T. Then absolute values are applied component-wise on the resulting vector. As a special case of this result, we see that σj= ‖aj‖1, for  all j = 1, …, n whenever the Hessian is a matrix with nonnegative entries. In this case this scaling matrix possesses attractive theoretical properties stated above.We could consider building M from the quantities (2) as(3)M=diag{σ˜1,σ˜2,…,σ˜n},where for j = 1, …, n,(4)σ˜j={σjσj>δ,yTsyTyotherwise,for some small δ > 0. The parameter δ is chosen to be 10−6. Therefore a simple modification into Roma’s approach (Roma, 2005) is applied to the definition of the diagonal matrix M that uses the current curvature information. Scaling is also performed on those columns for which σjis small. As observed, at each iteration of the LBFGS method, the positive diagonal matrix M that equilibrates the Hessian, is approximately computed by means of a single product of the Hessian matrix times the vector e = (1, 1, …, 1)T, thus limiting the additional cost only to an extra matrix–vector product, for each iteration.In this section, we present a description of our “equilibration of the LBFGS” (ELBFGS) method. As we mentioned in Section 2, the LBFGS method does not require the storage of the n × n full inverse Hessian matrix, instead it stores a modified version of H implicitly using 2m vectors of length n, where m is a small positive integer and n is the number of variables.To describe the updating process, suppose at step k, we have stored m most recent correction pairs{si,yi}i=k−mk−1. We first choose the initial inverse Hessian approximationHk0=γkI,whereγk=sk−1Tyk−1yk−1Tyk−1,see Nocedal and Wright (2006). Then the matrixHk0which is now the multiple of the identity matrix is modified to take account of nearly column equilibration of Hkin ℓ1-norm. For this purpose, we invoke Algorithm 1 involving the Hessian–vector product to compute Hke. This results in a diagonal matrix M by which those elements of M that are greater than 10−6 take the place of the corresponding elements ofHk0while other elements of the matrixHk0are left unchanged. The yielded matrixHk0is used both to equilibrate and to scale Hkusing the recursion procedure of Algorithm 1 in which the next search direction − Hkgkis computed. Therefore ELBFGS method invokes the Algorithm 1 twice to calculate the required matrix–vector products.In this section, the global and local convergence properties of the ELBFGS algorithm are stated. Technically, the equilibrated LBFGS matrix Hkis obtained by updating a bounded matrixHk0,m times, using the BFGS formula. For the purposes of our analysis, we shall use the direct updateBk=Hk−1,which is the Hessian approximation to the objective function, associated with the Eq. (1). It is worth noting that the R-linear convergence result for the LBFGS method obtained under mild conditions by Liu and Nocedal (1989) is still valid for the modified LBFGS method considered here. The reason is that the new updating formula can be written like the LBFGS formula, except that the initial matrix is replaced by a diagonal matrix with different entries rather than a diagonal matrix with identical entries. Therefore at the beginning of each iteration we assume thatBk0=Hk0−1whereHk0−1is a diagonal matrix whose elements are defined by (4).Algorithm 2ELBFGSGiven an initial point x0, m > 0, 0 < c1 < c2 < 1, a symmetric positive definite initial matrixB0=B00;k ← 0;repeatCompute the search directionpk←−Bk−1gk;Compute xk + 1 = xk+ αkpk where αk is chosen to satisfy the Wolfe conditions(5)f(xk+αkpk)≤f(xk)+c1αkgkTpk,(6)gT(xk+αkpk)pk≥c2gkTpk,where c1 ∈ (0, 1)  and  c2 ∈ (c1, 1)are some constants.If k > m, discard the vector pair {sk − m, yk − m} from storage.Compute and save sk= xk + 1 − xk and yk= gk + 1 − gk;UpdateBk0,mtimes, using the pairs{si,yi}k−m+1k,via following formula:forl = m − 1   downto   0(7)Bk(m−l)=Bk(m−1−l)−Bk(m−1−l)sk−lsk−lTBk(m−1−l)sk−lTBk(m−1−l)sk−l+yk−lyk−lTyk−lTsk−l;SetBk+1=Bk(m);k ← k + 1;until terminationNow to state the global convergence of Algorithm 2 and its convergence rate, we make the following assumptions on the objective function f.Assumption 1(a)Ω={x∈Rn:f(x)≤f(x0)}is a convex set.f is twice continuously differentiable.f is uniformly convex function, i.e. there exist positive constants M1and MLBFGS such that(8)M1∥z∥2≤zTG(x)z≤M2∥z∥2,for allz∈Rnand all x ∈ Ω.Let x0be a starting point, for which f satisfiesAssumption 1, and suppose matricesBk0are symmetric positive definite initial matrices, for which{∥Bk0∥}and{∥Bk0−1∥}are bounded. Then, the sequence {xk}generated byAlgorithm 2converges to the unique x*on Ω, and the convergence rate is R-linear, that is, there is a constant 0 ≤ r < 1, such that(9)f(xk)−f(x*)≤rk(f(x0)−f(x*)).The proof of Theorem 3 can be found in Liu and Nocedal (1989).This section evaluates the performance of the ELBFGS algorithm on a set of 1022 unconstrained optimization test problems. At the same time, we compare the performance of ELBFGS with the standard LBFGS algorithm proposed by Liu and Nocedal (1989) and the modified limited memory BFGS method described by Al-Baali (1995), denoted by MLBFGS in this paper. All codes are written in Fortran 77 in double precision and implement the same stopping criterion∥g∥2≤ϵmax{1,∥x∥2},where ε = 10−5. All algorithms use exactly the same line search strategy, based on quadratic and cubic interpolations. Line searches are terminated when the following strong Wolfe conditions are satisfied.(10)f(xk+αkpk)≤f(xk)+c1αkgkTpk,(11)|gT(xk−1+αk−1pk−1)pk−1|≤c2|gk−1Tpk−1|,with c1 = 0.01 and c2 = 0.9 (see for example Fletcher, 1987; Zhu et al., 1999).We examine 73 test problems (the same were used by Al-Baali, 1992; 1995) which are selected from Andrei (2008) with the standard starting points. For each test problem, 14 runs with dimension ranging from 1000 to 50,000 are performed. Numerical testing of the methods is carried out with m = 5. A small sample of results is given in Tables 1–3. They are representative of what has been observed.The meanings of some of the variables used in the following tables are as follows:–DIM: the number of variables.–n-I: the number of iterations.–n-F: the total number of function or gradient evaluations.We observe that the performance of the ELBFGS method is substantially better than those of LBFGS and MLBFGS in most cases. We note that the number of iterations and the number of function and gradient calls required to solve some problems by ELBFGS are smaller than approximately 35 percent of the number required by LBFGS (e.g., DIXMAANG). The opposite is on some test problems where for solving them the original scaling of limited memory BFGS and MLBFGS is better than the dynamic scaling. However, cases like this rarely occurred.For the methods being analyzed, the fraction P of problems is plotted for any given method within a factor τ of the best number of iterations and number of function-gradient evaluations, based on the performance profiles of Dolan and Moré (2002). The percentage of the test problems for which a method is better is given on the left axis of the plot. The right hand side of the plot gives the percentage of the test problems that are successfully solved by these algorithms, respectively. As it can be observed, the use of the diagonal scaling ELBFGS enables a considerable computational saving on most problems with respect to LBFGS and MLBFGS methods, in terms of number of iterations and number of function and gradient calls, and on the most test problems the gain is impressive.It is observed from Figs. 1 and 2that the ELBFGS is always the top performer for the most values of τ.Comparison can be proceeded by means of overall relative geometric means and arithmetic means of algorithms ELBFGS and MLBFGS over LBFGS method based on measurements including number of iterations and number of function and gradient evaluations. The results are given in Table 4.Table 4 shows favorable relative reduction in the criteria being considered for ELBFGS method over LBFGS method.Another analysis of the results can be carried on by considering the cumulative results, that is the total number of iterations and the total number of function and gradient evaluations needed to solve all the problems in toto. These cumulative results are reported in Table 5. They confirm the effectiveness of ELBFGS in terms of all the criteria considered.Note that the storage is exactly the same for both methods. But from arithmetics point of view, ELBFGS method implements two loop recursion procedure twice, resulting (4m + 1)n additional multiplications which are inexpensive. This means that the cost of building and applying initial diagonal approximation to the inverse Hessian is very low, thus succeeding not only to offset the additional computational effort, but also allowing to obtain an overall computational saving.

@&#CONCLUSIONS@&#
