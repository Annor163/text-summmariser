@&#MAIN-TITLE@&#
Geological mapping using remote sensing data: A comparison of five machine learning algorithms, their response to variations in the spatial distribution of training data and the use of explicit spatial information

@&#HIGHLIGHTS@&#
Robust comparison of five machine learning strategies.Random Forests is a good first choice for classifying lithology.Spatial distribution of training data has considerable influence on predictions.Using coordinates and geophysics data generates accurate and plausible predictions.

@&#KEYPHRASES@&#
Geological mapping,Remote sensing,Machine learning,Supervised classification,Spatial clustering,Spatial information,

@&#ABSTRACT@&#
Machine learning algorithms (MLAs) are a powerful group of data-driven inference tools that offer an automated means of recognizing patterns in high-dimensional data. Hence, there is much scope for the application of MLAs to the rapidly increasing volumes of remotely sensed geophysical data for geological mapping problems. We carry out a rigorous comparison of five MLAs: Naive Bayes, k-Nearest Neighbors, Random Forests, Support Vector Machines, and Artificial Neural Networks, in the context of a supervised lithology classification task using widely available and spatially constrained remotely sensed geophysical data. We make a further comparison of MLAs based on their sensitivity to variations in the degree of spatial clustering of training data, and their response to the inclusion of explicit spatial information (spatial coordinates). Our work identifies Random Forests as a good first choice algorithm for the supervised classification of lithology using remotely sensed geophysical data. Random Forests is straightforward to train, computationally efficient, highly stable with respect to variations in classification model parameter values, and as accurate as, or substantially more accurate than the other MLAs trialed. The results of our study indicate that as training data becomes increasingly dispersed across the region under investigation, MLA predictive accuracy improves dramatically. The use of explicit spatial information generates accurate lithology predictions but should be used in conjunction with geophysical data in order to generate geologically plausible predictions. MLAs, such as Random Forests, are valuable tools for generating reliable first-pass predictions for practical geological mapping applications that combine widely available geophysical data.

@&#INTRODUCTION@&#
Machine learning algorithms (MLAs) use an automatic inductive approach to recognize patterns in data. Once learned, pattern relationships are applied to other similar data in order to generate predictions for data-driven classification and regression problems. MLAs have been shown to perform well in situations involving the prediction of categories from spatially dispersed training data and are especially useful where the process under investigation is complex and/or represented by a high-dimensional input space (Kanevski et al., 2009). In this study we compare MLAs, applied to the task of supervised lithology classification, i.e. geological mapping, using airborne geophysics and multispectral satellite data. The algorithms that we evaluate represent the five general learning strategies employed by MLAs: Naive Bayes (NB) – statistical learning algorithms, k-Nearest Neighbors (kNN) – instance-based learners, Random Forests (RF) – logic-based learners, Support Vector Machines (SVM), and Artificial Neural Networks (ANN) – Perceptrons (Kotsiantis, 2007).The basic premise of supervised classification is that it requires training data containing labeled samples representing what is known about the inference target (Kotsiantis, 2007; Ripley, 1996; Witten and Frank, 2005). MLA architecture and the statistical distributions of observed data guides the training of classification models, which is usually carried out by minimizing a loss (error) function (Kuncheva, 2004; Marsland, 2009). Trained classification models are then applied to similar input variables to predict classes present within the training data (Hastie et al., 2009; Witten and Frank, 2005).The majority of published research focusing on the use of MLAs for the supervised classification of remote sensing data has been for the prediction of land cover or vegetation classes (e.g., Foody and Mathur, 2004; Ham et al., 2005; Huang et al., 2002; Pal, 2005; Song et al., 2012; Waske and Braun, 2009). These studies use multi or hyperspectral spectral reflectance imagery as inputs and training data is sourced from manually interpreted classes. MLAs such as RF, SVM and ANN are commonly compared in terms of their predictive accuracies to more traditional methods of classifying remote sensing data such as the Maximum Likelihood Classifier (MLC). In general, RF and SVM outperform ANN and MLC, especially when faced with a limited number of training samples and a large number of inputs and/or classes. Previous investigations into the use of MLAs for supervised classification of lithology (e.g., Leverington, 2010; Leverington and Moon, 2012; Oommen et al., 2008; Waske et al., 2009; Yu et al., 2012) focus on comparing MLAs, such as RF and/or SVM, with more traditional classifiers.Common to all remote sensing image classification studies is the use of geographical referenced input data containing co-located pixels specified by coordinates linked to a spatial reference frame. Despite this, inputs used in the majority of studies cited do not include reference to the spatial domain. This is equivalent to carrying out the classification task in geographic space where samples are only compared numerically (Gahegan, 2000). To date few investigations have evaluated the performance of MLAs in conjunction with the input of spatial coordinates. Kovacevic et al. (2009), for example, investigated the performance of SVM using Landsat ETM+ multispectral bands and spatial coordinates, concluding that, given training data of suitable quality, there was sufficient information in the spatial coordinates alone to make reliable predictions. However, when applying trained classification model to regions outside the spatial domain of the training data the information in Landsat ETM+ bands became increasingly important.Our supervised lithology classification example evaluates MLA performance in the economically important Broken Hill area of western New South Wales, a region of Paleoproterozoic metasedimentary, metavolcanic and intrusive rocks with a complex deformation history. In this study, we use airborne geophysics and Landsat ETM+ imagery to classify lithology. Airborne geophysics, unlike satellite spectral reflectance imaging, it is not affected by cloud and/or vegetation cover and represents the characteristics of surface and near surface geological materials (Carneiro et al., 2012). Landsat ETM+ data are freely available and have extensive coverage at medium resolutions over large regions of the globe (Leverington and Moon, 2012). Although hyperspectral data has been shown to generate excellent results in sparsely vegetated regions due to high spectral and spatial resolutions (Waske et al., 2009) this data is limited in its coverage and ability to penetrate dense vegetation for the characterization of geological materials (Leverington and Moon, 2012).We explore and compare the response of MLAs to variations in the spatial distributions and spatial information content of training data, and their ability to predict lithologies in spatially disjoint regions. We facilitate this comparison by conducting three separate experiments: (1) assessing the sensitivity of MLA performance using different training datasets on test samples not located within training regions; (2) random sampling of multiple training datasets with contrasting spatial distributions; and (3) using three different combinations of input variables, X and Y spatial coordinates (XY Only), geophysical data (i.e. airborne geophysics and Landsat ETM+ imagery; no XY), and combining geophysical data and spatial coordinates (all data). These experiments are combined to provide a robust understanding of the capabilities of MLAs when faced with training data collected by geologists in challenging field sampling situations using widely available remotely sensed input data.Classification can be defined as mapping from one domain (i.e. input data) to another (target classes) via a discrimination function y=f(x). Inputs are represented as d vectors of the form 〈x1, x2, …, xd〉 and y is a finite set of c class labels {y1, y2, …, yc}. Given instances of x and y, supervised machine learning attempts to induce or train a classification model f', which is an approximation of the discrimination function, ŷ=f'(x), and maps input data to target classes (Gahegan, 2000; Hastie et al., 2009; Kovacevic et al., 2009). In practice, as we only have class labels for a limited set of data, Τ, it is necessary to divide available data into separate groups for training and evaluating MLAs. Training data, Τa, are used to optimize and train classification models via methods such as cross-validation. Test data, Τb, contains an independent set of samples not previously seen by the classifier and is used to provide an unbiased estimate of classifier performance (Hastie et al., 2009; Witten and Frank, 2005).The task of MLA supervised classification can be divided into three general stages, (1) data pre-processing, (2) classification model training, and (3) prediction evaluation. Data pre-processing aims to compile, correct, transform or subset available data into a representative set of inputs. Pre-processing is motivated by the need to prepare data so that it contains information relevant to the intended application (Guyon, 2008; Hastie et al., 2009).MLAs require the selection of one or more algorithm specific parameters that are adjusted to optimize their performance given the available data and intended application (Guyon, 2009). With only Τaavailable for training and estimating performance, the use of techniques such as k-fold cross-validation is required. Trained classification model performance is usually estimated by summing or averaging over the results of k folds. Parameters that generate the best performing classifier, given the conditions imposed, are used to train a MLA using all of the samples in Τa(Guyon, 2009; Hastie et al., 2009).An unbiased evaluation of the ability of MLAs to classify samples not used during training, i.e. to generalize, is achieved using Τb(Hastie et al., 2009; Witten and Frank, 2005). Classifier performance metrics such as overall accuracy and kappa (Lu and Weng, 2007) are easily interpretable and commonly used measures of MLA performance for remote sensing applications (Congalton and Green, 1998).Naive Bayes (NB) is a well known statistical learning algorithm recommended as a base level classifier for comparison with other algorithms (Guyon, 2009; Henery, 1994). NB estimates class-conditional probabilities by “naively” assuming that for a given class the inputs are independent of each other. This assumption yields a discrimination function indicated by the products of the joint probabilities that the classes are true given the inputs. NB reduces the problem of discriminating classes to finding class conditional marginal densities, which represent the probability that a given sample is one of the possible target classes (Molina et al., 1994). NB performs well against other alternatives unless the data contains correlated inputs (Hastie et al., 2009; Witten and Frank, 2005).The k-Nearest Neighbors (kNN) algorithm (Cover and Hart, 1967; Fix and Hodges, 1951) is an instance-based learner that does not train a classification model until provided with samples to classify (Kotsiantis, 2007). During classification, individual test samples are compared locally to k neighboring training samples in variable space. Neighbors are commonly identified using a Euclidian distance metric. Predictions are based on a majority vote cast by neighboring samples (Henery, 1994; Kotsiantis, 2007; Witten and Frank, 2005). As high k can lead to over fitting and model instability, appropriate values must be selected for a given application (Hastie et al., 2009).Random Forests (RF), developed by Breiman (2001), is an ensemble classification scheme that utilizes a majority vote to predict classes based on the partition of data from multiple decision trees. RF grows multiple trees by randomly subsetting a predefined number of variables to split at each node of the decision trees and by bagging. Bagging generates training data for each tree by sampling with replacement a number of samples equal to the number of samples in the source dataset (Breiman, 1996). RF implements the Gini Index to determine a “best split” threshold of input values for given classes. The Gini Index returns a measure of class heterogeneity within child nodes as compared to the parent node (Breiman et al., 1984; Waske et al., 2009). RF requires the selection of mtry which sets the number of possible variables that can be randomly selected for splitting at each node of the trees in the forest.Support Vector Machines (SVM), formally described by Vapnik (1998), has the ability to define non-linear decision boundaries in high-dimensional variable space by solving a quadratic optimization problem (Hsu et al., 2010; Karatzoglou et al., 2006). Basic SVM theory states that for a non-linearly separable dataset containing points from two classes there are an infinite number of hyperplanes that divide classes. The selection of a hyperplane that optimally separates two classes (i.e. the decision boundary) is carried out using only a subset of training samples known as support vectors. The maximal margin M (distance) between the support vectors is taken to represent the optimal decision boundary. In non-separable linear cases, SVM finds M while incorporating a cost parameter C, which defines a penalty for misclassifying support vectors. High values of C generate complex decision boundaries in order to misclassify as few support vectors as possible (Karatzoglou et al., 2006). For problems where classes are not linearly separable, SVM uses an implicit transformation of input variables using a kernel function. Kernel functions allow SVM to separate non-linearly separable support vectors using a linear hyperplane (Yu et al., 2012). Selection of an appropriate kernel function and kernel width, σ, are required to optimize performance for most applications (Hsu et al., 2010). SVM can be extended to multi-class problems by constructing c(c-1)/2 binary classification models, the so called one-against-one method, that generate predictions based on a majority vote (Hsu and Lin, 2002; Melgani and Bruzzone, 2004).Artificial Neural Networks (ANN) have been widely used in science and engineering problems. They attempt to model the ability of biological nervous systems to recognize patterns and objects. ANN basic architecture consists of networks of primitive functions capable of receiving multiple weighted inputs that are evaluated in terms of their success at discriminating the classes in Τa. Different types of primitive functions and network configurations result in varying models (Hastie et al., 2009; Rojas, 1996). During training network connection weights are adjusted if the separation of inputs and predefined classes incurs an error. Convergence proceeds until the reduction in error between iterations reaches a decay threshold (Kotsiantis, 2007; Rojas, 1996). We use feed-forward networks with a single hidden layer of nodes, a so called Multi-Layer Perceptron (MLP) (Venables and Ripley, 2002), and select one of two possible parameters: size, the number nodes in the hidden layer.This study covers an area of ~160km2 located near Broken Hill, far western New South Wales, Australia (Fig. 1). The geology of the Broken Hill Domain (BHD) (Webster, 2004) features an inlier of the Paleoproterozoic Willyama Supergroup (WSG) (Willis et al., 1983). WSG contains a suite of metamorphosed sedimentary, volcanic and intrusive rocks, deposited between 1710 and 1704±3Ma (Page et al., 2005a; Page et al., 2005b). WSG features complex lithology distributions resulting from a long history of folding, shearing, faulting and metamorphism (Stevens, 1986; Webster, 2004). BHD is of significant economic importance as it is the location of the largest and richest known Broken Hill Type stratiform and stratabound Pb–Zn–Ag deposit in the world (Webster, 2004; Willis et al., 1983).Within the study area defined for this experiment are 13 lithology classes that form a chronological sequence younging from west to east. In general, WSG basal units are dominated by quartzo-feldspathic composite gneisses (i.e., Thorndale Composite Gneiss and Thackaringa Group), and are overlain by dominantly psammitic and pelitic metasedimentary rocks (i.e., Allendale Metasediments, Purnamoota Subgroup and Sundown Group).Table 1 provides a detailed summary of lithology classes present within the study area.BHD deformation history can be summarized into four events (Stevens, 1986; Webster, 2004). The first two events of the Olarian Orogeny (1600–1590Ma; Page et al., 2005a; Page et al., 2005b) are associated with amphibolite–granulite facies metamorphism and north-northeast to south–southwest trending regional fabric. A third event is characterized by localized planar or curviplanar zones of Retrograde Schist. These zones fulfill the role of faults in the BHD and display well developed and intense schistosity, strongly deformed metasediment bedding, and generally displace the units they intersect (Stevens, 1986). The fourth deformation event, associated with the Delamerian Orogeny (458–520Ma) (Webster, 2004), is interpreted from gentle dome and basin structures.The published 1:250,000 Broken Hill digital geological map, compiled by Buckley et al. (2002), was used to obtain labeled samples representing lithology classes for training and to evaluate MLA predictions. We maintained the number of training samples at 10% (~6500) of the total number of samples in the selected scene. Multiple sets of Tawere randomly sampled from approximately circular regions, randomly seeded across the study area. The number of training regions were varied from 1 to 1024, such that the number of regions was equal to 2a, where a represents sequential integers from 0 to 10. In all cases, the total coverage of training regions equates to >10% and <20% of the study area (Fig. 2).Spatially disjoint Tbsamples were sampled from all other pixels in the study area not contained within Τa, equating to >80% of the total number of samples. We randomly sample 10 sets of Τaand Τbfor each combination of training clusters in order to avoid any bias associated with individual groups of Τaor Τb. Due to the natural variability of lithological class distributions, Tasampled from low numbers of training regions often did not contain representatives for all 13 lithological classes. This leads to biased accuracy assessments due to the incorporation of samples that cannot be accurately classified. Therefore, those classes not represented in Τawere eliminated from their corresponding Τbwhen evaluating MLA predictions.Airborne geophysical data used in this study contained a Digital Elevation Model (DEM ASL), Total Magnetic Intensity TMI (nT), and four Gamma-Ray Spectrometry (GRS) channels comprising Potassium (K%), Thorium (Th ppm), Uranium (U ppm), and Total Count channels. The Landsat 7 ETM+ data contained 8 bands supplied with Level 1 processing applied. Landsat 7 ETM+ band 8, which covers the spectral bandwidths of bands 2, 3, and 4 (Williams, 2009) was not included.

@&#CONCLUSIONS@&#
We compared five machine learning algorithms (MLAs) in terms their performance with respect to a supervised lithology classification problem in a complex metamorphosed geological terrane. These MLAs, Naive Bayes, k-Nearest Neighbors, Random Forests, Support Vector Machines and Artificial Neural Networks, represent the five general machine learning strategies for automated data-driven inference. MLA comparisons included their sensitivity to variations in spatial distribution of training data, and response to the inclusion of explicit spatial information.We conclude that Random Forests is a good first choice MLA for multiclass inference using widely available high-dimensional multisource remotely sensed geophysical variables. Random Forests classification models are, in this case, easy to train, stable across a range of model parameter values, computationally efficient, and when faced with spatially dispersed training data, substantially more accurate than other MLAs. These traits, coupled with the insensitively of Random Forests to noise and over fitting, indicate that it is well suited to remote sensing lithological classification applications.As the spatial distribution of training data becomes more dispersed across the region under investigation, MLA predictive accuracy (and kappa) increases. In addition, the sensitivity of MLA categorical predictions to different training datasets decreases. The inclusion of explicit spatial information (i.e. spatial coordinates) proved to generate highly accurate MLA predictions when training data was dispersed across the study area. Despite resulting in lower test accuracy (and kappa), the use of geophysical data provided MLAs with information that characterized geological structural trends. Therefore, combining spatial and geophysical data is beneficial, especially in situations where training data is moderately spatially clustered. Our results illustrate the use of machine learning techniques to address data-driven spatial inference problems and will generalize readily to other geoscience applications.