@&#MAIN-TITLE@&#
A multi-start central force optimization for global optimization

@&#HIGHLIGHTS@&#
Central force optimization (CFO) is deterministic metaheuristic algorithm.A multi-start strategy is designed to enhance the search capability of CFO.The proposed algorithm performs well, compared to other algorithms.

@&#KEYPHRASES@&#
Central force optimization,Deterministic algorithm,Multi-start strategy,Global optimization,

@&#ABSTRACT@&#
Central force optimization (CFO) is an efficient and powerful population-based intelligence algorithm for optimization problems. CFO is deterministic in nature, unlike the most widely used metaheuristics. CFO, however, is not completely free from the problems of premature convergence. One way to overcome local optimality is to utilize the multi-start strategy. By combining the respective advantages of CFO and the multi-start strategy, a multi-start central force optimization (MCFO) algorithm is proposed in this paper. The performance of the MCFO approach is evaluated on a comprehensive set of benchmark functions. The experimental results demonstrate that MCFO not only saves the computational cost, but also performs better than some state-of-the-art CFO algorithms. MCFO is also compared with representative evolutionary algorithms. The results show that MCFO is highly competitive, achieving promising performance.

@&#INTRODUCTION@&#
Metaheuristics have been a very active research area of optimization techniques for several decades. The term metaheuristic, introduced by Glover [1], derives from the composition of two Greek words. The word heuristic origins from the old verb heuriskein, which means “to discover”, while the prefix meta means “upper level methodology”. Most metaheuristic algorithms are nature-inspired. Famous examples of metaheuristics include simulated annealing (SA) [2–5], genetic algorithm (GA) [6–9], ant colony optimization (ACO) [10–13], and particle swarm optimization (PSO) [14–18], although many more exist. Metaheuristics are generally used to solve complex problems where other optimization methods have failed to be either effective or efficient. They have rapidly progressed in recent years and successfully been applied to diverse domains of science and engineering. A common characteristic of almost all metaheuristics is that they have a stochastic behavior. They produce different results with the same settings to the parameters. In addition, the results are calculated for these methods through statistical analysis of a large number of simulations. It is impractical if the computation cost required to evaluate the objective function is large.Central force optimization (CFO), proposed by Formato [19], is deterministic nature-inspired metaheuristic algorithm, which is based on the metaphor of gravitational kinematics. CFO uses a group of probes to search for global optimum. The trajectory of each probe is adjusted by two deterministic equations of motion. CFO is completely deterministic at every step. Its inherently deterministic feature is a significant distinction from almost all other metaheuristic algorithms that are fundamentally stochastic, GA, and ACO being good examples. Because of the inherent randomness, GA or ACO with the same parameters generates a different outcome. By contrast, CFO with the same parameters produces exactly the same result. Moreover, only one run is needed to assess the performance of CFO, no statistics required [20].Due to CFO's deterministic nature, easy implementation yet effectiveness, it has become quite popular in optimization techniques. It has been successfully applied in many real-world applications, such as the Fano Load Equalizer and linear array synthesis problems [19], antenna design [21], leakage detection for drinking water networks [22], neural networks training [23], and nonlinear circuits [24].However, the original CFO may easily get trapped in a local optimum when solving complex problems [25]. Suppose at some iteration of the search process, a metaheuristic algorithm fall in a local optimum. For the metaheuristic which is inherently stochastic, it may escape from the local optimum at the next iteration due to its randomness. However, as CFO is inherently deterministic, it may still be trapped in the local optimum at the next iteration. Therefore, CFO which seeks to find the global optimum requires some form of diversification to overcome local optimality. Without such diversification, CFO can be confined to a small area of the solution space and may not be able to explore any better region of the search space, thereby making it impossible to get a global optimum. A large number of methods have been proposed to achieve diversification. The multi-start strategy can provide a way to help avoid being trapped in poor local optima [26]. Consequently, the combination of the advantages of CFO in effectively achieving convergence and the multi-start strategy in avoiding local optima provides the rationale to develop a multi-start central force optimization (MCFO) algorithm to solve global optimization problems.The remainder of this paper is organized as follows. Section 2 introduces the original CFO and a short survey of previous research. Section 3 describes the proposed MCFO algorithm. The results and comparative discussions are presented in Section 4. Section 5 concludes the paper.CFO is a deterministic nature-inspired metaheuristic, which is based on an analogy to classical particle kinematics in a gravitational field. CFO is regarded as a population-based method that starts with an initial group of probes. A probe is defined by its mass, its position, and its acceleration. Probes are originally initialized in a uniform manner in the search space, and their acceleration are initialized to zero.These probes then “fly” throughout the search space by a simple group of update equations. The acceleration and position of each probe are updated according to the following rules:(1)ajp=G∑k=1,k≠pNpU(mjk−mjp)(mjk−mjp)αRjk−RjpRjk−Rjpβ(2)Rj+1p=Rjp+12ajpwhere Npis the number of the probes; p=1, 2, …, Npis the index of the probe, and j is the iteration number.ajprepresents the acceleration of the pth probe;Rjpis the position of the pth probe. G is gravitational constant. α and β are the exponents of CFO, which bring added flexibility in implementing CFO.mjp=f(Rjp)is the fitness value of probe p in the jth iteration. U(·) is the unit step function, and it is defined as follows:(3)ifz≥0thenU(z)=1(4)ifz<0thenU(z)=0Mass is a positive-definite user-defined function of the fitness of the objective function. A heavier mass represents a more efficient probe. In gravitational field of probe p, the mass of probe k in the jth iteration,Mjk, is given as follows:(5)Mjk=U(mjk−mjp)(mjk−mjp)α,k≠p∈{1,2,…,Np}Clearly, the value of mass is calculated using the difference in fitness values raised to the α power multiplied by the unit step. In this definition, the unit step is a key element because it generates positive mass and guarantees the gravity in the algorithm is attractive.Since its introduction in 2007 [19], CFO has attracted a high level of interest. Over the past few years, some variations of CFO have been developed to improve the ultimate performance of the CFO algorithm. Random sequences are very important for the existing metaheuristic belonging to stochastic algorithms. Inspired by this view, deterministic sequences which act in many ways like random sequences have been introduced to improve the performance of the original CFO. Formato [27] proposed a CFO algorithm with such sequences injected in the following ways: (1) the initial probe distribution; (2) the repositioning factor; and (3) changing the decision space boundaries. Qubati and Dib [21] developed the acceleration clipping scheme to put a limit for maximum acceleration of the probes. When the length of the acceleration vector is greater than the diagonal length of the search space multiplied by a predefined factor, the acceleration vector will be clipped by multiplying it by this same factor. After clamping, the probes keep the original direction of the acceleration. Ding et al. [28] proposed an extended CFO algorithm by defining an adaptive mass. A new landscape of mass is introduced into the basic CFO algorithm, which is based on an adaptive mean threshold. Mahmoud [29] proposed a combination of CFO and the Nelder–Mead algorithm. CFO is used as a global optimization solver, and the Nelder–Mead algorithm is adopted as a local optimization solver. After the final global iteration, a local optimization controlled by the Nelder–Mead algorithm are followed to refine the solutions generated by CFO.These techniques are very useful for improving the performance of the basic CFO algorithm. However, two common criticisms exist. Firstly, many exiting CFO variants avoid premature convergence at the cost of high computational burden. For example, the parameter-free CFO [27] achieves promising performance, suffering high computational expense measured by the number of function evaluations. Secondly, some modifications of CFO are restricted to low-dimensional problems. For example, the extended CFO [28] works well only in problems with two variables. Since low-dimensional problems are considered, the reductions in the computational costs of these variants have been accomplished. However, their performance severely degrades as the dimensionality of the problem increases.The MCFO algorithm has two stages: the first one where the solutions are produced by two different initialization methods and the second one where the initial solutions are improved by CFO. The proposed approach simply applies CFO algorithm twice, returning the best solution found over all starts.CFO is highly sensitive to the initial points consisting of potential solutions due to its deterministic nature. Therefore, it is important to find a good initial population. For the exiting metaheuristics, the population is usually initialized with random points [30]. However, this random method cannot be used to initialize the population of CFO which inherently deterministic. The initialization method of trial and error which is widely used in CFO can give better results but have a higher computational cost [27,31]. In the presented algorithm, two different approaches are used to initializing the population. The first one fills the position vector array with a uniform distribution of probes on each coordinate axis, and the second one places all probes uniformly spaced on the diagonals of a specific problem. A detailed description of these methods can be found in [19].The presented MCFO is illustrated in Algorithm 1, which consists of running CFO algorithm twice. In a new start of MCFO, a new initial population is generated using one method mentioned above. To avoid losing the best value obtained, in a new start, the value is stored. The MCFO is terminated when these two initializing methods are used.Algorithm 1Multi-start central force optimization1:f*←+∞;2:while stopping criterion not satisfieddo3:X← construct an initial population ();4:f← central force optimization (X);5:iff<f*then6:f=f*;7:end if8:end while9:returnf*;In Algorithm 2, the pseudo-code of CFO used is described. The initial solutions of CFO is given by MCFO which generates the initial population. At each iteration, CFO produces new solutions using the equations of acceleration and position. The CFO algorithm stops, when the allowed maximum number of iterations is reached.Regarding repositioning infeasible probes (i.e., lines 9–11 of Algorithm 2), a detailed description is given. In the CFO algorithm, a probe's acceleration may be too great to keep it inside of the search space. A determined boundary handling technique is used to reposit the errant probe and returns it to the feasible region. On a coordinate-by-coordinate basis, the probe that exceeds the boundary is placed a fraction Frepof the distance between the probe's starting coordinate and the corresponding boundary coordinate [27]. The method for repositioning infeasible probes can be described as follows:(6)ifRjp(i)<xi,min,thenRjp(i)=max{xi,min+Frep(Rj−1p(i)−xi,min),xi,min}(7)ifRjp(i)>xi,max,thenRjp(i)=min{xi,max−Frep(xi,max−Rj−1p(i)),xi,max}where Rpj(i) presents the position of pth agent in the ith dimension at step j. xi,min and xi,max are upper and lower bounds for dimension i, respectively.Frep, called repositioning factor, is set to an initial valueFrepinitand increased with each iteration. Frepafter the update is given by the following equation:(8)Frep=Frep+ΔFrepwhere ΔFrepis an arbitrary amount. If Frepis greater than one, then Frepis reset toFrepinit.Algorithm 2Central force optimization1:Compute the initial fitness of each probe;2:Select the best fitness f among probes;3:Assign the initial acceleration of each probe;4:Set the maximum number of iterations T;5:fori=1 to Tdo6:forj=1 to swarm sizedo7:Calculate the acceleration of probe j according to Eq. (1);8:Update the position of probe j using Eq. (2);9:ifprobe j exceeds the boundary then10:Retrieve errant probe j;11:end if12:Calculate the fitness fjof probe i;13:iffjis better than fthen14:Set fjto be f15:end if16:end for17:Shrink the search space every s iterations18:end for19:returnfAs for shrinking the search space (i.e., line 17 of Algorithm 2), a clear description is provided. In this way the algorithm can improve the convergence rate. The size of search space is adaptively reduced every s iterations around the position of the probe with the current best fitness. The boundary coordinates of search space are reduced by one-half the distance from the current best probe's location to the each boundary on a coordinate-by-coordinate basis [27]. The calculation is based on the following formulas:(9)xi,min′=xi,min+(Ri,best−xi,min)2(10)xi,max′=xi,max−(xi,max−Ri,best)2where Ri,bestis the ith component of the position of the current best probe.xi,min′andxi,max′are new upper and lower bounds for dimension i, respectively.Experimental tests with benchmark functions are carried out in this section to validate the proposed MCFO. We first give the test problems used in the experiment. Secondly, we introduce the parameter settings for the algorithm. And then, we compare the performance of MCFO with three state-of-the-art CFO algorithms. Finally, we demonstrate the effectiveness of MCFO by comparing the performance of MCFO with other state-of-the-art metaheuristic algorithms. Both the algorithms which are based on the law of universal gravitation and the methods which are not based on the law of universal gravitation are presented to evaluate the performance of MCFO.According to the No Free Lunch theorem [32], “any elevated performance over one class of problems is offset by performance over another”. In order to test the performance of MCFO, without a biased conclusion to some chosen functions. A set of 23 benchmark functions are used, which are commonly acceptable by other researchers. A summary of these functions is given in Table 1. A more detailed description of these test functions can be found in [33]. The benchmark functions can be grouped into three classes: (a) unimodal functions (f1–f7); (b) multimodal functions with many local minima (f8–f13); and (c) multimodal functions with a few local minima (f14–f23).The proposed algorithm is implemented using MATLAB 7.1 and run on a PC with an Intel Pentium 4 CPU and 2GB memory. The operating system is Microsoft Windows XP. The parameters in MCFO are set as follows:Frepinit=0.5, ΔFrep=0.1, α=1, and β=2. Npis set to ρ×Nd. The value of ρ is set to 2 when Ndis equal to 30, and 12 when Ndis less than 30. For each test function, the maximum number of iterations is set to 500 (i.e., the maximal number of function evaluations (FEs) is ρ×Nd×1000).

@&#CONCLUSIONS@&#
CFO is a relatively new metaheuristic algorithm, which is deterministic in nature. However, CFO tends to suffer from premature convergence when solving complex problems. To deal with this problem, some methods are proposed. Although these techniques demonstrate better performances in comparison to the original CFO, they expend a huge amount of computational costs. The MCFO algorithm is presented to improve the performance and reduce the computational burdens. The use of MCFO takes advantage of the main properties of the CFO (e.g. deterministic nature, and effective convergence) and those of the multi-start strategy (e.g. sufficient diversification, and capability to avoid being trapped in the local optimum). MCFO adds no randomness to algorithm design or implementation, and it is still a deterministic metaheuristic algorithm. The experiments conducted in this paper show that MCFO outperformed three state-of-the-art CFO algorithms, and three representative evolutionary algorithms.It is expected that MCFO may be an excellent alternative for optimization problems. One major future work is to extend MCFO to deal with constrained optimization problems and multiobjective optimization problems. Moreover, the application of MCFO to solve real-world optimization problems will be an interesting future direction.The source code of MCFO is available upon request from the corresponding author.