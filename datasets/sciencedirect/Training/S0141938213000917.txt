@&#MAIN-TITLE@&#
Color constancy for visual compensation of projector displayed image

@&#HIGHLIGHTS@&#
We model the sensed image by human eye.We explore the color constancy to reduce the screen effects on the projected image.We argue that the proposed model goes further than the von Kries theory.

@&#KEYPHRASES@&#
Computational color constancy,Image display,Projector,Viewing conditions,Projection screen,Von Kries,

@&#ABSTRACT@&#
The color of a displayed image by a projector can be distorted by features of the device, the ambient light, the projection screen, and also the observer. This has raised the need to correct the image during the display to eliminate these effects and to ensure a constancy of the color appearances. In this paper, we propose models for controlling the appearance of the displayed image. We argue that depending on the target application, the computational color constancy can be specified at different steps of the formation scheme of the sensed image by a human. Based on that observation and the image formation models, we reformulate the problem of the color constancy and we show that the resulting transformations cannot be explained by von Kries theory. Two compensation algorithms are deduced. The first allows preserving the appearance of the original image, and it can be used for the constancy of the acquired image whatever the environment conditions. The second algorithm allows to simulate appearances of a sensed image in a specific conditions. It can be used for the compensation of the screen reflectance or to create special effects or the camouflage. In addition, we propose a complementary operation for the contrast compensation which is derived from the Weber’s law. Experimental results show the merits of the proposed models and algorithms.

@&#INTRODUCTION@&#
Projectors become more proliferate and integrated in multiple areas. However, the quality of the displayed image can be adversely affected by the device used and the real viewing conditions. The displayed image under bright conditions may appear with washed colors and attenuated contrast. The appearance of an image captured under illuminant D65, will be also affected when it is perceived under another illuminant. The mismatch between the display environmental illuminant and the illuminant of the image acquisition may influence the perception of the image. In addition, the displayed image onto an ordinary surface will be affected, where its appearance will be modulated by the color and the geometry of the surface. The projector features could also affect the quality of the displayed image. The transfer function, and the embedded color signal processing influence the color generation. Moreover, the sensed image quality depends on the spectral abilities of the human visual system (HVS). As it is characterized by its nonlinear response to the intensity of the ambient light [1], this means that the perceived contrast of the image will be necessarily affected.The use of a projector anywhere is a challenging task that requires controlling the appearance of the sensed image. Indeed, we need for some applications, an image appears for a normal viewer the same regardless of the display conditions. Similarity is desired between the image as it was created and its sensed version. For other applications, we prefer to control the display conditions in order to cancel out or to create some distortion effects such as in cases of visual effects or digital art.Controlling effects of the viewing conditions can be done through two approaches: material and computational. The materiel approach consists in monitoring the projector settings according to the environmental features, such as the ambient light which can be measured by a photometer [2,3]. Some specific screens are designed in the manner that the ambient light will be reflected out of the observer direction [4], the area where viewers can be placed needs to be specified during the installation phase. In the materiel based approach, the concern is the measure of the light present in the environment rather than its perceptual impact [2], where the compensation is considered as a global phenomena. However, the local compensation of the ambient light intensity, the light color, and the projection screen are not available. This is the basis of the computational approach [5–11]. Our work falls within the last approach, in which we propose the transformation of the image according to the viewing conditions before sending it to the projector. The viewing conditions can be recovered through acquiring images of the projection screen. Indeed, the modified image displayed by the projector can create the desired appearance by interacting with the environmental parameters.This paper is organized as it follows. In the next section, we present the problem statement. After, in Section 3 we detail our proposed models of the compensation of viewing conditions. Then, Section 4 provides details on the method used in parameters estimation. In Section 5, the experimental results are presented. Finally, we conclude this paper with a summary of the work.As it is indicated above, several factors influence the appearance of the displayed image, among them, there is the projection surface and the surrounding lights. Consequently, the projected image will undergo two transformations noted S and L, respectively. Furthermore, the effect generated by a projector can be represented by a transformation D. Moreover, we note the effects due to HVS by a transformation O. Indeed, for an original image I that will be displayed by a projector, its corresponding image sensed by a viewer notedIO, can be written as:(1)IO=O(L(S(D(I)))).We note that the limitations of the HVS in terms of contrast perception and ambient conditions should be considered. Therefore, a correlation can be deduced between the intensity of the surrounding light, the response of the human eye and the perceived contrast. This can be expressed by a model which will be described later. Indeed, controlling the desired appearance of an image requires to be aware of all effects of viewing conditions that adversely influence the appearance.In this paper, we address the control of the viewing conditions for projector-based display system through the color constancy concept. The goal is to make the displayed images invariant to some factors depending on the use purpose. In the literature, the von Kries theory based computational color constancy has been widely studied for a chromatic adaptation and for white balancing [12]. But, to the best of our knowledge, it was never explicitly broached for projector-based display systems. For images displayed by a projector and viewed by a normal viewer, we show that the color constancy concept can be extended and used in different ways. As it is shown in Fig. 1, we categorize applications into two possible scenarios; (1) preservation of the image appearance and (2) simulation of the image appearance as it will be detailed later. By the first scenario, we propose to preserve the appearance of the displayed image sensed by a viewer to appear similar to the original image as it was created. Consequently, the sensed image should be invariant to the response of the projector, the ambient light, the projection screen and also the response of the viewer. For example, we can perceive a sunset image as it was captured by a camera independently of how or where it is displayed. By the second scenario, we were interested to make two images viewed under two different conditions appear identical. This scenario was particularly designed in the perspective of simulation, whereby, we can simulate other effects that can be produced under specific environments of visualization. For example, this can be used for visual arts and camouflage, where we can create some artistic effects. Therefore, the illuminant of the display environment and the target illuminant are introduced by the proposed model for this scenario, similar to the current and the target screen reflectance. In addition, aiming to produce the desired visual contrast of the displayed image under bright conditions, the Weber law is used for the image contrast enhancement according to the intensity of the surrounding light.In the field of the environmental conditions compensation, there exist in literature two families of methods: the heuristic methods and the image formation based methods. These methods were designed principally in order to reduce some effects of the environmental conditions, where the above transformations have been studied. For the heuristic methods, works [10,9,13] proposed frameworks considering the preceding transformations separately. In [11], the method was based on power law function to map the input range to the output range, considering the contrast enhancement. We note that the compensation algorithms based on heuristic arguments are different from ours. For image formation based methods, all are designed to deal with the color constancy between the original image and the image captured by a camera (first scenario). The transformation S is considered in [6,5], and L in [5]. They [6,5] proposed a set of parameters higher to a matrix 3×3 of color mixing for every pixel of the image. Authors in [7] proposed a method based on one color mixing matrix for the entire image. However, in a dark environment, this matrix is eliminated in [14] by using a color filter for both the projector and the camera. Note that the second scenario described above is not tackled before. Unlike these approaches, the one we propose deals with both scenarios in addition to the use of one global estimated matrix 3×3 for all images. More precisely, Table 1summarized the similarities and differences between different approaches including ours. We can resume contributions of the paper as the followings. (1) We reformulate and exploit the computational color constancy concept for projector systems which can be seen as a fundamental founding; (2) we propose models for both scenarios that are appearance preserving and appearance simulation; and (3) we take into account four transformations.In this section, we provide details of each model corresponding to two scenarios described above, by modeling the spectral response of the projector, the screen reflectance, the surrounding lights and the spectral response of the viewer. Transformations could be linear [5,6,8,10] or nonlinear [11]. In this work, we consider all transformations as linear.To this end, we need to further develop the model of the sensed image formation in Eq. (1). Then, we will explain how we can employ the color constancy between the sensed image and the original image (scenario 1), as well as two sensed images under two different environments (scenario 2). After that, as the HVS is affected by the surrounding light, it becomes harder to see details of the displayed image in bright conditions. Consequently, the visual contrast of the sensed image will be attenuated. To overcome this limitation, we propose a method to enhance the contrast of the displayed image according to the surrounding light intensity. Note that the projector has three-color channelsR,Gand B.The projector input pixelIkof thekth band of an image is modulated by its spectral responsetk(λ), which includes all spectral responses of the projector optical components such as the lens and the physical processing operations. Thekth light bandEk(λ)generated by the projector through its lens is given by Eq. (2)[5,6], whereλrepresents the wavelength of the visible spectrum. A linear transformation D is considered, since the DLP projectors used are assumed digital linear systems [7,15].(2)Ek(λ)=Ik(λ)tk(λ).Note that the colored point of the projection screen has a spectral reflectance represented byρkc(λ), where k in{R,G,B}, it is considered as the color channel of the screen. Letla(λ)be the spectral distribution of the actual surrounding illumination. The human eye receives a mixture of lights that are reflected by the projection screen. These lights come from the projector and the surrounding illumination. Yet the radianceRkof the scene point in the direction of the viewer’s eye is written as it follows:(3)Rk(λ)=Ik(λ)tk(λ)+lak(λ)ρkc(λ).Letsk(λ)be the spectral perceptual abilities of the viewer when the eye has adapted to the surrounding lightla. The effects of eyes optics can be considered as encoded inIk. Under the above conditions, the sensed imageAkby the viewer can be written as(4)Ak=∫Ik(λ)tk(λ)+lak(λ)ρkc(λ)sk(λ)dλ.Note that this is a defined integral in the range of visible spectrum. Now, as we indicated before, we can make two different color constants depending to the application purpose. In what follows, we will demonstrate the use of each one.The aim is to make two images sensed under two different environments appear identical. Letlbbe the spectral distribution of another illumination andρkw(λ)be the spectral reflectance of a white screen point. For the original pixel I, if the viewer is adapted to the visualization environment, the displayed image onto a white screen and under the illuminationlb, will be sensed. It can be written as:(5)Bk=∫Ik(λ)tk(λ)+lbk(λ)ρkw(λ)sk(λ)dλ.Our main interests is to make it possible that under the first conditions (i.e. the colorful screen and the illuminationla), the projected image appears the same as it appears under the second conditions (i.e. the white screen and the illuminationlb). To make this point clear, we refer to the example in Fig. 1(b). As we are interested by the HVS, the constancy concerns the images sensed by the retina of the human eye. We want the sensed image (A) to be similar by a viewer under the first conditions to the one sensed (B) under the second conditions. However, since the two environments are different, the equality between the two appearances could be done by modifying the original image under the first conditions, before sending it to the projector, where a transformation T will be applied to the original image that takes into account transformations due to the first conditions. This is can be expressed as(6)B(I,lb,screen2)=B(T(I),la,screen1).Formally and using Eqs. (4) and (5), the equality in Eq. (6) can be written as(7)∫Ik(λ)tk(λ)+lbk(λ)ρkw(λ)sk(λ)dλ=∫T(Ik(λ))tk(λ)+lak(λ)ρkc(λ)sk(λ)dλ.The goal is to find the transformation T by solving this integral equation. Let us consider that the function in Eq. (3) is continuous in the visible spectrum. This assumption is realistic because the quantities involved are of physical nature produced by electronic components or they go trough the lenses of the camera and the projector. In this case, under the assumption thats(λ)is the Dirac, the Lebesgue integral with respect to this measure allows to rewrite the previous equation as:(8)Ik(λ)tk(λ)+lbk(λ)ρkw(λ)=T(Ik(λ))tk(λ)+lak(λ)ρkc(λ).Which yields(9)T(Ik)=ρkwρkcIk-1tklak-ρkwρkclbk.Further we will explain how we overcome the Dirac assumption. For three-color channels, Eq. (9) leads to be written in matrix form as:(10)M=PI-Λ∇L,whereM=MRMGMB,I=IRIGIB,la=laRlaGlaB,lb=lbRlbGlbB,Λ=Λ11Λ12Λ13Λ21Λ22Λ23Λ31Λ32Λ33,P=ρRwρRc000ρGwρGc000ρBwρBc,and∇L=la-Plb.The compensated, and the original images are noted by M and I. The image M is sent to the projector, which produces the desired appearance. The matrixΛcaptures the spectral responses of the projector for three-color channels. Like [16], to overtake the assumption considerings(λ)as Dirac, we consider a full matrixΛ. Otherwise, the channelsR,Gand B are often correlated, hence, the use of a full matrixΛallows taking into consideration the correlation between them. The relationship between the current and the desired projection screen is given by the matrix P. In addition, the term∇Lrepresents the difference between the current and the target illuminant according to screens’ reflectance.In the previous scenario, the compensation is done for images sensed by the human’s retina. However, in some applications, we need to preserve the appearance of an original image when it is sensed. For example, in a picture of sunset, the colors may not appear the same when it is displayed in some conditions. The image need to be invariant to the viewing condition and the display device used. This could be done by verifying the color constancy between the sensed image and the original image. The goal is to make it possible that under specific conditions (i.e. the colorful screen and the illuminationla), the displayed image appears identical to its original image, as in Fig. 1(a). Therefore, we want the sensed image (A) to be similar by a viewer to the original image (I). Consequently, the equality between the two appearances could be done by modifying the original image under these conditions, before sending it to the projector. This could be done since a transformationT1will be applied to the original image that takes into account transformations due to the display environment.In the same manner as before, the transformation can be written as:(11)T1(Ik)=1tk1ρckIk-1tklk.For three-color channels, the compensation is given by(12)M1=ΛP1I-Λ1la.The compensated, and the original images are noted byM1and I. The matrixΛcaptures the spectral responses of the projector and the viewer for three-color channels. Similarly, a full matrixΛis considered to overcome the assumption takings(λ)as Dirac. The matrixP1allows to compensate the effects of the projection screen. We note thatP1will be equal to P if the desired screen is white. In addition, the surrounding light is given by the vectorla.We notice that Eq. (12) compensates all effects due to the surrounding lights, the projection screen, the spectral response of the projector and the spectral response of the viewer. These two spectral responses are compensated by the same matrixΛ.Now, considering the models in Eqs. (10) and (12), one important issue concerns the identification of images that can be displayed and screens that can be used. We will derive the constraint from Eq. (10), the case of Eq. (12) is similar. Since the color image M, is encoded by 24 bits (8 bits per channel), then values of each color channel vary between zero and 255. The following two constraints must be fulfilled. If the compensated image contains values above the maximal brightness or below the black level of the projector, artifacts will occur. To overcome this limitation, the compensation image is controlled by the first constraint:(13)Λ-1PI⩾∇L⩾Λ-1(PI-255).The second constraint relies on the reflectance values of the target screen and the current screen, it gives the relationship between them, where (Ik>0):(14)0⩽ρkw⩽255ρkcIk.Another important issue is the link between the derived transform and the von Kries transform. Let us recall that the von Kries based computational color constancy is a linear transformation, which depends on the scene illuminant (target) and the illuminant of the environment. Unlike von Kries, the linear transformation in Eq. (10) depends on both lights, projector, spectral sensitivity of human’s retina, and reflectances. Note that, von Kries model can be deduced from Eq. (8), if the ambient light is multiplicative (i.e.,lakandlbkare multiplied byIktkterm). However, this is not true because it means that under a dark environment (i.e.,lak=0orlbk=0) the displayed image is dark. It follows that von Kries transform is not suitable for the projector based display.Let us now focus on the additive term in Eq. (10). This term is free from the image content. It describes the light variation according to the relationship between target and current screen reflectances, and also, the transmittance ratio of the projector. According to the image formation model presented in Eq. (4), it can be seen as the change of the environment light when some specific image is displayed. From the human perception side, a change of the surrounding lights causes changes in the smallest perceptible contrast (Stevens law), saturation [3], and hue (Bezold–Brücke law) of the perceived image. Therefore, both chromaticity and contrast of the perceived image change. The contrast enhancement when lights change has been studied [17–19]. In this work, based on the performed experiments presented in Section 5 we find that: (1) chromaticity is compensated by Eq. (10) or Eq. (12) depending on the target application; (2) the contrast adaptation is achieved by Eq. (18), which will be detailed later on; and (3) the saturation component of the original image should be maintained for the compensated image before performing the contrast adaptation.Concerning the equation that guides the sensitivity of the HVS to the luminance changes and its impacts, we deal with the Weber law to derive this equation that increases the contrast. The local contrast of a single target depends on the surrounding luminance. It is given by the Weber fraction as [20]:(15)C=ΔLL,whereΔLis increasing or decreasing in the target luminance from the surrounding luminance L. If the intensity of the ambient light increases, it could be possible that the contrast variation of the sensed image will not be perceptible. Adding the surrounding light to the displayed image, reduces the visual contrast of the image. Therefore, the lower the environmental lights are, the higher the contrast of the image will be. Thus, to achieve a visible contrast enhancement, higher luminance patterns need a higher contrast increase [17]. Different measures of contrast exist [17]. We propose the use of the Laplacian transformation, which has been used with some success for the X-ray image enhancement in [21]. We define the local contrast C of an image to be proportional to the local LaplacianLTof the luminance component y of the image M. In other words,(16)C∝LT(My).Eq. (16) indicates that to achieve the same perceived increase in contrast across an image, the values of the Laplacian transformation need to be stretched. In fact, according to Weber’s law, the stretching should be performed in such a manner that the contrast increasing is proportional to the surrounding luminance. Consequently, this can be done by stretching the Laplacian values of the resulted compensated images. It is given by the following equation [21]:(17)Y′=My+aLT(My),whereY′represents the enhanced contrast result. Notice a is a constant of the model, its value is determined experimentally (a=0.2). We observed that the contrast enhancement given by Eq. (17) is proportional to the Laplacian transformation of the image, which depends on the surrounding lightsΔL.We noted that the compensation of the image is done in the RGB color space, except the contrast correction, which is performed by the luminance component of the YCbCr color space. The compensation has been tested with other color spaces, and it turns out that color spaces used are the ones giving a better colors.The model parameters of each scenario are estimated in the same manner. n this section, we will present only the parameters estimated for the scenario 2 in Eq. (10).Recovering parameters in Eq. (10) consists in finding the matricesΛand P. We noted that the matrixΛis valid for the entire image. However, it could also be estimated for different areas of the image such as: edges, lines, and textures. We will show how a global matrix can be estimated. It is estimated only once, and it is constant for every environment conditions. The matrix P is estimated for every pixel to compensate spatial distortion of color. We note that the method used for the parameters estimation is based on the environment simulation, and it relies on the human perception. The viewer uses its perceptual capabilities to decide when some displayed images under different conditions are similar. These displayed images are then acquired by an embedded camera and used to estimate the parameters as it will be discussed later. Consequently, a step of geometric calibration of the projector and the camera is necessary, for mapping projector pixels with their corresponding camera pixels. We propose the use of the presented method in [22], where the calibration is performed by exploiting the homography between the projected slide and the camera image. For accuracy, we aligned the projector and the camera to the projection screen (i.e. the optical axis of the projector and, the camera are perpendicular to the projection screen).Primarily, we estimated the matrixΛ. It can be estimated by a learning process. For this purpose, we were interested to compensate only the surrounding lights effects, when, we use a white planar screen (i.e.,ρkc=ρkw=1and P is the identity). The case when two reflectances are not equal is presented later. Therefore, Eq. (10) becomes:(18)M=I-Λ(la-lb).According to Eq. (18), for estimating the matrixΛ, three images are required. Two images corresponding to the two surrounding lights, are obtained by capturing the projection screen under the lights notedlaandlb. The third image corresponds to the compensated image M, which is displayed under the lightla, and it appears to be similar to the original image I displayed under the lightlb. This image is constructed by using a coloring graphical user interface, a similar protocol is used in [23]. An original image is displayed by the projector under the lightlband its appearance which represents a target appearance, is captured by the camera. Then, under the lightla, the same original image is displayed by the projector, their colors are modified until getting the target appearance. The obtained appearance is captured by the camera. For accuracy, the two appearances obtained underlaandlbare captured by the camera and similarity between the two images is verified using image color similarity metrics (e.g., S_CIELAB). Indeed, the modified image that produces the desired appearance represents the image M, which will be used in the estimation of the matrixΛ. Note that S_CIELAB is a standard metric widely used for color measuring [24].Therefore, once three images are acquired, the matrixΛis obtained from Eq. (18) as:(19)I-M=Λ(la-lb),F=ΛV,whereF=I1R-M1R⋯InR-MnRI1G-M1G⋯InG-MnGI1B-M1B⋯InB-MnB,V=la1R-lb1R⋯lanR-lbnRla1G-lb1G⋯lanG-lbnGla1B-lb1B⋯lanB-lbnB.Then, the matrixΛis estimated using the Moore–Penrose pseudo-inverse technique [25] as:(20)Λ̃=(FVT)(VVT)-1.Now, we will deal with the case of two screens which have a different reflectance. To simplify the estimation of the matrix P, we chose a fully dark environment to avoid the compensation of the surrounding lights. However, the matrix P can also be estimated with a bright environment. At no light, Eq. (12) will be written as follows:(21)M=PI,M and I represent the images acquired by the camera with the white and the colored projection screen. We noticed that Eq. (21) can be written as:(22)MRMGMB-P1000P2000P3IRIGIB=000.Consequently, for every point of the projection screen, its matrix P is given as:(23)P=MRIR000MGIG000MBIB.Due to the ratio between the target and the used screen reflectance expressed by the matrix P in Eq. (10), the compensation of an image depends on the analysis of the screen used and the image content that will be compensated.

@&#CONCLUSIONS@&#
