@&#MAIN-TITLE@&#
Studying emotion induced by music through a crowdsourcing game

@&#HIGHLIGHTS@&#
We collected ground-truth data on induced musical emotion for 400 musical excerpts.We designed an online game with a purpose to attract a big number of participants.We analyzed inter-rater agreement on emotional terms from GEMS model.We found that mood, gender and liking or disliking the music influence induced emotion.We suggested improvements to GEMS scale.

@&#KEYPHRASES@&#
Music information retrieval,Game with a purpose,Music induced emotion,Crowdsourcing,

@&#ABSTRACT@&#
One of the major reasons why people find music so enjoyable is its emotional impact. Creating emotion-based playlists is a natural way of organizing music. The usability of online music streaming services could be greatly improved by developing emotion-based access methods, and automatic music emotion recognition (MER) is the most quick and feasible way of achieving it. When resorting to music for emotional regulation purposes, users are interested in the MER method to predict their induced, or felt emotion. The progress of MER in this area is impeded by the absence of publicly accessible ground-truth data on musically induced emotion. Also, there is no consensus on the question which emotional model best fits the demands of the users and can provide an unambiguous linguistic framework to describe musical emotions. In this paper we address these problems by creating a sizeable publicly available dataset of 400 musical excerpts from four genres annotated with induced emotion. We collected the data using an online “game with a purpose” Emotify, which attracted a big and varied sample of participants. We employed a nine item domain-specific emotional model GEMS (Geneva Emotional Music Scale). In this paper we analyze the collected data and report agreement of participants on different categories of GEMS. We also analyze influence of extra-musical factors on induced emotion (gender, mood, music preferences). We suggest that modifications in GEMS model are necessary.

@&#INTRODUCTION@&#
With the current sizes of musical databases there is a growing need for automatic methods of music classification and similarity assessment, and emotion-based methods are potentially among the most useful access mechanisms for music collections. Implementing such methods is not a straightforward task, not only due to MER (music emotion recognition) limitations, but also because the emotional content of a musical piece is an intrinsically ambiguous part of it. Within music-related emotions, an important distinction can be made between emotions that are expressed by music (while listener is not necessarily feeling them), and the emotions felt by listener as a response to music (which we refer to as induced emotions). There is no doubt that music can indeed arouse strong emotions in listeners (Krumhansl, 1997; Rickard, 2004). Many people use music for purposes of emotional self-regulation and music therapy (Gabrielsson, 2011), and it is important to develop methods that could automatically categorize and select music by these criteria. In this paper, we contribute to solving this problem.The relationship between expressed and induced emotion is not direct. In Gabrielsson (2002), Gabrielsson argues that expressed and induced emotion can relate in four ways: positive, negative, no systematic relation or no relation, thus, positive relation should not always be assumed. Also, though a qualified listener can nearly always recognize emotion expressed in the music, emotion induction is less frequent. Recent studies suggest that listeners experience strong emotions only about 55% of the time they spend listening to music (Juslin & Laukka, 2004), or that in 65% of the musical episodes music affects how they feel (Juslin, Liljestrom, Vastfjall, Barradas, & Silva, 2008). Emotional responses can be measured from self-report, expressive behavior and physiological responses (heart rate, skin conductivity, blood pressure, as well as biochemical responses) (Krumhansl, 1997; Rickard, 2004). In case of music, pronounced expressive behavior is not the rule, and, arguably, self-report is the most widely used and the most informative measure, because it provides information on the otherwise inaccessible cognitive part of emotion (Zentner & Eerola, 2011). In this study, we will use self-report to measure induced emotional responses to music.Musical emotions are not directly translatable into words. There is still no consensus between researchers on the most suitable model, despite numerous attempts to find one (Vuoskoski & Eerola, 2013). The choice of model is essential to the performance of MER algorithms. A model that fails to describe the phenomenon precisely will result in poor agreement between listeners, conflicting musical cues associated to different emotions, and impede accuracy of prediction. On the other hand, a model that oversimplifies the problem might result in better agreement, but would be less useful for listeners. Currently, a wide variety of emotion ontologies can be found not only in research, but in music industry as well, from the valence–arousal model used by Musicovery,1www.musicovery.com.1or ten categories ranging from happy and fun to dramatic and stressful by Aupeo,2www.aupeo.com.2to no ontology at all, but providing emotional playlists non-systematically created by users3www.stereomood.com.3and user-generated tags4www.last.fm.4instead. In 2008, a new domain-specific model was suggested, Geneva Emotional Music Scales (GEMS) (Zentner, Grandjean, & Scherer, 2008). It was developed specifically to describe emotion induced by music, and, as compared to other categorical models, GEMS describes refined positive responses to music in much more detail. Since 2008, GEMS has been used in some smaller scale studies with promising results (Baltes, Avram, Miclea, & Miu, 2011; Jaimovich, 2013; Torres-Eliard, Labbe, & Grandjean, 2011; Vuoskoski & Eerola, 2010). There have been no large scale studies conducted using this model, and no public data have been released.For our work we decided to employ the GEMS model. Our motivation here was twofold. First: our need for a big data set about induced emotion. Secondly, we also felt that additional studies of GEMS were needed. The reasons are that in the original study (Zentner et al., 2008) that GEMS is based upon, mostly classical music was used, and, moreover, the study was conducted in French, and the terms were translated to English.Obtaining ground truth remains a challenging task for MER research, where both music copyright and costs of annotation (with music annotation being a particularly time-consuming task) pose problems. Outside the laboratory, there are two possible ways of assembling a dataset labeled with emotion annotations: through social tag mining (relying on websites such as last.fm or allmusic.com) and in a more systematic way through user surveys or data collection games. Social tag mining makes it possible to collect a huge dataset, but lacks the homogeneity and control that a preselected emotional model and a controlled experimental setting provides. In most cases it is unfeasible in tag mining to measure the level of agreement between multiple users on certain tags (or it would be necessary to apply an additional cross-verification procedure as it was done in case of the MIREX audio mood recognition task (Hu, Downie, Laurier, Bay, & Ehmann, 2007)). A controlled user experiment would be an ideal way of data collection. In this case, in addition to self-report, researchers can collect physiological measurements and exclude external factors that might influence the outcome. However, firstly, such a setup lacks ecological validity, and secondly, tasks involving music are very time-consuming. In the end, researchers seem to be left with a difficult choice between a small-scale or a very expensive survey.In this paper we approach the problems described above by collecting our ground-truth data using a game with a purpose (GWAP). We advertised our game, Emotify, through social networks, and it attracted a big and varied set of participants.In this paper, we describe an experiment designed to study emotions induced by music and to collect ground-truth data which could be used in training machine listening algorithms. We created a game with a purpose and collected annotations for 400 musical excerpts using a domain-specific emotional model GEMS (Zentner et al., 2008). The annotations are publicly available.5http://www.projects.science.uu.nl/memotion/emotifydata/.5We examine the model’s usability in online context and analyze comments and suggestions of game players. We report degree of agreement between listeners on different emotional categories and genres. We also study the extra-musical factors that influence induced musical emotion.The paper is organized as follows. In Section 2, related research, concerning music-related emotional models, datasets using GEMS, and musical GWAPs, is reviewed. Section 3 presents methods and procedure (the GWAP) of the experiment. In Section 4, we describe the dataset that we collected and released as an outcome of this study. In Section 5, we analyze the consistency of responses made using GEMS model, and the feedback and suggestions from game players. In Section 6, we analyze the extra-musical factors that influence emotion. In Section 7, we discuss the main findings. Section 8 concludes the paper and suggests future work.

@&#CONCLUSIONS@&#
One of the open research questions that we addressed with this study was whether music can express and induce a complex fine-grained range of emotions, or it is only possible to find crude counterparts of verbally expressible emotions in music. On basis of our study we conclude that there indeed is enough variety and expressive power in music to convey and induce such emotions as tenderness, nostalgia or peacefulness in such a way, that they can be distinguished by participants with sufficient inter-rater agreement. We also concluded that the GEMS model can be successfully used by participants from various linguistic backgrounds, though there obviously exists a lack of understanding concerning categories wonder and transcendence. It is a direction for future research to find how these categories could be modified.Apart from this modification, it might also be worthwhile to study whether the GEMS could be augmented. Our study suggests that some of the nuances of emotional experience might be absent from GEMS model (8% of our participants were not able to use GEMS to describe their induced emotions). We agree with Coutinho and Scherer (2012), that feelings of boredom and interest must be added to the model, but also suggest that more semantic categories are lacking from it. Such semantic groups as impetus (call to action), humor and contentment were repeatedly named by the players of our game.Another motivation for our study was collecting a dataset of music annotated with induced musical emotion which could be used as a ground-truth for MER research. The size of the dataset makes it possible to apply computational methods to explore the mechanisms underlying music emotional expressiveness, and to use these methods for automatic music classification and retrieval. A first study using our dataset has already been conducted (Aljanaki, Wiering, & Veltkamp, 2014b).We hope that this work will contribute to solving the problem of finding the most appropriate model of musical emotion. Though this problem is important both for research on music psychology and music industry, currently it is far from being solved. We also hope that ground-truth data for such a rich emotional model like GEMS will be useful for MER research.