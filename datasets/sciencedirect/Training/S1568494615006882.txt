@&#MAIN-TITLE@&#
A novel multiple rule sets data classification algorithm based on ant colony algorithm

@&#HIGHLIGHTS@&#
A novel classification model using multiple base classifiers is designed.A new heuristic function is proposed by considering the correlation and coverage.A weighted voting mechanism is presented to classify the test data set.

@&#KEYPHRASES@&#
Ant colony optimization,Data mining,Classification,Base classifier,

@&#ABSTRACT@&#
Ant colony optimization (ACO) algorithms have been successfully applied in data classification, which aim at discovering a list of classification rules. However, due to the essentially random search in ACO algorithms, the lists of classification rules constructed by ACO-based classification algorithms are not fixed and may be distinctly different even using the same training set. Those differences are generally ignored and some beneficial information cannot be dug from the different data sets, which may lower the predictive accuracy. To overcome this shortcoming, this paper proposes a novel classification rule discovery algorithm based on ACO, named AntMinermbc, in which a new model of multiple rule sets is presented to produce multiple lists of rules. Multiple base classifiers are built in AntMinermbc, and each base classifier is expected to remedy the weakness of other base classifiers, which can improve the predictive accuracy by exploiting the useful information from various base classifiers. A new heuristic function for ACO is also designed in our algorithm, which considers both of the correlation and coverage for the purpose to avoid deceptive high accuracy. The performance of our algorithm is studied experimentally on 19 publicly available data sets and further compared to several state-of-the-art classification approaches. The experimental results show that the predictive accuracy obtained by our algorithm is statistically higher than that of the compared targets.

@&#INTRODUCTION@&#
Many decision problems can be mathematically modeled as data classification problems in various application fields, such as medical science, economic management, social science and engineering [1–3]. The classification model is expected to reflect the relationships between predictor and class attributes’ values. After the construction of classification model, the classified information can help decision makers to make effective decisions. Two significant criteria for performance evaluation in data classification are considered; one is the predictive accuracy that shows the percentage of correct predictions in all the predictions of test data set [4,5], while the other one is the comprehensibility of classification model [6,7]. Although support vector machine (SVM) and neural networks can achieve very good accuracies, their models are difficult to be interpreted [8]. However, rule induction algorithms not only can obtain the promising predictive accuracy, but also build comprehensible classification models using the form of IF–THEN classification rules, as the list of rules can even be expressed in natural language [9].Recently, many nature-inspired heuristic algorithms have been designed for problem solving, and successfully applied in many research fields, e.g., optimization problems and classification problems [10–13]. Especially for data classification, ant colony optimization (ACO) algorithms have shown the promising performance, which are aimed at finding a rule of the type: “IF <term1> AND <term2> AND … <termn> THEN <class>” by an ant. The list of rules built by ACO algorithms can generally be expressed in natural language and thus have good comprehensibility of classification model [14]. However, due to the non-determinacy nature in ACO-based search, the same class label rules constructed by ACO-based algorithms may be distinctly different from one another. This is to say, multiple different rules may provide more useful information to enhance the prediction accuracy of the class label as one list of rules can be utilized to offset the shortcoming of other lists of rules, in which the correct data examples may not be covered.Therefore, in this paper, a novel ACO-based classification rule discovery algorithm named AntMinermbc is presented, which proposes a new model of multiple rule sets to mitigate the above potential disadvantage. A number of base classifiers are built in AntMinermbc to classify one giving data example and then a weighted voting mechanism is adopted as a criterion to finally decide the predicted class label. The proposed model of multiple rule sets is significantly different from the existing ACO-based classification algorithms that construct only one list of classification rules to classify a data sample. By this way, our algorithm can dig more useful information from the multiple rule sets so as to improve the predictive accuracy. Besides that, a new heuristic function is designed to guide the ants to find the relationship between attribute values and class values, which considers the coverage of the attributes to ensure the generation of high quality rules. In order to evaluate the performance of our algorithm, 19 publicly available data sets are employed. When compared with various classification algorithms, such as two well-known machine learning algorithms (C4.5 [15,16] and JRip [17]) and two recently proposed ACO-based algorithms (Ant-Tree-Miner [18] and cAnt-Minerpb[9]), our algorithm performs better in the predictive accuracy and the comprehensibility of classification model. Moreover, the effectiveness of the multiple rule sets model and the novel heuristic function are also analyzed experimentally.The remainder of this paper is organized as follows. Section 2 presents the background of ACO and gives an overview of existing ACO algorithms for classification rules discovery. Our algorithm is described in Section 3, which presents the new model of multiple rule sets and the novel heuristic function. Section 4 gives the experimental results of our algorithm in some publicly available data sets and the comparisons with other classification algorithms. At last, Section 5 summarizes the conclusions and future work.Ant colony optimization is a nature-inspired intelligent algorithm initially proposed by Dorigo et al. [19], which mimics the social behaviors of biological ants to look for food. Each ant randomly starts to search without any location information of food and communicates with each other by releasing a chemical substance called pheromone in the search path. The concentration of pheromone on a given path reflects that the path is pros or cons. If the search path is not visited by more ants, its pheromone will be evaporated gradually over time. Otherwise, it is enhanced when more ants pass. As ants that arrive in the vicinity prefer to select the path with higher pheromone concentration, ants can establish a short path between their nest and food source. By this way, the later ants are gradually approaching the location of food as guided by the pheromone.This food foraging behavior of ants has been modeled in ACO algorithm. An optimization problem whose solution can be represented by a combination of components is treated as food source and an artificial ant is used to find a solution with shorter path to food source (better solution for the optimization problem) [20]. By using graphs terminology, it is assumed that the vertices and edges are the possible components of the solution, and each edge is associated with a pheromone value. A potential solution is constructed by an ant through components selection, which is performed probabilistically based on the pheromone concentration in components. After that, the solution is assessed by an evaluation function and the components are assigned with the renewed pheromone concentrations that are proportional to its evaluated quality. Generally, the components of high-quality solution are allocated with more pheromone concentrations. Therefore, the later ants have higher probabilities to select the better components, which contribute to finding a better solution. After sufficient iterations are executed, ants will eventually gather toward a good solution.Since the early 1990s, there have been many ACO algorithms reported, most of which are designed by using different probability calculation and pheromone update methods. Ant colony system (ACS) [21] and MAX–MIN ant system (MMAS) [22] are two well-known state-of-the-art algorithms, which primarily differ in the construction of new solutions and the update of pheromone. In MMAS, each ant constructs a solution through probabilistically selecting its components in the graph. When an ant has selected a component i, the probability of choosing the next component j is given by(1)Pij(t)=τijαηijβ∑j=1componentsxk{τijαηijβ}where xkis set to 1 when the component j is not included in the partial solution constructed by the ant. Otherwise, xkis set to 0. τijis the concentration of pheromone associated with the edge between termiand termj, and ηijis the value of heuristic information in the current iteration. α and β are the two weight parameters that indicate the relative importance of the pheromone and heuristic values.In ACS, a random variable q uniformly distributed over [0,1] is adopted to decide whether Eq. (1) is used. The solution construction formula in ACS is implemented as follows.(2)pijt=τijαηijβ∑j=1componentsxk{τijαηijβ}ifq≥q0τijηijβotherwisewhere q0 is a predefined parameter in [0,1], which determines the probability that Eq. (1) is used. When a random value q is larger than q0, the probability calculation function uses Eq. (1). Otherwise, it is set as the product of pheromone and heuristic values. After all ants have constructed their solutions, the pheromone concentrations are updated according to their evaluated quality. In MMAS, the update of pheromone is accomplished by two stages: evaporation and reinforcement, where evaporation is achieved by using a factor to decrease the pheromone concentrations and reinforcement is only executed on the pheromone of the best solution in current iteration. This pheromone update is performed as follows.(3)τij=[ρ⋅τij+Δτijb]τmaxτminwhereΔτijb=1/f(sbest)and f(sbest) evaluates the iteration best solution or the global best solution. τmax and τmin are respectively the upper and lower bounds on the pheromone, and ρ is the pheromone evaporation factor.In ACS, there are two phases in pheromone update, such as local updating rule and global updating rule. The local update is defined as follows.(4)τij=(1−μ)⋅τij+μ⋅τ0where μ(0<μ<1) is the pheromone decay parameter and τ0 is an initial pheromone value at the beginning of the algorithm.The global updating rule is only performed on the best ant, as defined by(5)τij=(1−ρ)⋅τij+ρ⋅Δτijifedge(i,j)is the best solutionτijotherwisewhere Δτijdenotes the solution cost of the global best solution and ρ is a pheromone decay factor.In the above-mentioned ACO algorithms, ants create a path through the procedures of solution building, evaluation, and pheromone updating, which need to be repeated by numbers of iterations. Using the experience of previous iterations, the pheromone concentration values are accumulated and thus able to guide the ants to find a better solution.Many researches on classification task have been launched to apply ACO for the discovery of classification rules. The first ACO-based classification algorithm was proposed by Parpinelli et al. [23], named AntMiner algorithm. It uses a heuristic search method to find the rule information of data sets and a sequential covering strategy to discover a rule. After its discovery, the training set is further reduced by removing the training examples covered by the discovered rule. Thereafter, other rules are found using the remaining training examples. This process keeps iteratively until the training set is empty or almost empty.Generally, an ant builds a rule by starting with an empty rule set and incrementally adding one term each time. The addition of a new term into the current partial rule is mainly determined by its pheromone and heuristic values. The heuristic value is calculated through the entropy of terms and their normalized information gain [24]. After the rule is constructed, the classification consequence of the rule is assigned by a majority vote mechanism of the training examples covered by the rule, and the constructed rule prunes the irrelevant terms in order to raise its accuracy [25]. After that, the next artificial ant starts to construct another rule. Once all the ants have built their rules, the iteration best one among the constructed rules is found, and the pheromone is updated on its basis [26]. Then, if the quality of the iteration best rule is better than that of the global best rule, the iteration best rule will replace the global best one. Otherwise, it is discarded. The training examples correctly classified by the global best rule are removed from the training set. The next global best rule is found iteratively in the same manner by using the remaining training set. This process of constructing a global best rule is repeated until the maximum number of iterations is reached, or the current constructed best rule is the same as the best one constructed by a specified number of previous iterations. The list of discovered rules grows and the algorithm stops when a termination condition that the remaining examples are less than the user-defined maximum uncovered examples is met. At last, an ordered set of rules is outputted at the end of the algorithm, which is the classification model used for classifying the test data set. The high-level pseudocode of Ant-Miner is described in Fig. 1.In AntMiner2 [27] and AntMiner3 [28], a simple heuristic function calculated only once for each term in the discovery of a rule was proposed by using density estimation, which replaces the complex heuristic function introduced in AntMiner. Besides that, the distinct feature of AntMiner3 is to present a new pheromone update method, in which the pheromone is updated and evaporated only for those predefined conditions occurring in the rule. By this way, the behavior of exploration is encouraged. Working along the same research direction, an enhanced version of AntMiners (AntMiner+) designed a class-specific heuristic function, which enables the ants to know the class of an extracting rule [29]. The class label is chosen in AntMiner+ before ants construct their rules. The two important ACO parameters α and β are controlled adaptively to adjust the relative importance of the pheromone and heuristic values. Recently, a new heuristic function was presented in AntMiner-CC [30] based on the correlation between attributes of a data set, which considers the relationship between the previously selected term and the candidate term, given a preselected class label along with its potential for maximizing correct coverage. Moreover, a flexible order of term selection is introduced in AntMiner-CC, where the terms have no ordering in the search space instead of the user-defined bias for splitting the data by attributes occurring earlier. A similar research direction on unordered list of classification rules was also studied in [31], which discovers more modular rules that can be interpreted independently from other rules. It is unlike the rules in an ordered list that the interpretation of a rule requires knowledge of the previous rules in the list. Several extensions were embedded into the original AntMiner algorithms, to form a new algorithm μcAnt-Miner [32], which utilizes multiple pheromones level types related to the selection of the rule's consequent class prior to the antecedent of the rule construction. The update of pheromone occurs on the corresponding pheromone type of the class value.Traditionally, continuous attributes is handled in a data preprocessing phase before the ACO-based classification algorithm is run. However, cAnt-Miner [33] presented the entropy-based methods to create discrete intervals for continuous attributes discretization during the rule construction process. On the basis of cAnt-Miner, a new approach for handing continuous attributes was proposed for classification in ACO algorithms [34], which adopts the minimum description length (MDL) principle to allow the construction of discrete intervals in the lower and upper bounds. As the new discretization method enables the pheromone to deposit on edges instead of vertices of the construction graph, it can well solve the attribute interaction happened in the continuous attributes handling approach of cAnt-Miner [33]. Based on the discretization approaches in [33,34], a new sequential covering strategy for ACO named cAnt-Minerpb was designed to induce classification rules, which converges to the best list of rules instead of the list of best rules [9]. One important characteristic of this sequential covering strategy is that there is no predefined number of rules required to build a candidate list of rules and ants, which have high flexibility to construct lists with different lengths.Besides, the mix of ACO with other nature-inspired heuristic algorithm is very competitive as their advantages can be combined. A hybrid algorithm mixing AntMiner and particle swarm optimization [35] provided a very promising performance as they can work effectively by combining their strengths. The PSO algorithm is used to deal with the continuous valued attributes while the ACO algorithm is employed to handle the nominal valued ones. Both of them are jointly operated to construct rules. Moreover, ACO is also investigated to hybridize with the well-known machine learning algorithm, i.e., decision trees. A novel ACO algorithm Ant-Tree-Miner was proposed to induce decision trees rather than a set of rules [18], which is consequently very different from AntMiner and its variants. The decision trees have the advantage to represent a comprehensible model and they are easily expressed in a graphical form or in a set of classification rules that are constructed by the ACO algorithm. Other more relevant studies about AntMiners can be found in [36].The proposed AntMinermbc algorithm constructs each list of rules following the general structure of ACO algorithm. Some modifications are made in AntMinermbc. Firstly, a new classification model is designed to generate multiple base classifiers in order to dig more useful information between predictor and class attribute values. Secondly, a new heuristic function is presented to consider both of the correlation and coverage. At last, different from the traditional approach that prunes all the rules one by one, our scheme only prunes the local optimum rule and the iteration best rule is replaced by the global best rule for guiding ants to construct better rules in pheromone update process. The pseudo-code of AntMinermbc is described in Fig. 2. Each ant builds a rule from the training examples until all the ants obtain their own rules. Then, the constructed rules are evaluated using Klösgen measure [37], in which the precision is corrected for the class distribution. Only the irrelevant terms of the iteration best rule are pruned, which help to reduce the computational cost. After multiple assessment and screening, the pheromone concentration of the global best rule is updated to guide ants to find the better rules in the next iteration.At first, in line 1 of Fig. 2, it starts with the empty multiple lists. Each for loop is run in lines 2–28, which adds a list of rules to the multiple lists. Each list of rules is constructed by starting with an empty decision list of rules in lines 3–4. In lines 5–26, an outer while loop is performed, which iteratively adds one rule at a time to the decision list until the termination criteria is satisfied. An inner while loop is executed in lines 9-23, where a rule is constructed by an ACO procedure in each iteration. In order to create a rule, all ants probabilistically choose terms to be added to their current partial rule, according to the pheromone concentration and a new heuristic function that can avoid deceptive high accuracy. It will stop the list of rules construction process until the number of rules is the same with the number of ants. The rules created by ants are filtered in line 14 to remove duplicate rules in the list. Then, the quality of the rules in current iteration are evaluated and a best rule in this iteration is selected to store as the iteration best rule, as described in lines 15–17. The iteration best rule created by an ant is pruned in line 18 to remove irrelevant terms from the rule antecedent. In lines 19–21, if the iteration best rule is better than the global best rule, the former one has to take place of the latter one. Finally, the pheromone trails are updated in line 22 using the global best rule based on a quality measure. The inner iteration loop in line 9–23 runs repeatedly until all the pheromone values on the global best rule are τmaxor all the pheromone values on the other rules are τmin, where τmaxand τminare respectively the upper and lower bounds on the pheromone. The global best rule constructed along this iterative process is added to the decision list of rules and the covered training examples (training examples that satisfy the antecedent of the global best rule) are removed from the training set in line 25. The procedure of creating a rule is repeated until the accuracy on the validation begins to reduce. This prevents the classifier from the noise in the training data and the separate validation set can be monitored during the training phase.The directed acyclic graph is constructed that serves as the ants’ environment. Each value in the predictor attributes represents a vertex, which has its own heuristic information. These vertices are connected by edges that provide the pheromone concentration. All ants start in a virtual ‘Start’ vertex and run through the environment to a ‘Stop’ vertex. When an ant visits a vertex, it cannot be visited again by the ant. For example, an attribute called gender has two possible values {man, woman}. If an ant chooses a value ‘gender=man’, it cannot select a value ‘gender=woman’ again. Therefore, in this environment, the ant visits vertexes in accordance with the order of the attributes to avoid attributes conflict.An example of ants’ environment is shown in Fig. 3. Suppose that there are three attributes named A1, A2 and A3 respectively having the values of 3, 2 and 3. An ant begins from the ‘Start’ vertex and ends at the ‘Stop’ vertex. All ants select a vertex one by one. Finally, they will find their own routes, which represent the potential solutions for the target problem.Our algorithm follows this graph in Fig. 3, where the class attribute, the control parameters α and β are represented in the ants’ environment graph. Because all ants select the terms that are relevant to a specific class, they need to choose a class attribute value prior to the rule construction. Therefore, the class attribute constitutes the first layer of the environment graph. Then, the two parameters α and β constitute the vertices of the second and third layers, which completely connect with each other. Besides, the discretization of a continuous valued attribute is not accomplished in the preprocessing process, as it is handled within the construction graph. Each continuous valued attribute has two layers of vertices, which determine the lower and upper bounds. It includes multiple intervals, in which the best one interval is added to the constructed rule.On the other hand, in Ant-Tree-Miner (ATM) algorithm [18], it has a different ants’ environment as depicted in Fig. 4. The graph construction process generates a decision tree, which has a virtual ‘start’ vertex as the root of the candidate decision tree. All vertices represent the predictor attributes and one vertex per attribute. The attribute vertices are connected by edges to the ‘start’ node. Each ant starts to run from the root node until it finds a leaf node that is a class value. In the rule construction, a continuous attribute can be selected multiple times in the same path.After the graph is constructed, an artificial ant starts to select attribute vertexes as terms in the antecedent part of the rule construction. The pheromone concentration and the heuristic information can guide the ant to find the better terms. The selected term should not be already present in the current partial rule. The probability of selection of terms is given by the following formula.(6)pij(t)=τijα(t)ηjβ(t)∑k=1total next valuesτikα(t)ηkβ(t)where τij(t) is the concentration of pheromone between termiand termjfor the tth ant, ηj(t) is the value of the heuristic information in termj. τik(t) is the amount of pheromone concentration between termiand termk, where k is a value from 1 to the total number of next attribute values. ηk(t) is its current value of the heuristic function. All the selected terms belong to those attributes that have not become prohibited. The parameters α and β are the two weight parameters that adjust the relative importance of the pheromone and heuristic information to control the next movement of the ant.In the above graph, the heuristic information value is given for each vertex, which is important for an ant to select the next term. It is employed to guide the ant to take the next step and provide a useful indication. In AntMiner, the heuristic information is determined by the entropy and normalized information gain. A modified method presented in AntMiner2 and AntMiner3 calculates only once for the discovery of a rule based on the majority class. A further improvement in heuristic function is designed in AntMiner+, which is described as follows.(7)ηi(t)=|termi∩CLASS=classant||termi|where termiis ith value of one attribute and CLASS is a value of class attribute selected by ants prior to the rule construction. Eq. (7) is calculated using the heuristic value of a candidate termi, which relates to the previously chosen class value by an ant. This heuristic function only considers the correlation of the next term with the chosen class value. For example, suppose that the number of training examples including v1 is 2 and the heuristic information of an attribute value v1 is computed. Both two training examples with the same class labels are selected by ants. Thus, the heuristic values of v1 is calculated to be 1 according to Eq. (7), denoted by η1=100%. Supposing that the number of training examples including v2 is 50, to calculate the heuristic information of another attribute value v2, there are 45 class labels of those training examples being the same as the selected class. Thus, the heuristic values of v2 is 0.9 as computed by Eq. (7), indicated by η2=90%. As the attribute value v1 has a higher heuristic information value than v2, the ant selects v1 as the next vertex and adds it into the current partial rule. However, as v1 covers less training examples, its high heuristic value has potential fraudulence. Therefore, in fact, it can construct more advantageous rules by adding v2 into the current partial rule.Therefore, a new heuristic function is designed in our scheme, which considers both of the correlation and coverage to avoid the deceptive high accuracy. Our novel heuristic function is given by(8)ηi(t)=|termi∩CLASS=classant|+1|termi|+kwhere k is the total number of class. By using Eq. (8), an ant can select a more appropriate term to construct rules. Our proposed heuristic function can well handle the aforementioned problem by considering both of correlation and coverage. Suppose that the total training examples have three class labels, and k is same with the number of class labels. Using Eq. (8), the heuristic values of v1 and v2 are computed and their results are respectively 0.6 and 0.868, denoted by η1=60% and η2=86.80%. v2 will be selected by ants and added into the current partial rule. By this way, the ant can construct high quality rules by considering both of correlation and coverage.Even using the same training examples and classification algorithm, the constructed rules may be significantly different. This in turn means that a rule discovery algorithm is unstable in a certain sense. Thus, multiple base classifiers are designed in this paper to exploit the differential information of rules. The classification accuracy can be improved as the different construction rules can remedy the weakness of one another. The construction of multiple base classifiers model is shown in Fig. 5.Each based classifier is generated in sequence. At first, multiple subdata sets D1, D2, …, Dnfrom the original data set D are created and each subdata set keeps the same size with the original training data set. The distribution of each sample may be different in each subdata set. In other words, some samples may appear several times in the training set and some may never appear. Assuming that the sizes of original training data set and subdata set are N, and each sample in original training data set has a uniform probability (1/N) to be selected into subdata set. Thus, the final probability of each sample included in one subdata set is calculated [38], as follows.(9)pji=1−1−1NNwherepjiis the probability that the ith sample is included in jth subdata set. When N is set to be large enough, the probability converging to a stable value is 0.632. Thus, each subdata set contains about 63% of the original training data set. The generated subdata sets can enhance the ability of the expression of all the training samples and exploit each sample to create a classifier. Besides, this approach improves the generalization error by reducing a based classifier's variance. If the constructed base classifiers are unstable, this approach also reduces the training data error by random fluctuations.After that, each subdata set uses the classification algorithm with ACO to build a classifier. The subdata sets D1, D2, …, Dnrespectively correspond to different classifiers C1, C2, …, Cn. Different data sets lead to the different results that are provided by the same classification algorithm. Therefore, the different classifiers can compensate for each other's weakness. By this way, the instability of single base classifier can be mitigated and the algorithmic robustness is thus enhanced. At last, all the test examples are classified by the multiple base classifiers.As the multiple base classifiers introduced above will produce lists of rules, weighted voting mechanism is adopted to further classify the testing data sets. A testing sample needs to be matched with all the rules. In this process, one testing sample can achieve multiple class labels, which are memorized in a recorder to support the voting mechanism. At last, one class label that owns the most votes in the recorder is selected. By this way, a testing sample can be classified using our generated classification model. An example for classification is given in Fig. 6, in which there are three classes A, B and C in a data set. Suppose that our algorithm creates three classification models, i.e., Model1, Model2 and Model3, which respectively own 3, 2 and 4 rules. After the sample matches with the nine rules, nine classification results are obtained based on the rules and further memorized in the recorder to vote which class owns. As the votes for the classes A, B and C are 6, 2 and 1 respectively, the class A owns the maximum votes and the class label of this sample is decided to be A.However, the weighted voting mechanism does not consider the pros and cons of each rule. In each classifier, with the decrease of training samples, the performance of a rule is different with each subdata. The rules are sorted in descending order based on their performance. In general, the performance of the previous rule is better than that of the latter one. Therefore, the weights from the last rule to the first rule are gradually increased by 0.01 from 1.0 in order to distinguish the significance of different rules. It is noted that the majority class label rules are not discovered in our algorithm. Once one testing example does not match with one rule, the class label of the example is majority class, which helps to improve the accuracy of the classifier. As the probability of mismatch is high, the weight for the majority class requires a special consideration to be set smaller than other classes, which is set to 0.1 in our case. The weighted voting mechanism makes full use of the result from every rule, by strengthening the advantages of the excellent rule and weakening the disadvantage of the inferior rule. Therefore, the predictive accuracy of our proposed algorithm is effectively improved as validated in the simulation parts of Section 4.The pheromone values are associated with an edge between two vertices in the graph. In pheromone matrix, the decrease of the pheromone concentration is accomplished by pheromone evaporation. With the time going by, the amount of pheromone on all the edges reduces through an evaporation factor ρ, while the global best rule based on its quality reinforces its pheromone concentration. The quality of a rule is shown as follows.(10)Q=TP+FPP+N⋅TPTP+FP−PP+Nwhere TP and FP respectively refer to the numbers of correct and incorrect examples covered by the rule that have the same class label. P is the total number of examples whose class labels are the selected class. N is the total number of examples belonging to other classes. Eq. (10) is used to evaluate all the rules. Then, the pheromone update formula is given by(11)τi(t+1)=ρ⋅τi(t)+Qbestcwhere Qbestis the quality value of the global best rule. The parameter c is set as 10 to ensure that the pheromone values are bounded in [0,1]. With the accumulation of pheromone steady, subsequent ants are guided to construct better solutions.In this section, the experimental setup is first introduced and then the performance of our proposed AntMinermbc algorithm is evaluated using 19 publicly available data sets from UCI machine learning repository [39]. The main characteristics of the data sets are summarized in Table 1, which also can be found in [39]. These data sets include binary (two class values) and multiclass (more than two class values) classification problems with both nominal and continuous predictor attributes. Our proposed algorithm AntMinermbc is compared with two well-known machine learning algorithms (C4.5 [15,16] and JRip [17]), two recently proposed ACO-based algorithms (Ant-Tree-Miner [18] and cAnt-Minerpb[9]), and a variation of AntMinermbc, as follows.1.AntMiner2mbc (AM2mbc): It is a variation of AntMinermbc without the use of new heuristic function and its majority class weighting is set as 0.05 when classifying the testing examples.C4.5 (Weka's J48 algorithm) [15,16]: The decision tree is built to extract a set of rules. Each rule is constructed from the root node to a leaf node in the decision tree and the construction rules are ranked based on their predictive accuracy.JRip [17]: A set of rules for each class value is constructed by using a sequential covering strategy. The rules take into account both of the quality and length by employing a global optimization step.cAnt-Minerpb (cAMpb) [9]: It is a new sequential covering strategy for ACO classification algorithm, which conquers the drawback of rule interaction. The order of the rules is implicitly encoded as pheromone values and the search of the next solution is guided by the quality of a candidate list of rules.Ant-Tree-Miner (ATM) [18]: It is a combination of traditional decision tree induction algorithm and ACO algorithm. Different from Ant-Miner and its variations, ATM algorithm builds a decision tree rather than a set of rules.There are many performance metrics for evaluating the classification algorithms, while our main consideration is the classification accuracy, which is obtained based on the numbers of correct and incorrect class label predictions by the classifier. It is expressed by the percentage of testing examples correctly classified by the classifier. Generally, the more correct class label predictions indicate the better classification accuracy. It is used in many ACO-based classification algorithms as a significant performance metric for comparison. Besides that, the number of discovered rules and the number of terms per rule are used as indirect performance metrics, which can reflect the comprehensibility of generated classifiers [40,41].The generated classifiers classify the testing examples through a tenfold cross-validation procedure, where a data set is divided into 10 partitions equally and mutually exclusive subsets. Only a subset is used for testing, while the other nine subsets are employed for training. At last, the results of 10 runs are averaged, which is reported as the final result.AntMinermbc algorithm has five parameters: the maximal number of iterations, the number of ants, the evaporation factor, the control parameter of pheromone (α), and the control parameter of heuristic (β). The more numbers of iterations and ants are generally expected to give the better results. However, continually increasing the two parameter values may result in more execution time and no significant increase in accuracy. We have used the F-Race [42] racing procedure to find a better configuration of parameters. Three different values for the parameters are tested, i.e., the number of ants={200, 500, 1000}, and each of them with three different values of the maximal number of iterations={50, 200, 500}. There are nine combinations of parameter settings in total, which are commonly used in ACO-based algorithms [9,18,30]. Experiments show that the algorithm is able to converge to a better solution when the maximal number of iterations is set to 200. Meanwhile, it saves a lot of computational resources. Besides, the number of ants is set to 1000, which ensures that more ants are employed to find a better solution. For a given data set, the increase of evaporation factor ρ can result in a slower convergence process. It is validated in lots of experiments that the evaporation factor set to 0.85 can obtain higher accuracy while maintaining a reasonable execution time [29]. The control parameters α and β are initialized in advance and then adaptively adjusted during ACO-search process. As introduced in Section 3.1, two new vertex groups in the construction graph are respectively employed for searching the optimal values of α and β, which are limited to integers in [1,3] [29]. All the parameter settings for ACO algorithms, i.e., AntMinermbc, AntMiner2mbc, cAnt-Minerpb and Ant-Tree-Miner, are shown in Table 2, while the parameter settings of C4.5 and JRip are set as recommended in their papers [15–17].In this subsection, the performance of our algorithm (AMmbc) is compared with two ACO-based classification algorithms (Ant-Tree-Miner (ATM) [18] and cAnt-Minerpb (cAMpb) [9]), two classical classification algorithms (C4.5 [15,16] and JRip [17]), and a variant of our algorithm AntMiner2mbc (AMmbc2). AMmbc and AMmbc2 are implemented by us in MATLAB R2013b. The software myra-3.7 [43] is adopted to run ATM and cAMpb, while the Weka machine learning tool [44] is used to execute C4.5 and JRip. The predictive accuracies of all the algorithms are given in Table 3, where all the results are obtained by using the tenfold cross-validation and the best accuracy for each data set is identified with boldface. The resultant results in Table 3 represent the average accuracy achieved by the cross-validation procedure followed by the standard error for all the algorithms in the corresponding data sets. The experimental results concerning the size of the constructed classification model are summarized in Table 4, where the lowest model size for each data set is marked with boldface. For C4.5 and ATM algorithms, their results are measured as average numbers of leaf nodes in the generated decision tree. For the remaining algorithms, the size of classification model is obtained by recording the average number of rules.In order to measure the statistical significance of the experimental results, the Vargha-Delaney A-test [45] is executed, which is a useful non-parametric effect magnitude test that can differentiate between two samples of observations. Assuming that two samples A and B are respectively obtained by two algorithms, Vargha-Delaney A-test returns a probability value (p-value) in [0,1], which indicates the probability that a randomly selected observation from A is bigger (or smaller) than a randomly selected observation from B. By this way, it represents how much the two samples are overlapped. The thresholds for p-value are suggested in Vargha-Delaney A-test for interpreting the effect size, where [0.44, 0.56] means no difference, [0.36, 0.44] and [0.56, 0.64] indicate a small difference, [0.29, 0.36] and [0.64, 0.71] mean a medium difference, [0,0.29] and [0.71, 1.0] indicate a large difference.When considering the predictive accuracy in Table 3, AntMinermbc obtains the best performance in 9 out of 19 data sets. The second row “rank” for each data set indicates the performance rank obtained by the corresponding algorithms, and the second last row “A.rank” calculates the average rank for all the data sets, which can reflect the comprehensive performance in solving all the data sets. As observed from A.rank, AntMinermbc performs best as it obtains 2.74 ranks over all the data sets, while AntMiner2mbc, cAnt-Minerpb, Ant-Tree-Miner, C4.5, and JRip respectively achieve the A.rank values with 3.42, 3.42, 3.55, 3.63, and 4.24. Moreover, as indicated by the p-value from Vargha-Delaney A-test, the predictive accuracy obtained by AntMinermbc has no statistical difference with cAnt-Minerpb in 5 data sets, with AntTreeMiner in 3 data sets, with C4.5 in 3 data sets, with JRip in 2 data sets, and with AntMiner2mbc in 10 data sets. Besides that, AntMinermbc performs significantly better than cAnt-Minerpb in 8 data sets, than AntTreeMiner in 11 data sets, than C4.5 in 10 data sets, than JRip in 13 data sets, and than AntMiner2mbc in 8 data sets. The last row “Better/Similar/Worse” summarizes the number of data sets that the performance of AntMinermbc is significantly better than, similar with, and significantly worse than the compared algorithms. Observed from the last row in Table 3, it is reasonable to conclude that our algorithm performs best in the predictive accuracy as it gives better performance on most of the data sets when compared with other algorithms.When concerning the average size of classification model, different criteria are adopted for different types of algorithms and it is still fair to the compared algorithms. For discovered decision tree, the leaf nodes are class labels, which can reflect the size of a tree as illustrated in Fig. 4. Generally, more leaves indicate more complexity of tree. In rule discovery classification algorithms, the discovered classification model has a list of rules. As each rule has a special class label, the number of rules can reflect the size of the rule list. As observed in Table 4, AntMiner2mbc that discovers the classification model obtains the lowest size, as its A.rank value is 1.71 over all the data sets. AntMinermbc and JRip receive the second and third ranks respectively, which have the A.rank values 2.13 and 2.79; cAnt-Minerpb and Ant-Tree-Miner get the fourth and fifth ranks with the A.rank values 3.68 and 4.82 respectively; C4.5 produces the biggest model size as its A.rank value is 5.71. Based on the Vargha-Delaney A-test, AntMinermbc obtains the lower size of classification model than cAnt-Minerpb in 15 out of 19 data sets and has no statistical difference in 3 data sets. Compared to AntTreeMiner and C4.5, AntMinermbc performs significantly better in 18 data sets and obtains no statistical difference in only one data set. Besides that, AntMinermbc is significantly better than JRip in 11 data sets and not statistically different in 3 data sets. Compared to our algorithm's variant AntMiner2mbc, AntMinermbc can only perform better in 3 data sets, but they have no statistical difference in 13 data sets. Therefore, when considering the average size of model, AntMinermbc can achieve sufficiently better results than cAnt-Minerpb, AntTreeMiner, C4.5 and JRip in most of data sets. AntMiner2mbc performs better than AntMinermbc with no statistically difference in most of data sets.To sum up, it is reasonable to conclude that the experimental results obtained by AntMinermbc are very promising. When considering the predictive accuracy, it obtains the best A.rank value and outperforms all the compared algorithms in most of data sets. Regarding to the classification model size, AntMinermbc only performs slightly worse than our variant AntMiner2mbc and significantly better than the remaining compared algorithms. Therefore, except AntMiner2mbc, AntMinermbc has the obvious competitiveness when compared with the other algorithms.Fig. 7illustrates the average model size rank versus the average predictive accuracy rank of all the compared algorithms. Using the concept of Pareto dominance in multi-objective optimization [46], an algorithm A dominates another algorithm B if and only if the following two conditions are true: (1) A is not worse than B with respect to both objectives, i.e., accuracy and the model size. (2) A is strictly better than B at least in one objective. An algorithm is said to be Pareto-optimal only if it is not dominated by any other algorithms. As observed from Fig. 7, AntMinermbc and AntMiner2mbc are two Pareto-optimal algorithms as AntMinermbc ranks first in the predictive accuracy and second in the model size, while AntMienr2mbc gets the first rank of the model size and second rank of the predictive accuracy. They are highlighted by plotting a straight line, which illustrates the Pareto-optimal front [47].It is particularly interesting to note the performances of AntMinermbc and AntMiner2mbc are different mainly because they adopt different heuristic functions. AntMinermbc obtains higher accuracy than AntMiner2mbc in 8 data sets and similar accuracy with AntMiner2mbc in 10 out of 19 data sets, while AntMinermbc gets smaller model sizes than AntMiner2mbc in 3 data sets and similar model sizes with AntMiner2mbc in 12 out of 19 data sets. As the proposed model of multiple base classifiers is adopted in both of AntMinermbc and AntMiner2mbc, their obvious advantages over the other compared algorithms validate the effectiveness of the multiple base classifiers model.In order to further study the impact of multiple base classifiers on the performance of AntMinermbc and find a suitable value for the total numbers of base classifiers in AntMinermbc, the following experiments are conducted by using the F-Race [42] racing procedure with different numbers of classifiers in AntMinermbc, such as 2, 5, 10 and 15 base classifiers. The experimental results including various numbers of base classifiers in AntMinermbc are summarized in Table 5, where the best accurate value for each data set is marked with boldface. It is found that AntMinermbc using 10 base classifiers is able to obtain the highest predictive accuracy as its A.rank value is 1.94 over all the data sets. AntMinermbc with 15, 2 and 5 base classifiers respectively achieve the A.rank values of 2.5, 2.72 and 2.83. Moreover, AntMinermbc with 10 base classifiers obtains the best accuracy values in 9 out of the 19 data sets, while AntMinermbc with 2, 5 and 15 base classifiers respectively perform best in 3 out of 19 data sets. It is noted that AntMinermbc with 15 base classifiers costs unbearable execution time while AntMinermbc with 2 and 5 base classifiers obviously lower the predictive accuracy. Therefore, 10 base classifiers are suggested in AntMinermbc as it performs best in the predictive accuracy with reasonable execution time.

@&#CONCLUSIONS@&#
In this paper, a novel ACO-based classification algorithm called AntMinermbc, is proposed, which obtains a high predictive accuracy with low model size. Three new features are designed in our algorithm. The first one is a new heuristic function, which takes into account the coverage to avoid the fraudulence information. By this way, it can effectively guide ants to select a better term into the current partial rule. The second one is the construction of multiple base classifiers, which can compensate for each other's weakness as rules may miss some useful and correct information. Thus, it obviously reinforces the predictive accuracy of the classification algorithm. The last one is a weighted voting mechanism, which further classifies the test examples based on the outputs from multiple base classifiers. It assigns different weighted values to each rule and the majority class, which fully exploits of the advantages of different rules and majority class. These new features are implemented in our algorithm and 19 publicly available data sets are used to evaluate the classification performance of AntMinermbc. When compared with four state-of-the-art commonly used classification algorithms, i.e., C4.5, JRip, Ant-Tree-Miner, and cAnt-Minerpb, our algorithm performs better in the predictive accuracy and obtains preferable comprehensibility.There are some potential directions for future research in our algorithm. As it constructs multiple base classifiers instead of building one base classifier, our algorithm will take more execution time to complete tenfold cross-validation. Parallel technology is a promising approach for our algorithm to further reduce the execution time since each ant builds, prunes and evaluates a candidate solution independently. In addition, the clustering ideas and approaches can be embedded into rule construction process, which may guide ants to find the better solutions.