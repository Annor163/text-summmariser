@&#MAIN-TITLE@&#
A novel monochromatic cue for detecting regions of visual interest

@&#HIGHLIGHTS@&#
We study monochromatic cues in the modeling of bottom-up attention.We propose a novel monochromatic cue for ROI detection.Experimental results demonstrate the effectiveness of our method.

@&#KEYPHRASES@&#
Regions of interest (ROIs),Monochromatic cues,Visual attention,Taxonomy,Performance comparison of algorithms and systems,

@&#ABSTRACT@&#
Finding regions of interest (ROIs) is a fundamentally important problem in the area of computer vision and image processing. Previous studies addressing this issue have mainly focused on investigating chromatic cues to characterize visually salient image regions, while less attention has been devoted to monochromatic cues. The purpose of this paper is the study of monochromatic cues, which have the potential to complement chromatic cues, for the detection of ROIs in an image. This paper first presents a taxonomy of existing ROI detection approaches using monochromatic cues, ranging from well-known algorithms to the most recently published techniques. We then propose a novel monochromatic cue for ROI detection. Finally, a comparative evaluation has been conducted on large scale challenging test sets of real-world natural scenes. Experimental results demonstrate that the use of our proposed monochromatic cue yields a more accurate identification of ROIs. This paper serves as a benchmark for future research on this particular topic and a steppingstone for developers and practitioners interested in adopting monochromatic cues to ROI detection systems and methodologies.

@&#INTRODUCTION@&#
Upon seeing images, humans have a natural tendency to pay more attention to some parts of the images. Over the last few decades, computational modeling of such a mechanism (i.e., selective visual attention) has attracted much research interest in computer vision because of its potential usefulness: many computer vision tasks (e.g., image compression [1,2], image quality assessment [3,4], image segmentation [5–7], image watermarking [8], image classification [9], image retrieval [10], surveillance [11], object detection and recognition [12], robot control, navigational assistance, etc.) can greatly benefit from the ability to identify regions of interest (ROIs), which attract the observer's attention, in an image. A large number of methods for detecting such visually conspicuous image regions have been reported in the literature so far, and they mainly belong to either of two broad categories: top-down and bottom-up. The top-down approaches are directed by prior expectations, i.e., they are task-specific or goal-driven. More specifically, with the proliferation of (pretrained) object recognition systems, they may attempt to estimate the likelihood of existence of interesting objects (e.g., face [13]) at each location and scale in an image. The vast majority of ROI detection methods focus on the behavior of bottom-up attention. The bottom-up approaches often use findings from psychophysics and physiology. Treisman and Gelade [14] suggest the feature integration theory (which is one of the most influential psychological models of bottom-up attention). They claim that a visual stimulus is composed of separable features (e.g., intensity, orientation, and color). Koch and Ullman [15] introduce the concept of saliency map, which quantifies visual attractiveness at each pixel in an image. Building upon their achievement, Itti and colleagues [16] suggest a biologically motivated computational model, and many variants of it are proposed (e.g., [17]).The central aim of this paper is to study “monochromatic cues” in the modeling of bottom-up attention. Due to their importance in such a modeling [18,19], chromatic cues have been extensively studied in the literature [20–23], whereas investigating monochromatic cues has received less attention. Note here that the monochromatic cues are an important and beneficial complement to the chromatic cues, and this is the motivation of this study: for that reason, to detect salient regions, several authors have employed the monochromatic cues as well as the chromatic cues (e.g., [11,16,24,25]). We first present a taxonomy of existing methods to find ROIs from monochromatic cues, and then introduce a new validated monochromatic cue for the detection of ROIs in an image. We note that the proposed approach is targeted towards “natural scenes” (see examples in Figs. 1 and 7, 8, and 9). Our proposed method is inspired by the fact that human visual perception is highly adaptive and sensitive to structural information in images [26]: we show how such an attribute of human visual system (HVS) can be effectively extended to solve the problem of ROI detection and prove that, unlike existing biologically inspired methods, the proposed method has great ability of providing powerful contextual information regarding ROIs. We finally conducted an empirical comparative evaluation of the algorithms.The rest of this paper is organized as follows. In Section 2, a taxonomy of previous techniques to detect ROIs from monochromatic cues is provided. Section 3 describes the details of our proposed method. Experimental results are reported in Section 4, followed by concluding remarks in Section 5.In this section, we present a taxonomy of existing bottom-up methods to find ROIs from monochromatic cues. One important advantage of the bottom-up approaches is that they do not need any priors. Based on the nature of approach and features used, these methods may be broadly divided into two categories: 1) biologically inspired methods and 2) purely computational methods. ROI detection methodologies in each category may be further classified, i.e., spatial-domain and frequency-domain approaches or local and global approaches. Table 1summarizes the taxonomy of 16 different ROI identification techniques using monochromatic cues.The biologically inspired methods attempt to simulate mechanisms of preattentive vision: specifically, they focus on the neurobiological hypothesis that human vision may preferentially respond to “high contrast stimuli” [27]. These approaches first extract low-level visual features such as intensity, orientation, and texture, then identify ROIs with local or global contrast analysis (e.g., [16]). Ma and Zhang [28] compute the local spatial contrast of image intensity at each location. The authors argue that the locations with high feature contrast also often have rich information. Liu et al. [24] propose to calculate such a visual feature in a multi-scale manner. Seo and Milanfar [29] suggest a bottom-up method motivated by the center-surround contrast mechanism [16,30] of preattentive biological vision. Rather than taking the classical image features, they propose the use of nonparametric kernel density, which is designed to capture local data structure, as a feature. Kim et al. [11] also exploit the center-surround paradigm based on an ordinal signature of feature distribution. The rationale behind this approach is the fact that the ordinal measure of edge orientation histogram indicates the location of interesting regions. Mancas et al. [31] propose an information-theoretic global approach for ROI detection. The underlying hypothesis of this method is that sparse features over the whole scene attract the observer's attention or interest. In their implementation, the saliency (at each pixel) is defined as self-information of the local mean and variance of image intensity. Gopalakrishnan et al. [25] endeavor to take advantages of both local and global contrast analysis schemes. In particular, the authors suggest using the feature entropy, instead of the naive use of low-level feature.As alternatives to biologically inspired solutions, numerous purely computational methods have been proposed. These approaches essentially attempt to find ROIs without mimicking attention mechanisms of human vision. Rosin [32] suggests an edge-based ROI detection method. It assumes that dense regions in an edge map correspond to interesting locations. Given a grayscale image, the distance transform is applied to each of the edge maps obtained from threshold decomposition. The transformed signals are summed and then binarized to detect ROIs. Deng and Luo [33] also employ edge information. They formulate the ROI identification problem as finding closed curves in an edge map. Cohen and Basri [34] introduce a computational approach to locate ROIs from not only grayscale images but also binary images. The authors claim that most of ROIs are circle-shaped. This method computes saliency values based on contour criteria such as closure, convexity, and size. Caron et al. [35] suggest the use of power law distribution, which originates from natural language processing, in detecting ROIs. Unlike the above spatial-domain computational approaches, Hou and Zhang [36] show that the frequency-domain approach is a promising solution for ROI detection. They find ROIs from the spectral residual between the log amplitude spectrum of an image and its blurred counterpart. Guo et al. [2,37] argue that the phase spectrum of an image plays an important role rather than the spectral residual in ROI detection. Bian and Zhang [38] demonstrate that the flattened amplitude spectrum is a good indicator to identify ROIs. Recently, Xu et al. [39] propose to use the spatial-frequency information (i.e., one-dimensional or two-dimensional pseudo-Wigner–Ville distribution) due to its excellence in localizing ROIs. In the method, the statistical property of Rényi entropy for the distribution is exploited. The principle of sparse coding is employed to detect ROIs. In such an approach, a set of sparse coding basis functions is pre-computed and to this end the independent component analysis is usually used. For example, Wang et al. [40] estimate the site entropy rate of the random walk on the graph (which is constructed for each feature map) to compute a saliency map.In this section, we propose a simple yet powerful biologically inspired method for finding ROIs. As discussed in the previous section, most of prior biologically motivated techniques definitely depend on the “contrast-sensitivity” attribute of human vision, but they often fail to discriminate between ROIs and irrelevant clutter (see Fig. 1(b)–(f)). Unlike the traditional (biologically based) approaches, our method does not rely on any contrast analysis models. The method makes use of the fact that human visual perception is highly adaptive and sensitive to structural information in images [26]. Indeed, it allows robust identification of ROIs even in cluttered natural scenes (see Figs. 1(j) and 7(h), 8(h), and 9(h)). Note that our method does not require any training bases and is targeted towards “natural scenes” (see examples in Figs. 1 and 7, 8, and 9).The motivation of our new approach is to develop a quantitative measure of structural information. From our study, we find that such an image structure metric (ISM) has great ability of providing powerful contextual information regarding ROIs: we will demonstrate that this framework outperforms existing ROI detection techniques using monochromatic cues (refer to Section 4). This new paradigm can be best understood through comparison with the contrast-sensitivity paradigm. Note that, for this demonstration, we have chosen the generalized contrast (GC) analysis model [28,24] (which is the simplest and most widely used contrast measure). A motivating example is shown in Fig. 2(a)–(d), where some different image patches exhibit nearly identical GC. However, their visual attractiveness is drastically different. With the contrast-sensitivity paradigm, it is difficult to explain why Fig. 2(a) and (b) are much more visually appealing than Fig. 2(c) and (d), whereas it is easily discerned with our new paradigm, i.e., the attractiveness scores of Fig. 2(a) and (b) are much higher than those of Fig. 2(c) and (d).Now, we construct a specific example of an ISM with a series of well-motivated steps. We define the image structure measure building upon image patches, assuming that the distribution of pixel intensities in an image patch is either structured or random. Here, we take advantage of the fact that the presence of structured image components (e.g., edges, ridges, corners, junctions, etc.) is indicated by dominant gradient directions, whereas the absence of dominant gradient directions indicates that there is no structured component. Let ψ denote an image patch of size N×N. In this paper, a fixed size square image patch (i.e., N=64) is used: it is demonstrably observed that the best results are obtained with the size of 64×64 patch (refer to section 4). Let ψxand ψydenote the first-order intensity derivatives of ψ, i.e., ψx=∂ψ/∂x and ψy=∂ψ/∂y. We start with transforming an image patch into a gradient vector flow as follows (see Fig. 3(a)):(1)Fψ=Ψ11⋯ΨN1⋮⋱⋮Ψ1N⋯ΨNN,where Ψ denotes [ψxψy]⊤. The next step is to identify dominant (vector) directions (see superimposed arrows in Fig. 3(a)). LetHψdenote an oriented gradient histogram of Fψ. For each gradient vector Ψ(m, n), a bin (of the histogram) is increased in value: the orientation (θ=arctan(ψy/ψx)) of the gradient determines which bin, and the magnitude (|Ψ|=[ψx2+ψy2]1/2) how much is added to it. In this paper, we formulate the identification of dominant orientations as a problem of finding modes in an oriented gradient histogram and solve it using the mean shift algorithm [41] (see Fig. 3(b) and (c)), which is useful to seek modes of data represented as arbitrary-dimensional vector [42]. This allows us to reliably detect dominant gradient directions with a simple thresholding as follows (see Fig. 3(c)):(2)Ωψ=ωH˜ψω>ε,−180∘≤ω<180∘,0≤ε≤1,whereH˜ψωrepresents the normalized mean shift filtered histogram for ω=−180∘, ⋯, 179∘. Ωψand ε represent a set of dominant directions and a predefined threshold, respectively. In this paper, we set the threshold to be 0.6: it is empirically verified that the best detection performance is achieved when ε=0.6 (refer to Section 4). We then compute the directional residual between the gradient orientation and the nearest dominant orientation, θ(m, n)−ϕ(m, n), for each gradient vector Ψ(m, n), where(3)ϕmn=argminω∈Ωψθmn−ω.It is important to note that the directional residual indicates the consistency of gradient orientation: the smaller the directional residuals are, the higher the consistency is (i.e., the more likely the vector flow has dominant orientations). Finally, we define the image structure metric as follows:(4)Sψ=∑m=1N∑n=1NΨmncosθmn−ϕmn.Note that our metric exhibits higher values in structured image components (such as edges, ridges, corners, junctions), allowing discrimination between structured and random components (see examples in Fig. 2). Building upon the proposed measure, the likelihood of each pixel being ROI is given by(5)Rxy=S¯ψxy,whereS¯ψand ψ(x, y) represent the normalized ISM and the image patch centered at pixel (x, y), respectively. Given an image, our proposed method can be summarized as follows:Algorithm 1ROI detection from gradient vector flowFor each local image patch ψ(x, y),1.Compute the first-order intensity derivatives (∂ψ/∂x and ∂ψ/∂y)Build gradient vector flow FψFind dominant orientations of Fψ1create oriented gradient histogramHψof FψadjustHψusing the mean shift algorithmdetect modes from the adjusted histogramH˜ψFor each gradient vector Ψ(m, n),1compute directional residual θ−ϕCalculate ISMSψ=∑∑Ψmncosθmn−ϕmnIn the following section, to confirm the effectiveness of the proposed method, extensive experimental results on large scale challenging test sets of natural scenes are reported. It will be shown that the ISM provides great contextual information for ROI detection.

@&#CONCLUSIONS@&#
In this paper, we have studied “monochromatic cues” in the modeling of bottom-up attention. We have presented a taxonomy of existing bottom-up approaches to identify ROIs from monochromatic cues. We specifically have proposed a new validated monochromatic cue for the detection of ROIs in an image. Extensive comparative evaluation results reported in the previous section demonstrate that the use of our proposed monochromatic cue yields a more accurate identification of ROIs. Currently, our implementation on a 3.0-GHz Intel processor has taken 4.15s for an image of size 400×300 on average. The proposed algorithm can be parallelized on multiple CPUs for significant speedup.