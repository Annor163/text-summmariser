@&#MAIN-TITLE@&#
An integrated driver warning system for driver and pedestrian safety

@&#HIGHLIGHTS@&#
A driver assistance system that correlates obstacles with driver’ view is proposed.Fuzzy-rules-based subsystems are used for analysis under different road conditions.One set of fuzzy rules for when a vehicle or pedestrian ahead is detected.One set of fuzzy rules for when no immediate obstacle is detected.Lab experiments and comparisons using real-world data show good performance.

@&#KEYPHRASES@&#
Driving safety,Stereo vision,Histogram of oriented gradient,Adaboost,Fuzzy,

@&#ABSTRACT@&#
A semi-integrated system for driver assistance and pedestrian safety is presented. This system is composed of a single camera which focuses on the driver for picking up visual cues and a stereo rig that focus on the road ahead for the detection of road obstructions and pedestrians. While the car is in motion, the driver's viewing direction is obtained and analyzed along with information of road condition and any moving vehicle ahead in order to determine if the current driving condition is safe. In addition, when the vehicle is moving slowly, the system can also detect the existence of a pedestrian ahead and warns the driver if the pedestrian moves in front of the car. This system contains algorithm-based safety analysis as well as fuzzy rules-based analysis for interaction between variables. Our experimental results show that the condition for driver safety can be accurately classified in 94.5% of the tested driving conditions, and the pedestrians can be identified in 93.18% of the tested cases. These were compared to the results of similar systems and shown to be superior.

@&#INTRODUCTION@&#
As the number of vehicles on the road rises every year, so does the number of traffic accidents and related fatalities. Chief reasons among the causes are distracted drivers and unfocused pedestrians, according to a study by the National Highway Traffic Safety Administration of USA [1]. In order to help reduce the number of fatal accidents, we propose a primarily vision-based smart digital assistant which would alert the drivers in critical situations. There were many previous studies into different driver assistants [2,3,15,17–19], but most of these studies do not detail the correlation between the driver's attention and the vehicle's environment, or ignore it altogether. For example, Broggi [19] and his team developed the ARGO system by using stereo vision to detect lane edges, and obstructions such as vehicles, but driver's intentions were not taken into account. Trivedi et al. [18] developed the LiLo framework which proposed to utilize multiple types of cameras for tracking vehicle movements and driver's intents, but no detail was provided in their paper on how and if they were correlated. It is the belief of our team that a momentary lapse of attention may not be immediately observable by the vehicle movements alone, and a fatal crash could still occur because the driver may fail to notice a danger ahead, so our proposed driver's safety system not only includes evaluating the car's motions but also analyzing the driver's visual focus simultaneously in order to reduce the number of fatal accidents caused by momentary lapses of attention.Our system determines the vehicle's movements by using the front-facing cameras to locate the lane boundaries, and then determine deviation of the vehicle from the lane center. Other useful information that can be obtained using the front-facing cameras includes the distance to the vehicle ahead, and the type of lane markings. All these information are then analyzed together in a fuzzy system to determine whether the direction the car is moving is both legal and safe during regular driving in the analyses stage. Another piece of useful information is the presence of a pedestrian crossing in front of the vehicle. This piece of information is especially useful when the car is moving slowly, at a time when the driver may assume that it would safe to take his attention off the road for a moment. However, it is in moments like this, when an unfocused driver combined with the unpredictable behaviors of pedestrians, that fatal collisions may result. Many studies have shown pedestrian detection to be an essential function for any driver assistance system [3,4,13,14]. But a pedestrian detection system that can identify humans in different poses and under different weather conditions is a challenging task in itself [14], and in addition, the problem of accurately gauging the distance between the pedestrian and the car make this it even more complex. But by making use of the stereo rig and calculating the disparity map between the cameras’ images, the distance between the pedestrian and the car can be accurately gauged at slower speeds. The camera setup for our system is shown in Fig. 1.Our driver safety system is composed of four analysis modules, each would issue its own evaluation of the driver safety level, and the most severe warning amongst them would be then issue by the system. The safety level is divided into three tiers: ‘safe’, ‘risky’, and ‘dangerous’. The four modules are: anti-collision, anti-swerving, fuzzy rules-based driving safety analyses in the presence of obstacles and fuzzy-rules-based driving safety analysis in obstacle-less driving situations; an obstacle is defined as either a vehicle or a pedestrian.The organization of this paper is as follows: the driver safety system's overview will be presented in Section 2, how each piece of information is extracted will be discussed in Section 3, how the information are analyzed will be presented in Section 4, a lab simulation of the entire system using the parameters from real-world driving situations will be presented in Section 5, the results of our experiments and comparison of some of these module with other solutions will be presented in Section 6, and conclusions follow in the final section.The basic system block diagram is shown below in Fig. 2. As it can be seen, raw data such as speed and images from both the front-facing and driver-facing cameras are fed into the data extraction stage, where pieces of information are extracted from the input raw data. These pieces of information include the locations and types of lane boundaries, the deviation of the vehicle from the center of the lane, and the alignment between the driver's line-of-sight and the center of the driving lane. In the cases where either a vehicle or pedestrians are detected, additional pieces of information include deviation of driver's line-of-sight from the obstacle, and the distance to the obstacle will also be calculated or estimated. Each piece of data will either be passed through untouched to the analysis stage or fuzzified before being analyzed, depending on the type of analysis to be done.A more detailed system flowchart is shown in Fig. 3. It shows that instead using the distance-to-obstacle value as an input directly; it was first divided by the safety distance so as to express it as a percentage of the safety distance. The safety distance is the minimum braking distance between the vehicle and any obstacle in front of it. Because the safety distance is dependent mostly on the speed of the vehicle, so we use the general rule of thumb of one vehicle length per every 8km/h to determine the minimum safety distance, which is then used to normalize the distance-to-obstacle value. The value of the normalized distance to the obstacle is used to determine whether obstacle is near enough that the fuzzy-rules-based safety analysis in the presence of obstacle should be activated. Or if the obstacle is judged to be too far to be a threat, as determined by the normalized distance-to-obstacle value, then the safety-rules-based analysis for obstacle-less driving is activated instead.As shown in Fig. 3, the fuzzy-rules-based safety analysis for obstacle-less driving module takes as input: the driver's sight deviation from lane center, vehicle's deviation from lane center, and speed. The fuzzy-rules-based safety analysis in the presence of obstacle(s) takes as input: the deviation of the driver's sight from obstacle, vehicle's deviation from lane center, and the normalized distance to obstacle. The anti-swerving analysis module takes as input: the locations of lane boundaries and their types. Lastly, the anti-collision analysis module simply uses normalized distance to the obstacle as input. The following section will discuss how these raw data turn into useful information for the analysis modules.The line-of-sight detection extraction stage uses the driver's image to determine the driver's viewing angle by combining the turning angle of the head plus the glancing angle of the pupils within their sockets. So tracking of the driver's face and eyes is required.The driver's line-of-sight detection is based on a sequence of operations, from face detection, to face tracking, to locating both eyes and nose, finally to determining facial orientation, as illustrated by the flowchart in Fig. 4. For the initial detection of face and eyes, we refer the reader to Viola and Jones’ original paper [5]. For face tracking, we chose the “particle filter” tracking method [6] for both its ease in implementation and its robustness in the presence of occlusions, and because particle-filtering is known to perform better than the traditional Kalman filtering, which was used in the design of Trivedi et al. [18], in applications such as head tracking [20].Particle filtering is a hypothesis object tracker which seeks to approximate the filtered posterior distribution of the object state by a set of particles which are weighed based on a score derived from previous weights, likelihood, prior and importance densities. These N particles are each part of a sample set S, as described in Eq. (1), where each sample is consisted of an element s, a hypothesis state of the object to be tracked, and its corresponding sampling probability, π.(1)S=(s(n),π(n)),n=1…Nwhere∑n=1Nπ(n)=1The samples in S are moved according to a motion model, and weighed according to the observations before these N samples are replaced based on survival of the fittest; with the mean state, E[S], of the object being tracked estimated at each time step by Eq. (2).(2)E[S]=∑n=1Nπ(n)s(n)These steps are then repeated until an estimate can be made. The flowchart of this process is illustrated in Fig. 5.In our system, we used the color histogram-based particle filter tracking of a human head, the pixels enclosing the head be tracked would form the reference window and its color histogram is calculated. The samples in S are possible locations where the head could be in the next time step, which form the target windows. The color histogram intersection values of the reference window with the target windows are calculated and used to weigh the samples, so the more similar are the distributions between the sample window and the target window the larger the weight. Once the face and eyes are found and continuously tracked, the next stage is the determination of the driver's line-of-sight.The driver's line-of-sight is defined as the head-turn angle plus the angle of the position of pupils within the eyes. Since the camera is positioned directly at the face of the driver, the triangle defined by the eyes and the nose would be equilateral when the driver is facing ahead, as shown in Fig. 6, but a turning of the head would be reflected in the change of the shape of this triangle with respect to the camera. The locations of the face and eyes are found using a sufficiently trained Adaboosted classifier, while the location of the nose is estimated by putting the locations of the eyes in an anthropometric face model [7,8], verified using the nasal cavities (since the camera would be pointed slightly upward). The resultant triangle is shown in Fig. 6, with the ratio of the lengths of the lines a and b used to estimate the turning angle of the head. For example, if, from the perspective of the camera, the length of line a is greater than that of line b, then it can be assumed that the driver's head is turning to his right. After several experiments with this method, we can calculate the turning angle of the head from the ratio of a to b, shown below in Table 1. If the driver's head turns toward his left, then the inverse ratio is calculated. The pupil's location within its eye is used to adjust focus angle from −π/4 to+π/4 radians, as illustrated in Fig. 7.The lane detection routine uses the images from the front-facing cameras to detect lane boundaries on both sides of the vehicle, and determines their relative distance from the vehicle center as well as the types of boundaries such as whether they are crossable or not. Both pieces of information will be used in the fuzzy rules-based safety analysis modules and the anti-swerve analysis modules. In order to obtain these pieces of information, an inverse perspective transform is first applied to the road images in order to compensate for the effect of vanishing point, so the boundaries would be detected as parallel line segments. Secondly, lane detection is done by applying the line-detector, Hough Transform, after the application of a global grayscale threshold. The false-positive lines are eliminated by accepting only candidate lines with lengths over a certain threshold. Once the boundary lines on both side of the driving lane are detected, the center of the driving lane is automatically estimated, as shown in Fig. 8(c), where the red lines mark the detected boundary lines and the blue line is the estimated center of lane. After some experiments, we found that this method fails when there are lengthy tire tread marks or missing (unclear) segments of boundary lines. In the case of the tire marks, the length of the mark temporarily fool our system into assuming the tire marks to be boundaries, but this error was re-adjusted once the tread marks disappear. In the case of missing or unclear boundary lines, the boundary line of a neighboring lane was temporarily assumed to be one of the boundary lines and caused false-positive alerts. Future research will seek to deal with these problems.Once we have the candidate boundary segments, we can separate the boundary lines into one of three classes: double solid lines, single solid lines or dashed lines. These are the lane boundaries as defined by the local traffic laws in Taiwan. The line segments are characterized by using the distances between candidate line segments to determine which one of the three types of boundary is most likely. The first of the classes of lane boundary is an opposing traffic divider, marked by parallel yellow lines, as shown in the leftmost figure of Fig. 9, denoted by DS. The second class is the regular traffic divider, marked by single solid lines, as shown in the middle figure of Fig. 9, denoted by SS. The last class is the lane divider, marked by single dashed lines, as shown in the rightmost figure of Fig. 9, denoted by SD. The opposing traffic divider is an absolute boundary, where any attempt to cross or veer toward it will be heavily penalized. The regular traffic divider divides traffic moving in the same direction, and no crossing is allowed. The lane dividers do not constitute absolute boundary, and no penalty is associated with veering or crossing this type of boundary. The legality of vehicle maneuvers based on the types of boundaries on both sides it is shown in Table 2below. The penalties for illegal maneuver are based on whether the boundaries are absolute or not.The vehicle detection module decides whether a vehicle exists ahead, and estimates the distance to it. The distance value is then normalized by the safety distance for use by the anti-collision analysis module which decides whether the vehicles are too close. We first test several features as input to Viola and Jones’ [5] Adaboosted classifier for detecting traveling vehicles from the rear, and found that the best feature is the simplest: the horizontal edge information. Fig. 10shows the morphological operations that took an image containing a vehicle, extract the horizontal and diagonal edges, then filtered out the diagonal edges altogether. The basic outline of the vehicle is visible throughout this sequence of operation mainly due to the obvious edge information describing the rear bumper, the rear glass window and its roof top. So we chose to use this feature exclusively as a compromise between speed and accuracy, and trained a classifier with 486 positive samples of images of vehicles under various weather conditions, which were sufficient.After a vehicle is found, the distance to it must also be gauged. However, the concern here is speed over accuracy, since at high speed of travel, an accident may occurr even for a momentary hesitation. So we decided to abandon the more accurate but time-consuming methods and opted for a rough estimate based on the vehicle traveling speed and the size of the bounding box surrounding a detected vehicle. An example of the result using real-world driving data is shown in Fig. 11, which shows that even though the roof top of the car ahead was not accurately detected, the width of the bounding box surrounding the car ahead along with the vehicle traveling speed was sufficient for a rough estimate of the distance between the two vehicles.Pedestrian detection routine kicks in at lower speeds, so accuracy in detection is of higher concern. The first operation the pedestrian detection routine performs is to calculate the U–V disparity map [16] between the images captured by the stereo rig, based on the setup in Fig. 12, where (X,Y,Z) denotes world coordinate system, and (U,V) denotes the camera coordinate system, with θ being the angle between the optical axis and the Z-axis.The disparity information is used to locate foreground objects in the regions of interest as defined by the disparity map, then determine if each of these regions contains a pedestrian and if so, subsequently calculate the distance between the car and the pedestrian. The flowchart for this process is shown in Fig. 13.The disparity map is calculated as follows: if there is one point in 3D that exists in both left and right images, denoted in camera coordinates as (uL,vL) and (uR,vR), then ▿ is defined as Eq. (3), and by assuming the distance between the camera and the pedestrian is d, and b is the distance between the cameras, f is the focal length, and ▿ being the disparity, then we can obtain the V-disparity map, I(v,▿) using Eq. (4) for the entire width of the image, and U-disparity map, can be obtained likewise for the entire height of the image.(3)∇=uR−uLifvR=VL(4)∇=bd(v−v0)sinθ+bdfcosθLine segments in each image indicate possible foreground objects, and can be projected onto the 2D disparity map to find the candidate regions for pedestrians, the ROIs, as illustrated in Fig. 14.In order to determine whether each of the ROI contains a pedestrian, an Adaboosted classifier that uses, in addition to the Haar-like features proposed by Viola and Jones [5], the histogram of oriented gradients information [12,13], HOG, and geometric information extracted from HOG are used. The histogram of oriented gradients information is calculated using Eq. (5), where f(u,v) is the differential statistics in either u or v direction, according to camera coordinates, and the orientation of gradient is obtained using Eq. (6).(5)m(u,v)=fu2(u,v)+fv2(u,v)(6)θ=tan−1fvu,vfuu,vIn practice, the HOG features are usually obtained by first calculating the gradients of the entire image, in both u and v direction. Followed by dividing the entire image into small cells, where the histograms of oriented gradients are calculated for each cell. Then a vector, V, is obtained which contained the histograms of each cell then normalized, in L2-space, according to Eq. (7), where ɛ can be set equal to 1.(7)V=Vvk22+εSome examples of HOG statistics for pedestrian images are shown in Fig. 15.In addition to using HOG as features, geometric edge information is also added to the HOG features to form a new feature vector for training. In Fig. 16, the center of mass, m(x,y), of the image is shown as the bright pixel in the middle, while the other non-white pixels are strong edge pixels. Eq. (8) is used to convert each pixel values into a vector V, which is the vector from the center of mass to each edge pixels. This vector will be added to HOG as part of the feature vector to be trained.(8)Vem=e(i,j)−m(x,y)The Haar-like features of Viola and Jones’ [5] method, plus the HOG map information, are concatenated to the shape information to form feature vectors for an Adaboosted classifier. Example of the results of pedestrian detection using real world data is shown in Fig. 17, where most of the pedestrians are detected either in the left image or the right image. Finally, the distance from the cameras to each pedestrian, R, can be calculated using Eq. (9) where f is the focal length of the camera, b denotes the baseline distance between the focal viewpoints, and d is the disparity distance. The disparity, d, is the difference between the distance of the detected pedestrian in the left camera image to the center of the left camera image, and the distance of the same pedestrian in the right camera image to the center of the right camera image.(9)R=f×(b/d)Once the pedestrians are located and distances to them are calculated as mentioned above, then the distance will be normalized using the safe braking distance. However, if the normalized distance is less than safe braking distance, then a warning will be issued.The extracted data are used in four analysis modules, two of which are algorithm-based, and the rest are fuzzy-rules-based. This section will present how the extracted data are used in these modules.The idea of automatic fuzzy rules extraction [9,10] is to distribute the entire input space into overlapping intervals, then according some grouping methods output the fuzzy rules for classifying these values, and then finally defuzzify the results to obtain a crisp output value. Fig. 18illustrates one of the fuzzy interference systems for extracting fuzzy rules. By taking, according to input values in overlapping and non-overlapping segments in each interval, the minimum of each interval's maximum values as basis for generating the fuzzy rules, then pass them into a de-fuzzification stage which results in a crisp output value, y.For membership function for the fuzzy interference system, the trapezoidal membership function is chosen, which is shown in Fig. 19.For de-fuzzification, we choose the center-of-gravity method [11], the equation of which is described in Eq. (10), where m is the mean within each interval, σ is the standard deviation, and d is the degree of membership, as shown in Eq. (10). The output, y, will be a crisp value which is an indicator whether the danger level is small, medium, or high, which are non-overlapping levels, evenly distributed intervals between 0 and 1. No action is taken when the indicator shows a low danger level. A simple warning will be given to the driver when a medium level of danger is detected, but a full-blown safety alert will be issued when the system determines that the danger level is high.(10)y=∑i=1nmiσidi(x)∑i=1nσidi(x),wheredk(x)=1,whereμ<x≤U1−max(0,min(1,r(μ−x))),wherex≤μ1−max(0,min(1,r(x−U))),wherex>UThe same fuzzy-inference structure is used for analysis under obstacle-less conditions as well as conditions in which obstacles exist. We will present each separately.Three pieces of information are used as input features in this fuzzy-rule based safety analysis for obstacle-less driving module in order to determine if the driver is driving in a safe manner when in the presence of obstacle(s). These features include the amount of the vehicle's deviation from lane center, LDeviation, the speed of the vehicle, and the deviation of driver's line-of-sight from the lane center in angles. The definition of LDeviation can be seen in Fig. 20and Eq. (11).(11)LDeviation=LCenter−LTraveling(WLane/2)The fuzzy-rules-based analysis system fuzzified each piece of these input information into three levels, “low”, “medium”, and “high”, and output a warning based on the interactions of these variables. For example, if the driver's focus matches the vehicle's maneuver at a relatively safe speed and the maneuver is legal, as the scenario illustrated in Fig. 21, then the fuzzy inference system would issue a “safe” level of danger, and no action would be taken to alert the driver. However, if the driver's focus mismatches the vehicle's maneuver for longer than a short duration, e.g. for about 6 video frames, then even at a relatively safe speed the fuzzy inference system would issue a “risky” warning to the driver to refocus from his current distraction, as illustrated by the scenario in Fig. 22.The input information for the fuzzy-rules-based analysis for driving in the presence of obstacles include LDeviation as defined in Eq. (11), the distance to the obstacle express as a percentage of the safe distance, and deviation of the driver's line-of-sight from the obstacle expressed in angles. The fuzzy-rules-based analysis system then divide each of these information into “low”, “medium”, and “high”, and output a warning based on their interactions. For example, in Fig. 23, LDeviation was low, the obstacle in the same lane was 155% of the safety distance away, but the driver's focus is largely out of alignment with the obstacle, so ultimately the module issued a “risky” warning.The basic flowcharts of the operations of the anti-swerving and anti-collision modules are shown below in Fig. 24. In the anti-swerving module, two pieces of data are required before the driver's safety level can be determined: the amount of vehicle deviation from lane center, LDeviation, and the types of lane boundary surround the vehicle. If LDeviation is large but the maneuver is legal, then no warning will be issued, but if LDeviation is large and the maneuver is deemed illegal, then a warning will be issued.In the anti-collision module, once a vehicle is found, the distance to it will be estimated based on the speed and the width of bounding box. Once the distance is estimated, it will be divided by the safe braking distance, and if the quotient is less than 1.0, i.e. the distance to the vehicle is less than the safe braking distance, and then a warning will be issued.The parameters from 36 real-world driving situations were extracted and used as input to our system to test the system integration. The simulation results of four of the situations are presented. These four situations are illustrated in Fig. 25.These four situations are: (a) following the lane into a left turn at medium speed with the driver's gaze out of aligned with the middle of the road, (b) straight ahead driving at high speed with the gaze unaligned with the middle of the road, (c) slow to a stop with a vehicle ahead with a gap that is 1.45 times the braking distance, and the driver's focus aligned with the vehicle, and (d) changing lane in the middle at a medium speed with a pedestrian crossing at 0.6 times the braking distance away, but the driver's focus was on the pedestrian. Fig. 26shows the results of obstacle detection for the four scenarios. Fig. 27shows the process of analysis by the fuzzy-rules modules. Fig. 27(a) and (b) are the analyses and outputs of fuzzy-rules module for obstacle-less driving, where the variables are speed, gaze deviation, and lane center deviation, respectively. Fig. 27(c) and (d) are the analyses and outputs of fuzzy-rules module for driving in the presence of obstacles, where the variables are lane center deviation, normalized distance to obstacle, and gaze deviation, respectively.The entries in Table 3show the input values for the four cases described above, and the response of each of the four analysis modules. The parameter “Dnorm”, stands for the gap between vehicle and obstacle as a percentage of the braking distance, “Lane” is the type of lane boundary closest to the vehicle, “LDev” is the length the vehicle center deviates from the lane center, “GDol” is the deviation of the driver's gaze from the center of the lane, “GDob” is the deviation of the driver's gaze from an obstacle, and “Speed” is the speed of the vehicle. The four analysis modules are “AS” for anti-swerving, “AC” for anti-collision, “FRol” for fuzzy-rules-based module for obstacle-less driving, and “FRob” for fuzzy-rules-based module for driving in the presence of obstacle. The column “System” presents the resultant warnings which were outputted by selecting the highest level of warning issued by the four analysis modules.Let us follow the system flowchart in Fig. 3 to see how each module responded to, for example, the situation in Fig. 25(a). Fig. 28shows the subroutines and modules that were used this driving scenario. The vehicle ahead was turning just outside of the field-of-view of the cameras so it was not detected, so the “Does Obstacle Exist?” test yielded a “No” answer, which implies that the “anti-collision” and “fuzzy-rules-based analysis in obstacle-less driving” modules did not received certain input, and defaulted to “safe” output. Because, by design, any analysis module that did not receive input will output “safe”, and this is seen in the outputs of AC and FRolmodules which show “safe” in Table 3 for scenario (a). The front-cameras images were also used to detect the lane boundaries, boundaries types and vehicle deviation from the lane center. And because the car was moving in the middle of the lane, the “anti-swerving” module output a “safe” safety level. The driver's face image was used to determine the driver's line-of-sight, which was then calculated along with the estimated center of lane to yield the deviation of line-of-sight from lane center value, which was fed, along with speed and vehicle deviation, into the fuzzy-rules-based analysis in obstacle-less driving module. This module fuzzied the speed value to “medium”, and the lane and sight deviations to “low” and resulted in a “safe” safety level, as shown in Table 3(a). Finally, the output from the four modules was compared and “safe” was determined to be the most severe, thus used as the “System” level warning.The results of other simulations were as expected and verified that system performs like the flowchart in Fig. 3. In the next section, we will compare the performance of certain parts of our system with others methods and present the results.

@&#CONCLUSIONS@&#
We have presented a vision-based driver-assistance system that integrates a driver safety sub-system and a pedestrian safety sub-system in this paper. Individual functions within the system have been verified to work as expected, these include the detection of driver's line-of-sight, the detection of the boundaries and their types of the lane on which the vehicle is traveling, the detection of swerving behavior by the deviations from center of the lane, the detection of a car ahead and approximated the distance to it, the determination of driver's safety level using automatic fuzzy rules extraction, and finally, the detection of pedestrians and determined the distance of the gap between the car and the pedestrian. Each of these modules has been tested to work and provides sufficiently accurate data to help the driver to prevent the occurrence of fatal accidents. Afterward we performed lab simulations to test the performance of the entire system using 36 real-world driving conditions, and the results of the simulation confirmed that our system do perform as expected. We have also shown, by experimental results, that key parts of our system, which are the face tracker, fuzzy rules-based safety level analysis for obstacle-less driving, and the pedestrian detection modules have each performed better than a sufficiently trained Adaboosted classifier, an ANN with 12 hidden nodes, and an Adaboosted classifier trained with a special feature set, respectively. In summary, this paper presented a driver-assistance system that uses fuzzy-rule-based systems which analyze the driver's head and eyes motions in connection with the road conditions ahead in order to determine the driver's safety level. This system is also integrated with a pedestrian detection module that keeps the driver alert to dangerous situations even in moments of driver distraction. Last, the design of this system is composed of loosely integrated modules that react to different ways the vehicle may move, and will be able to accommodate future inclusion of new modules without problem.