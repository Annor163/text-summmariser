@&#MAIN-TITLE@&#
Frame detection using gradients fuzzy logic and morphological processing for distant color eye images in an intelligent iris recognition system

@&#HIGHLIGHTS@&#
Bespectacled eye image will reduce the iris segmentation accuracy.HSV color space provided better space for the segmentation process.Sobel operator and high pass filter provide gradient information on the frame edges.Fuzzy logic is performed as the decision making for frame detection.Dilation operation fills the discontinuity edges of the frame.

@&#KEYPHRASES@&#
Iris recognition,Frame detection,Color eye image,Fuzzy logic,Iris segmentation,

@&#ABSTRACT@&#
The capture of an eye image with the occlusion of spectacles in a non-cooperative environment compromises the accuracy in identifying a person in an iris recognition system. This is due to the obstruction of the iris by the frame which tends to produce an incorrect estimation of the initial center of the iris and the pupil during the iris segmentation process. In addition, it also causes incorrect localization of the upper eyelid during the process of iris segmentation and sometimes, the edges of the frame are wrongly identified as the edges of the upper eyelid. A frame detection method which involves the combination of two gradients, namely the Sobel operator and high pass filter, followed by fuzzy logic and the dilation operation of morphological processing is proposed to identify the frame on the basis of different frame factors in the capture of a distant eye image. In addition, a different color space is applied and only a single channel is used for the process of frame detection. The proposed frame detection method provides the highest frame detection rate compared to the other methods, with a detection rate of more than 80.0%. For the accuracy of the iris localization, upper eyelid localization and iris recognition system, the proposed method gives more than 96.5% accuracy compared to the other methods. The index of decidability showed that the proposed method gives more than 2.35 index compared to the existing methods.

@&#INTRODUCTION@&#
Current studies on the iris recognition system show that the eye images were captured without the cooperation of the person, at different distances and in motion (also known as non-cooperative environment) in order to make the system more accurate and reliable in identifying and verifying a person in a real environment [15,28]. For this reason, the captured eye image has very low quality [26], varying lighting conditions [27,35] and contains a noise factor such as the occlusion of spectacles [14,17,20,28,33]. Spectacles (also known as eyeglasses) are defined as a frame that contains lenses, which are normally used for vision correction [20], eye protection [19] and esthetics or fashion purposes [5,18]. Several studies have shown that, in certain circumstances, wearing spectacles could have a negative impact on the performance of human–computer interaction systems such as driver-fatigue detection systems [5], face recognition systems [4] and iris recognition systems [28]. It is crucial to detect or remove the spectacles from a captured image to increase the performance of systems, and several studies [11,43,44] have been conducted for this purpose except in the case of the iris recognition system. The occlusion of spectacles in the eye image has caused several problems in the iris segmentation performance such as the existence of reflections and incorrect iris and eyelid localizations. The existence of reflections from the lenses (see Fig. 1a) has caused the incorrect localization of iris boundaries due to the edges of reflections being wrongly detected as the edges of the iris boundaries. Thus, several researchers [3,14,31,34,41] have proposed reflection detection and removal methods in order to solve this problem. During the process of iris segmentation, the obstruction of the iris by the frame (see Fig. 1b) tends to produce an incorrect estimation of the initial center of the iris and the pupil [3,14]. Thus, it leads to incorrect iris boundary localization. It can also cause incorrect localization of the eyelid, specifically in the upper region due to the upper region being obstructed by the frame (see Fig. 1c) and the edges of the frame being falsely recognized as the edges of the upper eyelid. The degradation of iris segmentation performance has caused errors in the iris feature selection due to the frame being wrongly extracted as the iris features, thus, reducing the accuracy of current iris recognition systems.Several methods have been proposed to solve the problems existing for frames in human–computer interaction systems. These can be divided into detection and removal approaches. For example, in a driver-fatigue detection system, Cheng et al. [6] used a binarization based method, with a threshold of 40, to detect the frame and implement a dilation operation and skin color model to extend and remove the frame. They concluded that their method is good for detecting and removing the frame if the image was captured in a laboratory environment. However, they suggested an adaptive thresholding method to detect the frame in the real environment. In the face recognition system, Wang et al. [43] implemented an adaptive thresholding method and a reconstruction based method involving iterative principal component analysis to detect and remove the frame from the face images. Wong and Zhao [44] also applied a reconstruction based method involving kernel principal component analysis to remove the frame from the face images. A disadvantage of using the reconstruction based method is that it requires large datasets that contain images with and without spectacles. In addition, sufficient information or additional knowledge is essential for reconstructing the extracted spectacle region [44]. In iris recognition systems, several researchers have claimed that their iris segmentation methods succeeded in localizing the iris even with the presence of spectacles. For example, Sahmoud and Abuhaiba [33] proposed the K-means clustering algorithm and circular Hough transform to determine the iris region and compute the iris center and radius. Then, the upper eyelid is localized using the intensity difference between the sclera and the upper eyelid in the sclera region rather than in the iris region because the contrast intensity between the iris and the upper eyelid is low. Tan et al. [41] assumed that the frame is usually dark in color and approximately rectangle shape. To localize the iris, they introduced a clustering based coarse and integrodifferential constellation; while to localize the eyelids, the one dimensional horizontal rank filter and eyelid curvature model are implemented.Even though several existing iris segmentation methods are able to localize the iris boundaries in the presence of spectacles, the problem remains open due to several frame factors. Firstly, the color of the frame is not usually dark in color; rather, according to Chen et al. [5], the frame color can be categorized into three types, namely: dark, medium and light. Secondly, the shape of the frame does not always approximate a rectangular shape [18], nor is the frame usually in a complete form when captured in non-cooperative environments. The frame shape in non-cooperative environments can be divided into five symmetrical types: (i) full (consists of rectangle and circular shapes), (ii) horizontal, (iii) vertical, (iv) diagonal and (v) undefined. Thirdly, the types of frame can be categorized into full-frame, half-frame and no-frame [18]. Lastly, the thickness of the frame can be classified as thick or thin. The frame factors are summarized in Table 1. Hence, a method that is capable of resolving these factors is needed in order to increase the performance of iris segmentation for the eye images with the occlusion of frames.In this study, two gradients (namely, the Sobel operator [42] and high pass filter [24]), a fuzzy logic [12] and the dilation operation of morphological processing [38] are proposed to identify the frame in distant color eye images in an iris recognition system. The gradients are able to provide the information on the frame edges, the fuzzy logic is performed as the decision making for frame detection and the dilation operation of morphological processing functions to fill the discontinuity edges of the frame. This method is able to extract the edges of the frame from different factors and its performance is better than the existing frame detection methods. The process of frame detection starts with converting the color image into a different color space which is the hue-saturation-value (HSV). Next, each gradient is defined using the appropriate membership function and then the fuzzy inference system is applied based on the inference rules. Lastly, the dilation operation is used to fill the gap in the edges of frame. To explain the proposed method in detail, this paper is structured as follows: Section 2 describes the selection of the database for the bespectacled eye images, namely UBIRIS.v2, Section 3 explicates the proposed frame detection method, Section 4 describes the iris recognition system methods, Section 5 discusses the experimental results and lastly, Section 6 states the conclusions and suggestions for future work.Two databases [10] containing distant eye images are available for iris recognition purposes, namely version four of the Institute of Automation, Chinese Academy of Sciences database (CASIA.v4-Distance) [40] and version two of the University of Beira Interior database (UBIRIS.v2) [28]. During the process of image acquisition, the eye images in the CAISA.v4-Distance were captured using near-infrared wavelength illumination at distances of 3m while the eye images in UBIRIS.v2 were captured using visible wavelength illumination with distances of 4–8m, with motion and under lighting variations. According to Proenca and Santos [25], Shin et al. [36] and Proenca et al. [28], the use of the near-infrared wavelength to capture the distant eye images requires a high level of illumination which could threaten eye safety during the process of acquisition. Moreover, the visible image provides more information than the near-infrared image [32]. The UBIRIS.v2 database was chosen for this study because the eye images in this database has meet the study's requirements, that is, the eye images were acquired at long-range distances, in motion and using visible wavelength illumination. In addition, the eye images in the UBIRIS.v2 database contain more realistic noise factors such as low illumination [29], reflections [3,14,31,34,40], off-focus [37], off-angle [39] and the occlusion of spectacles compared to the eye images in the CASIA.v4-Distance database.The performance of the frame detection and iris recognition system methods was evaluated using approximately 500 bespectacled eye images with less than 30° angles. This was done to avoid inaccurate iris segmentation or decreased of iris recognition system accuracy due to less extracted iris area from the eye images with more than 30° angle. The bespectacled eye images were divided according to different frame factors and distances, with each distance consisting of 100 bespectacled eye images as shown in Table 2. However, eye images with the occlusion of medium color and half-frame spectacles, and light color and half-frame spectacles were not available in the UBIRIS.v2 database. In addition, some of the eye images did not contain frames due to the eye images being captured at a very close distance of 4m. For example, at 4m, the eye images included dark, half-frame and thick factors, or light, full-frame and thick frame factors.Fig. 2presents the framework of the proposed intelligent iris recognition system in this study, which consists of five stages: (i) preprocessing, (ii) segmentation, (iii) normalization, (iv) feature extraction and (v) template matching. The frame is detected at the preprocessing stage to distinguish the edges of the frame and the edge of the iris and eyelids during the process of iris segmentation. The process of frame detection is explained in this section while the other processes and stages are described in Section 4. As mentioned in the introduction, detecting the frame is crucial in order to get the correct localization of the iris and the eyelid boundaries. In order to get the desired frame edges, the process of reflection removal or contrast enhancement is performed. The function of both processes is to reduce the existence of reflections which could be wrongly detected as a frame or iris edge, or to increase the intensity of the level of pixels in the low lighting or contrast eye image (the detailed processes are presented in Section 4). The proposed frame detection method consists of three sub-processes which firstly convert RGB into the HSV color space (as described in Section 3.1), followed by information extraction using two gradients and fuzzy logic (Section 3.2) and lastly, the dilation operation of morphological processing (Section 3.3).In digital image processing, according to Ibraheem et al. [9], object detection and segmentation in images are better in the color spectrum compared to gray images. However, direct practice in the RGB spectrum is not appropriate because a different color space or a color representation system is required in order to produce better results. In this study, the HSV color space was selected because it is the most commonly used color in digital image processing [32] and for computer vision and image analysis, this color space is good for the segmentation process [9]. The conversion into HSV takes place in the following steps:Step 1: Find the maximum, α and minimum, β values of the pixel intensities in the eye image.Step 2: Standardize the RGB values into the range [0,1]:(1)red=(α−R)(α−β),(2)green=(α−G)(α−β),(3)blue=(α−B)(α−β).Step 3: Calculate the maximum of luminance, V value.Step 4: Determine the saturation, S value:(4)ifα=0,thenS=0and hue=180°,(5)ifα<>0,thenS=(α−β)α.Step 5: Compute the hue, H value:(6)ifR=α,thenH=60(blue–green),(7)ifG=α,thenH=60(2+red–blue),(8)ifB=α,thenH=60(4+green–red),(9)ifH≥360,thenH=H−360,(10)ifH<0,thenH=H+360.After conversion into the HSV color space, only the V channel is managed. This is because the intensities in the V channel are more stable than the intensities in the gray image (see Fig. 3).Fuzzy logic is widely known as a powerful tool in decision making, such as in image processing [23]. The fuzzy inference system in image processing involves three stages: (i) fuzzification, (ii) modification of the membership function and (iii) defuzzification [32]. The modification of the membership values in the middle stage is a fundamental key in fuzzy image processing. In addition, many of the existing edge detection methods are based on gradients [12]. In this study, for the fuzzification stage, two gradient methods are proposed, namely the Sobel operator [42] and high pass filter [43], in order to obtain the edges of frame. These two gradients have been widely used and show promising results in edge detection in image processing [1]. The horizontal (HD) direction and vertical direction (VD) were applied to the Sobel kernel with 3×3 pixel elements as shown in Eq. (11). Next, a bidimensional convolution operation with the preprocessed image, I was calculated and lastly, norm-2 was computed to get the filtered images, S, using Eq. (12)–(14). A high pass filter, H was given as in Eq. (15) and a bidimensional convolution operation was calculated with the preprocessed image to get the filtered image, Eq. (16).(11)HD=−101−202−101,VD=121000−1−2−1,(12)ConvHD=HD*I,(13)ConvVD=VD*I,(14)S=absConvHD.^2+absConvVD.^2,(15)H=−1−2−1−212−2−1−2−1,(16)ConvH=H*I.Fuzzy sets were produced to define each variable's intensities with “Sobel” and “high pass” for input and “edge” and “non-edge” for output. A Gaussian membership function was used for the input fuzzy set while a triangular membership function was used for the output fuzzy set. After the image data were transformed to the fuzzy membership domain (fuzzification), appropriate fuzzy approaches were used to modify the membership values. According to Mondal et al. [22], fuzzy approaches can be categorized into four classes: (i) thresholding, (ii) clustering, (iii) supervised and (iv) rule-based. Among these categories, the rule-based approach is able to take advantage of the applicant's dependent heuristic knowledge and model it in the form of a fuzzy rule-based system. The fuzzy rule-based system consists of two formats: (i) if-then and (ii) min–max. The if-then rule-based format was applied because it is commonly used by researchers in image processing [13] and was suitable for this study. For the defuzzification stage, the default method proposed by Mamdani [12] was chosen. Examples of the outputs of the fuzzy logic properties for the frame detection can be found in Table 3. The process of fuzzy logic is performed as follows:Step 1: Input the pixel to fuzzy inference system and fuzzified it into different degree of Sobel and high pass filter.Step 2: Apply fuzzy rules for each input.Step 3: A centroid method is performed for defuzzification.Step 4: Edge and non-edge is extracted for the fuzzy output.Step 5: The first derivative is computed and threshold it to obtain the edge of the image.According to Borza et al. [4], in order to extract the image components, morphological processing is an effective tool to represent and describe the region shape. In addition, morphological processing involves non-linear filtering which enables noises to be filtered without preprocessing and thus the correct edge can be identified. In order to fill the gap between the edges of spectacles, the dilation operation of morphological processing [38] is applied whereby the dilation operation is a fundamental operation in morphological processing. The dilation operation can be processed as follows:(17)Idilate=dilateIdilate−1,S∩Pwhere Idilateis the area after the convergence fills the gap, S is the structuring element and P is the negative of the boundary. Examples of the outputs of the dilation operation of morphological processing for frame detection can be found in Table 3. The basic block diagram of the fuzzy inference system is shown in Fig. 4while the fuzzy logic properties for the detection of spectacles in the distant eye image are presented in Table 3.This section describes each process in the proposed intelligent iris recognition system. Section 4.1 explains the preprocessing processes which consist of reflection removal and contrast enhancement. The segmentation process of the iris is clarified in Section 4.2 while Section 4.3 explains the process of iris normalization. The process of iris feature extraction is described in Section 4.4 and lastly, Section 4.5 explains the process of template matching.For the eye image with the occlusion of spectacles, the aimed of preprocessing is to remove the reflections that exist in the glasses and increase the low intensity or contrast ratio between the iris and the pupil as both of these noises could cause the incorrect boundary localization of iris. The process of reflection removal is explained in Section 4.1.1 and the process of contrast enhancement is described in Section 4.1.2.The process of reflection removal consists of three sub-processes which are identification, classification and filling in the reflections. In the method proposed by Raffei et al. [31], the line intensity profile (LIP) and support vector machine (SVM) is used to identify and classify the reflections in the eye image. The LIP identifies the reflections using Eq. (18) as follows:(18)PGreen,Blue=255−PGreen⋅Blue,where each pixel of the green and blue intensities is subtracted from the maximum intensity value [16,31] and these intensities must be lower than the red intensity.The SVM is applied to distinguish whether the pixel is a reflection or non-reflection according to the pixel classification. A one dimensional space with two sets of vectors is used in which each vector set represents a set of reflection areas and a set of non-reflection areas. The labeling for the data would be either 1 (reflections) or −1 (non-reflections) and a radial basis function is used as a non-linear kernel function as follows:(19)K(m,n)=expγ||m−n||22,where m, n are two samples (reflections and non-reflections), and γ=−1/2. An example of the output of the reflection identification and classification is presented in Table 4. After classifying the reflections, the method proposed by Sankowski et al. [34] is applied to remove the reflections. The procedure starts with the morphological processing which involves dilation and closure operations in order to enhance the reflections. The dilation operation increases the reflection areas while the closure operation associates the neighboring reflections by filling in the discontinuity and smoothing the outer edges. Then, the intensity of the reflection areas is substituted with adjacent intensity interpolation located outside the reflections using Eq. (20).(20)Afill=∑k=14lkAk∑k4lk,where lk=1/bkfor k=1, 2, …, 4, and Akis the RGB color elements of the kth adjacent pixel. For each kth neighboring pixel, its weight, lkwas measured as equivalent to the inverse distance, bkto the pixel being inserted. Then, the inserted pixel RGB value, Afillcomponent was filled based on the pixel elements of its four neighbors. An example output for the process of reflection removal is presented in Table 4.The process of contrast enhancement begins with a calculation of the root mean square (RMS) in which the value of the RMS must be below 0.4 in order to initiate the process. Then, an iso-contrast limited adaptive histogram equalization method [29] is carried out to increase the low lighting or contrast ratio between the iris and the pupil. This method starts with partitioning the image into 8×8 sub-regions. Next, an entropy based method [21] is applied to select the clip limit parameter of amplification for each sub-region. A cumulative distribution function [8] is calculated to provide uniform distribution of the iso-luminance plane. An example of the output for the process of contrast enhancement is presented in Table 4.The segmentation stage consists of three sub-processes: (i) iris localization, (ii) eyelid occlusion, and (iii) eyelash occlusion. The iris localization process is performed to obtain the edges of the limbic and pupillary iris for which a circular Hough transform method is applied [2]. The binary edge map is then analyzed the votes on the circular Hough transform space to estimate the three parameters of one circle which is (pk, qk, r) according to Eq. (21).(21)CHTpk,qk,r=∑NCHTpt,qt,pk,qk,r,where (pt, qt) is a line pixel and CHT(pt, qt, pk, qk, r) is equivalent to 0 if (pt, qt) is not on the circle and 1 if otherwise. The maximum value of CHT(pk, qk, r) and the area of (pk, qk, r) is selected as the circular boundary parameter. This process starts with the outer boundary (iris/sclera) rather than the entire eye region because the pupil is normally within the iris region. Noises that could affect the iris recognition accuracy such as occlusions by eyelids and eyelashes in the iris area are removed. To remove the occlusion of eyelid, a linear Hough transform method based on Eq. (22) is applied.(22)R(θ)=pcosθ+qsinθ,where R is the algebraic distance between the lines, the origin is determined byθε0,π, and (p,q) is a pixel coordinate. A thresholding method based on Eq. (23) is implemented to remove the eyelash occlusions.(23)IfI(p,q)<T,the pixel is occluded by an eyelashwhere I is the intensity of a pixel and T is a threshold value set at 20 [30]. The intensity of the pixel is identified as an eyelash if it is lower than the threshold value. An example of the output for the iris segmentation is presented in Table 4.Next, a homogenous rubber sheet model is used to convert the circular dimension of the segmented iris into a rectangle dimension, with a size of 20×240 pixels, for the iris templates comparison [29–31,35]. Each point in the iris is remapped to a pair of polar coordinates (R, θ) by the model where R is between the interval [0,1] and θ is defined as angle0,2π. The normalized iris is formed as follows:(24)I(m(R,θ),n(R,θ))→I(R,θ),(25)m(R,θ)=(1−R)ma(θ),(26)n(R,θ)=(1−R)na(θ)+Rnb(θ),where I(m, n) is the iris region image, (m, n) are the actual Cartesian coordinates, (R, θ) are the standardized polar coordinates, and (ma, na) and (mb, nb) are coordinates of the pupil and iris boundaries along the θ direction. An example of the output for the iris normalization is presented in Table 4.The feature extraction process is essential in order to obtain high accuracy in an individual's identification because the unique features of each person must be extracted. A one dimensional log Gabor filter [7,29,31] is an extension of the Gabor filter which is limited to a maximum bandwidth of one octave and is not optimal for dealing with spectral information with maximal spatial localization. In this process, the one dimensional log Gabor filter based on Eq. (27) is used to get the local features of the iris.(27)LOG(T)=exp−logT/T022logσ/T02,where T is the orientation, T0 is the frequency of the center and σ is the filter bandwidth.A Hamming distance [29,31,37] is utilized to match the iris templates during the process of iris template matching and it is formulated as follows:(28)hd=1k∑i=1kPi⊕Qiwhere P and Q are the two different templates and k is the number of bits between the two templates. If the value of hd between the two irises templates is more than 0.5, the two templates are generated from a different iris or person, and vice versa.A set of 500 distant bespectacled color eye images was evaluated and analyzed to compare the performance of the proposed iris recognition system and the existing methods proposed by Cheng et al. [6], Sahmoud and Abuhaiba [33], Tan et al. [40] and Wang et al. [43]. For satisfactory comparisons, eye images from the reflections removal method developed by Raffei et al. [31] were used as input images for all methods. The evaluations were divided into two sections, namely frame detection methods (Section 5.1) and the iris recognition system (Section 5.2). To measure the performance of the proposed and existing frame detection methods, assessment of the frame detection rate and the iris segmentation accuracy in localization of the iris and the upper eyelid were completed. To analyze the performance of the iris recognition, measurements on the accuracy and decidability index were required.Tables 5–13present the outputs for the frame detection according to the frame factors, methods and distances. The frame detection rate was calculated as follows:(29)Frate=Fdetected/Fmask×100%,where Fdetectedis a frame detected by the proposed and existing methods, while Fmaskis a frame mask created manually for each eye image. The method proposed by Cheng et al. [43] which depends on the threshold of 40 to detect the frame, gave the lowest frame detection rate which was less than 92.9% for a dark color frame. In addition, due to variations in the lighting conditions, only certain areas of a frame which contained intensities of less than 40 could be detected by this method. As shown in Table 6, this method gave a frame detection rate of less than 21.2%. Due to very high lighting, some of the eye images contained intensities of more than 40; in those cases, the frame could not be detected by Cheng et al.’s [43] method. As shown in Table 5, at distances of 4m and 5m, Cheng et al.’s [43] method provided a frame detection rate of zero for those images. The adaptive thresholding method proposed by Wang et al. [43] performed considerably better than the thresholding method proposed by Cheng et al. [6]. This is due to the method of Wang et al. [43] does not depend on having the same threshold value throughout the whole image. Thus, this method could provide a frame detection rate of more than 78.3% of frame detection rate to determine the frame in dark color for different category of frame factors and distances. Although the eye images contained lighting variations, such as in Table 5 at distances of 4m and 5m, the dark frame could still be detected by this method with more than 79.8% of frame detection rate. The proposed method provided the highest frame detection rate for dark color frames in different categories of frame factors and distances, with a frame detection rate of more than 84.6%. This is due to the application of the combined gradients, the decision making of fuzzy logic and the dilation operation of morphological processing which could detect the frame in the eye images despite the lighting variations. In the category of medium color frames, the intensities of the frame are higher than the intensities of the dark color frame. Therefore, the frame detection method proposed by Cheng et al. [6] could only detect the frame in the eye image at a distance of 4m (Table 9), with a frame detection rate of 9.3%. This is due to the very low lighting in the eye image which caused the intensities of the medium color frame to have the same value as the method proposed by Cheng et al. [6]. However, the medium color frame in the other eye images (Tables 9 and 10) could not be detected by this method. The frame detection method proposed by Wang et al. [43] gave a higher frame detection rate compared to the method proposed by Cheng et al. [6], with a frame detection rate of more than 67.6%. On the other hand, the proposed method provided the highest frame detection rate of more than 90.1% for category of medium color frame. In the category of light color frames (see Tables 11 and 12), the intensities contained in the frame were almost similar or higher than the intensities of the neighboring area. Therefore, most of the frames in the eye images could not be detected by the existing methods except for the method proposed by Wang et al. [43]. In Table 12 at distances of 5–8m, the method of Wang et al. [43] gave a frame detection rate of less than 13.5%. This is because some areas in the frame had the same intensity as the adaptive threshold value, thus the frame areas could be detected by that method. On the other hand, the proposed method provided a frame detection rate of more than 89.9% for this category of frame. For the eye images with the occlusion of no-frame (Table 13), the proposed method gave the highest rate of frame detection among all methods with more than 93.5% accuracy for all distances except the distance of 8m where it gave only 67.3%. The other methods depend on a certain threshold value at which the intensity of the frame is similar to the intensity of the neighboring area while the proposed method uses combined gradients, the fuzzy logic and the dilation operation of morphological processing to detect the frame. As well as the analysis on the rate of frame detection, the outputs of the frame detection area are also presented in Tables 5–13. The proposed method detected the largest area of the frame followed by Wang et al.’s [43] method and Cheng et al.’s [6] method. It is noted that the eye image in Table 7 at a distance of 4m, was not occluded by a frame or spectacles. Therefore, no frame detection evaluation or analysis was performed for this eye image.The output images for the iris localization are presented in Tables 5–13 and the results on the accuracy of the iris localization are presented in Table 14. The method proposed by Cheng et al. [6] achieved the lowest accuracy in iris localization with 97.9% accuracy at a distance of 4m, 97.1% accuracy at a distance of 5m, 93.3% accuracy at a distance of 6m, 91.8% accuracy at a distance of 7m and 89.5% accuracy at a distance 8m. The method proposed by Wang et al. [43] obtained the second lowest accuracy in iris localization with 97.9% accuracy at distances of 4m and 5m, 96.4% accuracy at a distance of 6m, 94.8% accuracy at a distance of 7m and 94.1% accuracy at a distance of 8m. Both methods depend on threshold values and some of the eye images contained lighting variations which meant that they could not be detected by the threshold values. Thus, some of the iris boundaries in the distant eye images were incorrectly localized due to the initial center of the iris and the pupil being wrongly determined. This can be seen in the examples in Table 10 at distances of 6–8m, where the iris boundaries were falsely detected in the frame areas by Cheng et al.’s [6] method. Although the methods proposed by Cheng et al. [6] and Wang et al. [43] gave the lowest accuracy compared to the other methods applied in the analysis, they still achieved more than 90.0% accuracy in iris localization with Cheng et al.’s [6] method obtaining 91.4% accuracy and Wang et al.’s [43] method obtaining 95.7% accuracy overall. The methods proposed by Sahmoud and Abuhaiba [33] and Tan et al. [41] do not perform the process of frame detection. However, these methods gave higher accuracy in the iris localization, achieving more than 97.0% accuracy for each distance. This is because they use clustering based methods to determine the iris areas. Sahmoud and Abuhaiba's [33] method obtained 98.3% accuracy at a distance of 4m, 97.6% accuracy at a distance of 5m, 97.4% accuracy at distances of 6m and 7m, 97.1% accuracy at a distance of 8m and 97.9% accuracy overall. In contrast, Tan et al.’s [41] method provided 98.5% accuracy at a distance of 4m, 97.8% accuracy at a distance of 5m, 97.5% accuracy at distances of 6m and 7m, 97.1% accuracy at a distance of 8m and 98.0% accuracy overall. At distances of 4m and 5m, all the methods achieved more than 97.0% accuracy in iris localization due to the eye images having been captured at close range. Thus, the methods could detect the correct initial center of the iris and the pupil. In general, due to the implementation of the two gradients, the fuzzy logic and the dilation operation of morphological processing in identifying the frame in the distant eye images, the proposed method achieved the highest accuracy in iris localization among all the methods with 98.9% accuracy at distances of 4m and 5m, 97.8% accuracy at a distance of 6m, 97.3% accuracy at a distance of 7m and 97.1% accuracy at a distance of 8m.The output images for the upper eyelid localization are presented in Tables 5–13 and the accuracy of the upper eyelid localization is presented in Table 14. Most of the methods did not have any problem in localizing the upper eyelid in the distant eye images if the frame was outside the upper eyelid area such as the distant eye images in Tables 6–11. However, if the frame was in the area of the upper eyelid, the frame area was wrongly identified as an area of the upper eyelid. For example, in Table 5 at a distance of 5m, Cheng et al.’s [6] method produced a slight error in the upper eyelid localization due to the method being wrongly to detect the frame area as an area of the upper eyelid. In addition, due to the incorrect localization of the iris boundaries (such as in Table 10 at a distance of 6m), the upper eyelid was also wrongly localized by Cheng et al.’s [6] method. Due to such errors, Cheng et al.’s [6] method gave the lowest accuracy for the upper eyelid localization with 97.7% accuracy at a distance of 4m, 89.8% accuracy at a distance of 5m, 87.1% accuracy at distances of 6m and 7m, 86.9% accuracy at a distance of 8m and 88.7% accuracy overall. On the other hand, due to a high rate of frame detection, the method proposed in this study provided the highest accuracy in the upper eyelid localization with 98.5% accuracy at a distance of 4m, 98.1% accuracy at a distance of 5m, 97.7% accuracy at distances of 6–8m and 97.9% accuracy overall. Although the methods by Tan et al. [41] and Sahmoud and Abuhaiba [33] do not perform the process of frame detection, both methods produce high accuracy in upper eyelid localization due to the eyelid curvature model implemented by Tan et al. [41] and the different regions between the upper eyelid and the sclera by Sahmoud and Abuhaiba [33]. Tan et al.’s [41] method provided 98.5% accuracy at a distance of 4m, 98.0% accuracy at a distance of 5m, 97.5% accuracy at distances of 6m and 7m, 97.3% accuracy at a distance 8m and 97.7% accuracy overall. In contrast, Sahmoud and Abuhaiba's [33] method obtained 98.0% accuracy at distances of 4m and 5m, 97.3% accuracy at distances of 6m and 7m, 97.1% accuracy at a distance of 8m and 97.5% accuracy overall.The accuracy of the iris recognition by the proposed and existing methods is presented in Table 14. The method proposed by Cheng et al. [6] achieved the lowest accuracy in iris recognition among the tested methods with 91.8% accuracy at a distance of 4m, 89.9% accuracy a at distance of 5m, 85.7% accuracy at a distance of 6m, 83.3% accuracy at a distance of 7m, 80.7% accuracy at a distance of 8m and 85.3% accuracy overall. Wang et al.’s [43] method obtained the second lowest accuracy in iris recognition among the tested methods with 93.6% accuracy at a distance of 4m, 92.5% accuracy at a distance of 5m, 89.5% accuracy at a distance of 6m, 87.3% accuracy at a distance of 7m, 86.9% accuracy at a distance of 8m and 89.7% accuracy overall. These two methods depend on specific threshold values, which mean that some of the frame areas could not be detected due to the eye images containing lighting variations. In addition, due to the frame in the distant eye images containing higher values than the specific threshold values (such as for the medium and light color frames) the frames could not be detected by either of these methods. Due to the incorrect localization of the iris and the upper eyelid during the process of iris segmentation, incorrect features were also obtained during the process of feature extraction. Hence, these methods decreased the accuracy of the iris recognition. The methods proposed by Tan et al. [41] and Sahmoud and Abuhaiba [33] achieved higher accuracy than the methods by Wang et al. [43] and Cheng et al. [6]. Tan et al.’s [41] method obtaining 98.0% accuracy at a distance of 4m, 97.9% accuracy at a distance of 5m, 97.5% accuracy at a distance of 6m, 95.7% accuracy at a distance of 7m, 95.3% accuracy at a distance of 8m and 97.1% accuracy overall. On the other hand, Sahmoud and Abuhaiba's [33] method provided 97.5% accuracy at distances of 4m and 5m, 96.9% accuracy at a distance of 6m, 95.3% accuracy at distances of 7m and 8m, and 96.3% accuracy overall. Nevertheless, the method proposed in this study provided the highest accuracy in iris recognition with 98.1% accuracy at a distance of 4m, 97.9% accuracy at a distance of 5m, 97.4% accuracy at a distance of 6m, 96.7% accuracy at distances of 7m and 8m, and 97.5% accuracy overall. This is due to the high rate of frame detection in the proposed method which reduces errors during the iris feature extraction.The verification and validation of a person in the proposed iris recognition system could also be done using the index of decidability. The decidability index [28] determines the relationship distance between same person (intra-class) and different person (inter-class) distributions. A large distance distribution between the intra-class and inter-class could be obtained if decidability index is large. In addition, according to Proenca et al. [27], the largest relationship between the intra-class and inter-class is obtained with accurate segmentation. The method proposed by Cheng et al. [6] had the lowest decidability index which was an index of 2.23 at a distance of 4m, an index of 2.21 at a distance of 5m, an index of 2.12 at a distance of 6m, an index of 2.08 at a distance of 7m, an index of 2.05 at a distance 8m and an index of 2.19 overall. This was followed by Wang et al.’s [43] method which had an index of 2.29 at a distance of 4m, an index of 2.25 at a distance of 5m, an index of 2.18 at a distance of 6m, an index of 2.15 at a distance of 7m, an index of 2.13 at a distance of 8m and an index of 2.23 overall. This is because both methods gave the lowest accuracy in iris recognition among all the tested methods. Thus, the distribution between the intra-class and the inter-class in these methods was very low. The separation between the intra-class and inter-class distribution for the methods by Sahmoud and Abuhaiba [33] and Tan et al. [41] was larger than the separation in the methods by Cheng et al. [6] and Wang et al. [43]. The Sahmoud and Abuhaiba [33] method placed in the second highest index and the Tan et al. [41] method placed in the third highest index. Sahmoud and Abuhaiba's [33] method had an index of 2.39 at distances of 4m and 5m, an index of 2.37 at a distance of 6m, an index of 2.36 at distances of 7m and 8m and an index of 2.37 overall. On the other hand, Tan et al.’s [41] method had an index of 2.46 at a distance of 4m, an index of 2.43 at a distance of 5m, an index of 2.39 at a distance of 6m, an index of 2.37 at distances of 7m and 8m and an index of 2.39 overall. Instead, due to the high accuracy in the iris segmentation, the proposed method had the highest index which was an index of 2.46 at a distance of 4m, an index of 2.43 at a distance of 5m, an index of 2.41 at a distance of 6m, an index of 2.38 at distances of 7m and 8m and an index of 2.40 overall. Thus, the proposed method had the largest separation between the intra-class and inter-class.

@&#CONCLUSIONS@&#
This study proposed a frame detection method which consists of a combination of two gradients, fuzzy logic and the dilation operation of morphological processing, to solve the obstruction of the iris by the frame in distant eye images which has caused incorrect estimations of the initial center of the iris and the pupil, and also the upper eyelid localization during the process of iris segmentation. A different color space, namely the HSV color space was used to provide better results in the frame detection. The proposed method was able to identify the frames according to different frame factors in the distant eye images. The two gradients which consist of the Sobel operator and high pass filter were able to extract the information of the frames from different frame factors. The use of fuzzy logic in the proposed method provided better decision making in identifying the frames. Finally, the dilation operation of morphological processing was used to fill the gap in the edges of the detected frames. The proposed method provided higher accuracy and a higher decidability index in the iris recognition system. A combination of multimodal biometric would be required to increase the accuracy and decidability index of the iris recognition system when the iris recognition system has a high error rate due to the poor quality of the captured eye image such as a closed eye image.