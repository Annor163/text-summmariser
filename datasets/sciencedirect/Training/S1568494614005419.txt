@&#MAIN-TITLE@&#
Compensative weighted averaging aggregation operators

@&#HIGHLIGHTS@&#
Introduction of parameter based compensative aggregation operators.New approaches for learning the compensation parameter.Empirical evaluation of the proposed operators in four case-studies.Applications in the real world decision making.Comparison with existing operators is drawn.

@&#KEYPHRASES@&#
Aggregation,Compensation,Connective of fuzzy sets,Multi-criteria decision making,OWA,Induced OWA,

@&#ABSTRACT@&#
A class of compensative weighted averaging (CWA) aggregation operators, having a dedicated parameter to model compensation, is presented. The variants of CWA operator with ordered weighted averaging (OWA) operator are developed. The proposed operators are compared, both theoretically and empirically, with other operators in terms of their compensation abilities. Two approaches are proposed to learn the compensation parameter and the weight vector from the given data. The proposed learning approaches are applied in four case-studies, involving real experimental data. The usefulness of CWA operators in strategic multi-criteria decision making and supplier selection is also highlighted.

@&#INTRODUCTION@&#
The fuzzy set theory has proved to be a useful instrument to cope with the problems of uncertainty in multi-criteria decision making (MCDM), often characterized by in-exactness, ill-definedness and vagueness. MCDM problems under uncertainty typically feature a collection of objects (alternatives) evaluated against multiple attributes. These evaluations need to be aggregated in a suitable way. In this regard, a phenomenal amount of research has been witnessed for development of various aggregation operators. Recently, some aggregation operators are proposed in [12–16,30].In most of the existing operators, the quality of the optimal solution is impugned by the inability of the aggregation model to mimic the human aggregation process that inevitably demonstrates some degree of compensation. For instance, the intersection of the fuzzy sets by applying minimum, product, or another conjunctive operator implies no compensation. On the other hand, the maximum, algebraic sum or another disjunctive operator represents full compensation.In the real world, a decision is often an outcome of compensatory aggregation. It may also be argued that the compensatory tendencies in human aggregation mechanisms are the reason for the failure of the classical operators like min, max and product, in empirical investigations [1]. The nature of human decision making necessitates new operators implying a degree of compensation, lying somewhere between the minimum and the maximum of the membership degrees [5]. This would reduce the gap between theoretically and empirically obtained results.One of the first attempts in this direction were made in [2,3], in which the classical concept of connectives (like ‘and” and “or”) was generalized. Later, a few compensative aggregation approaches were proposed in [4–6]. In the same vein, ordered weighted averaging (OWA) operator was proposed in [7] to generate a parameterized family of aggregation operators including the maximum, minimum and the average. Since the appearance of OWA operator, it has been applied in various applications [17–22]. Besides, it has been extended in [23–32]. In OWA operator and its extensions, the “andness” or “orness” is controlled by modifying the weight vector. Therefore, in situations where the weight vector is fixed and a degree of compensation is also required, OWA operators cannot be applied.In the present work, we develop a class of general and compensative aggregation operators that leads to a large number of operators as their special cases. We term these operators as compensative weighted averaging (CWA) operators. The generalization in CWA operators is achieved by providing an additional adjustable parameter that may be interpreted as the grade of compensation. The parameter provides a natural control mechanism to control the softness in the “and” or hardness in the “or” operator. Besides, the weight vector is retained to indicate the importance of the arguments of aggregation.The aggregated yield in CWA operators varies in accordance with the degree of compensation. This leads to a continuum of compensative aggregation operators with the non-compensatory minimum (“and”) and the fully compensatory maximum (“or”) forming the two cornerstones of the continuum. The weighted mean forms one of the special cases of CWA operator. The parameter for compensation, combined with the weight vector, leads to a wide range of aggregation operators, with varying compensatory effects. CWA operators are combined with ordered weighted averaging (OWA) and induced OWA (IOWA) operators, resulting in compensative OWA (COWA) and compensative induced OWA (CIOWA) operators. The properties and special cases of CWA, COWA and CIOWA operators are studied in detail.We also learn the parameter for compensation from the given data to empirically evaluate the compensation abilities of CWA operators. An operational rule is developed to predict the value of compensation parameter. Besides, an approach is developed to learn both the weight vector as well as the parameter for compensation simultaneously. The aggregated values obtained with CWA operators are compared with those obtained empirically. Towards this, four experimental case-studies are conducted. The usefulness of CWA operators in managerial MCDM situations is shown through two real world case-studies. The parameter of compensation is put to use as a natural measure to indicate the attitudinal character of a decision maker (DM) in the aggregation process.The key contributions of this paper are summarized as follows:•The class of compensative weighted averaging (CWA) operators.-Generation of a range of parameters (max, min, etc.) from CWA operator.Compensative variants of OWA operator, termed as COWA and CIOWA operators.A detailed investigation of properties of CWA operators.Approaches for learning the compensation parameter and weight vector.Empirical evaluation of CWA in four experimental case-studies.-Learning of the compensation parameter and weight vector.Comparison drawn between the predicted and the actual aggregated values.Two case-studies are included to show the usefulness of CWA in MCDM.The remainder of the paper is organized as follows. Section 2 presents the class of compensative weighted averaging (CWA) operators. In Section 3, we develop approaches to learn the value of compensation parameter and the weights of the criteria from the given observations. These learning approaches are applied in four experimental studies, in Section 4. The performance accuracy of CWA operator is compared with other compensative operators in terms of closeness of match between the empirical and theoretically obtained aggregated values. Section 5 gives applications of CWA operator in the real world MCDM problems. Section 6 concludes the paper.We shall begin with the definition of CWA operator and shall examine its properties, followed by a discussion on the limits and some special cases of CWA operator. Next, the compensative ordered weighted averaging (COWA) and compensative induced ordered weighted averaging (CIOWA) operators are introduced.We consider n values that we want to aggregate using a weighting vectorw. These values are denoted as (a1, a2, …, an).Definition 2.1CWAA compensative weighted averaging (CWA) operator of dimension n is a mappingCWA:ℝ+n→ℝ+defined by arguments (a1, a2, …, an), an associated weight vectorw=(w1,w2,…,wn)such that∑i=1nwi=1andwi∈[0,1], and a parameter 0⪅λ≤∞, λ≠1. The aggregated value using CWA operator is obtained as(1)CWA(a1,a2,…,an)=logλ∑i=1nwiλaiCWA operator is characterized with the limit cases ofmini{ai}andmaxi{ai}that correspond with the extreme values of λ∈(0, 1]. Besides, the presence of adjustable parameter λ leads to a broad range of the mean operators, including the simple weighted averaging as a special case. Before investigating the special cases and the capabilities of the CWA operator in generating the various aggregation operators, we delve upon the properties of CWA operator.In the following theorems, we show that CWA operator exhibits the properties of commutativity, boundness, idempotency and monotonicity.Theorem 2.1CommutativityLet (d1, d2, …, dn) be any permutation of arguments (a1, a2, …, an). ThenCWA(a1,a2,…,an)=CWA(d1,d2,…,dn)From the definition of CWA operator in (1), we getCWA(a1,a2,…,an)=logλ∑i=1nwiλaiCWA(d1,d2,…,dn)=logλ∑i=1nwiλdiSince, the association of the weight vector with the arguments remains intact in the permutation (d1, d2, …, dn), as in the (a1, a2, …, an). Therefore,CWA(a1,a2,…,an)=logλ∑i=1nwiλai=logλ∑i=1nwiλdi=CWA(d1,d2,…,dn)□Letmaxi{ai}=Mandmini{ai}=m. Thenmini{ai}≤CWA(a1,a2,…,an)≤maxi{ai}Since,∑i=1nwi=1,CWA(a1,a2,…,an)=logλ∑i=1nwiλai≤logλ∑i=1nwiλM=logλ∑i=1nwiλai≤logλλM∑i=1nwi=logλ(λM)=MSimilarly,CWA(a1,a2,…,an)=logλ∑i=1nwiλai≥logλ∑i=1nwiλm=logλ∑i=1nwiλai≥logλλm∑i=1nwi=log(λm)=mTherefore,mini{ai}≤CWA(a1,a2,…,an)≤maxi{ai}□If ai=a, for all i. ThenCWA(a1,a2,…,an)=aProofSince, ai=a for all i and,∑i=1nwi=1, we haveCWA(a,…,a)=logλ∑i=1nwiλa=a□If ai≥di, for all i. ThenCWA(a1,a2,…,an)≥CWA(d1,d2,…,dn)ProofSince, ai≥di, for all i, we haveλai≥λdi,∀iTherefore,CWA(a1,a2,…,an)=logλ∑i=1nwiλai≥CWA(d1,d2,…,dn)=logλ∑i=1nwiλdi□Theorem 2.5Monotonicity with respect toλFor given arguments (a1, a2, …, an) and weights(w1,w2,…,wn), CWA operator is monotonic with respect to λ.Standardizing the base of log, we rewrite (1) as(2)logλ∑i=1nwiλai=log∑i=1nwiλailog(λ)In order to prove that the function in (2) is monotonic with respect to λ, we need to show that with every increase in the λ value, the rate of increase in the numeratorlog(∑i=1nwiλai), is higher than that in the denominator, log(λ). Since the log function is monotonically related to its arguments, this can be achieved by showing that the logarithmic argument in the numerator, i.e.h(λ)=∑i=1nwiλaiis an increasing function with respect to λ.The function h(λ) is continuous for all λ (0<λ≤∞) and continuously differentiable for allλ∈ℝ+,λ≠1, with derivativeh′(λ)=∑i=1nwiaiλai−1>0, which establishes that h(λ) is an increasing function. Hence, CWA operator is a monotonically increasing function, and a strictly monotonic function, with respect to λ with the exception of the case, when we have only a single argument to be aggregated. □f(λ1)=CWA(a1,a2,…,an)=logλ1∑i=1nwiλ1aif(λ2)=CWA(a1,a2,…,an)=logλ2∑i=1nwiλ2aiThen we have(3)f(λ1)=f(λ2)+(λ1−λ2)f′(λ)The value of λ helps to achieve the desired level of “andness” or “orness” in the aggregation process, naturally, and can be used to represent the attitudinal character of a DM.CWA operator provides a range of operators depending upon the values of λ andw. Here, we investigate these special cases of CWA operator. We divide these cases into two categories, one corresponding to the different values of λ and regardless ofw, and another corresponding to the values ofwand regardless of λ. We begin with the discussion of former followed by that of the latter.•When λ=0+ϵ, then for any value ofwCWA(a1,a2,…,an)→mini(a1,a2,…,an)where ϵ is infinitesimally small value.When λ=1+ϵ, then regardless of the weight vectorwCWA(a1,a2,…,an)→WA(a1,a2,…,an)where WA refers to the simple weighted averaging operatorWhen λ→∞, thenCWA(a1,a2,…,an)→maxi(a1,a2,…,an)We now look at some special cases obtained with the different choices forwin the CWA operator.•Whenwi=1/n,∀i, thenCWA(a1,a2,…,an)=logλ1n∑i=1nλai=Cμwhere Cμdenotes the compensative simple mean11Simple mean with an additional compensation parameter λ is referred to as compensative simple mean..Whenwi=1/n,∀i, and λ=1+ϵ thenCWA(a1,a2,…,an)→μwhere μ denotes simple mean.Note:- We would like to emphasize that in the class of CWA operators,∑i=1nwi=1. If all values (a1, a2, …, an) have equal weights, thenwi=1/n,∀i.We take the following examples to show the main characteristics of CWA operator.Example 2.1: Apply CWA operator to aggregate valuesa=(8, 4, 6, 10), with the weight vectorw=(0.4,0.3,0.2,0.1), and λ=10.The aggregated value through CWA operator is obtained asCWA10(8, 4, 6, 10)=log10((0.4×(10)8)+(0.3×(10)4)+(0.2×(10)6)+(0.1×(10)10))=9.0171It can be observed that the aggregated value lies between the minimum (4) and the maximum (10) values.Example 2.2: We compare the results for Example 2.1 by taking λ=10−29, λ=1.00001 and λ=1029.CWA10−29(8,4,6,10)=4.0180;CWA1.00001(8,4,6,10)=6.6000;CWA1029(8,4,6,10)=9.9655It is found that at λ=1.00001, the aggregated value obtained with CWA operator is 6.6000 that is equal to the weighted mean. And, at λ=10−29 and λ=1029, we are getting the net aggregated values as 4.0180 and 9.9655, respectively, which are quite close to the minimum and maximum of the values to be aggregated. This shows that values of λ∈(0, ∞] generates a continnum of monotonically increasing aggregated values. At the extreme values of λ=0+ϵ and λ=∞, the minimum and maximum operators are obtained, analogous to “and” and “or” operators. The “andness” and “orness” can be modulated by choice of λ. At λ=1+ϵ, weighted mean is obtained.OWA (ordered weighted averaging) operator provides a parameterized family of aggregation operators, which includes the minimum, the mean and the maximum. Combining OWA operator with the proposed CWA operator, we introduce the compensative ordered weighted averaging (COWA) and compensative induced ordered weighted averaging (CIOWA) operators.Definition 2.2COWAA compensative ordered weighted averaging (COWA) operator of dimension n is a special case of CWA operator defined as(4)COWA(a1,a2,…,an)=logλ∑i=1nwiλbiwhere (b1, b2, …, bn) are reordered (a1, a2, …, an) from largest to smallest. That is, bjis the jth largest ai.Definition 2.3CIOWAA compensative induced ordered weighted averaging (CIOWA) operator of dimension n is a special case of COWA operator, characterized with an additional set of valuesu=(u1, u2, …, un), the order inducing variables, for reordering the arguments (a1, a2, …, an). The aggregated value is obtained as(5)CIOWA(a1,a2,…,an)=logλ∑i=1nwiλbiwhere (b1, b2, …, bn) are reordered (a1, a2, …, an) as per the decreasing values for (u1, u2, …, un).On this note, a distinction that may be of importance to note between CWA and OWA operators is that in OWA operator, the range of means is provided as a result of the weight vector. For example, in the case of OWA operator, the extreme points min{ai} or max{ai}, i=1, …, n, are obtained whenw=0,…,1, orw=1,…,0, respectively. Hence, the element of compensation in weighted aggregation of arguments is absent in OWA operator. In contrast, CWA operator gives a DM a wide choice to choose the degree of compensation from the interval (0, ∞], while also allowing him to retain the inherent weights of the different criteria, at the same time. Also, the extremal values of min{ai} or max{ai} are obtained in CWA operator, naturally, at the endpoints of the interval (0, ∞], regardless of the weight vector, something which is unique to the proposed CWA operator.This section is devoted to approaches for learning the compensation parameter. Here, we focus on the issue of acquisition of λ, and develop an operational definition for learning the same. We first develop an algorithm which takes as input data consisting of tuples of individual scores (evaluations in MCDM, feature values in machine learning) along with their aggregated value, and predict the overall degree of compensation. Next, we shall also examine the issue of simultaneous learning of the degree of compensation as well as the weights of the arguments of aggregation.Given the memberships values in individual concepts along with the membership value in the intersection of the fuzzy concepts, the idea here is to learn the degree of compensation that would have been in the mind of the onlooker while arriving at the aggregated membership component. The existing mean operators have often proved inadequate to model the human aggregation of fuzzy sets that are, more often than not, characterized with a compensatory effect. One of the reasons for this, may be, is the fact that the aggregation operation in MCDM is often subjective depending upon the context and the problem that exist only in the mind of the DM.The proposed algorithm makes an attempt to determine the degree of compensation which would have been at play in the mind of the DM in arriving at the aggregated membership. Towards this, we consider a practical situation in which we are given with a collection of m samples (observations), each comprising of a n-tuple of valuesak=(ak1, ak2, …, akn), called the arguments of aggregation, an associated weight vectorw=(wk1,wk2,…,wkn), and the aggregated value (corresponding to the arguments of aggregation), denoted as dk, k=1, …, m. In a typical MCDM situation, the arguments (ak1, ak2, …, akn) to be aggregated are the evaluation scores given by an expert for an alternative, sayakagainst a set of n criteria. The arguments (ak1, ak2, …, akn) are aggregated by the DM to arrive at a score for alternativeak, which is used to decide the best alternative.Our goal, here, is to learn the degree of compensation λ that models the aggregation process through CWA operator such that for the entire collection of the values in the dataset, the following condition is satisfied to the best possible extent.(6)CWAλ(ak1,ak2,…,akn)=dk,for any k=1, …, m.The central principle of our algorithm is to minimize the mean square instantaneous error, ekcorresponding to alternative ak. The error ekis the difference between the estimate (computed by CWA operator) and the actual aggregated value dkfor ak. It is given as(7)ek=12(ak1w1+ak2w2+⋯+aknwn−dk)2Since,(ak1w1+ak2w2+⋯+aknwn)is computed by CWA operator, our learning problem is essentially(8)minimize:ek=12logλ∑i=1nwiλaki−dk2In fact, the above minimization problem is a constrained optimization problem, since λ has to satisfy the following properties:1)λ>0λ≠1We use the gradient descent technique to solve this learning problem. Using the gradient descent method, the following learning rule is obtained for updating the parameter λ:(9)λ(I+1)=λ(I)−β∂ek∂λλ=λ(I)where β denotes the learning rate (0≤β≤1), and λ(I) and λ(I+1) denote the values of λ at ith and (i+1)th iterations, respectively.The partial derivative ∂ek/∂λ is given as(10)∂ek∂λ=logλ∑i=1nwiλaki−dk1log(λ)∑i=1nwiakiλaki−1∑i=1nwiλaki−log(∑i=1nwiλaki)λ(log(λ))2wherelogλ(∑i=1nwiλai)gives the current estimate of the aggregated value dk. We denote this value asdˆk, for notational simplicity. With this, (10) can be simplified as(11)∂ek∂λ=1λlog(λ)(dˆk−dk)∑i=1nwiakiλaki−dˆk−dˆkIn light of (11), the final form of our learning rule can be obtained as(12)λ(I+1)=λ(I)−βλ(I)log(λ(I))(dˆk−dk)∑i=1nwiakiλ(I)aki−dˆk−dˆkNow, we formalize an algorithm for learning the degree of compensation.1)We begin with choosing a reasonable initial point for λ. At any given moment, we have the current estimate of λ, that is λ(I).With every new observation consisting of the arguments(ak1w1,ak2w2,…,aknwn)and dk, we use the current estimate of λ to compute the estimated aggregated value as(13)dˆk=logλ(I)∑i=1nwi(λ(I))aiUsing the current estimate ofdˆk, obtained in (13), we now update our estimates of λ in accordance with (12).The above steps are performed iteratively to obtain the minimizer λ* for the error function in (7), at which there is no change in λ, which happens when ∂ek/∂λ=0.An important issue associated with the problem of modelling the aggregation process is to find the weight vector. Here, we present an algorithm to learn the weight vector that would have been in the mind of a DM while aggregating the individual values. As above in Section 3.1, the tuples of criteria-wise evaluations and the aggregated value form the input data to such an algorithm. The problem of determining the weight vector can be formalized as(14)(ak1+ak2+…+akn)=dkfor any k=1, …, m. In order to learn the weights of CWA operator, we minimize the error as given in (7). Our learning rule (for weights of CWA operator) can be given as(15)minimize:ek=12logλ∑i=1nwiλaki−dk2s.t.∑i=1nwi=1wi∈[0,1],i=(1,…,n).Using the gradient descent method, the weights can be updated by applying the following rule:(16)wi(I+1)=wi(I)−β∂ek∂wiwi=wi(I),wherewi(I)andwi(I+1)denote the weight corresponding to the ith feature or criterion at Ith and (I+1)th iterations, respectively.Taking partial derivative of ekw.r.twi, we obtain(17)∂ek∂wi=logλ∑i=1nwiλaki−dk1log(λ)λaki∑i=1nwiλaki⇒∂ek∂wi=1log(λ)(dˆk−dk)(λaki−dˆk)The final form of the learning rule is obtained by replacing (17) in (16), shown as(18)wi(I+1)=wi(I)−βlog(λ)(dˆk−dk)(λaki−dˆk)The algorithm for learning the weights for various criteria from a set of observations in the form(ak1w1,ak2w2,…,aknwn)and dkcan be formalized by fitting CWA operator, in the following steps.1)As in the conventional gradient descent method, a reasonable initial point is chosen forw. At any given moment, we have the current estimate ofw, denoted asw(I)=(w1(I),w2(I),…,wn(I)).we use the current estimate ofwto compute the estimated aggregated value as(19)dˆk=logλ∑i=1nwi(I)λaiThe estimates ofware now updated in accordance with (18) by using the value ofdˆk, obtained in (19).The above steps are performed iteratively to obtain the minimizerw*for the error function in (7), at which there is no change inw, which happens when∂ek/∂wi=0.We study the compensative qualities and usefulness of the proposed aggregation operator in modelling the real world aggregation applications.We consider a real world dataset from the experiments reported in [3–6]. This data has been collected from an experiment in which a set of students were asked to give their judgments, on a scale of 0 to 1, for two stochastically independent attributes of tiles (‘good solidity’ and ‘good dovetailing’), along with that for the ideal tile (which is the empirically obtained aggregated value of the memberships of the two attributes). Here, ‘good solidity’ and ‘good dovetailing’ could be visualized as fuzzy concepts. The respective judgments by kth student for these fuzzy concepts in the interval [0, 1], are denoted by (sk) and (tk), respectively. The weights of the two attributes are taken to be equal, as in the original experiment.In order to determine the closeness of match between the empirically obtained and “theoretically” obtained aggregated values, we perform the following steps.1)We find the optimal values of the compensation parameter λ by applying the algorithm as proposed in Section 3.1. This optimality is achieved by minimizing the sum of squared errors, ek, which is formulated in (7). The operational rule can be given as(20)minimizeE=∑k=1Nek=12∑k=1N(CWA(ak1,ak2,…+akn)−dk)2where N refers to the number of samples. In the present case N=24.The optimal value λ* is obtained as 0.1377 with Emin=0.086.Next, we perform the aggregation of the membership grade values, skand tkfor each sample k through CWA operator, by using the learnt value of λ*=0.1377.The computed predictions using CWA operator are then compared with the corresponding empirical values for the ideal tile, denoted by dk. For comparison, we also report the results obtained using the existing compensative operators and approaches.The sum of squared errors value is computed for each of the methods.It can be observed from Table 1 that the proposed CWA operator is quite competitive with the existing operators, in terms of closeness of match between theoretically and empirically obtained aggregated values. At λ=0+ϵ, CWA operator gets reduced to conventional ‘and’ operator, when it yields the minimum value of the arguments of aggregation as the aggregated value. However, in samples 1, 8, 19 and 20, the empirical aggregated value (as reported by human subjects) is even lesser than the minimum of the arguments of aggregation. These samples contribute the maximum least square error, as aggregated output in these cases ranges from minimum to maximum values.Here, we take up an example in [8], where the data is about a collection of samples with four arguments (can be seen as membership grades in different fuzzy sets) each, along with the aggregated value dk. The arguments are represented asFik,i=1,…,4. We fit the proposed CWA operator in the given data, and apply the algorithm proposed in Section 3.2 to learn the compensation parameter λ* along with the weight vectorw*. The values for λ* andw*are obtained as λ*=1.5941×10−5 andw*=(0.2171,0.1745,0.3209,0.2875)with Emin=6.6969×10−6. The learnt values of λ* andw*are used to obtain the aggregated valuesdˆkby applying (1). The results are shown in Table 2. The plot of observed versus computed grades of membership is given in Fig. 2.The difference of squared errors in Table 2 is quite close to 0, which shows the efficiency of the proposed approaches to learn λ andw. Needless to say that since, we are learning both λ andw, the learning process is more flexible in Case-study 2, leading to a much smaller difference of squared errors than that obtained in Case-study 1.Here, we take the data as generated in an experiment, reported in [1]. The experiment was done in two stages. In the first, a set of 60 students were shown the names of 20 objects and asked for their judgments (on a scale of 0–1) about the degree to which an object conforms to a “metallic container”. In the second stage of the experiment, the same subjects were asked for their judgments regarding the grades to which they consider an object to be “metallic object” and “container”. The weights of the two attributes, “metallic object” and “container” were taken to be equal. For more details about the experiment, we refer the reader to [1].We fit CWA operator with the given empirical data. The value of the compensation parameter obtained is 0.00001 and the sum of squared errors is observed as 0.084. The aggregated values obtained through CWA operator and their comparison with the empirically obtained values are reported in Table 3and Fig. 3.This experiment shows that CWA operator could be fitted to any set of empirical observations. The presence of weight vector and adjustable parameter λ helps CWA operator to adapt well to the given data. A small difference of squared errors (=0.084) in Table 3 with learnt value of λ=0.00001 empirically validates this premise.In order to illustrate the capabilities of CWA operator in representing any empirically obtained aggregated value, we perform an exercise taking the data of Case-study 1, in which we predict, using the algorithm proposed in Section 3.1, the individual values of λ corresponding to each of the 24 tuples. We observe that in most of the cases, quite a close match could be obtained between the theoretical and empirical results of the aggregated output. For the samples 1, 8, 19 and 20, the deviation between the empirical and theoretical output is obvious due to the fact that the empirical value is lesser than the permissible extreme <min{ai}. The results (values of λ and the obtained aggregated output for 24 tuples) are shown in Table 4. The comparison between the theoretically and empirically obtained outputs is shown through a plot of empirical vs. predicted outputs, given in Fig. 4.We observe from Table 4 that almost a perfect match is obtained between empirical and theoretical aggregated values with individual learning of λ (except samples 1, 8, 19 and 20). This shows that the generated continuum of aggregated values is inclusive of all possible aggregated values. Once, the learning algorithm is fed as input the values to be aggregated, and the empirically aggregated value, it accurately fits CWA operator with the data, learn λ and predicts the aggregated valuedˆk.For samples 1, 8, 19 and 20, as we clarified earlier, the empirically obtained aggregated value is even lesser than the minimum of the arguments of aggregation. This uncommon situation is beyond representation by any mathematical aggregation operator, and in these cases, some difference in empirical and predicted aggregated values is observed.

@&#CONCLUSIONS@&#
