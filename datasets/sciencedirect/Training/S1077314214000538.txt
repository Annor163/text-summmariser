@&#MAIN-TITLE@&#
Statistical quantization for similarity search

@&#HIGHLIGHTS@&#
Formulate a k-means hashing model based on generalized likelihood ratio analysis.Introduce statistical analysis into the out-of-sample extension of quantization.Extend a more generalized observation for the product quantization series.

@&#KEYPHRASES@&#
Machine learning,Computer vision,Similarity search,Hashing,Quantization,Binary code,

@&#ABSTRACT@&#
Approximate nearest neighbor search has attracted much attention recently, which allows for fast query with a predictable sacrifice in search quality. Among the related works, k-means quantizers are possibly the most adaptive methods, and have shown the superiority on search accuracy than the others. However, a common problem shared by the traditional quantizers is that during the out-of-sample extension process, the naive strategy considers only the similarities in Euclidean space without taking into account the statistical and geometrical properties of the data. To cope with this problem, in this paper a novel approach is proposed by formulating a generalized likelihood ratio analysis. In particular, the proposed method takes a physically meaningful discrimination on the affiliations of the new samples with respect to the obtained Voronoi cells. This discrimination essentially imposes the measure of statistical consistency on out-of-sample extension. The experimental studies on two large data sets show that the proposed method is more effective than the benchmark algorithms.

@&#INTRODUCTION@&#
With the explosive data growth on the web, fast similarity indexing and search are considered to be one of the most fundamental problems for the multimedia communities [1–3]. This problem is also known as Nearest Neighbor (NN) search, which is defined as accurately finding the close samples for a given query within a large database [4,5]. It is of great importance to a wide range of multimedia applications, such as content-based image/video retrieval [6–8], image/video auto-tagging [9–11], image classification [12,13], and scene recognition [14]. Naively searching for the neighbors according to their similarities entails exhaustively comparing the queries with the examples over the entire database. This strategy has linear complexity with respect to the scale of the database, which is infeasible on ever larger databases. Besides, to achieve satisfying performance on such databases, most of the related applications have to rely on the high-dimensional or structured representations, as well as the computationally considerable distance functions [15,16]. Therefore, the naive strategy is prohibitively expensive in practical situations.To make similarity indexing and search scalable, several Approximate Nearest Neighbor (ANN) techniques have been developed, through which a fast query is allowed with the predictable sacrifice in accuracy [17,18]. Instead of performing a completely NN search on a databaseXthrough linear scanning withO(|X|)query time, ANN techniques are desired to achieve the fast yet accurate indexing with sublinearo(|X|)[19,20], logarithmicO(log|X|)[21,22], or even with constantO(1)complexity. Among these techniques, hashing-based ones have attracted more attention and announced laudable performances recently. This type of ANN techniques is preferable for its constant query time and the substantially reduced memory [23]. In this work, we focus on the later aspect that relies on generating compact binary codes for the high dimensional samples in a large database while maintaining the structure of the original database. By constraining the similar data points with close binary codes, similarity search is then accelerated by finding the neighbors within a small Hamming distance from the query.Broadly, researches toward hashing can be divided into two main categories: Hamming-based and lookup-based. Both these two categories involve a quantization process, through which the original feature space is partitioned into some unique cells, and a corresponding strategy for distance computation. For the Hamming-based methods, the quantization is achieved by using hyperplanes [24,25] or kernel hyperplanes [26,27]. These hyperplanes are generally determined by the signs of the employed hashing functions. Each hyperplane is then used to encode a unique bit of the desired compact code. In regard of hashing function, several strategies have been developed to cope with the practical scenarios. For instance, in [28] the post-combination strategy is employed on the linear hash functions of different types of features for content-based image retrieval. In [8] a pre-concatenation strategy is proposed for contend-based video retrieval, which equally concatenates all the employed features as one and then constructs the hashing function. Differently, in [29,30], multiple features are non-linearly concatenated and then projected using linearly combined multiple kernel hyperplanes. As for the lookup-based methods [31–33], they usually partition the feature space through k-means clustering. Such a quantization is considered to be more adaptive than those on the basis of hyperplane construction, and is likely to be more accurate with the same code-length [34].Though the lookup-based methods have shown success in many large-scale searching scenarios, there is a problem seldom exploited. Given b bits for quantization, k-means quantizers are implemented by mapping the original descriptions to the codebook containing at most2bcodewords [31–33]. Specifically, the classical k-means quantizers use the cluster centers as codewords, and then assign to any data point a nearest codeword according to the distance measure in Euclidean space. However, in spite of minimal quantization error during off-line training [15,32], assigning a new sample the cluster index through such strategy lacks statistical interpretability, and may fail in many practical cases. For example, as shown in Fig. 1, the new sample x will be assigned the cluster indexc2if only the Euclidean distances to the cluster centers are consulted, but it may have the property more similar to those contained in the clusterc1.In order to cope with the above problem, in this paper a novel quantizer is proposed for effective similarity search. This method improves the classical k-means quantization by taking into account the statistical consistency of a new sample with respect to each partitioned cell. Our purpose is achieved by formulating a Generalized Likelihood Ratio (GLR) analysis, through which each sample can be identified as an inlier or not in the examined cell. We claim that the proposed method is physically meaningful and practically preferable in the out-of-sample extension process.The rest of this paper is organized as follows: Section 2 gives a brief review on the background of quantization methods. Section 3 presents the detailed formulation of the proposed method. Then, the experimental comparison is conducted and analyzed in Section 4 to verify the effectiveness of the proposed method. Finally, the conclusion is given in Section 5 to summarize this paper.Quantization has been the topic of prolonged and extensive study, and has a large body of literatures in information theory [31,35]. Its purpose is to provide a low-cardinal representation space to a database, which can facilitate further processing especially when the tasks suffer from the curse of dimensionality. This section starts with the presentation for the classical concept of quantizing the feature space in a k-means fashion. Then, a brief review on the generalization of this quantization strategy to product space is presented, which improves the practicability of the quantizer when the bit number is large.For the classical vector quantization, the quantizer is a functionq(·)mapping a m-dimensional vectorx∈Rmto another vector [31], such that(1)q(x)∈C={ci|ci∈Rm,i∈I},where the setCis the codebook of sizek,ciis the codeword usually given by the k-means centers [15,32], andI={0,1,…,k-1}. Each set of vectors mapped to the same codewordciis referred to as a unique Voronoi cellVi, which is defined as(2)Vi={x|x∈Rm,q(x)=ci}.Then, the k cells together characterize the partition that the quantizier induces on the input spaceRm. The relationship of these concepts is illustrated in Fig. 2.By definition, each input vector will be represented by the assigned codeword. The quality of a given quantizer is usually measured in term of the averaged distortion between the original vector x and the mappingq(x)[15,31],(3)D(q)=Ex[d(x,q(x))],where the distortion measured(x,y)can take various specific forms, and is typically the Euclidean distance between x and y[15,32]. Then, applying the triangle inequality for (3) leads to(4)Ex[|d(x,x′)-d(x,q(x′))|]⩽D(q).This indicates an upper bound on the expected error for estimating the inter-sample distances, when one sample in a pair is approximated by its quantization result. Therefore, a quantizer that minimizesD(q)for a given codebook of size k can claim its effectiveness for NN search within the database.In order for a quantizer to be optimal subject to the underlying probability distribution, it has to satisfy the following two properties:•q(x)={ci|d(x,ci)⩽d(x,cj),∀j∈I};ci=argminx′Ex[d(x,x′)|x,x′∈Vi].The first property regularizes that the quantization cells consist of samples no further from its centroid than from any other codewords. The second condition is that the codeword for a given cell must be the expectation of the data points lying in the cell. These two properties are also known as the Lloyd optimality conditions [36], which contribute to the theoretical basis of k-means.Given b bits for vector quantization, the classical k-means quantizers have to contain2bcodewords. For a large b (e.g.,b=64such thatk=264), it is not practical to directly use the Lloyd optimality conditions due to the expensive computational cost. Actually, it is even impossible to load thek×mfloating-point values into memory. Towards this issue, the product quantization method is an efficient solution, of which the problem is reduced to be the optimization in a set of independent subspaces [15,32].In the case that the probability distribution of x can be considered to be independent in its components, the product quantization method forms the Cartesian product ofRmyielding L subspaces. That is, splitting each vectorx∈Rminto L distinct subvectors:x=[xˆ1,…,xˆl,…,xˆL], wherexˆldenotes thelthsubvector of dimensionmˆ=m/L. For each subspace, a low-complexity sub-quantizer is obtained independently by minimizing the expected distortion for the corresponding subvectors [15,32]. To be specific, thelthquantizerqlassociated with thelthsubspace is optimized by minimizing(5)D(ql)=Exˆl[d(xˆl,ql(xˆl))].A sub-quantizerqlcontains a sub-codebookCˆlwithkˆsub-codewords. Any codeword c of the product quantizer is constructed as a concatenation of L sub-codewords drawn from L sub-codebooks. Correspondingly, the final codebook is also defined as the Cartesian product(6)C=C1×⋯×Cl⋯×CL.In such a way, the total number of distinct codewords inRmreachesk=(kˆ)L. This property makes a product quantizer to be powerful to produce a large set of codewords from several subsets. But the algorithm only needs to calculate and storekˆ×L×mˆ=kˆ×mfloating-point values, instead of the original amountk×m. Thus, the product quantizer turns out to be efficient sincekˆcan be much less than k.In the classical k-means quantizers, the binary codes assigned to the training samples are determined according to their distances from the codewords in Euclidean space. Then, the same criterion is also applied to tackle the out-of-sample extension problem [15,31–33]. This strategy implies an assumption that including the new samples will not introduce any distortion on the statistical and geometrical properties of the original database. However, this assumption does not hold for many practical cases, particularly the ones where the statistics vary greatly among different clusters.Toward this problem, in this paper the out-of-sample extension process is formulated by a physically meaningful GLR analysis, through which the statistical consistency of a sample with respect to each partitioned cell is analyzed to determine its affiliation.Considering an undesirable signalθ∈Rm, which will lead to deviations from the distribution of an examined cellV, the two hypotheses to be distinguished for a new sample are given by(7)H0:x=z,H1:x=z+sθ.The hypothesisH1represents the presence of an undesirable signal with strength s, andH0indicates the opposite case.z∈Rmis a vector representation of residual “background” or “clutter” signal fitting the desired distribution. Thus, for a data set X including the examined cellVand the new samplex,X={V∪x}, the two hypothesis are formulated as(8)H0:X=Z,H1:X=Z+STθ,whereS=[s1,s2,…,sN]is the strength vector, which also reflects the spatial position ofθacross the data set of size N.Without loss of generality, we impose a mean removal process on Z. Then the residual clutter could be assumed to be multivariate Gaussian and independent from sample to sample according to [37]. As a consequence, X should be also Gaussian distributed. Then we have(9)X∼N(0,INΣ),H0,N(STθ,INΣ),H1.where(10)Σ=E{[x-E(x)]T[x-E(x)]},is the covariance of x, andINdenotes theN×Nidentity matrix.By definition [38], the likelihood function for theH1case depending onθandΣis in the form of(11)L(θ,Σ)=[(2π)m|Σ|]-N/2·exp-12∑i[(x-E(x))Σ-1(x-E(x))T]=[(2π)m|Σ|]-N/2·exp-12tr[(X-STθ)Σ-1(X-STθ)T].Also, a similar expression gives the functionL(0,Σ)for theH0hypothesis.Considering the statistical consistency between the new sample and each partitioned cell, the proposed method aims at constructing a physically meaningful and practically preferable criteria for the out-of-sample extension process. After introducing the formulation of the problem and the definition of the likelihood functions, we now details the GLR analysis to determine the acceptability of a new sample as an inlier in a data set. Given the likelihood functions for the alternative hypotheses, the GLR is calculated by(12)Λ(x)=maxθ,ΣL(θ,Σ)maxΣL(0,Σ).For this definition, a larger value ofΛ(x)indicates the high possibility of the new sample x being inconsistent with respect to the examined cell.According to the well-known Maximum Likelihood Estimator (MLE) [38], the two items in (12) for the unknown parametersθandΣare given by(13)maxθ,ΣL(θ,Σ)=[(2π)m|Σθ|]-N/2exp(-mN/2),(14)maxΣL(0,Σ)=[(2π)m|Σ0|]-N/2exp(-mN/2),where the covariance matrixes under the difference hypothesesH0andH1are respectively calculated as(15)Σ0=1NXTX,and(16)Σθ=1N(X-STθˆ)T(X-STθˆ),with the approximation of the undesirable signalθˆgiven by(17)θˆ=STXSTS.Thus, after applying the maximum likelihood estimation results on the definition of GLR, (12) can be further translated to(18)Λ(x)=|Σ0|m/2|Σθ|m/2∝|XTX||XTX-(STX)T(STS)-1(STX)|∝(STX)(XTX)-1(STX)TSTS.It should be noted that the vector S essentially reflects the spatial position of the new sample in the constructed data set, i.e.,S=[0,…,0,1,0,…,0]in practice. Besides, the mean of the data set has to be removed to match the zero-mean assumption on the hypothesisH0. These yield(19)Λ(x)=(x-μX)ΣX-1(x-μX)T,whereμXandΣXare the sample mean and covariance of X, respectively.Once the statistical consistencies of the new samples with respect to the partitioned cells have been obtained, the out-of-sample extension process can be accomplished by assigning each new sample to the cell associated with the corresponding minimum GLR.

@&#CONCLUSIONS@&#
