@&#MAIN-TITLE@&#
Determining shape and motion from non-overlapping multi-camera rig: A direct approach using normal flows

@&#HIGHLIGHTS@&#
We determine camera motion and shape directly from normal flows.We do not use the current arts such as optical flows and feature correspondences.We use a multi-camera rig without overlapping visual fields.Translation and rotation components of motion can be estimated separately.Our performance is similar to the current arts but a faster computation time.

@&#KEYPHRASES@&#
Camera motion,Normal flow,Non-overlapping multiple cameras,Optical flow,Spherical camera,

@&#ABSTRACT@&#
In this paper, we explore how a wide field-of-view imaging system that consists of a number of cameras in a network arranged to approximate a spherical eye can reduce the complexity of estimating camera motion. Depth map of the imaged scene can be reconstructed once the camera motion is there. We present a direct method to recover camera motion from video data, which neither requires establishment of feature correspondences nor recovery of optical flow, but from normal flow which is directly observable. With a wide visual field, the inherent ambiguities between translation and rotation disappear. Several subsets of normal flow pairs and triplets can be utilized to constraint the directions of translation and rotation separately. The intersection of solution spaces arising from normal flow pairs or triplets yields the estimate on the direction of motion. In addition, the larger number of normal flow measurements so resulted can be used to combat the local flow extraction error. Rotational magnitude is recovered in a subsequent stage. This article details how motion recovery can be improved with the use of such an approximate spherical imaging system. Experimental results on synthetic and real image data are provided. The results show that the accuracy of motion estimation is comparable to those of the state-of-the-art methods that require to use explicit feature correspondences or full optical flows, and our method has a much faster computational speed.

@&#INTRODUCTION@&#
A classical and important research area in computer vision is to determine the relative motion in space between an observer and the surrounding environment from video data alone. A good solution to it has many applications including 3D reconstruction, visual control, tracking, augmented reality, and more. In the literature, the existing works on motion determination are largely about establishing explicit correspondences across images. This is partially due to the rich development of various variational methods to recover optical flow (also known as full flow) such as TV-L1[1], and feature descriptors such as SIFT [2] for matching of feature correspondences. The optical flow methods can be used for monocular camera [3–5], multiple cameras [6–9], and spherical camera [10,11]. The feature-based method which is generally without assuming small camera motion can also be used for various vision systems such as monocular camera [12–14], multiple cameras [15–17], and spherical camera [18]. The successes of all the above schemes assume the availability of abundant computational resource.Starting from the seminal works from Horn and Schunck [19], and Lucas and Kanade [20], various methods have been developed to recover optical flow. The major challenge lies on the fact that the optical flow, which is induced by the spatial motion at any image position, is only partially observable in general due to the familiar aperture problem. Only the component of the optical flow along or opposite to the direction of the local intensity gradient, termed normal flow, is directly measurable from the spatial–temporal image derivatives. The partial observability of the flow is what makes full flow computation and in turn motion determination a challenge. An assumption such as local flow smoothness is generally required to recover optical flow. On the other hand, the feature-based scheme requires tracking of distinct features across the images, which might not be always present in the video. Ambiguity in establishing correspondences across multiple images exists if the imaged scene contains repetitive patterns. This in turn affects the accuracy in determining camera motion.A few methods have been proposed to determine camera motion from normal flows directly without prior computation of correspondences. They are collectively known as the direct methods in the literature. Such methods originated from visual displacement of brightness in image data. Unlike optical flow, normal flow can be obtained directly from image data, without involving minimization of certain cost-functional, which is often computationally demanding. A good compromise between the data term and the regularization term in the functional is often essential to recover accurate optical flow field [19,21,1]. A huge amount of computation time is required for the processing if no Graphical Processing Unit (GPU) is available. The optical flow estimation used in the work [22] by Xu et al. is very promising but it required 420s for a 640×480 image pair on a laptop computer with i7 CPU. In contrast, the direct method to be described in this article was implemented in merely Matlab (a high level programming language), and still it only required less than 5.3s for four 640×360 image pairs from a multi-camera rig executing on a rather low-performance computer that has only Pentium D CPU. This computation time has already included the processing to recover normal flow field as well. Furthermore, there are ways (using modern CMOS vision sensor [23]) to measure normal flows directly from an electronic chip. This gives additional support on the use of normal flow to determine camera motion directly.Even though each normal flow data point represents only partial information, since the total number of image positions where normal flow is observable generally far exceeds the number of motion parameters, one would expect that motion can be recovered directly from normal flow without imposing additional assumptions like local smoothness onto the image flow field. In this work, we make an effort not to use the smoothness assumption which is invalid at image positions where the depth of the imaged scene is discontinuous. This paper addresses the question of whether camera motion determination can be practically achieved using methods other than the conventional ones that are based on either optical flow or feature correspondences. It shows that the direct methods proposed 3 decades ago can still be considered. We fill the research gap by showing that not only can normal flow be used to determine camera motion in similar quality as the state-of-the-art’s, their use can also outperform others in several circumstances.Standard camera usually has limited field of view (FoV). The video perceived under say a pure translation (of the camera) in the x-direction would be similar to that under a pure left-hand rotation about the y-axis, where the x and y axes are the orthogonal coordinate axes of the image domain. In other words, there would be ambiguity in distinguishing or recovering motion from the video information. If the camera has only limited image resolution and video sampling rate, which is inevitable in practice and which results in error in extracting local flow, the ambiguity becomes inaccuracy in motion recovery. Remedies to the above two issues include using denser sampling rate and wider FoV imaging system. Denser sampling rate could help in determining the flow more precisely at each image position where intensity gradient is present. However, if the scene is not densely textured, which is often the case in practice, the number of feature positions in the image data where flow can be estimated is the same, and there would be a limit of how much of the stated issue is addressed.Many innovative engineering ideas are inspired by the biological world. There is no exception in computer vision. Flying insects are good pilots even though they have rather simple brains compared with those of the birds. They execute navigational tasks accurately, relying on the visual clue from optical flows [24]. Whether it is a bird or a flying insect, the excellence of their navigation relies upon the use of a wide FoV vision system. Due to the advance of technology, digital camera with high quality lens becomes more affordable and its size is continuously decreasing. Camera system which uses a wide FoV camera [10,11], and spherical imaging system constructed from several narrow FoV cameras [17,7,15,16], draw researchers’ attention in recent years. Although the proposal of using wide FoV camera was introduced as early as in the 1980s such as [25], real-image experiments have become practically feasible only in recent years. We pursue the use of a wide FoV imaging system to determine the self-motion.We present a new direct method that uses a certain approximation of the spherical eye to recover motion parameters. We provide more details about our recent research work [26], and extend the direction constraint by considering the use of normal flow triplets. The approximate spherical eye comprises a number of cameras that have optical centers placed near one another without necessarily having overlapped FoVs. The advantage of having widened visual view is fivefold. First, it addresses the motion ambiguity issue that troubles a moving imaging system that has narrow FoV. Second, the directions of the translation and rotation components of the motion can be recovered separately in an efficient manner. As a result, no error propagates from the direction of translation to the direction of rotation, and vice versa. Third, tighter constraints on the motion solution (two pairs of specific normal flows are enough to reduce the possible motion solution by34of a 4D parameter space) are available to improve both the result and the computational speed. Fourth, large visual field could make motion estimation more accurate [25,27,28]. Fifth, more normal flow data points due to the widened visual field would be available to combat flow extraction error.The contribution of this work is to provide a mechanism which separately estimates the directions of translation and rotation components from general motion through the use of several particular subsets of normal flows available on the approximate spherical imaging system. This approach allows determining the direction of motion not only in general motion, but also in dominant translation or dominant rotation scenarios. We do not fuse the multiple visual inputs from multiple cameras in a loosely coupled manner, but we consider the underlying geometrical constraints in a coherent and effective way. Rotational magnitude is determined in the subsequent stage.The organization of this paper is as follows. Section 2 reviews related works in the literature. Section 3 presents how optical and normal flows are induced on the image plane and also on the image sphere when the imaging surface is spherical. Section 4 introduces a variant of the brightness change equation [29] for a camera having planar image plane, and then extends it to the case of a multi-camera rig. Section 5 discusses how normal flows can be used to determine the directions of translation and rotation from general motion separately through the use of three subsets of normal flow pairs and four subsets of normal flow triplets. The motion solution can be refined and rotational magnitude is determined using detranslation and derotation together. Section 6 presents the experimental results that were conducted by simulation and images of real scenes with motion ground truth available. We extensively evaluated the performance of the proposed method against the current state-of-the-art methods. Finally, conclusion is given in Section 7.

@&#CONCLUSIONS@&#
