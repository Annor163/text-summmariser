@&#MAIN-TITLE@&#
The cost of quality: Implementing generalization and suppression for anonymizing biomedical data with minimal information loss

@&#HIGHLIGHTS@&#
We study a frequently recommended transformation model for health data anonymization.We prove that common assumptions about privacy models do not hold in this context.As a consequence, existing approaches provide only sub-optimal data quality.We show that a simple alternative method is inefficient in terms of execution times.We propose a novel approach to overcome these limitations.

@&#KEYPHRASES@&#
Security,Privacy,De-identification,Anonymization,Statistical disclosure control,Optimization,

@&#ABSTRACT@&#
ObjectiveWith the ARX data anonymization tool structured biomedical data can be de-identified using syntactic privacy models, such as k-anonymity. Data is transformed with two methods: (a) generalization of attribute values, followed by (b) suppression of data records. The former method results in data that is well suited for analyses by epidemiologists, while the latter method significantly reduces loss of information. Our tool uses an optimal anonymization algorithm that maximizes output utility according to a given measure. To achieve scalability, existing optimal anonymization algorithms exclude parts of the search space by predicting the outcome of data transformations regarding privacy and utility without explicitly applying them to the input dataset. These optimizations cannot be used if data is transformed with generalization and suppression. As optimal data utility and scalability are important for anonymizing biomedical data, we had to develop a novel method.MethodsIn this article, we first confirm experimentally that combining generalization with suppression significantly increases data utility. Next, we proof that, within this coding model, the outcome of data transformations regarding privacy and utility cannot be predicted. As a consequence, existing algorithms fail to deliver optimal data utility. We confirm this finding experimentally. The limitation of previous work can be overcome at the cost of increased computational complexity. However, scalability is important for anonymizing data with user feedback. Consequently, we identify properties of datasets that may be predicted in our context and propose a novel and efficient algorithm. Finally, we evaluate our solution with multiple datasets and privacy models.ResultsThis work presents the first thorough investigation of which properties of datasets can be predicted when data is anonymized with generalization and suppression. Our novel approach adopts existing optimization strategies to our context and combines different search methods. The experiments show that our method is able to efficiently solve a broad spectrum of anonymization problems.ConclusionOur work shows that implementing syntactic privacy models is challenging and that existing algorithms are not well suited for anonymizing data with transformation models which are more complex than generalization alone. As such models have been recommended for use in the biomedical domain, our results are of general relevance for de-identifying structured biomedical data.

@&#INTRODUCTION@&#
Collaborative collection and sharing of sensitive personal data have become an important element of biomedical research. To protect patient privacy in complex research environments, a broad spectrum of safeguards must be implemented, including legal, contractual as well as technical methods. Data anonymization is a central building block in this context. It aims at sanitizing datasets in ways that prevent attackers from breaching the subjects’ privacy. A number of incidents have shown that simply removing all directly identifying information (e.g. names) is not sufficient [1–3]. As a consequence, different definitions of privacy and techniques for sanitizing datasets have been proposed [4–7]. As sanitization inevitably leads to loss of information and thus a decrease in data utility, a balance has to be sought between privacy risks on one side and suitability for a specific use case on the other.According to national laws, such as the US Health Insurance Portability and Accountability Act (HIPAA) [8], and international regulations, such as the European Directive on Data Protection [9], different methods may be used. In particular, the HIPAA Privacy Rule defines two basic methods for de-identifying datasets [10]. The first method requires the removal or the modification of a pre-defined set of attributes and attribute values. The second method, which is called “expert determination” requires that a professional “determines that the risk is very small that the information could be used […] to identify an individual”[10]. For this purpose, methods of statistical disclosure control may be used.In this work, we will focus on statistical disclosure control for structured data, which can be represented in a tabular form with each row corresponding to the data about one individual [6]. Specifically, we will describe methods implemented in ARX, a data anonymization tool that we have developed for the biomedical domain [11,12]. A typical use case is the de-identification of research data prior to sharing. To our knowledge, ARX offers the most comprehensive support of methods for anonymizing structured data to date. Its highlights include methods for risk analyses, risk-based anonymization, syntactic privacy models and methods for automated and manual analysis of data utility. Moreover, the tool implements an intuitive coding model, is highly scalable and provides a sophisticated graphical user interface with several wizards and visualizations that guide users through different aspects of the anonymization process.

@&#CONCLUSIONS@&#
Our work is the first systematic study describing how to implement generalization and suppression when biomedical data is to be anonymized with common syntactic privacy models and utility measures. Implementing this transformation model leads to a strong increase in utility compared to anonymizing data with generalization only. The resulting recommendation is to use this model preferably. Moreover, common privacy models and utility measures are non-monotonic when data are transformed with generalization followed by tuple suppression. As a consequence, existing anonymization algorithms fail to produce output with optimal quality. Our novel approach overcomes this limitation while providing scalability.By considering the findings described in this article, we were able to significantly improve the data anonymization tool ARX. With the implementation of the described methods, the software is able to anonymize data with generalization and suppression while guaranteeing the optimality of results in terms of data utility. Moreover, ARX remains scalable and supports the anonymization of very large datasets with complex privacy models on commodity hardware. Our tool is available as open source software [12].While the coding model, privacy models and utility measures covered in this article are well known, our work is the first to investigate an integrated implementation. Our results apply to methods which are being used for protecting datasets from re-identification, which is a requirement in many laws and regulations worldwide. Moreover, the coding model investigated in this work has been recommended for use in the biomedical domain. Our results should therefore be considered when anonymizing structured biomedical data.None declared.Supplementary data associated with this article can be found, in the online version, at http://dx.doi.org/10.1016/j.jbi.2015.09.007.Supplementary appendises