@&#MAIN-TITLE@&#
A multi-label feature extraction algorithm via maximizing feature variance and feature-label dependence simultaneously

@&#HIGHLIGHTS@&#
We derive a least-squares formulation for MDDMp technique.A novel multi-label feature extraction algorithm is proposed.Our algorithm maximizes both feature variance and feature-label dependence.Experiments show that our algorithm is a competitive candidate.

@&#KEYPHRASES@&#
Multi-label classification,Dimensionality reduction,Feature extraction,Principal component analysis,Hilbert–Schmidt independence criterion,Eigenvalue problem,

@&#ABSTRACT@&#
Dimensionality reduction is an important pre-processing procedure for multi-label classification to mitigate the possible effect of dimensionality curse, which is divided into feature extraction and selection. Principal component analysis (PCA) and multi-label dimensionality reduction via dependence maximization (MDDM) represent two mainstream feature extraction techniques for unsupervised and supervised paradigms. They produce many small and a few large positive eigenvalues respectively, which could deteriorate the classification performance due to an improper number of projection directions. It has been proved that PCA proposed primarily via maximizing feature variance is associated with a least-squares formulation. In this paper, we prove that MDDM with orthonormal projection directions also falls into the least-squares framework, which originally maximizes Hilbert–Schmidt independence criterion (HSIC). Then we propose a novel multi-label feature extraction method to integrate two least-squares formulae through a linear combination, which maximizes both feature variance and feature-label dependence simultaneously and thus results in a proper number of positive eigenvalues. Experimental results on eight data sets show that our proposed method can achieve a better performance, compared with other seven state-of-the-art multi-label feature extraction algorithms.

@&#INTRODUCTION@&#
Traditional supervised classification solves problems in which one instance has one label only [1], which is regarded as single-label classification. However, in many real-world applications, one instance are possibly associated with multiple labels simultaneously. For example, a sunrise picture is annotated by sky, sun and sea at the same time [2]; a piece of news belongs to environment protection, haze, and weather [3]; a protein is located in different sub-cellular locations simultaneously [4]. Such a learning task is referred to as multi-label classification. Recently multi-label classification has been paid much attention to in machine learning, pattern recognition, data mining and statistics. Thus a variety of multi-label classification methods were proposed, such as, kNN-type methods [5,6], SVM-like techniques [7–10], ensemble classifiers [11–13], and collective learning techniques [14,15], most of which have been reviewed extensively in [16–20].On the other hand, recent technological innovations allow us to collect massive amount of data with a large number of features. Bellman [21] coined a well-known term, curse of dimensionality. For classification issue, such a term implies that for a given instance size, there is a maximum number of features above which the performance of a classifier will degrade rather than improve [22]. Additionally, this cure also results in a high computational complexity in practice. To alleviate its possible effect, dimensionality reduction becomes an effective way to remove the irrelevant, redundant and noisy features, which generally covers feature extraction [22], and feature selection [23]. In this paper, we only focus on feature extraction for dimensionality reduction.Regardless of single-label and multi-label classification, feature extraction techniques mainly could be grouped into two categories: unsupervised and supervised. What differentiates them is whether class label information is exploited or not.Unsupervised methods extract a small number of features without using label information to retain as much discriminant information as possible. A representative linear method is principal component analysis (PCA), which could be described from two different viewpoints. One is to find an orthogonal low-dimensional sub-space via maximizing feature variance [1,24,25], and the other is via minimizing a squared reconstruction error [25–29]. In addition, some complicated methods, e.g., locally linear embedding(LLE) [30], Laplacian eigenmap [31], locality preserving projections(LPP) [32] and ISOMAP [33], aim at finding a nonlinear low-dimensional sub-space through preserving the data manifold structures. In principle, these unsupervised approaches primarily in single-label classification could be directly applied to multi-label classification.Conversely, supervised methods sufficiently and elaborately exploit label information in feature extraction procedures. Linear discriminant analysis (LDA) [1,25,34] is a typical single-label technique which obtains an optimal low-dimensional sub-space by maximizing the between-class scatter measure and minimizing the within-class scatter one at the same time. However, LDA cannot be directly applied to multi-label classification, since a multi-label instance belongs to several different classes simultaneously, and how much it should contribute to two scatter measures remains ambiguous. Therefore three generalized versions have been proposed. In [35], a multi-label instance is duplicated into several single-label ones and then a multi-label data set is converted into a single-label one, to fit LDA directly. To correct the over-counting problem in [35], multi-label LDA (MLDA) [36] is proposed as a weighted form, where the weights of each instance to all labels are estimated by label correlation information. Since the dimensions of low-dimensional sub-space from the above two methods does not exceed the number of classes minus 1, the between-class scatter matrix is changed in direct multi-label LDA (DMLDA) [37], to distinguish training instances that do not have a specific label from the mean vector of the instances belonging to this label.Latent semantic indexing (LSI) turns to be a successful linear single-label feature extraction approach in document analysis and information retrieval [38]. Multi-label informed latent semantic indexing (MLSI) [39] extends LSI to obtain a low-dimensional sub-space which maximizes feature variance and binary label variance via a linear combination way. Theoretically, canonical correlation analysis (CCA) could be directly used in multi-label situation [40]. Further a least-squares formulation and its several regularized variants are extended for CCA in [41], which means that CCA could be converted into slightly different least-squares problems. Hilbert–Schmidt independence criterion (HSIC) [42] is a non-parametric dependence measure which considers all modes of dependencies between all variables. Multi-label dimensionality reduction via dependence maximization (MDDM) [43] attempts to search for a low-dimensional sub-space by maximizing feature-label dependence using HSIC with orthonormal projection directions and orthonormal projected features respectively (MDDMp and MDDMf in detail).It is worth noting that most of the aforementioned feature extraction methods can fall in a least-squares framework, including PCA, LDA, CCA, LPP, LLE and Laplacian eigenmaps [28], MDDMf [44] and MLDA [45]. But whether MDDMp could be associated with a least-squares issue is still an open problem. Additionally, PCA is concerned with feature data structure only, MLSI deals with feature and label data structures at the same time, CCA and MDDM consider dependence from features to labels, and multi-label LDA forms implicitly express the relationship between features and labels. According to matrix theory [46], PCA, MLSI and DMLDA could produce many small positive eigenvalues, whereas CCA, MDDM and MLDA result in a few large positive eigenvalues, both of which could deteriorate the multi-label classification performance because of an improper number of projection directions.In this study, we will take feature data structure and feature-label dependence simultaneously into consideration explicitly, and derive a proper number of positive eigenvalues or projection directions. We prove that MDDMp also falls in the least-squares framework and then propose a novel multi-label feature extraction method to integrate two least-squares formulations in PCA and MDDMp linearly, which both maximizes feature variance and maximizes feature-label dependence at the same time. Therefore our method is referred to as MVMD simply. Based on multi-output linear ridge regression [1,25] as our multi-label baseline classifier, our experimental results on eight benchmark data sets illustrate that, with a proper number of projected features, our MVMD is overall superior to the aforementioned multi-label feature extraction techniques including PCA, two MDDM versions, CCA, MLSI, and MLDA and DMLDA, according to six instance-based performance evaluation measures (Hamming loss, accuracy, F1, precision, recall and subset accuracy) and computational time.Summarily, the main contributions of this paper are highlighted as follows: (a) we propose a least squares formulation for multi-label dimensionality reduction via dependence maximization with orthonormal projection directions (MDDMp); (b) via combing such a formulation with that of PCA linearly, a novel multi-label feature extraction approach is presented, which maximizes both feature variance and feature-label dependence simultaneously; (c) the extensive experiments on eight data sets demonstrate the effectiveness and efficiency of our proposed method.The rest of this paper is organized as follows. Multi-label feature extraction setting is introduced in Section 2. In Section 3, PCA and its least-squares form are briefly introduced. Two MDDM versions are reviewed firstly and then a least-squares formulation for MDDMp is derived in Section 4. In Section 5 we propose and analyze our novel feature extraction algorithm (MVMD). The related work is formally summarized in Section 6. Section 7 is devoted to experiments with eight benchmark data sets. Finally this paper ends with some conclusions in Section 8.LetQ={1,2,…,q}be a finite set of q class labels, and 2Qall possible subsets of Q. We denote a multi-label training data set of size l drawn identically and independently from an unknown probability distribution in a D-dimensional real space by,(1){(x1,L1),…,(xi,Li),…,(xl,Ll)},where xi∈ RDand Li∈ 2Qrepresent the ith instance and its relevant label set. Additionally, the complement of Li, i.e.,L¯i=Q∖Li,is referred to as the irrelevant label set of xi.The goal of multi-label classification is to learn a classifier f(x): RD→ 2Qwhich can predict the relevant labels for unseen instances [5,8,9,16–20].The multi-label linear feature extraction is to derive a projection matrix P ∈ RD × dthat maps any original instance vector x in the D-dimensional space into a projected one x′ in the lower d-dimensional sub-space (d < D), to preserve the information and structure of original data based on a certain criterion [22],(2)x′=PTx,where T indicates the transpose operation of matrix or vector. For the convenience of representation, we also adopt a binary vectoryi=[yi1,yi2,…,yiq]Tto label the instance xi, whereyik=1if the kth label is in Li, and−1otherwise. Furthermore, we utilize the following two matrices to depict feature and binary label data,(3)X=[x1,…,xi,…,xl]T,Y=[y1,…,yi,…,yl]T,where the ith rows in X and Y correspond to the ith training instance. In this study, such two matrices are centered for all feature extraction methods but LDA-type techniques, i.e.,(4)X⇐HX,Y⇐HY,through the centered matrix,(5)H=I−uuT/l,where I is an identity matrix of size l and u denotes an all-one column vector of length l. Without the loss of generality, we still use X and Y to represent the centered feature and label data matrices in the next sections.Principal component analysis (PCA) is one of the most popular linear feature extraction methods under unsupervised paradigm, which can be described from two different perspectives. One is to find an optimal projection matrix through maximizing feature variance [1,24,25], and the other is via minimizing a squared reconstruction error [25–29]. In this section, we review PCA according to the second perspective. PCA is formulated as the following least-squares problem,(6)minJ(A,P)=∥X−APT∥F2,s.t.PTP=I,where F indicates the Frobenius norm of matrix, P ∈ RD × Dis a projection direction matrix, A ∈ Rl × Drepresents a projection coefficient matrix. By constrainingPTP=I,we restrict the projection directions (i.e., column vectors of P) are orthonormal. The objective function in (6) can be expanded via∥X∥F2=tr(XTX)andtr(AB)=tr(BA)[46] as follows,(7)J(A,P)=∥X−APT∥F2=tr((X−APT)T(X−APT))=tr(XTX−2XTAPT+PATAPT)=tr(XTX)−2tr(PTXTA)+tr(ATA),where tr() is the trace of matrix. Through using∂tr(XA)/∂A=XTand∂tr(ATA)/∂A=2A[46], and setting the derivatives ofJ(A,P)with respect to A to be zero, we obtain,(8)∂J(A,P)∂A=2(A−XP)=0⇒A=XP.To insert (8) intoJ(A,P),the objective function (7) is simplified as,(9)J(P)=tr(XTX)−tr(PTXTXP),where the first term at the right side is constant. Since minimizingJ(P)is equivalent to maximizingtr(PTXTXP),the optimization problem (6) can be converted into,(10)maxtr(PTXTXP),s.t.PTP=I.Through the Lagrangian multiplier technique, such a problem is transformed into the following eigenvalue problem,(11)XTXP=ΛP,whereΛis a diagonal matrix with eigenvalues(λ1,…,λD≥0),i.e.,(12)Λ=[λ10⋯00λ2⋯⋮⋮⋮⋮⋮0⋯⋯λD]∈RD×D,and the projection matrix P consists of corresponding eigenvectors. Now the covariance matrix of PCA projection coefficient matrix A is,(13)ATA=PTXTXP=Λ,which shows that the PCA projection coefficients are de-correlated. Moreover, (9) becomes,(14)J(P)=tr(XTX)−tr(Λ).To minimize J(P), we should choose the d(< D) largest eigenvalues and their eigenvectors to construct P, which also corresponds to the d largest variances according to (13). Therefore we also explain that PCA maximizes feature variance essentially.Now, there are two widely-used ways to determine the reduced dimensions d after all eigenvalues are sorted in descending order. One is to detect the d directly, and the other is adaptively to choose a smallest d such that(15)∑i=1dλi≥t∑i=1Dλi,where t ∈ (0, 1) is a proper threshold that measures how much information in the original data is preserved.According to matrix theory [46], the rank of the real symmetric matrix XTX is min(l, D) at most; therefore, PCA produces many small positive eiginvalues and needs many projection directions for a larger threshold t, e.g., 0.999.In this section, we review multi-label dimensionality reduction via dependence maximization (MDDM) [43] and then derive a corresponding least-squares formulation for MDDM form with orthonormal projection directions (i.e., MDDMp).The Hilbert–Schmidt independence criterion (HSIC) [42] is a non-parametric dependence measure which considers all modes of dependencies between all variables. With the linear kernels for both feature space and label one, the empirical estimate of HSIC is described as,(16)HSIC=(l−1)−2tr(HXXTHYYT).The MDDM [43] aims at finding a projection matrix P by maximizing the above HSIC. The original optimization problem of MDDM with orthonormal projection directions (MDDMp) is depicted as,(17)maxtr(HXPPTXTHYYT)=tr(PTXTHYYTHXP),s.t.PTP=I,which then is converted into an eigenvalue problem via Lagrangian multiplier technique,(18)XTHYYTHXP=ΛP.In [43], a MDDM form with orthonormal projected features (i.e., MDDMf) is proposed, wherePTXTXP=Ioriginally. To deal with the singularity problem of the covariance matrix XTX, a regularized optimization problem on constraints for MDDMf is adopted,(19)maxtr(PTXTHYYTHXP),s.t.PT(βXTX+(1−β)I)P=I,and its corresponding generalized eigenvalue problem is formulated as follows,(20)XTHYYTHXP=Λ(βXTX+(1−β)I)P,where β ∈ [0, 1] represents a pre-defined regularization constant on XTX. Whenβ=0,MDDMf degrades into MDDMp.Additionally, regardless of MDDMp and MDDMf, the rank of their matrix XTHYYTHX is q at most when q < D and q < l, which implies that such two methods result in the q positive eigenvalues at most. Through the threshold t, the q projection directions at most are chosen. It is ambiguous and difficult to select more directions since the remainingD−qeigenvalues are zeros theoretically.Now, we prove that the above MDDMp is also associated with a least-squares formulation, similarly in PCA. Our optimization problem is formulated as,(21)minJ(B,P)=∥YTHX−BPT∥F2,s.t.PTP=I,where YTHX characterizes the dependence between features and labels, P ∈ RD × Dindicates a projection direction matrix, and B ∈ Rq × Drepresents a projection coefficient matrix. We rewrite the objective function in (21) as,(22)J(B,P)=∥YTHX−BPT∥F2=tr((YTHX−BPT)T(YTHX−BPT))=tr(XTHYYTHX−2XTHYBPT+PBTBPT)=tr(XTHYYTHX)−2tr(PTXTHYB)+tr(BTB).Via setting the derivatives ofJ(B,P)with respect to B to be zero, we have,(23)∂J(B,P)∂B=2(B−YTHXP)=0⇒B=YTHXP.To insert (23) intoJ(B,P),a simplified objective functionJ(P)is achieved,(24)J(P)=tr(XTHYYTHX)−tr(PTXTHYYTHXP).Here, the first term of the right side in (24) is a constant which measures the dependence from original features to labels, and the second one evaluates the dependence between projected features and labels, which depends on P. Now the original optimization problem (21) could be converted into,(25)maxtr(PTXTHYYTHXP),s.t.PTP=I,which is identical to the original optimization problem (17) of MDDMp and further can be transformed into the same eigenvalue problem as (18). Consequently we derive a least-squares formulation (21) for MDDMp (17) in this paper. Using (18), the above (24) becomes,(26)J(P)=tr(XTHYYTHX)−tr(Λ),which inspires us to choose the d(<D) largest eigenvalues and their eigenvectors to build P. Meanwhile, the projection coefficients are de-correlated too, i.e.,(27)BTB=PTXTHYYTHXP=PTΛP=Λ.But, in MDDMp, the projected features are defined as,(28)X′=XP,and thus we have(29)(X′)TX′=PTXTXP≠Λ,which implies that the projected features are still correlated in MDDMp rather than de-correlated in PCA. Additionally, since in (22)PTP=Iis used, the above (21) is not suitable for MDDMf version.In [44], the generalized eigenvalue problem (20) for MDDMf is converted into an eigenvalue one, and then its least-squares formulation is derived. But this transformation means that orthonormal projection featuresPTXTXP=Iin MDDMf are replaced by orthonormal projection directionsPTP=Iin MDDMp, since almost all eigenvalue solvers automatically produce orthonormal eigenvectorsPTP=I.In this section, we propose a novel multi-label linear feature extraction technique which linearly combines the objective function (6) in PCA with that (21) in MDDMp, to both maximize feature variance and maximize feature-label dependence simultaneously. Therefore our proposed method is referred to as MVMD simply.The optimization problem of MVMD with orthonormal projection directions is designed as,(30)minJ(A,B,P)=(1−β)∥X−APT∥F2+β∥YTHX−BPT∥F2,s.t.PTP=I.where β ∈ [0, 1] is a balance factor to control the trade-off between two squared error terms from PCA and MDDMp respectively, A and B indicate two projection coefficient matrices, and P represents a projection direction matrix. Obviously, whenβ=0this model degenerates to original PCA, whereas whenβ=1our model is equivalent to MDDMp. We expand the objective function in (30) as,(31)J(A,B,P)=(1−β)tr((X−APT)T(X−APT))+βtr((YTHX−BPT)T(YTHX−BPT))=(1−β)(tr(XTX)−2tr(PTXTA)+tr(ATA))+β(tr(XTHYYTHX)−2tr(PTXTHYB)+tr(BTB)).After the derivatives ofJ(A,B,P)with respect to A and B are forced to be zero, we obtain,(32)∂J(A,B,P)∂A=2(1−β)(A−XP)=0⇒A=XP,∂J(A,B,P)∂B=2β(B−YTHXP)=0⇒B=YTHXP.According to the above A and B, the objective function (31) could be simplified as,(33)J(P)=tr((1−β)XTX+βXTHYYTHX)−tr(PT((1−β)XTX+βXTHYYTHX)P).We define a new matrix,(34)G=(1−β)XTX+βXTHYYTHX,and then obtain,(35)J(P)=tr(G)−tr(PTGP),where the first term in the right side in (35) is constant. Therefore, the original optimization problem (30) is converted into the following problem,(36)maxtr(PTGP)s.t.PTP=I.Through Lagrangian multiplier technique, this (36) is transformed into an eigenvalue problem,(37)GP=ΛP.Now, the objective function (35) is(38)J(P)=tr(G)−tr(Λ),which means that we should choose the d largest eigenvalues and their eigenvectors to construct the mapping matrix P.It is noted that(1−β)ATA+βBTB=Λnow rather thanATA=Λin PCA andBTB=Λin MDDMp. Therefore the projected features (i.e.,X′=XP) are not de-correlated yet.Compared our MVMD with MDDMf, there are two main differences. One is that our MVMD produces the orthonormal projection directions, whereas MDDMf the orthonormal projection features. The other is that the covariance matrix XTX occurs at the left side in MVMD which depends on the pre-defined balance factor, and at the right side in MDDMf which is associated with both the different eigenvalues and fixed regularization constant.Additionally, according to matrix theory [46], the number of positive eigenvalues from our MVMD is min(l, D) at most. But it will be observed experimentally that MVMD would produce a few large eigenvalues and many small ones, which benefits to choose the proper reduced dimensions through the threshold t. The pseudo-code of MVMD is listed in Algorithm 1.

@&#CONCLUSIONS@&#
Principal component analysis (PCA) and multi-label dimensionality reduction via dependency maximization (MDDM) are two effective multi-label feature extraction techniques. The former is already associated with a least-squares formulation. In this paper, we derive a least-squares formulation for MDDM with orthonormal projection directions too. Then we combine such two least-squares formulae via a linear combination way and thus construct a novel multi-label feature extraction approach to maximize both feature variance and feature-label dependence, i.e., MVMD simply. Furthermore, our extensive experiments on eight benchmark data sets demonstrate that, with a proper number of projected features, our MVMD is a competitive candidate for multi-label feature extraction, compared with seven existing methods, according to six instance-based performance measures (Hamming loss, accuracy, F1, precision, recall and subset accuracy) and computational costs. In future work, we will solve MVMD through its least-squares problem directly.