@&#MAIN-TITLE@&#
ALISA: An automatic lightly supervised speech segmentation and alignment tool

@&#HIGHLIGHTS@&#
ALISA can align speech with imperfect transcripts in any alphabetic language.On average, 70% of the data is being correctly aligned, with a WER of less than 0.5%.Subjective listening tests showed a slight preference for the fully supervised system.

@&#KEYPHRASES@&#
Speech segmentation,Speech and text alignment,Grapheme acoustic models,Lightly supervised system,Imperfect transcripts,

@&#ABSTRACT@&#
This paper describes the ALISA tool, which implements a lightly supervised method for sentence-level alignment of speech with imperfect transcripts. Its intended use is to enable the creation of new speech corpora from a multitude of resources in a language-independent fashion, thus avoiding the need to record or transcribe speech data. The method is designed so that it requires minimum user intervention and expert knowledge, and it is able to align data in languages which employ alphabetic scripts. It comprises a GMM-based voice activity detector and a highly constrained grapheme-based speech aligner. The method is evaluated objectively against a gold standard segmentation and transcription, as well as subjectively through building and testing speech synthesis systems from the retrieved data. Results show that on average, 70% of the original data is correctly aligned, with a word error rate of less than 0.5%. In one case, subjective listening tests show a statistically significant preference for voices built on the gold transcript, but this is small and in other tests, no statistically significant differences between the systems built from the fully supervised training data and the one which uses the proposed method are found.

@&#INTRODUCTION@&#
Over the past decade, speech-enabled applications have progressed to the point where their presence in human–computer interfaces is almost ubiquitous. However, this is true only for the languages for which a sufficient degree of effort has been invested in creating purposely built tools and resources. Any speech-based application requires a large amount of high-quality data and expert knowledge, which are time consuming and computationally expensive to collect.In this paper we try to alleviate one of the major problems which occurs when migrating a speech-based solution either from one language to another, or from one set of resources to another: speech data preparation. In both speech recognition and speech synthesis, the performance of the resulting system is highly dependent on the quality and amount of training data. But the most widespread method for gathering speech resources nowadays is either by recording a voice talent in a studio or by manually transcribing existing recorded data. However, both of these methods are tedious and usually deter developers from expanding their language and/or speaker environment portfolio.Accordingly, we turn our attention towards the theoretically unlimited supply of speech data available on the internet, of which the majority is recorded in professional or semi-professional environments – an essential requirement to begin with. Examples of such data include audiobooks, podcasts, video lectures, video blogs, company presentations, news bulletins, etc. These resources are even more appealing when accompanied by an approximate transcript, even though the precise synchronisation between text and audio is not generally available.The automatic alignment of speech and text has long been studied, and in Section 2 we present some of the most prominent methods. However, our goal is slightly different than theirs, as we aim to rely on no previous knowledge or prepared data, and try to provide a solution for any language with an alphabetic writing system.11Other types of writing system might be viable, but within this work we do not investigate them.This is of course highly error prone, but we will show in the Results section that the errors are minimal and negligible both for speech recognition and speech synthesis tasks.The paper is structured as follows: in Section 2 we describe the state-of-the-art for speech and text alignment methods. Section 3 gives a brief overview of the proposed method, its individual steps being expanded in Sections 4–8. Objective and subjective evaluations of the tool are presented in Sections 9 and 10 respectively. Section 11 provides discussion and concludes the paper.

@&#CONCLUSIONS@&#
Using speech resources available online is an important step in adapting existing speech-based technologies to a multitude of new languages. We have therefore introduced a two-step method for aligning speech with the type of imperfect transcripts often found in online resources. The two steps consist of a GMM-based sentence-level segmentation and an iterative grapheme-level acoustic model building and aligning. The method uses only 10min of manually labelled data, and it is able to extract on average 70% of it, with a word error rate of less than 0.5%. This is possible due to the use of a segmentation model which is built from the original data itself, and the use of an iterative acoustic model training procedure, which makes use of a highly restricted word network, called a skip network.Due to the aim of aligning data in a number of different languages, the lack of expert or prior knowledge when training the acoustic models imposes a series of additional problems, such as defining a confidence measure for the recognised output, deciding between the use of word or grapheme level lattices for the discriminative training, selecting a proper tri-grapheme set, or defining a set of questions for the state tying. All these issues are analysed using an English audiobook, for which the GOLD standard segmentation and transcription are available. Additional results were presented for a French audiobook, but only for the processing pipeline previously found to be best. We also compared our results against a state-of-the-art speech recognition system, as well as a version of our tool which uses a supervised lexicon instead of graphemes. These results showed that ALISA with a supervised lexicon outperforms even our best ASR system.Subjective listening tests were conducted to determine how much the errors within the aligned dataset influence the quality of a text-to-speech synthesis system as this is the major goal of our work. In one case, our evaluations showed a statistically significant preference for voices built on error-free transcripts, but this was small and in other tests, no statistically significant preferences for systems built on the supervised segmentation and alignment were found.In conclusion, the ALISA tool can represent a starting point in building new speech datasets in various languages, starting from found data and with minimal user intervention.As future work, we would like to investigate the use of non-alphabetic languages, multi-speaker databases, and suboptimal recording conditions of the speech data. One other important aspect would be the full alignment of the speech resource, including spoken insertions and substitutions, as a means of aligning small datasets.