@&#MAIN-TITLE@&#
Modelling discrete longitudinal data using acyclic probabilistic finite automata

@&#HIGHLIGHTS@&#
We introduce APFA as graphical models for discrete longitudinal data.We propose a novel model selection algorithm based on penalized likelihood.We compare its rate of convergence and goodness-of-fit to Beagle.We use data from molecular genetics and social science in the comparisons.Our algorithm performs as least as well or better than the algorithm in Beagle.

@&#KEYPHRASES@&#
Context-specific graphical model,Acyclic probabilistic finite automata,State merging,Discrete longitudinal data,

@&#ABSTRACT@&#
Acyclic probabilistic finite automata (APFA) constitute a rich family of models for discrete longitudinal data. An APFA may be represented as a directed multigraph, and embodies a set of context-specific conditional independence relations that may be read off the graph. A model selection algorithm to minimize a penalized likelihood criterion such as AIC or BIC is described. This algorithm is compared to one implemented in Beagle, a widely used program for processing genomic data, both in terms of rate of convergence to the true model as the sample size increases, and a goodness-of-fit measure assessed using cross-validation. The comparisons are based on three data sets, two from molecular genetics and one from social science. The proposed algorithm performs at least as well as the algorithm in Beagle in both respects.

@&#INTRODUCTION@&#
Discrete longitudinal data consisting of an ordered sequence of discrete variables–often, repeated measurements of the same variable–arise in many and varied applications. Examples include randomized clinical studies, in which a categorical response is recorded at a series of visits to the clinic at pre-specified times; cohort studies, in which an outcome such as morbidity or employment status is followed over time for one or more groups of individuals; and molecular genetics studies employing DNA chips, which record the values of a large number of genetic markers located at a series of physical positions on a chromosome. Often the studies include explanatory variables, and there is interest in relating the pattern of responses to these.The wide range of study designs and objectives is reflected in the wide range of statistical models that may be applied to such data. These may be (Diggle et al., 2013; Molenberghs and Verbeke, 2005) grouped into three broad classes: (i) marginal models, that seek to relate the marginal distribution of the response variable at each time point to explanatory variables; (ii) subject-specific or random effect models, which account for heterogeneity between individuals by adopting regression-type models with random subject effects; and (iii) conditional or transition models, that focus on the conditional distribution of the response at each time-point given prior responses and possibly explanatory variables.For example, in randomized clinical trials the focus of interest is often on comparing the outcome between treatment groups at each point of time, and for these marginal modelling approaches may be appropriate. The approach that is studied in this paper is of the conditional type: the aim is to construct parsimonious models for the conditional distribution of the response at each time point given prior responses. Before we examine these models in detail, it is useful to give a brief sketch of their relations with other competitive models.The transition models often used in the analysis of discrete longitudinal data specify conditional distributions that only involve the previousqresponses, whereqis called the order of the model. Theqprior responses and other covariates are treated on an equal footing as explanatory variables in a convenient parametric model, for example, a generalized linear model (Diggle et al., 2013, Chapter 10). Time-homogeneity is assumed, so that the conditional distributions are constant over the time interval spanned by the study. A simple example for binary data with no covariates is aqth order Markov chain, in which the conditional distribution is specified by a2qtable of conditional probabilities. Although Markov chains are very general models for time-homogeneous discrete processes, they suffer from the curse of dimensionality, since the number of parameters required increases exponentially withq. In practice this hinders the use of Markov chains with long range dependences.Variable length Markov chains (Bühlmann and Wyner, 1999) relax the assumption that the conditional distribution involvesqprior responses, instead allowing the number of previous variables that enter the conditioning to vary according to the values of these variables. This is an example of context-specificity, that is, allowing the dependence structure to vary locally depending on the values of the variables. Variable length Markov chains provide a rich model space and allow the choice of structured models that incorporate longer range dependences. Efficient model selection methods are available (Bühlmann, 2000). Other ways to construct parsimonious higher-order Markov chain models have been proposed (Raftery and Tavare, 1994).In some applications the assumption of time-homogeneity may be unwarranted: for example, transition probabilities between morbidity outcomes may be age-dependent, and so vary over the course of the study. The Biofam and Duroc studies described in Section  5 provide two other examples where homogeneity is implausible. Directed graphical models (Bayesian networks) avoid the assumption. These models are represented by directed graphs, in which the nodes represent the variables, and the parentspa(v)of a nodevin the graph (the nodesusuch that there is a directed edge fromutov) represent the determinants ofv. There is no requirement that the determinants ofvbe immediately prior tovin the ordering. An example concerning side-effect profiles in a clinical trial of neuroleptica is given in Edwards (2000, Section 7.1.3). Some recent work has extended Bayesian network methodology to support context-specific modelling (Boutilier et al., 1996; Myers and Troyanskaya, 2007).In this paper we study a class of models due to Ron et al. (1998) called acyclic probabilistic finite automata. Note that we use the same acronym, APFA, for both singular and plural forms (automaton and automata). APFA combine aspects of Bayesian networks and variable length Markov chains, in that they are time-heterogeneous and involve context-specific dependences, in a way that is more general than simply varying the order of dependence. They provide a very rich class of models for discrete longitudinal data, allowing long-range dependences to be modelled, and may be represented as graphs. So an APFA can be regarded as a time-heterogeneous context-specific graphical model for discrete longitudinal data. See Edwards and Ankinakatte (in press, Section 10) for a more precise comparison of APFA with Bayesian and Markov networks.APFA may be useful when there is interest in understanding the dependence structure between the variables, and when this structure is expected to vary over the time interval of the study. The methodology assumes that the variables are measured at common times (or, in the case of genomic data, at common spatial positions) and so is not appropriate when times between transitions vary. It has proven to be well-suited to highly-structured, high-dimensional data such as DNA chip data, but we believe that it may be of more general interest and utility. Note that here, except for a brief mention in Section  6, we do not include explanatory variables in the models; for a way to do this see Edwards and Ankinakatte (in press, Section 8).The structure of the paper is as follows. Section  2 introduces APFA from a statistical perspective. Section  3 describes the model selection algorithm of Ron et al. (1998) which (in a modified form) forms the core of the haplotype clustering algorithm implemented in the Beagle program (Browning and Browning, 2007a,b) that is widely used for phasing and imputation of DNA chip data. Section  3.3 proposes a further modification to the algorithm based on a penalized likelihood criterion. The steps involved in building an APFA from a given discrete longitudinal data set are illustrated using an example. In the subsequent sections the performance of the proposed algorithm is compared to the one implemented in Beagle. Section  4 details the methods used to compare the algorithms; Section  5 describes the data sets used; Section  6 presents the results, and Section  7 describes the software used. Finally, Section  8 provides a brief discussion.This section gives a brief introduction to APFA from a statistical perspective: see Edwards and Ankinakatte (in press) for a more detailed exposition. We first describe sample trees, that are closely related to APFA.A set of discrete longitudinal data withNobservations ofpdiscrete variablesX1,…,Xpcan be represented as a tree in the following way. Starting with the root node, edges branch out to nodes at the first level or stage. (Previous papers Ron et al., 1998, Browning and Browning, 2007b and Edwards and Ankinakatte, in press use level. But since in statistics this term usually refers to the value of a discrete variable, we here choose to use stage.) The number of branches corresponds to the number of distinct values ofX1, and the count on each edge corresponds to the frequency of occurrence of the respective value. An example is shown in Fig. 1. There are 36 observations withX1=1and 34 withX1=2. From each node at stage one, edges branch out to stage two, based on the distinct values ofX2givenX1. The process continues up to stagep, and results in the construct called a sample tree. Sample trees are also known as prefix tree acceptors in machine learning (Carrasco and Oncina, 1994), and event trees in Bayesian decision theory (Smith and Anderson, 2008).Automata are essentially finite state machines that output (or input) strings of symbols. Probabilistic finite automata (PFA) are automata in which strings are generated in a probabilistic manner (Vidal et al., 2005a,b), and APFA are the subclass of PFA that generate symbol strings of constant length, and so can be regarded as probability models for ordered sequences of discrete random variables.An APFA is represented as a directed multigraph, that is, a directed graph in which there may be multiple edges between node pairs. Two examples are shown in Fig. 2. The graph contains nodes (often called states) and directed edges between them. One node, the root or initial state, has only outgoing edges, and another, the sink or final state, has only incoming edges. All paths from the root to the sink have the same length, sayp. Each edgeeis associated with a probabilityπ(e)and a symbolσ(e). The outgoing edges from each node have distinct symbols and the sum of their probabilities is one. The stage of a node is its distance from the root.The two APFA shown in Fig. 2 are both models for the data in Table 1. The first is derived from the sample tree in Fig. 1 by setting the edge probabilities to be the relative frequencies of the out-edges from each node, and contracting the leaves to a single node (the sink). We call this the sample APFA. The second is selected using an algorithm described below in Section  3.3 which starts off from the sample APFA and then performs a series of simplifications.An APFA serves as a device to randomly generate strings of lengthp, which it does in the following way. Starting from the root, an outgoing edge is chosen at random according to the edge probabilities, the corresponding symbol is generated and the edge is traversed to the next node. This process continues until it reaches the sink. In a statistical context, thepsymbols in an output string are regarded as realizations ofpdiscrete random variables,X1,…,Xp. Then the nodes in the graph correspond to sets of partial outcomes(x1,…,xq)for0≤q≤p, and the edges correspond to transitions between these. For example, in Fig. 2(b), A (the root) represents the null outcome, B representsX1=1, D represents the event(X1,X2)=(1,1)or(2,1), and similarly E represents(X1,X2)=(1,2)or(2,2). The edge from E to F represents the occurrence ofX3=1following E. Thus F represents the event(X1,X2,X3)=(1,2,1)or(2,2,1). The significance of the two incoming edges meeting at D is that the distribution of(X3,X4)given(X1,X2)=(1,1)is identical to that given(X1,X2)=(2,1), being in both cases determined by the probabilities on the edges downstream from D.More formally, letAbe an APFA,pbe the length of the root-to-sink paths inA, and letXbe a discrete randomp-vectorX=(X1,…,Xp), whereXihas sample spaceXi. TheXiare not required to be identical. Given a root-to-sink pathe=(e1,e2,…,ep), we equate the associatedp-vector of symbolsσ(e)=(σ(e1),σ(e2),…,σ(ep))with a realization ofX. Distinct root-to-sink paths generate distinct realizations ofX. The sample space ofXis given byX(A)={σ(e):e∈E(A)}, whereE(A)is the set of root-to-sink paths inA. HereX(A)is some subspace of the product spaceX=∏Xi. For anyx∈X(A)we can find the unique root-to-sink pathesuch thatx=σ(e): we write this ase=σ−1(x). The sample spaceXicorresponds to the set of symbols generated by incoming edges to a stageinode.The edge probabilitiesπ(e)specify the marginal and conditional probabilities appearing in the standard factorization of the joint density ofX(1)Pr(X=x)=Pr(X1=x1)∏i=2…pPr(Xi=xi|X<i=x<i),where here and throughout we use shorthand expressions such asX<i=(X1,…,Xi−1),x≥i=(xi,…,xp),Y≥i;≤j=(Yi,…,Yj)and so forth.By construction, when the process arrives at nodewat stagei, the distribution of the future observationsX>idoes not depend on the path the process took to arrive atw. This implies a conditional independence constraint on the joint distribution ofXthat can be written as(2)X>i⫫X≤i|X≤i∈C(w),whereC(w)={σ(e):e∈P(w)}, andP(w)is the set of paths from the root tow. We might express this loosely by saying that the future is independent of the past in a certain context, namely that the past is in a given set of pasts. We call this a context-specific conditional independence relation. Thus for any node with multiple incoming edges, a conditional independence statement of the form (2) can be read off the graph.In Fig. 2(b), for example, the two incoming blue edges to node E imply that givenX2=2,(X3,X4)is independent ofX1, which we write as(X3,X4)⫫X1|X2=2.No such conditional independence constraints hold in sample APFA, since no nodes in these have multiple incoming edges.Thus an APFA expresses a set of context-specific conditional independence constraints on the distribution ofX, and in this respect it resembles the dependence graph of a traditional graphical model (Lauritzen, 1996; Edwards, 2000).Maximum likelihood estimation in APFA is very straightforward. Suppose thatAis an APFA whose edge probabilitiesπ(e)are unknown, and that independent samplesx(v)=(x1(v),…,xp(v))forv=1…Nare drawn fromA. We wish to estimate theπ(e). We know that for anyx∈X(A), the probability of drawingxisPr(x)=∏i=1…pπ(ei)wheree=σ−1(x), so that the likelihood of the sample is(3)∏v=1…NPr(x(v)∣π)=∏v=1…N∏i=1…pπ(ei(v))(4)=∏e∈E(A)π(e)n(e),wheree(v)=σ−1(x(v)),π={π(e):e∈E(A)}is the parameter vector, andn(e)is the edge count, i.e. the number of observations in the sample whose root-to-sink path traverses the edgee. We similarly define the node countsn(v)to be the number of observations in the sample whose root-to-sink path passes throughv∈V. The likelihood of the model is simply maximized by setting edge probabilitiesπ(e)to be the relative frequencies of the corresponding counts, that is,πˆ(e)=n(e)/n(v), wherevis the source node ofe. The maximized log-likelihood underAis thenℓˆ(A)=∑e∈E(A)n(e)logπˆ(e).Section  3 describes an algorithm in which the sample APFA is simplified in a series of state merging operations. The idea is to merge two nodesvandwat stageiwhenever the sample distributions of the futureX>i, given that the data generating process has passed throughvorw, are similar. The decision may be based on a likelihood ratio test (LRT) of the hypothesis that the corresponding true, unknown distributions are identical. The merging operation is illustrated in Fig. 3(b), which shows the result of merging nodes labelled 3 and 5 in Fig. 3(a). Note that since APFA require that all outgoing edges from each node have distinct symbols, mergingvandwmay require that further node-pairs are merged: in the example, merging nodes 3 and 5 induces the merging of nodes 7 and 10, and of nodes 8 and 11.The state merging operation and corresponding LRTs are studied in detail in Edwards and Ankinakatte (in press). It is shown that the tests are closely related to standard LRTs of independence (G2) in two-way contingency tables. In particular, it is shown that the LRT associated with mergingvandwis equal to the sum of the deviance statistics forv,wand the induced descendent node-pairs. The computation of the deviance and the corresponding degrees of freedom associated with merging nodes 3 and 5 in Fig. 3(a) is illustrated in Table 2.The algorithm proposed by Ron et al. (1998) to select an APFA given a data sample proceeds as follows. The sample APFA is constructed and then simplified in a series of state merging operations. As mentioned above, the idea is to merge two nodesvandwat stageiwhenever the distributions of the futureX>i, given that the process has passed throughvorw, are similar. To assess this (Ron et al., 1998) proposed a dissimilarity score between nodesvandw, writtenδ(v,w), and a fixed threshold,μ. Whenδ(v,w)is small the conditional distributions ofX>i, given that the process has passed throughvandwrespectively, are similar. More precisely,vandware called similar ifδ(v,w)<μ: otherwise they are called dissimilar. Dissimilar nodes are not merged.The algorithm proceeds from stages 1 top−1. At each stage, similar nodes are merged, the process continuing until all the resulting nodes at the stage are pairwise dissimilar: these form a partition of the original nodes at the stage. The algorithm then proceeds to the next stage.The dissimilarity score proposed by Ron et al. (1998) wasδR(v,w)=maxk=i+1…pmaxxi+1,…,k|Prˆ(Xi+1,…,k=xi+1,…,k|X≤i∈C(v))−Prˆ(Xi+1,…,k=xi+1,…,k|X≤i∈C(w))|.Note that the estimated conditional probability differences here take the form(5)|n(e)n(v)−n(f)n(w)|,whereeandfare corresponding descendent edges ofvandwrespectively, that is to say, for which there exists paths fromvandwto their respective targetst(e)andt(f)with the same symbol sequence.In Browning (2006) the same dissimilarity score is used but the algorithm is modified in two ways. In Ron et al. (1998) the order in which nodes within a stage are compared and possibly merged was unspecified. Instead, Browning (2006) describes a greedy approach in which dissimilarities between all pairs of nodes at the stage are computed, and the most similar pair(v,w)is merged. The scores are re-computed as necessary, and the process is repeated until the resulting nodes are pairwise dissimilar. The second modification is to allow the threshold to depend on the nodes countsn(v)andn(w), usingμ(v,w)=n(v)−1+n(w)−1.The rationale is that the variability of (5) depends on the node countsn(v)andn(w), and is greatest when these are small. Consequently when a constant threshold is used, nodes with small counts tend to be judged dissimilar by chance. The proposed threshold is twice the maximum of the asymptotic standard deviation under the null hypothesis. In Browning and Browning (2007a) the threshold was further modified to take the formμ(v,w)=mn(v)−1+n(w)−1+b,wheremandbare scale and shift tuning parameters, respectively. Increasing the threshold results in simpler partitions: since the number of nodes at higher stages is reduced accordingly this also increases the efficiency of the selection process. The valuesm=4andb=0.2were recommended in Browning and Browning (2007a), based on unpublished simulation studies.We propose instead to base model selection on the penalized likelihood criterion(6)IC(A)=−2ℓˆ(A)+αdim(A),wheredim(A)is the number of free parameters underA, andαis a tuning parameter. For the Akaike information criterion (Akaike, 1973),α=2and for the Bayesian information criterion (Schwarz et al., 1978),α=log(N). The criteria are often abbreviated to AIC and BIC, respectively. The latter penalizes parameters more heavily and so selects simpler models. Under suitable regularity conditions, minimizing the BIC is consistent in the sense that for largeNit selects the simplest model consistent with the data (Ripley, 1996, Section 2.6).To do this, we introduce a new dissimilarity score(7)δIC(v,w)=IC(A0)−IC(A)=G2(A,A0)−αk,whereG2is the deviance statistic, given by(8)G2(A,A0)=−2[ℓˆ(A)−ℓˆ(A0)],whereA0is the APFA obtained after mergingvandwinA, andkis the corresponding degrees of freedom. We apply the greedy approach just described and set the thresholdμ=0, so that two nodes are judged to be dissimilar whenever merging them would increaseδIC. Thus the selection algorithm seeks to minimize (6).To illustrate the method we apply it to the data in Table 1. There areN=70observations andp=4variables. The algorithm proceeds from stage 1 top−1. The calculations are shown in Table 3, and the graphs passed through are shown in Fig. 3. We use the BIC tuning parameter,α=log(N). The algorithm starts with the sample APFA (Fig. 3(a)). At stage 1, no merging occurs. At stage 2,δICfor nodes labelled 3 and 5 are negative and minimal, so we merge these nodes to get a single node labelled 3, resulting in Fig. 3(b). The algorithm continues to search for stage 2 nodes to merge, which results in merging nodes 4 and 6, giving Fig. 3(c). At each merging, the corresponding descendent nodes are also merged, and the edge counts are summed. Here the algorithm labels the resultant node with the smaller number among the merged nodes. At stage three, nodes 9 and 7 are merged, and the resulting model is shown in Fig. 3(d).We compare the model selection algorithm described in the previous section with that described by Browning and Browning (2007a) and implemented in Beagle in two ways: firstly, by comparing their rate of convergence as the sample size increases using simulation, and secondly by assessing the goodness-of-fit of the selected model using ten-fold cross-validation. In both cases we use the three data sets described in Section  5.We compare the performance of the proposed model selection algorithm based on penalized likelihood criteria (both AIC and BIC) to that implemented in Beagle. Here we use both the settings suggested in Browning and Browning (2007a) (m=4andb=0.2) and the settings implicit in Browning (2006) (m=1andb=0). We compare the algorithms in respect to the rate at which the selected model converges to the true model as the sample size increases.For various values ofN∗, we takeN∗independent samples from a given APFAA0, using the data generating process described in Section  2. The simulated data sets of varying sizes are then used to build the APFA,A, using the model selection methods under comparison. Then we compute the dissimilarity of the selected modelAto the true modelA0, using two measures: the Kullback–Leibler divergence (KLD), and the Kullback–Leibler increment (KLI) (see Appendix A). This is replicated ten times, and the average KL-divergenceKLD(A,A0)and KL-incrementKLI(A,A0)are reported.This process is performed three times: once for each of the three data sets described in Section  5. The “true” modelA0is constructed by applying the minimal AIC selection procedure described above to the data set. But note that since computation of the KL-divergence is computationally demanding (see Appendix A), only the first 10 variables were used to constructA0for the Biofam and Duroc data sets for the KL-divergence computations. For the KL-increment comparisons, all the variables were used.Suppose we are given an APFAAwith known edge probabilitiesπ(e), and a commensurate data setDof the formx(v)=(x1(v),…,xp(v))forv=1…m. As a measure of how well the modelAfits the data set,D, Thollard et al. (2000) and others suggest using a quantity called the per symbol log likelihood (psLL) for this purpose. It is defined as(9)psLL=−1mp∑e∈E(A)n(e)log(π(e)),wheren(e)is the number of observations inDwhose root-to-sink path inApasses throughe, andπ(e)are the known edge probabilities. Note that since each observation inDpasses throughpedges inA, psLL is the average value of−log(π(e))obtained whenAgeneratesD. Thus psLL is a measure of how wellAfitsD.For ease of interpretation we prefer to use a slightly modified measure we call the mean edge probability (meP) defined as(10)meP=e−psLL=[∏e∈E(A)π(e)n(e)]1mp.This is the geometric mean of theπ(e)obtained whenAgeneratesD. This is easier to interpret since like a probability it lies between 0 and 1: the higher the value, the better the fit.We assess the goodness-of-fit of the models selected by the algorithms as applied to a given data set using 10-fold cross validation. First the data are divided randomly into 10 subsets of approximately the same size. For each data subset we then1.Take the subset as the test data,Take the remaining subsets as the training data,Apply the model selection algorithm to the training data,Compute themePof the selected model as applied to the test data.We report the averagemePover the ten subsets for the different model selection algorithms applied to the data sets described in Section  5.The procedure described in Section  4 was applied to the following three data sets.The mildew data stem from a cross between two isolates of the barley powdery mildew fungus (Christiansen and Giese, 1990). For each ofN=70offspring,p=6binary markers, each corresponding to a single locus, were recorded. One objective of the analysis is to determine the order of the loci along the chromosome. The data were obtained from the experimenters, are analysed in Edwards (1992) and are available from the Comprehensive R Archive Network (CRAN), being supplied along with the package gRapfa.The Duroc SNP data come from a study in which 4239 pigs of the Duroc breed were genotyped using the Illumina Porcine SNP60 BeadChip. In all approximately 60000 single nucleotide polymorphisms (SNPs) were recorded for each pig. The data and its preprocessing are further described in Edwards (2013). The data analysed here consist ofN=4239observations ofp=100SNPs (the first 100 SNPs on chromosome 1). From a statistical point of view, SNPs are trichotomous variables (two homozygotes and a heterozygote).To illustrate application of the methodology to data outside of genetics, we consider the analysis of a social science data set. The Biofam data set is derived from data obtained in a retrospective biographical survey carried out by the Swiss Household Panel in 2002. The data are freely available to the scientific community, and can be downloaded from CRAN as part of the package TraMineR (Gabadinho et al., 2011). They contain sequences of family life states recorded once a year from age 15 to 30 forN=2000individuals born between 1909 and 1972, including only individuals who were at least 30 years old at the time of the survey. Family life state is classified into 8 categories: (i) living with parents, (ii) left home, (iii) married, (iv) left home and married, (v) have children, (vi) left home and have children, (vii) left home, married and have children, and (viii) divorced. In addition, a large number of covariates were recorded. Here for the sake of simplicity we only include sex and religion, the latter coded as ‘catholic’, ‘protestant’ or ‘other’. We combine these into one factor with six levels, i.e. the six combinations of sex and religion, and we organize the data so that this variable is placed prior to the family life state variables.

@&#CONCLUSIONS@&#
