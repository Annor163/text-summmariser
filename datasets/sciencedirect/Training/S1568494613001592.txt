@&#MAIN-TITLE@&#
Free Pattern Search for global optimization

@&#HIGHLIGHTS@&#
We propose a new algorithm called Free Pattern Search.Some solutions are better than the solutions of some other approaches.The convergence speed is much higher than some other approaches.This new algorithm is scalability to the increasing of dimension.

@&#KEYPHRASES@&#
Free Pattern Search,Pattern Search,Free Search,Global optimization,

@&#ABSTRACT@&#
An efficient algorithm named Pattern search (PS) has been used widely in various scientific and engineering fields. However, even though the global convergence of PS has been proved, it does not perform well on more complex and higher dimension problems nowadays. In order to improve the efficiency of PS and obtain a more powerful algorithm for global optimization, a new algorithm named Free Pattern Search (FPS) based on PS and Free Search (FS) is proposed in this paper. FPS inherits the global search from FS and the local search from PS. Two operators have been designed for accelerating the convergence speed and keeping the diversity of population. The acceleration operator inspired by FS uses a self-regular management to classify the population into two groups and accelerates all individuals in the first group, while the throw operator is designed to avoid the reduplicative search of population and keep the diversity. In order to verify the performance of FPS, two famous benchmark instances are conducted for the comparisons between FPS with Particle Swarm Optimization (PSO) variants and Differential Evolution (DE) variants. The results show that FPS obtains better solutions and achieves the higher convergence speed than other algorithms.Pattern SearchFree Searchparticle swarm optimizationFree Pattern SearchHooke and Jeeves Pattern Searchdifferential evolutiondirect search simulated annealingdifferential evolution with biogeography-based optimizationdirected tabu searchcellular particle swarm optimizationintegrated learning particle swarm optimizerlearning-enhanced differential evolutionclustering-based differential evolutionthe pattern matrixthe current point in Pattern Searchthe previous point in Pattern Searchthe base point in Pattern Searchthe search step size of patternthe search step sizes factor of the patternthe reduce factor of patternthe population size of Free Pattern Searchthe number of the dimensionthe numbers of steps for local Pattern Searchthe rate of the detection rangethe jth individual in the populationthe ith element of an individual

@&#INTRODUCTION@&#
Nowadays, the problems become more and more complicated in scientific research, engineering, financial and management field and so on, which creates a demand of more powerful optimizer to solve them within limited time and memory. Over the last decades, nature-inspired algorithms took the responsibility for that and dedicated themselves to the complex problems. Lots of intelligence algorithms had been proposed such as genetic algorithm [1,2] (GA), particle swarm optimization[3] (PSO), differential evolution [4] (DE), human search [5] (HS) and artificial bee colony algorithm [6] (ABC). They show a great potential for global optimization and have been applied to solve various engineering problems [7,8] for their simplification and effectiveness [9].However, almost all of the algorithms suffer from the prematurity. In order to get a higher quality solution and better performance, hybrid algorithms are used for handling this, such as CPSO [10] (hybrid PSO and Cellular automata), DSSA [11] (hybrid SA and Direct search), DE/BBO [12] (hybrid DE and BBO), DTS (hybrid TS and Direct search) [13]. All of these hybrid algorithms have achieved the good performance. However, the real problems become more and more complex, and the benchmarks which are used to test the performance of algorithms also become more and more complex, at the same time the dimensions of benchmarks increase as well, which makes them more difficult to be solved. Even lots of good algorithms have been designed, this road never ends. This research focuses on this topic and a new algorithm called Free Pattern Search (FPS) has been designed for optimization.FPS is inspired by Pattern Search (PS) and Free Search (FS), and it extends PS into a population based formation. PS was firstly proposed by Hooke and Jeeves [14] in 1961. Torczon [15] provided a detailed formal definition of PS. As a kind of direct search, PS does not require gradient information at all, so it shares some similarities with modern evolutionary algorithms. The search step size of PS is very crucial for global convergence of PS and it should be reduced only when no increase or decrease in any one parameter further improved the fit [16], even though PS has been proofed to be a global convergence algorithm [15].Traditional PS did not perform well on the complex and high dimension problems. Hvattum and Glover [17] proposed a new direct search method called SS (Scatter Search) and proved that HJPS (Hooke and Jeeves Pattern Search) was worse than CS (Compass Search) and SS even though the convergence properties of them were similar. Lots of researchers have been dedicated to modify PS for better performances [18–20]. In fact, the empirical convergence is different from theoretical convergence. The former one is the real performance, which has more impacts on the quality of the results.FS [21,22] is the other resource for FPS. Because of the self-regular management, FS which is a population based algorithm has good global search ability [22]. The acceleration part in FPS uses a FS-inspired part, exactly, a self-regular part to classify the population, and then PS take the responsibility to accelerate them. In order to enhance the exploitation phase and keep the diversity of population, FS works with PS to ensure the exploitation and exploration of FPS as well as other operators.The rest of the paper is organized as follows. The next section introduces the traditional HJPS. The detailed description of FPS is presented in Section 3. In Section 4, two sets of experiment are applied to evaluate the performance of FPS. Section 5 is the conclusion and future researches.The traditional HJPS is a single-point search method, which generates a sequence of non-increasing solutions in whole iterations. The “pattern” which is very important is the neighborhood structure of the base point, and the points on the neighborhood structure are called trials. Fig. 1shows the HJPS pattern on 2D. The white points are the trials of the black one (base point), as it can be seen clearly that the trials form a grid and the grid is the neighborhood structure of the base point called “pattern”.Typically, the columns of the pattern are the unit vectors in HJPS and it is denoted by the pattern matrix M as showed below:M=Δ1−Δ100⋅⋅0000Δ2−Δ2⋅⋅00⋅⋅⋅⋅Δi−Δi⋅⋅0000⋅⋅Δn−ΔnIn HJPS, the search step sizes Δ on each dimension are same (Δi=Δj). Actually, there are so many types of pattern in the paradigm of PS, such as canonical PS [14] using square with n×2n pattern, Rank Ordered PS [23] using n×(n+1) pattern, Compass Search [24] using n×2n pattern. The HJPS pattern is a n×2n type and it is simple and effective.There are three kinds of points in HJPS, they are the current point ψ, the base point φ, the previous point θ. The current point is the current solution of the HJPS, the base point is used for detecting better solutions, and then it would be assigned to the current point if it is better. The previous point is the last current point, and it helps the base point to find a profit solution. Initially, the base point is initialized by the current point.The HJPS contains exploration move (EMove, same with the coordinate search) and pattern move (PMove). The EMove completes the coordinate search on all of the dimensions of the base point to find the best trial near it. The step of EMove is as followed:Step 1:Construct the pattern M for the base point φ. Initialize the parameter i=0;Testing iff(φ)>fφ+M:,2ithen φ=φ+M:,2i;Otherwise testing iff(φ)>fφ+M:,2i+1then φ=φ+M:,2i+1.Increment i;If i>n, terminated, otherwise go to Step 2.After the EMove, the PMove would be implemented when the best trial is better than the current point, meaning a better solution has been found. Then the current point ψ and the previous point θ would be upgraded, a new base point φ would be constructed by the current point and the previous point. The formulation of PMove is:θ=ψ,ψ=φ,φ=2φ−θThe flow chart of HJPS is presented in Fig. 2. The search step size of pattern is very important in the PS, and it determines the size of pattern. During the search, it will decrease by κ when no trial is better than the current point.Such as the minimization f(x)=x2, x∈(−0.5, 0.5), and assume that the initialization current position is the point 0.4 (point A) and the search step sizes is 0.1 shown in Fig. 3. Then the pattern matrix would be [0.1, −0.1] indicating that the trials are the points 0.5 and 0.3. Search the pattern of the base point (EMove) and find that 0.3 is the best trial, so the base point is 0.3 (point a) now. From Fig. 3, it can be clearly seen that the profit direction is along the decrease the value of x. Then assign the current position to 0.3 (point B) and move another step toward the profit direction (PMove), and find the new base point 2*0.3−0.4=0.2. EMove again and find that the point 0.1 (point b) is the best trial in this round. Then move the current position to 0.1 (point C) and PMove the base point to 2*0.1−0.3=−0.1 (point c). Repeat above procedure until the base point is not better than the current position, then reduce the search step sizes.This section presents a new algorithm inspired by HJPS and FS called Free Pattern Search. This algorithm integrates HJPS method to be the local search and some operators from FS to keep the diversity by avoiding the individuals’ repetitious search.FPS is a population-based PS-like algorithm. The main parts of FPS consist of initialization, exploration and termination. The most important part is the exploration part including three operators. Search operator aims at searching the local optimum solution, it executes an HJPS based procedure as the local search engine. The acceleration and throw operators are under the premise of the high efficiency of search operator. The acceleration operator is inspired by FS and learned the self-regular management from FS. It works on a pair with two individuals and accelerates the worse one with the help of the better one. The throw operator is used to avoid the individual's repetitious search and to make full use of individuals’ search ability. The pseudo code of FPS is showed in Fig. 4.There are many kinds of initialization methods which contain their own advantages. In this research, the random initialization strategy is selected by FPS in this research because of its uniformity of the initial population. m is the population size and n is the number of dimensions, i=1,2, …, n, j=1,2, …, m, Upiand Lowiare the search space borders, the random initialization is:(1)Xj,i=(Upi−Lowi)×rand(0,1)+LowiFor fairness to the elements in different dimensions, the initial search step sizes used in FPS are different than HJPS. They are proportional to their boundary sizes by the size factor α (as showed in Fig. 1, Δ1≠Δ2 in FPS). For each dimension i, the initial search step size is:(2)Δi,init=(Upi−Lowi)×αDuring the searching process, it should to prevent the infeasible solution and modify them when they appear, and it can be formulated by:(3)xi=Lowiifxi<LowiUpiifxi>UpixiotherwiseThe termination is a hybrid, which combines the max step, max function calls and the accuracy of the best solution. Once one of them is satisfied, the process will be terminated and output the results.The search operator is one of the most important parts in FPS. It executes the HJPS based procedure for all individuals in order to find a local optimum. However, the times of EMove is limited to T, because excessive local search will lead to premature. The search operator is shown in Fig. 5.Upon the completion of the search operator, some individuals may be trapped into local optimum and it is necessary to exchange the solution space information among the whole population, in order to get a better performance including the better solution and convergence speed.The acceleration operator separates the population into two groups. The individuals in the first group will be accelerated. This classification comes from the self-regular management in FS [25], and it uses sensibilities to judge. The sensibility sjis the luck degree to the individual and it is a random number in the fitness interval of current population. It is generated as followed:(4)Fmax=max{f(Xj)}Fmin=min{f(Xj)}sj=Fmin+(Fmax−Fmin)×rand(0,1)When the sensibility of an individual is better than its fitness, this individual will be abandoned because of its bad fitness and luck. Then, it would be classified into the first group. Otherwise its fitness is good enough and should be marked to the second group. For the minimization optimization, better stands for less.(5)Ifsj<f(Xj)Xj∈first groupIfsj≥f(Xj)Xj∈second groupThe classification in FPS is very similar with FS. The only difference is how to deal with the abandoned individuals. FPS accelerates them while FS abandons them. The acceleration is a PMove-like process. It uses the profit direction to accelerate the solution. Each individual in the first group should select a partner in the second group randomly, and then make a pair to the PMove-like process and product a new solution in the profit direction.In the acceleration operator, the individual in the first group is denoted byXj1, and the individual denoted byXr2would be randomly selected in the second group. There is no way to guarantee that the fitness ofXr2is better thanXj1, even though the first group is the abandoned group. The PMove-like process needs the fitness order of individuals. The new solution is the accelerated result ofXj1,Xr2stays unchanged. The acceleration formula is:(6)Xj1=2×Xj1−Xr2iff(Xi1)<f(Xr2)2×Xr2−Xj1otherwiseAfter the acceleration, almost all worst individuals will be accelerated, but the best individual stays unchanged because of its best fitness.During the local search, the individuals would gather, which may cause a reduplicative search in a small space. In order to keep the diversity of population, throw operator detects those reduplications and scatters them.The detection range denoted byD⊂ℝnis a constant vector and it is independent of the population. Actually it is proportional to the initial search step size (Δi,init). ∂ is the scaling factor.(7)Di=Δi,init×∂The distanced⊂ℝnbetween two individuals (X1 and X2) is also a vector, it is the absolute difference of those two individuals:(8)d=absX1−X2When one individual is beyond the detection range of an other individual, it means these two individuals are not reduplicative and they are searching at different places. Otherwise, they will be regarded as identical and need to be scattered in order to make full use of population's search ability. It should be pointed out that di<Difor all dimensions is an essential condition for the equality of two individuals, meaning that:(9)max(di−Di)<0There are two positions needed to be distinct about an individual. The start position is the original position of an individual before the search and accelerations operators, while the current position is the current position of the individual. The start position almost determines the searching result. When two individuals are identical, meaning the current positions of them are too close, but the start positions should be responsible for this.In the throw process, scattering the gathering is realized by moving the worse individual away. Adding or subtracting a Δi,initlength to every dimension of the start position of the worse individual is enough to change the gathering. Then, the worse one will start a new searching round at the new start position with the initial search step size.As showed in Fig. 6, the worse individual and the better individual are gathering. Even the start positions of them are totally far, the current positions of them are gathering. All dimensions of worse individual are moved Δi,initlength away based on the better individual and the new position is denoted by the throw position.The throw position xtwill be accepted, when it was feasible. Otherwise a new feasible position should be generated to replace it. As showed in Fig. 6, the throw position is outside of the feasible domain, then the throw operator would generate a new position xnew, and the relationship between the throw position and the new position is:(10)xinew=Upi−Lowi+xitifxit<Lowi−Upi+Lowi+xitifxit>UpixitotherwiseTo evaluate the performance of FPS, various famous benchmark instances have been used and two famous algorithms have been adopted to compare with the proposed FPS. The first one is the PSO variants, and the other one is the DE variants.There are two sets of testing instances. The first set is conducted for the comparison with PSO variants using the traditional benchmarks, while DE variants are selected for the second set using the first ten functions in CEC 2005 benchmark set [26]. Two lastest PSOs and two lastest DEs are selected for the testing; they are Cellular Particle Swarm Optimization (CPSO) [10], Integrated Learning Particle Swarm Optimizer (ILPSO) [27], Learning-Enhanced Differential Evolution (LeDE) [28] and Clustering-Based Differential Evolution (CDE) [29] respectively. The first testing shows the performance of FPS in the traditional benchmarks. FPS outperforms CPSO in 10D and 30D and also outperforms ILPSO slightly in 10D, 30D and 50D. In the second testing, the first ten functions in CEC 2005 benchmark set are marked as F1–F10. Contrasting to the traditional benchmarks, these functions are more difficult to be solved [30,31] because of the three features, including the shift of global optimum solution from origin, the bias of global optimum from zero and the rotation of function coordinate, making. FPS is dedicated to them and the results show the convergence speed of FPS is much faster than DE variants and the quality of solutions is also comparable with DEs.The default parameters of FPS are listed below:(a)Population size (POP): 10.Search step sizes rate (α): 0.1.Numbers of step (T): 5.The rate of individual domain (∂): 0.5.The reduce factor of pattern (κ): 0.5.The max step of FPS is 500 when compared with PSO variants, while this is replaced by max function evaluations (FEs) when compared with DE variants.This section focuses on the comparison of FPS and PSO variants. Two PSOs are selected from various PSO variant. They both show a good performance on continuous function optimization. One PSO variant is the CPSO proposed by Shi et al. [10]; the other is the ILPSO introduced by Sabat et al. [27]. They are both the newest variants of PSO and achieve better results than other PSO variants. The traditional PSO-w is also used for comparison.In order to make a fair comparison, the benchmarks are chosen from the original literature [13,32,33] and tested by FPS. The comparison was separated, since the benchmarks each algorithm used are not exactly same and the dimensions of the benchmark functions are different too.This section describes the detailed results of CPSO and FPS. CPSO-outer and CPSO-inner are two kinds of CPSO, while the performance of CPSO-outer is better [10]. In order to show the performance of FPS, CPSO-outer (CPSO for short in the rest sections) is selected. PSO-w is also compared with FPS, because it is a traditional PSO variants used widely, and the performance of FPS can be valid with the comparison with PSO-w.Ten well-known functions used as benchmarks are taken from the original literature of CPSO [10]. The dimensions of the benchmark functions were set to 10 and 30. The best, worst and mean optimums were set as the terms for comparison as well as the standard deviation of the results. Table 1represents the result of 10D, and Table 2gives the result of 30D. The successful rates of all algorithms are showed in Table 3.From Table 1, the results show the performance of FPS. CPSO wins 7 functions totally, while FPS scores 6. The performance of CPSO and FPS is almost equal on 10 dimensions as showed in Table 1. There are 3 functions which CPSO and FPS both get the global optimum zero. FPS is good at some multi-modal functions such as Rosenbrock and Schwefel functions, while CPSO is better on the other multi-modal functions like Ackeley function and Nonc-Rasti.When making a comparison of FPS and PSO-w, the result will be deterministic and the scalability of FPS is valid. The result of FPS is much better in most functions in all terms, except FPS losses slightly on Quadric and Ackeley functions.The results of 30 dimensions testing are presented in Table 2. This round CPSO only scores 2 while FPS gets 8 points. That means FPS beats CPSO by 8:2 in 30 dimension functions optimization. It implies that FPS is much more scalable than CPSO with the increase of dimension of functions. CPSO still can get the best solution of Ackeley function and perform better than FPS in all terms of Ackeley function, while FPS gets the best solution of Schwefel function with 0 of STD term. Compared to CPSO, FPS still performs well on most multi-modal functions, and outperforms CPSO.The results of PSO-w support the above points. When compared with PSO-w, FPS only loses on Quadric function, and most of the results of FPS are much better.Table 3 represents the successful rate of each test. The result shows that CPSO totally gets 8 scores and wins in 10 dimensions while FPS wins in 30 dimensions. In fact, CPSO only won 4 functions when the dimension was 30. The successful rate of FPS decreases little when dimensions increases, but in Griewank function the successful rate even increases from 40% to 60%. For most functions, FPS was scalable to the dimension increasing.This section discusses the comparison of ILPSO and FPS. ILPSO modifies the learning strategy of PSO in order to enhance the convergence speed and quality of the search. It achieves good results on some functions. In this research, first nine functions used in the original literature of ILPSO [27] are set as benchmarks. The rotated version of function 10–18 had not been tested now because the original rotate matrix M used in ILPSO cannot be found. It is very vital to the result. This round, mean results and standard deviation of results are studied, and the dimensions are set to 10, 30 and 50. And the result of PSO-w is also selected for the comparison. There are 10 trials for FPS, and the results are listed below.The Table 4presents that the result of FPS and ILPSO in all dimensions. It shows that FPS and ILPSO end in a tie. FPS is better on 4 functions while it lost on the other 4 function. The result indicates that some functions are proper to FPS while some are more fit to ILPSO. During the testing, FPS wins on some unimodal functions like SumSquare function and some multi-modal functions like Schwefel functions; this verifies the performances of both global search and local accuracy of FPS.In this section, ten functions are selected as benchmarks from the special session on real-parameter optimization of 2005 IEEE Congress on Evolutionary Computation (CEC2005) [26]. They are notated as F1–F10 in this research. CEC2005 is a novel function set and almost all functions are shifted and/or rotated, which made it more difficult to be solved by many algorithms. According to Salomon [30], the performance of algorithm loss can be observed by a rotation of the coordinate system of a function.The global optimum of traditional benchmark functions has five shortages. Liang et al. [31] proposed three methods to avoid these shortcomings by shifting the global optimum to a random position, locating the optimum on the bounds and rotating the function using Salomon's method [30]. F1–F5 are unimodal functions and F6–F10 are multi-modal. Among them, F3, F7, F8 and F10 are the rotated version.This section discusses the comparison with FPS and DE variants. Two famous DE variants are chosen for the comparison. One DE variants is Learning-enhanced Differential Evolution (LeDE/exp) while the other one is Clustering-based Differential Evolution (CDE). The DE/rand/1/bin is used for the contrast. The termination of FPS is the max FEs while the dimension is 30 and 50.In this section, a newest DE variant is compared with FPS, which is the LeDE/exp proposed by Cai et al. [28]. The conventional DE algorithm named DE/rand/1/bin is used for controlling to demonstrating the result of FPS and LeDE/exp.The testing was organized into 2 groups. The dimensions of them are set to 30 and 50 respectively. The global optimums and the convergence speed are set as the term for comparison, and the tolerance error was 1E-008. For a fair comparison, the terminal of the algorithms is the max FEs instead of max steps of algorithm. When an algorithm needs more FEs to get the global optimum, it means that it is more consumptive. The max FEs are set to 300,000.The experimental results of the first group are shown in Tables 5 and 6. The results highlighted in boldface represent it is the best results while the one who's interface isbold and italic with underlinesmeans these results are tied to each other. Since the global optimums of these functions are not at the origin, the truncation error forbids getting a very accurate result below 1E-008. The results below 1E-008 will be regarded as the same.The result showed in Table 5, LeDE/exp owns the most of best results which wins 3 best and 4 tie results; this is followed by FPS which has 1 best result and 4 tie results. But if only comparing FPS with LeDE/exp strict, FPS gets slightly better result on F3,F5,F7 and F8 and losses on F4, F6 and F10, which makes FPS slightly win. In fact, FPS totally failed on F4. The result of DE/rand/1/bin was the worst among this testing, but it still gets two best results. The result shows that FPS outperforms DE/rand/1/bin in this round.In Table 6, another view of the results was given. The results present the FEs needed for an algorithm to get 1E-008 accuracy and the successful rate of each algorithm. In this view, FPS shows a great superiority than other algorithms. On function F1, F7 and F9, LeDE/exp and FPS both perform well, but FPS needs less FEs. Especially on F4, the FEs needed for LeDE/exp is almost 4 times than FPS. The rest results in this table show the similar trend. DE/rand/1/bin lost completely when compared with FPS, and FPS outperforms these two variants of DEs.The results of the second group were showed in Tables 7 and 8. The only difference between group 1 and group 2 is dimension. The dimension of group 2 is 50, and the result shows the same result with group 1, which means the effect caused by dimension increasing affects DE/rand/1/bin, LeDE/exp and FPS in a same degree.The Figs. 7–10present the convergence graphs of F1–F10 which are marked as CECf1 to CECf10 in the graphs. Figs. 7 and 8 draws the curves of 30D testing while Figs. 9 and 10 present the 50D testing.This section conducts a comparison between another DE variants and FPS. CDE was proposed by Cai et al. [29] and it hybrid a one-step k-means clustering with Differential Evolution. For a fair contrast, the original DE in was employed too and the result of it is taken from the same literature with CDE.The testing of function of CDE on CEC benchmark functions is very limited, only F1–F4 and F6–F9 these eight functions were tested and the dimensions of them is only 30. The tolerance of error is 1E-008 too. Note that the max EFs of CDE and DE in Table 10 is 500,000, while that is only 300,000 in Table 9, but the max EFs is 300,000 for FPS in all 30D testing. The result shows similar trend that the FEs needed by FPS is less while the quality of solution of FPS is slightly better. FPS performs better on function F3, F7, F8 and losses on F4 and F6 with tie on F1, F2 and F9 when taking the result seriously. The score was 3:2 and this round FPS won by luck. There is a note on Table 10. The numbers with a line in middle means that the function calls exceed 300,000 and actually it is unfair for FPS.

@&#CONCLUSIONS@&#
