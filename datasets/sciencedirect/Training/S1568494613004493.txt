@&#MAIN-TITLE@&#
Ovarian cancer diagnosis using a hybrid intelligent system with simple yet convincing rules

@&#HIGHLIGHTS@&#
This paper proposes a self-organizing neural fuzzy system as a decision support system for ovarian cancer diagnoses.It learns with limited parameters and constraints to derive simple yet convincing inference rules without intervention.Feature selection and attribute reduction are performed during training.Derived rules possess a high level of interpretability.The experimental results are encouraging when benchmarked against other computational intelligence based models.Its automatically derived rules are consistent with expert knowledge.

@&#KEYPHRASES@&#
Ovarian cancer diagnosis,Decision support system,Hybrid intelligent system,Interpretability,Feature selection,

@&#ABSTRACT@&#
Ovarian cancer is the ninth most common cancer among women and ranks fifth in cancer deaths. Statistics show that the five-year survival rate is greater than 75% if diagnosis occurs before the cancer cells have spread to other organs (stage I), but it drops to 20% when the cancer cells have spread to upper abdomen (stage III). Therefore, it is crucial to detect ovarian cancer as early as possible and to correctly identify the stage of the cancer to prevent any further delay of appropriate treatments. In this paper, we propose a novel self-organizing neural fuzzy inference system that functions as a reliable decision support system for ovarian cancer diagnoses. The system only requires a limited number of control parameters and constraints to derive simple yet convincing inference rules without human intervention and expert guidance. Because feature selection and attribute reduction are performed during training, the inference rules possess a great level of interpretability. Experiments are conducted on both established medical data sets and real-world cases collected from hospital. The experimental results of our proposed model in ovarian cancer diagnoses are encouraging because it achieves the most number of correct diagnoses when benchmarked against other computational intelligence based models. More importantly, its automatically derived rules are consistent with expert knowledge.

@&#INTRODUCTION@&#
Ovarian cancer begins with the formation of a tumor in a woman's ovary. It is classified as the epithelial cancer caused by genetic alterations that disrupts the regulation of proliferation, apoptosis, senescence, and DNA repair [1]. Ovarian cancer ranks fifth in cancer deaths among women and accounts for more deaths than any other cancer of the female reproductive system [2]. It is estimated that in United States, 2012, about 22,280 women receive new diagnoses of ovarian cancer and about 15,500 patients die from it [2]. Early detection of ovarian cancer is crucial because statistics [3] show that the five-year survival rate of ovarian cancer is greater than 75% if diagnosis occurs before the cancer cells have spread to other organs (stage I), but it drops to 20% when the cancer cells have spread to upper abdomen (stage III). However, early detection of ovarian cancer is not easy because it is rather hard to tell by the symptoms when a tumor in the ovary turning from benignancy to malignancy. Even annual routine gynecologic and pelvic examinations have only detected 3% of the early stage ovarian cancer cases [4]. Furthermore, the accurate staging of ovarian cancer is also important because the treatment options depend on the type and the stage that ovarian cancer has advanced to [5]. Therefore, early detection and accurate staging of ovarian cancer are of great interests and importance because they are the keys to improve the survival rate of a patient [6].There are a number of research works in literature focused on cancer diagnoses using various approaches, such as statistical methods, clustering techniques, decision trees, neural networks, and neural fuzzy inference systems. Valerio et al. [7] used χ2-test to identify unique protein peaks in pancreatic cancer diagnoses. Lee [8] applied ANOVA to select gene markers in ovarian cancer diagnoses. Generally speaking, statistical approach identifies important features and usually requires other methods or doctor expertise to perform actual cancer diagnoses. Petricoin et al. [9] proposed a self-organizing clustering technique to separate patients with or without prostate cancer. Poon et al. [10] applied a two-way hierarchical clustering algorithm to differentiate hepatocellular carcinoma from chronic liver disease. Generally speaking, classifications based on the clustering results are conceptually equivalent to similarity-based diagnoses, which do not provide concrete decisive statements. Adam et al. [11] developed a classification decision tree to identify better biomarkers for early detection of prostate cancer. Qu et al. [12] further improved the results on the same prostate cancer data set using boosted decision trees. Decision trees are commonly known as comprehensive inference tools, which employ crisp decision rules and perform feature selections. However, decision trees are sensitive to noisy inputs and the boosted decision trees are difficult to interpret although they increase diagnostic accuracies. Ball et al. [13] proposed a three-layered neural network, which employs back propagation learning to analyze mass spectra for the prediction of astroglial tumor grades. Tsai et al. [14] used t-test to cull most of unconcerned genes and then applied the selected oncogenes to a three-layered neural network for ovarian cancer diagnoses. Although neural networks are accurate prediction tools, most of them function as black boxes [15] because users cannot make senses of the reasoning processes. Neural fuzzy inference system (or fuzzy neural network) combines the learning capabilities of neural networks and the transparent properties of fuzzy systems together by performing respective fuzzy or non-fuzzy operations in each layer of the network. Tan et al. [16] proposed neural fuzzy inference systems, which are inspired by the hippocampus and neocortex memory structures to diagnose ovarian cancer. When benchmarked against other models, their experimental results illustrated clear advantages of the proposed neural fuzzy inference systems that achieved higher accuracies and employed smaller number of interpretable fuzzy rules. Tung et al. [17] first used the Monte Carlo evaluative selection method [18] to select important features and subsequently trained the proposed neural fuzzy inference system for ovarian cancer diagnoses. However, their experimental results did not show superior performances even with feature selection applied. In recent years, there were studies focusing on obtaining highly compact fuzzy inference rules [19,20]. The models were able to derive extraordinarily small numbers of rules through the systematic pruning process from both established medical data sets and real-world disease diagnoses. Furthermore, the hybridization of multiple techniques combines the advantages of each individual and alleviates certain limitations. Therefore, hybrid intelligent systems receive increasing attentions in the past few years. For example, evolutionary algorithms are often combined with other techniques to solve optimization problems [21,22] and to find optimal parameters [23]. A recent comparative study is reported in [24]. In this paper, we propose a novel neural fuzzy inference system, which self-organizes its network structure and performs feature selection and attribute reduction during training. Therefore, simple yet convincing fuzzy inference rules are systematically derived to diagnose both established medical data sets and real-world ovarian cancer cases collected from hospital for stage identifications.The rest of this paper is organized as follows: Sections 2 and 3 describe two techniques, respectively, to introduce the foundations of our proposed model. Section 4 provides the details of our proposed clustering method. Section 5 defines the system architecture of our proposed model, which employs the fuzzy rules automatically derived by the proposed clustering method. Section 6 benchmarks the performances of our proposed model on two established medical data sets. Section 7 presents the experimental results of applying our proposed model to diagnose ovarian cancer cases collected from hospital and analytically benchmarks the results against other computational intelligence based models. Section 8 concludes this paper.Rough sets [25] are always compared to fuzzy sets [26]. To describe belongingness, fuzzy sets use straightforwardly defined membership functions, while rough sets use relative relations named the lower and upper approximations. Both theories aim to achieve the same type of goal [27]. However, it is always better to have them both in one system to take advantages of their complements [28].Rough set theory is initiated to model relations in a given data set or knowledge base [25]. It is a formally defined methodology, which can be applied to reduce the dimensionality of a given data set [29], proceeding to train an inference system.To express mathematically how rough set theory is applied in knowledge reduction, decision logic language is used to model the Knowledge Representation System (KRS). Such a system is represented in a pair S=(U, A), where U is a nonempty and finite set named the universe of discourse and A is a nonempty and finite set of primitive attributes.Decision tables can be defined in terms of KRS. If we have S=(U, A) and C, D⊂A, which denote the condition and decision attributes, respectively, then S with distinguished (C, D) pairs is essentially a decision table, i.e. T={U, C, D} or T is a CD-based decision table. Moreover, each element in U is a CD-based decision rule and it represents a cluster of data. Subsequently, U is the union of all clusters found in the data set. A simple example of decision table is given in Table 1to illustrate the concept.Indiscernible relation over knowledge K, named IND(K), is defined in Eq. (1). The family of all equivalence classes of the equivalence relation IND(K) is named U/IND(K).(1)IND(K)={(x,y)∈U2|∀r∈K,r(x)=r(y)}.Rough set theory approximates knowledge using a pair of relational approximations. The lower and upper approximations of a set, when given an equivalence relation IND(K), are defined in Eq. (2). By presenting knowledge K, lower approximationK_Yis the set of elements that can be certainly classified by K and Y and upper approximationK¯Yis the set of elements that can be possibly classified by K and Y.(2a)K_Y=⋃{X:X∈U/IND(K),X⊆Y,Y⊆U},(2b)K¯Y=⋃{X:X∈U/IND(K),X∩Y≠ϕ,X⊆Y,Y⊆U}.Rough set theory performs knowledge reduction with two fundamental concepts named reduct and core. Intuitively, a reduct of knowledge is an essential subset of knowledge suffices to define all basic relations, whereas a core is the most fundamental subset of knowledge consists of the common attributes of all reducts.Given a decision table T={U, C, D}, an attribute a is dispensable if and only if IND(C)=IND(C−{a}). Otherwise, a is indispensable. The family C is independent if ∀a∈C is indispensable in C. Attribute reduction is performed during the process of finding independent C with minimum cardinality. If for all elements in U, an attribute b is dispensable, then b can be removed from the training data set. This process of eliminating attributes that do not contribute any information for inferences is considered as feature selection, which is extremely useful to handle problems with high dimensionality.Based on indispensable relations, reduct and core are defined as follows: Q⊆P is a reduct of P, if Q is independent and IND(P)=IND(Q). The core of P is defined as the common parts of all reducts, i.e. CORE(P)=⋂REDUCT(P). It is easy to infer that P can have many reducts, but only has one core.When a training data set is given, continuous data can be represented with categorical values if separation boundaries in every dimension are set. After removing all dispensable attributes and merging all duplicates, a simplified decision rule base is obtained. This knowledge reduction process is actually the process of finding a reduct of the knowledge representation system constructed from the given data set.Genetic algorithm (GA) [30] is in the field of evolutionary algorithms. The idea of GA is inspired by the survival of the fittest theory proposed by Charles Darwin. GAs are searching algorithms based on the mechanics of natural selection and natural genetics [31]. GA normally starts with a randomly initialized population, which consists of artificial creatures named chromosomes. Based on their fitness values, some of them are selected in pairs as parents and granted the opportunities to produce offspring by means of crossover operators. Subsequently, some chromosomes are randomly selected for mutation, which means their genes are to be varied. The chromosomes in the next generation are expected to perform better and the process goes on iteratively until any termination criterion is met. Although randomized, GA is not a random walk. It efficiently exploits historical information to speculate on new search points with expected improvements [31].Because rough set theory only applies to categorical values, discretization of the training data set is required. GA is employed by our proposed model to search for optimal or at least satisfactory suboptimal separation boundaries in every input dimension. Different strategies that can be applied to each step of GA is not covered in this paper. However, all strategies applied to our proposed model is introduced with details in the following section.Genetic Algorithm based Rough Set Clustering (GARSC) is a clustering technique that integrates genetic algorithm and rough set theory together. Genetic algorithm is applied to determine optimal or at least satisfactory suboptimal solutions. Rough set theory is incorporated to alleviate the curse of dimensionality problem [32], which leads to unnecessarily large network sizes of many established inference systems. By applying rough set approximations, the original knowledge base is greatly reduced without losing essential information. This wonderful characteristic of rough set theory is extremely helpful to improve the interpretability [33] of an existing inference rule base, i.e. reduce the number of features needed for reasoning, the number of rules employed, and the number of arguments stated in each rule. Therefore, the overall proposed system achieves a high level of interpretability without sacrificing accuracy. The overall GARSC process is illustrated in Fig. 1.Please note that up to here the inference rules are crisp decision rules and we need to transform them into fuzzy rules by generating Gaussian type fuzzy membership functions based on the clustering results and assigning corresponding linguistic terms. Subsequently, the transformed fuzzy rules are used to evaluate the performance of the current solution. This knowledge transfer process is illustrated in Fig. 2.This particular process of knowledge transfer from crisp to fuzzy cannot be omitted because the crispness of separation adopted in rough set theory does not tolerant overlapping. Fuzzy membership functions are employed to represent the derived clusters to deal with inexact information and unforeseen circumstances. This kind of knowledge transfer has a great advantage because it naturally prevents the fuzzy membership functions from overlapping or separating too much with adjacent ones, which is another important aspect of interpretability in fuzzy modeling. Furthermore, because clustering is performed in each individual feature, no transformation or normalization is required and more importantly, semantic meanings of the assigned linguistic labels are preserved.Before the detailed introduction of GARSC, there are certain constraints on data discretization to be defined. One is the maximum number of separation boundaries allowed in each dimension. This constraint straightforwardly determines the maximum number of fuzzy membership functions allowed in each dimension. However, the actual number of fuzzy membership functions formulated in each dimension is also determined by the knowledge reduction process. The smallest number of separation boundaries actually in use is zero, which means that particular feature is not considered in the simplified knowledge base. This constraint should not be set to a large value because employing a huge number of fuzzy membership functions degrades interpretability.The other constraint is on the minimum distance between any pair of adjacent separation boundaries in the same dimension, which is termed mindis. This constraint is imposed to make sure that the actual constructed fuzzy membership functions have high level of generalization such that any adjacent pair of them should not be merged into one. The minimum distance constraint mindis is defined in Eq. (3). The max function in the denominator defines the level of generalization.(3)mindisi=ubi−lbimax(nopi,M),where i denotes the ith dimension; ubiand lbidenote the upper and lower boundaries of values in the ith dimension, respectively; nopidenotes the total number of possible separation boundaries in the ith dimension, which if not specifically stated otherwise, is assigned to the total number of different values in the ith dimension where corresponding values in the decision attribute are changed across; and M denotes the user specified minimum number of separation boundaries in every dimension, which has a default value of 10.It has been discussed earlier in this paper that after the removal of all dispensable attributes, the finalized decision table is independent, which is actually one of the many possible reducts of the originally constructed decision table. Therefore, attribute reduction is performed during the process of constructing the independent decision table with minimum cardinality. If an attribute is dispensable in all rules, then that attribute can be removed from the given data set. This process of removing attributes that do not contribute to the essential knowledge base is referred as feature selection.Decision rule reduction is conceptually similar to attribute reduction. Other than merging each set of duplicate rules into one single rule, a decision rule in the rule set is dispensable if and only if the performance of the rule base does not decrease with the removal of that rule. This is also referred as the pruning process of decision rules. Moreover, the removal of inconsistent rules is necessary to maintain the integrity of the knowledge base.Inconsistent rules, which are the rules with the same conditional attributes but different decision attribute, do often exist in real-world applications. Only one rule from each inconsistent rule set should be preserved to remove ambiguities. To determine which rules should be preserved in the simplified decision table, we propose Eq. (4) (in rough set theory terms) to compute the confidence of the kth rule. The min function is applied to penalize information incompatibility.(4)conf(k)=mincard(Ui(ki)∩dk)card(Ui(ki)),∀i∈C,where card denotes the function to compute cardinality; Uidenotes the function to generate a union of decision attributes of every rule in the decision table that shares the same value on the ith attribute; kidenotes the value of the ith attribute of the kth rule; and dkdenotes the decision attribute of the kth rule.Based on the confidence evaluation function, the criteria to remove inconsistence rules from the decision table are defined as follows: (1) the rule with the maximum confidence value will be preserved in the decision table while all the other rules from the same inconsistent rule set will be removed; (2) if multiple rules are tied at the maximum confidence value in the same inconsistent rule set, the rule covers the most number of data elements is preserved; (3) if multiple rules still tie in this situation, a random rule will be selected as the winner with equal probability.In genetic algorithm, the population size defines the total number of chromosomes existing in each generation. Increasing the population size is equivalent to increasing the number of possible solutions. Therefore, more candidates will be examined and a wider range of area in the universe of discourse will be explored.In GARSC, real number coding strategy is employed to construct chromosomes. Each gene used in the chromosome represents a separation boundary in its respective dimension. Because GARSC technique only constrains the maximum number of partitions allowed in each dimension, the actual number of partitions varies, i.e. chromosomes in GARSC have different lengths.Elitism replacement strategy is applied in GARSC. The elitism ratio μ, which is in the interval [0, 1), defines how many percentage of highly fit individuals in the current generation P(t) will be directly brought into the next generation P(t+1). Normally μ is assigned to a relatively small number to prevent the domination of species on local optimums.The overall clustering process stops when genetic algorithm reaches the predefined number of generations, which should sufficiently ensure the convergence of the genetic algorithm.Fitness function evaluates the quality of each chromosome. It is probably the most important component in genetic algorithm, because it directly defines the performance of each chromosome and intimately characterizes the ideal solution that the user attempts to search for. Because the intension of the proposed soft computing model is to produce highly interpretable rules without sacrificing accuracy, fitness function f is defined in Eq. (5), where performance evaluations of both accuracy and interpretability are incorporated. We use capital letters to represent constants and small letters to represent variables. Term-1 and term-5 in Eq. (5) represent the accuracy of the model and the rest three terms represent the interpretability of the model because they are the scores on the number of selected features, the number of derived fuzzy rules, and the total number of arguments in the antecedent part of all rules, respectively. This fitness function is to be minimized by the genetic algorithm.(5)f(x)=τ1(1−a)NODNOF︸1+τ2nofNOF︸2+τ3norNOD︸3+τ4noaNOF·NOD︸4+τ5mseNOF︸5,where x denotes the chromosome to be evaluated; τ1, …, τ5 denote associated coefficient parameters for the user to specify; a denotes the accuracy of applying data set to the model derived using x; NOD is the total number of data elements in the data set; NOF denotes the total number of input features in the data set; nof denotes the number of features actually used to construct the model; nor denotes the number of rules actually used to construct the model; noa denotes the total number of arguments in the antecedent part of all rules in the rule set; and mse denotes mean squared error computation, which is defined in Eq. (6).(6)mse=1NOD∑i=1NOD(yi−yiˆ)2,where yidenotes the predicted value andyiˆdenotes the expected value.We propose five terms to evaluate two different types of performance in the fitness function. The employment of this type of fitness function is one way to implement a multi-objective genetic algorithm [34]. If τ1, …, τ5 are assigned equally to a positive number, the five terms in Eq. (5) are placed in descending order of importance and the proof is given as follows.Term-1 of Eq. (5) can be expanded into Eq. (7).(7)(1−a)NODNOF=1−nocNODNODNOF=NOD−nocNOF,where noc denotes the number of correctly classified data elements.If the number of correctly classified data elements decreases by one, i.e. noc→noc−1, then we can rewrite Eq. (7) into Eq. (8).(8)NOD−(noc−1)NOF=NOD−nocNOF+1NOF.Comparing Eq. (7) to Eq. (8), if all the other terms in Eq. (5) remain unchanged, then fitness value is increased by 1/NOF, which denotes a lesser fit of the chromosome.If term-2 of Eq. (5) changes in the slightest amount, i.e. nof→nof+1, which means one more feature is used to construct the model, then the fitness value is increased by 1/NOF, which also means a lesser fit of the chromosome. The derivation steps are similar to the transformation from Eq. (7) to Eq. (8). Therefore, the derivation steps will not be repeated hereinafter.Up to here, we show that the first two terms in Eq. (5) have the same amount of effect for the slightest changes. In other words, the amount of effect on accuracy is on the same level of feature selection. Comparing to many other neural fuzzy inference systems that only concern accuracy, our proposed model prefers more generalized solutions that utilize lesser amount of inference knowledge if accuracy remains unchanged.If one more rule is employed in the inference knowledge base, i.e. nor→nor+1, then the fitness value is increased by 1/NOD. The number of dimensions in the given data set is normally smaller than the number of data elements (except for gene data), i.e. NOF<NOD⇒(1/NOF)>(1/NOD), which implies that the effect of the slightest change in term-2 of Eq. (5) is greater than that of term-3.If one more argument is employed in the rule set, i.e. noa→noa+1, then the fitness value is increased by 1/NOF·NOD. Because NOF>1 in all real-world problems, the effect of the slightest change in term-3 of Eq. (5) is greater than that of term-4.If all the first four terms of Eq. (5) remain the same, then the classification accuracy and interpretability of the constructed model do not vary. If there is a small change in term-5, then the model is slightly lesser fit for the given data set, which leads to an increase in the mean squared error. By substituting Eq. (6) into term-5 of Eq. (5), we can derive Eq. (9).(9)mseNOF=∑(yi−yiˆ)2NOF·NOD.According to Eq. (9), the change in the sum of squared error has the same level of effect as the change in the number of arguments employed in the rule set because they have the same denominator. The slightest change in sum of squared error is assumed to be smaller than one, if only one predicted value among all the data elements is computed differently without changing the value of term-1 in Eq. (5). Therefore, term-4 takes on a greater effect than that of term-5 in a rather stable solution, where predictions do not vary drastically.Thus, the complete proof has been given that the amount of effect decreases along the five terms of Eq. (5) if their coefficient parameters are set to the same positive value. Term-1, which concerns about accuracy, guarantees the correctness of the constructed model. Term-2, -3, and -4, which concern about interpretability, encourage the constructed model to achieve the same level of accuracy with lesser amount of inference knowledge. According to Occam's razor that all other things being equal, the simplest solution is the best [35], a higher level of interpretability with the same level of accuracy constructs better model in terms of better generalization, which prevents the model from over-fitting. Term-5, which concerns about the mean squared error, is employed to refine the constructed model to achieve a better fitting on the given data set as long as accuracy and the amount of information employed are kept unchanged. The tradeoff between accuracy and interpretability should vary in different application domains and under different user requirements. However, tuning the coefficients, τ1, …, τ5, is sufficient to leverage the balance between accuracy and interpretability.To produce new chromosomes, parents with high fitness values are selected from the current generation to produce offspring in the next generation. Tournament selection strategy [36] is employed in GARSC because we can easily control the selection stress by adjusting only the tournament size m and the selection probability p.For each chromosome to be selected for crossover, m candidates are randomly selected from the current generation for consideration and they are sorted in descending order based on their fitness values. The selection process starts with the first candidate and the probability of selecting the nth candidate p(n) is defined by Eq. (10).(10)p(n)=p(1−p)n−1,0.5<p≤1.If m is large, it is more stressful for lesser fit candidates to be selected. On the other hand, a small tournament size increases the probability of lesser fit candidates being selected as they are competing with a lesser number of the others. In general, a large value of m is used in simple and unimodal application domains to accelerate the converging process and a small value of m is used in complex and multimodal application domains to better explore the universe of discourse [36].In the early generations of GA, p should be set to a small value to give more chances to those lesser fit candidates to get selected. In this way, the search is prevented from premature convergence because more possible candidates are considered even if they have smaller fitness values. However, in the late generations, p should set to a large value because only those highly fit candidates are expected to lead the selection towards the best solution in more in-depth exploitation. Tournament selection probability p in GARSC is defined by Eq. (11).(11)p=0.51+cgiNOG,where cgi denotes the current generation index and NOG denotes the maximum number of generations.Because cgi is in the interval [1, NOG], p is equally distributed in the interval [0.5+(0.5/NOG), 1], which exactly fulfils the constraint defined in Eq. (10).When sufficient number of parents has been selected, the crossover operator is applied to each pair of parents to produce offspring. The probability for a pair of selected parents eventually mate is defined as the crossover rate. However, because elitism replacement strategy is applied in GARSC, the crossover rate is set to one, i.e. every pair of parents is crossovered to produce offspring. Because chromosomes in GARSC consist of real number coded genes to represent sets of separation boundaries that are different in length, there is no simple crossover operator feasible to perform the proposed information exchange between parents.A modified uniform crossover operator is proposed to deal with chromosomes in different lengths. Similar to the conventional uniform crossover operator, a binary string is randomly created to control in each position, from which parent the child should inherit the gene. The length of the control string is assigned to the number of conditional attributes in the given data set. Therefore, there will be no risk taken on the misunderstanding of the dimensionality during the creation process of offspring. The modified uniform crossover operator is illustrated in Fig. 3.Because feature selection is applied in GARSC, many chromosomes have no gene in certain input dimension as indicated with the empty brackets “[]” in Fig. 3. Therefore, when a pair of chromosomes is uniformly crossovered, there is a chance that one of the offspring has no gene in every input dimension. In such case, the “empty” chromosome will be replaced with a randomly constructed non-empty one.Unlike conventional mutation operators that simply vary the values of the selected genes, a set of three different mutation operators is proposed in GARSC. Whenever a gene is selected to be mutated, one of the following three operators are performed with equal probability: (1) add one separation boundary to the gene, if applicable; (2) remove one separation boundary from the gene, if applicable; (3) vary the value of a randomly selected separation boundary in the gene, if applicable.Similar to tournament selection probability p, the mutation rate mrate, which defines the probability for each gene to be mutated, should increase from a small value in the early generations to a large value in the late generations. Based on this policy, mrate is defined in Eq. (12).(12)mrate=1NOF+(NOF−1)·cgiNOF·NOG.Up to here, we have introduced all the details of our proposed Genetic Algorithm based Rough Set Clustering (GARSC) method. The overall GARSC algorithm is summarized in Table 2.The architecture of Genetic Algorithm and Rough Set Incorporated Neural Fuzzy Inference System (GARSINFIS) is illustrated in Fig. 4. GARSINFIS is a six-layered, feed-forward, and partially connected architecture. In each layer, neurons are not connected to each other but only connected to neurons in the adjacent layers.Each layer of GARSINFIS performs respective fuzzy or non-fuzzy operations that are described as follows. Input layer receives the input data vector and translates it into fuzzy singletons in each dimension. Because feature selection is applied, not all given linguistic variables are presented to the connected neurons in the next layer. Condition layer stores respective fuzzy membership functions with respect to each selected linguistic variable and subsequently presents the activation values to the connected neurons in the next layer. Rule-base layer performs fuzzy reasoning and subsequently presents the activation value of the corresponding fuzzy rule to all the neurons in the next layer. Normalization layer scales the activation values of all fuzzy rules to the same level of reference and subsequently presents the normalized values to the connected neuron in the next layer. Consequence layer computes the prediction of each rule and subsequently present it to the only neuron in the next layer. Output layer consists of only one neuron, which aggregates all the inputs received and presents the result as the overall inference output.In Fig. 4, the rectangular boxes used to represent neurons in condition layer and consequence layer denote the antecedent and consequent parts of the employed fuzzy rules, respectively. GARSINFIS employs first-order TSK type of fuzzy rules [37,38], which are defined in Eq. (13).(13)IFx1isA(1,i)and⋯andxnisA(n,i)and⋯andxNisA(N,i),THENyi=α(0,i)+α(1,i)x1+⋯+α(n,i)xn+⋯+α(N,i)xN,where xndenotes the nth input linguistic variable; A(n,i) denotes the fuzzy label defined in the ith rule on the nth input linguistic variable; N denotes the total number of linguistic variables; yidenotes the output of the ith rule; and α(n,i) denotes the coefficient of xnin the ith rule.In GARSINFIS, fuzzy membership functions of Gaussian type are employed. The input function f and output function o of neurons in the condition layer are defined in Eq. (5).(14a)fij=−(xi−cij)22σij2,(14b)oij=exp(fij),where fijdenotes the input of the jth neuron in the ith dimension; xidenotes the ith element of the input data vector; cijdenotes the centroid of the jth cluster in the ith dimension; σijdenotes the standard deviation of the jth cluster in the ith dimension; and oijdenotes the output of the jth neuron in the ith dimension.During the iterative fitness function optimization process, different sets of fuzzy membership functions are evaluated. Therefore, when the clustering process terminates, the derived membership functions of the best solution are empirically proven to be accurate and precise. However, at this time, the derived fuzzy rules are zero-order ones and tuning of their consequent parts is desired if user wants to further increase the accuracy (at the same time decrease interpretability because first-order rules are difficult to comprehend). In GARSINFIS, we apply the Recursive Least Squares (RLS) algorithm, which is an adaptive version of the Kalman Filter (KF) algorithm [39], to estimate the optimal coefficient matrix W*. Assume there are P training samples and let apdenotes the pth row of the weighted input matrix A, then W* can be recursively estimated using Eq. (5).(15a)S0=γI,(15b)W0=[c10⋯cM00⋯0︷MN]T,(15c)Sp=Sp−1−Sp−1apTapSp−11+apSp−1apT,p=1,…,P,(15d)Wp=Wp−1+SpapT(Dp−apWp−1)︸predictionerror,(15e)W*=WP,where γ is a large positive number; I is the identity matrix; cm0 denotes the value of the consequent part of the mth zero-order fuzzy rule; M is the total number of derived fuzzy rules; N is the total number of selected features; Spdenotes the error covariance matrix for the pth input vector; and D is the matrix of desired values.The general function of mapping input X=(x1, …, xi, …, xI) into output y using M number of first-order TSK type of fuzzy rules is defined in Eq. (16).(16)y=∑m=1Mαm(cm0+cm1x1+⋯+cmixi+⋯+cMIxI)∑m=1Mαm,where αmis the firing strength of the mth rule.When the implication operator is selected as the min operator, then αmis defined by Eq. (17).(17)αm=min(μAm1(x1),⋯,μAmi(xi),⋯,μAmI(xI)),whereμAmi(xi)denotes the membership value of xiaccording to the membership function Amiof the mth rule in the ith dimension.Because GARSINFIS directly employs the fuzzy rules derived by Genetic Algorithm based Rough Set Clustering (GARSC) technique, GARSINFIS self-organizes its network structure. Moreover, because GARSC systematically derives simplified fuzzy inference rules, the network size of GARSINFIS is smaller than that of the other similar models. Because feature selection is performed, not all input dimensions in the given data set are utilized. Because the maximum number of fuzzy membership functions in each dimension is constrained by GARSC, the number of neurons employed to represent the derived membership functions is also constrained. Because attribute reduction in the antecedent part of fuzzy rules is performed, hence the derived fuzzy rules may not necessarily employ all the fuzzy membership functions in every selected dimension. Because rule pruning is performed during the clustering process, the number of neurons used to represent the fuzzy rule set is also minimized. The compact GARSINFIS architecture is now ready for performance evaluations.Before moving on to ovarian cancer diagnoses, in this section, we present the experimental results of our proposed model on two established medical data sets [40], namely Pima Indian Diabetes (PID) and Wisconsin Breast Cancer (WBC). In order to benchmark the results against other established neural fuzzy inference systems, training and testing data sets are organized in accordance with [19,20].Pima Indian Diabetes (PID) data set consists of 768 samples, in which 268 (35%) are diagnosed as diabetic and the reminder as healthy. In accordance with [20], in each experiment, 75% of the samples are randomly selected for training and the remainder 25% for testing. The performance of GARSINFIS is taken the average of ten experiments to remove randomness and the same training and testing data sets are applied to C4.5 [41]. The experimental results reported in [19,20] are included as benchmarks because both established models focus on the compactness of the derived rules through systematic pruning. MFMM-FIS [19] constructs hyper-boxes to differentiate data samples of each class in the feature space and subsequently translates them into fuzzy inference rules. After pruning, the expressiveness of the rules is significantly improved because the number of rules is kept extraordinarily small. EGART-FIS [20] applies adaptive resonance theory [42] to incrementally learn associations in the input and output spaces, respectively. Again, after pruning the extracted fuzzy rules, the derived rule size is kept small.There are three common parameters of GARSC to be defined. (1) Elitism ratio of GA is set to 0.1, i.e. in every generation, the top 10% chromosomes with the highest fitness values are directly brought into the next generation. (2) Tournament size is set to two to ease the stress during selection. (3) The maximum number of separation boundaries allowed in any input dimension is set to two. Therefore, the maximum number of fuzzy membership functions in any input dimension is three. This guarantees a great level of interpretability because there are at most three linguistic labels, namely small, medium, and large, assigned in any input dimension.To ensure convergence, the population size of GARSC is set to 100 and the number of generations is set to 10. Because it is difficult to achieve relatively high accuracy on PID data set, GARSINFIS employs first-order TSK type of fuzzy rules and GARSC employs the fitness function to maximize accuracy only (τ1=τ5=1 & τ2=τ3=τ4=0 in Eq. (5)). The benchmarks on PID data set are shown in Table 3.The number of total arguments (noa) listed in Table 3 refers to the accumulated number of arguments of every fuzzy inference rule. E.g., EGART-FIS derives 6 rules and employs all 8 features for PID data set without attribute reduction, the number of total arguments is then yield as 48.In Table 3, it is noticeable that although GARSINFIS achieves the highest accuracy on the PID testing data sets, it constructs the largest inference rule set. Although each rule derived by GARSINFIS only utilizes 130.2/38.1=3.42 arguments on average, the number of employed rules is at least twice larger than those of the other models. One possible reason of why GARSINFIS derives larger inference rule set is that there are missing values (recorded as 0) in the PID data set, which may cause difficulties when GARSINFIS tries to merge inference knowledge. To show GARSINFIS can do better, we apply it to another established data set in the following subsection.Wisconsin Breast Cancer (WBC) data set consists of 699 samples, in which 458 (65.52%) are diagnosed as benign and the remainder as malignant. In accordance with [19,20], in each experiment, 80% of the samples are randomly selected for training and the remainder 20% for testing. The performance of GARSINFIS is taken the average of ten experiments to remove randomness and the same training and testing data sets are applied to C4.5. The results of MFMM-FIS and EGART-FIS are taken from [20]. The parameter values and constraints remain the same as reported in Section 6.1, except that GARSINFIS employs zero-order rules and GARSC employs the fitness function to maximize both accuracy and interpretability (τ1=τ2=τ3=τ4=τ5=1 in Eq. (5)). Because WBC data set is relatively easy to achieve high diagnosis accuracy, we aim to maximize the interpretability of the derived rule set in this subsection. The benchmarks on WBC data set are shown in Table 4.It is shown in Table 4 that although MFMM-FIS and EGART-FIS employ extraordinarily small numbers of fuzzy inference rules, their overall rule sizes (if measured by noa) are not smaller than the one derived by GARSINFIS because they do not perform feature selection and attribute reduction. Furthermore, although C4.5 employs more inference knowledge, it achieves a slightly higher accuracy on the WBC testing data sets than GARSINFIS does. One possible reason of why C4.5 achieves the highest accuracy is that every feature of the WBC data set consists of categorical values, which may favor crisp decision techniques such as C4.5. To show GARSINFIS achieves its best performance when applied to complex and continuous data sets, we apply it to real-world cases collected from hospital in the next section.To illustrate the fuzzy rules systematically derived and optimized by GARSC are easy to comprehend by human, all the eight rules derived in one experiment are shown in Table 5(the numbers shown in the brackets are mean and variance, respectively, to represent corresponding fuzzy membership functions). The shown set of rules achieves 96.35% accuracy on the testing data set and 98.1% on the whole data set. It is obvious to notice from Table 5 that although eight rules are employed (four positive ones and four negative ones), they only utilize seven features (x2 and x3 are excluded) and most of them (seven out of eight) only consist two or three terms in the antecedent part. It is shown that GARSINFIS employs highly interpretable rules to achieve relatively high accuracy.Other than benchmarking the performances of our proposed model on the established medical data sets, the experimental results on the ovarian cancer cases collected from hospital are benchmarked and analyzed in this section.The ovarian cancer diagnoses data set is based on the blood test results of 171 patients collected from Department of Oncology and Gynecology, National University Hospital, Singapore. All samples in the data set are categorized into one of the five groups according to the staging system defined by International Federation of Gynecology and Obstetrics [43]. The distribution of the data set is shown in Table 6. The seriousness of every patient is determined by 28 features that consist of the age of the patient and certain blood test results. The list of all features is given in Table 7.Experiments on the ovarian cancer diagnoses data set evaluate the capability of GARSINFIS to automatically derive human-like reasoning rules and subsequently perform accurate inferences without human intervention and expert guidance. To resemble the diagnosis processes of doctors, a systematic decision support system to diagnose ovarian cancer is constructed and shown in Fig. 5.When a new instance arrives, it is sequentially processed until the diagnoses system reports the final result. In such a way, at each stage, the two important measurements in medical diagnosis, namely sensitivity and specificity, are reported. The two measurements are defined in Eq. (18) and they assess the degree of true positive and true negative, respectively. In ovarian cancer diagnoses, sensitivity is critical because early treatment is crutial and beneficial to the patients and specificity is critical because unnecessary treatment is harmful to the patients both mentally and physically.(18a)Sensitivity=NumberofpositiveinstancescorrectlydiagnosedTotalnumberofpositiveinstances,(18b)Specificity=NumberofnegativeinstancescorrectlydiagnosedTotalnumberofnegativeinstances.There are four diagnoses in the overall system. In each diagnosis, two thirds of randomly selected data samples are used for training and the remaining one third for testing. Furthermore, to remove randomness and show robustness, ten experiments are conducted in each diagnosis. In this way, each diagnosis is evaluated as a threefold cross validation problem and runs for ten times to average the performance.For all diagnoses and all experiments, the three common parameters of GARSC use the same values as introduced in Section 6.1. The population size and the maximum number of generations vary from different diagnoses. Therefore, their values are reported in the corresponding subsections. Diagnosis-1 is the simplest diagnosis to separate normal persons from patients at risks. Therefore, GARSINFIS employs zero-order TSK type of fuzzy rules (for better interpretability) and GARSC employs the fitness function to maximize both accuracy and interpretability (τ1=τ2=τ3=τ4=τ5=1 in Eq. (5)). In the other three diagnoses, GARSINFIS employs first-order TSK type of fuzzy rules and GARSC employs the fitness function to maximize accuracy only (τ1=τ5=1 and τ2=τ3=τ4=0 in Eq. (5)). Please note that although evaluations on interpretability measurements are no longer included in the fitness function, GARSC still performs knowledge reduction (without keeping scores) to optimize the inference rule base for a higher level of interpretability.When benchmarking the experimental results, some well established models such as C4.5 decision tree [41], Naive Bayes classifier [44], Multi-Layer Perceptron (MLP) network [45], Radial Basis Function (RBF) network [45], Adaptive Network-based Fuzzy Inference System (ANFIS) [46] and Dynamic Evolving Neural-Fuzzy Inference System (DENFIS) [47], are used for comparisons. All these models are applied to the same pairs of training and testing data sets that are applied to GARSINFIS. In this way, their performances are fairly compared on the same references.In terms of comparisons on the selected features, only GARSINFIS and C4.5 are discussed, because all the other models utilize all given features without preferences. Even if the associated weights of certain input dimensions can be significantly small in some models, it is still not considered as performing feature selection.In terms of the size of the employed rule set, the actual number of employed rules is recorded for GARSINFIS, ANFIS, and DENFIS. For C4.5, the number of tree leaves is recorded because it is equivalent to the number of crisp decision rules. For Naive Bayes classifier, the number of rules is not applicable. For MLP, the number of hidden neurons, which is systematically determined by the total number of input dimensions and the number of output classes, is recorded. For RBF, the number of radial basis neurons, which is pre-determined by the number of clusters in every output class, is recorded.In the comparison tables, the winner of any particular column is highlighted in bold. The winner is either the largest accuracy value or the least amount of information employed. Because multiple experiments are conducted on each model for each diagnosis, stability (standard deviations) of every model is also recorded and compared. Winners in stability are also highlighted in bold.In this paper, ANFIS utilizes clustering results derived by the employed Fuzzy C-Means (FCM) clustering technique [48]. In this way, ANFIS is more comparable to GARSINFIS because the number of rules employed by ANFIS is greatly reduced. However, extra efforts are taken to carefully select the optimal number of pre-defined clusters through trial-and-error. This is a great disadvantage of ANFIS, because its network structure is not self-organized but has to be pre-defined by employing additional knowledge.The Evolving Clustering Method (ECM) [49] employed by DENFIS constructs clusters based on the distances among data samples in the high-dimensional space. ECM often derives a large number of clusters and as a result, DENFIS employs a large number of rules. Generally speaking, DENFIS often fails to alleviate the curse of dimensionality problem [32].In diagnosis-1, 22 samples are normal and 149 are abnormal. The population size of GARSC is set to 100 and the number of generations is set to 10. Table 8reports the performance of GARSINFIS on ovarian cancer diagnosis-1 data set. On average, GARSINFIS utilizes 1.7 input features and employs 2 rules (1 negative and 1 positive) to achieve 98.95% accuracy on the testing data sets. Each rule employs averagely 2.7/2=1.35 arguments in the antecedent part. The average training time is less than 1.5min.Table 8 presents the astonishing performance of GARSINFIS on ovarian cancer diagnosis-1 data set. All ten experiments achieve 100% specificity, which means all normal patients are correctly diagnosed. Furthermore, all seven experiments that employ two input features achieve 100% sensitivity. The selected features among all ten experiments are highly concentrated that TPS antigen is selected for ten times and CA-125 for seven times.To illustrate the derived simple yet convincing inference rules, fuzzy membership functions and the corresponding rules employed in experiment-1 are shown in Fig. 6and Table 9, respectively.The most simple diagnosis criterion adopted by doctors is that ovarian cancer usually happens in women over age 50, but it is not reliable because more deterministic tests are required to make a definite diagnosis [50]. GARSINFIS automatically discovers this knowledge because the input feature “age” is not utilized in all experiments. “A normal CA-125 level is less than 35. However, this test is not always an accurate way to tell whether a woman has ovarian cancer” [51] and “the combination of TPS and CA-125 will increase the accuracy” [52]. These statements validate the correctness of GARSINFIS in terms of feature selection because only the two doctor suggested features are utilized. Furthermore, in [52,53], the cut-off values for TPS antigen and CA-125 are given as 97U/l and 35mU/ml, respectively. These diagnosis criteria are fulfilled in Fig. 6. Therefore, the rules derived by GARSINFIS are consistent with those doctor adopted ones. This significant finding best illustrates the superior property of GARSINFIS, which systematically discovers intrinsic knowledge embedded in the data set without any prior knowledge or human intervention.Table 10reports the performance comparisons of GARSINFIS against other benchmarking models on ovarian cancer diagnosis-1 data set. Although GARSINFIS does not achieve the highest accuracy and sensitivity, the differences are as small as approximately 0.5 and 0.6 data sample, respectively. However, only GARSINFIS and C4.5 perform feature selection and all the other models utilize all the 28 input features. Although C4.5 utilizes lesser number of input features, its accuracy is lower than that of GARSINFIS. More importantly, among all ten C4.5 decision trees, CA-125 is only utilized three times and TPS antigen is not utilized at all. This implies that although C4.5 utilizes lesser number of input features (1.1 comparing to 1.7 utilized by GARSINFIS) in this ovarian cancer diagnosis-1 application, it performs worse in terms of knowledge discovery.In diagnosis-2, 78 samples are benign and 71 are malignant. The population size of GARSC is set to 100 and the number of generations is set to 20. Table 11reports the performance of GARSINFIS on ovarian cancer diagnosis-2 data set. On average, GARSINFIS utilizes 5 input features and employs 8.5 rules (2.7 negative ones and 5.8 positive ones) to achieve 71.26% accuracy on the testing data sets. Each rule employs averagely 20.8/8.5=2.45 arguments in the antecedent part. The average training time is less than 39.5min.Among all input features, D-dimer is employed in nine experiments. This again proves the capability of GARSINFIS in terms of feature selection because D-dimer is utilized by doctors. In [54], it is stated that “in patients with ovarian cancer before they receive chemotherapy, the level of fibrin degradation products (D-dimer) is correlated with the tumor load.”Table 12reports the performance comparisons of GARSINFIS against other benchmarking models on ovarian cancer diagnosis-2 data set. Comparing Table 12 to Table 10, performances of all models drop significantly. However, this observation is within expectation because diagnosis-2 data set is the most complex one among all four diagnoses. In nature, benign and malignant tumors are easily separable using extra information such as ultrasound image or CT scan. However, these more distinctive features are not included in this data set, which leads to the highest accuracy of 71.26% on the testing data set.GARSINFIS performs best in terms of both accuracy and sensitivity and second best in terms of specificity (the difference is as small as approximately 0.3 data sample). Furthermore, although GARSINFIS utilizes lesser number of input features and employs lesser number of rules, it achieves higher accuracy when comparing to C4.5. Among all models, ANFIS employs the least number of rules. However, it does not perform feature selection and requires extra efforts to determine the number of clusters for the employed FCM clustering technique through trial-and-error.In diagnosis-3, 10 samples are borderline and 61 are beyond borderline. The population size of GARSC is set to 200 and the number of generations is set to 10. Table 13reports the performance of GARSINFIS on ovarian cancer diagnosis-3 data set. On average, GARSINFIS utilizes 2.4 input features and employs 2.7 rules (1.2 negative ones and 1.5 positive ones) to achieve 81.58% accuracy on the testing data sets. Each rule employs averagely 4.1/2.7=1.52 arguments in the antecedent part. The average training time is less than 1.5min.Sensitivity on the testing data set is 90.54% on average because ovarian cancer in late stages is easier to be discovered. Once the cancer grows beyond the borderline size, it will develop rapidly in ovary and may spread to other organs. At that time, the characteristics of ovarian cancer become increasingly distinctive. However, specificity on the testing data set varies drastically from 0% to 100% with the average below 50% because there are not enough data instances. In each experiment, only six or seven instances from the borderline class are available for training and their common characteristics may not be distinctive enough. On the other hand, there are only three or four instances from the borderline class in the testing data set. Both of these lead to the drastic variations of specificities on the testing data set.Among all input features, the most frequently selected feature is PAI activity, which is utilized in five experiments. PAI antigen and CA-125 are both utilized in four experiments, respectively. These features selected by GARSINFIS once again inosculate with the ones that doctor uses. In [55], it is stated that PAI activity and antigen in blood and tumor fluids are examined to determine the seriousness of the tumor in ovary.Table 14reports the performance comparisons of GARSINFIS against other benchmarking models on ovarian cancer diagnosis-3 data set. GARSINFIS performs best in terms of both accuracy and sensitivity and third best in terms of specificity (the difference is as small as approximately 0.6 data sample). Furthermore, although GARSINFIS utilizes lesser number of input features and employs lesser number of rules, it achieves higher accuracy when comparing to C4.5. Among all models, ANFIS employs the least number of rules. However, the credit belongs to the employed FCM clustering technique, which determines the number of clusters through trial-and-error.In diagnosis-4, 19 samples are in early stage and 42 are in late stage. The population size of GARSC is set to 200 and the number of generations is set to 20. Table 15reports the performance of GARSINFIS on ovarian cancer diagnosis-3 data set. On average, GARSINFIS utilizes 2.5 input features and employs 2.7 rules (1.5 negative ones and 1.2 positive ones) to achieve 83.31% accuracy on the testing data sets. Each rule employs averagely 3.9/2.7=1.44 arguments in the antecedent part. The average training time is less than 4min. Among all input features, D-dimer is utilized in all ten experiments and CA-125 is utilized in eight. These two features are the ones suggested by doctors [54].Table 16reports the performance comparisons of GARSINFIS against other benchmarking models on ovarian cancer diagnosis-4 data set. GARSINFIS performs best in terms of both accuracy and sensitivity and second best in terms of specificity (the difference is as small as approximately 0.3 data sample). Furthermore, GARSINFIS utilizes the least number of input features and employs the least number of rules.The features selected by doctors, GARSINFIS, and C4.5 are summarized in Table 17. It is obvious that GARSINFIS performs better than C4.5 in terms of feature selection and more importantly, the features selected by GARSINFIS is more consistent with the ones suggested by doctors.To systematically perform diagnoses on new input data instances, all the four diagnoses are integrated to form one decision support system, which is illustrated in Fig. 5. Table 18summaries the accumulated accuracies of all models and Fig. 7visualizes Table 18 with respect to GARSINFIS.It is shown in Fig. 7 that GARSINFIS is always one of the best performing models and it is stated in the last column of Table 18 that GARSINFIS achieves the most number of correct diagnoses if the testing data set has the same distribution presented in Table 6. In each diagnosis, GARSINFIS utilizes a small number of features that varies from 1.7 to 5 and employs a small number of rules that varies from 2 to 8.5. Therefore, GARSINFIS utilizes only the most essential knowledge to achieve a high level of accuracy. More importantly, it is shown in Table 17 that the rules employed by GARSINFIS are consistent with those suggested by doctors. This proves that GARSINFIS can automatically discover the intrinsic knowledge embedded in the ovarian cancer diagnoses data set without any prior information or human intervention.

@&#CONCLUSIONS@&#
Ovarian cancer diagnosis is an important study because early detection and accurate staging are the keys to significantly increase the survival rate of the patient. In this paper, we propose a novel hybrid intelligent system, which derives simple yet convincing fuzzy inference rules to diagnose ovarian cancer and identify its stage according to the level of seriousness. Our proposed self-organizing model is named Genetic Algorithm and Rough Set Incorporated Neural Fuzzy Inference System (GARSINFIS), which utilizes the inference rule base automatically derived by our proposed Genetic Algorithm based Rough Set Clustering (GARSC) technique. By fusing various soft computing techniques together, we combine the advantages of the individuals and alleviate certain limitations. Before GARSINFIS is applied to the real-world cases collected from hospital, two established medical data sets are benchmarked against other established models, which also focus on the compactness of the derived inference rules. All experimental results are encouraging, especially the ovarian cancer diagnoses. Because GARSINFIS only requires a limited number of control parameters and constraints and no human intervention and expert guidance to achieve the most number of correct diagnoses when benchmarked against other models. More importantly, it automatically selects features and derives rules that are suggested and applied by doctors.Ovarian cancer data sets that consist of DNA microarray gene expressions [8,14,16] and actual images (CT, MRI, ultrasound, etc.) [56–58] besides the blood test results are also studied in literature. However, these information corresponding to the samples in our data set is not available. We will look into the possibilities to obtain an ovarian cancer diagnoses data set with richer distinctive information and we believe at that time our proposed model will increase its accuracy and become more reliable.