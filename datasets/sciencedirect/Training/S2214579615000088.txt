@&#MAIN-TITLE@&#
Hierarchical Collective I/O Scheduling for High-Performance Computing

@&#HIGHLIGHTS@&#
Hierarchical I/O scheduling for two phase collective I/O.In-depth cost analysis of collective I/O.A model to predict the shuffle cost.Implementation in MPI-IO and PVFS file systems.Concurrent applications' cost analysis and comparison.

@&#KEYPHRASES@&#
Collective I/O,Scheduling,High-performance computing,Big data,Data intensive computing,

@&#ABSTRACT@&#
The non-contiguous access pattern of many scientific applications results in a large number of I/O requests, which can seriously limit the data-access performance. Collective I/O has been widely used to address this issue. However, the performance of collective I/O could be dramatically degraded in today's high-performance computing systems due to the increasing shuffle cost caused by highly concurrent data accesses. This situation tends to be even worse as many applications become more and more data intensive. Previous research has primarily focused on optimizing I/O access cost in collective I/O but largely ignored the shuffle cost involved. Previous works assume that the lowest average response time leads to the best QoS and performance, while that is not always true for collective requests when considering the additional shuffle cost. In this study, we propose a new hierarchical I/O scheduling (HIO) algorithm to address the increasing shuffle cost in collective I/O. The fundamental idea is to schedule applications' I/O requests based on a shuffle cost analysis to achieve the optimal overall performance, instead of achieving optimal I/O accesses only. The algorithm is currently evaluated with the MPICH3 and PVFS2. Both theoretical analysis and experimental tests show that the proposed hierarchical I/O scheduling has a potential in addressing the degraded performance issue of collective I/O with highly concurrent accesses.

@&#INTRODUCTION@&#
The volume of data collected from instruments and simulations for scientific discovery and innovations keeps increasing rapidly. For example, the Global Cloud Resolving Model (GCRM) project [1], part of DOE's Scientific Discovery through Advanced Computing (SciDAC) program, is built on a geodesic grid that consists of more than 100 million hexagonal columns with 128 levels per column. These 128 levels will cover a layer of 50 kilometers of atmosphere up from the surface of the earth. For each of these grid cells, scientists need to store, analyze, and predict parameters like the wind speed, temperature, pressure, etc. Most of these global atmospheric models process data in a 100-kilometer scale (the distance on the ground); however, scientists desire higher resolution and finer granularity, which can lead to significant larger sizes of datasets. Table 1shows the data requirements of representative scientific applications run at Argonne Leadership Computing Facility (ALCF) through the DOE's INCITE program [34]. The data volume processed online by many applications has exceeded TBs or even tens of TBs; the off-line data is near PBs of scale.During the retrieval and analysis of the large volume of datasets on high-performance computing (HPC) systems, scientific applications generate huge amounts of non-contiguous requests [27,38], e.g., accessing the 2-D planes in a 4-D climate dataset. Those non-contiguous requests can be considerably optimized by performing a two-phase collective I/O [11]. However, the performance of the collective I/O could be dramatically degraded when solving big data problems on a highly-concurrent HPC system [12,30]. A critical reason is that the increasing shuffle cost of collective requests can dominate the performance. This increasing shuffle cost is due to the high concurrency caused by intensive data movement and concurrent applications in today's HPC system. The shuffle phase is the second phase of a two-phase collective I/O. A collective I/O will not finish until the shuffle phase is done. Previous research has primarily focused on the optimization of the other phase, the I/O phase, of a collective I/O for data-intensive applications. In this study, instead of only considering the service time during the I/O phase, we argue that a better scheduling algorithm in collective I/O should also consider the requests' shuffle costs on compute nodes. An aggregator who has the longest shuffle time can dominate an application's overall performance, due to the reason that the slowest aggregator actually determines the overall performance of a collective I/O. In this research, we propose a new hierarchical I/O (HIO) scheduling to address this issue. The basic idea is, by saturating the aggregators' ‘acceptable delay’, the algorithm schedules each application's slowest aggregator earlier. The proposed algorithm is named as hierarchical I/O scheduling, because the predicted shuffle cost is considered at the MPI-IO layer on compute nodes and the server-side file system layer. Both layers leverage the shuffle cost analysis to perform an improved scheduling for collective I/O. The current analyses and experimental tests have confirmed the improvements over existing approaches. The proposed hierarchical I/O scheduling has a potential in addressing the degraded performance issue of collective I/O with highly concurrent accesses.The contribution of this research is three-fold. First, we propose an idea of scheduling collective I/O requests with considering the shuffle cost. Second, we have derived functions to calculate and predict the shuffle cost. Third, we have carried out theoretical analyses and experimental tests to verify the efficiency of the proposed hierarchical I/O (HIO) scheduling. The results have confirmed that the HIO approach is promising in improving data accesses for high-performance computing. This work is an extension of our previous work [25]. The major difference is we generalize the HIO idea to a broader view, in which not only collective read but also write operation scheduling is designed, analyzed, and evaluated. The second difference is we add more evaluation results to demonstrate the potential of HIO. The third improvement is the Time Window concept. In the previous work, we only discussed how to use HIO to perform the scheduling on the queuing I/O requests, but we did not consider the starvation and interruption of aggregators within the same application, in other words, the aggregators from same application's different instance could also mess up with each other. We address the problem in this paper by applying a flexible time window concept. Besides, the shuffle cost prediction and HIO implementation are also extended a lot.The rest of this paper is organized as follows. Section 2 reviews collective I/O and motivates this study by analyzing a typical example of interrupted collective read. Section 3 introduces the HIO scheduling algorithm. Section 4 presents the theoretical analysis of the HIO scheduling. Section 5 discusses the implementation. The experimental results are discussed in Section 6. Section 7 discusses related work and compares them with this study. Section 8 summarizes this study and discusses future work.

@&#CONCLUSIONS@&#
