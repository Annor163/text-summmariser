@&#MAIN-TITLE@&#
GroupTracker: Video tracking system for multiple animals under severe occlusion

@&#HIGHLIGHTS@&#
We developed “GroupTracker”, a new multiple animal tracking system.This system accurately tracks individuals even under severe occlusion.We devised an EM scheme with constraints on the eigenvalues of the covariance matrix.We aim at revealing behavioral patterns behind animal interactions using our software.

@&#KEYPHRASES@&#
Bioimage informatics,Computational ethology,Animal tracking,

@&#ABSTRACT@&#
Quantitative analysis of behaviors shown by interacting multiple animals can provide a key for revealing high-order functions of their nervous systems. To resolve these complex behaviors, a video tracking system that preserves individual identity even under severe overlap in positions, i.e., occlusion, is needed. We developed GroupTracker, a multiple animal tracking system that accurately tracks individuals even under severe occlusion. As maximum likelihood estimation of Gaussian mixture model whose components can severely overlap is theoretically an ill-posed problem, we devised an expectation–maximization scheme with additional constraints on the eigenvalues of the covariance matrix of the mixture components. Our system was shown to accurately track multiple medaka (Oryzias latipes) which freely swim around in three dimensions and frequently overlap each other. As an accurate multiple animal tracking system, GroupTracker will contribute to revealing unexplored structures and patterns behind animal interactions. The Java source code of GroupTracker is available at https://sites.google.com/site/fukunagatsu/software/group-tracker.

@&#INTRODUCTION@&#
Animals display a wide range of complex behaviors. Among them, behaviors shown by interacting multiple animals are particularly interesting. Video analyses of those behaviors can reveal high-order functions of their nervous systems (Silverman et al., 2010; Ofstad et al., 2011). Furthermore, advancements in quantitative behavioral measurements have broad applicability in studies of neurological disease, by enabling systematic isolation of mutant strains of model organisms and objective analyses of their phenotypes on inter-individual interactions (Penagarikano et al., 2011).In this study, we focus on tracking of multiple animal individuals that have similar appearances. While this is a general framework for analyzing a group of individuals of the same species, the issue of identity preservation during tracking is not trivial. In particular, this problem becomes serious when individuals severely overlap each other in image frames, i.e., under severe occlusion. Such occlusion occurs not only under typical experimental settings but also during interesting inter-individual behaviors such as courtships (Ono and Uematsu, 1957). Thus, most studies so far required laborious manual annotations of identities and positions of individuals, and the ability to perform large-scale systematic analyses is greatly inhibited.To date, a number of video tracking systems have been developed, for example, for nematodes (Baek et al., 2002; Ramot et al., 2008; Swierczek et al., 2011), mice (de Chaumont et al., 2012; Giancardo et al., 2013; Ohayon et al., 2013), fruit flies (Dankert et al., 2009; Branson et al., 2009)), ants (Khan et al., 2005), and fish (Kato et al., 2004; Mirat et al., 2013), although none focused on solving the severe occlusion problem. Some developed methods for single individual tracking (e.g., Baek et al., 2002), dealt with cases with few occlusions (e.g., Delcourt et al., 2009), or physically prohibited occlusions (e.g., Mirat et al., 2013). This was partly because frequently adopted methods, such as mathematical morphology (Kato et al., 2004) and watershed algorithm (Giancardo et al., 2013), had difficulties in separating severely overlapping individuals.Recently, the Gaussian mixture model has been adopted by several multiple animal tracking methods, where animal individuals are represented by components of a Gaussian mixture (Dankert et al., 2009; Branson et al., 2009; Tsai and Huang, 2012; Ohayon et al., 2013). Through this approach, latent variables such as true positions of individuals are explicitly represented. The associated probability models and numerical methods are also well-established. Although a Gaussian distribution cannot represent, for example, bending shapes of a nematode, it has been successfully applied to many animals such as mice and fruit flies (Dankert et al., 2009; Ohayon et al., 2013). Nevertheless, methods adopting the Gaussian mixture model also suffer from the severe occlusion problem, because the maximum likelihood estimation of the Gaussian mixture model is theoretically an ill-posed problem under the condition where multiple components can overlap (Bishop and Nasrabadi, 2006).In this study, we developed a Gaussian mixture model-based, multiple animal tracking system that accurately tracks individuals even under severe occlusion. The key idea was the introduction of constraints to the eigenvalues of the covariance matrices of the Gaussian mixture components, by taking advantage of the fact that the size of each individual usually remains almost constant during a video sequence. We developed algorithm that effectively estimates the Gaussian mixture parameters under these additional constraints, and implemented a publicly available software tool named ‘GroupTracker’ (GROUP: Gaussian Reinterpretation of OcclUsion Problem).Medaka fish (Oryzias latipes) was selected for demonstrative purposes in this study. As fish swim around in three dimensions and frequently overlap each other, they are suitable for evaluating multiple animal tracking system under occlusion conditions. It should be noted that, partly because of these characteristics, tracking systems for fish are underdeveloped compared with those for other organisms (Delcourt et al., 2013). Furthermore, medaka fish has been used as a model organism in many fields of animal sciences. It shows various interesting behaviors that involve inter-individual interactions such as schooling and aggressive behaviors (Magnuson, 1962), while rich resources are available for its neurobiology and genomics (Anken and Bourrat, 1998; Kasahara et al., 2007; Okuyama et al., 2014).Five ten-minute video sequences that recorded one, two, four, eight, and sixteen individuals were prepared. Medaka fish (Hd-rR strain) were hatched and bred in laboratory aquariums. In each case, equal numbers of female and male individuals (one female in the case of one individual) at six months of age (adult, body lengths ≈ 3cm) were transferred to a white, opaque, cylindrical ring-shaped, plastic water tank (outer radius=46cm, inner radius=24cm, depth≈4cm, water temperature=26°C; Fig. 1A). This shape of the tank enhanced the schooling behavior of medaka. A white polarized LED lamp (10.7cm×22.5cm) located above the tank was used as the light source during video recording (Supplementary Fig. 1). A high-definition digital video recorder (HDR-HC9 Sony Corp., Japan) was set approximately 140cm above the water surface. A polarizing filter (VF-37CPKS, Sony Corp., Japan) was used to reduce light reflection. Videos were recorded in eleven-minute sequences during daytime (from 2pm to 5pm) using default video settings. Blackout curtain was set up surrounding the entire apparatus to prevent external (human) interference. Final Cut Pro (Apple Inc., U.S.A.) was used to convert the videos into the Motion JPEG format (frames per second=30, resolution of the image frames=872×480). The first one-minute segment was deleted from each video sequence.The method consists of three major steps: preprocessing, tracking, and post-processing. At the preprocessing step, objects outside of the movable areas (i.e., outside of the water boundary in case of fish) are removed and pixels composing the animal shapes are extracted from every image frame using conventional image-processing methods (Gonzalez and Woods, 2007) (Fig. 2A). Then, the tracking step determines the precise position of each individual by fitting the Gaussian mixture model to the preprocessed image frames (Fig. 2B). The post-processing step consists of three minor steps: identity-swapping alert, identity-swapping correction, and head-direction determination. At the identity-swapping alert step, the system alerts the user to image frames that may contain identity-swapping errors. The identity-swapping correction step then automatically correct a portion of these errors. Finally, at the head-direction determination step, the direction of the head of each individual is determined in each image frame.At this step, first, every image frame in the video sequence is converted to 8-bit grayscale (into the 0–255 range from dark to bright by the NTSC conversion) and, to remove light reflection, any values higher than the threshold value of 100 is set to this value. Next, dynamic threshold binarization and statistical background subtraction are conducted to select pixels that likely constitute animal shapes. The former technique selects every pixel whose brightness value is lower than a dynamic threshold that is the average brightness value of the surrounding pixels (5×5 square pixels) plus or minus a user-defined value. Because medaka's body were darker than the surrounding environment, the user-defined value was set to −5. The latter technique selects every pixel whose brightness value is lower than a static threshold calculated as follows. Thirty image frames are collected at even intervals from the entire video sequence and, for each pixel coordinates, the mean μ and variance σ of the brightness values are calculated. The static threshold is then set to μ−2σ. Common pixels selected by both techniques are obtained and a median filter is applied to remove noises. Finally, the remaining pixel set is passed on to the tracking step.At this step, the two-dimensional Gaussian mixture model is applied to the preprocessed images (Fig. 3A and B) using the same number of mixture components as that of animal individuals. Hence, the mean value and covariance matrix of each component represent the position and shape of each individual, respectively.First, the system processes the first image frame. K-means++ algorithm (Lloyd, 1982; Arthur and Vassilvitskii, 2007) is applied to divide the pixels identified during the preprocessing step into K clusters, where K is the number of individuals. Because K-means++ algorithm can converge to local optima, the clustering process is repeated R=100 times and the result with the smallest K-distance calculated as follows is chosen.K−distance=∑k=1K∑x∈Ck1|Ck|(d(x,ck))2where x is a pixel coordinate, Ckis a cluster, ckis the coordinate of its centroid, and d(·, ·) is the Euclidean distance. Then, the mean value μkand the covariance matrix Σkof each mixture component are set to ckand K−distance×0.1×I, where I is the identity matrix, respectively. The mixture ratio of each component πkis set to 1/K.Then, for each successive image frame, the parameters of the Gaussian mixture distributions are estimated by the expectation–maximization (EM) algorithm (Bishop and Nasrabadi, 2006) using the parameter estimate of the previous frame as the initial values. This relies on an assumption that the position and shape of an individual do not change abruptly between adjacent frames, which is generally true when the number of frames per unit time is sufficiently large. It should be noted that this approach naturally preserves the identities of individuals in most cases.In its original formulation, the EM algorithm described is as follows (Bishop and Nasrabadi, 2006). The log-likelihood function is defined as:lnp(X|π,μ,Σ)=∑n=1Nln∑k=1KπkN(xn|μk,Σk)where N is the number of pixels determined during the preprocessing step of each image frame andNis the Gaussian probability density function. The E step calculatesγ(znk)=πkN(xn|μk,Σk)∑lπlN(xn|μl,Σl), where znkindicates whether xnbelongs to the mixture component k and γ(znk) represents ‘responsibility’ that the mixture component k explains the observation xn. Then, the M step updates the parameters using γ(znk). The E and M steps are repeated until the likelihood function converges to a local maximum.Nevertheless, this EM algorithm could not be applied to the current problem because the maximum likelihood estimation of the Gaussian mixture model is intrinsically an ill-posed problem if any two components can severely overlap (Bishop and Nasrabadi, 2006) (Fig. 3C). In this case, a Gaussian mixture component can collapse to a single pixel x and the likelihood function can contain the termN(x|x,Σ)=(2π|Σ|12)−1, which diverges to infinity as |Σ|→0.Therefore, we developed a novel algorithm that overcomes this limitation. The key idea was to fix the eigenvalues of Σksince they represent the sizes of the individuals, which can be considered constant during a video sequence (Fig. 3D). If the eigenvalues are fixed, a Gaussian mixture component cannot collapse to a single pixel and |Σ| cannot approach 0. First, the original EM algorithm described above is applied to the first image frame and the eigenvalues of Σkare calculated. This requires that all animal individuals do not overlap in the first frame, though it is trivial to choose any frame that fulfills this condition in a video sequence. Then, the adapted EM algorithm that maximizes the likelihood function while fixing the eigenvalues is applied to the first and subsequent frames, using the eigenvalues calculated above as input. Note that the likelihood function does not change even if the eigenvalues are fixed; in other words, only the M step needs to be revised. Since the covariance matrix of a Gaussian distribution is a real symmetric matrix, we can choose the eigenvectors that form an orthonormal set (Bishop and Nasrabadi, 2006). Given eigenvalues λikand eigenvectors uik, the covariance matrix is written byΣk=λ1ku1ku1kT+λ2ku2ku2kT=λ1kcos2θk+λ2ksin2θk(λ1k−λ2k)sinθkcosθk(λ1k−λ2k)sinθkcosθkλ1ksin2θk+λ2kcos2θkNote that we can set u1kto (cosθk, sinθk)T and u2kto (−sinθk, cosθk)T (0≤θk<π), where θk∈[0, π) is the angle of the major axis of the Gaussian component.The log-likelihood function can also be represented by using θk, λ1k, and λ2k. By calculating its partial derivatives with respect to θkand setting it to zero, we obtain the following equation:∑n=1Nγ(znk)λ2k−λ1kλ1kλ2k×12(a1nk2−a2nk2)sin2θk−a1nka2nkcos2θk=0where (a1nk, a2nk) is (xn−μk)T. The solution of this equation is given byif∑n=1Nγ(znk)(a1nk2−a2nk2)=0⇒θk=π4and3π4otherwiseθk′=12arctan2∑γ(znk)a1nka2nk∑γ(znk)(a1nk2−a2nk2)θk=θk′+π2andθk′+π(θk′<0)θk′andθk′+π2(θk′≥0)The two possible solutions represent the local maximum and local minimum. By selecting the one whose second order differential is negative, the solution for the local maximum is obtained and passed on to the next iteration of the EM algorithm.When it comes to real datasets, animal individuals sometimes move too fast and the solutions to the EM algorithm from the previous frame could become inappropriate as the initial parameters. These ‘loss of individual’ events are detected by calculating the likelihood function for a mixture component k with the initial parameter values μkand Σk. If the calculated likelihood is less than a threshold α, a round of K-means++ algorithm is performed by fixing the parameters of all other components, and μkand Σkare updated as described earlier. On the other hand, if no ‘loss of individual’ events are detected, noise reduction is then conducted where any pixel whose likelihood, according to the initial parameters, lies below a threshold β is regarded as noise and removed. In the current implementation, α=β=10−15.As described earlier, the tracking step preserves the identity of each individual across frames in most cases; however, identity-swapping errors may occur at frames that contain occlusion. This step alerts the user to them.First, for each pixel xnin each frame, this step finds k1, k2∈{k|1≤k≤K} that constitute the largest and second largest values of γ(znk), i.e., the top two mixture components that best explain xn. A γ(znk1) value less than a threshold a=0.7 indicates that these components get close in that frame. In this case, a combination of the frame number, k1, and k2 are recorded. A series of successive frames allowing at most one-frame gaps with the same recorded component pair (k1, k2) are then grouped into an “incident”. Incidents spanning less than a threshold b=5 frames are discarded to exclude potential false positives. In addition, the differences between angles θk1 and θk2 of the two recorded components are calculated for all frames within an incident and, if the minimum difference is larger than a threshold c=π/6, that incident is discarded. This is because large angle differences result in large Kullback-Leibler divergences between the mixture components that prevent identity-swapping errors. Finally, the remaining incidents are presented to the user as possible cases of identity-swapping errors.Not every identity-swapping error can be corrected completely automatically. This step aims at correcting errors by detecting unnatural sudden changes in directions or speed of each individual's movement.Given a user-defined value t0 (default t0=10) and an incident beginning at frame number ffirst and ending at frame number flast, the values of μkfor the recorded components k1 and k2 at frames (ffirst−t0), (1/2)(ffirst+flast), and (flast+t0) are extracted. For simplicity, we defined them as μk_pre, μk_mid, and μk_post, respectively. To look for sudden changes in directions, the angle formed by μk1_pre, μk1_mid, and μk1_post and the angle formed by μk2_pre, μk2_mid, and μk2_post are examined. If both angles are smaller than a threshold d=π/2, the incident is judged as an identity-swapping error and corrected by re-swapping the identities mapped to the two components starting from the frames of the incident (Supplementary Fig. 2A). To detect sudden changes in speed, we first define the distances d1pre, d1post, d2pre, and d2post as |μk1_pre−μk1_mid|, |μk1_mid−μk1_post|, |μk2_pre−μk2_mid|, and |μk2_mid−μk2_post|, respectively. Then, if dscore=|d1pre−d1post|+|d2pre−d2post|−|d1pre−d2post|−|d2pre−d1post| is greater than a threshold e=20 pixels, the incident is judged as an identity-swapping error and corrected in the same manner (Supplementary Fig. 2B).At the tracking step, we introduced θk∈[0, π), which represents the angle of the major axis of the Gaussian component representing individual k. The upper limit was π instead of 2π, because the covariance matrices are diagonal and did not discriminate between the head and tail of an individual. At this step, the head directions of the individuals are explicitly determined and θkis updated to be in the range [0, 2π).First, because the head direction of the individual k does not abruptly change between successive image frames, frames are grouped if the differences between their θkvalues are less than π/4 (or greater than 3π/4). Note that, if the individual k does not overlap with any other individuals during the entire video sequence, all frames usually formed a single group. This process is repeated for each individual. For each frame f in the frame group for individual k, the velocity vk(f) is obtained as the difference vector between μkat frames f−t0 and f+t0. At the frame fmax where |vk(fmax)| is maximized, the movement of individual k is assumed to be its head direction. Thus, if the difference between the angle of vk(fmax) and θkat fmax is greater than π/2, to the value of θkat fmax is updated to θk+ π. Finally, π is added to θkat any frame so that the differences between θkfrom adjacent frames are always less than π/4 (or greater than 7π/4).

@&#CONCLUSIONS@&#
