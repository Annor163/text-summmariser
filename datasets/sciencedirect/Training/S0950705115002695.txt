@&#MAIN-TITLE@&#
On improving parsing with automatically acquired semantic classes

@&#HIGHLIGHTS@&#
80% of parsing mistakes in appositions are due to a lack of semantic information.We automatically gather evidence on class-instance semantic compatibility from text.Classes are common nouns; instances are entities characterized by name and type.Our best model uses both sources of evidence with smoothed conditional probability.Experiments reach 91.4% accuracy, a 12.9% relative improvement over the baseline.

@&#KEYPHRASES@&#
Apposition parsing,Semantic class extraction,Unsupervised knowledge acquisition,

@&#ABSTRACT@&#
Parsing mistakes impose an upper bound in performance on many information extraction systems. In particular, syntactic errors detecting appositive structures limit the system’s ability to capture class-instance relations automatically from texts. The article presents a method that considers semantic information to correct appositive structures given by a parser.First, we build automatically a background knowledge base from a reference collection, capturing evidence of semantic compatibility among classes and instances. Then, we evaluate three different probabilistic-based measures to identify the correct dependence on ambiguous appositive structures.Results reach a 91.4% of correct appositions which is a relative improvement of 12.9% with respect to the best baseline (80.9%) given by a state of the art parser.

@&#INTRODUCTION@&#
Information extraction systems typically preprocess text through a syntactic parser. However this step is an important bottleneck because parser errors are hard to solve or even identify at the extraction phase.We can see in literature that parsers achieve overall performances over 90%, but it is well known that their performance drop in highly ambiguous dependencies such as PP attachments [1] or long distance dependencies [2].Here we focus on appositions, which are grammatical constructions with two contiguous noun phrases where one defines or modifies the other. They are one of the most useful and productive dependence given by parsers because they are frequently used to denote the semantic class of a particular instance [3]. We develop a method to improve the apposition dependence detection that impact the retrieval of instance-class relations.Consider the following example:David supports the team of hiswife,Julia.2We denote in bold the named entity and underline the candidate common nouns.2Dependency parsers often determine that there is an apposition between the common noun wife and the named entity Julia, leading to interpret that Julia belongs to the class wife. People would accept this interpretation using their background knowledge where Julia is a common name for a female person. Now consider the following example with exactly the same syntactic structure:David supports theteamof hiswife,The Vikings.Here it is not possible to determine the scope of the apposition without semantic information. However we know that The Vikings is not an usual name for a wife, whereas in the close context there is a different candidate, the common noun team, which is semantically more compatible. Thus, we want parsers to reproduce this behavior and link The Vikings and team. Fig. 1illustrates the problem with an example that shows the original output given by a state of the art parser and the intended result.Given this scenario, we formulate the next research questions:1.What kind of errors do parsers commit on appositions because of the missing semantic information?Is it possible to overcome these errors considering information captured previously from text collections? What evidence can it provide to characterize the named entity?What is the most effective way to measure the semantic compatibility between the candidates and the named entity?What configuration of evidence and measures achieves the best results?To answer these questions we have parsed a large text corpus with two objectives: first to have a base to acquire background knowledge, and second, to get a sample of appositions where we compare the behavior of two different dependency parsers in the state of the art.In Section 2 we define the scope of our study and check the source of errors in parsing. In particular we focus on cases where there are many candidates to govern an apposition. Our hypothesis is that these errors can be overcome by considering some background knowledge automatically extracted from large text collections. In Section 3 we explore how to gather this knowledge. We assume that the most semantically compatible candidate is the correct one. To measure this compatibility we study different configurations to combine the available evidence in Section 4, and show the results on Section 5. In Section 6 we discuss related work, and we provide some conclusions in Section 7. Finally in Section 8 we propose some future work.Our goal is to study appositions where dependency parsers have a chance to make a mistake selecting a governor because there are several grammatically correct candidates. In particular, we focus on cases where errors come from not taking into account the semantic compatibility between the two parts involved.One structure where these errors can be found is a phrase where one side of the apposition is a named entity, and the other one has two or more common nouns that can be the nucleus of the noun phrase, and therefore a semantic class of the named entity. We refer to those common nouns as candidates.We search for this structure in a large collection of textual data belonging to the TAC KBP task [4], composed by around 1.5million documents that belong to different categories including newswire, blogs and phone calls transcriptions. We parse the collection with the Stanford Parser [5].In this collection we obtain a total of 41,285,844 sentences, with 691,394 apposition dependencies, where 240,392 (34.7%) have more than one candidate.We took a sample of 300 sentences to build a Gold Standard for evaluation. The following subsections show the qualitative analysis of this Gold Standard. According to the semantic compatibility between candidate classes an instances, we can distinguish the following cases:In the simplest case, such as the examples in the introduction, the common noun that acts as nucleus of the first noun phrase should be the governor of the apposition and it is more suitable than the other. Consider the next sentence:…theleaderof its largest rebelgroup,Manuel Marulanda, …We target these cases as the ones to solve. Ideally, background knowledge should be able to discriminate suitable candidates. For example, a person as Manuel Marulanda is more frequently a leader than a group.However, sometimes there are multiple common nouns that are valid candidates to be the governor of the relation. This occurs when either there is an implicit or explicit conjunction. An example of explicit conjunction would be:…a prominent Jewishwriterand Holocaustsurvivor,Ralph Giordano …What we assume is that there are different simultaneous classes of the named entity. In some cases it could be interesting to add an apposition for each of the candidates.An implicit conjunction occurs when there are noun compounds where common nouns act as modifiers, like in the next examples:…by the IOC ’schiefBeijingorganiser,Hein Verbruggen, …In these cases, we perform a linguistic test, where we check if the nouns are a semantic class of the named entity. Following the example, we check if Hein Verbruggen belongs to the classes chief and organiser. As it happens, we say that both candidates are valid.3Recall that the first noun of a noun-noun compound should not be a candidate. Even so, our method is robust enough to consider even these candidates.3In other cases the noun phrase includes a subordinated sentence that also has a common noun candidate to govern the apposition, for example:Anotherpassengerwho gave only hissurname,Chen …In some sentences there are two classes referred to two different entities, but some extra-linguistic knowledge is needed to decide how they are related. For example:…at least onebrotherof anotherdefendant,Ali Dayeh Ali.In the previous example there are two classes, brother and defendant, that refer to two different entities, Ali Dayeh Ali and an unknown entity. Without external knowledge it cannot be decided if Ali Dayeh Ali belongs to the class brother or to the class defendant.In the manual inspection we have found multiple examples where the parser detects an incorrect apposition relation. To correct these errors is a line of work by itself, but here we limit ourselves to show what cases we have found. These are: Conjunction between two sentences, wrong apposition relation between a noun phrase and a verbal phrase, structures that denote the relation location-region, enumerations and nonsense text.We have shown how parsers depend on semantic knowledge to solve apposition dependencies with several grammatical alternatives. We have classified the sentences according to the semantic compatibility of the candidates and the named entity. Table 1shows the number and percentage of appositions in each case.In general, there are two main sources of errors in apposition parsing: When actually there is no apposition and the parser makes a bad choice of the dependency type, and when there are several candidates to govern the apposition and the parser makes a bad identification of the governor of the apposition. We tackle the latter, which include cases with one valid candidate, several valid candidates and undecidable candidates. These cases represent the 78.6% of the appositions of the sample.We compose a Gold Standard with these appositions where we annotate the right apposition dependence. When there are several valid candidates, or it is undecidable which candidate is valid, we consider all valid choices as correct. Even if this cases are easier to solve, we include them to maintain generalization (i.e. there are cases with several valid candidates but at the same time another one that it is not). For our experimentation, we ignore the non-apposition cases. The result is a Gold Standard of 236 appositions.In the previous section we saw that there are appositions with several candidates and the parser has to choose among them. We assume that the correct candidate is the most semantically compatible with the named entity. In this section we explain how we gather evidence to get a measure for this compatibility. Our hypothesis is that we can get it from large sources of text following the Open Information Extraction paradigm [6].The goal in this phase is to extract relations between classes and instances with an associated probability. To acquire this information we use a graphical document representationGdgenerated from a document d in the set of documents D.Each document d is processed with Stanford CoreNLP (see Section 2) to obtain part-of-speech annotations [7], named entities [8], syntactic dependencies [5] and coreferences [9].GraphsGdare defined by a set of nodesVdand set of edgesEd. A nodev∈Vdcontains lexical information about words and multiwords. Edgese∈Edare typed syntactic dependencies obtained by the parser. Finally, coreferences are used to collapse different mentions of the same entity into a single node.We obtain a set of classes from the graphs using very simple hand-crafted patterns based on syntactic dependencies where a common noun is a class for a named entity. Named entities have an entity type associated (person, organization or location) given by the named entity recognizer. When we find a match, we assign the common noun as semantic class of the named entity and we get an instanceNE-Class-Type. Table 2details the patterns and provides with some examples of instances collected.We do not pretend to obtain all the relations instance-class expressed in the collection with these patterns, but to acquire a representative and sufficient number of classes to evaluate their compatibility with named entities. We found a total of 4,410,293 instances.After obtaining the semantic class assignments, we aggregate the information from all the collection to obtain the frequencies of named entities, classes and types. As a matter of example, Table 3contains some tuples extracted with the associated joint probability.To measure the semantic compatibility we use the joint probability between the classes and entity names, and also between classes and entity types (see Section 5.2). To get them we use a maximum likelihood estimator and marginalize:(1)p(c,ne)=∑tp(ne,c,t)(2)p(c,t)=∑nep(ne,c,t)where c is the candidate, ne the entity name and t is the entity type. Table 4shows a sample of some of the higher probabilities obtained.Algorithm 1 sums up the process of acquiring the background knowledge.D={d0,…,dn}is document collection,P={p0,…,p6}is the set of patterns andKB={t0,…,tm}is the knowledge base extracted.Gdis the graph of the document d, and it is defined by a set of nodesVd={v0,…,vn}and a set of edgesEd={e0,…,em}.Cdrepresent the set of coreferences between pairs of nodes.Algorithm 1Knowledge base acquisitionInput:D={d0,…,dn},P={p0,…,p6}Output:KB={t0,…,tm}whereti=〈ne name, class, ne type, frequency〉for eachd∈Ddo//transform d intoGd:<Vd,Ed,Cd>←parse(d)do//collapse co-referent nodesfor each coreference(vi,vj)∈Cddoforeach edge(vj,vk)∈EddoEd←Ed∪{<vi,vk>}Ed←Ed⧹{<vj,vk>}Vd←Vd⧹{vj}//apply extraction patternsfor eachp∈Pt←match(Gd,p)KB=KB∪{t}In this section we explain our approach to identify and select the candidates to govern an apposition relation. We define the following variables in our method.S={s0,…,sn}represents the set of sentences, each one of them is composed by several candidatesCsi={c0,…,sm};Esi={ne,t}is the named entity with annotated evidence. FinallyF={f1,…,f8}denotes the set of configurations. We explain them and their use in more detail in the following sections.

@&#CONCLUSIONS@&#
In this article we have studied how semantic classes can be used to solve structural ambiguities in apposition dependencies. To do so we have answered the following research questions:1.What kind of errors do parsers commit on appositions because of the missing semantic information?Parsers fail when they lack of the semantic information to choose between several grammatically correct candidates. We have estimated that these cases represent the 78.6% of errors on apposition parsing, and have categorized them on three classes: appositions with one valid candidate (70.6%), with several valid candidates (7.3%) or with undecidable candidates (0.6%). We have focus on solving these three cases.The remaining 21.3% errors correspond to bad choices of the dependency type.Is it possible to overcome these errors considering semantic information captured previously from text collections? What evidence can they provide to characterize the named entity?We have used automatically acquired semantic classes as background knowledge to measure the semantic compatibility of candidates and named entities. This knowledge was divided in two different evidence, one relating semantic classes with entity names and other relating semantic classes with entity types.The results obtained reinforce our hypothesis that considering semantic compatibility between the two parts of the apposition can help to overcome parsing errors.What is the most effective way to measure the semantic compatibility that allows a better identification of the dependencies?We have used two different evidence (entity name and entity type) and three different measures (normalized pointwise mutual information, conditional probabilities and smoothed conditional probabilities).Using the entity name as evidence alone does not improve the results, since many names could be missing in the reference collection. On the other hand, the main problem of taking entity type alone as evidence is that some classes are very dominant (chief, business), and tend to be overassigned. According with the results, it is more effective to combine both entity name and entity type to get an accurate measure of semantic compatibility.What configuration of evidence and measures achieves the best results?Regarding the configurations tested, the best results are obtained when combining both sources of evidence with smoothed conditioned probabilities. We reach a 91.4% accuracy which is a 12.9% of relative improvement with respect to the best baseline (80.9%).