@&#MAIN-TITLE@&#
Identifying multiple objects from their appearance in inaccurate detections

@&#HIGHLIGHTS@&#
Localize and learn appearances of targets from object detections in video.Associate targets to features instead of detections to deal with partial occlusions.Post-processing provides pixel-level segmentation of individual objects.Scenarios can have inaccurate detections, occlusions, erratic motion, frame skips.Outperforms state-of-the-art batch and online multi-view trackers on fight scenes.

@&#KEYPHRASES@&#
Object recognition,Segmentation,Generative model,Unsupervised learning,Latent Dirichlet Allocation,Video surveillance,

@&#ABSTRACT@&#
We propose a novel method for keeping track of multiple objects in provided regions of interest, i.e. object detections, specifically in cases where a single object results in multiple co-occurring detections (e.g. when objects exhibit unusual size or pose) or a single detection spans multiple objects (e.g. during occlusion). Our method identifies a minimal set of objects to explain the observed features, which are extracted from the regions of interest in a set of frames. Focusing on appearance rather than temporal cues, we treat video as an unordered collection of frames, and “unmix” object appearances from inaccurate detections within a Latent Dirichlet Allocation (LDA) framework, for which we propose an efficient Variational Bayes inference method. After the objects have been localized and their appearances have been learned, we can use the posterior distributions to “back-project” the assigned object features to the image and obtain segmentation at pixel level. In experiments on challenging datasets, we show that our batch method outperforms state-of-the-art batch and on-line multi-view trackers in terms of number of identity switches and proportion of correctly identified objects. We make our software and new dataset publicly available for non-commercial, benchmarking purposes.

@&#INTRODUCTION@&#
Traditional tracking-by-detection approaches contain a data association step in which detections are matched to inferred object properties such as appearance, size, and location. However, this poses problems when objects are temporarily (partially) occluded or undetected. Wrong data association can deteriorate the learned object appearances, which further affects future associations. This exposes a chicken-and-egg problem: To localize objects in a scene, image observations need to be correctly associated to candidate objects, which requires knowledge of the object-specific identifying properties. Learning such properties from the observations, however, requires prior knowledge of the presence and location of the objects in the scene. Additionally, camera calibration may be unreliable (i.e. camera orientation may have changed) or unavailable, thus predefined detection windows may not necessarily fit the targets, and objects may exhibit unusual pose or size, resulting in low confidence detections that complicate association even more.This paper focuses on these related problems, and presents a novel graphical model to determine the number of objects, their appearance, and their location per frame, from possibly inaccurate detections. To exploit detections that contain multiple partially occluding objects and background, we seek to loosen the traditional one-to-one relation between detections and objects, and instead infer which of the low-level appearance features present in a detection belong to what target. Key to our method is the adaptation of Latent Dirichlet Allocation (LDA) to “unmix” a number of consistent object appearances in the comparatively large number of detected regions, which are represented as a bag-of-features. This allows us to infer the presence and appearance of an object, even when a single object is responsible for multiple detections and when a single detection spans multiple objects, as often happens in the case of partial occlusion of one object by another. Hence, occluding objects are separated at the feature level and we eliminate the need for special treatment of assignments and appearance updates under occlusion. Additionally, we exploit that in a single frame an object is local to a part of that frame, so that non-overlapping detections are unlikely to both contain the same object. This spatial constraint is enforced by modeling each frame as a mixture of objects whose feature locations have a Gaussian distribution centered at an object’s image location. In post-processing, we can optionally “back-project” the feature labels in all images, and segment individual targets. These steps are illustrated in Fig. 1on a frame from a challenging fight scene.To demonstrate the appearance “unmixing” paradigm, we address multi-target association (up to the pixel level) in a batch of frames for scenes with static background and a reasonable upper limit on the expected object count, but where detections from an external object detector may be inaccurate. Our method is compatible with traditional tracking frameworks where motion models reduce the positional uncertainty, since our model incorporates a prior distribution over each object’s location. Many different approaches have been proposed to enforce temporal consistency (e.g. merging tracklets [32], searching the space–time volume for globally consistent paths [13,22] or particle filters [11,31]), and state-of-the-art trackers have used strong motion models and the explicit specification of entry/exit regions to push performance. Therefore, our work focuses on appearance without dictating how to exploit such additional cues, and can be seen as complementing recent work on tracking under occlusion [2,22] which solely relies on temporal information. As a result, this paper treats video as an unordered collection of frames without temporal information, using the same positional prior for each individual frame, though for evaluation the output of our model will be compared to that of complete multi-target trackers.Our proposed method is related to tracking, image segmentation and object-recognition methods. The body of literature on these is extensive, and here we can only discuss a small set of papers which are most directly relevant.Tracking-by-detection employs some method to detect target objects in video frames, and combine the detections into consistent tracks. For example, person detectors can be trained on HOG features [12], or by reasoning about spatial occupancy to explain background/foreground masks [5,13,21]. In terms of tracking, we can distinguish between on-line trackers that incorporate new observations on a frame-by-frame basis [11,15,21,31], and batch methods that perform global optimization for multiple frames at once [1–3,22,5,24,33].Multi-view trackers observe targets simultaneously from various overlapping views, and are therefore more robust against occlusion in a single view. Using Probabilistic Occupancy Maps (POM) [13] for detection in calibrated views, [5] applies global appearance constraints to formulate a network flow optimization problem over all frames. This requires defining a priori appearance templates for distinct object classes, rather than learning the appearance of individuals from data. The on-line multi-view tracker of [21] relies on background subtraction and voxel carving to find candidate object locations in 3D. Using the Hungarian method [18], candidates are assigned to tracks or labeled as ‘ghosts’, i.e. faulty correspondences of foreground from different views, based on similarity scores for appearance, size, and Kalman filtered position. Back-projecting voxels to the images yields per-view object masks that take inter-object occlusion into account, which are used to learn the object appearances.In the single view tracker of [24], appearances are first learned by clustering body part patches from a generic part-based person detector. The trained model is then used to track an individual and makes it possible to reason about self-occlusion. In [33] a part-model is used to deal with other types of occlusions, and fuses tracklets (i.e. trajectory fragments) into consistent tracks while learning a discriminative appearance model for each person. Another use of part-based models is to exploit the dynamics of parts to disambiguate tracks and recover after occlusion, e.g. [1] distinguishes multiple people seen in side-view from their articulated leg pose within a walking cycle.Part-based models are not the only way to deal with occlusion. [2] adds occlusion reasoning to a continuous energy minimization framework [3] that globally optimizes detection-to-track associations under temporal constraints. In [22] this framework is extended to mixed discrete–continuous optimization for improved data association, with additional global constraints to consider track dynamics and exclude collisions, and keep overlapping tracks separated.Others treat occlusions as temporary occurrences where no association can reliably be made. For instance, [15] weighs a large set of features to construct an affinity matrix between tracks and detections, and applies the Hungarian algorithm to find an optimal assignment. Ground truth annotations are required to optimize the weights for the various features using SVM and to handle entering/leaving correctly, and only short-term occlusions are dealt with by maintaining a history of features (occlusions of about a second can result in track termination). Likewise, [31] learns scene-specific feature weights from ground-truth with SVM too, but considerably extends the set of candidate regions with predictions from particle filters. Ref. [11] does not rely on annotations, but uses detector outputs as an object confidence map to weigh a particle filter, and learn discriminative target classifiers on-line. These methods thus focus on reducing the periods where objects are undetected, and/or rely on discriminative appearance models to improve track recovery after such gaps. However, occlusions must still be short and treated as a special case where the discriminative appearance model cannot be applied or updated.Let us now look at topic models, which originate from the unsupervised analysis for text documents, and have been successfully applied to computer vision tasks by defining these in terms of ‘visual document’ and a codebook of ‘visual words’ [26,25,23,29,19,14,17]. Latent Dirichlet Allocation (LDA) [8] is a topic model that represents a document as an unordered bag-of-words, i.e. occurrence counts of each word, and then jointly infers topics as distributions over co-occurring words, and infers per document the mixture of topics within it. There are various techniques for approximate inference in such graphical models [7], e.g. [8] presents variational inference for LDA.In the context of image segmentation, [25] uses LDA to discover object classes and their appearance in an image database, relying on an external image segmentation algorithm to provide relevant regions. Spatial LDA [28] discovers object classes too, but also segments each image by assigning each feature to one of many overlapping regions to enforce spatially consistent labeling. In the hierarchical model for object recognition by [26], LDA was adapted for unsupervised learning of part-models in a supervised object recognition task. Note that [25,26,28] focus on recognizing distinct object classes, not distinct instances within the same class. Topic models have also been used to discover common motion patterns exhibited by moving objects in video, though none of these methods perform any tracking. Refs. [30,14] avoid the need to track individuals in far-field surveillance. Regarding individual frames as documents, a visual codebook is created by quantizing optical flow in both location and direction, such that topics capture typical [30] (or rare [14]) co-occurring regions of motion. If an external tracker can provide tracks, one can also treat these as documents, and discover typical motion patterns that co-occur within a single track [29,17]. Ref. [34] takes an intermediate approach, finding scene wide patterns from given tracklets without merging them into complete tracks. These methods either quantize the track position and motion into visual words [14,29,34], or learn continuous distributions in the spatial domain [17]. LDA has also been used to compare person appearances in a multi-camera setup [23] from externally provided trajectories, and establish probable matches among non-overlapping viewpoints. In [19] LDA is adapted to discover behavior patterns in a multi-camera CCTV setup from quantized optical flow.Our approach processes a collection of input frames simultaneously (i.e. in batch mode), and consists of a feature extraction step, a joint inference step, and optionally an image segmentation step. In the feature extraction step, we use the output of an external object detector to determine in all images the presence of the object class of interest (e.g. person) at a given set of (possible overlapping) candidate regions. We keep those detections for which the detector confidence is sufficiently large, though we use a low threshold to keep inaccurate detections too. In each detection we extract low-level appearance features. A feature is described by a visual word, obtained by feature quantization, and its spatial position. Our inference algorithm however will represent each detection by a bag-of-features, i.e. the word occurrence counts, and mean and variance of the spatial distribution of the features in the detection. In our experiments we regard each pixel in a detection as a feature, and use its binned color value as visual word, thus we record per detection only a color histogram. The spatial distribution is derived analytically from the detection bounding box.Since detections can contain several (occluding) objects, we do not seek to associate whole detections to a unique target. Instead, the observed features in a detection are considered to be distributed as a mixture of objects. Each object thus has a specific appearance distribution over the feature words, and per image a spatial distribution over feature positions. Hence, we wish to learn a relatively low number of object appearances from a relatively high number of mixed detections with unknown mixture proportions. This “unmixing” task is analogous to LDA, where documents, described as a bag-of-words, are decomposed into a small set of topics, i.e. word distributions. However, our generative model additionally accounts for the fact that a target can only be supported in (at most) one spatial region within each frame. The model therefore has latent variables that represent each object’s appearance, and location per time step, and the mixture coefficients per detection. Joint approximate Bayesian inference yields posterior distributions over these latent variables, as illustrated in Fig. 2. One can extract object tracks from this posterior (though we have not included any temporal constraints), as it describes for each time step which objects are present, and where.As noted before, while the generative model is expressed in terms of individual features, described in Section 4, inference will exploit an efficient bag-of-features representation (similar to LDA), as discussed in Section 5. In the optional image segmentation step we can back-project the posterior distributions of the feature assignments to the original images to obtain object foreground masks, as will be discussed in Section 5.2.One parameter that has to be set in advance is K, the upper limit of distinct objects that will be detected. Unlike traditional clustering methods, such as K-means and maximum likelihood estimates for mixture models, the variational inference scheme avoids overfitting even when K is set much larger than the true number of objects, due to the use of priors on the model parameters [7]. During the iterative learning processes, candidate objects that are redundant are assigned fewer times to observed features, which makes their appearance and spatial position less specific. This further reduces the probability of assigning such objects in future iterations, until they are not assigned at all anymore. In practice it is preferable to set K not too large either since complexity grows linearly with K, but one could test increasing values for K until the found object count stabilizes.False positive detections (i.e. background identified as a target object) may affect our method, but in general the model copes with these by either (a) learning an appearance for the background region, regarding it as a mostly static object, or (b) identifies it as background if we include the option of a background distribution in our model (see Section 5.1). A target is not located in an image if it is fully occluded, has (temporarily) left the scene, or not yet entered, or if there are missing detections (i.e. false negatives). The presented generative model does not by itself distinguish or resolve such cases, but should (re-)identify the object in the other frames. A low detection threshold reduces the risk of false negatives. If the frame rate is high and targets move predictably, one could extrapolate an object’s motion, though we do not make such assumptions here as we focus on exploiting available detections instead. Further, the proposed method does not rely on accurate detections with non-maximum suppression, no calibrated cameras [21,33], nor knowledge of part-configurations [1,24,33], but uses only on an appropriate object detector to select regions of interest.Our experiments will focus on keeping track of multiple persons against a static background, while in principle other object types could be dealt with too. Whereas part-based methods try to explicitly model the non-rigid nature of people, our bag-of-features representation drops any rigidity assumption. Because we can use a low detection confidence threshold, we found that a trained person detector based on HOG features [12] even yields usable detections when a person is partially occluded by another person or scenery.The proposed adaptation of LDA has similarities to [26,28], but there the goal is to recognize various object classes in a set of images, which results distinct models and inference schemes. Ref. [26] performs supervised image classification by learning common parts and their spatial configurations per image class. Spatial LDA [28] performs unsupervised image segmentation by learning class appearances that account for consistently labeled pixel neighborhoods. It does not however constraint the occurrence of a single appearance at various places in the image, nor segment occluding instances of the same class, nor use an object detector to focus on a specific class as our method does. And, [28] jointly Gibbs samples the labels of individual features, which we avoid with the bag-of-features representation. Topic models have also been used for video analysis to discover typical objects motion patterns for a particular environment, e.g. without tracking using optical flow at the image level [30,34], or by analyzing tracks provided by an external tracker [29,34,17]. These applications did not address the track association problem itself, nor resolve partial occlusions.In summary, the main contributions of this paper are:1.A novel model to jointly localize and learn appearances of an unknown number of objects in a batch of images. We introduce an efficient Variational Bayesian inference algorithm with linear complexity w.r.t. frames, objects.Address scenarios with inaccurate detections, and that contain erratically moving objects and/or have irregular or low frame rates (since objects are identified in the images without considering temporal information).Introducing feature-to-object instead of detection-to-object correspondences to deal with partial occlusion.Pixel-level segmentation of individual objects from object detectors and color histograms only.We now specify the variables and full distribution of our generative model, which is depicted as a graphical model in Fig. 3. There are T time steps withDtdetections at time t obtained from the external object detector, and detection(j,t)at window (i.e. image region) j containsNjtfeatures. The tuple(j,t,i)identifies the i-th feature in(j,t), which has two observed properties: a discrete visual word, which we represent as an integeryjit∈[1,V], and a (2D) positionxjitin the image plane. We define the word occurrence count of word v in detection window(j,t)asNjvt=∑iδ(yjit,v), withδ(a,b)=1iffa=band 0 otherwise. For instance, when quantized pixel colors are used as words, then the vectorNj·t=[Nj1t⋯NjVt]is the color histogram of(j,t), whereV=c3if each color channel is discretized in c bins.We assume that the feature is generated by one out of K objects, indicated by the latent variablezjit∈[1,K]. Since each detection contains a mixture of objects, thezjitin a detection(j,t)follow a multinomial1For clarity later on we use the multinomial distributions as a generalization of the categorical distribution, for instancezjitcan be represented equivalently as a one-of-K vector.1distribution with parameter vectorθjt, e.g. elementθj(k)tis the mixture weight of object k in(j,t). Each of the K objects defines a multinomial appearance distributionβkover the V visual words, which is shared by detections in all time steps, and each object has per time step a multivariate Gaussian distribution with meanμktand precision matrixΛktover the image positions of features. The full distribution is thus factorized into the following terms:(1)p(xjit|zjit,{μk,Λk})=N(xjit|μzjitt,(Λzjitt)-1)(2)p(yjit|zjit,{βk})=Mult(yjit|βzjit)(3)p(zjit|θjt)=Mult(zjit|θjt).We place conjugate priors on the latent variables,(4)p(θjt|α0)=Dir(θjt|α0)(5)p(βk|η0)=Dir(βk|η0)(6)p(Λkt|W0,ν0)=W(Λkt|W0,ν0)(7)p(μkt|m0,Λkt,λ0)=Nμkt|m0,(λ0Λkt)-1where Dir andWrepresent the (symmetric) Dirichlet and Wishart distribution respectively. Details of the used distribution can be found in Appendix A.Inference on the latent variables is performed on all detections of all time steps jointly. Since exact inference is intractable, we resort to Variational Bayesian (VB) inference to find good approximate solutions. In VB, one approximates the target distribution p by a simpler distribution q, typically by assuming independence between various (latent) variables in p. Inference then proceeds by minimizing the Kullback–Leibler (KL) divergenceKL(q||p), see [7].Unfortunately, inferring a variational distribution for each latent assignment labelzjitis computationally demanding. In VB for standard LDA this problem is mitigated by noting that per document all observations of the same word v are exchangeable. Instead of reasoning about individual indicators z, a multinomial distribution over the number of words v assigned to object k in a document can be used, which leads to a more efficient variational inference scheme [8]. In our model, however, features with the same visual word in a detection are not exchangeable, since features do not share the same positionxjit. We therefore remove the correspondence between positionsxjitand wordsyjit, and instead model thexjitin the detection window as i.i.d. normally distributed random variables. More precisely, if window j has centerx∼j, width w and height h, then we analytically derive (see Appendix B for details)(8)xjit∼Nx∼j,Σ∼jwithΣ∼j=w2/1200h2/12.We can now avoid inferring the assignment of individual features in the variational approximation, and use instead the multinomial distribution per detection over the number of features assigned to each of the K candidate objects, as is the case with VB for standard LDA.In Appendix C we present detailed derivations of the variational distribution, and all update equations. We group the update equations into the three parts, which must be executed iteratively since updates are coupled:•the updates for the object priors and appearances, which are similar to those for standard LDA [8,4].the updates for the spatial distributions, which follow the VB updates for standard MoG (see [7], Chapter 10).the assignment updates which, as expected, are a combination of the assignment updates found in LDA [8] and MoG [7], but where special care has to be taken to include the uncertainty of a feature’s positionxjit.To initialize inference, we sample the parameters of the variational distributions on all feature-to-object assignment counts uniformly, and normalize the distribution for each detection (see Appendix C.6). Effectively, all observations are initially assigned almost uniformly to all K candidate objects, and therefore all objects have almost the same spatial distribution in each image, and similar appearances. At subsequent iterations however, the small random variations in the appearance and spatial distributions becoming increasingly distinctive. As more features are assigned to certain objects, the uncertainty on their appearance and spatial distributions decreases, while the distributions of objects with few features reduce to the prior which avoids overfitting. Fig. 4illustrates the procedure at different iterations, showing for two frames in a sequence how the spatial and assignment distributions change, until eventually all features are assigned to only four candidate objects.Direct application of the variational update equations can unfortunately get stuck in a suboptimal solution. Initially when appearances distributions are not yet representative of the true objects appearances, we observe that the positional likelihood tends to outweigh the appearance likelihood, forcing inferred objects to quickly cluster detections spatially without discovering meaningful and distinct appearances. To guide inference through this parameter space with many local optima, the variational updates initially only include the spatial distribution at every fifth iteration, until the appearance distributions converge. We found that this provides good initial distributions for the final variational updates, which include every term at each iteration, and are again performed until convergence.The complexity of the variational inference updates isO(T×D×K×V), whereT×Dis the total number of detections. Computationally, the appearance and prior updates are mostly sums and multiplications that are fast to execute. Updating the spatial locations is the most costly operation, though these could be run in parallel for objects and time steps. Assignment updates can also benefit from parallelization.The features within a detection may stem from one or more objects, but could also come from the background (or occluding foreground) of the scene. In our experience, when the background is sufficiently uniform this tends not to be a problem, especially when the detections are accurate and therefore contain many object features and few background features. In those cases, all object appearances include some low probability of observing the background features. But when the detections contain a lot of background, and/or the background varies across the image, the model will infer additional objects associated with the background features, resulting in many false positives. For this reason, and with the upcoming image segmentation step of Section 5.2 in mind, we wish to exploit background information in our model to segment fore- and background.For each detector window location we create a background object, such that each detection(j,t)can be a mixture ofK+1possible object models: the K objects plus the background of window j. Background object j then has appearance distributionβ̇j, and we fix its spatial distribution toN(x∼j,Σ∼j)as in Eq. (8). This means that background objects are not treated differently from the K target objects, except that we do not update their location (the most computationally expensive part of our method), and that instead of priorη0we use window specific background priorη̇0j. In our experiments we obtainη̇0jby taking the input images, removing the features in the windows that will be kept as detections (i.e. erase object appearances), and count the remaining features within each window j over all images. Standard background subtraction, performed as pre-processing step, evaluates only once the observation likelihood under a background distribution, and may discard actual foreground with appearance similar to the background. Our approach (re)evaluates the feature likelihood for both background and all object appearances during inference, and background segmentation is a result of inferring the assignment labels. One may come up with different background models for different scenarios. For instance, if the background has little variation a single background appearance shared by all windows might be used. Or, if reliable foreground masks are in fact available, those could be used to remove background features and no background model would be necessary.A benefit of using a generative appearance model instead of a discriminative one (e.g. [11]) is that the model can be utilized for image segmentation. After the variational distributions converged, the distributionsq(zjit=k|·)express the probability that a wordyjitin detection(j,t)belongs to object k. In a post-processing step we can then compute the object appearance responses in the original input image frames by back-projecting. The object probabilities from overlapping detections are averaged per pixel, and pixels that are not in any detection window are fixed to background. Subsequently, we use the object responses as input for standard multi-label image segmentation software.2http://vision.csd.uwo.ca/code/.2We use energy minimization [10,9,16] to determine per pixel the optimal object label, taking into account both object responses and labeling consistency between neighboring pixels. The energy minimization problem formulation makes assumptions complementary to the bag-of-features representation in our model, which does not enforce consistent neighborhood labeling. While that enables very fast variational updates, it also results in more noisy assignments of features to objects. Since the human visual system is very sensitive to edges, approximate global optimization of neighborhood consistency on the complete frame obtains visually more pleasing results.

@&#CONCLUSIONS@&#
