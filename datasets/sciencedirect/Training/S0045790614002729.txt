@&#MAIN-TITLE@&#
Process fault detection based on dynamic kernel slow feature analysis

@&#HIGHLIGHTS@&#
A nonlinear dynamic process monitoring method is presented.The proposed method can extract the inherent slow features from the high-dimensional observed data.A statistic index is built based on slow features to carry out process monitoring.The method is more sensitive to process faults than the conventional KPCA-based method.

@&#KEYPHRASES@&#
Fault detection,Slow feature analysis,Kernel principal component analysis,Nonlinear dynamic process,

@&#ABSTRACT@&#
A fault detection method based on dynamic kernel slow feature analysis (DKSFA) is presented in the paper. SFA is a new feature extraction technology which can find a group of slowly varying feature outputs from the high-dimensional inputs. In order to analyze the nonlinear dynamic characteristics of the process data, DKSFA is presented which applies the augmented matrix to consider the dynamic characteristic and uses kernel slow feature analysis (KSFA) to extract the nonlinear slow features hidden in the observed data. For the purpose of fault detection, the D monitoring statistic index is built based on DKSFA model and its confidence limit is computed by kernel density estimation. Simulations on a nonlinear system and Tennessee Eastman (TE) benchmark process show that the proposed method has a better fault detection performance compared with the conventional (kernel principal component analysis) KPCA-based method.

@&#INTRODUCTION@&#
Process monitoring is very important to ensure safe operation and production of high quality products in the industrialproduction process. Once a fault happens, enormous economic loss may be made, even it can cause casualty and pollute the environment severely. Data-driven methods have gained great attention and become a research hotspot gradually in the fields of fault diagnosis in recent years. However, the high-dimensional data often include some redundant information and the key point of data-driven methods is to extract the feature information from multi-dimensional measured signals [1–4].Multi-variable statistical process monitoring technology such as principal component analysis (PCA) [5], partial least squares (PLS) [6], and independent component analysis (ICA) [7,8], have been used widely and successfully in the research fields. However, these methods mentioned above are the linear dimensional reduction techniques which may cause incorrect results for nonlinear industrial process. Besides, nonlinear transformations aiming to the original variables are not easy to construct. To address the nonlinear characteristics of the process, several nonlinear extensions of the traditional PCA method have already been proposed such as principle curves [9] and neural network [10–12]. Lawrence proposed Gaussian process latent variable model (GP-LVM)[13] which could easily be non-linearized through Gaussian processes, and related to popular spectral techniques. Recently, nonlinear process monitoring methods based on kernel function were proposed [14–17], which have been applied for fault detection and fault diagnosis successfully. Kernel PCA (KPCA) [18,19] firstly mapped the inputs into a linear feature space by a nonlinear mapping, and then extracted principal components in the kernel feature space.Slow feature analysis (SFA), a new learning principle, is inspired by slow change phenomenon that occurs in real life. SFA aims for invariant features out of the high-dimensional measurements. It can extract the slowly varying features which carry significant information. The obtained feature components are mutually uncorrelated and independent from each other. SFA differs from simple low-pass filter fundamentally. If the input signals have some underlying and slowly varying causes, SFA can always extract slowly varying output signals. Through SFA algorithm [20], we can obtain the slowly varying features which are very useful for classification and identification from input signals. Berkes and Wiskott employed it to learn the self-organized receptive field of cortical neuron from synthetic image sequences [21,22]. Zhang considered four kinds of SFA learning strategies to extract slow feature functions from a large amount of training cuboids for human action recognition [23]. MA proposed kernel-based method to solve the nonlinear expansion problem of SFA and supplied an algorithm evaluation criterion [24]. All the above findings suggest that SFA can extract the useful slow feature information for data analysis.In this paper, a nonlinear dynamic fault detection method is introduced based on dynamic kernel slow feature analysis (DKSFA), which can generate a set of output signals that vary as slowly as possible but carry significant information from the input signals. The monitoring statistic index is constructed for process monitoring with the extracted information. The effectiveness of the proposed method is demonstrated through a numerical sample and Tennessee Eastman (TE) process. The following sections sketch the introduction of the algorithm and its application. Firstly, the SFA algorithm is reviewed in Section 2, as it is necessary for understanding the remainder of the work. The nonlinear dynamic fault detection method named DKSFA is also presented in Section 2. Section 3 describes the procedure of the fault detection method based on DKSFA. Simulation studies on a simple nonlinear system and the TE process are shown in Section 4, followed by a discussion of the proposed method in Section 5.Given a multi-dimensional input signalx(t), SFA algorithm can find a set of function, and the output signals yj(t)=fj(x(t)) can be generated through fj. SFA focuses on finding slowly varying components from the input signals.Letx(t)=[x1(t),x2(t),…,xm(t)]Tbe the m-dimensional input signals,y(t)=[y1(t),y2(t),…ym1(t)]Tbe the m1-dimensional output signals, andf(x)=[f1(x),f2(x),…,fm1(x)]Tbe a set of real valued functions. The primary objective of SFA is as follows:(1)minΔyj(t)=〈ẏj2〉tunder the constraints of(2)〈yj〉t=0(3)〈yj2〉t=1and(4)∀i<j,〈yiyj〉t=0whereẏdenotes the first order derivative of y and 〈·〉tis the mean of signal y over time. Eq. (1) introduces the temporal variation measure of the output signals, which is equal to the mean of the squared first order derivative of the original measured signals. Its value is large for quickly varying signals and is close to zero for lower varying signals. The zero mean constraint (2) is introduced for convenience. Constraint (3) means that the transformed output signals should carry some information and prevent constant signals yj(t)=const from emerging. Different output components carry different aspects of information, and they can reflect different characteristics of input information. Besides, it also induces an order [21]. The first output signal is the slowest one, and the second is the second slowest while obeying the constraint (4), etc.The nonlinear mapping relationship between input data and output data is:(5)y(t)=ψ(x(t))However, it is not easy to obtain the implicit mapping ψ directly. Therefore, the idea of KPCA is introduced to extend SFA for extracting nonlinear slow features in the paper.The input space can be mapped into a high-dimensional feature space by kernel transformation. SFA algorithm is a linear transformation method, which is expanded to the nonlinear method by introducing the idea of KPCA and a set of slowly varying outputs can be obtained by the nonlinear SFA method.The kernel transformationKij=k(x(i),x(j)) is used on the inputsxto calculate kernel characteristics. It is necessary to carry on centralization operation on kernel matrixKto getK̃[24]. The kernel transformation vector can be found by resolving the eigenvalue problem as follows:(6)nλα=K̃αwhere n is the number of samples. The new kernel principal component features can be acquired by(7)νkj′=∑i=1dαijK̃(x,xi)The vectorνk′is zero mean, uncorrelated from each other andνk′=[νk1′,νk2′,…νkd′]. In order to meet the application conditions in Eq. (3), normalized operation is then carried onνk′to getνk.Further orthogonal transformation is implemented to the vectorsvk, and it does not change the vector mean, variance and non-correlation characteristic.(8)y(t)=WTνk(t)whereνk=[νk1,νk2,…νkd], andWis the projection transformation vector. In the following text, the objective function (1) can be obtained to getW. The optimization problem is as follows:(9)minΔyj(t)=〈ẏj2〉t=wjT〈ν̇k(t)ν̇k(t)T〉wj=wjTVkwjThus, the nonlinear slow feature analysis algorithm can be transformed into the problems of eigenvalue solution. The covariance matrix ofνkcan be obtained, which is defined asCνk(10)Cνk=ν̇k(t)ν̇k(t)TThe transformation vectorwjcan be obtained by resolving the following eigenvalue-decomposition problem:(11)Cνkwj=λwjwherew0,w1, …wd−1 are the eigenvectors corresponding to the first d eigenvaluesλ0,λ1,…λd-1withλ0>λ1>…λd-1. Therefore, the projection between nonlinear principal components and slow feature outputs can be established as follows:(12)y=WTνkwhereWT=[w0,w1, …wd−1] andw1 is the optimal solution corresponding to a normalized feature vector of the smallest eigenvalue ofCνk, while the sub-optimal solutionw2 corresponds to a normalized feature vector of the second smallest eigenvalue ofCνk, etc.Therefore, the nonlinear vectors can be calculated by projecting the original space into a high-dimensional space. Then the SFA algorithm is carried on the nonlinear principal components, and the nonlinear slowly varying features can be extracted finally.The process monitoringmethods such as PCA, assume the currentsampling data are statistically independent from the previous observations. This assumptionis effective only forthe longersampling intervalin the practical industrial processes. However, the dynamic characteristic is obvious in the observed data. Thus furtherconsideration of the serial correlation of the data is required. For monitoring the dynamic process efficiently, KSFA is extended to dynamic KSFA (DKSFA) by augmenting each observation vector with the previous d observations [25], which can well account for the correlation of the data. The augmented data matrix can be given as follows.(13)Xt(d)=[X(t)X(t-1)···X(t-d)]=xtTxt-1T…xt-dTxt-1Txt-2T…xt-d-1T…………xt+d-nTxt+d-n-1T…xt-nTThe selection of d is closely related to the dynamic nature of the process. The value should not be too large, and 1 or 2 is suited according toKu’sproposal [25].Based on the DKSFA model, a new monitoring statistic is built for fault detection and process monitoring. The D statistic is built to measure the output deviation between the new sample and all the observed data under normal condition, which is defined as(14)D=∑12log10∑ynew/ynorwhereynewis the nonlinear slow feature output of the new sample andynoris the output vector under normal condition. By comparing the current outputs with all the outputs from the modeling data, the current operating status can be monitored.Therefore, fault detection method based on DKSFA is implemented as follows. Firstly, each observation vector is extended to the augmented data matrix with the previous d-dimensional observations to account for the dynamic characteristics of the process. The original linear SFA algorithm is extended with kernel principal component analysis to solve the nonlinear problem in the next step. Further, based on the slow feature outputs generated by DKSFA, the monitoring statistic index is built to carry out process monitoring.The process monitoring based on DKSFA includes two stages: offline modeling stage and online fault detection stage. The normal operating model is set up by DKSFA in the offline modeling stage, and the corresponding confidence limit of D chart is calculated. During online detection stage, the online statistic of new collected data is calculated to determine whether process is under normal operation condition.Stage I: offline modeling stagei.Obtain the augmented data matrix in (13) and calculate its mean and variance.Compute the kernel matrixKby nonlinear kernel transformation, and carry out mean centering in the high-dimensional feature space to getK̃.Solve generalized eigenvalue problem in (6) to get the eigenvectors further.Calculate the kernel principal componentsνk′(t)in the feature space in (7), and normalize it to get the vectorsvk(t).Calculate the covariance matrixCνkofvk(t) in (10), and solve eigenvalue problem in (11).Get the slow featureywith output function in (12).Calculate the monitoring statistic of the normal operating data and determine the control limit for D chart in (14).Stage II: online detection stagei.Get a new data and preprocess it with mean and variance of normal data in the offline modeling stage.Compute the kernel principal components of the new data in (6).Compute the nonlinear slow features outputs in (12).Calculate the monitoring statistic D based on slow features. If D exceeds its confidence limit, a fault is detected and the fault alarming should be given next.In this section, the proposed monitoring method is evaluated by two cases including a simple nonlinear system and the benchmark Tennessee Eastman process. In both the two case studies, the proposed method is compared with the conventional KPCA-based fault detection method. The results are detailed in the following context.In this section, a simple nonlinear system with three monitored variables is simulated to test the robustness of the DKSFA method. The mathematical description of the system is given as follows [9]:(15)x1=t+e1x2=x12-3x1+e2x3=x12+3x2+e3where e1,e2,e3∼N(0, 0.12) are three independent noise variables under normal condition respectively,t∈[0,2]is a random variable with uniform distribution and the outputx=[x1x2x3]Tare measured as monitored variables. Therefore, three process variables are measured and collected in the simulation. There are 1500 samples to build fault detection model under normal condition. The dimension of the low-dimensional space in both the two methods is selected by cumulative variance contribution method, and its value is 2 just as the intrinsic dimension of the nonlinear system in (15). And the explained variance information using the two low-dimensional variables is approximately 99.94% of the total variance. Furthermore, the first dataset is used to test the performance of the proposed method, and to build the control limit of the monitoring statistic. The confidence limit of fault detection also known as alarming threshold is set as 95%.Two groups of normal datasets with different noise information from 1001st sample are generated respectively, and each dataset involves 1500 samples. Noise variance of the first group dataset is still with e1, e2, e3∼N(0, 0.12) while the noise variance of the second group test dataset is changed from the 1001st sample with e1,e2,e3∼N(0,0.152). The second group of dataset is still collected under normal condition, which can be used to verify the robustness of the proposed method against noise interference. Besides, the monitoring statistics T2,Q,D are plotted as solid line and their confidence limits are plotted as dot-dash line respectively in the paper. As we can see in Fig. 1(b), the Q monitoring chart based on KPCA indicates the abnormality from the 1001st sample. Although T2 monitoring statistic is below its control limit, the fault alarming is provided to the operators when one of the monitoring indexes exceeds its control limit. In contrast, from Fig. 1(c), the proposed DKSFA method indicates that the process is in normal operation, and thus the robustness of the proposed method against noise disturbances is verified clearly. Slow features are useful information as has been mentioned earlier, through which we can monitor the process more effectively. The changing rate of the disturbances is comparatively large, and then the disturbances can be filtered as the non-useful information.The TE process is presented by Downs and Vogel [4], and since then it has been widely used as a benchmark simulation in applications such as model predictive control (MPC), process monitoring, and fault diagnosis. The simulator is based on a nonlinear industrial process consisting of five major units: a reactor, a condenser, a compressor, a separator, and a stripper. TE is a first-order irreversible exothermic reaction, and it contains 148 algebraic equations. A flowchart of the TE process is illustrated in Fig. 2.The simulator includes a set of programmed fault modes, and all these 21 faults are listed in Table 1. The training data consist of 500 samples under normal operating conditions and the number of test data from each kind of fault mode is 960. All faults in the test datasets are introduced after the 160th sample. There are 52 variables used for monitoring the TE process, including 12 manipulated variables and 41 measurements. Further details can be found in the relevant literature [4]. The simulation data of the TE process used in the paper are downloaded from the corresponding website http://brahms.scs.uiuc.edu.

@&#CONCLUSIONS@&#
