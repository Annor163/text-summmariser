@&#MAIN-TITLE@&#
Grammar induction using bit masking oriented genetic algorithm and comparative analysis

@&#HIGHLIGHTS@&#
A background on theory of grammar induction is presented.The effect of premature convergence is discussed in detail.Proposed a system for grammar inference by utilizing the mask-fill reproduction operators and Boolean based procedure with minimum description length principle.Comparative analysis, discussion and observation of obtained results are given in an effective manner.Statistical tests (F-test and post hoc test) are conducted.

@&#KEYPHRASES@&#
Bit-masking oriented data structure,Context free grammar,Genetic algorithm,Grammar induction mask-fill operator,Premature convergence,

@&#ABSTRACT@&#
This paper presents bit masking oriented genetic algorithm (BMOGA) for context free grammar induction. It takes the advantages of crossover and mutation mask-fill operators together with a Boolean based procedure in two phases to guide the search process from ith generation to (i+1)th generation. Crossover and mutation mask-fill operations are performed to generate the proportionate amount of population in each generation. A parser has been implemented checks the validity of the grammar rules based on the acceptance or rejection of training data on the positive and negative strings of the language. Experiments are conducted on collection of context free and regular languages. Minimum description length principle has been used to generate a corpus of positive and negative samples as appropriate for the experiment. It was observed that the BMOGA produces successive generations of individuals, computes their fitness at each step and chooses the best when reached to threshold (termination) condition. As presented approach was found effective in handling premature convergence therefore results are compared with the approaches used to alleviate premature convergence. The analysis showed that the BMOGA performs better as compared to other algorithms such as: random offspring generation approach, dynamic allocation of reproduction operators, elite mating pool approach and the simple genetic algorithm. The term success ratio is used as a quality measure and its value shows the effectiveness of the BMOGA. Statistical tests indicate superiority of the BMOGA over other existing approaches implemented.

@&#INTRODUCTION@&#
Grammar induction or grammar learning deals with idealized learning procedures for acquiring grammars on the basis of the evidence about the languages [45,62,63]. It was extensively studied [6,46–50,63] due to its wide fields of application to solve practical problems in a variety of different fields, which includes a compilation and translation, human machine interaction, graphic languages, design of programming language, data mining, computational biology, natural language processing, software engineering and machine learning, etc.The first learning model was proposed by Gold in 1967 [26]. Gold addressed the question “Is the information sufficient to determine which of the possible languages is the unknown language?” [26]. It was shown that an inference algorithm can identify an unknown language in the limit from complete information in a finite number of steps. The key issue with the Gold's approach is that there is no information present with inference algorithm about identification of correct grammar because it is always possible that next sample may invalidate the previous hypothesis. Angluin [58] proposed “tell tales” (a unique string makes the difference between languages) to avoid the drawback. Although Gold [26] laid the foundation of grammar inference, Bunke and Sanfeliu [38] presented the first usable grammatical inference algorithm in syntactic pattern recognition community with the aim of classification and analysis of patterns, classification of biological sequence, character recognition, etc. The main drawback of this algorithm was it only dealt with positive data, was unable to deal with noisy data, did not fit exactly into a finite state machine and therefore good formal language theories were lost. Stevenson and Cordy [39,40] explain that theorists and empiricists are the two main groups contributing in the field of grammar inference. Language classes and learning models were considered by theorists group to set up the boundaries of what is learnable and how efficiently it can be learned. On the other hand empiricists group dealt with a practical problem by solving it; finally they have made a significant contribution in grammatical inference. Teacher and query is another learning model, where a teacher also referred to as an Oracle knows the target languages and is capable to answer particular types of questions/queries from the inference algorithm. Six types of queries were described by Angluin [59], two of which are membership and equivalence queries that have significant impact on learning. In case of membership queries, the inference algorithm presents either “yes” or “no” as answer to the oracle, whereas oracle receives “yes” if the hypothesis is true and “no” otherwise by inference algorithm. Valiant [60] presented Probably Approximately Correct (PAC) Learning Model, which takes the advantages of both identification of the limit and teachers and queries learning models. The PAC learning model is different from other two former learning models because of two reasons: first it does not guarantee exact identification with certainty, second compromises between accuracy and certainty. The problem with the PAC model is that the inference algorithm must learn in polynomial time under all distributions, but it is believed to be too strict in reality. These problems occur because many apparently simple classes are either known to be NP-hard or at least not known to be polynomial learnable for all distributions [39]. To mitigate this issue, Li et al. [61] proposed inference algorithm to consider the simple distribution only. Apart from the above popular learning models, many researchers explain the suitability of the neural network for grammar inference problem. Neural network shown the ability to maintain a temporal internal state like a short term memory [40]. In case of the neural network, a set of inputs, and their corresponding outputs (Yes: string is in the target language, No: otherwise) and a defined function need to learn, which describes those input-output pairs [40]. Alex et al. [54] conducted experiments for handwriting recognition using neural network and it was explained that this network has the capability to predict subsequent elements from an input sequence of elements. Cleeremans et al. [53] implemented a special case of a recurrent network presented by Elman [55] known a simple recurrent network to approximate a DFA (deterministic finite automata). Delgado and Pegalajar [56] presented a multi-objective genetic algorithm to analyze the optimal size of a recurrent neural network to learn positive and negative examples. Authors of [56] utilized the merits of self organizing map to determine the automation, once the training is completed. Although neural network is widely used for grammar inference since it was found good at simulating an unknown function, but it was observed that there is no way to reconstruct the function from the connections in a trained network [40]. A detailed survey of various grammar inference algorithms is presented in [6,39,40,51,52,57]. Inductive inference is the process of making generalization from the input (string). Wyard [3] presented the impact of different grammar representation and experimental result show that the evolutionary algorithm (EA) using standard context free grammar (CFG) (Backus Naur Form (BNF)) outperformed others. Thanaruk and Okumaru [27] classified grammar induction methods into three categories, namely; supervised, semi-supervised and unsupervised depending on the type of required data. Javed et al. [28] presented genetic programming (GP) based approach to learn CFG. The work presented in [28] was the extension of the work done in [3] by utilizing grammar specific heuristic operator. In addition, better construction of the initial population was suggested. Choubey and Kharat [29] presented a sequential structuring approach to perform coding and decoding of binary coded chromosomes into terminal and non-terminals and vice versa. A CFG induction library was presented using the GA contains various Java classes to perform grammar inference process [30,9]. The proposed approach for grammar induction is discussed in Section 2.Section 3 discusses an important issue in GA known as premature convergence. In evolutionary search, the diversity (difference among individual at the genotype or phenotype levels) decreases as the population converges to a local optimum. It is the situation when an extraordinary individual takes over a significant proportion of finite population and leads towards an undesirable convergence. Diverse population is a prerequisite for exploration in order to avoid premature convergence to local optima [44,64]. On the other hand, promoting diversity in an evolutionary process is good where exploitation is needed. Hence, to reach to the global solution and explore the search space adequately, maintaining population diversity is very important. Also, it had been explained clearly that degree of population diversity is one of the main cause of premature convergence [42,43]. It is necessary to select the best solution of the current generation to direct the GA to reach to the global optimum. The tendency to select the best member of the current generation is known as selective pressure. It is also a key factor that plays an important role in maintaining genetic diversity. A proper balancing is required between genetic diversity and selection pressure to direct the GA search to converge in a time effective manner and to achieve global optima. High selective pressure reduces the genetic diversity; hence in this situation premature convergence can occur while the little selective pressure prohibits the GA to converge to an optimum in reasonable time. An approach to increase the population size is not sufficient to alleviate premature convergence because any increase in population size will add twofold cost both extra computing time and number of generations to converge on global optima. Applying mutation alone converts GA search into a random search whereas crossover alone generates a sub-optimal solution. Also, applying selection, crossover and mutation together may result the GA search to noise tolerant hill climbing approach. Several approaches to handle the premature convergence are suggested [41]. Although these approaches address the premature convergence in their ways by maintaining population diversity, these methods suffer with its own strengths and weaknesses. Pandey et al. [41] presented a detailed and comprehensive comparison of various premature convergence handling approaches on the basis of their merits, demerits and other important factors.The focus of this paper is towards development of a grammar induction tool and addressing premature convergence in GA. An effort is made towards utilizing the applicability of the bit masking oriented data structures to apply mask-fill reproduction operators and the Boolean based procedure. The diversity of the population is created through the Boolean based procedure during offspring generation, which helps in avoiding premature convergence in a grammar inference problem using the GA. A learning system uses a finite set of examples and to train the system sufficient training set is needed, which can be achieved employing generalization and specialization. Authors [65–67] discussed the importance of generalization and specialization for the learning systems. It was discussed that overgeneralization is a major issue in the identification of grammars for formal languages from the positive data [40][65]. Therefore, the minimum description length principle is used to address this issue (discussed in Section 2.4). This paper makes five main contributions, as enumerated below:(a).The four steps of grammar induction are presented. An example is considered to illustrate the step by step process of grammar induction.A sequential mapping of chromosome is presented. In addition, the applicability of the parser is shown helps in removing insufficiencies and decide acceptance and rejection of training data.An algorithm “BMOGA” is presented for CFG induction from the positive and negative corpus. Furthermore, we show the crossover and mutation mask-fill reproduction operators and detailed procedure of the new offspring generation.Applicability of the minimum description length principle is shown in CFG induction.We bring to light the effectiveness of the proposed algorithm by comparing it with existing algorithms such as a simple genetic algorithm (SGA), random offspring generation genetic algorithm (ROGGA), elite mating pool genetic algorithm (EMPGA) and dynamic allocation of reproduction operators (DARO).The simulation model for grammar induction is discussed in Section 4 followed by concluding remarks for the paper in Section 5. Lastly but not the least important literatures for grammar inference and approaches to alleviate premature convergence is presented in “References”.Earlier work conducted shows the relatively simple and deterministic CFG has been inferred [3,9,10] using the GA's. A grammar induction library for CFG induction is presented [9,30], which consists of a four Java classes and dealt only with simple languages. A case study on grammar induction is presented in [10], which utilizes the basics of the GA that uses simple crossover (one and two points) and mutation (bit inversion) operators for grammar induction. This section presents a grammar induction algorithm, which takes the advantages of the bit-masking oriented data structure and the GA known as bit mask oriented genetic algorithm (BMOGA) for CFG induction. Fig. 1shows the block diagram of the implemented GA. From the Fig. 1 it can be seen that the BMOGA implemented for the experiment, forms the sub-section of the intermediate population by applying crossover and mutation using the mask-fill operations (separately for crossover and mutation) (Phase-1) and a procedure (P1, P2,crossover, mutation) (Phase-2) and then merges them with the original population to get populated in the next generation. The reproduction operators adapted for the purpose of experimentation are explained in Section 2.2 whereas Section 2.1 talks about the bit masking oriented data structure and its representations. The process of grammar induction can be broken down into four steps:(a)Mapping: Mapping binary chromosome into terminals and non-terminals.Representation in Backus Naur Form (BNF): Apply biasing on the symbolic chromosome to get non-terminal on the left hand side.Production rules generation: Divide the symbolic chromosome to produce the production rules up to the desired length.Resolve the insufficiency: Addressing the issues such as unit production, left recursion, left factoring, multiple production, useless production, ambiguity [7,8], etc.A parser has been implemented for the validity of the CFG rules based on the acceptance or rejection of the rules. An individual is revised in each generation by testing the ability of every chromosome to parse the objective sentence [9].Example 1: A case of palindrome over (0+1)* is given to demonstrate the grammar induction process (Fig. 1). The isomorphic mapping of binary chromosome into terminals and non-terminals is shown. The coding mechanism, i.e. 3-bit/4-bit have been used which depends on the number of symbols used in the language. In the present scenario 3-bit mapping scheme has been applied where symbol ‘S’ represents the start symbol and mapped at “000” and ‘?’ shows the null symbol, mapped at “010” and “110”. Encoding procedure of the grammar maps the binary chromosome into terminals and non-terminals of symbolic chromosome in a sequential manner. The symbolic representation contains the block size of five, equal to the production rule length (PRL=5) chosen for the experiment. Symbolic grammar is traced from start symbol ‘S’ to terminal to remove useless productions. The remaining production rules are tested for removal of left recursion, unit production, ambiguity and left factor. At this stage, string to be tested from the selected sample set is passed for the validity (acceptability of the CFG rules equivalent to the chromosome). The test string and CFG rules are the input to the finite state controller, which verifies the acceptability through proliferation on the push down automata.In evolutionary algorithms (EAs), an individual can only survive depending on its fitness value [1,2]. In case of grammar induction, the fitness value of an individual chromosome largely depends on acceptance and rejection of the positive and negative corpus. It increases for accepting each positive sample (APS) and rejecting each negative sample (RNS) while decreases for the accepting each negative sample (ANS) and rejecting each positive sample (RPS). The number of rules (NPR) also influences the fitness value. As discussed the number of input strings accepted or rejected affects the fitness of an individual chromosome. Therefore, the fitness of an individual chromosome is calculated using Eq. (1), which then map to the grammars in the proposed BMOGA:(1)Fitness=∑C*((APS+RNS)+(ANS+RPS))+(2*C−NPR)S.T.APS+RNS≤Number of positive samples in corpus dataANS+RPS≤Number of negative samples in corpus dataNPR: number of grammar rulesC: constantThe fitness calculation, for example-1: The chromosome size=120 is taken which derives maximum 8 rules. The value of APS=RNS=25 because total 25 each positive and negative sample string are considered for the simulation. In order to achieve the maximum fitness value, ANS=RPS=0 is assumed, i.e. the proposed system is not accepting any negative sample and not rejecting any positive sample. NPR=4, is the number of grammar rules. C is a constant (C=10), is taken, so that grammar with less rules should have high fitness value. Putting these values in Eq. (1), we get Fitness=516. This is the initial fitness value; it increases in a generative manner. The noticeable thing is: any increase in the value of C, would lead to high value of fitness by that factor. But according to the chromosome size, only 8 (=120/15) grammar rules can be extracted. Further, substitution/break, for removal of left recursion and other pre-processing, leads to at most of additional 4–5 rules approximately. Therefore, C=10 (i.e. 2*C=20) is assumed, which differentiate between various grammar based on the number of rules. As discussed, increasing C will produce high fitness value, but it will be just for the sake of increasing the fitness value and not for representing the difference between various grammars. Hence, if the chromosome size is increased to produce more rules, higher value of C may be taken, but there is no need of doing this because by setting C=10, the same task can be done satisfactorily.Iuspa and Francesco [12] presented bit masking oriented data structure to enhance the GA search. The masked data structure was suggested to improve the implementation of crossover and mutation operator as it replaces various algorithms and codify specialized rules of mating. Working of standard procedural method and bit-masking approach was given based on vector function and relative arguments. Compared to standard procedure, bit-masking provides the facility of formal separation between the searching for the proper bit composition and effective string achievement of offspring individuals. As suggested in previous research, binary code based GAs can be grouped into explicit and implicit binary formulation [13]. But using bit-masking approach, there is no need to use explicit data structure. The reason behind this is: only high level operations, working on integer values mapped into a discrete representation domain are executed [12]. To construct the data structure of this type 2-D integer array is used which performs the crossover and mutation operations via two integer arrays known as crossmask (CM) and mutmask (MM).It can be seen from Fig. 2that an integer genome array has been defined to maintain the crossover and mutation operation, where set of integer values are linked with the design variables. Binary images are used to represent the masks and it is used to obtain the crossover and mutation masks. Following convention have been made to represent a binary image: high value, i.e. one or true for the current image bit is pointer to P1 while low value i.e. zero or false is a pointer to P2. Similarly, for mutation mask an integer sequence has been used that indicates its binary image using the following convention: “if the pointed bit of the target string has to be inverted (i.e. high value) or not (i.e. low value)”.To get the generic child individual there is a need of a vector function. The implementation of bit-masking data structure for any real life problem or industrial problems is a two-step process: first apply crossover and mutation mask-fill and then apply mask application (see Sections 2.2 and 2.3) on selected parent strings.CM and MM have been applied to perform the mask fill operations. Three basic optimization procedures are suggested to perform mask-fill for crossover namely: single cut, bit-by-bit and local cut crossover [12].Algorithm-1: cutcrossover (P1, P2)Ndv: Number of design variables, L: String length, R: Keep bit resolution related to each design variable, C: a random cut value ranging from 1 to L-1, RND: Random, Sum: Binary string partial length.S1.Apply cut randomly on (1 to L-1)S2.Initially Set Sum=0S3.For I=1 to Ndv doS4.Set Sum=Sum+R (I)S5.If Sum<C thenS6.Set Mask (I)=-1S7.ElseS8.Set J=IS9.If J ≠ Ndv thenS10.For K=J+1 to NdvS11.Set Mask (K)=0S12.ElseS13.Set I=NdvS14.End forS15.Set Mask (J)=2R(J)–2Sum-CFig. 3depicts the working of cutcrossover (P1, P2). The string length chosen for the example is 60. The genetic heritage from parent-to-child strings can be maintained through high/low bit strings. The value −1 (minus one) has been filled in the first section of CM array which indicate a first parent gene preemption. Under 2's complement codification, −1 indicates the whole binary string of the current mask array element set to high [12]. The second section is filled with value 0, indicates that genetic materials are coming from the second parent string, whereas the third section represent the P1 to P2 bit transition and to fill this part Eq. (2) has been used:(2)CMtrC=2Rtr−2Sum(tr)−Cwithtr∈[1÷Ndv]where tr represents P1 to P2 bit transition.Algorithm-2: bitbybit (P1, P2)S1.For I=1 to Ndv doS2.Set Mask (I)=INT (RND*R (I))–1S3.End forAlgorithm-3: localcut (P1, P2)S1.For I=1 to Ndv doS2.Set C=RND [1÷R(I)–1]S3.Set Mask (I)=2R (I)–2CS4.End forSimilar to Eq. (2), P1 to P2 bit transition is given for Algorithm-2 and Algorithm-3 [12]. The key benefit of applying this mechanism is no extra operation is needed for bit manipulation to set the appropriate integer value according to the associate resolution. However, the redundant bit set to high for the selected bit resolution for the current design variable is ignored via specific Boolean function based technique applied to assembly of child individuals.On the other hand, the mutation mask-fill operation is similar to the classical random binary inversion driven, depending on the specific mutation rate. Eq. (3) is used to perform mutation mask [12]:(3)mutmaski=∑j=0Rj−12j.δlmi=1,Ndvwhereδlm=l=mifrand<MutRatel≠motherwisewhere, δlmis the Kronecker operator.Kronecker product is a special operator used in matrix algebra. It gives the possibility to get a composite matrix of elements of any pair of matrices [31][32]. The merit of Kronecker product is that it works without the assumption on the size of composing matrices, so it is well suited for large data. Equation (3) clearly explains that if the random value rand is less than the mutation rate MutRate then apply the binary inversion otherwise there is no need of applying the mutation mask fill.The new offspring generation process starts once the crossover and mutation masks are filled in with the proper values to excel the chosen crossover and assigned mutation rate. The new offspring is generated using special Boolean based operators, which utilize parent string, CM, and MM. A couple of parent string must be chosen using an appropriate selection technique [11]. We have used roulette wheel selection technique for the proposed approach. Two complementary child vectors, as to crossover operator are generated, as given in Eq. (4).(4)off1=f1(P1,P2,CM,MM)off2=f2(P1,P2,CM,MM)where offj, Pjand fj(j=1,2) are respectively the offspring, parent vectors and a Boolean function, which determines assembly style of new individuals. The arguments CM and MM are used to find the suitable crossover and mutation rules.For the sake of simplicity, i.e. to understand crossover and mutation operation effectively, Eq. (4) can be converted into a new form to show both crossover and mutation operation separately.Eq. (5) represents the crossover vector, a binary image, which allows P1 or P2 to child bit transfer as per the correlated CM value:(5)off1=(P1ANDCM)OR(P2AND(NOTCM))off2=(P2ANDCM)OR(P1AND(NOTCM))Eq. (6) expresses the mutation section of Eq. (4), under the situation that a single MM vector of both child strings is set.(6)offj=offiXORMMThe step by step mechanism of generating the new offspring is depicted in Fig. 4, whereas Fig. 5demonstrates the genetic reproduction (application of Eqs. (5) and (6)) with the help of an example. The interesting thing to note that, as P1, P2, CM and MM vectors are considered as an argument of the function (f1 and f2); new individual has no strict correlation with a specific type of crossover scheme or parent pairs, as happens in an explicit binary formulation depending on the algorithm. In some situation, if the evolutionary strategies needed some couples for an identical crossover such as bit-by-bit crossover with a constant random seed, the only operation to perform and fill the mask properly, apply Eq. (5) multiple times, changing the selected parent pairs only. The Boolean based procedure introduces diversity during reproduction operations, which helps in maintaining the diversity in population and therefore avoids the premature convergence.The problem with inductive and statistical inference systems is: How to take decision for selecting an appropriate model that should present the competing explanation of data using limited observations? An envision where a sender who wants to transmit some data to the receiver and therefore they want to select a best model which can maximally compress the observed data by which data can be delivered to the receiver using as few bits as possible.Formally, the selection of the best model is the process to decide among model classes based on the data. The Principle of Parsimony (Occam's razor) is the soul of model selection, states that given a choice of theories, the simplest is preferable [4,5]. In other words, the purpose to implement Parsimony Principle is to find out a model which can fit well to the data. Rissanen (1978) extracted the essence of Occam's theory and presented the Principle of Minimum Description Length (MDL): “choose the model that gives the shortest description of data” [4,14].For any set of corpus, one can construct a grammar without using the MDL that does not reflect regularities in data as depicted in Fig. 6(a). In such situation, constructed grammar can be considered as very simple grammar since it simply shows the validity of any combination of words. In this case, grammars do not show any regularity, hence high amount of information needed to specify them. At the opposite, one can construct grammars which can list all possible sentences/corpus but not for all sentences since grammars are not capable in terms of generalization (see Fig. 6(a)). Although this type of grammar shows some sort of regularity, but fails to present a generalization since it contains the information about each observed corpus therefore always shows poor performance and assumed to be very complex. On the other hand, construction of grammar using the MDL shows both regularities in the data and make generalizations beyond the observed corpus (see Fig. 6(b)). Therefore, the MDL behaves as a middle level and fills the gaps presented in Fig. 6(a).Bayes theorem can be used to derive the MDL principle, but the working of the MDL principle is not similar to the Bayes, since the MDL uses code length rather probabilities [4,14]. The MDL was used widely in grammar induction problem [5,15–18].In order to see the working of the MDL, an example of L1=(10)* is presented (see Fig. 7). (1) First ellipse indicates the sample space of positive and negative training data for L1=(10)*. (2) It can be seen that initially we get very complex CFG rules with a very less fitness value which can be refined by applying the GA reproduction operator in each generation where MDL helps in compressing grammar rules and to generate positive and negative string set required during the execution. (3) After a few generations, we get a simple grammar, but non-constraint CFG rules. (4) But when the GA search process reaches to threshold/termination condition, it produces grammar's rules with fitness value. We can assume such grammars as good CFG rules with best fitness value. In the fourth ellipse six CFG rules are provided: first CFG rules have NPR=3, fitness value=1013. In second, third and fifth CFG, NPR=4, fitness value=1012 but here the noticeable thing is rules generated are different for the same language. At fourth CFG, NPR=5, fitness value=1011. In case of sixth CFG NPR=2, fitness value=1014, it indicated that the MDL compressed the data more in case of sixth CFG rules with maximum fitness value and therefore system learned more.In the present scenario, for selecting the corpus, strings of terminals are generated for the length L from the given language. Initially, L=0 is chosen, which gradually increases up to the required length to represent the language features. Here, a corpus of twenty five positive and negative strings is found to be sufficient to represent the chosen languages L1 through L8 for CFG induction.The algorithm implemented using the bit-masking oriented data structure with crossover and mutation mask-fill operator is presented in this section. A procedure (P1, P2, CM, MM) is given in step 4 of the proposed algorithm to enter from ith generation to (i+1)th generation. As soon as the system enters into the new generation, an individual population is updated with its fitness value, merges the population, and accordingly updates the best individual. Algorithm-4 is given below shows the step by step working of the proposed model.Algorithm-4: CFG Induction uses BMOGATerminology Used: TER: Terminals, NTER: Non-terminals, PSIZE: Population Size, BI: Best Individual, THERD: Threshold (maximum value of fitness achieved), PO: Population, NEWPO: New Population, BS: Binary String, TOR: Total Runs, UPFIT: Updated Fitness, APS: A set of accepted positive strings, RNS: A set of rejected negative strings, RPS: A set of rejected positive strings, ANS: A set of accepted negative strings, NPR: Number of production rules, P1, P2: parents pair, CM: crossmask, MM: mutmask, Temp: Temporary individuals, OS: offspring, Gmax: Maximum number of generations.Input: Set of corpora (positive and negative)Output: Context Free Grammar rules with fitness value and elapsed time.S1. Initialize random population up to PSIZE.S2. Grammar Induction Process• Generating variable length chromosome• Use sequential mapping to map of BS into TER or NTER and represent the grammar using Backus Naur Form.f:V→B and f:Σ*→Bh:B→V and h:B→Σ*Where, V represents the set of non-terminals and ∑ represents the set of terminals, while B represents set binary chromosomes, which is represented as 3-bit or 4-bit binary equivalent of non-terminals and terminal, f and h are the function used for mapping.• If (TER<4 AND NTER<4) ThenApply 3-bit representationElseApply 4-bit representation• If (BS==“010” OR BS==“110”) thenSet TER ←NULL (?) //’?’ indicate nullElseSet symbol as appropriate• Eliminate left recursion, left factoring, multiple production rules, unit production if presentS3. Evaluate Fitness of an individual chromosomeFitness=∑C*((APS+RNS)+(ANS+RPS))+(2*C−NPR)S4. Apply loop for performing GA operators• Until (BI >THERD) OR (TOR = = Gmax)• Apply crossover and mutation mask-fill operation using bit-masking oriented data structure.• Apply Boolean procedure based mixture (P1, P2, CM, MM)P1, P2 ← Select parent pairs via selection techniques as appropriate.CM← initialize crossmaskMM← initialize mutmaskPerform Temp1 ←P1 AND CMPerform Temp2 ←P2 AND (NOT CM)Perform Temp3 ←P2 AND CMPerform Temp4 ←P1 AND (NOT CM)Perform OS1 ←Temp1 OR Temp2Perform OS2 ← Temp3 OR Temp4Update OS1 ← OS1 XOR MMUpdate OS2 ← OS2 XOR MMUse OS1 and OS2 in next generation as appropriate.• NEWPO ← PO after crossover and mutation• UPFIT←FITNESS (updated fitness)• Merge the population and Update the BIS5. Display CFG rules, fitness value and elapsed time.S6. StopThe study is conducted on four different algorithms: a simple genetic algorithm (SGA), random offspring generation genetic algorithm (ROGGA), elite mating pool genetic algorithm (EMPGA) and dynamic allocation of reproduction operator (DARO). ROGGA, EMPGA and DARO are mainly proposed to address premature convergence. The SGA is given to lay down the basic understanding of the GA, whereas EMPGA and DARO algorithms are chosen because these approaches were implemented on CFG induction problems [25].SGA works using initial random population of fixed length chromosomes. The SGA starts with a set of solutions represented by chromosomes. Using an appropriate selection technique a solution from one population is picked depending on its fitness and used to form a new offspring. This process is repeated until the GA reaches to the threshold or the maximum number of generations. Fig. 8shows the flowchart of the SGA. For the purpose of an evolution, the SGA performs a single crossover and a single mutation operator for reproduction. Then the individuals are picked depending on their fitness value to act as parents to generate offspring in the new generation.Nicoară [33] presented an advanced version that uses more than one crossover and mutation reproduction operators, known as crossover-mutation combination reproduction to produce offspring based on their performance in the previous generation. Choubey and Kharat [25] executed the SGA multiple times in their proposed approach (EMPGA) to address premature convergence creating a mating pool (see Section 3.3).Rocha and Jose [19] presented the ROGGA to resolve premature convergence. Fig. 9shows the working of the ROGGA and how this approach prevents premature convergence. In this approach before applying reproduction operations, a test for similarity of the genetic material is done first. If found similar then generate a random offspring which will produce a random solution for the problem otherwise apply reproduction operators in normal fashion.Two different strategies were used, namely 1-RO (one offspring per two parents) and 2-RO. In case of 1-RO only one offspring was generated and other one obtained from their parent to reach to the result while in 2-RO both the offspring was randomly used. It was well proven that the ROGGA perform better than the adaptive mutation rate (AMR) technique [20] and social disaster techniques [21]. Rocha and Jose [19] accepted that the weakness of this approach is the computational overheads disregarded since there is no change in the selection methodology compared to other approaches such as Crowding [22], Sharing [24] and Scheduled Sharing [23].Choubey and Kharat [25] presented the EMPGA approach to prevent premature convergence and for context free grammar induction. Here, the SGA was executed multiple times on different populations to choose the best solution. The EMPGA approach works in the following manner:S1Generate a mating pool of size n<N, where N is the initial random population.S2Calculate the fitness of all individuals in the population.S3Sort the mating pool on the fitness of the individuals.S4Perform crossover of the first parent with the elite member of the mating pool and create two offspring.S5Replace old weaker individuals in the mating pool with newly created offspring in sorted order.S6Perform mutation on the newly created offspring.S7Replace old weaker individuals in the mating pool with newly created offspring in sorted order.S8Once repeat steps S4 through S7.In order to test the effectiveness of the EMPGA, GA generations were processed with various probabilities ranging from 0% to 50%. The EMPGA had shown the capability of breaking the local optimum convergence. The main issue with this approach was extra processing time and spaces are required in maintaining the separate mating pool to get the elite members. Authors [25] accepted that the execution time of the EMPGA is assumed to be greater than a normal SGA approach.Choubey and Kharat (2011) extended the work done in [33] and presented a Dynamic Application of Reproduction Operator (DARO) [25] where equal probability was assigned to each reproduction operator combination (crossover-mutation operator combination, CMOC) to begin the genetic algorithm generations. Three different crossover strategies, namely two Point Crossover (TPC), Two-Point Crossover with Internal Swapping (TPCIS) and Uniform Crossover (UC) and four mutation techniques such as: Stochastic Mutation, Inverse Mutation, Block Copier with Fixed Length Mutation and Block Copier with Random Length Mutation were implemented in DARO. The same set of languages (languages used for EMPGA [25]) was considered for the experiment purpose. It was found that this method works effectively for simple languages, but there is scope for applying this method for more complex grammar sets. It was also observed that the DARO consume comparatively less time than the EMPGA, but it is not fit to handle the situation of local optimum convergence, whereas the EMPGA approach handle such situation effectively.In order to evaluate the performance of the BMOGA experiments have been conducted, which utilize CM and MM for the CFG induction using a set of positive and negative corpora. Java programming on Net Beans IDE 7.0.1, Intel Core™ 2processor (2.8GHz) with 2GB RAM have been used. Context Free, and Regular languages have been chosen with varying patterns of 0's and 1's is given in Table 1. Table 2shows the parameters used for the implementation of the proposed algorithm. The MDLP has been used to generate the positive and a negative string set required during the execution [5].GA is a stochastic search technique and therefore, the results are collected as the average of ten runs for each language. The GA search process continues until it reaches to a maximum number of generations or reaches to the threshold, where threshold indicates the highest rank solution's fitness. This stopping criterion is common for each algorithm implemented. The results show that the presented approach is capable for CFG induction. MDL principle is applied to generate a proportionate number of positive and negative sample strings. The masks fill operation for crossover and mutation methods have been found effective in improving the performance. It was observed that the MDL principle works effectively in selecting the correct sample strings with minimum length. For the validation purpose experimentally obtained grammars have been tested against the best known available grammar. The standard representation <V, Σ, P, S> is considered to show the best grammar. Table 3shows the best generated grammars with a fitness value (FV) and total number of rules received.In syntactic pattern analysis number of rules represent the pattern is a crucial factor and it must be kept minimum. Therefore, comparative analyses have been done based on the number of grammar's rules obtained for the languages L1 through L8. The Average Production Rules (APR) has been computed for each GA approach as presented in Table 4. Eq. (7) is used to compute the APR in which term NPR represents the Number of Production Rules obtained from one (the best) chromosome, when the search process reached to a maximum number of generations or to the threshold. For example, the APR value for L1 in case of the BMOGA is 2.8 (see Table 4). As discussed, the GA is a stochastic search algorithm, the results are collected for ten runs, for which the value of NPR's is respectively 3, 2, 3, 3, 3, 3, 4, 3, 2, and 2, which results APR=2.8 after applying Eq. (7).It is clear from Fig. 10that for each language L1 through L8 the value of the APR obtained implementing the BMOGA is the lowest, i.e. it outperformed other GAs. It can be seen from Fig. 10 and Table 4 that the EMPGA, ROGGA and the DARO showed the average values of the APR whereas the SGA showed the maximum value of APR.The success ratio for each algorithm is calculated helps in judging the suitability of each approach. The high value of success ratio indicates that the approach is more suitable for grammar induction. Eq. (8) has been used for calculating the percentage of success ratio.(7)APR=∑i=1nNPRi∑j=110NRjwhere NRjrepresent the number of runs and NRj=1 is chosen.(8)Succ. Ratio=∑i=1nNOPRi∑j=110NRj×100where NOPR: number of optimal production rules is taken from (the best) chromosome and NRj(NRj=1 is chosen) represent the number of runs. The term NOPR indicates production rules for which maximum value of fitness is achieved. It indicates the learning capability of the system. The following conventions are followed: production rules with a maximum value of fitness indicate that the system has learned more and vice versa.Table 4 shows the percentage success ratio for each language. The value of Succ. Ratio is found higher for the BMOGA compared to other algorithms: SGA, ROGGA, DARO and EMPGA for the chosen languages. Table 5shows the execution time consumed to reach to the threshold value. It is depicted in Fig. 11that the computational cost of the BMOGA is slightly higher in reaching to the global solution since the whole process runs in two separate phases. Although, the execution time for the BMOGA is comparatively higher than the other GAs, but it produces better results in both the aspects: APR and success ratio.Table 6presents generation range, mean and standard deviation. The results are collected as the average of first successful ten runs. The number of generations taken over ten generation run varies, therefore generation range is given. The phenomenon involved with the generation range can be understood with the help of an example: the generation range for L1 in case of the SGA is 6±2, means the number of generations taken over ten generation run varies between 04 (6−2) and 08 (6+2) similarly for others.A statistical test is conducted to evaluate the performance significance of the BMOGA compare to the other GA approach. F-test is conducted on the collected sample considering the hypothesis: “there is no significant difference in the mean of samples at the 5% level of confidence” i.e.H0:μ1=μ2=μ3=μ4=μ5HA: At least one mean is different than others.F-test is used to test whether two samples may be regarded as drawn from the normal population have the same variance. The detailing of the F-test is given in [37]. The purpose of applying the F-test based on the analysis of variance (ANOVA) is to verify whether the samples collected from the five algorithms lie within the same group or is there any possibility that one or the other methods different from the group. The total of 15 samples was drawn from each algorithm for the randomly chosen languages. The descriptive analysis is depicted in Table 7and Fig. 12graphically displays the means for the five algorithm groups. The X-axis represents the five approaches (SGA, EMPGA, ROGGA, DARO and BMOGA). The Y-axis represents the estimated marginal mean fitness values. The performance of the BMOGA is found better than the other approaches whereas the EMPGA showed slightly better performance than the DARO and ROGGA, and the SGA's performance is found worst. The main ANOVA result is given in Table 8. The significance value (p=0.001) comparing the group (algorithms) is less than 0.05 (0.001<0.05), so we could reject the null hypothesis. The result indicates that one of the samples is better than the other ones. However, this result does not indicate which methods are responsible for the difference, therefore it is necessary to apply post hoc test.The post hoc tests compare individual group results with each other. The least significant difference (LSD) test is applied. It was developed by Fisher with the purpose to explore all possible pairwise comparisons of means comparing a factor (in this case fitness) using the equivalent of multiple t-test. It is interesting to note that, there are various tests one can choose, and they may produce different results. Table 9shows the result of multiple comparisons using the LSD method. First column “(I) Samples” and the second column “(J) Samples” represent the combination of the paired algorithm for comparison. For example, the first row contains SGA (in (I) Samples) compared with the other approaches (EMPGA, ROGGA, DARO and BMOGA) given in the (J) sample column. The mean difference (I−J) represents the mean difference of (I) samples and (J) samples. In case of the SGA-EMPGA pair: mean of SGA and EMPGA is 666.731867 and 777.673333 respectively given in Table 7 and the difference is −110.9414667 similarly for others. One need to pay attention to the astrix mark (*) given next to the mean difference, indicates the difference is significant. The last row of Table 9 indicates that the mean difference of the BMOGA and the other algorithm is significantly different, since the mean difference has astrix (*) marks immediately to the difference value. The significant difference can also be verified by comparing the p-value present in column five of Table 9. If the obtained p-value is less than 0.05, then the algorithmic combination is significantly different. It can be seen that the algorithmic combination BMOGA and others (SGA, EMPGA, ROGGA and DARO) has the p value 0.000, .027, .004 and .003 less than 0.05, so the mean of the BMOGA is significantly different than the other algorithms and therefore we can conclude that the BMOGA's performance is better than other approaches.Although the LSD produces results accurately, it is very sensitive to the violation to the assumptions of the ANOVA, so the most likely to lead to the Type 1 error, i.e. rejecting the null hypothesis when it is true. To avoid this situation, the Tukey HSD (Tukey–Kramer honestly significant difference) test is also conducted. It is very similar to the LSD test, but less liable to a Type 1 error. In addition, it is suitable for the experiment that have an equal number of observations (in our case N=15).Here, pay attention to the last row of Table 10, the pairwise comparison for the BMOGA-SGA is strongly significantly different (0.000<0.05), BMOGA-ROGGA has been significantly different since 0.034<0.05, BMOGA-DARO has also been significantly different since 0.027<0.05, but BMOGA-EMPGA is not significantly different since 0.170>0.05. Hence, it can be concluded that the performance of the BMOGA is better than SGA, ROGGA and DARO but slightly better than the EMPGA or the performance of the BMOGA and EMPGA is almost similar. Homogeneity test is conducted to verify the similarity using the TukeyHSD test. The means of algorithms in homogeneous subsets are shown in Table 11. Looking at Table 11, it can be seen that the SGA, ROGGA and DARO fall in the group1 at 0.211 significance level, whereas the BMOGA and EMPGA are in the same group at 0.170 significance level, since both the algorithms showing almost the similar performance (where BMOGA is slightly better than EMPGA).The comparative generation chart (fitness vs. generation) for each language is depicted in Fig. 13. Overall, the performance of the BMOGA is found better than other algorithms except for L6 for which each method showed the similar performance (SGA showed worst). For each language L1 through L8 the BMOGA outperformed other approaches. It was observed that the EMPGA showed a higher tendency of getting local minima during the execution.

@&#CONCLUSIONS@&#
In this paper, we have presented the BMOGA, when compared with other approaches such as SGA, ROGGA, EMPGA and DARO is found effective for the CFG induction. The analysis explains the effectiveness of the proposed approach. Although the execution time required is higher but it showed promising results in terms of APR and success ratio. It was observed that the BMOGA outperformed SGA, ROGGA, DARO and showed better results than the EMPGA. Though the execution time varies from language to language, it follows the way that for complex language, it needs more time and vice versa. The role of the MDL is discussed, i.e. how the MDL can be used to generate the corpus of desired length to represent the language features. However, it is thereby found that the BMOGA is a better choice for grammar induction process since it produces optimal results. An important issue of the GA known as premature convergence and its effects is discussed. It was observed that the bit masking oriented data structure and the Boolean based procedure helps in alleviating premature convergence by introducing diversity in the population during offspring generation. It was discussed in [41] that the ROGGA perform better than the adaptive mutation rate (AMR) technique [20] and social disaster techniques [21], whereas the DARO showed better results than the work conducted in [33]. Therefore, one can say that the BMOGA can also perform better than the AMR, social disaster techniques and the work done in [33]. The F-test is conducted to verify the hypothesis H0 and HA (see Section 4), the results indicates that one of the samples showed better results than others, hence rejects the null hypothesis. In order to understand, which method is responsible for the difference, the post hoc tests (LSD and TukeyHSD) are conducted, which indicates that the BMOGA perform better than the SGA, ROGGA and DARO, whereas the performance of the EMPGA is close to the BMOGA. Although, the comparative results showed the suitability of the BMOGA for grammar induction and in addressing premature convergence, but still there is scope for improvements. Some of them are: improving the fitness function dynamically by setting the weight factor for every training example, applying it for more difficult problems such as natural languages and it would be interesting to compare the proposed algorithms with other approaches given to avoid premature convergence since it showed the tendency of getting trapped at local optimum convergence in some situations.