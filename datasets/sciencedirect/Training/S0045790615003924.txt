@&#MAIN-TITLE@&#
Digital hardware implementation of a radial basis function neural network

@&#HIGHLIGHTS@&#
A high accuracy method to compute the Gaussian function in the radial basis function neural network (RBF NN) is proposed.The computational accuracy of the propose method in the Gaussian function and the overall RBF NN can reach up10−6–10−7.Fast computational ability for a complicated RNF NN is presented by using the digital hardware implementation.Succeed in making the co-simulation environment between Simulink and ModelSim to verify the correctness and the effectiveness of the VHDL (Very high-speed IC Hardware Description Language) code for a 3-5-1 RBF NN.The application of the VHDL code for a 3-5-1 RBF NN to the dynamic identification in the linear system and the PMSM drive system are successfully demonstrated.

@&#KEYPHRASES@&#
Radial basis function neural network (RBF NN),Stochastic gradient descent (SGD),Very high-speed IC hardware description language (VHDL),Simulink and ModelSim co-simulation,Electronic design automation (EDA),Permanent magnet synchronous motor (PMSM) drive,

@&#ABSTRACT@&#
Graphical abstractImage, graphical abstract

@&#INTRODUCTION@&#
Differing from other conventional computing, neural networks have the capability of capturing unnecessarily deterministic nonlinear and take the advantages of the fast, highly accurate computation for the non-parametric model. Radial basis function (RBF) becomes a very powerful tool in neural network architecture due to its faster learning and higher nonlinear activated function used in hidden layer, compared with back propagation topology [1]. In particular, RBF neural networks process the information responds to inputs according to a defined learning rule and then the trained network can be used to perform certain tasks depending on the application. Many certain problems such as human face recognition [2], functional approximation, nonlinear system identification, motor control [3] etc. are considered to solve in the practical applications of neural networks. Due to their vigorous approximation properties, RBF neural networks can present a good mapping on the complex models [4].The RBF NN is specified by a three-layer with a simple topology, where the hidden unit implements a radial basis activated function. These networks have a good ability to identify or approximate the function of unconventional systems with the given set of inputs and their corresponding outputs. To fit the network outputs to the given inputs, the networks need to be trained to optimize their parameters. If the parameters of the RBF are fixed, in other words, the position of neurons and the “bell curve” of exponential function could not be changed, the RBF effectiveness reduces gradually to the linear problem solving. To cover the highly nonlinear problems, the sufficient learning algorithm that allows modifying the parameters of RBF neural network [5] is introduced, called a stochastic gradient descent (SGD) method. The SGD [6] method is a popular algorithm for training a wide range of models by following the negative gradient of the objective after using only a single or a few training examples and computes the next updated parameters at each iteration. It tends to converge very well to global optima when the objective function is convex or pseudo convex, otherwise converge to local optima [7]. Therefore, its adaptability due to the ability of adjustable network parameters may lead to improving the accuracy and the stability of approximations.As the preliminary survey on several papers, some studies give the various methods for the implementation of artificial neural networks (ANN) in a digital system. One of the basic problems in the research [8–10] is how to implement the ANN in fixed-point arithmetic as well as floating point behavior due to the nonlinear characteristic of exponential activation functions and perform the good responses undergoing the associated training algorithm for the particular task. Although the floating point number formats are more precise in computation [11–13] but they spend much hardware resources in the FPGA (Field programmable gate array) than the fixed-point number formats. To solve this problem, a 32-bit Q24 fixed-point number is firstly adopted to reduce the hardware resources and still assure high numerical accuracy. The next, this work proposes an efficient algorithm by combining the Taylor series expansion and a look-up table (LUT) to calculate the exponential activation function in RBF NN. For a 32-bit Q24 numerical type, the 24-bit length in the fraction part (Q24) can make the RNF NN or nonlinear Gaussian function have a good numerical precision. Although the range of the integer part (8-bit) is only between +127 to –128, it still can cover the data operation to avoid the numerical overflow condition occurred if the normalized/de-normalized operation in input/output data is applied.In realization, to speed up the computing performance of RBF NN, FPGA [14–16] gives the best solution for this application due to its characteristics of the programmable hard-wired feature, fast computation power, higher density, etc. However, to describe the behavior of computing the RBF NN in FPGA implementation, VHDL, and Open computing language (OpenCL) [17–19] provide two possible programming tools. VHDL uses in electronic design automation (EDA) to describe digital and mixed-signal systems such as FPGA and integrated circuits [20]. VHDL can also be used as a general purpose parallel programming language. OpenCL is a framework for writing programs that execute across heterogeneous platforms consisting of central processing units (CPUs), graphics processing units (GPUs), digital signal processors (DSPs), FPGAs and other processors [20]. OpenCL provides parallel computing using task-based and data-based parallelism. In this study, VHDL is adopted to describe the behavior of computing the RBF NN, and finite state machine (FSM) is applied. Due to the FSM belongs to the sequential processing method; the FPGA resources usage can be greatly reduced.Recently, a co-simulation work by electronic design automation (EDA) Simulator Link has been applied to verify the effectiveness of the VHDL code in a digital system [21]. The EDA Simulator Link provides a co-simulation interface between Simulink and ModelSim [22]. This work intends to verify the performance of RBF NN function in a co-simulation work constructed by Simulink and ModelSim based on EDA simulator link. This sufficient correlative tools strongly provide the obvious results for comparing with the floating point results in Matlab. Therefore, in this paper, a co-simulation by EDA Simulator Link is applied to design and verify the proposed computation algorithm for RBF NN. The input stimuli and output results are performed in Simulink and the algorithm to compute the RBF NN is executed in ModelSim.The remainder of this paper is organized as follows: Section 2 introduces the architecture of RNF NN with SGD-based learning mechanism. Section 3 describes the digital hardware implementation of RBF NN. Section 4 illustrates the simulation results to evaluate the performance of the proposed method. Section 5 gives a summary of the contribution in this work.This section aims to give a brief introduction of the structure and specifically orient to the mathematical analysis of RBF NN [23]. However, the RBF NN adopted here is a three-layer architecture that is shown in Fig. 1. It comprises of one input layer, one hidden layer, one output layer and one supervising learning machine using stochastic gradient descent method.In neural architecture, the input layer is mapped on hidden layer via the nonlinear activated function or radial basis functions (known as neurons), whereas the connection from the hidden layer to output layer performs a linear transformation. The input layer has n1 inputs by x1,x2,…,xn1and its vector form is represented by(1)X=[x1x2…xn1]TIn hidden layer, the multivariate Gaussian function is chosen as the activated function, and its formulation is shown as follows.(2)φi=exp(−∥X−Ci∥22σi2),i=1,2,…,n2where n2 is the number of neuron in hidden layer,Ci=[ci1ci2…cin1]T,σidenote the node center of radial basis function and node variance (or width) of ith neuron, and∥X−Ci∥is the norm value (Euclidean distance) which is measured by the inputs and the node center at each neuron. Finally, the network output in Fig. 1 can be written as(3)yj=∑i=1n2wjiφi,j=1,2,…,n3where yjis the jth output value, wjiis the weight from the jth output to the ith neuron, φiis the output of the ith neuron, and n3 is the output number.In RBF NN, learning algorithm plays an important role for determining the optimal cost function that lets the specified error between the output of the neural network and the desired output become minimize. The learning algorithm herein is based on the stochastic gradient descent (SGD) method that has an advantage of being faster computation than the others. This approach is for training RBF NN by tuning the network's parameters, such as the weights, node centers and node variances of the radial basis function, to get a good convergence. For the SGD-based supervised learning process, the error instantaneous cost function is firstly defined as(4)J=12∑j=1n3ej2=12∑j=1n3(yjd−yj)2where the error ejis the difference between the desired output yjdand the jth output yjin RNF NN. Secondly, the updated equations for the parameters of RBF NN based on SGD method are given by(5)wji(k+1)=wji(k)−μw∂J∂wji(k),i=1,2,…,n2andj=1,2,…,n3(6)cir(k+1)=cir(k)−μc∂J∂cir(k),i=1,2,…,n2andr=1,2,…,n1(7)σi(k+1)=σi(k)−μσ∂J∂σi(k),i=1,2,…,n2where μw,μc,μσare the learning rate. Thirdly, applying the chain rule,(8)∂J∂wji=∂J∂ej∂ej∂yj∂yj∂wji(9)∂J∂cir=∑j=1n3(∂J∂ej∂ej∂yj∂yj∂φi)∂φi∂cir(10)∂J∂σi=∑j=1n3(∂J∂ej∂ej∂yj∂yj∂φi)∂φi∂σiFurthermore, from (3) and (4), we can get(11)∂J∂ej∂ej∂yj∂yj∂wji=−ejφiand(12)∂J∂ej∂ej∂yj∂yj∂φi=−ejwjiAlso, from (2), it can be obtained(13)∂φi∂cir=φixr−cirσi2and(14)∂φi∂σi=φi∥X−Ci∥2σi3Finally, substituting (11)–(14) into (8)–(10) then into (5)–(7), the updated equations for the parameters of the weights, node centers and node variances in RBF NN can be shown as follows(15)wji(k+1)=wji(k)+μejφi,i=1,2,…,n2andj=1,2,…,n3(16)cir(k+1)=cir(k)+μ(∑j=1n3ejwjiφixr−cir(k)σi2),i=1,2,…,n2andr=1,2,…,n1(17)σi(k+1)=σi(k)+μ(∑j=1n3ejwji)φi∥X−Ci∥2σi3(k)),i=1,2,…,n2whereμ=μw=μc=μσis a learning rate.This section shows a detailed digital hardware implementation of RBF NN. However, we firstly introduce the concept of finite state machine (FSM). Then use FSM to design the RBF NN.To reduce the use of the FPGA resource, FSM is adopted to describe the complicated control algorithm. Herein, the computation of a sum of product (SOP) shown below is considered as a case study to present the advantage of FSM.(18)Y=a1*x1+a2*x2+a3*x3Two kinds of the design method that one is parallel processing method and the other is FSM method are introduced to realize the computation of SOP. In the former method, the designed SOP circuit is shown in Fig. 2(a), and it will operate continuously and simultaneously. The circuit needs two adders and three multipliers, but only one clock time can complete the overall computation. Although the parallel processing method has fast computation ability, it consumes much more FPGA resources. To reduce the resource usage in FPGA, the designed SOP circuit adopted by using the FSM method is proposed and shown in Fig. 2(b), which uses one adder, one multiplier and manipulates 5 steps (or 5 clocks time) machine to carry out the overall computation of SOP. Although the FSM method needs more operation time (if one clock time is 20ns, the 5 clocks needs 0.1 microseconds) than the parallel processing method in executing SOP circuit, it does not lose any computation power. Therefore, the more complicated computation in the algorithm, the more FPGA resources can have economized if the FSM is applied. Further, VHDL code with 32-bit Q24 data type to implement the computation of SOP is shown in Fig. 3.The random selection of the number of neurons in hidden layer might cause either overfitting or underfitting problems. However, using many neurons in the hidden layer can give much information processing capacity but increase the training time in artificial neural networks and spend many hardware resources in FPGA. There are some rules for determining the suitable number of neurons to use in the hidden layer [24], as follows:-The number of hidden neurons should be between the size of the input layer and the size of the output layer.The number of hidden neurons should be 2/3 the size of the input layer, plus the size of the output layer.The number of hidden neurons should be less than twice the size of the input layer.According to the suggestion as mentioned above, a basic scheme of a 3-5-1 RBF NN is considered as a developed example in this paper.In realization, a 3-5-1 RBF NN with three inputs, five neurons and one output structure in Fig. 4(a) is used as a designed example to evaluate the proposed method, and its corresponding digital hardware component is shown in Fig. 4(b). In this Figure, the whole network is specified by two main components. One component performs the function of the forward computation, and the other component performs the function of the learning algorithm based on SGD method. To reduce the digital hardware resource usage, a finite state machine (FSM) is employed, and VHDL is applied to model the behavior of these two components. Further, the data type adopts 32-bit length with Q24 format and 2’s complement operation in the numerical system.In the forward computation component, the main inputs are the network inputs X, weights wji, node centers Ciand node variances σi, as well as the outputs are the network out yiand the neuron outputs φi. The internal circuit design of the forward computation component in Fig. 4(b) is presented in Fig. 5which uses five neuron components, two adders, and two multipliers and manipulates 25 steps machine to carry out the overall forward computation. In Fig. 3, the steps s0–s19 execute the parallel calculation of five neuron components which perform the Gaussian function in (2); and s20–s24 perform the computation of network output in (3). The operation of each step in Fig. 5 is 20 ns (50 MHz); therefore total 25 steps need only 500 ns operation time.The neuron component in Fig. 3 performs the Gaussian function in (2). The digital hardware implementation of the Gaussian function is very complicated due to it needs to execute the exponential function. To solve this problem, the combination of Taylor series expansion technique and look-up table (LUT) technique is conduced. Firstly, the exponential formulae−xis computed by using Taylor series expansion. The equation is shown as follows.(19)f(x)=f(x0)+f′(x0)(x−x0)+f′′(x0)2!(x−x0)2+f(3)(x0)3!(x−x0)3+f(4)(x0)4!(x−x0)4+f(5)(x0)5!(x−x0)5+⋯The fifth order polynomial equation within the vicinity of x0is considered to expand as follows.(20)f(x)=e−x≅a0+a1(x−x0)+a2(x−x0)2+a3(x−x0)3+a4(x−x0)4+a5(x−x0)5with(21)a0=f(x0)=e−x0(22)a1=f′(x0)=−e−x0(23)a2=f′′(x0)=e−x0/2(24)a3=f(3)(x0)=−e−x0/6(25)a4=f(4)(x0)=e−x0/24(26)a5=f(5)(x0)=−e−x0/120In computation, only fifth order expansion in (20) is not enough to obtain an accuracy approximation due to the reason that the large error will be occurred when the input x is far from x0. Therefore, in this paper, the combination of look-up table (LUT) technique and Taylor series expansion technique is proposed. To set up the LUT, several specific values for x0 within the range 1 ≤ x ≤ 0 are firstly chosen; then the parameters of a0 to a5 in (21) to (26) are computed. Those data included by x0, a0 to a5 will be stored in LUT. At the next, when it needs to computee−xin (20), x0 which the most approximate to input x, and the related a0 to a5 will be selected from LUT then perform the computing task. Secondly, under the aforementioned design method, the internal circuit design of the neuron component in Fig. 5 is shown in Fig. 6which uses two adders, two multipliers, one divider, seven look-up tables and manipulates 20 steps machine to carry out the overall computation. The computation equations to design the circuit in Fig. 6 are listed in (2) and (20) to (26). However, total 20 steps need 400 ns operation time for computing the Gaussian function in (2).In the learning algorithm component in Fig. 4(b), the main inputs are previous weight wji(k), previous node centers Ci(k), previous node variances σi(k), learning rate μ, error ejand the neuron outputs φias well as the outputs are the current weightwji(k+1), current node centersCi(k+1)and current node variancesσi(k+1). The internal circuit design of the learning algorithm component in Fig. 4(b) is presented in Fig. 7which uses five multipliers, three adders, three dividers and manipulates 46 steps machine to carry out the overall SGD-based learning algorithm. In Fig. 7, the steps s0–s4 execute the weights update in (15); the steps s5–s28 perform the tuning of node centers in (16); the steps s27–s45 indicate the process of the perform the node variances adjustment in (17). However, total 46 steps need 920 ns operation time for computing the SGD-based learning algorithm in (15)–(17).Furthermore, based on Altera FPGA kit (DE2-Cyclone IV– EP4CE115F29C7) which has 114,480 logic elements and 532 Embedded Multiplier 9-bit elements, the hardware resource usages in implementing the overall 3-5-1 RBF NN are listed as follows:(1)For the forward computation component:Total logic elements: 14,067/114,480 (12%).Embedded Multiplier 9-bit elements: 64/532 (12%)For the SGD learning algorithm component:Total logic elements: 17,309/114,480 (15%).Embedded Multiplier 9-bit elements: 40/532 (8%)To verify the effectiveness of the proposed VHDL code of the 3-5-1 RBF NN (three inputs, five neurons, and one output) in Fig. 4(b), the co-simulation architecture by using Simulink and ModelSim is applied. In this architecture, the input stimuli and output responses are run in Simulink, and the function of the RBF NN is executed in ModelSim. However, the VHDL code of the 3-5-1 RBF NN is firstly tested, and the applications of the 3-5-1 RBF NN to the system dynamic identification (ID) in the general linear system and PMSM drive system are further evaluated.The accuracy and performance of the forward computation function and the overall function with learning algorithm for a 3-5-1 RBF NN are separately evaluated. In the former case, the co-simulation architecture using Simulink and ModelSim is built up and shown in Fig. 8. The input stimuli are generated, and output responses are displayed in Simulink, as well as the computation of the RBF NN is performed in ModelSim. Those input signals are also sent to the embedded Matlab function which executive the computation of a 3-5-1 RBF NN using the floating numerical system and the results will be sent back to Fig. 8 and further make a comparison with the output from Modelsim. To evaluate the computational accuracy, two tested cases with different weighting values are considered.Case-1:(27)w11=−0.23,w12=0.35,w13=0.65;w14=−0.48,w15=0.36Case-2:(28)w11=0.45,w12=−0.23,w13=0.45;w14=0.62,w15=−0.12Also, the centers and variances in five neurons are set by(29)Neuron--1:c11=0.55,c12=−0.7,c13=0.32,=0.6(30)Neuron--2:c21=0.5,c22=0.3,c23=−0.52,=0.7(31)Neuron--3:c31=0.5,c32=0.3,c33=0.4,=0.65(32)Neuron--4:c41=0.4,c42=0.35,c43=−0.43,=0.8(33)Neuron--5:c51=0.55,c52=−0.35,c53=0.44,=0.85With two selected cases in (27) and (28), the simulation results by the specific inputs are listed in Table 1. This table also displays the output from Modelsim and Matlab, and the error between them. The results show that the output of y from Modelsim and y’ from Matlab is very close in these two cases. The maximum error between them is 3.422351 × 10−7. It presents that the forward computation function of the proposed VHDL code for a 3-5-1 RBF NN is correct and effective.In the latter case, the overall function with forward computation and the learning algorithm for a 3-5-1 RBF NN in Fig. 4(b) is evaluated. The test condition is designed as follows. Constant values set the inputs, and the desired output y1dis varied with time. In addition, the initial parameters of RBF NN are set by a random number and the SGD learning algorithm will continuously tune those parameters according to the error of the desired output and the output of RBF NN. In this test condition, it is expected that the output of RBF NN can quickly track the desired output. Therefore, in simulation, the inputs value are set withx1=0.7,x2=0.8andx3=0.9, the learning rate is chosen by 0.25, and the desired output is a square wave with 10 ms period and with a varied magnitude of 0.65→1→-1→2.5→-2.5→1.5→-0.85→0.8→2→0.5. Finally, the simulation result is shown in Fig. 9. It demonstrates that the output of RBF NN can not track the desired output at the initial condition, but after the parameters of RBF NN is tuned to the suitable values, it can track the desired output very well. Even the value of the desired output change, the output of RBF NN can quickly track it within 5 ms, and the steady-state tracking error can keep less than 10−6. The simulation result presents that the forward computation and the learning algorithm of the proposed VHDL code for a 3-5-1 RBF NN are correct and effective.After confirming the correctness and effectiveness of the designed VHDL code for 3-5-1 RBF NN, we apply it to the system dynamic ID issue. Two cases with a general linear system and a PMSM drive system are tested as follows. However, choosing a suitable learning rate is not only an important but also a difficult issue because it depends on the system characteristics. In this paper, a try-and-error method is adopted to choose the learning rate in all simulation cases.-In the first case, the 3-5-1 RBF NN is applied to identify the dynamic of a linear system and its identification block diagram is shown in Fig. 10. In the linear plant, a dynamic model [25] is determined by the follows(34)yp(k+1)=0.5yp(k)+0.3yp(k−1)+u(k)In addition, the inputs of RBF NN are the previous plant output (yp(k),yp(k−1)) and the plant input u(k) as well as the output is the neural network outputyrbf(k+1). It is expected that the neural network outputyrbf(k+1)can quickly track the current plant outputyp(k+1)in this design. In the simulation, the input signal is firstly designed by the following equation:(35)u(k)=125(sin(10πk/100)+sin(25πk/100+0.5))Considering the different learning rate by 0.1, 0.25 and 0.5, the simulation results for the plant output and neural network output between them are shown in Fig. 11. It apparently reveals that the small learning rate gives slow tracking response, but the large learning rate causes unstable tracking response with oscillation phenomenon. In addition, a mean square error (MSE) is defined in (36) which will be taken as the evaluation index for the tracking performance in the different learning rate.(36)MSE=∑iN(yd−yrbf)2Nwhere N is the number of sampling data. Therefore, based on (36), the MSE in Fig. 11 are calculated by 0.064348, 0.034982 and 0.139052 for the different learning rate at 0.1, 0.25 and 0.5, respectively. It presents that the learning rate chosen by 0.25 appears better-tracking response in this case. Finally, the simulation result demonstrates that the proposed VHDL code for a 3-5-1 RBF NN can be applied well to identify the dynamic of a linear system.-In the second case, it is considered to focus on identifying the dynamic of the PMSM drive system. The block diagram regarding as the dynamic identification of PMSM drive system [22] is shown in Fig. 12and the Simulink/ModelSim co-simulation architecture is presented in Fig. 13. The PMSM, IGBT-based inverter, and speed command are performed in Simulink, and the speed/current controllers and the identification (ID) system based on a 3-5-1 RBF NN are executed in ModelSim by three works. The work-1, work-2 and work-3 of ModelSim in Figs. 12 and 13, respectively performs the function of the PI speed controller, the function of the current controller, coordinate transformation and space vector pulse width modulation (SVPWM), as well as the function of identifying the dynamic of PMSM drive. The numerical data type in work-1 and work-2 are specified in 16-bit length with Q15 format and 2’s complement operation, but in work-3 adopts 32-bit length with Q24 format. The inputs in RBF NN are the previous rotor speed (ωr(k−1),ωr(k−2)) and the current command iq(k) as well as the output is the estimated rotor speedω^r(k). In addition, the inputs to and the output from RBF NN both need to be normalized and de-normalized process that assures that the values of input and output data for RBF NN can keep within –1 to 1 and can avoid the numerical overflow condition occurred during the computation in RBF NN. Therefore, a normalized process for input data (current command) from –1.5A to 1.5A mapping to –1 to +1 is considered and a de-normalized process for output data (rotor speed) from –1 to +1 mapping to –2000 to 2000 rpm is applied. In this test condition, it is expected that the estimated rotor speedω^r(k)can quickly track the current rotor speed ωr(k). In the simulation, the three works in Fig. 13 are implemented in digital hardware design using VHDL. The learning rate is chosen by 0.25. The PI controller gains designed in work-1 are chosen by 0.367 (Q15 format) and 0.0036 (Q15 format), respectively. The sampling frequency for the speed control, the current control, and the identification (ID) system are designed with 2 KHz, 16 KHz, and 16 KHz, respectively. The clocks of 50 MHz and 12.5 MHz will supply all works of ModelSim. Further, the designed PMSM parameters used in Fig. 13 are that pole pairs is 4, stator phase resistance is 1.3 Ω, stator inductance is 6.3 mH, inertia is J = 0.000054 kg*m2 and friction factor is F = 0.00065 N*m*s. When the speed command is step speed values varied from 200→400→600→400→200 rpm with the 0.05 s period, the simulation results for the rotor speed response, the output of RBF NN and the tracking error between them are shown in Fig. 14. When the speed command is a sinusoidal wave with the following equation,(37)ωr*(k)=200sin(10π(k/100))+350sin(10π(k/100)+0.5)

@&#CONCLUSIONS@&#
This study successfully presents a digital hardware implementation of an RBF NN with SGD-based learning mechanism and demonstrates the results of an excellent accurate and fast computation characteristic based on EDA simulation environment. The contribution of this work can be summarized as follows(1)A high accuracy method, which combines Taylor series expansion technique and look-up table (LUT) technique to compute the Gaussian function in RBF NN, is proposed. Via to the co-simulation from the Simulink/ModelSim, the computational accuracy of the proposed method in Gaussian function and overall RBF NN can reach up10−6–10−7.Fast computational ability for a complicated RNF NN is presented by using the digital hardware implementation. For an example, an RBF NN with three inputs, five neurons, one output and one learning mechanism structure that the computational time only needs 500 ns in performing the function of the forward computation and 920 ns in performing the function of the learning algorithm.Succeed in making the co-simulation environment between Simulink and ModelSim verify the correctness and the effectiveness of the VHDL code for a 3-5-1 RBF NN. In addition, the application of the VHDL code for a 3-5-1 RBF NN to the dynamic ID in the linear system and the PMSM drive system are also successfully demonstrated