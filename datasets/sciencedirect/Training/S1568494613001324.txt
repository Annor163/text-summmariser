@&#MAIN-TITLE@&#
Evolving intelligent system for the modelling of nonlinear systems with dead-zone input

@&#HIGHLIGHTS@&#
The evolving intelligent algorithm is introduced for the modelling of nonlinear systems with dead-zone input.The stability of the evolving intelligent algorithm is guaranteed.The evolving intelligent algorithm is used for the modelling of two synthetic problems.

@&#KEYPHRASES@&#
Evolving systems,Uniform stability,Neural networks,Fuzzy systems,Nonlinear systems with dead-zone input,

@&#ABSTRACT@&#
In this paper, the modelling problem of nonlinear systems with dead-zone input is considered. To solve this problem, an evolving intelligent system is proposed. The uniform stability of the modelling error for the aforementioned system is guaranteed by means of a Lyapunov-like analysis. The effectiveness of the proposed technique is verified by simulations.

@&#INTRODUCTION@&#
Non-smooth nonlinear characteristics such as dead-zone, backlash, and hysteresis are common in actuators, sensors such as mechanical connections, hydraulic servo-valves, and electric servomotors; they also appear in biomedical systems. Dead-zone is one of the most important nonsmooth nonlinearities in many industrial processes, which can severely limit the system performance; and its study has been drawing much interest in the control community for a long time. Some important results are shown in [1,23,30,31,46,49,50,54,55,58], and [59]. In many works, controllers are proposed; however, a modelling system has not been introduced. The modelling system can be used for the failure prediction, disturbance rejection, trajectory generation, observer, and controller designs on the systems where the nonlinear behavior which includes the dead-zone is unknown.On the other hand, the evolving intelligent systems are characterized by abilities to adjust their structure as well as parameters to the varying characteristics of the environment (with the term of environment embracing processes/phenomena in which the system has to interact or deal with the users using the system). Some important results are presented by [4–8,10–12,14,16,18,20–22,24–27,29,33,34,38,45,47,57]. From the above papers, [4,6–8,16,18,20,21,27,33,34,38] use interesting clustering algorithms, and [4,6,8,21,33,34] present novel pruning algorithms as in this study; nevertheless, an evolving intelligent system for the modelling of recurrent nonlinear systems with dead-zone input is rarely presented.Finally, the stable intelligent systems are characterized to be systems where some kind of stability is guaranteed, i.e.; if there is boundedness on inputs of the algorithm, then there is also boundedness on outputs. Some important studies are given by [2,3,9,30–32,38,39,42–44,51–53,56]. The aforementioned papers do not consider the stability analysis of a recurrent evolving intelligent system for the modelling of a nonlinear system with dead-zone input.In this paper, a stable evolving intelligent system is addressed for the modelling of nonlinear systems with dead-zone input. In addition, the stability of the proposed algorithm is guaranteed.The paper is organized as follows. In Section 2, the nonlinear system with dead-zone input is presented. In Section 3, the evolving intelligent system is introduced. In Section 4, the evolving intelligent system is linearized. In Section 5, the structure updating of the evolving intelligent system is described. In Section 6, the stability of the above algorithm is guaranteed. In Section 7, the proposed algorithm is summarized. In Section 8, the proposed algorithm is used for the modelling of two synthetic problems. Section 9 presents conclusions and suggests future research directions.In this study, the system which will be modeled is composed by a nonlinear plant preceded by an actuator with a non-symmetric dead-zone in such a way that the dead-zone output is the input of the plant:(1)xi(k)=xi(k−1)+Txi+1(k−1)i=1,…,n−1xn(k)=xn(k−1)+Tfx(k−1)+gx(k−1),u(k−1)where i=1…n,xikis the ith state,xk−1=[x1k−1,x2k−1,…,xnk−1]∈Rn,uk−1∈Ris the output of the dead-zone and input of the system,uk−1andxk−1are known. f and g are the unknown nonlinear smooth functions.T∈Ris the sample time. The non-symmetric dead-zone can be represented by:(2)u(k−1)=DZ(v(k−1))=mrv(k−1)−brv(k−1)≥br0bl<v(k−1)<brmlv(k−1)−blv(k−1)≤blwhere mrand mlare the right and left constant slopes for the dead-zone characteristic, brand blrepresent the right and left breakpoints. Note thatv(k−1)∈Ris the input of the dead-zone.The nonlinear system (1)–(2) can be rewritten in the multivariable Brunovsky form [30]:(3)xi(k)=xi(k−1)+Txi+1(k−1)i=1,…,n−1xn(k)=hnx(k−1),u(k−1)where i=1…n,xikis the ith state,uk−1∈Ris the dead-zone output given by (2),xk−1=[x1k−1,x2k−1,…,xnk−1]∈Rn.hn∈Ris an unknown nonlinear smooth function.Remark 1The nonlinear systems with dead-zone (3) are inspired by the actuators used to move the links of robotic systems which are second order systems with the Brunovsky form [40,41].The following parallel [15,28] recurrent neural network is used to model the nonlinear system (3):(4)xˆi(k)=xˆi(k−1)+Txˆi+1(k−1)i=1,…,n−1xˆn(k)=sxˆn(k−1)+fˆk−1+gˆk−1where i=1…n,fˆk−1=V1k−1σ(k−1),gˆk−1=V2k−1ϕ(k−1)u(k−1),xˆikrepresents the ith state of the neural network,xˆk=xˆ1k,xˆ2k,…,xˆnk∈Rn. The parameters∈Ris a stable scalar (where its value should lie within the unit circle). The weights in the output layer areV1k∈R1×m1,V2k∈R1×m2. σ is m1−dimension vector function, andϕ(·)∈Rm2×m2is a diagonal matrix which are given as follows:(5)σ(k−1)=σ1(k−1),σ2(k−1),⋯σm1(k−1)Tϕ(k−1)=diagϕ1(k−1),ϕ2(k−1),⋯ϕm2(k−1)where σiand ϕiare given later. Each input variable xihas n fuzzy sets. From [17,48], it is known, by using product inference, center-average defuzzifier and center fuzzifier, called Sugeno fuzzy inference system with weighted average (FIS), the output of the fuzzy logic system can be expressed as:(6)fˆk−1=a1(k−1)b1(k−1)a1(k−1)=∑j=1m1v1j(k−1)z1j(k−1)b1(k−1)=∑j=1m1z1j(k−1)z1j(k−1)=exp[−γ1j2(k−1)]γ1j(k−1)=w1j(k−1)(xˆn(k−1)−c1j(k−1))gˆk−1=a2(k−1)b2(k−1)a2(k−1)=∑j=1m2v2j(k−1)z2j(k−1)uj(k−1)b2(k−1)=∑j=1m2z2j(k−1)z2j(k−1)=exp[−γ2j2(k−1)]γ2j(k−1)=w2j(k−1)(xˆn(k−1)−c2j(k−1))wherexˆn(k−1)is the nth state of the system (4), c1j(k−1), andw1j(k−1)are the centers and the widths of the membership function of the antecedent part, respectively, (j=1…m1),vj(k−1)is the center of the membership function of the consequent part.Remark 2The weighted average radial basis function of [17] is again (6) wherexˆn(k−1)is the state of the system (4), c1j(k−1), andw1j(k−1)are the centers and widths of the hidden layer, respectively, (j=1…m1),vj(k−1)are the weights of the output layer. In the radial basis functions networks of [17], 1/(σ1j(k−1)) is used instead ofw1j(k−1). In this study,w1j(k−1)is used instead of 1/(σ1j(k−1)) to avoid singularity in the modelling process.Define σj(k−1) and ϕj(k−1) as follows:(7)σj(k−1)=z1j(k−1)b1(k−1)ϕj(k−1)=z2j(k−1)b2(k−1)The above functions are the same given in (5); therefore, (6) can be written as follows:(8)fˆk−1=∑j=1m1v1j(k−1)σj(k−1)=V1,k−1σ(k−1)gˆk−1=∑j=1m2v2j(k−1)ϕj(k−1)uj(k−1)=V2,k−1ϕ(k−1)u(k−1)whereV1,k−1=[v11(k−1)…v1m1(k−1)]TϵRm1andV2,k−1=[v21(k−1)…v2m2(k−1)]TϵRm2. The parameter m1 is changing with the algorithm structure, while the parameter m2 is fixed and it is the dimension of u(k−1). See Fig. 1.Remark 3The proposed algorithm of Fig. 1 is different with the Kalman filter method of [36,51,53], and Fig. 2, for three reasons: The first reason is that the Kalman filter approximates all the functions while the proposed algorithm approximates only the last function, i.e., the first n−1 states are linear and dependent of the n state because the system has the multivariable Brunovsky form [30]; therefore, only the last function gives the approximation of the system and less computation is required. Other way to explain this fact is that the Kalman filter of [36,51,53] uses n algorithms while the proposed technique uses only one algorithm to obtain the modelling of the system. The second reason is that the Kalman filter method only the parameters are changing with the time while in the proposed algorithm the parameters and structure are changing with the time. The third reason is that in the Kalman filter of [36,51,53], the series-parallel model is used [15,28] where xn(k) of (3) is considered as the input offˆk−1andgˆk−1; while in this work, the parallel model is used [15,28] where the statexˆn(k)of (4) is considered as the input offˆk−1andgˆk−1.Remark 4There are three differences between the proposed algorithm of Fig. 1 with the evolving method of [37] and Fig. 2. The first difference is that the evolving system of [37] approximates all the functions while the proposed algorithm approximates only the last function, i.e., the first n−1 states are linear and dependent of the n state because the system has the multivariable Brunovsky form [30]; therefore, only the last function gives the approximation of the system and less computation is required. Other way to explain this fact is that the evolving system of [37] uses n algorithms while the proposed technique uses only one algorithm to obtain the modelling of the system. The second difference is that in the evolving method of [37], the series-parallel model is used [15,28] where xn(k) of (3) is considered as the input offˆk−1andgˆk−1; while in this work, the parallel model is used [15,28] where the statexˆn(k)of (4) is considered as the input offˆk−1andgˆk−1, consequently, less information of the system is required in the proposed algorithm. And finally, the third difference is that the evolving method of [37] is applied on biological nonlinear systems, while the proposed method is applied on nonlinear systems with dead-zone input.In this section, the model is linearized to find the parameters updating and to prove the stability of the proposed algorithm. The stability of the structure and output is required because this algorithm works on-line.According to the Stone-Weierstrass theorem [19], the unknown nonlinear system (3) can be written in the following form:(9)xi(k)=xi(k−1)+Txi+1(k−1)i=1,…,n−1xn(k)=sxn(k−1)+fk−1+gk−1wherefk−1=V1,k−1*σ*(k−1)+∈k−1f,gk−1=V2,k−1*ϕ*(k−1)u(k−1)+∈k−1g,∈k−1f+∈k−1g=hn[x(k),u(k)]−sxn(k−1)−fk−1−gk−1represents unmodeled dynamics. By [19], it is known that the term∈k−1f+∈k−1gcan be made arbitrarily small by simply selecting appropriate number of the hidden neurons. The unknown nonlinear function fk−1 of (9) is:(10)fk−1=∑j=1m1v1j*(k−1)σj*(k−1)+∈k−1f=V1,k−1*σ*(k−1)+∈k−1fwhereϕj*(k−1)=z1j*(k−1)/b1*(k−1),b1*(k−1)=∑j=1m1z1j*(k−1),z1j*(k−1)=exp−γ1j*2(k−1),γ1j*(k−1)=w1j*xn(k−1)−c1j*,v1j*,w1j*, andc1j*are the optimal parameters which can minimize the modelling error∈k−1f[19]. In the case of three independent variables, smooth function has a Taylor series as follows:(11)f(α1,α2,α3)=f(α10,α20,α30)+∂f(α1,α2,α3)∂α1α1−α10+∂f(α1,α2,α3)∂α2α2−α20+∂f(α1,α2,α3)∂α3α3−α30+Rk−1fwhereRk−1fis the remainder of the Taylor series. Let α1, α2 and α3 correspond c1j(k−1),w1j(k−1)andv1j(k),α10,α20,α30correspondc1j*,w1j*, andv1j*. Definec˜1j(k−1)=c1j(k−1)−c1j*,w˜1j(k−1)=w1j(k−1)−w1j*, andv˜1j(k−1)=v1j(k−1)−v1j*. Thus, the Taylor series is applied to linearize V1,k−1σ(k−1) of (6) and (8) as follows:(12)V1,k−1σ(k−1)=V1,k−1*σ*(k−1)+∑j=1m1∂V1,k−1σ(k−1)∂c1j(k−1)c˜1j(k−1)+∑j=1m1∂V1,k−1σ(k−1)∂w1j(k−1)w˜1j(k−1)+∑j=1m1∂V1,k−1σ(k−1)∂v1j(k−1)v˜1j(k−1)+Rk−1fConsidering (6), (7), (8), and using the chain rule [38,39,48], it gives:∂V1,k−1σ(k−1)∂c1j(k−1)=∂V1,k−1σ(k−1)∂a1i(k−1)∂a1i(k−1)∂z1j(k−1)∂z1j(k−1)∂γ1j(k−1)∂γ1j(k−1)∂c1j(k−1)+∂V1,k−1σ(k−1)∂b1(k−1)∂b1(k−1)∂z1j(k−1)∂z1j(k−1)∂γ1j(k−1)∂γ1j(k−1)∂c1j(k−1)=2γ1j(k−1)z1j(k−1)w1j(k−1)v1j(k−1)−fˆk−1b1(k−1)∂V1,k−1σ(k−1)∂w1j(k−1)=∂V1,k−1σ(k−1)∂a1i(k−1)∂a1i(k−1)∂z1j(k−1)∂z1j(k−1)∂γ1j(k−1)∂γ1j(k−1)∂w1j(k−1)+∂V1,k−1σ(k−1)∂b1(k−1)∂b1(k−1)∂z1j(k−1)∂z1j(k−1)∂γ1j(k−1)∂γ1j(k−1)∂w1j(k−1)=2γ1j(k−1)z1j(k−1)xˆn(k−1)−c1j(k−1)fˆk−1−v1j(k−1)b1(k−1)∂V1,k−1σ(k−1)∂v1j(k−1)=∑j=1m1v1j(k−1)σj(k−1)∂v1j(k−1)=σj(k−1)Substituting (∂V1,k−1σ(k−1))/(∂c1j(k−1)),(∂V1,k−1σ(k−1))/(∂w1j(k−1)), and(∂V1,k−1σ(k−1))/(∂v1j(k−1))in Eq. (12), it gives:(13)V1,k−1σ(k−1)=∑j=1m1σj(k−1)v˜1j(k−1)+∑j=1m12γ1j(k−1)z1j(k−1)w1j(k−1)v1j(k−1)−fˆk−1b1(k−1)c˜1j(k−1)+∑j=1m12γ1j(k−1)z1j(k−1)[xˆn(k−1)−c1j(k−1)]fˆk−1−v1j(k−1)b1(k−1)w˜1j(k−1)+V1,k−1*σ*(k−1)+Rk−1fDefineB1jc(k−1),B1jw(k−1)andB1jv(k−1)as:(14)B1jc(k−1)=2γ1j(k−1)z1j(k−1)w1j(k−1)v1j(k−1)−fˆk−1b1(k−1)B1jw(k−1)=2γ1j(k−1)z1j(k−1)xˆn(k−1)−c1j(k−1)fˆk−1−v1j(k−1)b1(k−1)B1njv(k−1)=σj(k−1)Note that σj(k−1) is repeated for each i inB1jv(k−1), using the above definitions in (13), it gives:(15)V1,k−1σ(k−1)=∑j=1m1B1jc(k−1)c˜1j(k−1)+∑j=1m1B1jw(k−1)w˜1j(k−1)+∑j=1m1B1jv(k−1)v˜1j(k−1)+V1,k−1*σ*(k−1)+Rk−1fDefinef˜k−1=fˆk−1−fk−1, and substituting (8), (10) andf˜k−1into (15), it gives:(16)f˜k−1=Bk−1fTθ˜f(k−1)+μf(k−1)whereBk−1fT=[B11c(k−1),…,B1m1c(k−1),B11w(k−1),…,B1m1w(k−1),B11v(k−1),…,B1m1v(k−1)]∈R1×3m1,θ˜f(k−1)=[c˜11(k−1),…,c˜1m1(k−1),w˜11(k−1),…,w˜1m1(k−1),v˜11(k−1),…,v˜1m1(k−1)]T∈R3m1×1, thusθ˜f(k−1)=θf(k−1)−θf*,B1jc(k−1),B1jw(k−1), andB1jv(k−1)are given in (14),μf(k−1)=Rk−1f−∈k−1f. Similarly ing˜k−1=gˆk−1−gk−1, it gives:(17)g˜k−1=Bk−1gTθ˜g(k−1)+μg(k−1)whereBk−1gT=[B21c(k−1),…,B2m2c(k−1),B21w(k−1),…,B2m2w(k−1),B21v(k−1),…,B2m2v(k−1)]∈R1×3m2,θ˜g(k−1)=[c˜21(k−1),…,c˜2m2(k−1),w˜21(k−1),…,w˜2m2(k−1),v˜21(k−1),…,v˜2m2(k−1)]T∈R3m2×1, thusθ˜g(k−1)=θg(k−1)−θg*,B2jc(k−1)=2uj(k−1)γ2j(k−1)z2j(k−1)w2j(k−1)(v2j(k−1)−gˆk−1)/(b2(k−1)),B2jw(k−1)=2uj(k−1)γ2j(k−1)z2j(k−1)[xˆn(k−1)−c2j(k−1)](gˆk−1−v2j(k−1))/(b2(k−1)), andB2jv(k−1)=uj(k−1)ϕj(k−1),μg(k−1)=Rk−1g−∈k−1g. Define the modelling error as follows:(18)e(k−1)=yˆ(k−1)−y(k−1)whereyˆ(k−1)=fˆk−1+gˆk−1=xˆn(k)−sxˆn(k−1)is the network output and y(k−1)=fk−1+gk−1=xn(k)−sxn(k−1) is the nonlinear system output,e(k−1)∈R; therefore, substitutingyˆ(k−1), y(k−1), (16) and (17) in (18), it gives:(19)ek−1=Bk−1Tθ˜(k−1)+μ(k−1)whereBk−1T=[BgT(k−1),BfT(k−1)]∈R1×3(m2+m1),θ˜(k−1)=[θ˜g(k−1),θ˜f(k−1)]T∈R3(m2+m1)×1,μ(k−1)=μg(k−1)+μf(k−1)∈R,θ˜f(k−1)and BfT(k−1) are given in (16),θ˜g(k−1)and BgT(k−1) are given in (17).Define the state error asx˜n(k)=xˆn(k)−xn(k). From (18), it givesxˆn(k)=sxˆn(k−1)+yˆ(k−1)and xn(k)=sxn(k−1)+y(k−1), subtracting the second equation to the first givesxˆn(k)−xn(k)=sxˆn(k−1)−xn(k−1)+yˆ(k−1)−y(k−1). Substitutingx˜n(k)and e(k−1) of (18) in the above equation gives:(20)x˜n(k)=sx˜n(k−1)+e(k−1)Choosing an appropriate number of hidden neurons is important in designing evolving intelligent systems, because too many hidden neurons result in a complex evolving system that may be unnecessary for the problem and it can cause overfitting [17], whereas too few hidden neurons produce a less powerful neural system that may be insufficient to achieve the objective. The number of hidden neurons is considered as a design parameter and it is determined based on the input–output pairs and on the number of elements of each hidden neuron. The basic idea is to group the input–output pairs into clusters and use one hidden neuron for one cluster; i.e., the number of hidden neurons equals the number of clusters [4,6–8,16,18,20,21,27,33,34,38,39].One of the simplest clustering algorithms is the nearest neighborhood clustering algorithm. In this algorithm, the first data is considered as the center of the first cluster. Then, if the distances from a data to the cluster centers are less than a pre-specified value (the radius r), this data is set into the closest cluster; otherwise, set this data as a new cluster center. The details are given as follows.Consider xn(k−1) as newly incoming pattern, then from (6) it is obtained:(21)p(k−1)=max1≤j≤m1z1j(k−1)If p(k−1)<r, then a new hidden neuron is generated (each hidden neuron correspond to each center) and m1=m1+1 where r is a selected radius,r∈0,1.Once a new hidden neuron is generated, the next step is to assign initial centers and widths of the network, a new density with value 1 is generated for this hidden neuron.(22)c1,m1+1(k)=xn(k)w1,m1+1(k)=randv1m1+1(k)=y(k)dm1+1(k)=1If p(k−1)≥r, then a hidden neuron is not generated. If z1j(k−1)=p(k−1) the winner neuron j* is obtained, the winner neuron is a neuron that increments its importance in the algorithm, then its density must be increased and is updated as follows:(23)dj*(k)=dj*(k)+1The above algorithm is no longer a practical system if the number of input–output pairs is large because the number of hidden neurons (clusters) grow, even some data are grouped into hidden neurons (clusters). Therefore, a pruning method is required [4,6], [8,21,33,34,39]. The pruning algorithm based in the density where the density is the number of times each hidden neuron is used in the algorithm. From (22), it is obtained that when a new hidden neuron is generated its density starts at one, and from (23), it is known that when a data is grouped in an existing hidden neuron, the density of this hidden neuron is increased by one. Then, each cluster (hidden neuron) has its own density. The least important hidden neuron is the hidden neuron which has the smallest density. After of some iterations (ΔL) the least important hidden neuron is pruned if the value of its density is smaller to a specified umbral (du). The details are given as follows.Each ΔL iterations where ΔL∈ℵ, consider:(24)dmin(k)=min1≤j≤m1dj(k)If m1≥2 (If there is one hidden neuron given as m1=1, the hidden neuron can not be pruned) and if dmin(k)≤duthis hidden neuron is pruned, where du∈ℵ is the minimum selected allowed density and it is called the umbral parameter. Once a hidden neuron is pruned, the next step is to assign centers and widths of the network. When dj(k)=dmin(k) the looser neuron j* is obtained, the looser neuron is the least important neuron of the algorithm, if j≤j* do nothing, but if j>j* all the parameters are updated as follows:(25)c1,j−1(k)=c1,j(k)w1,j−1(k)=w1,j(k)v1j−1(k)=v1j(k)dj−1(k)=dj(k)The above parameters updating moves the looser neuron j* to the last element (j=m1). For j=m1 the looser neuron is pruned as follows:(26)c1,m1(k)=0w1,m1(k)=0v1m1(k)=0dm1(k)=0Then m1 is updated as m1=m1−1 to decrease the size of the network.If dmin(k−1)>duor m1=1 do nothing.Finally L is updated as L=L+ΔL.Remark 5The parameters L and ΔL are because the pruning algorithm does not work in each iteration. The initial value of L is ΔL, the pruning algorithm works at the first time when k=L, then L is increased by ΔL. The pruning algorithm works each ΔL iterations. The parameter ΔL is found empirically as 5du; thus, the pruning algorithm only has duas the design parameter.First, an important definition and theorem are mentioned. Later, the main stability theorem is presented.Consider the following discrete-time nonlinear system:(27)xk+1=f[xk,uk]whereuk∈Rmis the input vector,xk∈Rnis the state vector, ukand xkare known. f is an unknown nonlinear smooth function f∈C∞.Definition 1The system (27) is said to be uniformly stable if ∀ϵ>0, ∃ δ=δ(ϵ) such that:(28)∥xk1∥<δ⇒∥xk∥<ϵ∀k>k1If the system has δ=δ(ϵ, k), then the system (27) is simply stable.Now, a theorem for the stability of discrete-time nonlinear systems taken from [39] will be given.Theorem 1Let Lk(x(k)) be a Lyapunov-like function of the discrete-time nonlinear system(27), if it satisfies:(29)γ1∥xk∥≤Lk(xk)≤γ2∥xk∥ΔLk(xk)≤−γ3∥xk∥+γ3δwhere δ is a positive constant,γ1·andγ2·are K∞functions, andγ3·is a K function, then the system(27)is uniformly stable.ProofSee[39]for the proof. □Remark 6The continuous time version of the above theorem is given by [35]. The main difference between the continuous time stability theorem of [35] and the discrete time stability theorem of [39] is that in the first, the derivative of the Lyapunov function is used, and in the second, the difference of the Lyapunov-like function is used.Now, the stability of the proposed algorithm is analyzed.Theorem 2Consider the evolving intelligent system(4), (6), (22), (30)to model the nonlinear systems with dead-zone input(1), (2), (3)and use the recursive least square updating function:(30)θ(k)=θ(k−1)−1Qk−1PkBk−1e(k−1)Pk=Pk−1−1Rk−1Pk−1Bk−1Bk−1TPk−1whereQk−1=10+Bk−1TPk−1Bk−1,Rk−1=2Qk−1+Bk−1TPk−1Bk−1,Bk−1Tand θ(k−1) are given in(19),Pk−1∈R3(m2+m1)×3(m2+m1)is a positive definite covariance matrix. Therefore, the average error of the modelling error is uniformly stable and will converge to:(31)limsupT→∞∑k=2T(Bk−1TPk−1Bk−1)2Qk−12Rk−1e2(k−1)≤μ¯10whereμ¯is the upper bound of the uncertainty μ(k−1),|μ(k−1)|<μ¯.ProofSee Appendix A for the proof. □Remark 7From (47) and (43), it can be observed that the final iteration parameter (time) T tends to infinity, thus the stability of the proposed algorithm is preserved when T→∞.Remark 8The parameter m1 (number of neurons) is finite because the clustering and pruning algorithms do not let m1 to become infinity. The number of neurons m1 is changed by the clustering and pruning algorithms, and m1 only changes the dimension ofBk−1Tand θ(k−1), thus the stability result is preserved.The proposed algorithm is finally as follows:1Select the following parameters: for the clustering algorithm as0<r<1∈R, and for the pruning algorithm as du∈N, (L=L+ΔL, ΔL=5du). If r is bigger, more neurons could be generated. If duis smaller, more neurons could be pruned. If there are many neurons that are generated and pruned, then it could cause like a chattering in the modelling. Consequently, only the required neurons should be generated and pruned in the algorithm.For the first data k=1, (where k is the iterations number) m1=1 (where m1 is the hidden neurons number), the initial parameters of the least square algorithm areP1∈R3(m2+m1)×3(m2+m1)with diagonal elements,v11(1)=y(1), c11(1)=x(1) andw11(1)=rand∈(5,15), (v11is the initial parameter of the consequent part, c11 andw11are the centers and widths of the membership function of the antecedent part), rand is a random number which lets to find some similar alternative results,v21(1)=y(1), c21(1)=xn(1),w21(1)=rand, m2=size of the input u(k), the initial parameter of the clustering and pruning algorithm is d1(1)=1, (where d is the density parameter).For the other data k≥2, evaluate the network parameters z1j(k−1), b1(k−1), z2j(k−1) and b2(k−1) with (6), evaluate the output of the networkyˆ(k−1)with (7), (8) and (18), evaluate the modelling error e(k−1) with (18), update the parameters of the least square algorithmv1j(k), c1j(k),w1j(k),v2j(k), c2j(k) andw2j(k)with (30) (where j=1…m1 forfˆk−1and j=1…m2 forgˆk−1), evaluate the parameter of the clustering and pruning algorithms p(k−1) with (21).The updating of the clustering algorithm is as follows:if p(k−1)<r, then a new neuron is generated (m1=m1+1) wherer∈0,1, (i.e. the number of neuron is increased by one), assign initial values to the new neuron asc1m1+1(k),w1m1+1(k),v1m1+1(k), anddm1+1(k)with (22), the values are assigned forPk∈R3(m2+m1+1)×3(m2+m1+1)from elements m2+1 to m2+m1+1 with diagonal elements, (where Pk,v1j(k), c1j(k), andw1j(k)are the parameters of the least square algorithm, dj(k) is the density parameter, j=1…m1), go to 3.If p(k−1)≥r, then a neuron is not generated, if z1j(k−1)=p(k−1) the winner neuron j* is obtained, the value of the densitydj*(k)of this hidden neuron is updated with (23), the winner neuron is a hidden neuron that increments its importance in the algorithm, go to 3.The updating of the pruning algorithm is as follows:For the case that k=L the pruning algorithm works (the pruning algorithm does not work in each iteration), evaluate the minimum density dmin(k) with (24), L is updated as L=L+ΔL.If m1≥2 and if dmin(k)≤duthis hidden neuron is pruned, where du∈N is the density umbral, if dj(k−1)=dmin(k) the looser neuron j* is obtained, the looser neuron is the least important neuron of the algorithm, assign values to c1j(k),w1j(k),v1j(k), and dj(k) with (25) and (26) to prune the looser neuron j*, assign values forPk∈R3(m2+m1−1)x3(m2+m1−1)from elements m2+1 to m2+m1−1 with diagonal elements to prune the looser neuron j*, (where Pk,v1j(k)c1j(k), andw1j(k)are the parameters of the least square algorithm and dj(k) is the density parameter j=1…m1), update m1 as m1=m1−1, (i.e. the number of hidden neurons is decreased by one), go to 3.If dmin(k)>duor m1=1 this neuron is not pruned, go to 3.In this section, the suggested on-line evolving intelligent system is applied for the modelling of nonlinear system with dead-zone input. Note that the structure and parameters updating of the proposed approach work at the same time. The algorithm of this paper is compared with the Kalman filter algorithm of [36,51,53], and with the evolving algorithm of [37] because the above neuro-fuzzy systems have similar structure. In this section the proposed algorithm is called ModifiedEvolving, the Kalman filter is called KalmanFilter and the evolving algorithm is called Evolving.The root mean square error (RMSE) is used to obtain the algorithms performance and it is given as follows [38,39]:(32)RMSE=1N∑k=1nx˜i2(k)1/2wherex˜i(k)is the states error of (20), n is states number.Example 1The nonlinear system used for the modelling is expressed as follows:(33)x1(k)=x1(k−1)+Tx2(k−1)x2(k)=x2(k−1)+T1.31−exp(−x1(k−1))1+exp(−x2(k−1))−2sin(x1(k−1))cos(x2(k−1))x12(k−1)+x22(k−1)+1+sin(x1(k−1)x2(k−1))+cos2(x2(k−1))+1.51.5u(k−1)+0.2randwherev(k−1)=0.18sin1.5π(k−1)T+0.28sin0.5π(k−1)T, T=0.01 is the system input, the dead-zone u(k−1) is given as (2) with mr=0.1, ml=0.1, br=0.1, bl=−0.1 for the first half of the time, and mr=0.2, ml=0.2, br=0.2, bl=−0.2 for the second half of the time. x1(1)=0.5, x2(1)=0 are the initial conditions. The nonlinear system has the form (1). The data for 2000 iterations is used for the modelling. The signal 0.2rand is a noise signal where rand are random numbers.Kalman Filter is given in [36,51,53], with parametersx(k)=x1k,x1kT,xˆk∈R2,P11=P21=diag(1×10−6)∈R2(1+1)×2(1+1).Evolving is given in [37] with parametersx(k)=x1k,x1kT,xˆk∈R2,S=diag(0.1)∈R2×2,P11=P21=diag(100)∈R3(1+1)×2(1+1), r=0.7, and du=4.ModifiedEvolving is given as (4), (6) or (4), (7), (8) with parameters x(k)=[x1(k), x2(k)]T,xˆ(k)∈R2, s=0.1,P1=diag(100)∈R3(1+1)×2(1+1), r=0.7, and du=4.Fig. 3shows the dead-zone. The states approximation are shown in Figs. 4 and 5, and state errors are shown in Fig. 5. The growth of the hidden neurons is shown in Fig. 6. Table 1shows the comparison of the RMSE and neurons for the three algorithms modelling.From Fig. 3, it is shown that the dead-zone changes in the half of the time. From Figs. 4–7, and Table 1, it can be seen that ModifiedEvolving achieves better accuracy when compared with both the Evolving and KalmanFilter because the first follows better the signals than the others, also the RMSE and neurons number for the first is smaller than for the others. Consequently, the proposed algorithm is good for the modelling of the first nonlinear system with dead-zone input.Example 2The nonlinear system used for the modelling is expressed as follows [30]:(34)x1(k)=x1(k−1)+Tx2(k−1)x2(k)=x2(k−1)+T−2.31−exp(−x1(k−1))1+exp(−x2(k−1))+3.7x2(k−1)sin(x1(k−1)x2(k−1))cos(x2(k−1))x12(k−1)+x22(k−1)+1+1.5x1(k−1)x2(k−1)+0.7x1(k−1)x23(k−1)sin(2x1(k−1))+0.4x12(k−1)x2(k−1)+3.5u(k−1)−0.5randwherev(k−1)=0.18sin1.5π(k−1)T+0.28sin0.5π(k−1)T, T=0.01 is the system input, the dead-zone u(k−1) is given as (2) with mr=0.1, ml=0.1, br=0.1, bl=−0.1 for the first half of the time, and mr=0.05, ml=0.05, br=0.05, bl=−0.05 for the second half of the time. x1(1)=0.5, x2(1)=0 are the initial conditions. The nonlinear system has the form (1). The data for 2000 iterations is used for the modelling. The signal −0.5rand is a noise signal where rand are random numbers.KalmanFilter is given in [36,51,53], with parameters x(k)=[x1(k), x1(k)]T,xˆ(k)∈R2,P11=P21=diag(1×10−6)∈R2(1+1)×2(1+1).Evolving is given in [37] with parameters x(k)=[x1(k), x1(k)]T,xˆ(k)∈R2,S=diag(0.1)∈R2×2,P11=P21=diag(100)∈R3(1+1)×2(1+1), r=0.7, and du=4.ModifiedEvolving is given as (4), (6) or (4), (7), (8) with parameters x(k)=[x1(k), x2(k)]T,xˆ(k)∈R2, s=0.1,P1=diag(100)∈R3(1+1)×3(1+1), r=0.7, and du=4.Fig. 8shows the dead-zone. The states approximation are shown in Figs. 9 and 10, and state errors are shown in Fig. 11. The growth of the hidden neurons is shown in Fig. 12. Table 2shows the comparison of the RMSE and neurons for the three algorithms modelling.From Fig. 8, it is shown that the dead-zone changes in the half of the time. From Figs. 9–12, and Table 2, it can be seen that ModifiedEvolving achieves better accuracy when compared with both the Evolving and KalmanFilter because the first follows better the signals than the others, also the RMSE and neurons number for the first is smaller than for the others. Consequently, the proposed algorithm is good for the modelling of the second nonlinear system with dead-zone input.Remark 9The nonlinear systems (33) and (34) are different because in the first, fk−1 is bounded and gk−1 changes with the time, while in the second, fk−1 is not bounded and gk−1 is constant. In addition, the noise signals are different for both models.Remark 10The proposed algorithm approximates the behavior of the nonlinear systems (33), (34) which includes fk−1 and gk−1, and the dead-zone u(k−1) is inside of gk−1; therefore, the proposed algorithm approximates the behavior of the nonlinear systems considering the dead-zone behavior.

@&#CONCLUSIONS@&#
