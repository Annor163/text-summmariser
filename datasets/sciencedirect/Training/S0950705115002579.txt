@&#MAIN-TITLE@&#
Automatic classification of personal video recordings based on audiovisual features

@&#HIGHLIGHTS@&#
A system for automatic classification of personal videos is presented.Personal video recordings are classified into 24 categories frame by frame.Features derived from both audio and image data are used to classify.The system learns the most appropriate parameters and classifiers for the task.

@&#KEYPHRASES@&#
Feature extraction,Personal video recording,Video classification,Meta-classification,

@&#ABSTRACT@&#
The aim of the present work is to design a system for automatic classification of personal video recordings based on simple audiovisual features that can be easily implemented in different devices. Specifically, the main objective is to classify frame by frame personal video recordings into 24 semantically meaningful categories. Such categories include information about the environment like indoor or outdoor, the presence or absence of people and their activity, ranging from sports to partying. In order to achieve a robust classification, features derived from both audio and image data will be used and combined with state of the art classifiers such as Gaussian Mixture Models or Support Vector Machines. In the process, several combination schemes of features and classifiers are defined and evaluated over a real data set of personal video recordings. The system learns which parameters and classifiers are most appropriate for this task.The experiments show that the approach using specific classifiers for audio features (Mel-Frequency Cepstral Coefficients (MFCCs)) and image features (color, edge histograms), and using a meta-classification combination schema attains significant performance. The best performance obtained over the different approaches evaluated gave a promisingf-measurelarger than 57% in average for all the categories and larger than 73% over diverse categories.

@&#INTRODUCTION@&#
Nowadays, due to the development of new and low-cost audiovisual technologies (such as video cameras, mobile phones, photo cameras, etc.) and the wide use of Internet, the audiovisual content available is huge. Furthermore, the amount of personal audiovisual content is often large enough to demand an automatic system to help classify personal recordings. Thus, diverse methods of indexing, classification and retrieval of audiovisual information have emerged. Most of the works in this topic have focused on the classification of high quality TV videos, being restricted to a small number of categories (sport, cartoon, news, commercial and music) [1,2].Audiovisual information contains audio features as well as image features. The audio features normally used are the well known Mel-Frequency Cepstral Coefficients (MFCCs) [3]. Regarding video features, image features can be used [4,5], but also motion features [6] or a combination of both [2,3].On the other hand, there exist several classification schemes that are used to combine the audiovisual features like Gaussian Mixture Models (GMMs) [2,3], Hidden Markov Models (HMMs) [2], Support Vector Machines (SVMs) [7,8] and combination schemes [9].At the present time, modern personal computers allow the development of complex systems to classify audiovisual content as well as new cloud computing techniques. Personal video recordings have several particular features that make the classification task much more complex than with TV Videos or movies. The videos are often of low quality and they present unclear scenes. In addition, in order to perform a useful classification of the personal video recordings, a wide range of categories is required. In this way, Lee et al. [10] investigate a method for automatic classification of consumer video clips based on their soundtracks, using a set of 25 overlapping semantic classes (e.g. dancing, sports, sunset, music, wedding, etc.). Chang et al., in [11], deal with the same problem but using visual information solely, whereas Sugano et al., in [12], present a genre classification system for home video recordings that considers 5 categories based on audiovisual features.In this paper, we will deal with the combination of audio and visual features, as well as with the combination of different kinds of classifiers to achieve an efficient frame by frame classification of personal video recordings into semantically meaningful categories. Such categories include information about the environment like indoor or outdoor, the presence or absence of people, and their activity, like sports or partying. The designed system learns which parameters and classifiers are most appropriate for this task.Summing up, we will use 24 categories, in contrast to most works in which the classification considers a set of around 5 categories [1,3,7,13]. Also, we will work with a real data set of personal video recordings composed of 366 videos manually labeled at frame level and the classification will be done frame by frame.The usage of frame level annotation and classification and the utilization of 24 different semantic categories constitute a main novelty of the proposed scheme. Other approaches commonly label the full videos in the database as a whole [5,12,14,15] and the number of different classes employed is smaller. For example, only 5 categories are considered in [3,7,12,16].Also, unlike other works, we will be working on a data set of real personal video recordings instead of professional videos [1,2]. A comparative performance analysis is included to assess the difference in performance between several combinations of features and classifiers for the specific task described: frame by frame classification of personal video recordings. Another important objective of the designed system is its feasibility to work in as many devices as possible, so the selected audiovisual features should be kept as simple as possible.This paper is organized as follows: in Section 2, we present the audio and video features selected. In Section 3, we describe the types of classifiers selected for the study, as well as several combination methods for features and classifiers. In Section 4, the evaluation results over a real data set are shown. Finally, Section 5 draws some conclusions on the work presented.One of the main stages for the video classification task is the selection and extraction of numerical features from video sources that can be used to characterize a frame or an audio segment.In this section, we review the features to use for video classification. Then, we select some of the more appropriate features, for classification into the 24 selected categories. The features will be taken from both audio an image domains and their definition will be based on the performance showed in previous works [17,18] and their easiness to be obtained and used. Since one of the objectives of the proposed scheme is that it can be easily implemented in different devices, we have focused on the simplicity and usability of the features. Because of this, we have avoided the selection of a large number of features, local features [5,19–21] and the implementation of feature selection algorithms [22,23]. However, we will deal with audio and image features, as well as with combination.There is a huge variety of audio features [17,18,24,25] but some of them give poor separability among different classes or they are depending on the language, and the inclusion of these features will not improve performance. We could use the exhaustive combination method or a feature selection algorithm [22,23] in order to find the best feature vector, but the computational burden of this is huge.Finally, the decision on the audio features was done according to previous works in Sony Corporation and taking into account the features more widely used in automatic speech recognition (ASR) [1,26]. The aim is to select a set of good audio features with low computational complexity to recognize the presence of humans. So, we decided to use the Mel-Frequency Cepstral Coefficients (MFCCs) due to their proved excellent performance (see [27,28]). Furthermore, MFCCs are widely used features as audio features in video analysis and classification [17,18].We also use the first order derivative of the MFCCs (delta coefficients) and their second order derivatives (acceleration coefficients) in order to incorporate the ongoing changes over multiple frames [29]. The frames are defined like in [30]. In order to discard irrelevant information, we use telephone quality audio. This choice also helps to bound the computational load.The classifier will receive a concatenation of the static (MFCCs) and dynamic features (delta and acceleration coefficients).There exist a vast variety of image features that can be used for image and video classification [17–19,31], being more computationally complex than audio features. So it is important to carefully select the more significant ones for the classification of personal video recordings. Among the different types of image features, color based features are widely used [17,18].Therefore in this paper, among the different color based features [17], color and edge histograms are used to gather the visual information (Sections 2.2.1 and 2.2.2). The use of texture was discarded since this type of feature shows very poor performance because of the huge variety of textures in individual frames in personal video recordings and the computational burden involved. The use of local features, such as the scale invariant feature transform (SIFT) [5,19], and the use of human action recognition features, such as the spatio-temporal steerable pyramid (STSP) [21], have also been dismissed because of their computational complexity. Also, note that these last features presented must be dismissed because the 24 video classification categories selected in this paper correspond to generic scenes which do not agree with the categories designed for the classification of human actions like walk, stand, sit, situp, etc [21].Color histograms are used to compare images in many applications [32,33]. Their main advantages are related to their efficiency and insensitivity to small changes in the camera viewpoint.Among the several color representation models available, the HSV (Hue, Saturation and Value) color space [34] has been selected. The HSV color space needs to be quantized into several bins. We decided to use the quantization scheme proposed in [4] where the color space is quantized into 36 non-uniform colors. After the analysis of the features processed, we observed that very few pixels were classified in the bins representing high saturation. This means that colors with high saturation are quite unusual in the personal video recording analyzed. In order to avoid the presence of empty bins we combined them with samples with lower saturation. The result was a descriptor formed by 20 bins.In addition, personal recordings often show very different aspect in the upper and the lower part (i.e. sky-nature, sky-sea, etc.). With the aim of taking into account this kind of spatial information, the image was split vertically into an upper and a lower part. Consequently the HSV feature used in this work has 40 bins: 20 bins from the upper part of the frame and 20 bins from the lower part of the frame.Edges constitute and important feature representing image content. One way to use such important feature is by means of the edge histogram. The edge histogram of an image represents the frequency and the directionality of its changes of brightness [35].Specifically, we will use the edge distribution used in MPEG-7 [35]: the Edge Histogram Descriptor (EHD). Since it is important to keep the size of the descriptor as compact as possible, for the efficient storage of the metadata, the MPEG-7 edge histogram is designed to contain only 80 bins that describe the local edge distribution. These 80 histogram bins are the only standardized semantics for the MPEG-7 EHD. Basically, the EHD represents the distribution of 5 types of edges (vertical, horizontal, 45 degrees diagonal, 135 degrees diagonal and non-directional edges) in an area named sub-image. The sub-image is defined by the division of the image into 4×4 non-overlaping blocks. The details of MPEG-7 EHD can be found in [35].In this section, we deal with the design of the classifiers in order to get the best results from the vector of features. First, we start with the general design cycle of classifiers based on machine learning (Section 3.1). After presenting the basic schema of the classifiers, we continue our study, in Section 3.2, combining different kinds of features in order to find complementary information between the different sources of information to improve the performance of the classifier. In Section 3.3, instead of combining the features, we develop several schemes of classifier combinations aimed at using specific classifiers for the different kinds of features and combining their results in an optimum way. Finally, in Section 3.4, an innovative technique of classifier combination is developed by means of meta-classifiers.First, we define the general design cycle of every individual classifier. The type of classification used is binary classification based on supervised learning. This choice is due to the facility of implementation of both probabilistic and discriminative methods and the easily achieved multi-label classification by using a binary classifier per class [36].Fig. 1shows the different elements and phases of the design cycle that will be presented in the following sub-sections. First, in Section 3.1.1, the set of categories considered to label the personal video recordings is defined. In Section 3.1.2, the set of personal video recordings used for training and testing is presented, together with some characteristics related to the feature extraction process. Next, the statistical model based on machine learning methods for classification will be chosen. This model will be trained to get adapted to our video data. In Section 3.1.3, we analyze two of the state of the art machine learning models, Gaussian Mixture Models (GMMs) and Support Vector Machines (SVMs). In Section 3.1.4, we observe the probabilities or scores GMMs and SVMs attain. In Section 3.1.5, the usage of the probabilities or scores provided by the model, in order to get the best classification results, is analyzed. Finally, in Section 3.1.6, the metrics used for the evaluation of the different approaches are described.The set of categories used for video classification is very variable across different works [11,37,38]. The set of 24 distinct classes, selected to categorize the personal video recordings, is based on the human presence, the geographic situation and the observed activity. Therefore, all the considered categories are objective, that is subjective categories like happiness, sadness, anger, disgust, fear and surprise [18], bright, dark, warm, cold, sensations, etc. [39], are not included. The complete list of categories follows:Sea (scenes with great water presence), Sand (v.gr. beach or deserts), Snow (v.gr. people skying, snow boarding, etc.), Mountains, Green nature, Outdoor (scenes that are not indoor), Natural daylight, Indoor (note that this category is actually the opposite of “Outdoor”, so it is kind of redundant have both categories, outdoor and indoor, in our list. However, we decide to let the classifiers perform the classification into these two specific categories in order to be able to evaluate the behavior of the classifiers with these categories. Thus, this choice allows, for example, to know if it is easier to identify an outdoor or an indoor scene.), Night, Sunset (it also includes the sunrise), Artificial light, Party (people gathering together at an event, including grill parties with neighbors, birthday parties, etc.), Urban, Individual buildings (v.gr. recordings of cathedrals), Sports, Children, Small groups (small groups of people or individual persons. v.gr. filming friends and family.), Crowd, Voice only (when you can hear one or more persons speaking or screaming, but you do not see them or you see only their hands or back of the head, v.gr. driving or seminars), In public buildings (v.gr. train station, airport, museum, gym, etc.), At home, Performance (including in between shots of the audience. v.gr. on a stage, theater, dance, etc.), Restaurants (including bar, pub, both outdoors and indoors), In vehicle (only when the camera is in the motorized vehicle. v.gr. car, ship, airplane, train, bus, etc.).These categories are used to label frame by frame all the videos. It must be noted that a frame can be classified into more than one category (v.gr. Party and Artificial light).The training and the evaluation will be done using the personal video recordings repository provided by Sony Stuttgart Technology Center [40]. This repository contains 366 videos for a total of 8h 40min of recording time.Since the set of videos is limited, cross-validation[41] was used in training-evaluation in order to develop models as general as possible. The S-fold cross-validation scheme divides the available data into S groups. In our approach, we chose S=3. Then, S-1 groups are used for training and one group for testing. This procedure is repeated for all the S possible selections of the testing group.Most of the personal video recordings are in .avi format but there is also a set of video files in .mpg format. The image features are extracted at a sampling frequency of 5 fps. This choice is low enough so as not to increase the computational cost unnecessarily and, at the same time, it is high enough so as not to miss information for categorization; note that it is not realistic to consider a scene, v.gr. sports, shorter than 0.2s.GMM is a probabilistic method that has been widely used in classification tasks with success [1]. In the Gaussian Mixture Model classifier the probability density function (pdf) of each class is assumed to be a mixture of a specific number K of multidimensional Gaussian distributions. The iterative Expectation Maximization algorithm (EM)[41] is commonly used to estimate the parameters of each Gaussian component and the mixture weight.SVM is a discriminative method successfully used in a variety of pattern recognition tasks [42,43]. SVMs are built by mapping the input patterns into a higher dimensional feature space by using a nonlinear transformation (kernel function). Then optimal hyperplanes are built in the feature space as decision surfaces between classes.An SVM classifier finds the hyperplanes that separates the classes in the training data with a maximal margin.In our implementations, we make use of the LIBSVM library with MATLAB [44] and the parameter selection is done according to the recommendations in [44].The way in which the probabilities attained by the models are calculated differently depending on its type.•Gaussian Mixture Model: The model directly obtains the probabilities in the log-domain.Support Vector Machines: LIBSVM provides with the possibility of directly getting the probabilities [44].The inputs to this block are the probabilities of the positive and negative class for every frame, v.gr. the probability of the classifications or not, of a frame in a certain class. The classification method will use these probabilities in order to attain the best possible classification results.After evaluating several classification methods, the Viterbi path based smoothing scheme has been selected because it is the one that provides the best results. Viterbi path based smoothing finds the best state sequence for the sequence of observations by using a Viterbi state trellis [45].The evaluation is done by using thef-measure:(1)f-measure=2·Precision·RecallPrecision+Recallwhere:•Precision is the probability of classification of a sample in a given class if it actually belongs to that class.(2)Precision=tptp+fpwheretpstands for the number of true positives andfpfor the false positives.Recall is the probability of positively classifying a sample:(3)Recall=tptp+fnwherefnstands for the number of false negatives.Therefore,f-measureis the harmonic mean of precision and recall it represents the trade-off between precision and recall [46].The previous section showed the basic design of the classifier using a single kind of image or audio features. Now, we address the usage of the combination of both types of sources of information in order to extract complementary useful information from both of them for our classification task. Thus, we will combine the features from the audio and image sources. In this way, during the training phase, the model will extract cross-domain information that will help in the evaluation phase.For example, in a video scene representing a group of persons in a restaurant, the audio will provide with the necessary information to know that there are persons in the scene and the image will be able to provide with complementary information to know that the scene takes place in an indoor room (artificial light, walls with parallel edges, etc.). However, this approach has some potential problems that need to be observed: the model is not optimized for every specific source of information. The dimension of the feature vector can grow too large, leading to problems related to the curse of dimensionality [41]. The curse of dimensionality problem makes reference to the necessity of a large amount of training data for feature vectors with high dimensionality, since the number of variables increase. This relationship between the dimension of the feature vector and the observation space to be covered with the training data is exponential.Among the different options to combine the feature vectors studied in Section 2, the best results were attained using the concatenation of the color histogram, edge histogram and MFCCs.These three different features were accordingly synchronized and scaled. The dimension of the combined feature vector is 195. SVM was selected for the classification because of the very high dimensionality of the feature vector. The results are analyzed in Section 4.In the feature combination schema, a single model is used to train and perform the classification using both audio and image information sources. In our case, we chose SVM [42,43] for that task. This implies that we lose the capability of optimizing the classifier for every different type of information (v.gr. to use GMM [1] for audio and SVM [42,43] for image features). Precisely, the classifier combination technique is strong in this sense.By using this approach, we will select the best classifier for every type of information source and combine their results in order to perform the classification with the complete set of information sources.Therefore, the advantage of this approach is that it considers the optimum classifier per type of information source, additionally, it makes use of feature vectors of reduced dimensionality, diminishing the problem of the curse of dimensionality.Nevertheless, this approach presents some clear drawbacks because it is not possible to get the cross-domain information from the different information sources in the training phase. Different types of models imply different types of parameter optimization steps increasing the complexity of both the overall process and the algorithm used to combine the decisions from the different classifiers.Fig. 2shows the schema used in this paper for score combination.As shown in Fig. 2, the SVM model deals with the image features and the GMM employs audio features. The scores provided by both models are post-processed: the scores are normalized (since the outputs are in different formats, as we saw in Section 3.1.4) and and combined using weights optimized per class. So, the combination of scores is calculated by using the following expresion:(4)PC=α·PSVM+β·PGMMwhere α and β are the weights per class. In order to simplify the scheme, only one variable W is used, such that:α=Wandβ=(1-W). In this way,W=1means only image features are used in the classification (with a classifier based on SVM), whereasW=0means that only the audio features are used (with a classifier based on GMM).PSVMstands for the score vector, coming from the classifier based on SVM. These scores are normalized in mean and variance. Similarly,PGMMis the score vector for the classifier based on GMM.The calculation of the weigh, W, was done by evaluating the results with the available data using 3-fold cross-validation. We observed the performance with respect to the value of W from 0 to 1 in steps of 0.1. The best result was found withW=0.3.This approach shows better performance than using feature combination. The results found are analyzed in Section 4.Meta-Classification or Stacked Generalization [47] is a different way of combining multiple models. Although developed some years ago, it is less widely used than probability combining, majority voting or other combining schemes. This is so partly, because this approach is difficult to assess analytically and, also, because it is not generally accepted as the best choice.The main idea behind meta-classification is to represent the decision of every individual classifier as a feature vector and, then, perform a subsequent classification in the new feature space. The final decision is made by the meta-classifier of by linearly combining the judgments of each classifier.However, the advantage of meta-classification is two-fold. First, when combining multiple classifiers, the similarity score or probability obtained by each classifier does not necessarily convey all the information. The distribution of the scores of each class judged by the classifier reveals its degree of confidence in the decision, which is a characteristic that can be captured by a classification feature vector, unlike conventional combining schemes such as linear mixtures. Second, there may be some patterns across several classification vectors, which can be learned by a meta-classifier. For example, consider a scene in a very noisy environment, v.gr. outdoor, the audio classification stage can easily fail but the image classification step should perform well. Meta-classification can learn this pattern from synthesized feature vectors. Thus, the normal linear combination strategy will act unstably in this circumstance, whereas the meta-classifier, can make a better decision by observing the pattern of the results from multi-modal classifiers.So, we adopted this method for our problem making use of the decisions of the specialized classifiers described in Section 3.3. The general schema can be seen in Fig. 3. Our meta-classification scheme has two levels: Level 0 and Level 1. The levels will require independent and sequential training and evaluation. In Level 0, the specialized classifiers perform with the image and audio features, providing their scores. These scores are gathered and concatenated to form the “decision vector” which is used as input to the meta-classifier.The training schema becomes of special relevance because of the complexity of training the two classifier levels. Fig. 4shows the training and evaluation schema in our 3-fold-cross-validation selection.The notation used is as follows:•F1, F2, F3: Since we are using of 3-fold-cross-validation, the sample data was split into 3 sets of videos: F1, F2, F3.In Level 0, the features used in the classifier based on SVM are always the color histogram and the edge histogram. The features used in the classifier based on GMM are the MFCC+Δ+ΔΔ(see Fig. 3).Pxy-zMrepresents the score matrices given by the classifier identified by M, withM∈SVM,GMM. These are trained with the set of videosFx+Fyand evaluated usingFz.[FzPxy-zSVMPxy-zGMM], is the decision vector used by the meta-classifier (input to Level 1). It is obtained by the concatenation of the score vectorsPxy-zSVMandPxy-zGMM, corresponding to the evaluation of the set of videosFz.By using this combination method and the selected training approach, we have obtained interesting results that will be shown in Section 4. The good performance attained is held empirically by the advantages derived from the usage of the meta-classifiers: Possibility of using the specialized classifier for every source of information (SVM for the image features and GMM for the audio features), capacity to learn the goodness and badness of the specialized classifiers (the meta-classifier learns, from the training data, the cases in which every specialized classifier is more reliable) and capacity to learn cross-patterns built upon different types of information (image and audio) and use them together in order to attain the best classification performance.

@&#CONCLUSIONS@&#
