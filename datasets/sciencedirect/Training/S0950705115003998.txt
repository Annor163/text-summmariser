@&#MAIN-TITLE@&#
ODOC-ELM: Optimal decision outputs compensation-based extreme learning machine for classifying imbalanced data

@&#HIGHLIGHTS@&#
The reason of the damage caused by class imbalance for ELM is analyzed in theory.The influence factors about the performance of ELM on skewed data are investigated.An optimal decision outputs compensation-based ELM called ODOC-ELM is presented.Exploring prior data distributions helps improve quality of ELM.Statistical results indicate the superiority of the proposed ODOC-ELM algorithm.

@&#KEYPHRASES@&#
Extreme learning machine,Class imbalance learning,Decision outputs compensation,Prior data distributions,Optimization,

@&#ABSTRACT@&#
Extreme learning machine (ELM) has been one widely used learning paradigm to train single hidden layer feedforward network (SLFN). However, like many other classification algorithms, ELM may learn undesirable class boundaries from data with unbalanced classes. This paper first tries to analyze the reason of the damage caused by class imbalance for ELM, and then discusses the influence of several data distribution factors for the damage. Next, we present an optimal decision outputs compensation strategy to deal with the class imbalance problem in the context of ELM. Specifically, the outputs of the minority classes in ELM are properly compensated. For a binary-class problem, the compensation can be regarded as a single variable optimization problem, thus the golden section search algorithm is adopted to find the optimal compensation value. For a multi-class problem, the particle swarm optimization (PSO) algorithm is used to solve the multivariate optimization problem and to provide the optimal combination of compensations. Experimental results on lots of imbalanced data sets demonstrate the superiority of the proposed algorithm. Statistical results indicate that the proposed approach not only outperforms the original ELM, but also yields better or at least competitive results compared with several widely used and state-of-the-art class imbalance learning methods.

@&#INTRODUCTION@&#
Extreme learning machine (ELM), which is a new learning paradigm for training single hidden layer feedforward network (SLFN), has become popular for solving classification problem due to its high classification accuracy and light computational requirements. Different from the traditional back-propagation (BP) algorithm [1], ELM does not need to tune the parameters of the hidden layer iteratively, but assigns them randomly. Meanwhile, in ELM, the output weights of SLFN are calculated by making use of a least-square method [2–5]. Specifically, in contrast with those conventional training algorithms, ELM is generally more robust as it places emphasis on achieving both the small norm of output weights and the least training errors. It has been found that ELM often provides similar or better classification accuracy at a much faster learning speed than classifiers including BP neural network (BPNN), support vector machine (SVM) and least-square support vector machine (LS-SVM) [2–4]. In recent years, ELM has also been widely adopted in various real-world problems, such as face recognition [6], fashion retailing forecasting [7], sales forecasting [8,9], hyperspectral image classification [10], electricity price forecasting [11], wind power generation forecasting [12], and bioinformatics [13,14].Like other classifiers, on an imbalanced data set, ELM can produce an undesirable model that is biased toward the majority class and has a low performance on the minority class [15]. In other word, on imbalanced data, the decision boundary of ELM tends to be pushed towards the region of the minority class. Imbalanced data can be found almost everywhere, from biomedical applications [16] to network intrusion detection [17]. In recent years, a number of methods have been developed to deal with class imbalance problem, including resampling [18–23], cost-sensitive learning [24–26], decision boundary moving [27–29], ensemble learning [30–35], active learning [36] and one class classifier [37,38].In the context of ELM, however, only a few work focused on the class imbalance problem. Zong et al. [15] profited from the idea of cost-sensitive learning to present a weighted ELM classifier (WELM). By designating different penalty factors for the training errors belonging to different categories, the performance of the minority classes could be highlighted. Zhang and Ji [39] presented a similar method called FELM, which changes the distributions of penalty factors by inserting a fuzzy matrix. However, they failed to provide a unified design rule for the fuzzy matrix. Xia et al. [40] proposed a kernel clustering-based possibilistic fuzzy ELM (PFELM) algorithm to deal with class imbalance problem, and found it performs better than FSVM-CIL algorithm [41]. Li et al. [42] provided a solution by embedding WELMs into a modified Boosting framework to automatically update the weights of the training samples, obtaining improved classification performance. Vong et al. [43] adopted a modified random oversampling method named prior duplication to promote the recognition rate of the level of suspended particulate matter. Sun et al. [44] integrated synthetic minority oversampling technology (SMOTE) [18] into a multiple ELMs framework to improve the prediction of corporate life cycle. Mirza et al. [45] proposed an ensemble algorithm of subset online sequential ELM (ESOS-ELM) to implement incremental class imbalance learning. In particular, a change detection mechanism is used in ESOS-ELM algorithm to detect concept drifts.In this article, we first attempt to analyze the reason of the damage caused by class imbalance for ELM in theory. Also, we try to investigate the influence of several data distribution factors for the damage. Then, we would like to present an optimal decision output compensation-based ELM (ODOC-ELM) for promoting the classification performance of ELM in the scenario of class imbalance. ODOC-ELM can be regarded as a family member of decision boundary moving. In ODOC-ELM, the decision outputs of the minority classes are properly compensated based on the geometric mean (G-mean) performance metric, which pushes the decision boundary back towards the appropriate position of the majority class. For binary-class data, the problem of finding the optimal compensation value can be regarded as a single variable optimization problem, thus we adopt golden section search algorithm [46]. For multiclass data, there are multiple minority classes, thus it is a multivariate optimization problem. Particle swarm optimization (PSO) algorithm [47,48] is used to find the optimal combination of compensations. In addition, readers are also encouraged to replace them with other optimization algorithms in their practical applications.To present the superiority of the proposed ODOC-ELM algorithm, it has been compared with some popular and state-of-the-art class imbalance learning algorithms on 30 binary-class imbalanced data sets and 12 multiclass imbalanced data sets randomly acquired from the Keel data repository [49]. The statistical results indicated that ODOC-ELM not only outperforms the original ELM, but also yields better or at least competitive results compared with several other methods.The remainder of this paper is organized as follows. Section 2 provides some preliminary knowledge about ELM, including the basic ELM algorithm and three kinds of bias correction strategies in the context of ELM. Also, in Section 2, we also analyze the reason of the damage caused by class imbalance for ELM in theory, further investigate several data distribution factors which can influence on this damage. The proposed ODOC-ELM algorithm is described in Section 3 in detail. The experimental results and discussions are provided in Section 4. Finally, we conclude in Section 5.In supervised learning, the learning algorithms always use a finite number of input-output instances for training. Suppose there are N arbitrary distinct training instances (xi, ti) ∈ Rn× Rm, where xiis one n×1 input vector and tiis one m×1 target vector. If an SLFN with L hidden nodes can approximate these N samples with zero error, it then implies that there exist βi, aiand bi, such that:(1)fL(xj)=∑i=1LβiG(ai,bi,xj)=tj,j=1,...,Nwhere aiand bidenote the weight and bias of the ith hidden layer node, βiis the weight vector connecting the ith hidden node to the output nodes. Then Eq. (1) can be written compactly as:(2)Hβ=Twhere(3)H(a1,...,aL,b1,...,bL,x1,...,xN)=[G(a1,b1,x1)...G(aL,bL,x1)⋮...⋮G(a1,b1,xN)...G(aL,bL,xN)](4)β=[β1T⋮βLT]L×mandT=[t1T⋮tNT]N×mHere, G(ai, bi, xj) denotes the activation function which is used to calculate the output of the ith hidden node for the jth training instance. In ELM, many nonlinear activation functions can be used, including sigmoid, sine, hardlimit and radial basis functions [2,3]. H is called hidden layer output matrix of the network, where its ith column denotes the ith hidden node's output vector with respect to inputs x1, x2...xNand its jth row represents the output vector of the hidden layer with respect to input xj. Fig. 1provides the basic structure of a SLFN.In SLFN, if the number of hidden nodes, L, is less than the number of training samples, N, and if the data distribution is sufficiently complex, then the training error cannot be made exactly zero but can approach a nonzero training error ɛ. ELM differs from other training algorithms in that the hidden node parameters aiand bi are not tuned during training, but are instead assigned with random values according to any continuous sampling distribution[2,3]. Eq. (2) then becomes a linear system and the output weights β are estimated as:(5)β^=H†Twhere H† is the Moore–Penrose generalized inverse of the hidden layer output matrix H.H†=(HTH)−1HTif HTH is nonsingular orH†=HT(HHT)−1if HHTis nonsingular. Here,β^is the minimum-norm least squares solution of Eq. (2)[2].ELM can also be derived in a regularized form for improved train-test generalization. The norm of the output weights ||β|| is closely related with the generalization ability of a neural network [50], ELM can be derived in a form that tries to minimize both∥Hβ−T||2and ||β||2 simultaneously, in which case Eq. (2) is replaced [3] by:(6)Minimize:LpELM=12∥β||2+C12∑i=1N∥ɛi||2Subjectto:h(xi)β=ti−ɛiwhereɛi=[ɛi,1,…,ɛi,m]is the training error vector of the m output nodes corresponding to the training instance xi, C is the trade-off regularization parameter between the minimization of training errors and the maximization of the marginal distance, and h(xi) denotes the hidden layer output vector corresponding to the ith sample. The solution of Eq. (6) can be obtained based on the KKT theorem [51]. Given a new instance x, the output function of ELM is obtained [3] by:(7)f(x)={h(x)HT(IC+HHT)−1T,whenN<Lh(x)(IC+HTH)−1HTT,whenN≥Lwheref(x)=[f1(x),…,fm(x)]is the output vector. Then users may use the following equation to find out the predicted class label of x:(8)label(x)=argmaxifi(x),i∈{1,…,m}In this article, we adopt the regularized version of ELM which is described in Eq. (6).Classifiers that pursue the minimization of training errors are apt to be destroyed by imbalanced class distributions, including Naïve Bayes classifiers [52], K nearest neighbors (KNN) classifiers [53], multilayer perceptrons (MLP) [54] and support vector machines (SVM) [29]. The fact is that the training errors often appear in the overlapping region between two different categories, and the desired classification boundary is the center of the class overlapping region. In this region, however, the number of instances belonging to the majority class is much more than that of the minority class. To guarantee the minimization of training errors, the minority class has to sacrifice more than the majority class. In this article, we try to analyze the reason of the damage caused by class imbalance for ELM and investigates the influence of several data distribution factors for this damage.Without loss of generality, suppose the classification task is binary, and the target outputs of the minority class and majority class are assigned as 1 and −1, respectively. Consider a small and compact boundary region, there are N majority class instances and 1 minority class sample, where N is much more than 1. The words “small and compact” mean that all the instances in the described region have quite similar inputs, and the imbalance ratio (IR) is N: 1. Let's describe the feature vector of the minority class instance as x0 = (x01, x02,…, x0n), and the feature vectors of the majority class instances as xi= (x01 + Δxi1, x02 + Δxi2,…, x0n+ Δxin), wherei∈{1,2,…,N}and Δxijdenotes a small either positive or negative deviation for the jth feature component of the ith instance in comparison with that of the instance x0. Also, we use Δxi= (Δxi1, Δxi2,…, Δxin) to denote the deviation of the feature vector of the ith instance compared with the minority instance x0. Then based on Eq. (1), the outputs of these instances can be represented as:(9)f(xj)={∑i=1LβiG(ai,bi,x0),ifj=0∑i=1LβiG(ai,bi,x0+Δxj),ifj=1,…,NBased on Eq. (9), we can obtain Δf(xj) which is the variation of output between the jth majority instance xjand the minority instance x0 as follows:(10)Δf(xj)=∑i=1LβiG(ai,bi,x0+Δxj)−∑i=1LβiG(ai,bi,x0)=∑i=1Lβi(G(ai,bi,x0+Δxj)−G(ai,bi,x0))Here, it is not difficult to observe that when the activation function G is a continuous function, such as sigmoid or rbf, meanwhile the deviation Δxj, the norm of hidden layer parameters ||a||, ||b|| and the norm of output weights ||β|| are all small enough, Δf(xj) is a quite small real number, either positive or negative. In ELM, the hidden layer parameters are previously assigned with random values according to any continuous sampling distribution [2,3], while according to Eq. (6), ||β|| would reduce with the decrease of the trade-off regularization parameter C. That means in ELM, the sampling distribution and regularization parameter can be chosen to make two closely adjacent instances have quite similar outputs.Suppose β has been determined, then the mean squared training errors of the subset Qsubcan be represented as:(11)Qsub=(f(x0)−1)2+∑i=1N(f(x0)+Δf(xi)−(−1))2=(f(x0)−1)2+∑i=1N(f(x0)+Δf(xi)+1)2=f(x0)2−2f(x0)+1+∑i=1N(f(x0)2+2f(x0)(Δf(xi)+1)+Δf(xi)2+2Δf(xi)+1)To minimize the training errors for the subset, we set the quantity to zero:(12)∂Qsub∂f(x0)=2f(x0)−2+∑i=1N(2f(x0)+2(Δf(xi)+1))=(2N+2)f(x0)+2∑i=1NΔf(xi)+2N−2That means the solution for the sub-optimization problem is:(13)f(x0)=1−N−∑i=1NΔf(xi)N+1If∑i=1NΔf(xi)is small enough, f(x0) tends to output a negative value. Moreover, we found that with the increase of the imbalance ratio N, the output value gradually approaches to −1. Therefore, it is not difficult to understand why the classification boundary is usually pushed towards the region of the minority class in ELM.Next, we wish to investigate the influence of several data distribution factors for ELM. According to Liu et al. [31] and our previous work [52], not all imbalanced classification tasks are harmful, and for those unharmful imbalanced classification tasks, adopting specific class imbalance learning methods might increase unnecessary temporal and/or spatial costs. In fact, the damage is related to multiple potential data distribution factors, including class overlap, imbalance ratio, the size of training instances, noisy data and small disjunctions [52,55–57]. Here, we investigate the influences of the first three factors for ELM.Suppose there is a two-dimensional binary-class data set, and both classes satisfy different Gaussian distributions. Fig. 2presents the classification boundaries trained by ELM (L = 10, C = 10, sigmoidal G function) with different class overlap proportions, imbalance ratios and the total number of training instances. Fig. 2(a1) shows that when there is a large margin between two classes’ instances, ELM is not damaged by imbalanced class distributions. When class overlap proportion increases, the classification boundary gradually move towards the region of the minority class (see Fig. 2(a2) and (a3)), eventually reaching a situation in which all minority instances are misclassified (Fig. 2(a4)). Fig. 2(b1)–(b4) shows that the higher the imbalance ratio is, the more severe the damage is. If the data set is approximately balanced, there exist almost equal number of instances for both classes in the overlapping region, therefore the risk of misclassification is shared almost equally by both classes. The number of training instances is another important impact factor for ELM. Fig. 2(c1)–(c4) show that an increase in the number of training instances is helpful for improving the recognition accuracy of the minority class.Besides the three factors shown in Fig. 2, if there exists some noisy information and small disjunctions, the quality of the trained ELM model would further decline. Therefore, for ELM, the damage of class imbalance can be caused by a combination of multiple factors. The appropriate methods and parameters should be determined by investigating the actual data distributions.Based on Eq. (13), we analyze three kinds of bias correction strategies for ELM in the scenario of class imbalance.Resampling is a family of algorithms which is widely used to solve the class imbalance problem. As its name indicates, resampling either increases some instances for the minority class or takes away some samples from the majority class. The former is called oversampling, while the latter is called undersampling. The simplest resampling strategies are random oversampling (ROS) and random undersampling (RUS). ROS adds instances by randomly duplicating the existing minority samples, thus it often overfits the data. RUS randomly removes some majority instances to re-balance the training set, causing some information loss. To overcome the drawbacks of ROS and/or RUS, more complicated resampling strategies have been developed. Synthetic minority oversampling technique (SMOTE), proposed by Chawla et al. [18], is the most widely used resampling strategy. Different from the instance duplication used in ROS, SMOTE generates each synthetic minority instance by interpolating between two existing minority samples. Besides RUS, ROS and SMOTE, other resampling strategies include random walk over-sampling (RWOS) [20], MLSMOTE [21], clustering-based undersampling [58] etc.Resampling has little dependence on the used classifier, as it occurs in the procedure of data preprocessing. Therefore, any resampling strategy can be combined with ELM to deal with a class imbalance problem. After resampling, the distribution of the original boundary instances changes, which causes N in Eq. (13) to approach 1, thus the risk of the misclassification is shared equally by both classes.Weighted extreme learning machine (WELM) proposed by Zong et al. [15] is also an effective method to cope with the problem of imbalanced data classification. WELM can be regarded as a cost-sensitive learning method, as it offers a larger penalty to the training errors of the minority instances than to those of the majority ones. In WELM, Eq. (6) is rewritten as:(14)Minimize:LDELM=12∥β||2+C12∑i=1N(Wii×∥ɛi||2)Subjectto:h(xi)β=ti−ɛiwhere W is an N×N diagonal matrix associated with each training instance xi. Usually if xicomes from the minority class, the associated weight Wiiis relatively larger than those corresponding to the instances from the majority class. WELM's solution to the problem can be understood by supposing that the weight of the minority class is M times more than that of the majority class, then Eq. (11) should be rewritten as:(15)Qsub=M(f(x0)−1)2+∑i=1N(f(x0)+Δf(xi)−(−1))2=M(f(x0)−1)2+∑i=1N(f(x0)+Δf(xi)+1)2=Mf(x0)2−2Mf(x0)+M+∑i=1N(f(x0)2+2f(x0)(Δf(xi)+1)+Δf(xi)2+2Δf(xi)+1)To minimize the weighted training errors, we have:(16)∂Qsub∂f(x0)=2Mf(x0)−2M+∑i=1N(2f(x0)+2(Δf(xi)+1))=(2N+2M)f(x0)+2∑i=1NΔf(xi)+2N−2MTo make the quantity as zero, then the solution of Eq. (16) is:(17)f(x0)=M−N−∑i=1NΔf(xi)N+MObviously, when the weight ratio M approximates the imbalance ratio N, the minority instance x0 tends to appear near the classification boundary. Therefore, WELM can effectively solve the class imbalance problem in the context of ELM. In the reference [15], Zong et al. provides two weighted strategies as follows:(18)WELM1:Wii=1/#(ti)and(19)WELM2:Wii={0.618/#(ti)if#(ti)>AVG(ti)1/#(ti)if#(ti)≤AVG(ti)where #(ti) and AVG(ti) denote the number of samples belonging to class tiand the average number of examples for each class, respectively.Besides resampling and weighting, decision boundary moving [27–29] is also an effective method to deal with class imbalance. As described above, a skewed instance distribution tends to push the classification boundary towards the minority class, thus the aim of decision boundary moving is to push the boundary back towards the appropriate position. Some previous work has explored its effectiveness and feasibility in the context of MLP and SVM. Zhou and Liu [27] proposed a method named threshold moving that first adopts 0–1 outputs to train MLP, then normalizes the sum of outputs as 1, and finally multiplies by the corresponding cost for the output of each class. They found that the threshold moving is resource-saving and relatively efficient in class imbalance tasks. Lin and Chen [28] considered boundary moving for SVM classifier, and presented a feasible computational formula which considers the class imbalance ratio in the training set. The method, however, often produces relatively lower classification performance than other learning strategies. Therefore, in our previous work, we have proposed an iterative optimization algorithm to automatically find the optimal position for the hyperplane of SVM [29]. Specifically, the algorithm satisfies both the needs of accuracy and generalization.For ELM, boundary moving means adding a small positive real value ζ to the decision output of the minority class. Then Eq. (13) can be rewritten as:(20)f(x0)=1−N−∑i=1NΔf(xi)N+1+ζIf the compensated threshold is large enough, the output of the minority instance x0 becomes positive. Actually, the purpose of decision output compensation is to push the classification boundary back towards the majority class.In this article, we focused on developing decision boundary moving strategy to deal with class imbalance problem in the context of ELM. According to Eq. (20), we know that in order to determine the optimal classification boundary, we need to find the optimal compensation threshold ζ. Optimality can be measured using the G-mean measure, which is computed by finding the largest G-mean value on the training set. G-mean is useful because, for imbalanced classification tasks, overall accuracy is not an appropriate performance measure any more. F-measure and G-mean can better represent the desired classification behavior than overall accuracy. F-measure and G-mean are functions of the confusion matrix as shown in Table 1.They are calculated as follows:(21)F−measure=2×Precision×RecallPrecision+Recall(22)G−mean=TPR×TNRwhere Precision, Recall, TPR and TNR are further defined as:(23)Precision=TPTP+FP(24)Recall=TPR=TPTP+FN(25)TNR=TNTN+FPAs shown in Eq. (22), G-mean is the geometric mean of recall score of the minority class and that of the majority class, and therefore, unlike overall accuracy, it cannot be high when the minority-class recall rate is low.Returning to our discussions on setting a proper value of ζ, for a binary-class problem, if we gradually increase the value of ζ from 0 to a pre-assigned upper value, and use G-mean metric to assess their performances, the curve will tend to be U-shaped as follows: it first gradually rises until the peak value ζ*, and then drops to a relatively small value again. The problem is an unconstrained optimization problem of single variable ζ. To rapidly find ζ* which is the optimal value of ζ, we adopt the golden section search algorithm [46] for the optimization process. In addition, considering the target output is either 1 or −1, the value range is pre-assigned between 0 and 2. Fig. 3shows an example using the abalone19 data set acquire from the Keel data repository [49] to present the variation of G-mean with the increase of compensated threshold on the training set (randomly extracted 80% instances) and testing set (the remaining 20% instances), respectively. Although there are some differences between the two curves, a nearly identical position of the peak value is observed. That means the optimal compensation threshold ζ* obtained from the training set produces approximately optimal G-mean value on the testing set, too.For a multi-class imbalance problem, the problem becomes more complex [59,60]. Except the class with the most instances, the outputs of all other classes need to be compensated by different thresholds. Therefore, it becomes a multivariate optimization problem. To solve this optimization problem, particle swarm optimization (PSO) algorithm [47,48] is adopted. PSO is a population-based stochastic optimization technique, inspired by the social behavior of bird flocking. Specifically, PSO is originally designed to address continuous optimization problem, thus it can be directly used to deal with our problem without any modifications. During the optimization process of PSO, each particle dynamically changes its position and velocity by recalling its historical optimal position (pbest) and observing the position of the optimal particle (gbest). On each round, the position of each particle is updated by:(26){vidk+1=vidk+c1×r1×(pbest−xidk)+c2×r2×(gbest−xidk)xidk+1=xidk+vidk+1wherevidkandvidk+1represent the velocities of the dth dimension of the ith particle in the kth round and the (k+1)st round, whilexidkandxidk+1denote their positions, respectively. c1 and c2 are two nonnegative constants that are called acceleration factors, while r1 and r2 are two random variables in the range of [0, 1]. In this study, the size of particle swarm and the search times are both set as 50, as well c1 and c2 are both set to 1. Meanwhile, the position x is restricted in the range of [0, 2] and the velocity v is restricted between −1 and 1. Fig. 4shows an example of the PSO evolution curve for a three-class data set balance acquired from the Keel data repository [49], where fitness denotes the G-mean metric. From Fig. 4, it can be observed that PSO helps find an approximately optimal combination of compensated thresholds in a limited search time.Fig. 5uses the balance data set as an example to present the variation of G-mean with the variation of the two compensated thresholds on the training set (randomly extracted 80% of instances) and testing set (the remaining 20% of instances), respectively (see Fig. 5). From Fig. 5, it is not difficult to observe that on both sets, the peak values emerge at similar positions, too.According to what mentioned above, we provide a description for the procedure of the proposed ODOC-ELM algorithm as follows:Input: The training set S, the testing set T, the activation function G, the number of hidden nodes L, trade-off regularization parameter C.Output: The optimal compensated threshold(s), G-mean value on the testing set T.Procedure:1.Generate hidden-layer weights and biases randomly, and then train an ELM classifier on the training set S using Eq. (6);Obtain the outputs of all training instances;Adopt the golden section search algorithm for a binary-class problem and the PSO algorithm for a multiclass problem to find the optimal compensated threshold or combination of compensated thresholds, which corresponds to the largest G-mean value on S;Record the optimal compensated threshold ζ* for a binary-class problem or the optimal combination of compensated thresholds (ζ1*, ζ2*,…, ζm−1*) for a multiclass problem, where m denotes the number of classes;Get the actual outputs of all samples on the testing set T using the trained ELM;Compensate the outputs of instances belonging to the minority class(es) using the optimal compensated threshold(s) recoded in step 4;Calculate and outputting G-mean value on the testing set T.To present the effectiveness of ODOC-ELM algorithm and its differences with several popular class imbalance learning algorithms, we construct a binary-class artificial data set to show the classification boundaries of various algorithms. On the data set, the minority class has 100 instances which satisfy the Gaussian distribution with mean value μ+ = 0.7 and the variance σ = 0.3, while the majority class owns 1000 instances satisfying the Gaussian distribution with mean value μ- = 0.3 and the variance σ = 0.3 (see Fig. 6(a)). Fig. 6(b)–(h) present the classification boundaries trained by seven different algorithms, including ELM, WELM1, WELM2, RUS-ELM, ROS-ELM, SMOTE-ELM and the proposed ODOC-ELM. From Fig. 6, it is not difficult to observe that ELM can be severely destroyed by class imbalance, while all six other classification algorithms can effectively alleviate class imbalance. Different from other algorithms, ODOC-ELM neither undersamples, oversamples, nor re-weights the training set, thus if there are enough training instances, and no concept drift between training set and testing set, then ODOC-ELM can find a better classification boundary than the other algorithms.

@&#CONCLUSIONS@&#
Extreme learning machine is one of the most popular learning paradigms in the machine learning field. However, it is sensitive to class imbalance distributions. This article has interpreted imbalance sensitivity in theory and reviewed some widely adopted solutions. Furthermore, we presented a decision output compensation algorithm called ODOC-ELM, which embeds an optimization procedure within itself. The optimal compensation thresholds are automatically found by the golden section search algorithm for binary-class problems, and by particle swarm optimization for multiclass problems. Of course, they can also be simply replaced by some other optimization algorithms. Experimental results indicate that ODOC-ELM outperforms many popular and state-of-the-art algorithms in terms of both F-measure and G-mean performance metrics. It is expected to be adopted in practical applications with class imbalance distributions in the near future.The following research topics deserve further investigation:1.As discussed in Section 4, the proposed ODOC-ELM algorithm is time-consuming, thus we also wish to improve the algorithm for further saving the time consumption.When there are only a few training instances, ODOC-ELM cannot precisely estimate the real distributions, consequently causing the classifier to be overfitting. In this scenario, perhaps combining the results of several different class imbalance algorithms can further improve the classification performance.One-class classifiers have been widely used to address class imbalance problem [37,38], so it may be worthwhile to investigate the possibility of adapting ELM as one-class classifier.