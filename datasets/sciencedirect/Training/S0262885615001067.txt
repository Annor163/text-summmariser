@&#MAIN-TITLE@&#
Skewed stereo time-of-flight camera for translucent object imaging

@&#HIGHLIGHTS@&#
A new skewed stereo ToF camera for detecting and imaging translucent objects under minimal prior of environmentTranslucent region detection methodTranslucent region recovery methodExtensive analysis and evaluations

@&#KEYPHRASES@&#
Translucent object imaging,ToF depth camera,Three-dimensional image processing,

@&#ABSTRACT@&#
Time-of-flight (ToF) depth cameras have widely been used in many applications such as 3D imaging, 3D reconstruction, human interaction and robot navigation. However, conventional depth cameras are incapable of imaging a translucent object which occupies a substantial portion of a real world scene. Such a limitation prohibits realistic imaging using depth cameras. In this work, we propose a new skewed stereo ToF camera for detecting and imaging translucent objects under minimal prior of environment. We find that the depth calculation of a ToF camera with a translucent object presents a systematic distortion due to the superposed reflected light ray observation from multiple surfaces. We propose to use a stereo ToF camera setup and derive a generalized depth imaging formulation for translucent objects. Distorted depth value is refined using an iterative optimization. Experimental evaluation shows that our proposed method reasonably recovers the depth image of translucent objects.

@&#INTRODUCTION@&#
Due to the ability of direct three dimensional geometry acquisitions, consumer depth cameras have widely applied to many applications such as 3D reconstruction, human interaction, mixed reality and robotics. However, depth images recorded by ToF depth cameras present limited accuracy because 1) commercial active sensors have fairly limited infrared emission power and 2) their performance varies upon the characteristics of surface material such as reflectivity and translucency inherited by the ToF sensing principle. Many postprocessing algorithms for 3D imaging have been proposed under the Lambertian assumption of matte surface. Depth cameras also work under the Lambertian assumption. In particular, depth cameras are infeasible to detect and image translucent objects while they occupy a substantial portion of a real world scene. In order to achieve an accurate interaction and realistic imaging, it is critical to handle the translucent object.Many researchers have tried to detect transparent and translucent object region [7] using conventional color cameras [9], laser beams [8] or based on a prior shape knowledge [4]. This is particularly important for navigating a robot or vehicle because collision detection for a transparent object is of substance in real situations [10]. McHenry et al. [12] and McHenry and Ponce [11] consider distinct properties like the texture distortion and specularity of transparent object in color image to characterize the transparency. Lysenkov et al. [4] require a prior knowledge on the shape of transparent object. During the training, they obtain a 3D model of the object and try to match the model with the captured depth image. However, having such a shape prior is often unrealistic and it is hard to extend this idea to general applications. Wang et al. [5] detect the transparent region from aligned depth and color distortion. Phillips et al. [6] use stereo color cameras to segment transparent object regions at each color image. Murase [20][21] introduce a method for water surface reconstruction. He places a pattern at the bottom of a water tank and captures the water surface using a fixed color camera. If the water surface changes over time, the captured pattern at the bottom also will be distorted. Given the camera position, the water surface normal is estimated at each frame reconstructing the shape of the water surface. Similarly, Kutulakos and Steger [17][18] use a piece-wise light triangulation method to recover the refractive surface with a known refractive index and a pattern. Recently, Morris and Kutulakos [19] developed a stereo imaging system using a known background pattern and reconstructed dynamic liquid surface without refractive index. Inoshita et al. [16] assume a homogeneous surface with known refractive index and explicitly calculate the height of a translucent object surface in the presence of inter-reflection. Meriaudeau et al. [13] introduce new imaging architecture for transparent object reconstruction such as shape from polarization using the IR lights.Some researchers have used multiple depth cameras for shape recovery. They collect transparent object silhouettes from multiple viewpoints and apply a visual hull for reconstruction. Klank et al. [2] assume a planar background and take multiple IR intensity images of a transparent object using a ToF depth camera. Since each shadow on the planar background represents the silhouette of the transparent object corresponding to one camera, the volume of the object is estimated by a visual hull. Similarly, Albrecht and Marsland [1] use shadows on a planar background and take multiple depth images to collect the shadow regions of transparent or highly reflective objects at different viewpoints. Alt et al. [3] propose a similar framework using multiple depth images from moving cameras and show rough reconstruction results. In these frameworks, the number of viewpoints significantly affects the quality of volume recovery and it is only applicable to a static scene. Kadambi et al. [14] propose a coded time-of-flight camera and perform sparse deconvolution of modulated lights to separate multi path from reflected signals. The depth of transparent object is recovered by extracting a reflected light path from a transparent object's surface. These previous methods work in special background conditions or require a lot of cameras to obtain reasonable reconstruction performance. Translucent object reconstruction without environmental constraints using practical imaging setup still remains a challenging problem.In this paper, we propose a new approach to detecting a translucent object and recovering its distorted depth using a skewed stereo ToF depth camera. Our approach is implemented with a pair of commercial depth cameras without any assumption except that observed scene is composed of two layers: single foreground and background. Klank et al. [2] show that ray intersections observed by multiple color cameras reveal the rough location of original depth. We are inspired by this preliminary idea and generalize it for imaging arbitrary translucent surfaces with detailed shape. In fact, translucent surfaces present the systematic depth distortion which interferes to employ traditional stereo matching scheme directly. In order to account for depth distortions in translucent surfaces, we develop a new framework of modeling these systematic depth distortions based on the understanding of sensing principle and empirical study. The proposed algorithm consists of two stages. Utilizing the behavior of depth distortion on translucent surfaces, we first detect the translucent surface region. To process the translucent regions, we formulate an energy function for our depth optimization so as to recover the depth of translucent surfaces (Sec. 3.2). The optimization process is constrained by three cost terms: a modified stereo depth matching cost for accounting systematic depth distortions, a regularization cost for noise elimination and a depth topology cost for shape recovery of target object. Especially, the topology term is iteratively updated by the IR intensity observation (e.g., a texture image of ToF depth sensor) and the analysis of ToF principle in translucent objects (Sec. 2). As a result, we can recover the original shape of the translucent object (Sec. 3.3). The contributions of the proposed work include (1) a generalized framework of detecting a translucent object and recovering its distorted depth in real-time, without minimal prior knowledge and (2) a thorough analysis of the ToF principle on translucent surface for the recovery of detailed shape in translucent surface.A time-of-flight camera emits the IR signal with a fixed wavelength and measures the traveling time from a target object to the camera. Knowing the speed of light and its traveling time, it is easy to derive its traveling distance. For the practical implementation, the phase delay of the reflected IR from the emitted IR is used to alternate the traveling time. For commercial depth cameras, the phase delay is defined by the relation between N different electric charge values, collected in different time slots. In this paper, we choose the case of N=4 and Fig. 1(a) illustrates the example of depth calculation using four electric charges. Considering the emitted IR signal as a square wave, we can derive the depth D as follows:(1)D=c2tan−1AQ3−Q4AQ1−Q2=c2tan−1Q3−Q4Q1−Q2,A is the amplitude of reflected IR, Q1~Q4 are the normalized electric charges and c represents the speed of light. Note that the amplitude A varies along the distance and the albedo of the surfaces. From Fig. 1, we know that |Q1−Q2|+|Q3−Q4|=K and Q1+Q2=Q3+Q4=K, where K is a constant. This principle, however, becomes invalid for a translucent object because it assumes a single surface producing a single reflected light path.On the translucent surface, a subset of incident rays is reflected and the other penetrates through the object media upon its translucency. We consider that our target scene is composed of translucent foreground and opaque background, namely two-layer surface model as seen in Fig. 2. Layer 1 is the translucent foreground and Layer 2 is the opaque background where τ (0≤τ≤1) is translucency. Observed IR signal has two components: reflected IR from Layer 1 ((1−τ)I) and reflected IR from Layer 2 (τ2ρ2I) as shown in Fig. 2.Then, the reflected IR is the superposition of two different IR signals. Under this circumstance, the single path assumption in Eq. (1) is invalid and the resulting depth is incorrect. Under this two-layer translucent model, Eq. (1) ought to be rewritten as:(2)Dtr=c2tan−11−τAfQf3−Qf4+τ2AbQb3−Qb41−τAfQf1−Qf2+τ2AbQb1−Qb2,where τ is a normalized translucency (0≤τ≤1), where τ=1 means that the object is perfectly transparent. f and b denote foreground and background respectively. This modified formulation explains that the ToF camera observes the overlapped IR signals reflected from foreground and background objects. In this case, the depth value is determined by translucency (τ) and two amplitudes (Afand Ab) of reflected IR signals arrived at the sensor. Unlike Eq. (1), the IR amplitude terms Afand Abare not eliminated in the modified depth formula. Note that the IR amplitude Afand Abvary upon the traveling distance and the surface texture of target object. This introduces a new and critical depth distortion: for example, the objects at the same distance but at different amplitudes present different depth values.Fig. 3illustrates the depth distortions affected by the background texture behind a translucent object. Fig. 3(a) shows both the IR intensity and depth map of a normal opaque object. The IR amplitude varies along the texture on the object while the calculated depth shows identical distance from the camera because the IR amplitude does not affect the depth calculation as shown in Eq. (1). In Fig. 3(b), however, we put a translucent object in between the opaque pattern object and depth camera. Now Eq. (1) is not valid anymore and we have to use a multi-layer object model like our two layer model in Eq. (2) to analyze the overlapped IR light rays. First of all, the depth of foreground object with high translucency cannot be correctly measured because most of emitted IR light rays penetrate the object and fly to the background (e.g., a textured object) Furthermore, the IR amplitude of the background object affects the depth calculation in this case. We are able to see that the different IR amplitudes of the background object come to get a different foreground depth value in Fig. 3(b). In Fig. 3(c), we move the translucent object toward the depth camera. Because the traveling distance of reflected light from the translucent foreground is decreased, the attenuation of corresponding IR amplitude is also decreased. As a result, Afbecomes more dominant in Eq. (2), especially in the region of darker background. Therefore, the calculated depth becomes closer to the real translucent foreground object location (becomes darker blue in Fig. 3(c)). However, the region of brighter background keeps its background depth value, where Afis relatively less dominant due to higher Ab. We can see that the foreground depth error in darker background decreases when we move the translucent object closer to the camera (from (b) to (c) in Fig. 3). If we can control the background and decrease Ab, foreground terms will be more dominant and the calculated depth will be closer to the ground truth foreground depth value. However, we have no such control or knowledge of the background object in real situations and we need a generalized solution for translucent object imaging.We observe a similar effect with translucency (τ) variation that also changes the dominance of Afand Abin Eq. (2). This experiment shows that the previous depth calculation fails to obtain the correct depth of the translucent object. We have verified that our new Eq. (2) for the two layer model can explain what happens with a translucent foreground object in front of opaque background in real situations.Another interesting behavior of the depth of translucent objects is depth reversal due to its miscalculated depth value. In other words, the miscalculated depth following Eq. (2) does not always lie in between the ground truth depth of the translucent foreground and opaque background. Under some conditions, the miscalculated depth can be farther or closer than both the foreground and the background (Fig. 4). Fig. 4 shows two sample depth images of a translucent object. In these two cases, the translucent foreground object distance from the camera has been changed with a fixed background object located at the maximum operating range (5m in this camera). In Fig. 4(b), the foreground object is located 2.5m away from the camera and the observed depth of the translucent objects is pushed toward the background object. In the case of Fig. 4(c), however, the foreground object is located at around 1m away from the camera and the observed depth of the translucent objects is pulled toward the camera that is closer than both the foreground and the background. This situation easily can be explained mathematically using Eq. (2). Based on this two layer model, we try to find if there is any case where the following condition is satisfied.(3)c2tan−11−τAfQf3−Qf4+τ2AbQb3−Qb41−τAfQf1−Qf2+τ2AbQb1−Qb2−c2tan−1Qf3−Qf4Qf1−Qf2>0Let us consider a special case when 1−τ=τ2 and we replace Ab(Qb1−Qb2)=B1, Ab(Qb3−Qb4)=B2, Af(Qf1−Qf2)=F1 and Af(Qf3−Qf4)=F2. We rewrite the condition as follows:(4)tan−1F1B2−F2B1F1F1+B1+F2F2+B2>0Fig. 1(b) shows four possible cases of (Q1−Q2) and (Q3−Q4) status. We easily can find examples satisfying this condition. For example, when our background is located at the maximum operating range of the ToF camera (in the fourth quarter) and the foreground in the first quarter in Fig. 1(b), B1>0, B2≈0, F1>0 and F2>0 satisfying Eq. (4). As a result, we will observe the situation like Fig. 4(c). In this work, we adopt the two layer translucent model assuming that there is only one media change in the translucent object. The study of ToF principle in this section will be employed in Sec. 3.3 for the recovery of original shape of a translucent object.

@&#CONCLUSIONS@&#
